[
  {
    "title": "VisionLLaMA: A Unified LLaMA Interface for Vision Tasks",
    "link": "https://arxiv.org/pdf/2403.00522.pdf",
    "upvote": "38",
    "text": "VisionLLaMA: A Unified LLaMA Interface for Vision Tasks\nXiangxiang Chu1,\nJianlin Su3,\nBo Zhang1,\nChunhua Shen2\n1 Meituan Inc.\n2 Zhejiang University, China\n3 Moonshot AI, China\nFigure 1. Generated image samples by our DiT-LLaMA-XL of resolution (256, 256) with a CFG ratio of 4.0. Best viewed on screen.\nAbstract\nLarge language models are built on top of a transformer-\nbased architecture to process textual inputs.\nFor exam-\nple, the LLaMA family of models stands out among many\nopen-source implementations. Can the same transformer\nbe used to process 2D images? In this paper, we answer\nthis question by unveiling a LLaMA-like vision transformer\nin plain and pyramid forms, termed VisionLLaMA, which\nis tailored for this purpose. VisionLLaMA is a unified and\ngeneric modeling framework for solving most vision tasks.\nWe extensively evaluate its effectiveness using typical pre-\ntraining paradigms in a good portion of downstream tasks\nof image perception and especially image generation. In\nmany cases, VisionLLaMA have exhibited substantial gains\nover the previous state-of-the-art vision transformers. We\nbelieve that VisionLLaMA can serve as a strong new base-\nline model for vision generation and understanding. Our\ncode will be released at https://github.com/Mei\ntuan-AutoML/VisionLLaMA.\n1. Introduction\nLarge language models have aroused great interest in the\nresearch community. One of the most influential and rep-\nresentative work is LLaMA [66, 67]. Many recent works\nhave converged to this architecture and solutions for var-\nious applications are built upon the open-sourced models.\nBesides, we have witnessed the blooming of multimodal\nmodels, where many methods also heavily rely on LLaMA\nfor text processing and CLIP-fashioned [51] vision trans-\nformers [22] for visual perception. Meanwhile, many en-\ndeavors [23, 38, 73] have been devoted to accelerating the\ninference speed and/or the memory cost of LLaMA. In a\nword, LLaMA is now the de facto architecture.\nObserving its success, a straightforward and interesting\n1\narXiv:2403.00522v1  [cs.CV]  1 Mar 2024\nquestion is whether the LLaMA architecture can be another\nvictory in the vision modality. If the answer is affirmative,\nthen both vision and language models can use the same uni-\nfied architecture and enjoy various deployment techniques\ndesigned for LLaMA on the fly. Unfortunately, it is non-\ntrivial to answer this question because there are some dis-\ntinct differences between these two modalities. Firstly, it is\ncommon sense that text sequences are organized into one\ndimension, while vision requires two or more. Secondly,\nnumerous vision tasks rely on pyramid backbones to per-\nform better, while the LLaMA is a plain encoder. Thirdly, it\nis necessary to handle input images and videos with differ-\nent resolutions. Our paper aims to resolve these difficulties\nand bridge the architectural gap between different modali-\nties. Our main contributions are summarized as follows:\n1. We propose VisionLLaMA, a vision transformer archi-\ntecture similar to LLaMA to reduce the architectural\ndifferences between language and vision.\n2. We investigate means to adapt VisionLLaMA to tackle\ncommon vision tasks, including image comprehension\nand creation (Figure 1). We examine two well-known\nvision architecture schemes (plain and pyramid) and\nassess their performance under supervised and self-\nsupervised learning scenarios. Additionally, we intro-\nduce AS2DRoPE (i.e. auto-scaled 2D RoPE), which\nexpands rotated positional encoding from 1D to 2D\nand utilizes interpolation scaling to accommodate ar-\nbitrary resolutions.\n3. Without bells and whistles, VisionLLaMA signifi-\ncantly outperforms the widespread and carefully fine-\ntuned vision transformer by clear margins across many\nrepresentative tasks such as image generation, clas-\nsification, semantic segmentation, and object detec-\ntion.\nExtensive experiments indicate that VisionL-\nLaMA demonstrates faster convergence speed and bet-\nter performance than existing vision transformers.\n2. Related Work\nVision Transformer. ViT [22] successfully applied Trans-\nformer [68] from natural language processing to the vi-\nsion world and many more efficient and powerful follow-up\nworks are induced, like DeiT [65], Swin [43], PVT [70],\nand Twins [12]. The pre-training paradigm has been shifted\nfrom supervised learning on large-scale categorically la-\nbeled datasets like ImageNet [19] to unsupervised learn-\ning [25], and to contrastive learning on huge amounts of\nimage-text pairs as in CLIP [51]. DiT [50] adopts a trans-\nformer that operates on latent patches for diffusion mod-\nels [28,60], outperforming the commonly used U-Net back-\nbone [54].\nLarge Language/Multi-modal Models Proprietary mod-\nels like GPT4 [48] have been taking the lead in the LLM\ncompetition, though their technical details are hidden from\nthe public. In contrast, the community has blossomed to re-\nlease a myriad of open-source counterparts. For instance,\nBLOOM [57] and LLaMA [66] catch up with the perfor-\nmance of the closed model GPT-3 [6]. Later in copious de-\ntail, LLaMA-2 [67] describes a pack of architectural tweak-\nings including pre-normalization called RMSNorm [80],\nthe activation function SwiGLU [59], rotary positional\nembeddings RoPE [62], as well as a dedicated training\npipeline, which comprises self-supervised pre-training and\nsupervised fine-tuning enhanced by Reinforcement Learn-\ning with Human Feedback (RLHF). Many vision language\nmodels [36, 40, 41, 72, 83] are built on LLaMA and show\nimpressive results on the visual dialog, reasoning, percep-\ntion, and so on. The LLaMA architecture has also been\napplied in resource-limited multimodal scenarios such as\nmobile phones [10, 11] recently and shows potential appli-\ncations.\nDiffusion Models. Diffusion models, represented by De-\nnoising Diffusion Probabilistic Models (DDPMs)\n[28,\n60], score-based generative models (SGMs) [32, 61] and\nclassifier-free diffusion guidance [29], are the new de facto\nparadigm for image generation, surpassing the previous\nmethodology GAN [24]. The mechanism of diffusion mod-\nels is based on the idea of gradually adding noise to data and\nthen learning to denoise it. Challenges remain for the com-\nputationally expensive training and sampling process, the\nneed for large amounts of data for training, and the difficulty\nin controlling the generation process.\nMost lately, Ope-\nnAI brings about transformer-based text-conditional diffu-\nsion models (the largest one called Sora) [5] jointly trained\non videos and images of variable durations, resolutions, and\naspect ratios to deliver high-fidelity videos simulating real-\nworld scenes. The recent and concurrent work [45] explores\nhow to deal with image generation with flexible target reso-\nlutions. Compared with [45], our target is to build a univer-\nsal vision transformer for various vision tasks.\nPositional Encoding for Transformers. Transformer [68]\noriginally comes with 2D absolute position embeddings in\nsinusoidal forms. In contrast, the relative ones as in [58]\npay attention to the relations of input tokens and can han-\ndle variable lengths of sequences. Rotary positional em-\nbeddings [62] are introduced to encode both absolute and\nrelative positional information, which is proven to be effec-\ntive in large language models [66]. Conditional positional\nembeddings [13] are proposed to add positional informa-\ntion for vision transformers according to the input image,\nwith the benefit of boosted performance and generalizabil-\nity to arbitrary input resolutions. As for LLMs, the mod-\nels are usually pre-trained with a given fixed context length\n[66, 67, 77] and then fine-tuned to a larger context length\n2\nto support long context inference. [8] extends the context\nlength of LLaMA by simple positional interpolations. Base\nfrequency adjustment of RoPE is also studied by [76] to\nenable long-context continued training. NTK-Aware scaled\nRoPE allows LLaMA to have an extended context size with-\nout fine-tuning and minimal perplexity degradation [55].\nMasked Image Modeling. Masked image modeling is a\npowerful pre-training scheme that learns strong represen-\ntations. BEiT [3] extends BERT [20] to computer vision\nby pre-training a Transformer model with masked embed-\ndings to predict discrete visual tokens. Masked Autoen-\ncoder (MAE) [25] is a self-supervised learning approach\nthat masks random patches of input images and trains an\nautoencoder to reconstruct the original images. SiMMIM\n[75] is a simplified version of the MAE approach that\nuses a lightweight one-layer head to predict raw pixel val-\nues. MaskFeat [71] is an extension of the MAE approach\nthat involves predicting not only the raw pixel values of\nthe masked patches but also additional features such as\nhandcrafted HOG descriptor [17] and deep features, which\ncan improve the performance of the model on downstream\ntasks.\n3. Method\n3.1. Plain Transformer\n(b) VisionLLaMA Pyramid Block\nAS2DRoPE\nLayerNorm\nSwiGLU\nLayerNorm\nEmbedded Patches\n(a) VisionLLaMA Block\nMHSA\nAS2DRoPE\nLayerNorm\nSwiGLU\nLayerNorm\nEmbedded Patches\nLSA\nAS2DRoPE\nLayerNorm\nSwiGLU\nLayerNorm\nGSA\nFigure 2. Our VisionLLaMA block (a) in plain Transformer and\nits variant block (b) in pyramid Transformer.\nOur plain VisionLLaMA follows the pipeline of ViT\n[22] and we retain the architecture design of LLaMA as\nclosely as possible. For an image of H \u00d7 W, it\u2019s firstly\ntransformed and flattened into N = H\u00d7W\nP 2\nnon-overlapped\npatches X \u2208 RN\u00d7C. Then a class token is prepended at the\nbeginning of the sequence and the whole sequence is pro-\ncessed by L VisionLLaMA blocks. Unlike [22], we do not\nadd positional encodings to the input sequence since our\nbasic block readily contains positional encoding. Specifi-\ncally, the basic block differs from the standard ViT block\nby two components: self-attention with positional encoding\n(RoPE) [62] and SwiGLU activation [59]. We still utilize\nLayerNorm [2] instead of RMSNorm [80] since we find the\nformer behave better through the classification experiment\n(see Table 11g). The basic block is illustrated in Figure 2\n(a). It should be noted that directly applying 1D RoPE in vi-\nsion tasks cannot well generalize to other resolutions, which\nis different from the training resolution. Therefore, we ex-\ntend it to the 2D form. It can be formally written as,\nzl\nij = MHSA\n\u0000AS2DRoPE\n\u0000LayerNorm\n\u0000zl\u22121\nij\n\u0001\u0001\u0001\n+ zl\u22121\nij ,\nzl\nij = SwiGLU\n\u0000LayerNorm\n\u0000zl\nij\n\u0001\u0001\n+ zl\nij,\ni \u2208 {1, 2, ...., m}, j \u2208 {1, 2, ...., n}.\n(1)\nwhere zl\nij means the output of the l block at position (i, j).\n3.2. Pyramid Transformer\nIt\u2019s straightforward to apply VisionLLaMA to window-\nbased transformers that utilize additive relative position en-\ncoding, such as Swin [43].\nIn this paper, we choose a\nstronger baseline Twins [12] to explore how to build a pow-\nerful pyramid transformer under strictly controlled settings.\nThe original architecture of Twins exploits a conditional po-\nsition encoding and interleaved local-global information ex-\nchange in the form of local and global attention. These com-\nponents can be found in various transformers, which means\nit is not difficult to apply VisionLLaMA in other pyramid\ntransformer variants by following our method. Note that our\ntarget is not to invent a novel pyramid vision transformer,\nbut to show how we adapt the basic design of VisionLLaMA\nbased on the existing ones. Therefore, we simply conform\nto the smallest modifications to the architecture and hyper-\nparameters. Following the name convention of [12], the two\nconsecutive blocks can be written as,\n\u02c6zl\nij = LSA\n\u0000AS2DRoPE\n\u0000LayerNorm\n\u0000zl\u22121\nij\n\u0001\u0001\u0001\n+ zl\u22121\nij ,\nzl\nij = SwiGLU\n\u0000LayerNorm\n\u0000\u02c6zl\nij\n\u0001\u0001\n+ \u02c6zl\nij,\n\u02c6zl+1 = GSA\n\u0000AS2DRoPE\n\u0000LayerNorm\n\u0000zl\u0001\u0001\u0001\n+ zl,\nzl+1 = SwiGLU\n\u0000LayerNorm\n\u0000\u02c6zl+1\u0001\u0001\n+ \u02c6zl+1,\ni \u2208 {1, 2, ...., m}, j \u2208 {1, 2, ...., n}.\n(2)\nwhere LSA is the local self-attention operation within a\ngroup and GSA is the global sub-sampled attention by inter-\nacting with the representative keys from each sub-window\n\u02c6zij \u2208 Rk1\u00d7k2\u00d7C and m \u00d7 n is the sub-window shape.\nWe remove the conditional position encoding in our\npyramid VisionLLaMA since AS2DRoPE already contains\npositional information. Besides, we also remove the class\ntokens and use GAP (global average pooling) before the\nclassification head as [12, 13]. The basic block in this set-\nting is illustrated in Figure 2(b).\n3\n3.3. Training or Inference Beyond the Sequence\nLength\nFrom 1D RoPE to 2D. Handling different input res-\nolutions is a common requirement in vision tasks. Con-\nvolutional neural networks use the sliding window mech-\nanism to deal with the variable length. In contrast, most\nvision transformers apply local window operations or in-\nterpolations.\nFor instance, DeiT [65] adopts bicubic in-\nterpolations when trained on different resolutions. CPVT\n[13] uses convolution-based position encoding. Here we\nevaluate the performance of 1D RoPE [62]. Specifically,\nour pyramid VisionLLaMA based on Twins-SVT-S with\n1D RoPE achieves 81.5% top-1 accuracy on an input of\n224\u00d7224. However, the performance severely degrades to\nzero when evaluated on 448\u00d7448. Therefore, we extend\nthe 1D RoPE to 2D. As for the multi-head self-attention,\nthe 2D RoPE is shared across different heads. Specifically,\ngiven a token xi,j \u2208 Rd, we obtain its position-encoded to-\nken xPE\ni,j = Ri,jxi,j, and the diagonal matrix Ri,j \u2208 Rd\u00d7d\ncan be written as,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ncos(i\u03b80)\n\u2212 sin (i\u03b80)\n0\n0\n. . .\n0\n0\n0\nsin(i\u03b80)\ncos (i\u03b80)\n0\n0\n. . .\n0\n0\n0\n0\n0\ncos(j\u03b80)\n\u2212 sin (j\u03b80)\n. . .\n0\n0\n0\n0\n0\nsin(j\u03b80)\n\u2212 cos (j\u03b80)\n. . .\n0\n0\n0\n0\n0\n0\n. . .\ncos(i\u03b8d\u22124)\n\u2212 sin (i\u03b8d\u22124)\n0\n0\nsin(i\u03b8d\u22124)\ncos (i\u03b8d\u22124)\n0\n0\n0\n0\n0\n. . .\n0\n0\ncos(j\u03b8d\u22124)\n\u2212 sin (j\u03b8d\u22124)\n0\n0\n0\n. . .\n0\n0\ncos(j\u03b8d\u22124)\n\u2212 sin (j\u03b8d\u22124)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwhere \u03b8m = 10000\u2212m/d and m \u2208 {0, 4, 8, ..., d \u2212 4}. Note\nthat R is an orthogonal matrix. We make minor modifi-\ncations to the frequency selection [62] and make two axes\nshare the same frequency. It is easy to verify that\nRT\ni1,j1Ri2,j2 = Ri1\u2212i2,j1\u2212j2.\n(3)\nPositional interpolation helps 2D RoPE to better gen-\neralize. Inspired by [8], which uses interpolation to extend\nthe context window of LLaMA, involving higher resolution\nis analogous to extending the 2D context window of Vi-\nsionLLaMA. Unlike the language task [8] with an enlarged\nfixed context length, vision tasks like object detection usu-\nally deal with different sampled resolutions at different iter-\nations. We train our small model using an input resolution\nof 224\u00d7224 and evaluate the performance on the larger res-\nolutions without re-training, which guides us to apply good\nstrategies of interpolation or extrapolation. Consequently,\nwe apply auto-scaled interpolation (so-called AS2DRoPE)\nbased on an \u2018anchor resolution\u2019. Without loss of general-\nity, we assume handling the square image of H \u00d7 H and an\nanchor resolution B \u00d7 B during the training, we calculate\nR\u2032\ni,jxi,j = Ri\u00b7B/H,j\u00b7B/H,\n(4)\nwhich can be efficiently implemented and does not intro-\nduce an extra cost. Note if the training resolution is kept\nunchanged, AS2DRoPE degenerates as a 2D RoPE.\n(0,0)\n(0,1)\n(0,2)\n(1,0)\n(1,1)\n(1,2)\n(2,0)\n(2,1)\n(2,2)\n(3,0)\n(3,1)\n(3,2)\n(0,3)\n(1,3)\n(2,3)\n(3,3)\nFigure 3. Position calibration for GSA\u2019s keys using a simple case\nof 4 \u00d7 4 resolution and a kernel size of 2 \u00d7 2. The positions of\nthe four points (abstraction keys) are (0.5, 0.5), (1, 2.5), (2.5, 0.5),\n(2.5, 2.5).\nAs for the GSA under the pyramid setting, we require\nspecial treatments since we need to add positional informa-\ntion to the summarized keys. These sub-sampled keys are\ngenerated by abstraction on the feature maps. Without loss\nof generality, we use a convolution with a kernel size of\nk \u00d7 k and stride of k. The coordinate of the generated key\ncan be formulated as the average of the sampled features.\nWe show a simple example in Figure 3.\n4. Experiments\nWe evaluate the effectiveness of VisionLLaMA on im-\nage generation, classification, segmentation, and detection.\nUnless otherwise specified, all models are trained on 8\nNVIDIA Tesla A100 GPUs.\n4.1. Image Generation\nImage generation based on the DiT framework. We\napply VisionLLaMA under the DiT framework [50], which\nis a representative work of image generation using vi-\nsion transformers and DDPM [28].\nSpecifically, we re-\nplace the original vision transformer of DiT with Vision-\nLLaMA while keeping other components unchanged. This\ncontrolled experiment manifests the generality of VisionL-\nLaMA on the image generation task. Moreover, we do not\nchange the original hyper-parameters, although it may be\nsub-optimal to achieve the best performance. We also use\nthe pre-trained VAE [34] (the ft-EMA VAE model) from SD\n[53], which has a down-sample factor of 8. For classifier-\nfree guidance, we use a coefficient of 1.5. The training res-\nolution of the image is 256 \u00d7 256. As suggested by [50], we\nchoose the strongest adaLN-Zero version as our implemen-\ntation. We also use flash attention [18] and mixed precisions\nto speed up the training. Note that FID is known to be sensi-\ntive to small implementation details [49]. To make accurate\ncalculations and fair comparisons, we use the TensorFlow\ntool from [21] as [50].\nWe choose 250 sample steps of DDPM as [50] and show\n4\nthe result in Table 1. As a common practice, FID is re-\ngarded as a primary metric. We also report other secondary\nmetrics such as sFID [47], Precision/Recall [35], and In-\nception Score [56].\nMost experiments are controlled on\n400k training steps.\nVisionLLaMA significantly outper-\nforms DiT across various model sizes.\nWe also extend\nthe training steps of XL models to 2352k steps to evaluate\nwhether our models have the faster convergence advantage\nor still behave better under the setting of longer training\nepochs. DiT-LLaMA-XL/2 has 0.83 lower FID [27] than\nDiT-XL/2, indicating that VisionLLaMA not only has bet-\nter computing efficiency but higher performance than DiT.\nWe show some generated samples in Figure 1 using our XL\nmodel.\nImage generation based on the SiT framework. SiT\n[46] has a flexible choice of drift and diffusion coefficients,\nwhich is supported by the recently proposed interpolant\nframework [1]. It improves the performance of image gen-\neration using vision transformers by clear margins. Orthog-\nonally, we replace the vision transformer in SiT with Vi-\nsionLLaMA to evaluate the benefits of better model archi-\ntecture, which we call SiT-LLaMA. Our implementation is\nbased on the released code of [46] with carefully controlled\nexperiments. Specifically, we do not change the hyperpa-\nrameters, although its default setting may be sub-optimal.\nAll the models are trained using the same number of steps.\nWe use linear interpolant and the velocity model for all ex-\nperiments. To make fair comparisons, we also rerun the\nreleased code and sample 50k 256\u00d7256 images using the\n250 steps SDE sampler (Euler) and report the result in Ta-\nble 2. SiT-LLaMA uniformly outperforms SiT across mod-\nels with various levels of capacities by clear margins. Com-\npared with SiT-L/2, SiT-LLaMA-L/2 decreases by 5.0 FID,\nwhose magnitude is larger than the boost from the invention\nof a new framework (4.0 FID). We also report the more ef-\nficient ODE sampler (dopri5) in Table 13, our performance\ngap remains. Similar to the observation of [46], SDE has\nbetter performance than its ODE counterpart.\n4.2. Classification on ImageNet\n4.2.1\nSupervised Training\nIn this section, we focus on supervised training on the\nImageNet-1K dataset [19] to make fair comparisons. We\nexclude other datasets or distillation tricks. All the mod-\nels are trained using the ImageNet-1K training set, and we\nreport the accuracy of the validation set in Table 3.\nPlain Vision Transformer Comparison.\nDeiT3 [65] is\nthe state-of-the-art plain vision transformer, which proposes\nspecial data augmentations and performs extensive hyper-\nparameter search to boost the performance of DeiT [64].\nDuring the reproduction of DeiT3, we observe that it is sen-\nsitive to hyperparameters and prone to overfitting. Replac-\ning the class token with GAP (global average pooling) [13]\nleads to a 0.7% top-1 accuracy drop for the DeiT3-Large\nmodel after 800 epochs of training. Therefore, we use the\nclass token instead of GAP in the plain transformer and re-\nport the result in Table 3, where VisionLLaMA achieves a\ntop-1 accuracy comparable to DeiT3. The detailed hyper-\nparameter is listed in the appendix. Note that the accuracy\non a single resolution does not provide comprehensive com-\nparisons, we also evaluate the performance across different\nimage resolutions as [13] and report the result in Table 4. As\nfor DeiT3, we use the bicubic interpolation for the learnable\npositional encoding. Although these two models have com-\nparable performance at the resolution of 224\u00d7224, the gap\nis enlarged when the resolution is increased, which means\nour method generalizes better across different resolutions,\nwhich is a vital function for many downstream tasks such\nas object detection.\nPyramid Vision Transformer. We use the same architec-\nture as Twins-SVT [12] and the detailed configuration is\nlisted in Table 17. We remove the conditional position en-\ncoding since VisionLLaMA already contains one kind of\nrotary position encoding.\nTherefore, VisionLLaMA is a\nconvolution-free architecture. We do not tune the hyper-\nparameters and directly follow the setting provided in [12].\nAlthough it\u2019s suboptimal, it can still achieve competitive\nperformance. As [12,13], we do not use the class token and\napply GAP. In particular, all the models are trained for 300\nepochs with a batch size of 1024. The learning rate is ini-\ntialized to be 0.001 and decayed to zero within 300 epochs\nfollowing the cosine strategy. The result is shown in Table 3\nand our method achieves comparable performance as Twins\nacross various levels of models and outperforms Swin [43]\nconsistently. We further compare the pyramid transformers\nusing popular downstream tasks, which are shown in the\nlater sections.\n4.2.2\nSelf-Supervised Training\nThere are two common approaches to evaluating the perfor-\nmance of the self-supervised vision transformers [25] using\nthe ImageNet dataset. In this section, we make comparisons\nbased on these two ways. To make fair comparisons, we\nlimit the training data to ImageNet-1K. We also exclude any\ncomponent that utilizes CLIP [51], DALLE [52], or distilla-\ntion, which can be orthogonally combined to further boost\nthe performance. Our implementation is based on the MM-\nPretrain framework [15]. We utilize the MAE framework\nand replace the encoder using VisionLLaMA while keeping\nother components unchanged. This minor modified setting\nforms a controlled experiment to evaluate the role of our\napproaches. Moreover, we use the same hyperparameter\nas [25], which is suboptimal to our method. Fortunately,\nthis simple setting still achieves a significant performance\nboost over the strong baseline.\n5\nModel\nCFG Flops (G) Params (M) Training Steps (K) Learning Rate FID\u2193 sFID\u2193 Precision\u2191 Recall\u2191\nIS\u2191\nDiT-B/4\nN\n5.56\n130\n400\n0.0001\n68.38 12.66\n36.07\n54.71\n20.27\nDiT-LLaMA-B/4\nN\n5.56\n130\n400\n0.0001\n63.17 12.63\n38.27\n56.75\n22.47\nDiT-B/4\nY\n5.56\n130\n400\n0.0001\n45.38\n9.97\n46.89\n53.66\n34.27\nDiT-LLaMA-B/4\nY\n5.56\n130\n400\n0.0001\n39.51\n9.82\n50.46\n54.75\n40.17\nDiT-L/4\nN\n19.70\n458\n400\n0.0001\n44.37\n8.97\n48.16\n61.53\n32.25\nDiT-LLaMA-L/4\nN\n19.70\n458\n400\n0.0001\n40.32\n9.04\n49.87\n61.61\n36.56\nDiT-L/4\nY\n19.70\n458\n400\n0.0001\n22.51\n7.08\n62.67\n55.27\n66.58\nDiT-LLaMA-L/4\nY\n19.70\n458\n400\n0.0001\n18.64\n7.01\n65.40\n54.35\n78.52\nDiT-XL/4\nN\n29.05\n675\n400\n0.0001\n43.01\n-\n-\n-\n-\nDiT-LLaMA-XL/4\nN\n29.05\n675\n400\n0.0001\n35.99\n8.48\n52.31\n61.65\n41.18\nDiT-XL/4\nY\n29.05\n675\n400\n0.0001\n22.52\n7.09\n62.68\n55.27\n66.58\nDiT-LLaMA-XL/4\nY\n29.05\n675\n400\n0.0001\n18.69\n7.02\n65.67\n55.57\n78.32\nDiT-XL/2\nN\n118.64\n675\n2352\n0.0001\n10.67\n-\n-\n-\n-\nDiT-LLaMA-XL/2\nN\n118.64\n675\n2352\n0.0001\n9.84\n6.47\n67.45\n66.71\n117.72\nDiT-LLaMA-XL/2\nY\n118.64\n675\n2352\n0.0001\n2.42\n4.51\n83.03\n56.82\n265.39\nTable 1. Image generation comparisons using the DiT framework [50]. All the models are trained using an image resolution of 256\u00d7256\nwith a batch size of 256. Metrics are calculated using the sampled 50k images. IS: inception score [56].\nModel\nFlops (G) Params (M) Training Steps (K) Learning Rate FID\u2193 sFID\u2193 Precision\u2191 Recall\u2191\nIS\u2191\nSiT-S/2 \u2020\n6.06\n33\n400\n0.0001\n58.15\n9.12\n41.01\n60.23\n24.72\nSiT-LLaMA-S/2\n6.06\n33\n400\n0.0001\n53.90\n8.78\n42.98\n60.36\n26.74\nSiT-B/2 \u2020\n23.01\n130\n400\n0.0001\n35.54\n6.57\n52.68\n64.38\n42.33\nSiT-LLaMA-B/2\n23.01\n130\n400\n0.0001\n29.53\n6.32\n56.07\n64.07\n50.13\nDiT-L/2\n80.71\n458\n400\n0.0001\n23.3\n-\n-\n-\n-\nSiT-L/2 \u2020\n80.71\n458\n400\n0.0001\n19.34\n5.28\n63.00\n63.60\n70.47\nSiT-LLaMA-L/2\n80.71\n458\n400\n0.0001\n14.32\n5.17\n66.39\n63.64\n86.85\nSiT-XL/2 \u2020\n118.64\n675\n400\n0.0001\n16.98\n5.07\n65.12\n64.10\n77.06\nSiT-LLaMA-XL/2\n118.64\n675\n400\n0.0001\n12.20\n5.03\n67.86\n63.08\n95.28\nTable 2. Image generation comparisons using the SiT framework [46]. All the models are trained using an image resolution of 256\u00d7256\nwith a global batch size of 256. Metrics are calculated using the sampled 50k images without classifier-free guidance. IS: inception score.\nThe FID is calculated by 250 steps SDE Euler sampler. \u2020: reproduced result using the released code.\nFull fine-tuning. In such a setting, the model is first ini-\ntialized using the pre-trained weights and then trained for\nextra epochs with totally trainable parameters. Trained by\n800 epochs on the ImageNet, VisionLLaMA-Base achieves\n84.0% top-1 accuracy, which exceeds ViT-Base by 0.8%.\nNote that our method uses a mask ratio of 0.75 as [25],\nwhose training speed is about 3 times faster than SimMIM\n[75]. We also increased the training epochs to 1600 to ver-\nify whether VisionLLaMA keeps the advantage given suffi-\ncient training resources. VisionLLaMA-Base achieves new\nstate-of-art result among MAE variants, 84.3% top-1 accu-\nracy, which outperforms ViT-Base by 0.9%. This result is\neven higher than MaskFeat [71] where new training objec-\ntives are proposed. Regarding full fine-tuning having a risk\nof performance saturation [42, 69], our boost is significant.\nNext we resort to the linear probing metric to provide extra\nevaluations, which is considered a more reliable evaluation\nfor representative learning by a recent work [9].\nLinear probing. In this setting, the model is initialized by\nthe pre-trained weights from the SSL stage. Then, the whole\nbackbone is frozen except for the classifier head during the\ntraining. The result is shown in Table 5. With a training\ncost of 800 epochs, VisionLLaMA-Base outperforms ViT-\nBase-MAE by 4.6%. It also exceeds ViT-Base-MAE, which\nis trained for 1600 epochs. When VisionLLaMA is trained\nfor 1600 epochs, VisionLLaMA-Base achieves 71.7% top-\n1 accuracy. We also scale up to have VisionLLaMA-Large,\nwhere our method exceeds ViT-Large by 3.6%.\n6\nModel Param\nSetting\nTop-1\n(M)\n(%)\nDeiT-Small [64]\n22\n224I 300E\n79.9\nCPVT-Small-GAP [13]\n23\n224I 300E\n81.5\nDeiT3-Small [65]\n22\n224I 800E\n81.4\nVisionLLaMA-S [65]\n22\n224I 800E\n81.6\nSwin-T [43]\n29\n224I 300E\n81.3\nTwins-SVT-S [12]\n24\n224I 300E\n81.7\nPyramid VisionLLaMA-S\n24\n224I 300E\n81.6\nSwin-S [43]\n50\n224I 300E\n83.0\nTwins-SVT-B [12]\n56\n224I 300E\n83.2\nPyramid VisionLLaMA-B\n56\n224I 300E\n83.2\nDeiT3-Base [65]\n86\n192I 800E + 224I 20E 83.8\nVisionLLaMA-B\n86\n192I 800E + 224I 20E 83.6\nSwin-B [43]\n88\n224I 300E\n83.3\nTwins-SVT-L [13]\n99\n224I 300E\n83.7\nPyramid VisionLLaMA-L\n99\n224I 300E\n83.6\nDeiT3-Large\u2020 310\n160I 800E+224I 20E 84.5\nVisionLLaMA-L 310\n160I 800E+224I 20E 84.6\nTable 3. Comparisons on ImageNet-1K supervised classification.\nAll the models are trained using the ImageNet-1K dataset. \u2020: re-\ntrained using the official code. 160I 800E+224I 20E means two-\nstage training, the model is firstly trained for 800 epochs using\n160\u00d7160, then trained for 20 epochs with higher image resolution\n224\u00d7224.\nModel\n160\n224\n256\n288\n512\n768\nDeiT3-Large [65]\n83.1 84.5 84.7 84.6 82.1 76.5\nVisionLLaMA-L\n83.1 84.6\n84.7\n84.8\n83.5 79.1\nTable 4. Top-1 accuracy comparison on different resolutions. The\nmodels are trained on 224 and directly evaluated on other resolu-\ntions.\n4.3. Semantic Segmentation on ADE20K\n4.3.1\nSupervised Training\nFollowing [12, 43], we evaluate our method using seman-\ntic segmentation on the ADE20K [82] dataset. To make\nfair comparisons, we limit the baselines to only using\nImageNet-1K in the pre-training stage.\nSpecifically, we\nmake use of the UperNet [74] framework and replace the\nbackbone with pyramid VisionLLaMA. Our implementa-\ntion is based on the MMSegmentation framework [14]. Our\nmodels are trained for 160k steps with a global batch size of\n16. The detailed setting of the hyperparameter is shown in\nSection B.7. We report the result in Table 6. Under similar\nFLOPs, our method outperforms both Swin and Twins by\nmore than 1.2% mIoU.\nModels Pretrain Epochs SFT Acc LP Acc\n(%)\n(%)\nViT-Base-MAE\u2020 [25]\n800\n83.2\n65.1\nSemMAE [37]\n800\n83.4\n65.0\nSimMIM [75]\n800\n83.8\n56.7\nMFF-MAE [42]\n800\n83.6\n67.0\nVisionLLaMA-Base-MAE\n800\n84.0\n69.7\nViT-Base-MAE [25]\n1600\n83.4\n67.0\nMaskFeat [71]\n1600\n84.0\n62.3\nVisionLLaMA-Base-MAE\n1600\n84.3\n71.7\nViT-Large-MAE\u2020 [25]\n800\n85.4\n73.7\nVisionLLaMA-Large-MAE\n800\n85.5\n77.3\nTable 5. Comparison with masked image modeling SSL methods\non the ImageNet validation set. \u2020: reproduced in MMPretrain.\nModels\nParam\nmIoU\n(M)\n(%)\nSwin-S [43]\n81.3\n47.6\nTwins-SVT-B [12]\n88.5\n47.7\nPyramid VisionLLaMA-B\n88.5\n49.1\nSwin-B [43]\n121\n48.1\nTwins-SVT-L [12]\n133\n48.8\nPyramid VisionLLaMA-L\n133\n50.0\nTable 6. Performance comparisons with different backbones on\nADE20K validation dataset.\nAll backbones are pre-trained on\nImageNet-1K with labels. mIoU is evaluated by the single scale\nsetting.\n4.3.2\nSelf-Supervised Training\nWe use the UperNet [74] framework to perform semantic\nsegmentation on the ADE20K dataset, which is a popular\nbenchmark for backbones. We carefully control the exper-\niment and replace the ViT backbone with VisionLLaMA\nwhile keeping other components and hyperparameters un-\nchanged. Our implementation is based on MMSegmenta-\ntion [14] and the detailed hyperparameters are provided in\nSection B.6. The result is given in Table 7. As for the 800\nepoch pre-training groups, VisionLLaMA-B significantly\nboosts ViT-Base by 2.8% mIoU. It also outperforms some\nother modifications such as introducing extra training objec-\ntives or features [42,71] by clear margins. Moreover, those\napproaches introduce extra overhead for the training pro-\ncess and slow down the training speed. We emphasize that\nthe training speed of a method is becoming more and more\nimportant in the age of large models. In contrast, VisionL-\nLaMA only involves the replacement of the base model and\nhas the same fast training speed as [25]. In principle, our\nmethod can be seamlessly combined with these modifica-\ntions. We further evaluate the performance of longer pre-\n7\ntraining epochs of 1600, VisionLLaMA-B achieves 50.2%\nmIoU on the ADE20K validation set, which boosts ViT-B\nby 2.1% mIoU.\nModels\nPretrain Epochs\nmIoU\n(%)\nViT-B\u2020\n800\n46.2\nSemMAE [37]\n800\n46.3\nMFF-MAE [42]\n800\n47.9\nVisionLLaMA-B\n800\n49.0\nViT-B\n1600\n48.1\nMaskFeat [71]\n1600\n48.3\nVisionLLaMA-B\n1600\n50.2\nTable 7.\nPerformance comparisons with different SSL trained\nbackbones on ADE20K validation dataset. All backbones are pre-\ntrained on ImageNet-1K without labels. mIoU is evaluated by\nthe single scale setting. \u2020: reproduce result using [14].\n4.4. Object Detection on COCO\n4.4.1\nSupervised Training\nWe evaluate the performance of pyramid VisionLLaMA on\nthe COCO objection detection task. Specifically, we use\nthe Mask RCNN framework [26] and replace the backbone\nwith pyramid VisionLLaMA, which is pre-trained for 300\nepochs on the ImageNet-1K dataset as [12, 43]. Therefore,\nour model has the same number of parameters and FLOPs\nas Twins. Since our target is not to achieve a new state-\nof-the-art detector, this carefully controlled experiment is\nused to verify the validity of our method without loss of\ngenerality. Our implementation is based on the MMDetec-\ntion framework [7] and the hyperparameter setting is pro-\nvided in Section B.8. We report the result on standard 36\nepochs (3\u00d7) in Table 8. Under this carefully controlled set-\nting, our model outperforms both Swin and Twins. Specifi-\ncally, VisionLLaMA-B exceeds Swin-S by 1.5% box mAP\nand 1.0 mask mAP. Compared with the stronger baseline\nTwins-B, our method also has an advantage of 1.1% higher\nbox mAP and 0.8% higher mask mAP.\n4.4.2\nSelf-Supervised Training\nWe apply VisionLLaMA based on the ViTDet frame-\nwork [39], which utilizes plain vision transformers to\nachieve comparable performance as the pyramid counter-\npart. Specifically, we use the Mask RCNN detector and\nreplace the vit-Base backbone (trained for 1600 epochs us-\ning MAE) with our VisionLLaMA-Base model, which is\npre-trained for 800 epochs using MAE. The original ViT-\nDet converges slowly and requires dedicated training strate-\ngies like longer training epochs (e.g. 100) to achieve opti-\nmal performance. During the training process, we find Vi-\nBackbone FLOPs\n(G)\nMask R-CNN 3\u00d7 + MS\nAPb APb\n50 APb\n75 APm APm\n50 APm\n75\nSwin-S [43]\n222\n47.6 69.4 52.5 42.8 66.5 46.4\nTwins-SVT-B [12]\n224\n48.0 69.5 52.7 43.0 66.8 46.6\nPyramid VisionLLaMA-B\n224\n49.1 70.5 54.0 43.8 67.4 47.0\nTable 8. Object detection and instance segmentation performance\non the COCO val2017 dataset using the Mask R-CNN frame-\nwork. FLOPs are evaluated on an 800\u00d7600 image. All the back-\nbones are trained for 300 epochs on the ImageNet-1K dataset.\nsionLLaMA achieves similar performance after 30 epochs.\nTherefore, we directly utilize the standard 3x training strat-\negy. We use AdamW optimizer with \u03b21 = 0.9 and \u03b22 =\n0.999. We also use a layer-wise learning rate of 0.7 as [39].\nThe initial learning rate is 0.0001 and decayed by 0.1 at\nepochs 27 and 33. We use a weight decay of 0.1 and a global\nbatch size of 64. The input image resolution is 1024\u00d71024.\nTherefore, our training cost is only 36% of the baseline.\nUnlike [39], we do not search for the optimal hyperparam-\neter. The result is shown in Table 9 and VisionLLaMA out-\nperforms ViT-B by 0.6% Box mAP and 0.8 % mask mAP.\nModel\nPretrained\nmAPBox mAPMask Epochs\nSwin-S [43]\nImageNet sup 300e\n47.6\n42.8\n36\nTwins-SVT-B [12] ImageNet sup 300e\n48.0\n43.0\n36\nViT-B [39]\nMAE 1600e\n51.6\n45.7\n100\nVisionLLaMA-B\nMAE 800e\n52.2\n46.3\n36\nTable 9. Object detection result on COCO 2017 dataset based on\nViTDet [39]. sup: supervised training on ImageNet-1K\nMethod\n100k\n200k\n300k\n400k\nSiT-S/2\n89.9\n71.9\n64.5\n59.6\nSiT-LLaMA-S/2\n82.88\n67.1\n59.3\n54.6\nSiT-B/2\n65.76\n48.37\n41.05\n36.90\nSiT-LLaMA-B/2\n56.60\n40.62\n34.09\n30.22\nSiT-L/2\n45.07\n29.11\n23.40\n20.14\nSiT-LLaMA-L/2\n35.39\n21.82\n17.23\n14.91\nSiT-XL/2\n42.25\n26.49\n20.89\n17.83\nSiT-LLaMA-XL/2\n40.46\n19.00\n14.84\n12.79\nTable 10. FID calculated with the 250-step ODE sampler in view\nof efficiency based on the SiT framework.\n8\n5. Ablation Study and Discussion\n5.1. Ablation Studies\nUnless otherwise specified, we choose the ViT-Large\nmodel (160I 800E+224I 20E) to perform ablations because\nwe observe that it generates small variance across multiple\nruns, where a performance gap of more than 0.2 suffices as\na guide to choosing appropriate components.\nAblation of FFN and SwiGLU. We replace FFN with\nSwiGLU and report the result in Table 11a.\nWe do not\nobserve performance gaps, therefore, we utilize SwiGLU\nand avoid introducing extra modifications to the LLaMA\narchitecture. This also motivates us to focus on the ablation\nof the self-attention block. As we apply multi-head self-\nattention, the remaining two differences become the nor-\nmalization and positional encoding.\nAblation of the normalization strategy. We compare the\ntwo widely used normalization methods in transformers:\nRMSNorm [80] and LayerNorm [2] and report the result in\nTable 11g. The latter has a better final performance, which\nindicates that re-centering invariance is also important in\nthe vision tasks. We also report the training speed by the\naverage time spent per iteration, where LayerNorm is only\n2% slower than RMSNorm. Therefore, we choose Layer-\nNorm instead of RMSNorm for better tradeoff. Note that\nthe training speed might differ across different hardware de-\nvices and might also be affected by the overall architecture.\nNext, we evaluate the role of positional encoding in two\naspects, a static case using a fixed resolution and a dynamic\ncase using variable resolutions. The former is common in\nthe classification task while the latter is vital in downstream\ntasks such as segmentation and object detection.\nPartial PE. We adjust the ratio of overall channels using\nRoPE to report the result in Table 11b, which shows good\nperformance can be achieved if the ratio is set above a small\nthreshold value. We do not observe significant differences\nacross these settings. Therefore, we keep the default setting\nof [66] and do not follow [4,30].\nFrequency base. We change the base frequency and report\nthe result in Table 11c, which means the performance is ro-\nbust to a large range of frequencies. As a result, we keep\nthe default value of [66] to avoid extra special treatments\nfor deployment.\nShared PE for each head. We find that sharing the same\nPE across different heads (the frequency varies from 1 to\n10000 in each head) is better than independent ones (the\nfrequency varies from 1 to 10000 across all channels). The\nresult is shown in Table 11d.\nFeature abstraction strategy. We compare the two com-\nmon feature extraction strategies: class token [22] and GAP\n[13] using the plain \u2018large\u2019 model and report the result in\nTable 11e. Using a class token is better than GAP, which\nis different from [13]. However, the training settings of the\ntwo cases are quite different. We also make an extra ex-\nperiment using DeiT3-L to observe a similar performance\ngap of 0.3%. We further evaluate the performance of the\n\u2018small\u2019 and \u2018base\u2019 models. It\u2019s interesting to see the oppo-\nsite conclusions for the small model. We suspect that the\nhigher drop-path rate used in [65] makes it difficult for the\nparameter-free abstraction such as GAP to fit in the purpose.\nPositional encoding strategy. We also add other absolute\nposition encoding strategies such as a learnable PE [64] and\nPEG [13] on pyramid VisionLLaMA-S. We use the \u2018small\u2019\nmodel due to the existence of a strong baseline and re-\nport the result in Table 11f. While the learnable PE does\nnot boost performance, PEG slightly improves the baseline\nfrom 81.6% to 81.8%. However, we do not include PEG as\na basic component regarding three aspects. Firstly, we try\nto keep the smallest modifications on LLaMA [66]. Sec-\nondly, our target is proposing a universal approach for vari-\nous tasks like ViT [22]. For masked image frameworks like\nMAE [25], it is non-trivial to keep the reduced training cost\nof masked tokens if the backbone contains PEG. If we mask\npatches in the input like [75], it would greatly slow down\nthe training speed. Moreover, containing masked patches\nin the encoder would incur a data distribution shift to the\nencoder, which severely hurts the performance of down-\nstream tasks. In principle, we can apply sparse PEG under\nthe MAE framework, but it will introduce the deployment-\nunfriendly operators. It remains an open problem whether\nsparse convolution contains enough positional information\nas its dense version [13, 33]. Thirdly, avoiding modality-\nbound designs paves the way for further studies that cover\nother modalities beyond text and vision.\nSensitivity to the input size. We further compare the per-\nformance on the enlarged and commonly used resolutions\nwithout training to report the result in Table 12. Here we use\nthe pyramid transformer since it is more popular in down-\nstream tasks than the plain counterpart. It is not surpris-\ning that 1D-RoPE severely suffers from the changed resolu-\ntions. NTK-Aware interpolation with \u03b1 = 2 achieves sim-\nilar performance as the 2D-RoPE1, which is indeed NTK-\nAware (\u03b1 = 1). AS2DRoPE shows the best performance\nfor larger resolution.\n5.2. Discussion\nWe further investigate the underlying mechanisms be-\nhind our method\u2019s superior performance over ViT in various\ntasks. As the ablation studies have indicated, our positional\nencoding strategy makes a big difference. In this section,\nwe discuss the boosted convergence speed and attempt to\ntheoretically rationalize the underlying mechanism.\nConvergence speed. For image generation, we study\n1Although we can apply the dynamic NTK-Aware to keep the perfor-\nmance at 224, it does not bring in boosted performance on larger resolu-\ntions.\n9\ncase\nAcc\nSwiGLU 84.6\nFFN\n84.6\n(a) FFN or SwiGLU.\nRatio Acc\n25% 84.5\n50% 84.5\n100% 84.6\n(b) Partial ratio of RoPE. A\npartial ratio does not bring im-\nproved performance.\nBase Acc\n100 84.6\n1000 84.6\n10000 84.6\n100000 84.4\n(c) Base frequency of RoPE.\nShared PE Acc\nN\n84.2\nY\n84.6\n(d) Shared PE across different heads. Shared\nPE for all heads is better.\nMethod\nClass Head Acc\nVisionLLaMA-S Class Token 81.6\nVisionLLaMA-S\nGAP\n81.8\nVisionLLaMA-B Class Token 83.6\nVisionLLaMA-B\nGAP\n83.6\nVisionLLaMA-L Class Token 84.6\nVisionLLaMA-L\nGAP\n84.3\nDeiT3-L [65]\nClass Token 84.5\nDeiT3-L\u2020\nGAP\n84.2\n(e) Feature extraction strategy. The class token is bet-\nter than GAP in the training setting of DeiT3 [65].\ncase\nAcc\nPyramid LLaMA-S\n81.6\nPyramid LLaMA-S + learnable PE [64] 81.6\nPyramid LLaMA-S + PEG [13]\n81.8\n(f) PE comparison. Applying PEG [13] can further im-\nprove the performance.\ncase\nAcc Train Speed\nLayerNorm [2] 84.6\n0.4971s\nRMSNorm [80] 84.4\n0.4874s\n(g) Normalization choice.\nTable 11. Ablation experiments with plain transformer ViT-L/16 (DeiT3-L) on ImageNet-1K. We report the top-1 accuracy (%). If not\nspecified, the default is: and the pre-training length is 800 epochs under an image resolution of 160\u00d7160 and 20 epochs using 224\u00d7224.\nDefault settings are marked in gray . \u2020: running the release code. All accuracies are top-1.\nModel\n224\n448\n512\n1D-RoPE\n81.5 0.01 0.01\n2D-RoPE\n81.6 79.5 78.4\nNTK(\u03b1 = 2)\n81.6 79.6 78.5\nNTK(\u03b1 = 5)\n81.3 79.6 78.6\nNTK(\u03b1 = 10) 81.1 79.6 78.6\nAS2DRoPE\n81.6\n80.3 79.5\nTable 12. Top-1 accuracy on different resolutions of the pyramid\nsmall model. The models are trained on 224x224 and directly eval-\nuated on other resolutions.\nthe performance w.r.t the training steps. Specifically, we\nstore the checkpoint at 100k, 200k, 300k, and 400k itera-\ntions to calculate the fidelity metrics. Since SDE is signif-\nicantly slower than ODE, we opt to use the ODE sampler\ninstead. The result of the strictly controlled experiment is\nlisted in Table 10. It appears that VisionLLaMA converges\nmuch faster than ViT across all models. SiT-LLaMA with\n300k training iterations even outperforms the baseline with\n400k steps.\nWe also compare the convergence speed using the\nDeiT3-Large under the supervised training setting on Ima-\ngeNet to show the top-1 validation accuracy during the 800\nepochs in Figure 4. It also indicates that VisionLLaMA con-\nverges faster than DeiT3-L. We further compare the train-\ning loss across 800 epochs of the ViT-Base model under the\nMAE framework [25] and illustrate it in Figure 5. VisionL-\nLaMA has lower training loss at the beginning and the trend\nis kept till the end.\n0\n100\n200\n300\n400\n500\n600\n700\n800\nEpoch\n0\n20\n40\n60\n80\nTop-1 Accuracy\nVisionLLaMA-L\nDeiT3-Large\nFigure 4. Faster convergence of VisionLLaMA using the setting\nof DeiT3.\n0\n100\n200\n300\n400\n500\n600\n700\n800\nEpoch\n0.4\n0.5\n0.6\n0.7\nLoss\nViT-B\nVisionLLaMA-B\nFigure 5. Loss curve of MAE pre-training on VisionLLaMA com-\npared with ViT-B.\nTheoretical Reasoning. We dive into the mechanism\nof our positional encodings from the theoretical viewpoint.\nWithout loss of generality, given an input embedding of di-\n10\nmension d = 4, the query at location (i, j) can be written\nas qi,j. We use ki,j to represent the key vector at (i, j) and\npi,j to be the positional encoding using 2D sin-cos encod-\ning [25,46]. The inner dot product between qi1,j1 and ki2,j2\nusing this additive encoding can be written as,\nqT\ni1,j1ki2,j2 = (qi1,j1 + pi1,j1)T (ki2,j2 + pi2,j2)\n= qT\ni1,j1ki2,j2 + pT\ni1,j1pi2,j2 + qT\ni1,j1pi2,j2 + pT\ni1,j1ki2,j2\n= qT\ni1,j1ki2,j2 + f(i1 \u2212 i2, j1 \u2212 j2) + M.\n(5)\nThe first item is the inner dot product of contents.\nThe\nsecond item reflects the positional effect in the form of\nf(i1\u2212i2, j1\u2212j2), which plays a long-distance decaying ef-\nfect. However, the third item M = qT\ni1,j1pi2,j2 +pT\ni1,j1ki2,j2\nmeans positions directly interacting with the content fea-\ntures, which slows down the learning process.\nIn contrast, the inner dot product using RoPE can be\nwritten as,\n(Ri1,j1qi1,j1)T (Ri2,j2ki2,j2) = qT\ni1,j1RT\ni1,j1Ri2,j2ki2,j2\n= qT\ni1,j1Ri1\u2212i2,j1\u2212j2ki2,j2.\n(6)\nRi1\u2212i2,j1\u2212j2 contributes a larger absolute value if the po-\nsitions of q and k are close, and a smaller value if op-\nposite.\nThis introduces certain localities as a prior bias,\nwhich resembles the function of a convolution. Moreover,\nRi1\u2212i2,j1\u2212j2 adjusts the dot product by the multiplication of\na factor between 0 and 1, which is more flexible and faster\nthan the addition of f(i1 \u2212 i2, j1 \u2212 j2). We believe that\nthis flexibility allows the transformer to leverage its model\ncapacity effectively, learning a good representation without\ndedicating some of that capacity to introducing bias or sepa-\nrating position from content. In this way, VisionLLaMA not\nonly converges faster but also has better final performance.\n6. Conclusion\nIn a nutshell, we present VisionLLaMA to enjoy the ben-\nefits of the LLaMA architecture in the vision modality. It\nis trained either in supervised or self-supervised schemes\nto validate the power in a myriad of downstream vision\ntasks like image classification, detection, and segmentation.\nWe particularly explore its image generation capacity un-\nder the diffusion framework DiT and SiT to confirm its po-\ntency. We conclude that VisionLLaMA has strong potential\nto serve as a new vision backbone to facilitate a large realm\nof downstream applications.\nAcknowledgements:\nThis work was in part supported by\nNational Key R&D Program of China (No. 2022ZD0118-\n700).\nReferences\n[1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-\nEijnden.\nStochastic interpolants: A unifying framework\nfor flows and diffusions. arXiv preprint arXiv:2303.08797,\n2023. 5\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3, 9, 10\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021. 3, 16\n[4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory An-\nthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. Pythia: A suite for analyz-\ning large language models across training and scaling. In In-\nternational Conference on Machine Learning, pages 2397\u2013\n2430. PMLR, 2023. 9\n[5] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei\nGuo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric\nLuhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators.\n2024. 2\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 2\n[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n8\n[8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian.\nExtending context window of large lan-\nguage models via positional interpolation.\narXiv preprint\narXiv:2306.15595, 2023. 3, 4\n[9] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. De-\nconstructing denoising diffusion models for self-supervised\nlearning. arXiv preprint arXiv:2401.14404, 2024. 6\n[10] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\nXiaolin Wei, et al.\nMobilevlm: A fast, reproducible and\nstrong vision language assistant for mobile devices. arXiv\npreprint arXiv:2312.16886, 2023. 2\n[11] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu,\nFei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang\nLin, Bo Zhang, et al.\nMobilevlm v2:\nFaster and\nstronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024. 2\n[12] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. In Adv. Neural Inform. Process. Syst., 2021. 2,\n3, 5, 7, 8, 15\n[13] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and\nChunhua Shen. Conditional positional encodings for vision\n11\ntransformers. In The Eleventh International Conference on\nLearning Representations, 2023. 2, 3, 4, 5, 7, 9, 10\n[14] MMSegmentation Contributors.\nMmsegmentation: Open-\nmmlab semantic segmentation toolbox and benchmark,\n2020. 7, 8\n[15] MMPreTrain Contributors. Openmmlab\u2019s pre-training tool-\nbox and benchmark. https://github.com/open-\nmmlab/mmpretrain, 2023. 5\n[16] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmen-\ntation with a reduced search space.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 702\u2013703, 2020. 16\n[17] Navneet Dalal and Bill Triggs. Histograms of oriented gra-\ndients for human detection. In 2005 IEEE computer soci-\nety conference on computer vision and pattern recognition\n(CVPR\u201905), volume 1, pages 886\u2013893. Ieee, 2005. 3\n[18] Tri Dao.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023. 4\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 2, 5\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[21] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 4, 15\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 1, 2, 3, 9\n[23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh.\nGptq:\nAccurate post-training quantization\nfor generative pre-trained transformers.\narXiv preprint\narXiv:2210.17323, 2022. 1\n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems (NIPS), 2014. 2\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000\u2013\n16009, 2022. 2, 3, 5, 6, 7, 9, 10, 11, 15\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 8\n[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 5\n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 4\n[29] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2\n[30] https://stability.ai/.\nStable code 3b: Coding on the edge.\n2024. 9\n[31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11\u201314, 2016, Proceed-\nings, Part IV 14, pages 646\u2013661. Springer, 2016. 15\n[32] Aapo Hyv\u201darinen and Peter Dayan.\nEstimation of non-\nnormalized statistical models by score matching.\nJournal\nof Machine Learning Research, 2005. 2\n[33] Md Amirul Islam*, Sen Jia*, and Neil D. B. Bruce. How\nmuch position information do convolutional neural networks\nencode? In International Conference on Learning Represen-\ntations, 2020. 9\n[34] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[35] Tuomas Kynk\u00a8a\u00a8anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall met-\nric for assessing generative models. Advances in Neural In-\nformation Processing Systems, 32, 2019. 5\n[36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 2\n[37] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing\nSu, and Changwen Zheng. Semmae: Semantic-guided mask-\ning for learning masked autoencoders. Advances in Neural\nInformation Processing Systems, 35:14290\u201314302, 2022. 7,\n8\n[38] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu.\nNorm tweaking: High-performance low-bit quantization of\nlarge language models. In Thirty-Eighth AAAI Conference\non Artificial Intelligence, 2024. 1\n[39] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In European Conference on Computer Vision, pages\n280\u2013296. Springer, 2022. 8, 15\n[40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 2\n[41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. NeurIPS, 2023. 2\n[42] Yuan Liu, Songyang Zhang, Jiacheng Chen, Zhaohui Yu, Kai\nChen, and Dahua Lin. Improving pixel-based mim by re-\nducing wasted modeling capability. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5361\u20135372, 2023. 6, 7, 8\n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 2, 3, 5, 7, 8\n12\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 15\n[45] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xi-\nhui Liu, Wanli Ouyang, and Lei Bai.\nFit:\nFlexible\nvision transformer for diffusion model.\narXiv preprint\narXiv:2402.12376, 2024. 2\n[46] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Explor-\ning flow and diffusion-based generative models with scalable\ninterpolant transformers. arXiv preprint arXiv:2401.08740,\n2024. 5, 6, 11, 16\n[47] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W\nBattaglia.\nGenerating images with sparse representations.\narXiv preprint arXiv:2103.03841, 2021. 5\n[48] OpenAI. Gpt-4 technical report. 2023. Technical Report. 2\n[49] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11410\u201311420, 2022.\n4\n[50] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195\u20134205,\n2023. 2, 4, 6\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2, 5\n[52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv, 2022. 5\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 4, 15\n[54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 2\n[55] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,\nTal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foun-\ndation models for code. arXiv preprint arXiv:2308.12950,\n2023. 3\n[56] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 5, 6\n[57] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00b4e,\nAlexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022. 2\n[58] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-\nattention with relative position representations.\narXiv\npreprint arXiv:1803.02155, 2018. 2\n[59] Noam Shazeer.\nGlu variants improve transformer.\narXiv\npreprint arXiv:2002.05202, 2020. 2, 3\n[60] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[61] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[62] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, page 127063,\n2023. 2, 3, 4\n[63] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2818\u20132826, 2016. 16\n[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers and distillation through at-\ntention. In International Conference on Machine Learning,\nvolume 139, pages 10347\u201310357, July 2021. 5, 7, 9, 10\n[65] Hugo Touvron, Matthieu Cord, and Herv\u00b4e J\u00b4egou. Deit iii:\nRevenge of the vit. In European Conference on Computer\nVision, pages 516\u2013533. Springer, 2022. 2, 4, 5, 7, 9, 10, 15\n[66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, and Faisal Azhar.\nLlama:\nOpen and efficient foundation language models.\n2023. 1, 2, 9\n[67] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 1, 2\n[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[69] Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet\nvs transformer, supervised vs clip: Beyond imagenet accu-\nracy. arXiv preprint arXiv:2311.09215, 2023. 6\n[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions.\nIn Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 568\u2013578, 2021. 2\n[71] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In Proceedings of\n13\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14668\u201314678, 2022. 3, 6, 7, 8\n[72] Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, and Xi-\nangxiang Chu. Lenna: Language enhanced reasoning detec-\ntion assistant. arXiv preprint arXiv:2312.02433, 2023. 2\n[73] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien\nDemouth, and Song Han. Smoothquant: Accurate and effi-\ncient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages\n38087\u201338099. PMLR, 2023. 1\n[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In Proceedings of the European conference on computer\nvision (ECCV), pages 418\u2013434, 2018. 7\n[75] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9653\u20139663, 2022. 3, 6, 7, 9\n[76] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,\nKarthik Abinav Sankararaman, Barlas Oguz, et al. Effective\nlong-context scaling of foundation models. arXiv preprint\narXiv:2309.16039, 2023. 3\n[77] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce\nBian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan,\net al. Baichuan 2: Open large-scale language models. arXiv\npreprint arXiv:2309.10305, 2023. 2\n[78] Yang You, Igor Gitman, and Boris Ginsburg.\nLarge\nbatch training of convolutional networks.\narXiv preprint\narXiv:1708.03888, 2017. 16\n[79] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 6023\u20136032, 2019. 16\n[80] Biao Zhang and Rico Sennrich. Root mean square layer nor-\nmalization. Advances in Neural Information Processing Sys-\ntems, 32, 2019. 2, 3, 9, 10\n[81] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 16\n[82] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 7\n[83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. MiniGPT-4: Enhancing vision-language\nunderstanding with advanced large language models. In The\nTwelfth International Conference on Learning Representa-\ntions, 2024. 2\n14\nA. More Experiments\nWe evaluate the image generation using the 250 steps\nODE sampler (dopri5) based on the SiT framework in Ta-\nble 13.\nB. Hyperparameters\nB.1. Supervised Training of VisionLLaMA on\nImageNet-1K\nAs for the plain transformer, we use the same hyperpa-\nrameters as [65]. The detailed setting is provided in Ta-\nble 14.\nVisionLLaMA-S is trained on ImageNet-1K for\n800 epochs with a resolution of 224\u00d7224. VisionLLaMA-\nB is first trained for 800 epochs with an input size of\n192\u00d7 192 and then fine-tuned for 20 epochs on 224 \u00d7 224.\nVisionLLaMA-L is first trained for 800 epochs with a res-\nolution of 160 \u00d7 160 and then finetuned for 20 epochs on\n224 \u00d7 224.\nB.2. Supervised Training of Pyramid VisionL-\nLaMA\nWe use the same setting as [12]. Specifically, all the\nmodels are trained on ImageNet-1K for 300 epochs with a\nglobal batch size of 1024 using the AdamW optimizer. The\nlearning rate is increased to 0.001 within 5 warm-up epochs\nand decayed to zero following the cosine schedule. We use\nthe same data augmentation as [12] and an image resolution\nof 224\u00d7224 for all models. To avoid overfitting, we use a\nweight decay of 0.05 and drop path [31] (0.2, 0.3, 0.5 for\nsmall base and large models respectively).\nB.3. Mask Image Modeling on ImageNet\nWe use AdamW optimizer with momentum \u03b21 = 0.9\nand \u03b22 = 0.95. The global batch size is 4096. The ini-\ntial learning rate is 1.5\u00d710\u22124 and decayed to zero within\n800 or 1600 epochs.\nWe also use 40 epochs to warm\nup the learning rate. We only use simple data augmenta-\ntion RRC(random-resize-crops)as [25]. Besides, we use a\nweight decay of 0.05.\nB.4. Linear Probing on ImageNet\nWe follow the setting of [25] and show the details in Ta-\nble 15.\nB.5. SFT on ImageNet for SSL-pre-trained Models\nWe follow the same setting of [25] and show the details\nin Table 16. The only modification is the layer-wise learn-\ning rate decay because we find 0.75 of [25] is overfitting for\nour method, and we set it to 0.45.\nB.6. Segmentation for SSL-pre-trained Models\nWe follow the default setting of [12]. We use AdamW\n[44] optimizer with \u03b21 = 0.9 and \u03b22 = 0.999. The global\nbatch size is 16. The initial learning rate is 6\u00d7 10\u22125 and\nlinearly decayed to zero. We also use 1500 iterations to\nwarm up. We also utilize l2 weight decay of 0.05 and a\ndrop-path rate [31] of 0.1.\nB.7. Segmentation for Pyramid Transformers\nWe follow the setting of [12], which is almost the same\nas B.6.\nWe use a drop-path rate of 0.2 for the pyramid\nVisionLLaMA-B model.\nB.8. Object Detection of Pyramid Transformers\nWe use the same setting as [12]. We use the AdamW op-\ntimizer with \u03b21 = 0.9 and \u03b22 = 0.999. All the models are\ntrained for 36 epochs with a global batch size of 16. The\ninitial learning rate is 1\u00d710\u22124 with 1000 iterations warm\nup and decayed by 10.0 at epoch 27 and 33. To avoid over-\nfitting, we apply a l2 weight decay for all models.\nB.9. Object Detection of Plain Transformers\nWe use the AdamW optimizer with \u03b21 = 0.9 and \u03b22 =\n0.999. The training resolution is fixed as 1024 \u00d7 1024 as\n[39]. Our model is trained for 36 epochs with a global batch\nsize of 64. The initial learning rate is 1\u00d710\u22124 with 1000\niterations warm up and decayed by 10.0 at epoch 27 and 33.\nWe use a L2 weight decay of 0.1. We also apply layer-wise\nlearning rate decay with 0.7 as [39].\nB.10. Image Generation of DiT-LLaMA\nWe use the same VAE as [53].\nWe make use of the\nAdamW optimizer with momentum \u03b21 = 0.9 and \u03b22 =\n0.999. We use a global batch size of 256 across all mod-\nels. The learning rate is fixed as 1 \u00d7 10\u22124. The training\nresolution is 256 \u00d7 256. As for inference, we use 250 steps\nDDPM. We keep the default setting of ADM [21] without\ntuning. Specifically, we use tmax = 1000 linear schedule\nwith \u03b2 from 0.0001 to 0.02 and learnable variance \u03c3\u03b8.\nB.11. Image Generation of SiT-LLaMA\nWe use the same VAE as SD [53]. As for the ODE sam-\npler, we utilize dopri5 and set atol and rtol to 1e-6 and 1e-3\nrespectively.\nC. Architecture Setting\nC.1. Pyramid VisionLLaMA\nThe detailed setting of the pyramid architecture is shown\nin Table 17.\nC.2. Plain Transformer for Vision Understanding\nThe detailed setting of the architecture is shown in Ta-\nble 18.\n15\nModel\nFlops (G) Params (M) Training Steps (K) Learning Rate FID\u2193 sFID\u2193 Precision\u2191 Recall\u2191\nIS\u2191\nSiT-S/2 \u2020\n6.06\n33\n400\n0.0001\n59.60\n9.16\n39.41\n58.48\n23.32\nSiT-LLaMA-S/2\n6.06\n33\n400\n0.0001\n54.62\n8.81\n41.69\n61.16\n26.07\nSiT-B/2 \u2020\n23.01\n130\n400\n0.0001\n36.90\n6.70\n51.00\n64.10\n39.78\nSiT-LLaMA-B/2\n23.01\n130\n400\n0.0001\n30.23\n6.36\n54.99\n64.90\n48.34\nDiT-L/2\n80.71\n458\n400\n0.0001\n23.3\n-\n-\n-\n-\nSiT-L/2 \u2020\n80.71\n458\n400\n0.0001\n20.14\n5.34\n61.53\n64.53\n67.08\nSiT-LLaMA-L/2\n80.71\n458\n400\n0.0001\n14.91\n5.16\n64.97\n64.30\n82.23\nSiT-XL/2 \u2020\n118.64\n675\n400\n0.0001\n17.83\n5.13\n63.52\n64.61\n73.64\nSiT-LLaMA-XL/2\n118.64\n675\n400\n0.0001\n12.79\n5.02\n66.77\n64.37\n90.93\nTable 13. Image generation comparisons using the SiT framework [46] All the models are trained using an image resolution of 256\u00d7256\nwith a global batch size of 256. Metrics are calculated using the sampled 50k images without classifier-free guidance. IS: inception score.\nThe FID is calculated by 250 steps ODE sampler because of the efficiency, which is a bit different from [46]. \u2020: reproduced result using\nthe released code.\nSec B.2\nSec B.1\nBatch size\n1024\n2048\nOptimizer\nAdamW\nLAMB\nLR\n1.10\u22123\n3.10\u22123\nLR decay\ncosine\ncosine\nWeight decay\n0.05\n0.02\nWarmup epochs\n5\n5\nLabel smoothing \u03b5\n0.1\n\u2717\nDropout\n\u2717\n\u2717\nStoch. Depth\n\u2713\n\u2713\nRepeated Aug\n\u2713\n\u2713\nGradient Clip.\n\u2717\n1.0\nH. flip\n\u2713\n\u2713\nRRC\n\u2713\n\u2713\nRand Augment\n9/0.5\n\u2717\n3 Augment (ours)\n\u2717\n\u2713\nLayerScale\n\u2717\n\u2713\nMixup alpha\n0.8\n0.8\nCutmix alpha\n1.0\n1.0\nErasing prob.\n0.25\n\u2717\nColorJitter\n\u2717\n0.3\nTest crop ratio\n0.875\n1.0\nLoss\nCE\nBCE\nTable 14. Training procedures with ImageNet-1K.\nC.3. Plain Transformer for Generation\nThe detailed setting of the architecture is shown in Ta-\nble 19.\nconfig\nvalue\noptimizer\nLARS [78]\nbase learning rate\n0.1\nweight decay\n0\noptimizer momentum\n0.9\nbatch size\n16384\nlearning rate schedule\ncosine decay\nwarmup epochs\n10\ntraining epochs\n90\naugmentation\nRandomResizedCrop\nTable 15. Linear probing setting.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n1e-3\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nlayer-wise lr decay [3]\n0.45\nbatch size\n1024\nlearning rate schedule\ncosine decay\nwarmup epochs\n5\ntraining epochs\n100 (B), 50 (L)\naugmentation\nRandAug (9, 0.5) [16]\nlabel smoothing [63]\n0.1\nmixup [81]\n0.8\ncutmix [79]\n1.0\ndrop path\n0.1\nTable 16. End-to-end fine-tuning setting for SSL.\n16\nOutput Size\nLayer Name\nS\nB\nL\nStage 1 H\n4 \u00d7 W\n4\nPatch Embedding P1 = 4; C1 = 64 P1 = 4; C1 = 96 P1 = 4; C1 = 128\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\nStage 2 H\n8 \u00d7 W\n8\nPatch Embedding P2 = 2; C2 = 128 P2 = 2; C2 = 192 P2 = 2; C2 = 256\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\n\u0014 LSA\nGSA\n\u0015\n\u00d7 1\nStage 3 H\n16 \u00d7 W\n16\nPatch Embedding P3 = 2; C3 = 256 P3 = 2; C3 = 384 P3 = 2; C3 = 512\n\u0014 LSA\nGSA\n\u0015\n\u00d7 5\n\u0014 LSA\nGSA\n\u0015\n\u00d7 9\n\u0014 LSA\nGSA\n\u0015\n\u00d7 9\nStage 4 H\n32 \u00d7 W\n32\nPatch Embedding P4 = 2; C4 =512 P4 = 2; C4 =768 P4 = 2; C4 =1024\n\u0002 GSA \u0003\n\u00d7 4\n\u0002 GSA \u0003\n\u00d7 2\n\u0002 GSA \u0003\n\u00d7 2\nTable 17. Configuration details of Pyramid VisionLLaMA.\nModel\nLayers\nDims\nHeads\nVisionLLaMA-S\n12\n384\n6\nVisionLLaMA-B\n12\n768\n12\nVisionLLaMA-L\n24\n1024\n16\nTable 18. Architecture settings for VisionLLaMA on image un-\nderstanding tasks.\nModel\nLayers\nDims\nHeads\nDiT-LLaMA-S / SiT-LLaMA-S\n12\n384\n6\nSiT-LLaMA-B / SiT-LLaMA-B\n12\n768\n12\nSiT-LLaMA-L / SiT-LLaMA-L\n24\n1024\n16\nSiT-LLaMA-XL / SiT-LLaMA-XL\n28\n1152\n16\nTable 19. Architecture settings for VisionLLaMA on image gen-\neration.\n17\n"
  },
  {
    "title": "Learning and Leveraging World Models in Visual Representation Learning",
    "link": "https://arxiv.org/pdf/2403.00504.pdf",
    "upvote": "22",
    "text": "Learning and Leveraging World Models in Visual\nRepresentation Learning\nQuentin Garrido1,2, Mahmoud Assran1, Nicolas Ballas1, Adrien Bardes1,3, Laurent Najman2,\nYann LeCun1,4,5\n1FAIR at Meta, 2Univ Gustave Eiffel, CNRS, LIGM, F-77454 Marne-la-Vall\u00e9e, France, 3INRIA,\n4Courant Institute, New York University, 5Center for Data Science, New York University\nJoint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach\nthat learns by leveraging a world model. While previously limited to predicting missing parts of an\ninput, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We\nintroduce Image World Models, an approach that goes beyond masked image modeling and learns\nto predict the effect of global photometric transformations in latent space. We study the recipe of\nlearning performant IWMs and show that it relies on three key aspects: conditioning, prediction\ndifficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can\nbe adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or\nsurpasses the performance of previous self-supervised methods. Finally, we show that learning with\nan IWM allows one to control the abstraction level of the learned representations, learning invariant\nrepresentations such as contrastive methods, or equivariant representations such as masked image\nmodelling.\nCorrespondence: Quentin Garrido at garridoq@meta.com\n1\nIntroduction\nLearning and leveraging world models is common\npractice in reinforcement learning (RL), with demon-\nstrable success in the last few years in particular Ha\nand Schmidhuber (2018); Hafner et al. (2019, 2023).\nWorld models are commonly learned by training a\nnetwork to predict the consequence of an action, ei-\nther in input space (Yang et al., 2023), or in latent\nspace (Hu et al., 2023; Hafner et al., 2023). Given\nsuch a broad view of world modelling, we seek to ex-\nplore whether learning and leveraging world models\ncan also be benificial in visual representation learning.\nA wide family of self-supervised learning approaches\nare based on encoder-predictor architectures, wherein\nthe encoder-predictor networks are trained to pre-\ndict transformations of the data; e.g., masked image\nmodelling (Bao et al., 2021; He et al., 2021), joint-\nembedding architectures (Grill et al., 2020; Xie et al.,\n2022; Assran et al., 2023; Baevski et al., 2022), or\nequivariant prediction objectives (Gupta et al., 2023;\nGarrido et al., 2023b). If we regard transformations\nof the data as \u201cactions,\u201d then we can easily relate self-\nsupervised learning approaches to world-modelling\nin reinforcement learning; see figure 2.\nFor instance, the decoder network in masked au-\nBrightness: 1.19\nContrast: 1.72\nSaturation: 0.96\nHue: 0.16\nColorized\nBrightness: 0.95\nContrast: 1.22\nSaturation: 0.97\nHue: 0.11\nDe-blurred\nBrightness: 1.74\nContrast: 1.08\nSaturation: 0.97\nHue: 0.01\nBrightness: 1.31\nContrast: 1.61\nSaturation: 1.00\nHue: -0.08\nFigure 1 Visualisation of predictions in latent space with a learned\nImage World Model. We apply an action on a source image\nin latent space and retrieve the nearest neighbour of the pre-\ndicted representation in a bank of 256 images. We see that\nIWM is capable of modeling transformations and undo cor-\nruptions, showing an understanding of the underlying image\ntransformations. Image from: ai.meta.com/blog/yann-lecun-\nadvances-in-ai-research/\ntoencoders (He et al., 2021) can be thought of as a\ngenerative image world model, which learns to infer\nthe effect of the \u201cmasking action\u201d T (a) on an image\n1\narXiv:2403.00504v1  [cs.CV]  1 Mar 2024\nEncoder\nPredictor/World Model\nTransformation, Action\nDecoder/World Model\nTransformation, Action\nTransformation, Action\nJEPA world model\nGenerative World model\nJoint Embedding\nUnconditional\nConditional\nBYOL, SimSiam\nLatent world models, I-JEPA, \nEquivariant SSL, IWM (Ours)\nDenoising Autoencoders\nVariational Autoencoders\nGenerative World Models\nMasked Image Modeling\nN/A\nSiamese, SimCLR, VICReg, DINO\nFigure 2 Multiple families of methods with related architectures can be distinguished, in which the conditioning or not of their\nworld model is a key distinction. Generative World Models are trained to invert a transformation in input space, leveraging\nan autoencoder framework. Methods for world modeling and representation learning can be instantiated in this way. Joint\nEmbedding methods get rid of the world model but operate in latent space by encoding what is common between transformed\ninputs. It is the main class of SSL methods. JEPA World Models can be seen as a more general framework where a world model is\ntrained in latent space. This family has been very successful both in reinforcement learning and in representation learning, and is\nwhere Image World Models (IWM) falls.\ny; in this case, the transformation parameters a (lo-\ncations of masked image patches), are also fed to the\ndecoder network. Methods based on joint-embedding\npredictive architectures (JEPAs), such as I-JEPA (As-\nsran et al., 2023) or data2vec (Baevski et al., 2022),\noperate similarly, but can be seen as learning a latent\nimage world model, which learns to infer the effect\nof the masking action on the representation of an\nimage. If one does not condition the predictor on\nthe transformation parameters, then the best we can\nhope for is learning representations that are invariant\nto the data transformations, as in BYOL (Grill et al.,\n2020) and SimSiam (Chen and He, 2020), wherein\nthe image transformations correspond to various pho-\ntometric and geometric data augmentations.\nHowever, despite some of the apparent similarities\nbetween world modelling in reinforcement learn-\ning and self-supervised learning from images, the\nlearned world model in reinforcement learning is typ-\nically leveraged in downstream tasks, e.g., for plan-\nning (Hansen et al., 2022). In contrast, the learned\nworld model in self-supervised learning is typically\ndiscarded after pretraining, as the main focus is often\non the representation quality of the learned encoder\nnetwork. This stems from the fact that most down-\nstream tasks in computer vision are unrelated to the\nworld modeling task. Common tasks of interest focus\non discriminative aspects and as such, even when\nthe predictor learns useful information, it is simply\ndiscarded. We postulate that discarding the world\nmodel in representation learning is wasteful, and that\njust like in RL, we can reuse this world model for\ndownstream tasks. This motivates us to study, in\nmore depth, learning world models as a paradigm for\nrepresentation learning. We thus introduce Image\nWorld Models (IWM, illustrated to the right of fig-\nure 2) as a way to learn both good representations\nand strong reusable world models. IWM is based on\nJEPA and extends the usual latent inpainting to also\ninclude photometric transformations, allowing us to\ndemonstrate the key aspects in learning a capable\nworld model, which include the choice of predictor\nconditioning, the strength of the transformations,\nand the capacity of the world model.\nWe then focus on leveraging the learned world model\nfor downstream tasks, and find that it can be lever-\naged through finetuning. Specifically, we find that\nfinetuning the world model on top of the frozen en-\ncoder for downstream tasks provides improved perfor-\nmance over encoder finetuning; this is also achieved\nat a fraction of the cost and number of finetuned\nparameters. Moreover, only the world model learned\nby IWM exhibits this behavior; finetuning a ran-\ndomly initialized network of the same architecture as\nthe predictor does not provide such a performance\nimprovement. This suggests that the world model\nshould be a key part of the inference process, in-\nstead of being discarded.\nInspired by instruction\ntuning (Wei et al., 2022; Zhang et al., 2023), we fur-\nther show that the world model can be finetuned\nto solve multiple tasks at once, further improving\nefficiency.\nOur study reveals another key aspect of representa-\ntion learning with world models: the capacity given\nto the world model has a direct influence on the\n2\nlevel of abstraction of the learned representations.\nIntuitively, if the predictor is the identity (i.e., no\npredictor, middle of figure 2), the network will cap-\nture high level semantic information, as it will only\nlearn to encode what is shared between the input y\nand its transformation x. This is the driving force be-\nhind the representation quality of contrastive learning,\nwhere transformations are selected to only preserve\nthe semantics of the image. On the other hand, as\nthe predictor has more capacity and can effectively\ninvert the effect of the transformations, the output\nof the encoder can retain more information about its\ninput. These two ideas are at the core of equivariant\nrepresentation learning; a predictor that can apply\ntransformations effectively is equivariant, whereas a\npredictor that cannot is invariant. We find that a\nworld model that is invariant to transformations per-\nforms better in linear evaluation, whereas one that is\nequivariant correlates with better world model fine-\ntuning. This gives a tradeoff between ease of adaption\nand raw performance. As such, learning representa-\ntions by learning a world model gives us flexibility in\nthe properties of the representations, making this an\nattractive representation learning framework.\nOur contributions can be summarized as follows:\n\u2022 We show how to leverage JEPAs to learn an\nImage World Model (IWM). The key aspects are:\ncomplexity of transformations, conditioning on\ntransformations, and capacity of the predictor.\n\u2022 We show that equivariant world models can be\nleveraged for discriminative tasks. Finetuning\nthe predictor leads to better performance com-\npared to encoder finetuning, at a fraction of the\ncost. Inspired by instruction tuning, we also\ndemonstrate that it can be finetuned on several\ntasks at once.\n\u2022 We show that controlling the capabilities of the\nworld model gives us representations with differ-\nent properties. An invariant world model gives\nus more abstract representations and performs\nbetter in linear evaluation, akin to contrastive\nlearning. An equivariant world model preserves\nmore information about the input, giving better\npeak performance with predictor finetuning.\n2\nRelated works\n2.1\nAugmentation invariant Self-Supervised\nLearning\nAt the core of contrastive methods lies augmentation\ninvariance. Multiple augmented views of an image\nshould lead to the same representation in latent space.\nThe core of these methods is thus in how to avoid\nthese representations collapsing. Sample-contrastive\nmethods (Chen et al., 2020a; He et al., 2020; Chen\net al., 2020b; Caron et al., 2021; Chen et al., 2021; Yeh\net al., 2021; HaoChen et al., 2021; Oquab et al., 2023)\navoid this phenomenon by pushing away represen-\ntations coming from other data points. Dimension-\ncontrastive methods (Bardes et al., 2021; Zbontar\net al., 2021; Ermolov et al., 2021; Li et al., 2022;\nBardes et al., 2022) avoid collapse by considering the\nrepresentations as a whole and encouraging maximiza-\ntion of information content. Both dimension- and\nsample-contrastive methods have been shown to lead\nto very similar representations (Garrido et al., 2023a).\nPrediction based methods (Grill et al., 2020; Chen\nand He, 2020) learn by predicting the augmented\nrepresentations, but they also lead to invariant rep-\nresentations due to a lack of conditioning on the\ntransformations.\n2.2\nWorld modeling in visual representation\nlearning\nWhile world modeling is a successful paradigm in\nreinforcement learning Hafner et al. (2019, 2023) or\nvideo prediction Yang et al. (2023); Hu et al. (2023),\nit has yet to show clear benefits in representation\nlearning. However, multiple families of approaches\ncan be reframed in light of this. Equivariant self-\nsupervised learning methods (Devillers and Lefort,\n2022; Park et al., 2022; Garrido et al., 2023b; Gupta\net al., 2023; Dangovski et al., 2021) aim to predict\ntransformations of data when such transformations\nform a group.\nMasked Image Modeling He et al.\n(2021); Bao et al. (2021); El-Nouby et al. (2024);\nXie et al. (2022) learns representations by predicting\nmasked parts of the image. While these approaches\npredict in pixel space, their decoders can be seen as\ninstantiations of world models. Similarly, JEPAs (As-\nsran et al., 2023; Baevski et al., 2022) predict masked\nparts of the image, but in the latent space.\nRe-\ncently, generative approaches have been applied to\nrepresentation learning Hudson et al. (2023); Clark\nand Jaini (2023); Chen et al. (2024), and while these\napproaches seem promising, their performance still re-\nmains below contrastive or MIM approaches. Recent\nwork has also shown negative correlations between\ngeneration quality and representation quality (Chen\net al., 2024). One shared aspect among these works is\nthat the world model (predictor or decoder) is either\ndiscarded for evaluations, or only used to augment\ndata (Hudson et al., 2023). We propose to go beyond\nthese practices and show that we can learn a world\nmodel that is reusable for downstream tasks while\nstill learning high-quality representations.\n3\n3\nMethod\nWe now describe Image World Models (IWM). It\nfollows a Joint Embedding Predictive Architecture\nframework (LeCun, 2022) akin to I-JEPA (Assran\net al., 2023).\nIn this framework, the predictor\nis the instantiation of the world model.\nWe con-\nsider a world model to be capable if it can apply\ntransformations in latent space, and thus learns\nequivariant representations.\nAs such, we call a\ncapable world model equivariant 1 and a poor world\nmodel invariant.\nAn appealing aspect of using JEPAs is that ap-\nproaches which learn equivariant representations\nusing contrastive methods often have to rely on an\ninvariance loss to increase representation quality,\nwhether explicitly (Gupta et al., 2023; Garrido et al.,\n2023b), or implicitly (Chavhan et al., 2023a). On\nthe other hand, a JEPA style approach does not\nhave this drawback, as the semantic aspect of the\nrepresentation is learned through latent inpainting.\nWorking in latent space further allows the network to\nremove unnecessary information, or that which is too\nhard to predict. This makes the JEPA formulation\nattractive since, for reconstructive methods, the\nquality of the reconstruction is not necessarily\ncorrelated with representation quality Chen et al.\n(2024).\nTo train IWM, the first step is to generate source\nand target views \u2014 x and y respectively in figure 2\n\u2014 from an image I.\nTarget y. The target view is generated by applying\na random horizontal flip, a crop, and color jitter\n(brightness, contrast, saturation, hue) to the origi-\nnal image I. No destructive augmentations such as\ngrayscale are applied on the target to ensure that\nthe target has as much information as possible. We\nfurther elaborate on this choice in appendix C.\nSource x. For the source view, we start from the\ntarget y which we further transform. We first apply\nanother color jitter, as well as destructive augmen-\ntations: grayscale, blur and solarization. This set of\naugmentations is the same as the one used in con-\ntrastive SSL. Finally, we also mask parts of the image\nfollowing I-JEPA. We define our mask Mx (a set of\nindices) as the union of 4 rectangular masks. Confer\nappendix A for exact implementation details.\nAction a. We denote by ax\u2192y the transformation pa-\nrameters associated with the transformation of x to\ny, i.e., the invert of the initial transformation process.\n1This is an abuse of language as not all considered trans-\nformations form a group, but it is used for clarity.\nax\u2192y contains information about the color jitter dif-\nference between x and y as well as information on\nwhether or not each destructive augmentation was\napplied.\nWorld modeling with p\u03d5. The source and target are\nthen fed respectively through an encoder f\u03b8 and its\nexponential moving average f EMA\n\u03b8\n.\nThis gives us\nrepresentations zx = f\u03b8(x) and zy = f EMA\n\u03b8\n(y). The\nuse of the EMA network is crucial to avoid collapsed\nsolutions. To condition the predictor, acting as our\nworld model, it is fed with geometric information\nabout the target in the form of mask tokens as well\nas ax\u2192y. We denote these mask tokens as ma, which\ncorrespond to the positions in M C\nx . The predictor p\u03d5\nthen takes as input the embedded source patches xc,\ntransformation parameters ax\u2192y and mask tokens ma.\nIts objective is then to match p\u03d5 (zx, ax\u2192y, ma) = \u02c6zy\nto zy.\nLoss. The loss function used is a squared L2 distance\nbetween the predictions \u02c6zy and their targets zy:\nL(x, y) =\nX\ni\u2208M C\nx\n\u2225p\u03d5 (f\u03b8(x), ax\u2192y, ma)i\u2212f EMA\n\u03b8\n(y)i\u22252\n2.\n3.1\nArchitecture and nomenclature\nOur encoder is a Vision Transformer (Dosovitskiy\net al., 2021), in particular we use the ViT-B/16 archi-\ntecture. Our predictor is based on the same architec-\nture with different depth and embedding dimension.\nWe denote instances of IWM as IWMZ\nX,Y where X\nis the depth of the predictor, Y its embedding di-\nmension, and Z is either Inv or Equi depending on\nthe capabilities of the world model. For example\nIWMEqui\n18,384 means that the predictor is 18 layers deep,\nwith 384 dimensional embeddings and exhibits equiv-\nariant behavior, i.e., has learned a versatile world\nmodel.\n4\nLearning an Image World Model for\nrepresentation learning\n4.1\nEvaluating the quality of the world model\nAs discussed previously, learning equivariant represen-\ntations and learning a world model are closely related\nproblems. As such, we can borrow metrics from the\nequivariance literature to evaluate the quality of a\ntrained world model. We rely on Mean Reciprocal\nRank (MRR) (Kipf et al., 2019) as our main metric.\nTo compute it, we generate a bank of augmented\ntarget images (256 in practice). We feed the repre-\nsentation of the clean image through the predictor\n4\nTable 1 Influence of predictor conditioning on the quality of the\nworld model. Both Sequence and Feature conditioning lead to\ngood world models .Gray is our default setting.\nConditioning:\nNone\nSequence\nFeature\nMRR\n0.00\n0.82\n0.79\nwith the goal of predicting the target image. We\nthen compute the distance between the prediction\nand the augmented representation bank from which\nwe get the rank of the target in this NN-graph. Aver-\naging the reciprocal ranks over multiple images and\ntransformations gives us MRR which tells us about\nthe quality of the world model. A MRR close to\n1 means that the world model is able to apply the\ntransformation, on the contrary a MRR close to 0\nmeans that it cannot.\n4.2\nLearning a strong Image World Model\nIn order to build a performant IWM, we isolate three\nkey aspects: conditioning the predictor on transfor-\nmations (or actions), controlling the complexity of\nthe transformations, and controlling the capacity of\nthe predictor. We show that not caring properly for\neither of those leads to invariant representations.\nWorld model conditioning. We study two approaches\nto condition the predictor on the transformation in-\nformation.\nSequence conditioning. One approach is simply to add\ntokens representing the transformation to the input of\nthe predictor. Although this seems straightforward,\nit needs to be implemented in a way that breaks the\npermutation equivariance of the transformer predic-\ntor. To do so, every token is fed through a unique\nlinear layer that allows the network to transform the\ninformation in a way that can be disambiguated by\nthe predictor.\nFeature conditioning. Another option is to mix the\ninformation between the transformation and mask\ntokens by adding the conditioning as extra dimen-\nsions, then feeding the mask tokens through a 1x1\nconvolutional neural network to mix the information\nin the mask tokens and map back to the right dimen-\nsionality.\nAs we can see in Table 1, no conditioning leads to\na world model that cannot apply transformations\nwhereas both conditioning using the sequence or fea-\nture axes leads to good world models. We use the\nfeature conditioning in practice as it leads to higher\ndownstream performance.\nTransformation complexity. We rely on data augmen-\ntation as used in contrastive approaches, consisting\nof color jitter (brightness, hue, contrast, saturation),\nTable 2 Impact of predictor architecture and transformations\non MRR. Learning an effective world model requires complex\ntransformations and adequate predictor capacity. Gray is our\ndefault setting. Red and Green respectively indicate invariant\nand equivariant behavior.\nPredictor:\nI-JEPA\nIWM\n(depth, dim.):\n(12,384)\n(12,384)\n(18,384)\nJitter\n0.00\n0.11\n0.25\n+ Destructive\n0.00\n0.09\n0.79\n+ Strong Jitter\n0.00\n0.81\n0.85\ngrayscale, blur, and solarization. We refer to the\nlast three as destructive since they remove informa-\ntion. Beyond the set of transformations modeled,\ntheir strength must also be adequate to learn a use-\nful world model. If the prediction task is too easy,\nthen the predictor will not learn anything useful. As\npresented in Table 2, the stronger the augmentations,\nthe easier it is to learn a strong world model. We\nprovide more detailed ablations on the augmentations\nin Appendix C, where we see the trend continuing\non a wider range of augmentation scenarios.\nWorld model capacity. If the transformation is com-\nplex, the predictor needs more capacity to be able\nto apply it, motivating capacity as a crucial factor\nin learning Image World Models. As we can see in\nTable 2, a deeper predictor enables us to learn a\nstrong world model on a wider range of augmenta-\ntions, and is key to the success of IWM. We study\nin more detail the influence of depth on achieving a\ngood world model in appendix C. For 12 layers, jitter\nequivariance is achieved 1 out of 5 times whereas for\nthe 18 layers, it is achieved 4 out of 5 times. As such,\npredictor capacity is a key component of a strong\nworld model.\n4.3\nVisualizing predictions.\nIn the same way that we computed MRR, we can\ncompare the predicted representations to a bank of\ntransformed images and look at the image associated\nto the prediction\u2019s nearest neighbor. As we see in\nFigure 1 the world model learned by IWM is able to\nproperly apply transformations in latent space. We\ncan however see some inaccuracies when inverting\ngrayscale as it is not properly invertible. These visu-\nalisations help reinforce the fact that IWM is able to\nlearn strong world models for image transformations.\nConfer appendix I for more visualizations.\n5\nTable 3\nHow to predict for predictor finetuning.\nUsing the\nteacher improves performance, and the exact prediction task is\nnot crucial. Null latents are more flexible and perform better.\nFor better efficiency, a full prediction is not needed but leads\nto a small drop in performance. Gray is our default setting.\nSetting\nImageNet Top-1 (%)\nGap\nDefault\n82.9\n-\n+ Teacher\n83.2\n+ 0.3\n+ Null latents\n83.3\n+ 0.1\n+ Pred only one token\n82.8\n-0.5\n5\nLeveraging world models for down-\nstream tasks\nA limitation of world models learned on images is\nthat the task they solve is not aligned with most\ndownstream tasks. We showed that IWM can apply\ncolor jitter or colorize images, but these are not the\ntasks that drive applications of computer vision. This\nis in contrast with LLMs where predicting the next\ntoken is one of the main applications of such mod-\nels. We thus study how to leverage a world model\nin vision, for tasks that go beyond applying transfor-\nmations. We focus on discriminative tasks such as\nimage classification and image segmentation.\n5.1\nPredictor finetuning\nFor any task, the evaluation head needs to under-\nstand the learned latent space and leverage it to solve\nthe problem at hand. This is something our learned\npredictor can do, suggesting that it has learned use-\nful information that is not necessarily present in the\nencoder. However, since the predictor is trained to\npredict another valid representation, its output has\nno reason to lead to better downstream performance\nif used as is.\nThis is why the predictor needs to\nbe finetuned to solve discriminative tasks. We thus\nfocus on comparisons with finetuning protocols, fol-\nlowing He et al. (2021). All methods studied are\npretrained and evaluated on ImageNet Deng et al.\n(2009) and use ViT-B/16 as encoders.\nPrediction task. When finetuning the predictor, we\nstill need to use it for a prediction task. In Table 3,\nwe study various ways to define the prediction task\nand how it impacts performance. The first aspect\nwe notice is that using the teacher network improves\nperformance over the student. Using a random trans-\nformation or not is not an important factor, and the\nmost important one is to predict another full image.\nThis makes the evaluation more flexible as we do\nnot have to reuse the pretraining objective for our\nevaluation. Using a CLS token to aggregate infor-\n1\n2\n3\n4\n6\n8\n12\nNumber of finetuned parameters\n in number of encoder blocks\n77\n78\n79\n80\n81\n82\n83\nImageNet Top-1 (%)\nMAE\nIWMInv\n12, 384\nIWMEqui\n18, 384\nIWMEqui pred. ft.\nFigure 3\nFinetuning efficiency.\nWhen taking into account\nthe number of finetuned parameters, predictor finetuning is\nsignificantly more efficient than finetuning the encoder.\nmation instead of a full image prediction is also a\nvalid strategy, though it lowers the performance by\nhalf a point. This techniques has the advantage of\nbeing cheaper (N + 1 tokens vs 2N) so it can be a\ngood alternative depending on the use case. Over-\nall, the simplest approach is the best: predicting an\nuntransformed version of the full image. This makes\nthe finetuning protocol easily reusable as it is not\ndependent on the pretraining task. We provide more\ndetailed ablations in appendix D.\nGeneral Results. In Table 4, we compare predictor\nfinetuning to encoder finetuning and end-to-end fine-\ntuning of both the predictor and encoder, using ViT-\nB/16 for the encoder. We see that IWM maintains\nor improves performance over I-JEPA and that an\ninvariant behavior is better in encoder finetuning.\nInterestingly, predictor finetuning of the equivariant\nIWM is able to match the performance of finetuning\nof the invariant model\u2019s encoder. This shows that\nthe protocol can be competitive as it trades param-\neters at inference time for a more computationally\nfriendly adaptation. While this evaluation increases\nthe number of parameters used at inference time, it\nstill amortizes the forward pass through the back-\nbone, something that full finetuning does not do. As\nsuch, as soon as multiple tasks are considered, using\nthe finetuned predictor provides a higher throughput\nthan regular finetuning.\nWhen comparing the use of a randomly initialized\npredictor (i.e., a large evaluation head) versus a pre-\ntrained predictor, we see negligible gains for MAE.\nThis suggests that the world model learned by MAE\nis not better than a randomly initialized network for\nclassification. For I-JEPA and IWM with an invari-\nant world model, we see gains in performance lower\nthan 1 point, suggesting that the world model is not\npowerful enough to be leveraged. However, when\nlooking at IWM with an equivariant world model, we\nsee a gain of 1.8 points over a random predictor. This\nshows that the predictor has learned useful informa-\ntion and properties that bring additional benefit to\n6\nTable 4 Finetuning evaluations on ImageNet-1k. We evaluate prediction based methods by finetuning their encoder, by keeping\nthe encoder frozen and finetuning their predictive world model or by finetuning both. Finetuning the world model is highly\neffective with IWM when it exhibits an equivariant behavior. This behavior is absent or less clear with other methods, showing\nthe importance of a strong world model.\nMethod\nEpochs\nNo predictor\nFrozen encoder, tuned predictor\nEnd to end\nEncoder\nRandom Init.\nPretrained\nMAE\n300\n82.7\n82.4\n82.7 (+0.3)\n82.3\n1600\n83.6\n83.0\n83.1 (+0.1)\n83.3\nI-JEPA\n300\n83.0\n79.1\n80.0 (+0.9)\n82.0\nIWMInv\n12,384\n300\n83.3\n80.5\n81.3 (+0.8)\n82.7\nIWMEqui\n18,384\n300\n82.9\n81.5\n83.3 (+1.8)\n84.4\nTable 5 Peak performance achieved from a single pretraining\ninstance. We compare ImageNet Top-1 accuracy with a frozen\nencoder or when allowing any evaluation head with any proto-\ncol, finetuning or not, with a predictor on top of the encoder\nor not.\nMethod\nEpochs\nFrozen Encoder\nAny protocol\nDINO\n1600\n82.0\n82.8\nMOCOv3\n300\n76.4\n83.2\niBOT\n1600\n83.0\n84.0\nMAE\n1600\n83.1\n83.6\nI-JEPA\n300\n80.0\n82.0\nIWMInv\n12,384\n300\n81.3\n83.3\nIWMEqui\n18,384\n300\n83.3\n84.4\nwhat the encoder has learned.\nThe performance can be pushed further by finetuning\nend-to-end both the encoder and predictor, and IWM\nis able to outperform every other finetuning proto-\ncols. This allows us to get more performance out of\na single pretraining since the world model is always\ntrained. We hypothesize that the lack of performance\nfor most approaches on end-to-end finetuning comes\nfrom the optimization complexity of finetuning a part\nof the network (encoder) while training from scratch\nanother part (the predictor). We see in Table 5 that\nwhen aggregating the performance over all protocols,\nleveraging our IWM leads to the best performance\nwith a frozen encoder, that is when allowed to lever-\nage every part of the pretraining. Confer Appendix A\nfor detailed performances.\nImage Segmentation. We study in Table 6 the perfor-\nmance of I-JEPA and IWM on an image segmentation\ntask on ADE20k. We observe similar trends as in\nimage classification where the invariant model leads\nto the best encoder. However, finetuning the pre-\ndictor with an equivariant model leads to significant\ngain over it, outperforming encoder finetuning by a\nlarge margin. Again, we observe gains in end-to-end\nfinetuning. This further validates the potential of our\nIWM to be leveraged for a wide range of tasks. We\nTable 6\nFinetuning for segmentation on ADE20k.\nSimilar\nto image classification, we observe that predictor finetuning\nimproves performance and outperforms encoder finetuning.\nMethod\nEncoder\nPredictor\nEnd to end\nI-JEPA\n44.2\n45.4\n45.1\nIWMInv\n12,384\n45.6\n45.7\n46.5\nIWMEqui\n18,384\n44.2\n46.8\n47.0\nprovide additional details in Appendix A.2.\nEfficiency. In Figure 3, we study the efficiency of\npredictor finetuning compared to encoder finetuning.\nWe see that when the number of parameters is com-\nparable, and at multiple predictor sizes, predictor\nfinetuning with IWM outperforms encoder finetuning\nby around 1 point compared to MAE, and by 1.5\npoints over IWM. This means that predictor finetun-\ning is not only is a competitive protocol performance\nwise, but also with respect to efficiency of adaptation.\nWe further study the behavior of IWM with a ViT-\nL/16 in section E. When comparing the end-to-end\nfinetuning of a ViT-B with encoder finetuning of a\nViT-L, we observe a gain in performance (84.4% vs\n84.3%) with a fraction of the parameters (121M vs\n307 M). This further shows how efficient leveraging\nthe world model learned by IWM is, and that reusing\nall parts of your pretraining can prove as effective as\nscaling the encoder.\n5.2\nMultitask predictor tuning\nWe previously discussed efficiency gains when com-\npared to encoder finetuning, but can improve effi-\nciency even further. One of the main goal of repre-\nsentation learning is to obtain representations that\ncan be used for a variety of tasks. And just like the\npredictor was trained to solve a variety of task (col-\norization, inpainting, changing color) we show that\nit can be finetuned on multiple tasks, inspired by\nprefix tuning (Li and Liang, 2021) and instruction\n7\nTable 7 Multi-task finetuning. Finetuning the predictor on\nmultiple tasks at once performs similarly as finetuning it\non each task separately. This enables the use of a single\nprediction head for multiple task, amortizing its cost.\nDataset\nSingle-task\nMulti-task\nDifference\nImageNet\n80.8\n79.6\n-1.2\niNat18\n72.4\n72.0\n-0.4\nSUN397\n75.6\n78.2\n+2.6\nPlaces205\n64.8\n64.1\n-0.7\nAverage\n73.4\n73.5\n+0.1\ntuning Wei et al. (2022); Zhang et al. (2023) in LLMs.\nThe general idea, that we illustrate graphically in sup-\nplementary Figure S2, is to give new learned tokens\nto the predictor to indicate which task it is trying to\nsolve. This is reminiscent of DyTox Douillard et al.\n(2022) which uses task tokens for continual learning.\nFor each task, we thus have a task token, as well as\na task specific head and/or loss function. All of the\ntask losses are then combined, and the predictor, as\nwell as task specific heads, are updated. We study\na simple scenario where the batch is evenly split be-\ntween tasks, noting that other sampling strategies\nmay lead to further improved performance.\nWe evaluate in Table 7 IWMEqui\n18,384 (pretrained on Ima-\ngeNet) on ImageNet, iNaturalist18 (Horn et al., 2018),\nSUN397 (Xiao et al., 2010), and Places205 (Zhou\net al., 2014). For each task we train a single-task\nbaseline where the total number of iterations is iden-\ntical to the multi-task training. As such, training all\nfour single-task baselines has exactly the same cost\nas the multi-task, although it leads to four different\nmodels instead of one. The multi-task predictor is\nable to achieve similar performance as the single-\ntask predictors, with a moderate drop on most tasks\nbut a significant increase in performance on SUN397.\nOn average it achieves the same performance as the\nsingle-task predictors.\nThis further demonstrates\nthe efficiency gains of leveraging good world models,\nwhere the parameters are now shared across all tasks,\nmaking predictor finetuning lightweight at inference\ntime for every task.\nOverall, when a good world model is learned, it can\nbe reused for downstream tasks by finetuning it. This\nleads to performance rivaling with encoder-finetuning\nat a fraction of the cost. It can be made even more\nefficient by doing a multi-task finetuning, highlighting\nthe versatility of this approach.\nTable 8 Linear and attentive probing performance on ImageNet-\n1k. IWMInv performs similarly to contrastive methods and\nIWMEqui to mask modeling ones.\nMethod\nEffective Epochs\nLinear\nAttentive\nMoCoV3\n300\n76.3\n76.4\nMAE\n300\n60.2\n73.5\nMAE\n1600\n68.0\n76.0\nI-JEPA\n300\n70.0\n75.0\nIWMInv\n12,384\n300\n74.5\n77.0\nIWMEqui\n18,384\n300\n67.5\n75.1\n6\nImage World Models enable flexible\nrepresentations\nTo complete our analysis of IWM for representation\nlearning, we study how it performs on lightweight\nevaluation protocols that are commonly used in self-\nsupervised learning. We focus on linear Chen et al.\n(2021) and attentive probing Chen et al. (2023).\nAs we see in Table 8, when IWM learns an invariant\nworld model, it achieves a behavior akin to contrastive\napproaches such as MoCov3 with significant perfor-\nmance gains in linear evaluation compared to MIM\nor other JEPA based approaches. Similarly, when\nIWM learns an equivariant world model, its behavior\nis akin to MIM methods such as MAE with lower\nperformance in linear evaluation but more competi-\ntive performance in attentive probing.\nThis suggests that a big difference between methods\nis not necessarily in the quality of the representation\nbut in their abstraction level, i.e., how easy it is to\nextract information from them. Linear probing be-\ning one of the simplest evaluations, attentive being\nslightly more elaborate and finetuning being a more\ncomplex protocol.\nIn Figure 4, we see clear links between most suited\nevaluation protocols and equivariance of the world\nmodel. More invariant world models excel in linear\nevaluation and equivariant world models shine with\nlarger evaluation heads such as in predictor finetun-\ning. We also note that the richer representations\nstemming from equivariant world models lead to bet-\nter performance on OOD datasets(see appendix F).\nThis allows us to place families of approaches on a\nspectrum of representation abstraction in Figure 5.\nContrastive methods occupy the high abstraction end\nof the spectrum, with information that is easily ex-\ntractible with a simple protocol. However they suffer\nfrom lower peak performance when ignoring the adap-\ntation cost, as seen in Table 5. On the opposite end\nlies Masked Image Modeling, which offers stronger\nperformance with complex evaluations such as fine-\n8\n0.00\n0.25\n0.50\n0.75\nMRR\n66\n68\n70\n72\n74\nImageNet Top-1 (%)\n= -0.85\nLinear\n0.00\n0.25\n0.50\n0.75\nMRR\n74\n75\n76\n77\n= -0.37\nAttentive\n0.00\n0.25\n0.50\n0.75\nMRR\n80\n81\n82\n83\n= 0.93\nPredictor finetuning\nEqui.\nInv.\nFigure 4\nWhile the level of equivariance influences perfor-\nmance in Linear and Predictor finetuning setting, it is hardly\ncorrelated to Attentive probing. This suggests that there is a\ntrade-off in terms of the level of abstraction of the representa-\ntion, and that different evaluation protocols evaluate different\nproperties.\nRepresentation\nAbstraction\nRepresentation\ncontent\nBest evaluations\nMore equivariant More invariant\nMIM\nJEPA\nContrastive\nTwo red Macaws \non the right\nMacaw\nFinetuning\nAttentive Linear\nHigh\nLow\nSimple\nComplex\nPred. ft.\nImage World Models\nFigure 5 Image World Models allow representation modularity.\nDifferent families of methods offer representations with differ-\nent properties, but IWM allows exploring the whole spectrum.\ntuning but suffers in linear probing as information is\nnot as easily accessible. By varying the equivariance\nof the world model, IWM is able to occupy the spec-\ntrum in between contrastive approaches and MIM, as\nwe can see in Figure 4 and Table 8 with IWMInv\n12,384\nand IWMEqui\n18,384 being the two extremes of the IWM\nspectrum.\nThis spectrum can be summarized by the SSL ethos\nof \"Learning what is predictible\". Learning with a\nweak world model means that it cannot model the\nworld properly and the encoder removes the informa-\ntion that cannot be predicted. On the other hand, if\nthe world model is very powerful, the representation\ndoes not need to be as abstract or semantic as it can\nfind a way to predict representations in any situation.\nThis means that learning a world model offers a mea-\nsurable way to control the level of abstraction of the\nrepresentations.\n7\nConclusion and future perspectives\nWe introduced IWM, an approach to learn self-\nsupervised visual representations with world mod-\nels. With an in-depth study, we provided guidelines\nand key components for learning a good image world\nmodel. Conditioning the world model with the im-\nage transformation is crucial to avoid collapsing to\nclassical SSL behavior. Using strong transformations\nis also key to ensure that the world model learns to\nmodel more complex behavior and be useful. Finally,\nenough capacity is needed for modeling complex be-\nhaviors. We showed that only a capable world model\ncan be reused for discriminative task. This led to\nour predictor finetuning protocol that matches en-\ncoder finetuning at a fraction of the cost, showing\nthat world models are versatile evaluation heads. We\nfurther adapted it to solve multiple tasks at once\nwithout losing performance. Finally, we studied how\nlearning a world model impacts representation quality.\nA capable world model learns rich representations\nthat improve performance on downstream tasks such\nas image classification and semantic segmentation.\nAdditionally, learning an invariant world model led\nto better representations for linear evaluation. While\nMIM and Contrastive approaches are two ends of\na spectrum in terms of representation abstraction,\nImage World Models allow us to interpolate between\nthem. As such, we believe that learning image world\nmodels is a very promising framework for visual rep-\nresentation learning.\n8\nBroader impact statement\nThis paper presents work whose goal is to advance the\nfield of Machine Learning. There are many potential\nsocietal consequences of our work, none of which we\nfeel must be specifically highlighted here.\nReferences\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr\nBojanowski, Pascal Vincent, Michael Rabbat, Yann\nLeCun, and Nicolas Ballas. Self-supervised learning\nfrom images with a joint-embedding predictive archi-\ntecture. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n15619\u201315629, 2023.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu,\nJiatao Gu, and Michael Auli. Data2vec: A general\nframework for self-supervised learning in speech, vision\nand language. In International Conference on Machine\nLearning, pages 1298\u20131312. PMLR, 2022.\nHangbo Bao, Li Dong, and Furu Wei.\nBeit:\nBert\npre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021.\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg:\nVariance-invariance-covariance regularization for self-\nsupervised learning. arXiv preprint arXiv:2105.04906,\n2021.\nAdrien Bardes, Jean Ponce, and Yann LeCun. VICRegl:\nSelf-supervised learning of local visual features.\nIn\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and\n9\nKyunghyun Cho, editors, Advances in Neural Infor-\nmation Processing Systems, 2022. https://openreview.\nnet/forum?id=ePZsWeGJXyp.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herve Je-\ngou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision\ntransformers. In ICCV, 2021.\nRuchika Chavhan, Henry Gouk, Da Li, and Timothy\nHospedales. Quality diversity for visual pre-training.\nIn Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 5384\u20135394,\nOctober 2023a.\nRuchika Chavhan,\nJan Stuehmer,\nCalum Heggan,\nMehrdad Yaghoobi, and Timothy Hospedales. Amor-\ntised invariance learning for contrastive self-supervision.\nIn The Eleventh International Conference on Learn-\ning Representations, 2023b. https://openreview.net/\nforum?id=nXOhmfFu5n.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. A simple framework for contrastive\nlearning of visual representations.\nIn ICML, pages\n1597\u20131607. PMLR, 2020a.\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin,\nShentong Mo, Yunhao Wang, Shumin Han, Ping Luo,\nGang Zeng, and Jingdong Wang. Context autoencoder\nfor self-supervised representation learning. Interna-\ntional Journal of Computer Vision, pages 1\u201316, 2023.\nXinlei Chen and Kaiming He. Exploring simple siamese\nrepresentation learning. In CVPR, 2020.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming\nHe. Improved baselines with momentum contrastive\nlearning. arXiv preprint arXiv:2003.04297, 2020b.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised vision transformers.\nIn ICCV, 2021.\nXinlei Chen, Zhuang Liu, Saining Xie, and Kaiming\nHe. Deconstructing denoising diffusion models for self-\nsupervised learning, 2024.\nKevin Clark and Priyank Jaini.\nText-to-image diffu-\nsion models are zero-shot classifiers. arXiv preprint\narXiv:2303.15233, 2023.\nMMSegmentation Contributors.\nMMSegmentation:\nOpenmmlab\nsemantic\nsegmentation\ntoolbox\nand\nbenchmark.\nhttps://github.com/open-mmlab/\nmmsegmentation, 2020.\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and\nQuoc Le.\nRandaugment:\nPractical automated\ndata augmentation with a reduced search space.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages\n18613\u201318624. Curran Associates, Inc., 2020.\nhttps:\n//proceedings.neurips.cc/paper_files/paper/2020/\nfile/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf.\nRumen Dangovski, Li Jing, Charlotte Loh, Seungwook\nHan, Akash Srivastava, Brian Cheung, Pulkit Agrawal,\nand Marin Solja\u010di\u0107. Equivariant contrastive learning.\narXiv preprint arXiv:2111.00899, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nAlexandre Devillers and Mathieu Lefort. Equimod: An\nequivariance module to improve self-supervised learn-\ning. arXiv preprint arXiv:2211.01244, 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR, 2021.\nArthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon,\nand Matthieu Cord. Dytox: Transformers for continual\nlearning with dynamic token expansion. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2022.\nAlaaeldin El-Nouby,\nMichal Klein,\nShuangfei Zhai,\nMiguel Angel Bautista, Alexander Toshev, Vaishaal\nShankar, Joshua M Susskind, and Armand Joulin. Scal-\nable pre-training of large autoregressive image models.\narXiv preprint arXiv:2401.08541, 2024.\nAleksandr\nErmolov,\nAliaksandr\nSiarohin,\nEnver\nSangineto,\nand Nicu Sebe.\nWhitening for self-\nsupervised representation learning, 2021.\nQuentin Garrido, Yubei Chen, Adrien Bardes, Laurent\nNajman, and Yann LeCun. On the duality between con-\ntrastive and non-contrastive self-supervised learning.\nIn The Eleventh International Conference on Learn-\ning Representations, 2023a. https://openreview.net/\nforum?id=kDEL91Dufpa.\nQuentin Garrido, Laurent Najman, and Yann Lecun.\nSelf-supervised learning of split invariant equivariant\nrepresentations.\nIn Proceedings of the 40th Inter-\nnational Conference on Machine Learning, volume\n202 of Proceedings of Machine Learning Research,\npages 10975\u201310996. PMLR, 23\u201329 Jul 2023b. https:\n//proceedings.mlr.press/v202/garrido23b.html.\nJean-Bastien\nGrill,\nFlorian\nStrub,\nFlorent\nAltch\u00e9,\nCorentin\nTallec,\nPierre\nH.\nRichemond,\nElena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal\nPiot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal\nValko. Bootstrap your own latent: A new approach to\nself-supervised learning. In NeurIPS, 2020.\nSharut Gupta, Joshua Robinson, Derek Lim, Soledad\nVillar, and Stefanie Jegelka. Structuring representa-\n10\ntion geometry with rotationally equivariant contrastive\nlearning, 2023.\nDavid Ha and J\u00fcrgen Schmidhuber. Recurrent world mod-\nels facilitate policy evolution. In Advances in Neural\nInformation Processing Systems 31, pages 2451\u20132463.\n2018.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and\nMohammad Norouzi.\nDream to control:\nLearn-\ning behaviors by latent imagination. arXiv preprint\narXiv:1912.01603, 2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timo-\nthy Lillicrap. Mastering diverse domains through world\nmodels. arXiv preprint arXiv:2301.04104, 2023.\nNicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang,\nVikash Kumar, and Aravind Rajeswaran.\nModem:\nAccelerating visual model-based reinforcement learning\nwith demonstrations. arXiv preprint arXiv:2212.05698,\n2022.\nJeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu\nMa. Provable guarantees for self-supervised deep learn-\ning with spectral contrastive loss. NeurIPS, 34, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Doll\u00e1r, and Ross Girshick.\nMasked autoen-\ncoders are scalable vision learners.\narXiv preprint\narXiv:2111.06377, 2021.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin\nCui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species\nclassification and detection dataset. In CVPR, 2018.\nAnthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and\nGianluca Corrado. Gaia-1: A generative world model\nfor autonomous driving, 2023.\nDrew A. Hudson, Daniel Zoran, Mateusz Malinowski, An-\ndrew K. Lampinen, Andrew Jaegle, James L. McClel-\nland, Loic Matthey, Felix Hill, and Alexander Lerchner.\nSoda: Bottleneck diffusion models for representation\nlearning, 2023.\nThomas Kipf, Elise Van der Pol, and Max Welling. Con-\ntrastive learning of structured world models. arXiv\npreprint arXiv:1911.12247, 2019.\nYann LeCun. A path towards autonomous machine intel-\nligence version 0.9. 2, 2022-06-27. Open Review, 62(1),\n2022.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing\ncontinuous prompts for generation, 2021.\nZengyi Li, Yubei Chen, Yann LeCun, and Friedrich T\nSommer. Neural manifold clustering and embedding.\narXiv preprint arXiv:2201.10000, 2022.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight\ndecay regularization. In International Conference on\nLearning Representations, 2019. https://openreview.\nnet/forum?id=Bkg6RiCqY7.\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\net al. Dinov2: Learning robust visual features without\nsupervision. arXiv preprint arXiv:2304.07193, 2023.\nJung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan Willem\nvan de Meent, and Robin Walters. Learning Symmetric\nEmbeddings for Equivariant World Models, June 2022.\nhttp://arxiv.org/abs/2204.11371.\narXiv:2204.11371\n[cs].\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are\nzero-shot learners, 2022.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. Sun database: Large-\nscale scene recognition from abbey to zoo. In 2010\nIEEE computer society conference on computer vision\nand pattern recognition, pages 3485\u20133492. IEEE, 2010.\nTete Xiao, Liu Yingcheng, Bolei Zhou, Jiang Yuning,\nand Sun Jian. Unified perceptual parsing for scene\nunderstanding. In ECCV, 2018.\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A\nsimple framework for masked image modeling, 2022.\nMengjiao Yang,\nYilun Du,\nKamyar Ghasemipour,\nJonathan Tompson, Dale Schuurmans, and Pieter\nAbbeel.\nLearning interactive real-world simulators.\narXiv preprint arXiv:2310.06114, 2023.\nChun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-\nLuh Liu, Yubei Chen, and Yann LeCun. Decoupled\ncontrastive learning. arXiv preprint arXiv:2110.06848,\n2021.\nYang You, Igor Gitman, and Boris Ginsburg. Large batch\ntraining of convolutional networks.\narXiv preprint\narXiv:1708.03888, 2017.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\nRegularization strategy to train strong classifiers with\nlocalizable features. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages\n6023\u20136032, 2019.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\nSt\u00e9phane Deny. Barlow twins: Self-supervised learning\nvia redundancy reduction.\nIn ICML, pages 12310\u2013\n12320. PMLR, 2021.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin,\nand David Lopez-Paz. mixup: Beyond empirical risk\n11\nminimization. In International Conference on Learn-\ning Representations, 2018.\nhttps://openreview.net/\nforum?id=r1Ddp1-Rb.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei\nZhang, Fei Wu, and Guoyin Wang. Instruction tuning\nfor large language models: A survey, 2023.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio\nTorralba, and Aude Oliva. Learning deep features for\nscene recognition using places database. In NeurIPS,\n2014.\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic\nunderstanding of scenes through the ade20k dataset.\nIJCV, 2019.\n12\nContext\nTarget\nTransformation \nparameters\nPositions to \npredict\nWorld Model\nFigure S1 IWM (Image World Model). Starting from an image, two augmented views are produced: the source and\nthe target. The source view is partially masked to form the context and then encoded to be used as conditioning\nfor the world model, instantiated by the predictor. The target is encoded through an exponential moving average\nof the encoder, and target positions are sampled as the masked patches of the source image. Conditioned on the\ntransformation parameters between the source and target, the encoded source image, and the positions to predict, the\npredictor is trained to predict the target representations.\nA\nExperimental details\nA.1\nPretraining\nWe provide a more detailed architecture for IWM in figure S1.\nArchitecture and optimization. All of our models use a ViT-B/16 encoder trained for 300 epochs on ImageNet.\nWe use the AdamW optimizer Loshchilov and Hutter (2019) with 1 \u00d7 10\u22123 as our learning. We further use\n\u03b21 = 0.9 and \u03b22 = 0.999. The learning rate follows a linear warmup for 40 epochs and then a cosine annealing.\nWe use an iteration per epoch scale of 1.25 for the scheduler, which stretches the scheduler and makes the\ntraining end before the end of the schedule. Not having a 0 learning rate near the end of training was found\nbeneficial in our experiments. We use a cosine weight decay schedule which goes from 0.04 to 0.4.\nSource and target. In practice we build the source and target separately by first applying a random crop of\nscale between 0.3 and 1. We then apply a horizontal flip with probability 0.5. We will call the resulting image\nI\u2032.\nTarget transformations. Starting from I\u2032 we then apply a color jitter with probability 0.8, brightness maximum\nstrength 0.4, contrast maximum strength 0.4, hue maximum strength 0.1, and saturation maximum strength\n0.2.\nSource transformations. Starting from I\u2032 we apply a color jitter with probability 0.8, brightness maximum\nstrength 0.4, contrast maximum strength 0.4, hue maximum strength 0.1, and saturation maximum strength\n0.2. A gaussian blur of radius between 0.1 and 2 is applied with probability 0.2, solarization with probability\n0.2 and grayscale with probability 0.2. These augmentations correspond to the ones used in BYOL (Grill\net al., 2020). We then generate a mask Mx as the union of 4 masks of area between 0.15 and 0.2 of the image,\nwith aspect ratios between 0.75 and 1.5. All of the patches in Mx are then dropped from the source x.\nPredictor conditioning. We rely on the feature mixing strategy. Consider a mask token m \u2208 Rd and a \u2208 Rk a\nvector of k scalars corresponding to augmentation parameters. We first add position embeddings to m to\nindicate which patch of the target it needs to predict. We then concatenate m and a and feed them through a\nthree layer fully-connected network with ReLU activation and dimensions d, d, d. This gives us a mask token\nthat contains information about all of the transformation. Both the geometric aspect of where to predict and\ndetails on the photometric augmentations.\nA.2\nEvaluation\nFor all evaluations on image classification, the augmentations applied to compute the validation accuracy\nare a resize to 256 followed by a 224 by 224 center crop.All hyperparameters reported are the optimal ones,\nchosen after careful tuning for every method.\nLinear.\nWe take inspiration from the protocol of Chen et al. (2021). We train for 90 epochs on ImageNet.\nWe sample random crops of images with scale between 0.08 and 1, then apply a horizontal flip with probability\n13\n0.5.\nThe features are average pooled along the sequence axis to obtain a global representation which is then fed to\na linear layer. We use a batch size of 16,384, with the LARS (You et al., 2017) optimizer and a learning rate\nof 6.4 with a warmup of 10 epochs. The learning rate then follows a cosine annealing schedule. Weight decay\nis set to 0 and momentum to 0.9.\nAttentive.\nThe attentive head is taken from Chen et al. (2023). It consists of a cross attention block where\nthe attention is computed between an additional token the unpooled representations. This allows an adaptive\npooling strategy. We train for 90 epochs on ImageNet. We sample random crops of images with scale between\n0.3 and 1, then apply a horizontal flip with probability 0.5. We also apply the same augmentations as used\nfor the source transformations besides masking. We use a batch size of 1024 and AdamW optimizer with a\nlearning rate of 1 \u00d7 10\u22124,\u03b21 = 0.9, and \u03b22 = 0.999. It follows a cosine annealing schedule. We use a weight\ndecay of 0.01 kept constant during training.\nEncoder finetuning.\nWe append a linear layer to the end of the encoder as for the linear evaluation and train\nfor 100 epochs on ImageNet. We use the same RandAugment (Cubuk et al., 2020) strategy as MAE (He et al.,\n2021) as well as CutMix (Yun et al., 2019) and MixUp (Zhang et al., 2018). For RandAugment we use the\nstring \u2019rand-m9-mstd0.5-inc1\u2019. We use random erasing with probability 0.25 in pixel mode. We use a mixup\n\u03b1 of 0.8, cutmix \u03b1 of 1 and label smoothing of 0.1.\nFor the optimization we use AdamW with a learning rate of 2 \u00d7 10\u22123 with 5 epochs of warmup followed by a\ncosine annealing schedule, weight decay of 0.005 and a batch size of 1024. We also use a drop path rate of 0.2\nthrough the encoder and a layer wise learning rate decay of 0.65.\nPredictor finetuning.\nWhen finetuning the predictor we use an attentive head on top of the predictor output.\nWe plug the predictor on top of the teacher network and it is tasked with predicting the whole target image,\nwith null transformation parameters. We use the same augmentation protocol as for encoder finetuning. We\ntrain for 100 epochs on ImageNet with a batch size of 1024. We use AdamW for the optimizer, a learning rate\nof 1 \u00d7 10\u22123 with a 5 epoch warmup then cosine annealing schedule. We use a weight decay of 0.1, no layer\nwise lr decay and a drop path rate of 0.2 through the predictor. Importantly, if the predictor is pretrained we\ndivide it\u2019s learning rate by 10, and keep it identical to the attentive if head if random.\n+\n\u2026\nSplit batch per \ntask\n\u2026\nEven batch per task\nFigure S2 Multitask tuning of the predictor. We sample a batch uniformly across task which is then fed through the\npredictor with an additional task token, indicating which task is being solved. The predictions are then fed through a\ntask specific head and losses are summed.\nMultitask predictor finetuning.\nTo give a clearer presentation of the protocol, we provide a graphical version\nof multitask predictor finetuning in figure S2. For the training in itself, we follow the same protocol as for\npredictor finetuning but train for the equivalent of 50 ImageNet epochs. The batch size used is 512 for each\n14\ntask, where the batch is independently split between tasks. When training on a single task, we simply use 512\nas the batch size and also train for 50 ImageNet epochs.\nEnd to end finetuning.\nWe follow the protocol of predictor finetuning but tweak certain parameters. First,\nthe encoder also gets his learning rate divided by 10 like the predictor. The factors are treated separately\nand ablated for all methods. We use a 0.9 layer decay across the combination of predictor and encoder. The\nlearning rate used is 2 \u00d7 10\u22123 and all other parameters are identical to predictor finetuning.\nSegmentation.\nWe give here details about our protocol for semantic segmentation evaluations. We use the\nMMSegmentation library Contributors (2020). We fine-tune our pretrained models (either encoder only,\npredictor only, or end-to-end) with an UperNet head Xiao et al. (2018) on the ADE20k semantic segmentation\ndataset Zhou et al. (2019) for 160k iterations and report the validation mIoU. We concatenate the last 4 layers\nof the predictor, or encoder for encoder only finetuning, and feed the result to the segmentation head. At\ntraining time we resize the images at the pretraining resolution. At testing time we do not resize the images\nand interpolate the positional embeddings to the original resolution. For all setups and methods we pick the\nbest run among several learning rate values: 1e \u2212 5, 2e \u2212 5 and 3e \u2212 5. We use a weight decay of 0.01 and a\nlinear learning rate decay schedule.\nB\nComplete finetuning results\nTable S1 Complete results of tables 4 and 5.\nMethod\nEpochs\nNo predictor\nFrozen encoder, tuned predictor\nEnd to end\nEncoder\nRandom Init.\nPretrained\nDINO\n1600\n82.8\n82.0\nN/A\n82.1\nMOCOv3\n300\n83.2\n56.4\nN/A\n79.4\niBOT\n1600\n84.0\n83.0\nN/A\n82.8\nMAE\n300\n82.7\n82.4\n82.7 (+0.3)\n82.3\n600\n83.2\n82.8\n83.0 (+0.2)\n83.1\n1600\n83.6\n83.0\n83.1 (+0.1)\n83.3\nI-JEPA\n300\n83.0\n79.1\n80.0 (+0.9)\n82.0\nIWMInv\n12,384\n300\n83.3\n80.5\n81.3 (+0.8)\n82.7\nIWMEqui\n12,384\n300\n82.7\n81.3\n82.7 (+1.4)\n83.3\nIWMEqui\n18,384\n300\n82.9\n81.5\n83.3 (+1.8)\n84.4\nWe provide complete results for table 4 and table 5 in table S1. Some interesting behaviors are IWMEqui\n12,384\nand MoCov3 in predictor finetuning. For IWMEqui\n12,384, we see the same behavior as IWMEqui\n18,384 but with slightly\nlower performance. This is consistent across all evaluations. Yet, even when accounting for scale of the\npredictor to compare with I-JEPA and IWMInv\n12,384, all of our previous conclusions still hold. For MoCov3,\nit was the only method which did not perform well when attaching a random predictor to it. While we do\nnot have conclusive evidence, we hypothesize that it is related to the low norm of its output. Adding a\nnormalization between the encoder and predictor did not help.\n15\nC\nImpact of data augmentation\nTable S2 Impact of data augmentation strategy on IWM\u2019s performance. In all settings, destructive augmentations are\nnever applied to the target.\nPredictor\nStrengths\nProbabilities\nPerformance\nBright.\nContrast\nSat.\nHue\nJitter\nBlur\nGray.\nSolarize\nMRR\nLinear\nAttentive\nPred. ft.\nIWM12,384\n0.4\n0.4\n0.2\n0.1\n0.8\n0.2\n0.2\n0.1\n0.09\n74.5\n77.0\n81.3\n0.4\n0.4\n0.2\n0.1\n0.8\n0.11\n72.1\n76.5\n80.5\n0.4\n0.4\n0.2\n0.1\n0.8\n0.4\n0.4\n0.2\n0.22\n71.0\n74.9\n80.9\n0.5\n0.5\n0.4\n0.2\n0.8\n0.2\n0.2\n0.1\n0.81\n69.3\n75.5\n82.7\n0.5\n0.5\n0.4\n0.2\n0.8\n0.07\n73.3\n76.3\n80.1\n0.2\n0.2\n0.1\n0.02\n72.9\n76.3\n80.7\nIWM18,384\n0.4\n0.4\n0.2\n0.1\n0.8\n0.2\n0.2\n0.1\n0.79\n67.5\n75.1\n83.3\n0.4\n0.4\n0.2\n0.1\n0.8\n0.25\n70.1\n74.8\n81.4\n0.4\n0.4\n0.2\n0.1\n0.8\n0.4\n0.4\n0.2\n0.85\n56.1\n74.5\n83.1\n0.5\n0.5\n0.4\n0.2\n0.8\n0.2\n0.2\n0.1\n0.85\n34.3\n71.0\n81.7\n0.5\n0.5\n0.4\n0.2\n0.8\n0.83\n69.2\n75.8\n83.3\n0.2\n0.2\n0.1\n0.02\n70.9\n74.8\n81.5\nWe study in table S2 the impact of augmentations used during pretraining, along with the depth of the\npredictor. We notice that depth is a deciding factor in the quality of the learned world model, where 4 out 5\nscenarios with color are able to achieve color equivariance for the 18 layer predictor, compared to only 1 for\nthe 12 layer predictor. The strength of the augmentations also plays a role and too weak augmentations do\nnot lead to an equivariant model.\nOn the asymmetry of augmentations.\nThe asymmetry of augmentations is both a conceptual choice, to make\nthe augmentations used more similar to contrastive approaches, but also a practical one. When learning\nan invariant world model with symmetric augmentations we noticed a drop in performance of 2 points on\nImageNet in attentive probing and 1.5 points on linear probing. While this drop is not catastrophic, it is\nsufficient to recommend using asymmetric augmentations. As the depth of the predictor decreases, we expect\nthis gap to widen.\nOn the other hand, when looking at an equivariant predictor, we did not notice any notable change in\nperformance. This suggests that learning world models can also help improve stability over the choice of\naugmentations. The predictor does not have to be designed by keeping in mind which information may get\nremoved but only by whether or not it can apply the transformation.\nD\nImpact of the prediction task on predictor finetuning performance\nIn order to use predictor finetuning to solve downstream tasks, we need to apply a prediction task. We aim at\ngiving a more combinatorial view of table 3 in this appendix.\nTable S3 Predictor finetuning performance which different prediction tasks.\nMethod\nNull latents\nOn teacher\nPred only one token\nAccuracy\nIWM12,384\n\u2713\n\u2713\n83.3\n\u2713\n\u2713\n\u2713\n82.8\n\u2713\n83.1\n\u2713\n\u2713\n82.6\n\u2713\n83.2\n\u2713\n\u2713\n82.8\n82.9\n\u2713\n82.9\n16\nWe can see in table S3 that the conclusions drawn from table 3 still hold over a larger setting. Notably, using\nnull latents is more flexible while not changing performance, using the teacher always gives a small boost in\nperformance, and predicting only one token lowers performane by roughly half a point.\nE\nScaling to larger models\nIn order to scale to larger models, such as a ViT-L/16 encoder, multiple challenges need to be overcome. Notably,\nboth the depth and the width of the predictor must be scaled in order to increase the number of parameters\nof the predictor to a suitable number. Scaling the width can lead to instabilities and hyperparameters such as\nthe EMA schedule become more important. We noticed that a ratio of predictor weights/encoder weights of\naround 0.3 is suitable to learn a good world model.\nTable S4 With a ViT-L/16 encoder, we observe a similar trend as with the base model. Significant gains are observed\nwith a good world model, allowing it to surpass encoder finetuning.\nMethod\nEpochs\nEncoder\nPredictor\nEnd to end\nI-JEPA\n300\n84.1\n79.9\nIWMInv\n18,384\n300\n84.3\n81.5\nIWMEqui\n36,512\n300\n83.7\n85.0\n85.4\nWe study in table S4 the performance when scaling to a larger ViT-L/16. We see that the observation we made\nwith the smaller ViT-B/16 still hold. The invariant model is the best on encoder finetuning, and predictor\nfinetuning improves the performance significantly. Here again, end-to-end finetuning leads to performance\ngains.\nF\nEvaluation on downstream datasets beyond ImageNet\nWe evaluated I-JEPA and IWM on iNaturalist18 (Horn et al., 2018), SUN397 (Xiao et al., 2010) and\nPlaces205 (Zhou et al., 2014) using attentive probing. We train our models for 50 epochs on iNaturalist18, 12\nfor Places205 and 28 for SUN397.\nTable S5 When evaluating with attentive probing on downstream task, being equivariant improves performance across\nthe board. All methods use ViT-B/16 encoders and were pretrained for 300 epochs on ImageNet\nMethod\nImageNet\niNat18\nSUN397\nPlaces205\nMAE\n73.5\n50.1\n70.2\n60.3\nI-JEPA\n75.0\n50.4\n69.2\n58.3\nIWMInv\n12,384\n77.0\n51.6\n71.0\n59.4\nIWMEqui\n18,384\n75.1\n54.2\n71.7\n60.5\nAs we can see in table S5, IWM consistently improves over I-JEPA and MAE when pretraining all methods\nfor 300 epochs. We notice that while IWMEqui\n18,384 is not the top performing model on ImageNet, it significantly\noutperforms it\u2019s invariant counterpart, with gains of 2.6 points on iNaturalist, 0.7 points on SUN397 and 1.1\npoint on Places205. This suggests that while the richness of the representation of an equivariant model is not\noptimal for in domain performance, it helps improve generalisation to downstream tasks.\n17\nG\nVisualizing representation differences between invariant and equivariant\nbehavior\n(a) IWMInv\n12,384\n(b) IWMEqui\n18,384\n(c) IJEPA\nFigure S3 Difference in embedding space between invariant and equivariant behaviours. Each image is augmented 16\ntimes and we compute the similarity matrix between all images. The yellow regions indicate high similarities between\nsamples originating from the same image. We can see more variations in the equivariant model, or in I-JEPA where\ninvariance is not enforced. This suggests that augmentations influence the representation more in these models.\nAs we see in figure S3, the invariant model collapses augmented views to very similar embeddings, as shown\nby the high similarity in the diagonal blocks. On the other hand the equivariant model shows more variation,\nwhich shows that augmentation information is more present in the representation. Interestingly, I-JEPA has a\nbehaviour in between because it was not trained to be either invariant or equivariant. I-JEPA has no force\ncontrolling how information is kept or removed from the representation.\nH\nOn the meaning and role of invariance in Self-Supervised learning\nOne of the key component of the success of self-supervised learning is augmentation invariance Chen et al.\n(2020a). We can say that we have learned invariant representations if \u2200a, f\u03b8(x) = f\u03b8(T (a, x)). However there\nare many scenarios that satisfy this property. The two main ones that we are interested in are:\n\u2022 Any augmented view leads to the same information as the clean image\n\u2022 The encoder removes the information related to the transformation\nIn the first case, the representations still contain all of the information about the input, whereas in the second\nwe are removing information that can be deemed superfluous. In the case of contrastive methods, the focus is\nusually on removing information. Indeed if an image and its grayscale version are made to have the same\nrepresentation, the encoder must remove color information. This is one of the key drivers of performance of\nsuch methods. By removing information until only the semantics of the image remains, the representations\nwill be easy to leverage for a task such as classification.\nWe can thus wonder if the first invariance scenario also leads to improved performance, and if we can even\nleverage it. As we have demonstrated how IWM is able to preserve information, and we have a predictor that\ncan apply transformations, we can marginalize over augmentations to create invariant representations in an\nefficient way. Here, we do not need to apply the encoder on all augmented views, but can directly use the\npredictor which is more compute efficient. If we consider a set of randomly sampled augmentations A such\nthat card(A) = N we can compute an invariant representation as\nzInv\nx\n= 1\nN\nN\nX\ni=1\np\u03d5 (f\u03b8(x), Ai, mAi)\n18\nWe can then visualize which image has representation most similar to zInv\nx\nand see if using zInv\nx\nimproves\nperformance on classification task.\nExample\n augmented views\nNN of average\n representation\n1-NN\n2-NN\n3-NN\n4-NN\n5-NN\nClean image\nFigure S4 Retrieval of invariant representations computed using 256 augmentations in latent space. In the top row we\nvisualize some of the corresponding image and on the bottom the nearest neighbours of the invariant representation.\nWe can notice that the nearest neighbour is the original non-augmented image, followed by images with small\ntransformations.\nAs we can see in figure S4, the images that have representations which are most similar with zInv\nx\nare the\nclean image and images with small transformations. We also know that our encoder preserves augmentation\nrelated information and is thus not invaraint to transformations. Combining these two facts tells us that the\nmarginalizaiton process creates a clean representations, akin to the first kind of invariance.\nTable S6 Linear evaluation on marginalized representations. Using more augmented prediction to create an invariant\nrepresentation does not improve performance.\nNumber of predictions (N)\n1 (default)\n8\n16\n32\n128\nImageNet Top-1 accuracy (%)\n64.5\n64.3\n64.6\n64.6\n64.4\nHowever, when looking at table S6 we can see that no performance gain is present when using invariant\nrepresentations obtained by marginalizing over predictions. This is true even with 128 augmented views,\nwhich already increases the compute budget by a factor of around 64. As such, using invariant representations\nthat preserve the content of the image is not necessarily beneficial for downstream evaluation.\nOverall, the key to the success of augmentation invariance in contrastive learning is not just in building\ninvariant representations, but in the way that the representations are invariant. Building invaraince by\nremoval of information has been shown to be very effective (Chen et al., 2020a), whereas we see here that\ninvariance by always predicting the representation of the clean image is not necessarily helpful. This does not\nmean that equivariant representations cannot build invariances that are useful from downstream tasks, as the\ncontrary was shown in Chavhan et al. (2023b), but that we have to be careful in how we create invariant\nrepresentations.\n19\nI\nAdditional qualitative evaluations of the world model\nSource\nGroundtruth\nNN of \nPrediction\nFigure S5 Randomly selected retrieval samples of our world model. For each image, we generate 256 augmented views\nand apply transformations in latent space. We then retrieve the nearest neighbor of the prediction and visualize\nwhether it is close to the groundtruth or not. The learned world model performs well in most settings but has some\ninaccuracies with inverting grayscale.\nSource\nGroundtruth\nNN of \nPrediction\nFigure S6 Randomly selected retrieval samples of our world model. For each image, we generate 256 augmented views\nand apply transformations in latent space. We then retrieve the nearest neighbor of the prediction and visualize\nwhether it is close to the groundtruth or not. The learned world model performs well in most settings but has some\ninaccuracies with inverting grayscale.\n20\nSource\nGroundtruth\nNN of \nPrediction\nFigure S7 Randomly selected retrieval samples of our world model. For each image, we generate 256 augmented views\nand apply transformations in latent space. We then retrieve the nearest neighbor of the prediction and visualize\nwhether it is close to the groundtruth or not. The learned world model performs well in most settings but has some\ninaccuracies with inverting grayscale.\nSource\nGroundtruth\nNN of \nPrediction\nFigure S8 Randomly selected retrieval samples of our world model. For each image, we generate 256 augmented views\nand apply transformations in latent space. We then retrieve the nearest neighbor of the prediction and visualize\nwhether it is close to the groundtruth or not. The learned world model performs well in most settings but has some\ninaccuracies with inverting grayscale.\n21\nBrightness\nMin\nMax\nContrast\nSaturation\nHue\nFigure S9 Application of the world model on precise transformations. For each parameter, we vary its value on a grid\nto see whether the model is able to predict small changes. The model is able to show the gradient of transformations,\nhighlighting again the capabilities of the world model. We can still notice some imperfections however, as the model\nwas only trained on combinations of augmentations. To make changes more visible, we used a model trained with a\nstrong color jitter for this figure.\nBrightness\nMin\nMax\nContrast\nSaturation\nHue\nFigure S10 Application of the world model on precise transformations. For each parameter, we vary its value on a grid\nto see whether the model is able to predict small changes. The model is able to show the gradient of transformations,\nhighlighting again the capabilities of the world model. We can still notice some imperfections however, as the model\nwas only trained on combinations of augmentations. To make changes more visible, we used a model trained with a\nstrong color jitter for this figure.\n22\nBrightness\nMin\nMax\nContrast\nSaturation\nHue\nFigure S11 Application of the world model on precise transformations. For each parameter, we vary its value on a grid\nto see whether the model is able to predict small changes. The model is able to show the gradient of transformations,\nhighlighting again the capabilities of the world model. We can still notice some imperfections however, as the model\nwas only trained on combinations of augmentations. To make changes more visible, we used a model trained with a\nstrong color jitter for this figure.\n23\n"
  },
  {
    "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
    "link": "https://arxiv.org/pdf/2403.00071.pdf",
    "upvote": "17",
    "text": "Resonance RoPE: Improving Context Length Generalization of\nLarge Language Models\nSuyuchen Wang1,2, Ivan Kobyzev3, Peng Lu1, Mehdi Rezagholizadeh3 and Bang Liu1,2\u2020\n1DIRO, Universit\u00e9 de Montr\u00e9al\n2Mila - Quebec AI Institute\n3Huawei Noah\u2019s Ark Lab\n{suyuchen.wang, peng.lu, bang.liu}@umontreal.ca\n{ivan.kobyzev, mehdi.rezagholizadeh}@huawei.com\nAbstract\nThis paper addresses the challenge of train-\nshort-test-long (TSTL) scenarios in Large Lan-\nguage Models (LLMs) equipped with Rotary\nPosition Embedding (RoPE), where models\npre-trained on shorter sequences face difficulty\nwith out-of-distribution (OOD) token positions\nin longer sequences.\nWe introduce RESO-\nNANCE ROPE, a novel approach designed to\nnarrow the generalization gap in TSTL scenar-\nios by refining the interpolation of RoPE fea-\ntures for OOD positions, significantly improv-\ning the model performance without additional\nonline computational costs. Furthermore, we\npresent POSGEN, a new synthetic benchmark\nspecifically designed for fine-grained behav-\nior analysis in TSTL scenarios, aiming to iso-\nlate the constantly increasing difficulty of to-\nken generation on long contexts from the chal-\nlenges of recognizing new token positions. Our\nexperiments on synthetic tasks show that after\napplying RESONANCE ROPE, Transformers\nrecognize OOD position better and more ro-\nbustly. Our extensive LLM experiments also\nshow superior performance after applying RES-\nONANCE ROPE to the current state-of-the-art\nRoPE scaling method, YaRN, on both upstream\nlanguage modeling tasks and a variety of down-\nstream long-text applications.1\n1\nIntroduction\nRecent advancements in Large Language Models\n(LLMs) have demonstrated their potential across a\nwide spectrum of natural language processing tasks,\nshowcasing their ability to handle complex interac-\ntions, document analyses, professional writing, and\nadvanced reasoning with a unified approach (Ope-\nnAI, 2023; Touvron et al., 2023a,b; Jiang et al.,\n2024). As these models are increasingly adapted\nfor complex applications, challenges arise in sce-\nnarios requiring the comprehension or generation\n1https://github.com/sheryc/resonance_rope.\n\u2020Canada CIFAR AI Chair. Corresponding author.\nof long texts. Specifically, the train-short-test-long\n(TSTL) scenario (Press et al., 2022) highlights a\nlimitation where LLMs, pre-trained on shorter se-\nquences, struggle with out-of-distribution (OOD)\ntoken positions in longer sequences, impacting\ntheir performance in real-world applications (Zhao\net al., 2023).\nRecent efforts to enhance TSTL performance\nhave focused on LLMs equipped with Rotary Posi-\ntion Embedding (RoPE) (Su et al., 2024), such\nas LLaMA (Touvron et al., 2023a,b) and Mis-\ntral (Jiang et al., 2023), owing to their excep-\ntional capabilities and widespread adoption. These\ninitiatives aim to refine the test-time computa-\ntion of RoPE position embedding by introducing\na scaling factor to either the position index of\neach token (Chen et al., 2023) or RoPE\u2019s base\nvalue (Xiong et al., 2023; Liu et al., 2023b; Peng\net al., 2023). These methods ensure that the po-\nsition embeddings for out-of-distribution (OOD)\npositions remain within the range experienced dur-\ning pre-training. This minimizes the need for the\nmodel to adapt to new position embedding value\nranges, a task that is inherently difficult.\nIn this paper, we introduce RESONANCE ROPE,\na novel technique designed to further narrow the\ngeneralization gap on position embeddings in\nTSTL scenarios. Recognizing that RoPE\u2019s position\nembedding is governed by a complex, non-linear\nfunction, we posit that minimizing extrapolation on\nOOD positions, while crucial, is insufficient. We\nargue that it is equally vital to address the inter-\npolation of RoPE features at the OOD positions.\nBy implementing RESONANCE ROPE, we effec-\ntively eliminate the generalization gap for more\nthan half of the position embedding features in\nLLaMA and LLaMA2 in TSTL scenarios. Fur-\nthermore, our approach is compatible with RoPE\nand any RoPE-based scaling techniques, enhancing\ntheir performance in TSTL situations without the\nneed for additional computational resources during\n1\narXiv:2403.00071v1  [cs.CL]  29 Feb 2024\ntraining or inference.\nAdditionally, to facilitate further research on\nposition embeddings, we present a new syn-\nthetic benchmark tailored for TSTL scenarios,\nnamed POSGEN. Improving position embeddings\nfor TSTL requires a detailed analysis of the cause\nof failures in handling longer contexts. However,\ncurrent benchmarks, such as those measuring per-\nplexity in long context (Rae et al., 2020; Huang\net al., 2021; Wu et al., 2022) and most synthetic\nTSTL tasks (Liu et al., 2023a; Kazemnejad et al.,\n2023) face a common issue: the difficulty of gener-\nating the next token increases with context length.\nThis makes it difficult to determine whether a\nmodel\u2019s failure is due to its inability to generate\nmore complex tokens or its failure to recognize\nout-of-distribution (OOD) positions. POSGEN ad-\ndresses this limitation by standardizing the diffi-\nculty level of token generation across all positions.\nThis ensures that any observed shortcomings are\ndirectly related to the model\u2019s inability to identify\nand handle new token positions effectively.\nOur contributions in this study are threefold:\n1. We propose RESONANCE ROPE, an innova-\ntive modification to RoPE based on an in-\ndepth analysis of the wavelengths of RoPE\nfeatures, aiming to narrow the generalization\ngap in TSTL scenarios across RoPE and sim-\nilar RoPE-based scaling techniques, without\nnecessitating extra computational resources\nduring runtime.\n2. We present POSGEN, a newly developed syn-\nthetic benchmark tailored for TSTL scenarios.\nThis benchmark is specifically designed to\ndisentangle the complexities associated with\ngenerating tokens in longer contexts from the\nchallenges posed by recognizing new posi-\ntions or position embedding values.\n3. Through rigorous testing of RESONANCE\nROPE on both RoPE and YaRN within\nthe POSGEN benchmark, we demonstrate\nits ability to enhance performance on out-\nof-distribution (OOD) positions, surpassing\nexisting methods that do not include RESO-\nNANCE ROPE. Moreover, when applied to\nYaRN, RESONANCE ROPE further improves\nLLM\u2019s length extrapolation ability, as evi-\ndenced by lower perplexity in upstream TSTL\nlanguage modeling and enhanced outcomes in\ndownstream tasks involving lengthy contexts.\n2\nRelated Work\n2.1\nScaling of RoPE Position Encoding\nRecent efforts in extending LLMs\u2019 context window\nfocus on manipulating position embedding (PE),\nparticularly RoPE (Su et al., 2024), which is used\nin LLMs like LLaMA (Touvron et al., 2023a,b) and\nMistral (Jiang et al., 2023). Main strategies include\nembedding scaling (Chen et al., 2023; Liu et al.,\n2023b; Peng et al., 2023) and randomizing token\npositions (Ruoss et al., 2023; Zhu et al., 2023). Our\nemphasis is on the embedding scaling strategies.\nExisting embedding scaling strategies adjust po-\nsition embedding for longer sequences to match\nthe pre-training range, avoiding feature extrapola-\ntion. For instance, Chen et al. (2023) compresses\nposition indices to fit the pre-training range, ex-\ntending LLaMA\u2019s (Touvron et al., 2023a) context\nto 16K with 1,000 steps of fine-tuning. Alterna-\ntively, Liu et al. (2023b); Rozi\u00e8re et al. (2023);\nXiong et al. (2023) modify RoPE\u2019s rotary base\nand employ fine-tuning on extended sequences,\ntermed Adjusted Base Frequency (ABF) or \"NTK-\naware\" scaling. Code LLaMA (Rozi\u00e8re et al., 2023)\nachieved 16K context length with this method af-\nter 10,000 fine-tuning steps. YaRN (Peng et al.,\n2023) improved NTK-aware scaling by segment-\ning RoPE features and applying tailored extrapo-\nlation strategies, achieving 64K context length for\nLLaMA2 (Touvron et al., 2023b) with 400 fine-\ntuning steps. Distinguishingly, our RESONANCE\nROPE focus on reducing feature interpolation on\nOOD positions, which we argue is another impor-\ntant factor in improving the length extrapolation\ncapability of Transformer.\n2.2\nLong Context Evaluations\nEvaluations of Transformer-based LLMs\u2019 long-\ncontext capabilities are twofold: synthetic task as-\nsessments for length extrapolation strategies and\nreal-world task evaluations at the LLM scale. Syn-\nthetic evaluations target simple tasks such as long\nsequence classification (Tay et al., 2021) and arith-\nmetic language modeling (Liu et al., 2023a; Kazem-\nnejad et al., 2023). LLM scale evaluations mea-\nsure metrics such as perplexity (PPL) in extensive\ntext corpora (e.g., PG19 (Rae et al., 2020), GovRe-\nport (Huang et al., 2021), GitHub (Wu et al., 2022))\nand complex tasks including summarization, ques-\ntion answering, and mathematical reasoning (An\net al., 2023; Bai et al., 2023; Shaham et al., 2023).\n2\n3\nBackground\n3.1\nRotary Position Embedding (RoPE)\nIn Transformers (Vaswani et al., 2017), the self-\nattention scores are softmax-normalized scaled at-\ntention logits q\u22a4k:\nam,n = Softmax\n\u0012qm\u22a4kn\n\u221a\nd\n\u0013\nSuppose the input to a single attention head\nis x1, x2, . . . , xl \u2208 Rd, where l is the sequence\nlength and d is the dimension of an attention head.\nRoPE injects the position information of each token\ninto the q and k vectors by the following equations\nin the complex space:\nqm,[2j:2j+1] = Wqxmeim\u03b8j\nkm,[2j:2j+1] = Wkxmeim\u03b8j\n\u03b8j = b\n\u22122j\nd ,\n(1)\nwhere Wq, Wk are trainable parameters, and b\nis a constant called the rotary base, which is set\nto 10, 000 (Su et al., 2024) or other integers or\nfractions (Xiong et al., 2023; Peng et al., 2023).\nThis form makes the dot product between the m-th\nquery qm and n-th key kn only depend on the input\nxm, xn and their relative distance (m \u2212 n):\n\u27e8qm,[2j:2j+1], kn,[2j:2j+1]\u27e9\n=\u211c\nh\nq\u2217\nm,[2j:2j+1]kn,[2j:2j+1]\ni\n=\u211c\nh\n(Wqxm)\u2217 (Wkxn) ei(m\u2212n)\u03b8j\ni\n=g(xm, xn, m \u2212 n).\nRoPE\u2019s real-number implementation divides the d-\ndimension space into multiple 2-dimensional sub-\nspaces and applies real rotation matrix to each of\nthem. Formally, define a d \u00d7 d block-diagonal\nmatrix:\nRd\n\u0398,m =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nR\u03b80,m\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n0\n0\nR\u03b81,m\n\u00b7 \u00b7 \u00b7\n0\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\nR\u03b8 d\n2 \u22121,m\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\n(2)\nwhere \u0398 = {\u03b80, \u03b81, \u00b7 \u00b7 \u00b7 , \u03b8 d\n2 \u22121}, and each R\u03b8j,m is\na 2 \u00d7 2 rotation matrix:\nR\u03b8j,m =\n\u0012cos m\u03b8j\n\u2212 sin m\u03b8j\nsin m\u03b8j\ncos m\u03b8j\n\u0013\n.\n(3)\nRoPE computes the attention logit q\u22a4k as follows:\nqm = Rd\n\u0398,mWqxm\n(4)\nkn = Rd\n\u0398,nWkxn\n(5)\nq\u22a4\nmkn = x\u22a4\nmWqRd\n\u0398,n\u2212mWkxn\n(6)\nFor each two dimensions [2j : 2j + 1] of q and\nk, its corresponding \u03b8j reflects a temporal wave-\nlength \u03bbj. This wavelength describes the token\nlength for the corresponding RoPE features to en-\ncounter approximately the same rotary angle m\u03b8j\nin Equation 3:\n\u03bbj = 2\u03c0\n\u03b8j\n= 2\u03c0b\n2j\nd\n(7)\nAs an example, the wavelengths of LLaMA /\nLLaMA2\u2019s RoPE features range from 2\u03c0 \u2248 6.28\nfor \u03b80 to 2 \u2217 10000126/128\u03c0 \u2248 54410.14 for \u03b8 d\n2 \u22121.\n3.2\nCritical Dimensions of RoPE\nIn a TSTL scenario (Press et al., 2022), one takes\na model trained on texts with lengths up to L, and\ntests it on a task with input lengths up to L\u2032 = sL,\nwith the scaling factor s > 1. Recently, Liu et al.\n(2023b) discovered that there may exist two \u201ccrit-\nical dimensions\u201d in RoPE features, which corre-\nspond to the dimensions [2c : 2c + 1] that satis-\nfies \u03bbc \u2265 L and \u03bbc\u22121 < L. The dimensions of\nRoPE features above and below the critical dimen-\nsion (which we denote as \u201cpost-critical dimensions\u201d\nand \u201cpre-critical dimensions\u201d, respectively) have\ndifferent behaviors in TSTL: for post-critical di-\nmensions (i.e., j > c), since their wavelengths sat-\nisfy \u03bbj > L, the training corpus does not cover all\npossible rotary angles m\u03b8j on a unit circle. Thus,\nthese dimensions will encounter OOD value range\non longer sequences. This is not an issue for pre-\ncritical dimensions due to their shorter temporal\nwavelengths.\nThe concept of RoPE\u2019s critical dimensions im-\nplicitly guides the development of RoPE scaling\nmethods. For example, previous RoPE scaling\nmethods (Chen et al., 2023; Xiong et al., 2023;\nPeng et al., 2023) mainly focus on reducing or\navoiding value extrapolation on post-critical dimen-\nsions, and minimize post-training modifications to\nthe pre-critical dimensions.\n3.3\nYet another RoPE extensioN (YaRN)\nYaRN (Peng et al., 2023) is the current state-of-the-\nart RoPE scaling method for TSTL. It introduces\n3\nthe \u201cNTK-by-parts\u201d scaling for RoPE, which ap-\nplies different scaling strategies to each RoPE fea-\nture according to its temporal wavelength.\nIn a TSTL scenario with scaling factor s, YaRN\nscales the wavelength of the j-th RoPE feature \u03bbj\nto \u02c6\u03bbj and further fine-tune the model:\n\u02c6\u03bbj = (1 \u2212 \u03b3j)s\u03bbj + \u03b3j\u03bbj,\nwhere \u03b3j is a piece-wise function depending on\nits corresponding wavelength \u03bbj, and two hyperpa-\nrameters \u03b1 and \u03b2:\n\u03b3j =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1,\nif \u03bbj < L/\u03b2\n0,\nif \u03bbj > L/\u03b1\nL/\u03bbj \u2212 \u03b1\n\u03b2 \u2212 \u03b1\n,\notherwise\nEmpirically, for the LLaMA family, Peng et al.\n(2023) suggests using \u03b1 = 1 and \u03b2 = 32. This\nsetting avoids value range extrapolation on post-\ncritical dimensions, while reducing modifications\nto the original pre-critical dimensions.\nIn addition to the \u201cNTK-by-parts\u201d RoPE scaling\nstrategy mentioned above, YaRN also comprises\na scaling strategy on the attention scores, which\nreduces the change in the entropy of the attention\nscore on longer sequences. We maintain the com-\nplete design of YaRN in our experiments, but our\nanalysis will focus on its RoPE scaling strategy.\n4\nProposed Method: RESONANCE ROPE\nIn this section, we introduce RESONANCE ROPE, a\nuniversal improvement for RoPE and RoPE-based\nscaling methods to (further) improve their length\nextrapolation performance.\nSuppose we abstract RoPE\u2019s Equation 4, 5: for\nany x \u2208 Rd, we define f(x, m) = Rd\n\u0398,mW x. In a\nTSTL scenario where we generalize an LLM from\nlength L to length L\u2032, let us denote a scaled RoPE\nfunction by \u02dcf. To perform well on OOD positions it\nshould reduce the feature gap h( \u02dcf) between token\nfeatures seen during training and token features\nafter scaling that we can define for each i-th feature\nas:\nhi( \u02dcf) = max\nx\u2208X\nmin\nm\u2208{0,\u00b7\u00b7\u00b7 ,L\u22121}\nn\u2208{L,\u00b7\u00b7\u00b7 ,L\u2032\u22121}\n| \u02dcf(x, m)i \u2212 \u02dcf(x, n)i|,\n(8)\nwhere i = 0, . . . , d \u2212 1 and X \u2282 Rd is the set\nof feature vectors to which we apply a position\nembedding.\nRoPE Features'\nRotation Angles\nResonance RoPE Features'\nRotation Angles\n0\n32\n64\n96\n128\nToken Position\nTraining Position Range\nOOD Position Range\nFigure 1: An illustration of RoPE\u2019s rotation angles m\u03b86\nand RESONANCE ROPE\u2019s rotation angles m\u02dc\u03b86 in Eqn. 3\nin a TSTL scenario with training max length 64 and test-\ning max length 128. RoPE\u2019s non-integer feature wave-\nlengths create a feature gap between the RoPE features\nof the training and OOD testing positions, while RESO-\nNANCE ROPE reduces this gap to 0.\nExisting RoPE scaling methods (Xiong et al.,\n2023; Peng et al., 2023) mainly focus on the post-\ncritical dimensions of RoPE, since the rotary angle\nm\u03b8j on these dimensions extrapolates on OOD\npositions, hence creating a feature gap. In this\nsection, we argue that reducing RoPE\u2019s feature\ninterpolation on the pre-critical dimensions is also\nbeneficial for better length extrapolation.\nDue to a non-linear relationship between RoPE\nfeature R\u0398\nm and the token position m in Equation 3,\nthe interpolation on RoPE features is potentially\nhard for the model to generalize to. We found\nthat such potentially hard interpolation appears on\nthe pre-critical dimensions [0 : 2c \u2212 1], which\nhave wavelengths \u03bbj shorter than the pre-trained\nsequence length L. By default, the rotary base b\nof RoPE features is an integer or a fraction, which\nmakes their wavelength \u03bbj = 2\u03c0b\n2j\nd not an integer.\nAs the position index m \u2208 N increases, a phase\nshift of \u2206\u03d5 occurs for the rotary angle m\u03b8j after\neach full rotation. This could potentially result in a\nlarge distribution gap between the RoPE features\non positions seen during training and the OOD po-\nsitions. This phenomenon is illustrated in Figure 1.\nWe tackle this issue by developing a synergistic\nmodification to the conventional RoPE embedding,\nreferred to as RESONANCE ROPE. It aims to iden-\ntify the optimal angular frequency that minimizes\nthe interpolation gap, which ensures the corre-\nsponding wavelength closely matches the original\none while imposing alignment of the wavelength\nto an integer. More specifically, for a given angular\n4\nAlgorithm 1 Pseudocode of RESONANCE ROPE.\nRequire: \u03b80, \u03b81, \u00b7 \u00b7 \u00b7 , \u03b8 d\n2 \u22121 \u2208 \u0398\nfor i \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , d\n2 \u2212 1} do\n\u03bbi = 2\u03c0/\u03b8i\n\u02dc\u03bbi = round(\u03bbi)\n\u25b7 Round to integer wavelength\n\u02dc\u03b8i = 2\u03c0/\u02dc\u03bbi\nend for\n\u02dc\u0398 = {\u02dc\u03b80, \u02dc\u03b81, \u00b7 \u00b7 \u00b7 , \u02dc\u03b8 d\n2 \u22121}\nCompute Rd\n\u02dc\u0398 by Equation 2\nCompute q, k by Equation 4, 5\nfrequency set of RoPE \u0398 =\n\b\n\u03b81, \u03b82, . . . , \u03b8d/2\n\t\n, we\nround their wavelengths to their nearest integer to\neliminate new rotary angles on each feature. We\nprovide a pseudocode for RESONANCE ROPE in\nAlgorithm 1.\nAfter applying this technique, each RoPE feature\nrepeats after \u02dc\u03bbi tokens, and therefore \u201cresonates\u201d\nwith a specific span length and eliminates the in-\nterpolation gap between pre-trained and OOD po-\nsitions on pre-critical dimensions. We illustrate\nthe effect of RESONANCE ROPE on RoPE\u2019s fea-\nture gap on one of the pre-critical dimensions in\nFigure 1. Moreover, we can prove the feature gap\nreducing ability of our method. As for above, we\nformalize RESONANCE ROPE\u2019s computation rule\nas \u02dcf(x, m) = Rd\n\u02dc\u0398,mW x.\nTheorem 1. For a RoPE-equipped model with con-\ntext window L, RESONANCE ROPE \u02dcf reduces the\nfeature gap on pre-critical dimensions to 0. Specifi-\ncally, \u2200x \u2208 X, \u2200n \u2208 N\\{0, \u00b7 \u00b7 \u00b7 , L \u2212 1}, we have:\nmin\nm\u2208{0,\u00b7\u00b7\u00b7 ,L\u22121} | \u02dcf(x, m)i \u2212 \u02dcf(x, n)i| = 0\nfor all i = 0, . . . , 2c \u2212 1.\nSee the proof in Appendix A. Note that although\neach pre-critical RoPE feature R\u02dc\u03b8j,m repeats, the\ncombination of all {R\u02dc\u03b8j,m}j<c only repeats af-\nter the least common multiple (LCM) of all pre-\ncritical dimensions\u2019s wavelengths. For LLaMA2,\nthis LCM value is greater than 7 \u00d7 1051.\nBecause\nof\nits\nsimplicity,\nRESONANCE\nROPE can be applied on top of RoPE and all\nRoPE-based scaling methods to reduce their\nfeature gap in TSTL and further improve their\nperformance.\nMeanwhile, this method only\ninvolves an offline computation of the scaled\n\u03b8, thus does not introduce online computation\noverhead.\n5\nEvaluating Position Embeddings\nwith POSGEN\nIn this section, we propose our new position em-\nbedding evaluation suite: POSGEN, based on an\nanalysis of common failure patterns on existing\nposition embedding evaluation methods.\nWe consider a next token prediction task, where\nwe expect the model to generate the token xl given\nthe input sequence {x0, \u00b7 \u00b7 \u00b7 , xl\u22121}. In TSTL sce-\nnarios, when a model succeeds in correctly generat-\ning a token up to position L but fails systematically\nafterwards, we observe two failure patterns:\n\u2022 Failure due to harder algorithmic difficulty\non generating later tokens. The rule of gen-\nerating a new token xl may vary with the se-\nquence length l. Generally, tokens placed\nlater in the sequence depend on more con-\ntext tokens, which incurs a more complex de-\npendency pattern. During training on shorter\nsequences, the model only learns the token\ndependency rules involving up to L tokens,\nand might fail on longer sequences because it\nhas never been exposed to the more complex\ndependency rules.\n\u2022 Failure due to unrecognized new token po-\nsitions. The difference between training and\ntesting lengths in the TSTL setting creates\na feature gap between the position indices\nor position embeddings in training and infer-\nence. This feature gap makes it difficult for the\nmodel to generalize to new positions due to\nunrecognized features. RoPE scaling methods\nmainly focus on reducing this type of length\nextrapolation failure.\nCurrently,\nneither perplexity-based evalua-\ntions (Rae et al., 2020; Huang et al., 2021; Wu et al.,\n2022) nor synthetic TSTL evaluations (Kazemne-\njad et al., 2023; Liu et al., 2023a) can effectively\ndistinguish these two failure patterns, since the\ntoken generation difficulty tends to increase with\nrespect to the sequence length in these tasks. To fa-\ncilitate research on better position representations,\nwe design POSGEN, which controls the difficulty\nin generating tokens throughout the sequence to be\nidentical, which effectively distinguishes the two\ntypes of TSTL failures. Failures in this benchmark\nare only due to the inability to recognize new token\npositions in TSTL scenarios.\n5\nHuawei Proprietary - Restricted Distribution\n2\nRecursive\nCoT\nSemi-\nRecursive\n2\n1\n2\n0\n3\n1\n2\n0\n3\n1\n2\n0\n3\n1\n6\n3\n6\n2\n3\n0\n4\n6\n5\n0\n6\n6\n0\n0\n6\n5\n5\n2\n1\n4\n1\n0\nInput\nOutput\nSeed tokens\n(first \ud835\udc57\ud835\udc57 + \ud835\udc58\ud835\udc58 tokens)\nGenerated by the \ncorresponding rules\n= \u210e(\n)\n=\nmod \ud835\udc5a\ud835\udc5a\n,\n,\n,\n+\n+\n+\nFigure 2: An example of the three subtasks of POSGEN. In this example, h is a modular addition task with the\nmodulus m = 7 and the difficulty-controlling parameters j = 1, k = 3. The output token depends on: (1) only the\nlocal j + k tokens in the recursive task; (2) k local tokens and the beginning j tokens in the CoT task; and (3) k\nlocal tokens and j tokens with a varied dependency distance in the semi-recursive task.\nOur POSGEN framework comprises three sub-\ntasks, with each extracting the general token de-\npendency pattern of a different type of reason-\ning task. Suppose that we define a fixed function\nh : Vj+k \u2192 V, where V is the model\u2019s vocabu-\nlary and j, k are predefined constants controlling\nthe task\u2019s difficulty. The three subtasks of POS-\nGEN are as follows:\n1. Recursive. This task simulates the token de-\npendency pattern of generating a Fibonacci-\nstyle sequence, where new tokens depend\non j + k neighboring tokens only: xl =\nh(xl\u2212(j+k)), \u00b7 \u00b7 \u00b7 , xl\u22121) when l \u2265 j + k.\n2. Chain-of-Thought (CoT). This task simu-\nlates the token dependency pattern of CoT rea-\nsoning (Wei et al., 2022), where new tokens\ndepend on k neighboring tokens (simulating\nthe previous reasoning step) and j tokens in\nthe front (simulating the original question):\nxl = h(x0, \u00b7 \u00b7 \u00b7 , xj\u22121, xl\u2212k, \u00b7 \u00b7 \u00b7 , xl\u22121) when\nl \u2265 j + k.\n3. Semi-recursive.\nThis task simulates the\ntoken dependency pattern of the last-letter\nconcatenation task (Zhou et al., 2023),\nwhere\nnew\ntokens\ndepend\non\nboth\nk\nneighboring tokens (simulating the current\nprogress) and j tokens with varied distances\n(simulating the word sequence):\nxl\n=\nh(x\u230al\u2212(j+k)/2\u230b\u2212j, \u00b7 \u00b7 \u00b7 , x\u230al\u2212(j+k)/2\u230b\u22121,\nxl\u2212k, \u00b7 \u00b7 \u00b7 , xl\u22121) when l \u2265 j + k.\nBased on the equation for each subtask, when\ngiven the first j + k tokens, one can generate a\nsequence with unlimited length as the ground truth\nsequence. We show an example of POSGEN in Fig-\nure 2. As a TSTL benchmark, we train a model on\na subtask with sequence length up to L, and evalu-\nate the model\u2019s accuracy on a longer sequence with\nlength L\u2032 > L generated by the same rule on the\nunseen positions L < m \u2264 L\u2032, which we refer to\nas the \u201cOOD Accuracy\u201d (OOD Acc). This met-\nric measures how well a model can recognize the\nOOD positions and continue following the genera-\ntion rule learned during training. As a benchmark\nfor position embeddings, a standard usage of this\nbenchmark is to train a small Transformer (e.g., a\n2-layer Transformer as used in our experiments)\nwith different position embeddings on its training\nset with only short sequences, and test its OOD\nAccuracy on the test set with longer sequences.\nWe provide our experiment setting for POSGEN in\nmore details in Section 6.1.1 and Appendix B.1.\n6\nExperiments\nWe evaluate RESONANCE ROPE on three different\nTSTL tasks: a small-scale evaluation on our pro-\nposed POSGEN task, and LLM-scale evaluations\nwith LLaMA2-Chat (Touvron et al., 2023b) on both\nlanguage modeling perplexity and real-world long\ncontext applications.\n6.1\nSynthetic Task Evaluation\n6.1.1\nExperiment Setup\nWe first apply RESONANCE ROPE on RoPE and\nYaRN, assessing the model\u2019s performance on POS-\nGEN for unseen position recognition. We test on\na modular addition task, which was proved to be\nlearnable by a one-layer Transformer (Nanda et al.,\n2023). We configured j = 1, k = 3, and defined\nh(x0, x1, x2, x3) = P3\ni=0 xi mod 17 with vocab-\nulary V = {0, . . . , 16}. Our experiments involved\ntraining a two-layer Transformer with different\nRoPE-based embeddings on sequences of length\nL = 64, and evaluating on lengths of L\u2032 = 256\nfor OOD Accuracy. We generated 10,000 training\nsequences, and 1,000 each for validation and test-\ning, and ensure that the first j + k = 4 tokens in\n6\n0\n20\n40\n60\n80\n100\n120\n140\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLoss (Val)\nRecursive\n0\n20\n40\n60\n80\n100\n120\n140\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nCoT\n0\n20\n40\n60\n80\n100\n120\n140\nEpoch\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nSemi-Recursive\nRoPE\nResonance RoPE (Ours)\nYaRN (s=4)\nResonance YaRN (s=4) (Ours)\nFigure 3: The validation loss curves of Transformers using RoPE and YaRN PEs with and without our RESONANCE\nscaling on the three subtasks of POSGEN.\nSetting\nRecursive\nCoT\nSemi-Rec.\nRoPE\n65.29\u00b10.43\n69.56\u00b10.33\n17.96\u00b10.03\nRes. RoPE (Ours)\n62.64\u00b10.15\n75.25\u00b10.10\n29.78\u00b10.07\nYaRN\n95.93\u00b10.04\n98.71\u00b10.00\n33.70\u00b10.04\nRes. YaRN (Ours)\n98.30\u00b10.00\n99.58\u00b10.00\n48.46\u00b10.03\nTable 1: The accuracy on OOD Positions (OOD Acc.)\non POSGEN\u2019s test set. All results are in percentage (%).\nWe report both the mean and variance across five runs\nwith different random seeds. We compare the same\nRoPE-based PE with or without our RESONANCE scal-\ning. The best performance for each pair of settings on\neach subtask is marked in Bold.\neach sequence do not overlap to testify whether\nthe model learns the correct generation mechanism.\nWe averaged results over 5 seeds. A more detailed\nsetting is provided in Appendix B.1.\n6.1.2\nResults and Analysis\nTable 1 displays the comparison of the OOD accu-\nracy. In most cases, RESONANCE ROPE and RES-\nONANCE YARN outperform their counterparts\nlacking the Resonance technique, showcasing sig-\nnificantly better performance and reduced variance\nin OOD scenarios. This improvement indicates a\nsuperior adaptation to OOD position embeddings\nthrough minimized Positional Encoding (PE) in-\nterpolation. An exception is observed when apply-\ning RESONANCE ROPE to the Recursive subtask,\nlikely due to the dominance of extrapolated post-\ncritical dimensions in OOD positions. This issue\ncan be mitigated by employing a RoPE scaling tech-\nnique such as YaRN, which effectively counters the\nextrapolation of post-critical dimensions. Among\nall configurations, RESONANCE YARN exhibits\nthe highest OOD performance, demonstrating the\nsynergy between RoPE scaling methods and the\nResonance technique.\nFigure 3 plots validation losses against training\nepochs for different PEs, illustrating the training\ndynamics. The introduction of the Resonance tech-\nnique leads to a reduction in the lowest validation\nloss for both RoPE and YaRN, with RESONANCE\nROPE achieving even lower validation losses than\nYaRN in the Semi-Recursive subtask. Furthermore,\nthe validation loss trajectories for RESONANCE\nROPE and RESONANCE YARN remain lower than\nthose of their counterparts in all subtasks, further\ndemonstrating the enhanced OOD generalization\ncapability of our approach.\n6.2\nLLM Fine-tuning Evaluation\n6.2.1\nExperiment Setup\nIn this section, we apply our proposed RESO-\nNANCE ROPE to the current state-of-the-art RoPE\nscaling method, YaRN (Peng et al., 2023). More\nspecifically, we replace the original position em-\nbeddings of LLaMA2 7B and 13B (Touvron et al.,\n2023b) with a series of scaled position embeddings,\nincluding the NTK-Aware scaling (bloc97, 2023;\nXiong et al., 2023; Liu et al., 2023b), Dynamic\nNTK-Aware Scaling (Peng et al., 2023; Rozi\u00e8re\net al., 2023), and YaRN (Peng et al., 2023).\nFor YaRN and RESONANCE YARN, We use a\nscaling factor of 8 and 4 for LLaMA2 7B and 13B\nto extend their context window from 4K to 32K\nand 16K, respectively. For the configurations that\nrequire fine-tuning, we fine-tune the LLM with the\nscaled position embedding on the training set of\nPG19 (Rae et al., 2020) with the fine-tuning set-\nting and hyperparameters adopted directly from\nYaRN (Peng et al., 2023), with the only difference\nbeing that we control the total training token count\nto be approximately 100M. A more detailed fine-\ntuning setting can be found in Appendix B.2. We\ntest the model\u2019s performance on two TSTL sce-\nnarios: language modeling evaluation on long-text\nsequences and long-text downstream application\nperformance.\n7\nSetting\nCtx Len.\nCoursera\nGSM\nQuALITY\nTOEFL\nCodeU\nSFiction\nAvg.\nLLaMA2-Chat 7B\nDynamic NTK-Aware (no FT)\n32K\n31.98\n32.00\n34.65\n59.11\n1.11\n36.72\n32.59\nNTK-Aware (s = 8, no FT)\n32K\n36.77\n3.00\n26.73\n34.2\n1.11\n50.78\n25.43\nYaRN (s = 8, FT@32K, 50 epcs.)\n32K\n36.05\n19.00\n33.17\n50.56\n4.44\n56.25\n33.24\nResonance YaRN (s = 8, FT@32K, 50 epcs.)\n32K\n36.48\n22.00\n34.16\n55.76\n0.00\n57.03\n34.24\nYaRN (s = 8, FT@4K, 400 epcs.)\n32K\n35.03\n24.00\n37.62\n57.62\n4.44\n60.94\n36.61\nResonance YaRN (s = 8, FT@4K, 400 epcs.)\n32K\n36.34\n27.00\n40.59\n56.51\n3.33\n61.72\n37.58\nLLaMA2-Chat 13B\nDynamic NTK-Aware (no FT)\n16K\n29.22\n39.00\n40.59\n63.94\n1.11\n39.84\n35.62\nNTK-Aware (s = 4, no FT)\n16K\n40.26\n21.00\n38.12\n65.43\n1.11\n46.88\n35.47\nYaRN (s = 4, FT@16K, 100 epcs.)\n16K\n38.08\n39.00\n43.07\n65.43\n0.00\n63.28\n41.48\nResonance YaRN (s = 4, FT@16K, 100 epcs.)\n16K\n38.66\n39.00\n43.56\n65.06\n1.11\n62.50\n41.65\nYaRN (s = 4, FT@4K, 400 epcs.)\n16K\n41.72\n34.00\n41.09\n66.91\n2.22\n48.44\n39.06\nResonance YaRN (s = 4, FT@4K, 400 epcs.)\n16K\n41.86\n35.00\n42.57\n65.80\n5.56\n48.44\n39.87\nTable 2: Long text evaluations on some closed-ended tasks in L-Eval. \u201cCtx Len\u201d means the target context length of\nthe model after scaling its PE. \u201cFT@32K, 50 epcs\u201d means the model is fine-tuned on 32K sequence length for 50\nepochs. The settings with \u201cno FT\u201d are not fine-tuned after modifying its position embedding. We highlight the best\nand second-best performance for each base model in Bold and Underline, respectively.\n10000\n20000\n30000\nContext Length\n4\n5\n6\n7\nPerplexity (PPL)\nGovReport\n10000\n20000\n30000\nContext Length\n2\n4\n6\nProofpile\nDynamic NTK\nYaRN (s=8)\nNTK-Aware Scaling (s=8)\nResonance YaRN (s=8) (Ours)\nFigure 4: The perplexity of LLaMA-Chat 7B with differ-\nent position embeddings on GovReport and Proofpile.\n6.2.2\nPerplexity on Long Sequence\nWe evaluate the model\u2019s language modeling per-\nformance on GovReport (Huang et al., 2021) and\nProofpile (Azerbayev, 2022). We randomly select\n50 samples from each dataset and report the final\nperplexity in text fragments of gradually increased\nlength. We report the results in Figure 4. Of the\ntested methods, RESONANCE YARN achieves the\nlowerst perplexity across all context lengths. Espe-\ncially, RESONANCE YARN achieves a lower per-\nplexity compared to YaRN with the same set of\nhyperparameters optimized for YaRN, demonstrat-\ning the benefit of applying the Resonance technique\nto existing RoPE scaling methods.\n6.2.3\nReal-world Task Evaluation\nLastly, we test the real-world task performance of\nLLaMA2-Chat 7B and 13B\u2019s performance with dif-\nferent RoPE scaling strategies on L-Eval (An et al.,\n2023)\u2019s close ended task suite, a long-text LLM\nbenchmark covering a wide range of domains such\nas school lectures, long conversations and novels.\nWe fine-tune the model with different RoPE scaling\nstrategies using two different strategies: training on\nshorter sequences (4K length) for more epochs, and\ntraining on longer sequences (32K or 16K length)\nfor less epochs. All settings requiring fine-tuning\nkeep the training token count to be approximately\n100M. The results are listed in Table 2.\nAlthough no single setting in the experiment\nachieves the best result on all subtasks, we observe\nthat applying RESONANCE YARN achieves better\naverage performance in different training settings\nand model sizes compared to its counterpart YaRN\nsetting. This further proves the compatibility of the\nResonance technique and RoPE scaling methods,\nand the better length extrapolation performance\nbrought by our proposed method.\n7\nConclusion\nWe introduce RESONANCE ROPE, a novel en-\nhancement of RoPE that focuses on minimizing\nthe interpolation of RoPE features for OOD posi-\ntions, thereby reducing the generalization gap and\nimprove LLM\u2019s performance on train-short-test-\nlong (TSTL) scenarios. Additionally, we present\na novel synthetic benchmark, POSGEN, which\nprovides a fine-grained analysis on the model\u2019s\nTSTL performance regarding various token de-\npendency patterns. Extensive experiments on our\nproposed POSGEN and two LLM-based evalua-\ntions demonstrate RESONANCE ROPE\u2019s efficacy\nin identifying OOD positions and its compatibil-\nity with current RoPE scaling strategies. Future\nwork includes exploring RESONANCE ROPE\u2019s per-\nformance on other foundational models, and the\nidentification of more optimal wavelength combi-\nnations for RoPE features.\n8\nLimitations\nOur proposed RESONANCE ROPE focus on reduc-\ning the interpolation of only RoPE\u2019s pre-critical\ndimensions on OOD positions.\nHowever, this\nmethod does not solve the extrapolation issue on\nRoPE\u2019s post-critical dimensions, which has been\nshown to be also detrimental to LLM\u2019s length\nextrapolation performance. Thus, the technique\nof RESONANCE ROPE needs to be combined with\nanother RoPE scaling method that can reduce ex-\ntrapolation on RoPE\u2019s post-critical dimensions,\ne.g., YaRN, to achieve the full potential of LLM in\nTSTL scenarios. Such combination has been our\nfocus in Section 6.2.\nSecondly, applying LLMs to long text sequences\nrequires considerations on both performance and\nefficiency due to the super-linear complexity of\nTransformers w.r.t input length. As an improve-\nment of the position embeddings, we focus only\non improving Transformers\u2019 performance in TSTL\nscenarios. An interesting future direction would\nbe to apply RESONANCE ROPE to efficient Trans-\nformers for both performance and efficiency en-\nhancements.\nLastly, benchmarking LLMs is still an open ques-\ntion, as there is currently no benchmark to thor-\noughly test the performance of LLMs, especially\non long-sequence tasks. We expect that a more\ncomprehensive long-text benchmark would further\nimprove the validity of the experiment results.\nReferences\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li,\nJun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\nL-eval: Instituting standardized evaluation for long\ncontext language models. CoRR, abs/2307.11088.\nZhangir Azerbayev. 2022.\nzhangir-azerbayev/proof-\npile.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2023. Longbench: A bilingual, mul-\ntitask benchmark for long context understanding.\nCoRR, abs/2308.14508.\nbloc97. 2023. NTK-Aware Scaled RoPE allows LLaMA\nmodels to have extended (8k+) context size without\nany fine-tuning and minimal perplexity degradation.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\nCoRR, abs/2306.15595.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning.\nCoRR,\nabs/2307.08691.\nLuyang Huang, Shuyang Cao, Nikolaus Nova Parulian,\nHeng Ji, and Lu Wang. 2021. Efficient attentions for\nlong document summarization. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1419\u20131436.\nAssociation for Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nth\u00e9e Lacroix, and William El Sayed. 2023. Mistral\n7b. CoRR, abs/2310.06825.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de Las Casas,\nEmma Bou Hanna, Florian Bressand, Gianna\nLengyel,\nGuillaume Bour,\nGuillaume Lample,\nL\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nTh\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mix-\ntral of experts. CoRR, abs/2401.04088.\nAmirhossein\nKazemnejad,\nInkit\nPadhi,\nKarthikeyan Natesan Ramamurthy,\nPayel Das,\nand Siva Reddy. 2023.\nThe impact of positional\nencoding on length generalization in transformers.\nCoRR, abs/2305.19466.\nBingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Kr-\nishnamurthy, and Cyril Zhang. 2023a. Transformers\nlearn shortcuts to automata. In The Eleventh Inter-\nnational Conference on Learning Representations.\nOpenReview.net.\nXiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,\nXipeng Qiu, and Dahua Lin. 2023b. Scaling laws of\nrope-based extrapolation. CoRR, abs/2310.05209.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations. OpenRe-\nview.net.\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess\nSmith, and Jacob Steinhardt. 2023. Progress mea-\nsures for grokking via mechanistic interpretability. In\nThe Eleventh International Conference on Learning\nRepresentations. OpenReview.net.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context win-\ndow extension of large language models.\nCoRR,\nabs/2309.00071.\n9\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In The Tenth International\nConference on Learning Representations. OpenRe-\nview.net.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations. OpenReview.net.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\nJie Ren, Samyam Rajbhandari, Reza Yazdani Am-\ninabadi, Olatunji Ruwase, Shuangyan Yang, Min-\njia Zhang, Dong Li, and Yuxiong He. 2021. Zero-\noffload: Democratizing billion-scale model train-\ning. In 2021 USENIX Annual Technical Conference,\npages 551\u2013564. USENIX Association.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-\nish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D\u00e9fossez, Jade Copet,\nFaisal Azhar, Hugo Touvron, Louis Martin, Nico-\nlas Usunier, Thomas Scialom, and Gabriel Synnaeve.\n2023. Code llama: Open foundation models for code.\nCoRR, abs/2308.12950.\nAnian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi\nGrau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane\nLegg, and Joel Veness. 2023. Randomized positional\nencodings boost length generalization of transform-\ners. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 1889\u20131903, Toronto, Canada.\nAssociation for Computational Linguistics.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,\nand Omer Levy. 2023. ZeroSCROLLS: A zero-shot\nbenchmark for long text understanding.\nIn Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 7977\u20137989, Singapore.\nAssociation for Computational Linguistics.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng\nPan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transform-\ners. In 9th International Conference on Learning\nRepresentations. OpenReview.net.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, pages 5998\u2013\n6008.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. In Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2022.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In The Tenth International Conference on Learn-\ning Representations. OpenReview.net.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz,\nMadian Khabsa, Han Fang, Yashar Mehdad, Sharan\nNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,\nSergey Edunov, Mike Lewis, Sinong Wang, and Hao\nMa. 2023. Effective long-context scaling of founda-\ntion models. CoRR, abs/2309.16039.\nLiang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin,\nand Ting Liu. 2023. Length extrapolation of trans-\nformers: A survey from the perspective of position\nencoding. CoRR, abs/2312.17044.\n10\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations. OpenReview.net.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-\nhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-\ncient context window extension of llms via positional\nskip-wise training. CoRR, abs/2309.10400.\n11\nA\nProof of Theorem 1\nProof. All we need is to prove that for each x \u2208\nRd, each n \u2208 N\\{0, \u00b7 \u00b7 \u00b7 , L \u2212 1} and each i =\n0, . . . , 2c \u2212 1 we can find m \u2208 {0, \u00b7 \u00b7 \u00b7 , L \u2212 1} ,\nsuch that \u02dcf(x, m)i = \u02dcf(x, n)i. By definition, it is\nequivalent to solving the equations:\n(Rd\n\u02dc\u0398,mW x)i = (Rd\n\u02dc\u0398,nW x)i\nfor m, given i, n, and x.\nThe RoPE feature matrix Rd\n\u0398,m is defined as\nblock-diagonal with 2 \u00d7 2 blocks given by Equa-\ntion 3. Hence, given i, x and n, the equation re-\nduces to equality of a linear combination of trigono-\nmetric functions:\na cos m\u02dc\u03b8i + b sin m\u02dc\u03b8i = a cos n\u02dc\u03b8i + b sin n\u02dc\u03b8i\nfor a, b \u2208 R, depending on x and i. This equality\nclearly holds if m\u02dc\u03b8i \u2212 n\u02dc\u03b8i is a multiple of 2\u03c0:\n(m \u2212 n)\u02dc\u03b8i = 2\u03c0k,\nfor some k \u2208 Z. By our construction, 2\u03c0\n\u02dc\u03b8i is a\nnatural number. Hence, to finish the proof that we\ncan solve our initial equation for m, we need to\nshow that we can find integer k to satisfy:\n\u0012\nn \u2212 2\u03c0\n\u02dc\u03b8i\nk\n\u0013\n\u2208 {0, \u00b7 \u00b7 \u00b7 , L \u2212 1}\nfor n \u2208 N\\{0, \u00b7 \u00b7 \u00b7 , L \u2212 1}. This is where we\nuse the pre-critical dimension condition: for i =\n0, . . . , 2c \u2212 1, by definition of c, we have the in-\nequality 0 \u2264 2\u03c0\n\u02dc\u03b8i < L. Taking k = \u230a n\u03b8i\n2\u03c0 \u230b will give\nus the required range for m and hence finish the\nproof.\nB\nDetailed Experiment Settings\nIn this section, we provide the detailed experi-\nment settings for both our synthetic task evalua-\ntion on POSGEN and LLM-based evaluations on\nboth upstream language modeling evaluation and\ndownstream real-world application evaluations.\nB.1\nSynthetic Task Evaluation on POSGEN\nFor the synthetic task experiments in Section 6.1.1,\nwe train a two-layer Transformer on each of the\nsubtasks, with each layer following the configu-\nration of a T5-Small model (Raffel et al., 2020).\nFor each subtask, we train the model with different\nposition embeddings on a training set with 10,000\nsequence samples of length 64. The validation and\ntest sets each contain 1,000 sequence samples with\nlength 256. The sequences in the training, valida-\ntion and test sets do not overlap in the first j + k\ntokens. For all YaRN and RESONANCE YARN set-\ntings, we train the model with YaRN and RESO-\nNANCE YARN applied to the model with a scal-\ning factor s = 4, which corresponds to the TSTL\nsetting of our evaluation. Each model is trained\non each subtask for 150 epochs with a language\nmodeling-style cross-entropy loss. Training was\ndone with AdamW optimizer (Loshchilov and Hut-\nter, 2019), using learning rate 2 \u00d7 10\u22124 and weight\ndecay 1 \u00d7 10\u22122. We use a batch size of 128 for\nall experiments. All hyperparameters were tuned\nto maximize YaRN\u2019s validation set performance\non the Semi-Recurrent subtask. All synthetic task\nevaluations were performed on a single NVIDIA\nV100 32G GPU.\nB.2\nLLM Evaluations\nFor the LLM-based evaluations in Section 6.2,\nwe fine-tune LLaMA2-Chat 7B or LLaMA2-Chat\n13B (Touvron et al., 2023b) after replacing its orig-\ninal RoPE position embedding with RoPE scaled\nwith different strategies:\n\u2022 NTK-Aware Scaling (bloc97, 2023; Xiong\net al., 2023; Liu et al., 2023b), which scales\nthe base b in Equation 1 to s \u00b7 b, where s is the\nscaling factor. We evaluate the performance\nwithout fine-tuning as used in bloc97 (2023).\n\u2022 Dynamic NTK-Aware Scaling (Peng et al.,\n2023; Rozi\u00e8re et al., 2023). This method dy-\nnamically computes the scaling factor consid-\nering the current sequence length Lc and the\noriginal context window length L: s = Lc\nL .\nDue to the high cost of frequently recomput-\ning RoPE features, we evaluated its perfor-\nmance without fine-tuning.\n\u2022 YaRN (Peng et al., 2023). We evaluate its\nperformance after fine-tuning.\nFor NTK-Aware scaling and Dynamic NTK-\nAware scaling settings, we replace the original\nRoPE position embeddings in the model with the\nscaled ones and test their performance without fine-\ntuning following (bloc97, 2023; Peng et al., 2023).\nFor YaRN and RESONANCE YARN settings, we\nfine-tune the model for approximately 100M to-\nkens on PG19\u2019s training set (Rae et al., 2020). Our\n12\ntarget scaled length for the 7B and 13B models is\n32K and 16K, respectively, which corresponds to\na scaling factor s = 8 and s = 4 for the position\nembeddings of the two models.\nFor both the long-sequence perplexity evalua-\ntion in Section 6.2.2 and real-world task evalua-\ntions in Section 6.2.3, the hyperparameters for the\nLLM experiments follow the configurations pro-\nvided in Peng et al. (2023)2, with the only modifica-\ntion that we fine-tune the model on approximately\n100M tokens. More specifically, we use \u03b1 = 1\nand \u03b2 = 32 for YaRN and RESONANCE YARNas\nsuggested by Peng et al. (2023). The model was\ntrained with a language modeling-style cross en-\ntropy loss. Training was done with the AdamW op-\ntimizer (Loshchilov and Hutter, 2019) using learn-\ning rate 2 \u00d7 10\u22125 and weight decay 1 \u00d7 10\u22122. We\nuse a batch size of 1 on each of the GPUs. The\nlearning rate warm-up is applied to the first 5% of\nthe total training steps. Models were fine-tuned\nwith BF16 precision, FlashAttention 2 (Dao, 2023)\nand DeepSpeed ZeRO-3 Offload (Ren et al., 2021)\non four NVIDIA A100 40G GPUs.\nFor the real-world task evaluations in Sec-\ntion 6.2.3, we further compare two different fine-\ntuning strategies:\n1. Fine-tuning on long sequences for less\nepochs. We directly fine-tune the model on\nthe target sequence lengths after applying the\nscaled position embeddings. For LLaMA2-\nChat 7B and 13B, we fine-tune the model on\nsequences with length 32,768 for 50 steps and\nsequences with length 16,384 for 100 steps,\nrespectively.\n2. Finetuning on short sequences for more\nepochs. We fine-tune the model on the origi-\nnal pre-training sequence length after apply-\ning the scaled position embeddings. For both\nLLaMA2-Chat 7B and 13B, we fine-tune the\nmodel on sequences with length 4,096 for 400\nsteps.\n2https://github.com/jquesnelle/yarn.\n13\n"
  },
  {
    "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
    "link": "https://arxiv.org/pdf/2403.00745.pdf",
    "upvote": "8",
    "text": "2024-02-23\nAtP\u2217: An efficient and scalable method for\nlocalizing LLM behaviour to components\nJ\u00e1nos Kram\u00e1r1, Tom Lieberum1, Rohin Shah1 and Neel Nanda1\n1Google DeepMind\nActivation Patching is a method of directly computing causal attributions of behavior to model compo-\nnents. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of\nmodel components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We\ninvestigate Attribution Patching (AtP) (Nanda, 2022), a fast gradient-based approximation to Activation\nPatching and find two classes of failure modes of AtP which lead to significant false negatives.\nWe propose a variant of AtP called AtP\u2217, with two changes to address these failure modes while retaining\nscalability. We present the first systematic study of AtP and alternative methods for faster activation\npatching and show that AtP significantly outperforms all other investigated methods, with AtP\u2217 providing\nfurther significant improvement. Finally, we provide a method to bound the probability of remaining\nfalse negatives of AtP\u2217 estimates.\n1. Introduction\nAs LLMs become ubiquitous and integrated into numerous digital applications, it\u2019s an increasingly\npressing research problem to understand the internal mechanisms that underlie their behaviour \u2013\nthis is the problem of mechanistic interpretability. A fundamental subproblem is to causally attribute\nparticular behaviours to individual parts of the transformer forward pass, corresponding to specific\ncomponents (such as attention heads, neurons, layer contributions, or residual streams), often at\nspecific positions in the input token sequence. This is important because in numerous case studies of\ncomplex behaviours, they are found to be driven by sparse subgraphs within the model (Meng et al.,\n2023; Olsson et al., 2022; Wang et al., 2022).\nA classic form of causal attribution uses zero-ablation, or knock-out, where a component is deleted\nand we see if this negatively affects a model\u2019s output \u2013 a negative effect implies the component was\ncausally important. More recent work has generalised this to replacing a component\u2019s activations with\nsamples from some baseline distribution (with zero-ablation being a special case where activations\nare resampled to be zero). We focus on the popular and widely used method of Activation Patching\n(also known as causal mediation analysis) (Chan et al., 2022; Geiger et al., 2022; Meng et al., 2023)\nwhere the baseline distribution is a component\u2019s activations on some corrupted input, such as an\nalternate string with a different answer (Pearl, 2001; Robins and Greenland, 1992).\nGiven a causal attribution method, it is common to sweep across all model components, directly\nevaluating the effect of intervening on each of them via resampling (Meng et al., 2023). However,\nwhen working with SoTA models it can be expensive to attribute behaviour especially to small\ncomponents (e.g. heads or neurons) \u2013 each intervention requires a separate forward pass, and so the\nnumber of forward passes can easily climb into the millions or billions. For example, on a prompt of\nlength 1024, there are 2.7 \u00b7 109 neuron nodes in Chinchilla 70B (Hoffmann et al., 2022).\nWe propose to accelerate this process by using Attribution Patching (AtP) (Nanda, 2022), a faster,\napproximate, causal attribution method, as a prefiltering step: after running AtP, we iterate through\nthe nodes in decreasing order of absolute value of the AtP estimate, then use Activation Patching to\nmore reliably evaluate these nodes and filter out false positives \u2013 we call this verification. We typically\nCorresponding author: janosk@google.com\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2403.00745v1  [cs.LG]  1 Mar 2024\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\ncare about a small set of top contributing nodes, so verification is far cheaper than iterating over all\nnodes.\nOur contributions:\n\u2022 We investigate the performance of AtP, finding two classes of failure modes which produce false\nnegatives. We propose a variant of AtP called AtP\u2217, with two changes to address these failure\nmodes while retaining scalability:\n\u2013 When patching queries and keys, recomputing the attention softmax and using a gradient\nbased approximation from then on, as gradients are a poor approximation to saturated\nattention.\n\u2013 Using dropout on the backwards pass to fix brittle false negatives, where significant positive\nand negative effects cancel out.\n\u2022 We introduce several alternative methods to approximate Activation Patching as baselines to\nAtP which outperform brute force Activation Patching.\n\u2022 We present the first systematic study of AtP and these alternatives and show that AtP significantly\noutperforms all other investigated methods, with AtP\u2217 providing further significant improvement.\n\u2022 To estimate the residual error of AtP\u2217 and statistically bound the sizes of any remaining false\nnegatives we provide a diagnostic method, based on using AtP to filter out high impact nodes,\nand then patching random subsets of the remainder. Good diagnostics mean that practitioners\nmay still gauge whether AtP is reliable in relevant domains without the costs of exhaustive\nverification.\nFinally, we provide some guidance in Section 5.4 on how to successfully perform causal attribution\nin practice and what attribution methods are likely to be useful and under what circumstances.\n2. Background\n2.1. Problem Statement\nOur goal is to identify the contributions to model behavior by individual model components. We\nfirst formalize model components, then formalize model behaviour, and finally state the contribution\nproblem in causal language. While we state the formalism in terms of a decoder-only transformer\nlanguage model (Radford et al., 2018; Vaswani et al., 2017), and conduct all our experiments on\nmodels of that class, the formalism is also straightforwardly applicable to other model classes.\nModel components.\nWe are given a model M : \ud835\udc4b \u2192 \u211d\ud835\udc49 that maps a prompt (token sequence)\n\ud835\udc65 \u2208 \ud835\udc4b := {1, . . . , \ud835\udc49}\ud835\udc47 to output logits over a set of \ud835\udc49 tokens, aiming to predict the next token in the\nsequence. We will view the model M as a computational graph (\ud835\udc41, \ud835\udc38) where the node set \ud835\udc41 is the\nset of model components, and a directed edge \ud835\udc52 = (\ud835\udc5b1, \ud835\udc5b2) \u2208 \ud835\udc38 is present iff the output of \ud835\udc5b1 is a\ndirect input into the computation of \ud835\udc5b2. We will use \ud835\udc5b(\ud835\udc65) to represent the activation (intermediate\ncomputation result) of \ud835\udc5b when computing M(\ud835\udc65).\nThe choice of \ud835\udc41 determines how fine-grained the attribution will be. For example, for transformer\nmodels, we could have a relatively coarse-grained attribution where each layer is considered a single\nnode. In this paper we will primarily consider more fine-grained attributions that are more expensive\nto compute (see Section 4 for details); we revisit this issue in Section 5.\n2\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) MLP neurons, on CITY-PP.\n(b) Attention nodes, on IOI-PP.\nFigure 1 | Costs of finding the most causally-important nodes in Pythia-12B using different methods,\non sample prompt pairs (see Table 1). The shading indicates geometric standard deviation. Cost\nis measured in forward passes, thus each point\u2019s y-coordinate gives the number of forward passes\nrequired to find the top \ud835\udc65 nodes. Note that each node must be verified, thus \ud835\udc66 \u2265 \ud835\udc65, so all lines are\nabove the diagonal, and an oracle for the verification order would produce the diagonal line. For a\ndetailed description see Section 4.3.\n(a) MLP neurons, on CITY-PP.\n(b) Attention nodes, on IOI-PP.\nFigure 2 | Relative costs of methods across models, on sample prompt pairs. The costs are relative\nto having an oracle, which would verify nodes in decreasing order of true contribution size. Costs\nare aggregated using an inverse-rank-weighted geometric mean. This means they correspond to the\narea above the diagonal for each curve in Figure 1 and are relative to the area under the dotted\n(oracle) line. See Section 4.2 for more details on this metric. Note that GradDrop (difference between\nAtP+QKfix and AtP\u2217) comes with a noticeable upfront cost and so looks worse in this comparison\nwhile still helping avoid false negatives as shown inFigure 1.\n3\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nModel behaviour.\nFollowing past work (Chan et al., 2022; Geiger et al., 2022; Wang et al., 2022),\nwe assume a distribution D over pairs of inputs \ud835\udc65clean, \ud835\udc65noise, where \ud835\udc65clean is a prompt on which the\nbehaviour occurs, and \ud835\udc65noise is a reference prompt which we use as a source of noise to intervene\nwith1. We are also given a metric2 L : \u211d\ud835\udc49 \u2192 \u211d, which quantifies the behaviour of interest.\nContribution of a component.\nSimilarly to the work referenced above we define the contribution\n\ud835\udc50(\ud835\udc5b) of a node \ud835\udc5b to the model\u2019s behaviour as the counterfactual absolute3 expected impact of replacing\nthat node on the clean prompt with its value on the reference prompt \ud835\udc65noise.\nUsing do-calculus notation (Pearl, 2000) this can be expressed as \ud835\udc50(\ud835\udc5b) := |I(\ud835\udc5b)|, where\nI(\ud835\udc5b) := \ud835\udd3c(\ud835\udc65clean,\ud835\udc65noise)\u223cD\nh\nI(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\ni\n,\n(1)\nwhere we define the intervention effect I for \ud835\udc65clean, \ud835\udc65noise as\nI(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise) := L(M(\ud835\udc65clean | do(\ud835\udc5b \u2190 \ud835\udc5b(\ud835\udc65noise)))) \u2212 L(M(\ud835\udc65clean)).\n(2)\nNote that the need to average the effect across a distribution adds a potentially large multiplicative\nfactor to the cost of computing \ud835\udc50(\ud835\udc5b), further motivating this work.\nWe can also intervene on a set of nodes \ud835\udf02 = {\ud835\udc5b\ud835\udc56}. To do so, we overwrite the values of all nodes in\n\ud835\udf02 with their values from a reference prompt. Abusing notation, we write \ud835\udf02(\ud835\udc65) as the set of activations\nof the nodes in \ud835\udf02, when computing M(\ud835\udc65).\nI(\ud835\udf02; \ud835\udc65clean, \ud835\udc65noise) := L(M(\ud835\udc65clean | do(\ud835\udf02 \u2190 \ud835\udf02(\ud835\udc65noise)))) \u2212 L(M(\ud835\udc65clean))\n(3)\nWe note that it is also valid to define contribution as the expected impact of replacing a node\non the reference prompt with its value on the clean prompt, also known as denoising or knock-in.\nWe follow Chan et al. (2022); Wang et al. (2022) in using noising, however denoising is also widely\nused in the literature (Lieberum et al., 2023; Meng et al., 2023). We briefly consider how this choice\naffects AtP in Section 5.2.\n2.2. Attribution Patching\nOn state of the art models, computing \ud835\udc50(\ud835\udc5b) for all \ud835\udc5b can be prohibitively expensive as there may be\nbillions or more nodes. Furthermore, to compute this value precisely requires evaluating it on all\nprompt pairs, thus the runtime cost of Equation (1) for each \ud835\udc5b scales with the size of the support of D.\nWe thus turn to a fast approximation of Equation (1). As suggested by Figurnov et al. (2016);\nMolchanov et al. (2017); Nanda (2022), we can make a first-order Taylor expansion to I(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\naround \ud835\udc5b(\ud835\udc65noise) \u2248 \ud835\udc5b(\ud835\udc65clean):\n1This precludes interventions which use activation values that are never actually realized, such as zero-ablation or mean\nablation. An alternative formulation via distributions of activation values is also possible.\n2Common metrics in language models are next token prediction loss, difference in log prob between a correct and\nincorrect next token, probability of the correct next token, etc.\n3The sign of the impact may be of interest, but in this work we\u2019ll focus on the magnitude, as a measure of causal\nimportance.\n4\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n\u02c6IAtP(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise) := (\ud835\udc5b(\ud835\udc65noise) \u2212 \ud835\udc5b(\ud835\udc65clean))\u22ba \ud835\udf15L(M(\ud835\udc65clean))\n\ud835\udf15\ud835\udc5b\n\f\f\f\n\ud835\udc5b=\ud835\udc5b(\ud835\udc65clean)\n(4)\nThen, similarly to Syed et al. (2023), we apply this to a distribution by taking the absolute value\ninside the expectation in Equation (1) rather than outside; this decreases the chance that estimates\nacross prompt pairs with positive and negative effects might erroneously lead to a significantly smaller\nestimate. (We briefly explore the amount of cancellation behaviour in the true effect distribution in\nAppendix B.2.) As a result, we get an estimate\n\u02c6\ud835\udc50AtP(\ud835\udc5b) := \ud835\udd3c\ud835\udc65clean,\ud835\udc65noise\nh\f\f\f \u02c6IAtP(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\f\ni\n.\n(5)\nThis procedure is also called Attribution Patching (Nanda, 2022) or AtP. AtP requires two forward\npasses and one backward pass to compute an estimate score for all nodes on a given prompt pair, and\nso provides a very significant speedup over brute force activation patching.\n3. Methods\nWe now describe some failure modes of AtP and address them, yielding an improved method AtP*.\nWe then discuss some alternative methods for estimating \ud835\udc50(\ud835\udc5b), to put AtP(*)\u2019s performance in context.\nFinally we discuss how to combine Subsampling, one such alternative method described in Section 3.3,\nand AtP* to give a diagnostic to statistically test whether AtP* may have missed important false\nnegatives.\n3.1. AtP improvements\nWe identify two common classes of false negatives occurring when using AtP.\nThe first failure mode occurs when the preactivation on \ud835\udc65clean is in a flat region of the activation\nfunction (e.g. produces a saturated attention weight), but the preactivation on \ud835\udc65noise is not in that\nregion. As is apparent from Equation (4), AtP uses a linear approximation to the ground truth\nin Equation (1), so if the non-linear function is badly approximated by the local gradient, AtP ceases\nto be accurate \u2013 see Figure 3 for an illustration and Figure 4 which denotes in color the maximal\ndifference in attention observed between prompt pairs, suggesting that this failure mode occurs in\npractice.\nAnother, unrelated failure mode occurs due to cancellation between direct and indirect effects:\nroughly, if the total effect (on some prompt pair) is a sum of direct and indirect effects (Pearl,\n2001) I(\ud835\udc5b) = Idirect(\ud835\udc5b) + Iindirect(\ud835\udc5b), and these are close to cancelling, then a small multiplicative\napproximation error in \u02c6Iindirect\nAtP\n(\ud835\udc5b), due to non-linearities such as GELU and softmax, can accidentally\ncause | \u02c6Idirect\nAtP\n(\ud835\udc5b) + \u02c6Iindirect\nAtP\n(\ud835\udc5b)| to be orders of magnitude smaller than |I(\ud835\udc5b)|.\n3.1.1. False negatives from attention saturation\nAtP relies on the gradient at each activation being reflective of the true behaviour of the function with\nrespect to intervention at that activation. In some cases, though, a node may immediately feed into a\nnon-linearity whose effect may not be adequately predicted by the gradient; for example, attention\nkey and query nodes feeding into the attention softmax non-linearity. To showcase this, we plot the\n5\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 3 | A linear approximation to the attention probability is a particularly poor approximation in\ncases where one or both of the endpoints are in a saturated region of the softmax. Note that when\nvarying only a single key, the softmax becomes a sigmoid of the dot product of that key and the query.\ntrue rank of each node\u2019s effect against its rank assigned by AtP in Figure 4 (left). The plot shows\nthat there are many pronounced false negatives (below the dashed line), especially among keys and\nqueries.\nNormal activation patching for queries and keys involves changing a query or key and then\nre-running the rest of the model, keeping all else the same. AtP takes a linear approximation to the\nentire rest of the model rather than re-running it. We propose explicitly re-computing the first step of\nthe rest of the model, i.e. the attention softmax, and then taking a linear approximation to the rest.\nFormally, for attention key and query nodes, instead of using the gradient on those nodes directly,\nwe take the difference in attention weight caused by that key or query, multiplied by the gradient\non the attention weights themselves. This requires finding the change in attention weights from\neach key and query patch \u2014 but that can be done efficiently using (for all keys and queries in total)\nless compute than two transformer forward passes. This correction avoids the problem of saturated\nattention, while otherwise retaining the performance of AtP.\nQueries\nFor the queries, we can easily compute the adjusted effect by running the model on \ud835\udc65noise\nand caching the noise queries. We then run the model on \ud835\udc65clean and cache the attention keys and\nweights. Finally, we compute the attention weights that result from combining all the keys from the\n\ud835\udc65clean forward pass with the queries from the \ud835\udc65noise forward pass. This costs approximately as much as\nthe unperturbed attention computation of the transformer forward pass. For each query node \ud835\udc5b we\nrefer to the resulting weight vector as attn(\ud835\udc5b)patch, in contrast with the weights attn(\ud835\udc5b)(\ud835\udc65clean) from\nthe clean forward pass. The improved attribution estimate for \ud835\udc5b is then\n6\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n\u02c6I\ud835\udc44\nAtPfix(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise) :=\n\u2211\ufe01\n\ud835\udc58\n\u02c6IAtP(attn(\ud835\udc5b)\ud835\udc58; \ud835\udc65clean, \ud835\udc65noise)\n(6)\n= (attn(\ud835\udc5b)patch \u2212 attn(\ud835\udc5b)(\ud835\udc65clean))\u22ba \ud835\udf15L(M(\ud835\udc65clean))\n\ud835\udf15 attn(\ud835\udc5b)\n\f\f\f\nattn(\ud835\udc5b)=attn(\ud835\udc5b) (\ud835\udc65clean)\n(7)\nKeys\nFor the keys we first describe a simple but inefficient method. We again run the model on \ud835\udc65noise,\ncaching the noise keys. We also run it on \ud835\udc65clean, caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be \ud835\udc5b\ud835\udc58\n1, . . . , \ud835\udc5b\ud835\udc58\n\ud835\udc47 and let queries(\ud835\udc5b\ud835\udc58\n\ud835\udc61 ) = {\ud835\udc5b\ud835\udc5e\n1, . . . , \ud835\udc5b\ud835\udc5e\n\ud835\udc47} be the set of\nquery nodes for the same head as node \ud835\udc5b\ud835\udc58\n\ud835\udc61 . We then define\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e) := attn(\ud835\udc5b\ud835\udc5e)(\ud835\udc65clean | do(\ud835\udc5b\ud835\udc58\n\ud835\udc61 \u2190 \ud835\udc5b\ud835\udc58\n\ud835\udc61 (\ud835\udc65noise)))\n(8)\n\u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e) := attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e) \u2212 attn(\ud835\udc5b\ud835\udc5e)(\ud835\udc65clean)\n(9)\nThe improved attribution estimate for \ud835\udc5b\ud835\udc58\n\ud835\udc61 is then\n\u02c6I\ud835\udc3e\nAtPfix(\ud835\udc5b\ud835\udc58\n\ud835\udc61 ; \ud835\udc65clean, \ud835\udc65noise) :=\n\u2211\ufe01\n\ud835\udc5b\ud835\udc5e\u2208queries(\ud835\udc5b\ud835\udc58\n\ud835\udc61 )\n\u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e)\u22ba \ud835\udf15L(M(\ud835\udc65clean))\n\ud835\udf15 attn(\ud835\udc5b\ud835\udc5e)\n\f\f\f\nattn(\ud835\udc5b\ud835\udc5e)=attn(\ud835\udc5b\ud835\udc5e) (\ud835\udc65clean)\n(10)\nHowever, the procedure we just described is costly to execute as it requires O(\ud835\udc473) flops to naively\ncompute Equation (9) for all \ud835\udc47 keys. In Appendix A.2.1 we describe a more efficient variant that\ntakes no more compute than the forward pass attention computation itself (requiring O(\ud835\udc472) flops).\nSince Equation (6) is also cheaper to compute than a forward pass, the full QK fix requires less than\ntwo transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4 (middle).\nWe observe that the propagation of Q/K effects has a major impact on reducing the false negative\nrate.\n3.1.2. False negatives from cancellation\nThis form of cancellation occurs when the backpropagated gradient from indirect effects is combined\nwith the gradient from the direct effect. We propose a way to modify the backpropagation within the\nattribution patching to reduce this issue. If we artificially zero out the gradient at a downstream layer\nthat contributes to the indirect effect, the cancellation is disrupted. (This is also equivalent to patching\nin clean activations at the outputs of the layer.) Thus we propose to do this iteratively, sweeping\nacross the layers. Any node whose effect does not route through the layer being gradient-zeroed will\nhave its estimate unaffected.\nWe call this method GradDrop. For every layer \u2113 \u2208 {1, . . . , \ud835\udc3f} in the model, GradDrop computes an\nAtP estimate for all nodes, where gradients on the residual contribution from \u2113 are set to 0, including\nthe propagation to earlier layers. This provides a different estimate for all nodes, for each layer that\nwas dropped. We call the so-modified gradient \ud835\udf15L\u2113\n\ud835\udf15\ud835\udc5b = \ud835\udf15L\n\ud835\udf15\ud835\udc5b (M(\ud835\udc65clean | do(\ud835\udc5bout\n\u2113\n\u2190 \ud835\udc5bout\n\u2113\n(\ud835\udc65clean)))) when\ndropping layer \u2113, where \ud835\udc5bout\n\u2113\nis the contribution to the residual stream across all positions. Using \ud835\udf15L\u2113\n\ud835\udf15\ud835\udc5b in\nplace of \ud835\udf15L\u2113\n\ud835\udf15\ud835\udc5b in the AtP formula produces an estimate \u02c6IAtP+GD\u2113 (\ud835\udc5b). Then, the estimates are aggregated\n7\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 4 | Ranks of \ud835\udc50(\ud835\udc5b) against ranks of \u02c6\ud835\udc50AtP(\ud835\udc5b), on Pythia-12B on CITY-PP. Both improvements\nto AtP reduce the number of false negatives (bottom right triangle area), where in this case most\nimprovements come from the QK fix. Coloration indicates the maximum absolute difference in\nattention probability when comparing \ud835\udc65clean and patching a given query or key. Many false negatives\nare keys and queries with significant maximum difference in attention probability, suggesting they\nare due to attention saturation as illustrated in Figure 3. Output and value nodes are colored in grey\nas they do not contribute to the attention probability.\nby averaging their absolute values, and then scaling by\n\ud835\udc3f\n\ud835\udc3f\u22121 to avoid changing the direct-effect path\u2019s\ncontribution (which is otherwise zeroed out when dropping the layer the node is in).\n\u02c6\ud835\udc50AtP+GD(\ud835\udc5b) := \ud835\udd3c\ud835\udc65clean,\ud835\udc65noise\n\"\n1\n\ud835\udc3f \u2212 1\n\ud835\udc3f\n\u2211\ufe01\n\u2113=1\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\f\n#\n(11)\nNote that the forward passes required for computing \u02c6IAtP+GD\u2113 (\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise) don\u2019t depend on \u2113, so\nthe extra compute needed for GradDrop is \ud835\udc3f backwards passes from the same intermediate activations\non a clean forward pass. This is also the case with the QK fix: the corrected attributions \u02c6IAtPfix are\ndot products with the attention weight gradients, so the only thing that needs to be recomputed for\n\u02c6IAtPfix+GD\u2113 (\ud835\udc5b) is the modified gradient\n\ud835\udf15L\u2113\n\ud835\udf15 attn(\ud835\udc5b) . Thus, computing Equation (11) takes \ud835\udc3f backwards\npasses4 on top of the costs for AtP.\nWe show the result of applying GradDrop on attention nodes in Figure 4 (right) and on MLP nodes\nin Figure 5. In Figure 5, we show the true effect magnitude rank against the AtP+GradDrop rank,\nwhile highlighting nodes which improved drastically by applying GradDrop. We give some arguments\nand intuitions on the benefit of GradDrop in Appendix A.2.2.\nDirect Effect Ratio\nTo provide some evidence that the observed false negatives are due to cancella-\ntion, we compute the ratio between the direct effect \ud835\udc50direct(\ud835\udc5b) and the total effect \ud835\udc50(\ud835\udc5b). A higher direct\neffect ratio indicates more cancellation. We observe that the most significant false negatives corrected\nby GradDrop in Figure 5 (highlighted) have high direct effect ratios of 5.35, 12.2, and 0 (no direct\neffect) , while the median direct effect ratio of all nodes is 0 (if counting all nodes) or 0.77 (if only\ncounting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which\n4This can be reduced to (\ud835\udc3f + 1)/2 by reusing intermediate results.\n8\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 5 | True rank and rank of AtP estimates with and without GradDrop, using Pythia-12B on\nthe CITY-PP distribution with NeuronNodes. GradDrop provides a significant improvement to the\nlargest neuron false negatives (red circles) relative to Default AtP (orange crosses).\nin fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions,\nsince all disconnected nodes have a direct effect of 0 by definition.\n3.2. Diagnostics\nDespite the improvements we have proposed in Section 3.1, there is no guarantee that AtP* produces\nno false negatives. Thus, it is desirable to obtain an upper confidence bound on the effect size of\nnodes that might be missed by AtP*, i.e. that aren\u2019t in the top \ud835\udc3e AtP* estimates, for some \ud835\udc3e. Let the\ntop \ud835\udc3e nodes be Top\ud835\udc3e\n\ud835\udc34\ud835\udc61\ud835\udc43\u2217. It so happens that we can use subset sampling to obtain such a bound.\nAs described in Algorithm 1 and Section 3.3, the subset sampling algorithm returns summary\nstatistics: \u00af\ud835\udc56\ud835\udc5b\n\u00b1, \ud835\udc60\ud835\udc5b\n\u00b1 and count\ud835\udc5b\n\u00b1 for each node \ud835\udc5b: the average effect size \u00af\ud835\udc56\ud835\udc5b\n\u00b1 of a subset conditional on the\nnode being contained in that subset (+) or not (\u2212), the sample standard deviations \ud835\udc60\ud835\udc5b\n\u00b1, and the sample\nsizes count\ud835\udc5b\n\u00b1. Given these, consider a null hypothesis5 \ud835\udc3b\ud835\udc5b\n0 that |I(\ud835\udc5b)| \u2265 \ud835\udf03, for some threshold \ud835\udf03, versus\nthe alternative hypothesis \ud835\udc3b\ud835\udc5b\n1 that |I(\ud835\udc5b)| < \ud835\udf03. We use a one-sided Welch\u2019s t-test6 to test this hypothesis;\nthe general practice with a compound null hypothesis is to select the simple sub-hypothesis that gives\nthe greatest \ud835\udc5d-value, so to be conservative, the simple null hypothesis is that I(\ud835\udc5b) = \ud835\udf03 sign(\u00af\ud835\udc56\ud835\udc5b\n+ \u2212 \u00af\ud835\udc56\ud835\udc5b\n\u2212),\ngiving a test statistic of \ud835\udc61\ud835\udc5b = (\ud835\udf03 \u2212 |\u00af\ud835\udc56\ud835\udc5b\n+ \u2212 \u00af\ud835\udc56\ud835\udc5b\n\u2212|)/\ud835\udc60\ud835\udc5b\nWelch, which gives a \ud835\udc5d-value of \ud835\udc5d\ud835\udc5b = \u2119\ud835\udc47\u223c\ud835\udc61\ud835\udf08\ud835\udc5b\nWelch (\ud835\udc47 > \ud835\udc61\ud835\udc5b).\nTo get a combined conclusion across all nodes in \ud835\udc41 \\ Top\ud835\udc3e\n\ud835\udc34\ud835\udc61\ud835\udc43\u2217, let\u2019s consider the hypothesis\n\ud835\udc3b0 = \u00d4\n\ud835\udc5b\u2208\ud835\udc41\\Top\ud835\udc3e\n\ud835\udc34\ud835\udc61\ud835\udc43\u2217 \ud835\udc3b\ud835\udc5b\n0 that any of those nodes has true effect |I(\ud835\udc5b)| > \ud835\udf03. Since this is also a compound\nnull hypothesis, max\ud835\udc5b \ud835\udc5d\ud835\udc5b is the corresponding \ud835\udc5d-value. Then, to find an upper confidence bound with\n5This is an unconventional form of \ud835\udc3b0 \u2013 typically a null hypothesis will say that an effect is insignificant. However, the\nframework of statistical hypothesis testing is based on determining whether the data let us reject the null hypothesis, and\nin this case the hypothesis we want to reject is the presence, rather than the absence, of a significant false negative.\n6This relies on the populations being approximately unbiased and normally distributed, and not skewed. This tended to\nbe true on inspection, and it\u2019s what the additivity assumption (see Section 3.3) predicts for a single prompt pair \u2014 but a\nnonparametric bootstrap test may be more reliable, at the cost of additional compute.\n9\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) IOI-PP\n(b) IOI\nFigure 6 | Upper confidence bounds on effect magnitudes of false negatives (i.e. nodes not in the top\n1024 nodes according to AtP\u2217), at 3 confidence levels, varying the sampling budget. On the left we\nshow in red the true effect of the nodes which are ranked highest by AtP\u2217. We also show the true\neffect magnitude at various ranks of the remaining nodes in orange.\nWe can see that the bound for (a) finds the true biggest false negative reasonably early, while for (b),\nwhere there is no large false negative, we progressively keep gaining confidence with more data.\nNote that the costs involved per prompt pair are substantially different between the subplots, and in\nparticular this diagnostic for the distributional case (b) is substantially cheaper to compute than the\nverification cost of 1024 samples per prompt pair.\nspecified confidence level 1 \u2212 \ud835\udc5d, we invert this procedure to find the lowest \ud835\udf03 for which we still have at\nleast that level of confidence. We repeat this for various settings of the sample size \ud835\udc5a in Algorithm 1.\nThe exact algorithm is described in Appendix A.3.\nIn Figure 6, we report the upper confidence bounds at confidence levels 90%, 99%, 99.9% from\nrunning Algorithm 1 with a given \ud835\udc5a (right subplots), as well as the number of nodes that have a true\ncontribution \ud835\udc50(\ud835\udc5b) greater than \ud835\udf03 (left subplots).\n3.3. Baselines\nIterative\nThe most straightforward method is to directly do Activation Patching to find the true\neffect \ud835\udc50(\ud835\udc5b) of each node, in some uninformed random order. This is necessarily inefficient.\nHowever, if we are scaling to a distribution, it is possible to improve on this, by alternating between\nphases of (i) for each unverified node, picking a not-yet-measured prompt pair on which to patch\nit, (ii) ranking the not-yet-verified nodes by the average observed patch effect magnitudes, taking\nthe top |\ud835\udc41|/|D| nodes, and verifying them. This balances the computational expenditure on the two\ntasks, and allows us to find large nodes sooner, at least as long as their large effect shows up on many\nprompt pairs.\nOur remaining baseline methods rely on an approximate node additivity assumption: that when\nintervening on a set of nodes \ud835\udf02, the measured effect I(\ud835\udf02; \ud835\udc65clean, \ud835\udc65noise) is approximately equal to\n\u00cd\n\ud835\udc5b\u2208\ud835\udf02 I(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise).\nSubsampling\nUnder the approximate node additivity assumption, we can construct an approxi-\nmately unbiased estimator of \ud835\udc50(\ud835\udc5b). We select the sets \ud835\udf02\ud835\udc58 to contain each node independently with\n10\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nsome probability \ud835\udc5d, and additionally sample prompt pairs \ud835\udc65clean\n\ud835\udc58\n, \ud835\udc65noise\n\ud835\udc58\n\u223c D. For any node \ud835\udc5b, and sets\nof nodes \ud835\udf02\ud835\udc58 \u2282 \ud835\udc41, let \ud835\udf02+(\ud835\udc5b) be the collection of all those that contain \ud835\udc5b, and \ud835\udf02\u2212(\ud835\udc5b) be the collection\nof those that don\u2019t contain \ud835\udc5b; we\u2019ll write these node sets as \ud835\udf02+\n\ud835\udc58 (\ud835\udc5b) and \ud835\udf02\u2212\n\ud835\udc58 (\ud835\udc5b), and the corresponding\nprompt pairs as \ud835\udc65clean\n\ud835\udc58\n+(\ud835\udc5b), \ud835\udc65noise\n\ud835\udc58\n+(\ud835\udc5b) and \ud835\udc65clean\n\ud835\udc58\n\u2212(\ud835\udc5b), \ud835\udc65noise\n\ud835\udc58\n\u2212(\ud835\udc5b). The subsampling (or subset sampling)\nestimator is then given by\n\u02c6ISS(\ud835\udc5b) :=\n1\n|\ud835\udf02+(\ud835\udc5b)|\n|\ud835\udf02+(\ud835\udc5b)|\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02+\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65clean\n\ud835\udc58\n+(\ud835\udc5b), \ud835\udc65noise\n\ud835\udc58\n+(\ud835\udc5b)) \u2212\n1\n|\ud835\udf02\u2212(\ud835\udc5b)|\n|\ud835\udf02\u2212 (\ud835\udc5b) |\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02\u2212\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65clean\n\ud835\udc58\n\u2212(\ud835\udc5b), \ud835\udc65noise\n\ud835\udc58\n\u2212(\ud835\udc5b))\n(12)\n\u02c6\ud835\udc50SS(\ud835\udc5b) := | \u02c6ISS(\ud835\udc5b)|\n(13)\nThe estimator \u02c6ISS(\ud835\udc5b) is unbiased if there are no interaction effects, and has a small bias proportional\nto \ud835\udc5d under a simple interaction model (see Appendix A.1.1 for proof).\nIn practice, we compute all the estimates \u02c6\ud835\udc50SS(\ud835\udc5b) by sampling a binary mask over all nodes from\ni.i.d. Bernoulli|\ud835\udc41|(\ud835\udc5d) \u2013 each binary mask can be identified with a node set \ud835\udf02. In Algorithm 1, we\ndescribe how to compute summary statistics related to Equation (13) efficiently for all nodes \ud835\udc5b \u2208 \ud835\udc41.\nThe means \u00af\ud835\udc56\u00b1 are enough to compute \u02c6\ud835\udc50SS(\ud835\udc5b), while other summary statistics are involved in bounding\nthe magnitude of a false negative (cf. Section 3.2). (Note, count\u00b1\n\ud835\udc5b is just an alternate notation for\n|\ud835\udf02\u00b1(\ud835\udc5b)|.)\nAlgorithm 1 Subsampling\nRequire: \ud835\udc5d \u2208 (0, 1), model M, metric L, prompt pair distribution D, num samples \ud835\udc5a\n1: count\u00b1, runSum\u00b1, runSquaredSum\u00b1 \u2190 0|\ud835\udc41|\n\u22b2 Init counts and running sums to 0 vectors\n2: for \ud835\udc56 \u2190 1 to \ud835\udc5a do\n3:\n\ud835\udc65clean, \ud835\udc65noise \u223c D\n4:\nmask+ \u2190 Bernoulli|\ud835\udc41|(\ud835\udc5d)\n\u22b2 Sample binary mask for patching\n5:\nmask\u2212 \u2190 1 \u2212 mask+\n6:\n\ud835\udc56 \u2190 I({\ud835\udc5b \u2208 \ud835\udc41 : mask+\n\ud835\udc5b = 1}; \ud835\udc65clean, \ud835\udc65noise)\n\u22b2 \ud835\udf02+ = {\ud835\udc5b \u2208 \ud835\udc41 : mask+\n\ud835\udc5b = 1}\n7:\ncount\u00b1 \u2190 count\u00b1 + mask\u00b1\n8:\nrunSum\u00b1 \u2190 runSum\u00b1 + \ud835\udc56 \u00b7 mask\u00b1\n9:\nrunSquaredSum\u00b1 \u2190 runSquaredSum\u00b1 + \ud835\udc562 \u00b7 mask\u00b1\n10: \u00af\ud835\udc56\u00b1 \u2190 runSum\u00b1/count\u00b1\n11: \ud835\udc60\u00b1 \u2190\n\u221a\ufe01\n(runSquaredSum\u00b1 \u2212 (\u00af\ud835\udc56\u00b1)2)/(count\u00b1 \u2212 1)\n12: return count\u00b1, \u00af\ud835\udc56\u00b1, \ud835\udc60\u00b1\n\u22b2 If diagnostics are not required, \u00af\ud835\udc56\u00b1 is sufficient.\nBlocks & Hierarchical\nInstead of sampling each \ud835\udf02 independently, we can group nodes into fixed\n\u201cblocks\u201d \ud835\udf02 of some size, and patch each block to find its aggregated contribution \ud835\udc50(\ud835\udf02); we can then\ntraverse the nodes, starting with high-contribution blocks and proceeding from there.\nThere is a tradeoff in terms of the block size: using large blocks increases the compute required to\ntraverse a high-contribution block, but using small blocks increases the compute required to finish\ntraversing all of the blocks. We refer to the fixed block size setting as Blocks. Another way to handle\nthis tradeoff is to add recursion: the blocks can be grouped into higher-level blocks, and so forth. We\ncall this method Hierarchical.\nWe present results from both methods in our comparison plots, but relegate details to Ap-\npendix A.1.2. Relative to subsampling, these grouping-based methods have the disadvantage that on\n11\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\ndistributions, their cost scales linearly with size of D\u2019s support, in addition to scaling with the number\nof nodes7.\n4. Experiments\n4.1. Setup\nNodes\nWhen attributing model behavior to components, an important choice is the partition of the\nmodel\u2019s computational graph into units of analysis or \u2018nodes\u2019 \ud835\udc41 \u220b \ud835\udc5b (cf. Section 2.1). We investigate\ntwo settings for the choice of \ud835\udc41, AttentionNodes and NeuronNodes. For NeuronNodes, each MLP\nneuron8 is a separate node. For AttentionNodes, we consider the query, key, and value vector for\neach head as distinct nodes, as well as the pre-linear per-head attention output9. We also refer to\nthese units as \u2018sites\u2019. For each site, we consider each copy of that site at different token positions as\na separate node. As a result, we can identify each node \ud835\udc5b \u2208 \ud835\udc41 with a pair (\ud835\udc47, \ud835\udc46) from the product\nTokenPosition \u00d7 Site. Since our two settings for \ud835\udc41 are using a different level of granularity and are\nexpected to have different per-node effect magnitudes, we present results on them separately.\nModels\nWe investigate transformer language models from the Pythia suite (Biderman et al., 2023)\nof sizes between 410M and 12B parameters. This allows us to demonstrate that our methods are\napplicable across scale. Our cost-of-verified-recall plots in Figures 1, 7 and 8 refer to Pythia-12B.\nResults for other model sizes are presented via the relative-cost (cf. Section 4.2) plots in the main\nbody Figure 9 and disaggregated via cost-of-verified recall in Appendix B.3.\nEffect Metric L\nAll reported results use the negative log probability10 as their loss function L.\nWe compute L relative to targets from the clean prompt \ud835\udc65clean. We briefly explore other metrics in\nAppendix B.4.\n4.2. Measuring Effectiveness and Efficiency\nCost of verified recall\nAs mentioned in the introduction, we\u2019re primarily interested in finding the\nlargest-effect nodes \u2013 see Appendix D for the distribution of \ud835\udc50(\ud835\udc5b) across models and distributions.\nOnce we have obtained node estimates via a given method, it is relatively cheap to directly measure\ntrue effects of top nodes one at a time; we refer to this as \u201cverification\u201d. Incorporating this into our\nmethodology, we find that false positives are typically not a big issue; they are simply revealed during\nverification. In contrast, false negatives are not so easy to remedy without verifying all nodes, which\nis what we were trying to avoid.\nWe compare methods on the basis of total compute cost (in # of forward passes) to verify the \ud835\udc3e\nnodes with biggest true effect magnitude, for varying \ud835\udc3e. The procedure being measured is to first\ncompute estimates (incurring an estimation cost), and then sweep through nodes in decreasing order\n7AtP* also scales linearly in the same way, but with far fewer forward passes per prompt pair.\n8We use the neuron post-activation for the node; this makes no difference when causally intervening, but for AtP it\u2019s\nbeneficial, because it makes the \ud835\udc5b \u21a6\u2192 L(\ud835\udc5b) function more linear.\n9We include the output node because it provides additional information about what function an attention head is serving,\nparticularly in the case where its queries have negligible patch effects relative to its keys and/or values. This may happen as\na result of choosing \ud835\udc65clean, \ud835\udc65noise such that the query does not differ across the prompts.\n10Another popular metric is the difference in logits between the clean and noise target. As opposed to the negative\nlogprob, the logit difference is linear in the final logits and thus might favor AtP. A downside of logit difference is that it is\nsensitive to the noise target, which may not be meaningful if there are multiple plausible completions, such as in IOI.\n12\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nof estimated magnitude, measuring their individual effects \ud835\udc50(\ud835\udc5b) (i.e. verifying them), and incurring a\nverification cost. Then the total cost is the sum of these two costs.\nInverse-rank-weighted geometric mean cost\nSometimes we find it useful to summarize the\nmethod performance with a scalar; this is useful for comparing methods at a glance across different\nsettings (e.g. model sizes, as in Figure 2), or for selecting hyperparameters (cf. Appendix B.5). The\ncost of verified recall of the top \ud835\udc3e nodes is of interest for \ud835\udc3e at varying orders of magnitude. In order\nto avoid the performance metric being dominated by small or large \ud835\udc3e, we assign similar total weight\nto different orders of magnitude: we use a weighted average with weight 1/\ud835\udc3e for the cost of the top\n\ud835\udc3e nodes. Similarly, since the costs themselves may have different orders of magnitude, we average\nthem on a log scale \u2013 i.e., we take a geometric mean.\nThis metric is also proportional to the area under the curve in plots like Figure 1. To produce a\nmore understandable result, we always report it relative to (i.e. divided by) the oracle verification\ncost on the same metric; the diagonal line is the oracle, with relative cost 1. We refer to this as the\nIRWRGM (inverse-rank-weighted relative geometric mean) cost, or the relative cost.\nNote that the preference of the individual practitioner may be different such that this metric is no\nlonger accurately measuring the important rank regime. For example, AtP* pays a notable upfront\ncost relative to AtP or AtP+QKfix, which sets it at a disadvantage when it doesn\u2019t manage to find\nadditional false negatives; but this may or may not be practically significant. To understand the\nperformance in more detail we advise to refer to the cost of verified recall plots, like Figure 1 (or\nmany more in Appendix B.3).\n4.3. Single Prompt Pairs versus Distributions\nWe focus many of our experiments on single prompt pairs. This is primarily because it\u2019s easier to set\nup and get ground truth data. It\u2019s also a simpler setting in which to investigate the question, and one\nthat\u2019s more universally applicable, since a distribution to generalize to is not always available.\nClean single prompt pairs\nAs a starting point we report results on single prompt pairs which we\nexpect to have relatively clean circuitry11. All singular prompt pairs are shown in Table 1. IOI-PP is\nchosen to resemble an instance from the indirect object identification (IOI) task (Wang et al., 2022),\na task predominantly involving attention heads. CITY-PP is chosen to elicit factual recall which\nprevious research suggests involves early MLPs and a small number of late attention heads (Geva\net al., 2023; Meng et al., 2023; Nanda et al., 2023). The country/city combinations were chosen such\nthat Pythia-410M achieved low loss on both \ud835\udc65clean and \ud835\udc65noise and such that all places were represented\nby a single token.\nWe show the cost of verified 100% recall for various methods in Figure 1, where we focus on\nNeuronNodes for CITY-PP and AttentionNodes for IOI-PP. Exhaustive results for smaller Pythia\nmodels are shown in Appendix B.3. Figure 2 shows the aggregated relative costs for all models on\nCITY-PP and IOI-PP.\nInstead of applying the strict criterion of recalling all important nodes, we can also relax this\nconstraint. In Figure 7, we show the cost of verified 90% recall in the two clean prompt pair settings.\n11Formally, these represent prompt distributions via the delta distribution \ud835\udc5d(\ud835\udc65clean, \ud835\udc65noise) = \ud835\udeff\ud835\udc65clean\n1\n,\ud835\udc65noise\n1\n(\ud835\udc65clean, \ud835\udc65noise)\nwhere \ud835\udc65clean\n1\n, \ud835\udc65noise\n1\nis the singular prompt pair.\n13\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) NeuronNodes on CITY-PP\n(b) AttentionNodes on IOI-PP\nFigure 7 | Costs of finding the most causally-important nodes in Pythia-12B using different methods\non clean prompt pairs, with 90% target recall. This highlights that the AtP* false negatives in Figure 1\nare a small minority of nodes.\nRandom prompt pair\nThe previous prompt pairs may in fact be the best-case scenarios: the\ninterventions they create will be fairly localized to a specific circuit, and this may make it easy for AtP\nto approximate the contributions. It may thus be informative to see how the methods generalize to\nsettings where the interventions are less surgical. To do this, we also report results in Figure 8 (top)\nand Figure 9 on a random prompt pair chosen from a non-copyright-protected section of The Pile (Gao\net al., 2020) which we refer to as RAND-PP. The prompt pair was chosen such that Pythia-410M still\nachieved low loss on both prompts.\nWe find that AtP/AtP* is only somewhat less effective here; this provides tentative evidence that\nthe strong performance of AtP/AtP* isn\u2019t reliant on the clean prompt using a particularly crisp circuit,\nor on the noise prompt being a precise control.\nDistributions\nCausal attribution is often of most interest when evaluated across a distribution, as laid\nout in Section 2. Of the methods, AtP, AtP*, and Subsampling scale reasonably to distributions; the\nformer 2 because they\u2019re inexpensive so running them |D| times is not prohibitive, and Subsampling\nbecause it intrinsically averages across the distribution and thus becomes proportionally cheaper\nrelative to the verification via activation patching. In addition, having a distribution enables a more\nperformant Iterative method, as described in Section 3.3.\nWe present a comparison of these methods on 2 distributional settings. The first is a reduced\nversion of IOI (Wang et al., 2022) on 6 names, resulting in 6 \u00d7 5 \u00d7 4 = 120 prompt pairs, where we\nevaluate AttentionNodes. The other distribution prompts the model to output an indefinite article\n\u2018 a\u2019 or \u2018 an\u2019, where we evaluate NeuronNodes. See Appendix B.1 for details on constructing these\ndistributions. Results are shown in Figure 8 for Pythia 12B, and in Figure 9 across models. The results\nshow that AtP continues to perform well, especially with the QK fix; in addition, the cancellation\nfailure mode tends to be sensitive to the particular input prompt pair, and as a result, averaging across\n14\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) RAND-PP MLP neurons.\n(b) RAND-PP Attention nodes.\n(c) A-AN MLP neurons.\n(d) IOI Attention nodes.\nFigure 8 | Costs of finding the most causally-important nodes in Pythia-12B using different methods,\non a random prompt pair (see Table 1) and on distributions. The shading indicates geometric standard\ndeviation. Cost is measured in forward passes, or forward passes per prompt pair in the distributional\ncase.\n15\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) RAND-PP MLP neurons.\n(b) RAND-PP Attention nodes.\n(c) A-AN MLP neurons.\n(d) IOI Attention nodes.\nFigure 9 | Costs of methods across models, on random prompt pair and on distributions. The costs are\nrelative to having an oracle (and thus verifying nodes in decreasing order of true contribution size);\nthey\u2019re aggregated using an inverse-rank-weighted geometric mean. This means they correspond to\nthe area above the diagonal for each curve in Figure 8.\n16\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nIdentifier\nClean Prompt\nNoise Source Prompt\nCITY-PP\nBOS City : \u2423Barcelona \\n\nCountry : \u2423Spain\nBOS City : \u2423Beijing \\n\nCountry : \u2423China\nIOI-PP\nBOS When \u2423Michael \u2423and \u2423Jessica\n\u2423went \u2423to \u2423the \u2423bar , \u2423Michael\n\u2423gave \u2423a \u2423drink \u2423to \u2423Jessica\nBOS When \u2423Michael \u2423and \u2423Jessica\n\u2423went \u2423to \u2423the \u2423bar , \u2423Ashley\n\u2423gave \u2423a \u2423drink \u2423to \u2423Michael\nRAND-PP\nBOS Her \u2423biggest \u2423worry \u2423was \u2423the\n\u2423festival \u2423might \u2423suffer \u2423and\n\u2423people \u2423might \u2423erroneously \u2423think\nBOS also \u2423think \u2423that \u2423there\n\u2423should \u2423be \u2423the \u2423same \u2423rules\n\u2423or \u2423regulations \u2423when \u2423it\nTable 1 | Clean and noise source prompts for singular prompt pair distributions. Vertical lines denote\ntokenization boundaries. All prompts are preceded by the BOS (beginning of sequence) token. The\nlast token is not part of the input. The last token of the clean prompt is used as the target in L.\na distribution diminishes the benefit of GradDrops.\nAn implication of Subsampling scaling well to this setting is that diagnostics may give reasonable\nconfidence in not missing false negatives with much less overhead than in the single-prompt-pair\ncase; this is illustrated in Figure 6.\n5. Discussion\n5.1. Limitations\nPrompt pair distributions\nWe only considered a small set of prompt pair distributions, which often\nwere limited to a single prompt pair, since evaluating the ground truth can be quite costly. While we\naimed to evaluate on distributions that are reasonably representative, our results may not generalize\nto other distributions.\nChoice of Nodes \ud835\udc41\nIn the NeuronNodes setting, we took MLP neurons as our fundamental unit of\nanalysis. However, there is mounting evidence (Bricken et al., 2023) that the decomposition of signals\ninto neuron contributions does not correspond directly to a semantically meaningful decomposition.\nInstead, achieving such a decomposition seems to require finding the right set of directions in neuron\nactivation space (Bricken et al., 2023; Gurnee et al., 2023) \u2013 which we viewed as being out of scope\nfor this paper. In Section 5.2 we further discuss the applicability of AtP to sparse autoencoders, a\nmethod of finding these decompositions.\nMore generally, we only considered relatively fine-grained nodes, because this is a case where\nvery exhaustive verification is prohibitively expensive, justifying the need for an approximate, fast\nmethod. Nanda (2022) speculate that AtP may perform worse on coarser components like full layers\nor entire residual streams, as a larger change may have more of a non-linear effect. There may still be\nbenefit in speeding up such an analysis, particularly if the context length is long \u2013 our alternative\nmethods may have something to offer here, though we leave investigation of this to future work.\nIt is popular in the literature to do Activation Patching with these larger components, with short\n17\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\ncontexts \u2013 this doesn\u2019t pose a performance issue, and so our work would not provide any benefit here.\nCaveats of \ud835\udc50(\ud835\udc5b) as importance measure\nIn this work we took the ground truth of activation\npatching, as defined in Equation (1), as our evaluation target. As discussed by McGrath et al. (2023),\nEquation (1) often significantly disagrees with a different evaluation target, the \u201cdirect effect\u201d, by\nputting lower weight on some contributions when later components would shift their behaviour to\ncompensate for the earlier patched component. In the worst case this could be seen as producing\nadditional false negatives not accounted for by our metrics. To some degree this is likely to be\nmitigated by the GradDrop formula in Eq. (11), which will include a term dropping out the effect of\nthat downstream shift.\nHowever, it is also questionable whether we need to concern ourselves with finding high-direct-\neffect nodes. For example, direct effect is easy to efficiently compute for all nodes, as explored by\nnostalgebraist (2020) \u2013 so there is no need for fast approximations like AtP if direct effect is the\nquantity of interest. This ease of computation is no free lunch, though, because direct effect is also\nmore limited as a tool for finding causally important nodes: it would not be able to locate any nodes\nthat contribute only instrumentally to the circuit rather than producing its output. For example, there\nis no direct effect from nodes at non-final token positions. We discuss the direct effect further in\nSection 3.1.2 and Appendix A.2.2.\nAnother nuance of our ground\u2013truth definition occurs in the distributional setting. Some nodes\nmay have a real and significant effect, but only on a single clean prompt (e.g. they only respond\nto a particular name in IOI12 or object in A-AN). Since the effect is averaged over the distribution,\nthe ground truth will not assign these nodes large causal importance. Depending on the goal of the\npractitioner this may or may not be desirable.\nEffect size versus rank estimation\nWhen evaluating the performance of various estimators, we\nfocused on evaluating the relative rank of estimates, since our main goal was to identify important\ncomponents (with effect size only instrumentally useful to this end), and we assumed a further\nverification step of the nodes with highest estimated effects one at a time, in contexts where knowing\neffect size is important. Thus, we do not present evidence about how closely the estimated effect\nmagnitudes from AtP or AtP* match the ground truth. Similarly, we did not assess the prevalence of\nfalse positives in our analysis, because they can be filtered out via the verification process. Finally, we\ndid not compare to past manual interpretability work to check whether our methods find the same\nnodes to be causally important as discovered by human researchers, as done in prior work (Conmy\net al., 2023; Syed et al., 2023).\nOther LLMs\nWhile we think it likely that our results on the Pythia model family (Biderman et al.,\n2023) will transfer to other LLM families, we cannot rule out qualitatively different behavior without\nfurther evidence, especially on SotA\u2013scale models or models that significantly deviate from the\nstandard decoder-only transformer architecture.\n5.2. Extensions/Variants\nEdge Patching\nWhile we focus on computing the effects of individual nodes, edge activation patching\ncan give more fine-grained information about which paths in the computational graph matter. However,\nit suffers from an even larger blowup in number of forward passes if done naively. Fortunately, AtP is\n12We did observe this particular behavior in a few instances.\n18\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\neasy to generalize to estimating the effects of edges between nodes (Nanda, 2022; Syed et al., 2023),\nwhile AtP* may provide further improvement. We discuss edge-AtP, and how to efficiently carry over\nthe insights from AtP*, in Appendix C.2.\nCoarser nodes \ud835\udc41\nWe focused on fine-grained attribution, rather than full layers or sliding win-\ndows (Geva et al., 2023; Meng et al., 2023). In the latter case there\u2019s less computational blowup to\nresolve, but for long contexts there may still be benefit in considering speedups like ours; on the other\nhand, they may be less linear, thus favouring other methods over AtP*. We leave investigation of this\nto future work.\nLayer normalization\nNanda (2022) observed that AtP\u2019s approximation to layer normalization may\nbe a worse approximation when it comes to patching larger/coarser nodes: on average the patched\nand clean activations are likely to have similar norm, but may not have high cosine-similarity. They\nrecommend treating the denominator in layer normalization as fixed, e.g. using a stop-gradient\noperator in the implementation. In Appendix C.1 we explore the effect of this, and illustrate the\nbehaviour of this alternative form of AtP. It seems likely that this variant would indeed produce better\nresults particularly when patching residual-stream nodes \u2013 but we leave empirical investigation of\nthis to future work.\nDenoising\nDenoising (Lieberum et al., 2023; Meng et al., 2023) is a different use case for patching,\nwhich may produce moderately different results: the difference is that each forward pass is run on\n\ud835\udc65noise with the activation to patch taken from \ud835\udc65clean \u2014 colloquially, this tests whether the patched\nactivation is sufficient to recover model performance on \ud835\udc65clean, rather than necessary. We provide\nsome preliminary evidence to the effect of this choice in Appendix B.4 but leave a more thorough\ninvestigation to future work.\nOther forms of ablation\nFurther, in some settings it may be of interest to do mean-ablation, or\neven zero-ablation, and our tweaks remain applicable there; the random-prompt-pair result suggests\nAtP* isn\u2019t overly sensitive to the noise distribution, so we speculate the results are likely to carry over.\n5.3. Applications\nAutomated Circuit Finding\nA natural application of the methods we discussed in this work is the\nautomatic identification and localization of sparse subgraphs or \u2018circuits\u2019 (Cammarata et al., 2020). A\nvariant of this was already discussed in concurrent work by Syed et al. (2023) who combined edge\nattribution patching with the ACDC algorithm (Conmy et al., 2023). As we mentioned in the edge\npatching discussion, AtP* can be generalized to edge attribution patching, which may bring additional\nbenefit for automated circuit discovery.\nAnother approach is to learn a (probabilistic) mask over nodes, similar to Cao et al. (2021); Louizos\net al. (2018), where the probability scales with the currently estimated node contribution \ud835\udc50(\ud835\udc5b). For\nthat approach, a fast method to estimate all node effects given the current mask probabilities could\nprove vital.\nSparse Autoencoders\nRecently there has been increased interest by the community in using sparse\nautoencoders (SAEs) to construct disentangled sparse representations with potentially more semantic\ncoherence than transformer-native units such as neurons (Bricken et al., 2023; Cunningham et al.,\n19\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n2023). SAEs usually have a lot more nodes than the corresponding transformer block they are applied\nto. This could pose a larger problem in terms of the activation patching effects, making the speedup\nof AtP* more valuable. However, due to the sparseness of the SAE, on a given forward pass the\neffect of most features will be zero. For example, some successful SAEs by Bricken et al. (2023)\nhave 10-20 active features for 500 neurons for a given token position, which reduces the number of\nnodes by 20-50x relative to the MLP setting, increasing the scale at which existing iterative methods\nremain practical. It is still an open research question, however, what degree of sparsity is feasible\nwith tolerable reconstruction error for practically relevant or SOTA\u2013scale models, where the methods\ndiscussed in this work may become more important again.\nSteering LLMs\nAtP* could be used to discover single nodes in the model that can be leveraged for\ntargeted inference time interventions to control the model\u2019s behavior. In contrast to previous work (Li\net al., 2023; Turner et al., 2023; Zou et al., 2023) it might provide more localized interventions with\nless impact on the rest of the model\u2019s computation. One potential exciting direction would be to use\nAtP* (or other gradient-based approximations) to see which sparse autoencoder features, if activated,\nwould have a significant effect.\n5.4. Recommendation\nOur results suggest that if a practitioner is trying to do fast causal attribution, there are 2 main factors\nto consider: (i) the desired granularity of localization, and (ii) the confidence vs compute tradeoff.\nRegarding (i), the desired granularity, smaller components (e.g. MLP neurons or attention heads)\nare more numerous but more linear, likely yielding better results from gradient-based methods like\nAtP. We are less sure AtP will be a good approximation if patching layers or sliding windows of layers,\nand in this case practitioners may want to do normal patching. If the number of forward passes\nrequired remains prohibitive (e.g. a long context times many layers, when doing per token \u00d7 layer\npatching), our other baselines may be useful. For a single prompt pair we particularly recommend\ntrying Blocks, as it\u2019s easy to make sense of; for a distribution we recommend Subsampling because it\nscales better to many prompt pairs.\nRegarding (ii), the confidence vs compute tradeoff, depending on the application, it may be\ndesirable to run AtP as an activation patching prefilter followed by running the diagnostic to increase\nconfidence. On the other hand, if false negatives aren\u2019t a big concern then it may be preferable to\nskip the diagnostic \u2013 and if false positives aren\u2019t either, then in certain cases practitioners may want\nto skip activation patching verification entirely. In addition, if the prompt pair distribution does not\nadequately highlight the specific circuit/behaviour of interest, this may also limit what can be learned\nfrom any localization methods.\nIf AtP is appropriate, our results suggest the best variant to use is probably AtP* for single prompt\npairs, AtP+QKFix for AttentionNodes on distributions, and AtP for NeuronNodes (or other sites that\naren\u2019t immediately before a nonlinearity) on distributions.\nOf course, these recommendations are best-substantiated in settings similar to those we studied:\nfocused prompt pairs / distribution, attention node or neuron sites, nodewise attribution, measuring\ncross-entropy loss on the clean-prompt next token. If departing from these assumptions we recommend\nlooking before you leap.\n20\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n6. Related work\nLocalization and Mediation Analysis\nThis work is concerned with identifying the effect of all\n(important) nodes in a causal graph (Pearl, 2000), in the specific case where the graph represents a\nlanguage model\u2019s computation. A key method for finding important intermediate nodes in a causal\ngraph is intervening on those nodes and observing the effect, which was first discussed under the\nname of causal mediation analysis by Pearl (2001); Robins and Greenland (1992).\nActivation Patching\nIn recent years there has been increasing success at applying the ideas of\ncausal mediation analysis to identify causally important nodes in deep neural networks, in particular\nvia the method of activation patching, where the output of a model component is intervened on.\nThis technique has been widely used by the community and successfully applied in a range of\ncontexts (Conmy et al., 2023; Cunningham et al., 2023; Feng and Steinhardt, 2023; Finlayson et al.,\n2021; Geva et al., 2023; Goldowsky-Dill et al., 2023; Hanna et al., 2023; Hase et al., 2023; Hendel\net al., 2023; Huang et al., 2023; Lieberum et al., 2023; McDougall et al., 2023; Meng et al., 2023;\nMerullo et al., 2023; Nanda et al., 2023; Olsson et al., 2022; Soulos et al., 2020; Stolfo et al., 2023;\nTigges et al., 2023; Todd et al., 2023; Vig et al., 2020; Wang et al., 2022).\nChan et al. (2022) introduce causal scrubbing, a generalized algorithm to verify a hypothesis\nabout the internal mechanism underlying a model\u2019s behavior, and detail their motivation behind\nperforming noising and resample ablation rather than denoising or using mean or zero ablation \u2013 they\ninterpret the hypothesis as implying the computation is invariant to some large set of perturbations,\nso their starting-point is the clean unperturbed forward pass.13\nAnother line of research concerning formalizing causal abstractions focuses on finding and verifying\nhigh-level causal abstractions of low-level variables (Geiger et al., 2020, 2021, 2022, 2023). See\nJenner et al. (2022) for more details on how these different frameworks agree and differ. In contrast\nto those works, we are chiefly concerned with identifying the important low-level variables in the\ncomputational graph and are not investigating their semantics or potential groupings of lower-level\ninto higher-level variables.\nIn addition to causal mediation analysis, intervening on node activations in the model forward\npass has also been studied as a way of steering models towards desirable behavior (Belrose et al.,\n2023; Jorgensen et al., 2023; Li et al., 2023; Rimsky et al., 2023; Turner et al., 2023; Zou et al.,\n2023).\nAttribution Patching / Gradient-based Masking\nWhile we use the resample\u2013ablation variant of\nAtP as formulated in Nanda (2022), similar formulations have been used in the past to successfully\nprune deep neural networks (Figurnov et al., 2016; Michel et al., 2019; Molchanov et al., 2017),\nor even identify causally important nodes for interpretability (Cao et al., 2021). Concurrent work\nby Syed et al. (2023) also demonstrates AtP can help with automatically finding causally important\ncircuits in a way that agrees with previous manual circuit identification work. In contrast to Syed\net al. (2023), we provide further analysis of AtP\u2019s failure modes, give improvements in the form of\nAtP\u2217, and evaluate both methods as well as several baselines on a suite of larger models against a\nground truth that is independent of human researchers\u2019 judgement.\n13Our motivation for focusing on noising rather than denoising was a closely related one \u2013 we were motivated by\nautomated circuit discovery, where gradually noising more and more of the model is the basic methodology for both of the\napproaches discussed in Section 5.3.\n21\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n7. Conclusion\nIn this paper, we have explored the use of attribution patching for node patch effect evaluation. We have\ncompared attribution patching with alternatives and augmentations, characterized its failure modes,\nand presented reliability diagnostics. We have also discussed the implications of our contributions\nfor other settings in which patching can be of interest, such as circuit discovery, edge localization,\ncoarse-grained localization, and causal abstraction.\nOur results show that AtP* can be a more reliable and scalable approach to node patch effect\nevaluation than alternatives. However, it is important to be aware of the failure modes of attribution\npatching, such as cancellation and saturation. We explored these in some detail, and provided\nmitigations, as well as recommendations for diagnostics to ensure that the results are reliable.\nWe believe that our work makes an important contribution to the field of mechanistic interpretabil-\nity and will help to advance the development of more reliable and scalable methods for understanding\nthe behavior of deep neural networks.\n8. Author Contributions\nJ\u00e1nos Kram\u00e1r was research lead, and Tom Lieberum was also a core contributor \u2013 both were highly\ninvolved in most aspects of the project. Rohin Shah and Neel Nanda served as advisors and gave\nfeedback and guidance throughout.\nReferences\nN. Belrose, D. Schneider-Joseph, S. Ravfogel, R. Cotterell, E. Raff, and S. Biderman. Leace: Perfect\nlinear concept erasure in closed form. arXiv preprint arXiv:2306.03819, 2023.\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit,\nU. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal.\nPythia: A suite for\nanalyzing large language models across training and scaling. In A. Krause, E. Brunskill, K. Cho,\nB. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning\nResearch, pages 2397\u20132430. PMLR, 2023. URL https://proceedings.mlr.press/v202/\nbiderman23a.html.\nT. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison,\nA. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds,\nA. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah.\nTowards monosemanticity: Decomposing language models with dictionary learning. Transformer\nCircuits Thread, 2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html.\nN. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, and S. K. Lim.\nThread: Circuits. Distill, 2020. doi: 10.23915/distill.00024. https://distill.pub/2020/circuits.\nN. D. Cao, L. Schmid, D. Hupkes, and I. Titov.\nSparse interventions in language models with\ndifferentiable masking, 2021.\nL. Chan, A. Garriga-Alonso, N. Goldwosky-Dill, R. Greenblatt, J. Nitishinskaya, A. Radhakrishnan,\nB. Shlegeris, and N. Thomas. Causal scrubbing, a method for rigorously testing interpretabil-\nity hypotheses.\nAI Alignment Forum, 2022.\nhttps://www.alignmentforum.org/posts/\nJvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing.\n22\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nA. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated\ncircuit discovery for mechanistic interpretability, 2023.\nH. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly\ninterpretable features in language models, 2023.\nJ. Feng and J. Steinhardt. How do language models bind entities in context?, 2023.\nM. Figurnov, A. Ibraimova, D. P. Vetrov, and P. Kohli. Perforatedcnns: Acceleration through elim-\nination of redundant convolutions. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates,\nInc., 2016.\nURL https://proceedings.neurips.cc/paper_files/paper/2016/file/\nf0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf.\nM. Finlayson, A. Mueller, S. Gehrmann, S. Shieber, T. Linzen, and Y. Belinkov. Causal analysis\nof syntactic agreement mechanisms in neural language models. In C. Zong, F. Xia, W. Li, and\nR. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 1828\u20131843, Online, Aug. 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.144. URL https://aclanthology.org/2021.acl-long.144.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\nS. Presser, and C. Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027, 2020.\nA. Geiger, K. Richardson, and C. Potts. Neural natural language inference models partially embed\ntheories of lexical entailment and negation, 2020.\nA. Geiger, H. Lu, T. Icard, and C. Potts. Causal abstractions of neural networks, 2021.\nA. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. D. Goodman, and C. Potts. Inducing causal\nstructure for interpretable neural networks, 2022.\nA. Geiger, C. Potts, and T. Icard. Causal abstraction for faithful model interpretation, 2023.\nM. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in\nauto-regressive language models, 2023.\nN. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora. Localizing model behavior with path patching,\n2023.\nW. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a\nhaystack: Case studies with sparse probing, 2023.\nM. Hanna, O. Liu, and A. Variengien. How does gpt-2 compute greater-than?: Interpreting mathe-\nmatical abilities in a pre-trained language model, 2023.\nP. Hase, M. Bansal, B. Kim, and A. Ghandeharioun. Does localization inform editing? surprising\ndifferences in causality-based localization vs. knowledge editing in language models, 2023.\nR. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors, 2023.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den\nDriessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and\n23\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nL. Sifre.\nAn empirical analysis of compute-optimal large language model training.\nIn\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 30016\u201330030. Curran Associates,\nInc., 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nc1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf.\nJ. Huang, A. Geiger, K. D\u2019Oosterlinck, Z. Wu, and C. Potts. Rigorously assessing natural language\nexplanations of neurons, 2023.\nE.\nJenner,\nA.\nGarriga-Alonso,\nand\nE.\nZverev.\nA\ncomparison\nof\ncausal\nscrub-\nbing,\ncausal\nabstractions,\nand\nrelated\nmethods.\nAI\nAlignment\nForum,\n2022.\nhttps://www.alignmentforum.org/posts/uLMWMeBG3ruoBRhMW/\na-comparison-of-causal-scrubbing-causal-abstractions-and.\nO. Jorgensen, D. Cope, N. Schoots, and M. Shanahan. Improving activation steering in language\nmodels with mean-centring, 2023.\nK. Li, O. Patel, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. Inference-time intervention: Eliciting truthful\nanswers from a language model, 2023.\nT. Lieberum, M. Rahtz, J. Kram\u00e1r, N. Nanda, G. Irving, R. Shah, and V. Mikulik. Does circuit analysis\ninterpretability scale? evidence from multiple choice capabilities in chinchilla, 2023.\nC. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through \ud835\udc590 regularization,\n2018.\nC. McDougall, A. Conmy, C. Rushing, T. McGrath, and N. Nanda. Copy suppression: Comprehensively\nunderstanding an attention head, 2023.\nT. McGrath, M. Rahtz, J. Kram\u00e1r, V. Mikulik, and S. Legg. The hydra effect: Emergent self-repair in\nlanguage model computations, 2023.\nK. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt, 2023.\nJ. Merullo, C. Eickhoff, and E. Pavlick. Circuit component reuse across tasks in transformer language\nmodels, 2023.\nP. Michel, O. Levy, and G. Neubig.\nAre sixteen heads really better than one?\nIn\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems, volume 32. Curran Associates,\nInc., 2019.\nURL https://proceedings.neurips.cc/paper_files/paper/2019/file/\n2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf.\nP. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for\nresource efficient inference. In International Conference on Learning Representations, 2017. URL\nhttps://openreview.net/forum?id=SJGCiw5gl.\nN. Nanda. Attribution patching: Activation patching at industrial scale. 2022. URL https://www.\nneelnanda.io/mechanistic-interpretability/attribution-patching.\nN.\nNanda,\nS.\nRajamanoharan,\nJ.\nKram\u00e1r,\nand\nR.\nShah.\nFact\nfinding:\nAt-\ntempting\nto\nreverse-engineer\nfactual\nrecall\non\nthe\nneuron\nlevel,\nDec\n2023.\nURL\nhttps://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/\nfact-finding-attempting-to-reverse-engineer-factual-recall.\n24\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nnostalgebraist. interpreting gpt: the logit lens. 2020. URL https://www.alignmentforum.org/\nposts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.\nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,\nA. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones,\nJ. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah.\nIn-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-\ncircuits.pub/2022/in-context-learning-and-induction-heads/index.html.\nJ. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000.\nJ. Pearl. Direct and indirect effects, 2001.\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by\ngenerative pre-training, 2018.\nN. Rimsky, N. Gabrieli, J. Schulz, M. Tong, E. Hubinger, and A. M. Turner. Steering llama 2 via\ncontrastive activation addition, 2023.\nJ. M. Robins and S. Greenland.\nIdentifiability and exchangeability for direct and indirect ef-\nfects. Epidemiology, 3:143\u2013155, 1992. URL https://api.semanticscholar.org/CorpusID:\n10757981.\nP. Soulos, R. T. McCoy, T. Linzen, and P. Smolensky.\nDiscovering the compositional structure\nof vector representations with role learning networks.\nIn A. Alishahi, Y. Belinkov, G. Chru-\npa\u0142a, D. Hupkes, Y. Pinter, and H. Sajjad, editors, Proceedings of the Third BlackboxNLP Work-\nshop on Analyzing and Interpreting Neural Networks for NLP, pages 238\u2013254, Online, Nov. 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.blackboxnlp-1.23.\nURL\nhttps://aclanthology.org/2020.blackboxnlp-1.23.\nA. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language\nmodels using causal mediation analysis, 2023.\nA. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery, 2023.\nC. Tigges, O. J. Hollinsworth, A. Geiger, and N. Nanda. Linear representations of sentiment in large\nlanguage models, 2023.\nE. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. Function vectors in large\nlanguage models, 2023.\nA. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition:\nSteering language models without optimization, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need, 2017.\nA. Veit, M. J. Wilber, and S. Belongie.\nResidual networks behave like ensembles of rela-\ntively shallow networks.\nIn D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems, volume 29. Curran Associates,\nInc., 2016.\nURL https://proceedings.neurips.cc/paper_files/paper/2016/file/\n37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf.\n25\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nJ. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender\nbias in language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\npages 12388\u201312401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\npaper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.\nK. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a\ncircuit for indirect object identification in gpt-2 small, 2022.\nB. L. Welch. The generalization of \u2018Student\u2019s\u2019 problem when several different population variances are\ninvolved. Biometrika, 34(1-2):28\u201335, 01 1947. ISSN 0006-3444. doi: 10.1093/biomet/34.1-2.28.\nURL https://doi.org/10.1093/biomet/34.1-2.28.\nA. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski,\nS. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z.\nKolter, and D. Hendrycks. Representation engineering: A top-down approach to ai transparency,\n2023.\n26\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nA. Method details\nA.1. Baselines\nA.1.1. Properties of Subsampling\nHere we prove that the subsampling estimator \u02c6ISS(\ud835\udc5b) from Section 3.3 is unbiased in the case of no\ninteraction effects. Furthermore, assuming a simple interaction model, we show the bias of \u02c6ISS(\ud835\udc5b) is\n\ud835\udc5d times the total interaction effect of \ud835\udc5b with other nodes. We assume a pairwise interaction model.\nThat is, given a set of nodes \ud835\udf02, we have\nI(\ud835\udf02; \ud835\udc65) =\n\u2211\ufe01\n\ud835\udc5b\u2208\ud835\udf02\nI(\ud835\udc5b; \ud835\udc65) +\n\u2211\ufe01\n\ud835\udc5b,\ud835\udc5b\u2032\u2208\ud835\udf02\n\ud835\udc5b\u2260\ud835\udc5b\n\ud835\udf0e\ud835\udc5b,\ud835\udc5b\u2032(\ud835\udc65)\n(14)\nwith fixed constants \ud835\udf0e\ud835\udc5b,\ud835\udc5b\u2032(\ud835\udc65) \u2208 \u211d for each prompt pair \ud835\udc65 \u2208 support(D). Let \ud835\udf0e\ud835\udc5b,\ud835\udc5b\u2032 = \ud835\udd3c\ud835\udc65\u223cD\n\u0002\n\ud835\udf0e\ud835\udc5b,\ud835\udc5b\u2032(\ud835\udc65)\n\u0003\n.\nLet \ud835\udc5d be the probability of including each node in a given \ud835\udf02 and let \ud835\udc40 be the number of node\nmasks sampled from Bernoulli|\ud835\udc41|(\ud835\udc5d) and prompt pairs \ud835\udc65 sampled from D. Then,\n\ud835\udd3c\nh\n\u02c6ISS(\ud835\udc5b)\ni\n= \ud835\udd3c\n\"\n1\n|\ud835\udf02+(\ud835\udc5b)|\n|\ud835\udf02+(\ud835\udc5b)|\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02+\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65+\n\ud835\udc58) \u2212\n1\n|\ud835\udf02\u2212(\ud835\udc5b)|\n|\ud835\udf02\u2212 (\ud835\udc5b) |\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02\u2212\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65\u2212\n\ud835\udc58 )\n#\n(15a)\n= \ud835\udd3c\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\ud835\udd3c\n\"\n1\n|\ud835\udf02+(\ud835\udc5b)|\n|\ud835\udf02+(\ud835\udc5b)|\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02+\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65+\n\ud835\udc58) \u2212\n1\n|\ud835\udf02\u2212(\ud835\udc5b)|\n|\ud835\udf02\u2212 (\ud835\udc5b) |\n\u2211\ufe01\n\ud835\udc58=1\nI(\ud835\udf02\u2212\n\ud835\udc58 (\ud835\udc5b); \ud835\udc65\u2212\n\ud835\udc58 )\n\f\f\f\f\f\f\n|\ud835\udf02+(\ud835\udc5b)|\n#\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(15b)\n= \ud835\udd3c\n\u0014\n\ud835\udd3c\n\u0014 |\ud835\udf02+(\ud835\udc5b)|\n|\ud835\udf02+(\ud835\udc5b)|\ud835\udd3c [I(\ud835\udf021; \ud835\udc651)|\ud835\udc5b \u2208 \ud835\udf021] \u2212 |\ud835\udf02\u2212(\ud835\udc5b)|\n|\ud835\udf02\u2212(\ud835\udc5b)|\ud835\udd3c [I(\ud835\udf021; \ud835\udc651)|\ud835\udc5b \u2209 \ud835\udf021]\n\f\f\f\f|\ud835\udf02+(\ud835\udc5b)|\n\u0015\u0015\n(15c)\n= \ud835\udd3c [I(\ud835\udf021; \ud835\udc651)|\ud835\udc5b \u2208 \ud835\udf021] \u2212 \ud835\udd3c [I(\ud835\udf021; \ud835\udc651)|\ud835\udc5b \u2209 \ud835\udf021]\n(15d)\n= \ud835\udc50(\ud835\udc5b) + \ud835\udd3c\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2211\ufe01\n\ud835\udc5b\u2032\u2260\ud835\udc5b\n\ud835\udfd9[\ud835\udc5b\u2032 \u2208 \ud835\udf021] \u00a9\u00ad\n\u00ab\n\ud835\udc50(\ud835\udc5b\u2032) + \ud835\udf0e\ud835\udc5b\ud835\udc5b\u2032 + 1\n2\n\u2211\ufe01\n\ud835\udc5b\u2032\u2032\u2209{\ud835\udc5b\u2032,\ud835\udc5b}\n\ud835\udfd9[\ud835\udc5b\u2032 \u2208 \ud835\udf021]\ud835\udf0e\ud835\udc5b\u2032\ud835\udc5b\u2032\u2032\n\f\f\f\f\f\f\n\ud835\udc5b \u2208 \ud835\udf021\u00aa\u00ae\n\u00ac\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(15e)\n\u2212 \ud835\udd3c\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2211\ufe01\n\ud835\udc5b\u2032\u2260\ud835\udc5b\n\ud835\udfd9[\ud835\udc5b\u2032 \u2208 \ud835\udf021] \u00a9\u00ad\n\u00ab\n\ud835\udc50(\ud835\udc5b\u2032) + 1\n2\n\u2211\ufe01\n\ud835\udc5b\u2032\u2032\u2209{\ud835\udc5b\u2032,\ud835\udc5b}\n\ud835\udfd9[\ud835\udc5b\u2032 \u2208 \ud835\udf021]\ud835\udf0e\ud835\udc5b\u2032\ud835\udc5b\u2032\u2032\u00aa\u00ae\n\u00ac\n\f\f\f\f\f\f\n\ud835\udc5b \u2209 \ud835\udf021\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(15f)\n= \ud835\udc50(\ud835\udc5b) + \ud835\udc5d\n\u2211\ufe01\n\ud835\udc5b\u2032\u2260\ud835\udc5b\n\ud835\udf0e\ud835\udc5b\ud835\udc5b\u2032\n(15g)\nIn Equation (15g), we observe that if the interaction terms \ud835\udf0e\ud835\udc5b\ud835\udc5b\u2032 are all zero, the estimator is\nunbiased. Otherwise, the bias scales both with the sum of interaction effects and with \ud835\udc5d, as expected.\nA.1.2. Pseudocode for Blocks and Hierarchical baselines\nIn Algorithm 2 we detail the Blocks baseline algorithm. As explained in Section 3.3, it comes with a\ntradeoff in its \u201cblock size\u201d hyperparameter \ud835\udc35: a small block size requires a lot of time to evaluate all\nthe blocks, while a large block size means many irrelevant nodes to evaluate in each high-contribution\nblock.\nThe Hierarchical baseline algorithm aims to resolve this tradeoff, by using small blocks, but\ngrouped into superblocks so it\u2019s not necessary to traverse all the small blocks before finding the key\n27\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nAlgorithm 2 Blocks algorithm for causal attribution.\nRequire: block size \ud835\udc35, compute budget \ud835\udc40, nodes \ud835\udc41 = {\ud835\udc5b\ud835\udc56}, prompts \ud835\udc65clean, \ud835\udc65noise, intervention function\n\u02dcI : \ud835\udf02 \u21a6\u2192 I(\ud835\udf02; \ud835\udc65clean, \ud835\udc65noise)\n1: numBlocks \u2190 \u2308|\ud835\udc41|/\ud835\udc35\u2309\n2: \ud835\udf0b \u2190 shuffle ({\u230anumBlocks \u00b7 \ud835\udc56\ud835\udc35/|\ud835\udc41|\u230b | \ud835\udc56 \u2208 {0, . . . , |\ud835\udc41| \u2212 1}})\n\u22b2 Assign each node to a block.\n3: for \ud835\udc56 \u2190 0 to numBlocks \u2212 1 do\n4:\nblockContribution[\ud835\udc56] \u2190 | \u02dcI(\ud835\udf0b\u22121({\ud835\udc56}))|\n\u22b2 \ud835\udf0b\u22121({\ud835\udc56}) := {\ud835\udc5b : \ud835\udf0b(\ud835\udc5b) = \ud835\udc56 | \ud835\udc5b \u2208 \ud835\udc41})\n5: spentBudget \u2190 \ud835\udc40 \u2212 numBlocks\n6: topNodeContribs \u2190 CreateEmptyDictionary()\n7: for all \ud835\udc56 \u2208 {0 to numBlocks \u2212 1} in decreasing order of blockContribution[\ud835\udc56] do\n8:\nfor all \ud835\udc5b \u2208 \ud835\udf0b\u22121({\ud835\udc56}) do\n\u22b2 Eval all nodes in block.\n9:\nif spentBudget < \ud835\udc40 then\n10:\ntopNodeContribs[\ud835\udc5b] \u2190| \u02dcI({\ud835\udc5b})|\n11:\nspentBudget \u2190 spentBudget + 1\n12:\nelse\n13:\nreturn topNodeContribs\n14: return topNodeContribs\nnodes. In Algorithm 3 we detail the hierarchical algorithm in its iterative form, corresponding to\nbatch size 1.\nOne aspect that might be surprising is that on line 21, we ensure a subblock is never added to the\npriority queue with higher priority than its ancestor superblocks. The reason for doing this is that in\npractice we use batched inference rather than patching a single block at a time, so depending on the\nbatch size, we do evaluate blocks that aren\u2019t the highest-priority unevaluated blocks, and this might\nimpose a significant delay in when some blocks are evaluated. In order to reduce this dependence on\nthe batch size hyperparameter, line 21 ensures that every block is evaluated at most \ud835\udc3f batches later\nthan it would be with batch size 1.\nA.2. AtP improvements\nA.2.1. Pseudocode for corrected AtP on attention keys\nAs described in Section 3.1.1, computing Equation (10) na\u00efvely for all nodes requires O(\ud835\udc473) flops at\neach attention head and prompt pair. Here we give a more efficient algorithm running in O(\ud835\udc472). In\naddition to keys, queries and attention probabilities, we now also cache attention logits (pre-softmax\nscaled key-query dot products).\nWe define attnLogits\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e) and \u0394\ud835\udc61 attnLogits(\ud835\udc5b\ud835\udc5e) analogously to Equations (8) and (9). For\nbrevity we can also define attnLogitspatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n:= attnLogits\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61 and \u0394 attnLogits(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n:=\n\u0394\ud835\udc61 attnLogits(\ud835\udc5b\ud835\udc5e)\ud835\udc61, since the aim with this algorithm is to avoid having to separately compute ef-\nfects of do(\ud835\udc5b\ud835\udc58\n\ud835\udc61 \u2190 \ud835\udc5b\ud835\udc58\n\ud835\udc61 (\ud835\udc65noise)) on any other component of attnLogits than the one for key node \ud835\udc5b\ud835\udc58\n\ud835\udc61 .\nNote that, for a key \ud835\udc5b\ud835\udc58\n\ud835\udc61 at position \ud835\udc61 in the sequence, the proportions of the non-\ud835\udc61 components of\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61 do not change when attnLogits(\ud835\udc5b\ud835\udc5e)\ud835\udc61 is changed, so \u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e) is actually onehot(\ud835\udc61) \u2212attn(\ud835\udc5b\ud835\udc5e)\nmultiplied by some scalar \ud835\udc60\ud835\udc61; specifically, to get the right attention weight on \ud835\udc5b\ud835\udc58\n\ud835\udc61 , the scalar must be\n\ud835\udc60\ud835\udc61 :=\n\u0394 attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61 . Additionally, we have log\n\u0012\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0013\n= log\n\u0010\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0011\n+ \u0394 attnLogits(\ud835\udc5b\ud835\udc5e)\ud835\udc61; note\nthat the logodds function \ud835\udc5d \u21a6\u2192 log\n\u0010\n\ud835\udc5d\n1\u2212\ud835\udc5d\n\u0011\nis the inverse of the sigmoid function, so attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e) =\n28\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nAlgorithm 3 Hierarchical algorithm for causal attribution, in iterative form. In practice we do\nadditional batching rather than evaluating a single block at a time on line 14.\nRequire: branching factor \ud835\udc35, num levels \ud835\udc3f, compute budget \ud835\udc40, nodes \ud835\udc41 = {\ud835\udc5b\ud835\udc56}, intervention function\nI\n1: numTopLevelBlocks \u2190 \u2308|\ud835\udc41|/\ud835\udc35\ud835\udc3f\u2309\n2: \ud835\udf0b \u2190 shuffle \u0000\b\u0004\nnumTopLevelBlocks \u00b7 \ud835\udc56\ud835\udc35\ud835\udc3f/|\ud835\udc41|\n\u0005\f\f\ud835\udc56 \u2208 {0, . . . , |\ud835\udc41| \u2212 1}\n\t\u0001\n3: for all \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc41 do\n4:\n(\ud835\udc51\ud835\udc3f\u22121, \ud835\udc51\ud835\udc3f\u22122, . . . , \ud835\udc510) \u2190 zero-padded final \ud835\udc3f base-\ud835\udc35 digits of \ud835\udf0b\ud835\udc56\n5:\naddress(\ud835\udc5b\ud835\udc56) = (\u230a\ud835\udf0b\ud835\udc56/\ud835\udc35\ud835\udc3f\u230b, \ud835\udc51\ud835\udc3f\u22121, . . . , \ud835\udc510)\n6: \ud835\udc44 \u2190 CreateEmptyPriorityQueue()\n7: for \ud835\udc56 \u2190 0 to numTopLevelBlocks \u2212 1 do\n8:\nPriorityQueueInsert(\ud835\udc44, [\ud835\udc56], \u221e)\n9: spentBudget \u2190 0\n10: topNodeContribs \u2190 CreateEmptyDictionary()\n11: repeat\n12:\n(addressPrefix, priority) \u2190 PriorityQueuePop(\ud835\udc44)\n13:\nblockNodes \u2190 {\ud835\udc5b \u2208 \ud835\udc41|StartsWith(address(\ud835\udc5b), addressPrefix)}\n14:\nblockContribution \u2190 |I (blockNodes) |\n15:\nspentBudget \u2190 spentBudget + 1\n16:\nif blockNodes = {\ud835\udc5b} for some \ud835\udc5b \u2208 \ud835\udc41 then\n17:\ntopNodeContribs[\ud835\udc5b] \u2190 blockContribution\n18:\nelse\n19:\nfor \ud835\udc56 \u2190 0 to \ud835\udc35 \u2212 1 do\n20:\nif {\ud835\udc5b \u2208 blockNodes| StartsWith(address(\ud835\udc5b), addressPrefix + [\ud835\udc56]} \u2260 \u2205 then\n21:\nPriorityQueueInsert(\ud835\udc44, addressPrefix + [\ud835\udc56], min(blockContribution, priority))\n22: until spentBudget = \ud835\udc40 or PriorityQueueEmpty(\ud835\udc44)\n23: return topNodeContribs\n29\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n\ud835\udf0e\n\u0012\nlog\n\u0012\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0013\u0013\n. Putting this together, we can compute all attnLogitspatch(\ud835\udc5b\ud835\udc5e) by combining\nall keys from the \ud835\udc65noise forward pass with all queries from the \ud835\udc65clean forward pass, and proceed to\ncompute \u0394 attnLogits(\ud835\udc5b\ud835\udc5e), and all \u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61, and thus all \u02c6I\ud835\udc3e\nAtPfix(\ud835\udc5b\ud835\udc61; \ud835\udc65clean, \ud835\udc65noise), using O(\ud835\udc472) flops\nper attention head.\nAlgorithm 4 computes the contribution of some query node \ud835\udc5b\ud835\udc5e and prompt pair \ud835\udc65clean, \ud835\udc65noise to the\ncorrected AtP estimates \u02c6\ud835\udc50\ud835\udc3e\nAtPfix(\ud835\udc5b\ud835\udc58\n\ud835\udc61 ) for key nodes \ud835\udc5b\ud835\udc58\n1, . . . , \ud835\udc5b\ud835\udc58\n\ud835\udc47 from a single attention head, using \ud835\udc42(\ud835\udc47)\nflops, while avoiding numerical overflows. We reuse the notation attn(\ud835\udc5b\ud835\udc5e), attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e), \u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e),\nattnLogits(\ud835\udc5b\ud835\udc5e), attnLogitspatch(\ud835\udc5b\ud835\udc5e), and \ud835\udc60\ud835\udc61 from Section 3.1.1, leaving the prompt pair implicit.\nAlgorithm 4 AtP correction for attention keys\nRequire: a := attnLogits(\ud835\udc5b\ud835\udc5e), apatch := attnLogitspatch(\ud835\udc5b\ud835\udc5e), g := \ud835\udf15L(M(\ud835\udc65clean))\n\ud835\udf15 attn(\ud835\udc5b\ud835\udc5e)\n1: \ud835\udc61\u2217 \u2190 argmax\ud835\udc61(\ud835\udc4e\ud835\udc61)\n2: \u2113 \u2190 a \u2212 \ud835\udc4e\ud835\udc61\u2217 \u2212 log (\u00cd\n\ud835\udc61 \ud835\udc52\ud835\udc4e\ud835\udc61\u2212\ud835\udc4e\ud835\udc61\u2217)\n\u22b2 Clean log attn weights, \u2113 = log(attn(\ud835\udc5b\ud835\udc5e))\n3: d \u2190 \u2113 \u2212 log(1 \u2212 \ud835\udc52\u2113)\n\u22b2 Clean logodds, \ud835\udc51\ud835\udc61 = log\n\u0010\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0011\n4: \ud835\udc51\ud835\udc61\u2217 \u2190 \ud835\udc4e\ud835\udc61\u2217 \u2212 max\ud835\udc61\u2260\ud835\udc61\u2217 \ud835\udc4e\ud835\udc61 \u2212 log \u0000\u00cd\n\ud835\udc61\u2032\u2260\ud835\udc61\u2217 \ud835\udc52\ud835\udc4e\ud835\udc61\u2032 \u2212max\ud835\udc61\u2260\ud835\udc61\u2217 \ud835\udc4e\ud835\udc61\u0001\n\u22b2 Adjust d; more stable for \ud835\udc4e\ud835\udc61\u2217 \u226b max\ud835\udc61\u2260\ud835\udc61\u2217 \ud835\udc4e\ud835\udc61\n5: \u2113patch \u2190 logsigmoid(d + apatch \u2212 a)\n\u22b2 Patched log attn weights, \u2113patch\n\ud835\udc61\n= log(attn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61)\n6: \u0394\u2113 \u2190 \u2113patch \u2212 \u2113\n\u22b2 \u0394\u2113\ud835\udc61 = log\n\u0012\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0013\n7: \ud835\udc4f \u2190 softmax(a)\u22bag\n\u22b2 \ud835\udc4f = attn(\ud835\udc5b\ud835\udc5e)\u22bag\n8: for \ud835\udc61 \u2190 1 to \ud835\udc47 do\n\u22b2 Compute scaling factor \ud835\udc60\ud835\udc61 := \u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n9:\nif \u2113patch\n\ud835\udc61\n> \u2113\ud835\udc61 then\n\u22b2 Avoid overflow when \u2113patch\n\ud835\udc61\n\u226b \u2113\ud835\udc61\n10:\n\ud835\udc60\ud835\udc61 \u2190 \ud835\udc52\ud835\udc51\ud835\udc61+\u0394\u2113\ud835\udc61+log(1\u2212\ud835\udc52\u2212\u0394\u2113\ud835\udc61 )\n\u22b2 \ud835\udc60\ud835\udc61 =\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0012\n1 \u2212\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0013\n11:\nelse\n\u22b2 Avoid overflow when \u2113patch\n\ud835\udc61\n\u226a \u2113\ud835\udc61\n12:\n\ud835\udc60\ud835\udc61 \u2190 \u2212\ud835\udc52\ud835\udc51\ud835\udc61+log(1\u2212\ud835\udc52\u0394\u2113\ud835\udc61 )\n\u22b2 \ud835\udc60\ud835\udc61 = \u2212 attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n1\u2212attn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0012\n1 \u2212\nattn\ud835\udc61\npatch(\ud835\udc5b\ud835\udc5e)\ud835\udc61\nattn(\ud835\udc5b\ud835\udc5e)\ud835\udc61\n\u0013\n13:\n\ud835\udc5f\ud835\udc61 \u2190 \ud835\udc60\ud835\udc61(\ud835\udc54\ud835\udc61 \u2212 \ud835\udc4f)\n\u22b2 \ud835\udc5f\ud835\udc61 = \ud835\udc60\ud835\udc61(onehot(\ud835\udc61) \u2212 attn(\ud835\udc5b\ud835\udc5e))\u22bag = \u0394\ud835\udc61 attn(\ud835\udc5b\ud835\udc5e) \u00b7 \ud835\udf15L(M(\ud835\udc65clean))\n\ud835\udf15 attn(\ud835\udc5b\ud835\udc5e)\n14: return r\nThe corrected AtP estimates \u02c6\ud835\udc50\ud835\udc3e\nAtPfix(\ud835\udc5b\ud835\udc58\n\ud835\udc61 ) can then be computed using Equation (10); in other words,\nby summing the returned \ud835\udc5f\ud835\udc61 from Algorithm 4 over queries \ud835\udc5b\ud835\udc5e for this attention head, and averaging\nover \ud835\udc65clean, \ud835\udc65noise \u223c D.\nA.2.2. Properties of GradDrop\nIn Section 3.1.2 we introduced GradDrop to address an AtP failure mode arising from cancellation\nbetween direct and indirect effects: roughly, if the total effect (on some prompt pair) is I(\ud835\udc5b) =\nIdirect(\ud835\udc5b) + Iindirect(\ud835\udc5b), and these are close to cancelling, then a small multiplicative approximation\nerror in \u02c6Iindirect\nAtP\n(\ud835\udc5b), due to nonlinearities, can accidentally cause | \u02c6Idirect\nAtP\n(\ud835\udc5b) + \u02c6Iindirect\nAtP\n(\ud835\udc5b)| to be orders\nof magnitude smaller than |I(\ud835\udc5b)|.\nTo address this failure mode with an improved estimator \u02c6\ud835\udc50AtP+GD(\ud835\udc5b), there\u2019s 3 desiderata for\nGradDrop:\n1. \u02c6\ud835\udc50AtP+GD(\ud835\udc5b) shouldn\u2019t be much smaller than \u02c6\ud835\udc50AtP(\ud835\udc5b), because that would risk creating more false\n30\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nnegatives.\n2. \u02c6\ud835\udc50AtP+GD(\ud835\udc5b) should usually not be much larger than \u02c6\ud835\udc50AtP(\ud835\udc5b), because that would create false\npositives, which also slows down verification and can effectively create false negatives at a given\nbudget.\n3. If \u02c6\ud835\udc50AtP(\ud835\udc5b) is suffering from the cancellation failure mode, then \u02c6\ud835\udc50AtP+GD(\ud835\udc5b) should be significantly\nlarger than \u02c6\ud835\udc50AtP(\ud835\udc5b).\nLet\u2019s recall how GradDrop was defined in Section 3.1.2, using a virtual node \ud835\udc5bout\n\u2113\nto represent the\nresidual-stream contributions of layer \u2113:\n\u02c6\ud835\udc50AtP+GD(\ud835\udc5b) := \ud835\udd3c\ud835\udc65clean,\ud835\udc65noise\n\"\n1\n\ud835\udc3f \u2212 1\n\ud835\udc3f\n\u2211\ufe01\n\u2113=1\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\f\n#\n= \ud835\udd3c\ud835\udc65clean,\ud835\udc65noise\n\"\n1\n\ud835\udc3f \u2212 1\n\ud835\udc3f\n\u2211\ufe01\n\u2113=1\n\f\f\f\f(\ud835\udc5b(\ud835\udc65noise) \u2212 \ud835\udc5b(\ud835\udc65clean))\u22ba \ud835\udf15L\u2113\n\ud835\udf15\ud835\udc5b\n\f\f\f\f\n#\n= \ud835\udd3c\ud835\udc65clean,\ud835\udc65noise\n\"\n1\n\ud835\udc3f \u2212 1\n\ud835\udc3f\n\u2211\ufe01\n\u2113=1\n\f\f\f\f(\ud835\udc5b(\ud835\udc65noise) \u2212 \ud835\udc5b(\ud835\udc65clean))\u22ba \ud835\udf15L\n\ud835\udf15\ud835\udc5b (M(\ud835\udc65clean | do(\ud835\udc5bout\n\u2113\n\u2190 \ud835\udc5bout\n\u2113\n(\ud835\udc65clean))))\n\f\f\f\f\n#\nTo better understand the behaviour of GradDrop, let\u2019s look more carefully at the gradient \ud835\udf15L\n\ud835\udf15\ud835\udc5b .\nThe total gradient \ud835\udf15L\n\ud835\udf15\ud835\udc5b can be expressed as a sum of all path gradients from the node \ud835\udc5b to the output.\nEach path is characterized by the set of layers \ud835\udc60 it goes through (in contrast to routing via the skip\nconnection). We write the gradient along a path \ud835\udc60 as \ud835\udf15L\ud835\udc60\n\ud835\udf15\ud835\udc5b .\nLet S be the set of all subsets of layers after the layer \ud835\udc5b is in. For example, the direct-effect path is\ngiven by \u2205 \u2208 S. Then the total gradient can be expressed as\n\ud835\udf15L\n\ud835\udf15\ud835\udc5b =\n\u2211\ufe01\n\ud835\udc60\u2208S\n\ud835\udf15L\ud835\udc60\n\ud835\udf15\ud835\udc5b .\n(16)\nWe can analogously define \u02c6I\ud835\udc60\nAtP(\ud835\udc5b) = (\ud835\udc5b(\ud835\udc65noise) \u2212 \ud835\udc5b(\ud835\udc65clean))\u22ba \ud835\udf15L\ud835\udc60\n\ud835\udf15\ud835\udc5b , and break down \u02c6IAtP(\ud835\udc5b) =\n\u00cd\n\ud835\udc60\u2208S \u02c6I\ud835\udc60\nAtP(\ud835\udc5b). The effect of doing GradDrop at some layer \u2113 is then to drop all terms \u02c6I\ud835\udc60\nAtP(\ud835\udc5b) with \u2113 \u2208 \ud835\udc60:\nin other words,\n\u02c6IAtP+GD\u2113 (\ud835\udc5b) =\n\u2211\ufe01\n\ud835\udc60\u2208S\n\u2113\u2209\ud835\udc60\n\u02c6I\ud835\udc60\nAtP(\ud835\udc5b).\n(17)\nNow we\u2019ll use this understanding to discuss the 3 desiderata.\nFirstly, most node effects are approximately independent of most layers (see e.g. Veit et al.\n(2016)); for any layer \u2113 that \ud835\udc5b\u2019s effect is independent of, we\u2019ll have \u02c6IAtP+GD\u2113 (\ud835\udc5b) = \u02c6IAtP(\ud835\udc5b). Letting\n\ud835\udc3e be the set of downstream layers that matter, this guarantees\n1\n\ud835\udc3f\u22121\n\u00cd\ud835\udc3f\n\u2113=1\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\f \u2265\n\ud835\udc3f\u2212|\ud835\udc3e|\u22121\n\ud835\udc3f\u22121\n\f\f\f \u02c6IAtP(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\f, which meets the first desideratum.\nRegarding the second desideratum: for each \u2113 we have\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f \u2264 \u00cd\n\ud835\udc60\u2208S\n\f\f\f \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f, so overall\nwe have\n1\n\ud835\udc3f\u22121\n\u00cd\ud835\udc3f\n\u2113=1\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f \u2264 \ud835\udc3f\u2212|\ud835\udc3e|\u22121\n\ud835\udc3f\u22121\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f + |\ud835\udc3e|\n\ud835\udc3f\u22121\n\u00cd\n\ud835\udc60\u2208S\n\f\f\f \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f. For the RHS to be much larger\n31\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(e.g. \ud835\udefc times larger) than\n\f\f\f\u00cd\n\ud835\udc60\u2208S \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f = | \u02c6IAtP(\ud835\udc5b)|, there must be quite a lot of cancellation between\ndifferent paths, enough so that \u00cd\n\ud835\udc60\u2208S\n\f\f\f \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f \u2265\n(\ud835\udc3f\u22121)\ud835\udefc\n|\ud835\udc3e|\n\f\f\f\u00cd\n\ud835\udc60\u2208S \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f. This is possible, but seems\ngenerally unlikely for e.g. \ud835\udefc > 3.\nNow let\u2019s consider the third desideratum, i.e. suppose \ud835\udc5b is a cancellation false negative, with\n| \u02c6IAtP(\ud835\udc5b)| \u226a |I(\ud835\udc5b)| \u226a |Idirect(\ud835\udc5b)| \u2248 | \u02c6Idirect\nAtP\n(\ud835\udc5b)|. Then,\n\f\f\f\u00cd\n\ud835\udc60\u2208S\\\u2205 \u02c6I\ud835\udc60\nAtP(\ud835\udc5b)\n\f\f\f =\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f \u226b |I(\ud835\udc5b)|.\nThe summands in \u00cd\n\ud835\udc60\u2208S\\\u2205 \u02c6I\ud835\udc60\nAtP(\ud835\udc5b) are the union of the summands in \u00cd\n\ud835\udc60\u2208S\n\u2113\u2208\ud835\udc60\n\u02c6I\ud835\udc60\nAtP(\ud835\udc5b) = \u02c6IAtP(\ud835\udc5b)\u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\nacross layers \u2113.\nIt\u2019s then possible but intuitively unlikely that \u00cd\n\u2113\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f would be much smaller\nthan\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f.\nSuppose the ratio is \ud835\udefc, i.e.\nsuppose \u00cd\n\u2113\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f\n=\n\ud835\udefc\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f. For example, if all indirect effects use paths of length 1 then the union is\na disjoint union, so \u00cd\n\u2113\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f \u2265\n\f\f\f\u00cd\n\u2113\n\u0010\n\u02c6IAtP(\ud835\udc5b) \u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\u0011\f\f\f =\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f, so\n\ud835\udefc \u2265 1. Now:\n\u2211\ufe01\n\u2113\u2208\ud835\udc3e\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f \u2265\n\u2211\ufe01\n\u2113\u2208\ud835\udc3e\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f \u2212 |\ud835\udc3e|\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f\n(18)\n= \ud835\udefc\n\f\f\f \u02c6IAtP(\ud835\udc5b) \u2212 \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f \u2212 |\ud835\udc3e|\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f\n(19)\n\u2265 \ud835\udefc\n\f\f\f \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f \u2212 (|\ud835\udc3e| + \ud835\udefc)\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f\n(20)\n\u2234\n1\n\ud835\udc3f \u2212 1\n\ud835\udc3f\n\u2211\ufe01\n\u2113=1\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f =\n1\n\ud835\udc3f \u2212 1\n\u2211\ufe01\n\u2113\u2208\ud835\udc3e\n\f\f\f \u02c6IAtP+GD\u2113 (\ud835\udc5b)\n\f\f\f + \ud835\udc3f \u2212 |\ud835\udc3e| \u2212 1\n\ud835\udc3f \u2212 1\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f\n(21)\n\u2265\n\ud835\udefc\n\ud835\udc3f \u2212 1\n\f\f\f \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f + \ud835\udc3f \u2212 2|\ud835\udc3e| \u2212 1 \u2212 \ud835\udefc\n\ud835\udc3f \u2212 1\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f\n(22)\n(23)\nAnd the RHS is an improvement over\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f so long as \ud835\udefc\n\f\f\f \u02c6Idirect\nAtP\n(\ud835\udc5b)\n\f\f\f > (2|\ud835\udc3e| + \ud835\udefc)\n\f\f\f \u02c6IAtP(\ud835\udc5b)\n\f\f\f, which\nis likely given the assumptions.\nUltimately, though, the desiderata are validated by the experiments, which consistently show\nGradDrops either decreasing or leaving untouched the number of false negatives, and thus improving\nperformance apart from the initial upfront cost of the extra backwards passes.\nA.3. Algorithm for computing diagnostics\nGiven summary statistics \u00af\ud835\udc56\u00b1, \ud835\udc60\u00b1 and count\u00b1 for every node \ud835\udc5b, obtained from Algorithm 1, and a\nthreshold \ud835\udf03 > 0 we can use Welch\u2019s \ud835\udc61-test Welch (1947) to test the hypothesis that |\u00af\ud835\udc56+ \u2212 \u00af\ud835\udc56\u2212| \u2265 \ud835\udf03.\nConcretely we compute the \ud835\udc61-statistic via\n\ud835\udc60\u00af\ud835\udc56\u00b1 =\n\ud835\udc60\u00b1\n\u221acount\u00b1\n(24)\n\ud835\udc61 = \ud835\udf03 \u2212 |\u00af\ud835\udc56+ \u2212 \u00af\ud835\udc56\u2212|\n\u221a\ufe03\n\ud835\udc602\n\u00af\ud835\udc56+ + \ud835\udc602\n\u00af\ud835\udc56\u2212\n.\n(25)\n32\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nThe effective degrees of freedom \ud835\udf08 can be approximated with the Welch\u2013Satterthwaite equation\n\ud835\udf08Welch =\n\u0010\n\ud835\udc602\n+\ncount+ +\n\ud835\udc602\n\u2212\ncount\u2212\n\u00112\n\ud835\udc604\n+\ncount2+(count+\u22121) +\n\ud835\udc604\u2212\ncount2\u2212 (count\u2212\u22121)\n(26)\nWe then compute the probability (\ud835\udc5d-value) of obtaining a \ud835\udc61 at least as large as observed, using the\ncumulative distribution function of Student\u2019s \ud835\udc61\n\u0010\n\ud835\udc65; \ud835\udf08Welch\n\u0011\nat the appropriate points. We take the max\nof the individual \ud835\udc5d-values of all nodes to obtain an aggregate upper bound. Finally, we use binary\nsearch to find the largest threshold \ud835\udf03 that still has an aggregate \ud835\udc5d-value smaller than a given target\n\ud835\udc5d value. We show multiple such diagnostic curves in Appendix B.3, for different confidence levels\n(1 \u2212 \ud835\udc5dtarget).\nB. Experiments\nB.1. Prompt Distributions\nB.1.1. IOI\nWe use the following prompt template:\nBOS When \u2423[A] \u2423and \u2423[B] \u2423went \u2423to \u2423the \u2423bar , \u2423[A/C] \u2423gave \u2423a \u2423drink \u2423to \u2423[B/A]\nEach clean prompt \ud835\udc65clean uses two names A and B with completion B, while a noise prompt \ud835\udc65noise\nuses names A, B, and C with completion A. We construct all possible such assignments where names\nare chosen from the set of {Michael, Jessica, Ashley, Joshua, David, Sarah}, resulting in 120\nprompt pairs.\nB.1.2. A-AN\nWe use the following prompt template to induce the prediction of an indefinite article.\nBOS I \u2423want \u2423one \u2423pear . \u2423Can \u2423you \u2423pick \u2423up \u2423a \u2423pear \u2423for \u2423me ?\n\u2423I \u2423want \u2423one \u2423orange . \u2423Can \u2423you \u2423pick \u2423up \u2423an \u2423orange \u2423for \u2423me ?\n\u2423I \u2423want \u2423one \u2423[OBJECT] . \u2423Can \u2423you \u2423pick \u2423up \u2423[a/an]\nWe found that zero shot performance of small models was relatively low, but performance improved\ndrastically when providing a single example of each case. Model performance was sensitive to the\nordering of the two examples but was better than random in all cases. The magnitude and sign of the\nimpact of the few-shot ordering was inconsistent.\nClean prompts \ud835\udc65clean contain objects inducing \u2018\u2423a\u2019, one of {boat, coat, drum, horn, map, pipe,\nscrew, stamp, tent, wall}. Noise prompts \ud835\udc65noise contain objects inducing \u2018\u2423an\u2019, one of {apple,\nant, axe, award, elephant, egg, orange, oven, onion, umbrella}. This results in a total of 100\nprompt pairs.\nB.2. Cancellation across a distribution\nAs mention in Section 2, we average the magnitudes of effects across a distribution, rather than\ntaking the magnitude of the average effect. We do this because cancellation of effects is happening\n33\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) Pythia-410M\n(b) Pythia-1B\n(c) Pythia-2.8B\n(d) Pythia-12B\nFigure 10 | Cancellation ratio across IOI for various model sizes. A ratio of 1 means positive and\nnegative effects cancel out across the distribution, whereas a ratio of 0 means only either negative or\npositive effects exist across the distribution. We report cancellation ratio for different percentiles of\nnodes based on \u00cd\n\ud835\udc65clean,\ud835\udc65noise\n\f\fI(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f.\nfrequently across a distribution, which, together with imprecise estimates, could lead to significant\nfalse negatives. A proper ablation study to quantify this effect exactly is beyond the scope of this\nwork. In Figure 10, we show the degree of cancellation across the IOI distribution for various model\nsizes. For this we define the Cancellation Ratio of node \ud835\udc5b as\n1 \u2212\n\f\f\u00cd\n\ud835\udc65clean,\ud835\udc65noise I(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f\n\u00cd\n\ud835\udc65clean,\ud835\udc65noise\n\f\fI(\ud835\udc5b; \ud835\udc65clean, \ud835\udc65noise)\n\f\f.\nB.3. Additional detailed results\nWe show the diagnostic measurements for Pythia-12B across all investigated distributions in Figure 11,\nand cost of verified 100% recall curves for all models and settings in Figures 12 and 13.\nB.4. Metrics\nIn this paper we focus on the difference in loss (negative log probability) as the metric L. We provide\nsome evidence that AtP(\u2217) is not sensitive to the choice of L. For Pythia-12B, on IOI-PP and IOI,\nwe show the rank scatter plots in Figure 14 for three different metrics.\nFor IOI, we also show that performance of AtP\u2217 looks notably worse when effects are evaluated\n34\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 11 | Diagnostic of false negatives for 12B across distributions.\n(a.i) IOI-PP\n(a.ii) RAND-PP\n(a.iii) IOI\n(a) AttentionNodes\n(b.i) CITY-PP\n(b.ii) RAND-PP\n(b.iii) A-AN\n(b) NeuronNodes\nvia denoising instead of noising (cf. Section 2.1). As of now we do not have a satisfactory explanation\nfor this observation.\nB.5. Hyperparameter selection\nThe iterative baseline, and the AtP-based methods, have no hyperparameters. In general, we used 5\nrandom seeds for each hyperparameter setting, and selected the setting that produced the lowest\nIRWRGM cost (see Section 4.2).\nFor Subsampling, the two hyperparameters are the Bernoulli sampling probability \ud835\udc5d, and the\nnumber of samples to collect before verifying nodes in decreasing order of \u02c6\ud835\udc50SS. \ud835\udc5d was chosen from\n{0.01, 0.03}14. The number of steps was chosen among power-of-2 numbers of batches, where the\nbatch size depended on the setting.\nFor Blocks, we swept across block sizes 2, 6, 20, 60, 250. For Hierarchical, we used a branching\nfactor of \ud835\udc35 = 3, because of the following heuristic argument. If all but one node had zero effect, then\ndiscovering that node would be a matter of iterating through the hierarchy levels. We\u2019d have number\nof levels log\ud835\udc35 |\ud835\udc41|, and at each level, \ud835\udc35 forward passes would be required to find which lower-level\nblock the special node is in \u2013 and thus the cost of finding the node would be \ud835\udc35 log\ud835\udc35 |\ud835\udc41| =\n\ud835\udc35\nlog \ud835\udc35 log |\ud835\udc41|.\n\ud835\udc35\nlog \ud835\udc35 is minimized at \ud835\udc35 = \ud835\udc52, or at \ud835\udc35 = 3 if \ud835\udc35 must be an integer. The other hyperparameter is the\nnumber of levels; we swept this from 2 to 12.\n14We observed early on that larger values of \ud835\udc5d were consistently underperforming. We leave it to future work to investigate\nmore granular and smaller values for \ud835\udc5d.\n35\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 12 | Cost of verified 100% recall curves, sweeping across models and settings for NeuronNodes\n(a.i) Pythia 410M\n(a.ii) Pythia 1B\n(a.iii) Pythia 2.8B\n(a.iv) Pythia 12B\n(a) CITY-PP\n(b.i) Pythia 410M\n(b.ii) Pythia 1B\n(b.iii) Pythia 2.8B\n(b.iv) Pythia 12B\n(b) RAND-PP\n(c.i) Pythia 410M\n(c.ii) Pythia 1B\n(c.iii) Pythia 2.8B\n(c.iv) Pythia 12B\n(c) A-AN distribution\n36\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 13 | Cost of verified 100% recall curves, sweeping across models and settings for AttentionNodes\n(a.i) Pythia 410M\n(a.ii) Pythia 1B\n(a.iii) Pythia 2.8B\n(a.iv) Pythia 12B\n(a) IOI-PP\n(b.i) Pythia 410M\n(b.ii) Pythia 1B\n(b.iii) Pythia 2.8B\n(b.iv) Pythia 12B\n(b) RAND-PP\n(c.i) Pythia 410M\n(c.ii) Pythia 1B\n(c.iii) Pythia 2.8B\n(c.iv) Pythia 12B\n(c) IOI distribution\n37\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 14 | True ranks against AtP\u2217 ranks on Pythia-12B using various metrics L. The last row shows\nthe effect in the denoising (rather than noising) setting; we speculate that the lower-right subplot\n(log-odds denoising) is similar to the lower-middle one (logit-diff denoising) because IOI produces a\nbimodal distribution over the correct and alternate next token.\n38\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nC. AtP variants\nC.1. Residual-site AtP and Layer normalization\nLet\u2019s consider the behaviour of AtP on sites that contain much or all of the total signal in the residual\nstream, such as residual-stream sites. Nanda (2022) described a concern about this behaviour: that\nlinear approximation of the layer normalization would do poorly if the patched value is significantly\ndifferent than the clean one, but with a similar norm. The proposed modification to AtP to account\nfor this was to hold the scaling factors (in the denominators) fixed when computing the backwards\npass. Here we\u2019ll present an analysis of how this modification would affect the approximation error of\nAtP. (Empirical investigation of this issue is beyond the scope of this paper.)\nConcretely, let the node under consideration be \ud835\udc5b, with clean and alternate values \ud835\udc5bclean and\n\ud835\udc5bnoise; and for simplicity, let\u2019s assume the model does nothing more than an unparametrized RM-\nSNorm M(\ud835\udc5b) := \ud835\udc5b/|\ud835\udc5b|. Let\u2019s now consider how well M(\ud835\udc5bnoise) is approximated, both by its first-\norder approximation\n\u02c6\nMAtP(\ud835\udc5bnoise) := M(\ud835\udc5bclean) + M(\ud835\udc5bclean)\u22a5(\ud835\udc5bnoise \u2212 \ud835\udc5bclean) where M(\ud835\udc5bclean)\u22a5 =\n\ud835\udc3c \u2212 M(\ud835\udc5bclean)M(\ud835\udc5bclean)\u22ba is the projection to the hyperplane orthogonal to M(\ud835\udc5bclean), and by the\nvariant that fixes the denominator: \u02c6\nMAtP+frozenLN(\ud835\udc5bnoise) := \ud835\udc5bnoise/|\ud835\udc5bclean|.\nTo quantify the error in the above, we\u2019ll measure the error \ud835\udf16 in terms of Euclidean distance. Let\u2019s\nalso assume, without loss of generality, that |\ud835\udc5bclean| = 1. Geometrically, then, M(\ud835\udc5b) is a projection onto\nthe unit hypersphere, MAtP(\ud835\udc5b) is a projection onto the tangent hyperplane at \ud835\udc5bclean, and MAtP+frozenLN\nis the identity function.\nNow, let\u2019s define orthogonal coordinates (\ud835\udc65, \ud835\udc66) on the plane spanned by \ud835\udc5bclean, \ud835\udc5bnoise, such that \ud835\udc5bclean\nis mapped to (1, 0) and \ud835\udc5bnoise is mapped to (\ud835\udc65, \ud835\udc66), with \ud835\udc66 \u2265 0. Then, \ud835\udf16AtP :=\n\f\f\f \u02c6\nM(\ud835\udc5bnoise) \u2212 M(\ud835\udc5bnoise)\n\f\f\f =\n\u221a\ufe02\n2 + \ud835\udc662 \u2212 2\n\ud835\udc65+\ud835\udc662\n\u221a\n\ud835\udc652+\ud835\udc662 , while \ud835\udf16AtP+frozenLN :=\n\f\f\f \u02c6\nMfix(\ud835\udc5bnoise) \u2212 M(\ud835\udc5bnoise)\n\f\f\f =\n\f\f\f\n\u221a\ufe01\n\ud835\udc652 + \ud835\udc662 \u2212 1\n\f\f\f.\nPlotting the error in Figure 15, we can see that, as might be expected, freezing the layer norm\ndenominators helps whenever \ud835\udc5bnoise indeed has the same norm as \ud835\udc5bclean, and (barring weird cases with\n\ud835\udc65 > 1) whenever the cosine-similarity is less than 1\n2; but largely hurts if \ud835\udc5bnoise is close to \ud835\udc5bclean. This\nillustrates that, while freezing the denominators will generally be unhelpful when patch distances\nare small relative to the full residual signal (as with almost all nodes considered in this paper), it will\nlikely be helpful in a different setting of patching residual streams, which could be quite unaligned\nbut have similar norm.\nC.2. Edge AtP and AtP*\nHere we will investigate edge attribution patching, and how the cost scales if we use GradDrop and/or\nQK fix. (For this section we\u2019ll focus on a single prompt pair.)\nFirst, let\u2019s review what edge attribution patching is trying to approximate, and how it works.\nC.2.1. Edge intervention effects\nGiven nodes \ud835\udc5b1, \ud835\udc5b2 where \ud835\udc5b1 is upstream of \ud835\udc5b2, if we were to patch in an alternate value for \ud835\udc5b1, this\ncould impact \ud835\udc5b2 in a complicated nonlinear way. As discussed in 3.1.2, because LLMs have a residual\nstream, the \u201cdirect effect\u201d can be understood as the one holding all other possible intermediate nodes\nbetween \ud835\udc5b1 and \ud835\udc5b2 fixed \u2013 and it\u2019s a relatively simple function, composed of transforming the alternate\nvalue \ud835\udc5b1(\ud835\udc65noise) to a residual stream contribution \ud835\udc5fout,\u21131(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise))), then carrying it\nalong the residual stream to an input \ud835\udc5fin,\u21132 = \ud835\udc5fin,\u21132(\ud835\udc65clean) + (\ud835\udc5fout,\u21131 \u2212 \ud835\udc5fout,\u21131(\ud835\udc65clean)), and transforming\n39\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\n(a) \ud835\udf16AtP\n(b) \ud835\udf16AtP+frozenLN\n(c) \ud835\udf16AtP+frozenLN \u2212 \ud835\udf16AtP\nFigure 15 | A comparison of how AtP and AtP with frozen layernorm scaling behave in a toy setting\nwhere the model we\u2019re trying to approximate is just M(\ud835\udc5b) := \ud835\udc5b/|\ud835\udc5b|. The red region is where frozen\nlayernorm scaling helps; the blue region is where it hurts. We find that unless \ud835\udc65 > 1, frozen layernorm\nscaling always has lower error when the cosine-similarity between \ud835\udc5bnoise and \ud835\udc5bclean is < 1\n2 (in other\nwords the angle > 60\u25e6), but often has higher error otherwise.\n40\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nthat into a value \ud835\udc5bdirect\n2\n.\nIn the above, \u21131 and \u21132 are the semilayers containing \ud835\udc5b1 and \ud835\udc5b2, respectively. Let\u2019s define n(\u21131,\u21132)\nto be the set of non-residual nodes between semilayers \u21131 and \u21132. Then, we can define the resulting\n\ud835\udc5bdirect\n2\nas:\n\ud835\udc5bdirect\u21131\n2\n(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise))) := \ud835\udc5b2(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise)), do(n(\u21131,\u21132) \u2190 n(\u21131,\u21132) (\ud835\udc65clean))).\nThe residual-stream input \ud835\udc5fdirect\u21131\nin,\u21132\n(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise))) is defined similarly.\nFinally, \ud835\udc5b2 itself isn\u2019t enough to compute the metric L \u2013 for that we also need to let the forward\npass M(\ud835\udc65clean) run using the modified \ud835\udc5bdirect\u21131\n2\n(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise))), while removing all other\neffects of \ud835\udc5b1 (i.e. not patching it).\nWriting this out, we have edge intervention effect\nI(\ud835\udc5b1 \u2192 \ud835\udc5b2; \ud835\udc65clean, \ud835\udc65noise) := L(M(\ud835\udc65clean| do(\ud835\udc5b2 \u2190 \ud835\udc5bdirect\u21131\n2\n(\ud835\udc65clean| do(\ud835\udc5b1 \u2190 \ud835\udc5b1(\ud835\udc65noise))))))\n\u2212L(M(\ud835\udc65clean)).\n(27)\nC.2.2. Nodes and Edges\nLet\u2019s briefly consider what edges we\u2019d want to be evaluating this on. In Section 4.1, we were able\nto conveniently separate attention nodes from MLP neurons, knowing that to handle both kinds of\nnodes, we\u2019d just need to be able handle each kind of node on its own, and then combine the results.\nFor edge interventions this of course isn\u2019t true, because edges can go from MLP neurons to attention\nnodes, and vice versa. For the purposes of this section, we\u2019ll assume that the node set \ud835\udc41 contains the\nattention nodes, and for MLPs either a node per layer (as in Syed et al. (2023)), or a node per neuron\n(as in the NeuronNodes setting).\nRegarding the edges, the MLP nodes can reasonably be connected with any upstream or down-\nstream node, but this isn\u2019t true for the attention nodes, which have more of a structure amongst\nthemselves: the key, query, and value nodes for an attention head can only affect downstream nodes\nvia the attention output nodes for that head, and vice versa. As a result, on edges between different\nsemilayers, upstream attention nodes must be attention head outputs, and downstream attention\nnodes must be keys, queries, or values. In addition, there are some within-attention-head edges,\nconnecting each query node to the output node in the same position, and each key and value node to\noutput nodes in causally affectable positions.\nC.2.3. Edge AtP\nAs with node activation patching, the edge intervention effect I(\ud835\udc5b1 \u2192 \ud835\udc5b2; \ud835\udc65clean, \ud835\udc65noise) is costly to\nevaluate directly for every edge, since a forward pass is required each time. However, as with AtP, we\ncan apply first-order approximations: we define\n\u02c6IAtP(\ud835\udc5b1 \u2192 \ud835\udc5b2; \ud835\udc65clean, \ud835\udc65noise) :=\n\u0010\n\u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise)\n\u0011\u22ba\n\u2207AtP\n\ud835\udc5f\ud835\udc5b2 L(M(\ud835\udc65clean)),\n(28)\nwhere \u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise) := Jac\ud835\udc5b1(\ud835\udc5fout,\u21131)(\ud835\udc5b1(\ud835\udc65clean))(\ud835\udc5b1(\ud835\udc65noise) \u2212 \ud835\udc5b1(\ud835\udc65clean))\n(29)\nand \u2207AtP\n\ud835\udc5f\ud835\udc5b2 L(M(\ud835\udc65clean)) :=\n\u0010\nJac\ud835\udc5fin,\u21132 (\ud835\udc5b2)(\ud835\udc5fin,\u21132(\ud835\udc65clean))\n\u0011\u22ba\n\u2207\ud835\udc5b2(L(M(\ud835\udc65clean)))(\ud835\udc5b2(\ud835\udc65clean)),\n(30)\nand this is a close approximation when \ud835\udc5b1(\ud835\udc65noise) \u2248 \ud835\udc5b1(\ud835\udc65clean).\n41\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nA key benefit of this decomposition is that the first term depends only on \ud835\udc5b1, and the second term\ndepends only on \ud835\udc5b2; and they\u2019re both easy to compute from a forward and backward pass on \ud835\udc65clean\nand a forward pass on \ud835\udc65noise, just like AtP itself.\nThen, to complete the edge-AtP evaluation, what remains computationally is to evaluate all\nthe dot products between nodes in different semilayers, at each token position. This requires\n\ud835\udc51resid\ud835\udc47(1\u2212 1\n\ud835\udc3f)|\ud835\udc41|2/2 multiplications in total15, where \ud835\udc3f is the number of layers, \ud835\udc47 is the number of tokens,\nand |\ud835\udc41| is the total number of nodes. This cost exceeds the cost of computing all \u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise) and\n\u2207AtP\n\ud835\udc5f\ud835\udc5b2 L(M(\ud835\udc65clean)) on Pythia 2.8B even with a single node per MLP layer; if we look at a larger model,\nor especially if we consider single-neuron nodes even for small models, the gap grows significantly.\nDue to this observation, we\u2019ll focus our attention on the quadratic part of the compute cost,\npertaining to two nodes rather than just one \u2013 i.e. the number of multiplications in computing all\n(\u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise))\u22ba\u2207AtP\n\ud835\udc5f\ud835\udc5b2 L(M(\ud835\udc65clean)). Notably, we\u2019ll also exclude within-attention-head edges from\nthe \u201cquadratic cost\u201d: these edges, from some key, query, or value node to an attention output node\ncan be handled by minor variations of the nodewise AtP or AtP* methods for the corresponding key,\nquery, or value node.\nC.2.4. MLPs\nThere are a couple of issues that can come up around the MLP nodes. One is that, similarly to the\nattention saturation issue described in Section 3.1.1, the linear approximation to the MLP may be\nfairly bad in some cases, creating significant false negatives if \ud835\udc5b2 is an MLP node. Another issue is\nthat if we use single-neuron nodes, then those are very numerous, making the \ud835\udc51resid-dimensional dot\nproduct per edge quite costly.\nMLP saturation and fix\nJust as clean activations that saturate the attention probability may have\nsmall gradients that lead to strongly underestimated effects, the same is true of the MLP nonlinearity.\nA similar fix is applicable: instead of using a linear approximation to the function from \ud835\udc5b1 to \ud835\udc5b2, we\ncan linearly approximate the function from \ud835\udc5b1 to the preactivation \ud835\udc5b2,pre, and then recompute \ud835\udc5b2\nusing that, before multiplying by the gradient.\nThis kind of rearrangement, where the gradient-delta-activation dot product is computed in \ud835\udc51\ud835\udc5b2\ndimensions rather than \ud835\udc51resid, will come up again \u2013 we\u2019ll call it the factored form of AtP.\nIf the nodes are neurons then the factored form requires no change to the number of multiplications;\nhowever, if they\u2019re MLP layers then there\u2019s a large increase in cost, by a factor of \ud835\udc51neurons. This increase\nis mitigated by two factors: one is that this is a small minority of edges, outnumbered by the number\nof edges ending in attention nodes by 3\u00d7 (# heads per layer); the other is the potential for parameter\nsharing.\nNeuron edges and parameter sharing\nA useful observation is that each edge, across different to-\nken16 positions, reuses the same parameter matrices in Jac\ud835\udc5b1(\ud835\udc5fout,\u21131)(\ud835\udc5b1(\ud835\udc65clean)) and Jac\ud835\udc5fin,\u21132 (\ud835\udc5b2)(\ud835\udc5fin,\u21132(\ud835\udc65clean)).\nIndeed, setting aside the MLP activation function, the only other nonlinearity in those functions is a\nlayer normalization; if we freeze the scaling factor at its clean value as in Appendix C.1, the Jacobians\nare equal to the product of the corresponding parameter matrices, divided by the clean scaling factor.\nThus if we premultiply the parameter matrices then we eliminate the need to do so at each token,\n15This formula omits edges within a single layer, for simplicity \u2013 but those are a small minority.\n16Also across different batch entries, if we do this on more than one prompt pair.\n42\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nwhich reduces the per-token quadratic cost by \ud835\udc51resid (i.e. to a scalar multiplication) for neuron-neuron\nedges, or by \ud835\udc51resid/\ud835\udc51site (i.e. to a \ud835\udc51site-dimensional dot product) for edges between neurons and some\nattention site.\nIt\u2019s worth noting, though, that these premultiplied parameter matrices (or, indeed, the edge-AtP\nestimates if we use neuron sites) will in total be many times (specifically, (\ud835\udc3f \u2212 1) \ud835\udc51neurons\n4\ud835\udc51resid times) larger\nthan the MLP weights themselves, so storage may need to be considered carefully. It may be worth\nconsidering ways to only find the largest estimates, or the estimates over some threshold, rather than\nfull estimates for all edges.\nC.2.5. Edge AtP* costs\nLet\u2019s now consider how to adapt the AtP* proposals from Section 3.1 to this setting. We\u2019ve already seen\nthat the MLP fix, which is similarly motivated to the QK fix, has negligible cost in the neuron-nodes\ncase, but comes with a \ud835\udc51neurons/\ud835\udc51resid overhead in quadratic cost in the case of using an MLP layer\nper node, at least on edges into those MLP nodes. We\u2019ll consider the MLP fix to be part of edge-AtP*.\nNow let\u2019s investigate the two corrections in regular AtP*: GradDrops, and the QK fix.\nGradDrops\nGradDrops works by replacing the single backward pass in the AtP formula with \ud835\udc3f\nbackward passes; this in effect means \ud835\udc3f values for the multiplicand \u2207AtP\n\ud835\udc5f\ud835\udc5b2 L(M(\ud835\udc65clean)), so this is a\nmultiplicative factor of \ud835\udc3f on the quadratic cost (though in fact some of these will be duplicates, and\ntaking this into account lets us drive the multiplicative factor down to (\ud835\udc3f + 1)/2). Notably this works\nequally well with \u201cfactored AtP\u201d, as used for neuron edges; and in particular, if \ud835\udc5b2 is a neuron, the\ngradients can easily be combined and shared across \ud835\udc5b1s, eliminating the (\ud835\udc3f + 1)/2 quadratic-cost\noverhead.\nHowever, the motivation for GradDrops was to account for multiple paths whose effects may\ncancel; in the edge-interventions setting, these can already be discovered in a different way (by\nidentifying the responsible edges out of \ud835\udc5b2), so the benefit of GradDrops is lessened. At the same\ntime, the cost remains substantial. Thus, we\u2019ll omit GradDrops from our recommended procedure\nedge-AtP*.\nQK fix\nThe QK fix applies to the \u2207\ud835\udc5b2(L(M(\ud835\udc65clean)))(\ud835\udc5b2(\ud835\udc65clean)) term, i.e. to replacing the linear\napproximation to the softmax with a correct calculation to the change in softmax, for each different\ninput \u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise). As in Section 3.1.1, there\u2019s the simpler case of accounting for \ud835\udc5b2s that are\nquery nodes, and the more complicated case of \ud835\udc5b2s that are key nodes using Algorithm 4 \u2013 but these\nare both cheap to do after computing the \u0394 attnLogits corresponding to \ud835\udc5b2.\nThe \u201cfactored AtP\u201d way is to matrix-multiply \u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise) with key or query weights and\nwith the clean queries or keys, respectively. This means instead of the \ud835\udc51resid multiplications required\nfor each edge \ud835\udc5b1 \u2192 \ud835\udc5b2 with AtP, we need \ud835\udc51resid\ud835\udc51key +\ud835\udc47\ud835\udc51key multiplications (which, thanks to the causal\nmask, can be reduced to an average of \ud835\udc51key(\ud835\udc51resid + (\ud835\udc47 + 1)/2)).\nThe \u201cunfactored\u201d option is to stay in the \ud835\udc5fin,\u21132 space: pre-multiply the clean queries or keys with\nthe respective key or query weight matrices, and then take the dot product of \u0394\ud835\udc5fAtP\n\ud835\udc5b1 (\ud835\udc65clean, \ud835\udc65noise) with\neach one. This way, the quadratic part of the compute cost contains \ud835\udc51resid(\ud835\udc47 + 1)/2 multiplications;\nthis will be more efficient for short sequence lengths.\nThis means that for edges into key and query nodes, the overhead of doing AtP+QKfix on the\nquadratic cost is a multiplicative factor of min\n\u0010\n\ud835\udc47+1\n2 , \ud835\udc51key\n\u0010\n1 +\n\ud835\udc47+1\n2\ud835\udc51resid\n\u0011\u0011\n.\n43\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nAtP variant\nO\u2192V\nO\u2192Q,K\nO\u2192MLP\nMLP\u2192V\nMLP\u2192Q,K\nMLP\u2192MLP\nMLP layers\n\ud835\udc37\ud835\udc3b2\n2\ud835\udc37\ud835\udc3b2\n\ud835\udc37\ud835\udc3b\n\ud835\udc37\ud835\udc3b\n2\ud835\udc37\ud835\udc3b\n\ud835\udc37\nQKfix\n\ud835\udc37\ud835\udc3b2\n(\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b2\n\ud835\udc37\ud835\udc3b\n\ud835\udc37\ud835\udc3b\n(\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b\n\ud835\udc37\nQKfix+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc37\ud835\udc3b2\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc37\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\nAtP*\n\ud835\udc37\ud835\udc3b2\n(\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc37\ud835\udc3b\n(\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b\n\ud835\udc41\ud835\udc37\nAtP*+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc37\ud835\udc3b\n\ud835\udc41\ud835\udc37\nQKfix (long)\n\ud835\udc37\ud835\udc3b2\n(2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc37\ud835\udc3b\n\ud835\udc37\ud835\udc3b\n(2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b\n\ud835\udc37\nQKfix+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n\ud835\udc3f+1\n2 (2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n\ud835\udc3f+1\n2 (2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\nATP*\n\ud835\udc37\ud835\udc3b2\n(2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc37\ud835\udc3b\n(2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b\n\ud835\udc41\ud835\udc37\nAtP*+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n\ud835\udc3f+1\n2 (2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b\n\ud835\udc3f+1\n2 (2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b\n\ud835\udc41\ud835\udc37\nNeurons\n\ud835\udc37\ud835\udc3b2\n2\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc49\ud835\udc41\ud835\udc3b\n2\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nMLPfix\n\ud835\udc37\ud835\udc3b2\n2\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc49\ud835\udc41\ud835\udc3b\n2\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nAtP*\n\ud835\udc37\ud835\udc3b2\n(\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc49\ud835\udc41\ud835\udc3b\n(\ud835\udc47 + 1)\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nAtP*+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n\ud835\udc3f+1\n2 (\ud835\udc47 + 1)\ud835\udc37\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc49\ud835\udc41\ud835\udc3b\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nATP* (long)\n\ud835\udc37\ud835\udc3b2\n(2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc49\ud835\udc41\ud835\udc3b\n(\ud835\udc47 + 1)\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nAtP*+GD\n\ud835\udc3f+1\n2 \ud835\udc37\ud835\udc3b2\n\ud835\udc3f+1\n2 (2\ud835\udc37 + \ud835\udc47 + 1)\ud835\udc3e\ud835\udc3b2\n\ud835\udc49\ud835\udc41\ud835\udc3b\n\ud835\udc3f+1\n2 \ud835\udc49\ud835\udc41\ud835\udc3b\n(\ud835\udc3f+1) (\ud835\udc47+1)\n2\n\ud835\udc3e\ud835\udc41\ud835\udc3b\n\ud835\udc412\nTable 2 | Per-token per-layer-pair total quadratic cost of each kind of between-layers edge, across edge-\nAtP variants. For brevity, we omit the layer-pair \u0000\ud835\udc3f\n2\n\u0001 factor that would otherwise be in every cell, and use\n\ud835\udc37 := \ud835\udc51resid, \ud835\udc3b := # heads per layer, \ud835\udc3e := \ud835\udc51key, \ud835\udc49 := \ud835\udc51value, \ud835\udc41 := \ud835\udc51neurons.\nQK fix + GradDrops\nIf the QK fix is being combined with GradDrops, then the first multiplication\nby the \ud835\udc51resid \u00d7 \ud835\udc51key matrix can be shared between the different gradients; so the overhead on the\nquadratic cost of QKfix + GradDrops for edges into queries and keys, using the factored method, is\n\ud835\udc51key\n\u0010\n1 + (\ud835\udc47+1) (\ud835\udc3f+1)\n4\ud835\udc51resid\n\u0011\n.\nC.3. Conclusion\nConsidering all the above possibilities, it\u2019s not obvious where the best tradeoff is between correct-\nness and compute cost in all situations. In Table 2 we provide formulas measuring the number of\nmultiplications in the quadratic cost for each kind of edge, across the variations we\u2019ve mentioned. In\nFigure 16 we plug in the 4 sizes of Pythia model used elsewhere in the paper, such as Figure 2, to\nenable numerical comparison.\nD. Distribution of true effects\nIn Figure 17, we show the distribution of \ud835\udc50(\ud835\udc5b) across models and distributions.\n44\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 16 | A comparison of edge-AtP variants across model sizes and prompt lengths. AtP* here is defined\nto include QKfix and MLPfix, but not GradDrops. The costs vary across several orders of magnitude for each\nsetting.\nIn the setting with full-MLP nodes, MLPfix carries substantial cost for short prompts, but barely matters for\nlong prompts.\nIn the neuron-nodes setting, MLPfix is costless. But GradDrops in that setting continues to impose a large cost;\neven though it doesn\u2019t affect MLP\u2192MLP edges, it does affect MLP\u2192Q,K edges, which come out dominating\nthe cost with QKfix.\n45\nAtP\u2217: An efficient and scalable method for localizing LLM behaviour to components\nFigure 17 | Distribution of true effects across models and prompt pair distributions\nAttentionNodes\nNeuronNodes\n(a.i)\n(a.ii)\n(a) Pythia-410M\n(b.i)\n(b.ii)\n(b) Pythia-1B\n(c.i)\n(c.ii)\n(c) Pythia-2.8B\n(d.i)\n(d.ii)\n(d) Pythia-12B\n46\n"
  },
  {
    "title": "RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization",
    "link": "https://arxiv.org/pdf/2403.00483.pdf",
    "upvote": "8",
    "text": "RealCustom: Narrowing Real Text Word for Real-Time Open-Domain\nText-to-Image Customization\nMengqi Huang1*, Zhendong Mao1\u2020, Mingcong Liu2, Qian He2, Yongdong Zhang1\n1 University of Science and Technology of China; 2ByteDance Inc.\n{huangmq}@mail.ustc.edu.cn, {zdmao, zhyd73}@ustc.edu.cn, {liumingcong, heqian}@bytedance.com\nAbstract\nText-to-image customization, which aims to synthesize\ntext-driven images for the given subjects, has recently rev-\nolutionized content creation.\nExisting works follow the\npseudo-word paradigm, i.e., represent the given subjects as\npseudo-words and then compose them with the given text.\nHowever, the inherent entangled influence scope of pseudo-\nwords with the given text results in a dual-optimum para-\ndox, i.e., the similarity of the given subjects and the con-\ntrollability of the given text could not be optimal simultane-\nously. We present RealCustom that, for the first time, dis-\nentangles similarity from controllability by precisely lim-\niting subject influence to relevant parts only, achieved by\ngradually narrowing real text word from its general conno-\ntation to the specific subject and using its cross-attention\nto distinguish relevance.\nSpecifically, RealCustom intro-\nduces a novel \u201ctrain-inference\u201d decoupled framework: (1)\nduring training, RealCustom learns general alignment be-\ntween visual conditions to original textual conditions by\na novel adaptive scoring module to adaptively modulate\ninfluence quantity; (2) during inference, a novel adaptive\nmask guidance strategy is proposed to iteratively update\nthe influence scope and influence quantity of the given sub-\njects to gradually narrow the generation of the real text\nword. Comprehensive experiments demonstrate the supe-\nrior real-time customization ability of RealCustom in the\nopen domain, achieving both unprecedented similarity of\nthe given subjects and controllability of the given text for\nthe first time. The project page is https://corleone-\nhuang.github.io/realcustom/.\n1. Introduction\nRecent significant advances in the customization of pre-\ntrained large-scale text-to-image models [6, 24, 25, 28] (i.e.,\ntext-to-image customization) has revolutionized content cre-\n*Works done during the intership at ByteDance.\n\u2020Zhendong Mao is the corresponding author.\nFigure 1. Comparison between the existing paradigm and ours.\n(a) The existing paradigm represents the given subject as pseudo-\nwords (e.g., S\u2217), which has entangled the same entire influence\nscope with the given text, resulting in the dual-optimum paradox,\ni.e., the similarity for the given subject and the controllability for\nthe given text could not achieve optimum simultaneously. (b) We\npropose RealCustom, a novel paradigm that, for the first time dis-\nentangles similarity from controllability by precisely limiting the\ngiven subjects to influence only the relevant parts while the rest\nparts are purely controlled by the given text. This is achieved by\niteratively updating the influence scope and influence quantity of\nthe given subjects. (c) The quantitative comparison shows that\nour paradigm achieves both superior similarity and controllability\nthan the state-of-the-arts of the existing paradigm. CLIP-image\nscore (CLIP-I) and CLIP-text score (CLIP-T) are used to evaluate\nsimilarity and controllability. Refer to the experiments for details.\nation, receiving rapidly growing research interest from both\nacademia and industry.\nThis task empowers pre-trained\nmodels with the ability to generate imaginative text-driven\nscenes for subjects specified by users (e.g., a person\u2019s clos-\nest friends or favorite paintings), which is a foundation for\nAI-generated content (AIGC) and real-world applications\nsuch as personal image&video creation [7]. The primary\ngoal of customization is dual-faceted: (1) high-quality simi-\nlarity, i.e., the target subjects in the generated images should\nclosely mirror the given subjects; (2) high-quality control-\n1\narXiv:2403.00483v1  [cs.CV]  1 Mar 2024\nFigure 2. Generated customization results of our proposed novel\nparadigm RealCustom.\nGiven a single image representing the\ngiven subject in the open domain (any subjects, portrait painting,\nfavorite toys, etc.), RealCustom could generate realistic images\nthat consistently adhere to the given text for the given subjects\nin real-time (without any test-time optimization steps).\nlability, i.e., the remaining subject-irrelevant parts should\nconsistently adhere to the control of the given text.\nExisting literature follows the pseudo-word paradigm,\ni.e., (1) learning pseudo-words (e.g., S\u2217 [10] or rare-tokens\n[27]) to represent the given subjects; (2) composing these\npseudo-words with the given text for the customized gener-\nation. Recent studies have focused on learning more com-\nprehensive pseudo-words [1, 8, 22, 32, 38] to capture more\nsubject information, e.g., different pseudo-words for differ-\nent diffusion timesteps [1, 38] or layers [32]. Meanwhile,\nothers propose to speed up pseudo-word learning by train-\ning an encoder [11, 18, 30, 34] on object-datasets [17]. In\nparallel, based on the learned pseudo-words, many works\nfurther finetune the pre-trained models [16, 18, 27, 34] or\nadd additional adapters [30] for higher similarity. As more\ninformation of the given subjects is introduced into pre-\ntrained models, the risk of overfitting increases, leading to\nthe degradation of controllability. Therefore, various regu-\nlarizations (e.g., l1 penalty [10, 16, 34], prior-preservation\nloss [27]) are used to maintain controllability, which in\nturn sacrifices similarity. Essentially, existing methods are\ntrapped in a dual-optimum paradox, i.e., the similarity and\ncontrollability can not be optimal simultaneously.\nWe argue that the fundamental cause of this dual-\noptimum paradox is rooted in the existing pseudo-word\nparadigm, where the similarity component (i.e., the pseudo-\nwords) to generate the given subjects is intrinsically en-\ntangled with the controllability component (i.e., the given\ntext) to generate subject-irrelevant parts, causing an over-\nall conflict in the generation, as illustrated in Fig.\n1(a).\nSpecifically, this entanglement is manifested in the same\nentire influence scope of these two components. i.e., both\nthe pseudo-words and the given text affect all generation\nregions.\nThis is because each region is updated as a\nweighted sum of all word features through built-in textual\ncross-attention in pre-trained text-to-image diffusion mod-\nels. Therefore, increasing the influence of the similarity\ncomponent will simultaneously strengthen the similarity in\nthe subject-relevant parts and weaken the influence of the\ngiven text in other irrelevant ones, causing the degrada-\ntion of controllability, and vice versa. Moreover, the nec-\nessary correspondence between pseudo-words and subjects\nconfines existing methods to either lengthy test-time opti-\nmization [10, 16, 27] or training [18, 34] on object-datasets\n[17] that have limited categories. As a result, the existing\nparadigm inherently has poor generalization capability for\nreal-time open-domain scenarios in the real world.\nIn this paper, we present RealCustom, a novel cus-\ntomization paradigm that, for the first time, disentangles the\nsimilarity component from the controllability component\nby precisely limiting the given subjects to influence only\nthe relevant parts while maintaining other irreverent ones\npurely controlled by the given texts, achieving both high-\nquality similarity and controllability in a real-time open-\ndomain scenario, as shown in Fig. 2. The core idea of Real-\nCustom is that, instead of representing subjects as pseudo-\nwords, we could progressively narrow down the real text\nwords (e.g., \u201ctoy\u201d) from their initial general connotation\n(e.g., various kinds o toys) to the specific subjects (e.g., the\nunique sloth toy), wherein the superior text-image align-\nment in pre-trained models\u2019 cross-attention can be lever-\naged to distinguish subject relevance, as illustrated in Fig.\n1(b). Specifically, at each generation step, (1) the influence\nscope of the given subject is identified by the target real\nword\u2019s cross-attention, with a higher attention score indi-\ncating greater relevance; (2) this influence scope then de-\ntermines the influence quantity of the given subject at the\ncurrent step, i.e., the amount of subject information to be\ninfused into this scope; (3) this influence quantity, in turn,\nshapes a more accurate influence scope for the next step,\nas each step\u2019s generation result is based on the output of the\nprevious. Through this iterative updating, the generation re-\nsult of the real word is smoothly and accurately transformed\ninto the given subject, while other irrelevant parts are com-\npletely controlled by the given text.\nTechnically,\nRealCustom\nintroduces\nan\ninnovative\n\u201ctrain-inference\u201d decoupled framework: (1) During train-\ning, RealCustom only learns the generalized alignment ca-\npabilities between visual conditions and pre-trained mod-\nels\u2019 original text conditions on large-scale text-image\ndatasets through a novel adaptive scoring module, which\nmodulates the influence quantity based on text and cur-\nrently generated features. (2) During inference, real-time\ncustomization is achieved by a novel adaptive mask guid-\nance strategy, which gradually narrows down a real text\nword based on the learned alignment capabilities. Specif-\n2\nically, (1) the adaptive scoring module first estimates the\nvisual features\u2019 correlation scores with the text features and\ncurrently generated features, respectively. Then a timestep-\naware schedule is applied to fuse these two scores. A subset\nof key visual features, chosen based on the fused score, is\nincorporated into pre-trained diffusion models by extend-\ning its textual cross-attention with another visual cross-\nattention. (2) The adaptive mask guidance strategy consists\nof a text-to-image (T2I) branch (with the visual condition\nset to 0) and a text&image-to-image (TI2I) branch (with the\nvisual condition set to the given subject). Firstly, all lay-\ners\u2019 cross-attention maps of the target real word in the T2I\nbranch are aggregated into a single one, selecting only high-\nattention regions as the influence scope. Secondly, in the\nTI2I branch, the influence scope is multiplied by currently\ngenerated features to produce the influence quantity and\nconcurrently multiplied by the outputs of the visual cross-\nattention to avoid influencing subject-irrelevant parts.\nOur contributions are summarized as follows:\nConcepts. For the first time, we (1) point out the dual-\noptimum paradox is rooted in the existing pseudo-word\nparadigm\u2019s entangled influence scope between the similar-\nity (i.e., pseudo-words representing the given subjects) and\ncontrollability (i.e., the given texts); (2) present RealCus-\ntom, a novel paradigm that achieves disentanglement by\ngradually narrowing down real words into the given sub-\njects, wherein the given subjects\u2019 influence scope is limited\nbased on the cross-attention of the real words.\nTechnology.\nThe proposed RealCustom introduces a\nnovel \u201ctrain-inference\u201d decoupled framework: (1) during\ntraining, learning generalized alignment between visual\nconditions to original text conditions by the adaptive scor-\ning module to modulate influence quantity; (2) during in-\nference, the adaptive mask guidance strategy is proposed\nto narrow down a real word by iterative updating the given\nsubject\u2019s influence scope and quantity.\nSignificance. For the first time, we achieve (1) superior\nsimilarity and controllability simultaneously, as shown in\nFig. 1(c); (2) real-time open-domain customization ability.\n2. Related Works\n2.1. Text-to-Image Customization\nExisting customization methods follow the pseudo-words\nparadigm, i.e., representing the given subjects as pseudo-\nwords and then composing them with the given text for cus-\ntomization. Since the necessary correspondence between\nthe pseudo-words and the given subjects, existing works\nare confined to either cumbersome test-time optimization-\nbased [1, 8\u201310, 16, 22, 27, 32] or encoder-based [7, 11,\n14, 18, 30, 34] that trained on object-datasets with limited\ncategories. For example, in the optimization-based stream,\nDreamBooth [27] uses a rare-token as the pseudo-word and\nfurther fine-tunes the entire pre-trained diffusion model for\nbetter similarity. Custom Diffusion [16] instead finds a sub-\nset of key parameters and only optimizes them. The main\ndrawback of this stream is that it requires lengthy optimiza-\ntion times for each new subject. As for the encoder-based\nstream, the recent ELITE [34] uses a local mapping net-\nwork to improve similarity, while BLIP-Diffusion [18] in-\ntroduces a multimodal encoder for better subject represen-\ntation. These encoder-based works usually show less simi-\nlarity than optimization-based works and generalize poorly\nto unseen categories in training. In summary, the entangled\ninfluence scope of pseudo-words and the given text natu-\nrally limits the current works from achieving both optimal\nsimilarity and controllability, as well as hindering real-time\nopen-domain customization.\n2.2. Cross-Attention in Diffusion Models\nText guidance in modern large-scale text-to-image diffusion\nmodels [2, 6, 24, 25, 28] is generally performed using the\ncross-attention mechanism.\nTherefore, many works pro-\npose to manipulate the cross-attention map for text-driven\nediting [3, 12] on generated images or real images via inver-\nsion [31], e.g., Prompt-to-Prompt [12] proposes to reassign\nthe cross-attention weight to edit the generated image. An-\nother branch of work focuses on improving cross-attention\neither by adding additional spatial control [20, 21] or post-\nprocessing to improve semantic alignment [5, 19]. Mean-\nwhile, a number of works [33, 35, 36] propose using cross-\nattention in diffusion models for discriminative tasks such\nas segmentation. However, different from the existing lit-\nerature, the core idea of RealCustom is to gradually narrow\na real text word from its initial general connotation (e.g.,\nwhose cross-attention could represent any toy with various\ntypes of shapes and details) to the unique given subject (e.g.,\nwhose cross-attention accurately represents the unique toy),\nwhich is completely unexplored.\n3. Methodology\nIn this study, we focus on the most general customization\nscenario: with only a single image representing the given\nsubject, generating new high-quality images for that subject\nfrom the given text. The generated subject may vary in loca-\ntion, pose, style, etc., yet it should maintain high similarity\nwith the given one. The remaining parts should consistently\nadhere to the given text, thus ensuring controllability.\nThe proposed RealCustom introduces a novel \u201ctrain-\ninference\u201d decoupled paradigm as illustrated in Fig.\n3.\nSpecifically, during training, RealCustom learns general\nalignment between visual conditions and the original text\nconditions of pre-trained models. During inference, based\non the learned alignment capability, RealCustom gradually\nnarrow down the generation of the real text words (e.g.,\n\u201ctoy\u201d) into the given subject (e.g., the unique brown sloth\n3\nFigure 3. Illustration of our proposed RealCustom, which employs a novel \u201ctrain-inference\u201d decoupled framework: (a) During training,\ngeneral alignment between visual and original text conditions is learned by the proposed adaptive scoring module, which accurately derives\nvisual conditions based on text and currently generated features. (b) During inference, progressively narrowing down a real word (e.g.,\n\u201ctoy\u201d) from its initial general connotation to the given subject (e.g., the unique brown sloth toy) by the proposed adaptive mask guidance\nstrategy, which consists of two branches, i.e., a text-to-image (T2I) branch where the visual condition is set to 0, and a text&image-to-image\n(TI2I) branch where the visual condition is set to the given subject. The T2I branch aims to calculate the influence scope by aggregating\nthe target real word\u2019s (e.g., \u201ctoy\u201d) cross-attention, while the TI2I branch aims to inject the influence quantity into this scope.\ntoy) by iterative updating each step\u2019s influence scope and\ninfluence quantity of the given subject.\nWe first briefly introduce the preliminaries in Sec. 3.1.\nThe training and inference paradigm of RealCustom will be\nelaborated in detail in Sec. 3.2 and Sec. 3.3, respectively.\n3.1. Preliminaries\nOur paradigm is implemented over Stable Diffusion [25],\nwhich consists of two components, i.e., an autoencoder and\na conditional UNet [26] denoiser. Firstly, given an image\nx \u2208 RH\u00d7W \u00d73, the encoder E(\u00b7) of the autoencoder maps\nit into a lower dimensional latent space as z = E(x) \u2208\nRh\u00d7w\u00d7c, where f = H0\nh = W0\nw is the downsampling factor\nand c stands for the latent channel dimension. The corre-\nsponding decoder D(\u00b7) maps the latent vectors back to the\nimage as D(E(x)) \u2248 x. Secondly, the conditional denoiser\n\u03f5\u03b8(\u00b7) is trained on this latent space to generate latent vectors\nbased on the text condition y. The pre-trained CLIP text en-\ncoder [23] \u03c4text(\u00b7) is used to encode the text condition y into\ntext features fct = \u03c4text(y). Then, the denoiser is trained\nwith mean-squared loss:\nL := Ez\u223cE(x),fy,\u03f5\u223cN (0,I),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, fct)\u22252\n2\ni\n, (1)\nwhere \u03f5 denotes for the unscaled noise and t is the timestep.\nzt is the latent vector that noised according to t:\nzt =\np\n\u02c6\u03b1tz0 +\np\n1 \u2212 \u02c6\u03b1t\u03f5,\n(2)\nwhere \u02c6\u03b1t \u2208 [0, 1] is the hyper-parameter that modulates the\nquantity of noise added. Larger t means smaller \u02c6\u03b1t and\n4\nthereby a more noised latent vector zt. During inference, a\nrandom Gaussian noise zT is iteratively denoised to z0, and\nthe final generated image is obtained through x\n\u2032 = D(z0).\nThe incorporation of text condition in Stable Diffusion is\nimplemented as textual cross-attention:\nAttention(Q, K, V ) = Softmax(QK\u22a4\n\u221a\nd\n)V ,\n(3)\nwhere the query Q = WQ \u00b7 fi, key K = WK \u00b7 fct and\nvalue V = WV \u00b7 fct. WQ, WK, WV are weight param-\neters of query, key and value projection layers. fi, fct are\nthe latent image features and text features, and d is the chan-\nnel dimension of key and query features. The latent image\nfeature is then updated with the attention block output.\n3.2. Training Paradigm\nAs depicted in Fig. 3(a), the text y and image x are first\nencoded into text features fct \u2208 Rnt\u00d7ct and image fea-\ntures fci \u2208 Rni\u00d7ci by the pre-trained CLIP text/image en-\ncoders [23] respectively. Here, nt, ct, ni, ci are text feature\nnumber/dimension and image feature number/dimension,\nrespectively. Afterward, the adaptive scoring module takes\nthe text features fct, currently generated features zt \u2208\nRh\u00d7w\u00d7c, and timestep t as inputs to estimate the score for\neach features in fci, selecting a subset of key ones as the\nvisual condition \u02c6\nfci \u2208 R\u02c6ni\u00d7ci, where \u02c6ni < ni is the se-\nlected image feature number. Next, we extend textual cross-\nattention with another visual cross-attention to incorporate\nthe visual condition \u02c6\nfyi. Specifically, Eq. 3 is rewritten as:\nAttention(Q, K, V , Ki, Vi) =\nSoftmax(QK\u22a4\n\u221a\nd\n)V + Softmax(QKi\n\u22a4\n\u221a\nd\n)Vi,\n(4)\nwhere the new key Ki = WKi \u00b7 \u02c6\nfci, value Vi = WV i \u00b7 \u02c6\nfci\nare added. WKi and WV i are weight parameters. Dur-\ning training, only the adaptive scoring module and projec-\ntion layers WKi, WV i in each attention block are trainable,\nwhile other pre-trained models\u2019 weight remains frozen.\nAdaptive Scoring Module. On the one hand, the gen-\neration of the diffusion model itself, by nature, is a coarse-\nto-fine process with noise removed and details added step\nby step. In this process, different steps focus on different\ndegrees of subject detail [2], spanning from global struc-\ntures in the early to local textures in the latter. Accord-\ningly, the importance of each image feature also dynam-\nically changes.\nTo smoothly narrow the real text word,\nthe image condition of the subject should also adapt syn-\nchronously, providing guidance from coarse to fine grain.\nThis requires equipping RealCustom with the ability to es-\ntimate the importance score of different image features. On\nthe other hand, utilizing all image features as visual condi-\ntions results in a \u201ctrain-inference\u201d gap. This arises because,\nunlike the training stage, where the same images as the vi-\nsual conditions and inputs to the denoiser \u03f5\u03b8, the given sub-\njects, and the inference generation results should maintain\nsimilarity only in the subject part. Therefore, this gap can\ndegrade both similarity and controllability in inference.\nThe above rationale motivates the adaptive scoring mod-\nule, which provides smooth and accurate visual conditions\nfor customization. As illustrated in Fig. 4, the text fct \u2208\nRnt\u00d7ct and currently generated features zt \u2208 Rh\u00d7w\u00d7c =\nRnz\u00d7c are first aggregated into the textual context Ctextual\nand visual context Cvisual through weighted pooling:\nAtextual = Softmax(fctW t\na) \u2208 Rnt\u00d71 (5)\nAvisual = Softmax(ztW v\na ) \u2208 Rnz\u00d71 (6)\nCtextual = A\u22a4\ntextualfy \u2208 R1\u00d7ct, Cvisual = A\u22a4\nvisualzt \u2208 R1\u00d7c, (7)\nwhere W t\na \u2208 Rct\u00d71, W v\na \u2208 Rc\u00d71 are weight parameters,\nand \u201cSoftmax\u201d is operated in the number dimension. These\ncontexts are then spatially replicated and concatenated with\nimage features fci \u2208 Rni\u00d7ci to estimate the textual score\nStextual \u2208 Rni\u00d71 and visual score Svisual \u2208 Rni\u00d71 respec-\ntively. These two scores are predicted by two lightweight\nscore-net, which are implemented as two-layer MLPs.\nConsidering that the textual features are roughly accurate\nand the generated features are gradually refined, a timestep-\naware schedule is proposed to fuse these two scores:\nS = (1 \u2212\np\n\u02c6\u03b1t)Stextual +\np\n\u02c6\u03b1tSvisual,\n(8)\nwhere \u221a\u02c6\u03b1t is the hyperparameter of pre-trained diffusion\nmodels that modulate the amount of noise added to gener-\nated features. Then a softmax activation is applied to the\nfused score since our focus is on highlighting the compar-\native significance of each image feature vis-`a-vis its coun-\nterparts: S = Softmax(S). The fused scores are multiplied\nwith the image features to enable the learning of score-nets:\nfci = fci \u25e6 (1 + S),\n(9)\nwhere \u25e6 denotes the element-wise multiply. Finally, given a\nTop-K ratio \u03b3num \u2208 [0, 1], a sub-set of key features with\nhighest scores are selected as the output \u02c6\nfyi \u2208 R\u02c6ni\u00d7ci,\nwhere \u02c6ni = \u03b3numni. To enable flexible inference with dif-\nferent \u03b3num without performance degradation, we propose\nto use a uniformly random ratio during training:\n\u03b3num = uniform[\u03b3low\nnum, \u03b3high\nnum],\n(10)\nwhere \u03b3low\nnum, \u03b3high\nnum are set to 0.3, 1.0, respectively.\n3.3. Inference Paradigm\nThe inference paradigm of RealCustom consists of two\nbranches, i.e., a text-to-image (T2I) branch where the visual\ninput is set to 0 and a text&image-to-image (TI2I) branch\n5\nFigure 4. Illustration of adaptive scoring module. Text features\nand currently generated features are first aggregated into the tex-\ntual and visual context, which are then spatially concatenated with\nimage features to predict textual and visual scores. These scores\nare then fused based on the current timestep. Ultimately, only a\nsubset of the key features is selected based on the fused score.\nwhere the visual input is set to given subjects, as illustrated\nin Fig. 3(b). These two branches are connected by our pro-\nposed adaptive mask guidance strategy. Specifically, given\nprevious step\u2019s output zt, a pure text conditional denois-\ning process is performed in T2I branch to get the output\nzT\nt\u22121, where all layers cross-attention map of the target real\nword (e.g., \u201ctoy\u201d) is extracted and resized to the same res-\nolution (the same as the largest map size, i.e., 64 \u00d7 64 in\nStable Diffusion). The aggregated attention map is denoted\nas M \u2208 R64\u00d764. Next, a Top-K selection is applied, i.e.,\ngiven the target ratio \u03b3scope \u2208 [0, 1], only \u03b3scope \u00d7 64 \u00d7 64\nregions with the highest cross-attention score will remain,\nwhile the rest will be set to 0. The selected cross-attention\nmap \u00af\nM is normalized by its maximum value as:\n\u02c6\nM =\n\u00af\nM\nmax( \u00af\nM),\n(11)\nwhere max(\u00b7) represents the maximum value. The rationale\nbehind this is that even in these selected parts, the subject\nrelevance of different regions is also different.\nIn the TI2I branch, the influence scope \u02c6\nM is first multi-\nplied by currently generated feature zt to provide accurate\nvisual conditions for current generation step. The reason\nis that only subject-relevant parts should be considered for\nthe calculation of influence quantity. Secondly, \u02c6\nM is mul-\ntiplied by the visual cross-attention results to prevent nega-\ntive impacts on the controllability of the given texts in other\nsubject-irrelevant parts. Specifically, Eq. 4 is rewritten as:\nAttention(Q, K, V , Ki, Vi) =\nSoftmax(QK\u22a4\n\u221a\nd\n)V + (Softmax(QKi\n\u22a4\n\u221a\nd\n)Vi) \u02c6\nM,\n(12)\nwhere the necessary resize operation is applied to match\nthe size of\n\u02c6\nM with the resolution of each cross-attention\nblock. The denoised output of TI2I branch is denoted as\nzT I\nt\u22121. The classifer-free guidance [13] is extended to pro-\nduce next step\u2019s denoised latent feature zt\u22121 as:\nzt\u22121 = \u03f5\u03b8(\u2205)+\u03c9t(zT\nt\u22121\u2212\u03f5\u03b8(\u2205))+\u03c9i(zT I\nt\u22121\u2212zT\nt\u22121), (13)\nwhere \u03f5\u03b8(\u2205) is the unconditional denoised output.\nWith the smooth and accurate influence quantity of the\ngiven subject injected into the current step, the generation\nof the real word will gradually be narrowed from its ini-\ntial general connotation to the specific subject, which will\nshape a more precise influence scope for the generation of\nthe next step. Through this iterative updating and genera-\ntion, we achieve real-time customization where the similar-\nity for the given subject is disentangled with the controlla-\nbility for the given text, leading to an optimal of both. More\nimportantly, since both the adaptive scoring module as well\nas visual cross-attention layers are trained on general text-\nimage datasets, the inference could be generally applied to\nany categories by using any target real words, enabling ex-\ncellent open-domain customization capability.\n4. Experiments\n4.1. Experimental Setups\nImplementation. RealCustom is implemented on Sta-\nble Diffusion and trained on the filtered subset of Laion-5B\n[29] based on aesthetic score, using 16 A100 GPUs for 16w\niterations with 1e-5 learning rate. Unless otherwise speci-\nfied, DDIM sampler [31] with 50 sample steps is used for\nsampling and the classifier-free guidance \u03c9t, \u03c9i is 7.5 and\n12.5. Top-K ratios \u03b3num = 0.8, \u03b3scope = 0.25.\nEvaluation. Similarity. We use the state-of-the-art seg-\nmentation model (i.e., SAM [15]) to segment the subject,\nand then evaluate with both CLIP-I and DINO [4] scores,\nwhich are average pairwise cosine similarity CLIP ViT-\nB/32 or DINO embeddings of the segmented subjects in\ngenerated and real images. Controllability. We calculate\nthe cosine similarity between prompt and image CLIP ViT-\nB/32 embeddings (CLIP-T). In addition, ImageReward [37]\nis used to evaluate controllability and aesthetics (quality).\nPrior SOTAs.\nWe compare with existing paradigm\nof both optimization-based (i.e., Textual Inversion[10],\nDreamBooth [27], CustomDiffusion [16]) and encoder-\nbased (ELITE[34], BLIP-Diffusion[18]) state-of-the-arts.\n4.2. Main Results\nQuantitative results. As shown in Tab. 1, RealCustom\noutperforms existing methods in all metrics: (1) for control-\nlability, we improve CLIP-T and ImageReward by 8.1% and\n223.5%, respectively. The significant improvement in Im-\nageReward shows that our paradigm generates much higher\nquality customization; (2) for similarity, we also achieve\nstate-of-the-art performance on both CLIP-I and DINO-I.\n6\nMethods\ncontrollability\nsimilarity\nefficiency\nCLIP-T \u2191\nImageReward \u2191\nCLIP-I \u2191\nDINO-I \u2191\ntest-time optimize steps\nTextual Inversion [10]\n0.2546\n-0.9168\n0.7603\n0.5956\n5000\nDreamBooth [27]\n0.2783\n0.2393\n0.8466\n0.7851\n800\nCustom Diffusion [16]\n0.2884\n0.2558\n0.8257\n0.7093\n500\nELITE [34]\n0.2920\n0.2690\n0.8022\n0.6489\n0 (real-time)\nBLIP-Diffusion [18]\n0.2967\n0.2172\n0.8145\n0.6486\n0 (real-time)\nRealCustom(ours)\n0.3204\n0.8703\n0.8552\n0.7865\n0 (real-time)\nTable 1. Quantitative comparisons with existing methods. Left: Our proposed RealCustom outperforms existing methods in all metrics, i.e.,\n(1) for controllability, achieving 8.1% and 223.5% improvements on CLIP-T and ImageReward, respectively. The significant improvement\non ImageReward also validates that RealCustom could generate customized images with much higher quality (higher aesthetic score);\n(2) for similarity, we also achieve state-of-the-art performance on both CLIP-I and DINO-I. Right: We plot the \u201cCLIP-T verse DINO\u201d,\nshowing that the existing methods are trapped into the dual-optimum paradox, while RealCustom completely get rid of it and achieve both\nhigh-quality similarity and controllability. The same conclusion in \u201cCLIP-T verse CLIP-I\u201d can be found in Fig. 1(c).\nFigure 5. Qualitative comparison with existing methods. RealCustom could produce much higher quality customization results that have\nbetter similarity with the given subject and better controllability with the given text compared to existing works. Moreover, RealCustom\nshows superior diversity (different subject poses, locations, etc.) and generation quality (e.g., the \u201cautumn leaves\u201d scene in the third row).\nThe figure of \u201cCLIP-T verse DINO\u201d validates that the ex-\nisting paradigm is trapped into the dual-optimum paradox,\nwhile RealCustom effectively eradicates it.\nQualitative results. As shown in Fig. 5, RealCustom\ndemonstrates superior zero-shot open-domain customiza-\ntion capability (e.g., the rare shaped toy in the first row),\ngenerating higher-quality custom images that have better\nsimilarity with the given subject and better controllability\nwith the given text compared to existing works.\n4.3. Ablations\nEffectiveness of adaptive mask guidance strategy. We first\nvisualize the narrowing down process of the real word by\nthe proposed adaptive mask guidance strategy in Fig. 6.\nWe could observe that starting from the same state (the\nsame mask since there\u2019s no information of the given sub-\nject is introduced at the first step), RealCustom gradually\nforms the structure and details of the given subject, achiev-\ning the open-domain zero-shot customization while remain-\n7\nFigure 6. Illustration of gradually narrowing the real words into the given subjects. Upper: RealCustom generated results (first row) and\nthe original text-to-image generated result (second row) by pre-trained models with the same seed. The mask is visualized by the Top-25%\nhighest attention score regions of the real word \u201ctoy\u201d. We could observe that starting from the same state (the same mask since there\u2019s no\ninformation of the given subject is introduced at the beginning), RealCustom gradually forms the structure and details of the given subject\nby our proposed adaptive mask strategy, achieving the open-domain zero-shot customization. Lower: More visualization cases.\ninference setting\nCLIP-T \u2191\nCLIP-I \u2191\n\u03b3scope = 0.1\n0.32\n0.8085\n\u03b3scope = 0.2\n0.3195\n0.8431\n\u03b3scope = 0.25\n0.3204\n0.8552\n\u03b3scope = 0.25, binary\n0.294\n0.8567\n\u03b3scope = 0.3\n0.3129\n0.8578\n\u03b3scope = 0.4\n0.3023\n0.8623\n\u03b3scope = 0.5\n0.285\n0.8654\nTable 2. Ablation of different \u03b3scope, which denotes the influence\nscope of the given subject in RealCustom during inference. \u201cbi-\nnary\u201d means using binary masks instead of max norm in Eq. 11.\nFigure 7. Visualization of different influence scope.\nID\nsettings\nCLIP-T \u2191\nCLIP-I \u2191\n1\nfull model, \u03b3num = 0.8\n0.3204\n0.8552\n2\nw/o adaptive scoring module\n0.3002\n0.8221\n3\ntextual score only, \u03b3num = 0.8\n0.313\n0.8335\n4\nvisual score only, \u03b3num = 0.8\n0.2898\n0.802\n5\n(textual + visual) / 2, \u03b3num = 0.8\n0.3156\n0.8302\n6\nfull model, \u03b3num = 0.9\n0.315\n0.8541\n7\nfull model, \u03b3num = 0.7\n0.3202\n0.8307\nTable 3. Ablation of the adaptive scoring module, where \u03b3num\nmeans the influence quantity of the given subject during inference.\ning other subject-irrelevant parts (e.g., the city background)\ncompletely controlled by the given text.\nWe then ablate on the Top-K raito \u03b3scope in Tab. 2: (1)\nwithin a proper range (experimentally, \u03b3scope \u2208 [0.2, 0.4])\nthe results are quite robust; (2) the maximum normalization\nin Eq. 11 is important for the unity of high similarity and\ncontrollability, since different regions in the selected parts\nhave different subject relevance and should be set to differ-\nent weights. (3) Too small or too large influence scope will\ndegrade similarity or controllability, respectively.\nThese\nconclusions are validated by the visualization in Fig. 7.\nEffectiveness of adaptive scoring module. As shown in\nTab. 3, (1) We first compare with the simple use of all image\nfeatures (ID-2), which results in degradation of both sim-\nilarity and controllability, proving the importance of pro-\nviding accurate and smooth influence quantity along with\nthe coarse-to-fine diffusion generation process; (2) We then\nablate on the module design (ID-3,4,5, ID-5), finding that\nusing image score only results in worse performance. The\nreason is that the generation features are noisy at the begin-\nning, resulting in an inaccurate score prediction. Therefore,\nwe propose a step-scheduler to adaptively fuse text and im-\nage scores, leading to the best performance; (3) Finally, the\nchoice of influence quantity \u03b3num is ablated in ID-6 & 7.\n5. Conclusion\nIn this paper, we present a novel customization paradigm\nRealCustom that, for the first time, disentangles similar-\nity of given subjects from controllability of given text by\nprecisely limiting subject influence to relevant parts, which\ngradually narrowing the real word from its general conno-\ntation to the specific subject in a novel \u201ctrain-inference\u201d\nframework: the adaptive scoring module learns to adap-\ntively modulate influence quantity during training; (2) the\nadaptive mask guidance strategy iteratively updates the in-\n8\nfluence scope and influence quantity of given subjects dur-\ning inference. Extensive experiments demonstrate that Re-\nalCustom achieves the unity of high-quality similarity and\ncontrollability in the real-time open-domain scenario.\nReferences\n[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel\nCohen-Or. A neural space-time representation for text-to-\nimage personalization.\narXiv preprint arXiv:2305.15391,\n2023. 2, 3\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 3, 5\n[3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 6\n[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 3\n[6] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 1, 3\n[7] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, Mengqi\nHuang, Yongdong Zhang, and Zhendong Mao. Dreamiden-\ntity: Improved editability for efficient face-identity preserved\nimage generation. arXiv preprint arXiv:2307.00300, 2023.\n1, 3\n[8] Giannis Daras and Alexandros G Dimakis. Multiresolution\ntextual inversion. arXiv preprint arXiv:2211.17115, 2022. 2,\n3\n[9] Ziyi\nDong,\nPengxu\nWei,\nand\nLiang\nLin.\nDrea-\nmartist: Towards controllable one-shot text-to-image gen-\neration via contrastive prompt-tuning.\narXiv preprint\narXiv:2211.11337, 2022.\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2, 3, 6, 7\n[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Designing an encoder\nfor fast personalization of text-to-image models.\narXiv\npreprint arXiv:2302.12228, 2023. 2, 3\n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[14] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 3\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 6\n[16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2, 3, 6, 7\n[17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. Interna-\ntional Journal of Computer Vision, 128(7):1956\u20131981, 2020.\n2\n[18] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 2, 3, 6, 7\n[19] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva.\nDivide & bind your attention for improved generative seman-\ntic nursing. arXiv preprint arXiv:2307.10864, 2023. 3\n[20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 3\n[21] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yan-\nfeng Wang, and Weidi Xie.\nGuiding text-to-image diffu-\nsion model towards grounded generation.\narXiv preprint\narXiv:2301.05221, 2023. 3\n[22] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai\nZhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang\nCao. Cones 2: Customizable image synthesis with multiple\nsubjects. arXiv preprint arXiv:2305.19327, 2023. 2, 3\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 4, 5\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 3\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 3, 4\n9\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 4\n[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3, 6, 7\n[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 3\n[29] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 6\n[30] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2,\n3\n[31] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3, 6\n[32] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman.\np+: Extended textual conditioning in text-to-\nimage generation. arXiv preprint arXiv:2303.09522, 2023.\n2, 3\n[33] Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin\nZhou, Qian Yu, Lu Sheng, and Dong Xu. Diffusion model is\nsecretly a training-free open vocabulary semantic segmenter.\narXiv preprint arXiv:2309.02773, 2023. 3\n[34] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 6,\n7\n[35] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou,\nand Chunhua Shen. Diffumask: Synthesizing images with\npixel-level annotations for semantic segmentation using dif-\nfusion models. arXiv preprint arXiv:2303.11681, 2023. 3\n[36] Changming Xiao, Qi Yang, Feng Zhou, and Changshui\nZhang.\nFrom text to mask: Localizing entities using the\nattention of text-to-image diffusion models. arXiv preprint\narXiv:2309.04109, 2023. 3\n[37] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation.\narXiv preprint arXiv:2304.05977,\n2023. 6, 11, 12\n[38] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang,\nHaibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver\nDeussen, and Changsheng Xu. Prospect: Expanded condi-\ntioning for the personalization of attribute-aware image gen-\neration. arXiv preprint arXiv:2305.16225, 2023. 2\n10\n6. Supplementary\n6.1. More Qualitative Comparison\nAs shown in Fig. 8, we provide more qualitative compari-\nson between our proposed RealCustom and recent state-of-\nthe-art methods of previous pseudo-word paradigm in the\nreal-time customization scenario. Compared with existing\nstate-of-the-arts, we could draw the following conclusions:\n(1) better similarity with the given subjects and better\ncontrollability with the given text at the same time, e.g., in\nthe 7th row, the toy generated by RealCustom exactly on the\nGreat Wall while existing works fail to adhere to the given\ntext. Meanwhile, the toy generated by RealCustom exactly\nmimics all details of the given one while existing works fail\nto preserve them. (2) better image quality, i.e., with bet-\nter aesthetic scores, e.g., the snow scene in the second row,\nthe dirt road scene in the third row, etc. The conclusion\nadheres to our significant improvement (223.5% improve-\nment) on ImageReward [37] in the main paper since Im-\nageReward evaluates both controllability and image qual-\nity. (3) better generalization in open domain, i.e., for any\ngiven subjects, RealCustom could generate realistic images\nthat consistently adhere to the given text for the given sub-\njects in real-time, including the common subject like dogs\n(e.g., 5th, 6th rows) and rare subjects like the unique back-\npack (i.e., 1st row), while existing state-of-the-arts works\npoorly on the rare subjects like the backpack in the first\nrow, the special toy in the last row, etc. The reason lies that\nfor the very first time, our proposed RealCustom progres-\nsively narrows a real text word from its initial general con-\nnotation into the unique subject, which completely get rid\nof the necessary corresponding between given subjects and\nlearned pseudo-words, and therefore is no longer confined\nto be trained on object-datasets with limited categories.\n6.2. More Visualization\nWe provide more comprehensive visualization of the nar-\nrowing down process of the real word of our proposed Re-\nalCustom in Fig. 9 and Fig. 10. Here, we provide four\ncustomization cases that with the same given text \u201ca toy in\nthe desert\u201d and four different given subjects. The real text\nword used for narrowing is \u201ctoy\u201d. The mask is visualized by\nthe Top-25% highest attention score regions of the real text\nword \u201ctoy\u201d. We visualize all the masks in the total 50 DDIM\nsampling steps. We could observe that the mask of the \u201ctoy\u201d\ngradually being smoothly and accurately narrowed into the\nspecific given subject. Meanwhile, even in these subject-\nrelevant parts (Top-25% highest attention score regions of\nthe real text word \u201ctoy\u201d in these cases), their relevance is\nalso different, e.g., in Fig. 9, the more important parts like\nthe eyes of the first subject are given higher weight (brighter\nin the mask), in Fig. 10, the more important parts like the\neyes of the second subject are given higher weight.\n6.3. Impact of Different Real Word\nThe customization results in using different real text words\nare shown in Fig. 11. The real text word narrowed down\nfor customization is highlighted in red. We could draw the\nfollowing conclusions: (1) The customization results of our\nproposed RealCustom are quite robust, i.e., no matter we\nuse how coarse-grained text word to represent the given\nsubject, the generated subject in the customization results\nare always almost identical to the given subjects. For exam-\nple, in the upper three rows, when we use \u201ccorgi\u201d, \u201cdog\u201d or\n\u201canimal\u201d to customize the given subject, the results all con-\nsistently adhere to the given subject. This phenomenon also\nvalidates the generalization and robustness of our proposed\nnew paradigm RealCustom. (2) When using completely dif-\nferent word to represent the given subject, e.g., use \u201cparrot\u201d\nto represent a corgi, our proposed RealCustom opens a door\nfor a new application, i.e., novel concept creation. That\nis, RealCustom will try to combine these two concepts and\ncreate a new one, e.g., generating a parrot with the appear-\nance and character of the given brown corgi, as shown in the\nbelow three rows. This application will be very valuable for\ndesigning new characters in movies or games, etc.\n11\nFigure 8. Qualitative comparison between our proposed RealCustom and recent state-of-the-art methods of previous pseudo-word paradigm\nin the real-time customization scenario. We could conclude that (1) compared with existing state-of-the-arts, RealCustom shows much\nbetter similarity with the given subjects and better controllability with the given text at the same time, e.g., in the 7th row, the toy\ngenerated by RealCustom exactly on the Great Wall while existing works fail to adhere to the given text. Meanwhile, the toy generated by\nRealCustom exactly mimics all details of the given one while existing works fail to preserve them. (2) RealCustom generates customization\nimages with much better quality, i.e., better aesthetic scores, e.g., the snow scene in the second row, the dirt road scene in the third\nrow, etc. The conclusion adheres to our significant improvement (223.5% improvement) on ImageReward [37] in the main paper since\nImageReward evaluates both controllability and image quality. (3) RealCustom shows better generalization in open domain, i.e., for any\ngiven subjects, RealCustom could generate realistic images that consistently adhere to the given text for the given subjects in real-time,\nincluding the common subject like dogs (e.g., 5th, 6th rows) and rare subjects like the unique backpack (i.e., 1st row), while existing\nstate-of-the-arts works poorly on the rare subjects like the backpack in the first row, the special toy in the last row, etc.\n12\nFigure 9. Illustration of gradually narrowing the real words into the given subjects. Here we provide two customization cases that with the\nsame given text \u201ca toy in the desert\u201d and two different given subjects. The real text word used for narrowing is \u201ctoy\u201d. The mask is visualized\nby the Top-25% highest attention score regions of the real text word \u201ctoy\u201d. We visualize all the masks in the total 50 DDIM sampling\nsteps, which are shown on the left. We could observe that the mask of the \u201ctoy\u201d gradually being smoothly and accurately narrowed into\nthe specific given subject. Meanwhile, even in these subject-relevant parts (Top-25% highest attention score regions of the real text word\n\u201ctoy\u201d in these cases), their relevance is also different, e.g., the more important parts like the eyes of the first subject are given higher weight\n(brighter in the mask).\n13\nFigure 10. Illustration of gradually narrowing the real words into the given subjects. Here we provide two customization cases that with the\nsame given text \u201ca toy in the desert\u201d and two different given subjects. The real text word used for narrowing is \u201ctoy\u201d. The mask is visualized\nby the Top-25% highest attention score regions of the real text word \u201ctoy\u201d. We visualize all the masks in the total 50 DDIM sampling\nsteps, which are shown on the left. We could observe that the mask of the \u201ctoy\u201d gradually being smoothly and accurately narrowed into the\nspecific given subject. Meanwhile, even in these subject-relevant parts (Top-25% highest attention score regions of the real text word \u201ctoy\u201d\nin these cases), their relevance is also different, e.g., the more important parts like the eyes of the second subject are given higher weight\n(brighter in the mask).\n14\nFigure 11. The customization results in using different real text words. The real text word narrowed down for customization is highlighted\nin red. We could draw the following conclusions: (1) The customization results of our proposed RealCustom are quite robust, i.e., no\nmatter we use how coarse-grained text word to represent the given subject, the generated subject in the customization results are always\nalmost identical to the given subjects. For example, in the upper three rows, when we use \u201ccorgi\u201d, \u201cdog\u201d or \u201canimal\u201d to customize the\ngiven subject, the results all consistently adhere to the given subject. This phenomenon also validates the generalization and robustness\nof our proposed new paradigm RealCustom. (2) When using completely different word to represent the given subject, e.g., use \u201cparrot\u201d to\nrepresent a corgi, our proposed RealCustom opens a door for a new application, i.e., novel concept creation. That is, RealCustom will try\nto combine these two concepts and create a new one, e.g., generating a parrot with the appearance and character of the given brown corgi,\nas shown in the below three rows. This application will be very valuable for designing new characters in movies or games, etc.\n15\n"
  }
]