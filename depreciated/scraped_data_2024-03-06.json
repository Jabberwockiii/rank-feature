[
  {
    "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
    "link": "https://arxiv.org/pdf/2403.03163.pdf",
    "upvote": "89",
    "text": "Design2Code: How Far Are We From\nAutomating Front-End Engineering?\nChenglei Si\u2217\nStanford University\nclsi@stanford.edu\nYanzhe Zhang\u2217\nGeorgia Tech\nz_yanzhe@gatech.edu\nZhengyuan Yang\nMicrosoft\nRuibo Liu\nGoogle DeepMind\nDiyi Yang\nStanford University\n\u2217 Equal Contribution\nProject Page: https://salt-nlp.github.io/Design2Code/\nAbstract\nGenerative AI has made rapid advancements in recent years, achieving unprece-\ndented capabilities in multimodal understanding and code generation. This can\nenable a new paradigm of front-end development, in which multimodal LLMs\nmight directly convert visual designs into code implementations. In this work, we\nformalize this as a Design2Code task and conduct comprehensive benchmarking.\nSpecifically, we manually curate a benchmark of 484 diverse real-world webpages\nas test cases and develop a set of automatic evaluation metrics to assess how well\ncurrent multimodal LLMs can generate the code implementations that directly\nrender into the given reference webpages, given the screenshots as input. We also\ncomplement automatic metrics with comprehensive human evaluations. We develop\na suite of multimodal prompting methods and show their effectiveness on GPT-4V\nand Gemini Pro Vision. We further finetune an open-source Design2Code-18B\nmodel that successfully matches the performance of Gemini Pro Vision. Both\nhuman evaluation and automatic metrics show that GPT-4V performs the best on\nthis task compared to other models. Moreover, annotators think GPT-4V generated\nwebpages can replace the original reference webpages in 49% of cases in terms of\nvisual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V\ngenerated webpages are considered better than the original reference webpages.\nOur fine-grained break-down metrics indicate that open-source models mostly lag\nin recalling visual elements from the input webpages and in generating correct\nlayout designs, while aspects like text content and coloring can be drastically\nimproved with proper finetuning.\n1\nIntroduction\nImplementing visual designs of websites into functional code is a challenging task as it requires\nunderstanding visual elements and their layouts and then translating them into structured code. Such\ndependencies on sophisticated skills have prevented many laypeople from building their own web\napplications, even when they have concrete ideas for what to build or design. Furthermore, the\nrequirement for domain expertise complicates the whole webpage production pipeline, requiring\ncollaboration among people with different skill sets and potentially causing discrepancies between the\nintended design and actual implementation. Effective automatic generation of functional code from\narXiv:2403.03163v1  [cs.CL]  5 Mar 2024\nFigure 1: Examples from the prior WebSight dataset (first row) and our new Design2Code benchmark\n(last two rows). We use real-world webpages for benchmarking to ensure they are realistic and\ndiverse, while WebSight uses synthetically generated webpages for scalability.\nvisual designs has the potential to democratize the development of front-end web applications [Nguyen\nand Csallner, 2015], allowing non-experts to build applications easily and quickly.\nWhile code generation from natural language has advanced rapidly in recent years [Yin and Neubig,\n2017, Le et al., 2020, Li et al., 2023b], generating code implementation from user interface (UI)\ndesign has not received much attention due to a wide range of challenges, such as diversity in visual\nand text signals on the user interface and the vast search space in the resulting code. Beltramelli\n[2018] made a notable attempt back in 2017 with CNN and RNN models on a narrow set of simplistic\nuser interface designs. Over the years, despite many follow-up attempts along this quest [Robinson,\n2019, Soselia et al., 2023], they are all constrained to simplistic or synthetic examples with a narrow\nset of layout designs, hardly useful for real-world front-end development applications. Until recently,\nthe development of multimodal LLMs has entered a new era where large-scale pretrained models\ncan process both visual and text input and generate text output for various visually grounded tasks,\nwith representative examples being Flamingo [Alayrac et al., 2022], GPT-4V [OpenAI, 2023], and\nGemini [Google, 2023]. Such advancement has unlocked a brand new paradigm for this long-standing\nunsolved task 1: Take a screenshot of the user\u2019s website design and give this image to the system to\nobtain the full code implementation that can render into the desired webpage in a fully end-to-end\nmanner. We term this task as Design2Code and tackle it with current multimodal models we have in\nour toolbox to benchmark and understand how far we are from automating front-end engineering.\nToward systematic and rigorous benchmarking, we construct the first-ever real-world benchmark for\nDesign2Code (examples in Figure 1). To best reflect realistic use cases, we use real-world webpages\nin the wild as our test examples rather than synthetically generated ones as in prior works [Soselia\net al., 2023, Huggingface, 2024]. We scrape webpages in the C4 [Raffel et al., 2019] validation set and\nperform careful manual curation over all examples to obtain a set of 484 high-quality, challenging, and\ndiverse webpages representing a variety of real-world use cases with different level of complexities.\nWe show both quantitatively and qualitatively that our benchmark covers a wide spectrum of HTML\n1Greg Brockman\u2019s presentation during the GPT-4 release:\nhttps://www.youtube.com/live/\noutcGtbnMuQ?si=5Yge32m5mnB85r4E&t=980\n2\ntag uses, domains, and complexity levels. To facilitate efficient evaluation and model development, we\nalso develop automatic metrics for this task that compare the generated webpage\u2019s screenshot with the\ngiven screenshot input. Our metrics consider a comprehensive set of dimensions including bounding\nbox matches, text content, position, and color of all matched visual elements on the webpages, which\nwe later show highly correlate with human judgment.\nWe then investigate how current multimodal LLMs like GPT-4V and Gemini perform on this task.\nTo elicit their best capabilities, we introduce a variety of prompting methods, including our text-\naugmented prompting that complements visual input with extracted text elements from the webpage\nto reduce the load on OCR, as well as a self-revision prompting method that asks the model to\ncompare its previous generation and the input webpage for self-improvement. We see consistent\nimprovement from the text-augmented prompting method as compared to direct prompting on both\nGPT-4V and Gemini Pro, while only observing positive effect of self-revision on GPT-4V.\nDespite demonstrating state-of-the-art performance, these commercial models are black boxes with\nlimited transparency. To this end, we contribute an open-source 18B finetuned model for this\ntask: Design2Code-18B. Concretely, we build upon the state-of-the-art open-source model CogA-\ngent [Hong et al., 2023] and finetune it with synthetically generated Design2Code data [Huggingface,\n2024]. Surprisingly, this \u201csmall\u201d open-source model performs competitively on our benchmark,\nmatching the performance of Gemini Pro Vision despite the discrepancy between synthetic training\ndata and realistic testing data (an overview of automatic evaluation results is in Figure 3), indicating\nthe potential of specialized \u201csmall\u201d open models and skill acquisition from synthetic data.\nTo summarize, our contributions in this work include:\n1. Formalize the Design2Code task and construct the manually curated Design2Code bench-\nmark with 484 diverse real-world test examples.\n2. Develop a comprehensive suite of automatic evaluation metrics that capture both high-level\nvisual similarity and low-level element matching, which complement the human evaluation.\n3. Propose new multimodal prompting methods that improve over direct prompting baselines.\n4. Finetune our open-source Design2Code-18B model that matches the performance of Gem-\nini Pro Vision as judged by both human and automatic evaluation.\n2\nThe Design2Code Benchmark\nIn this section, we describe the curation and processing of our benchmark data. We first scrape all\nwebsite links in the C4 [Raffel et al., 2019] validation set. We then embed all CSS code into the\nHTML file to obtain one single code implementation file for each webpage. This results in a total of\n127.9k webpages, which we perform further filtering and processing as described below.\n2.1\nTest Set Curation\nOur overall goal is to obtain a set of well-formed webpages that represent diverse real-world use\ncases. We follow the following steps for automatic processing and manual filtering.\nAutomatic Length and Layout Filtering\nWe first apply a round of automatic filtering. We strip all\ncomments from the code files and then apply a length filter to exclude examples where the source\ncode file has over 100k tokens (based on the GPT-2 tokenizer), as a way to avoid excessively long\nwebpages that current multimodal LLMs cannot process as input or cannot decode such long outputs.\nNext, we filter all webpages whose layout consists of only images or only texts, in which cases\nthe layout designs tend to be too simplistic to be interesting for benchmarking. This results in 14k\nwebpages after filtering and deduplication.\nMaking Webpages Stand-alone\nWe assume a setting where we will only provide the screen-\nshot of the webpage for the model, without providing all the external dependencies such as\nmultimedia files (images, audio, videos, etc.).\nTo make this possible, we strip all such ex-\nternal file dependencies to make all the webpages stand-alone, this includes: removing all\n<script><audio><iframe><map><svg> tags, removing all <link> tags that link to external sites,\nremoving all href links in <a> tags, and removing all external files in <object> elements. For all the\n3\nWebSight (Huggingface)\nDesign2Code (Ours)\nPurpose\nTraining\nTesting\nSource\nSynthetic (Deepseek-Coder)\nReal-World (C4)\nSize\n823K\n484\nAvg Length (tokens)\n647\u00b1216\n31216\u00b123902\nAvg Tag Count\n19\u00b18\n158\u00b1100\nAvg DOM Depth\n5\u00b11\n13\u00b15\nAvg Unique Tags\n10\u00b13\n22\u00b16\nTable 1: Comparison of datasets statistics between the WebSight dataset and our new Design2Code\nbenchmark. WebSight only provides the training set while Design2Code only provides the test set.\nExamples in our Design2Code benchmark are much more complex on all measures and have a wider\nvariety of difficulty levels as indicated by the bigger standard deviations.\nimage and video files, we replace them with a placeholder file, and during benchmarking we will\ninstruct the models to insert this placeholder file wherever applicable to preserve the original layout.\nManual Curation\nAfter the above processing, we perform a final round of manual curation to filter\nexamples based on the following criteria: (1) The webpage has no external file dependency and can\nrender in a stand-alone manner from the processed code file and provided placeholder image file.\n(2) The webpage does not contain any private, sensitive, or potentially harmful information (e.g.,\nwe removed profile pages from dating websites). (3) The rendered webpage is well-formatted (e.g.,\nthere should not be overlaps between different layout elements and the automatic processing above\nshould not disrupt any part of the webpage design). The first two authors of this paper performed this\ncuration step by checking every single example from the sampled 7k examples. They first annotated\n200 examples together to reach an 75% agreement, then split the annotation work on 7k randomly\nsampled examples from the filtered set of 14k examples above. This entire manual curation process\ntook approximately one week. We tend to be more aggressive in the manual filtering process to only\nkeep high-quality webpages with a variety of HTML and CSS elements involved. In the end, we\nobtained 484 test examples that we use as our benchmark.\n2.2\nData Statistics and Diversity\nQuantitative Metrics\nTo provide an estimate of the difficulty levels of the test examples, we\nprovide some quantitative measures. We compare with the most recent and most similar existing\ndataset \u2013 WebSight [Huggingface, 2024] in Table 1. (1) Length: We tokenize the scraped code files\nwith the GPT-2 tokenizer. The average number of tokens per file is 31215.6 (min=784, max=98637,\nstd=23902.9). This is much longer than WebSight the typical max output length of modern language\nmodels, posing a unique challenge (although we note that the code files here are only for the reference\nimplementation, there can be much more succinct ways to reproduce the given webpages). (2) Total\nnumber of tags: We count the total number of HTML tags involved, which is 158.3 on average\n(min=12, max=528, std=100.4). The examples in our benchmark cover 84 types of standard HTML5\ntags. We present a chart of the most frequently used tags in Table 5 (Appendix A). (3) DOM tree\ndepth: We measure the depth of the Document Object Model (DOM) tree as another measure of\ncomplexity. The average depth is 12.5 (min=4, max=32, std=4.7). (4) Number of unique tags:\nLastly, we compute the number of unique HTML tags in each example, and the mean is 22.2 (min=8,\nmax=45, std=6.0), suggesting that our benchmark covers a wide range of HTML tags. Overall, the\nexamples in our benchmark are much more challenging and cover a wider spectrum of complexities\nthan prior efforts like WebSight.\nDomain Distribution\nTo get a sense of the range of domains covered in our benchmark, we\nrandomly sample 25% examples (N=120) from the benchmark and manually annotate what type of\nwebpages they are based on their functions. We present the pie chart of the most frequent domains in\nFigure 2. The most prominent genres are websites of companies or organizations, personal blogs\n(including technical blogs), and personal homepages. Other genres include information-sharing sites\n(e.g., Wikipedia pages, FAQ pages, tax policy pages, online dictionaries), online forums, news article\npages, and product description pages. Sampled examples are shown in Figure 1.\n4\nproduct\n4.8%\nblog\n25.3%\ncompany/org\n27.7%\nhomepage\n13.3%\nnews\n4.8%\nforum\n3.6%\ninformation\n10.8%\nothers\n9.6%\nFigure 2: Main topics of the webpages in the\nDesign2Code benchmark.\nBlock-Match\nText\nPosition\nColor\nCLIP\n64.0\n78.0\nGPT-4V (Self-Revision)\nGemini Pro Vison (Text-Augmented)\nWebsight VLM-8B\nDesign2Code-18B\n88.0\n94.0\n76.0\n82.0\n70.0\n80.0\n83.0\n86.0\nFigure 3: Performance of the benchmarked models\nbroken down by our fine-grained metrics.\n2.3\nAutomatic Metrics\nPreviously, generated HTML codes are usually evaluated by text-based similarity metrics, such\nas Normalized Edit Distance [Lv et al., 2023] and htmlBLEU [Soselia et al., 2023]. However,\nsuch metrics cannot directly assess whether the visual design of the original screenshot is correctly\ngenerated as there can be many different ways of implementing the same webpage, and minor\ndifferences in generated code could result in major visual differences in the rendered output. To this\nend, we propose to automatically evaluate generated webpages by calculating the similarity between\nthe screenshots of reference webpages IR and the rendered screenshots of generated webpages IG.\nWe break down the evaluation into both high-level visual similarity and low-level element matching.\nHigh-level Visual Similarity\nTo evaluate the visual similarity of IR and IG, we use the similarity\nof their CLIP [Radford et al., 2021] embedding, denoted as CLIP(IR, IG). Specifically, we extract\nfeatures by CLIP-ViT-B/32 after resizing screenshots to squares. To rule out the texts in the\nscreenshots, we use the inpainting algorithm from Telea [2004] to mask all detected text boxes using\ntheir bounding box coordinates. 2\nLow-level Element Matching\nMetrics like CLIP similarity only capture the similarity of the overall\nimages rather than the matching of all the details like text. Moreover, the metric itself does not offer\nany fine-grained breakdown to help diagnose model weaknesses. To complement that, we introduce\na suite of element-matching metrics. Specifically, we consider whether the generated webpages\nmanage to recall all visual elements, and whether the corresponding visual elements in the reference\nand generated webpages have aligned text content, position, and color.\nGiven a reference webpage screenshot IR and a generated webpage screenshot IG, we use a text\ndetection module to output a set of detected visual element blocks for each: R = {r1, r2, ..., rm} and\nG = {g1, g2, ..., gn}, where each block contains its textual content and bounding box coordinates.\nSee Appendix B for the details of implementing the block detection module. Based on the two sets of\ndetected blocks, we use the Jonker-Volgenant algorithm [Crouse, 2016] to get the optimal matching\nM between R and G based on text similarity, where (p, q) \u2208 M indicates rp is matched with gq.\nGiven R, G, and matched pairs in M, we evaluate similarity along the following aspects:\n\u2022 Block-Match: The first desideratum of the task is that all visual elements from the reference\nwebpage should be reproduced in the generated webpage, and the generated webpage should\nnot hallucinate non-existent new elements. We measure this by computing the total sizes of\nall matched blocks divided by the total sizes of all blocks, including unmatched ones (either\nbecause the generated webpages missed them or because the generated webpages contain\nhallucinated blocks):\n2https://docs.opencv.org/4.3.0/df/d3d/tutorial_py_inpainting.html\n5\nmatchblock(rp, gq) =\nS(rp) + S(gq)\nP\n(i,j)\u2208M(S(ri) + S(gj)) + (P\ni\u2208UR S(ri) + P\nj\u2208UG S(gj)),\nmatchblock(R, G) =\nX\n(p,q)\u2208M\nmatchblock(rp, gq),\nwhere S(\u00b7) returns the size of the blocks, UR and UG denotes the unmatched blocks in R\nand G. The intuition here is that unmatched blocks will lower the score as they indicate\nmissing original blocks or generating hallucinated blocks, and the larger the unmatched\nblocks are, the lower this score is.\n\u2022 Text:\nGiven two strings from two matched blocks rp and gq, the text similarity\nsimtext(rp, gq) is calculated as twice the number of overlapping characters divided by\nthe total number of characters in the two strings (character-level S\u00f8rensen-Dice similarity).\nThe overall score is averaged across all matched pairs.\n\u2022 Position: The positioning of the blocks largely impacts the overall layout. For each matched\npair (p, q), we calculate the position similarity simpos(rp, gq) = 1 \u2212 max(abs(xq \u2212\nxp), abs(yq \u2212 yp)), where (xp, yp) and (xq, yq) are normalized coordinates (in [0, 1]) of rp\nand gq\u2019s centors. The overall score is averaged across all matched pairs.\n\u2022 Color: We use the CIEDE2000 color difference formula [Luo et al., 2001] to assess the\nperceptual difference between the colors of the generated text in block gq and the reference\ntext in block rp, denoted as simcolor(rp, gq)), where the formula considers the complexities\nof human color vision. The overall score is averaged across all matched pairs.\nNote that we intentionally do not compute an aggregate score over these different dimensions because\nthey are designed as fine-grained diagnostic scores. Ideally, models and methods should score well\nalong all these dimensions.\n3\nBenchmarking: Prompting and Finetuning\nWe benchmark a variety of models and methods to compare their performance on our benchmark,\nincluding both prompting commercial API models and finetuning open-source models.\n3.1\nMultimodal Prompting Methods\nAs per modern deep learning practice, we resort to prompting commercial LLMs as the first set of\nbaselines. We develop a suite of multimodal prompting methods for our benchmark. We assume\naccess to a model that can take both image input and text prompts and then produce code as output.\nWe will experiment with GPT-4V [OpenAI, 2023] and Gemini Pro Vision [Google, 2023] as the two\nbest-performing publicly available APIs.\nDirect Prompting\nWe start with the most simple direct prompting baseline, We provide the\nreference webpage screenshot, along with the following instruction:\nYou are an expert web developer who specializes in HTML and CSS. A user will provide\nyou with a screenshot of a webpage.\nYou need to return a single html file that uses\nHTML and CSS to reproduce the given website.\nInclude all CSS code in the HTML file\nitself.\nIf it involves any images, use \"rick.jpg\" as the placeholder.\nSome images\non the webpage are replaced with a blue rectangle as the placeholder, use \"rick.jpg\"\nfor those as well.\nDo not hallucinate any dependencies to external files.\nYou\ndo not need to include JavaScript scripts for dynamic interactions.\nPay attention\nto things like size, text, position, and color of all the elements, as well as the\noverall layout.\nRespond with the content of the HTML+CSS file.\nText-Augmented Prompting\nThe above prompt asks the model to do everything at once: recognize\nall the text and layout elements and generate the corresponding code. In reality, users often have an\nidea of what content they want to put on their webpage. Instead, they are only looking for expertise\n6\nin converting the design into code implementation. To reflect such a setting, we also explore a\ntext-augmented prompting method, where we extract all text elements from the original webpage\nfirst 3 and append these texts after the instruction prompt along with the screenshot input. In this\nsetting, we mitigate the difficulty of performing OCR and instead allow the model to focus more on\nlayout design, where the model could copy text content from the prompt and insert it into the correct\npositions.\nSelf-Revision Prompting\nInspired by recent works on using LLMs to self-improve their own\ngenerations [Madaan et al., 2023, Shinn et al., 2023], we also develop a self-revision prompt where\nwe provide the following as input: (1) the screenshot of the input webpage, (2) the screenshot of the\ngenerated webpage from text-augmented prompting, (3) the generated code from text-augmented\nprompting as the initial solution; then we ask the model to improve the generated implementation\ncode so that the result can look closer to the reference webpage (full prompt is in Appendix C).\nWe use the same prompts for both GPT-4V and Gemini Pro Vision, and we use the high-resolution\nmode with max output tokens 4096 and temperature 0.0 for all generations.\n3.2\nFinetuning Design2Code-18B\nWhile commercial API models are performant and easy to use, they are opaque and limited in\ntransparency. To enable open-source alternatives, we finetune an open-source model for this task and\ncompare it with commercial API models.\nBase Model\nWe use CogAgent-18B Hong et al. [2023] as our base model, which supports high-\nresolution input (1120 \u00d7 1120) and is pretrained on intensive text-image pairs [Byeon et al., 2022,\nSchuhmann et al., 2022], synthetic documents [Kim et al., 2022], LaTeX papers [Blecher et al., 2023],\nand a small amount of website data.\nTraining Data\nWe finetune the base model with the recently released Huggingface WebSight\ndataset 4, which consists of website screenshot and code implementation pairs. The dataset is\ngenerated in two steps: (i) Generate random website ideas from Mistral-7B-v0.1 [Jiang et al.,\n2023]. (ii) Prompt Deepseek-Coder-33b-Instruct [Guo et al., 2024a] with the generated ideas\nto generate a simple and short website. While the original WebSight dataset has 823K examples, we\nonly randomly sample 20% for training due to the limited computation resources. We also reverse the\norder of HTML style and body as we find that it lead to a lower loss in our preliminary experiment.\nNote that we have also experimented with training on real-world webpage data scraped from the C4\ntraining set. Such training is extremely unstable and difficult because real-world code implementation\ndata tend to be extremely long and noisy, resulting in even lower performance than training on\nsynthetic data. We thus leave such exploration to future work.\nSettings\nWe use LoRA [Hu et al., 2021] to fine-tune the base model, where the LoRA modules are\nadded to the language decoder with LoRA rank 8. Using a batch size of 32 and a learning rate of\n1e-5, we fine-tune the model for 5000 steps with 100 steps warmup. Using 4\u00d7 NVIDIA A6000, this\ntakes about 2 days of training. We use a temperature of 0.5 and a repetition penalty of 1.1 during\ninference and select the best checkpoint based on the average of all automatic metrics on a small dev\nset (20 examples).\nAdditional Baselines\nApart from our own finetuned Design2Code-18B, we also compare with two\nother open-source baselines. For both baselines, we follow their default sampling settings. First, we\ncompare it with the original CogAgent-18B model so that we can analyze the gains from finetuning.\nWe use the screenshot and a simple prompt Write the HTML code. as the input. Second, we\ncompare with the Huggingface WebSight VLM-8B, which is presumably (at least) finetuned on the\nfull WebSight dataset, where the base models (SigLIP [Zhai et al., 2023] and Mistral-7B [Jiang et al.,\n2023]) are fully finetuned. Note that this is a beta version without any paper release or detailed\ndocumentation of the training details. We include this baseline purely for comprehensiveness despite\nit is not an apple-to-apple comparison with Design2Code-18B given the different base models and\namount of training data.\n3We use the Beautiful Soup library.\n4https://huggingface.co/datasets/HuggingFaceM4/WebSight\n7\nBlock-Match\nText\nPosition\nColor\nCLIP\nGPT-4V\nDirect Prompting\n85.8\n97.4\n80.5\n73.3\n86.9\nText-Augmented Prompting\n87.6\n98.2\n80.2\n73.0\n87.2\nSelf-Revision Prompting\n88.8\n98.1\n81.1\n72.9\n87.2\nGemini Pro Vision\nDirect Prompting\n80.2\n94.6\n72.3\n66.2\n83.9\nText-Augmented Prompting\n84.8\n96.9\n70.4\n66.3\n84.0\nSelf-Revision Prompting\n84.1\n96.6\n70.1\n66.2\n83.7\nOpen-Source Models\nWebSight VLM-8B\n55.9\n86.6\n77.3\n79.4\n86.5\nCogAgent-Chat-18B\n7.1\n18.1\n13.3\n13.0\n75.5\nDesign2Code-18B\n78.5\n96.4\n74.3\n67.0\n85.8\nTable 2: Automatic evaluation results of the four fine-grained similarity measures as well as the\nhigh-level visual similarity with CLIP. The best result per dimension is highlighted in bold. GPT-4V\nis the best on all dimensions apart from color, on which WebSight VLM-8B is leading. Note that\nWebSight VLM-8B is finetuned on 5 times more data than our Design2Code-18B.\n0\n20\n40\n60\n80\n100\nPercentage (%)\nGPT-4V Direct Prompting\nGPT-4V Text-Augmented Prompting\nGPT-4V Self-Revision Prompting\nGemini Self-Revision Prompting\nGemini Text-Augmented Prompting\nWebSight VLM-8B\nDesign2Code-18B\n63%\n11%\n26%\n68%\n14%\n18%\n76%\n9%\n15%\n34%\n26%\n40%\n45%\n16%\n39%\n54%\n11%\n35%\n38%\n25%\n37%\nWin\nTie\nLose\nFigure 4: Human pairwise preference evaluation results with Gemini Pro Vision Direct Prompting as\nthe baseline (this method itself is not shown in the table since it serves as the baseline for pairwise\ncomparison). We sample 100 examples and ask 5 annotators for each pair of comparisons, and we\ntake the majority vote on each example. Higher win rate and lower lose rate suggest best quality as\njudged by human annotators.\n4\nResults: Automatic and Human Evaluation\n4.1\nAutomatic Evaluation\nWe present all automatic evaluation results in Table 2 and Figure 3. Note that the comparisons here\nare by no means fair comparisons, given the differences in model sizes and training data. We compare\nthem as they are the most relevant and accessible baselines for our benchmark. We observe that:\n(1) GPT-4V is the best on all dimensions apart from color, on which WebSight VLM-8B is leading.\n(2) Text-augmented prompting successfully increases the block-match score and text similarity\nscore on both GPT-4V and Gemini Pro Vision, indicating the usefulness of providing extracted text\nelements. (3) Self-revision has some minor improvement on block-match and position similarity for\nGPT-4V, but brings no improvement on Gemini Pro Vision, potentially due to the limited capabilities\nof LLMs to perform intrinsic self-correction without external feedback [Huang et al., 2023]. (4)\n8\ncoef\nstd err\np\nBlock-Match\n2.0442\n0.648\n0.002\nText\n1.3758\n2.217\n0.535\nPosition\n7.8037\n1.312\n0.000\nColor\n2.0731\n0.757\n0.006\nCLIP\n10.2353\n2.855\n0.000\nTable 3:\nCoefficients of predicting human annotations (Win/Lose) using logistic regression on\nautomatic metric\u2019s differences of the same pair. Human evaluation mostly correlates with position\nand CLIP similarity.\nFinetuning achieves huge improvement on all dimensions as indicated by the comparison between\nDesign2Code-18B and the base version CogAgent-18B. (5) Our finetuned Design2Code-18B is\nbetter at block-match and text similarity, but worse at position similarity and color similarity as\ncompared to WebSight VLM-8B. We could potentially attribute the first two to the stronger and larger\nbase model and the latter two to the larger amount of finetuning data. We provide an in-depth analysis\nof the learning process of our finetuned model in Section 5.2.\n4.2\nHuman Evaluation\nWhile the above automatic metrics provide a fine-grained breakdown of model performance, it is\nalso crucial to ask what humans, the ultimate audience of these webpages, think of the generated\nwebpages. By recruiting human annotators (paid at the rate of $16/hour) from Prolific 5, we conducted\na series of human evaluations to compare across models and methods, as well as to directly assess the\nquality of the best-performing model. We sample 100 examples from our benchmark for the human\nevaluations. In all human evaluations, each question is annotated by 5 human annotators, and we\nderive the results by majority voting. We provide all instructions that we provided to annotators in\nAppendix D and we outline the main protocols and results below.\nPairwise Model Comparison\nFollowing the conventional practice of evaluating instruction-\nfollowing LLMs (e.g., [Zhou et al., 2023, Dubois et al., 2023]), we ask human annotators to rank\na pair of generated webpages (one from the baseline, the other from the tested methods) to decide\nwhich one is more similar to the reference. We use Gemini Pro Vision Direct Prompting as the\nbaseline and collect the other seven methods\u2019 Win/Tie/Lose rates against this baseline (we randomly\nshuffle the ordering to avoid position biases). Each pair will count as Win (Lose) only when Win\n(Lose) receives the majority vote (\u2265 3). All other cases are considered Tie.\nBased on the human evaluation in Figure 4, we find that: (1) GPT-4V is substantially better than other\nbaselines, while both text-augmented prompting and self-revision prompting can further improve over\ndirect prompting. (2) Text-augmented prompting can slightly improve the Gemini direct prompting\nbaseline, while further adding self-revision is not helpful. Intuitively, self-revision needs the model to\nunderstand the differences between the two given images (the reference screenshot and the screenshot\nof the initial model generation) and reflect them correspondingly in the modified HTML code, which\nis harder than leveraging text augmentation and thus might require more advanced model capabilities.\n(3) WebSight VLM-8B performs better than Gemini direct prompting (54% win rate and 35% lose\nrate), suggesting that finetuning on a large amount of data can match commercial models in specific\ndomains. (4) Our model Design2Code-18B matches the performance of Gemini Pro Vision direct\nprompting (38% win rate and 37% lose rate).\nDirect Assessment\nWhile the automatic and human evaluation offer a comparison among different\nmodels and methods, readers might still wonder: \u201cHow far are we from automating front-end\nengineering?\u201d To offer a more intuitive answer to this question, we further ask human annotators to\ncompare each reference webpage with the best AI-generated webpage (using GPT-4V self-revision\nprompting). All examples are annotated by 5 annotators, and we take the majority vote. Full\n5We restrict the annotators to people in the U.S. who have completed 2,500 surveys with a pass rate of 98%\nor higher.\n9\ninstructions given to the annotators can be found in Appendix D. Concretely, we perform direct\nassessment from two perspectives:\n1. Can the AI-generated webpage replace the original webpage? We shuffle the ordering\nof all examples and ask annotators to judge whether the two webpages are similar enough in\nterms of appearance and content so that they can be deployed interchangeably. We find that\n49% of the AI-generated webpages are considered exchangeable with the reference\nwebpages.\n2. Is the reference webpage or AI generation better? We then ask a different question,\nwhere we shuffle the example ordering and ask annotators which webpage is better designed\n(annotators do not know which one is the reference and which one is AI-generated). Perhaps\nsurprisingly, webpages generated by GPT-4V are preferred in 64% cases, i.e., they are\nconsidered better designed than even the original reference webpages. We hypothesize\nit is possible that the model has more access to modern and popular webpage design\nprinciples [Ivory and Megraw, 2005, Beaird et al., 2020], such that it can automatically\nimprove the original design based on these best practices. This also opens up many new\nopportunities for future work on website design improvement tools.\n4.3\nAutomatic Evaluation vs Human Evaluation\nIt is worth noting that there are some interesting discrepancies between the automatic evaluation\nresults and human evaluation results. For example, human evaluation ranks GPT-4V self-revision\nprompting better than text-augmented prompting, while the automatic metrics show mixed results.\nMoreover, even though humans rank WebSight VLM-8B as better than Design2Code-18B, it has\nmuch worse block-match and text similarity as measured by the automatic metrics. In this part, we\ntake a closer look at such discrepancy and discuss why such discrepancy is a feature rather than a bug.\nWe study the correlation between the automatic metrics and human pairwise preferences. Specifically,\nwe randomly split 588 pairwise human annotations (Win/Lose only) into a 50% training set and\na 50% test set. Given one reference R and two candidates G1, G2, we use the difference of each\ndimension (e.g., matchblock(R, G1) \u2212 matchblock(R, G2)) as features and predict Win (1) or\nLose (0) by logistic regression. The derived logistic regression model achieves 76.9% accuracy, and\nthe features\u2019 coefficients and significance are in Table 3. We find that text similarity has almost\nno correlation with humans. In contrast, the position similarity of matched blocks and the CLIP\nsimilarity are the two most correlated automatic metrics. This suggests that humans usually pay\nmore attention to high-level visual effects and the layout rather than the detailed content,\nreflecting the top-down processing [Gilbert and Li, 2013] of humans. In summary, we argue\nthat human evaluation should not be blindly trusted as the oracle here due to their cognitive bias to\nonly consider \u201cprinciple components\u201d of the webpages. Instead, both high-level similarity (human\npairwise preference and CLIP similarity) and low-level elements (fine-trained block-wise similarity)\nshould be taken into consideration when evaluating new models and methods, and ideally, they should\nscore well on both fronts.\n5\nAnalysis\n5.1\nWhat Makes A Webpage Difficult?\nTo understand what makes a webpage difficult to generate, we compute the correlation between\nautomatic metrics and various difficulty indicators, including: (1) total number of tags in the reference\nimplementation; (2) number of unique tags in the reference implementation; and (3) DOM tree depth\nof the reference implementation. Table 4 shows that the total number of tags is a strong indicator\nof the difficulty, where webpages with more tags tend to have lower scores along all fine-grained\ndimensions.\n5.2\nWhat is the Learning Process of Different Dimensions?\nWe further plot the learning process of different automatic evaluation dimensions in Figure 5 to help\nus better understand the performance differences in Table 2. Specifically, we show the normalized\nperformance of each aspect (so that 0 before training and 1 after training) for the base model\n10\nTotal Num of Tags\nNum of Unique Tags\nDOM Tree Depth\nMetric\nCorr\nMetric\nCorr\nMetric\nCorr\nBlock-Match\n-0.28*\nBlock-Match\n-0.16*\nBlock-Match\n-0.04\nText\n-0.13*\nText\n-0.08\nText\n0.01\nPosition\n-0.19*\nPosition\n-0.15*\nPosition\n-0.10*\nColor\n-0.13*\nColor\n-0.09\nColor\n-0.04\nCLIP\n-0.12\nCLIP\n-0.02\nCLIP\n0.03\nTable 4: Correlation between automatic metrics and three proxy difficulty indicator variables on\nGPT-4V self-revision prompting. The total number of tags is the strongest indicator, where webpages\nwith more tags tend to be more challenging for the model. * indicates p-value < 0.05.\n0\n1000\n2000\n3000\n4000\n5000\nSteps\n0.4\n0.6\n0.7\n0.8\n0.9\n1.0\nRatio\nBlock-Match\nText\nPosition\nColor\nCLIP\nFigure 5: Learning process for different automatic evaluation dimensions, where we plot the perfor-\nmance for the base model checkpoint and all training checkpoints. For each dimension, the score\nis re-scaled so that it is 0 before training (0 steps) and 1 after training (5000 steps). The y-axis is\nrescaled to highlight the differences for bigger values.\ncheckpoint and all training checkpoints. On the one hand, performance on block-match, text, and\nposition quickly saturate after training for 2000 steps and remain stable afterward, possibly because\nthese are the most transferable capabilities from the base model. On the other hand, the color similarity\nand the CLIP similarity steadily increase until 4000 \u2212 5000 steps. We assume that generating the\ncorrect color codes for texts and backgrounds benefits more from the HTML training data than other\naspects and might be further improved by using the full Websight dataset and fully finetuning.\n5.3\nQualitative Analysis\nWe manually check through examples where our proposed text-augment prompting and self-revision\nprompting achieve improvement on GPT-4V.\nExamples of text-augmented prompting improving over direct prompting\nWe find that text-\naugmented prompting mostly improves over direct prompting by having higher recall in the generated\ncontent, especially texts, as exemplified in Figure 6, where the output from direct prompting misses\nmost of the text contents but text-augmented prompting recovers them, improving the block-match\nscore from 0.25 to 0.84.\nExamples of self-revision prompting improving over text-augmented prompting\nWe next\nanalyze examples where self-revision improves upon the initial generations from text-augmented\nprompting. We find two main sources of improvement. The first example in Figure 7 shows that\ncase where self-revision brings back missing elements from the webpage, increasing the block-match\n11\nReference Webpage\nGPT-4V Direct Prompting\nGPT-4V Text-Augmented Prompting \nFigure 6: Example of text-augmented prompting improving over the direct prompting baseline, where\nmissing texts are successfully generated.\nFigure 7: Examples of self-revision prompting improving over text-augmented prompting. The\nself-revision can either add the missing texts or fix layout errors.\nscore from 0.48 to 1.00, and consequently CLIP similarity from 0.87 to 0.91. The second example in\nFigure 7 shows the case where layout errors are fixed through self-revision, improving the overall\nCLIP similarity from 0.85 to 0.91.\nWebSight VLM-8B vs Design2Code-18B\nWe show a representative example in Figure 8, where\nWebSight VLM-8B is much better in coloring than Design2Code-18B (color score 0.99 vs 0.66) and\noverall layout (position score 0.91 vs 0.63 and CLIP similarity 0.90 vs 0.83). However, WebSight\nVLM-8B tends to hallucinate texts and results in lower block-match (0.85 vs 0.99) and text similarity\nscores (0.98 vs 1.0). In general, we find that WebSight VLM-8B tends to have lower precision and\nrecall than our model in terms of text matching.\n6\nRelated Work\nMultimodal LLMs\nTo enable multimodal understanding and grounded generation, multimodal\nLLMs are usually augmented with an extra encoder to accept multimodal input, prominently visual\n12\nFigure 8: Comparison of WebSight VLM-8B and Design2Code-18B. WebSight VLM-8B excels at\ncolor recognition but hallucinates text contents.\ninput. As a representative example, BLIP-2 [Li et al., 2023a] connects ViT [Dosovitskiy et al., 2020]\nwith large language models with Q-Former. To further improve generalization capability on unseen\ntasks, instruction tuning is introduced to multimodal LLMs, where LLaVA [Liu et al., 2023] generates\ncomplex image-based QA based on prompting GPT-4 [OpenAI, 2023] with COCO captions and\nInstructBLIP [Dai et al., 2023] transform 13 datasets into the same format of instruction-following.\nYe et al. [2023] further scales up the pretraining data while Bai et al. [2023] includes grounding and\nOCR data into the multitask finetuning. While commercial models like GPT-4V have demonstrated\npromising performance on a wide range of vision-language tasks, Yan et al. [2023], Zhang et al.\n[2023], Zheng et al. [2024] adapt them to operate smartphone UIs and websites. Our works offers a\nnew challenging benchmark to assess the capabilities in the realistic front-end engineering task.\nUI Code Generation\nNguyen and Csallner [2015] reverse engineer mobile UI by identifying\nelements through classic text recognition and computer vision techniques (OCR, edge detection, etc)\nand generating code on top of them. Pix2Code [Beltramelli, 2018] builds an end-to-end system for\nUI-to-code transformation based on CNN and RNN, which faces the challenge of complex visual\nencoding and long text decoding while dealing with real-world UIs. Robinson [2019], A\u00b8s\u0131ro\u02d8glu et al.\n[2019] further incorporate neural network-based object detection and semantic segmentation into the\npipeline. Recently, Soselia et al. [2023] utilize advanced visual encoders (e.g., ViT, Dosovitskiy et al.,\n2020) and language decoders (e.g., LLaMA, Touvron et al., 2023a,b) and finetune the pipeline using\nvisual similarity as the signal. However, their training and testing examples mainly contain a small\nnumber of simple elements (e.g., a square, a circle, a button).\nCode LLMs and Programming Support Tools\nOur work also connects to code language mod-\nels and programming support tools. LLMs trained on code, such as Codex [Chen et al., 2021],\nStarCoder Li et al. [2023b], InCoder [Fried et al., 2022], CodeLlama [Rozi\u00e8re et al., 2023], and\nDeepSeek-Coder [Guo et al., 2024b], enable a wave of programming support applications such as\nautomatic code completion and infilling, and allowing users to chat with a codebase 6. This also leads\nto a new wave of HCI studies on how to design better programming tools to facilitate human-AI\ncollaboration [Kalliamvakou, 2022, Vasconcelos et al., 2023, Liang et al., 2023]. Our benchmark\noffers realistic evaluation for code LLMs and aims to enable more powerful programming support to\nfront-end designers who do not have to code by themselves and can just collaborate with LLMs.\n7\nConclusion and Future Work\nIn this work, we introduced the Design2Code benchmark consisting of diverse real-world webpages\nas test examples. We develop comprehensive automatic metrics and conduct a series of human\nevaluations to compare various multimodal code LLMs, showing that finetuned open-source models\ncan match prompting Gemini Pro Vision, but still lag behind GPT-4V. Moreover, human annotators\nfind 49% of the GPT-4V generations to be good enough to replace the original references, while 64%\nare judged as even better designed than the original references.\nWe believe Design2Code can serve as a useful benchmark to power many future research directions.\nWe highlight a few of them:\n6https://github.com/features/copilot\n13\n1. Better prompting techniques for multimodal LLMs, especially in handling complex web-\npages, for example by incrementally generating different parts of the webpage.\n2. Training open multimodal LLMs with real-world webpages. Our preliminary experiments\nshowed the difficulty of directly training on real webpages since they are too long and noisy,\nfuture work could explore data cleaning pipelines to make such training stable.\n3. Extending beyond screenshot inputs, for example, to collect Figma frames or sketch designs\nfrom front-end designers as the test input. Such extension also requires careful re-design of\nthe evaluation paradigm.\n4. Extending from static webpages to also include dynamic webpages. This also requires the\nevaluation to consider interactive functions, beyond just visual similarity.\nEthical Considerations\nPrivacy\nWe used the dataset C4 which is released under ODC-By license, allowing free share,\nmodification, and use subject to the attribution requirements. We release our dataset under the same\nlicense. Moreover, when performing manual filtering, we explicitly filtered out webpages containing\nprivate or sensitive information (e.g., dating website profiles).\nDual Use\nDespite our intention of democratizing webpage building, we recognize the potential\ndual use danger of Design2Code technologies, such as automated generation of malicious websites,\nor even generating code for licensed websites. We emphasize is intended for research purposes and\nfor the community to better understand multimodal LLM capabilities. We will provide clear ethical\nuse guidelines for all data, code, and model releases to define acceptable and unacceptable use cases.\nAcknowledgement\nWe thank Aryaman Arora, Jihyeon Je, Irena Gao, Will Held, Ryan Louie, Weiyan Shi, Dora Zhao,\nRose Wang, Caleb Ziems, Michael Ryan, Camille Harris, Harshit Joshi, Yijia Shao, Jiaao Chen,\nOmar Shaikh, Julie Kallini, Lucia Zheng, Julia Kruk, Tianyu Gao and Tristan Thrush for their helpful\ncomments and discussion.\nReferences\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro,\nJ. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira,\nO. Vinyals, A. Zisserman, and K. Simonyan.\nFlamingo: a visual language model for few-\nshot learning. ArXiv, abs/2204.14198, 2022. URL https://api.semanticscholar.org/\nCorpusID:248476411.\nB. A\u00b8s\u0131ro\u02d8glu, B. R. Mete, E. Y\u0131ld\u0131z, Y. Nal\u00e7akan, A. Sezen, M. Da\u02d8gtekin, and T. Ensari. Automatic\nhtml code generation from mock-up images using machine learning techniques. In 2019 Scientific\nMeeting on Electrical-Electronics & Biomedical Engineering and Computer Science (EBBT),\npages 1\u20134, 2019. doi: 10.1109/EBBT.2019.8741736.\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A versatile\nvision-language model for understanding, localization, text reading, and beyond, 2023.\nJ. Beaird, A. Walker, and J. George. The principles of beautiful web design. SitePoint Pty Ltd, 2020.\nT. Beltramelli. pix2code: Generating code from a graphical user interface screenshot. In Proceedings\nof the ACM SIGCHI Symposium on Engineering Interactive Computing Systems, pages 1\u20136, 2018.\nL. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding for\nacademic documents, 2023.\nM. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset.\nhttps://github.com/kakaobrain/coyo-dataset, 2022.\n14\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,\nS. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such,\nD. W. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol,\nI. Babuschkin, S. Balaji, S. Jain, A. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford,\nM. M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. Mc-\nCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. ArXiv,\nabs/2107.03374, 2021. URL https://api.semanticscholar.org/CorpusID:235755472.\nD. F. Crouse. On implementing 2d rectangular assignment algorithms. IEEE Transactions on\nAerospace and Electronic Systems, 52(4):1679\u20131696, 2016. doi: 10.1109/TAES.2016.140952.\nW. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip:\nTowards general-purpose vision-language models with instruction tuning, 2023.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale, 2020.\nY. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. Hashimoto.\nAlpacafarm: A simulation framework for methods that learn from human feedback. ArXiv,\nabs/2305.14387, 2023. URL https://api.semanticscholar.org/CorpusID:258865545.\nD. Fried, A. Aghajanyan, J. Lin, S. I. Wang, E. Wallace, F. Shi, R. Zhong, W. tau Yih, L. Zettlemoyer,\nand M. Lewis. Incoder: A generative model for code infilling and synthesis. ArXiv, abs/2204.05999,\n2022. URL https://api.semanticscholar.org/CorpusID:248157108.\nC. D. Gilbert and W. Li. Top-down influences on visual processing. Nature Reviews Neuroscience,\n14(5):350\u2013363, 2013.\nGoogle. Gemini: A family of highly capable multimodal models. ArXiv, abs/2312.11805, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:266361876.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming \u2013\nthe rise of code intelligence, 2024a.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming \u2013\nthe rise of code intelligence, 2024b.\nW. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Zhang, J. Li, B. Xu, Y. Dong,\nM. Ding, and J. Tang. Cogagent: A visual language model for gui agents, 2023.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\nadaptation of large language models, 2021.\nJ. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language\nmodels cannot self-correct reasoning yet. ArXiv, abs/2310.01798, 2023. URL https://api.\nsemanticscholar.org/CorpusID:263609132.\nHuggingface.\nHuggingface websight, 2024.\nURL https://huggingface.co/datasets/\nHuggingFaceM4/WebSight.\nM. Y. Ivory and R. Megraw. Evolution of web site design patterns. ACM Transactions on Information\nSystems (TOIS), 23(4):463\u2013497, 2005.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\nE. Kalliamvakou. Quantifying github copilot\u2019s impact on developer productivity and happiness, 2022.\n15\nG. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocr-free\ndocument understanding transformer. In European Conference on Computer Vision, pages 498\u2013517.\nSpringer, 2022.\nT. H. Le, H. Chen, and M. A. Babar. Deep learning for source code modeling and generation: Models,\napplications, and challenges. ACM Computing Surveys (CSUR), 53(3):1\u201338, 2020.\nJ. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models, 2023a.\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim,\nQ. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier,\nJ. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi, J. Zhu,\nB. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca,\nM. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas,\nM. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf,\nJ. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor,\nS. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. M. Hughes, T. Wolf, A. Guha,\nL. von Werra, and H. de Vries. Starcoder: may the source be with you! ArXiv, abs/2305.06161,\n2023b. URL https://api.semanticscholar.org/CorpusID:258588247.\nJ. T. Liang, C. Yang, and B. A. Myers. A large-scale survey on the usability of ai programming\nassistants: Successes and challenges. In International Conference on Software Engineering, 2023.\nURL https://api.semanticscholar.org/CorpusID:257833548.\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023.\nM. R. Luo, G. Cui, and B. Rigg. The development of the cie 2000 colour-difference formula:\nCiede2000. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour\nGroup (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch\nSociety for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia,\nCentre Fran\u00e7ais de la Couleur, 26(5):340\u2013350, 2001.\nT. Lv, Y. Huang, J. Chen, L. Cui, S. Ma, Y. Chang, S. Huang, W. Wang, L. Dong, W. Luo, S. Wu,\nG. Wang, C. Zhang, and F. Wei. Kosmos-2.5: A multimodal literate model, 2023.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine:\nIterative refinement with self-feedback. ArXiv, abs/2303.17651, 2023. URL https://api.\nsemanticscholar.org/CorpusID:257900871.\nS. Mori, C. Y. Suen, and K. Yamamoto. Historical review of ocr research and development. Proceed-\nings of the IEEE, 80(7):1029\u20131058, 1992.\nT. A. Nguyen and C. Csallner. Reverse engineering mobile application user interfaces with remaui\n(t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE),\npages 248\u2013259. IEEE Computer Society, 2015.\nOpenAI.\nGpt-4v(ision) system card.\n2023.\nURL https://api.semanticscholar.org/\nCorpusID:263218031.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language\nsupervision, 2021.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints,\n2019.\nA. Robinson. Sketch2code: Generating a website from a paper mockup. ArXiv, abs/1905.13750,\n2019. URL https://api.semanticscholar.org/CorpusID:173188440.\n16\nB. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,\nA. Kozhevnikov, I. Evtimov, J. Bitton, M. P. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,\nA. D\u2019efossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Syn-\nnaeve. Code llama: Open foundation models for code. ArXiv, abs/2308.12950, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:261100919.\nC. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta,\nC. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk,\nand J. Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models,\n2022.\nN. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language\nagents with verbal reinforcement learning. 2023. URL https://api.semanticscholar.org/\nCorpusID:258833055.\nD. Soselia, K. Saifullah, and T. Zhou. Learning ui-to-code reverse generator using visual critic\nwithout rendering. 2023. URL https://api.semanticscholar.org/CorpusID:265302631.\nA. Telea. An image inpainting technique based on the fast marching method. Journal of graphics\ntools, 9(1):23\u201334, 2004.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient\nfoundation language models, 2023a.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023b.\nH. Vasconcelos, G. Bansal, A. Fourney, Q. V. Liao, and J. W. Vaughan. Generation probabilities are\nnot enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code comple-\ntions. ArXiv, abs/2302.07248, 2023. URL https://api.semanticscholar.org/CorpusID:\n256846746.\nA. Yan, Z. Yang, W. Zhu, K. Lin, L. Li, J. Wang, J. Yang, Y. Zhong, J. McAuley, J. Gao, et al. Gpt-4v\nin wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint\narXiv:2311.07562, 2023.\nQ. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen,\nJ. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: Modularization empowers large language\nmodels with multimodality, 2023.\nP. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. arXiv preprint\narXiv:1704.01696, 2017.\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In\n2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Oct. 2023. doi: 10.\n1109/iccv51070.2023.01100. URL http://dx.doi.org/10.1109/ICCV51070.2023.01100.\nC. Zhang, Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, and G. Yu. Appagent: Multimodal\nagents as smartphone users, 2023.\nB. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v(ision) is a generalist web agent, if grounded,\n2024.\nC. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, S. Zhang, G. Ghosh,\nM. Lewis, L. Zettlemoyer, and O. Levy. Lima: Less is more for alignment, 2023.\n17\nTag\nFrequency\nTag\nFrequency\nTag\nFrequency\n<div>\n17790\n<style>\n1181\n<head>\n486\n<a>\n13309\n<td>\n997\n<body>\n486\n<li>\n6883\n<input>\n995\n<tr>\n436\n<span>\n6813\n<h3>\n759\n<b>\n429\n<meta>\n4629\n<h2>\n709\n<nav>\n416\n<p>\n3413\n<strong>\n595\n<i>\n400\n<br>\n2453\n<h1>\n536\n<section>\n381\n<ul>\n2078\n<button>\n525\n<label>\n339\n<img>\n1870\n<title>\n492\n<form>\n292\n<option>\n1194\n<html>\n486\n<h4>\n289\nTable 5: The most frequent HTML tags in the reference implementations of our benchmark examples.\nA\nAdditional Dataset Statistics\nWe present the table of most frequent HTML tags in Table 5.\nB\nText Detection and Merging Details\nThe common approach to detect the texts in a given screenshot is to use OCR tools [Mori et al., 1992],\nwhich returns a list of text segments with their bounding boxes. However, in our case, we find that\nopen-source OCR tools usually output noisy outputs, which may affect the stability of downstream\nevaluation. Since we already have the source HTML codes for reference webpage screenshots, we\napply an alternative approach: we alter the color differently for different text segments in the source\nHTML code and detect text segments in the webpage by taking two extra screenshots and tracking\npixels with different colors. This helps us locate text segments from the HTML source code in the\nscreenshots without text recognition errors.\nBased on the two sets of detected blocks, we use the Jonker-Volgenant algorithm [Crouse, 2016]\n(implemented in Scipy 7) to get the optimal matching M between R and G, where (p, q) \u2208 M\nindicates rp is matched with gq. Specifically, we use the negative sequence similarity between textual\ncontents (\u2212simtext(, )) to initialize the cost matrix and ignore the matched pairs with a sequence\nsimilarity lower than 0.5. Since detected text blocks might be in different granularity, we also\nenumerate merging neighbor text blocks to search for matching with the highest similarity. However,\nthe matching may still not be perfect, especially when there are large granularity differences (our\nsearch does not consider merging non-contiguous blocks).\nC\nPrompts\nWe use the following prompt for self-revision prompting:\nYou are an expert web developer who specializes in HTML and CSS. I have\nan HTML file for implementing a webpage but it has some missing or wrong\nelements that are different from the original webpage.\nThe current\nimplementation I have is:\n[generated code from text-augmented prompting].\nI will provide the reference webpage that I want to build as well as the\nrendered webpage of the current implementation.\nI also provide you all\nthe texts that I want to include in the webpage here:\n[extracted texts\nfrom the original webpage].\nPlease compare the two webpages and refer to\nthe provided text elements to be included, and revise the original HTML\nimplementation to make it look exactly like the reference webpage.\nMake\nsure the code is syntactically correct and can render into a well-formed\nwebpage.\nYou can use \"rick.jpg\" as the placeholder image file.\nPay\n7https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_\nassignment.html\n18\nattention to things like size, text, position, and color of all the\nelements, as well as the overall layout.\nRespond directly with the content\nof the new revised and improved HTML file without any extra explanations.\nD\nHuman Annotation Details\nIn the instructions, the annotators are asked to check the pair following the order of priority (content >\nlayout > style). This priority list is based on two intuitions: (i) Layout comparison is only meaningful\nwhen the content is (almost) complete. (ii) The style of independent elements is easier to fix than the\nlayout of multiple elements. The detailed instructions are below:\nTask Overview\nIn this survey, you will be given a reference webpage\u2019s screenshot, as well as two\ncandidate webpages (Example 1 and Example 2) that try to replicate the reference\nwebpage. Your task is to judge which of the two candidates is closer to the reference.\nEach (Reference, Example 1, Example 2) is presented in a row, where the original\nboundary of screenshot is marked by black.\nComparison Guide\nInitial Step: Content Check\n\u2022 Text Content: Examine if the text on the candidate webpages matches the\nreference. Pay special attention to missing or extra content, especially key\nelements like titles.\n\u2022 Image Content: Assess the placement of the blue placeholder blocks (for\nimages).\n\u2022 Primary Judgment Criterion: If one example has significant missing or addi-\ntional content compared to the other, it should be considered less similar to the\nreference.\nSecond Step: Layout Check\n\u2022 Element Arrangement: If the content (text and images) of both examples is\nsimilarly good or bad, proceed to evaluate the arrangement of these elements.\nCheck if their organization, order, and hierarchy match the reference.\n\u2022 Secondary Judgment Criterion: If differences in layout are observed, the\nexample with the layout most similar to the reference should be rated higher.\nFinal Step: Style Check\n\u2022 Style Attributes: Only if Example 1 and Example 2 are comparable in content\nand layout, examine the style elements like font style, color, and size.\n\u2022 Tertiary Judgment Criterion: In cases where content and layout are equally\nmatched, preference should be given to the example with style attributes closer\nto the reference.\nOverall Judgment\nBased on the criteria in the order of priority (Content > Layout > Style), make an\noverall judgment on which example (Example 1 or Example 2) is more similar to the\nreference webpage.\nJudgment Options\n1. Select \"Example 1 better\" if Example 1 is closer to the reference.\n2. Select \"Example 2 better\" if Example 2 is closer to the reference.\n3. Opt for \"Tie\" only if both examples are similarly accurate or equally distant from the\nreference.\nAdditional Tips\n1. Use zoom-in for detailed inspection.\n2. Focus on major discrepancies in each step before moving to the next.\n3. Your judgment should be based on a cumulative assessment of content, layout, and\nstyle.\n19\nWe also provide 8 examples after the instruction. The UI of the annotation question is Figure 9.\nFleiss\u2019 kappa for pairwise model comparison is 0.36 (5 annotators).\nFigure 9: User Interface for pairwise model comparison.\nFurthermore, we provide the instructions for direct assessment (comparing the reference and webpages\ngenerated by GPT-4V self-revision prompting). The Fleiss\u2019 kappa is 0.32 (5 annotators) for the first\nquestion and 0.26 (5 annotators) for the second question.\nCan the AI-generated webpage replace the original webpage?\nTask Overview\nIn each question, you will be given two webpage screenshots.\nBy comparing the two webpages, you need to decide whether they are exchangeable.\nPlease zoom in to take a closer look at the screenshots if necessary.\nYou should answer \"Yes\", if:\n1. They look roughly similar.\n2. They have similar content.\n3. They can serve the same functions.\n(Minor details don\u2019t matter that much)\nOtherwise, you should answer \"No\".\nIs the reference webpage or AI generation better?\nTask Overview\nIn each question, you will be given two webpage screenshots.\nBy comparing the two webpages, you need to decide which one is better.\nPlease zoom in to take a closer look at the screenshots if necessary.\nTo decide which one is better, you might consider the following aspects:\n1. More readable\n20\n2. Better layout\n3. Better style\n21\n"
  },
  {
    "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
    "link": "https://arxiv.org/pdf/2403.03206.pdf",
    "upvote": "37",
    "text": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\nPatrick Esser *\nSumith Kulal\nAndreas Blattmann\nRahim Entezari\nJonas M\u00a8uller\nHarry Saini\nYam Levi\nDominik Lorenz\nAxel Sauer\nFrederic Boesel\nDustin Podell\nTim Dockhorn\nZion English\nKyle Lacey\nAlex Goodwin\nYannik Marek\nRobin Rombach *\nStability AI\nFigure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following\nand spatial reasoning, attention to fine details, and high image quality across a wide variety of styles.\nAbstract\nDiffusion models create data from noise by invert-\ning the forward paths of data towards noise and\nhave emerged as a powerful generative modeling\ntechnique for high-dimensional, perceptual data\nsuch as images and videos. Rectified flow is a re-\ncent generative model formulation that connects\ndata and noise in a straight line. Despite its better\ntheoretical properties and conceptual simplicity, it\nis not yet decisively established as standard prac-\ntice. In this work, we improve existing noise sam-\npling techniques for training rectified flow mod-\nels by biasing them towards perceptually relevant\nscales. Through a large-scale study, we demon-\n*Equal contribution . <first.last>@stability.ai.\nstrate the superior performance of this approach\ncompared to established diffusion formulations\nfor high-resolution text-to-image synthesis. Ad-\nditionally, we present a novel transformer-based\narchitecture for text-to-image generation that uses\nseparate weights for the two modalities and en-\nables a bidirectional flow of information between\nimage and text tokens, improving text comprehen-\nsion, typography, and human preference ratings.\nWe demonstrate that this architecture follows pre-\ndictable scaling trends and correlates lower vali-\ndation loss to improved text-to-image synthesis as\nmeasured by various metrics and human evalua-\ntions. Our largest models outperform state-of-the-\nart models, and we will make our experimental\ndata, code, and model weights publicly available.\n1\narXiv:2403.03206v1  [cs.CV]  5 Mar 2024\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n1. Introduction\nDiffusion models create data from noise (Song et al., 2020).\nThey are trained to invert forward paths of data towards\nrandom noise and, thus, in conjunction with approximation\nand generalization properties of neural networks, can be\nused to generate new data points that are not present in\nthe training data but follow the distribution of the training\ndata (Sohl-Dickstein et al., 2015; Song & Ermon, 2020).\nThis generative modeling technique has proven to be very\neffective for modeling high-dimensional, perceptual data\nsuch as images (Ho et al., 2020). In recent years, diffusion\nmodels have become the de-facto approach for generating\nhigh-resolution images and videos from natural language\ninputs with impressive generalization capabilities (Saharia\net al., 2022b; Ramesh et al., 2022; Rombach et al., 2022;\nPodell et al., 2023; Dai et al., 2023; Esser et al., 2023;\nBlattmann et al., 2023b; Betker et al., 2023; Blattmann et al.,\n2023a; Singer et al., 2022). Due to their iterative nature\nand the associated computational costs, as well as the long\nsampling times during inference, research on formulations\nfor more efficient training and/or faster sampling of these\nmodels has increased (Karras et al., 2023; Liu et al., 2022).\nWhile specifying a forward path from data to noise leads to\nefficient training, it also raises the question of which path\nto choose. This choice can have important implications\nfor sampling. For example, a forward process that fails to\nremove all noise from the data can lead to a discrepancy\nin training and test distribution and result in artifacts such\nas gray image samples (Lin et al., 2024). Importantly, the\nchoice of the forward process also influences the learned\nbackward process and, thus, the sampling efficiency. While\ncurved paths require many integration steps to simulate the\nprocess, a straight path could be simulated with a single\nstep and is less prone to error accumulation. Since each step\ncorresponds to an evaluation of the neural network, this has\na direct impact on the sampling speed.\nA particular choice for the forward path is a so-called Rec-\ntified Flow (Liu et al., 2022; Albergo & Vanden-Eijnden,\n2022; Lipman et al., 2023), which connects data and noise\non a straight line. Although this model class has better\ntheoretical properties, it has not yet become decisively es-\ntablished in practice. So far, some advantages have been\nempirically demonstrated in small and medium-sized ex-\nperiments (Ma et al., 2024), but these are mostly limited to\nclass-conditional models. In this work, we change this by in-\ntroducing a re-weighting of the noise scales in rectified flow\nmodels, similar to noise-predictive diffusion models (Ho\net al., 2020). Through a large-scale study, we compare\nour new formulation to existing diffusion formulations and\ndemonstrate its benefits.\nWe show that the widely used approach for text-to-image\nsynthesis, where a fixed text representation is fed directly\ninto the model (e.g., via cross-attention (Vaswani et al.,\n2017; Rombach et al., 2022)), is not ideal, and present\na new architecture that incorporates learnable streams for\nboth image and text tokens, which enables a two-way flow\nof information between them. We combine this with our\nimproved rectified flow formulation and investigate its scala-\nbility. We demonstrate a predictable scaling trend in the val-\nidation loss and show that a lower validation loss correlates\nstrongly with improved automatic and human evaluations.\nOur largest models outperform state-of-the art open models\nsuch as SDXL (Podell et al., 2023), SDXL-Turbo (Sauer\net al., 2023), Pixart-\u03b1 (Chen et al., 2023), and closed-source\nmodels such as DALL-E 3 (Betker et al., 2023) both in\nquantitative evaluation (Ghosh et al., 2023) of prompt un-\nderstanding and human preference ratings.\nThe core contributions of our work are: (i) We conduct a\nlarge-scale, systematic study on different diffusion model\nand rectified flow formulations to identify the best setting.\nFor this purpose, we introduce new noise samplers for recti-\nfied flow models that improve performance over previously\nknown samplers. (ii) We devise a novel, scalable architec-\nture for text-to-image synthesis that allows bi-directional\nmixing between text and image token streams within the\nnetwork. We show its benefits compared to established back-\nbones such as UViT (Hoogeboom et al., 2023) and DiT (Pee-\nbles & Xie, 2023). Finally, we (iii) perform a scaling study\nof our model and demonstrate that it follows predictable\nscaling trends. We show that a lower validation loss cor-\nrelates strongly with improved text-to-image performance\nassessed via metrics such as T2I-CompBench (Huang et al.,\n2023), GenEval (Ghosh et al., 2023) and human ratings. We\nmake results, code, and model weights publicly available.\n2. Simulation-Free Training of Flows\nWe consider generative models that define a mapping be-\ntween samples x1 from a noise distribution p1 to samples\nx0 from a data distribution p0 in terms of an ordinary differ-\nential equation (ODE),\ndyt = v\u0398(yt, t) dt ,\n(1)\nwhere the velocity v is parameterized by the weights \u0398 of\na neural network. Prior work by Chen et al. (2018) sug-\ngested to directly solve Equation (1) via differentiable ODE\nsolvers. However, this process is computationally expensive,\nespecially for large network architectures that parameterize\nv\u0398(yt, t). A more efficient alternative is to directly regress\na vector field ut that generates a probability path between\np0 and p1. To construct such a ut, we define a forward\nprocess, corresponding to a probability path pt between p0\nand p1 = N(0, 1), as\nzt = atx0 + bt\u03f5\nwhere \u03f5 \u223c N(0, I) .\n(2)\n2\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFor a0 = 1, b0 = 0, a1 = 0 and b1 = 1, the marginals,\npt(zt) = E\u03f5\u223cN (0,I)pt(zt|\u03f5) ,\n(3)\nare consistent with the data and noise distribution.\nTo express the relationship between zt, x0 and \u03f5, we intro-\nduce \u03c8t and ut as\n\u03c8t(\u00b7|\u03f5) : x0 7\u2192 atx0 + bt\u03f5\n(4)\nut(z|\u03f5) := \u03c8\u2032\nt(\u03c8\u22121\nt\n(z|\u03f5)|\u03f5)\n(5)\nSince zt can be written as solution to the ODE z\u2032\nt = ut(zt|\u03f5),\nwith initial value z0 = x0, ut(\u00b7|\u03f5) generates pt(\u00b7|\u03f5). Re-\nmarkably, one can construct a marginal vector field ut which\ngenerates the marginal probability paths pt (Lipman et al.,\n2023) (see B.1), using the conditional vector fields ut(\u00b7|\u03f5):\nut(z) = E\u03f5\u223cN (0,I)ut(z|\u03f5)pt(z|\u03f5)\npt(z)\n(6)\nWhile regressing ut with the Flow Matching objective\nLF M = Et,pt(z)||v\u0398(z, t) \u2212 ut(z)||2\n2.\n(7)\ndirectly is intractable due to the marginalization in Equa-\ntion 6, Conditional Flow Matching (see B.1),\nLCF M = Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t) \u2212 ut(z|\u03f5)||2\n2 ,\n(8)\nwith the conditional vector fields ut(z|\u03f5) provides an equiv-\nalent yet tractable objective.\nTo convert the loss into an explicit form we insert\n\u03c8\u2032\nt(x0|\u03f5) = a\u2032\ntx0 + b\u2032\nt\u03f5 and \u03c8\u22121\nt\n(z|\u03f5) = z\u2212bt\u03f5\nat\ninto (5)\nz\u2032\nt = ut(zt|\u03f5) = a\u2032\nt\nat\nzt \u2212 \u03f5bt(a\u2032\nt\nat\n\u2212 b\u2032\nt\nbt\n) .\n(9)\nNow, consider the signal-to-noise ratio \u03bbt := log a2\nt\nb2\nt . With\n\u03bb\u2032\nt = 2( a\u2032\nt\nat \u2212 b\u2032\nt\nbt ), we can rewrite Equation (9) as\nut(zt|\u03f5) = a\u2032\nt\nat\nzt \u2212 bt\n2 \u03bb\u2032\nt\u03f5\n(10)\nNext, we use Equation (10) to reparameterize Equation (8)\nas a noise-prediction objective:\nLCF M = Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t) \u2212 a\u2032\nt\nat\nz + bt\n2 \u03bb\u2032\nt\u03f5||2\n2 (11)\n= Et,pt(z|\u03f5),p(\u03f5)\n\u0012\n\u2212bt\n2 \u03bb\u2032\nt\n\u00132\n||\u03f5\u0398(z, t) \u2212 \u03f5||2\n2 (12)\nwhere we defined \u03f5\u0398 :=\n\u22122\n\u03bb\u2032\ntbt (v\u0398 \u2212 a\u2032\nt\nat z).\nNote that the optimum of the above objective does not\nchange when introducing a time-dependent weighting. Thus,\none can derive various weighted loss functions that provide\na signal towards the desired solution but might affect the\noptimization trajectory. For a unified analysis of different\napproaches, including classic diffusion formulations, we\ncan write the objective in the following form (following\nKingma & Gao (2023)):\nLw(x0) = \u22121\n2Et\u223cU(t),\u03f5\u223cN (0,I)\n\u0002\nwt\u03bb\u2032\nt\u2225\u03f5\u0398(zt, t) \u2212 \u03f5\u22252\u0003\n,\nwhere wt = \u2212 1\n2\u03bb\u2032\ntb2\nt corresponds to LCF M.\n3. Flow Trajectories\nIn this work, we consider different variants of the above\nformalism that we briefly describe in the following.\nRectified Flow\nRectified Flows (RFs) (Liu et al., 2022;\nAlbergo & Vanden-Eijnden, 2022; Lipman et al., 2023)\ndefine the forward process as straight paths between the\ndata distribution and a standard normal distribution, i.e.\nzt = (1 \u2212 t)x0 + t\u03f5 ,\n(13)\nand uses LCF M which then corresponds to wRF\nt\n=\nt\n1\u2212t.\nThe network output directly parameterizes the velocity v\u0398.\nEDM\nEDM (Karras et al., 2022) uses a forward process\nof the form\nzt = x0 + bt\u03f5\n(14)\nwhere (Kingma & Gao, 2023) bt = exp F \u22121\nN (t|Pm, P 2\ns )\nwith F \u22121\nN\nbeing the quantile function of the normal distribu-\ntion with mean Pm and variance P 2\ns . Note that this choice\nresults in\n\u03bbt \u223c N(\u22122Pm, (2Ps)2)\nfor t \u223c U(0, 1)\n(15)\nThe network is parameterized through an F-prediction\n(Kingma & Gao, 2023; Karras et al., 2022) and the loss\ncan be written as LwEDM\nt\nwith\nwEDM\nt\n= N(\u03bbt| \u2212 2Pm, (2Ps)2)(e\u2212\u03bbt + 0.52)\n(16)\nCosine\n(Nichol & Dhariwal, 2021) proposed a forward\nprocess of the form\nzt = cos\n\u0000\u03c0\n2 t\n\u0001\nx0 + sin\n\u0000\u03c0\n2 t\n\u0001\n\u03f5 .\n(17)\nIn combination with an \u03f5-parameterization and loss, this\ncorresponds to a weighting wt = sech(\u03bbt/2). When com-\nbined with a v-prediction loss (Kingma & Gao, 2023), the\nweighting is given by wt = e\u2212\u03bbt/2.\n3\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n(LDM-)Linear\nLDM (Rombach et al., 2022) uses a mod-\nification of the DDPM schedule (Ho et al., 2020). Both are\nvariance preserving schedules, i.e. bt =\np\n1 \u2212 a2\nt, and de-\nfine at for discrete timesteps t = 0, . . . , T \u2212 1 in terms\nof diffusion coefficients \u03b2t as at = (Qt\ns=0(1 \u2212 \u03b2s))\n1\n2 .\nFor given boundary values \u03b20 and \u03b2T \u22121, DDPM uses\n\u03b2t\n= \u03b20 +\nt\nT \u22121(\u03b2T \u22121 \u2212 \u03b20) and LDM uses \u03b2t\n=\n\u0010p\n\u03b20 +\nt\nT \u22121(\np\n\u03b2T \u22121 \u2212\np\n\u03b20)\n\u00112\n.\n3.1. Tailored SNR Samplers for RF models\nThe RF loss trains the velocity v\u0398 uniformly on all timesteps\nin [0, 1]. Intuitively, however, the resulting velocity predic-\ntion target \u03f5 \u2212 x0 is more difficult for t in the middle of\n[0, 1], since for t = 0, the optimal prediction is the mean\nof p1, and for t = 1 the optimal prediction is the mean of\np0. In general, changing the distribution over t from the\ncommonly used uniform distribution U(t) to a distribution\nwith density \u03c0(t) is equivalent to a weighted loss Lw\u03c0\nt with\nw\u03c0\nt =\nt\n1 \u2212 t\u03c0(t)\n(18)\nThus, we aim to give more weight to intermediate timesteps\nby sampling them more frequently. Next, we describe the\ntimestep densities \u03c0(t) that we use to train our models.\nLogit-Normal Sampling\nOne option for a distribution\nthat puts more weight on intermediate steps is the logit-\nnormal distribution (Atchison & Shen, 1980). Its density,\n\u03c0ln(t; m, s) =\n1\ns\n\u221a\n2\u03c0\n1\nt(1 \u2212 t) exp\n\u0010\n\u2212(logit(t) \u2212 m)2\n2s2\n\u0011\n,\n(19)\nwhere logit(t) = log\nt\n1\u2212t, has a location parameter, m, and\na scale parameter, s. The location parameter enables us to\nbias the training timesteps towards either data p0 (negative\nm) or noise p1 (positive m). As shown in Figure 11, the\nscale parameters controls how wide the distribution is.\nIn practice, we sample the random variable u from a nor-\nmal distribution u \u223c N(u; m, s) and map it through the\nstandard logistic function.\nMode Sampling with Heavy Tails\nThe logit-normal den-\nsity always vanishes at the endpoints 0 and 1. To study\nwhether this has adverse effects on the performance, we\nalso use a timestep sampling distribution with strictly posi-\ntive density on [0, 1]. For a scale parameter s, we define\nfmode(u; s) = 1 \u2212 u \u2212 s \u00b7\n\u0010\ncos2\u0000\u03c0\n2 u\n\u0001\n\u2212 1 + u\n\u0011\n.\n(20)\nFor \u22121 \u2264 s \u2264\n2\n\u03c0\u22122, this function is monotonic, and we\ncan use it to sample from the implied density \u03c0mode(t; s) =\n\f\f d\ndtf \u22121\nmode(t)\n\f\f. As seen in Figure 11, the scale parameter\ncontrols the degree to which either the midpoint (positive\ns) or the endpoints (negative s) are favored during sam-\npling. This formulation also includes a uniform weighting\n\u03c0mode(t; s = 0) = U(t) for s = 0, which has been used\nwidely in previous works on Rectified Flows (Liu et al.,\n2022; Ma et al., 2024).\nCosMap\nFinally, we also consider the cosine schedule\n(Nichol & Dhariwal, 2021) from Section 3 in the RF setting.\nIn particular, we are looking for a mapping f : u 7\u2192 f(u) =\nt, u \u2208 [0, 1], such that the log-snr matches that of the cosine\nschedule: 2 log cos( \u03c0\n2 u)\nsin( \u03c0\n2 u) = 2 log 1\u2212f(u)\nf(u) . Solving for f, we\nobtain for u \u223c U(u)\nt = f(u) = 1 \u2212\n1\ntan( \u03c0\n2 u) + 1,\n(21)\nfrom which we obtain the density\n\u03c0CosMap(t) =\n\f\f\f\f\nd\ndtf \u22121(t)\n\f\f\f\f =\n2\n\u03c0 \u2212 2\u03c0t + 2\u03c0t2 .\n(22)\n4. Text-to-Image Architecture\nFor text-conditional sampling of images, our model has to\ntake both modalities, text and images, into account. We\nuse pretrained models to derive suitable representations and\nthen describe the architecture of our diffusion backbone. An\noverview of this is presented in Figure 2.\nOur general setup follows LDM (Rombach et al., 2022)\nfor training text-to-image models in the latent space of a\npretrained autoencoder. Similar to the encoding of images to\nlatent representations, we also follow previous approaches\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text\nconditioning c using pretrained, frozen text models. Details\ncan be found in Appendix B.2.\nMultimodal Diffusion Backbone Our architecture builds\nupon the DiT (Peebles & Xie, 2023) architecture. DiT only\nconsiders class conditional image generation and uses a\nmodulation mechanism to condition the network on both\nthe timestep of the diffusion process and the class label.\nSimilarly, we use embeddings of the timestep t and cvec\nas inputs to the modulation mechanism. However, as the\npooled text representation retains only coarse-grained infor-\nmation about the text input (Podell et al., 2023), the network\nalso requires information from the sequence representation\ncctxt.\nWe construct a sequence consisting of embeddings of the\ntext and image inputs. Specifically, we add positional en-\ncodings and flatten 2 \u00d7 2 patches of the latent pixel rep-\nresentation x \u2208 Rh\u00d7w\u00d7c to a patch encoding sequence of\nlength 1\n2 \u00b7 h \u00b7 1\n2 \u00b7 w. After embedding this patch encoding\nand the text encoding cctxt to a common dimensionality, we\n4\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nCaption\nCLIP-L/14\nCLIP-G/14\nT5 XXL\nPooled\nLinear\nc\nMLP\nMLP\nSinusoidal Encoding\nTimestep\n+\ny\nNoised Latent\nPatching\nLinear\n+\nPositional\nEmbedding\nx\nMM-DiT-Block 1\nMM-DiT-Block 2\n. . .\nMM-DiT-Block d\nModulation\nLinear\nUnpatching\nOutput\n77 + 77 tokens\n4096\nchannel\n(a) Overview of all components.\nc\nx\nLayernorm\nMod: \u03b1c \u00b7 \u2022 + \u03b2c\nLinear\nAttention\nLinear\n\u2217\n+\nLayernorm\nMod: \u03b4c \u00b7 \u2022 + \u03f5c\nMLP\n\u2217\n+\n\u03b1c\n\u03b2c\n\u03b3c\n\u03b4c\n\u03f5c\n\u03b6c\ny\nSiLU\nLinear\nLayernorm\nMod: \u03b1x \u00b7 \u2022 + \u03b2x\nLinear\nLinear\n\u2217\n+\nLayernorm\nMod: \u03b4x \u00b7 \u2022 + \u03f5x\nMLP\n\u2217\n+\n\u03b1x\n\u03b2x\n\u03b3x\n\u03b4x\n\u03f5x\n\u03b6x\nSiLU\nLinear\nK\nQ\nV\n\u2299\n\u2299\n\u2299\nOpt.\nRMS-\nNorm\nOpt.\nRMS-\nNorm\nOpt.\nRMS-\nNorm\nOpt.\nRMS-\nNorm\n(b) One MM-DiT block\nFigure 2. Our model architecture. Concatenation is indicated by \u2299 and element-wise multiplication by \u2217. The RMS-Norm for Q and K\ncan be added to stabilize training runs. Best viewed zoomed in.\nconcatenate the two sequences. We then follow DiT and\napply a sequence of modulated attention and MLPs.\nSince text and image embeddings are conceptually quite\ndifferent, we use two separate sets of weights for the two\nmodalities. As shown in Figure 2b, this is equivalent to\nhaving two independent transformers for each modality, but\njoining the sequences of the two modalities for the attention\noperation, such that both representations can work in their\nown space yet take the other one into account.\nFor our scaling experiments, we parameterize the size of\nthe model in terms of the model\u2019s depth d, i.e. the number\nof attention blocks, by setting the hidden size to 64 \u00b7 d\n(expanded to 4 \u00b7 64 \u00b7 d channels in the MLP blocks), and the\nnumber of attention heads equal to d.\n5. Experiments\n5.1. Improving Rectified Flows\nWe aim to understand which of the approaches for\nsimulation-free training of normalizing flows as in Equa-\ntion 1 is the most efficient. To enable comparisons across\ndifferent approaches, we control for the optimization algo-\nrithm, the model architecture, the dataset and samplers. In\naddition, the losses of different approaches are incomparable\nand also do not necessarily correlate with the quality of out-\nput samples; hence we need evaluation metrics that allow for\na comparison between approaches. We train models on Ima-\ngeNet (Russakovsky et al., 2014) and CC12M (Changpinyo\net al., 2021), and evaluate both the training and the EMA\nweights of the models during training using validation losses,\nCLIP scores (Radford et al., 2021; Hessel et al., 2021), and\nFID (Heusel et al., 2017) under different sampler settings\n(different guidance scales and sampling steps). We calcu-\nlate the FID on CLIP features as proposed by (Sauer et al.,\n2021). All metrics are evaluated on the COCO-2014 valida-\ntion split (Lin et al., 2014). Full details on the training and\nsampling hyperparameters are provided in Appendix B.3.\n5.1.1. RESULTS\nWe train each of 61 different formulations on the two\ndatasets. We include the following variants from Section 3:\n\u2022 Both\n\u03f5-\nand\nv-prediction\nloss\nwith\nlinear\n(eps/linear, v/linear) and cosine (eps/cos,\nv/cos) schedule.\n\u2022 RF loss with \u03c0mode(t; s) (rf/mode(s)) with 7 val-\nues for s chosen uniformly between \u22121 and 1.75, and\n5\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nrank averaged over\nvariant\nall\n5 steps\n50 steps\nrf/lognorm(0.00, 1.00)\n1.54\n1.25\n1.50\nrf/lognorm(1.00, 0.60)\n2.08\n3.50\n2.00\nrf/lognorm(0.50, 0.60)\n2.71\n8.50\n1.00\nrf/mode(1.29)\n2.75\n3.25\n3.00\nrf/lognorm(0.50, 1.00)\n2.83\n1.50\n2.50\neps/linear\n2.88\n4.25\n2.75\nrf/mode(1.75)\n3.33\n2.75\n2.75\nrf/cosmap\n4.13\n3.75\n4.00\nedm(0.00, 0.60)\n5.63\n13.25\n3.25\nrf\n5.67\n6.50\n5.75\nv/linear\n6.83\n5.75\n7.75\nedm(0.60, 1.20)\n9.00\n13.00\n9.00\nv/cos\n9.17\n12.25\n8.75\nedm/cos\n11.04\n14.25\n11.25\nedm/rf\n13.04\n15.25\n13.25\nedm(-1.20, 1.20)\n15.58\n20.25\n15.00\nTable 1. Global ranking of variants. For this ranking, we apply\nnon-dominated sorting averaged over EMA and non-EMA weights,\ntwo datasets and different sampling settings.\nImageNet\nCC12M\nvariant\nCLIP\nFID\nCLIP\nFID\nrf\n0.247\n49.70\n0.217\n94.90\nedm(-1.20, 1.20)\n0.236\n63.12\n0.200\n116.60\neps/linear\n0.245\n48.42\n0.222\n90.34\nv/cos\n0.244\n50.74\n0.209\n97.87\nv/linear\n0.246\n51.68\n0.217\n100.76\nrf/lognorm(0.50, 0.60)\n0.256\n80.41\n0.233\n120.84\nrf/mode(1.75)\n0.253\n44.39\n0.218\n94.06\nrf/lognorm(1.00, 0.60)\n0.254\n114.26\n0.234\n147.69\nrf/lognorm(-0.50, 1.00)\n0.248\n45.64\n0.219\n89.70\nrf/lognorm(0.00, 1.00)\n0.250\n45.78\n0.224\n89.91\nTable 2. Metrics for different variants. FID and CLIP scores of\ndifferent variants with 25 sampling steps. We highlight the best,\nsecond best, and third best entries.\nadditionally for s = 1.0 and s = 0 which corresponds\nto uniform timestep sampling (rf/mode).\n\u2022 RF loss with \u03c0ln(t; m, s) (rf/lognorm(m, s))\nwith 30 values for (m, s) in the grid with m uniform\nbetween \u22121 and 1, and s uniform between 0.2 and 2.2.\n\u2022 RF loss with \u03c0CosMap(t) (rf/cosmap).\n\u2022 EDM (edm(Pm, Ps)) with 15 values for Pm chosen\nuniformly between \u22121.2 and 1.2 and Ps uniform be-\ntween 0.6 and 1.8. Note that Pm, Ps = (\u22121.2, 1.2)\ncorresponds to the parameters in (Karras et al., 2022).\n\u2022 EDM with a schedule such that it matches the log-SNR\nweighting of rf (edm/rf) and one that matches the\nlog-SNR weighting of v/cos (edm/cos).\nFor each run, we select the step with minimal validation loss\nwhen evaluated with EMA weights and then collect CLIP\nscores and FID obtained with 6 different sampler settings\nboth with and without EMA weights.\nFor all 24 combinations of sampler settings, EMA weights,\nand dataset choice, we rank the different formulations using\na non-dominated sorting algorithm. For this, we repeatedly\ncompute the variants that are Pareto optimal according to\nCLIP and FID scores, assign those variants the current iter-\nation index, remove those variants, and continue with the\nremaining ones until all variants get ranked. Finally, we\naverage those ranks over the 24 different control settings.\nWe present the results in Tab. 1, where we only show the\ntwo best-performing variants for those variants that were\nevaluated with different hyperparameters. We also show\nranks where we restrict the averaging over sampler settings\nwith 5 steps and with 50 steps.\nWe observe that rf/lognorm(0.00, 1.00) consis-\ntently achieves a good rank.\nIt outperforms a rectified\nflow formulation with uniform timestep sampling (rf) and\nthus confirms our hypothesis that intermediate timesteps are\nmore important. Among all the variants, only rectified flow\nformulations with modified timestep sampling perform bet-\nter than the LDM-Linear (Rombach et al., 2022) formulation\n(eps/linear) used previously.\nWe also observe that some variants perform well in some\nsettings but worse in others, e.g. rf/lognorm(0.50,\n0.60) is the best-performing variant with 50 sampling\nsteps but much worse (average rank 8.5) with 5 sampling\nsteps. We observe a similar behavior with respect to the\ntwo metrics in Tab. 2. The first group shows representa-\ntive variants and their metrics on both datasets with 25\nsampling steps. The next group shows the variants that\nachieve the best CLIP and FID scores. With the exception\nof rf/mode(1.75), these variants typically perform very\nwell in one metric but relatively badly in the other. In con-\ntrast, we once again observe that rf/lognorm(0.00,\n1.00) achieves good performance across metrics and\ndatasets, where it obtains the third-best scores two out of\nfour times and once the second-best performance.\nFinally, we illustrate the qualitative behavior of different\nformulations in Figure 3, where we use different colors\nfor different groups of formulations (edm, rf, eps and\nv). Rectified flow formulations generally perform well and,\ncompared to other formulations, their performance degrades\nless when reducing the number of sampling steps.\n5.2. Improving Modality Specific Representations\nHaving found a formulation in the previous section that\nallows rectified flow models to not only compete with estab-\nlished diffusion formulations such as LDM-Linear (Rom-\nbach et al., 2022) or EDM (Karras et al., 2022), but even\noutperforms them, we now turn to the application of our\nformulation to high-resolution text-to-image synthesis. Ac-\n6\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n10\n20\n30\n40\n50\n60\n80\n100\n120\n140\nedm(-1.20, 1.20)\neps/linear\nrf/lognorm(0.00, 1.00)\nrf\nv/cos\nv/linear\nnumber of sampling steps\nFID\nFigure 3. Rectified flows are sample efficient. Rectified Flows\nperform better then other formulations when sampling fewer steps.\nFor 25 and more steps, only rf/lognorm(0.00, 1.00) re-\nmains competitive to eps/linear.\nMetric\n4 chn\n8 chn\n16 chn\nFID (\u2193)\n2.41\n1.56\n1.06\nPerceptual Similarity (\u2193)\n0.85\n0.68\n0.45\nSSIM (\u2191)\n0.75\n0.79\n0.86\nPSNR (\u2191)\n25.12\n26.40\n28.62\nTable 3. Improved Autoencoders. Reconstruction performance\nmetrics for different channel configurations. The downsampling\nfactor for all models is f = 8.\ncordingly, the final performance of our algorithm depends\nnot only on the training formulation, but also on the parame-\nterization via a neural network and the quality of the image\nand text representations we use. In the following sections,\nwe describe how we improve all these components before\nscaling our final method in Section 5.3.\n5.2.1. IMPROVED AUTOENCODERS\nLatent diffusion models achieve high efficiency by operating\nin the latent space of a pretrained autoencoder (Rombach\net al., 2022), which maps an input RGB X \u2208 RH\u00d7W \u00d73 into\na lower-dimensional space x = E(X) \u2208 Rh\u00d7w\u00d7d. The\nreconstruction quality of this autoencoder provides an upper\nbound on the achievable image quality after latent diffusion\ntraining. Similar to Dai et al. (2023), we find that increasing\nthe number of latent channels d significantly boosts recon-\nstruction performance, see Table 3. Intuitively, predicting\nlatents with higher d is a more difficult task, and thus mod-\nels with increased capacity should be able to perform better\nfor larger d, ultimately achieving higher image quality. We\nconfirm this hypothesis in Figure 10, where we see that the\nd = 16 autoencoder exhibits better scaling performance in\nterms of sample FID. For the remainder of this paper, we\nthus choose d = 16.\n5.2.2. IMPROVED CAPTIONS\nBetker et al. (2023) demonstrated that synthetically gen-\nerated captions can greatly improve text-to-image models\ntrained at scale. This is due to the oftentimes simplistic\nOriginal Captions\n50/50 Mix\nsuccess rate [%]\nsuccess rate [%]\nColor Attribution\n11.75\n24.75\nColors\n71.54\n68.09\nPosition\n6.50\n18.00\nCounting\n33.44\n41.56\nSingle Object\n95.00\n93.75\nTwo Objects\n41.41\n52.53\nOverall score\n43.27\n49.78\nTable 4. Improved Captions.\nUsing a 50/50 mixing ratio of\nsynthetic (via CogVLM (Wang et al., 2023)) and original cap-\ntions improves text-to-image performance.\nAssessed via the\nGenEval (Ghosh et al., 2023) benchmark.\nnature of the human-generated captions that come with\nlarge-scale image datasets, which overly focus on the image\nsubject and usually omit details describing the background\nor composition of the scene, or, if applicable, displayed\ntext (Betker et al., 2023). We follow their approach and\nuse an off-the-shelf, state-of-the-art vision-language model,\nCogVLM (Wang et al., 2023), to create synthetic annotations\nfor our large-scale image dataset. As synthetic captions may\ncause a text-to-image model to forget about certain concepts\nnot present in the VLM\u2019s knowledge corpus, we use a ratio\nof 50 % original and 50 % synthetic captions.\nTo assess the effect of training on this caption mix, we train\ntwo d = 15 MM-DiT models for 250k steps, one on only\noriginal captions and the other on the 50/50 mix. We evalu-\nate the trained models using the GenEval benchmark (Ghosh\net al., 2023) in Table 4. The results demonstrate that the\nmodel trained with the addition of synthetic captions clearly\noutperforms the model that only utilizes original captions.\nWe thus use the 50/50 synthetic/original caption mix for the\nremainder of this work.\n5.2.3. IMPROVED TEXT-TO-IMAGE BACKBONES\nIn this section, we compare the performance of existing\ntransformer-based diffusion backbones with our novel mul-\ntimodal transformer-based diffusion backbone, MM-DiT, as\nintroduced in Section 4. MM-DiT is specifically designed to\nhandle different domains, here text and image tokens, using\n(two) different sets of trainable model weights. More specif-\nically, we follow the experimental setup from Section 5.1\nand compare text-to-image performance on CC12M of DiT,\nCrossDiT (DiT but with cross-attending to the text tokens\ninstead of sequence-wise concatenation (Chen et al., 2023))\nand our MM-DiT. For MM-DiT, we compare models with\ntwo sets of weights and three sets of weights, where the lat-\nter handles the CLIP (Radford et al., 2021) and T5 (Raffel\net al., 2019) tokens (c.f. Section 4) separately. Note that DiT\n(w/ concatenation of text and image tokens as in Section 4)\ncan be interpreted as a special case of MM-DiT with one\n7\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\na space elevator,\ncinematic scifi art\nA cheeseburger with juicy\nbeef patties and melted\ncheese sits on top of a toilet\nthat looks like a throne and\nstands in the middle of the\nroyal chamber.\na hole in the floor of my\nbathroom with small\ngremlins living in it\na small office made out of car\nparts\nThis dreamlike digital art\ncaptures a vibrant,\nkaleidoscopic bird in a lush\nrainforest.\nhuman life depicted entirely\nout of fractals\nan origami pig on fire\nin the middle of a\ndark room with a\npentagram on the\nfloor\nan old rusted robot wearing pants and a jacket riding skis in a supermarket.\nsmiling cartoon dog sits at a table, coffee mug on hand, as a room goes up in flames. \u201cThis is fine,\u201d the\ndog assures himself.\nA whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. This imaginative creature features the distinctive, bulky body of a hippo, but with a texture and\nappearance resembling a golden-brown, crispy waffle. The creature might have elements like waffle squares across its skin and a syrup-like sheen. It\u2019s set in a surreal environment that playfully combines a\nnatural water habitat of a hippo with elements of a breakfast table setting, possibly including oversized utensils or plates in the background. The image should evoke a sense of playful absurdity and culinary\nfantasy.\n8\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 4. Training dynamics of model architectures. Compara-\ntive analysis of DiT, CrossDiT, UViT, and MM-DiT on CC12M,\nfocusing on validation loss, CLIP score, and FID. Our proposed\nMM-DiT performs favorably across all metrics.\nshared set of weights for all modalities. Finally, we consider\nthe UViT (Hoogeboom et al., 2023) architecture as a hybrid\nbetween the widely used UNets and transformer variants.\nWe analyze the convergence behavior of these architectures\nin Figure 4: Vanilla DiT underperforms UViT. The cross-\nattention DiT variant CrossDiT achieves better performance\nthan UViT, although UViT seems to learn much faster ini-\ntially. Our MM-DiT variant significantly outperforms the\ncross-attention and vanilla variants. We observe only a small\ngain when using three parameter sets instead of two (at the\ncost of increased parameter count and VRAM usage), and\nthus opt for the former option for the remainder of this work.\n5.3. Training at Scale\nBefore scaling up, we filter and preencode our data to ensure\nsafe and efficient pretraining. Then, all previous consider-\nations of diffusion formulations, architectures, and data\nculminate in the last section, where we scale our models up\nto 8B parameters.\n5.3.1. DATA PREPROCESSING\nPre-Training Mitigations\nTraining data significantly im-\npacts a generative model\u2019s abilities. Consequently, data\nfiltering is effective at constraining undesirable capabili-\nties (Nichol, 2022). Before training at sale, we filter our\ndata for the following categories: (i) Sexual content: We\nuse NSFW-detection models to filter for explicit content.\n(ii) Aesthetics: We remove images for which our rating\nsystems predict a low score. (iii) Regurgitation: We use a\ncluster-based deduplication method to remove perceptual\nand semantic duplicates from the training data; see Ap-\npendix E.2.\nPrecomputing Image and Text Embeddings\nOur model\nuses the output of multiple pretrained, frozen networks as in-\nputs (autoencoder latents and text encoder representations).\nSince these outputs are constant during training, we precom-\npute them once for the entire dataset. We provide a detailed\ndiscussion of our approach in Appendix E.1.\nFigure 5. Effects of QK-normalization. Normalizing the Q- and\nK-embeddings before calculating the attention matrix prevents the\nattention-logit growth instability (left), which causes the attention\nentropy to collapse (right) and has been previously reported in the\ndiscriminative ViT literature (Dehghani et al., 2023; Wortsman\net al., 2023). In contrast with these previous works, we observe\nthis instability in the last transformer blocks of our networks. Max-\nimum attention logits and attention entropies are shown averaged\nover the last 5 blocks of a 2B (d=24) model.\n5.3.2. FINETUNING ON HIGH RESOLUTIONS\nQK-Normalization\nIn general, we pretrain all of our mod-\nels on low-resolution images of size 2562 pixels. Next, we\nfinetune our models on higher resolutions with mixed as-\npect ratios (see next paragraph for details). We find that,\nwhen moving to high resolutions, mixed precision train-\ning can become unstable and the loss diverges. This can\nbe remedied by switching to full precision training \u2014 but\ncomes with a \u223c 2\u00d7 performance drop compared to mixed-\nprecision training. A more efficient alternative is reported\nin the (discriminative) ViT literature: Dehghani et al. (2023)\nobserve that the training of large vision transformer models\ndiverges because the attention entropy grows uncontrollably.\nTo avoid this, Dehghani et al. (2023) propose to normalize\nQ and K before the attention operation. We follow this\napproach and use RMSNorm (Zhang & Sennrich, 2019)\nwith learnable scale in both streams of our MMDiT archi-\ntecture for our models, see Figure 2. As demonstrated in\nFigure 5, the additional normalization prevents the attention\nlogit growth instability, confirming findings by Dehghani\net al. (2023) and Wortsman et al. (2023) and enables efficient\ntraining at bf16-mixed (Chen et al., 2019) precision when\ncombined with \u03f5 = 10\u221215 in the AdamW (Loshchilov &\nHutter, 2017) optimizer. This technique can also be applied\non pretrained models that have not used qk-normalization\nduring pretraining: The model quickly adapts to the addi-\ntional normalization layers and trains more stably. Finally,\nwe would like to point out that although this method can\ngenerally help to stabilize the training of large models, it is\nnot a universal recipe and may need to be adapted depending\non the exact training setup.\nPositional Encodings for Varying Aspect Ratios\nAfter\ntraining on a fixed 256 \u00d7 256 resolution we aim to (i) in-\ncrease the resolution and resolution and (ii) enable inference\nwith flexible aspect ratios. Since we use 2d positional fre-\n9\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 6. Timestep shifting at higher resolutions. Top right: Hu-\nman quality preference rating when applying the shifting based\non Equation (23). Bottom row: A 5122 model trained and sam-\npled with\np\nm/n = 1.0 (top) and\np\nm/n = 3.0 (bottom). See\nSection 5.3.2.\nquency embeddings we have to adapt them based on the\nresolution. In the multi-aspect ratio setting, a direct inter-\npolation of the embeddings as in (Dosovitskiy et al., 2020)\nwould not reflect the side lengths correctly. Instead we use\na combination of extended and interpolated position grids\nwhich are subsequently frequency embedded.\nFor a target resolution of S2 pixels, we use bucketed sam-\npling (NovelAI, 2022; Podell et al., 2023) such that that each\nbatch consists of images of a homogeneous size H \u00d7 W,\nwhere H \u00b7 W \u2248 S2. For the maximum and minimum\ntraining aspect ratios, this results in the maximum values for\nwidth, Wmax, and height, Hmax, that will be encountered. Let\nhmax = Hmax/16, wmax = Wmax/16 and s = S/16 be the\ncorresponding sizes in latent space (a factor 8) after patching\n(a factor 2). Based on these values, we construct a vertical\nposition grid with the values ((p \u2212 hmax\u2212s\n2\n) \u00b7 256\nS )hmax\u22121\np=0\nand\ncorrespondingly for the horizontal positions. We then center-\ncrop from the resulting positional 2d grid before embedding\nit.\nResolution-dependent shifting of timestep schedules\nIn-\ntuitively, since higher resolutions have more pixels, we need\nmore noise to destroy their signal. Assume we are working\nin a resolution with n = H \u00b7 W pixels. Now, consider a\n\u201dconstant\u201d image, i.e. one where every pixel has the value\nc. The forward process produces zt = (1 \u2212 t)c1 + t\u03f5,\nwhere both 1 and \u03f5 \u2208 Rn. Thus, zt provides n observations\nof the random variable Y = (1 \u2212 t)c + t\u03b7 with c and \u03b7\nin R, and \u03b7 follows a standard normal distribution. Thus,\nE(Y ) = (1 \u2212 t)c and \u03c3(Y ) = t. We can therefore recover\nc via c =\n1\n1\u2212tE(Y ), and the error between c and its sam-\nple estimate \u02c6c =\n1\n1\u2212t\nPn\ni=1 zt,i has a standard deviation of\nFigure 7. Human Preference Evaluation against currrent\nclosed and open SOTA generative image models. Our 8B model\ncompares favorable against current state-of-the-art text-to-image\nmodels when evaluated on the parti-prompts (Yu et al., 2022)\nacross the categories visual quality, prompt following and typogra-\nphy generation.\n\u03c3(t, n) =\nt\n1\u2212t\nq\n1\nn (because the standard error of the mean\nfor Y has deviation\nt\n\u221an). So if one already knows that the\nimage z0 was constant across its pixels, \u03c3(t, n) represents\nthe degree of uncertainty about z0. For example, we imme-\ndiately see that doubling the width and height leads to half\nthe uncertainty at any given time 0 < t < 1. But, we can\nnow map a timestep tn at resolution n to a timestep tm at\nresolution m that results in the same degree of uncertainty\nvia the ansatz \u03c3(tn, n) = \u03c3(tm, m). Solving for tm gives\ntm =\np m\nn tn\n1 + (p m\nn \u2212 1)tn\n(23)\nWe visualize this shifting function in Figure 6. Note that the\nassumption of constant images is not realistic. To find good\nvalues for the shift value \u03b1 := p m\nn during inference, we\napply them to the sampling steps of a model trained at reso-\nlution 1024 \u00d7 1024 and run a human preference study. The\nresults in Figure 6 show a strong preference for samples with\nshifts greater than 1.5 but less drastic differences among the\nhigher shift values. In our subsequent experiments, we thus\nuse a shift value of \u03b1 = 3.0 both during training and sam-\npling at resolution 1024 \u00d7 1024. A qualitative comparison\nbetween samples after 8k training steps with and without\nsuch a shift can be found in Figure 6. Finally, note that\nEquation 23 implies a log-SNR shift of log n\nm similar to\n(Hoogeboom et al., 2023):\n\u03bbtm = 2 log 1 \u2212 tn\np m\nn tn\n(24)\n= \u03bbtn \u2212 2 log \u03b1 = \u03bbtn \u2212 log m\nn .\n(25)\n10\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nAfter the shifted training at resolution 1024\u00d71024, we align\nthe model using Direct Preference Optimization (DPO) as\ndescribed in Appendix C.\n5.3.3. RESULTS\nIn Figure 8, we examine the effect of training our MM-DiT\nat scale. For images, we conduct a large scaling study and\ntrain models with different numbers of parameters for 500k\nsteps on 2562 pixels resolution using preencoded data, c.f.\nAppendix E.1, with a batch size of 4096. We train on 2 \u00d7 2\npatches (Peebles & Xie, 2023), and report validation losses\non the CoCo dataset (Lin et al., 2014) every 50k steps. In\nparticular, to reduce noise in the validation loss signal, we\nsample loss levels equidistant in t \u2208 (0, 1) and compute\nvalidation loss for each level separately. We then average\nthe loss across all but the last (t = 1) levels.\nSimilarly, we conduct a preliminary scaling study of our\nMM-DiT on videos. To this end we start from the pretrained\nimage weights and additionally use a 2x temporal patching.\nWe follow Blattmann et al. (2023b) and feed data to the\npretrained model by collapsing the temporal into the batch\naxis. In each attention layer we rearrange the representation\nin the visual stream and add a full attention over all spatio-\ntemporal tokens after the spatial attention operation before\nthe final feedforward layer. Our video models are trained for\n140k steps with a batch size of 512 on videos comprising\n16 frames with 2562 pixels. We report validation losses on\nthe Kinetics dataset (Carreira & Zisserman, 2018) every 5k\nsteps. Note that our reported FLOPs for video training in\nFigure 8 are only FLOPs from video training and do not\ninclude the FLOPs from image pretraining.\nFor both the image and video domains, we observe a smooth\ndecrease in the validation loss when increasing model size\nand training steps. We find the validation loss to be highly\ncorrelated to comprehensive evaluation metrics (Comp-\nBench (Huang et al., 2023), GenEval (Ghosh et al., 2023))\nand to human preference. These results support the valida-\ntion loss as a simple and general measure of model perfor-\nmance. Our results do not show saturation neither for image\nnot for video models.\nFigure 12 illustrates how training a larger model for longer\nimpacts sample quality. Tab. 5 shows the results of GenEval\nin full.\nWhen applying the methods presented in Sec-\ntion 5.3.2 and increasing training image resolution, our\nbiggest model excels in most categories and outperforms\nDALLE 3 (Betker et al., 2023), the current state of the art in\nprompt comprehension, in overall score.\nOur d = 38 model outperforms current proprietary (Betker\net al., 2023; ide, 2024) and open (Sauer et al., 2023; pla,\n2024; Chen et al., 2023; Pernias et al., 2023) SOTA gener-\native image models in human preference evaluation on the\nModel\nOverall\nObjects\nCounting Colors Position\nColor\nSingle Two\nAttribution\nminDALL-E\n0.23\n0.73\n0.11\n0.12\n0.37\n0.02\n0.01\nSD v1.5\n0.43\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\nPixArt-alpha\n0.48\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\nSD v2.1\n0.50\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\nDALL-E 2\n0.52\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\nSDXL\n0.55\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\nSDXL Turbo\n0.55\n1.00\n0.72\n0.49\n0.80\n0.10\n0.18\nIF-XL\n0.61\n0.97\n0.74\n0.66\n0.81\n0.13\n0.35\nDALL-E 3\n0.67\n0.96\n0.87\n0.47\n0.83\n0.43\n0.45\nOurs (depth=18), 5122\n0.58\n0.97\n0.72\n0.52\n0.78\n0.16\n0.34\nOurs (depth=24), 5122\n0.62\n0.98\n0.74\n0.63\n0.67\n0.34\n0.36\nOurs (depth=30), 5122\n0.64\n0.96\n0.80\n0.65\n0.73\n0.33\n0.37\nOurs (depth=38), 5122\n0.68\n0.98\n0.84\n0.66\n0.74\n0.40\n0.43\nOurs (depth=38), 5122 w/DPO\n0.71\n0.98\n0.89\n0.73\n0.83\n0.34\n0.47\nOurs (depth=38), 10242 w/DPO\n0.74\n0.99\n0.94\n0.72\n0.89\n0.33\n0.60\nTable 5. GenEval comparisons. Our largest model (depth=38)\noutperforms all current open models and DALLE-3 (Betker et al.,\n2023) on GenEval (Ghosh et al., 2023). We highlight the best,\nsecond best, and third best entries. For DPO, see Appendix C.\nrelative CLIP score decrease [%]\n5/50 steps\n10/50 steps\n20/50 steps path length\ndepth=15\n4.30\n0.86\n0.21\n191.13\ndepth=30\n3.59\n0.70\n0.24\n187.96\ndepth=38\n2.71\n0.14\n0.08\n185.96\nTable 6. Impact of model size on sampling efficiency. The table\nshows the relative performance decrease relative to CLIP scores\nevaluated using 50 sampling steps at a fixed seed. Larger models\ncan be sampled using fewer steps, which we attribute to increased\nrobustness and better fitting the straight-path objective of rectified\nflow models, resulting in shorter path lengths. Path length is\ncalculated by summing up \u2225v\u03b8 \u00b7 dt\u2225 over 50 steps.\nParti-prompts benchmark (Yu et al., 2022) in the categories\nvisual aesthetics, prompt following and typography gener-\nation, c.f. Figure 7. For evaluating human preference in\nthese categories, raters were shown pairwise outputs from\ntwo models, and asked to answer the following questions:\nPrompt following: Which image looks more representative\nto the text shown above and faithfully follows it?\nVisual aesthetics: Given the prompt, which image is of\nhigher-quality and aesthetically more pleasing?\nTypography: Which image more accurately shows/displays\nthe text specified in the above description? More accurate\nspelling is preferred! Ignore other aspects.\nLastly, Table 6 highlights an intriguing result: not only do\nbigger models perform better, they also require fewer steps\nto reach their peak performance.\nFlexible Text Encoders\nWhile the main motivation for\nusing multiple text-encoders is boosting the overall model\nperformance (Balaji et al., 2022), we now show that this\nchoice additionally increases the flexibility of our MM-DiT-\nbased rectified flow during inference. As described in Ap-\npendix B.3 we train our model with three text encoders, with\nan individual drop-out rate of 46.3%. Hence, at inference\n11\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 8. Quantitative effects of scaling. We analyze the impact of model size on performance, maintaining consistent training\nhyperparameters throughout. An exception is depth=38, where learning rate adjustments at 3 \u00d7 105 steps were necessary to prevent\ndivergence. (Top) Validation loss smoothly decreases as a function of both model size and training steps for both image (columns 1 and 2)\nand video models (columns 3 and 4). (Bottom) Validation loss is a strong predictor of overall model performance. There is a marked\ncorrelation between validation loss and holistic image evaluation metrics, including GenEval (Ghosh et al., 2023), column 1, human\npreference, column 2, and T2I-CompBench (Huang et al., 2023), column 3. For video models we observe a similar correlation between\nvalidation loss and human preference, column 4. .\nAll text-encoders\nw/o T5 (Raffel et al., 2019)\n\u201cA burger patty, with the bottom bun and lettuce and tomatoes. \u201dCOFFEE\u201d written on it in mustard\u201d\n\u201cA monkey holding a sign reading \u201dScaling transformer models is awesome!\u201d\n\u201cA mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by\ncolorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters\nthrough a nearby window\u201d\nFigure 9. Impact of T5. We observe T5 to be important for\ncomplex prompts e.g. such involving a high degree of detail or\nlonger spelled text (rows 2 and 3). For most prompts, however, we\nfind that removing T5 at inference time still achieves competitive\nperformance.\ntime, we can use an arbitrary subset of all three text encoders.\nThis offers means for trading off model performance for im-\nproved memory efficiency, which is particularly relevant\nfor the 4.7B parameters of T5-XXL (Raffel et al., 2019)\nthat require significant amounts of VRAM. Interestingly, we\nobserve limited performance drops when using only the two\nCLIP-based text-encoders for the text prompts and replac-\ning the T5 embeddings by zeros. We provide a qualitative\nvisualization in Figure 9. Only for complex prompts involv-\ning either highly detailed descriptions of a scene or larger\namounts of written text do we find significant performance\ngains when using all three text-encoders. These observa-\ntions are also verified in the human preference evaluation\nresults in Figure 7 (Ours w/o T5). Removing T5 has no\neffect on aesthetic quality ratings (50% win rate), and only a\nsmall impact on prompt adherence (46% win rate), whereas\nits contribution to the capabilities of generating written text\nare more significant (38% win rate).\n6. Conclusion\nIn this work, we presented a scaling analysis of rectified\nflow models for text-to-image synthesis. We proposed a\nnovel timestep sampling for rectified flow training that im-\nproves over previous diffusion training formulations for\nlatent diffusion models and retains the favourable proper-\nties of rectified flows in the few-step sampling regime. We\nalso demonstrated the advantages of our transformer-based\nMM-DiT architecture that takes the multi-modal nature of\nthe text-to-image task into account. Finally, we performed\na scaling study of this combination up to a model size of\n8B parameters and 5 \u00d7 1022 training FLOPs. We showed\nthat validation loss improvements correlate with both exist-\ning text-to-image benchmarks as well as human preference\nevaluations. This, in combination with our improvements in\ngenerative modeling and scalable, multimodal architectures\nachieves performance that is competitive with state-of-the-\nart proprietary models. The scaling trend shows no signs of\nsaturation, which makes us optimistic that we can continue\nto improve the performance of our models in the future.\n12\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nBroader Impact\nThis paper presents work whose goal is to advance the field\nof machine learning in general and image synthesis in par-\nticular. There are many potential societal consequences\nof our work, none of which we feel must be specifically\nhighlighted here. For an extensive discussion of the gen-\neral ramifications of diffusion models, we point interested\nreaders towards (Po et al., 2023).\nReferences\nIdeogram v1.0 announcement, 2024. URL https://ab\nout.ideogram.ai/1.0.\nPlayground v2.5 announcement, 2024. URL https://bl\nog.playgroundai.com/playground-v2-5/.\nAlbergo, M. S. and Vanden-Eijnden, E. Building normaliz-\ning flows with stochastic interpolants, 2022.\nAtchison, J. and Shen, S. M. Logistic-normal distributions:\nSome properties and uses. Biometrika, 67(2):261\u2013272,\n1980.\nautofaiss. autofaiss, 2023. URL https://github.c\nom/criteo/autofaiss.\nBalaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nB., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image\ndiffusion models with an ensemble of expert denoisers,\n2022.\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving\nimage generation with better captions. Computer Science.\nhttps://cdn. openai. com/papers/dall-e-3. pdf, 2(3), 2023.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\nKilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V.,\nLetts, A., et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv preprint\narXiv:2311.15127, 2023a.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models,\n2023b.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 18392\u201318402, 2023.\nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag,\nV., Tramer, F., Balle, B., Ippolito, D., and Wallace, E.\nExtracting training data from diffusion models. In 32nd\nUSENIX Security Symposium (USENIX Security 23), pp.\n5253\u20135270, 2023.\nCarreira, J. and Zisserman, A. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset, 2018.\nChangpinyo, S., Sharma, P. K., Ding, N., and Soricut,\nR. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\n2021\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pp. 3557\u20133567, 2021. URL\nhttps://api.semanticscholar.org/Corp\nusID:231951742.\nChen, D., Chou, C., Xu, Y., and Hseu, J. Bfloat16: The\nsecret to high performance on cloud tpus, 2019. URL\nhttps://cloud.google.com/blog/produc\nts/ai-machine-learning/bfloat16-the-s\necret-to-high-performance-on-cloud-t\npus?hl=en.\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,\nZ., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-a: Fast\ntraining of diffusion transformer for photorealistic text-\nto-image synthesis, 2023.\nChen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud,\nD. K. Neural ordinary differential equations. In Neural\nInformation Processing Systems, 2018. URL https:\n//api.semanticscholar.org/CorpusID:49\n310446.\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR).\nIEEE, 2023. doi: 10.1109/cvpr52729.2023.00276. URL\nhttp://dx.doi.org/10.1109/CVPR52729.2\n023.00276.\nDai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R.,\nZhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M.,\nKadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y.,\nPetrovic, V., Singh, M. K., Motwani, S., Wen, Y., Song,\nY., Sumbaly, R., Ramanathan, V., He, Z., Vajda, P., and\nParikh, D. Emu: Enhancing image generation models\nusing photogenic needles in a haystack, 2023.\nDao, Q., Phung, H., Nguyen, B., and Tran, A. Flow match-\ning in latent space, 2023.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R.,\nAlabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,\nM., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., van Steenkiste, S.,\n13\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar,\nV., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov,\nA., Paveti\u00b4c, F., Tran, D., Kipf, T., Lu\u02c7ci\u00b4c, M., Zhai, X.,\nKeysers, D., Harmsen, J., and Houlsby, N. Scaling vision\ntransformers to 22 billion parameters, 2023.\nDhariwal, P. and Nichol, A. Diffusion models beat gans on\nimage synthesis, 2021.\nDockhorn, T., Vahdat, A., and Kreis, K. Score-based gener-\native modeling with critically-damped langevin diffusion.\narXiv preprint arXiv:2112.07068, 2021.\nDockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher-\norder denoising diffusion solvers, 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. ICLR,\n2020.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\nmanidis, A. Structure and content-guided video synthesis\nwith diffusion models, 2023.\nEuler, L. Institutionum calculi integralis. Number Bd. 1 in\nInstitutionum calculi integralis. imp. Acad. imp. Sa`ent.,\n1768. URL https://books.google.de/book\ns?id=Vg8OAAAAQAAJ.\nFischer, J. S., Gui, M., Ma, P., Stracke, N., Baumann, S. A.,\nand Ommer, B. Boosting latent diffusion with flow match-\ning. arXiv preprint arXiv:2312.07360, 2023.\nGhosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An\nobject-focused framework for evaluating text-to-image\nalignment. arXiv preprint arXiv:2310.11513, 2023.\nGupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L.,\nEssa, I., Jiang, L., and Lezama, J. Photorealistic video\ngeneration with diffusion models, 2023.\nHessel, J., Holtzman, A., Forbes, M., Le Bras, R., and\nChoi, Y. Clipscore: A reference-free evaluation metric for\nimage captioning. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 2021. doi:\n10.18653/v1/2021.emnlp-main.595. URL http://dx\n.doi.org/10.18653/v1/2021.emnlp-main.\n595.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance,\n2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\nand Salimans, T. Imagen video: High definition video\ngeneration with diffusion models, 2022.\nHoogeboom, E., Heek, J., and Salimans, T. Simple diffusion:\nEnd-to-end diffusion for high resolution images, 2023.\nHuang, K., Sun, K., Xie, E., Li, Z., and Liu, X.\nT2i-\ncompbench: A comprehensive benchmark for open-world\ncompositional text-to-image generation. arXiv preprint\narXiv:2307.06350, 2023.\nHyv\u00a8arinen, A. Estimation of non-normalized statistical\nmodels by score matching. J. Mach. Learn. Res., 6:695\u2013\n709, 2005. URL https://api.semanticschola\nr.org/CorpusID:1152227.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models,\n2020.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nArXiv, abs/2206.00364, 2022. URL https://api.se\nmanticscholar.org/CorpusID:249240415.\nKarras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,\nT., and Laine, S. Analyzing and improving the train-\ning dynamics of diffusion models.\narXiv preprint\narXiv:2312.02696, 2023.\nKingma, D. P. and Gao, R. Understanding diffusion ob-\njectives as the elbo with simple data augmentation. In\nThirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. arXiv preprint\narXiv:2107.06499, 2021.\nLee, S., Kim, B., and Ye, J. C. Minimizing trajectory curva-\nture of ode-based generative models, 2023.\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion noise\nschedules and sample steps are flawed. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 5404\u20135411, 2024.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft COCO:\nCommon Objects in Context, pp. 740\u2013755. Springer In-\nternational Publishing, 2014. ISBN 9783319106021. doi:\n14\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n10.1007/978-3-319-10602-1 48. URL http://dx.d\noi.org/10.1007/978-3-319-10602-1_48.\nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and\nLe, M. Flow matching for generative modeling. In The\nEleventh International Conference on Learning Repre-\nsentations, 2023. URL https://openreview.net\n/forum?id=PqvMRDCJT9t.\nLiu, X., Gong, C., and Liu, Q.\nFlow straight and fast:\nLearning to generate and transfer data with rectified flow,\n2022.\nLiu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow:\nOne step is enough for high-quality diffusion-based text-\nto-image generation, 2023.\nLoshchilov, I. and Hutter, F. Fixing weight decay regular-\nization in adam. ArXiv, abs/1711.05101, 2017. URL\nhttps://api.semanticscholar.org/Corp\nusID:3312944.\nLu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-\nsolver++: Fast solver for guided sampling of diffusion\nprobabilistic models, 2023.\nMa, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-\nEijnden, E., and Xie, S. Sit: Exploring flow and diffusion-\nbased generative models with scalable interpolant trans-\nformers, 2024.\nNichol, A. Dall-e 2 pre-training mitigations.\nhttps:\n//openai.com/research/dall-e-2-pre-t\nraining-mitigations, 2022.\nNichol, A. and Dhariwal, P. Improved denoising diffusion\nprobabilistic models, 2021.\nNovelAI. Novelai improvements on stable diffusion, 2022.\nURL https://blog.novelai.net/novelai\n-improvements-on-stable-diffusion-e10\nd38db82ac.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers.\nIn 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV). IEEE, 2023. doi:\n10.1109/iccv51070.2023.00387. URL http://dx.d\noi.org/10.1109/ICCV51070.2023.00387.\nPernias, P., Rampas, D., Richter, M. L., Pal, C. J., and\nAubreville, M. Wuerstchen: An efficient architecture for\nlarge-scale text-to-image diffusion models, 2023.\nPizzi, E., Roy, S. D., Ravindra, S. N., Goyal, P., and Douze,\nM. A self-supervised descriptor for image copy detection.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 14532\u201314542,\n2022.\nPo, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J. T.,\nBermano, A. H., Chan, E. R., Dekel, T., Holynski, A.,\nKanazawa, A., et al. State of the art on diffusion models\nfor visual computing. arXiv preprint arXiv:2310.07204,\n2023.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M\u00a8uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis, 2023.\nPooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C.,\nAmos, B., Lipman, Y., and Chen, R. T. Q. Multisam-\nple flow matching: Straightening flows with minibatch\ncouplings, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Man-\nning, C. D., and Finn, C. Direct Preference Optimiza-\ntion: Your Language Model is Secretly a Reward Model.\narXiv:2305.18290, 2023.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer, 2019.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents, 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE,\n2022. doi: 10.1109/cvpr52688.2022.01042. URL\nhttp://dx.doi.org/10.1109/CVPR52688.2\n022.01042.\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\ntional Networks for Biomedical Image Segmentation, pp.\n234\u2013241. Springer International Publishing, 2015. ISBN\n9783319245744. doi: 10.1007/978-3-319-24574-4 28.\nURL http://dx.doi.org/10.1007/978-3-3\n19-24574-4_28.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale\nvisual recognition challenge. International Journal of\nComputer Vision, 115:211 \u2013 252, 2014. URL https:\n//api.semanticscholar.org/CorpusID:29\n30547.\n15\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH 2022 Conference\nProceedings, pp. 1\u201310, 2022a.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den-\nton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J.,\nand Norouzi, M. Photorealistic text-to-image diffusion\nmodels with deep language understanding, 2022b.\nSaharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,\nand Norouzi, M. Image super-resolution via iterative\nrefinement. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 45(4):4713\u20134726, 2022c.\nSauer, A., Chitta, K., M\u00a8uller, J., and Geiger, A. Projected\ngans converge faster. Advances in Neural Information\nProcessing Systems, 2021.\nSauer, A., Lorenz, D., Blattmann, A., and Rombach,\nR.\nAdversarial diffusion distillation.\narXiv preprint\narXiv:2311.17042, 2023.\nSheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A.,\nAshual, O., Parikh, D., and Taigman, Y. Emu edit: Precise\nimage editing via recognition and generation tasks. arXiv\npreprint arXiv:2311.10089, 2023.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\nS., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D.,\nGupta, S., and Taigman, Y. Make-a-video: Text-to-video\ngeneration without text-video data, 2022.\nSohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan,\nN., and Ganguli, S. Deep unsupervised learning using\nnonequilibrium thermodynamics. ArXiv, abs/1503.03585,\n2015. URL https://api.semanticscholar.\norg/CorpusID:14888175.\nSomepalli, G., Singla, V., Goldblum, M., Geiping, J., and\nGoldstein, T. Diffusion art or digital forgery? investigat-\ning data replication in diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 6048\u20136058, 2023a.\nSomepalli, G., Singla, V., Goldblum, M., Geiping, J., and\nGoldstein, T.\nUnderstanding and mitigating copying\nin diffusion models. arXiv preprint arXiv:2305.20086,\n2023b.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models, 2022.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution, 2020.\nSong, Y., Sohl-Dickstein, J. N., Kingma, D. P., Kumar,\nA., Ermon, S., and Poole, B. Score-based generative\nmodeling through stochastic differential equations. ArXiv,\nabs/2011.13456, 2020. URL https://api.semant\nicscholar.org/CorpusID:227209335.\nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks,\nJ., Fatras, K., Wolf, G., and Bengio, Y. Improving and\ngeneralizing flow-based generative models with mini-\nbatch optimal transport, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need, 2017.\nVillani, C. Optimal transport: Old and new. 2008. URL\nhttps://api.semanticscholar.org/Corp\nusID:118347220.\nVincent, P. A connection between score matching and de-\nnoising autoencoders. Neural Computation, 23:1661\u2013\n1674, 2011. URL https://api.semanticscho\nlar.org/CorpusID:5560643.\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion Model Alignment Using Direct Preference\nOptimization. arXiv:2311.12908, 2023.\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\nexpert for pretrained language models. arXiv preprint\narXiv:2311.03079, 2023.\nWortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A.,\nAdlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak,\nR., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J.,\nGilmer, J., and Kornblith, S. Small-scale proxies for\nlarge-scale transformer training instabilities, 2023.\nYu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Va-\nsudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling\nAutoregressive Models for Content-Rich Text-to-Image\nGeneration. arXiv:2206.10789, 2022.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In CVPR, pp. 12104\u201312113, 2022.\nZhang, B. and Sennrich, R. Root mean square layer normal-\nization, 2019.\n16\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nSupplementary\nA. Background\nDiffusion Models\n(Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020) generate data by approximating the\nreverse ODE to a stochastic forward process which transforms data to noise. They have become the standard approach for\ngenerative modeling of images (Dhariwal & Nichol, 2021; Ramesh et al., 2022; Saharia et al., 2022b; Rombach et al., 2022;\nBalaji et al., 2022) and videos (Singer et al., 2022; Ho et al., 2022; Esser et al., 2023; Blattmann et al., 2023b; Gupta et al.,\n2023). Since these models can be derived both via a variational lower bound on the negative likelihood (Sohl-Dickstein et al.,\n2015) and score matching (Hyv\u00a8arinen, 2005; Vincent, 2011; Song & Ermon, 2020), various formulations of forward- and\nreverse processes (Song et al., 2020; Dockhorn et al., 2021), model parameterizations (Ho et al., 2020; Ho & Salimans, 2022;\nKarras et al., 2022), loss weightings (Ho et al., 2020; Karras et al., 2022) and ODE solvers (Song et al., 2022; Lu et al., 2023;\nDockhorn et al., 2022) have led to a large number of different training objectives and sampling procedures. More recently,\nthe seminal works of Kingma & Gao (2023) and Karras et al. (2022) have proposed unified formulations and introduced\nnew theoretical and practical insights for training (Karras et al., 2022; Kingma & Gao, 2023) and inference (Karras et al.,\n2022). However, despite these improvements, the trajectories of common ODEs involve partly significant amounts of\ncurvature (Karras et al., 2022; Liu et al., 2022), which requires increased amounts of solver steps and, thus, renders fast\ninference difficult. To overcome this, we adopt rectified flow models whose formulation allows for learning straight ODE\ntrajectories.\nRectified Flow Models\n(Liu et al., 2022; Albergo & Vanden-Eijnden, 2022; Lipman et al., 2023) approach generative\nmodeling by constructing a transport map between two distributions through an ordinary differential equation (ODE). This\napproach has close connections to continuous normalizing flows (CNF) (Chen et al., 2018) as well as diffusion models.\nCompared to CNFs, Rectified Flows and Stochastic Interpolants have the advantage that they do not require simulation\nof the ODE during training. Compared to diffusion models, they can result in ODEs that are faster to simulate than the\nprobability flow ODE (Song et al., 2020) associated with diffusion models. Nevertheless, they do not result in optimal\ntransport solutions, and multiple works aim to minimize the trajectory curvature further (Lee et al., 2023; Tong et al., 2023;\nPooladian et al., 2023). (Dao et al., 2023; Ma et al., 2024) demonstrate the feasibility of rectified flow formulations for\nclass-conditional image synthesis, (Fischer et al., 2023) for latent-space upsampling, and (Liu et al., 2023) apply the reflow\nprocedure of (Liu et al., 2022) to distill a pretrained text-to-image model (Rombach et al., 2022). Here, we are interested in\nrectified flows as the foundation for text-to-image synthesis with fewer sampling steps. We perform an extensive comparison\nbetween different formulations and loss weightings and propose a new timestep schedule for training of rectified flows with\nimproved performance.\nScaling Diffusion Models\nThe transformer architecture (Vaswani et al., 2017) is well known for its scaling properties in\nNLP (Kaplan et al., 2020) and computer vision tasks (Dosovitskiy et al., 2020; Zhai et al., 2022). For diffusion models,\nU-Net architectures (Ronneberger et al., 2015) have been the dominant choice (Ho et al., 2020; Rombach et al., 2022; Balaji\net al., 2022). While some recent works explore diffusion transformer backbones (Peebles & Xie, 2023; Chen et al., 2023;\nMa et al., 2024), scaling laws for text-to-image diffusion models remain unexplored.\n17\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nDetailed pen and ink drawing of a happy pig butcher selling meat in its shop.\na massive alien space ship that is shaped like a pretzel.\nA kangaroo holding a beer,\nwearing ski goggles and\npassionately singing silly\nsongs.\nAn entire universe inside a\nbottle sitting on the shelf at\nwalmart on sale.\nA cheesburger surfing the\nvibe wave at night\nA swamp ogre with a pearl\nearring by Johannes Vermeer\nA car made out of vegetables.\nheat death of the universe,\nline art\nA crab made of cheese on a plate\nDystopia of thousand of workers picking cherries and feeding them into a machine that runs on steam\nand is as large as a skyscraper. Written on the side of the machine: \u201dSD3 Paper\u201d\ntranslucent pig, inside is a smaller pig.\nFilm still of a long-legged cute big-eye anthropomorphic cheeseburger wearing sneakers relaxing on\nthe couch in a sparsely decorated living room.\n18\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\ndetailed pen and ink drawing of a massive complex alien space ship above a farm in the middle of\nnowhere.\nphoto of a bear wearing a suit and tophat in a river in the middle of a forest holding a sign that says \u201dI\ncant bear it\u201d.\ntilt shift aerial photo of a cute city made of sushi on a wooden table in the evening.\ndark high contrast render of a psychedelic tree of life illuminating dust in a mystical cave.\nan anthropomorphic fractal person behind the counter at a fractal themed restaurant.\nbeautiful oil painting of a steamboat in a river in the afternoon. On the side of the river is a large brick\nbuilding with a sign on top that says \u00a8SD3\u00a8.\nan anthopomorphic pink donut with a mustache and cowboy hat standing by a log cabin in a forest\nwith an old 1970s orange truck in the driveway\nfox sitting in front of a computer in a messy room at night. On the screen is a 3d modeling program\nwith a line render of a zebra.\n19\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nB. On Flow Matching\nB.1. Details on Simulation-Free Training of Flows\nFollowing (Lipman et al., 2023), to see that ut(z) generates pt, we note that the continuity equation provides a necessary\nand sufficient condition (Villani, 2008):\nd\ndtpt(x) + \u2207 \u00b7 [pt(x)vt(x)] = 0 \u2194 vt generates probability density path pt.\n(26)\nTherefore it suffices to show that\n\u2212\u2207 \u00b7 [ut(z)pt(z)] = \u2212\u2207 \u00b7 [E\u03f5\u223cN (0,I)ut(z|\u03f5)pt(z|\u03f5)\npt(z) pt(z)]\n(27)\n= E\u03f5\u223cN (0,I) \u2212 \u2207 \u00b7 [ut(z|\u03f5)pt(z|\u03f5)]\n(28)\n= E\u03f5\u223cN (0,I)\nd\ndtpt(z|\u03f5) = d\ndtpt(z),\n(29)\nwhere we used the continuity equation Equation (26) for ut(z|\u03f5) in line Equation (28) to Equation (29) since ut(z|\u03f5)\ngenerates pt(z|\u03f5) and the definition of Equation (6) in line Equation (27)\nThe equivalence of objectives LF M \u21cb LCF M (Lipman et al., 2023) follows from\nLF M(\u0398) = Et,pt(z)||v\u0398(z, t) \u2212 ut(z)||2\n2\n(30)\n= Et,pt(z)||v\u0398(z, t)||2\n2 \u2212 2Et,pt(z)\u27e8v\u0398(z, t) | ut(z)\u27e9 + c\n(31)\n= Et,pt(z)||v\u0398(z, t)||2\n2 \u2212 2Et,pt(z|\u03f5),p(\u03f5)\u27e8v\u0398(z, t) | ut(z|\u03f5)\u27e9 + c\n(32)\n= Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t) \u2212 ut(z|\u03f5)||2\n2 + c\u2032 = LCF M(\u0398) + c\u2032\n(33)\nwhere c, c\u2032 do not depend on \u0398 and line Equation (31) to line Equation (32) follows from:\nEpt(z|\u03f5),p(\u03f5)\u27e8v\u0398(z, t) | ut(z|\u03f5)\u27e9 =\nZ\ndz\nZ\nd\u03f5pt(z|\u03f5)p(\u03f5)\u27e8v\u0398(z, t) | ut(z|\u03f5)\u27e9\n(34)\n=\nZ\ndzpt(z)\u27e8v\u0398(z, t) |\nZ\nd\u03f5pt(z|\u03f5)\npt(z) p(\u03f5)ut(z|\u03f5)\u27e9\n(35)\n=\nZ\ndzpt(z)\u27e8v\u0398(z, t) | ut(z)\u27e9 = Ept(z)\u27e8v\u0398(z, t) | ut(z)\u27e9\n(36)\nwhere we extended with pt(z)\npt(z) in line Equation (35) and used the definition of Equation (6) in line Equation (35) to\nEquation (36).\nB.2. Details on Image and Text Representations\nLatent Image Representation We follow LDM (Rombach et al., 2022) and use a pretrained autoencoder to represent RGB\nimages X \u2208 RH\u00d7W \u00d73 in a smaller latent space x = E(X) \u2208 Rh\u00d7w\u00d7d. We use a spatial downsampling factor of 8, such\nthat h = H\n8 and w = W\n8 , and experiment with different values for d in Section 5.2.1. We always apply the forward process\nfrom Equation 2 in the latent space, and when sampling a representation x via Equation 1, we decode it back into pixel\nspace X = D(x) via the decoder D. We follow Rombach et al. (2022) and normalize the latents by their mean and standard\ndeviation, which are globally computed over a subset of the training data. Figure 10 shows how generative model training\nfor different d evolves as a function of model capacity, as discussed in Section 5.2.1.\nText Representation Similar to the encoding of images to latent representations, we also follow previous approaches\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text conditioning c using pretrained, frozen text models. In\nparticular, for all experiments, we use a combination of CLIP (Radford et al., 2021) models and a encoder-decoder text model.\nSpecifically, we encode c with the text encoders of both a CLIP L/14 model of Radford et al. (2021) as well as an OpenCLIP\nbigG/14 model of Cherti et al. (2023). We concatenate the pooled outputs, of sizes 768 and 1280 respectively, to obtain\na vector conditioning cvec \u2208 R2048. We also concatenate the penultimate hidden representations channel-wise to a CLIP\n20\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 10. FID scores after training flow models with different sizes (parameterized via their depth) on the latent space of different\nautoencoders (4 latent channels, 8 channels and 16 channels) as discussed in Section 5.2.1. As expected, the flow model trained on the\n16-channel autoencoder space needs more model capacity to achieve similar performance. At depth d = 22, the gap between 8-chn and\n16-chn becomes negligible. We opt for the 16-chn model as we ultimately aim to scale to much larger model sizes.\ncontext conditioning cCLIP\nctxt \u2208 R77\u00d72048. Next, we encode c also to the final hidden representation, cT5\nctxt \u2208 R77\u00d74096, of the\nencoder of a T5-v1.1-XXL model (Raffel et al., 2019). Finally, we zero-pad cCLIP\nctxt along the channel axis to 4096 dimensions\nto match the T5 representation and concatenate it along the sequence axis with cT5\nctxt to obtain the final context representation\ncctxt \u2208 R154\u00d74096. These two caption representations, cvec and cctxt, are used in two different ways as described in Section 4.\nB.3. Preliminaries for the Experiments in Section 5.1.\nDatasets We use two datasets to account for the missing of a standard text-to-image benchmark. As a widely used dataset,\nwe convert the ImageNet dataset (Russakovsky et al., 2014) into a dataset suitable for text-to-image models by adding\ncaptions of the form \u201ca photo of a \u2329class name\u232a\u201d to images, where \u2329class name\u232a is randomly chosen from one\nof the provided names for the image\u2019s class label. As a more realistic text-to-image dataset, we use the CC12M dataset\n(Changpinyo et al., 2021) for training.\nOptimization In this experiment, we train all models using a global batch size of 1024 using the AdamW optimizer\n(Loshchilov & Hutter, 2017) with a learning rate of 10\u22124 and 1000 linear warmup steps. We use mixed-precision training\nand keep a copy of the model weights which gets updated every 100 training batches with an exponential moving average\n(EMA) using a decay factor of 0.99. For unconditional diffusion guidance (Ho & Salimans, 2022), we set the outputs of each\nof the three text encoders independently to zero with a probability of 46.4%, such that we roughly train an unconditional\nmodel in 10% of all steps.\nEvaluation As described in Section 5.1, we use CLIP scores, FID and validation losses to evaluate our models regularly\nduring training on the COCO-2014 validation split (Lin et al., 2014).\nAs the loss values differ widely in magnitude and variance for different timesteps, we evaluate them in a stratified way on\neight equally spaced values in the time interval [0, 1].\nTo analyze how different approaches behave under different sampler settings, we produce 1000 samples for each of the\nsamplers which differ in guidance scales as well as number of sampling steps. We evaluate these samples with CLIP scores\nusing CLIP L/14 (Radford et al., 2021) and also compute FID between CLIP L/14 image features of these samples and the\nimages of the validation set. For sampling, we always use a Euler discretization (Euler, 1768) of Equation 1 and six different\nsettings: 50 steps with classifier-free-guidance scales 1.0, 2.5, 5.0, and 5, 10, 25 steps with classifier-free-guidance scale 5.0.\nB.4. Improving SNR Samplers for Rectified Flow Models\nAs described in Section 2, we introduce novel densities \u03c0(t) for the timesteps that we use to train our rectified flow models.\nFigure 11 visualizes the distributions of the logit-normal sampler and the mode sampler introduced in Section 3.1. Notably,\nas we demonstrate in Section 5.1, the logit-normal sampler outperforms the classic uniform rectified flow formulation (Liu\net al., 2022) and established diffusion baselines such as EDM (Karras et al., 2022) and LDM-Linear (Rombach et al., 2022).\n21\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 11. The mode (left) and logit-normal (right) distributions that we explore for biasing the sampling of training timesteps.\n\u201cA raccoon wearing formal clothes, wearing a\ntophat and holding a cane. The raccoon is\nholding a garbage bag. Oil painting in the style\nof abstract cubism.\u201d\n\u201cA bowl of soup that looks like a monster made\nout of plasticine\u201d\n\u201cTwo cups of coffee, one with latte art of a\nheart. The other has latte art of stars.\u201d\n\u201cA smiling sloth is wearing a leather jacket, a\ncowboy hat, a kilt and a bowtie. The sloth is\nholding a quarterstaff and a big book. The sloth\nis standing on grass a few feet in front of a\nshiny VW van with flowers painted on it.\nwide-angle lens from below.\u201d\nFigure 12. Qualitative effects of scaling. Displayed are examples demonstrating the impact of scaling training steps (left to right: 50k,\n200k, 350k, 500k) and model sizes (top to bottom: depth=15, 30, 38) on PartiPrompts, highlighting the influence of training duration and\nmodel complexity.\n22\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nC. Direct Preference Optimization\n\u201ca peaceful lakeside landscape with\nmigrating herd of sauropods\u201d\n\u201ca book with the words \u2018Don\u2019t Panic\u00a1,\nwritten on it\u201d\n2B base\n2B w/ DPO\n8b base\n8b w/ DPO\nFigure 13. Comparison between base models and DPO-finetuned models. DPO-finetuning generally results in more aesthetically pleasing\nsamples with better spelling.\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) is a technique to finetune LLMs with preference data. Recently,\nthis method has been adapted to preference finetuning of text-to-image diffusion models (Wallace et al., 2023). In this\nsection, we verify that our model is also amenable to preference optimization. In particular, we apply the method introduced\nin Wallace et al. (2023) to our 2B and 8B parameter base model. Rather than finetuning the entire model, we introduce\nlearnable Low-Rank Adaptation (LoRA) matrices (of rank 128) for all linear layers as is common practice. We finetune\nthese new parameters for 4k and 2k iteration for the 2B and 8B base model, respectively. We then evaluate the resulting\nmodel in a human preference study using a subset of 128 captions from the Partiprompts set (Yu et al., 2022) (roughly three\nvoter per prompt and comparison). Figure 14 shows that our base models can be effectively tuned for human preference.\nFigure 13 shows samples of the respective base models and DPO-finetuned models.\nD. Finetuning for instruction-based image editing\nA common approach for training instruction based image editing and general image-to-image diffusion models is to\nconcatenate the latents of the input image to the noised latents of the diffusion target along the channel dimension before\nfeeding the input into a U-Net (Brooks et al., 2023; Sheynin et al., 2023; Saharia et al., 2022a;c). We follow the same\napproach, concatenating input and target along the channels before patching, and demonstrate that the same method is\napplicable to our proposed architecture. We finetune the 2B parameter base model on a dataset consisting of image-to-image\nediting tasks similar to the distribution of the InstructPix2Pix dataset (Brooks et al., 2023) as well as inpainting, segmentation,\ncolorization, deblurring and controlnet tasks similar to Emu Edit and Palette (Sheynin et al., 2023; Saharia et al., 2022a).\nAs shown in Fig 15 we observe that the resulting 2B Edit model has the capability to manipulate text in a given image, even\n23\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nPrompt\nQuality\n0\n10\n20\n30\n40\n50\n60\nHuman Preference [ % ]\ndepth=24 (2B)\nbase\nw/ DPO\nPrompt\nQuality\ndepth=38 (8B)\nbase\nw/ DPO\nFigure 14. Human preference evaluation between base models and DPO-finetuned models. Human evaluators prefer DPO-finetuned\nmodels for both prompt following and general quality.\nModel\nMem [GB]\nFP [ms]\nStorage [kB]\nDelta [%]\nVAE (Enc)\n0.14\n2.45\n65.5\n13.8\nCLIP-L\n0.49\n0.45\n121.3\n2.6\nCLIP-G\n2.78\n2.77\n202.2\n15.6\nT5\n19.05\n17.46\n630.7\n98.3\nTable 7. Key figures for preencoding frozen input networks. Mem is the memory required to load the model on the GPU. FP [ms] is\nthe time per sample for the forward pass with per-device batch size of 32. Storage is the size to save a single sample. Delta [%] is how\nmuch longer a training step takes, when adding this into the loop for the 2B MMDiT-Model (568ms/it).\nthough no text manipulation tasks were included in the training data. We were not able to reproduce similar results when\ntraining a SDXL-based (Podell et al., 2023) editing model on the same data.\nE. Data Preprocessing for Large-Scale Text-to-Image Training\nE.1. Precomputing Image and Text Embeddings\nOur model uses the output of multiple pretrained, frozen networks as inputs (autoencoder latents and text encoder repre-\nsentations). Since these outputs are constant during training, we precompute them once for the entire dataset. This comes\nwith two main advantages: (i) The encoders do not need to be available on the GPU during training, lowering the required\nmemory. (ii) The forward encoding pass is skipped during training, saving time and total needed compute after the first\nepoch, see Tab. 7.\nThis approach has two disadvantages: First, random augmentation for each sample every epoch is not possible and we use\nsquare-center cropping during precomputation of image latents. For finetuning our model at higher resolutions, we specify\na number of aspect ratio buckets, and resize and crop to the closest bucket first and then precompute in that aspect ratio.\nSecond, the dense output of the text encoders is particularly large, creating additional storage cost and longer loading times\nduring training (c.f. Tab. 7). We save the embeddings of the language models in half precision, as we do not observe a\ndeterioration in performance in practice.\nE.2. Preventing Image Memorization\nIn the context of generative image models memorization of training samples can lead to a number of issues (Somepalli et al.,\n2023a; Carlini et al., 2023; Somepalli et al., 2023b). To avoid verbatim copies of images by our trained models, we carefully\nscan our training dataset for duplicated examples and remove them.\n24\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nInput\nOutput 1\nOutput 2\nWrite \u201dgo small\ngo home\u201d\ninstead\nGO BIG OR GO UNET\nis written on\nthe blackboard\nchange the\nword to\nUNOT\nmake the\nsign say\nMMDIT rules\nFigure 15. Zero Shot Text manipulation and insertion with the 2B Edit model\nDetails on Deduplication\nIn accordance with the methods outlined by Carlini et al. (2023) and Somepalli et al. (2023a),\nwe opt for SSCD (Pizzi et al., 2022) as the backbone for the deduplication process. The SSCD algorithm is a state-of-the-art\ntechnique for detecting near-duplicate images at scale, and it generates high-quality image embeddings that can be used for\nclustering and other downstream tasks. We also decided to follow Nichol (2022) to decide on a number of clusters N. For\nour experiments, we use N = 16, 000.\nWe utilize autofaiss (2023) for clustering. autofaiss (2023) is a library that simplifies the process of using Faiss (Facebook AI\nSimilarity Search) for large-scale clustering tasks. Specifically, leverage FAISS index factory1 functionality to train a custom\nindex with predefined number of centroids. This approach allows for efficient and accurate clustering of high-dimensional\ndata, such as image embeddings.\nAlgorithm 1 details our deduplication approach. We ran an experiment to see how much data is removed by different SSCD\nthreshold as shown in Figure 16b. Based on these results we selected four thresholds for the final run Figure 16a.\n1https://github.com/facebookresearch/faiss/wiki/The-index-factory\n25\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nE.3. Assessing the Efficacy of our Deduplication Efforts\nCarlini et al. (2023) devise a two-stage data extraction attack that generates images using standard approaches, and flags\nthose that exceed certain membership inference scoring criteria. Carlini et al. (2023) bias their search towards duplicated\ntraining examples because these are orders of magnitude more likely to be memorized than non-duplicated examples\n(Somepalli et al., 2023a;a; Lee et al., 2021).\nTo assess how well our SSCD-based deduplication works, we follow Carlini et al. (2023) to extract memorized samples from\nsmall, specifically for this purpose trained models and compare them before and after deduplication. Two main step of the\nmentioned procedure include: 1) Generate many examples using the diffusion model in the standard sampling manner and\nwith the known prompts. 2) Perform membership inference to separate the model\u2019s novel generations from those generations\nwhich are memorized training examples. Algorithm 2 shows the steps to find the memorized samples based on Carlini et al.\n(2023). Note that we run this techniques two times; one for SD-2.1 model with only exact dedup removal as baseline, and\nfor a model with the SD2.1 architecture but trained on removed exact duplication and near-duplication using SSCD (Pizzi\net al., 2022).\nWe select the 350,000 most-duplicated examples from the training dataset based on SSCD (Pizzi et al., 2022) with threshold\nof 0.5, and generate 500 candidate images for each text prompt to increase the likelihood of finding memorization. The\nintuition is that for diffusion models, with high probability Gen(p; r1) \u2248d Gen(p; r2) for two different random initial seeds\nr1,r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated\nsamples are memorized examples. To compute the distance measure d between two images, we use a modified Euclidean\nl2 distance. In particular, we found that many generations were often spuriously similar according to l2 distance (e.g.,\nthey all had gray backgrounds). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and\nmeasure the maximum of the l2 distance between any pair of image tiles between the two images. Figure 17 shows the\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove\nnear-duplicated samples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we\nalso explore different sizes for cliques. For all clique thresholds, SSCD is able to significantly reduce the number of\nmemorized samples. Specifically, when the clique size is 10, trained SD models on the deduplicated training samples cut off\nat SSCD= 0.5 show a 5\u00d7 reduction in potentially memorized examples.\nAlgorithm 1 Finding Duplicate Items in a Cluster\nRequire: vecs \u2013 List of vectors in a single cluster, items \u2013 List of item IDs corresponding to vecs, index \u2013 FAISS index\nfor similarity search within the cluster, thresh \u2013 Threshold for determining duplicates\nOutput: dups \u2013 Set of duplicate item IDs\n1: dups \u2190 new set()\n2: for i \u2190 0 to length(vecs) \u2212 1 do\n3:\nqs \u2190 vecs[i] {Current vector}\n4:\nqid \u2190 items[i] {Current item ID}\n5:\nlims, D, I \u2190 index.range search(qs, thresh)\n6:\nif qid \u2208 dups then\n7:\ncontinue\n8:\nend if\n9:\nstart \u2190 lims[0]\n10:\nend \u2190 lims[1]\n11:\nduplicate indices \u2190 I[start : end]\n12:\nduplicate ids \u2190 new list()\n13:\nfor j in duplicate indices do\n14:\nif items[j] \u0338= qid then\n15:\nduplicate ids.append(items[j])\n16:\nend if\n17:\nend for\n18:\ndups.update(duplicate ids)\n19: end for\n20: Return dups {Final set of duplicate IDs}\n26\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n(a) Final result of SSCD deduplication over the entire dataset\n(b) Result of SSCD deduplication with various thresholds over 1000\nrandom clusters\nFigure 16. Results of deduplicating our training datasets for various filtering thresholds.\nAlgorithm 2 Detecting Memorization in Generated Images\nRequire: Set of prompts P, Number of generations per prompt N, Similarity threshold \u03f5 = 0.15, Memorization threshold\nT\nEnsure: Detection of memorized images in generated samples\n1: Initialize D to the set of most-duplicated examples\n2: for each prompt p \u2208 P do\n3:\nfor i = 1 to N do\n4:\nGenerate image Gen(p; ri) with random seed ri\n5:\nend for\n6: end for\n7: for each pair of generated images xi, xj do\n8:\nif distance d(xi, xj) < \u03f5 then\n9:\nConnect xi and xj in graph G\n10:\nend if\n11: end for\n12: for each node in G do\n13:\nFind largest clique containing the node\n14:\nif size of clique \u2265 T then\n15:\nMark images in the clique as memorized\n16:\nend if\n17: end for\n27\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 17. SSCD-based deduplication prevents memorization. To assess how well our SSCD-based deduplication works, we extract\nmemorized samples from small, specifically for this purpose trained models and compare them before and after deduplication. We plot a\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove near-duplicated\nsamples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we also explore different sizes for cliques.\nFor all clique thresholds, SSCD is able to significantly reduce the number of memorized samples. Specifically, when the clique size is 10,\nmodels on the deduplicated training samples cut off at SSCD= 0.5 show a 5\u00d7 reduction in potentially memorized examples.\n28\n"
  },
  {
    "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
    "link": "https://arxiv.org/pdf/2403.03100.pdf",
    "upvote": "28",
    "text": "NaturalSpeech 3: Zero-Shot Speech Synthesis\nwith Factorized Codec and Diffusion Models\nZeqian Ju12\u2217, Yuancheng Wang3\u2217, Kai Shen41\u2217, Xu Tan1\u2217, Detai Xin15, Dongchao Yang1,\nYanqing Liu1, Yichong Leng1, Kaitao Song1, Siliang Tang4, Zhizheng Wu3, Tao Qin1,\nXiang-Yang Li2, Wei Ye6, Shikun Zhang6, Jiang Bian1, Lei He1, Jinyu Li1, Sheng Zhao1\n1Microsoft Research Asia & Microsoft Azure Speech\n2University of Science and Technology of China\n3The Chinese University of Hong Kong, Shenzhen\n4Zhejiang University, 5The University of Tokyo, 6Peking University\nhttps://aka.ms/speechresearch\nAbstract\nWhile recent large-scale text-to-speech (TTS) models have achieved significant\nprogress, they still fall short in speech quality, similarity, and prosody. Considering\nspeech intricately encompasses various attributes (e.g., content, prosody, timbre,\nand acoustic details) that pose significant challenges for generation, a natural idea\nis to factorize speech into individual subspaces representing different attributes\nand generate them individually. Motivated by it, we propose NaturalSpeech 3, a\nTTS system with novel factorized diffusion models to generate natural speech in\na zero-shot way. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion model\nto generate attributes in each subspace following its corresponding prompt. With\nthis factorization design, NaturalSpeech 3 can effectively and efficiently model\nthe intricate speech with disentangled subspaces in a divide-and-conquer way.\nExperiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS\nsystems on quality, similarity, prosody, and intelligibility. Furthermore, we achieve\nbetter performance by scaling to 1B parameters and 200K hours of training data.\nFactorized Diffusion\nIC In-Context Learning\nCodec Decoder\nCodec Encoder\nSpeech \ud835\udc99\nSpeech \ud835\udc99\n\ud835\udc9b\ud835\udc91 \ud835\udc9b\ud835\udc84 \ud835\udc9b\ud835\udc85\n\ud835\udc89\ud835\udc95\nText \ud835\udc9a\nProsody/Content/Detail Codes\nTimbre Embedding\n\ud835\udc9b\ud835\udc91/\ud835\udc84/\ud835\udc85\n\ud835\udc89\ud835\udc95\nDetail Diffusion\nContent Diffusion\nProsody Diffusion\nDuration Diffusion\nIC\nPhoneme Encoder\n500M\n1B\nModel Size\n0.73\n0.74\n0.75\n0.76\nSim-O\nModel Scaling\nSim-O\n2.6\n2.7\n2.8\n2.9\nWER\nWER\n(a)\n(b)\n1K\n200K\n0.67\n0.68\n0.69\n0.70\n0.71\n0.72\n0.73\n0.74\nSim-O\nData Scaling\nSim-O\n2.8\n2.9\n3.0\n3.1\n3.2\n3.3\n3.4\n3.5\nWER\nWER\n60K \nTraining Data\nFigure 1: (a) Overview of NaturalSpeech 3, with a neural speech codec for speech attribute factoriza-\ntion and a factorized diffusion model. (b) Data and model scaling of NaturalSpeech 3.\n\u2217The first four authors contributed equally to this work, and their names are listed in random order. Corre-\nsponding author: Xu Tan, xuta@microsoft.com\nPreprint. Work in progress.\narXiv:2403.03100v1  [eess.AS]  5 Mar 2024\n1\nIntroduction\nIn recent years, significant advancements have been achieved in text-to-speech (TTS) synthesis.\nTraditional TTS systems [1, 2, 3, 4] are typically trained on limited datasets recorded in studios,\nand thus fail to support high-quality zero-shot speech synthesis. Recent works [5, 6, 7] have made\nconsiderable progress for zero-shot TTS by largely scaling up both the corpus and the model sizes.\nHowever, the synthesis results of these large-scale TTS systems are not satisfactory in terms of voice\nquality, similarity, and prosody.\nThe challenges of inferior results stem from the intricate information embedded in speech, since\nspeech encompasses numerous attributes, such as content, prosody, timbre, and acoustic detail. Previ-\nous works using raw waveform [8, 9] and mel-spectrogram [1, 2, 10, 7, 11] as data representations\nsuffer from these intricate complexities during speech generation. A natural idea is to factorize\nspeech into disentangled subspaces representing different attributes and generate them individually.\nHowever, achieving this kind of disentangled factorization is non-trivial. Previous works [12, 13, 6]\nencode speech into multi-level discrete tokens using a neural audio codec [14, 15] based on residual\nvector quantization (RVQ). Although this approach decomposes speech into different hierarchical\nrepresentations, it does not effectively disentangle the information of different attributes of speech\nacross different RVQ levels and still suffers from modeling complex coupled information.\nTo effectively generate speech with better quality, similarity and prosody, we propose a TTS system\nwith novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically,\n1) we introduce a novel neural speech codec with factorized vector quantization (FVQ), named\nFACodec, to decompose speech waveform into distinct subspaces of content, prosody, timbre, and\nacoustic details and reconstruct speech waveform with these disentangled representations, leveraging\ninformation bottleneck [16, 17], various supervised losses, and adversarial training [18] to enhance\ndisentanglement; 2) we propose a factorized diffusion model, which generates the factorized speech\nrepresentations of duration, content, prosody, and acoustic detail, based on their corresponding\nprompts. This design allows us to use different prompts to control different attributes. The overview\nof our method, referred to NaturalSpeech 3, is shown in Figure 1.\nWe decompose complex speech into subspaces representing different attributes, thus simplifying\nthe modeling of speech representation. This approach offers several advantages: 1) our factorized\ndiffusion model is able to learn these disentangled representations efficiently, resulting in higher\nquality speech generation; 2) by disentangling timbre information in our FACodec, we enable our\nfactorized diffusion model to avoid directly modeling timbre. This reduces learning complexity and\nleads to improved zero-shot speech synthesis; 3) we can use different prompts to control different\nattributes, enhancing the controllability of NaturalSpeech 3.\nBenefiting from these designs, NaturalSpeech 3 has achieved significant improvements in speech\nquality, similarity, prosody, and intelligibility. Specifically, 1) it achieves comparable or better speech\nquality than the ground-truth speech on the LibriSpeech test set in terms of CMOS; 2) it achieves a\nnew SOTA on the similarity between the synthesized speech and the prompt speech (0.64 \u2192 0.67 on\nSim-O, 3.69 \u2192 4.01 on SMOS); 3) it shows a significant improvement in prosody compared to other\nTTS systems with \u22120.16 average MCD (lower is better), +0.21 SMOS; 4) it achieves a SOTA on\nintelligibility (1.94 \u2192 1.81 on WER). Furthermore, we demonstrate the scalability of NaturalSpeech\n3 by scaling it to 1B parameters and 200K hours of training data. Audio samples can be found in\nhttps://speechresearch.github.io/naturalspeech3.\n2\nBackground\nIn this section, we discuss the recent progress in TTS including: 1) zero-shot TTS; 2) speech\nrepresentations in TTS; 3) generation methods in TTS; 4) speech attribute disentanglement.\nZero-shot TTS. Zero-shot TTS aims to synthesize speech for unseen speakers with speech prompts.\nWe can systematically categorize these systems into four groups based on data representation and\nmodelling methods: 1) Discrete Tokens + Autoregressive [6, 19, 20]; 2) Discrete Tokens + Non-\nautoregressive [13, 21, 22]; 3) Continuous Vectors + Autoregressive [23]; 4) Continuous Vectors +\nNon-autoregressive [5, 11, 24, 25]. Discrete tokens are typically derived from neural codec, while\ncontinuous vectors are generally obtained from mel-spectrogram or latents from audio autoencoder\nor codec. In addition to the aforementioned perspectives, we disentangle speech waveforms into\n2\nsubspaces based on attribute disentanglement and propose a factorized diffusion model to generate\nattributes within each subspace, motivated by the principle of divide-and-conquer. Meanwhile, we\ncan reuse previous methods, employing discrete tokens along with autoregressive models.\nSpeech Representations in TTS. Traditional works propose using prior-based speech representation\nsuch as raw waveform [26, 27, 28] or mel-spectrogram [29, 30, 3, 31]. Recently, large-scale TTS\nsystems [6, 13, 5] leverage data-driven representation, i.e., either discrete tokens or continuous vectors\nform an auto-encoder [14, 15, 32]. However, these methods ignore that speech contains various\ncomplex attributes and encounter intricate complexities during speech generation. In this paper, we\nfactorize speech into individual subspaces representing different attributes which can be effectively\nand efficiently modeled.\nGeneration Methods in TTS. Previous works have demonstrated that NAR-based models [3, 33, 34,\n7, 5, 11] enjoy better robustness and generation speed than AR-based models, because they explicitly\nmodel the duration and predict all features simultaneously. Instead, AR-based models [2, 30, 6, 23, 35]\nhave better diversity, prosody, expressiveness, and flexibility than NAR-based models, due to their\nimplicitly duration modeling and token sampling strategy. In this study, we adopt the NAR modeling\napproach and propose a factorized diffusion model to support our disentangled speech representations\nand also extend it to AR modeling approaches. This allows NaturalSpeech 3 to achieve better\nexpressiveness while maintaining stability and generation speed.\nSpeech Attribute Disentanglement. Prior works [36, 37, 38] utilize disentangled representation\nfor speech generation, such as speech content from self-supervised pre-trained models [39, 40, 41],\nfundamental frequency, and timbre, but speech quality is not satisfying. Recently, some works\nexplore attribute disentanglement in neural speech codec. SpeechTokenizer [42] uses HuBERT [43]\nfor semantic distillation, aiming to render the first-layer RVQ representation as semantic information.\nDisen-TF-Codec [44] proposes the disentanglement with content and timbre representation, and\napplies them for zero-shot voice conversion. In this paper, we achieve better disentanglement with\nmore speech attributes including content, prosody, acoustic details and timbre while ensuring high-\nquality reconstruction. We validate such disentanglement can bring about significant improvements\nin zero-shot TTS task.\n3\nNaturalSpeech 3\n3.1\nOverall Architecture\nIn this section, we present NaturalSpeech 3, a cutting-edge system for natural and zero-shot text-\nto-speech synthesis with better speech quality, similarity and controllability. As shown in Figure 1,\nNaturalSpeech 3 consists of 1) a neural speech codec (i.e., FACodec) for attribute disentanglement;\n2) a factorized diffusion model which generates factorized speech attributes. Since the speech\nwaveform is complex and intricately encompasses various attributes, we factorize speech into five\nattributes including: duration, prosody, content, acoustic details, and timbre. Specifically, although\nthe duration can be regarded as an aspect of prosody, we choose to model it explicitly due to our\nnon-autoregressive speech generation design. We use our internal alignment tool to alignment speech\nand phoneme and obtain phoneme-level duration. For other attributes, we implicitly utilize the\nfactorized neural speech codec to learn disentangled speech attribute subspaces (i.e., content, prosody,\nacoustic details, and timbre). Then, we use the factorized diffusion model to generate each speech\nattribute representation. Finally, we employ the codec decoder to reconstruct the waveform with the\ngenerated speech attributes. We introduce the FACodec in Section 3.2 and the factorized diffusion\nmodel in Section 3.3.\n3.2\nFACodec for Attribute Factorization\n3.2.1\nFACodec Model Overview\nWe propose a factorized neural speech codec (i.e., FACodec2) to convert complex speech waveform\ninto disentangled subspaces representing speech attributes of content, prosody, timbre, and acoustic\ndetails and reconstruct high-quality speech waveform from these.\n2We will release the code and pre-trained checkpoint of FACodec soon.\n3\nProsody\nVQ\nContent\nVQ\nAcoustic Detail\nVQ\nNormalized F0\nGRL: Phone\nPhone\nGRL: Normalized F0\nGRL: Phone\nGRL: Normalized F0\n\ud835\udc9b\ud835\udc91\n\ud835\udc9b\ud835\udc84\n\ud835\udc9b\ud835\udc85\nTimbre Extractor\n\ud835\udc89\ud835\udc95\nConditional Layer Norm\nLatent \ud835\udc89\nEnc\nDec\nGRL: Speaker\nSpeaker\n: Gradient Reversal Layer\n: Supervision\nFigure 2: The framework of the FACodec for attribute factorization.\nAs shown in Figure 2, our FACodec consists of a speech encoder, a timbre extractor, three factorized\nvector quantizers (FVQ) for content, prosody, acoustic detail, and a speech decoder. Given a speech x,\n1) following [14, 5], we adopt several convolutional blocks for the speech encoder with a downsample\nrate of 200 for 16KHz speech data (i.e., each frame corresponding to a 12.5ms speech segment) to\nobtain pre-quantization latent h; 2) the timbre extractor is a Transformer encoder which converts\nthe output of the speech encoder h into a global vector ht representing the timbre attributes; 3)\nfor other attribute i (i = p, c, d for prosody, content, and acoustic detail, respectively), we use a\nfactorized vector quantizer (FVQi) to capture fine-grained speech attribute representation and obtain\ncorresponding discrete tokens; 4) the speech decoder mirrors the structure of speech encoder but\nwith much larger parameter amount to ensure high-quality speech reconstruction. We first add the\nrepresentation of prosody, content, and acoustic details together and then fuse the timbre information\nby conditional layer normalization [45] to obtain the input z for the speech decoder. We discuss how\nto achieve better speech attribute disentanglement in the next section.\n3.2.2\nAttribute Disentanglement\nDirectly factorizing speech into different subspaces does not guarantee the disentanglement of speech.\nIn this section, we introduce some techniques to achieve better speech attribute disentanglement: 1)\ninformation bottleneck, 2) supervision, 3) gradient reverse, and 4) detail dropout. Please refer to\nAppendix B.1 for more training details.\nInformation Bottleneck. Inspired by [16, 17], to force the model to remove unnecessary information\n(such as prosody in content subspace), we construct the information bottleneck in prosody, content,\nand acoustic details FVQ by projecting the encoder output into a low-dimensional space (i.e., 8-\ndimension) and subsequently quantize within this low-dimensional space. This technique ensures that\neach code embedding contains less information, facilitating information disentanglement [32, 46].\nAfter quantization, we will project the quantized vector back to original dimension.\nSupervision. To achieve high-quality speech disentanglement, we introduce supervision as auxiliary\ntask for each attribute. For prosody, since pitch is an important part of prosody [37], we take the\npost-quantization latent zp to predict pitch information. We extract the F0 for each frame and use\nnormalized F0 (z-score) as the target. For content, we directly use the phoneme labels as the target\n(we use our internal alignment tool to get the frame-level phoneme labels). For timbre, we apply\nspeaker classification on ht by predicting the speaker ID.\nGradient Reversal. Avoiding the information leak (such as the prosody leak in content) can enhance\ndisentanglement. Inspired by [47], we adopt adversarial classifier with the gradient reversal layer\n(GRL) [48] to eliminate undesired information in latent space. Specifically, for prosody, we apply\nphoneme-GRL (i.e., GRL layer by predicting phoneme labels) to eliminate content information;\nfor content, since the pitch is an important aspect of prosody, we apply F0-GRL to reduce the\nprosody information for simplicity; for acoustic details, we apply both phoneme-GRL and F0-GRL\nto eliminate both content and prosody information. In addition, we apply speaker-GRL on the sum of\nzp, zc, zd to eliminate timbre.\nDetail Dropout. We have the following considerations: 1) empirically, we find that the codec tends\nto preserve undesired information (e.g., content, prosody) in acoustic details subspace since there is\nno supervision; 2) intuitively, without acoustic details, the decoder should reconstruct speech only\n4\nText\nFactorized Diffusion\nIC\ncond\ncond\nLength \nRegulator\nIC\nIn-Context Learning\nProsody/Content/Detail Codes\n\ud835\udc9b\ud835\udc91/\ud835\udc84/\ud835\udc85\nUnmask Token\nMask Token\n\ud835\udc9b\ud835\udc91\nConditioning\ncond\nPrompt Token\nDuration Diffusion\nDuration Target\nReverse\nForward\nDuration Prompt \n. . . . . .\n. . . . . .\nPhoneme \nEncoder\n\ud835\udc9b\ud835\udc84\n\ud835\udc9b\ud835\udc85\nProsody Diffusion\nProsody Target\nReverse\nForward\nProsody Prompt \n. . . . . .\n. . . . . .\nContent Diffusion\nContent Target\nReverse\nForward\nContent Prompt \n. . . . . .\n. . . . . .\ncond\ncond\nDetail Diffusion\nDetail Target\nReverse\nForward\nDetail Prompt \n. . . . . .\n. . . . . .\nFigure 3: The framework of factorized diffusion model, which consists of 1) phoneme encoder, 2)\nduration diffusion and length regulator, 3) prosody diffusion, 4) content diffusion, 5) detail (acoustic\ndetail) diffusion. Note that modules 2-5 shares the same diffusion formulation.\nwith prosody, content and timbre, although in low-quality. Motivated by them, we design the detail\ndropout by randomly masking out zd during the training process with probability p. With detail\ndropout, we achieve the trade-off of disentanglement and reconstruction quality: 1) the codec can\nfully utilize the prosody, content and timbre information to reconstruct the speech to ensure the\ndecouple ability, although in low-quality; 2) we can obtain high-quality speech when the acoustic\ndetails are given.\n3.3\nFactorized Diffusion Model\n3.3.1\nModel Overview\nWe generate speech with discrete diffusion for better generation quality. We have the following\nconsiderations: 1) we factorize speech into the following attributes: duration, prosody, content, and\nacoustic details, and generate them in sequential with specific conditions. Firstly, as we mentioned in\nSection 3.1, due to our non-autoregressive generation design, we first generate duration. Secondly,\nintuitively, the acoustic details should be generated at last; 2) following the speech factorization\ndesign, we only provide the generative model with the corresponding attribute prompt and apply\ndiscrete diffusion in its subspace; 3) to facilitate in-context learning in diffusion model, we utilize the\ncodec to factorize speech prompt into attribute prompts (i.e., content, prosody and acoustic details\nprompt) and generate the target speech attribute with partial noising mechanism following [49, 13].\nFor example, for prosody generation, we directly concatenate prosody prompt (without noise) and\ntarget sequence (with noise) and gradually remove noise from target sequence with prosody prompt.\nWith these thoughts, as shown in Figure 3, we present our factorized diffusion model, which consists\nof a phoneme encoder and speech attribute (i.e., duration, prosody, content, and acoustic details)\ndiffusion modules with the same discrete diffusion formulation: 1) we generate the speech duration\nby applying duration diffusion with duration prompt and phoneme-level textural condition encoded\nby phoneme encoder. Then we apply the length regulator to obtain frame-level phoneme condition\ncph; 2) we generate prosody zp with prosody prompt and phoneme condition cph; 3) we generate\ncontent prosody zc with content prompt and use generated prosody zp and phoneme cph as conditions;\n4) we generate acoustic details zd with acoustic details prompt and use generated prosody, content\nand phoneme zp, zc, cph as conditions. Specifically, we do not explicitly generate the timbre attribute.\nDue to the factorization design in our FACodec, we can obtain timbre from the prompt directly and\ndo not need to generate it. Finally, we synthesize the target speech by combining attributes zp, zc, zd\nand ht and decoding it with codec decoder. We discuss the diffusion formulation in Section 3.3.2.\n3.3.2\nDiffusion Formulation\nForward Process. Denote X = [xi]N\ni=1 the target discrete token sequence, where N is the sequence\nlength, Xp is the prompt discrete token sequence, and C is the condition. The forward process at time\nt is defined as masking a subset of tokens in X with the corresponding binary mask Mt = [mt,i]N\ni=1,\nformulated as Xt = X\u2299Mt, by replacing xi with [MASK] token if mt,i = 1, and otherwise leaving\nxi unmasked if mt,i = 0. mt,i\niid\n\u223c Bernoulli(\u03c3(t)) and \u03c3(t) \u2208 (0, 1] is a monotonically increasing\nfunction. In this paper, \u03c3(t) = sin( \u03c0t\n2T ), t \u2208 (0, T]. Specially, we denote X0 = X for the original\ntoken sequence and XT for the fully masked sequence.\n5\nReverse Process. The reverse process gradually restores X0 by sampling from reverse distribution\nq(Xt\u2212\u2206t|X0, Xt), starting from full masked sequence XT . Since X0 is unavailable in inference,\nwe use the diffusion model p\u03b8, parameterized by \u03b8, to predict the masked tokens conditioned on Xp\nand C, denoted as p\u03b8(X0|Xt, Xp, C). The parameters \u03b8 are optimized to minimize the negative\nlog-likelihood of the masked tokens:\nLmask =\nE\nX\u2208D,t\u2208[0,T ] \u2212\nN\nX\ni=1\nmt,i \u00b7 log(p\u03b8(xi|Xt, Xp, C)).\nThen we can get the reverse transition distribution:\np(Xt\u2212\u2206t|Xt, Xp, C) =\nE\n\u02c6\nX0\u223cp\u03b8(X0|Xt,Xp,C)\nq(Xt\u2212\u2206t| \u02c6X0, Xt).\nInference. During inference, we progressively replace masked tokens, starting from the fully masked\nsequence XT , by iteratively sampling from p(Xt\u2212\u2206t|Xt, Xp, C). Inspire by [50, 51, 52], we\nfirst sample \u02c6X0 from p\u03b8(X0|Xt, Xp, C), and then sample Xt\u2212\u2206t from q(Xt\u2212\u2206t|\u02c6X0, Xt), which\ninvolves remask \u230aN \u00b7 \u03c3(t \u2212 \u2206t)\u230b tokens in \u02c6X0 with the lowest confidence score, where we define the\nconfidence score of \u02c6xi in \u02c6X0 to p\u03b8(\u02c6xi|Xt, Xp, C) if mt,i = 0, otherwise, we set confidence score of\nxi to 1, which means that tokens already unmasked in Xt will not be remasked.\nClassifier-free Guidance. Moreover, we adapt the classifier-free guidance technique [53, 54].\nSpecifically, in training, we do not use the prompt with a probability of pcfg = 0.15. In inference,\nwe extrapolate the model output towards the conditional generation guided by the prompt gcond =\ng(X|Xp) and away from the unconditional generation guncond = g(X), i.e., gcfg = gcond +\u03b1\u00b7(gcond \u2212\nguncond), with a guidance scale \u03b1 selected based on experimental results. We then rescale it through\ngfinal = std(gcond) \u00d7 gcfg/std(gcfg), following [55].\n3.4\nConnections to the NaturalSpeech Series\nNaturalSpeech 3 is an advanced TTS system of the NaturalSpeech series. Compared with the previous\nversions NaturalSpeech [4] and NaturalSpeech 2 [5], NaturalSpeech 3 has the following connections\nand distinctions:\n\u2022 Goal. The NaturalSpeech series aims to generate natural speech with high quality and diversity.\nWe approach this goal in several stages: 1) Achieving high-quality speech synthesis in single-\nspeaker scenarios. To this end, NaturalSpeech [4] generates speech with quality on par with human\nrecordings and only tackles single-speaker recording-studio datasets (e.g., LJSpeech). 2) Achieving\nhigh-quality and diverse speech synthesis on multi-style, multi-speaker, and multi-lingual scenarios.\nBoth NaturalSpeech 2 [5] and NaturalSpeech 3 focus on speech diversity by exploring the zero-shot\nsynthesis ability based on large-scale, multi-speaker, and in-the-wild datasets.\n\u2022 Architecture. The NaturalSpeech series shares the basic components such as encoder/decoder\nfor waveform reconstruction and duration prediction for non-autoregressive speech generation.\nDifferent from NaturalSpeech which utilizes flow-based generative models and NaturalSpeech\n2 which leverages latent diffusion models, NaturalSpeech 3 proposes the concept of factorized\ndiffusion models to generate each factorized speech attribute in a divide-and-conquer way.\n\u2022 Speech Representations. Due to the complexity of speech waveform, the NaturalSpeech series\nuses an encoder/decoder to obtain speech latent for high-quality speech synthesis. NaturalSpeech\nutilizes naive VAE-based continuous representations, NaturalSpeech 2 leverages the continuous\nrepresentations from the neural audio codec with residual vector quantizers, while NaturalSpeech\n3 proposes a novel FACodec to convert complex speech signal into disentangled subspaces (i.e.,\nprosody, content, acoustic details, and timbre) and reduces the speech modeling complexity.\n4\nExperiments and Results\n4.1\nExperimental Settings\nIn this subsection, we introduce the training, inference and evaluation for the Factorized Diffusion\nModel. Please refer to Appendix A.1 for model configuration.\n6\nTable 1: The evaluation results for NaturalSpeech 3 and the baseline methods on LibriSpeech test-\nclean. \u2660 means the results are obtained from the authors. \u2665 means the results directly obtained\nfrom the paper. \u2663 means the results are infered from offical checkpoints. \u2666 means the reproduced\nresults. Abbreviation: LT (LibriTTS), V (VCTK), LJ (LJSpeech), LL\u22c6 (Librilight Small, Medium),\nEX (Expresso), MS (MSSS Kor), NI (NIKL Kor).\nTraining Data\nSim-O \u2191\nSim-R \u2191\nWER\u2193\nCMOS\u2191\nSMOS\u2191\nGround Truth\n-\n0.68\n-\n1.94\n+0.08\n3.85\nVALL-E \u2665\nLibrilight\n-\n0.58\n5.90\n-\n-\nVALL-E \u2666\nLibrilight\n0.47\n0.51\n6.11\n-0.60\n3.46\nNaturalSpeech 2\u2660\nLibrilight\n0.55\n0.62\n1.94\n-0.18\n3.65\nVoicebox\u2660\nSelf-Collected (60kh)\n0.64\n0.67\n2.03\n-0.23\n3.69\nVoicebox\u2666\nLibrilight\n0.48\n0.50\n2.14\n-0.32\n3.52\nMega-TTS 2\u2660\nLibrilight\n0.53\n-\n2.32\n-0.20\n3.63\nUniAudio\u2660\nMixed (165kh)\n0.57\n0.68\n2.49\n-0.25\n3.71\nStyleTTS 2\u2663\nLT + V + LJ\n0.38\n-\n2.49\n-0.21\n3.07\nHierSpeech++\u2663\nLT + LL\u22c6 + EX + MS + NI\n0.51\n-\n6.33\n-0.41\n3.50\nNaturalSpeech 3\nLibrilight\n0.67\n0.76\n1.81\n0.00\n4.01\nImplementation Details. We use Librilight [56], which contains 60K hours of 16KHz unlabeled\nspeech data and around 7000 distinct speakers from LibriVox audiobooks, as the training set. In\nduration diffusion, we further improve the performance by conditioning phoneme-level prosody\ncodes. Specifically, we perform phoneme-level pooling according to duration on the pre-quantized\nvectors, and then feed these phoneme-level representations into the prosody quantizer in our codec\nto obtain the phoneme-level prosody codes. We employ an additional discrete diffusion to generate\nthese in inference. We perform 4 iterations in each diffusion process. We generate duration without\nclassifier-free guidance and generate others with a classifier-free guidance scale of 1.0. This strategy\nresults in 4 \u00d7 2 for phoneme-level prosody, 4 for duration, 4 \u00d7 2 for each token sequence of prosody,\ncontent, and acoustic details, totaling 60 forward passes due to the double computation with classifier-\nfree guidance. Please refer to Appendix B.1 for details of the FACodec and Appendix A.2 for more\ndetails of our factorization diffusion model.\nEvaluation Dataset. We employ two benchmark datasets: 1) LibriSpeech [57] test-clean, a widely-\nused testset for zero-shot TTS task. It contains 40 distinct speakers and 5.4-hour speech. Following [5],\nwe randomly select one sentence for each speaker for LibriSpeech test-clean benchmark. Specifically,\nwe randomly select 3-second clips as prompts from the same speaker\u2019s speech. 2) RAVDESS [58],\nan emotional TTS dataset featuring 24 professional actors (12 female, 12 male) across 8 emotions\n(neutral, calm, happy, sad, angry, fearful, surprise, and disgust) in 2 emotional intensity (normal and\nstrong). We use strong-intensity samples for RAVDESS benchmark. We adopt this benchmark for\nprosody evaluation, considering 1) for the same speaker, speech with the same emotion shares similar\nprosody, while speech with different emotions displays varied prosodies; 2) the benchmark provides\nspeech samples with the same text from the same speaker across eight different emotions.\nEvaluation Metrics. Objective Metrics: In the Librispeech test-clean benchmark, we evaluate\nboth speaker-similarity (SIM-O and SIM-R) and robustness (WER). In specific, 1) for SIM-O and\nSIM-R, we employ the WavLM-TDCNN3 speaker embedding model to assess speaker similarity\nbetween generated samples and the prompt. Results are reported for both similarity to original prompt\n(SIM-O) and reconstructed prompt (SIM-R). 2) for Word Error Rate (WER), we use an ASR model4\nto transcribe generated speech. The model is a CTC-based HuBERT pre-trained on Librilight and\nfine-tuned on the 960 hours training set of LibriSpeech. In the RAVDESS benchmark, we evaluate\nthe prosody similariy (MCD and MCD-Acc). In specific, 1) following [59], we adopt Mel-Ceptral\nDistortion (MCD) for prosody evaluation by measuring the differences between generated samples\nand ground truth samples. We report the results for eight emotions, along with the average result. 2)\nfor MCD-Acc, we evaluate the top-1 emotion accuracy of the generated speech on the RAVDESS\n3https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_\nverification\n4https://huggingface.co/facebook/hubert-large-ls960-ft\n7\nTable 2: The evaluation results for NaturalSpeech 3 and the baseline methods on RAVDESS. \u2660 means\nthe results are obtained from the authors. \u2663 means the results are inferred from official checkpoints.\n\u2666 means the reproduced results. Abbreviation: Avg (average MCD), Acc (MCD-Acc).\nAvg\u2193\nAcc\u2191\nCMOS\u2191\nSMOS\u2191\nGround Truth\n0.00\n1.00\n+0.17\n4.42\nVALL-E \u2666\n5.03\n0.34\n-0.55\n3.80\nNaturalSpeech 2\u2660\n4.56\n0.25\n-0.22\n4.04\nVoicebox\u2666\n4.88\n0.34\n-0.34\n3.92\nMega-TTS 2\u2660\n4.44\n0.39\n-0.20\n4.51\nStyleTTS 2\u2663\n4.50\n0.40\n-0.25\n3.98\nHierSpeech++\u2663\n6.08\n0.30\n-0.37\n3.87\nNaturalSpeech 3\n4.28\n0.52\n0.00\n4.72\nbenchmark for prosodic similarity measures. Specifically, we adopt a K-Nearest-Neighbors (KNN)\nmodel as emotion classifier. We compare MCD distances between the generated speech and the\nground-truth speech from the same speaker, across eight different emotions. Subjective Metrics: We\nemploy comparative mean option score (CMOS) and similarity mean option score (SMOS) in both\ntwo benchmarks to evaluate naturalness and similarity, respectively.\nEvaluation Baselines. We compare NaturalSpeech 3 with baselines: 1) VALL-E [6]. 2) Natural-\nSpeech 2 [5]. 3) Voicebox [11]. 4) Mega-TTS 2 [60]. 5) UniAudio [35]. 6) StyleTTS 2 [24]. 7)\nHierSpeech++ [25]. Please refer to Appendix A.3 for details.\n4.2\nExperimental Results on Zero-shot TTS\nIn this subsection, we compare NaturalSpeech 3 with baselines in terms of: 1) generation quality in\nSection 4.2.1; 2) generation similarity in Section 4.2.2; 3) robustness in Section 4.2.3. Specifically,\nfor generation similarity, we evaluate in two aspects: 1) speaker similarity; 2) prosody similarity.\nPlease refer to Appendix A.4 for latency analysis.\n4.2.1\nGeneration Quality\nTo evaluate speech quality, we conduct CMOS test, with 12 native as the judges. We randomly select\n20 utterances from both LibriSpeech test-clean and RAVDESS benchmarks. As shown in Table\n1, we find that 1) NaturalSpeech 3 is close to the ground-truth recording (\u22120.08 on Librispeech\ntest-clean, and \u22120.17 on RAVDESS), which demonstrates NaturalSpeech 3 can generate high-quality\nand natural speech; 2) NaturalSpeech 3 outperforms baselines by a substantial margin, verifying the\neffectiveness of NaturalSpeech 3 with factorization.\n4.2.2\nGeneration Similarity\nSpeaker Similarity. We evaluate the speech similarity with both objective metrics (Sim-O and Sim-\nR) and subjective metrics (SMOS), with 12 natives as the judges. We randomly select 10 utterances\nfor SMOS test. As shown in Table 1, we find that 1) NaturalSpeech 3 achieves parity in Sim-O and a\n0.16 increase in SMOS with ground truth, which indicates great speaker similarity achieved by our\nproposed method; 2) NaturalSpeech 3 outperforms all baselines on both objective and subjective\nmetrics, highlighting the superiority of our method with factorization in terms of speaker similarity.\nAdditionally, we notice certain discrepancy between Sim-O and SMOS. For instance, the SMOS is\nnot as competitive as SIM-O for Voicebox model, likely due to some unnatural prosody.\nProsody Similarity. We evaluate prosody similarity with both objective metrics (MCD and MCD-\nAcc) and subjective metrics (SMOS) on the RAVDESS benchmark. We randomly select 10 utterances\nfor SMOS test. As shown in Table 2, NaturalSpeech 3 consistently surpasses baselines by a remarkable\nmargin in MCD avg, MCD-Acc, and SMOS. It reveals that NaturalSpeech 3 achieves a significant\nimprovement in terms of prosodic similarity. Please refer to Appendix A.6 for the MCD scores across\n8 emotions.\n8\nTable 3: The ablation study of factorization and classifier-free guidance (cfg) on LibriSpeech test-\nclean.\nSim-O / Sim-R \u2191\nWER\u2193\nCMOS\u2191\nSMOS\u2191\nNaturalSpeech 3\n0.67 / 0.76\n1.81\n0.00\n4.01\n- factorization\n0.55 / 0.61\n2.49\n-0.25\n3.59\n- cfg\n0.64 / 0.72\n1.81\n-0.06\n3.80\nTable 4: The ablation study of prosody representation on RAVDESS. Denote \u201cMel 20 Bins\u201d using\nthe first 20 bins in the mel-spectrogram as the prosody representation.\nMCD Avg\u2193\nMCD-Acc\u2191\nNaturalSpeech 3\n4.28\n0.52\nMel 20 Bins\n4.34\n0.46\n4.2.3\nRobustness\nWe assess the robustness of our zero-shot TTS by measuring the word error rate of generated speech\non the LibriSpeech test-clean benchmark. The results in Table 1 indicate that 1) NaturalSpeech 3\nachieves a better WER than the ground truth, proving the high intelligibility; 2) NaturalSpeech 3\noutperforms other baselines by a considerable margin, which demonstrates the superior robustness of\nNaturalSpeech 3.\n4.3\nAblation Study and Method Analyses\n4.3.1\nAblation Study\nIn this subsection, we conduct ablation studies to verify the effectiveness of 1) factorization; 2)\nclassier-free guidance; 3) prosody representation. We also conduct ablation study to compare our\nduration diffusion model with traditional duration predictor in Appendix A.5.\nFactorization. To verify the proposed factorization method, we ablate it by removing factorization in\nboth codec and factorized diffusion model. Specifically, we 1) use the discrete tokens from Sound-\nStream, a neural codec which does not consider factorization, and 2) do not consider factorization\nin generation. As shown in Table 3, we could find a significant performance degradation without\nthe factorization, a drop of 0.12 in Sim-O, 0.15 in Sim-R, 0.68 in WER, 0.25 in CMOS and 0.42 in\nSMOS. This indicates the proposed factorized method can consistently improve the performance in\nterms of speaker similarity, robustness, and quality.\nClassier-Free Guidance. We conduct an ablation study by dropping the classifier-free guidance in\ninference to validate its effectiveness. We double the iterations to ensure the same 60 forward passes\nfor fair comparison. Table 3 illustrates a significant degradation without classifier-free guidance, a\ndecrease of 0.03 in Sim-O, 0.04 in Sim-R, 0.06 in CMOS and 0.21 in SMOS, proving that classifier-\nfree guidance can greatly help the speaker similarity and quality.\nProsody Representation. We compare different prosody representations on zero-shot TTS task. In\nspecific, we select handcrafted prosody features (e.g., the first 20 bins of mel-spectrogram [7, 61, 62])\nas the baseline. We drop the prosody FVQ module and directly quantize the first 20 bins of the\nmel-spectrogram, without the normalized F0 loss. Table 4 shows that using \u201cMel 20 Bins\u201d as prosody\nrepresentation demonstrates inferiority in terms of prosody similarity compared to the prosody\nrepresentations learned from codec (4.34 vs 4.28 in average MCD, 0.46 vs 0.52 in MCD-Acc).\n4.3.2\nMethod Analyses\nIn this subsection, we first discuss the extensibility of our factorization. We then introduce the\napplication of speech attributes manipulation in a zero-shot way.\nExtensibility. NaturalSpeech 3 utilizes a non-autoregressive model for discrete token generation with\nfactorization design. To validate the extensibility of our proposed factorization method, we further\n9\nTable 5: The reconstruction quality evaluation of codecs. \u2663 means results are infered from offical\ncheckpoints. \u22c6 means the reproduced checkpoint. \u2666 means the reproduced model following the\noriginal paper\u2019s implementation and experimental setup. All models use a codebook size of 1024.\nBold for the best result and underline for the second-best result. Abbreviation: H (Hop Size), N\n(Codebook Number).\nModels\nSampling Rate\nH\nN\nBandwidth\nPESQ \u2191\nSTOI \u2191\nMSTFT \u2193\nMCD \u2193\nEnCodec\u2663\n24kHz\n320\n8\n6.0 kbps\n3.28\n0.94\n0.99\n2.70\nHiFi-Codec\u2663\n16kHz\n320\n4\n2.0 kbps\n3.17\n0.93\n0.98\n3.05\nDAC\u2663\n16kHz\n320\n9\n4.5 kbps\n3.52\n0.95\n0.97\n2.65\nSoundStream\u2666\n16kHz\n200\n6\n4.8 kbps\n3.03\n0.90\n1.07\n3.38\nFACodec\n16kHz\n200\n6\n4.8 kbps\n3.47\n0.95\n0.93\n2.59\nTable 6: The comparison between autoregressive approach with (VALL-E + F) and without (VALL-E)\nour proposed factorization on LibriSpeech test-clean. \u2666 means the reproduced results. Abbreviation:\nSim-O/R (Sim-O / Sim-R).\nSim-O / R \u2191\nWER\u2193\nCMOS\u2191\nSMOS\u2191\nVALL-E + F\n0.57 / 0.65\n5.60\n+0.24\n3.61\nVALL-E\u2666\n0.47 / 0.51\n6.11\n0.00\n3.46\nexplore the autoregressive generative model for discrete token generation under our factorization\nframework. We utilize VALL-E for verification. We first employ an autoregressive language model\nto generate prosody codes, followed by a non-autoregressive model to generate the remaining content\nand acoustic details codes. This approach maintains a consistent order of attribute generation, allowing\nfor a fair comparison. We name it VALL-E + F. As shown in Table 6, VALL-E + F consistently\noutperforms VALL-E by a considerable margin in all objective and subjective metrics, demonstrating\nthe factorization design can enhance VALL-E in speech similarity, quality and generation robustness.\nIt further shows our factorization paradigm is not limited in the proposed factorization diffusion\nmodel and has a large potential in other generative models. We leave it for future work.\nSpeech Attribute Manipulation. As discussed in Section 3.3, our factorized diffusion model enables\nattribute manipulation by selecting different attributes prompts from different speech. We mainly\nfocus on manipulating duration, prosody, and timbre, since the content codes are dictated by the text\nin TTS, and the acoustic details do not carry semantic information. Leveraging the strong in-context\ncapability of NaturalSpeech 3, the generated speech effectively mirrors the corresponding speech\nattributes. For instance, 1) we can utilize the timbre prompt from a different speech to control the\ntimbre while keeping other attributes unchanged; 2) despite the correlation between duration and\nprosody, we can still solely adjust duration prompt to regulate the speed; 3) moreover, we can combine\ndifferent speech attributes from disparate samples as desired. This allow us to mimic the timbre while\nusing different prosody and speech speed. Samples are available on our demo page5.\n4.3.3\nExperimental Results on FACodec\nWe compare the proposed FACodec in terms of the reconstruction quality with strong baselines,\nsuch as EnCodec [15], HiFi-Codec [63], Descript-Audio-Codec (DAC) [32], and our reproduced\nSoundStream [14]. Table 5 shows that our codec significantly surpasses SoundStream in the same\nbandwidth setting (0.44 in PESQ, 0.05 in STOI, 0.14 in MSTFT and 0.79 in MCD, respectively).\nCheck more details in Appendix B.2. Compared with other baselines, FACodec also get comparable\nperformance. Additionally, since our codec decouples timbre information, it can enable zero-shot\nvoice conversion easily, we provide the details and experiment results in Appendix B.3. Appendix\nB.4 shows some ablation studies about our FACodec.\n5https://speechresearch.github.io/naturalspeech3\n10\nTable 7: The performance of NaturalSpeech 3 on an internal test set, with 500M model size and\ndifferent hours of training data.\nSim-O\u2191\nWER\u2193\n1K\n0.69\n3.39\n60K\n0.72\n3.03\n200K\n0.73\n2.83\nTable 8: The performance of NaturalSpeech 3 on an internal test set, with 200K hours of training\ndata and different model sizes.\nSim-O\u2191\nWER\u2193\n500M\n0.73\n2.83\n1B\n0.75\n2.62\n4.4\nEffectiveness of Data and Model Scaling\nIn this section, we study the effectiveness of data and model scaling on the proposed factorized\ndiffusion model. We use the same FACodec trained on LibriLight dataset for fair comparison. We\nevaluate the zero-shot TTS performance in terms of speaker similarity (Sim-O) and robustness (WER)\non an internal test set consisting of 30 audio clips.\nData Scaling. With a fixed model size of 500M parameters, we trained the factorized diffusion model\non three datasets: 1) a 1K-hour subset randomly drawn from the Librilight dataset, 2) a 60K-hour\nLibrilight dataset, and 3) an internal dataset with 200K hours of speech. In Table 7, we observe that:\n1) even with a mere 1K hours of speech data, our model attains a Sim-O score of 0.69 and a WER of\n3.39. It shows that with the speech factorization, NaturalSpeech 3 can generate the speech effectively.\n2) As we scale up training data from 1K hours to 60K hours, and then to 200K hours, NaturalSpeech\n3 displays continuously enhanced performance, with an improvement of 0.03 and 0.04 in terms of\nSim-O, and 0.33 and 0.56 in terms of WER, respectively, thus confirming the benefits of data scaling.\nNote that our method trained on 200K hours is still underfitting and longer training will result in\nbetter performance.\nModel Scaling. We scale up the model size from 500M to 1B parameters with the internal 200K\nhours dataset. Specifically, we double the number of transformer layers from 12 to 24. The results\nin Table 8 show a boost in both speaker similarity (0.02 in Sim-O) and robustness (0.21 in WER),\nvalidating the effectiveness of model scaling. In the future, we will scale up the model size even\nlarger to achieve better results.\n5\nConclusion\nIn this paper, we develop a TTS system that consists of 1) a novel neural speech codec with factorized\nvector quantization (i.e., FACodec) to decompose speech waveform into distinct subspaces of content,\nprosody, acoustic details and timbre and 2) novel factorized diffusion model to synthesize speech\nby generating attributes in subspaces with discrete diffusion. NaturalSpeech 3 outperforms the\nstate-of-art TTS system on speech quality, similarity, prosody, and intelligibility. We also show that\nNaturalSpeech 3 can enable speech attribute manipulation, by customizing speech attribute prompts.\nFurthermore, we demonstrate that NaturalSpeech 3 achieves better performance by scaling to 1B\nparameters and 200K hours of training data. We list the limitations and future works in Appendix C.\n6\nBoarder Impact\nSince our model could synthesize speech with great speaker similarity, it may carry potential risks in\nmisuse of the model, such as spoofing voice identification or impersonating a specific speaker. We\nconducted the experiments under the assumption that the user agree to be the target speaker in speech\nsynthesis. To prevent misuse, it is crucial to develop a robust synthesized speech detection model and\nestablish a system for individuals to report any suspected misuse.\n11\nReferences\n[1] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end\nspeech synthesis. Proc. Interspeech 2017, pages 4006\u20134010, 2017.\n[2] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,\nZhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by\nconditioning WaveNet on mel spectrogram predictions. In 2018 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 4779\u20134783. IEEE, 2018.\n[3] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech:\nFast, robust and controllable text to speech. In NeurIPS, 2019.\n[4] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong\nLeng, Yuanhao Yi, Lei He, et al. NaturalSpeech: End-to-end text to speech synthesis with\nhuman-level quality. arXiv preprint arXiv:2205.04421, 2022.\n[5] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and\nJiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and\nsinging synthesizers. arXiv preprint arXiv:2304.09116, 2023.\n[6] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen,\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text\nto speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.\n[7] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie\nHuang, Chunfeng Wang, Xiang Yin, et al. Mega-tts: Zero-shot text-to-speech at scale with\nintrinsic inductive bias. arXiv preprint arXiv:2306.03509, 2023.\n[8] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. arXiv preprint arXiv:2106.06103, 2021.\n[9] Dan Lim, Sunghee Jung, and Eesung Kim. Jets: Jointly training fastspeech2 and hifi-gan for\nend to end text to speech. arXiv preprint arXiv:2203.16852, 2022.\n[10] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-\nTTS: A diffusion probabilistic model for text-to-speech. arXiv preprint arXiv:2105.06337,\n2021.\n[11] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary\nWilliamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilin-\ngual universal speech generation at scale. arXiv preprint arXiv:2306.15687, 2023.\n[12] Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt\nSharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a\nlanguage modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.\n[13] Zal\u00e1n Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco\nTagliasacchi. Soundstorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636,\n2023.\n[14] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 2021.\n[15] Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\n[16] Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, and David Cox. Unsuper-\nvised speech decomposition via triple information bottleneck. In International Conference on\nMachine Learning, pages 7836\u20137846. PMLR, 2020.\n12\n[17] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. AutoVC:\nZero-shot voice style transfer with only autoencoder loss. In International Conference on\nMachine Learning, pages 5210\u20135219. PMLR, 2019.\n[18] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks\nfor efficient and high fidelity speech synthesis. Advances in Neural Information Processing\nSystems, 33, 2020.\n[19] Eugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier\nPietquin, Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt:\nHigh-fidelity text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023.\n[20] Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue\nJiang, Chao Weng, Zhou Zhao, and Dong Yu. Make-a-voice: Unified voice synthesis with\ndiscrete representation. arXiv preprint arXiv:2305.19269, 2023.\n[21] Dongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and\nDong Yu. Instructtts: Modelling expressive tts in discrete latent space with natural language\nstyle prompt. arXiv preprint arXiv:2301.13662, 2023.\n[22] Chenpeng Du, Yiwei Guo, Feiyu Shen, Zhijun Liu, Zheng Liang, Xie Chen, Shuai Wang, Hui\nZhang, and Kai Yu. Unicats: A unified context-aware text-to-speech framework with contextual\nvq-diffusion and vocoding. arXiv preprint arXiv:2306.07547, 2023.\n[23] Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mar-\niooryad, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Lms with a voice: Spoken\nlanguage modeling beyond speech tokens. arXiv preprint arXiv:2305.15255, 2023.\n[24] Yinghao Aaron Li, Cong Han, Vinay S Raghavan, Gavin Mischler, and Nima Mesgarani.\nStyletts 2: Towards human-level text-to-speech through style diffusion and adversarial training\nwith large speech language models. arXiv preprint arXiv:2306.07691, 2023.\n[25] Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, and Seong-Whan Lee. Hierspeech++:\nBridging the gap between semantic and acoustic representation of speech by hierarchical\nvariational inference for zero-shot speech synthesis. arXiv preprint arXiv:2311.12454, 2023.\n[26] A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[27] A\u00e4ron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray\nKavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel\nWaveNet: Fast high-fidelity speech synthesis. In International conference on machine learning,\npages 3918\u20133926. PMLR, 2018.\n[28] Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, A\u00e4ron Courville,\nand Yoshua Bengio. Char2wav: End-to-end speech synthesis. 2017.\n[29] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. Proc.\nICLR, pages 214\u2013217, 2018.\n[30] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis\nwith Transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 6706\u20136713, 2019.\n[31] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-TTS: A generative flow\nfor text-to-speech via monotonic alignment search. Advances in Neural Information Processing\nSystems, 33, 2020.\n[32] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar.\nHigh-fidelity audio compression with improved rvqgan. arXiv preprint arXiv:2306.06546,\n2023.\n13\n[33] Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron Weiss, and Yonghui Wu. Parallel\nTacotron: Non-autoregressive and controllable TTS. arXiv preprint arXiv:2010.11439, 2020.\n[34] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. DiffSinger: Singing voice\nsynthesis via shallow diffusion mechanism. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 11020\u201311028, 2022.\n[35] Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang,\nJiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foundation model\ntoward universal audio generation. arXiv preprint arXiv:2310.00704, 2023.\n[36] Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee, Hoon Heo, and Kyogu Lee. Neural\nanalysis and synthesis: Reconstructing speech from self-supervised representations. Advances\nin Neural Information Processing Systems, 34:16251\u201316265, 2021.\n[37] Hyeong-Seok Choi, Jinhyeok Yang, Juheon Lee, and Hyeongju Kim. Nansy++: Unified voice\nsynthesis with neural analysis and synthesis. arXiv preprint arXiv:2211.09407, 2022.\n[38] Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu,\nAbdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled\nself-supervised representations. arXiv preprint arXiv:2104.00355, 2021.\n[39] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and\nYonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling\nfor self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU), pages 244\u2013250. IEEE, 2021.\n[40] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.\nwav2vec 2.0:\nA framework for self-supervised learning of speech representations. Advances in Neural\nInformation Processing Systems, 33:12449\u201312460, 2020.\n[41] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised\npre-training for speech recognition. Proc. Interspeech 2019, pages 3465\u20133469, 2019.\n[42] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified\nspeech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023.\n[43] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed.\nHubert: Self-supervised speech representation learning by\nmasked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 29:3451\u20133460, 2021.\n[44] Xue Jiang, Xiulian Peng, Yuan Zhang, and Yan Lu. Disentangled feature learning for real-time\nneural speech coding. In ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.\n[45] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, sheng zhao, and Tie-Yan Liu.\nAdaSpeech: Adaptive text to speech for custom voice. In International Conference on Learning\nRepresentations, 2021.\n[46] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021.\n[47] SiCheng Yang, Methawee Tantrawenith, Haolin Zhuang, Zhiyong Wu, Aolan Sun, Jianzong\nWang, Ning Cheng, Huaizhen Tang, Xintao Zhao, Jie Wang, et al. Speech representation\ndisentanglement with adversarial mutual information learning for one-shot voice conversion.\narXiv preprint arXiv:2208.08757, 2022.\n[48] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.\nIn International conference on machine learning, pages 1180\u20131189. PMLR, 2015.\n[49] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence\nto sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022.\n14\n[50] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffu-\nsion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687,\n2022.\n[51] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706,\n2022.\n[52] Jos\u00e9 Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation\nwith token-critic. In European Conference on Computer Vision, pages 70\u201386. Springer, 2022.\n[53] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[54] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n[55] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and\nsample steps are flawed. In Proceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pages 5404\u20135411, 2024.\n[56] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-\nEmmanuel Mazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen,\net al. Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n7669\u20137673. IEEE, 2020.\n[57] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In 2015 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210. IEEE, 2015.\n[58] Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional\nspeech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north\namerican english. PloS one, 13(5):e0196391, 2018.\n[59] Mohammed Salah Al-Radhi, Tam\u00e1s G\u00e1bor Csap\u00f3, and G\u00e9za N\u00e9meth. Nonparallel expressive tts\nfor unseen target speaker using style-controlled adaptive layer and optimized pitch embedding.\nIn 2023 International Conference on Speech Technology and Human-Computer Dialogue\n(SpeD), pages 176\u2013181. IEEE, 2023.\n[60] Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng\nWang, Xiang Yin, Zejun Ma, et al. Mega-tts 2: Zero-shot text-to-speech with arbitrary length\nspeech prompts. arXiv preprint arXiv:2307.07218, 2023.\n[61] Hyung-Seok Oh, Sang-Hoon Lee, and Seong-Whan Lee. Diffprosody: Diffusion-based latent\nprosody generation for expressive speech synthesis with prosody conditional adversarial training.\narXiv preprint arXiv:2307.16549, 2023.\n[62] Yi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang, Qian Chen, Zhijie Yan, and Zhou Zhao.\nProsospeech: Enhancing prosody with quantized vector pre-training in text-to-speech. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7577\u20137581. IEEE, 2022.\n[63] Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou.\nHifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint\narXiv:2305.02765, 2023.\n[64] Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Tao Qin, and Tie-Yan Liu. Token-\nlevel ensemble distillation for grapheme-to-phoneme conversion. In INTERSPEECH, 2019.\n15\n[65] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked\ngenerative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11315\u201311325, 2022.\n[66] Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and\nHiroshi Saruwatari. Utmos: Utokyo-sarulab system for voicemos challenge 2022. arXiv\npreprint arXiv:2204.02152, 2022.\n[67] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: A\nuniversal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022.\n[68] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,\nShibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented\ntransformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.\n[69] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation\nlearning. In Proceedings of the 31st International Conference on Neural Information Processing\nSystems, pages 6309\u20136318, 2017.\n[70] Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge,\nand Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone. In International Conference on Machine Learning, pages 2709\u20132720. PMLR,\n2022.\n[71] Zhichao Wang, Yuanzhe Chen, Lei Xie, Qiao Tian, and Yuping Wang. Lm-vc: Zero-shot voice\nconversion via speech generation based on language models. arXiv preprint arXiv:2306.10521,\n2023.\n16\nA\nDetails of Factorization Diffusion Model\nA.1\nModel Configuration\nThe phoneme encoder uses a similar architecture as [5] and comprises a 6-layer Transformer with 8\nattention heads, 512 embedding dimensions, filter size 2048 and kernel size 9 for 1D convolution,\nand a dropout of 0.1. In prosody, content and acoustic details diffusion, we adopt a shared 12-layer\nTransformer, with 8 attention heads, 1024 embedding dimensions, filter size 2048 and kernel size\n3 for 1D convolution, and a dropout of 0.1. We additionally use conditional layer normalization\nin each Transformer block to support diffusion time input. In phoneme-level prosody and duration\ndiffusion, we adopt a 6-layer Transformer with 8 attention heads, 1024 embedding dimensions, filter\nsize 2048 and kernel size 3 for 1D convolution, and a dropout of 0.1. We also use conditional layer\nnormalization in the model to support diffusion time input.\nA.2\nTraining and Inference Details\nWe use Librilight [56], which contains 60K hours of 16KHz unlabeled speech data and around 7000\ndistinct speakers from LibriVox audiobooks, as the training set. We transcribe using an internal ASR\nsystem, convert transcriptions to phonemes via grapheme-to-phoneme conversion [64], and obtain\nduration with an internal alignment tool. We use 8 A100 80GB GPUs with a batch size of 10K frames\nof latent vectors per GPU for 1M steps. We use the AdamW optimizer with a learning rate of 1e \u2212 4,\n\u03b21 = 0.9, and \u03b22 = 0.98, 5K warmup steps following the inverse square root learning schedule.\nDuring inference, we perform 4 iterations in each diffusion process, including phoneme-level prosody,\nduration, prosody, content and acoustic details diffusion. We generate duration without classifier-free\nguidance, and generate others with a classifier-free guidance scale of 1.0. This strategy results a\n4 \u00d7 2 for phoneme-level prosody, 4 for duration, 4 \u00d7 2 for each token sequence of prosody, content\nand acoustic details, totaling 60 forward passes due to the double computation with classifier-free\nguidance. We use a top-k of 20, with sampling temperature annealing from 1.5 to 0. Following\n[65], Gumbel noises are added to token confidences when determining which positions to re-mask in\nq(Xt\u2212\u2206t|\u02c6X0, Xt), mentioned in Section 3.3.2.\nA.3\nEvaluation Baselines\nWe compare NaturalSpeech 3 with following strong zero-shot TTS baselines:\n\u2022 VALL-E [6]. It use an autoregressive and an additional non-autoregressive model for discrete\ntoken generation. We report the scores directly obtained from the paper. We additionally\nreproduce it using discrete tokens from SoundStream on Librilight.\n\u2022 NaturalSpeech 2 [5]. It use a non-autoregressive model for continuous vectors generation.\nWe obtain samples through communication with the authors.\n\u2022 Voicebox [11]. It use a non-autoregressive model for continuous vectors generation. We\nobtain samples through communication with the authors. We additionally reproduce it using\nmel-spectrogram on Librilight.\n\u2022 Mega-TTS 2 [60]. It use a non-autoregressive model for continuous vectors generation. We\nobtain samples through communication with the authors.\n\u2022 UniAudio [35]. It use an autoregressive model for discrete token generation. We obtain\nsamples through communication with the authors.\n\u2022 StyleTTS 2 [24]. It use a non-autoregressive model for continuous vectors generation. We\nuse official code and checkpoint6.\n\u2022 HierSpeech++ [25]. It use a non-autoregressive model for continuous vectors generation.\nWe use official code and checkpoint7. We do not use its super resolution model for fair\ncomparison.\n6https://github.com/yl4579/StyleTTS2\n7https://github.com/sh-lee-prml/HierSpeechpp\n17\nTable 9: The latency study on LibriSpeech test-clean. NaturalSpeech 3 one-step denotes using only\n1 iteration in each diffusion process instead of original 4. Abbreviation: NFE (number of function\nevaluation).\nModels\nNFE\nRTF \u2193\nSim-O \u2191\nSim-R \u2191\nUTMOS \u2191\nNaturalSpeech 2\n150\n0.366\n0.55\n0.62\n3.87\nVALL-E\n-\n4.520\n0.47\n0.51\n3.67\nNaturalSpeech 3\n60\n0.296\n0.67\n0.76\n4.30\nNaturalSpeech 3 one-step\n15\n0.067\n0.66\n0.75\n4.01\nTable 10: The ablation results of the design of the duration predictor on LibriSpeech test-clean.\nSim-O \u2191\nSim-R \u2191\nWER\u2193\nUTMOS\u2191\nNaturalSpeech 3\n0.67\n0.76\n1.94\n4.30\nGeneration ablation\n0.62\n0.73\n1.94\n4.18\nObjective ablation\n0.62\n0.72\n2.38\n4.13\nConditioning ablation\n0.62\n0.72\n2.49\n4.11\nPrompting ablation\n0.61\n0.71\n2.83\n4.08\nA.4\nLatency Analysis\nIn this subsection, we compare the inference latency of NaturalSpeech 3 with an autoregressive\nmethod (VALL-E) and a non-autoregressive method (NaturalSpeech 2). We also investigate the effect\nof reducing the number of iterations in each diffusion from 4 to 1, resulting in a total of 15 forward\npasses. We call this variant NaturalSpeech 3 one-step. We evaluate the performance on Librispeech\ntest-clean in terms of speaker similarity (Sim-O/Sim-R) and quality (UTMOS [66] 8, a surrogate\nobjective metric of CMOS). The latency tests are conducted on a server with E5-2690 Intel Xeon\nCPU, 512GB memory, and one NVIDIA V100 GPU. The results are shown in Table 9. From the\nresults, we have several observations. 1) NaturalSpeech 3 achieves a 15.27\u00d7 speedup over VALL-E\nand 1.24\u00d7 speedup over NaturalSpeech 2, while consistently surpasses these baselines on all metrics.\nThis demonstrate NaturalSpeech 3 is both effective and efficient. 2) when using fewer diffusion steps,\nNaturalSpeech 3 can still maintain robust performance (\u22120.01 in Sim-O, \u22120.01 in Sim-R, and \u22120.29\nin UTMOS) with a 4.41\u00d7 faster speed, proving the robustness of diffusion steps.\nA.5\nAblation Study on Duration Diffusion Model\nIn this subsection, we conduct an ablation study to compare our duration discrete diffusion model\nwith the traditional duration predictor, which regresses the duration in logarithmic domain. The\nablation study focus on 1) Generation: multi-step generation vs. one-step generation. 2) Objective:\nclassification-based cross-entropy loss vs. regression-based L2 loss. 3) Conditioning: with vs.\nwithout phoneme-level prosody conditioning. 4) Prompting: with vs. without duration prompting.\nWe evaluate them on Librispeech test-clean in terms of speaker similarity (Sim-O/Sim-R), robustness\n(WER) and qualtiy (UTMOS). As shown in Table 10, we can find that 1) without multi-step generation,\nthere\u2019s a significant drop in performance (-0.05 in Sim-O, -0.03 in Sim-R, and -0.12 in UTMOS).\n2) replacing cross-entropy loss with l2 loss affects the performance, causing a decrease of -0.05 in\nSim-O, -0.04 in Sim-R, 0.44 in WER and -0.17 in UTMOS. 3) dropping phoneme-level prosody\nconditioning will affect both speaker similarity (-0.05 in Sim-O and -0.04 in Sim-R), robustness (0.55\nin WER) and quality (-0.19 in UTMOS) 4) the duration prompting mechanism is crucial for speaker\nsimilarity, robustness and quality, with changes of -0.06 in Sim-O, -0.05 in Sim-R, 0.89 in WER and\n-0.22 in UTMOS. These results confirm that each design aspect of our duration predictor contributes\nto performance improvement.\n8https://github.com/tarepan/SpeechMOS\n18\nTable 11: The MCD scores on 8 different emotions of NaturalSpeech 3 and the baseline methods\non RAVDESS. \u2660 means the results are obtained from the authors. \u2663 means the results are inferred\nfrom official checkpoints. \u2666 means the reproduced results. We use bold to indicate the best result\nand underline to indicate the second-best result.\nMCD\u2193\nneutral\ncalm\nhappy\nsad\nangry\nfearful\ndisgust\nsurprised\nGround Truth\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nVALL-E \u2666\n3.97\n4.75\n4.83\n5.51\n5.19\n5.29\n5.45\n5.29\nVoicebox\u2666\n3.93\n4.90\n4.96\n4.93\n5.01\n5.03\n5.34\n4.89\nNaturalSpeech 2\u2660\n2.77\n3.51\n4.85\n4.88\n5.42\n5.23\n5.31\n4.52\nMega-TTS 2\u2660\n3.28\n4.39\n4.44\n4.67\n4.21\n5.00\n5.42\n4.14\nStyleTTS 2\u2663\n3.41\n4.38\n4.40\n4.64\n4.80\n4.69\n5.10\n4.57\nHierSpeech++\u2663\n5.54\n6.55\n5.78\n5.84\n6.37\n6.17\n6.74\n5.62\nNaturalSpeech 3\n3.23\n4.32\n4.26\n4.41\n4.64\n4.25\n4.80\n4.45\nA.6\nDetails of Prosody Similarity Evaluation\nIn Table 11, we present MCD on 8 different emotions, comparing NaturalSpeech 3 with the baseline\nmethods on the RAVDESS benchmark. NaturalSpeech 3 demonstrates robust performance across 8\nemotions, verifying the effectiveness and robustness in terms of prosody similarity.\nB\nDetails of FACodec\nB.1\nImplementation Details\nModel Architecture. The basic architecture of FACodec encoder and decoder follows [32] and em-\nploys the SnakeBeta activation function [67]. The timbre extractor consists of several conformer [68]\nblocks. We use Nqc = 2, Nqp = 1, Nqd = 3 as the number of quantizers for each of the three FVQ\nQc, Qp, Qd, the codebook size for all the quantizers is 1024.\nLoss Functions. We utilize the multi-scale mel-reconstruction loss Lrec as detailed in [32]. For the\nadversarial loss Ladv, we employ both the multi-period discriminator (MPD) and the multi-band\nmulti-scale STFT discriminator, as proposed by [32]. Additionally, we incorporate the relative\nfeature matching loss Lfeat. For codebook learning, we use the codebook loss Lcodebook and the\ncommitment loss Lcommit from VQ-VAE [69]. The training loss also includes the phone prediction\nloss Lph, the normalized F0 prediction loss Lf0, and the gradient reverse losses of phone prediction\nLgr-ph, normalized F0 prediction Lgr-f0, and speaker classification Lgr-spk for disentanglement learning.\nThe total training loss for the generator can be formulated as: \u03bbrecLrec + \u03bbadvLadv + \u03bbfeatLfeat +\n\u03bbcodebookLcodebook + \u03bbcommitLcommit + \u03bbphLph + \u03bbf0Lf0 + \u03bbgr-phLgr-ph + \u03bbgr-f0Lgr-f0 + \u03bbgr-spkLgr-spk,\nwhere \u03bbrec, \u03bbadv, \u03bbfeat, \u03bbcodebook, \u03bbcommit,\u03bbf0, \u03bbph, \u03bbgr-f0, \u03bbgr-ph and \u03bbgr-spk are coefficients for balancing\neach loss terms. In our paper, we set these coefficients as follows: \u03bbrec = 10.0, \u03bbadv = 2.0, \u03bbfeat = 2.0,\n\u03bbcodebook = 1.0, \u03bbcommit = 0.25, \u03bbf0 = 5.0, \u03bbph = 5.0, \u03bbgr-f0 = 5.0, \u03bbgr-ph = 5.0 and \u03bbgr-spk = 1.0.\nTraining Details. We use Librilight as the training set. We train the codec using 8 NVIDIA TESLA\nV100 32GB GPUs with a batch size of 32 speech clips of 16000 frames each per GPU for 800K steps.\nWe use the Adam optimizer with a learning rate of 2e \u2212 4, \u03b21 = 0.5, and \u03b22 = 0.9.\nB.2\nReconstruction Performance Comparison\nWe evaluate the reconstruction performance with the following objective metrics: Perceptual Eval-\nuation of Speech Quality (PESQ), Short-Time Objective Intelligibility (STOI), Multi-Resolution\nSTFT Distance (MSTFT), and Mel-Cepstral Distortion (MCD). These metrics collectively mea-\nsure the difference between the original and the reconstructed samples. We select the following\n19\nopen-source codec models as baselines: EnCodec [15]9, HiFi-Codec [63]10, and Descript-Audio-\nCodec (DAC) [32]11. We additionally reproduce SoundStream [14] following the original paper\u2019s\nimplementation and experimental setup. Table 12 shows that 1) FACodec significantly surpasses\nSoundStream in the same bandwidth setting (0.44 in PESQ, 0.05 in STOI, 0.14 in MSTFT and 0.79\nin MCD, respectively). Moreover, FACodec achieves on-par performance with SoundStream even\nwhen its bandwidth is doubled (0.02 in PESQ, 0.01 in STOI, \u22120.01 in MSTFT and 0.17 in MCD,\nrespectively). 2) For a fair comparison, we compare FACodec with other baselines in a similar\nbandwidth. FACodec achieve comparable or better result on most metrics than these strong baselines,\nwhich means that we can still achieve excellent reconstruction speech quality when disentangling\nspeech attributes.\nTable 12: The reconstruction quality evaluation of codecs. \u2663 means results are infered from offical\ncheckpoints. \u22c6 means the reproduced checkpoint. \u2666 means the reproduced model following the\noriginal paper\u2019s implementation and experimental setup. All models use a codebook size of 1024.\nWe use bold to indicate the best result and underline to indicate the second-best result. Abbreviation:\nH (Hop Size), N (Codebook Number).\nModels\nSampling Rate\nH\nN\nBandwidth\nPESQ \u2191\nSTOI \u2191\nMSTFT \u2193\nMCD \u2193\nEnCodec\u2663\n24kHz\n320\n8\n6.0 kbps\n3.28\n0.94\n0.99\n2.70\nEnCodec\u22c6\n16kHz\n320\n10\n5.0 kbps\n3.10\n0.92\n0.97\n3.10\nHiFi-Codec\u2663\n16kHz\n320\n4\n2.0 kbps\n3.17\n0.93\n0.98\n3.05\nDAC\u2663\n16kHz\n320\n9\n4.5 kbps\n3.52\n0.95\n0.97\n2.65\nSoundStream\u2666\n16kHz\n200\n6\n4.8 kbps\n3.03\n0.90\n1.07\n3.38\nSoundStream\u2666\n16kHz\n200\n12\n9.6 kbps\n3.45\n0.94\n0.92\n2.76\nFACodec\n16kHz\n200\n6\n4.8 kbps\n3.47\n0.95\n0.93\n2.59\nB.3\nZero-shot Voice Conversion\nVoice conversion aims to transform speech from a source speaker into that of a target speaker,\npreserving content while altering timbre. Zero-shot voice conversion achieves this by utilizing a\nprompt speech sample from the target speaker to convert the source speaker\u2019s speech. FACodec\nachieves zero-shot voice conversion by extracting the speaker embedding hprompt\nt\nfrom the prompt\nspeech to replace the speaker embedding hsource\nt\nfrom the source speech, and utilizing content codes\nzsource\nc\n, prosody codes zsource\np\n, and detail codes zsource\nd\nfrom the source speaker to reconstruct\nthe target speech D(zsource\nc\n, zsource\np\n, zsource\nd\n, hprompt\nt\n). We compare FACodec with some previous\nSOTA models: YourTTS [70], Make-A-Voice (VC) [20], LM-VC [71], and UniAudio [35]. We\nuse VCTK dataset for comparison. We use Sim-O12 to compare speaker similarity to baselines and\nWER to evaluate speech quality. Table 13 shows the evaluation results. The experimental results\ndemonstrate that FACodec solely achieves comparable similarity and superior intelligence compared\nto the state-of-the-art zero-shot VC models, which need additional training on this task. This implies\nthat FACodec achieves superior disentanglement, especially in timbre.\nB.4\nAblation Study\nIn this subsection, we study 1) the impact of the information bottleneck on the disentanglement of\nour FACodec; 2) the effect of gradient reversal on the disentanglement of our FACodec; 3) the role of\nthe acoustic details quantizers; 4) the effects of different prosody representations for TTS generation.\nInformation Bottleneck for Disentanglement.\nWe investigate the impact of the information bottleneck on speech disentanglement through qualitative\nanalysis. We find that without using information bottleneck (quantize in original dimensional space\nrather than low dimensional space) can lead to incomplete disentanglement. For example, we conduct\n9https://github.com/facebookresearch/encodec\n10https://github.com/yangdongchao/AcademiCodec\n11https://github.com/descriptinc/descript-audio-codec\n12https://huggingface.co/microsoft/wavlm-base-plus-sv\n20\nTable 13: The zero-shot voice conversion evaluation results for FACodec with previous SOTA\nmethods. We use bold to indicate the best result and underline to indicate the second-best result.\nModels\nSim-O \u2191\nWER \u2193\nGround Truth\n-\n3.25\nYourTTS\n0.72\n10.1\nMake-A-Voice (VC)\n0.68\n6.20\nLM-VC\n0.82\n4.91\nUniAudio\n0.87\n4.80\nFACodec\n0.86\n3.46\nzero-shot voice conversion in the same experimental setting using the FACodec without information\nbottleneck, as mentioned in Appendix B.3. We observe that the timbre of the converted speech is the\ninterpolation between the source and target, indicating its poor timbre disentanglement. Table 14\ndemonstrates that without the information bottleneck, the speaker similarity of zero-shot voice\nconversion decreases by 0.13.\nTable 14: Comparison of zero-shot voice conversion evaluation results for FACodec with and without\nusing information bottleneck.\nSim-O \u2191\nw. information bottleneck\n0.86\nw.o. information bottleneck\n0.73\nGradient Reversal for Disentanglement.\nWe investigate the impact of gradient reversal on the disentanglement of the FACodec through\nqualitative analysis. We observe that not using gradient reversal diminishes the disentangling ability\nof FACodec. For instance, removing the content and prosody gradient reversal from the acoustic\ndetail module results in some content and prosody information leaking into the detail acoustic. We\ncan confirm this by solely reconstructing the speech using detail codes and timbre embedding, where\npartial content and pitch variations can be heard.\nRole of Acoustic Details Quantizer.\nAlthough content, prosody, and timbre information already encompass the majority of speech\ninformation, Table 15 demonstrates that employing acoustic details quantizers enhances the speech\nreconstruction quality of FACodec. We find 1) without using acoustic details quantizers (only utilizing\nthree codebooks), FACodec achieves comparable or better results compared to SoundStream with\nusing three codebooks, which means that content codes, prosody codes, and timbre embedding\nalready contain most of the necessary information for speech reconstruction; 2) adding acoustic\ndetails achieves better reconstruction quality, which suggests that acoustic details codes primarily\nserve to supplement high-frequency details.\nTable 15: The reconstruction quality comparison between our FACodec with and without using\nacoustic details quantizers.\nCodebook Number\nPESQ \u2191\nSTOI \u2191\nMSTFT \u2193\nMCD \u2193\nFACodec\n6\n3.47\n0.95\n0.93\n2.59\n- acoustic details quantizers\n3\n3.09\n0.92\n1.08\n3.12\nSoundStream\n6\n3.03\n0.90\n1.07\n3.38\nC\nLimitation and Future Works\nDespite our proposed TTS system has achieved great progress, we still have the following limitations:\n21\nAttribute Coverage. In this work, we propose the factorization design for speech representation and\ngeneration, and have achieved significant improvement by factorizing speech into content, prosody,\nduration, acoustic details and timbre. However, these attributes can not coverage all speech aspects.\nFor example, we can not extract the background sounds, which is a common challenge for speech\ndisentanglement. In the future, we will explore more attributes including: 1. energy, 2. background\nsounds, and etc.\nData Coverage. Although we have achieved remarkable improvement on zero-shot speech synthesis\non speech quality, similarity and robustness, NaturalSpeech 3 is trained on English corpus from\nLibriVox audiobooks. Thus, it can not coverage real word people\u2019s diverse voice and can not support\nmultilingual TTS. In the future, we will address this limitation by collecting more speech data with\nlarger diversity.\nNeural Speech Codec. Although our FACodec can factorize speech into attributes and reconstruct\nwith high quality, it still has the following limitations: 1) we need phoneme transcription for content\nsupervision, which limits the scalability; 2) we only verified the disentanglement in zero-shot TTS\ntask. In the future, firstly, we will explore more general methods to achieve better disentanglement,\nespecially without supervision. Secondly, we would like to explore more tasks with the FACodec,\nsuch as zero-shot voice conversion and automatic speech recognition.\n22\n"
  },
  {
    "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
    "link": "https://arxiv.org/pdf/2403.02677.pdf",
    "upvote": "16",
    "text": "Finetuned Multimodal Language Models Are\nHigh-Quality Image-Text Data Filters\nWeizhi Wang1\nKhalil Mrini2\nLinjie Yang2\nSateesh Kumar2\nYu Tian2\nXifeng Yan1\nHeng Wang2\n1University of California, Santa Barbara\n2Bytedance, US\nhttps://mlm-filter.github.io\nAbstract\nWe propose a novel framework for filtering image-text data by leveraging fine-tuned\nMultimodal Language Models (MLMs). Our approach outperforms predominant\nfiltering methods (e.g., CLIPScore) via integrating the recent advances in MLMs.\nWe design four distinct yet complementary metrics to holistically measure the\nquality of image-text data. A new pipeline is established to construct high-quality\ninstruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore,\nour MLM filters produce more precise and comprehensive scores that directly\nimprove the quality of filtered data and boost the performance of pre-trained mod-\nels. We achieve significant improvements over CLIPScore on popular foundation\nmodels (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can\ngeneralize to different models and tasks, and be used as a drop-in replacement for\nCLIPScore. An additional ablation study is provided to verify our design choices\nfor the MLM filter.\n1\nIntroduction\nLarge-scale image-text datasets [SDGS18a, SDGS18b, SVB+21, SBV+22, BPK+22] have been the\nmajor driving force for the recent breakthrough in Vision-Language Models (VLMs) and Text-to-\nImage generation models. The ever-growing size of such datasets allows researchers to scale the\nmodels to unprecedented capacities with billions or even trillions of parameters. These humongous\nfoundation models lead to significant improvements in many down-stream tasks, such as image\nclassification, text-to-image retrieval, image captioning, visual question answering, image generation\nand editing, etc. One great example is the OpenAI CLIP [RKH+21] model, which is trained with\n400M web-crawled image-text pairs. The CLIP model demonstrates impressive zero-shot learning\ncapability across a wide range of different tasks.\nThe quality of image-text data plays a decisive role in the final performance of foundation models.\nBut web-crawled image-text data are often very noisy, e.g., the corresponding text data is low quality\nor does not match the content of the image. How to build high-quality image-text datasets is a\nchallenging research problem that attracts lots of interests recently. [XXT+23] try to re-create\nthe data curation process from CLIP. [NIW+22] advocate that data quality is more important than\nquantity for model robustness. The DATACOMP challenge [GIF+23] is introduced to systematically\nevaluate different data-filtering techniques.\nEach successful foundation model have their own secret recipes for data filtering. Before the invention\nof CLIP, most techniques are hand-designed or rule-based. For example, CC3M and CC12M design\na series of heuristics for image-based, text-based and image&text-based filtering. Model-based\nfiltering becomes popular since the introduction of CLIPScore [HHF+21], which leverages the\nCLIP model to compute the cosine similarity between image and text to measure their alignment.\narXiv:2403.02677v1  [cs.CV]  5 Mar 2024\nCaption: Two kids sat on a bench\nCLIPScore: 28.8\nImage-Text Matching Score: 92\nCaption: Knight of the Hokey Pokey\nCLIPScore: 86.9\nObject Detail Fulfillment Score: 20\nCaption: Formations fabricated this\ncustom``baby loaf'' bus that \nserves as a unique photo opportunity\nCLIPScore: 33.9\nObject Detail Fulfillment Score: 67\nCaption: Ethiopia apologizes for unexplained \ninternet blackout, customers compensated\nCLIPScore: 79.6 \nImage-Text Matching Score: 20\nFigure 1: CLIPScore fails in differentiating the fine-grained object-level image-text alignment, while\nthe image-text matching score generated by MLM Filter significantly captures such alignment.\nCLIPScore has become the predominant method for filtering image-text data. However, recent\nresearch [TJS23, TLZ+24] finds that visual features from CLIP are blind to subtle differences in the\nimage, e.g., object number, shape and position. Because the contrastive loss is applied to the whole\nimage, CLIPScore is less sensitive to capture the fine-grained object-level alignment information,\nshown in Figure 1. Additionally, the text encoder of CLIP can only process up to 77 tokens. The\ninformation loss from the text encoder can limit CLIPScore to process data with long captions.\nThis limitation can be serious for Text-to-Image generation models [BGJ+23] that rely on long and\nhighly-descriptive captions.\nCompared with the contrastively trained CLIP model, Multimodal Language Models (MLMs) have\ndemonstrated promising capability in predicting the quality of generated images or text and aligning\nwell with human preferences. More specifically, the image-text matching scores generated by\nGPT-4Vision [Ope23] are more consistent with human experts compared with CLIPScore in recent\nMLM-based evaluation [YLL+23, ZLW+23]. This motivates us to integrate recent advances in\nMLMs for high-quality data filtering:\n\u201cCan we adapt strong MLMs to generate scores for assessing image-text data\nquality and outperform CLIPScore for image-text data filtering?\u201d\nThough GPT-4V is better at measuring image-text alignment, directly applying GPT-4V-scale MLMs\nin filtering billions of image-text data is computationally too costly. A good filtering method should\nbe both effective and efficient due to the sheer amount of data we need to process. There are smaller\nMLMs (e.g., LLaVA [LLWL23], MiniGPT-4 [ZCS+23], etc), which are more efficient but fail to\ngenerate scores at a granularity that can reflect the subtle changes in the image-text data, since they\nare mainly instruction-tuned on task completion data. In this paper, we propose to combine the best\nof both worlds, leveraging proprietary LLMs or MLMs to construct high-quality instruction tuning\n2\ndata for effectiveness, and fine-tuning more accessible open-source MLMs to inject the knowledge\nfrom the high-quality data for efficiency.\nWe summarize our major contributions as follows:\n\u2022 We propose the MLM filter which incorporates the recent progress from MLMs for image-text data\nfiltering and can be used as a drop-in replacement to the popular CLIPScore.\n\u2022 We design four diverse metrics to measure the image-text data quality from different perspectives,\nand a new pipeline to construct high-quality instruction data to harvest the information from\nproprietary models.\n\u2022 Foundation models trained with our MLM filtered data demonstrate significant improvements, e.g.,\n1.7% better on 38 downstream tasks from DATACOMP comparing with CLIPScore.\n2\nRelated Work\nData Filters. Initial work, such as ImageNet [DDS+09], relies on manual data filtering to select high-\nquality images and captions. More recent work [RKH+21, JYX+21] pushes the size of image-text\ndataset to the order of hundreds of millions, and thus employs fixed rules and heuristics for filtering.\nLAION [SVB+21] introduce the CLIPScore metric computed by the pre-trained CLIP model in\nfiltering high-quality image-text pairs. CLIPScore filtering then becomes a widespread method\nof constructing large-scale web-crawled datasets [BPK+22, SBV+22, GIF+23]. Based on that,\nDATACOMP [GIF+23] is the first work to propose a benchmark for evaluating data filtering methods.\n[YTK+23] introduce a set of tools to improve data filtering including CLIP-FLIP, distribution\nmatching, de-duplication and clustering. Similarly, [MGL+23] propose text masking to improve\nfiltering. On the other hand, [FJJ+23] use high quality image-text pairs to train a new CLIP filtering\nnetwork instead of using OpenAI\u2019s original CLIPScore. These papers all build upon CLIP filtering\nand introduce various techniques to improve it. In contrast, we investigate an alternate approach\nto CLIP-based Filtering, which employs fine-tuned Multimodal Language Models for large-scale\nimage-text data filtering. Additionally, various works [CLY+23, WJHS23] deploys proprietary LLMs\nlike GPT-4 to score and filter text-only and visual instruction data.\nMultimodal Language Models. Recent Multimodal Language Models [ADL+22, HDW+24,\nWDC+22, LLSH23, ZCS+23, LLWL23] concatenate vision encoders with the latest LLMs via\ncross-model adapters to enable LLMs [TGZ+23, CLL+23, TMS+23] to take visual inputs. The\nmost typical vision encoders deployed in MLMs are still the vision transformer models in CLIP\npre-trained models [RKH+21] for extracting visual features of input images. Moreover, various\nadapter architectures are proposed to connect the feature space of different modalities, including\nQ-former proposed by BLIP-2 [LLSH23], a simple MLP layer used in LLaVA [LLWL23], and Visual\nExperts of CogVLM [WLY+23].\nMultimodal Instruction Tuning. Instruction tuning [MKBH21, WBZ+21, OWJ+22] is a fine-\ntuning paradigm that enables LLMs to perform unseen tasks. This zero-shot performance is enabled\nby training LLMs using natural language instructions to explain the goal of the task. Instruction tuning\nis much more computationally efficient than full-set fine-tuning, and can enable LLMs to achieve\nzero-shot performance scores that are competitive with fully supervised models. LLaVA [LLWL23]\nintroduces multimodal instruction tuning via fine-tuning MLMs on a set of visual instructions.\nMLMs that use instruction tuning [DLL+23, LLLL23] achieve SOTA performance on various vision-\nlanguage tasks, such as visual question answering and visual reasoning.\n3\nFine-Tuned Multimodal Language Models as Data Filters\n3.1\nOverview\nWe propose to adopt fine-tuned Multimodal Language Model as effective data filters to select high-\nquality image-text data to promote the VLM pre-training, which involves three stages: 1) constructing\nmultimodal instruction tuning data on proposed quality scoring tasks to fine-tune MLM to realize\naccurate quality assessment; 2) adopt the fine-tuned MLM Filter to generate quality scores for each\ndata point in the data pool and then select the high-quality data; 3) pre-train VLMs using the filtered\n3\ndataset and evaluate the pre-trained VLMs on downstream tasks to demonstrate the effectiveness of\nthe proposed filtering method. The detailed pipeline for the three stages is shown in Figure 2.\nInstruction Generation\nFine-Tuning MLM \nas Data Filter\nFine-Tuned MLM Filter\n Image: [Detailed Description] \n Caption: [Image Caption]\n Quality Scoring Task Description\n Score: [Value]\n [Rationales]\nTeacher LLM\nPrompting\nFine-Tuned MLM FIlter\nScoring and Filtering\nDataComp Data Pools\nFiltered High-Quality\nImage-Text Dataset\nPre-Train \nCLIP / BLIP-2\nImage Classification, Retrieval, Visual\nQuestion Answering\u2026 (Downstream Tasks)\nFiltered High-Quality\nImage-Text Dataset\nInstruction Tuning Stage\nData Filtering Stage\nEvaluation Stage\nFigure 2: Illustration of the pipeline of fine-tuning MLM Filter and employing it for data filtering.\n3.2\nConstructing Multimodal Instruction Tuning Data for Scoring Tasks\nIn order to work as an effective data filter, the MLM must generate quality scores for every single\nimage-text pair for data selection and filtering. To enable MLMs like LLaVA to reason accurately\non the quality score, we propose to fine-tune such MLMs on a set of scoring tasks to enhance their\nscoring capability. The multimodal instruction tuning data needed for scoring tasks are hard and\nexpensive to collect via human labeling, and thus we leverage proprietary models GPT-4 or GPT-4V\nto construct such multimodal instruction data for scoring tasks.\nDefining Metrics for Image-Text Quality Assessment. Conventional data filters like CLIPScore\nfocus on the overall holistic matching of image and text via computing the cosine similarity between\nhidden features of image and text. However, such implicit scoring is poor in discriminating hard or\nambiguous samples, leading to the false negative score predictions shown in Figure 1. We propose to\nleverage strong Multimodal Language Models to predict the quality scores towards image-text pairs.\nBeyond the overall image-text alignment assessment, the fine-tuned MLM filters can evaluate the\nquality of image-text pairs from multiple perspectives. We propose four quality evaluation metrics to\ncomprehensively evaluate the data quality:\n\u2022 Image-Text Matching (ITM): the ITM metric focuses on evaluating whether the image caption\naccurately represents the main features and objects of the image and captures its primary theme.\nThe fine-tuned MLM data filter can explicitly generate the ITM score on a scale of 100.\n\u2022 Object Detail Fulfillment (ODF): the ODF metric focuses on evaluating whether the image caption\nprovides detailed descriptions of objects that align with the image. Specifically, ODF assesses if\nthe caption sufficiently describes the properties of the objects in the image, e.g., number, color,\nsize, position, shape, etc. Compared with the ITM metric, the ODF metric focuses more on the\nfine-grained alignment between the detailed object properties in the image and the ones described\nin the corresponding caption.\n\u2022 Caption Text Quality (CTQ): the CTQ metric focuses on evaluating the text quality of image caption\nbased on the grammatical correctness, diversity of vocabulary (e.g., the range and uniqueness of\nwords), fluency (e.g., smoothness and natural flow of sentences), readability, length, and structure.\nPrevious data-centric research [YTK+23] finds that web-crawled data is poor in its text quality, as\nit contains various bad text patterns, such as repeated words or textual noise. Thus, we propose to\nfine-tune MLMs to assess the text quality of image captions for data filtering.\n\u2022 Semantic Understanding (SU): the SU metric focuses on determining if the image caption provides\nadditional semantic information that is not readily apparent just from the image itself. Such\nauxiliary semantic information can be 1) the professions of persons in the image; 2) the locations,\n4\n(a)\n(b)\nFigure 3: (a) image text matching score distribution of initial 10k instructions using GPT-4V on\nCC12M; (b) image text matching score distribution of final 1k instructions uniformly sampled from\n10 buckets.\naddresses, festivals, country names, city names; 3) the names or entities of buildings, people,\nbird species, animal breeds, car models, engines in the image; 4) the social relationships between\nthe people in the image, i.e., lovers, parent, or child. We suggest that adopting SU metric for\ndata filtering can select image-text pairs with auxiliary semantics, which can further enhance the\ncommonsense reasoning capability of pre-trained VLMs.\nPrompting the Teacher Models. We select two state-of-the-art teacher models, GPT-4 and GPT-4V,\nto construct the multimodal instruction data for quality scoring tasks. Constructing multimodal\ninstruction data with GPT-4V is much easier as GPT-4V can directly take visual inputs. As GPT-4 is\na text-only LLM, we transform the image into a detailed text description to prompt a text-only GPT-4.\nThe prompt for such dense captioning process is Please generate a dense caption in 4-6 sentences\nfor describing the image in detail as much as you can. These comprehensive image descriptions are\ngenerated using a SOTA image captioning models, such as LLaVA or ShareGPT4V [CLD+23]. With\nthe prompt to the teacher model and the generated output, the visual instruction data can be simply\nformatted as User: {Prompt} Assistant: {Output}.\nPrompting Strategies. As the scoring tasks involve a reasoning process to predict final accu-\nrate quality metrics for an image-text pair, we consider two prompting strategies to ensure the\nreasoning accuracy of the fine-tuned multimodal language model: Chain-of-Thought (CoT) Reason-\ning [WWS+22], and Rationalization Reasoning [CRLB18]. The major difference between the two\nprompting strategies are the generation order of the score and the generated reasoning steps. The\nexemplar prompts for two prompting strategies are presented in Appendix B Table 7. Between these\ntwo prompting strategies, we select the rationalization reasoning as we find it to be the most efficient\nand accurate. Computational efficiency is a concern as the scoring MLM should be able to score\nbillions of image-text pairs. If the MLM is fine-tuned to output the score value first, the model\u2019s text\ngeneration process can be stopped early in the inference stage as only the score value is needed for\nfiltering. Secondly, the experimental results of LLaVA demonstrate that the instruction tuning with\nrationalization reasoning leads to better performance on the ScienceQA benchmark [SGM+22] than\nCoT reasoning. Four final prompts for different scoring metrics are presented in Appendix A.\nSelecting Image-Text Pairs for Data Collection. The multimodal instruction data used for fine-\ntuning should contain image-text pairs of varying quality. Thus, data diversity is essential to enhance\nthe fine-tuned MLM filter, enabling it to effectively score image-text data across all quality levels.\nWe select two different image-text dataset as the data pool for constructing instruction tuning\ndata: the Conceptual Captions 12M (CC12m) [SDGS18b], and the DATACOMP Medium 128M\nDataset [GIF+23]. To enhance the diversity of the instruction set, we perform clustering and uniform-\nsampling on the sentence embeddings of each captioning text. The sentence embedding model we use\nis the pre-trained MPNet [STQ+20] encoder model, which is contrastively pre-trained on a mixture of\nretrieval and natural language inference datasets. We directly use the pre-trained MPNet provided by\nSentence Transformers [RG20] to generate the sentence embedding towards each image caption.\nWe set the number of clusters as 10k and 20k for CC12M and Datacomp-Medium, respectively. The\n5\nCaptioner\nData Resource\n#Sampling\nBuckets\nTeacher\nModel\nImageNet-1k\nImageNet\ndist. shifts\nVTAB\nRetrieval\nAverage over\n38 datasets\nLLaVA\nCC12M\n10\nGPT-4\n29.0\n24.5\n35.0\n29.3\n34.2\nShareGPT4V\nCC12M\n10\nGPT-4\n28.4\n24.9\n35.3\n28.2\n33.7\nN/A\nDataComp\n10\nGPT-4V\n29.6\n24.8\n34.2\n26.7\n33.2\nN/A\nCC12M\n10\nGPT-4V\n30.5\n25.3\n33.4\n28.0\n33.7\nShareGPT4V\nCC12M\n10\nGPT-4\n28.4\n24.9\n35.3\n28.2\n33.7\nShareGPT4V\nCC12M\n100\nGPT-4\n27.5\n23.0\n34.6\n28.8\n33.2\nLLaVA\nCC12M\n10\nGPT-4\n29.0\n24.5\n35.0\n29.3\n34.2\nN/A\nCC12M\n10\nGPT-4V\n30.5\n25.3\n33.4\n28.0\n33.7\nTable 1: Ablations on different design choices for constructing multimodal instruction data for quality\nscoring tasks.\nimage-text pairs for constructing instruction tuning data are uniformly sampled from each cluster, in\nwhich only one data point closest to the cluster centroid is selected.\nSampling Final Instructions for Scoring Tasks. As we find that the initial 10k instruction data\ngenerated by teacher models are not uniformly distributed on the score scale of 100 in Figure 3(a),\nwe need to sample the initial instruction data into a balanced instruction set to avoid learning bias.\nConsidering that the ideal size of multi-task instruction tuning dataset is 50k instructions [CLL+23,\nTMS+23], we decide to sample 1k instructions from 10k initial generated instruction data for each\nscoring tasks, which ensure the generalization capability of instruction-tuned MLM. Thus, there are\n4k instruction data of quality scoring tasks to be included in the total 50k instruction dataset, such\nthat there is 1k instruction data for each proposed quality metric. We experiment with two sampling\nmethods to ensure that the instruction data distribution is balanced on the scoring scale of 100: 1)\ngrouping all data into 10 buckets and uniformly sampling 100 instructions from each bucket; 2)\ngrouping all data into 100 buckets and uniformly sampling 10 instructions from each bucket. The\nscore distribution of sampled 10k instruction in Figure 3(b) are more diverse and uniform than the\noriginal score distribution in Figure 3(a). The code for sampling the final 4k instruction is presented\nin Appendix C.\nMixture with instruction data of multi-tasks. The multimodal instruction tuning process should\ninvolve a diverse set of tasks [DLL+23, LLLL23] to enhance the zero-shot reasoning capability of\nfine-tuned MLMs. In addition to 4k multimodal instruction data of the proposed data quality scoring\ntasks, we sample another 46k multimodal instructions from LLaVA-665k instruction datasets. We\nallocate a larger portion of our data mixture to reasoning tasks, such as complex reasoning [LLWL23]\nand GQA [HM19] as we regard that enhancing reasoning capabilities will improve the scoring\ncapability of our fine-tuned MLM. The detailed statistics on the size of each dataset sampled for data\nmixture are presented in Appendix D Table 8.\n3.3\nInstruction-Tuning on Multimodal Language Models\nWe adopt LLaVA-1.5 based on Vicuna-13B LLM [CLL+23, LLLL23] as the Multimodal Language\nModel architecture for instruction tuning on the mixed instructions of data quality scoring tasks and\nother multimodal tasks. The training process of LLaVA-1.5 involves pre-training on image-text pairs\nand instruction tuning on multimodal instructions. We directly take the pre-trained checkpoint and\nonly reimplement the instruction tuning stage with our mixed instruction set.\n3.4\nCreating Optimal MLM Data Filters\nWe propose various different design choices for constructing instruction data for data quality scoring\ntasks in Section 3.2. These design choices may make a significant difference in the effectiveness\nof instruction tuning. To create the optimal fine-tuned MLM data filter, we conduct comprehensive\nablation studies to investigate the effects of different design choices on the filtering performance.\nFour major design choices for constructing the instruction data for scoring tasks are investigated: 1)\nwe experiment with two captioning models to transform image into text-base detailed description\nfor prompting GPT-4, including LLaVA and ShareGPT4V [CLD+23]; 2) we experiment with two\ndifferent image-text datasets for constructing visual instructions, including CC12M and DataComp\nMedium 128M; 3) we experiment with two different numbers of grouping buckets, 10 and 100, for\n6\nFilter\nMetrics\nTeacher\nModel\nImageNet-1k\nImageNet\ndist. shifts\nVTAB\nRetrieval\nAverage over\n38 datasets\nNo Filtering\n-\n-\n17.6\n15.2\n25.9\n21.9\n25.8\nBasic Filtering\nRules\n-\n22.6\n19.3\n28.4\n25.1\n28.5\nLAION Filtering\nCLIPScore+Rules\n-\n23.0\n19.8\n30.7\n23.3\n29.2\nCLIPScore\nCLIPScore\n-\n27.3\n23.0\n33.8\n25.1\n32.8\nMLM-FILTER\nImage-Text Matching\nGPT-4\n28.6\n23.7\n34.4\n30.0\n33.4\nMLM-FILTER\nObject Detail Fulfillment\nGPT-4\n29.0\n24.5\n35.0\n29.3\n34.2\nMLM-FILTER\nCaption Text Quality\nGPT-4\n25.2\n20.9\n32.1\n26.4\n30.9\nMLM-FILTER\nSemantic Understanding\nGPT-4\n20.3\n16.1\n28.4\n20.2\n27.0\nMLM-FILTER\nImage-Text Matching\nGPT-4V\n29.4\n24.4\n36.1\n29.7\n34.2\nMLM-FILTER\nObject Detail Fulfillment\nGPT-4V\n30.5\n25.3\n33.4\n28.0\n33.7\nMLM-FILTER\nCaption Text Quality\nGPT-4V\n24.3\n20.4\n32.3\n24.5\n30.9\nMLM-FILTER\nSemantic Understanding\nGPT-4V\n16.2\n13.9\n23.3\n18.7\n24.0\nMLM-FILTER\nITM AND ODF\nGPT-4V\n30.3\n25.6\n36.0\n29.0\n34.5\nMLM-FILTER\nITM OR ODF\nGPT-4V\n28.9\n24.5\n35.2\n29.0\n33.9\nTable 2: Zero-shot performance of CLIP models pre-trained using baseline filtering methods and\nproposed MLM-FILTER on Medium scale pools of the DataComp benchmark. AND represents the\ncombination of ITM and ODF metrics using AND operation.\nsampling the final 4k instructions; 4) we experiment with different teacher models to get multimodal\ninstructions, including GPT-4 and GPT-4 Vision. Additionally, we use the DataComp benchmark to\nevaluate the effectiveness of different data filtering hyperparameters.\nDataComp Benchmark. The DataComp benchmark [GIF+23] has been introduced to systematically\ncompare the performance of different data filtering methods. In this benchmark, the training code and\ncomputational budget is fixed across all competing methods to facilitate direct comparison between\nmethods. The DataComp provides a fixed original image-text data pool for different filtering methods\nto ensure a fair comparison. The performance is measured by training a CLIP model on the filtered\ndataset and then testing the zero-shot capabilities of this CLIP model on a suite of 38 classification\nand retrieval tasks. We select the Medium scale training setting to train ViT-B/32 CLIP models on\ndatasets resulting from various MLM data filter configurations.\nAblation Results.\nTo investigate the effects of each design choice, we keep the selection of the\nother three design choices the same and only change one design choice for each experiment group.\nAs we propose four different metrics to assess data quality, we only adopt the metric of Object Detail\nFulfillment as the filtering metric to select a high-quality subset from the 128M medium scale data\npool. The ablation results for all four design choices are presented in Table 1.\nThe first two lines in Table 1 demonstrate that adopting LLaVA as the captioning model to transform\nimages into detailed descriptions for instruction data construction leads to better filtering performance.\nNext, adopting CC12M to sample image-text pairs for data construction outperforms the design\nchoice of using DataComp-Medium dataset. We suppose it is because the image quality of CC12M is\nsignificantly better than that of DataComp, enabling the instruction tuning process more knowledge\nintensive. Thirdly, grouping the initial instructions into 10 buckets for sampling illustrates priority\nover using 100 buckets. In terms of the selection of teacher models, the MLM filters learned from\ndifferent teacher models exhibit distinct strengths across different tasks. The MLM filter learned\nfrom GPT-4 performs better in VTAB [ZPK+19] classification and retrieval datasets, while the MLM\nfilter learned from GPT-4V obtains higher scores in ImageNet [DDS+09] related datasets. Finally,\nwe decide to fix the other three choices as LLaVA captioner, CC12M data resources, and 10 sampling\nbuckets. We report the two versions of MLM-based filters with different teacher models GPT4\nand GPT-4V for future experiments, denoted as MLM-FILTER-GPT4 and MLM-FILTER-GPT4V\nrespectively.\n4\nExperiments\nIn this section, we evaluate the effectiveness of adopting fine-tuned MLMs as high-quality image-text\ndata filters. We compare the performance of vision-language models pre-trained on datasets filtered\nusing a baseline filter with their performance using our MLM filter. We select two different VLM\n7\narchitectures for comprehensive evaluation: CLIP pre-training and BLIP-2 pre-training. Additionally,\nwe conduct human evaluation to compute the correlation between the scoring generated by our\nproposed MLM filter model and the baseline CLIP model.\n4.1\nCLIP Pre-Training on DataComp Medium and Large Scales\nFilter\nMetrics\nTeacher\nModel\nImageNet-1k\nImageNet\ndist. shifts\nVTAB\nRetrieval\nAverage over\n38 datasets\nNo Filtering\n-\n-\n45.9\n37.8\n42.6\n41.9\n43.7\nBasic Filtering\nRules\n-\n51.6\n42.3\n44.6\n48.0\n45.8\nLAION Filtering\nCLIPScore+Rules\n-\n55.3\n45.3\n51.0\n49.5\n50.1\nCLIPScore\nCLIPScore\n-\n57.8\n47.4\n53.8\n46.6\n52.9\nMLM-FILTER\nObject Detail Fulfillment\nGPT-4\n58.9\n48.9\n57.4\n52.5\n54.2\nTable 3: Zero-shot performance of CLIP models pre-trained using baseline filtering methods and\nproposed MLM-FILTER on Large scale pools of the DataComp benchmark.\nEvaluation Setup. We select the DataComp benchmark to evaluate the effectiveness of adopting\nfine-tuned MLM as data filter. The evaluation process involves the data filtering stage and evaluation\nstage, which are shown in Figure 2. During the data filtering stage, we adopt the MLM-Filter to\ngenerate quality scores on all 128M medium-scale data and 1.28B large-scale data. After that, an\ninteger filtering threshold is calculated based on the closest value that retains 30% of the overall data\npool, 38.4M for Medium and 384M for Large. Such threshold is set up to select all the image-text\npairs, of which the quality score is larger or equal to the threshold. We report the results using each\ndefined metric to filter data separately and we consider two MLM filters learning from different\nteacher models. Additionally, we also report the results of experiments with a combination of two\nmetrics for data filtering. Finally, we select a high-quality subset from the medium or large scale\nimage-text data pools based on different proposed quality metrics. During the evaluation stage, we\nadopt the selected high-quality data subset to pre-train a CLIP model and compare the performance\nof our CLIP model with CLIP models pre-trained on datasets filtered by other methods.\nBaselines. We compare the proposed MLM filter with other baseline filtering methods from Data-\nComp, including applying no filtering, basic filtering, LAION filtering and CLIPScore filtering. The\nbasic filtering method adopts three rule-based filters, filtering English only, filtering by caption length,\nand filtering by image size. The LAION filtering adopts both the CLIPScore filtering using ViT-B/32\nCLIP model and the English filtering. The CLIPScore filtering utilizes a larger ViT-L/14 CLIP model\nfor score generation and data filtering.\nTraining Details. We strictly follow the training setup provided by DataComp. The computational\nbudget and hyperparameters are fixed for pre-training CLIP using different filters. The CLIP model\narchitecture is determined by the data scale, in which the ViT-B/32 model is pre-trained on the\nmedium scale setting and ViT-B/16 model is on the large scale setting. We use 32 Nvidia A100 GPUs\nto train our models.\nResults on DataComp Medium and Large Scale. The DataComp results between the proposed\nMLM filter and other baselines are presented in Table 2 and Table 3 for Medium and Large scale\nrespectively. On the medium-scale DataComp benchmark, the proposed MLM Filter significantly\noutperforms the CLIPScore baseline on different task subgroups, achieving notable improvements\nof +3.2 accuracy on ImageNet-1k, +2.6 average accuracy on 6 ImageNet shifted datasets, +2.3\naverage accuracy on 13 VTAB datasets, and +4.9 average scores on 3 retrieval datasets. Moreover, the\nproposed MLM FILTER surpasses CLIPScore baseline by +1.7 and +1.3 improvements on the average\nscores over 38 datasets on DataComp Medium and Large Scale benchmarks, which demonstrates the\nproposed MLM Filter can work as more effective filtering method than CLIPScore filter. Additionally,\nwe can draw the following auxiliary conclusions from the results:\nThe MLM Filter learned from GPT-4V performs better on ImageNet related datasets than the\nMLM Filter learned from GPT-4. The MLM-FILTER-GPT4V achieves the best performance on\nboth ImageNet-1k and 6 ImageNet Shifted datasets. Both filtering metrics of Image Text Matching and\nObject Detail Fulfillment generated by MLM-FILTER-GPT4V outperforms the best ImageNet-1k\naccuracy of MLM-FILTER-GPT4, achieving a notable improvement of +1.1 accuracy.\n8\nFilter\nMetrics SVHN\nMNIST Avg.\nMLM-FILTER-GPT4\nITM\n8.2\n10.3\n9.2\nMLM-FILTER-GPT4\nODF\n14.6\n19.3\n16.9\nMLM-FILTER-GPT4V\nITM\n15.4\n8.3\n11.8\nMLM-FILTER-GPT4V\nODF\n9.0\n6.8\n7.9\nMLM-FILTER-GPT4V AND\n12.9\n11.6\n12.3\nTable 4:\nZero-shot performance of pre-trained CLIP on SVHN and MNIST digit classification\ndatasets. Avg. represents the average performance on two digit datasets. AND represents the\ncombination of ITM and ODF metrics using AND operation.\nThe optimal filtering metric varies for fine-tuned MLM Filter learned from different teacher\nmodels. For the proposed MLM FILTER learned from different teacher models, the optimal filtering\nmetric under single metric filtering setting is different. The Image-Text Matching is the optimal\nfiltering metric for MLM-FILTER-GPT4V, while the Object Detail Fulfillment metric helps the\nMLM-FILTER-GPT4 most. The other two metrics of Caption Text Quality and Semantic Under-\nstanding cannot work as effective filtering quality metrics in DataComp benchmark, leading to\nworse performance than CLIPScore baseline. We regard that it is because the most of DataComp\nevaluation datasets are image classification datasets, which did not aligh with the filtering directions\nand objectives of CTQ and SU metrics.\nImage-Text Matching is the best filtering metric for retrieval tasks. Our proposed MLM Filter\nachieves the SOTA performance on the three image-to-text and text-to-image datasets under Data-\nComp Medium setting. The two types of MLM Filters achieves 30.0 and 29.7 average performance\non three retrieval tasks using the ITM filtering metric, surpassing the CLIPScore baseline by 4.9\naverage scores. We also observe in results of both MLM Filter variants that the image-text matching\nmetric leads to better performance on retrieval tasks compared with other three filtering metrics.\nCombing different quality metrics effectively filters and identifies image-text pairs of better\nquality. The AND operation to combine ITM and ODF quality metrics means that the ITM and\nODF score of selected datapoints should exceed the filtering thresholds of both metrics, while the\nOR operation to combine two metrics means that the selected datapoints should either exceed the\nthreshold for ITM metric or that for ODF metric. The combination of ITM and ODF metrics using\nAND operation outperforms all the baseline filtering methods and other variants of MLM Filters,\nachieving the best average performance of 34.5 over 38 datasets.\nThe worse performance on digit classification tasks prevents MLM-FILTER-GPT4V from\nremarkably outperforming MLM-FILTER-GPT4.\nEven if MLM-FILTER-GPT4V outper-\nforms MLM-FILTER-GPT4 on 23 ImageNet, VTAB and retrieval datasets, it only achieves the\nsame average performance over 38 datasets as MLM-FILTER-GPT4. It is because the perfor-\nmance of MLM-FILTER-GPT4V on the two digit classification datasets significantly lags behind\nMLM-FILTER-GPT4 by 5.1 average score, shown in Table 4, which leads to 0.27 average score\nbehind on 38 datasets. The combination of two quality metrics promotes the digit classification\nperformance of MLM-FILTER-GPT4V, but does not resolve it.\n4.2\nBLIP2 Pre-Training\nTo demonstrate the effectiveness of our proposed MLM Filter across various VLM model architectures,\nwe pre-train BLIP-2 VLM on the filtered dataset and evaluate the zero-shot performance of such\nBLIP-2 model on VQA datasets to compare the effectiveness of filtering methods on high-level\nvision-language tasks.\nTraining setup. We directly use the filtered dataset from DataComp Large 1.28B data pool using\nCLIPScore filtering and our proposed MLM Filtering. The batch size and number of pre-training\nsteps are kept as the same as original implementation [LLSH23] for both the CLIPScore filtered\ndataset and MLM filtered dataset, in which both BLIP-2 models are iterated on 420M images for\npre-training stage 1 and 154M images for stage 2. We use the same hyperparameters and number\nof GPUs for training. The visual encoder and LLM we used for BLIP-2 architecture are Eva-CLIP\n9\nFilter\nMetric\nVQA\nGQA\nCLIPScore\nCLIPScore\n55.1\n34.8\nMLM-FILTER-GPT4\nODF\n56.8\n36.2\nTable 5: Zero-shot VQA performance of BLIP-2 models pre-trained on dataset filtered by different\nfiltering methods.\nFilter\nMetric\nPearson\nSpearman\nCLIPScore\n-\n0.164\n0.072\nMLM-FILTER-GPT4\nITM\n0.452\u2217\n0.430\u2217\nMLM-FILTER-GPT4\nODF\n0.410\u2217\n0.384\u2217\nMLM-FILTER-GPT4V\nITM\n0.328\u2217\n0.331\u2217\nMLM-FILTER-GPT4V\nODF\n0.368\u2217\n0.374\u2217\nTable 6:\nPearson and Spearman correlations between human-labeled quality scores and scores\ngenerated by MLM-Filter and CLIP. Images are scored on a scale of 100 for our MLMFilter, while\nCLIPScore is also normalized to the scale of 100. The \u2217 denotes significant correlations at p < 0.05.\nViT-g/14 [SFW+23] and Vicuna-7b [CLL+23] respectively. More training details are available in\nAppendix E Table 9.\nResults.\nTwo BLIP-2 models pre-trained on different filtered datasets are evaluated on\nVQAv2 [GKSS+17] and GQA [HM19] datasets in zero-shot manner and the results of zero-shot\nVQA performance are shown in Table 5. The BLIP-2 pre-trained with MLM-FILTER-GPT4 filtered\nimage-text data achieves +1.7 and + 1.4 improvements on VQAv2 and GQA datasets than the BLIP-2\npre-trained on CLIPSCore filtered dataset.\n4.3\nCorrelation with Human Scoring\nWe follow [ZLW+23] to compute the correlation between human scoring and model scoring to\nevaluate the alignment between human and the filtering model. A set of 100 image-text pairs are\nsampled from CC12M and MSCOCO [LMB+14] and labeled with human scores in terms of the\nimage-text matching. CLIPScore and fine-tuned MLM filters are used to generate the image-text\nmatching scores for the selected image-text pairs. Then, the Pearson and Spearman scores are reported\nbetween the human scores and model scores, as presented in Table 6. Our proposed MLM-FILTER\nscores are significantly aligned and correlated with human quality scores, while CLIPScore does\nnot demonstrate such correlations. The two quality metrics Image-Text Matching and Object Detail\nFulfillment all demonstrate significant correlations in similar levels.\n4.4\nAnalysis\nEffects of filtering fraction. We perform an ablation study to investigate the effects of the fraction of\nsamples selected for pre-training CLIP on DataComp Medium benchmark performance. We select\nfive fractions {0.2, 0.25, 0.3, 0.35, 0.4} of the total 128M images of DataComp medium pool. The\nresults are presented in Table 4. The top-30% of images selected for CLIP training achieve the best\nperformance, which is also observed in [GIF+23]. Even adding 5% poison data leads to a huge\nperformance drop on both ImageNet and average over 38 datasets.\nEfficiency of MLM Filters. The MLM Filter used for quality score generation is LLaVA-1.5 with\n14B model parameters , while CLIPScore adopts a CLIP ViT-L/14 model with 492M parameter in\ntotal. Even if the model size of the proposed MLM Filter is much larger than that of CLIPScore, due\nto the computation redundancy of the CLIP\u2019s dual-encoder architecture, the timecost for generating\nscores for 10k image-text pairs is average 24.3 mins for MLM Filter versus 11.2 mins for CLIPScore-\nViT/L using one A100 GPU. Additionally, with the help of the latest techniques in language model\ninference acceleration, the TensorRT-LLM toolkit1, we accelerate the score generation of our MLM\n1https://github.com/NVIDIA/TensorRT-LLM\n10\n0.20\n0.25\n0.30\n0.35\n0.40\nFraction of samples selected for training\n0.265\n0.270\n0.275\n0.280\n0.285\n0.290\n0.295\nImageNet\n0.20\n0.25\n0.30\n0.35\n0.40\nFraction of samples selected for training\n0.315\n0.320\n0.325\n0.330\n0.335\n0.340\nAverage over 38 datasets\nMLM-Filter-GPT-4V\nDataComp Medium CLIPScore Baseline\nFigure 4: Effects of fraction of images selected for training CLIP.\nFilter 4 times over, resulting in 6.1 mins in average for 10k samples. Thus, the proposed MLM Filter\ncan achieve much better efficiency than CLIPScore.\n5\nConclusion\nWe propose to instruction-tune Multimodal Language Model on quality scoring tasks and further\nleverage these fine-tuned MLM as effective data filters to select high-quality image-text pairs from\nlarge-scale web-crawled dataset. We find that, on CLIP and BLIP-2 models, pre-training on datasets\nfiltered by our proposed MLM Filter significantly outperforms pre-training on CLIPScore-filtered\ndatasets, demonstrating the superiority of our proposed MLM Filter over CLIPScore filtering.\nReferences\n[ADL+22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\n[BGJ+23] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3), 2023.\n[BPK+22] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset.\nhttps://github.com/kakaobrain/coyo-dataset,\n2022.\n[CLD+23] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023.\n[CLL+23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[CLY+23] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay\nSrinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data.\narXiv preprint arXiv:2307.08701, 2023.\n[CRLB18] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural\nlanguage inference with natural language explanations. Advances in Neural Information Processing\nSystems, 31, 2018.\n[DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248\u2013255. Ieee, 2009.\n[DLL+23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\n[FJJ+23] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal\nShankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023.\n11\n[GIF+23] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In\nsearch of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n[GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in\nvqa matter: Elevating the role of image understanding in visual question answering. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\n[HDW+24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning\nperception with language models. Advances in Neural Information Processing Systems, 36, 2024.\n[HHF+21] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n[HM19] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In CVPR, 2019.\n[JYX+21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In International conference on machine learning, pages 4904\u20134916.\nPMLR, 2021.\n[LLLL23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\n[LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[LMB+14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[MGL+23] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-\nmars: Improving visual representations by circumventing text feature learning. arXiv preprint\narXiv:2307.03132, 2023.\n[MKBH21] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task gener-\nalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773,\n2021.\n[MRFM19] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019.\n[MSSC19] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In 2019 international conference on document\nanalysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.\n[NIW+22] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality\nnot quantity: On the interaction between dataset design and robustness of clip. Advances in Neural\nInformation Processing Systems, 35:21455\u201321469, 2022.\n[Ope23] OpenAI. Gpt-4v(ision) technical work and authors. 2023.\n[OWJ+22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[RG20] Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using\nknowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, 11 2020.\n12\n[RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In ICML, 2021.\n[SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022.\n[SDGS18a] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018.\n[SDGS18b] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018.\n[SFW+23] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training\ntechniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023.\n[SGM+22] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa:\nA novel resource for question answering on scholarly articles. International Journal on Digital\nLibraries, 23(3):289\u2013301, 2022.\n[Sha23] ShareGPT. https://sharegpt.com/, 2023.\n[SHRS20] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for\nimage captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer,\n2020.\n[STQ+20] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted\npre-training for language understanding. Advances in Neural Information Processing Systems,\n33:16857\u201316867, 2020.\n[SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n[TJS23] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems\nwith language models. arXiv preprint arXiv:2306.12105, 2023.\n[TLZ+24] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide\nshut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209,\n2024.\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[WBZ+21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\n[WDC+22] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and\nFuru Wei. Visually-augmented language modeling. arXiv preprint arXiv:2205.10178, 2022.\n[WJHS23] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: A 200-instruction\nparadigm for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067, 2023.\n[WLY+23] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,\nLei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.\nCogvlm: Visual expert for pretrained language models, 2023.\n13\n[WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[XXT+23] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen\nLi, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv\npreprint arXiv:2309.16671, 2023.\n[YLL+23] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 9(1):1, 2023.\n[YTK+23] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the details: A\ndeep dive into the rabbit hole of data filtering. arXiv preprint arXiv:2309.15954, 2023.\n[ZCS+23] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n[ZLW+23] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\nWilliam Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as a generalist evaluator for vision-\nlanguage tasks. arXiv preprint arXiv:2311.01361, 2023.\n[ZPK+19] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The\nvisual task adaptation benchmark. 2019.\n14\nA\nPrompt Construction\nAfter manually writing the first version of prompts, we leverage the GPT-4 to refine the human-written\nprompts. The final prompts for four quality scoring tasks are shown below:\nImage Text Matching\nPlease evaluate if the provided text caption accurately represents the main features and objects\nof the image. The caption doesn\u2019t need to detail every aspect of the image, but it should\ncapture its primary theme. Rate the overall quality of the text caption\u2019s match to the image on\na scale of 1-100, considering the criteria mentioned.\nObject Detail Fulfillment\nPlease evaluate the text caption to determine if it provides detailed descriptions of objects\nthat align with the image description. Specifically, assess if the caption sufficiently describes\nthe color, size, position, shape, material, etc., of the objects. Afterward, rate the caption\u2019s\noverall accuracy in capturing object details from the image on a scale of 1-100, based on the\ncriteria provided.\nCaption Text Quality\nPlease evaluate the text caption based on the following criteria: Grammatical Correctness,\nDiversity of Vocabulary (e.g., the range and uniqueness of words used), Fluency (e.g.,\nsmoothness and natural flow of sentences), Readability, Length, and Structure. Assign an\noverall quality score on a scale of 1-100.\nSemantic Understanding\nPlease evaluate the given text caption in relation to its corresponding image description. Your\ngoal is to determine if the text caption provides additional semantic information that isn\u2019t\nreadily apparent just from the image itself.\nFor example:\n1. If the image description mentions \"a man\" but the caption elaborates he is a \"homeless\nman\" or a \"businessman,\" then the caption is enriching the semantic context.\n2. If the caption introduces concepts like the mathematical tangent function, which require\nin-depth knowledge to deduce, it is imparting external semantics.\n3. Captions revealing specific location addresses, festival details, or other nuanced data not\neasy to infer from the image also provide external semantic information.\n4. Directly identifying specific entities in the image such as buildings, people, bird species,\nanimal breeds, car models, engines, etc., in the caption introduces additional insights.\n5. Should the image act as a contextual backdrop and the caption describes elements not\nexplicitly showcased in the image, it has semantic depth.\n6. Lastly, if the caption depicts relationships between the subjects in the image, which need\ncommonsense knowledge to understand, it should be considered semantically rich.\nPlease assess and determine the extent of semantic enrichment the caption provides over the\nimage description. Rate the text caption\u2019s semantic depth on a scale from 1 to 100.\n15\nB\nExamples for Two Prompting Strategies\nExample for Chain-of-Thought Reasoning\nExample for Rationalization\nPlease evaluate if the provided text caption accu-\nrately represents the main features and objects of\nthe image. The caption doesn\u2019t need to detail ev-\nery aspect of the image, but it should capture its\nprimary theme. Rate the overall quality of the text\ncaption\u2019s match to the image on a scale of 1-100,\nconsidering the criteria mentioned.\nPlease evaluate if the provided text caption accu-\nrately represents the main features and objects of\nthe image. The caption doesn\u2019t need to detail ev-\nery aspect of the image, but it should capture its\nprimary theme. Rate the overall quality of the text\ncaption\u2019s match to the image on a scale of 1-100,\nconsidering the criteria mentioned.\nPlease think step by step to first output your\nreasons to give such a score. In the subsequent\nline, please output a single line containing the\nvalue indicating the scores.\nPlease first output a single line containing the\nvalue indicating the scores. In the subsequent\nline, please provide a comprehensive explana-\ntion of your evaluation, avoiding any potential\nbias.\nTable 7: Prompts for zero-shot Chain-of-Thought reasoning and Rationalization reasoning for\nassessing the image-text matching score.\nC\nSampling Final Instructions for Scoring Tasks\n1 final_data = []\n2 buckets = 10 * [[]]\n3 for d in data:\n4\nbuckets[d[\u2019score \u2019]//10]. append(d)\n5 threshold = 130\n6 num_downsample_bucket = 0\n7 for b in buckets:\n8\nif len(b) < threshold:\n9\nfinal_data += p\n10\nelse:\n11\nnum_downsample_bucket\n+= 1\n12 downsampling_num = 1000 - len(final_data)\n13 num_per_bucket = downsampling_num //\nnum_downsample_bucket\n14 for b in buckets:\n15\nif len(b) > threshold:\n16\nfinal_data += random.sample(b, num_per_bucket )\n17 total_data += final_data [:1000]\nD\nData Mixture of Multi-Task Multimodal Instructions\nE\nBLIP-2 Training Details\nThe detailed training hypperparamters and settings of BLIP-2 stage 1 and stage 2 are presented in\nTable 9.\n16\nData\nSize\nTask\nVisual Conversation [LLWL23]\n5K\nConversation\nComplex Reasoning [LLWL23]\n16k\nVisual Reasoning\nDetail Description [LLWL23]\n5k\nCaptioning\nShareGPT [Sha23]\n10K\nLanguage-Only Instructions\nVQAv2 [GKSS+17]\n2K\nVQA\nGQA [HM19]\n3K\nVisual Reasoning\nOKVQA [MRFM19]\n2K\nKnowledge Grounded VQA\nOCRVQA [MSSC19]\n1K\nOCR\nTextCaption [SHRS20]\n2K\nCaptioning\nITM Scoring\n1k\nData Quality Scoring\nODF Scoring\n1k\nCTQ Scoring\n1k\nSU Scoring\n1k\nTotal\n50k\nTable 8: Multimodal instruction data mixture of the data quality scoring tasks and other multimodal\ntasks.\nHyperparameter\nBLIP-2\nStage-1 Pre-training\n# Trainable Parameters\n188M\nPrecision\nfloat16\nGlobal Batch Size\n1680\n# Training Steps\n250k\n# GPUs\n16\n# Gradient Accumulation Steps\n1\nMin LR\n1e-5\nPeak LR\n1e-4\n# Warmup Steps\n2000\nLR Scheduler\nCosine LR Decay\nWeight Decay\n0.05\nAdam (\u03b21, \u03b22)\n(0.9, 0.98)\nStage-2 Pre-training\n# Trainable Parameters\n188M\nPrecision\nfloat16\nGlobal Batch Size\n1920\n# Training Steps\n80k\n# GPUs\n16\n# Gradient Accumulation Steps\n4\nMin LR\n5e-5\nPeak LR\n1e-4\n# Warmup Steps\n2000\nLR Scheduler\nCosine LR Decay\nWeight Decay\n0.05\nAdam (\u03b21, \u03b22)\n(0.9, 0.98)\nTable 9: Training details for BLIP-2 pre-training stage 1 and stage 2.\n17\n"
  },
  {
    "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
    "link": "https://arxiv.org/pdf/2403.02884.pdf",
    "upvote": "14",
    "text": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning\nZhengyang Tang 1 2 3 Xingxing Zhang 2 Benyou Wang 1 3 Furu Wei 2\nAbstract\nLarge language models (LLMs) have demon-\nstrated remarkable capabilities in problem-\nsolving. However, their proficiency in solving\nmathematical problems remains inadequate. We\npropose MathScale, a simple and scalable method\nto create high-quality mathematical reasoning\ndata using frontier LLMs (e.g., GPT-3.5). In-\nspired by the cognitive mechanism in human\nmathematical learning, it first extracts topics and\nknowledge points from seed math questions and\nthen build a concept graph, which is subsequently\nused to generate new math questions. MathScale\nexhibits effective scalability along the size axis of\nthe math dataset that we generate. As a result, we\ncreate a mathematical reasoning dataset (Math-\nScaleQA) containing two million math question-\nanswer pairs. To evaluate mathematical reasoning\nabilities of LLMs comprehensively, we construct\nMWPBENCH, a benchmark of Math Word Prob-\nlems, which is a collection of ten datasets (includ-\ning GSM8K and MATH) covering K-12, college,\nand competition level math problems. We ap-\nply MathScaleQA to fine-tune open-source LLMs\n(e.g., LLaMA-2 and Mistral), resulting in signifi-\ncantly improved capabilities in mathematical rea-\nsoning. Evaluated on MWPBENCH, MathScale-\n7B achieves state-of-the-art performance across\nall datasets, surpassing its best peers of equiva-\nlent size by 42.9% in micro average accuracy and\n43.7% in macro average accuracy, respectively.\n1. Introduction\nLarge language models (LLMs) have demonstrated remark-\nable capabilities in problem-solving. However, their pro-\nficiency in solving mathematical problems remains inade-\nquate, potentially due to the inherent necessity for multi-step\ncomplex reasoning in mathematical problem-solving. In-\n1The Chinese University of Hong Kong, Shenzhen, China\n2Microsoft Research Asia, Beijing, China 3Shenzhen Research\nInstitute of Big Data, Shenzhen, China.\nPreprint. Work in Progress.\nstruction Tuning (Wei et al., 2021) is an effective approach\nto unlock certain capabilities in LLMs. Unfortunately, this\napproach is constrained by the limited size of the currently\navailable datasets on mathematical reasoning. For exam-\nple, the most popular math datasets, GSM8K (Cobbe et al.,\n2021) and MATH (Hendrycks et al., 2021), each only con-\ntains around 7.5K training examples.\nAn effective method to tackle this challenge is to augment\nexisting high-quality math datasets using frontier LLMs\nsuch as GPT-3.5 and GPT-4. For instance, WizardMath\n(Luo et al., 2023) introduces an array of operations for\nGPT-3.5 to generate math questions with increased com-\nplexity. MetaMath (Yu et al., 2023) bootstraps questions in\nGSM8K and MATH through answer augmentation, ques-\ntion rephrasing, self-verification and FOBAR questions. The\nnewly generated examples by these methods exhibit sub-\nstantial similarity to the original examples contained within\nthe training set, which limits their power in generating large\nscale math datasets.\nWe therefore propose a conceptually simple and scalable\nmethod MathScale, which is less dependent on original\ntraining examples. Specifically, we first prompt GPT-3.5\nto extract high level concepts (i.e., topics and knowledge\npoints) from existing seed math questions. In this step, we\nconvert concrete math questions to extractions and the de-\npendency to original questions is largely removed. Given\nthese extractions, we then build a concept graph, which is\nused to estimate the connections between different concepts.\nFinally, we can instruct GPT-3.5 to generate new math\nquestions based on randomly sampled concepts from the\ngraph. Intuitively, we can generate significantly more ex-\namples using different combination of concepts than using\naugmentation-based methods, since the resulting number\nof new examples is bounded by the number of augmenta-\ntion operations. MathScale also bears resemblance to the\ncognitive mechanisms underlying the process of mathemat-\nical learning in humans (Tall, 2013). Tall (2013) argues\nthat the learning process of human involves two distinct\nsteps called concept compression and connection forging.\nConcept compression mirrors the process of high level con-\ncept extraction, while connection forging is similar to our\nconcept graph construction.\nMathematical capability evaluation is another issue arising\n1\narXiv:2403.02884v1  [cs.CL]  5 Mar 2024\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nfrom the lack of high-quality mathematical datasets. Re-\ncently, most LLMs employ GSM8K (Cobbe et al., 2021)\nand MATH (Hendrycks et al., 2021) for evaluation. How-\never, GSM8K focuses on elementary-level problems, while\nMATH offers competition-level challenges. There is a clear\ngap between the two kinds of capabilities measured. There-\nfore, we introduce MWPBENCH, a comprehensive and uni-\nfied benchmark to measure mathematical reasoning capa-\nbilities. MWPBENCH is composed of ten different math\nword problem datasets (including GSM8K and MATH) and\nit covers math word problems from elementary school to\ncollege level with different difficulty levels. Moreover, MW-\nPBENCH standardizes evaluations across all datasets with a\nunified protocol, promoting consistent and fair model com-\nparisons.\nMathScale exhibits effective scalability along the size axis\nof the math dataset that we generate. As a result, we create a\nmathematical reasoning dataset (MathScaleQA) containing\ntwo million math question-answer pairs. We apply Math-\nScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2\nand Mistral), resulting in significantly improved capabili-\nties in mathematical reasoning. Evaluated on MWPBENCH,\nMathScale-7B achieves 35.0% in micro average accuracy\nand 37.5% in macro accuracy, outperforming its best peers\nof equivalent size by 42.9% and 43.7%, respectively.\n2. MWPBENCH Evaluation Framework\n2.1. MWPBENCH\nExisting Datasets Our first endeavor is to collate estab-\nlished datasets, including GSM8K (Cobbe et al., 2021),\nMATH (Hendrycks et al., 2021), TAL-SCQ (TAL, 2023),\nMath23k (Wang et al., 2017), Ape210k (Zhao et al.,\n2020), GaokaoBench-Math (Zhang et al., 2023), and\nAGIEval (Zhong et al., 2023) series (see Table 1). Types\nof problems of these datasets are different. For example,\nmost datasets contain math word problems, while TAL-SCQ\ncomprises multi-choice questions. Intuitively, multi-choice\nquestions are simpler because LLMs only need to figure out\nwhich choice leads to a higher probability. Therefore, we\nconvert all multi-choice questions to math word problems\n(detailed in Appendix A.1). Secondly, some of the datasets\n(e.g., Math23k, Ape210k) are not in English and we translate\nthem to English to expand existing math datasets (detailed in\nAppendix A.2). Note that we translated part of their training\nsets and full test sets into English.\nCollegeMath Existing datasets does not cover college-level\nmathematics which requires diverse skills such as analytical\nthinking, logical reasoning, and quantitative analysis. We\ntherefore propose CollegeMath to bridge this gap.\nWe curated a collection of nine college mathematics text-\nbooks, each addressing a distinct topic (see Table 2 for more\ndetails). These textbooks encompass seven critical mathe-\nmatical disciplines: algebra, pre-calculus, calculus, vector\ncalculus, probability, linear algebra, and differential equa-\ntions. These textbooks are originally in PDF format and we\nconvert them to text format using the Mathpix API1, where\nequations are transformed to LaTeX format. Once converted\na textbook to text format, we are ready to extract exercises\nand their solutions. For each book, we first manually seg-\nment the book into chapter and identify pages with exercises\nand their solutions. Then we extract questions in exercises\nand their associated short answers (see more details of our\nprompts in Appendix A.3). In total, this dataset contains\n1281 examples for training and 2818 examples for test.\n2.2. Unified Evaluation Protocol\nOne of the challenges in benchmarking LLMs for mathemat-\nical reasoning is the inconsistency across evaluation metrics\nand protocols used in different work (Touvron et al., 2023;\nLuo et al., 2023; Yue et al., 2023).\nMWPBENCH aims to evaluate the mathematical reasoning\nabilities of instruction tuned LLMs using a unified evalua-\ntion protocol. We employ zero-shot setting for evaluation\nand use the accuracy metric. The reason behind that is we\nbelieve fine-tuned LLMs should be able to answer questions\ndirectly without demonstrations, while in few-shot setting\nthe final results may change with different set of demon-\nstrations. For prompt template, we choose the Alpaca tem-\nplate (Taori et al., 2023) as default, which is the most widely\nused for instruction tuning (Taori et al., 2023; Luo et al.,\n2023; Yu et al., 2023). However, we support customized\ntemplate just in case that LLMs are trained with a different\ninstruction template (e.g., OpenAI ChatGPT template). For\ndecoding, we choose greedy decoding to eliminate random-\nness in comparisons, selecting the top-1 completion as the\nsolution. To further standardize the evaluation, we care-\nfully implemented the answer extraction and verification\nprocesses (with high precision fuzzy match).\nWe plan to open-source our evaluation framework.\n3. MathScale: Scaling Instruction Tuning for\nMathematical Reasoning\nWe present details of MathScale in this section. MathScale\naims to generate large scale Mathematical Reasoning dataset\nby prompting ChatGPT and it contains four steps.\n1https://docs.mathpix.com/#process-a-pdf\n2Copyright (c) by Michael J. Evans and Jeffrey S. Rosenthal.\nIt may be copied and distributed without restriction, provided it\nis not altered, appropriate attribution is given and no money is\ncharged.\n2\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nDataset\nLevel\nDifficulty\nQuestion Type\nLanguage\n#Train\n#Test\nGSM8K\nElementary\nEasy\nWord\nEn\n7473\n1319\nMATH\nCompetition\nExHard\nWord\nEn\n7498\n5000\nTAL-SCQ\nK12 Math\nMedium\nMC\u2192Word\nEn\n2638\n1496\nMath23k\nElementary\nEasy\nWord\nZh\u2192En\n1000\n949\nApe210k\nElementary\nEasy\nWord\nZh\u2192En\n967\n4874\nGaokaoBench-Math\nHigh School\nHard\nMC\u2192Word\nZh\u2192En\n0\n508\nAGIEval-Gaokao-Math\nHigh School\nHard\nMC\u2192Word\nZh\u2192En\n0\n404\nAGIEval-SAT-Math\nHigh School\nHard\nMC\u2192Word\nEn\n0\n102\nAGIEval-Math\nCompetition\nExHard\nWord\nEn\n0\n938\nCollegeMath\nCollege\nExHard\nWord\nEn\n1281\n2818\nTotal\n\u2013\n\u2013\n\u2013\n\u2013\n20857\n18408\nTable 1. Statistics in MWPBENCH. In the \u201cQuestion Type\u201d column, \u201cWord\u201d stands for math word problem and \u201cMC\u201d stands for multiple-\nchoice problem. In the \u201cDifficulty\u201d column, \u201cExHard\u201d stands for extremely hard.\nTopic\nBook\nLicense\n#Train\n#Test\nAlgebra\nBeginning and Intermediate Algebra (Wallace, 2010)\nCC BY 3.0\n1171\n1000\nPrecalculus\nPRECALCULUS (Stitz & Zeager, 2013)\nCC\n80\n500\nCalculus\nCalculus (Guichard, 2009)\nCC BY-NC-SA\n30\n500\nVectorCalculus\nCORRAL\u2019s VECTOR CALCULUS (Corral, 2008)\nGFDL\n0\n110\nProbability\nIntroduction to Probability (Grinstead & Snell, 2006)\nGFDL\n0\n38\nProbability\nProbability and Statistics:\nThe Science of Uncertainty (Evans & Rosenthal, 2004)\nCustom2\n0\n101\nLinearAlgebra\nMatrix Theory and LINEAR ALGEBRA (Selinger, 2018)\nCC BY\n0\n123\nLinearAlgebra\nA First Course in LINEAR ALGEBRA (Kuttler & Farah, 2017)\nCC BY\n0\n137\nDifferentialEquations\nELEMENTARY DIFFERENTIAL EQUATIONS (Trench, 2001)\nCC BY-NC-SA 3.0\n0\n309\nTable 2. Details of permissively licensed books we use to construct the CollegeMath dataset.\n3.1. Concept Extraction\nAs shown in Figure 1, MathScale takes seed math questions\nas input and we use the training set of MWPBENCH (around\n20K math questions). In the first step, we extract high level\nconcepts (i.e., topics and knowledge points) from these seed\nquestions with prompt engineering of GPT-3.5. We aim to\nextract meta information needed to solve a particular math\nquestion. We believe \u201ctopics\u201d and \u201cknowledge points\u201d are\nimportant meta information for questions. A \u201ctopic\u201d refers\nto the mathematical subject name or the topic name of math\nbook chapter such as \u201cMoney and finance\u201d and \u201cArithmetic\noperations\u201d. While \u201cknowledge points\u201d refers to more fine\ngrained math concepts (e.g., theorems, skills) in problem\nsolving. Typical examples are \u201cDefinition and properties of\ndot product\u201d or \u201cConverting fractions to whole numbers\u201d.\nWe instruct GPT-3.5 to act as a Math teacher and extract\n1 or 2 topics and 1 to 5 knowledge points from a given seed\nquestion (see the prompt template in Table 3).\nTo ensure the diversity of the extracted topics and knowl-\nedge points, we use the training set of MWPBENCH, which\nincludes questions from different sources. We also remove\ntopics and knowlege points that appear only one time to re-\nduce noise. In total, we extracted around 2K topics and 8K\nknowledge points. The above process mirrors the concept\ncompression described in (Tall, 2013).\nAct as a Math Teacher and analyze the provided question.\nStart by identifying 1 or 2 general topics that a student is\nbeing assessed on. Next, highlight 1 to 5 specific knowledge\npoints that the question evaluates.\nProvided question: {seed question}\nAnalysis:\nTable 3. Prompt template for Concept Extraction.\n3.2. Concept Graph Construction\nConcept Graph Given the topics and knowledge points ex-\ntracted from the previous step, we move on to construct\na concept graph C, whose nodes are the extracted top-\nics T = {t1, t2, . . . , t\u2223T\u2223} and knowledge points (KPs)\nK = {k1, k2, . . . , k\u2223K\u2223}. As shown in Figure 2, we have\nthree types of edges in this graph (i.e., topic to topic edge,\ntopic to KP edge and KP to KP edge), which results to three\nsub-graphs (topic graph, topic-KP graph, KP graph). When\na topic (or KP) u is co-occurred with another topic (or KP)\nv, we build an edge between them and the edge weight is re-\nlated to their co-occurrence statistics. Define co-occurrence\nas u and v have been extracted from the seed question.\nFormally, let E = {(u, v)\u2223fco(u, v) > 0} denote edges\nin C and fco(u, v) is the edge weight between u and v.\n3\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nFigure 1. Overview of MathScale. MathScale starts from seed math questions and there are three steps in this pipeline (i.e., concept\nextract, concept graph construction and mathematical reasoning data generation). After these three steps, we obtain the MathScaleQA\ndataset, which is subsequently used to train open LLMs. Finally, we obtain MathScale models.\nIntuitively, two KPs (or topics) are more likely to be reason-\nable composition when they have been frequently used to\nsolve the same seed questions. Let wuv denote the raw co-\noccurrence count between node u and node v. The adjusted\nweight fco(u, v) is defined as follows:\nfco(u, v) = log(wuv + \u03b5)\n(1)\nwhere \u03b5 is a small constant introduced to maintain non-zero\ncounts and prevent computational issues.\nConcept Composition Given the graph C, we are ready to\nsample topics and KPs from it and the sampled topics and\nKPs are subsequently used to generate new math questions.\nWe use a graph random walk algorithm to create concept\ncompositions.\nWe start from a uniformly random sampling from the \u2223T\u2223\ntopics we have extracted. Note that in implementation, we\nsimply enumerate all extracted topics for multiple epochs.\nIn the second step, we do a random walk for one to two\nsteps in the topic sub-graph to search for related topics. The\nprobability distribution for the graph random walk is not\nuniform and defined as follows:\npuv =\nexp(fco(u, v))\n\u2211v\u2032\u2208N (u) exp(fco(u, v\u2032))\n(2)\nwhere N(u) denotes the set of nodes adjacent to u in the\ntopic sub-graph.\nIn the third step, we continue to randomly walk in the hy-\nbrid topic-KP graph for a single step with the probability\ndistribution calculated as in Equation (2) on the topic-KP\ngraph. So that we now have one sampled KP.\nIn the last step, we continue to expand to more KPs by ran-\ndomly walking on the KP graph for zero to four steps again\nwith the probability distribution computed as in Equation (2)\non KP graph. We finally obtained a set of sampled topics \u02c6T\nand KPs \u02c6K.\nThe whole process above is an analogy of the connection\nforging described in (Tall, 2013).\n3.3. Mathematical Reasoning Data Generation\nAct as a Math Teacher and create a new question and its solution\nbased on the provided topics and knowledge points. Ensure that\nthe created questions:\n1. Adhere to the provided topics.\n2. Necessitate the combined use of the associated knowledge\npoints.\n{few shot examples}\nTopics:\n{topics}\nKnowledge Points:\n{knowledge points}\nStructure your response as:\nFORMAT INSTRUCTIONS OF THE NEW QA-PAIR ...\nTable 4. Prompt template for Mathematical Reasoning Data Gen-\neration.\nWith the novel compositions of topics \u02c6T and KPs \u02c6K at hand,\nwe query GPT-3.5 to generate corresponding question-\n4\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nFigure 2. Running Examples of the concept graph construction process in the MathScale pipeline.\nanswer pairs. Inspired by how math teachers design ques-\ntions from existing exercises, we opt to include few-shot ex-\namples to guide GPT-3.5 in question formulation. These\nexamples are chosen from the seed questions, based on\nthe Jaccard distance of their knowledge points set. We ask\nGPT-3.5 to adhere to \u02c6T and encourage combine use of\nKPs \u02c6K. We present the template for prompts in Table 4.\nFurthermore, we apply a decontamination process, where all\nmath questions in the test set of MWPBENCH are removed.\n3.4. Validation\nWe observe that sometimes in the newly generated QA pairs,\nthe solution is incorrect. We therefore also tried to add an\nadditional validation process as follows. We first instruction\nGPT-4 to generate a reference solution for the question\nand then ask GPT-4 again to validate the GPT-4 solution\nagainst the solution generated in the previous step. We\nassume GPT-4 is more accurate than GPT-3.5. If GPT-4\nbelieve the orignal solution is incorrect, we replace it with\nthe new GPT-4 solution. Small scale experiments (Table\n7) show the step does not improve the results. Perhaps\nbecause essentially we are trying to distill GPT-3.5 using\nopen source LLMs. Although some solutions are incorrect,\nthey are still help open source LLMs to learn the model\ndistributions of GPT-3.5. Therefore, in our final pipeline,\nwe remove this validation step.\n4. Experiments\n4.1. Implementation\nData Generation\nIn concept extraction (Section 3.1), we\nuse the MWPBENCH training set, comprising around 20K\nquestions, as the seed questions for our MathScale pipeline\nand we employ GPT-3.5-Turbo-0613 for the extrac-\ntion. In total, we obtain 2,018 topics and 8,892 knowl-\nedge points. We then construct graphs to establish rela-\ntionships among these concepts (Section 3.2). The edge\nweight in the graph is smoothed using Equation (1) and\nwe set \u03b5 = 1e \u2212 5.\nIn the concept composition pro-\ncess, treating the iteration through all topic nodes as one\nepoch, we repeat this process for approximately 1K epochs,\nresulting 2 million unique concept compositions. Then\nwe instruct GPT-3.5-Turbo-0613 to create 2 million\nquestion-answer pairs with these compositions. We also\ndecontaminate the generated datasets by excluding all math\nquestions in the test set of MWPBENCH. To leverage the\nprecious high quality math reasoning data, we addition-\nally combine the generated data with the training set of\nMWPBENCH. We call the resulting dataset MathScaleQA.\nThe validation step (Section 3.4) is excluded from the final\npipeline, because we find that the validation step does not\nimprove results (see details in Section 5.3). Example out-\nputs for each step of the pipeline are provided in Appendix\nA.4.\nModel Training\nThe questions in MathScaleQA are for-\nmatted using the Alpaca prompt (Taori et al., 2023) as fol-\nlows.\n5\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nBelow is an instruction that describes\na task.\nWrite a response that\nappropriately completes the request.\n### Instruction:\n{question}\n### Response:\nOur training pipeline is adapted from the open-instruct\n(Wang et al., 2023) toolkit. We utilize the LLaMA-2 7B and\n13B models (Touvron et al., 2023) as well as the Mistral 7B\nmodel (Jiang et al., 2023) as our backbone models. We use\na batch size of 128 and train on the MathScaleQA dataset\nfor 3 epochs using a learning rate of 2e-5. We call the\nresulting models MathScale-7B, MathScale-13B and Math-\nScale-Mistral-7B. We leave exploration of the LLaMA-2\n70B model in future work.\n4.2. Models in Comparison\nFor a comprehensive evaluation, we select a diverse set of\nprevious LLMs specialized in mathematical reasoning for\ncomparison.\nClose-Source Models\nWe include the most capable\nGPT models developed by OpenAI, which are the light-\nweighted GPT-3.5-Turbo-0613 and the powerful\nGPT-4-0314. These models are known to be good at\nmathematical reasoning and serves as the upper bounds.\nOpen-Source Models: We also compare our model against\nopen-source math models. Specially, we compare with Wiz-\nardMath (Luo et al., 2023), GAIR-Abel (Chern et al., 2023),\nMetaMath (Yu et al., 2023), and MAmmoTH (Yue et al.,\n2023). WizardMath (Luo et al., 2023) is based on evol-\ninstruct (Xu et al., 2023) and reinforcement learning. Meta-\nMath (Yu et al., 2023) is trained on a dataset by augmenting\nGSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al.,\n2021) using answer or question side paraphrasing. The\ndataset used to train MAmmoTH (Yue et al., 2023) com-\nprises a collection of 13 existing math datasets with GPT-4\nCoT (Wei et al., 2022) and/or PoT (Gao et al., 2023; Chen\net al., 2022) annotations. We evaluate all models using\nCoT natural language style math solutions. We noticed that\nsome of the models (e.g., GPT-4 and MAmmoTH) can\nproduce code solution of math problems in addition to nat-\nural language solutions. For fair comparison, we refrain\nfrom comparing using code-interpreter style solutions, be-\ncause all models above can produce code-interpreter style\nsolutions if the solutions in their training data are replace\nby GPT annotated code solutions. Also note that Wizard-\nMath v1.1 is a Mistral based math model and we do not\nknow how its training data are constructed (the authors did\nnot release any detail of the training data of WizardMath\nv1.1). We evaluate all models on MWPBENCH, which con-\ntains 10 datasets on mathematical reasoning. We report\naccuracies of the 10 datasets as well as their micro-average\nand macro-average. We prompt all models using the Al-\npaca template (see Section 4.1). (Luo et al., 2023) rec-\nommended an improved prompt for during inference (i.e.,\nadding Let\u2019s think step by step after the stan-\ndard Alpaca template). However, we observe mixed results\non MWPBENCH for some models in comparison. For exam-\nple, we observe improved results on GSM8K, but decreased\nresults on MATH. We therefore do not use this optimization\nfor all models in comparison.\n4.3. Main Results\nAs shown in Table 5, MathScale obtains best micro av-\nerage and macro average scores on MWPBENCH com-\npared to other models based on LLaMA-2 7B, LLaMA-2\n13B or Mistral 7B. Specifically, On average, MathScale-\n7B achieves a 35.0% (micro) and 37.5% (macro) ac-\ncuracy across MWPBENCH, surpassing its best counter-\nparts of equivalent size by 42.9% and 43.7%, respectively.\nThe trends are similar for MathScale-13B and MathScale-\nMistral. This also confirms the effectiveness of our Math-\nScaleQA dataset regardless of the backbone model. Note\nthat in GaokaoBench-Math, AGIEval-Gaokao-MATH, and\nAGIEval-SAT-MATH, there is no training set. Even on these\nout-of-domain test sets, MathScale-7B wildly outperforms\nother open-source models in comparison. When compared\nto frontier LLMs, MathScale-Mistral demonstrates perfor-\nmance parity in both micro and macro averages relative\nto GPT-3.5-Turbo (see the first block in Table 5). We\nhave also included subset performances on the MATH and\nCollegeMath datasets in Appendix A.5 to analyze model\ncapabilities across different topics and disciplines.\n5. Analysis and Discussions\n5.1. Scaling Property of MathScale\nAs described in Section 3, given a fixed set of math concepts,\niterating over concept graphs allows us to generate different\ncompositions of mathematical concepts, thereby synthesiz-\ning large amount of new math data. We use LLaMA-2 7B as\nour base model to study the scaling property of MathScale.\nWhen scaling the size of the MathScaleQA dataset, we ob-\nserve a nearly logarithmic growth in the performance of the\nMathScale-7b model across all datasets within MWPBENCH,\nas depicted in Figure 3. We draw the scaling curve up to two\nmillion examples (size of the full MathScaleQA). We also\ncompare MathScale against WizardMath and MetaMath at\ntheir respective training sizes. MathScale outperforms both\nmodels across all datasets (except for GSM8K) when using\nan equivalent amount of training data. Given the scaling\ncurves in Figure 3, we anticipate that the performance of\n6\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nModels\nGSM8K MATH College\nMath\nTAL Math23k Ape210k\nGaokao\nBench\nMath\nAGIE\nGaokao\nMath\nAGIE\nSAT\nMath\nAGIE\nMATH\nMicro\nAverage\nMacro\nAverage\nClosed-source Models\nGPT-4\n92.9\n51.8\n24.4\n51.8\n76.5\n61.5\n35.4\n28.2\n68.6\n50.7\n52.0\n54.2\nGPT-3.5-Turbo\n74.1\n37.8\n21.6\n42.9\n62.5\n44.0\n23.2\n15.3\n55.8\n37.4\n39.8\n41.5\nModels based on LLaMA-2 13B\nLLaMA-2 13B\n7.1\n3.5\n1.2\n6.3\n9.5\n7.9\n0.7\n0.4\n6.8\n3.7\n4.5\n4.7\nWizardMath\n62.0\n14.3\n7.8\n18.7\n38.3\n25.2\n8.2\n3.4\n29.4\n15.8\n20.2\n22.3\nMAmmoTH\n56.5\n12.6\n6.5\n17.3\n39.5\n28.1\n5.9\n4.9\n20.5\n12.5\n18.9\n20.4\nGAIR-Abel\n66.4\n16.6\n7.9\n21.1\n42.2\n27.8\n7.0\n4.9\n30.3\n18.2\n22.3\n24.3\nMetaMath\n70.8\n22.8\n10.1\n25.4\n48.6\n31.6\n9.6\n5.6\n38.2\n22.9\n26.8\n28.6\nMathScale 13B\n71.3\n33.8\n20.4\n38.1\n61.1\n43.7\n20.0\n12.3\n55.8\n34.7\n37.1\n39.1\nModels based on LLaMA-2 7B\nLLaMA-2 7B\n4.5\n4.2\n2.3\n7.6\n6.8\n7.3\n2.1\n2.9\n2.9\n5.0\n4.7\n4.6\nWizardMath\n52.8\n10.3\n6.8\n14.0\n32.5\n19.2\n5.9\n6.1\n22.5\n11.7\n15.8\n17.1\nMAmmoTH\n50.0\n9.5\n6.2\n13.3\n34.6\n21.4\n3.9\n2.7\n19.6\n10.9\n15.6\n17.2\nGAIR-Abel\n57.6\n12.7\n6.6\n18.3\n35.4\n24.5\n4.3\n4.4\n23.5\n14.6\n18.5\n20.2\nMetaMath\n66.2\n20.6\n9.4\n22.5\n44.0\n29.9\n5.9\n5.1\n36.2\n20.8\n24.5\n26.1\nMathScale 7B\n66.3\n31.1\n20.9\n35.2\n59.0\n41.8\n19.6\n12.6\n57.8\n31.1\n35.0\n37.5\nModels based on Mistral 7B\nMistral 7B\n15.5\n10.1\n7.5\n17.9\n18.5\n15.5\n6.2\n5.9\n22.5\n10.4\n11.9\n13.0\nWizardMath v1.1\n78.1\n32.8\n16.0\n34.4\n58.3\n41.4\n16.1\n9.6\n55.8\n33.0\n35.4\n37.6\nMetaMath Mistral\n77.4\n28.4\n15.7\n31.4\n55.1\n38.1\n15.3\n10.1\n50.9\n28.4\n32.7\n35.1\nMathScale Mistral\n74.8\n35.2\n21.8\n39.9\n64.4\n46.0\n21.4\n14.3\n57.8\n32.9\n38.7\n40.8\nTable 5. Performance metrics on MWPBENCH. All evaluations were conducted utilizing the driver provided by MWPBENCH, ensuring a\nconsistent and fair comparison. Within each section, the highest results are highlighted in bold font. \u201cAGIE\u201d stands for AGIEval.\nMathScale may continue to improve with even more syn-\nthetic training examples. Due to resource constraints, we\nleave the training set scaling beyond two million examples\nto future work.\n5.2. Ablation on Concept Extraction\nIn the concept extraction process (Section 3.1), we use all\nthe 20K seed questions. We attempt to answer the follow-\ning two questions. 1) Does the number of seed questions\nmatter? 2) Does the number of extracted concepts matter?\nWe control the size of resulting training examples to 25K\nfor fast experimentation. In all experiments, we use the\nLLaMA-2 7B model as our backbone model.\nNumber of Seed Questions\nTo assess the influence of\nseed questions, we firstly randomly remove 50% of the seed\nquestions from the MWPBENCH training set (i.e., we use\nonly 10K seed questions). The results are shown in Table\n6. We observe the macro average on MWPBENCH drops\nby 2.9%. Further, when we limite the data source of seed\nquestions exclusively to the training sets of GSM8K and\nMATH, there is a performance decrease of 3.5%. These\nresults above indicate that incorporating of a larger and\nmore diverse set of seed questions is beneficial.\nNumber of Math Concepts\nAdditionally, we examine the\nimpact of extracted math concepts. As shown in Table 6, by\nremoving half of the topics or knowledge points, we observe\na notable decrease in the macro average on the MWPBENCH.\nParticularly, removing knowledge points lead to a greater\ndecrease in performance (i.e., -8.6% with 50% knowledge\npoints v.s. -2.3% with 50% of topics). This highlights the\nessential role that knowledge points play in enhancing the\neffectiveness of MathScale.\nMethods\nMacro\nAverage\nRelative\nChange\nMathScale\n14.5\n-\nRemove 50% Seed Questions\n14.0\n-2.9%\nRestrict Data Source\nto GSM8K and MATH only\n13.9\n-3.5%\nRemove 50% Topics\n14.1\n-2.3%\nRemove 50% Knowledge Points\n13.2\n-8.6%\nTable 6. Ablation studies of concept extraction with a control train-\ning size of 25K on MWPBENCH.\n7\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nFigure 3. Performance on MWPBENCH using different sizes of training dataset in MathScaleQA.\n5.3. On Validating Generated Data\nThe generated QA pairs in MathScaleQA might be incorrect.\nTherefore, we introduce a separate validation step in Sec-\ntion 3.4. In this section, we design controlled experiment\non 5K generated data from MathScaleQA and again using\nLLaMA-2 7B as our base model.\nGPT-4 v.s. GPT-3.5 Accuracy\nWe manually annotate\n100 randomly chosen generated data points and gener-\nate answers with GPT-3.5-Turbo and GPT-4. GPT-4\ndemonstrate an impressive accuracy of 87%, significantly\noutperforming the accuracy of 69% by GPT-3.5-Turbo.\nTherefore, we used GPT-4 to generate reference solutions\nand validate our synthetic solutions, replacing any incorrect\nsolutions with the GPT-4 reference solutions.\nResults\nWithin the 5K examples, 26% of the solutions are\nidentified as incorrect by GPT-4 and are replaced. We have\nanother two settings with either all GPT-3.5 solutions and\nGPT-4 solutions. The results are shown in Table 7 and we\nobserve that using original 3.5-Turbo solutions lead to a\nsimilar results as using the validation step.\nThis observation is counter-intuitive. Maybe because train-\ning on synthetic data generated from GPT-3.5 is essen-\ntial distillation. Even if some solutions are incorrect, they\nmay still help to the open-source LLMs (e.g., LLaMA-2 or\nMistral) to mimic the distirubtions of GPT-3.5. We also\nnotice that in neural machine translation distillation, the step\nof validating incorrect translations is also ignored (Kim &\nRush, 2016). Therefore, we opt to omit the validation and\ncorrection step from the final MathScale pipeline.\n5.4. Performance on a Fresh Math Dataset\nWhile MathScaleQA generated by GPT-3.5 is rigorously\ndecontaminated to prevent overlap with the MWPBENCH\ntest set, there may still be small chance that some of the test\nsets have been leaked to GPT-3.5-Turbo or contained in\nthe training data of LLaMA-2. Because GPT-3.5-Turbo\nuses human annotated queries submitted by users through\ntheir APIs3.\nThese queries may include test sets such\nGSM8K. The training set of LLaMA-2 is not released and\nwe are not sure if some examples in test sets of MWPBENCH\nare included or not.\n3https://openai.com/research/instruction-following\n8\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nMethods\nMicro Average\nMacro Average\n100% GPT-3.5 Solutions\n10.6\n11.5\n74% GPT-3.5 Solutions and 26% GPT-4 Corrected Solutions\n10.2\n11.1\n100% GPT-4 Solutions\n9.8\n10.9\nTable 7. Ablation studies of validation step with a control training size of 5K on MWPBENCH.\nTo address this issue, we manually curate a new dataset\ncomprising the latest 30 math questions from latest Gaokao\nMath exam, held in June for China National Higher Edu-\ncation Entrance Examination. We term this dataset, Fresh-\nGaokaoMath-2023, which we believe Fresh-GaokaoMath-\n2023 is not likely to be included in the training data of\nLLaMA-2 or GPT-3.5-Turbo. Because LLaMA-2 and\nGPT-3.5-Turbo are released before Fresh-GaokaoMath-\n2023 is created.\nWe compare our LLaMA-2 7B based model MathScale-\n7B against two other LLaMA-2 7B based models\n(i.e., WizardMath-7B and MetaMath-7B) as well as\nGPT-3.5-Turbo and GPT-4. Results are in Table 8.\nMathScale consistently surpasses WizardMath and Meta-\nMath, which aligns with the main results shown in Table\n5. It demonstrates the robustness and adaptability of Math-\nScale in handling fresh math questions.\nModel\nFresh-GaokaoMath-2023\nGPT-4\n43.3\nGPT-3.5-Turbo\n40.0\nWizardMath-7B\n13.3\nMetaMath-7B\n16.6\nMathScale-7B\n30.0\nTable 8. Performance metrics on Fresh-GaokaoMath-2023.\n6. Related Work\nChatGPT-based Instruction Tuning A pivotal aspect driv-\ning advancements in math instruction tuning is the use of\nChatGPT for data synthesis. For instance, WizardMath (Luo\net al., 2023) introduced reinforced evol-instruct which inte-\ngrates five operations: adding constraints, deepening, con-\ncretizing, increasing reasoning steps, and complicating in-\nput, thereby facilitating comprehensive evolution. Similarly,\nMetaMath (Yu et al., 2023) employs a bootstrapping strategy\nfor questions, incorporating answer augmentation, rephras-\ning, self-verification, and FOBAR. While these methods are\neffective, the breath space is inherently confined to manually\ndesigned operations. Our approach seeks to enable Chat-\nGPT to emulate cognitive processes in human mathematical\nlearning, thus overcoming the limitations faced by previous\nmethodologies.\nTool-Integration Instruction Tuning Recent studies have\nalso explored integrating tools into ChatGPT-based instruc-\ntion tuning for mathematics. ToRA (Gou et al., 2023) com-\nbines natural language reasoning with program-based tool\nusage to synthesize trajectory data. Each trajectory itera-\ntively concatenates reasoning, programming, and program\noutputs until the final answer is reached. Our current focus\nis solely on natural language reasoning. While tool integra-\ntion within the MathScale pipeline is an intriguing prospect,\nwe reserve its exploration for future research.\n7. Conclusions\nWe propose MathScale, a simple and scalable method to cre-\nate high-quality mathematical reasoning data using frontier\nLLMs. We also construct MWPBENCH, a comprehensive\nbenchmark of Math Word Problems covering K-12, col-\nlege, and competition level math problems. Evaluated on\nMWPBENCH, MathScale-7B achieves state-of-the-art per-\nformance across all datasets, surpassing its best peers of\nequivalent size by 42.9% in micro average accuracy and\n43.7% in macro average accuracy, respectively.\nBroader Impact\nThis paper seeks to advance mathematical reasoning by\nintroducing a scalable method for generating high-quality\nsynthetic data with large language models, along with new\nevaluation benchmarks to foster consistent and fair model\ncomparisons in academia. While our efforts center on as-\nsessing mathematical capabilities, it\u2019s crucial to note that\nthe models may exhibit biases not examined in our study.\nAddressing these biases and ensuring the models\u2019 alignment\nwith societal values is essential, highlighting the need for\ncomprehensive evaluations that encompass both technical\nperformance and ethical considerations.\nReferences\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nChern, E., Zou, H., Li, X., Hu, J., Feng, K., Li, J., and Liu,\nP. Generative ai for math: Abel. https://github.\ncom/GAIR-NLP/abel, 2023.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\n9\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nCorral, M. CORRAL\u2019S VECTOR CALCULUS. 2008.\nEvans, M. J. and Rosenthal, J. S. Probability and statistics:\nThe science of uncertainty. Macmillan, 2004.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\nY., Callan, J., and Neubig, G. PAL: Program-aided lan-\nguage models. In Krause, A., Brunskill, E., Cho, K.,\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\nceedings of the 40th International Conference on Ma-\nchine Learning, volume 202 of Proceedings of Machine\nLearning Research, pp. 10764\u201310799. PMLR, 23\u201329 Jul\n2023. URL https://proceedings.mlr.press/\nv202/gao23f.html.\nGou, Z., Shao, Z., Gong, Y., Yang, Y., Huang, M., Duan,\nN., Chen, W., et al. Tora: A tool-integrated reasoning\nagent for mathematical problem solving. arXiv preprint\narXiv:2309.17452, 2023.\nGrinstead, C. M. and Snell, J. L. Grinstead and Snell\u2019s\nintroduction to probability. Chance Project, 2006.\nGuichard, D. Calculus. 2009.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nKim, Y. and Rush, A. M. Sequence-level knowledge dis-\ntillation.\nIn Su, J., Duh, K., and Carreras, X. (eds.),\nProceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 1317\u20131327,\nAustin, Texas, November 2016. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/D16-1139. URL\nhttps://aclanthology.org/D16-1139.\nKuttler, K. and Farah, I. A First Course in Linear Algebra,\n2017A version (Lyryx). Lyryx, 2017.\nLuo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C.,\nGeng, X., Lin, Q., Chen, S., and Zhang, D. Wizard-\nmath: Empowering mathematical reasoning for large lan-\nguage models via reinforced evol-instruct. arXiv preprint\narXiv:2308.09583, 2023.\nSelinger,\nP.\nMatrix\ntheory\nand\nlinear\nalgebra,\n2018.\nURL https://www.mathstat.dal.ca/\n\u02dcselinger/linear-algebra/. An introduction\nto linear algebra for first or second year university stu-\ndents. Licensed under Creative Commons CC BY 4.0\nLicense. Last updated on October 26, 2018.\nStitz, C. and Zeager, J. Precalculus. Stitz Zeager Open\nSource Mathematics, 2013.\nTAL. Tal-scq5k, 2023. URL https://github.com/\nmath-eval/TAL-SCQ5K. GitHub repository.\nTall, D. How humans learn to think mathematically: Ex-\nploring the three worlds of mathematics. Cambridge\nUniversity Press, 2013.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\nStanford\nalpaca:\nAn\ninstruction-following\nllama\nmodel.\nhttps://github.com/tatsu-lab/\nstanford_alpaca, 2023.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTrench, W. F. Elementary Differential Equations. Brooks/-\nCole Thomson Learning, San Antonio, Texas, USA, 2001.\nURL http://ramanujan.math.trinity.edu/\nwtrench/texts/TRENCH_DIFF_EQNS_I.PDF.\nFree Edition 1.01 (December 2013).\nWallace, T. Beginning and intermediate algebra. 2010.\nWang, Y., Liu, X., and Shi, S. Deep neural solver for math\nword problems. In Proceedings of the 2017 conference\non empirical methods in natural language processing, pp.\n845\u2013854, 2017.\nWang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu,\nK. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy,\nI., et al. How far can camels go? exploring the state\nof instruction tuning on open resources. arXiv preprint\narXiv:2306.04751, 2023.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners.\narXiv preprint\narXiv:2109.01652, 2021.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824\u201324837, 2022.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\n10\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ. T., Li, Z., Weller, A., and Liu, W. Metamath: Boot-\nstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284, 2023.\nYue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su,\nY., and Chen, W. Mammoth: Building math generalist\nmodels through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nZhang, X., Li, C., Zong, Y., Ying, Z., He, L., and Qiu, X.\nEvaluating the performance of large language models on\ngaokao benchmark. 2023.\nZhao, W., Shang, M., Liu, Y., Wang, L., and Liu, J.\nApe210k: A large-scale and template-rich dataset of math\nword problems. arXiv preprint arXiv:2009.11506, 2020.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N.\nAgieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels. arXiv preprint arXiv:2304.06364, 2023.\n11\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nA. Appendix\nA.1. MWPBENCH: Transform Non-Word Problems into Word Problems\nFor datasets like TAL-SCQ (TAL, 2023), GaokaoBench-Math (Zhang et al., 2023), and AGIEval (Zhong et al., 2023), the\nproblems are presented in a multiple-choice format. To eliminate the influence of the problem type and concentrate on the\nintrinsic ability of LLMs to address mathematical problems, we converted these non-word problems into word problems.\nA.1.1. FILTERING QUESTIONS\nInitially, we identified and filtered out questions that rely heavily on the multiple-choice format. This filtering was done\nusing specific keywords and phrases that are indicative of multiple-choice questions.\n1\ndef is_bad_question(question):\n2\nquestion = question.lower()\n3\n4\nkeywords = [\n5\n\"?\",\n6\n\"which of the following\",\n7\n\"which one\",\n8\n\"which is\",\n9\n\"the following\",\n10\n\"which statement\"\n11\n]\n12\n13\nfor keyword in keywords:\n14\nif keyword in question:\n15\nprint(f\"Filtered question: {question}\")\n16\nreturn True\n17\nreturn False\nListing 1. Filtering questions\nA.1.2. CREATING QUESTION-ANSWER PAIRS\nAfter filtering out the aforementioned questions, the remaining questions were paired with their corresponding correct\nanswer choices. This transformation resulted in a format where each problem is presented as a word problem followed by its\nsolution.\nA.2. MWPBENCH: Translation of Non-English Problems to English\nFor several datasets, namely Math23k (Wang et al., 2017), Ape210k (Zhao et al., 2020), GaokaoBench-Math (Zhang et al.,\n2023), and AGIEval-Gaokao (Zhong et al., 2023), the problems are originally presented in Chinese. To ensure uniformity\nand mitigate the effects of multilingual representations, we translated these Chinese problems into English. The translation\nwas facilitated by the GPT-3.5-Turbo API. Due to parsing errors encountered during the post-processing, a few examples\nwere excluded. The prompt template employed for the translation request is provided below:\nI want you to act as a Math Translator.\nYour task is to translate Chinese math questions into English math\nquestions.\nMake sure to keep the original question numbers.\nMake sure to keep the math formula in Latex format.\nThe translations should be clear, accurate, and easily understandable for students who are native English speakers.\n# Chinese Math Questions #:\n<insert chinese questions>\n# English Math Questions #:\n12\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nA.3. CollegeMath: Extraction from textbooks\nTo construct the CollegeMath dataset, we made use of the GPT-3.5-Turbo API to parse and extract questions and answers\nfrom raw, segmented LaTeX exercises and their corresponding solutions.\nA.3.1. EXTRACTING QUESTIONS FROM EXERCISES\nThe primary goal was to convert raw, potentially unstructured questions from math textbooks into well-formulated LaTeX-\nformatted questions. Below is the prompt template we utilized for this extraction process:\nI want you to act as a Math Parser.\nYour task is to convert raw messy questions from a math textbook into\nwell-structured LaTeX-formatted questions.\nPlease ensure to retain the original question numbers.\nIf needed, prepend the original instructions to the parsed questions to make them more comprehensible.\nIf needed, skip the broken questions.\n<insert demo>\n#Raw Questions#:\n\u2018\u2018\u2018\n<insert a chapter of practice>\n\u2018\u2018\u2018\n#Well-structured LaTeX-formatted Questions#:\nA.3.2. EXTRACTING ANSWERS FROM SOLUTIONS\nSimilarly, for answers, our aim was to transform raw, messy answers from textbooks into clear, LaTeX-formatted answers.\nHere\u2019s the template for this task:\nI want you to act as a Math Parser.\nYour task is to convert raw messy answers from a math textbook into\nwell-structured LaTeX-formatted answers.\nPlease ensure to retain the original answer numbers.\nIf needed, skip the broken answers.\n<insert demo>\n#Raw Answers#:\n\u2018\u2018\u2018\n<insert a chapter of answer>\n\u2018\u2018\u2018\n#Well-structured LaTeX-formatted Answers#:\nBy employing the aforementioned prompt templates, we were able to extract a comprehensive set of questions and answers,\nthereby forming the foundation of the CollegeMath dataset.\nA.4. MathScale: Concrete Examples\nA.4.1. MORE EXTRACTED TOPICS\nA set of 30 topics, randomly chosen, is listed below to illustrate the variety:\n\"Arithmetic operations\" \"Word problem solving\" \"Mathematics\" \"Money and finance\" \"Problem-solving strategies\"\n\"Arithmetic\" \"Multiplication\" \"Proportions\" \"Basic arithmetic operations\" \"Conversion of units\" \"Measurement and\nweight\" \"Multiplication and addition\" \"Budgeting\" \"Basic arithmetic\" \"Wages and overtime\" \"Calculating earnings\"\n13\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\n\"Arithmetic Sequences\" \"Exponential Growth\" \"Financial calculations\" \"Problem solving\" \"Algebraic expressions\"\n\"Economics\" \"Time\" \"Business and finance\" \"Ratio and proportion\" \"Problem-solving\" \"Time calculations\" \"Addition\"\n\"Distance\" \"Speed\"\nA.4.2. MORE EXTRACTED KNOWLEDGE POINTS\nSimilarly, we provide a list of 30 knowledge points, chosen at random, to demonstrate the depth and breadth of content:\n\"Random selection of marbles\" \"Definition and properties of dot product\" \"Manipulation of complex numbers\"\n\"Calculation of time required to complete a task\" \"How to apply the concept of a seven-day cycle\" \"Distinct numbers\"\n\"Expectation of a function of a random variable\" \"Ability to calculate total time\" \"Combinations of numbers\"\n\"Calculation of weekly income\" \"Relative motion\" \"Understanding the relationship between centimeters and kilometers\"\n\"Diagonalizing a matrix\" \"Proportional relationships between two quantities\" \"Ergodic Markov chain\" \"Addition of\nvalues\" \"Counting the number of cars\" \"Converting fractions to whole numbers\" \"Identifying relationships between\ndifferent variables\" \"Ability to set up and solve a proportion equation\" \"Addition and subtraction of matrices\"\n\"Using logarithms to solve exponential equations\" \"Probability of rolling a specific number on a six-sided die\"\n\"Divisibility of polynomials\" \"Application of multiplication to calculate total revenue\" \"Identifying the highest\nand lowest scores\" \"Ability to calculate percentages.\" \"Geometric interpretation of dot product\" \"Dividing complex\nnumbers\" \"Understanding weight units\"\nA.5. Evaluation on Individual Topics\nWe examine the subset performances on MATH, as shown in Table 9. It is evident that MathScale consistently delivers\nexceptional results across diverse topics.\nModel\nMATH\nPrealgebra Algebra Intermediate\nAlgebra\nPrecalculus Probability Geometry Number\nTheory\nclosed-source models\nGPT-4\n75.2\n71.3\n25.3\n30.4\n52.5\n41.7\n45.7\nGPT-3.5\n59.3\n55.5\n17.3\n20.1\n30.1\n29.8\n30.3\nopen-source models fine-tuned on LLaMA-2 13B\nWizardMath\n23.6\n21.4\n7.5\n7.1\n10.9\n12.3\n6.8\nMAmmoTH\n21.4\n17.2\n6.9\n7.8\n11.8\n8.7\n6.2\nGAIR-Abel\n28.3\n23.3\n8.1\n9.1\n13.0\n15.0\n9.4\nMetaMath\n39.3\n32.1\n11.9\n10.2\n18.5\n17.7\n15.3\nMathScale\n52.9\n53.4\n13.6\n17.3\n24.6\n25.6\n25.7\nopen-source models fine-tuned on LLaMA-2 7B\nWizardMath\n16.5\n15.2\n6.3\n5.8\n6.7\n8.5\n5.9\nMAmmoTH\n15.1\n12.5\n6.5\n4.3\n9.9\n7.3\n6.1\nGAIR-Abel\n21.4\n17.6\n7.7\n6.9\n10.1\n9.8\n7.4\nMetaMath\n34.0\n29.6\n8.7\n9.8\n17.5\n15.4\n17.5\nMathScale\n48.9\n49.3\n12.4\n15.2\n23.2\n23.3\n23.8\nopen-source models fine-tuned on Mistral 7B\nWizardMath v1.1\n51.4\n50.7\n13.9\n19.9\n25.5\n24.4\n22.4\nMetaMath Mistral\n47.1\n41.4\n13.2\n12.6\n23.4\n23.7\n19.8\nMathScale\n55.9\n52.8\n14.6\n18.6\n28.9\n26.5\n27.5\nTable 9. Performance metrics across various topics on MATH. Within each section, the highest performing results are highlighted in bold\nfont.\nWe also detail the subset performances on CollegeMath at the College Level. As shown in Table 10, despite the MWP-\nBENCH training set\u2019s seed questions only encompassing algebra, precalculus, and calculus, MathScale demonstrates robust\nperformance in OOD\u2019s test sets including vector calculus, probability, and linear algebra. However, an area of challenge is\ndifferential equations, where all models show limited success.\n14\nMathScale: Scaling Instruction Tuning for Mathematical Reasoning\nModel\nCollegeMath\nAlgebra Precalculus Calculus Vector\nCalculus Probability Linear\nAlgebra\nDifferential\nEquation\nclosed-source models\nGPT-4\n41.1\n21.2\n20.6\n29.0\n11.5\n6.5\n1.2\nGPT-3.5\n37.7\n16.6\n17.8\n32.7\n10.0\n3.0\n1.2\nopen-source models fine-tuned on LLaMA-2 13B\nWizardMath\n12.0\n7.4\n8.2\n14.5\n2.8\n0.3\n0.3\nMAmmoTH\n11.2\n4.2\n7.0\n8.1\n2.8\n1.5\n0.0\nGAIR-Abel\n15.3\n6.0\n5.0\n3.6\n2.1\n1.9\n1.6\nMetaMath\n19.4\n9.8\n5.6\n8.1\n1.4\n1.1\n0.3\nMathScale\n35.0\n17.8\n15.8\n24.5\n7.9\n5.0\n1.9\nopen-source models fine-tuned on LLaMA-2 7B\nWizardMath\n9.7\n5.2\n10.2\n11.8\n1.4\n1.1\n0.3\nMAmmoTH\n9.5\n4.8\n7.0\n10.0\n2.1\n3.4\n0.0\nGAIR-Abel\n12.0\n4.2\n5.2\n6.3\n3.5\n1.5\n1.6\nMetaMath\n19.1\n6.8\n4.4\n5.4\n2.8\n2.6\n0.3\nMathScale\n34.2\n19.6\n18.8\n27.2\n7.9\n5.0\n0.6\nopen-source models fine-tuned on Mistral 7B\nWizardMath v1.1\n29.3\n14.0\n11.4\n16.3\n5.0\n2.3\n0.0\nMetaMath Mistral\n28.1\n12.2\n11.2\n21.8\n7.1\n3.8\n0.6\nMathScale\n37.1\n18.0\n19.4\n27.2\n8.6\n3.8\n1.6\nTable 10. Performance metrics across various topics on CollegeMath. Within each section, the highest performing results are highlighted\nin bold font.\n15\n"
  },
  {
    "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
    "link": "https://arxiv.org/pdf/2403.02545.pdf",
    "upvote": "14",
    "text": "Wukong: Towards a Scaling Law for Large-Scale Recommendation\nBuyun Zhang * 1 Liang Luo * 1 Yuxin Chen * 1\nJade Nie 1 Xi Liu 1 Daifeng Guo 1 Yanli Zhao 1 Shen Li 1\nYuchen Hao 1 Yantao Yao 1 Guna Lakshminarayanan 1\nEllie Dingqiao Wen 1 Jongsoo Park 1 Maxim Naumov 1 Wenlin Chen 1\nAbstract\nScaling laws play an instrumental role in the sus-\ntainable improvement in model quality. Unfor-\ntunately, recommendation models to date do not\nexhibit such laws similar to those observed in the\ndomain of large language models, due to the in-\nefficiencies of their upscaling mechanisms. This\nlimitation poses significant challenges in adapt-\ning these models to increasingly more complex\nreal-world datasets.\nIn this paper, we propose an effective network\narchitecture based purely on stacked factoriza-\ntion machines, and a synergistic upscaling strat-\negy, collectively dubbed Wukong, to establish a\nscaling law in the domain of recommendation.\nWukong\u2019s unique design makes it possible to\ncapture diverse, any-order of interactions simply\nthrough taller and wider layers.\nWe conducted extensive evaluations on six public\ndatasets, and our results demonstrate that Wukong\nconsistently outperforms state-of-the-art models\nquality-wise. Further, we assessed Wukong\u2019s scal-\nability on an internal, large-scale dataset. The\nresults show that Wukong retains its superior-\nity in quality over state-of-the-art models, while\nholding the scaling law across two orders of\nmagnitude in model complexity, extending be-\nyond 100 GFLOP/example or equivalently up to\nLarge Launguage Model (GPT-3) training com-\npute scale, where prior arts fall short.\n1. Introduction\nFrom movie recommendations to feed compilations, deep\nlearning-based recommendation systems (DLRS) are pow-\n*Equal contribution\n1Meta Platforms, Inc..\nCorrespon-\ndence to: Buyun Zhang <buyunz@meta.com>, Liang Luo <lian-\ngluo@meta.com>, Yuxin Chen <yuxinc@meta.com>.\n100\n101\n102\nGFLOP/example\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nWukong\nMaskNet\nDLRM\nDCNv2\nAFN+\nAutoInt+\nFinalMLP\nRelative LogLoss (%)\nFigure 1: Wukong outperforms existing state-of-the-art mod-\nels while demonstrating a scaling law in the recommendation\ndomain across two orders of magnitude in model complexity,\nextending beyond 100 GFLOP/example or equivalently up\nto Large Launguage Model (GPT-3) training compute scale,\nwhere prior arts fall short.\nering a wide range of online services as reported by major\ncontent providers (Naumov et al., 2019; Wang et al., 2021a;\nLian et al., 2021; Liu et al., 2022; Covington et al., 2016).\nModern DLRS are designed to process a blend of contin-\nuous dense features, such as date, and categorical sparse\nfeatures, like user clicked posts history. Each sparse fea-\nture is transformed into a dense embedding representation\nthrough a trainable embedding lookup table. These dense\nembeddings are then fed into an interaction component, de-\nsigned to capture the intricate interactions between features.\nWhile existing models demonstrate promising accuracy on\nsmaller datasets, their capability to adapt to the scale and\nintricacy of substantially larger datasets, and to sustain con-\ntinuous quality improvement as these models scale up, re-\nmains less certain. This scalability is increasingly crucial,\nas modern datasets have seen exponential growth. For ex-\nample, production datasets today might contain hundreds\nof billions of training examples (Wang et al., 2021a). Fur-\nthermore, foundational models (Bommasani et al., 2021)\n1\narXiv:2403.02545v2  [cs.LG]  8 Mar 2024\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nneed to operate at scale to handle larger and multiple com-\nplex input sources at the same time. Thus, the need for\na DLRS that can both upscale and downscale effectively,\nadjusting to varying dataset sizes and computational con-\nstraints, is paramount. This scalability is encompassed in\nwhat is known as a \"scaling law\" (Kaplan et al., 2020).\nTo date, the primary trend of DLRS up-scaling is through\nsparse scaling, i.e., expanding the sizes of embedding ta-\nbles by adding more entries to reduce collision and widen-\ning their dimensions for increased expressiveness. Conse-\nquently, DLRS have reached trillions of parameters (Kang\net al., 2020; Mudigere et al., 2021; Lian et al., 2021) with\nembedding tables dominating the parameter count.\nUnfortunately, the traditional way of up-scaling has a few\npractical drawbacks. Merely expanding the sparse com-\nponent of a model does not enhance its ability to capture\nthe complex interactions among an increasing number of\nfeatures. Moreover, this trend notably diverges from the\ntrend of hardware advancements, as most improvements in\nthe next generation accelerators lie in the compute capac-\nity (Luo et al., 2018; 2017), which embedding table lookups\ncan not utilize. Thus, simply expanding embedding table\nleads to prohibitive infrastructure costs with suboptimal\naccelerator utilization, especially in distributed settings.\nOur work aims to find an alternative scaling mechanism for\nrecommendation models, that can establish the scaling law,\nsimilar to that established in the LLM domain. Namely, we\nwould like to devise a unified architecture whose quality can\nbe continuously improved in conjunction with dataset size,\ncompute and parameter budgets, with a synergistic strategy.\nWe focus on upscaling interaction components, dubbed\ndense scaling, to mitigate the quality and efficiency draw-\nbacks from sparse scaling. However, existing models cannot\nbenefit from this paradigm for various reasons. For example,\nDLRM faces scalability constraints, being limited to captur-\ning only second-order interactions, which restricts its effec-\ntiveness on complex datasets. Moreover, existing models\nlack a comprehensive process for scaling up. For example,\nDCNv2 and AutoInt+ focus on tuning certain hyperparme-\nters, leading to rapidly diminishing returns when scaling up.\nAdditionally, even with modern tricks like residual connec-\ntion (He et al., 2016), layernorm (Ba et al., 2016), gradient\nclip (Pascanu et al., 2013)), up-scaling existing models is\nprone to training stability issues (Tang et al., 2023).\nTo establish a scaling law for recommendation models, we\npropose Wukong, a simple yet effective interaction architec-\nture that exhibits effective dense scaling properties. Inspired\nby the principles of binary exponentiation, our key inno-\nvation is to use a series of stacked Factorization Machines\n(FMs) to efficiently and scalably capture any-order feature\ninteractions. In our design, each FM is responsible of captur-\ning second order interactions with respect to its inputs, and\nthe outputs from these FMs are subsequently transformed by\nMLPs into new embeddings, which encode the interactions\nresults and serve as inputs to the next layers.\nWe evaluated Wukong\u2019s performance using six public\ndatasets and a large-scale internal dataset.\nThe results\ndemonstrate that Wukong outperforms state-of-the-art mod-\nels across all public datasets in terms of AUC, indicating\nthe effectiveness of Wukong\u2019s architecture and its ability\nto generalize across a wide range of recommendation tasks\nand datasets. On our internal dataset, Wukong not only sig-\nnificantly outperforms existing models in terms of quality\nat comparable levels of complexity but also shows contin-\nuous enhancements in quality when scaled up across two\norders of magnitude in model complexity, extending beyond\n100 GFLOP/example or equivalently up to 2336 PF-days\nof total training compute, normalized to 365 days assum-\ning online training (Zhai et al., 2024), comparable to Large\nLaunguage Model (GPT-3, 3640PF-days) training compute\nscale (Brown et al., 2020), where prior arts fall short.\n2. Related Work\nDeep Learning Recommendation Systems (DLRS)\nEx-\nisting DLRS share a similar structure. A typical model\nconsists of a sparse and a dense component. The sparse com-\nponent is essentially embedding lookup tables that transform\nsparse categorical features into dense embeddings, whereas\nthe dense component is responsible for capturing interac-\ntions among these embeddings to generate a prediction.\nDense Interaction Architectures\nCapturing interaction\nbetween features is the key to effective DLRS, and various\nprior arts have been proposed for this cause. For instance,\nAFN+ (Cheng et al., 2020) transforms features into a log-\narithmic space to capture arbitrary order of interactions;\nAutoInt+ (Song et al., 2019) uses multi-head self-attention;\nDLRM and DeepFM (Naumov et al., 2019; Guo et al., 2017)\nleverage Factorization Machines (FM) (Rendle, 2010) to\nexplicitly capture second order interactions; HOFM (Blon-\ndel et al., 2016) optimizes FM to efficiently capture higher\norder of interactions; DCNv2 (Wang et al., 2021a) uses\nCrossNet, which captures interactions via stacked feature\ncrossing, which can be viewed as a form of elementwise\ninput attention. FinalMLP (Mao et al., 2023) employs a\nbilinear fusion to aggregate results from two MLP streams,\neach takes stream-specific gated features as input. MaskNet\n(Wang et al., 2021b) adopts a series of MaskBlocks for inter-\naction capture, applying \u201cinput attention\u201d to the input itself\nand intermediate activations of DNN; xDeepFM (Lian et al.,\n2018) combines a DNN with a Compressed Interaction Net-\nwork, which captures interactions through outer products\nand compressing the results with element-wise summation.\n2\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nScaling up DLRS\nExisting literature mainly focuses on\nscaling up the sparse component of models (Kang et al.,\n2020; Mudigere et al., 2021; Lian et al., 2021). There is\nlimited discussion on the scalability of dense interaction ar-\nchitectures. Among the few investigated, their scalability are\noften unsatisfactory. For example, AutoInt+ observes dimin-\nishing gains with more than two layers on public datasets.\nSimilarly, DCNv2 experiences rapidly diminishing returns\nwhen expanding beyond two layers or increasing the rank\nsize, despite being tested on a large-scale production dataset.\n3. Design of Wukong\nWe keep two objectives in mind when designing Wukong\u2019s\narchitecture: (1) to effectively capture the intricate high-\norder feature interactions; and (2) to ensure Wukong\u2019s\nquality scale gracefully with respect to dataset size,\nGFLOP/example and parameter budgets.\n3.1. Overview\nIn Wukong, categorical and dense features initially pass\nthrough an Embedding Layer (Sec. 3.2), which transforms\nthese inputs into Dense Embeddings.\nAs shown in Figure 2, Wukong subsequently adopts an\nInteraction Stack (Sec. 3.3), a stack of unified neural net-\nwork layers to capture the interaction between embeddings.\nThe Interaction Stack draws inspiration from the concept\nof binary exponentiation, allowing each successive layer to\ncapture exponentially higher-order interactions. Each layer\nin the Interaction Stack consists of a Factorization Ma-\nchine Block (FMB, Sec. 3.4) and a Linear Compression\nBlock (LCB, Sec. 3.5). FMB and LCB independently take\nin input from last layer and their outputs are ensembled as\nthe output for the current layer. Following the interaction\nstack is a final Multilayer Perceptron (MLP) layer that maps\nthe interaction results into a prediction.\n3.2. Embedding Layer\nGiven a multi-hot categorical input, an embedding table\nmaps it to a dense embedding. This process involves a\nseries of lookups, each corresponding to a \u201chot\u201d dimensions\nwithin the input. The lookup results are then aggregated\nusing a pooling operation, with summation being the typical\nmethod in the realm of embeddings.\nIn our design, the embedding dimension is standardized for\nall embeddings generated by the Embedding Layer, known\nas the global embedding dimension d. To accommodate the\nvarying significance of different features, multiple embed-\ndings are generated for each feature deemed significant. In\ncontrast, less important features are allocated smaller under-\nlying embedding dimensions. These smaller embeddings are\nthen collectively grouped, concatenated, and transformed\nInteraction\nStack\nWukong\nLayer\nDense Embeddings\nFactorization \nMachine Block\nLinear \nCompress Block\nConcat\nMLP\nAdd & Norm\nWukong Layer\nWukong Layer\nOutput Predictions\nFigure 2: Wukong employs an interaction stack to capture\nfeature interactions. Each layer in the stack consists of a\nFactorization Machine Block and a Linear Compress Block.\ninto d-dimensional embeddings using a MLP.\nDense inputs are transformed by an MLP into latent embed-\ndings that share the same d dimension, and are joined with\nthe embedding outputs of categorical input. This yields an\noutput tensor of size X0 \u2208 Rn\u00d7d, where n is the total num-\nber of embeddings from the dense and sparse part. X0 is\nthen ready to be further processed by the Interaction Stack.\nNote that unlike conventional approaches like DCN (Wang\net al., 2021a), we interpret each embedding vector as a\nwhole unit (detailed later), and hence our representation of\nX0 \u2208 Rn\u00d7d as opposed to X0 \u2208 Rnd.\n3.3. Interaction Stack\nThe interaction modules stack l identical interaction layers,\nwhere each layer captures progressively higher-order fea-\nture interactions at an exponential rate using Factorization\nMachines (FMs).\nAn interaction layer has two blocks in parallel: a Factor-\nization Machine Block (FMB) and a Linear Compression\nBlock (LCB). FMB computes feature interactions between\ninput embeddings of the layer, and LCB simply forwards\nlinearly compressed input embeddings of the layer. The\noutputs of FMB and LCB are then concatenated.\nFor layer i in the stack, its results can contain feature inter-\nactions with arbitrary order from 1 to 2i. This can be simply\nshown by induction. Let\u2019s assume the input of layer i con-\ntains interactions of order from 1 to 2i\u22121, which is true for\nthe first layer (i.e. i = 1). Since FMB generates (o1 + o2)-\n3\nWukong: Towards a Scaling Law for Large-Scale Recommendation\norder feature interactions given o1 and o2-order interactions,\nthen we have immediately the output of layer i containing 1\nto 2i-order interactions, with the lower bound achieved from\nthe output of LCB and the upper bound achieved by the FM\ninteracting two 2i\u22121-order interactions from the input.\nTo help stabilize training, we also adopt residual connections\nacross layers, followed by layer normalization (LN). Putting\neverything together, we have\nXi+1 = LN(concat(FMBi(Xi), LCBi(Xi)) + Xi)\nDepending on the specific configurations of FMB and LCB,\nXi may have a different number of embeddings than Xi+1,\nwhich usually happens at the first layer. To handle this case,\nthe residual can be linearly compressed to match the shape.\n3.4. Factorization Machine Block (FMB)\nA FMB contains a Factorization Machine (FM) followed\nby a MLP. The FM is used to capture explicit feature in-\nteractions of the input embeddings, with the output being\na 2D interaction matrix where each element represents the\ninteraction between a pair of embeddings. This interaction\nmatrix is flattened and converted to a vector with shape of\n(nF \u00d7 d) through the MLP, and reshaped to nF embeddings\nfor later use.\nOperationally, a FMB does the following:\nFMB(Xi) = reshape(MLP(LN(flatten(FM(Xi)))))\nWukong\u2019s FM module is fully customizable: for example,\nin the most basic version, we followed the FM design in\n(Naumov et al., 2019), i.e., taking the dot product between\nall pairs of embedding vectors, FM(X) = XXT . We\ndiscuss more optimized FM designs in Sec. 3.6.\n3.5. Linear Compress Block (LCB)\nLCB simply linearly recombines embeddings without in-\ncreasing interaction orders, which is critical in ensuring that\nthe invariance of interaction order is maintained throughout\nthe layers. Specifically, it guarantees that the i-th interaction\nlayer captures interaction orders ranging from 1 to 2i. The\noperation performed by a LCB can be described as follows:\nLCB(Xi) = WLXi\nwhere WL \u2208 RnL\u00d7ni is a weight matrix, nL is a hyperpa-\nrameter indicating the number of compressed embeddings,\nand ni is the number of input embeddings of layer i.\n3.6. Optimized FM\nIn its basic form where a pairwise dot-product is used, FM\u2019s\ncomputation and storage complexity grows quadratically\nwith the number of embeddings. This quickly becomes pro-\nhibitive on real-world datasets with thousands of features.\nTo allow effective feature interaction while lowering com-\npute cost, we adopt a similar scheme to (Sharma, 2023;\nAnonymous, 2019) that leverage low-rank property in pair-\nwise dot product matrix, which was observed in many real-\nworld datasets (Wang et al., 2021a).\nWhen d <= n, the dot-product interaction XXT is a d-\nrank matrix, which is often the case on large datasets whose\nnumber of features is larger than the embedding dimension.\nTherefore, we can effectively reduce the size of output ma-\ntrix from n \u00d7 n to n \u00d7 k, where k is a hyperparameter,\nby multiplying XXT with a learnable projection matrix Y\nof shape n \u00d7 k (i.e., computing XXT Y ) without loss of\ninformation in theory. This reduces memory requirement to\nstore the interaction matrix. We can then take advantage of\nthe associative law to compute XT Y first, further reducing\ncompute complexity from O(n2d) to O(nkd) with k << n.\nFurthermore, to enhance the model quality, the projection\nmatrix Y can be made attentive to the input by processing\nlinearly compressed input through a MLP. We use the opti-\nmized FM in our following experiments by default, unless\nmentioned otherwise.\n3.7. Complexity Analysis\nWe assume each layer in the Interaction Stack uses the same\nhyperparameters, and the largest FC in the MLP has size h.\nFor the first layer, the time complexity of FMB is the sum\nof the FM and the MLP, which is O(nkd) \u2248 O(ndh) and\nO(nkh + h2 + nF dh) \u2248 O(ndh + h2), respectively. The\ntime complexity of LCB is O(nnLd) \u2248 O(ndh). For sub-\nsequent layers, the time complexity is O(n\u2032dh + h2), where\nn\u2032 = nL +nF . Hence, the total time complexity of Wukong\nis O(ndh + ln\u2032dh + h2) \u2248 O(ndhlogn + h2).\n3.8. Scaling Wukong\nHaving defined the Wukong architecture, we summarize the\nmain hyperparameters that are related to scale up and later\nwe describe our efforts to upscaling Wukong with respect to\nthese hyperparameters.\n\u2022 l: number of layers in the Interaction Stack.\n\u2022 nF : number of embeddings generated by FMB\n\u2022 nL: number of embeddings generated by LCB\n\u2022 k: number of compressed embeddings in optimized FM\n\u2022 MLP: number of layers and FC size in the MLP of FMB\nDuring scaling up, we initially focus on increasing l to en-\nable the model to capture higher-order interactions. Follow-\ning this, we enlarge other hyperparameters to augment the\nmodel\u2019s capacity of capturing broader range of interactions.\n4\nWukong: Towards a Scaling Law for Large-Scale Recommendation\n3.9. Intuition Behind Wukong\u2019s Enhanced Effectiveness\nCompared to existing work using FM as their primary in-\nteraction architecture, Wukong\u2019s innovative approach of\nstacking FMs greatly enhances the conventional FM\u2019s capa-\nbility. This allows Wukong to capture interactions of any\norder, making it highly effective for large-scale, complex\ndatasets that require higher-order reasoning. While there\nare efforts towards high-order FM, Wukong\u2019s exponential\nrate of capturing high-order interactions offers great effi-\nciency, bypassing the linear complexity seen in HOFM and\navoiding the costly outer product in xDeepInt.\nWhile MLPs have shown limitations in implicitly capturing\ninteractions (Beutel et al., 2018), Wukong diverges from ap-\nproaches that rely on MLPs for interaction capture. Instead,\nWukong primarily employs MLPs to transform the results of\ninteractions into embedding representations, which are then\nused for further interactions. This distinct use of MLPs en-\nhances the model\u2019s ability to process and interpret complex,\nheterogeneous features effectively.\nAdditionally, Wukong treats each embedding as a single\nunit, focusing on embedding-wise interactions. This ap-\nproach significantly reduces computational demands com-\npared to architectures that capture element-wise interac-\ntions, as it avoids the less useful intra-embedding and cross-\ndimensional interactions. Consequently, Wukong not only\nenhances the efficiency of the recommendation system but\nalso maintains its effectiveness in capturing relevant feature\ninteractions.\n4. Implementation\nThis section discusses practices to effectively train high-\ncomplexity Wukong on large-scale datasets.\nOverall, distributed training is required to make Wukong\ntraining feasible. For the embedding layer, we use a column-\nwise sharded embedding bag implementation provided by\nNeo (Mudigere et al., 2021) and NeuroShard (Zha et al.,\n2023). On the dense part, we balance the trade-off between\nperformance and memory capacity by adopting FSDP (Zhao\net al., 2023) and tune the sharding factor so that the model\nfits in the memory without creating too much redundancy.\nTo enhance training efficiency, we employ both automatic\noperator fusion through torch.fx (Reed et al., 2022) to im-\nprove training performance. In addition, we aggressively\napply quantization to reduce compute, memory, and com-\nmunication overheads simultaneously. Specifically, we train\nWukong\u2019s embedding tables in FP16, and communicate em-\nbedding lookup results in FP16 in the forward pass and\nBF16 in the backward pass; we use BF16 quantization dur-\ning the transport and reduction of gradients for dense param-\neters in the backward pass.\n#Samples\n#Features\nFrappe\n0.29M\n10\nMicroVideo\n1.7M\n7\nMovieLens Latest\n2M\n3\nKuaiVideo\n13M\n8\nTaobaoAds\n26M\n21\nCriteo Terabyte\n4B\n39\nInternal\n146B\n720\nTable 1: Statistics of our evaluation datasets.\n5. Overview of Evaluations\nWe evaluate Wukong using six public datasets and an inter-\nnal dataset, details of which are summarized in Table 1. The\nresults of these evaluations are organized in two sections.\nIn Section 6, we evaluate on six public datasets, focusing\non demonstrating the effectiveness of Wukong in the low\ncomplexity realm. Our results show that Wukong surpasses\nprevious state-of-the-art methods across all six datasets,\ndemonstrating its effectiveness.\nIn Section 7, we evaluate on our large-scale in-house dataset\nto demonstrate the scalability of Wukong. The dataset con-\ntains 30 times more samples and 20 times more features\ncompared to one of the largest dataset Criteo. Our results\nreveals that (1) Wukong consistently outperforms all base-\nline models in terms of both model quality and runtime\nspeed, maintaining this superiority across all complexity\nscales; (2) Wukong exhibits a better scaling trend in com-\nparison to baseline models. We also conduct an ablation\nstudy to gain understanding of the individual contributions\nand the effectiveness of each component within Wukong.\n6. Evaluation on Public Datasets\nIn this section, we aim to demonstrate the effectiveness of\nWukong across a variety of public datasets. Unless noted\notherwise, we use the preproc provided by the BARS bench-\nmark (Zhu et al., 2022b) for consistency with prior work.\n6.1. General Evaluation Setup\n6.1.1. DATASETS\nFrappe (Baltrunas) is an app usage log. This datasets\npredicts whether a user uses the app with the given contexts.\nMicroVideo (Chen et al., 2018) is a content understanding-\nbased dataset provided by THACIL work containing inter-\nactions between users and micro-videos. This log contains\nmultimodal embeddings, together with traditional features.\nMovieLens Latest (Harper & Konstan, 2015) is a well\nknown dataset that contains users\u2019 ratings on movies.\n5\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nFrappe\nMicroVideo\nMovieLens L.\nKuaiVideo\nTaobaoAds\nCriteo TB\nAUC LogLoss\nAUC LogLoss\nAUC LogLoss\nAUC LogLoss\nAUC LogLoss\nAUC\nLogLoss\nBaselines\nAFN+\n0.9812 0.2340\n0.7220 0.4142\n0.9648 0.3109\n0.7348 0.4372\n0.6416 0.1929\n0.8023 0.1242\nAutoInt+\n0.9806 0.1754\n0.7155 0.4203\n0.9693 0.2178\n0.7297 0.4376\n0.6437 0.1930\n0.8073 0.1233\nDCNv2\n0.9774 0.2325\n0.7187 0.4162\n0.9683 0.2169\n0.7360 0.4383\n0.6457 0.1926\n0.8096 0.1227\nDLRM\n0.9846 0.1465\n0.7173 0.4179\n0.9685 0.2160\n0.7357 0.4382\n0.6430 0.1931\n0.8076 0.1232\nFinalMLP\n0.9868 0.1280\n0.7247 0.4147\n0.9723 0.2211\n0.7374 0.4435\n0.6434 0.1928\n0.8096 0.1226\nMaskNet\n0.9816 0.1701\n0.7255 0.4157\n0.9676 0.2383\n0.7376 0.4372\n0.6433 0.1927\n0.8100 0.1227\nxDeepFM\n0.9780 0.2441\n0.7167 0.4172\n0.9667 0.2089\n0.7118 0.4565\n0.6342 0.1961\n0.8084 0.1229\nOurs\nWukong\n0.9868 0.1757\n0.7292 0.4148\n0.9723 0.1794\n0.7414 0.4367\n0.6488 0.1954\n0.8106 0.1225\nTable 2: Evaluation results on six public datasets. The model with best AUC and best LogLoss on each dataset are highlighted.\nKuaiVideo (Kuaishou)\nis the competition dataset re-\nleased by Kuaishou. The dataset is used to predict the click\nprobability of a user on new micro-videos. This dataset also\ncontains content understanding-based embeddings along\nwith other categorical and float features.\nTaobaoAds (Tianchi, 2018)\nThis dataset includes 8 days\nof ads click through rate (CTR) prediction on Taobao.\nCriteo Terabyte (Criteo)\nThis dataset contains 24 days of\nads click feedback. We used the last day of data for testing.\n6.1.2. BASELINES\nWe benchmark Wukong against seven widely recognized\nstate-of-the-art models used in both academia and industry,\nincluding AFN+ (Cheng et al., 2020), AutoInt+ (Song et al.,\n2019), DLRM (Naumov et al., 2019), DCNv2 (Wang et al.,\n2021a), FinalMLP (Mao et al., 2023), MaskNet (Wang et al.,\n2021b) and xDeepFM (Lian et al., 2018).\n6.1.3. METRICS\nAUC\nArea Under the Curve (AUC) provides an aggre-\ngated measure of the model\u2019s ability to correctly classify\npositives and negatives across all thresholds. A higher AUC\nvalue indicates a better model performance.\nLogLoss\nThe log loss quantifies the penalty based on how\nfar the prediction is from the actual label. The lower the log\nloss, the better the model.\n6.2. Model-Specific Setup\nFor the five smaller datasets, aside from Criteo, we adopted\nthe public BARS evaluation framework (Zhu et al., 2022a;\n2021). We directly use the best searched model configs\non BARS whenever possible, and use the provided model\ndefault hyperparameters for the rest. In addition to the\ndefault embedding dimension provided in the framework,\nwe further test an embedding dimension of 128 and report\nwhichever of these two configurations yielded better results.\nFor Wukong, we tune the dropout rate and optimizer settings\nand compression of LCB to adapt to the number of features.\nWe leverage the larger Criteo dataset to evaluate the model\nperformance on realistic online recommendation systems,\nwhere one-pass training is performed. In light of the new\ntraining setup, we conducted extensive grid search using the\nsystem described in Sec. 4 for all baselines and Wukong to\nfacilitate fair comparisons. This exhaustive process involved\nnearly 3000 individual runs. We provide the model-specific\nsearch space in Appendix A. The best searched model hy-\nperparameters were later used as the base config in Sec 7.\n6.3. Results\nWe summarize the results in Table 2. Overall, Wukong\nis able to achieve state-of-the-art results in terms of AUC\nacross all public datasets. This result demonstrates the effec-\ntiveness of Wukong\u2019s architecture and its ability to compre-\nhend diverse datasets and to generalize across a wide range\nof recommendation tasks.\n7. Evaluation on an Internal Dataset\nIn this section, our demonstrate the scalability of Wukong\nand gain a deep understanding of how different individual\ncomponents of Wukong contribute to its effectiveness, using\na large-scale dataset.\n7.1. Evaluation Setup\n7.1.1. DATASET\nThis dataset contains 146B entries in total and has 720\ndistinct features. Each feature describes a property of either\nthe item or the user. There are two tasks associated with\nthis dataset: (Task1) predicting whether a user has showed\ninterested in an item (e.g., clicked) and (Task2) whether a\nconversion happened (e.g., liked, followed).\n6\nWukong: Towards a Scaling Law for Large-Scale Recommendation\n7.1.2. METRICS\nGFLOP/example\nGiga Floating Point Operations per\nexample (GFLOP/example) quantifies the computational\ncomplexity during model training.\nPF-days\nThe total amount of training compute equivalent\nto running a machine operating at 1 PetaFLOP/s for 1 day.\n#Params\nModel size measured by the number of param-\neters in the model. The sparse embedding table size was\nfixed to 627B parameters.\nRelative LogLoss\nLogLoss improvement relative to a\nfixed baseline. We opt to use the DLRM with the basic con-\nfig as the baseline. A 0.02% Relative LogLoss improvement\nis considered as significant on this dataset. We report rela-\ntive LogLoss on the last 1B-window during online training.\n7.1.3. BASELINES\nWe adhere to the same baseline setup as detailed in Sec.\n6.1.2. However, xDeepFM was not included in the reported\nresults, due to the incompatibility of its expensive outer\nproduct operation with the large-scale dataset, consistently\ncausing out-of-memory issues even in minimal setups.\n7.1.4. TRAINING\nWe used the best optimizer configuration found in our pilot\nstudy across all experiments, i.e., Adam with lr=0.04 with\nbeta1=0.9, beta2=1 for dense part and Rowwise Adagrad\nwith lr=0.04 for sparse embedding tables. Models were\ntrained and evaluated in an online training manner. We fix\nthe embedding dimension to 160 across all runs.\nWe set the hyperparameters with the best configuration\nfound on the Criteo Terabyte evaluation described in Sec. 6\nas a starting point, and gradually scale up parameter count\nfor each model. We use a global batch size of 262,144 for\nall experiments. Each experiment was run on 128 or 256\nH100 GPUs depending on the model size.\n7.2. Results\nWe observed comparable results for both tasks, and report\nresults for Task1 in the main text, while the detailed results\nof Task2 are provided in Appendix B.\nQuality vs. Compute Complexity\nIn Fig. 1, we depict\nthe relationship between quality and compute complexity.\nThe results show that Wukong consistently outperforms all\nbaselines across various complexity levels, achieving over\n0.2% improvement in LogLoss. Notably, Wukong holds its\nscaling law across two orders of magnitude in model com-\nplexity \u2013 approximately translating to a 0.1% improvement\nfor every quadrupling of complexity. Among baselines,\nAFN+, DLRM and FinalMLP tend to reach a plateau after\n627 + 100\n627 + 101\n# Sparse + Dense Params (B)\n\u22120.75\n\u22120.50\n\u22120.25\n0.00\n0.25\nRelative LogLoss (%)\nWukong\nMaskNet\nFinalMLP\nDCNv2\nAFN+\nAutoInt+\nDLRM\nFigure 3: Scalability of Wukong with respect to # parameters.\na certain complexity level, while AutoInt+, DCNv2 and\nMaskNet failed to further enhance quality 1. Nonetheless,\neven DCNv2, the top-performing baseline, demands a 40-\nfold increase in complexity to match Wukong\u2019s quality.\nQuality vs. Model Size\nIn Fig. 3, we illustrate the\ncorrelation between model quality and model size. Echoing\nthe trends observed in compute complexity scaling above,\nWukong consistently outperforms all baselines by rougly\n0.2% across all scales of model size. while demonstrating a\nsteady improvement trend up to over 637 billion parameters.\nModel-Specifc Scaling\nThroughout the scaling process,\nwe employed distinct strategies per model. Detailed hyper-\nparameter settings for each run are provided in Appendix B.\nScaling processes of each model are summarized as follows:\nWukong We scaled up Wukong by tuning the hyperpa-\nrameters detailed in Sec. 3.8.\nAFN+ We scaled up AFN\u2019s hidden layers, ensemble\nDNN, and the number of logarithmic neurons. The results\nshow that scaling up AFN does not improve model quality.\nAutoInt+ We scaled up multi-head attention and the\nensemble DNN. Model quality of this model is initially\nworse than others, but improves notably when scaling up.\nDLRM We scaled up the top MLP. The results show that\nthe quality starts saturated beyond 31 GFLOP/example.\nDCNv2 We scaled up both Cross Network and Deep\nNetwork. Scaling up Cross Network did not yield any qual-\nity improvement. The training stability of DCNv2 is worse\nthan other models and we applied strict gradient clipping.\n1AutoInt+ and DCNv2 consistently faced significant training\ninstability issue when further scaled up. AutoInt+ recovered from\nloss explosion, albeit with reduced model quality; while DCNv2\nfailed to recover, and its quality was estimated from performance\nbefore the explosion. MaskNet was hindered by excessive memory\nconsumption, leading to out-of-memory errors, blocking further\nscaling up.\n7\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nFinalMLP We scaled up the two MLP streams and the\nFeature Selection modules. The results show that the model\nquality improves in the low complexity region, but starts to\nsaturate beyond 36 GFLOP/example.\nMaskNet We tested both Parallel and Serial MaskNet,\nand found that the Parallel variant is better. We decreased\nthe initial reduction ratio to ensure the model has a runnable\nsize, and progressively scaled up number of MaskBlocks,\nthe DNN and the reduction ratio.\n7.3. Ablation\nSignificance of Individual Components\nOur goal is to\ndemonstrate the importance of FMB, LCB and the residual\nconnection in Wukong\u2019s Interaction Stack. To this end,\nwe performed experiments in which each component was\nindividually deactivated by zeroing out its results.\nAs shown in Fig. 4, nullifying FMB results in a large quality\ndegradation. Interestingly, the deactivation of either LCB\nor the residual leads to only a modest decline in quality,\nwhile disabling both causes a substantial degradation. This\nobservation implies that by zero-padding FMB outputs and\nincorporating a residual connection, LCB can be simplified.\nZero Out\nFCB\nZero Out\nLCB\nNo\nResidual\nZero Out LCB\n& No Residual\n0.0\n0.5\n1.0\n1.5\n0.8\n0.03\n0.08\n1.84\nRelative LogLoss (%)\nFigure 4: Signi\ufb01cance of individual components.\nImpact of Scaling Individual Components\nWe aim to\ndissect the contributions in model quality when scaling up\neach hyperparameter within Wukong. We started from a\nbase configuration and proceeded to incrementally double\neach hyperparameter. The results are depicted in Fig. 5.\nWe observed that increasing the number of Wukong lay-\ners l leads to a substantial uplift in model quality, due to\nhigher-order interactions being captured. Additionally, aug-\nmenting the MLP size results in considerable performance\nenhancements. Elevating k and nF proves beneficial, while\nnL has plateaued for the base configuration. Notably, a\ncombined scale-up of k, nF , nL delivers more pronounced\nquality improvements than scaling each individually.\n8. Discussions\nPractically Serving Scaled-up Models\nScaling up to\nhigh complexity presents notable challenges for real-time\nserving. Potential solutions include training a multi-task\n100\n2 \u00d7 100\n3 \u00d7 100\nGFLOP/example\n\u22120.10\n\u22120.05\n0.00\n\u001f\u001e\u001d\nk=96; nF=32;\nnL=32\nk=96\nnF=32\nnL= 32\nMLP=\n3x8192\nRelative LogLoss (%)\nFigure 5: Impact of scaling individual components.\nfoundation model to amortize costs: distilling knowledge\nfrom the large models into small, efficient ones for serving.\nImplication on Resource-Limited Research and Busi-\nnesses\nWukong\u2019s efficiency and scalability offer signifi-\ncant advantages to researchers and businesses with limited\nresources. Its capability to match the accuracy of other\nmodels with forty times less computing resources enables\ncutting-edge recommendation research and applications,\neven in environments with limited computation resources.\nLimitation and Future Work\nWe also note limitations\nand caveats to our work, which can be goals in future work.\nUnderstanding the exact limit of Wukong\u2019s scalability is\nan important area of research. Due to the massive com-\npute requirement, we have not been able to reach a level of\ncomplexity where the limit applies.\nWhile Wukong demonstrates superior quality in various\nevaluations, a comprehensive theoretical understanding of\nits underlying principles, particularly in contrast to archi-\ntectures like transformers which share stacked dot product\nstructure, remains an area that needs further exploration.\nAdditionally, Wukong\u2019s generalizability beyond recommen-\ndation, particularly in domains that involve heterogeneous\ninput data sources similar to distinct features in recommen-\ndation, remains to be further explored and understood.\n9. Conclusion\nWe proposed an effective network architecture, named\nWukong. We demonstrated that Wukong establishes a scal-\ning law in the domain of recommendation that is not pre-\nviously observed \u2013 Wukong is able to efficiently scale up\nand down across two order of magnitude in compute com-\nplexity while maintaining a competitive edge over other\nstate of the art models, making it a scalable architecture\nthat can serve as a backbone from small vertical models to\nlarge foundational models across a wide range of tasks and\ndatasets.\n8\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nImpact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAnonymous. Dot product matrix compression for machine\nlearning. Technical Disclosure Commons, 2019.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBaltrunas, L. Frappe - mobile app usage. URL https:\n//www.baltrunas.info/context-aware.\nBeutel, A., Covington, P., Jain, S., Xu, C., Li, J., Gatto, V.,\nand Chi, E. H. Latent cross: Making use of context in\nrecurrent recommender systems. In Proceedings of the\neleventh ACM international conference on web search\nand data mining, pp. 46\u201354, 2018.\nBlondel, M., Fujino, A., Ueda, N., and Ishihata, M. Higher-\norder factorization machines. Advances in Neural Infor-\nmation Processing Systems, 29, 2016.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\nlut, A., Brunskill, E., et al. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258,\n2021.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, X., Liu, D., Zha, Z.-J., Zhou, W., Xiong, Z., and\nLi, Y. Temporal hierarchical attention at category- and\nitem-level for micro-video click-through prediction. In\nMM, 2018.\nCheng, W., Shen, Y., and Huang, L. Adaptive factorization\nnetwork: Learning adaptive-order feature interactions. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pp. 3609\u20133616, 2020.\nCovington, P., Adams, J., and Sargin, E. Deep neural net-\nworks for youtube recommendations. In Proceedings of\nthe 10th ACM conference on recommender systems, pp.\n191\u2013198, 2016.\nCriteo.\nCriteo\n1tb\nclick\nlogs\ndataset.\nhttps://ailab.criteo.com/\ndownload-criteo-1tb-click-logs-dataset/.\nGuo, H., Tang, R., Ye, Y., Li, Z., and He, X. Deepfm: a\nfactorization-machine based neural network for ctr pre-\ndiction. arXiv preprint arXiv:1703.04247, 2017.\nHarper, F. M. and Konstan, J. A. The movielens datasets:\nHistory and context. ACM Trans. Interact. Intell. Syst., 5\n(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872.\nURL https://doi.org/10.1145/2827872.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2016.\nKang, W.-C., Cheng, D. Z., Yao, T., Yi, X., Chen, T., Hong,\nL., and Chi, E. H. Learning to embed categorical features\nwithout embedding tables for recommendation. arXiv\npreprint arXiv:2010.10784, 2020.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKuaishou.\nURL https://www.kuaishou.com/\nactivity/uimc.\nLian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., and Sun, G.\nxdeepfm: Combining explicit and implicit feature inter-\nactions for recommender systems. In Proceedings of the\n24th ACM SIGKDD international conference on knowl-\nedge discovery & data mining, pp. 1754\u20131763, 2018.\nLian, X., Yuan, B., Zhu, X., Wang, Y., He, Y., Wu, H.,\nSun, L., Lyu, H., Liu, C., Dong, X., Liao, Y., Luo, M.,\nZhang, C., Xie, J., Li, H., Chen, L., Huang, R., Lin, J.,\nShu, C., Qiu, X., Liu, Z., Kong, D., Yuan, L., Yu, H.,\nYang, S., Zhang, C., and Liu, J. Persia: An open, hybrid\nsystem scaling deep learning-based recommenders up to\n100 trillion parameters. November 2021.\nLiu, Z., Zou, L., Zou, X., Wang, C., Zhang, B., Tang, D.,\nZhu, B., Zhu, Y., Wu, P., Wang, K., et al. Monolith: Real\ntime recommendation system with collisionless embed-\nding table. corr abs/2209.07663 (2022), 2022.\nLuo, L., Liu, M., Nelson, J., Ceze, L., Phanishayee, A., and\nKrishnamurthy, A. Motivating in-network aggregation\nfor distributed deep neural network training. In Workshop\non Approximate Computing Across the Stack, 2017.\nLuo, L., Nelson, J., Ceze, L., Phanishayee, A., and Krishna-\nmurthy, A. Parameter hub: a rack-scale parameter server\nfor distributed deep neural network training. In Proceed-\nings of the ACM Symposium on Cloud Computing, pp.\n41\u201354, 2018.\n9\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nMao, K., Zhu, J., Su, L., Cai, G., Li, Y., and Dong, Z.\nFinalmlp: An enhanced two-stream mlp model for ctr\nprediction. arXiv preprint arXiv:2304.00902, 2023.\nMudigere, D., Hao, Y., Huang, J., Tulloch, A., Sridharan,\nS., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., et al.\nHigh-performance, distributed training of large-scale\ndeep learning recommendation models. arXiv preprint\narXiv:2104.05158, 2021.\nNaumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sun-\ndaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.-J.,\nAzzolini, A. G., et al. Deep learning recommendation\nmodel for personalization and recommendation systems.\narXiv preprint arXiv:1906.00091, 2019.\nPascanu, R., Mikolov, T., and Bengio, Y. On the difficulty\nof training recurrent neural networks. In International\nconference on machine learning, pp. 1310\u20131318. Pmlr,\n2013.\nReed, J., DeVito, Z., He, H., Ussery, A., and Ansel, J. Torch.\nfx: Practical program capture and transformation for deep\nlearning in python. Proceedings of Machine Learning\nand Systems, 4:638\u2013651, 2022.\nRendle, S. Factorization machines. In 2010 IEEE Interna-\ntional Conference on Data Mining, pp. 995\u20131000. ieeex-\nplore.ieee.org, December 2010.\nSharma,\nS.\nFeature\nfusion\nfor\nthe\nuninitiated\n|\nby\nsiddharth\nsharma\n|\nmedium.\nhttps:\n//siddharth-1729-65206.medium.com/\nfeature-fusion-for-the-uninitiated-4c5938db28b7,\n2023. (Accessed on 01/24/2024).\nSong, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., and\nTang, J. Autoint: Automatic feature interaction learning\nvia self-attentive neural networks. In Proceedings of the\n28th ACM international conference on information and\nknowledge management, pp. 1161\u20131170, 2019.\nTang, J., Drori, Y., Chang, D., Sathiamoorthy, M., Gilmer,\nJ., Wei, L., Yi, X., Hong, L., and Chi, E. H. Improving\ntraining stability for multitask ranking models in recom-\nmender systems. arXiv preprint arXiv:2302.09178, 2023.\nTianchi. Ad display/click data on taobao.com, 2018. URL\nhttps://tianchi.aliyun.com/dataset/\ndataDetail?dataId=56.\nWang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong,\nL., and Chi, E. Dcn v2: Improved deep & cross network\nand practical lessons for web-scale learning to rank sys-\ntems. In Proceedings of the web conference 2021, pp.\n1785\u20131797, 2021a.\nWang, Z., She, Q., and Zhang, J.\nMasknet: Introduc-\ning feature-wise multiplication to ctr ranking models by\ninstance-guided mask. arXiv preprint arXiv:2102.07619,\n2021b.\nZha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu,\nY., Nie, J., Huang, Y., Tian, Y., Kejariwal, A., et al. Pre-\ntrain and search: Efficient embedding table sharding with\npre-trained neural cost models. Proceedings of Machine\nLearning and Systems, 5, 2023.\nZhai, J., Liao, L., Liu, X., Wang, Y., Li, R., Cao, X.,\nGao, L., Gong, Z., Gu, F., He, M., et al. Actions speak\nlouder than words: Trillion-parameter sequential trans-\nducers for generative recommendations. arXiv preprint\narXiv:2402.17152, 2024.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\nPytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023.\nZhu, J., Liu, J., Yang, S., Zhang, Q., and He, X. Open\nbenchmarking for click-through rate prediction.\nIn\nDemartini, G., Zuccon, G., Culpepper, J. S., Huang,\nZ., and Tong, H. (eds.), CIKM \u201921: The 30th ACM\nInternational Conference on Information and Knowl-\nedge Management, Virtual Event, Queensland, Australia,\nNovember 1 - 5, 2021, pp. 2759\u20132769. ACM, 2021.\ndoi: 10.1145/3459637.3482486. URL https://doi.\norg/10.1145/3459637.3482486.\nZhu, J., Dai, Q., Su, L., Ma, R., Liu, J., Cai, G., Xiao,\nX., and Zhang, R. BARS: towards open benchmark-\ning for recommender systems. In Amig\u00f3, E., Castells,\nP., Gonzalo, J., Carterette, B., Culpepper, J. S., and\nKazai, G. (eds.), SIGIR \u201922: The 45th International\nACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15,\n2022, pp. 2912\u20132923. ACM, 2022a.\ndoi: 10.1145/\n3477495.3531723.\nURL https://doi.org/10.\n1145/3477495.3531723.\nZhu, J., Dai, Q., Su, L., Ma, R., Liu, J., Cai, G., Xiao,\nX., and Zhang, R. BARS: towards open benchmark-\ning for recommender systems. In Amig\u00f3, E., Castells,\nP., Gonzalo, J., Carterette, B., Culpepper, J. S., and\nKazai, G. (eds.), SIGIR \u201922: The 45th International\nACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15,\n2022, pp. 2912\u20132923. ACM, 2022b.\ndoi: 10.1145/\n3477495.3531723.\nURL https://doi.org/10.\n1145/3477495.3531723.\n10\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nA. Model-Specific Grid Search Space on\nCriteo\nWe use Adam for dense arch optimization and use Rowwise\nAdaGrad for sparse arch optimization with a linear warmup\nperiod for the first 10% steps. We use 8 \u2217 16384 = 131, 072\nglobal batch size. All models use ReLU for activation. We\nopted to use 128 as embedding dimension, as it shows better\nresults on all models in our pilot experiments. We use FP32\nin all runs. Due to the dataset volume and model size, we\nuse (Mudigere et al., 2021) as the sparse distributed training\nframework and data parallel for dense synchronization.\nTo facilitate fair comparisons, we conducted extensive grid\nsearch (>3000 runs) over both general hyper-parameters and\nmodel-specific configs on Criteo Dataset.\nFor all the models, both sparse and dense learning rate was\nseparately tuned in {1e\u22123, 1e\u22122, 1e\u22121}. For MLPs in all the\nmodels, the number of hidden layers ranged in {1, 2, 3, 4}\nwith their layer sizes in {512, 1024, 2048}. To reduce the\nexcessively large search space, we did a pilot experiments\non the optimizer hyperparameters, and found that setting\nlearning rate to 1e\u22123 for dense and 1e\u22121 for sparse works\nthe best for all models. We fixed the learning rate in the\nfollowing runs. We now describe model-specific search\nspace:\nAFN+\nThe AFN hidden units and DNN hidden units\nare the same across all runs, followed the general MLP\nsearch space. The number of logarithmic neurons ranges in\n{128, 256, 512, 1024}.\nAutoInt+\nWe created the search space based on the best\nconfigurations reported in the paper (Song et al., 2019), with\na larger value being considered additionally per hyperpa-\nrameter. The number of attention layers ranged in {3, 4},\nwith attention dim ranged in {256, 512}. The number of\nattention heads are in {4, 8}. The DNN hidden units follow\nthe general MLP search space.\nDCNv2\nThe number of cross layers ranged from 1 to 4.\nRank searched in either full-rank or 512.\nDLRM\nThe bottom MLP layer sizes and numbers was\nset to [512, 256].\nFinalMLP\nWe followed the public benchmark setup (Zhu\net al., 2022a), by setting FeatureSelection (FS) to all float\nfeatures for one stream, and searching over one of 8 selected\nsparse features for the other stream. FS MLP is set to [800].\nNumber of heads is fixed to 256.\nMaskNet\nWe tested both Parallel MaskNet and Serial\nMaskNet. For the Parallel variant, we consider the number\nof blocks in {1, 8, 16} and the block dimension in {64, 128}.\nFor the Serial variant, we consider the number of layers in\n{1, 4, 8} with the layer size in {64, 256, 1024}. We fixed\nthe reduction ratio to 1 for both variants.\nxDeepInt\nWe considered Compressed Interaction Net-\nwork (CIN) with the number of layers in {3, 4} and the\nlayer dimension in {16, 32, 64}.\nWukong\nThe bottom MLP layer sizes and numbers was\nset to [512, 256]. l ranged from 1 to 4; nF and nL are set to\nthe same value, ranged in {8, 16}. k is fixed to 24.\nB. Model-Specific Scaling-up Configurations\nPlease refer to Table 3 for details.\nC. Analysis of High Order Interactions in\nWukong\nThe traditional factorization machine approach solves sec-\nond order interaction problem by minimizing (Naumov et al.,\n2019):\nmin \u03a3\ni,j\u2208Srij \u2212 X1X1T\nwhere rij \u2208 R is the rating of the i-th product by the j-th\nuser for i = 1, ..., m and j = 1, ..., n; X denotes the user and\nitem representations (embeddings), and the superscript 1\ndenotes the embedding contains 1st order information. The\ndot product of these embedding vectors yields a meaningful\nprediction of the subsequent rating for 2nd order interac-\ntions. In Wukong, this meaningful interactions are then\ntransformed to 2nd order interaction representations X2 us-\ning MLP. In the 2nd layer FMB, with a residual and LCB\nconnection, a dot product of (X1 + X2)(X1 + X2)T yield\nboth meaningful interaction from 1st order to 4th order. By\nanalogy, a l-layer Wukong solves a problem by minimizing:\nmin \u03a3\ni,j\u2208S(rij \u2212\n\u03a3\nk\u22081,2,...,2l\u22121XkXkT )\nThus, comparing to the traditional factorization approach,\nWukong is able to solve the recommendation problem with\na more sufficient interaction orders.\n11\nWukong: Towards a Scaling Law for Large-Scale Recommendation\nHyperparameters\nGFLOP/example #Params Relative LogLoss Relative LogLoss\n(Task1)\n(Task2)\nAFN+\nDNN=4x2048, afn=4x2048, nlog=1024\n4.41\n628.22\n0.11\n0.05\nDNN=4x4096, afn=4x2048, nlog=1024\n7.65\n628.74\n0.12\n0.06\nDNN=4x4096, afn=4x4096, nlog=2048\n13.08\n629.46\n0.21\n0.14\nDNN=4x8192, afn=4x8192, nlog=4096\n43.4\n633.95\n0.12\n0.06\nAutoInt+\nAttention=3x256, nhead=4, DNN=2x256\n7.72\n627.73\n0.39\n0.24\nAttention=3x512, nhead=4, DNN=2x256\n18.58\n627.77\n0.15\n0.05\nAttention=3x512, nhead=8, DNN=3x8192\n42.53\n631.49\n-0.09\n-0.16\nAttention=3x512, nhead=16, DNN=3x10240\n49.58\n632.59\n-0.1\n-0.2\nAttention=3x512, nhead=16, DNN=3x16384\n68.83\n635.57\n0.13 (LossX)\n0.01 (LossX)\nDCN\nl=2, rank=512, MLP=4x2048\n3\n628.11\n-0.27\n-0.27\nl=2, rank=512, MLP=4x4096\n4.67\n628.37\n-0.29\n-0.32\nl=2, rank=512, MLP=4x16384\n17.85\n630.42\n-0.38\n-0.41\nl=2, rank=512, MLP=4x32768\n43.88\n634.46\n-0.43\n-0.45\nl=2, rank=512, MLP=4x51200\n84.71\n640.79\n(LossX)\n(LossX)\nDLRM\nTopMLP=2x512\n1.37\n627.78\n(Baseline)\n(Baseline)\nTopMLP=4x512\n1.37\n627.78\n-0.11\n-0.08\nTopMLP=4x2048\n3.85\n628.17\n-0.23\n-0.21\nTopMLP=4x4096\n7.29\n628.7\n-0.28\n-0.27\nTopMLP=4x8192\n14.61\n629.84\n-0.32\n-0.31\nTopMLP=4x16384\n31\n632.39\n-0.37\n-0.35\nTopMLP=4x32768\n71.23\n638.62\n-0.36\n-0.34\nFinalMLP\nMLP1=4x4096, MLP2=2x1024, output_dim=64,\n3.93\n628.25\n-0.11\n-0.16\nno_fs\nMLP1=4x4096, MLP2=2x1024, output_dim=64,\n8.17\n628.91\n-0.23\n-0.27\nfs1=[0,57600], fs2=[57600,115200], fs_MLP=1x2048\nMLP1=4x8192, MLP2=2x2048, output_dim=64,\n16.9\n630.27\n-0.34\n-0.36\nfs1=[0,57600], fs2=[57600,115200], fs_MLP=1x4096\nMLP1=8x8192, MLP2=4x2048, output_dim=64,\n18.77\n630.56\n-0.37\n-0.38\nfs1=[0,57600], fs2=[57600,115200], fs_MLP=2x4096,\nMLP1=4x16384, MLP2=2x4096, output_dim=64,\n36.26\n633.27\n-0.34\n-0.34\nfs1=[0,57600], fs2=[57600,115200], fs_MLP=1x8192\nMLP1=4x24576, MLP2=2x6144, output_dim=64,\n58.12\n636.67\n-0.37\n-0.38\nfs1=[0,57600], fs2=[57600,115200], fs_MLP=1x12288\nMaskNet\nMLP=1x512, nblock=1, dim=128, reduction=0.01\n1.76\n627.92\n-0.09\n-0.12\nMLP=1x512, nblock=4, dim=128, reduction=0.01\n6.8\n628.7\n-0.22\n-0.25\nMLP=3x2048, nblock=4, dim=128, reduction=0.01\n6.88\n628.71\n-0.28\n-0.3\nMLP=3x2048, nblock=4, dim=128, reduction=0.05\n32.36\n632.67\n-0.37\n-0.37\nMLP=3x2048, nblock=4, dim=128, reduction=0.1\n64.21\n637.61\n-0.4\n-0.4\nWukong\nl=2, nL=8, nF=8, k=24, MLP=3x2048\n0.53\n627.74\n-0.35\n-0.32\nl=4, nL=32, nF=32, k=24, MLP=3x2048\n1.25\n627.82\n-0.45\n-0.43\nl=8, nL=32, nF=32, k=24, MLP=3x2048\n2.12\n627.95\n-0.53\n-0.49\nl=8, nL=48, nF=48, k=48, MLP=3x4096\n5.6\n628.46\n-0.6\n-0.6\nl=8, nL=96, nF=96, k=96, MLP=3x8192\n22.23\n630.96\n-0.67\n-0.66\nl=8, nL=96, nF=96, k=96, MLP=3x16384\n61\n636.99\n-0.7\n-0.69\nl=8, nL=192, nF=192, k=192, MLP=3x16384\n108\n644\n-0.76\n-0.76\nTable 3: Detailed hyperparameters, compute complexity, model quality and model size for each run evaluated in Sec. 7. LossX\nmeans loss exploded during training.\n12\n"
  },
  {
    "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
    "link": "https://arxiv.org/pdf/2403.03194.pdf",
    "upvote": "10",
    "text": "MAGID: An Automated Pipeline for Generating Synthetic\nMulti-modal Datasets\nHossein Aboutalebi\n\u2217, v\nHwanjun Song\u00c0\nYusheng Xie\u00c0\nArshit Gupta\u00c0\nJustin Sun\u00c0\nHang Su\u00c0\nIgor Shalyminov\u00c0\nNikolaos Pappas\u00c0\nSiffi Singh\u00c0\nSaab Mansour\u00c0\nv Cheriton School of Computer Science, University of Waterloo\n\u00c0 AWS AI Labs\nhaboutal@uwaterloo.ca\nAbstract\nDevelopment\nof\nmultimodal\ninteractive\nsystems is hindered by the lack of rich,\nmultimodal (text, images) conversational\ndata, which is needed in large quanti-\nties for LLMs. Previous approaches aug-\nment textual dialogues with retrieved im-\nages, posing privacy, diversity, and qual-\nity constraints.\nIn this work, we intro-\nduce Multimodal Augmented Generative\nImages Dialogues (MAGID), a framework\nto augment text-only dialogues with di-\nverse and high-quality images 1.\nSubse-\nquently, a diffusion model is applied to\ncraft corresponding images, ensuring align-\nment with the identified text.\nFinally,\nMAGID incorporates an innovative feed-\nback loop between an image description\ngeneration module (textual LLM) and im-\nage quality modules (addressing aesthet-\nics, image-text matching, and safety), that\nwork in tandem to generate high-quality\nand multi-modal dialogues.\nWe compare\nMAGID to other SOTA baselines on three\ndialogue datasets, using automated and\nhuman evaluation. Our results show that\nMAGID is comparable to or better than\nbaselines, with significant improvements in\nhuman evaluation, especially against re-\ntrieval baselines where the image database\nis small.\n1\nIntroduction\nIn recent years, advancements in large lan-\nguage models (LLMs) have expanded possibil-\nities and research directions in AI, with stud-\nies highlighting their extensive capabilities in\nhandling dialogue datasets (Liu et al., 2023c;\nPenedo et al., 2023).\nSpecifically, there is a\ngrowing interest in their application to multi-\nmodal dialogue datasets, given that sharing\nimages is an integral aspect of human-human\n\u2217 Work conducted while interning at AWS AI Labs.\n1The link to code: http://anon for review.com\nconversations (Alayrac et al., 2022; OpenAI,\n2023; Liu et al., 2023a).\nSeveral multi-modal dialogue datasets like\nMMDialog (Feng et al., 2022), DialogCC (Lee\net al., 2022)2, and PhotoChat (Zang et al.,\n2021) have been introduced for training multi-\nmodal LLMs.\nThese datasets either use a\nretrieval-based approach, pulling images from\nset image banks, such as MS-COCO (Lin\net al., 2014), or restrict the dialogue to only\none image per conversation, even if they in-\nvolve real human-human chats.\nMoreover,\nwhen leveraging real-world datasets from plat-\nforms like social media, issues related to pri-\nvacy concerns and image quality become sig-\nnificant challenges for training.\nAs a result, these methods limit the diver-\nsity of images since the small image database\ncannot adequately capture the wide range of\nreal human-human conversations (Lee et al.,\n2021, 2022). Additionally, they face challenges\nstemming from low-quality images contain-\ning harmful and private content (Feng et al.,\n2022) and shortage of accessible data (Lee\net al., 2022), particularly when utilizing real\nhuman-human conversations from social me-\ndia sources.\nTo address these challenges, we propose\nMAGID, a generative-based multi-modal di-\nalogue creation framework. As illustrated in\nFigure 1, MAGID aims at converting exist-\ning text-only data into context-enriched multi-\nmodal data by addressing the two research\nchallenges: (i) how to find the most suitable\nutterances that can be enhanced by adding\nimages and (ii) how to generate realistic and\ndiverse images that do not have harmful and\nprivate contents.\n2A recently released version of DialogCC utilizes\nLLM (Lee et al., 2023).\nAt the time of writing this\npaper, we did not have access to the newer version.\narXiv:2403.03194v1  [cs.CL]  5 Mar 2024\nYou left your Halloween \ncostume at my place\nHey, did I leave anything \nat your house?\nI was worried it was lost \nforever.\nLLM-based Scanner\nDiffusion-based Image \nGenerator\nQuality Assurance Module\nSelected\nImage of a 6-foot tall \npink men\u2019s bunny outfit.\nImage Description by LLM\nPrompt \nto Diffusion\nImage Quality Score\nAesthetic Score\nContent Safety Score\nGood?\nYes\nNo (Feedback Loop)\nText-only Dialogue\nMulti-modal Dialogue\nYou left your Halloween \ncostume at my place\nHey, did I leave anything \nat your house?\nI was worried it was lost \nforever.\nFigure 1: Overview of the MAGID framework. MAGID consists of three components: (1) LLM-based\nscanner to identify suitable utterances to augment with images, (2) diffusion-based image generator to\ncreate realistic images, and (3) quality assurance module to enhance the image quality, aesthetic and\nsafety scores. The text-only dialogue is automatically converted to multi-modal dialogue using MAGID.\nIn the former case, we introduce an LLM-\nbased scanner designed to pinpoint utterances\nrequiring images and subsequently generate\ncorresponding image descriptions, leveraging\nchain-of-thought prompting.\nIn the latter\ncase, we employ a diffusion-based image gen-\nerator, adept at crafting images with notable\ndiversity, drawing upon the generated image\ndescriptions as its input. Additionally, a qual-\nity assurance module is incorporated into our\nframework to ensure both the congruence and\nthe quality of the produced images, thereby\npreserving coherence and fidelity within the\nmulti-modal dialogue.\nShould the generated\nimage not satisfy the criteria of this module,\nMAGID initiates a feedback loop, revisiting\nthe processes of prompt and image generation.\nDistinct from numerous previous endeav-\nors that have depended on image-retrieval\ntechniques for curating multi-modal datasets\n(Lee et al., 2021, 2022)\u2014a method that\nmight result in restricted image diver-\nsity and potential mismatch with the di-\nalogue existing utterances\u2014we employ the\ngenerative model Stable Diffusion XL (Podell\net al., 2023).\nBy training on billions of im-\nages (Schuhmann et al., 2022), this approach\nguarantees an output that is both rich and var-\nied. Such outputs align well with the conversa-\ntional context provided by the LLM feedback,\nthereby elevating the quality and diversity of\nour multi-modal dataset.\nOur framework aligns with prior studies\nusing text-only datasets (Lee et al., 2021,\n2022), but it addresses the limitations asso-\nciated with their retrieval-based strategies by\nemploying a generative-based data creation\nmethod. Unlike Liu et al. (2023a); Lee et al.\n(2021), we do not restrict the inclusion of\nonly one image per dialogue.\nConsequently,\nMAGID generates synthetic yet more realistic\nmulti-modal dialogue datasets thus mitigating\ndata accessibility issues and facilitating the de-\nvelopment of advanced multi-modal models.\nTo summarize, our main contributions are:\n\u2022 We present MAGID, a generative-based\nmulti-modal dialogue data creation frame-\nwork\nthat\naddresses\nthe\nlimitation\nof\nretrieval-based approaches.\n\u2022 We\nconduct\nexperiments\nusing\nvarious\nprompt engineering strategies to optimize\ninteractions between the LLM-based scan-\nner and the diffusion-based image genera-\ntor.\n\u2022 We propose a novel quality assurance de-\nsign to control the performance of genera-\ntive models effectively.\n\u2022 We provide a medium-sized dataset as a\nproof of concept to showcase the effective-\nness of MAGID pipeline (section 5).\n\u2022 We conduct extensive human evaluations on\nthe dataset and test multiple LLM models\nto ensure robustness and reliability.\n2\nRelated Works\n2.1\nGenerative Models\nRecent advances in Generative AI has started\nnew trends in expanding capabilities of exist-\ning deep learning models. In NLP, works like\n(Radford et al., 2019; Ouyang et al., 2022)\nhave shown importance of training data to\nbuild better LLM models.\nIn this regard,\nrecent LLM models like Falcon-40b-Instruct\n(Penedo et al., 2023), Koala 13b (Geng et al.,\n2023), LLaMA 13b (Touvron et al., 2023),\nZero shot prompt\nYou are an AI assistant that helps augment textual dialogues with engaging images. As input, you will receive a conversation between\npeople which is represented as a sequence of utterances. As output, you will generate a description of images that can support the\nutterances in the conversation.\nThe format of the input is \u2019Utterance i: ...\u2019 where \u2019i\u2019 denotes the order of the Utterance in the conversation. Given this query, you\noutput in the format of\n<result>Utterance i: image description</result> <reason>explanation of choice </reason>\nwhere \u2019i\u2019 is the Utterance in the conversation and \u2019image description\u2019 is the short text description of an image that can be followed\nby that Utterance that can make the conversation more engaging. You should only identify the most appropriate utterances in the\nconversation.\nThe text inside <reason>explanation of choice</reason> is the explanation of why you picked the utterance with the image de-\nscription.\nFigure 2: The zero-shot prompt of the scanner module (Section 3.1) which selects turns in the dialogue\nto augment with images and generates descriptions of those images. Additional few-shot and chain-of-\nthought prompts are provided in the supplementary materials (section A).\nOpenLLaMA (Touvron et al., 2023), and Vi-\ncuna 13b (Chiang et al., 2023) use better cu-\nrated training datasets to achieve higher per-\nformances.\nIn this regard, paper like Chris-\ntiano et al. (2017) has shown the dramatic im-\npact of using higher quality data (from hu-\nman feedback) in faster training. Yet, using\nhuman feedback and crowd-sourcing is not al-\nways cheap. To address this, emerging works\nlike (Veselovsky et al., 2023; Kamalloo et al.,\n2023) suggests that LLM has the capabilities\nof performing the task of human generated\ndataset. In addition, diffusion models in com-\nputer vision have shown promising results in\ngenerating images indistinguishable from real\nones (Podell et al., 2023; Ho et al., 2020). Fi-\nnally, recent works focus on building multi-\nmodal LLM models including GPT-4 (Ope-\nnAI, 2023), LLaVA (Liu et al., 2023b), Any-\nMAL(Moon et al., 2023) which supports any\nmodality. Specifically, LLaVA accepts multi-\nmodal input, combining image and text em-\nbeddings to generate text-only output.\n2.2\nMulti-modal Dataset Creation\nThere are also works which focus on gener-\nating multi-modality datasets. In particular,\nMMDD (Lee et al., 2021) and DialogCC (Lee\net al., 2022) use image-retrieval approaches\nto augment text-only datasets to multi-modal\ndatasets. PhotoChat (Zang et al., 2021) hires\nworkers to discuss a particular image to build\nthe dataset.\nMMDialog (Feng et al., 2022)\ncollect multi-modal conversations from inter-\nnet to build the dataset which can poten-\ntially pose privacy concern to use as train-\ning set.\nThere are also works (Wang et al.,\n2023; Corona et al., 2021, 2020; Ciliberto et al.,\n2021; Abdrakhmanova et al., 2021) which fo-\ncuses modality beyond text and image includ-\ning video and voice.\nFor example, Corona\net al. (2021) provide a dataset that contains\nvideos for activity detection. IntenVid (Wang\net al., 2023) is another example that contains\nvideo in addition to text.\n3\nMAGID Pipeline\nIn transitioning from text-only to multi-modal\ndialogue, there exist two core challenges. The\nfirst is the identification of the most suitable\nutterances within the dialogue that can be en-\nhanced by images. The second is the creation\nof corresponding, accurate images that align\nwith the selected utterances. In this regard,\nwe need to ensure a harmonious and coherent\nmatch between the image and the text, achiev-\ning acceptable image-text alignment.\nWe have addressed these challenges through\nthe implementation of the following three\nkey modules in Figure 1, namely LLM-based\nscanner, diffusion-based image generator, and\nquality assurance module, which are detailed\nin the subsequent sections.\n3.1\nMAGID Scanner\nThe primary objective of this module is to\nidentify suitable utterances that can be visu-\nally represented by an image. Achieving best\nperformance requires precise control over the\nbehavior of the LLM model. We use prompt\nengineering and special formatting to control\nthe output of LLM.\nWe experimented with three prompt en-\ngineering strategies to fine-tune the system\nprompts of the LLM:\n\u2022 Zero-shot prompting: The LLM is pro-\nvided with only the format of the input and\nthe expected output, along with a general\nproblem description. Figure 2 shows an ex-\nample of the zero-shot prompt.\nWhat did you cook this Diwali? Share those recipes at \nthe link below and stand a chance to win a gift card \nworth INR 500 from us! Submit your entries here: \nDone team. Wish to win\nHey!! Please give it a look once\n<reason> \nUtterance 0 is asking for recipes and offering a gift card as a prize. An image of \nthe gift card can be used to attract more participants. \nUtterance 1 does not require an image as it is a response to the previous \nutterance. \nUtterance 2 is asking someone to look at something, so an image of what they \nwant the other person to look at would be appropriate. \n</reason>\n<result>\nUtterance: 0: An image of a gift card worth INR 500 \nUtterance: 2: An image of the thing the speaker wants the other person to look at\n</result>\nScanner (GPT-4 Answer):\nUtterance: 0\nUtterance: 2\nUtterance: 1\nFigure 3: MAGID\u2019s chain of thought prompting facilitates debugging and identification of corner cases,\nutilizing the SDXL 1.0 diffusion model and GPT-4 (OpenAI, 2023). The depicted conversation is sourced\nfrom a real human-human interaction in the MMDialog dataset (Feng et al., 2022).\n\u2022 Few-shot\nexample\nprompting:\nBe-\nsides the information provided in zero-\nshot prompting, LLM is also supplied with\nseveral input\u2013output exemplars to demon-\nstrate the anticipated response from the\nLLM model (Brown et al., 2020). We have\nincluded this type of prompt in supplemen-\ntary materials (section A).\n\u2022 Chain of Thought prompting: As per\n(Wei et al., 2022), this prompting strat-\negy involves imparting a series of intermedi-\nate reasoning steps for each example, facil-\nitating the LLM model\u2019s capacity for more\nadvanced reasoning.\nPlease refer to sup-\nplementary materials for example of this\nprompt (section A).\nIn section 4.3.1, we evaluated these prompt-\ning strategies. Based on the findings, we se-\nlected Chain of Thought prompting as the op-\ntimal choice for our MAGID framework.\n3.2\nControlling LLM Output Format\nWe introduce a method that seeks to stream-\nline the structuring of LLMs outputs by\nemploying HTML-like tags, aiming to facil-\nitate easier parsing and to shed light on\nthe decision-making process.\nThe utiliza-\ntion of < result > and < reason > tags is in-\ntended to envelope answers and rationales re-\nspectively, potentially making post-processing\nmore straightforward and offering a degree\nof transparency into the model\u2019s reasoning,\nwhich may be beneficial for debugging pur-\nposes.\nFigure 3 demonstrates the impact of using\nthe proposed HTML formatting inside chain\nof thought prompt, revealing how meticulous\nanalysis of responses identifies corner cases\nand ensures contextual congruency in pro-\nduced images. Whereas the first image aligns\nwith preceding text, the second lacks context.\nThe < reason > tag discloses that phrases like\n\u201dgive it a look\u201d influenced image generation.\nTo enhance contextual relevance and model\nreliability, the system prompt has been re-\nfined to instruct the LLM to only generate im-\nages when paired with a detailed description,\nthereby avoiding contextual discrepancies.\n3.3\nMAGID Image Generator\nAs illustrated in Figure 1, the LLM model\u2019s\nimage prompts are used by the diffusion model\nto generate corresponding images. In this re-\ngard, given the success of diffusion models in\nsuperior image generation (Rombach et al.,\n2022; Ho et al., 2020), were chosen over GANs\n(Goodfellow et al., 2014). Models tested in-\ncluded SDXl 1.0, SDXL 0.9, and Stable Diffu-\nsion versions from Stability AI (Podell et al.,\n2023), with a detailed comparison in supple-\nmentary materials (section C).\nUltimately, SDXl 1.0 was chosen for its\nstate-of-the-art\ncapabilities,\nbolstering\nthe\nquality and reliability of the generated images\nof the MAGID dataset. Nevertheless, future\nmodel developments can be incorporated to re-\nfine our MAGID dataset generation.\nPhiladelphia roll is a solid choice! I'm a sucker \nfor anything with spicy tuna and avocad\nDragon rolls are really good too. \nYes and spicy tuna!\nI'm considering ordering some sushi takeout \nHow does this look to you?  Whoops\nI had a thing that was sushi inside \nof an avocado. It was amazing.\nPhiladelphia roll is a solid choice! I'm a sucker \nfor anything with spicy tuna and avocad\nDragon rolls are really good too. \nYes and spicy tuna!\nI'm considering ordering some sushi takeout \nHow does this look to you?  Whoops\nI had a thing that was sushi inside \nof an avocado. It was amazing.\nOh wow\nOh wow\ni am about to take my dog frisket \nout for a walk . \nthat sounds fun i enjoy walks ! \nme too my favorite walks are in the \npark enjoying nature . \ni just opened my own custom cake shop .\noh cool ! mine are walking down the \nstreet to my bestfriends house .\nthat sounds so awesome ! i \njust bought a honda civic .\ni love the honda civic but i like the cr v \nmore . that trunk space .\nll . i like those old dependable cars ! \ni am about to take my dog frisket \nout for a walk . \nthat sounds fun i enjoy walks ! \nme too my favorite walks are in the \npark enjoying nature . \ni just opened my own custom cake shop .\noh cool ! mine are walking down the \nstreet to my bestfriends house .\nthat sounds so awesome ! i \njust bought a honda civic .\ni love the honda civic but i like the cr v \nmore . that trunk space .\nll . i like those old dependable cars ! \n(a) MAGID (left) vs. MMDD (right).\n(b) MAGID (left) vs. PhotoChat (right).\nFigure 4: Qualitative comparison of MAGID with an image retrieval-based synthetic MMDD and a real\nhuman image-based PhotoChat datasets.\n3.4\nMAGID Quality Assurance\nThe Quality Assurance (QA) module is essen-\ntial for improving the MAGID pipeline\u2019s effi-\nciency. It assures the generated images satisfy\nuser-set standards in three domains: Image-\nText Matching, Image Quality, and Im-\nage Safety.\n1- Image-text Matching:\nWe use the\nCLIP score (Radford et al., 2021) to validate\nthe match between the image and the LLM\nmodel\u2019s utterance. A low CLIP score triggers\nimage regeneration, with the count determined\nas a hyperparameter. In this work, we set the\nregeneration count to two.\n2- Image Quality: Images are rated based\non an aesthetic score from (Schuhmann et al.,\n2022; Schuhmann, 2023), which uses CLIP\nembedding followed by an MLP. This model\nidentifies artifacts in the diffusion model out-\nputs.\nA threshold of 0.51 efficiently detects\nmost artifacts, prompting image regeneration\nfor scores below this.\n3- Image Safety: Image safety, particu-\nlarly against NSFW content, is crucial. While\nmany models assess this, few unsafe images\nwere found in our dataset, indicating our pro-\ncess\u2019s reliability.\nThis robust QA ensures that MAGID can\noutput relevant, high-quality, and safe images.\n3.4.1\nFeedback Loop\nShould the diffusion model produce an image\nthat does not meet the quality assurance mod-\nule\u2019s stipulations, the issues might stem from\nthe LLM model\u2019s prompt. Faulty prompts can\nyield low image-text matches or unsafe im-\nages. To mitigate this, our design, showcased\nin Figure 1, includes a feedback loop, instruct-\ning the LLM model to generate a better image\ndescription given regenerated images with pre-\nvious image description continuously fall short\nof quality assurance standards.\nFigure 4 displays a comparison of MAGID\nsamples with two other datasets, MMDD (Lee\net al., 2021) and PhotoChat (Zang et al.,\n2021). A qualitative analysis shows MAGID\nyields quality comparable to real datasets,\nsuch as PhotoChat, and surpasses synthetic\ndatasets like MMDD in generating high-\nquality multi-modal dataset. More examples\nare included in supplementary (section H).\n4\nEvaluation\nWe scrutinize the efficacy and applicabil-\nity of the multi-modal dataset generated by\nMAGID. Here are three pivotal questions we\naddressed in evaluation:\n1. How does MAGID quantitatively compare\nagainst real multi-modal datasets? \u25b7 Sec-\ntion 4.1\n2. Can MAGID create a multi-modal dataset\nwith human-eye perceptible quality like a\nreal one? \u25b7 Section 4.2\n3. What is the impact of scanner prompt tun-\ning and the quality assurance module on\nMAGID? \u25b7 Section 4.3\nThe first and third question delves into a\nquantitative analysis, probing the accuracy\nand quality of the data generated by MAGID.\nMoreover, the second question is crucial, as\na failure of MAGID to meet human evalua-\ntion standards would result in a low-quality\ntraining dataset that is unable to get positive\nhuman-centric assessments.\nIn addition, in supplementary (section E),\nwe have studied training multimodal model\nTable 1: Scanner module performance as measured by turn selection for image augmentation (accuracy,\nprecision, recall, F1) and the resulting images from the generated descriptions (CLIP, MM-relevance,\naesthetic) on the MMDialog dataset as ground-truth. The quality assurance module is enabled. We\ncompare various LLMs powering the scanner module using chain of thought prompting.\nModel\nAccuracy Precision\nRecall\nF1 score\nCLIP score\nMM-Relevance\nAesthetic #images\nGPT 4\n67.24%\n70.49%\n46.87%\n0.56\n0.27\n294.52\n0.57\n1359\nGPT 3.5\n63.54%\n69.43%\n33.97%\n0.46\n0.26\n293.51\n0.58\n1001\nFalcon-40b-Ins.\n58.93%\n61.26%\n24.13%\n0.35\n0.25\n254.50\n0.58\n794\nKoala 13b\n56.28%\n62.33%\n6.91%\n0.12\n0.25\n243.31\n0.57\n223\nLlama 13b\n57.10%\n60.00%\n13.64%\n0.22\n0.25\n247.99\n0.57\n460\nOpenLLaMA\n57.94%\n64.36%\n12.69%\n0.21\n0.25\n250.96\n0.58\n390\nVicuna 13b\n58.77%\n66.60%\n14.38%\n0.24\n0.26\n255.18\n0.57\n506\nMMDialogue3\nN/A\nN/A\nN/A\nN/A\n0.262\nN/A\n0.47\n2717\nwith MAGID and compared it with using real\nimages for training.\n4.1\nQuantitative Evaluation\nSetup.\nAddressing\nthe\nfirst\nquestion,\na\nmulti-dimensional evaluation assessed the im-\nage quality and accuracy of MAGID in se-\nlecting right utterances.\nTo fairly compare\nMAGID\u2019s general-use applicability, we only\nutilized prompt engineering to guide the LLM\nmodel to select the right utterances. In this\nregard, as a ground truth, we selected human-\nhuman interaction datasets MMDialog and\nPhotoChat, and removed images from their\ntest sets and employed MAGID to transform\nthe text-only data into a multi-modal dataset.\nFor the LLM-based model, we adopted a\nrange of models, including GPT-4 (OpenAI,\n2023), GPT-3.5 (OpenAI, 2023), Falcon-40b-\nInstruct (Penedo et al., 2023), Koala 13b\n(Geng et al., 2023), LLaMA 13b (Touvron\net al., 2023), OpenLLaMA (Touvron et al.,\n2023), and Vicuna 13b (Chiang et al., 2023).\nFor image generation, SDXL 1.0 was consis-\ntently utilized across all models. We present\nthe results of the MMDialog dataset here, and\nthe PhotoChat results are included in supple-\nmentary (section B). In these experiments, we\nhave set the threshold for the CLIP model\nat 0.21 and the aesthetic score threshold of\n0.51. We used grid search to find these hyper-\nparameters.\nMore details on computational\ncost is provided in supplementary (section F).\nResult.\nTable 1 presents the performance\nof various LLM models on the MMDialog\ndataset.\nThe table quantifies MAGID\u2019s re-\nsponse generation using different LLM models\nin comparison to the MMDialog dataset. The\nfirst column lists the LLM models used, while\nthe subsequent four columns measure accu-\nracy, precision, recall, and F1 score in choosing\nthe correct utterance to be augmented with\nan image. The CLIP score gauges image-text\nmatching, and the MM-Relevance, as intro-\nduced in (Feng et al., 2022), denotes the sim-\nilarity between responses. In our context, it\ndetermines the resemblance of the produced\nimage to the MMDialog\u2019s original image. The\nnext column, the aesthetic score, indicates the\nimage quality as discussed in (Schuhmann,\n2023).\nLast row presents the ground truth\ndataset, highlighting the CLIP score, image\ncount, and aesthetic quality of its images.\nFrom the table, it is evident that GPT-4\nand GPT-3.5 outperforms other models across\nall metrics. Notably, the CLIP and aesthetic\nscores of MAGID using GPT-4 and GPT-3.5\nsurpass even the ground truth values.\nIn\nthe next section, we also examine image-text\nmatching and image quality in our human\nevaluation for MAGI against other datasets to\ntest if it is aligned with our quantitative find-\nings.\n4.2\nHuman Evaluation\nSetup.\nWe conducted a human evaluation\nusing a website with questionnaire.\nPartici-\npants viewed two dialogues: one with an im-\nage from MAGID and another from datasets\nMMDD (Lee et al., 2021), PhotoChat (Zang\net al., 2021), or MMDialog (Feng et al., 2022).\nMAGID used GPT-4 as its Language Model\nand SDXL 1.0 for image generation. From the\nmentioned datasets, we selected 20 dialogues\neach, totaling 60 dialogues, and replaced their\nimages with MAGID\u2019s.\nDuring evaluation,\nparticipants compared MAGID\u2019s multi-modal\ndialogues with the originals, without informa-\nTable 2: Human Evaluation results of MAGID created datasets versus a retrieval-based synthetic dataset,\nMMDD, and two real datasets, MMDialouge and PhotoChat, where the mean shows the percentage of\ntime the dialogues in one dataset were preferred among participants. (Q1: more realistic dialogue? Q2:\nimages in which dialogue provide more knowledge?, Q3: better text-image matched?, Q4: better context-\nimage matched?, Q5: more engaging?, Q6: hegher image quality?)\n(a) MAGID vs. MMDD\n(b) MAGID vs. MMDialogue\n(c) MAGID vs. PhotoChat\n#\nMean\nMAGID\nMean\nMMDD\nGwet\u2019s\nAC1\nMean\nMAGID\nMean\nMMDial.\nGwet\u2019s\nAC1\nMean\nMAGID\nMean\nPhoto.\nGwet\u2019s\nAC1\nQ1\n96.29%\n3.71%\n0.74\n48.17%\n51.83%\n0.63\n58.11%\n41.89%\n0.47\nQ2\n96.29%\n3.71%\n0.89\n49.33%\n50.67%\n0.65\n68.24%\n31.76%\n0.71\nQ3\n89.11%\n10.89%\n0.75\n52.72%\n47.28%\n0.54\n64.90%\n35.10%\n0.53\nQ4\n91.11%\n8.89%\n0.83\n46.31%\n53.69%\n0.65\n61.98%\n38.02%\n0.54\nQ5\n95.57%\n4.43%\n0.89\n51.94%\n48.06%\n0.63\n64.02%\n35.98%\n0.61\nQ6\n80.92%\n19.08%\n0.65\n63.90%\n36.10%\n0.55\n69.99%\n30.01%\n0.64\nTable 3: Utterance selection accuracy using three\ndifferent prompts on MMDialogue (ground-truth),\nwhere ZS, FS, and CoT stand for zero-shot, few-\nshot, and chain of thought respectively.\nPrompt\nAccuracy\nPrecision\nRecall\nF1\nscore\nZS\n65.53%\n73.12%\n36.16%\n0.48\nFS\n63.89%\n69.67%\n34.45%\n0.46\nCoT\n68.51%\n73.37%\n47.32%\n0.57\ntion about the dialogue origins.\nFor each dialogue pair (one from MAGID\nand one from the benchmark datasets), par-\nticipants responded to the following prompts:\nQ1: Which dialogue appears more realistic?\nQ2: Which dialogue\u2019s images convey greater\nknowledge?\nQ3: In which dialogue is there better match\nbetween images and the immediately pre-\nceding text?\nQ4: In which dialogue do the images more\nclosely match with the overall conversa-\ntion context?\nQ5: Which dialogue is more engaging?\nQ6: Which dialogue features higher quality\nimages?\nRespondents selected from binary choices (Di-\nalogue A or Dialogue B) for each prompt.\nFor this evaluation, 15 human annotators\nprovided their answers. Schema of the website\ninterface are available in the Supplementary\nmaterials (section D).\nResult.\nTable 2 displays MAGID\u2019s results\nagainst MMDD, MMDialog, and PhotoChat\ndatasets. The \u2018Mean MAGID\u2019 column shows\nthe percentage of annotators favoring MAGID,\nwhile \u2018Mean Other\u2019 indicates those preferring\nthe alternative dataset. Gwet\u2019s AC1 measure,\nfound in the last column, was used to as-\nsess inter-annotator reliability. It offers stabil-\nity over Cohen\u2019s Kappa (Wongpakaran et al.,\n2013) and is more resilient to outliers (For\nmore explanation, please refer to Supplemen-\ntary Materials section G.).\nFrom Table 2(a), it\u2019s evident that anno-\ntators favored MAGID over the synthetically\ngenerated MMDD dataset across all question\ncategories.\nMoreover, the high Gwet\u2019s AC1\nvalue indicates a strong consensus among an-\nnotators in choosing MAGID over MMDD. In\ncontrast, when examining Table 2(b), annota-\ntors exhibited a slight preference for the au-\nthentic MMDialog dataset in terms of realism.\nNotably, the Gwet\u2019s AC1 value is considerably\nlower here than in the MMDD results, suggest-\ning a reduced consensus among annotators.\nNevertheless, MAGID outperformed MMDi-\nalog in terms of image quality and image-\ntext matching.\nSuch findings affirm our\nquantitative evaluations and showcase\nthe potential of generative AI in produc-\ning superior data sources for training.\nAs for the PhotoChat dataset (Table 2(c)),\nwhile it is constructed from authentic human\ninteractions, human participants were told to\nmock real conversation. Interestingly, our an-\nnotators slightly leaned towards MAGID over\nPhotoChat. This outcome suggests MAGID\u2019s\npromising capability to serve as an alterna-\ntive to Mechanical Turk in the development\nof multi-modal datasets.\n4.3\nAblation Study of MAGID\nWe conducted ablation studies on (1) using\ndifferent prompts for utterance identification\nTable 4: Ablation results of the MAGID framework with and without the quality assurance (QA) module.\nResults on turn selection and image quality performance across four LLMs on MMDialog (ground-truth)\nare shown. The first four rows are the results with the QA module, while the last four are the results\nwithout. The system prompt is chain of thought.\nModel\nAccuracy Precision\nRecall\nF1 score\nCLIP score\nMM-Relevance\nAesthetic #images\nGPT 4\n67.24%\n70.49%\n46.87%\n0.56\n0.27\n294.52\n0.57\n1359\nGPT 3.5\n63.54%\n69.43%\n33.97%\n0.46\n0.26\n293.51\n0.58\n1001\nFalcon-40b-Ins.\n58.93%\n61.26%\n24.13%\n0.35\n0.25\n254.50\n0.58\n794\nOpenLLaMA\n57.94%\n64.36%\n12.69%\n0.21\n0.25\n250.96\n0.58\n390\nGPT 4\n67.86%\n69.70%\n50.64%\n0.59\n0.27\n282.25\n0.55\n1485\nGPT 3.5\n68.51%\n73.37%\n47.32%\n0.57\n0.26\n278.16\n0.55\n1109\nFalcon-40b-Ins.\n56.77%\n53.58%\n28.80%\n0.37\n0.23\n224.59\n0.55\n1075\nOpenLLaMA\n58.92%\n62.50%\n21.51%\n0.32\n0.21\n213.56\n0.56\n696\nand (2) investigating the impact of our quality\nassurance (QA) module.\n4.3.1\nPrompts for Scanner\nTable 3 displays the outcomes of three prompt\nstrategies, namely Zero-shot (ZS) prompt-\ning, Few-shot prompting (FS), and Chain of\nThought (CoT) prompting, as applied to the\nGPT-3.5 model for MAGID. These results are\nreported for the MMDialog dataset, with qual-\nity assurance deactivated, to solely measure\nthe accuracy of the LLM model. Notably, the\nChain of Thought strategy outperforms the\nother two across all evaluated metrics.\n4.3.2\nImpact of QA Module\nTable 4 showcases the performance of four\nLLM models in MAGID, contrasting when the\nQA module is either enabled or disabled. A\nperusal of Table 4 reveals a decline in the aes-\nthetic score, MM-Relevance, and CLIP score\nacross all models upon the deactivation of QA.\nMoreover, a noticeable decrement in the pre-\ncision of most models is observable, validating\nthat the QA module bolsters MAGID by en-\nhancing precision in pinpointing the optimal\nutterance for image generation. In contrast,\ndisabling QA leads to an elevation in recall,\nattributable to MAGID selecting a more ex-\ntensive array of utterances for image genera-\ntion, thereby reducing the ratio of false neg-\natives. Future research could explore the de-\nvelopment of a refined QA module capable of\nelevating the recall rate for the entire pipeline.\n5\nMAGID Dataset\nAs a proof of concept, and consistent with\nstudies like (Lee et al., 2021), we employed\ntext-only datasets such as DailyDialog (Li\net al., 2017), Persona-Chat (Zhang et al.,\n2018), and PhotoChat (Zang et al., 2021) (by\nTable 5: Statistics of the MAGID dataset.\nCategory\nTrain\nTest\nTotal dialogues\n47643\n5977\nAvg length of dialogues\n11.76\n11.36\nAvg length of sentences\n9.77\n9.60\nTotal images\n67951\n10229\nreplacing its images with MAGID) to generate\na multi-modal dataset 4 of 53,620 dialogues.\nBased on the results of our experiments, we\nused GPT-3.5 to transform 47,868 input dia-\nlogues and GPT-4 to augment the rest. Table\n5 shows the statistics of the generated dataset\nwith MAGID. The data and the code will be\nmade available to the public upon acceptance.\n6\nConclusion\nWe presented a generative, fully automated\npipeline\ndesigned\nto\ntransform\ntext-only\ndatasets into multi-modal variants, harness-\ning the power of LLMs through prompt en-\ngineering. This solution addresses limitations\nfaced by preceding methods, notably in terms\nof data privacy, accessibility, constrained im-\nage distribution, and occurrences of unsuit-\nable or non-consensual content.\nCrucially,\nour pipeline permits the substitution of real,\npotentially privacy-compromising images with\nsynthetic counterparts. We thoroughly evalu-\nated our multi-modal data generation method\nusing human assessment, quantitative analy-\nses with various LLMs, and an in-depth ab-\nlation study. The promising results highlight\ngenerative AI\u2019s capability to stand as an alter-\nnative to traditional data generation methods,\nlike mechanical turk.\nLooking ahead, our dataset paves the way\nfor developing large multi-modal language\nmodels that can engage with users via both\ntext and visuals.\n4The link to dataset: http://anon for review.com\nLimitations\nThis paper predominantly concentrates on\naugmenting the privacy, diversity, and quality\nof multi-modal dataset generation by employ-\ning LLM and diffusion models. Although uti-\nlizing generative diffusion models can mitigate\nissues related to privacy breaches\u2014given these\nmodels are also trained on extensive volumes\nof web images\u2014they are susceptible to copy-\nright infringement (Aboutalebi et al., 2023).\nAddressing this issue exceeds the ambit of this\npaper and presents a compelling avenue for fu-\nture work.\nMoreover, the current work exclusively em-\nphasizes image and text modalities.\nEx-\ntending considerations to additional modal-\nities\u2014such as video sharing, voice sharing,\nand more\u2014is recommended for subsequent re-\nsearch endeavors. In addition, fine-tunning of\nlarge language model to generate image is left\nto future works.\nImproving generated image consistency in\nthe dialogue is another important aspect that\ncan further improve the quality of the gen-\nerated multi-modal dataset by MAGID. Em-\nploying more recent diffusion models such as\nDALL-E 3 (Betker et al., 2023) can address\nthis problem as they can make more consis-\ntent image generation. In this regard, in the\nsection J of Supplementary materials, we have\nincluded further examples that shows the lim-\nitations of the proposed MAGID pipeline.\nIn conclusion, the enhancement of our qual-\nity assurance module is pivotal for developing\nmore realistic multi-modal datasets from text-\nonly inputs. In this regard, works like (Tian\net al., 2023) already showed that using syn-\nthesized images is effective. This work prior-\nitizes aspects like aesthetic score, clip score,\nand safety. Future research can explore addi-\ntional elements to further refine and add re-\nalism to the transformation into multi-modal\noutputs.\nReferences\nMadina Abdrakhmanova, Askat Kuzdeuov, Sheikh\nJarju, Yerbolat Khassanov, Michael Lewis, and\nHuseyin Atakan Varol. 2021.\nSpeakingfaces:\nA\nlarge-scale multimodal dataset of voice commands\nwith visual and thermal video streams.\nSensors,\n21(10):3465.\nHossein Aboutalebi,\nDaniel Mao,\nCarol Xu,\nand\nAlexander Wong. 2023. Deepfakeart challenge: A\nbenchmark dataset for generative ai art forgery\nand data poisoning detection.\narXiv preprint\narXiv:2306.01272.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural In-\nformation Processing Systems, 35:23716\u201323736.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks,\nJianfeng Wang,\nLinjie Li,\nLong Ouyang,\nJun-\ntang\nZhuang,\nJoyce\nLee,\nYufei\nGuo,\net\nal.\n2023.\nImproving image generation with better\ncaptions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared\nD\nKaplan,\nPrafulla\nDhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are\nfew-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep re-\ninforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nMathias Ciliberto, Vitor Fortes Rey, Alberto Calatroni,\nPaul Lukowicz, and Daniel Roggen. 2021. Opportu-\nnity++: A multimodal dataset for video-and wear-\nable, object and ambient sensors-based human ac-\ntivity recognition. Frontiers in Computer Science,\n3:792065.\nKellie Corona, Katie Osterdahl, Roderic Collins, and\nAnthony Hoogs. 2020. Meva: A large-scale multi-\nview. Multimodal Video Dataset for Activity Detec-\ntion.\nKellie Corona, Katie Osterdahl, Roderic Collins, and\nAnthony Hoogs. 2021. Meva: A large-scale multi-\nview, multimodal video dataset for activity detec-\ntion. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n1060\u20131068.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words:\nTransformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929.\nJiazhan Feng,\nQingfeng Sun,\nCan Xu,\nPu Zhao,\nYaming Yang, Chongyang Tao, Dongyan Zhao,\nand Qingwei Lin. 2022.\nMmdialog:\nA large-\nscale multi-turn dialogue dataset towards multi-\nmodal open-domain conversation.\narXiv preprint\narXiv:2211.05719.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023.\nKoala: A dialogue model for academic re-\nsearch. Blog post, April, 1.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. Advances in neural information pro-\ncessing systems, 27.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.\nDenoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840\u2013\n6851.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nan-\ndan Thakur,\nand Jimmy Lin. 2023.\nHagrid:\nA\nhuman-llm\ncollaborative\ndataset\nfor\ngenera-\ntive information-seeking with attribution.\narXiv\npreprint arXiv:2307.16883.\nMin Young Lee. 2023. Building multimodal ai chat-\nbots. arXiv preprint arXiv:2305.03512.\nNyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin\nChoi, and Sung-Hyun Myaeng. 2021.\nConstruct-\ning multi-modal dialogue dataset by replacing text\nwith semantically relevant images. arXiv preprint\narXiv:2107.08685.\nYoung-Jun Lee, Byungsoo Ko, Han-Gyu Kim, and Ho-\nJin Choi. 2022. Dialogcc: Large-scale multi-modal\ndialogue dataset. arXiv preprint arXiv:2212.04119.\nYoung-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jongh-\nwan Hyeon, and Ho-Jin Choi. 2023. Dialogcc: An\nautomated pipeline for creating high-quality multi-\nmodal dialogue datasets. In NeurIPS 2023 Work-\nshop on Instruction Tuning and Instruction Follow-\ning.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\nlabelled multi-turn dialogue dataset. arXiv preprint\narXiv:1710.03957.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014:\n13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer.\nHaotian\nLiu,\nChunyuan\nLi,\nQingyang\nWu,\nand\nYong Jae Lee. 2023a.\nVisual instruction tuning.\narXiv preprint arXiv:2304.08485.\nHaotian\nLiu,\nChunyuan\nLi,\nQingyang\nWu,\nand\nYong Jae Lee. 2023b. Visual instruction tuning.\nSiyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke,\nXiaoyan Zhu, and Minlie Huang. 2022.\nRethink-\ning and refining the distinct metric. arXiv preprint\narXiv:2202.13587.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023c. Sum-\nmary of chatgpt/gpt-4 research and perspective to-\nwards the future of large language models. arXiv\npreprint arXiv:2304.01852.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin,\nTushar Nagarajan, Matt Smith, Shashank Jain,\nChun-Fu Yeh, Prakash Murugesan, Peyman Heidari,\nYue Liu, et al. 2023.\nAnymal:\nAn efficient and\nscalable any-modality augmented language model.\narXiv preprint arXiv:2309.16058.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages\n2303\u201308774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural In-\nformation Processing Systems, 35:27730\u201327744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311\u2013318.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023.\nThe refinedweb dataset\nfor falcon llm: outperforming curated corpora with\nweb data, and web data only.\narXiv preprint\narXiv:2306.01116.\nDustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann,\nTim Dockhorn,\nJonas M\u00a8uller,\nJoe\nPenna, and Robin Rombach. 2023. Sdxl: improv-\ning latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry,\nAmanda Askell,\nPamela Mishkin,\nJack\nClark, et al. 2021. Learning transferable visual mod-\nels from natural language supervision. In Interna-\ntional conference on machine learning, pages 8748\u2013\n8763. PMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.\nOpe-\nnAI blog, 1(8):9.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion\nmodels.\nIn Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition,\npages 10684\u201310695.\nChristoph\nSchuhmann.\n2023.\nimproved-\naesthetic-predictor.\nhttps:https:\n//github.com/christophschuhmann/\nimproved-aesthetic-predictor.\nGitHub reposi-\ntory.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu,\nCade Gordon,\nRoss Wightman,\nMehdi\nCherti, Theo Coombes, Aarush Katta, Clayton\nMullis, Mitchell Wortsman, et al. 2022. Laion-5b:\nAn open large-scale dataset for training next gener-\nation image-text models. Advances in Neural Infor-\nmation Processing Systems, 35:25278\u201325294.\nYonglong Tian,\nLijie Fan,\nPhillip Isola,\nHuiwen\nChang,\nand Dilip Krishnan. 2023.\nStablerep:\nSynthetic images from text-to-image models make\nstrong visual representation learners. arXiv preprint\narXiv:2306.00984.\nHugo\nTouvron,\nThibaut\nLavril,\nGautier\nIzacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. 2023. Llama: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nVeniamin Veselovsky,\nManoel Horta Ribeiro,\nand\nRobert West. 2023. Artificial artificial artificial in-\ntelligence: Crowd workers widely use large language\nmodels for text production tasks.\narXiv preprint\narXiv:2306.07899.\nYi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo\nYu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping\nLuo, Ziwei Liu, et al. 2023. Internvid: A large-scale\nvideo-text dataset for multimodal understanding\nand generation. arXiv preprint arXiv:2307.06942.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Fun-\ntowicz, et al. 2019.\nHuggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nNahathai\nWongpakaran,\nTinakon\nWongpakaran,\nDanny Wedding, and Kilem L Gwet. 2013.\nA\ncomparison of cohen\u2019s kappa and gwet\u2019s ac1 when\ncalculating inter-rater reliability coefficients:\na\nstudy conducted with personality disorder samples.\nBMC medical research methodology, 13:1\u20137.\nXiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,\nHao Zhang, and Jindong Chen. 2021.\nPhotochat:\nA human-human dialogue dataset with photo shar-\ning behavior for joint image-text modeling. arXiv\npreprint arXiv:2108.01453.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? arXiv preprint arXiv:1801.07243.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019.\nDialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nSupplementary\nA\nCOT & FS Prompts\nIn the paper, we referenced the Few Shot\nand Chain of Thought prompts, which can\nbe found in Figures 5 and 6, respectively.\nWhen generating multi-modal versions from\neach text-only input dataset, it became evi-\ndent that distinct prompting is necessary for\nthe chain of thoughts due to variations in the\nformat of the input text.\nB\nPhotoChat results\nAs mentioned in section 4.1, here we have in-\ncluded the results of different LLM on Pho-\ntoChat dataset.\nTable 6 shows the results.\nOverall, GPT 3.5 shows better performance\ncompared with other LLM models. As it can\nbe seen, the precision is significantly lower\ncompared with the results reported on MMDi-\nalogue dataset (Table 1) and that is because\nthis dataset is limited to only one image per\ndialogue while our pipeline does not have such\nrestriction.\nC\nImage Generator Ablation Study\nTable 7 shows the performance of different\ndiffusion models (Podell et al., 2023; Rom-\nbach et al., 2022). The results are taken from\nMMDialog dataset and the quality assurance\nmodule is disabled to report the results with-\nout filtering unwanted ones. It is clear that\nSDXL 1.0 and SDXL 0.9 has very similar per-\nformance and higher aethetic score compared\nwith Stable Diffusion 2.0.\nAll models have\nsimilar CLIP score which is predictable as they\nare given the same prompt for image genera-\ntion.\nD\nHuman evaluation\nTo collect answers from annotators, we created\na website with a schema shown in Figure 7.\nFor each question, annotators were given two\nscreenshots of the same dialogue, one gener-\nated by MAGID and the other from a source\ndataset (PhotoChat, MMDialog, or MMDD).\nAt the start of the annotation session, annota-\ntors were instructed to ignore the conversation\nTable 6: Different LLM model testing on PhotoChat (ground-truth).\nQuality Assurance module is\nenabled. The system prompt is chain of thoughts.\nModel\nAccuracy Precision\nRecall\nF1 score\nCLIP score\nMM-Relevance\n#images\nAesthetic\nGPT 3.5\n86.11%\n28.62%\n25.91%\n0.27\n0.25\n313.64\n87\n0.57\nFalcon-40b-Ins.\n88.10%\n28.04%\n11.83%\n0.17\n0.24\n303.68\n403\n0.58\nKoala 13b\n89.61%\n30.43%\n2.94%\n0.05\n0.24\n283.44\n92\n0.61\nLlama 13b\n87.32%\n20.79%\n9.54%\n0.13\n0.23\n244.36\n433\n0.59\nOpenLLaMA\n88.75%\n27.31%\n8.03%\n0.12\n0.23\n270.36\n696\n0.59\nVicuna 13b\n88.40%\n25.48%\n8.35%\n0.13\n0.24\n244.97\n602\n0.55\nPhotoChat\nN/A\nN/A\nN/A\nN/A\n0.213\nN/A\n961\n0.49\nFew-shot example prompt\n- query: >\nUtterance: 0: So yeah, it was a mostly dismal year. But what\u2019s the best news you\u2019ve\nread/saw/heard in 2016? (Anything from the personal to world affairs.)\nUtterance: 1: grew 14 pumpkins on the formidable\nstrength of my chickens We are\nall proud! Here\u2019s one\nUtterance: 2: Very impressive!\nanswer: >\n<result> Utterance: 1: 14 pumpkins</result>\n- query: >\nUtterance: 0: Working from home with a tie today! Plenty of Zoom in my life today!\nUtterance: 1: I keep a polo handy that I throw on and off for zoom calls.\nWay to be extra fancy\nanswer: >\n<result>Utterance: 0: Working from home with a tie</result>\nFigure 5: The few-shot example prompt not only provides the format for both input and expected output\nalong with a problem description but also includes multiple exemplars to elucidate the desired response\nfrom the LLM. Here only exemplars are included.\nChain of Thoughts prompt\n- query: >\nUtterance: 0: So yeah, it was a mostly dismal year. But what\u2019s the best news you\u2019ve\nread/saw/heard in 2016? (Anything from the personal to world affairs.)\nUtterance: 1: grew 14 pumpkins on the formidable\nstrength of my chickens We are\nall proud! Here\u2019s one\nUtterance: 2: Very impressive!\nanswer: >\n<reason> Utterance 0 is just a descrption of last year without any information that\ncan be translated with image. We add photographic style as it is a personal sharing.\nUtterance 1 on the other hand is talking about growing 14 pumpkins. This can be\nrepresented with image.</reason>\n<result> Utterance: 1: 14 pumpkins</result>\n- query: >\nUtterance: 0: My attire for the SA Hip Hop Awards\nUtterance: 1: Are you a supporter of Kaizer Chiefs?...lol. Gorgeous!!\nanswer: >\n<reason>In Utterance 0 contains the sentence \"My outfit for\nthe SA hip hop awards\" which shows\nthe person is willing to share her outfit</reason>\n<result>Utterance: 0: My outfit for the SA hip hop awards </result>\nFigure 6: The chain of thoughts prompt, building upon the system prompt provided in the few-shot\nexample prompt, also incorporates the detailed reasoning on utterance selection.\ntext and focus only on the images and image-\ntext matching. Fifteen annotators completed\nthe task, each making 20 comparisons.\nE\nDownstream Training\nHere, we study how much MAGID can impact\ntraining a multi-modal model when changing\nthe original image with synthetic one gener-\nated by MAGID. In addition, we also com-\npare it with benchmark cases when no image is\npresent in the training and with MMDD (Lee\net al., 2021) approach to include image in the\ndialogue. In this regard, we used the same ar-\nchitecture suggested in (Lee, 2023) which is vi-\nsionTextDualEncoder from Huggingface (Wolf\net al., 2019) which projects the encoding of im-\nage with the the embedding of text to a shared\ncommon space. For encoding of image we used\nViT (Dosovitskiy et al., 2020), and for pro-\ncessing the text we used pretrained DialoGPT\n(Zhang et al., 2019). While the input is multi-\nmodal, the output is text only. In this task,\nwe omit the last text utterance and the model\nshould predict it given the prior image and\ntext.\nWe fine-tuned the model on MMDialog\ndataset and the results are reported in Table 8.\nFor this experiment, we used the learning rate\nof 0.00005 with Adam Optimizer. In Table 8,\nwe show the results on the test set when train-\ning set images is coming from MMDialogue,\nMAGID, MMDD and the case where the im-\nages are omitted.\nFor MMDD, we used the\nsame code they used to inject image into text-\nonly dialogue to make the comparison possi-\nble. For this expeiment, the training set con-\nsists of 5156 dialogues and the test set consists\nof 633 dialogues sampled from MMDialogue\ndataset.\nAs it can be seen, when we use the source\nimage as training set (MMDialog), we achieve\nhighest BLEU score (Papineni et al., 2002).\nThe perplexity of the model using MAGID is\nTable 7: Testing different Stable diffusion models\nwith MAGID pipeline\nModel\nAesthetic\nScore\nCLIP\nScore\nSDXL 1.0\n0.56\n0.26\nSDXL 0.9\n0.57\n0.26\nStable Diffusion 2.0\n0.53\n0.26\nTable 8: Downstream training. The model used is\nDialoGPT + ViT. BLUE score is in percentage.\nDataset\nPPL\nBLEU-\n1\nBLEU-\n2\ndistinct-\n1\ndistinct-\n2\nMMDialogue\n73.09\n8.3\n3.9\n0.94\n0.965\nMAGID\n70.99\n7.9\n3.9\n0.94\n0.971\nMMDD\n78.86\n7.5\n3.0\n0.93\n0.963\nNo image\n78.88\n7.9\n3.0\n0.92\n0.952\nlowest which shows the model is more confi-\ndent in making the prediction.\nIn addition,\nthe distinct score (Liu et al., 2022) which\nshows the diversity of response is highest with\nMAGID which can be attributed to higher\nimage-text match provided with MAGID im-\nages. It is important to note that since MM-\nDialog dataset is a real dataset, the quality\nof images shared does not necessarily matches\nthe text and this can make the model less con-\nfident and results in higher perplexity. On the\nother hand, the images generated by MAGID\nis more controlled.\nFor this experiment we used 4 NVIDIA RTX\nGPU each with 24 GiB memory and the train-\ning took for a full day.\nF\nExperiment Computational Cost\nFor running MAGID pipeline, it can be run\nwith one GPU with NVIDIA RTX with 24 GiB\nmemory.\nG\nDiscussion on Inter-rater\nReliability Measure Choice\nIn Section 4.2, we employed Gwet\u2019s AC1 for\nevaluating the consistency among reviewers,\nopting not to use Cohen\u2019s Kappa due to\nits susceptibility to outliers and potential for\nshowing inconsistent results despite high aver-\nage scores across all participants. As detailed\nin the study by Wongpakaran et al. (2013),\nGwet\u2019s AC1 is recognized for its greater con-\nsistency in inter-rater reliability assessments\nwhen compared to Cohen\u2019s Kappa, alongside\nits enhanced resilience to outliers, providing a\nmore reliable measure for our analysis (Wong-\npakaran et al., 2013). This approach ensures\na more stable and accurate assessment of re-\nviewer consistency, mitigating the impact of\nanomalies on the reliability scores.\nH\nFurther examples of MAGID\nFigures 8, 9, and 10 provide more examples\non comparing MAGID with MMDialog, Pho-\ntoChat, and MMD.\nI\nExperiment Setting\nFor determining the threshold for image-text\nmatching and aesthetic score, we employed\ncross-validation on the validation set. In this\nregard, the threshold for CLIP score was set\nfor 0.21 and the threshold for the aesthetic\nscore was set for 0.51. Based on our observa-\ntions, we established a protocol where a gen-\nerated image could fail up to two times be-\nfore being discarded and triggering the feed-\nback loop. This approach ensured a balance\nbetween generating high-quality images and\nmaintaining efficient processing. In all our ex-\nperiments, we used SDXL 1.0 model for image\ngeneration.\nJ\nLimitations\nIn Figures 11, 12, and 13, we showcase the\nmost common scenarios were MAGID can fail\nto generate the image which properly supports\nthe preceding utterance. Specifically, figure 11\nshows a common example, where the gener-\nated image usually fails to put the proper text\nsign in the generated image. In Figures 12 and\n13 showcase the examples where the generated\nimage does not follow the correct description\nin terms of number object should exist in the\nimage. We believe using more advanced dif-\nfusion models like DALL-E 3 should mitigate\nthis problem.\nFigure 7: Schema of the website used to perform human evaluation.\nFigure 8: MAGID (left) versus MMDialog (right)\nFigure 9: MAGID (left) versus PhotoChat (right)\nFigure 10: MAGID (left) versus MMDD (right)\nFigure 11: Generated image by MAGID fails to\nproperly show the sign HA\nFigure 12: Generated image by MAGID fails to\nproperly shows 4 cats instead of 3\nFigure 13: Generated image by MAGID fails to\nproperly shows 6 fishes instead of 5\n"
  },
  {
    "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
    "link": "https://arxiv.org/pdf/2403.02775.pdf",
    "upvote": "10",
    "text": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs\nHanlin Tang\nranchotang@tencent.com\nYifu Sun\nyifusun@tencent.com\nDecheng Wu\nwoodchenwu@tencent.com\nKai Liu\nraccoonliu@tencent.com\nJianchen Zhu\ndickzhu@tencent.com\nZhanhui Kang\nkegokang@tencent.com\nAbstract\nLarge language models (LLMs) have proven\nto be very superior to conventional methods in\nvarious tasks. However, their expensive com-\nputations and high memory requirements are\nprohibitive for deployment. Model quantiza-\ntion is an effective method for reducing this\noverhead. The problem is that in most previous\nworks, the quantized model was calibrated us-\ning a few samples from the training data, which\nmight affect the generalization of the quantized\nLLMs to unknown cases and tasks. Hence in\nthis work, we explore an important question:\nCan we design a data-free quantization method\nfor LLMs to guarantee its generalization per-\nformance?\nIn this work, we propose EasyQuant, a training-\nfree and data-free weight-only quantization al-\ngorithm for LLMs. Our observation indicates\nthat two factors: outliers in the weight and\nquantization ranges, are essential for reducing\nthe quantization error. Therefore, in EasyQuant,\nwe leave the outliers (less than 1%) unchanged\nand optimize the quantization range to reduce\nthe reconstruction error. With these methods,\nwe surprisingly find that EasyQuant achieves\ncomparable performance to the original model.\nSince EasyQuant does not depend on any train-\ning data, the generalization performance of\nquantized LLMs are safely guaranteed. More-\nover, EasyQuant can be implemented in parallel\nso that the quantized model could be attained\nin a few minutes even for LLMs over 100B. To\nour best knowledge, we are the first work that\nachieves comparable performance with data-\ndependent algorithms under a data-free setting\nand our algorithm runs over 10 times faster than\nthe data-dependent methods.\n1\nIntroduction\nRecent work has already proved the superior per-\nformance of Transformer (Vaswani et al., 2017)\nbased LLMs (Workshop, 2023; Zhang et al., 2022;\nTouvron et al., 2023; Brown et al., 2020; Rae et al.,\n2021; Smith et al., 2022; Chowdhery et al., 2022;\nZeng et al., 2022) on various tasks over traditional\nmethods, and has attracted massive interest in how\nto improve and utilize those LLMs. However, the\nmodel size also grows dramatically along with im-\nproved performance. Hence the memory footprint\nand computational cost become the bottleneck for\ndeploying those models. One promising solution to\nalleviate this overhead is model quantization (Fran-\ntar et al., 2023a; Xiao et al., 2023), where we quan-\ntize weight only or weight and activation both i\norder to reduce memory consumption and compu-\ntational cost.\nAlthough model quantization is a well-studied\narea for normal-sized models, such as BERT (De-\nvlin et al., 2018) and GPT-2 (Radford et al., 2019),\nit is still a quite challenging task for LLMs. One\nmajor reason is that previous lossless model quan-\ntization algorithms require retraining for the quan-\ntized model, which is too expensive for models\nover billions of parameters. Beyond this, previous\nmodels are usually designed for specific domain\ntasks, which means the training data are sampled\nfrom limited task domains. However, recent LLMs\nare usually trained on various domains of data cor-\npus, and they have shown to be quite effective for\nmulti-domain zero-shot tasks. In this case, if we\nonly retrain the quantized LLMs using partial do-\nmain corpus, the generalization ability of LLMs\nmight get worse. Therefore both efficiency and\ngeneralization guarantees are very important for\ndesigning LLMs quantization algorithms. To date,\nfor low-bits weight-only quantization, several post-\ntraining algorithms have been proposed (Frantar\net al., 2023a; Yao et al., 2022). However, those\nmethods also require a small calibration set sam-\npled from training data, which still takes at least\nseveral hours. Moreover, the use of those calibra-\ntion data also brings the risk of making the model\noverfit to the calibration set.\narXiv:2403.02775v1  [cs.AI]  5 Mar 2024\n\ud835\udc2a\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc20\ud835\udc1e = \ud835\udfcf\ud835\udfce\ud835\udfce\n\ud835\udc2a\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc20\ud835\udc1e = \ud835\udfd1\nOutlier isolation\nOptimizing \ud835\udc92\ud835\udc93\ud835\udc82\ud835\udc8f\ud835\udc88\ud835\udc86\n\ud835\udc2a\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc20\ud835\udc1e = \ud835\udfcf. \ud835\udfd3\nAdd\nEasyQuant\nFigure 1: Pipeline of EasyQuant.\nWe first find all the outliers in weight and keep them in full precision\n(fp32/fp16/bf16). Afterward, we optimize the quantization range (denoted as qrange) in order to approximate\nthe normal values more precisely. In the end, the normal values are quantized into lower bits (denoted as Q[\u00b7]) with\noptimized quantization ranges and we set the outliers unchanged in weight.\nOur Contribution:\nIn this work, we propose\na novel data-free model quantization algorithm,\nnamely EasyQuant, that potentially improves the\nperformance of low-bits quantized LLMs. The gen-\neralization ability of LLMs is inherently guaran-\nteed since EasyQuant does not need any input data.\nBy running EasyQuant for only a few minutes, we\ncan quantize public-available OPT-176B, BLOOM-\n176B, and LLAMA-65B into lower bits without\nsignificant loss on various benchmarks. To our best\nknowledge, this is the first data-free LLM quan-\ntization algorithm for LLM quantization without\nnotable system overhead.\nMoreover, our work reveals the essential fac-\ntors that cause the performance degradation of the\nquantized LLMs. We show that the outliers in\nweights are more critical to the model\u2019s perfor-\nmance compared to the normal elements. Beyond\nthis, we propose to use a gradient-based method\nfor optimizing the quantization range. These two\nstrategies can also be used in other scenarios, such\nas weight-activation quantization and quantization-\naware training (QAT).\nLast but not least, we develop efficient CUDA\nkernels for outlier isolation in dequantization, and\nproved that hold 1% outliers in weights unquan-\ntized brings negligible (less than 0.1%) overhead\nw.r.t to overall latency. We also propose to im-\nplement EasyQuant in parallel for quantizing each\nweight in the model, which means a 175B-sized\nmodel can be quantized into 4-bits within 10 min-\nutes.\n2\nBackground and Motivation\nThe most widely used quantization method, namely\nrounding to nearest-number (RTN), quantizes a\ntensor x into k-bits representation according to\nQ[x] = s \u00d7\nj\nclamp\n\u0010x\ns , lmin, lmax\n\u0011m\n(1)\nHere s is the quantization scale, lmin and lmax are\nthe lower and upper bound for clipping, and \u230a\u00b7\u2309\nis the rounding operator. Usually we set lmin =\n\u0000\u22122k\u22121 + 1\n\u0001\nand lmax = 2k\u22121 and set s to be the\nmaximum absolute value in x.\nThere are two major directions for finding the best\nconfiguration in weight-only LLM quantization.\nThe first is to minimize the reconstruction error\nof the weight parameter (denoted as W), which is\ndefined as\nr(W) := \u2225Q[W] \u2212 W\u22252.\nNotice that in this case we only need to have access\nto the weight itself, therefore it is data-free.\nBeyond this, recent studies (Frantar et al., 2023a;\nYao et al., 2022) propose to use the output error,\ndefined as\ne(W) =\nX\nX\u2208D\n\u2225Q[W]X \u2212 WX\u22252 ,\nwhere D is a calibration set sampled from the orig-\ninal training data, for optimization. This regulation\ntries to mimic the outputs from the original model\ndirectly hence achieving a more promising result\nthan reconstruction-based methods.\nData-dependent calibration might weaken the\ngeneralization ability of LLMs\nHowever, the\nperformance gain from using calibration data might\njeopardize the generalization of the quantized\nmodel, because it brings the risk of making the\nmodel overfit to the calibration set. For example,\n0\n100\n200\n300\n400\n500\nOptimizing step\n30000\n35000\n40000\n45000\nReconstruction Error\nreconstruction error\n13\n14\n15\n16\n17\n18\n19\n20\nPerplexity on WikiText2\nReconstruction Error v.s. Model's Performance\nperplexity\n0\n100\n200\n300\n400\n500\nOptimizing step\n20000\n25000\n30000\n35000\n40000\n45000\nReconstruction Error\nreconstruction error\n11.6\n11.7\n11.8\n11.9\n12.0\n12.1\nPerplexity on WikiText2\nReconstruction Error v.s. Model's Performance Without Outlier\nperplexity\nFigure 2: Smaller reconstruction error cannot guarantee a better model performance. Straightforwardly shrinking\nthe quantization ranges will clip most of the outliers to be very small, hence the perplexity increases severely\nsince those outliers are critical for preserving the model\u2019s performance. However, when keeping those outliers\nunquantized, the quantized model achieves a better performance as the reconstruction error decreases continuously.\nThis result clearly suggests that the outliers are more important than the normal values in weight, and optimizing the\nquantization ranges using gradient defined in (2) can significantly increase the accuracy of quantized models. More\ndetails about the experiment can be found in Section 5.\nboth ZeroQuant and GPTQ involve changing the\noriginal weight by training or OBS in order to min-\nimize the output error, therefore the distribution\nof the weight\u2019s parameters might deviate from the\noriginal. Since the calibration data is usually sam-\npled from a few specific domains, the performance\nof the calibrated model on other tasks may not be\nguaranteed.\nData-free quantization is challenging, but very\nimportant\nAlthough it\u2019s more challenging to use\nthe reconstruction error as a regulation because it\ncan only optimize the quantized model indirectly,\nstill it is a very important direction for researching\nbecause the generalization ability of the model is\ninherently guaranteed when using data-free quanti-\nzation since it uses no training data. Therefore in\nthis paper, we aim to answer the following ques-\ntion:\nHow can we efficiently recover the performance\nof the quantized model without using any input\ndata?\nIn this work we propose EasyQuant, a data-free fast\nalgorithm that could significantly improve the per-\nformance of quantized LLMs in a data-free setting,\nand more importantly, even outperforms the results\nfrom data-dependent quantization algorithms. Our\nexperiments reveal that the performance gap of the\nlower bits (e.g. 4-bits) quantized LLMs origins\nfrom two factors:\n1. Setting the quantization range as the maxi-\nmum absolute value of the weight induces a\nlarge reconstruction error for low-bits quanti-\nzation.\n2. The outliers in the weight matrix, which ac-\ncount for less than 0.1% of the parameters, im-\npose a very important influence on the model\u2019s\nperformance.\nIn EasyQuant, we use quantization range mini-\nmization and outlier isolation to address these two\nchallenges, and our results prove that EasyQuant\nachieves a significant improvement over RTN.\n3\nInsight behind EasyQuant\nAs mentioned above, the weight\u2019s outliers and\nquantization ranges are essential to the quantized\nmodel\u2019s performance. Below we present the sup-\nporting experiments in detail.\n3.1\nThe quantization range can be efficiently\noptimized using gradient\nAlthough the quantization operation itself is non-\ndifferentiable, the gradient of the reconstruction\nerror (\u2225Q[x] \u2212 x\u22252) w.r.t. the quantization range\ns is differentiable in most cases. We proved that\nthe gradient of the quantization range s admits (see\nSection 4 for more details)\n\u2202\u2225Q[x] \u2212 x\u22252\n\u2202s\n= 2\nX\ni\n\u0010\n(Q[xi] \u2212 xi)\njxi\ns\nm\u0011\n.\n(2)\nWith this gradient, the reconstruction error can be\nquickly minimized within hundreds of steps (see\nFigure 2 for more details). This result indicates that\nby shrinking the quantization range, most of the\nparameters in weight can be approximated more\nprecisely. However, as shown in Figure 2, the per-\nformance of the quantized weight gets even worse\nas the reconstruction error decreases. This is a very\ncounter-intuitive result.\nThrough in-depth analysis, we realized that\nwhen decreasing the quantization range, more\nsalient parameters outside the quantization range\nwould be clipped out. Although most of the weights\nget approximated more precisely as indicated by\nthe decreased reconstruction error, the salient pa-\nrameters are poorly represented. As the model per-\nformance drops severely in this case, we realized\nthat those outliers are way more important than the\nnormal elements for the model\u2019s performance.\n3.2\nOutliers in weight are very important, but\nnot sufficient\nBefore we further discuss the influence of those\noutliers, we first provide a (n\u03c3) criterion for defin-\ning the outliers in weight. For any weight W, we\nsay its (i, j)-th number Wi,j is an (n\u03c3) outlier if\n|Wi,j \u2212 mean(W)| \u2265 n \u2217 var(W),\n(3)\nwhere mean(W) and var(W) are the mean and\nvariance of W.\nNow the question is: Can we hold those out-\nliers unchanged and straightforwardly compress\nthe normal elements into lower bits? Unfortunately,\nour result suggests that excluding the outliers from\nquantization solely is not enough. As shown in\nTable 1, the performance gap still exists even when\nwe hold 1% numbers in fp16. The problem is that\nif we keep too many numbers in fp16, the overhead\nof the dequantization kernel would also increase\nand result in a decreased overall throughput.\n3.3\nEasyQuant potentially improve the\nperformance\nAs shown in Section 3.1 and Section 3.2, optimiz-\ning the quantization ranges directly reduces the\nmodel\u2019s performance drops severely because of the\nclipped outliers. These key observations inspire\nus to design EasyQuant, in which we isolate the\noutliers from quantization first and then optimizing\nthe quantization range for the remaining elements.\nAs shown in the right part of Figure 2, with outliers\nbeing kept unquantized, the performance of the\nquantized model increases continuously under de-\ncreased reconstruction. This clearly proves we can\npotentially improve the performance of quantized\nLLMs with this strategy.\n4\nMethodology\n4.1\nDriving of the gradient in (2)\nLet\u2019s say the original scale s gets an infinitely small\nvariation \u2206s, which means\n\u0016\nx\ns + \u2206s\n\u0019\n=\njx\ns\nm\n,\nif x\ns \u2212\n\u0016\nx\ns + \u2206s\n\u0019\n\u0338= 0.5.\nTherefore we get\nQs+\u2206s[x] =(s + \u2206s)\n\u0016\nx\ns + \u2206s\n\u0019\n=(s + \u2206s)\njx\ns\nm\n,\nthis leads to\n\u2202Q[x]\n\u2202s\n= Qs+\u2206s[x] \u2212 Qs[x]\n\u2206s\n=\njx\ns\nm\n.\nThis gives us\n\u2202\u2225Q[x] \u2212 x\u22252\n\u2202s\n=2\n\u001c\nQ[x] \u2212 x, \u2202Q[x]\n\u2202s\n\u001d\n=2\nD\nQ[x] \u2212 x,\njxi\ns\nmE\n=2\nX\ni\n\u0010\n(Q[xi] \u2212 xi)\njxi\ns\nm\u0011\n.\n4.2\nAlgorithm description\nIn EasyQuant, for each weight W, we first select all\n(n\u03c3) outliers (using (3)) and store its index Io(W).\nAfterward, for the normal elements, we optimize\nthe per-channel quantization range using an opti-\nmizer (in our case we use Adam for example) with\ngradients defined in (2). The final quantized weight\nfrom EasyQuant can be formulated as\nQEasyQuant[W]\n=Masko(W) \u2217 W + (1 \u2212 Masko(W)) \u2217 Q[W],\n(4)\nwhere Masko is a mask tensor defined as\nMasko\ni,j(W) =\n\u001a 1\nif (i, j) \u2208 Io(W),\n0\nif (i, j) /\u2208 Io(W).\n(5)\nThe detailed description of EasyQuant is in Algo-\nrithm 1.\nThreshold n (BLOOM-7B)\nBaseline\n1\n2\n4\n6\nPPL on WikiText2\n11.37\n12.153\n12.495\n12.518\n12.536\nTable 1: Isolating outliers in weight from quantization can increase the model\u2019s performance. Here n refers to the\nhyper-parameter in the outlier criterion (n\u03c3) as defined in (3) and baseline is the result from unquantized model.\nNotice that even with 10%(n = 1) numbers being held unquantized, there is still a large gap to the baseline. This\nmeans isolating the outliers is not enough to fully recover the accuracy of quantized models.\nAlgorithm 1 EasyQuant\n1: Initialize: outlier threshold n, hyper-parameters for opti-\nmizer A, original weight W.\n2: Quantize:\n3:\nAccording to (3), compute the index Io(W) of the\n(n\u03c3) outliers in W.\n4:\nOptimizing the quantization range s using optimizer\nA with gradient defined in (2).\n5:\nQuantize W into Q[W].\n6: Dequantize:\nQEasyQuant[W]\n=\nMasko(W) \u2217 W\n+\n(1 \u2212 Masko(W) \u2217 Q[W],\nwhere Masko(W) is\ndefined in (5).\n5\nExperiment\nBaselines:\nWe compare EasyQuant with several\nbaselines in the INT4 quantization setting below:\n\u2022 RTN: The model\u2019s weights are naively quan-\ntized according to (1).\n\u2022 ZeroQuant: The algorithm proposed in Yao\net al. (2022).\nAuthors treat each layer as\na small neural network and use the origi-\nnal as the teacher model to distill the quan-\ntized one. This is equivalently minimizing\nP\nx\u2208D \u2225f(W T ; x) \u2212 f(W S; x)\u22252 where x\nare the input activations, W T is the weight\nof the original model and W S is the quantized\nmodel.\n\u2022 GPTQ: This algorithm is proposed in Frantar\net al. (2023a). Authors use the same objective\nfunction P\nx\u2208D \u2225f(W T ; x)\u2212f(W S; x)\u22252 as\nin ZeroQuant. But they utilize OBS for min-\nimizing the loss function instead of using a\ngradient-based optimizer.\nExperiment Setup.\nFor all models, we set the\noutlier threshold n \u2208 [2.5, 3] in order to ensure that\nthe outliers account less than 1% of all numbers.\nFor BLOOM and LLAMA, we use n = 3. When\noptimizing the quantization ranges, we use Adam\nas the optimizer and set the learning rate 1e \u2212 3\nfor BLOOM and 1e \u2212 4 for LLAMA. We choose\nthe quantization ranges from step 100 for BLOOM\nand 500 for LLAMA. We use symmetric quanti-\nzation since the normal values are symmetrically\ndistributed with the outliers being excluded. For a\nfair comparison, we use per-channel quantization\nfor weight in all algorithms (which means each\ncolumn shares one common quantization range).\nEvaluation Tasks.\nAs for the evaluation tasks,\nwe mainly focus on perplexity-based tasks, as they\nare known to be particularly sensitive to model\nquantization Frantar et al. (2023b). The perplex-\nity tasks we include are WikiText2 (Merity et al.,\n2016), Penn Treebank (Marcus et al., 1994) and C4\n(Raffel et al., 2020). The zero-shot tasks\u2019 results\nare also provided, such as PIQA (Tata and Patel,\n2003), ARC (Boratko et al., 2018) and StoryCloze\n(Mostafazadeh et al., 2017).\nImplementation.\nSince each weight can be quan-\ntized in parallel, therefore we use 8\u2217 A100 for run-\nning EasyQuant, and we finish the quantization in\n1 \u223c 10 mins for all models. We store the index and\nvalue for all outliers together with the quantized\nnormal values. Our dequantization kernel is built\nusing CUDA.\n5.1\nExperiment Analysis\nWe focus our study on LLM by quantizing the\nentire BLOOM, and LLAMA model families to\n4-bit.\nPerplexity-base tasks.\nWe first study perplexity-\nbased tasks. On LLaMA models, Table 2 shows\nthat EasyQuant outperforms GPTQ in most cases.\nFor LLaMA-65B, GPTQ drops 4.21 points on\nPTB, performing worse than the 9 \u00d7 smaller\nfull-precision 7B model, while EasyQuant still\nperforms well on this task. On the other tasks,\nEasyQuant losing only 0.4\u20130.7 points. BLOOM\nshows a similar pattern (see Table 10 in ap-\npendix): EasyQuant drops only 0.1-0.16 points\non perplexity-based tasks. Notice that we observe\na smaller gap between our method and GPTQ on\nC4. It is mostly because, as a data-calibrated quan-\ntization method, GPTQ uses C4 dataset for calibra-\nPerplexity-based Task\nPerplexity-based Task\nWikiText2\nPTB\nC4\nWikiText2\nPTB\nC4\nLLAMA\u20137B\nfp16\n5.68\n8.80\n7.08\nLLAMA\u201333B\nfp16\n4.10\n7.30\n5.98\nRTN\n6.29\n11.25\n8.12\nRTN\n4.54\n8.65\n6.54\nGPTQ\n6.09\n11.56\n7.78\nGPTQ\n4.45\n8.44\n6.40\nEasyQuant\n6.01\n10.72\n7.71\nEasyQuant\n4.34\n8.45\n6.37\nLLAMA\u201313B\nfp16\n5.09\n8.07\n6.61\nLLAMA\u201365B\nfp16\n3.53\n6.91\n5.62\nRTN\n5.53\n9.77\n7.23\nRTN\n3.99\n10.67\n6.45\nGPTQ\n5.36\n9.49\n7.07\nGPTQ\n4.13\n11.12\n6.38\nEasyQuant\n5.29\n9.37\n6.97\nEasyQuant\n3.98\n9.61\n6.30\nTable 2: Perplexity results for LLAMA model family\ntions.\nZeroshot\ntasks.\nFor\nmost\nzero-shot\ntasks,\nEasyQuant achieves harmless performance with\nonly 0.1 %-0.52% accuracy drops as shown in\nTable 10 in appendix and outperforms GPTQ on\nmost cases. Here we simply use the implementa-\ntion of GPTQ on LLAMA from its git.1 We note\nthat EasyQuant can be further improved via finer-\ngranularity grouping. However, we will not include\nthis overhead in this paper.\noutlier ratio\noverhead\n0.01%\n0.027ms\n0.10%\n0.055ms\n0.50%\n0.093ms\n1%\n0.117ms\n5%\n0.186ms\n10%\n0.212ms\nTable 3: Overhead of outlier isolation on A100\nPractical Latency.\nWe evaluate the overhead\nof EasyQuant by comparing the overhead of out-\nlier isolation, int4 dequantization, and matrix mul-\ntiplication with batch size 1, sequence length\n1024, on a single A100 GPU. The matrix size is\n14336 \u00d7 53746 which is the same as the first FFN\nlayer in 176B BLOOM. For outlier isolation, we\ntest the latency of outliers ratio (fraction of outliers\nwithin the weight) in 6 settings: (0.01%, 0.10%,\n0.50%, 1%, 5%, 10%). The matrix multiplication\ntakes 83ms and dequantization takes 5ms. There-\nfore from Table 3 we can see that recovering the\noutliers in weight brings almost no overhead to the\noverall latency.\nAblation study.\nTo understand the effect of un-\nstructured outliers, we show the perplexity result of\nEasyQuant without outlier isolation or quantization\n1https://github.com/qwopqwop200/GPTQ-for-LLaMa\nrange optimization. As discussed in Section 3, both\nstrategies impose a very important influence on the\nfinal model performance.\nWe further conduct experiments proving whether\nthe performance gain mainly comes from the out-\nlier isolation: Actually, outlier isolation is a very\nimportant component of EasyQuant, but still not\nenough to fully recover the performance loss from\nquantization. Keeping even 10% of weights as fp16\noutliers still admits about 8% ppl increase while\nEasyQuant admits only 1% ppl increase. Below\nwe present the result of 4-bit quantized BLLOM-\n7B when we just keep 1% outliers in fp16 without\nquantization range optimization on various bench-\nmarks.\nBenchmark\nEasyQuant 1% fp16 outlier\nWikiText2(PPL)\n11.66\n12.52\nPTB (PPL)\n21.42\n23.32\nC4(PPL)\n15.46\n16.44\nPIQA (ACC)\n73.61%\n72.74%\nTable 4: Using outlier isolation solely is not enough to\nfully recover the performance loss. EasyQuant consis-\ntently outperforms outlier isolation in all benchmarks.\nOutlier influence.\nThe outlier isolation is a key\ncomponent in EasyQuant, but it can only impose\nan indirect influence on the model accuracy. The\ninteresting phenomenon we find is that the outliers\nbehave like a gating mechanism: without outlier\nisolation, the model achieves a much worse perfor-\nmance under a small reconstruction error; however,\nwhen keeping those outliers in fp16, the quantized\nLLM attains a continuously decreased ppl under\nsmaller reconstruction error:\nMoreover, we have also conducted a comple-\nmentary experiment testing the direct influence of\nthe weight outlier: We prune 1% of the values (\naccording to its magnitude) in weights into 0 and\nsee the ppl results (as shown in Table 6). It has\nreconstruction error\nint4 outlier\nfp16 outlier\n4.8E4\n12.65\n12.50\n3.5E4\n14.73\n11.61\n2.7E4\n19.71\n11.25\n2.3E4\nNA\n11.10\n1.9E4\nNA\n11.02\nTable 5: ppl results on Wikitext2 of BLOOM-7B with\nand without outlier isolation.\nshown that the largest value (outliers) imposes the\nsame influence on the model performance as the\nnormal values (median), which means those out-\nliers share the same direct influence on the model\naccuracy with normal values. Therefore outlier\nisolation imposes a key influence on the model\naccuracy indirectly.\npruned weights\nPPL\nsmallest (top-0% 1%)\n11.66\nmedian (top-49% 50%)\n19.16\nlargest (top-99% 100%)\n19.17\nTable 6: ppl results after pruning 1% weight with differ-\nent magnitude\nOutlier distribution.\nWe also explore the outlier\ndistribution along different modules and layers. It\nshows that the fraction of outliers shares different\npatterns in different modules and layers (as shown\nin Table 7 and 8). FFN.2 has a significantly higher\nfraction of outliers. However, it shows no pattern\nalong the layer index.\nmodule name\noutlier fraction (%)\nAtt.qkv\n0.2993\nAtt.output\n0.5036\nFFN.1\n0.288\nFFN.2\n0.7560\nTable 7: Outlier fraction distribution in different mod-\nules in BLOOM-7B under 3-sigma threshold\nQuantization range.\nThe dynamic of the quanti-\nzation range is shown in Table 9. Roughly speak-\ning, this range decreases fast in the early stage of\ntraining, which means a smaller quantization range\nwill make most of the parameters to be quantized\nmore precisely. After certain steps of training, the\nquantization range becomes stable, this means we\nhave already achieved the optimal range.\nLayer index\noutlier fraction (%)\n1\n0.3187\n5\n0.8579\n10\n0.3953\n15\n0.3975\n20\n0.3962\n25\n0.4399\n30\n0.3954\nTable 8: Outlier fraction distribution in different layer\nindex in BLOOM-7B under 3-sigma threshold\nsteps\nquantization range\n0\n0.078\n10\n0.069\n50\n0.052\n100\n0.048\n150\n0.047\n200\n0.047\nTable 9: The dynamic quantization range of different\noptimization steps. Here we take the quantization range\nof the Att.qkv module in layer 1 as an example.\n6\nRelated Work\nModel Quantization\nTraditional model quanti-\nzation algorithms mainly focus on the cases where\nboth parameters and activations of the model are\nquantized (Lin et al., 2015; Hubara et al., 2016;\nTailor et al., 2021; Ni et al., 2020). However, di-\nrectly quantizing the model will greatly decrease\nthe accuracy of the models, and one important tech-\nnique to improve the performance is Quantization\nAware Training (QAT) (Jacob et al., 2018), where\nit simulates the quantization procedure in training\nto improve the accuracy of the quantized model\nfurther. For Transformer based models, the bound-\nary of the compression level has been continuously\nadvanced. For example, 8-bits quantized transform-\ners as in FullyQT (Prato et al., 2019) and Q8BERT\n(Zafrir et al., 2019), 4-bits quantized BERT in Wu\net al. (2023) and tenary case as in TernaryBERT\n(Zhang et al., 2020).\nModel Quantization for LLMs.\nFor quantizing\nLLMs, due to their prohibitive training expense,\nwe can only use a few training data for calibration.\nThere are two major directions: 1) weight-only\nquantization, where the weights are quantized into\nlower bits. In Frantar et al. (2023a); Yao et al.\n(2022), authors optimize the output error on the\ncalibration set using OBS and gradient descent. 2)\nActivation and weight quantization, where both ac-\ntivations and weights are quantized into lower bits.\nIn this case, the major obstacle is the outliers in\nactivations. LLM.int8() (Dettmers et al., 2022) ad-\ndresses this problem by isolating those outliers in\nfp16/bf16. However, such implementation leads to\nlarge latency overhead and is even slower than fp16\ninference. Recent studies (Wei et al., 2023; Xiao\net al., 2023) found that the outliers only exist in cer-\ntain channels, and use the LayerNorm weights (Wei\net al., 2023) and calibrated scales (Xiao et al., 2023)\nto smooth those channels. Xiao et al. (2023) has\nalready proved that we can achieve almost loss-\nless W8A8 quantized LLMs using a few calibra-\ntion data, without manipulating the original model\nweights.\n7\nConclusion and Limitations\nIn this paper, we propose a data-free fast weight-\nonly quantization algorithm, namely EasyQuant,\nfor LLMs, that potentially improves the quantized\nmodel\u2019s performance without using any training\ndata. Our analysis reveals the intrinsic origins of\nthe performance loss when quantizing the model\nweights into lower bits. We show that by isolat-\ning the outliers from quantization, the accuracy\nof the quantized LLM increases accordingly with\ndecreased reconstruction error. Our experiment\nproved that EasyQuant significantly outperforms\nRTN in a data-free setting, and also behaves bet-\nter than data-dependent algorithms. EasyQuant\ncan finish the quantization for a 176B-sized model\nwithin 10 minutes and the overhead of dequantiza-\ntion in EasyQuant is negligible.\nHowever, we also point out some limitations of\nour work: The outlier recovery functionality in\nEasyQuant requires extra CUDA kernels for im-\nplementation. Moreover, weight-only quantization\ncan only reduce the memory footprint without any\ncomputation cost reduction, hence the latency of\nour model cannot be minimized. In addition, this\noutlier isolation will make the weight/activation\nquantization more challenging because the weight\nincludes numbers under different precision. We\nhave also noticed that EasyQuantcannot outper-\nform the data-dependent methods in all tasks, this\nmotivates us to investigate more effective algo-\nrithms in future studies.\nReferences\nMichael Boratko, Harshit Padigela, Divyendra Mikki-\nlineni, Pritish Yuvraj, Rajarshi Das, Andrew McCal-\nlum, Maria Chang, Achille Fokoue-Nkoutche, Pa-\nvan Kapanipathi, Nicholas Mattei, et al. 2018. A\nsystematic classification of knowledge, reasoning,\nand context within the arc dataset. arXiv preprint\narXiv:1806.00358.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\ncation for transformers at scale.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Cite arxiv:1810.04805Comment: 13 pages.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023a. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023b. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. 2016. Binarized neu-\nral networks. In Advances in neural information\nprocessing systems, pages 4107\u20134115.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018.\nQuanti-\nzation and training of neural networks for efficient\ninteger-arithmetic-only inference. pages 2704\u20132713.\nZhouhan Lin, Matthieu Courbariaux, Roland Memi-\nsevic, and Yoshua Bengio. 2015.\nNeural net-\nworks with few multiplications.\narXiv preprint\narXiv:1510.03009.\nMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz,\nRobert MacIntyre, Ann Bies, Mark Ferguson, Karen\nKatz, and Britta Schasberger. 1994. The penn tree-\nbank: Annotating predicate argument structure. In\nHuman Language Technology: Proceedings of a\nWorkshop held at Plainsboro, New Jersey, March\n8-11, 1994.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. Ls-\ndsem 2017 shared task: The story cloze test. In\nProceedings of the 2nd Workshop on Linking Models\nof Lexical, Sentential and Discourse-level Semantics,\npages 46\u201351.\nRenkun Ni, Hong-min Chu, Oscar Casta\u00f1eda, Ping-\nyeh Chiang, Christoph Studer, and Tom Gold-\nstein. 2020. Wrapnet: Neural net inference with\nultra-low-resolution arithmetic.\narXiv preprint\narXiv:2007.13242.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh.\n2019.\nFully\nquantized\ntrans-\nformer for improved translation.\narXiv preprint\narXiv:1910.10485.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nShyam A Tailor, Javier Fernandez-Marques, and\nNicholas D Lane. 2021. Degree-quant: Quantization-\naware training for graph neural networks. Interna-\ntional Conference on Learning Representations.\nSandeep Tata and Jignesh M Patel. 2003. Piqa: An\nalgebra for querying protein data sets. In 15th In-\nternational Conference on Scientific and Statistical\nDatabase Management, 2003., pages 141\u2013150. IEEE.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao\nGong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and\nXianglong Liu. 2023. Outlier suppression: Pushing\nthe limit of low-bit transformer language models.\nBigScience Workshop. 2023.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\nXiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi,\nZhewei Yao, and Yuxiong He. 2023. Understanding\nint4 quantization for transformer models: Latency\nspeedup, composability, and failure cases.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. Smoothquant:\nAccurate and efficient post-training quantization for\nlarge language models.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. arXiv\npreprint arXiv:1910.06188.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint\narXiv:2009.12812.\nA\nAppendix\nPerplexity-based Task\nZero-shot Task\nWikiText2 PTB\nC4\nPIQA\nARC-easy ARC-Challenge StoryCloze\nBLOOM fp16\n22.42\n43.69 26.6 65.07%\n41.71%\n24.15%\n61.94%\nRTN\n25.90\n51.10 29.89 63.11%\n39.40%\n23.89%\n60.15%\n560M\nGPTQ\n24.03\n46.97\n28\n64.31%\n40.24%\n23.46%\n61.17%\nEasyQuant\n23.74\n46.86 28.03 63.06%\n40.32%\n24.15%\n59.64%\nBLOOM fp16\n17.69\n57.96 22.05 67.14%\n45.41%\n25.68%\n63.27%\nRTN\n22.00\n66.85 24.44 65.29%\n42.51%\n23.34%\n60.66%\n1.1B\nGPTQ\n19.05\n62.48 23.25 66.05%\n44.49%\n25.51%\n62.32%\nEasyQuant\n18.51\n61.83 22.94 66.65%\n43.73%\n25.51%\n62.06%\nBLOOM fp16\n15.39\n30.00 19.49 69.97%\n48.11%\n26.79 %\n65.44%\nRTN\n16.97\n33.58 21.26 67.74%\n44.70%\n26.45 %\n62.95%\n1.7B\nGPTQ\n16.48\n31.84 20.55 68.77%\n44.49%\n25.94%\n64.48%\nEasyQuant\n16.01\n31.50 20.15 68.99%\n46.89%\n26.19%\n65.37%\nBLOOM fp16\n13.48\n25.34 17.49 70.51%\n53.24%\n30.55 %\n67.79%\nRTN\n14.76\n27.68 18.76 69.86%\n51.35%\n29.52%\n67.09%\n3B\nGPTQ\n14.2\n26.49 18.1 69.42%\n52.82%\n28.92%\n67.22%\nEasyQuant\n14.01\n26.12 17.96 69.80%\n50.72%\n28.58%\n67.35%\nBLOOM fp16\n11.37\n20.83 15.20 73.72%\n57.37%\n33.45 %\n71.99%\nRTN\n12.10\n22.42 16.06 72.69%\n56.14%\n32.17 %\n70.72%\n7.1B\nGPTQ\n11.73\n21.67 15.6 72.96%\n56.14%\n32.25%\n71.36%\nEasyQuant\n11.66\n21.47 15.52 73.23%\n55.72%\n32.51 %\n71.10%\nBLOOM fp16\n8.11\n14.59 11.71 79.16%\n67.47%\n44.97 %\n76.89%\nRTN\n8.37\n15.00 12.04 79.00%\n66.33%\n43.17 %\n76.00%\n176B\nGPTQ\n8.21\n14.75 11.81 79.00%\n67.42%\n44.10%\n76.32%\nEasyQuant\n8.21\n14.75 11.87 79.05%\n67.8%\n44.45%\n77.28%\nTable 10: Perplexity and zershot results for BLOOM model family\n"
  },
  {
    "title": "Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use",
    "link": "https://arxiv.org/pdf/2403.02626.pdf",
    "upvote": "9",
    "text": "Modeling Collaborator: Enabling Subjective Vision Classification With Minimal\nHuman Effort via LLM Tool-Use\nImad Eddine Toubal*, Aditya Avinash*, Neil Gordon Alldrin*, Jan Dlabal*, Wenlei Zhou*,\nEnming Luo*, Otilia Stretcu*, Hao Xiong*, Chun-Ta Lu*, Howard Zhou*,\nRanjay Krishna\u2020, Ariel Fuxman*, Tom Duerig*\nitdfh@umsystem.edu, {adity,nalldrin}@google.com\nAbstract\nFrom content moderation to wildlife conservation, the\nnumber of applications that require models to recognize\nnuanced or subjective visual concepts is growing. Tradi-\ntionally, developing classifiers for such concepts requires\nsubstantial manual effort measured in hours, days, or even\nmonths to identify and annotate data needed for train-\ning.\nEven with recently proposed Agile Modeling tech-\nniques, which enable rapid bootstrapping of image classi-\nfiers, users are still required to spend 30 minutes or more\nof monotonous, repetitive data labeling just to train a sin-\ngle classifier. Drawing on Fiske\u2019s Cognitive Miser theory,\nwe propose a new framework that alleviates manual effort\nby replacing human labeling with natural language inter-\nactions, reducing the total effort required to define a con-\ncept by an order of magnitude: from labeling 2,000 images\nto only 100 plus some natural language interactions. Our\nframework leverages recent advances in foundation mod-\nels, both large language models and vision-language mod-\nels, to carve out the concept space through conversation\nand by automatically labeling training data points. Most\nimportantly, our framework eliminates the need for crowd-\nsourced annotations. Moreover, our framework ultimately\nproduces lightweight classification models that are deploy-\nable in cost-sensitive scenarios. Across 15 subjective con-\ncepts and across 2 public image classification datasets, our\ntrained models outperform traditional Agile Modeling as\nwell as state-of-the-art zero-shot classification models like\nALIGN, CLIP, CuPL, and large visual question answering\nmodels like PaLI-X.\n1. Introduction\nThe field of computer vision has primarily focused on\nrecognizing concepts that are objectively agreed upon, such\n*Google Research\n\u2020University of Washington\nModeling Collaborator (Ours)\nDecision: Positive\nReasons: The image shows a plate of food with chopsticks on \nit. The food is a salad with tuna, pineapple, avocado, and \nsesame seeds. The salad is on a plate with chopsticks on \nit. The dish is gourmet and contains tuna.\nPaLI-X (baseline)\nPrompt: Is this tuna gourmet? \nJustify your answer.\nPaLI: No, this tuna is not \ngourmet. It is a simple salad with \ntuna, sesame seeds, and \nvegetables. It is not a fancy or \nelaborate dish.\nPrompt: Could this be an image of \ngourmet tuna?\nPaLI: Yes, this could be an image \nof gourmet tuna.\nPrompt: Is this tuna gourmet? Are \nyou sure?\nPaLI: I'm not sure if this tuna is \ngourmet. It looks like it could \nbe, but it's hard to say for sure \nwithout tasting it.\nFigure 1.\nWe introduce Modeling Collaborator: a framework\nthat allows anyone to train vision models using natural lan-\nguage interactions and minimal effort.\nWe show that today\u2019s\nbest models (e.g. PaLI-X [6]) change their answers depending on\nthe prompt when classifying subjective concepts like gourmet\ntuna. Meanwhile, Modeling Collaborator uses LLMs and tool-\nuse to train vision models by interacting with users to carve out\nthe concept space.\nas dogs, cats, or cars [11, 28, 30]. Even research on fine-\ngrained recognition (e.g.\n\u201cblack footed albatross\u201d) and\ncompositional concepts (e.g., \u201cred car next to a motorcy-\ncle\u201d) have universal consensus [22, 27, 32, 35]. However,\nmany practical real-world vision applications frequently in-\nvolve recognizing subjective concepts that suffer from sig-\nnificant disagreements amongst individuals. Applications\ninclude predicting emotions, measuring aesthetic appeal, or\ncontent moderation [10, 25, 26, 45]. A content moderator\nneeds a model to identify unsafe content according to their\ndefinition of what constitutes as unsafe; a food critic might\n1\narXiv:2403.02626v1  [cs.CV]  5 Mar 2024\nnot consider a tuna sandwich to be gourmet while others\nmight (Figure 1). To operationalize these applications, we\nneed user-centric training frameworks that enable anyone to\ntrain subjective vision models.\nRecently, Agile Modeling formalized the process for\nturning any visual concept into a vision model through a\nuser-in-the-loop framework [51].\nTheir work concluded\nthat crowd workers struggled to produce labels that were\nconsistent with the user\u2019s concept definition. Instead, they\nproposed an active learning algorithm, where the user itera-\ntively labels a series of training images themselves. Unfor-\ntunately, this process is tedious, repetitive, and labor inten-\nsive; users had to label \u223c 2000 images, which on average\ntook 30 minutes to train a binary classifier.\nExisting processes fall short because they do not lever-\nage a key capability that humans possess. People are adept\nat breaking down complex subjective concepts into more\nmanageable and objective components by applying first-\norder logic [14, 36]. This ability can be explained using\nSusan Fiske\u2019s Cognitive Miser Theory: people decompose\ncomplex work to avoid high cognitive load [13]. People\napply the same process to define complex concepts such\nas \u201cunsafe\u201d and \u201cgourmet\u201d. For instance, one food critic\nmight decompose the subjective concept of \u201cgourmet\u201d as\nimages that need to at least contain \u201ctuna\u201d; if it is \u201cahi tuna\u201d,\nthen it is likely gourmet; if it is \u201ccanned\u201d, then it is un-\nlikely to be gourmet; if the dish is a \u201csandwich\u201d, then it is\nstill not gourmet. This decomposition of the subject con-\ncept \u201cgourmet\u201d into conjunction clauses of objective con-\ncepts \u201cahi tuna\u201d, \u201ccanned\u201d, and \u201csandwich\u201d is a simple non-\nlaborious, cognitively effortless conversion.\nWith this grounding, we deliver Modeling Collabora-\ntor which empowers users to build classifiers while min-\nimizing manual effort.\nInstead of asking users to anno-\ntate thousands of images [51], Modeling Collaborator re-\nquires 100, along with a few natural language interac-\ntions that help decompose subjective concepts into its con-\nstituent sub-components. To enable Modeling Collabora-\ntor, we leverage advancements in large language models\n(LLMs) [2,3,9,12,37] and in particular, their ability to use\nvision-language models (VLMs) [6\u20138] and other tools [19].\nWhen users have a concept in mind and use Modeling Col-\nlaborator, it employs an LLM, which breaks the concept\ninto questions that are digestible for a Visual Question An-\nswering (VQA) model [8]. The LLM then summarizes the\nanswers provided by the VQA model and performs reason-\ning through chain-of-thought [57] to classify new images\nas positive or negative examples of the concept. Users are\nonly asked to manually label a small 100 image validation\nset. Finally, Modeling Collaborator labels a large amount of\nunlabeled images available online and uses it as distillation\ndata to train a light-weight deployment-ready vision model.\nOur method is shown to outperform existing zero-shot\nmethods (CLIP [43], CuPL [41] and PaLI-X [6]), especially\non harder subjective concepts. When compared to the orig-\ninal Agile Modeling [51] our system exceeds the quality\nof crowd-raters on hard concepts while simultaneously re-\nducing the need for manual user-provided ground-truth by\norders of magnitude. By reducing the barriers of manual\neffort and resulting costs needed to develop classification\nmodels, it will empower users to rapidly convert their ideas\ninto reality. This, in turn, has the potential to usher in a new\nwave of end-user applications.\n2. Related work\nOur work draws on advances in VLMs and LLMs and\nprovides an improved solution to the recently introduced\nAgile Modeling problem.\nAgile Modeling. Inspired by agile software development,\nAgile Modeling [51] focuses on rapid development of im-\nage classification models. In addition to speed, Agile Mod-\neling aims to tackle the challenges posed by subjective vi-\nsion models. As classification tasks become more nuanced,\nuser interaction becomes increasingly crucial. However, it\nis important to note that the human-in-the-loop approach\ncan be expensive due to the need of continuous human in-\nvolvement and expertise. While this work aims at reducing\ntime users spend on tuning their classification models, we\npropose an assisted method to automate parts of the pipeline\nand eliminate crowd-rater involvement.\nVision-language models (VLMs). In the rapidly evolving\ndomain of VLMs, two primary streams have emerged: con-\ntrastive and generative models. Contrastive models, such as\nCLIP [43] and ALIGN [23], leverage large-scale datasets\nto directly learn visual concepts from raw text, enabling\nhigh-accuracy zero-shot classification on open vocabular-\nies [11, 17]. Generative models such as PaLI [6\u20138, 56] and\nGPT-V [37, 38] focus on generating text from a combina-\ntion of visual and text inputs. For instance, PaLI, trained on\na vast collection of image-text pairs in various languages,\nachieves top performance across a range of vision and lan-\nguage tasks.\nSimilarly, GPT-V allows the processing of\nimage inputs, thereby enhancing the applicability of lan-\nguage models to multimodal tasks. Other methods such as\nCoCa [54, 63] proposed a hybrid approach for simultane-\nously learning with generative and contrastive objectives.\nDespite their strength, VLMs capture visual data semantics,\noften prioritizing salient image features over nuanced vi-\nsual cues. For instance, CLIP embeddings are intentionally\ncompressed to encapsulate its most prominent subject [49].\nAdditionally, PaLI may struggle to provide detailed descrip-\ntions of complex scenes with numerous objects, as its train-\ning data predominantly lacks detailed annotations. In con-\ntrast, our proposed method is more stable and less sensitive\nto question phrasing as observed in Fig. 1.\nLarge language models (LLMs) and tool-use. Large Lan-\n2\nPositive Questions\n-\nDoes the dish contain \ntuna?\n-\nIs the dish gourmet?\n-\nIs the tuna in the \nimage a gourmet tuna?\n-\nIs the dish a photo of \nthe fish tuna itself?\nNegative Questions\n-\nNone\nPositive Questions\n-\nYes\n-\nYes\n-\nYes\n-\nNo\nNegative Questions\n-\nN/A\nInput Image\nA. LLM Questions\nB. VQA Answers\nThe image shows two \nbowls of food. One \nbowl contains \nnoodles, lettuce, \nand avocado. The \nother bowl contains \ntuna, lettuce, and \navocado.\nC. Captioning\nName: Gourmet Tuna\nDescription: Photos of gourmet dishes (i.e. fancy, elegant) that must contain tuna. This includes sushi, sashimi, \nseared tuna, a fancy ahi tuna salad. This does not include canned tuna, tuna sandwich, a photo of the fish tuna itself.\nConcept\nLLM Chain-of- \nthought reasoning\nDecision: Positive\nReasons: The image \ncontains tuna and \nis gourmet.\nD. Final\nAnnotation\n\u2728\nFigure 2. Modeling Collaborator Annotator system. For a given image, concept name, and description, the Annotator outputs a positive or\nnegative label. Based on the name and description of the concept, the LLM generates relevant atomic questions to ask a VQA model (PaLI\nVQA in our case) (step A). These questions are fed into the VQA model that typically outputs a yes/no short answer (Step B). Additionally,\nwe use a captioning version of PaLI (Step C) to generate a detailed description capturing as much detail as possible from the image. Finally,\nthe LLM goes through a chain-of-thought reasoning process to output a decision and rationale (Step D).\nguage Models (LLMs) have revolutionized the landscape of\nartificial intelligence [1,3,12,40,55], particularly in the field\nof natural language processing (NLP) and cognitive reason-\ning. By leveraging advanced methodologies such as chain-\nof-thought reasoning [57], few-shot learning [4, 39], and\ntool-use [21,46], these models demonstrate exceptional per-\nformance across a wide spectrum of downstream tasks [44].\nThey can operate across various modalities and a broad\nrange of applications while maintaining high performance\nwithout the need for additional training. Recent progress\nin integrating external tools with LLMs [5, 20, 21, 29, 62]\nhas yielded systems like Toolformer [46]. This approach\nmakes intelligent decisions about which APIs to invoke,\noptimizing the timing, arguments passed, and the subse-\nquent assimilation of the results into future token predic-\ntions. This enhances zero-shot performance across a variety\nof tasks, establishing a solid foundation for LLMs to operate\nbeyond their inherent capabilities. For fine-grained VQA,\nAVIS [20] introduces an autonomous information-seeking\nmechanism. By dynamically leveraging an LLM in tan-\ndem with external tools, it adeptly traverses a combinatorial\nsearch space. This is achieved through its unique approach\nof mimicking human decision-making processes, crafting a\ntransition graph that guides the LLM\u2019s strategic decisions.\nAnother tool-use enabled LLM system is ViperGPT [52],\nwhich embodies an innovative approach to tackling visual\nqueries.\nIt leverages a code-generation strategy that en-\nables the seamless integration of vision-and-language mod-\nels through the generation of Python code. This method,\nalong with other similar methods (MMReact [61], Hug-\ngingGPT [50], Chameleon [34], and Visual ChatGPT [58])\ncircumvents the need for extended training and ensures re-\nsilience across a diverse set of visual tasks. Collectively,\nthese systems highlight the burgeoning synergy between\nLLMs and external tool use, pushing the frontiers of what\nLLMs can achieve. In our work, we adopt and extend ideas\nfrom these approaches to tackle the problem of subjective\nimage classification.\nCustomized prompts via language models. Customized\nPrompts via Language models (CuPL) [41] leverages\nCLIP\u2019s capabilities [43] to achieve zero-shot image classifi-\ncation. CuPL measures the similarity between an image and\neach visual class to perform classification. Typically, the\nclasses are passed into CLIP\u2019s text encoder within a tem-\nplate such as \u201cphoto of a bird\u201d for the class bird. CuPL em-\nploys GPT [3] to generate more comprehensive text descrip-\ntions for each class before feeding into CLIP. This straight-\nforward and zero-shot approach yields improved accuracy\nacross various zero-shot image classification benchmarks.\nHowever, its evaluation has been limited to objective classi-\nfication tasks and not on nuanced or subjective visual clas-\nsification tasks. This approach for automatically annotating\ndata improves upon CLIP but suffers from the same limita-\ntions compared to our work.\n3. Method\nWe propose an end-to-end system that streamlines the\ndevelopment of classifiers for nuanced visual concepts, ad-\ndressing the limitations of traditional classifier development\nmethods. The system consists of three core components, de-\nscribed in detail in the following subsections: (a) data min-\n3\ning, (b) annotation, (c) model training with active learning.\nTo build a classifier for a new concept, the user first pro-\nvides a concept name and an optional description. The sys-\ntem then automatically mines images relevant to the con-\ncept and annotates them using a mixture of Large Lan-\nguage Models (LLM), Vision-Language Models (VLM),\nand Visual-Question-Answering (VQA) models. The anno-\ntated images are used to train a basis classification model,\nwhich is further refined through multiple rounds of active\nlearning, resulting in a highly accurate classifier.\nThis setup mirrors the workflow of traditional classifier\ndevelopment, but it eliminates the need for costly and time-\nconsuming human annotation which is a significant bottle-\nneck in traditional methods. The Modeling Collaborator\nAnnotator component, powered by LLMs and VLMs, en-\nables zero-shot image labeling and drastically minimizes\nour dependence on user annotations.\n3.1. Data mining\nMining quality data for training has traditionally been a\nlabor-intensive process. This process begins with the clear\ndefinition of a concept, followed by the hunt for relevant\nimages, and ends in the manual annotation of each of these\nimages [11,30]. Particularly for nuanced visual tasks, there\nis a possibility that certain subtle visual patterns might be\noverlooked during data collection. Consequently, to ensure\na comprehensive capture of all visual patterns, multiple it-\nerations of refinement may be needed. In traditional Ag-\nile Modeling [51] this challenge is addressed by soliciting\nusers to annotate data or generate new search queries to find\nmore image examples. Each query results in a new seman-\ntic image search algorithm [23, 43] to gather other similar\npositive image examples for annotation from the public do-\nmain (LAION Dataset) [47]. Even with user intervention,\nuser queries may overlook essential cues, potentially lead-\ning to a deficit of hard negatives or a lack of coverage in\nspecific visual modes. Additionally, the labels can vary be-\ntween users, leading to potential human biases.\nTo address human bias and minimize manual effort, we\npropose a data mining algorithm based on LLM chain-of-\nthought reasoning. While LLMs are not inherently unbi-\nased [15] and may reflect biases present in their training\ndata, they can assess a wider range of concepts at large\nscales from their extensive knowledge base, thus identify-\ning a broader array of potential examples more efficiently.\nFirst, we prompt the LLM to generate multiple positive and\nnegative queries based on a concept\u2019s name and its descrip-\ntion. Note that we do not directly assign images as positive\nor negative based on the query; rather, the goal is obtain rep-\nresentative images spanning both positive and hard-negative\nexamples. To increase coverage and diversity, we expand\nthe queries by instructing the LLM to apply various mu-\ntations. For example, we may ask the LLM to iteratively\ncome up with broader or narrower versions of the queries,\nor come up with variations for specific parts of the queries.\nDrawing parallels to Agile Modeling, we use each query to\nextract image samples from the public domain [47].\n3.2. Modeling Collaborator Annotator\nFig. 2 describes the image annotation process. Our sys-\ntem effectively orchestrates the annotation process leverag-\ning LLM\u2019s ability to invoke VLMs and other tools. It com-\nprises three primary AI-driven modules: an LLM, a Cap-\ntioning VLM [56], and a VQA VLM [6]. The automated\nannotation process is structured as follows:\nConcept initialization: Initially, our system receives a con-\ncept name (e.g., gourmet tuna), and optionally a con-\ncept description. If a concept description is absent, the LLM\ngenerates an initial description. This template can be modi-\nfied by the user to cover all specifications and carve-outs.\nAttribute Extraction: Based on the concept specifications,\nthe LLM identifies objective attributes associated with the\nconcept, such as \u201cimage contains tuna\u201d, \u201cis tuna sandwich\u201d,\nand \u201cis tuna steak\u201d.\nAttribute decomposition: The LLM decomposes complex\nattributes into more granular and atomic attributes.\nQuestion generation: The LLM then formulates a series\nof questions tailored for the VQA model. Examples include\n\u201cdoes the image contain food\u201d, \u201cis the food tuna\u201d, and \u201cis it\ntuna steak\u201d.\nVisual assessment: When an image is input, the VQA\nmodel processes these questions, yielding concise answers\nfor each. Concurrently, the Captioning VLM provides a\ncomprehensive description of the image.\nFinal annotation: With the textual data from the VLMs and\nthe user\u2019s initial concept specification, the LLM employs\nchain-of-thought reasoning. It annotates the image as either\npositive or negative, also offering insights into its decision-\nmaking process.\nOur approach utilizes the strengths of VLM, VQA, and\nLLM models while simultaneously avoiding their short-\ncomings. For example, VLMs, despite their capabilities, of-\nten struggle with nuanced and subjective concepts in classi-\nfication tasks. Their performance hinges on the breadth and\nquality of training data, potentially leading to biases or gaps\nin understanding [53].\nAmbiguities in language and the\ninherent subjectivity of certain questions can further chal-\nlenge their accuracy [33]. Moreover, these models, lacking\nreal-world context and experiential understanding, might\nmiss deeper cultural or emotional nuances [16]. Thus, while\npowerful, VLMs have inherent limitations in addressing in-\ntricate or subjective visual-linguistic tasks. Fig. 1 shows an\nexample VLMs\u2019 (PaLI-X [6]) sensitivity to prompts.\nVLMs are primarily designed for understanding and an-\nswering questions related to visual content, rather than\nperforming deep chain-of-thought reasoning typical of ad-\n4\nDecision: Positive\nReasons: ... The dish is \ngourmet because it is made \nwith high-quality \ningredients and is prepared \nin a sophisticated manner.\nDecision: Negative\nReasons: The image shows a \nplate of stuffed \ntomatoes... The dish is \ngourmet and contains tuna, \nbut it is not a photo of \nthe fish tuna itself.\nDecision: Negative\nReasons: The image shows a \npiece of smoked salmon with \na filling of cream cheese \nand chives on a plate. The \ndish does not contain tuna.\nDecision: Negative\nReasons: The image shows a \nwatermelon salad with feta \ncheese and red onions. The \ndish is gourmet, but it \ndoes not contain tuna.\nDecision: Positive\nReasons: The image shows a \nplate of sushi on a wooden \ntable. The sushi roll contains \ntuna and rice. The dish is \ngourmet and the tuna is a \ngourmet tuna.\nDecision: Negative\nReasons: The object in the \nimage is not a stop sign. It \nis a stop sign drawn on a \npiece of paper.\nDecision: Negative\nReasons: The image shows a \ntraffic cone, not a stop \nsign. The traffic cone is \nnot in the real world, it \nis a toy.\nDecision: Positive\nReasons: The image \ncontains a stop sign in \nthe real world. The stop \nsign is official.\nDecision: Negative\nReasons: The stop sign is \nnot in the real world. It \nis a drawing of a stop \nsign.\nDecision: Negative\nReasons: The object in the \nimage is a pillow, not a \nstop sign...\nFigure 3. Modeling Collaborator Annotator examples for the concepts gourmet tuna (first row) and stop sign (second row). Hard\nnegatives mined from the LAION dataset are shown in addition to some actual positives for the visual concept. The Modeling Collaborator\nAnnotator is able to label the images as positive or negative as well as provide rationale. In some instances, the rationale could be incorrect\n(highlighted in red) due to error in VQA responses or hallucinations from the LLMs. Some of the reasons have been truncated for brevity.\nvanced LLMs [33,42,53,59]. While VLMs can comprehend\nsimpler questions about images, they usually operate in a\nsingle-shot manner, providing answers based on the imme-\ndiate visual and textual inputs without extended reasoning.\nOn the other hand, LLM question answering quality can be\nsignificantly improved through chain-of-thought reasoning,\nmaintaining a coherent line of thought across extended text.\nOther techniques such as prompt chaining involve using a\nmodel\u2019s output as part of the subsequent input, simulating\na sustained dialogue or iterative reasoning. Additionally,\nto extract deeper insights, users can guide LLMs with spe-\ncific instructions, such as asking the model to think step-by-\nstep [60] or weigh pros and cons, thus simulating a more\ndeliberate reasoning process [3].\n3.3. Training and active learning\nWhile one could directly use the annotator as a model,\nthis is prohibitive in many scenarios because of the high\ninference cost.\nFor this reason, we adopt an approach\nsimilar to [51] for model training and active learning.\nSpecifically, we first extract image features from a foun-\ndation vision model (CLIP or ALIGN) [23, 24]. We then\ntrain a shallow multi-layer perceptron (MLP) with layer\nsizes (128, 128, 128) to perform binary classification for the\ngiven concept. This can also be viewed as student-teacher\ndistillation [18] where we use the LLM-based annotator as\nthe teacher model. We use a learning rate of 3 \u00d7 10\u22124, a\nbatch size of 512, and optimize using AdamW [31].\nAfter the initial model is trained, we perform multiple\nrounds of active learning.\nEach active-learning iteration\nconsists of three stages. First, the lightweight classifica-\ntion model is applied to a large database of unlabeled im-\nages (LAION [47]). Then, we perform stratified sampling\nto acquire candidate images for further AL rounds [51]. The\nintention is to capture hard negatives and hard positives that\nwill boost precision and recall respectively. Second, our\nLLM-based annotator is autonomously applied to the se-\nlected images, providing additional training ground-truth.\nThirdly, the student classifier is retrained, leveraging all the\nextant labeled data. We experiment with both margin sam-\npling and stratified sampling techniques [48] to mine ex-\namples during this active learning phase. The overall sys-\ntem thus adeptly balances between exploration (achieved\nvia data mining through text search queries and expansion)\nand exploitation (achieved via active learning to mine visual\nmodes that reduce model uncertainties).\n3.4. Implementation details\nAs a large language model, we use PaLM 2 [2,9] which\nwas trained on a variety of different tasks, all of which helps\nPaLM 2 learn different aspects of language. Additionally,\nwe use both the VQA and MMIT (multimodal instruction-\ntuned [56]) variants of PaLI-X [6]. The particular choice\nof foundation models is based on their SOTA performance\nat the time of writing. These models have not been further\ntrained or fine-tuned in this work.\n5\n4. Experiments\nWe present our experimental setup and results with three\ntakeaways. First, we show that Modeling Collaborator An-\nnotator outperforms other zero-shot methods (CLIP [43],\nCuPL [41] and PaLI-X [6]). Second, while Modeling Col-\nlaborator Annotator is able to beat state-of-the-art methods\nin both easy and hard concepts, we see much larger gains on\nharder and more subjective concepts. Finally, when using\nour end to end system, we can produce deployable models\nof competitive quality with minimal user annotations (100\nannotations vs. 2,000 in traditional Agile Modeling).\nDatasets. In addition to the LAION dataset used for data\nmining in our system, we evaluate our methods on the pub-\nlic Hateful Memes dataset [26]. For evaluation and user-\nstudy, we use the Agile Modeling dataset [51] that is com-\nprised of 14 concepts, each with positive and negative im-\nages mined from the LAION dataset. This dataset is split\ninto easy and hard concepts depending on the zero-shot per-\nformance on each concept using CLIP as described in [51].\nModels. We benchmark Modeling Collaborator Annotator\nagainst state-of-the-art zero-shot and open-vocabulary clas-\nsifiers: CLIP [43], CuPL [41], and PaLI-X (55B) [6] as a\ngenerative VQA model. We evaluate CLIP by embedding\nthe name of the concept and measuring the cosine similarity\nto each image embedding. CuPL uses the same technique\nbut instead of embedding the concept name directly, we em-\nbed a description of the concept generated by an LLM. Both\nGPT3 and PaLM 2 models were experimented with but we\nchose PaLM 2 since it produced superior results. In the case\nof CLIP and CuPL, we select an operating point using a grid\nsearch maximizing the F1 score on a subset of the training\nset. We use PaLI-X VQA variant as a classifier by prompt-\ning it \u201cIs this an image of X?\u201d and we assign a positive or\nnegative prediction based on its answer.\nAnnotator Adaptation. While testing the system, we ob-\nserved some amount of concept-dependent variability in the\nAnnotator. For example, for simple concepts like \u201ccat\u201d a\nVLM might already have state-of-the-art performance and\nour system can even degrade quality in these cases. To ad-\ndress this we implemented six different Annotator strate-\ngies. While developing a classifier for a particular concept,\nwe have the concept owner build an on-the-fly validation set\nof 100 images which is then used to select the best perform-\ning strategy for that particular concept. Different param-\neters describing these configurations are explained in the\nAppendix.\nUsers, Crowd, and Modeling Collaborator. We measure\nthe agreement/alignment with the user for both the crowd\nand automatic annotation methods. The user is the source\nof ground-truth and the person manually annotating the test\nset. Crowd annotators are given a description and exam-\nples by the user and asked to annotate images at a larger\nscale. Modeling Collaborator Annotator is able to scale\nup the annotation process further due to its autonomy and\ncan encapsulate an image set of higher diversity in visual\nmodes. We measure the annotator alignment by comparing\nthe performance (auPR) on the distilled model trained on\ndata annotated by different human and machine annotators.\n4.1. Modeling Collaborator Annotator\nModeling Collaborator Annotator outperforms other\nzero-shot methods. We show the results of these experi-\nments in Tab. 1. We measure the alignment with the user\non the held-out test set of the Agile Modeling dataset us-\ning agreement scores (precision, recall, and F1). CLIP and\nCuPL contrastive models suffer from very low precision in\nfavor of high recall. PaLI-X outperforms contrastive mod-\nels, making it more suitable as a baseline for our proposed\nAnnotator. We achieve significant gains for subjective\n(hard) concepts while maintaining equivalent perfor-\nmance for less subjective (easy) concepts. Tab. 1 shows\na significant skew in concept improvement: over 25% of\nthe concepts showed an F1 score gain of 4% or higher, in-\ncluding hateful memes [26] at 15%, healthy-dish\nat 6%, and stop-sign at 5%, exhibiting substantial im-\nprovements in areas requiring more subjective classifica-\ntions. This trend indicates that our model is particularly\neffective for complex or subjective concepts, but may of-\nfer only marginal benefits for concepts that PaLI-X is al-\nready good at. Regardless, a Wilcoxon Signed-Rank Test on\nthe F1 scores comparing our system against PaLI-X yields\na statistically significant improvement across all concepts\n(p < 0.01). In addition to classification, our system outputs\nrationales shown in Fig. 3.\n4.2. Human-machine alignment\nModeling Collaborator can produce deployable models\nof competitive quality with minimal user annotations.\nWe measure the effect of using varying levels of human and\nautomated annotation in Tab. 2. We note that, while our\nmodel cannot exceed the distilled user model performance\n(distilled on 100% accurate annotations), we can outper-\nform crowd-raters. Our Annotator system significantly out-\nperforms crowd-raters on harder more nuanced concepts\n(different of 6%). Whereas it slightly under-performs on\neasy concepts. This is likely due to prediction errors from\nautomated VQA models (PaLI-X) where humans show bet-\nter performance. In comparison to using other state-of-the-\nart open-vocabulary zero-shot annotators (CLIP, CuPL and\nPaLI-X), our system outperforms these methods on both\neasy and hard concepts. Our fully automated system suc-\ncessfully generates distilled models that match the quality\nof ones crafted with classical Agile Modeling, with per-\nformance within a 2% margin of the user\u2019s output. Fig. 4\nshows that both crowd-annotators and Modeling Collabo-\nrator Annotator can improve the performance of the dis-\n6\nPaLI-X [6]\nCLIP [43]\nCuPL [41]\nOurs\nConcept\nPre\nRec\nF1\nPre\nRec\nF1\nPre\nRec\nF1\nPre\nRec\nF1\nEasy concepts\narts-and-crafts\n0.71\n0.97\n0.82\n0.68\n0.86\n0.76\n0.68\n0.90\n0.77\n0.96\n0.75\n0.84\ndance\n0.57\n0.87\n0.69\n0.51\n0.95\n0.66\n0.52\n0.89\n0.66\n0.67\n0.95\n0.79\nemergency-service\n0.67\n0.88\n0.76\n0.53\n0.87\n0.65\n0.54\n0.91\n0.67\n0.88\n0.73\n0.76\nhair-coloring\n0.76\n0.97\n0.85\n0.70\n0.99\n0.82\n0.70\n0.99\n0.82\n0.76\n0.97\n0.85\nin-ear-headphones\n0.70\n0.96\n0.81\n0.43\n0.95\n0.59\n0.44\n0.96\n0.60\n0.82\n0.86\n0.82\npie-chart\n0.80\n0.96\n0.88\n0.52\n0.80\n0.63\n0.50\n0.92\n0.65\n0.80\n0.96\n0.88\nsingle-sneaker\n0.65\n0.92\n0.76\n0.51\n0.99\n0.67\n0.51\n1.00\n0.67\n0.70\n0.88\n0.78\nEasy concepts average\n0.69\n0.93\n0.80\n0.55\n0.92\n0.68\n0.56\n0.94\n0.69\n0.80\n0.87\n0.82\n\u2206\n+11%\n-6%\n+2%\nHard concepts\nastronaut\n0.61\n0.87\n0.71\n0.40\n0.95\n0.56\n0.42\n0.95\n0.58\n0.72\n0.79\n0.72\nblock-tower\n0.45\n0.97\n0.62\n0.38\n0.99\n0.55\n0.37\n0.98\n0.54\n0.89\n0.68\n0.66\ngourmet-tuna\n0.52\n0.95\n0.67\n0.29\n1.00\n0.45\n0.29\n1.00\n0.45\n0.52\n0.95\n0.67\nhand-pointing\n0.56\n0.99\n0.71\n0.39\n0.87\n0.54\n0.39\n0.94\n0.55\n0.89\n0.79\n0.74\nhealthy-dish\n0.38\n1.00\n0.55\n0.37\n0.99\n0.54\n0.38\n1.00\n0.55\n0.84\n0.61\n0.61\nhome-fragrance\n0.57\n0.51\n0.54\n0.40\n0.95\n0.56\n0.40\n0.96\n0.57\n0.57\n0.51\n0.54\nstop-sign\n0.61\n0.99\n0.76\n0.48\n1.00\n0.65\n0.49\n0.99\n0.65\n0.83\n0.83\n0.81\nHard concepts average\n0.53\n0.90\n0.65\n0.39\n0.96\n0.55\n0.39\n0.97\n0.56\n0.75\n0.74\n0.68\n\u2206\n+22%\n-16%\n+3%\nOverall average\n0.61\n0.92\n0.72\n0.47\n0.94\n0.62\n0.47\n0.96\n0.62\n0.78\n0.79\n0.74\n\u2206\n+17%\n-13%\n+2%\nHateful memes [26]\n0.66\n0.42\n0.51\n0.49\n0.98\n0.66\n0.50\n0.87\n0.64\n0.58\n0.77\n0.66\n\u2206\n-8%\n+35%\n+15%\nTable 1. Teacher performance (Precision, Recall, and F1 scores). Modeling Collaborator outperforms state-of-the-art zero-shot methods\nincluding CLIP, CuPL, and visual query answering models (PaLI-X). Underlined results represent the baseline (PaLI-X) with which our\nperformance is compared to (deltas). We bold the best precision, recall, and F1 for easy concepts, hard concepts and Hateful memes\ndataset.\n0\n1k\n2k\n3k\n4k\n5k\n6k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPerformance\nPrecision\nCrowd Annotators\nModeling Copilot\n0\n1k\n2k\n3k\n4k\n5k\n6k\nRecall\n0\n1k\n2k\n3k\n4k\n5k\n6k\nF1\n0\n1k\n2k\n3k\n4k\n5k\n6k\nauPR\nNumber Crowd-rateddata (Additional 100 expert-rated data was used)\nFigure 4. Comparing the contribution of increasingly more training examples annotated by crowd-annotators vs. Modeling Collaborator\nAnnotator (fully automated). The y-axis shows the performance of the final distilled model. When user feedback is minimal (100 anno-\ntated examples), more crowd-annotators examples improve the final distilled model despite the noisy prediction. Modeling Collaborator\nAnnotator provides similar improvement of performance without any human interactions and can be scaled better to annotate a lot more\nexamples due to its autonomy.\ntilled model, even when user feedback is minimal. How-\never, Modeling Collaborator Annotator has the advantage\nof being fully automated and can scale to a larger number\nof examples.\nModeling Collaborator and other zero-shot and classical\nmethods fail in complex visual tasks that require com-\nplex understanding and reasoning. The effectiveness of\nour method on identifying hateful memes [26], as demon-\nstrated in Tab. 3, is further highlighted by its ability to match\nfully-trained models without relying on labeled data. Both\n7\nHuman Annotators\nMachine Annotators\nConcept\nUser\nCrowd\nCrowd\nCuPL\nPaLI-X\nOurs\nDataset size (per concept)\n\u223c600\n\u223c600\n\u223c3000\n\u223c3000\n\u223c3000\n\u223c3000\nEasy concepts\narts-and-crafts\n0.77\n0.73\n0.86\n0.78\n0.77\n0.78\ndance\n0.69\n0.70\n0.81\n0.72\n0.68\n0.68\nemergency-service\n0.75\n0.71\n0.78\n0.59\n0.66\n0.72\nhair-coloring\n0.85\n0.85\n0.83\n0.77\n0.58\n0.80\nin-ear-headphones\n0.73\n0.66\n0.67\n0.65\n0.73\n0.72\npie-chart\n0.77\n0.76\n0.76\n0.72\n0.82\n0.82\nsingle-sneaker\n0.74\n0.64\n0.68\n0.51\n0.61\n0.56\nEasy concepts average\n0.76\n0.72\n0.77\n0.68\n0.69\n0.73 (+1%)\nHard concepts\nastronaut\n0.67\n0.71\n0.66\n0.60\n0.65\n0.65\nblock-tower\n0.59\n0.58\n0.45\n0.48\n0.49\n0.50\ngourmet-tuna\n0.50\n0.51\n0.35\n0.54\n0.52\n0.52\nhand-pointing\n0.50\n0.56\n0.58\n0.56\n0.81\n0.81\nhealthy-dish\n0.59\n0.49\n0.47\n0.42\n0.45\n0.53\nhome-fragrance\n0.62\n0.60\n0.69\n0.56\n0.53\n0.53\nstop-sign\n0.70\n0.57\n0.55\n0.62\n0.51\n0.64\nHard concepts average\n0.60\n0.57\n0.54\n0.54\n0.57\n0.60 (+3%)\nOverall average\n0.68\n0.65\n0.65\n0.61\n0.63\n0.66 (+1%)\nTable 2. Quality comparison of different annotators (or teacher models) using the final distilled model performance (auPR). Concept\nowners provide the highest quality annotations because of their deep understanding of the nuanced concept. Modeling Collaborator\nannotator provides better quality labels compared with labor-intensive annotations from crowd raters, and compared to other automated\nmethods.\nMethod\nLabeler\n# Ex.\nF1\nAcc\nPre\nRec\nOurs (Teacher)\n-\n-\n0.66\n0.61\n0.58\n0.77\nCLIP [43]\n-\n-\n0.57\n0.53\n0.51\n0.65\nCuPL [41]\n-\n-\n0.51\n0.64\n0.50\n0.87\nPaLI-X [6]\n-\n-\n0.51\n0.61\n0.66\n0.42\nOurs (Student)\nMC\n7K\n0.56\n0.52\n0.50\n0.64\nCLIP+MLP\nHuman\n8.5K\n0.48\n0.60\n0.65\n0.38\nTable 3. Performance of our method (both Annotator and distilled\nmodels) on the Hateful Memes [26] public dataset. Zero-shot and\nVQA methods are used for comparison.\nthe teacher and student models outperform the traditional\ntraining approach without using any of the training datasets.\nHowever, the performance is still low, demonstrating the\nlimitations of our approach.\n5. Limitations\nAs our system is an orchestration of LLMs and VLMs,\nit can suffer from some of the limitations of its atomic com-\nponents (PaLM 2, PaLI-X, and CLIP). For example, we ob-\nserved that providing verbose and overly-complex descrip-\ntions of simple concepts (cats, dogs, etc.) can actually de-\ngrade performance in comparison to simply using PaLI-X.\nAnother issue is that for certain concepts, the CLIP features\ncan lead to poor distilled model quality. One example is\nstop sign (where the stop sign is expected to be a real\nstop sign in traffic), where the CLIP feature could capture\nthe overall semantics of stop signs, but could not easily dis-\ncriminate between physical instances vs depictions.\n6. Conclusion\nIn this paper, we presented Modeling Collaborator, a\nnovel framework that alleviates the manual effort required\nto develop classifiers for subjective and nuanced visual con-\ncepts. Our framework leverages advancements in large lan-\nguage models (LLMs) and vision-language models (VLMs)\nto carve out the concept space through conversation and by\nautomatically labeling training data points. We demonstrate\nthe effectiveness of our framework through a set of exper-\niments, showing that it can quickly build visual classifiers\nfor nuanced concepts and outperform both traditional Agile\nModeling and state-of-the-art zero-shot classification mod-\nels. Our work has the potential to significantly reduce the\ntime and effort required to develop classifiers for a wide\nrange of applications including content moderation and aes-\nthetic classification.\n8\nReferences\n[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane\nDebbah, Etienne Goffinet, Daniel Heslow, Julien Launay,\nQuentin Malartic, Badreddine Noune, Baptiste Pannier, and\nGuilherme Penedo.\nFalcon-40B: an open large language\nmodel with state-of-the-art performance. 2023. 3\n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2,\n5, 11\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS, 33:1877\u2013\n1901, 2020. 2, 3, 5\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS, 33:1877\u2013\n1901, 2020. 3\n[5] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Ze-\nfan Cai, Yuchi Wang, Tianyu Liu, and Baobao Chang. To-\nwards end-to-end embodied decision making with multi-\nmodal large language model: Explorations with gpt4-vision\nand beyond. In NeurIPS 2023 Foundation Models for Deci-\nsion Making Workshop, 2023. 3\n[6] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 1, 2, 4, 5, 6, 7, 8\n[7] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian\nGoodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.\nPali-3 vision language models: Smaller, faster, stronger.\narXiv preprint arXiv:2310.09199, 2023. 2\n[8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-\nscaled multilingual language-image model. In ICLR, 2022.\n2\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm:\nScaling language modeling\nwith pathways.\nJournal of Machine Learning Research,\n24(240):1\u2013113, 2023. 2, 5\n[10] Tee Connie, Mundher Al-Shabi, and Michael Goh. Smart\ncontent recognition from images using a mixture of convo-\nlutional neural networks. In IT Convergence and Security\n2017: Volume 1, pages 11\u201318. Springer, 2017. 1\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255, 2009. 1, 2, 4\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, pages\n4171\u20134186, 2019. 2, 3\n[13] Susan T Fiske and Shelley E Taylor.\nSocial cognition.\nMcgraw-Hill Book Company, 1991. 2\n[14] Gottlob Frege et al.\nBegriffsschrift, a formula language,\nmodeled upon that of arithmetic, for pure thought.\nFrom\nFrege to G\u00a8odel:\nA source book in mathematical logic,\n1931:1\u201382, 1879. 2\n[15] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab\nTanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi\nZhang, and Nesreen K Ahmed. Bias and fairness in large lan-\nguage models: A survey. arXiv preprint arXiv:2309.00770,\n2023. 4\n[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In CVPR, pages 6904\u20136913, 2017. 4\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\npages 770\u2013778, 2016. 2\n[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 5\n[19] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa\nFujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna,\nand Tomas Pfister.\nTool documentation enables zero-shot\ntool-usage with large language models.\narXiv preprint\narXiv:2308.00675, 2023. 2\n[20] Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou\nSun, David A Ross, Cordelia Schmid, and Alireza Fathi.\nAvis: Autonomous visual information seeking with large\nlanguage models. arXiv preprint arXiv:2306.08129, 2023.\n3\n[21] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei\nChang, Yizhou Sun, Cordelia Schmid, David A Ross, and\nAlireza Fathi. Reveal: Retrieval-augmented visual-language\npre-training with multi-source multimodal knowledge mem-\nory. In CVPR, pages 23369\u201323379, 2023. 3\n[22] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos\nNiebles. Action genome: Actions as compositions of spatio-\ntemporal scene graphs. In CVPR, pages 10236\u201310247, 2020.\n1\n[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, pages 4904\u2013\n4916, 2021. 2, 4, 5\n[24] Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Alek-\nsei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, An-\ndrew Tomkins, and Sujith Ravi.\nGraph-rise:\nGraph-\nregularized image semantic embedding.\narXiv preprint\narXiv:1902.10814, 2019. 5\n[25] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude\nOliva. Understanding and predicting image memorability at\na large scale. ICCV, pages 2390\u20132398, 2015. 1\n9\n[26] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and Davide\nTestuggine. The hateful memes challenge: Detecting hate\nspeech in multimodal memes.\nNeurIPS, 33:2611\u20132624,\n2020. 1, 6, 7, 8\n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 123(1):32\u201373, 2017. 1\n[28] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4. ICCV, 128:1956\u20131981, 2020. 1\n[29] Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, and Min\nZhang. Lmeye: An interactive perception network for large\nlanguage models. arXiv preprint arXiv:2305.03701, 2023. 3\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755, 2014. 1, 4\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2018. 5\n[32] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-\nFei. Visual relationship detection with language priors. In\nECCV, pages 852\u2013869, 2016. 1\n[33] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nHierarchical question-image co-attention for visual question\nanswering. NeurIPS, 29, 2016. 4, 5\n[34] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon:\nPlug-and-play compositional reasoning with\nlarge language models. NeurIPS, 36, 2024. 3\n[35] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-language\nfoundation models reason compositionally? In CVPR, pages\n10910\u201310921, 2023. 1\n[36] George A Miller. The magical number seven, plus or minus\ntwo: Some limits on our capacity for processing information.\nPsychological review, 63(2):81, 1956. 2\n[37] OpenAI. Gpt-4 technical report, 2023. 2\n[38] OpenAI.\nGpt-4v(ision) system card.\nhttps://cdn.\nopenai.com/papers/GPTV_System_Card.pdf,\n2023. Accessed: 2023-11-15. 2\n[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nNeurIPS, 35:27730\u201327744, 2022. 3\n[40] Guilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Lau-\nnay. The RefinedWeb dataset for Falcon LLM: outperform-\ning curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023. 3\n[41] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What\ndoes a platypus look like? generating customized prompts\nfor zero-shot image classification.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15691\u201315701, 2023. 2, 3, 6, 7, 8\n[42] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. Measuring and narrowing\nthe compositionality gap in language models. arXiv preprint\narXiv:2210.03350, 2022. 5\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763, 2021. 2, 3, 4, 6, 7, 8,\n12\n[44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1:9, 2019. 3\n[45] Arpita Roy, Anamika Paul, Hamed Pirsiavash, and Shimei\nPan.\nAutomated detection of substance use-related social\nmedia posts based on image and text analysis. In 2017 IEEE\n29th International Conference on Tools with Artificial Intel-\nligence (ICTAI), pages 772\u2013779. IEEE, 2017. 1\n[46] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language mod-\nels can teach themselves to use tools.\narXiv preprint\narXiv:2302.04761, 2023. 3\n[47] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeurIPS, 35:25278\u2013\n25294, 2022. 4, 5, 11\n[48] Burr Settles. Active learning literature survey. 2009. 5\n[49] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal,\nAnna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt\nKeutzer.\nHow much can clip benefit vision-and-language\ntasks? In ICLR, 2021. 2\n[50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai\ntasks with chatgpt and its friends in hugging face. NeurIPS,\n36, 2024. 3\n[51] Otilia Stretcu, Edward Vendrow, Kenji Hata, Krishnamurthy\nViswanathan, Vittorio Ferrari, Sasan Tavakkol, Wenlei Zhou,\nAditya Avinash, Enming Luo, Neil Gordon Alldrin, et al.\nAgile modeling: Image classification with domain experts in\nthe loop. ICCV, 2023. 2, 4, 5, 6, 16\n[52] D\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt:\nVisual inference via python execution for reasoning. arXiv\npreprint arXiv:2303.08128, 2023. 3\n[53] Hao Tan and Mohit Bansal.\nLxmert:\nLearning cross-\nmodality encoder representations from transformers.\nIn\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-\nIJCNLP). Association for Computational Linguistics, 2019.\n4, 5\n10\n[54] Imad\nEddine\nToubal,\nYi-Ting\nChen,\nKrishnamurthy\nViswanathan, Daniel Salz, Ye Xia, and Zhongli Ding. Multi-\nmodal dual-tower architectures for entity retrieval from im-\nage and text. In CVPRW, 2023. 2\n[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3\n[56] Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang,\nGeoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo\nPang, Michael Bendersky, et al.\nNon-intrusive adapta-\ntion: Input-centric parameter-efficient fine-tuning for versa-\ntile multimodal modeling. arXiv preprint arXiv:2310.12100,\n2023. 2, 4, 5\n[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. NeurIPS, 35:24824\u201324837, 2022. 2, 3\n[58] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023. 3\n[59] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\nZhang, and Chi Wang. Autogen: Enabling next-gen llm ap-\nplications via multi-agent conversation framework.\narXiv\npreprint arXiv:2308.08155, 2023. 5\n[60] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. Large language\nmodels as optimizers.\narXiv preprint arXiv:2309.03409,\n2023. 5\n[61] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 3\n[62] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu\nWang, Hammad A Ayyubi, Kai-Wei Chang, and Shih-Fu\nChang. Idealgpt: Iteratively decomposing vision and lan-\nguage reasoning via large language models. arXiv preprint\narXiv:2305.14985, 2023. 3\n[63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. arXiv preprint\narXiv:2205.01917, 2022. 2\nAppendix\nA. Concept names and descriptions\nA.1. Agile Modeling dataset concepts\nArts and crafts: Image must contain arts and crafts.\nAstronaut: Any picture that shows an astronaut, even if it\u2019s\na drawing, clip art, etc. The astronaut should show clearly\nthat they are associated with being an astronaut \u2013 usually\nindicated by a space suit or NASA jumpsuit.\nBlock tower: Image must contain a toy block tower made\nof legos or wood.\nDance: Photos of people dancing.\nEmergency service: Image must contain emergency ser-\nvice, paramedics, firefighters, police, or rescue teams.\nGourmet tuna: Photos of gourmet dishes (i.e. fancy, ele-\ngant) that must contain tuna. This includes sushi, sashimi,\nseared tuna, a fancy ahi tuna salad. This does not include\ncanned tuna, tuna sandwich, a photo of the fish tuna itself.\nHand pointing: A picture showing a hand pointing, with\njust the index finger extended. Does not include pictures of\nthumbs-up or pictures of hands with more than just the in-\ndex finger extended. Picture with a straight finger pointing\nat or tapping a screen are included.\nHair coloring: Pictures that focus on people during the pro-\ncess of hair coloring or right after, before & after photos.\nNegatives: store front of hairdresser, boxes of dye.\nHealthy dish: Photos of dishes with healthy food that is\nlow in carbs\nHome fragrance: Photos of any types of fragrances used\nfor houses, including home perfumes, air fresheners for the\nhouse, scented candles, essential oils.\nIn ear headphones: Any headphones that are worn inside\nthe ear, rather than covering it up. These types of head-\nphones are inserted into the ear canal. As long as an in-ear\nheadphone is in the picture, it is valid.\nPie chart: Any image with a pie chart, which is a circular\nstatistical graphic, which is divided into slices to illustrate\nnumerical proportion.\nSingle sneaker on white background: Images depicting\na single sneaker on a white or neutral-colored background\n(e.g beige). It can be a partial view of a sneaker (e.g. just the\nsole, or half of the sneaker is in view) but it cannot be just\nparts (e.g. just the shoe lace) . Negatives include images\nthat have more than one shoe, that have different colored\nbackground, or a different style of shoe.\nStop sign: This includes photos of real-world, official stop\nsigns. Imagine we would want to detect such stop signs for\nself-driving cars. Positives include any stop sign photos,\nincluding those temporary ones included in construction, or\nheld in hand by a construction worker. If there\u2019s a stop sign\non a banner or ads poster, even if it\u2019s in traffic, it would be a\nnegative (we don\u2019t want the self-driving car to stop at that).\nClip art or indoors stop sign are negative\nA.2. Public dataset concepts\nHateful memes: Memes that are harmful, racist, or sexist\nB. Search queries\nThe following is the set of search queries used to mine\ncandidate images from the LAION [47] dataset during the\ndata mining process of our system. All these search queries\nare generated using the LLM (PaLM-2 [2]) and encoded\n11\nin joint CLIP [43] embedding space to retrieve candidate\nimages.\nArts and crafts: craft room, crafts book, crafts for be-\nginners, crafts tutorial, arts and crafts, craft store, crafts\nfair, craft project, crafts for sale, crafts, crafts for kids, art,\ncraftsmanship, craft supplies, diy, crafts magazine, crafts\nfor adults, handmade\nAstronaut: astronaut, astronaut in space station module, as-\ntronaut in space gloves, astronaut in space boots, astronaut\nin space flag, astronaut in space shuttle cockpit, astronaut in\nspace suit, astronaut in orbit, astronaut in space backpack,\nastronaut in space station airlock, astronaut on moon, astro-\nnaut working in space, astronaut in space station, astronaut\nin space, astronaut in space helmet, astronaut in space shut-\ntle, astronaut in space station cupola, astronaut walking in\nspace\nBlock tower: tower of blocks, lego tower, tall lego tower,\ntall toy tower, tower of legos, tower of toys, towering wood,\ntowering toys, towering blocks, block tower, towering le-\ngos, tall block tower, tower made of blocks, tall wooden\ntower, wooden tower, toy tower, tower of wood\nDance:\nstreet dance, flamenco, ballet, modern dance,\nbachata, ballroom dance, zouk, samba, people dancing,\nbelly dance, salsa, merengue, dance performance, line\ndance, tap dance, hip hop, dancers, folk dance\nEmergency service: police emergency, police officer at\nwork, rescue boat, emergency response, police car, fire\ntruck, rescue worker at work, emergency worker, medical\nemergency, firefighter, paramedic at work, rescue team, fire\nrescue, police, emergency service, rescue operation, fire-\nfighter at work, rescue helicopter, emergency vehicle, am-\nbulance, paramedic\nGourmet tuna: tuna sushi, tuna sashimi, tuna salad, seared\ntuna, ahi tuna, gourmet tuna, tuna tartare, ahi tuna steak,\ntuna steak, fancy tuna\nHand pointing: hand pointing finger extended, hand point-\ning finger, hand pointing, hand pointing finger straight at\nthem, hand pointing finger straight at someone, hand point-\ning finger straight at screen, hand index finger extended,\nhand pointing finger straight at something, hand pointing\nfinger straight at person, hand pointing finger straight at\nme, hand pointing finger straight at us, hand pointing fin-\nger straight at you, hand pointing at screen, hand pointing\nfinger straight at thing, hand pointing screen, hand pointing\nfinger straight at object, hand pointing finger straight\nHair coloring: hair coloring, hair color salon, hair color be-\nfore and after, hair color inspiration, hair color horror story,\nhair color mishap, hair color tutorial, hair color tips, hair\ndye, hair color stylist, hair color fail, hair color at home,\nhair color process, hair color mistake, hair color disaster,\nhair color gone wrong, hair color ideas, hair color, hair color\ngone bad\nHealthy dish: healthy lunch, healthy sandwich, healthy\ndish, healthy burger, healthy meal, healthy food, low carb\ndish, healthy salad, healthy fish, healthy pizza, healthy\ndinner, healthy vegetarian, healthy vegan, healthy snack,\nhealthy breakfast, healthy pasta, healthy chicken, healthy\nsoup, healthy dessert\nHome fragrance: home fragrance, home fragrance dif-\nfuser, scented candle, home scented candle, home scent,\nessential oil, home air freshener, air freshener, home essen-\ntial oil, home scent diffuser, home room spray, home aroma\ndiffuser, home smell diffuser, home perfume, home aroma,\nfragrance diffuser, home smell, room spray\nIn ear headphones: earphone, in ear headphones, in ear\nheadphone, earbuds, in ear, headphone\nPie chart: pie chart maker, pie chart data, pie chart tutorial,\npie chart percentage, pie chart illustration, pie chart design,\npie chart template, pie chart chart, pie chart, pie chart info-\ngraphic, pie chart graphic, pie chart diagram, pie chart cre-\nator, pie chart graph, pie chart example, pie chart generator,\npie chart tool, pie chart software\nSingle sneaker on white background: sneaker on light\nbeige background, sneaker on beige background, sneaker\non neutral, sneaker on light background, sneaker on cream\nbackground, single sneaker, sneaker on white background,\nsneaker, sneaker on background, sneaker on solid back-\nground, sneaker on light off-white background, sneaker\non light tan background, sneaker on neutral background,\nsneaker on light gray background, sneaker on light cream\nbackground, sneaker on plain background, sneaker on\noff-white background, shoe, sneaker on tan background,\nsneaker on white, sneaker on gray background\nStop sign: stop sign on road, stop sign on street, held stop\nsign, stop sign held by person, traffic stop sign, stop sign in\ntraffic, stop sign in city, stop sign on interstate, stop sign,\nstop sign on highway, construction stop sign, stop sign in\nconstruction, real stop sign, stop sign on freeway, stop sign\nin rural area, official stop sign, stop sign in parking lot, stop\nsign in hand\nC. LLM Prompts\nWe list a set of example prompts used in Modeling Col-\nlaborator Annotator below. When a description is unavail-\nable for a given concept, we use the following prompt to\nauto-generate a structured description:\nYou are given a visual concept name.\nFollow these steps:\n<step1>You have to work as an expert\n,\u2192 linguist. There are some human\n,\u2192 annotators who need to determine\n,\u2192 if given images are in-scope or\n,\u2192 out-of-scope for this visual\n,\u2192 concept. Your task is to generate\n12\n,\u2192\ndescription of the visual\n,\u2192 concept which annotators can use\n,\u2192 to decide if any images are in-\n,\u2192 scope or out-of-scope for the\n,\u2192 given visual concept.</step1>\n<step2>Provide an concept definition of\n,\u2192\nthis visual image in a few\n,\u2192 sentences.</step2>\n<step3>Provide all the image attributes\n,\u2192\nthat an image must have in order\n,\u2192\nto be in-scope for this visual\n,\u2192 concept.</step3>\n<step4>Each attribute found in step2\n,\u2192 and step3 should be verbose,\n,\u2192 independent, self-explanatory and\n,\u2192\nmeaningful.</step4>\n<step7>Write your response in following\n,\u2192\nuser friendly and readable\n,\u2192 format:\nVisual concept definition:\n<Add 2-3 line concept definition of the\n,\u2192\nvisual concept here.>\nImage must have following attributes\n,\u2192 for it to be in-scope for this\n,\u2192 visual concept:\n<Add details here as bullet points.>\n</step7>\n<visualConceptName>{CONCEPT_NAME}</\n,\u2192 visualConceptName>\nThe prompt for generating positive search queries based\non a visual concept (used to fetch candidate positive images\nfrom an image database):\nYour task is to help in finding\n,\u2192 positive (in-scope) images for a\n,\u2192 visual concept. You are given the\n,\u2192\nname and the description of a\n,\u2192 visual concept. Description\n,\u2192 explains the attributes of an\n,\u2192 image that make it in-scope for\n,\u2192 this visual concept. It also\n,\u2192 explains the attributes of an\n,\u2192 image that make it out-of-scope\n,\u2192 for this visual concept.\nFollow these steps:\n<step1>List all the attributes of an\n,\u2192 image that make it in-scope for\n,\u2192 this visual concept.</step1>\n<step2>Each attribute should be\n,\u2192 objective, complete, and self-\n,\u2192 explanatory.</step2>\n<step3>Ensure that attributes you have\n,\u2192 found cover all the in-scope\n,\u2192 attributes or scenarios mentioned\n,\u2192\nin the description. If not, add\n,\u2192 the missing in-scope attributes\n,\u2192 .</step3>\n<step4>Based on all the in-scope\n,\u2192 attributes you have identified in\n,\u2192\nstep3, generate 20 Google Search\n,\u2192\nkeywords which can be used to do\n,\u2192\nGoogle image search for finding\n,\u2192 diverse images with those in-\n,\u2192 scope attributes.</step4>\n<step5>Ensure that your Google Search\n,\u2192 keywords cover all types of in-\n,\u2192 scope images mentioned in the\n,\u2192 description. If not, add Google\n,\u2192 Search keywords to find those\n,\u2192 types of in-scope images.</step5>\n<step6>This is an important step. Some\n,\u2192 of the keywords you selected so\n,\u2192 far could be be suitable for\n,\u2192 searching out-of-scope images.\n,\u2192 Identify those keywords which are\n,\u2192\nfor searching out-of-scope image\n,\u2192 . Remove those Google Search\n,\u2192 keywords from your response.</\n,\u2192 step6>\n<step7>Each search query should be 3-4\n,\u2192 words long, independent, self-\n,\u2192 explanatory and meaningful for\n,\u2192 internet image search.</step7>\n<step8>If any of these queries are\n,\u2192 longer than 4 words, summarize\n,\u2192 them into 3-4 words.</step8>\n<step9>Write your response in following\n,\u2192\nxml format. Since your response\n,\u2192 will be programmatically parsed,\n,\u2192 your response should strictly\n,\u2192 follow this format:\n\u2018\u2018\u2018xml\n<google_search_keywords>\n<keyword></keyword>\n...\n</google_search_keywords>\n\u2018\u2018\u2018\n</step9>\n<step10>Keep only xml in the response\n,\u2192 and remove other text.</step10>\n<concept>{CONCEPT_NAME}</concept>\n<description>{CONCEPT_DESCRIPTION}</\n,\u2192 description>\n13\nThe prompt for generating negative search queries based\non a visual concept (used to fetch hard negative images from\nan image database):\nYou have to work as an expert linguist.\n,\u2192\nYou are given a visual concept\n,\u2192 name and its description for the\n,\u2192 purpose of image classification.\nDescription might contains few carve-\n,\u2192 outs. Carve-outs are some special\n,\u2192\nsituations in which images\n,\u2192 should be classified as out-of-\n,\u2192 scope. Your task is to extract\n,\u2192 carve-out details from the\n,\u2192 description.\nFollow these steps:\n<step1>If the description does not\n,\u2192 contain any carve-outs, write\n,\u2192 your response in the following\n,\u2192 format and skip all of the\n,\u2192 following steps.\n\u2018\u2018\u2018xml\n<carveOutsInDescription>\n<carveOut>NOT_FOUND</carveOut>\n</carveOutsInDescription>\n\u2018\u2018\u2018\n</step1>\n<step2>If the description provides\n,\u2192 details on out-of-scope images\n,\u2192 for this visual concept, output\n,\u2192 the list of those carve-outs\n,\u2192 situations mentioned.</step2>\n<step3>Output those in the following\n,\u2192 xml format. Since your response\n,\u2192 will be programmatically parsed,\n,\u2192 your response should strictly\n,\u2192 follow this format:\n\u2018\u2018\u2018xml\n<carveOutsInDescription>\n<carveOut></carveOut>\n...\n</carveOutsInDescription>\n\u2018\u2018\u2018\n</step3>\n<step4>Keep only xml in the response\n,\u2192 and remove other text.</step4>\n<concept>{CONCEPT_NAME}</concept>\n<description>{CONCEPT_DESCRIPTION}</\n,\u2192 description>\nThe prompt template for generating a concept\u2019s positive\nattributes:\nYour task is to understand the scope of\n,\u2192\na visual concept for image\n,\u2192 classification. You are given a\n,\u2192 visual concept name and its\n,\u2192 description.\nDescription explains the attributes of\n,\u2192 an image that make it in-scope\n,\u2192 for this visual concept. It also\n,\u2192 explains the attributes of an\n,\u2192 image that make it out-of-scope\n,\u2192 for this visual concept.\nFollow these steps:\n<step1>List all the attributes of an\n,\u2192 image that make it in-scope for\n,\u2192 this visual concept.</step1>\n<step2>Each attribute should be\n,\u2192 objective, unambiguous, detailed,\n,\u2192\nverbose and self-explanatory.</\n,\u2192 step2>\n<step3>Check that attributes you have\n,\u2192 found cover all the positive\n,\u2192 attributes mentioned in the\n,\u2192 description. If not, add the\n,\u2192 missing attributes.</step3>\n<step4>Write your response in following\n,\u2192\nxml format. Since your\nresponse\n,\u2192\nwill be programmatically parsed,\n,\u2192\nyour response should strictly\n,\u2192 follow this format:\n\u2018\u2018\u2018xml\n<positiveAttributes>\n<attribute></attribute>\n...\n</positiveAttributes>\n\u2018\u2018\u2018\n</step4>\n<step5>Keep only xml in the response\n,\u2192 and remove other text.</step5>\n<concept>{CONCEPT_NAME}</concept>\n<description>{CONCEPT_DESCRIPTION}</\n,\u2192 description>\nThe prompt template for generating a concept\u2019s negative at-\ntributes:\nYou have to work as an expert linguist.\n,\u2192\nYou are given a visual concept\n14\n,\u2192 name and its description for the\n,\u2192 purpose of image classification.\nDescription might contains few carve-\n,\u2192 outs. Carve-outs are some special\n,\u2192\nsituations in which images\n,\u2192 should be classified as out-of-\n,\u2192 scope. Your task is to extract\n,\u2192 carve-out details from the\n,\u2192 description.\nFollow these steps:\n<step1>If the description does not\n,\u2192 contain any carve-outs, write\n,\u2192 your response in the following\n,\u2192 format and skip all of the\n,\u2192 following steps.\n\u2018\u2018\u2018xml\n<carveOutsInDescription>\n<carveOut>NOT_FOUND</carveOut>\n</carveOutsInDescription>\n\u2018\u2018\u2018\n</step1>\n<step2>If the description provides\n,\u2192 details on out-of-scope images\n,\u2192 for this visual concept, output\n,\u2192 the list of those carve-outs\n,\u2192 situations mentioned.</step2>\n<step3>Output those in the following\n,\u2192 xml format. Since your response\n,\u2192 will be programmatically parsed,\n,\u2192 your response should strictly\n,\u2192 follow this format:\n\u2018\u2018\u2018xml\n<carveOutsInDescription>\n<carveOut></carveOut>\n...\n</carveOutsInDescription>\n\u2018\u2018\u2018\n</step3>\n<step4>Keep only xml in the response\n,\u2192 and remove other text.</step4>\n<concept>{CONCEPT_NAME}</concept>\n<description>{CONCEPT_DESCRIPTION}</\n,\u2192 description>\nwhere CONCEPT NAME and CONCEPT DESCRIPTION\nare the subjective concept name and description.\nFor a final annotation decision for an image, we feed the\nfollowing prompt to PaLM-2:\nYou are given the name and description\n,\u2192 of a visual concept. We showed\n,\u2192 the image to raters and asked\n,\u2192 many questions about the image\n,\u2192 and they gave the\nanswers.\n,\u2192 Questions and answers are also\n,\u2192 provided below. Your task is to\n,\u2192 answer some questions about the\n,\u2192 image. Follow these steps:\n<step1>In the following steps, your\n,\u2192 text responses must be strictly\n,\u2192 based on the answers provided in\n,\u2192 raters\u2019 responses.</step1>\n<step2>Provide the out-of-scope\n,\u2192 attributes present in the image\n,\u2192 .</step2>\n<step3>Provide the in-scope attributes\n,\u2192 present in the image.</step3>\n<step4>Provide the the in-scope\n,\u2192 attributes missing in the image\n,\u2192 .</step4>\n<step5>Classify the image based on the\n,\u2192 following rules. Rules must be\n,\u2192 followed in the given order.\n<classificationRules>\n<rule1>If\nthe image has EVEN ONE of\n,\u2192 the out-of-scope attributes, it\n,\u2192 must be classified negative for\n,\u2192 this visual concept.</rule1>\n<rule2>The image must have all the\n,\u2192 required positive attributes to\n,\u2192 classify the image as positive\n,\u2192 for this visual concept.\nIf\n,\u2192 image has all the required\n,\u2192 positive attributes, classify the\n,\u2192\nimage as positive. Otherwise\n,\u2192 classify it as negative.</rule2>\n<rule3>In all other cases, classify the\n,\u2192\nimage as negative.</rule3>\n</classificationRules></step5>\n<step6>Add following details to your\n,\u2192 response strictly in this format:\nDecision: \"Positive\" or \"Negative\"\nReasons: <Provide list of reasons why\n,\u2192 this image is Positive or\n,\u2192 Negative> </step6>\n15\n<step7>Make sure your response only\n,\u2192 contains text and no python code\n,\u2192 .</step7>\n<concept>{CONCEPT_NAME}</concept>\n<conceptDescription>{\n,\u2192 CONCEPT_DESCRIPTION}</\n,\u2192 conceptDescription>\n<raterResponses>{\n,\u2192 PALI_QUESTIONS_AND_ANSWERS}</\n,\u2192 raterResponses>\nwhere PALI QUESTIONS AND ANSWERS is a formatted\nstring of questions fed to PaLI-X VQA and their respective\nanswers.\nD. Ablations\nTo measure the effect of the expert involvement we show\nFig. 6. Overall, expert collaboration improves the perfor-\nmance of the distilled model. As the number of expert-\nlabeled examples increases (0 to 2000 out of total train-\ning 4000 examples), the recall, F1, and auPR scores of the\nmodel also increase.\nTo show the impact of additional automatically anno-\ntated data on the performance of the final output model on\neasy vs hard concepts, we show Fig. 5.\nE. Annotator Configurations\nWe define the following settings for the Annotator:\nA. use positive attributes for questions:\nWhether to generate positive questions from the\nattributes or directly from the concept description.\nB. generate negative questions:\nWhether or\nnot to generate negative questions. Sometimes these\nquestions result in over-predicting negative classes.\nC. use captioning questions: Whether to use a\ncaptioning VLM to generate a detailed description of\nthe image\nD. generate fixed num of questions: Fix the\nnumber of questions instead of having the LLM gener-\nate as many questions as possible.\nE. final rating without attributes:\nWhether to use negative and positive attribute in\nthe final annotation stage.\nUsing a grid search, we use different configurations for\ndifferent concept as described in Tab. 4.\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nauPR\nemergency-service\nin-ear-headphones\nsingle-sneaker-on-\nwhite-background\ndance\npie-chart\nhair-coloring\narts-and-crafts\nEasy concepts\n0\n1000\n2000\n3000\n4000\n5000\n6000\nCopilot-annotated examples\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nauPR\ngourmet-tuna\nhealthy-dish\nhand-pointing\nastronaut\nblock-tower\nhome-fragrance\nstop-sign\nHard concepts\nFigure 5. The impact of adding additional automatically annotated\nimages on the final model quality (using the auPR metric). 100\nuser-annotated examples are used in addition to the thousands of\nModeling Collaborator examples.\nConfiguration\nConcept\nA\nB\nC\nD\nE\narts-and-crafts\n\u2713\n\u2713\nastronaut\n\u2713\n\u2713\n\u2713\nblock-tower\n\u2713\n\u2713\n\u2713\ndance\n\u2713\n\u2713\n\u2713\nemergency-service\n\u2713\ngourmet-tuna\n\u2713\n\u2713\nhair-coloring\n\u2713\nhand-pointing\n\u2713\nhealthy-dish\n\u2713\n\u2713\n\u2713\nhome-fragrance\n\u2713\n\u2713\nin-ear-headphones\n\u2713\n\u2713\n\u2713\npie-chart\n\u2713\n\u2713\n\u2713\nsingle-sneaker\n\u2713\n\u2713\nstop-sign\n\u2713\n\u2713\nTable 4. Configuration settings used for each concept of the Agile\nModeling [51] dataset.\n16\n0\n500\n1000\n1500\n2000\n0.2\n0.4\n0.6\n0.8\nPerformance\nPrecision\n0\n500\n1000\n1500\n2000\nRecall\n0\n500\n1000\n1500\n2000\nF1\n0\n500\n1000\n1500\n2000\nauPR\nNumber of expert interactions (images annotated)\nFigure 6. The impact of Modeling Collaborator and expert collaboration on the performance of the distilled model. 4,000 total training\nexamples were used per concept. The x-axis represents how many of those examples were labeled by the expert (concept owner), ranging\nfrom no examples (0%) to 2,000 examples (50%).\n17\n"
  },
  {
    "title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
    "link": "https://arxiv.org/pdf/2403.03003.pdf",
    "upvote": "7",
    "text": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal\nLarge Language Models\nGen Luo 1 2 Yiyi Zhou 1 3 Yuxin Zhang 1 3 Xiawu Zheng 1 3 Xiaoshuai Sun 1 3 Rongrong Ji 1 3\nAbstract\nDespite remarkable progress, existing multimodal\nlarge language models (MLLMs) are still infe-\nrior in granular visual recognition. Contrary to\nprevious works, we study this problem from the\nperspective of image resolution, and reveal that\na combination of low- and high-resolution visual\nfeatures can effectively mitigate this shortcom-\ning. Based on this observation, we propose a\nnovel and efficient method for MLLMs, termed\nMixture-of-Resolution Adaptation (MRA). In par-\nticular, MRA adopts two visual pathways for\nimages with different resolutions, where high-\nresolution visual information is embedded into the\nlow-resolution pathway via the novel mixture-of-\nresolution adapters (MR-Adapters). This design\nalso greatly reduces the input sequence length of\nMLLMs. To validate MRA, we apply it to a recent\nMLLM called LLaVA, and term the new model\nLLaVA-HR. We conduct extensive experiments\non 11 vision-language (VL) tasks, which show\nthat LLaVA-HR outperforms existing MLLMs on\n8 VL tasks, e.g., +9.4% on TextVQA. More im-\nportantly, both training and inference of LLaVA-\nHR remain efficient with MRA, e.g., 20 train-\ning hours and 3\u00d7 inference speed than LLaVA-\n1.5. Source codes are released at: https://\ngithub.com/luogen1996/LLaVA-HR.\n1. Introduction\nDriven by the remarkable success of large language models\n(LLMs) (Touvron et al., 2023; Chen et al., 2020), research\non multi-modal large language models (MLLMs) also re-\nceives an influx of interest in the machine learning com-\nmunity (Liu et al., 2023b; Luo et al., 2023a; Alayrac et al.,\n1Key Laboratory of Multimedia Trusted Perception and Ef-\nficient Computing, Ministry of Education of China, School of\nInformatics, Xiamen University, 361005, P.R. China 2Peng Cheng\nLaboratory, Shenzhen, 518000, China 3Institute of Artificial Intel-\nligence, Xiamen University, 361005, P.R. China. Correspondence\nto: Rongrong Ji <rrji@xmu.edu.cn>.\n224 pix\n448 pix\n448 pix\n384 pix\n768 pix\n1024 pix\n448 pix\n672 pix\n336 pix\n1024 pix \n37.80 Acc\nFigure 1. Zero-shot performance and inference speed of\nLLaVA-HR and existing MLLMs on TextVQA. Existing\nMLLMs often fall short of fine-grained VL tasks like TextVQA.\nIncreasing image resolution is an effective yet expensive solution.\nWith the proposed MRA, our LLaVA-HR can efficiently adopt\nhigh-resolution images to boost performance.\n2022; Chen et al., 2022; 2023b). Numerous efforts have\nbeen recently devoted to extending LLMs to more modal-\nities, achieving breakthroughs on various vision-language\ntasks (Goyal et al., 2017; Singh et al., 2019; Hudson & Man-\nning, 2019). Despite advances, existing MLLMs still fall\nshort of granular visual recognition. For instance, the pow-\nerful GPT4-V also suffers from hallucinations when identi-\nfying small and occluded objects (Tong et al., 2024). This\nshortcoming inevitably limits the practical use of MLLMs.\nTo compensate for this shortcoming, practitioners often re-\nsort to scaling up model size and increasing per-training\ndata size (Alayrac et al., 2022; Li et al., 2023b; Bai et al.,\n2023). For instance, InstructBLIP (Dai et al., 2023) adopts\nover 129M image-text pairs for vision-language (VL) align-\nments, and shows that a larger visual encoder is beneficial\nfor MLLMs. Motivated by this, Qwen-VL (Bai et al., 2023)\nfurther increases the parameters of visual encoder to 1.9 bil-\nlion and uses 1.5 billion pre-training data. Despite progress,\nthis paradigm is prohibitively expensive, which often con-\nsumes about thousands of GPU hours.\nOrthogonal to these works, we study the visual shortcoming\nof MLLMs from the perspective of input image resolutions.\nAs revealed in previous VL research (Jiang et al., 2020; Tong\n1\narXiv:2403.03003v1  [cs.CV]  5 Mar 2024\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nImage\nEncoder\nInter. \nNet.\nLLM\nImage\nEncoder\n(a) Framework of Existing MLLMs\nResolution: 224 ~ 448 pix\nImage\nEncoder\nLLM\nHigh-resolution Pathway\nLow-resolution Pathway\nResolution: 384 ~ 1,536 pix\nFusion\n(b) Our Mixture-of-Resolution Adaptation\nMLP\nMLP\nCross\nAttn\nIntermediate Networks \nHigh-Resolution Adaptation: Expensive\nHigh-Resolution Adaptation: Cheap\nLLaVA\nBLIP-2\nFigure 2. Comparison between existing MLLMs and LLaVA-HR. Due to high computation complexity, existing MLLMs (Liu et al.,\n2023a; Li et al., 2023b) often use input images of low-resolution, which are insufficient for granular visual reasoning. With our mixture-\nof-resolution adaptation, the proposed LLaVA-HR can increase the image resolution up to 1,536 \u00d7 1,536 with limited additional costs.\net al., 2024; Luo et al., 2023b), increasing the resolution of\ninput images is a straightforward solution to improve visual\nrecognition, which becomes more important for MLLMs\nthat involve visual chain-of-thought (Rose et al., 2023). As\nshown in Fig. 1, increasing the resolution of LLaVA-1.5 (Liu\net al., 2023a) from 384 \u00d7 384 to 672 \u00d7 672 can bring\nobvious performance gains (+4.6%) on TextVQA (Singh\net al., 2019). However, the use of high-resolution images\nwill greatly exacerbate the already high computational cost\nof MLLMs. For instance, 448\u00d7448 resolution will increase\nthe computation complexity of LLaVA by about 1.4 times\ncompared with the default 336 \u00d7 336. In addition, due to\nthe complex structure of MLLMs, the training will become\nunstable as the resolution is greatly increased, e.g., a sharp\ndrop at 1, 022 \u00d7 1, 022 resolution, as shown in Fig. 1. We\nassume that the length of visual sequences greatly exceeds\nthe pre-trained context length, leading to training instability.\nIn this paper, we propose a novel and efficient method for\nthe high-resolution image adaptation of MLLMs, namely\nmixture-of-resolution adaptation (MRA). As shown in\nFig. 1, MRA adopts an innovative dual visual pathway de-\nsign to process the input images of high- and low-resolutions\nsimultaneously. Specifically, one pathway aims to encode\nglobal information of low-resolution images, while the\nother one serves to capture fine-grained semantics from\nhigh-resolution images. Meanwhile, these two pathways\nare closely interacted via the novel mixture-of-resolution\nadapters (MR-Adapters), which embeds the high-resolution\nvisual information into the low-resolution modeling. In this\nway, we can use a much fewer number of visual tokens\nto represent the input images from macro- to micro-views.\nWith the careful design of dual-pathway structure, MRA\ncan easily increase the image resolution up to 1,536 \u00d7 1,536\npixels while maintaining high efficiency.\nTo validate MRA, we apply it to a recent MLLLM called\nLLaVA (Liu et al., 2023b;a), and term the new model\nas LLaVA-HR. We conduct extensive experiments on 11\nvision-language (VL) tasks, including common VL tasks\nlike VQA2.0 (Goyal et al., 2017) and emerging benchmarks\nsuch as POPE (Li et al., 2023c). Experimental results show\nthat LLaVA-HR outperforms existing MLLMs on 8 of 11\nVL tasks, e.g., +9.6% over LLaVA-1.5 on TextVQA. More\nimportantly, the training and inference of LLaVA-HR are\ncost-effective. The pre-training and instruction tuning of\nLLaVA-HR (7B, 1,024 \u00d7 1,024) only take a total of 20.7\nhours on 8 A800 GPUs, which is hundreds of times cheaper\nthan InstructBLIP (Dai et al., 2023) and Qwen-VL (Bai\net al., 2023). With the same resolution, its inference speed\nis 3 times faster than LLaVA-1.5 (Liu et al., 2023a).\nIn summary, our contributions are three folds:\n\u2022 We reveal the significance of image resolution for\nMLLMs and propose a novel and efficient adaptation\nscheme, termed mixture-of-resolution adaption (MRA),\nwhich adopts a novel dual visual pathway design to ob-\ntain the benefits of high-resolution visual information\nwhile keeping training and inference efficient.\n\u2022 We propose a novel mixture-of-resolution adapter\n(MR-Adapter) for MRA, which can embed the high-\nresolution information into the low-resolution visual\npathway to improve visual descriptive power.\n\u2022 Based on MRA, we propose a powerful MLLM, coined\nLLaVA-HR, which outperforms existing MLLMs on\n8 of 11 VL tasks and spends much cheaper training\nexpenditure than most MLLMs.\n2. Related Work\n2.1. Multimodal Large Language Models\nDriven by the great successes of large language mod-\nels (LLMs) (Gilardi et al., 2023; Touvron et al., 2023;\nChen et al., 2020), growing interest has been aroused in\nbuilding end-to-end multimodal large language models\n(MLLMs) (Liu et al., 2023b; Zhu et al., 2023; Luo et al.,\n2023a; Fuyu-8B, 2023; Peng et al., 2023; Liu et al., 2023c).\nIn particular, most existing MLLMs adopt a modular struc-\nture (Luo et al., 2023a; Liu et al., 2023b), which utilizes\n2\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nan intermediate network to project the visual features into\nthe word embedding space of the LLM. Then, the LLM is\nused to accomplish various VL tasks in an autoregressive\nmanner. Based on the modular structure, existing MLLMs\ncan be distinguished by the designs of the intermediate net-\nwork. Popular MLLMs represented by LLaVA (Liu et al.,\n2023b) often adopt a linear projection layer or an MLP\nlayer to connect the visual encoder and the LLM (Liu et al.,\n2023b;a; Chen et al., 2023a;b; Peng et al., 2023). The other\nworks employ sampler-based modules to bridge the gap\nbetween the visual encoder and the LLM (Bai et al., 2023;\nAlayrac et al., 2022; Li et al., 2023b; Dai et al., 2023). These\nsampler-based modules can effectively reduce the number of\nvisual tokens, but often requires a large-scale pre-training to\nachieve a promising performance (Bai et al., 2023; Li et al.,\n2023b). Despite the effectiveness, most existing MLLMs\nstill adopt a low visual resolution, e.g., 336 \u00d7 336, which\ngreatly limits their performance in fine-grained tasks.\n2.2. Visual Representations for MLLMs\nThe pursuit of better visual representations has been a pop-\nular research trend in the VL community (Lu et al., 2019;\nJiang et al., 2020; Radford et al., 2021; Ren et al., 2024).\nEarly endeavors mainly explore the object-level features for\nVL models (Lu et al., 2019; Zhang et al., 2021). Driven by\nthe large-scale image-text pre-training, grid features from\nCLIP (Radford et al., 2021) have demonstrated the great\nefficiency and generalization in MLLMs (Liu et al., 2023b;\nChen et al., 2022; Alayrac et al., 2022). Based on grid\nfeatures, existing researchers mainly improve visual repre-\nsentations by scaling up the visual encoder. For example,\nPaLI (Chen et al., 2022) increases the parameters of visual\nencoder to 3 billions and shows the significant performance\nboost of MLLMs. In contrast to these works, we improve\nthe visual representations for MLLMs from the perspec-\ntive of image resolution, and propose a novel and efficient\nsolution, namely mixture-of-resolution adaptation.\n3. Preliminary\nWe first recap the structure of multimodal large language\nmodels (MLLMs), which consists of an image encoder\nFI(\u00b7), an intermediate network FP(\u00b7) and an LLM FL(\u00b7).\nIn particular, given an input image I \u2208 RH\u00d7W \u00d73 and a tex-\ntual instruction T \u2208 RL, the visual tokens Fv \u2208 R(h\u00d7w)\u00d7d\nare obtained via the image encoder, and the text tokens\nft \u2208 Rl\u00d7d are represented by the corresponding word em-\nbeddings. Based on the visual and textual tokens, the LLM\nwill decode the target word step by step, formulated as\npt =\nS+1\nY\ns=1\nFL(Rs|FP(Fv), ft, R0:s\u22121).\n(1)\nHere, pt \u2208 Rm denotes the probabilities of the predicted\nword and m is the size of word vocabulary.\nIn some MLLMs (Liu et al., 2023b;a), FP(\u00b7) is often a stack\nof simple linear layers, which are used to directly project the\nvisual tokens onto the semantic space of LLMs. Although\nsimple and effective, this strategy inevitably leads to a longer\nvisual sequence as the resolution increases, e.g., 5,329 to-\nkens for 1,022 \u00d7 1,022 resolution in LLaVA-1.5. In practice,\nprocessing such a large number of tokens is computation-\nally expensive in MLLMs. To further reduce the number\nof visual tokens, recent advances adopt the sampler-based\nmodule for FP(\u00b7) , e.g., QFormer (Li et al., 2023b), which\naggregates visual features into several tokens that LLM can\ndirectly handle. Nevertheless, these methods often require\nlarge-scale pre-training to achieve VL alignments (Bai et al.,\n2023; Li et al., 2023b).\nBased on the above analyses, we conclude that the main\ndifficulty of high-resolution image adaptation lies in the\nrapidly growing visual sequence. This issue motivates us\nto further explore how to efficiently encode richer visual\ninformation with fewer visual tokens.\n4. Mixture-of-Resolution Adaptation\n4.1. Overview\nTo address the above issues, we propose a novel and efficient\nmethod for MLLMs, termed mixture-of-resolution adapta-\ntion (MRA), of which structure is depicted in Fig. 3. The\ncore idea of MRA is to embed high-resolution information\ninto the low-resolution one via a dual pathway design. In\nthis case, MRA can keep a smaller number of visual tokens\nwhile encoding richer visual information.\nParticularly, given the input images of two resolutions Il \u2208\nRHl\u00d7Wl\u00d73 and Ih \u2208 RHh\u00d7Wh\u00d73, the process of MRA can\nbe formulated as\nFv = FIl(Il, FA(Fvh)) + Fvh,\nFvh = FIh(Ih).\n(2)\nHere, Fvh \u2208 Rhh\u00d7wh\u00d7dh and Fv \u2208 Rh\u00d7w\u00d7d denote the\nhigh-resolution features and the final visual features, re-\nspectively. And FIl and FIh are the visual encoders for\nhigh-resolution and low-resolution images, respectively. FA\ndenotes the mixture-of-resolution adapter (MR-Adapter). In\nEq. 2, MRA adopts dual visual pathways to process high-\nand low- resolution images simultaneously. Then, a novel\nMR-Adapter is used to fuse the high-resolution information\nfrom the slow pathway to the fast one. Finally, the visual\nfeatures of two resolutions are combined and processed by\nthe LLM based on Eq. 1.\n3\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nConv\nStage\nHigh-resolution Image\nConv\nstage\nConv\nstage\nViT\nstage\nViT\nstage\nViT\nstage\nViT\nstage\nLow-resolution Image\nMR-\nAdapter\n\ud835\udfcf\ud835\udfce\ud835\udfd0\ud835\udfd2 \u00d7 \ud835\udfcf\ud835\udfce\ud835\udfd0\ud835\udfd2\n\ud835\udfd2\ud835\udfd2\ud835\udfd6 \u00d7 \ud835\udfd2\ud835\udfd2\ud835\udfd6\nMR-\nAdapter\nMR-\nAdapter\nConv\nstage\nMR-\nAdapter\nLow-resolution Pathway (Macro View)\nMulti-head Attention\nFeed-forward Network\nLLaMA2-7B\nText Instruction: \n\u201cdescribe this image in short.\u201d\nTokenizer\nOutput: A herd of \nelephants and deer are \ngathered around a \nwatering hole. The \nelephants are of various \nsizes, including a baby \nelephant. The deer are \nalso of different sizes, \nwith some appearing to \nbe young.\nMLP\nHigh-resolution Pathway (Micro View)\n\ud835\udfd1\ud835\udfd0 \u00d7 \ud835\udfd1\ud835\udfd0\n\ud835\udfd1\ud835\udfd0 \u00d7 \ud835\udfd1\ud835\udfd0\nFigure 3. Illustration of Mixture-of-Resolution Adaptation (MRA) and its deployment on LLaVA-HR. MRA employs dual visual\npathways to process high-resolution and low-resolution images, respectively. High-resolution information is embeded into the fast\npathway via a novel mixture-of-resolution adapter (MR-Adapter).\nConv\nBlock\nMLP\nBlock\nG\n\ud835\udf0f\u210e\n\ud835\udf0f\ud835\udc59\nGate\n\ud835\udc39\ud835\udc63\u210e\n\ud835\udc39\ud835\udc63\ud835\udc59\n+\nFusion\nFigure 4. Illustration of the mixture-of-resolution adapter (MR-\nAdapter).\nMR-Adapter can dynamically embed the high-\nresolution features into the low-resolution pathway.\n4.2. Dual Visual Pathways\nAs shown in Fig. 3, dual visual pathways are the key design\nof MRA, and their benefits are maximized from two aspects.\nVisual functionality. Firstly, the dual visual pathways pro-\ncess images from macro- and micro-views, which is inspired\nby the visual system of human being (Merigan & Maunsell,\n1993; Robertson & Lamb, 1991). Particularly, Robertson\n& Lamb (1991) find that the visual system processes local\nand global semantics via different pathways. Based on this\nfinding, we adopt a similar mechanism to our MRA. Specif-\nically, one visual pathway aims to capture fine-grained se-\nmantics from high-resolution images i.e., processing images\nfrom local view. In contrast, the other pathway is designed\nto encode global information from low-resolution images,\nachieving a larger receptive field.\nVisual alignment. Due to different resolutions, these two\npathways often produce visual features of different shapes,\nimpeding their quick alignments (Yu et al., 2019). To over-\ncome this limitation, we adopt different downsampling rates\nfor the low- and high-resolution pathways, respectively.\nThus, their output features can keep the same spatial shape.\nBased on the above observations, we design the dual visual\npathways with a convolutional network (CNN) (Liu et al.,\n2022) and a vision transformer (ViT) (Dosovitskiy et al.,\n2020). Specifically, CNN is equipped with a downsampling\nstride of 32 to process high-resolution images. ViT encodes\nlow-resolution images with a downsampling stride of 14.\nNotably, such designs also ensure the efficiency of MLLMs,\nwhere the high-resolution images are processed by the ef-\nficient CNN, and the number of visual tokens is also kept\nsmall via the large downsampling stride.\n4.3. Mixture-of-Resolution Adapter\nTo better collaborate the feature learning of two pathways,\nwe propose a mixture-of-resolution adapter (MR-Adapter)\nfor the fusion of visual information from different res-\nolution images. In particular, given the visual features\nFvh \u2208 Rh\u00d7w\u00d7dh extracted from a high-resolution image,\nwe embed them into the low-resolution visual pathway by\nF\u2032\nvl = Fvl + fl(Fvl) + g \u00b7 fh(Fvh).\n(3)\nHere, Fvl \u2208 Rh\u00d7w\u00d7dl are the features from the low-\nresolution pathway. fl(\u00b7) and fh(\u00b7) denote two mapping\nmodules, which are designed as a convolutional block and\nan MLP layer, respectively. g is a dynamic score to control\nthe weights of high-resolution information, defined by\ng = \u03b4(W2\u03c3(W1fv)),\nfv =\n1\nh \u00d7 w\nh\nX\ni\nw\nX\nj\n[fl(Fvl)i,j, fh(Fvh)i,j].\n(4)\nHere, [\u00b7] denotes the concatenation operation, and W1 \u2208\nR2d\u00d7 d\n2 and W2 \u2208 R\nd\n2 \u00d7d are two projection matrices.\nfv \u2208 Rd is the pooled visual features. \u03c3 and \u03b4 denote\nthe activation function of GELU and Tanh, respectively.\nAs shown in Fig. 3, high-resolution information can be\nfused with the features in each block of ViT. In this case, the\nlow-resolution features of ViT also contain rich semantics,\nimproving the visual descriptive power of MLLMs.\n4\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nTable 1. Performance and efficiency comparisons of LLaVA-HR and LLaVA-1.5 (Liu et al., 2023a) at different resolutions. Except\nresolution, the other configurations of LLaVA-HR and LLaVA-1.5 remain the same. The training and inference costs are measured on\nNVIDIA A800s. \u201cN/A\u201d denotes that GPU memory overflows1. \u201ctokens/s\u201d denotes the number of generated tokens per second.\nModels\nResolution\nVision-Language Tasks\nTraining\nTime \u2193\nGPU\nMemory \u2193\nInference\nSpeed \u2191\nVQAv2 \u2191\nTextVQA \u2191\nMME \u2191\nPOPE \u2191\nLLaVA-1.5\n336 pix\n80.44\n59.41\n1461.17\n86.2\n15.6h\n28G\n23.8 tokens/s\nLLaVA-HR (ours)\n384 pix\n80.47\n59.63\n1522.28\n86.3\n17.6h\n34G\n23.8 tokens/s\nLLaVA-1.5\n448 pix\n81.17\n62.17\n1493.12\n87.2\n19.4h\n49G\n19.9 tokens/s\nLLaVA-HR (ours)\n768 pix\n81.80\n64.36\n1524.75\n88.0\n18.2h\n38G\n23.5 tokens/s\nLLaVA-1.5\n672 pix\n81.54\n64.23\n1498.71\n87.9\n31.8h\n79G\n12.7 tokens/s\nLLaVA-HR (ours)\n1024 pix\n81.90\n67.11\n1554.90\n87.6\n20.7h\n40G\n19.7 tokens/s\nLLaVA-1.5\n1022 pix\n74.20\n37.80\n1266.90\n84.4\n69.4h\nN/A1\n5.6 tokens/s\nLLaVA-HR (ours)\n1536 pix\n81.82\n67.96\n1480.62\n87.7\n29.8h\n52G\n12.6 tokens/s\n4.4. The Deployment on MLLM\nWe apply MRA to a popular MLLM called LLaVA-1.5 (Liu\net al., 2023a), and construct a new model, namely LLaVA-\nHR. Its training consists of two stages, i.e., low-resolution\npre-training and high-resolution instruction tuning.\nStage 1:\nLow-Resolution Pre-training.\nSimilar to\nLLaVA (Liu et al., 2023b) and LLaVA-1.5 (Liu et al.,\n2023a), this stage aims to optimize the projector to align the\nvisual features with the word embeddings of LLM. There-\nfore, the image encoder and the LLM are frozen during\npre-training. Besides, we adopt low resolutions for two\npathways. In this stage, the MR-Adapter is not inserted, and\noutput features of dual pathways are directly combined.\nStage 2: High-Resolution Instruction Tuning. During\ninstruction tuning, we greatly increase the resolution of the\nhigh-resolution pathway, e.g., from 384\u00d7 384 to 1,024\u00d7\n1,024. And the low-resolution one is also accordingly ad-\njusted to ensure the visual alignment of two pathways, e.g.,\nfrom 336\u00d7 336 to 448\u00d7 448. Meanwhile, the MR-Adapter\nis then applied to connect two visual pathways. Different\nfrom the first training stage, the entire MLLM will be fully\noptimized to better accommodate high-resolution images.\n5. Experiments\n5.1. Evaluations and Metrics\nMultimodal benchmarks for MLLM. We evaluate LLaVA-\nHR on four emerging multimodal benchmarks for MLLMs,\nincluding MME (Fu et al., 2023), POPE (Li et al., 2023c),\nSEED (Li et al., 2023a) and MM-VET (Yu et al., 2023). In\nparticular, MME and MM-VET evaluate the multimodal per-\n1When memory overflows, we reduce the batch size and in-\ncrease the gradient accumulation steps to train LLaVA-1.5.\nception and cognition abilities of MLLMs. SEED extends\nthe modalities of evaluation to images and videos. POPE\naims to evaluate the visual hallucinations of MLLMs. The\nmetrics used in our paper follow their default settings. For\nMME, we follow LLaVA-1.5 to report the perception score.\nCommon vision-language benchmarks.\nWe also\nevaluate LLaVA-HR on seven VL datasets, including\nVQAv2 (Goyal et al., 2017), GQA (Hudson & Manning,\n2019), OKVQA (Marino et al., 2019), OCRVQA (Mishra\net al., 2019), ScienceQA (Lu et al., 2022), VizWiz (Gurari\net al., 2018) and TextVQA. In particular, ScienceQA (Lu\net al., 2022), VizWiz (Gurari et al., 2018) and TextVQA are\nthree zero-shot tasks, and their samples are not appeared\nin our training data. We report the accuracy on the test\nset of OCRVQA, the test set of VizWiz, and the val set of\nOKVQA. We organize samples of these tasks in instruction\nformats of LLaVA-1.5 (Liu et al., 2023a).\n5.2. Implementation Details\nIn LLaVA-HR, we use CLIP-ViT-L (Radford et al., 2021;\nIlharco et al., 2021) and CLIP-ConvNeXt-L (Liu et al.,\n2022) as the dual visual paths to encode low- and high-\nresolution images, respectively. In LLaVA-HR-X, the CLIP-\nConvNeXt-L is replaced with the stronger CLIP-ConvNeXt-\nXXL. The MR-Adapter is applied into the last three stages\nof ViT. Following LLaVA-1.5, we first pre-train LLaVA-\nHR on LCS-558K (Liu et al., 2023b), which contains 558k\nimage-text pairs. During the pre-training stage, both the\nvisual encoder and the LLM are frozen, and only the MLP\nprojector is fine-tuned. AdamW (Kingma & Ba, 2014) is\nused as the optimizer, and the learning rate and batch size\nare set to 1e-3 and 256, respectively. Visual resolutions are\nset to 336\u00d7336 and 384\u00d7384 for the ViT and the CNN, re-\nspectively. During instruction tuning, we follow LLaVA-1.5\nto use 665k VL instruction data. At this stage, the entire\n5\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nTable 2. Comparison of MRA and four baselines on LLaVA-\nHR. The visual resolution is set to about \u223c760\u00d7 760.\nSettings\nVQAv2 TextVQA MME POPE Speed\nViT+ MLP\n81.0\n63.2\n1436.1 87.6 10.7 t/s\nConv+MLP\n80.3\n64.6\n1415.9 86.6 23.7 t/s\nViT+Resampler\n79.8\n58.9\n1403.8 85.8 27.6 t/s\nViT+Pooling+MLP\n80.6\n59.6\n1480.6 86.5 23.9 t/s\nMRA (ours)\n81.8\n64.4\n1524.8 88.0 23.5 t/s\nmodel is updated with a learning rate of 2e-5. Besides, we\nincrease the resolution of ViT and CNN to 448\u00d7448 and\n1,024\u00d71,024, respectively. The training epoch is set to 1 for\npre-training and instruction tuning.\n5.3. Experimental Results\n5.3.1. QUANTITATIVE ANALYSIS\nComparison with baselines. In Tab. 1, we compare the\nperformance and efficiency of LLaVA-HR with LLaVA-\n1.5 (Liu et al., 2023a) with different image resolutions. From\nthis table, we observe that increasing image resolution ob-\nviously improves the performance of two models on four\ntasks, e.g., +4.8% of LLaVA-1.5 on TextVQA. However,\nthe performance of LLaVA-1.5 drops significantly at the\nresolution of 1,024\u00d71,024. To explain, the number of vi-\nsual tokens greatly exceeds the pre-trained context length of\nthe LLM, which easily causes the instability during training.\nIn contrast, the performance of LLaVA-HR is consistently\nimproved from 384 \u00d7 384 resolution to 1,024 \u00d7 1,024 reso-\nlution. Besides, the total gain of LLaVA-HR is more obvious\nthan that of LLaVA-1.5 (Liu et al., 2023a), e.g., +8.33% of\nLLaVA-HR vs. +4.82% of LLaVA-1.5, greatly confirming\nthe effectiveness of MRA.\nIn Tab. 2, we further compare four common baselines with\nthe similar resolution, i.e., \u223c760\u00d7760. \u201cViT+MLP\u201d is the\ndefault setting of LLaVA-1.5 as the reference. \u201cConv+MLP\u201d\nreplaces the visual backbone with ConvNeXt (Liu et al.,\n2022), which uses a larger downsampling rate to re-\nduce the number of visual tokens. \u201cViT+Resampler\u201d and\n\u201cViT+Pooling+MLP\u201d refer to the two pooling strategies for\nreducing the number of visual tokens. As can be seen,\nall compared methods are inferior to LLaVA-HR. In par-\nticular, using a convolutional network as the visual back-\nbone greatly improves efficiency, but its performance still\nlags behind LLaVA-HR by a large margin, e.g., -108.9 on\nMME (Fu et al., 2023). Similarly, \u201cViT+Resampler\u201d and\n\u201cViT+Pooling+MLP\u201d also sacrifice performance for effi-\nciency. Overall, these comparisons further confirm the de-\nsigns of MRA.\nDespite effectiveness, the expenditure of LLaVA-HR is also\ncost-effective. In particular, increasing resolution from 384\nTable 3. Ablation study of mixture-of-resolution adaptation on\nLLaVA-HR. The resolution is 768 \u00d7 768. Our final setting is\ncolored in gray. \u201cL-Res Path.\u201d, \u201cH-Res Path.\u201d, \u201cFusion Direct.\u201d,\n\u201cStruct.\u201d and \u201cGate Fuct.\u201d denote the low-resolution pathway, the\nhigh-resolution pathway, the fusion direction, the structure and the\ngate function, respectively.\nSettings\nChoices\nVQAv2 TextVQA MME POPE\nL-Res\nPath.\nViT-L\n81.8\n64.4\n1524.8 88.0\nNone\n80.3\n64.6\n1415.9 86.6\nViT-G\n81.7\n65.3\n1469.7 87.9\nH-Res\nPath.\nConvNeXt-L\n81.8\n64.4\n1524.8 88.0\nNone\n80.4\n59.4\n1461.2 86.2\nConvNeXt-XXL\n82.3\n66.5\n1479.2 87.9\nFusion\nDirect.\nHigh to Low\n81.8\n64.4\n1524.8 88.0\nLow to High\n81.0\n62.8\n1463.5 87.3\nFusion\nType\nSum\n81.8\n64.4\n1524.8 88.0\nConcat\n81.7\n64.7\n1508.8 87.3\nStruct.\nmlp-conv\n81.8\n64.4\n1524.8 88.0\nconv-conv\n81.6\n64.6\n1499.0 87.7\nconv-mlp\n81.5\n64.2\n1517.9 87.6\nGate\nFunct.\nTanh\n81.8\n64.4\n1524.8 88.0\nSigmoid\n81.7\n64.3\n1567.9 86.9\nH-sigmoid\n81.6\n64.4\n1525.9 87.8\n\u00d7 384 to 1,024 \u00d7 1,024 slows down the training and in-\nference of LLaVA-1.5 by 344.8% and 325%, respectively.\nHowever, these costs are reduced to only 17.6% and 20.8%\nin LLaVA-HR. Despite better performance, the training and\ninference speeds of LLaVA-HR are three times faster than\nLLaVA-1.5. Besides, the costs of GPU memory also remain\ncheap for LLaVA-HR. For example, adapting the resolution\nof 1,536 \u00d7 1,536 for LLaVA-HR only consumes 52G GPU\nmemory, but the same settings for LLaVA-1.5 will cause\nGPU memory overflow. These results greatly confirm the\nefficiency of our MRA and LLaVA-HR.\nAblation studies. In Tab. 3, we conduct comprehensive\nablation studies for MRA on four VL benchmarks. Firstly,\nwe validate the different designs of the dual visual pathways.\nFrom these results, we find that removing one pathway\nwill lead to significant performance drops, e.g., -1.5% on\nVQAv2. Besides, scaling up the high-resolution encoder\nbrings more gains than that of the low-resolution one, e.g.,\n+2.1% vs. +0.9% on TextVQA. We assume that the stronger\nhigh-resolution image encoder can better capture the fine-\ngrained visual information. Then, we ablate different fusion\ndirections and strategies in MRA. Specifically, changing the\nfusion direction obviously degenerates the performance, e.g.,\n6\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nTable 4. Comparison with existing methods on four MLLM benchmarks. \u201cParam.\u201d, \u201cRes.\u201d and \u201cData\u201d refer to the total parameters,\nthe visual resolution and the number of training data, respectively. \u201ct/s\u201d refers to tokens per second.\nMethod\nSettings\nMultimodal Benchmarks\nInference\nParam.\nRes.\nData\nMME\nPOPE\nSEED\nMM-Vet\nSpeed\nBLIP-2\n14.2B\n224\n129M\n1293.8\n85.3\n46.4\n22.4\n-\nInstructBLIP\n8.2B\n224\n130M\n-\n-\n53.4\n26.2\n22.6 t/s\nInstructBLIP\n14.2B\n224\n130M\n1212.8\n78.9\n-\n25.6\n-\nQWen-VL-Chat\n9.6B\n448\n1.4B\n1487.5\n-\n58.2\n-\n17.0 t/s\nFuyu-8B\n8B\n\u223c600\n-\n728.6\n74.1\n-\n21.4\n15.6 t/s\nmPLUG-Owl2\n8.2B\n448\n400M\n1450.2\n-\n57.8\n36.2\n19.6 t/s\nLLaVA-1.5\n7.2B\n336\n1.2M\n1510.7\n85.9\n58.6\n30.5\n23.8 t/s\nLLaVA-1.5\n13.2B\n336\n1.2M\n1531.3\n85.9\n61.6\n35.4\n-\nLLaVA-HR\n7.4B\n1024\n1.2M\n1554.9\n87.6\n64.2\n31.2\n19.7 t/s\nLLaVA-HR\n13.4B\n1024\n1.2M\n1540.9\n87.8\n64.5\n34.8\n15.0 t/s\nLLaVA-HR-X\n14B\n1024\n1.2M\n1487.3\n88.0\n65.3\n35.5\n12.9 t/s\nTable 5. Comparison with existing methods on seven vision-language tasks. SQAI refers to the IMG subset of ScienceQA.\nMethod\nSettings\nIn-domain Tasks\nZero-shot Tasks\nInfer.\nParam.\nRes.\nData\nVQAv2\nGQA\nOKVQA\nOCRVQA\nSQAI\nVizWiz\nTextVQA\nSpeed\nBLIP-2\n14.2B\n224\n129M\n41.0\n41.0\n45.9\n40.6\n61.0\n19.6\n42.5\n-\nInstructBLIP\n8.2B\n224\n130M\n-\n49.2\n-\n-\n60.5\n34.5\n50.1\n22.6 t/s\nInstructBLIP\n14.2B\n224\n130M\n-\n49.5\n-\n44.8\n63.1\n33.4\n50.7\n-\nShikra\n13.2B\n224\n6.1M\n77.4\n-\n-\n-\n-\n-\n-\n-\nIDEFICS-9B\n9B\n224\n354M\n50.9\n-\n38.4\n-\n-\n35.5\n25.9\n30.5 t/s\nIDEFICS-80B\n80B\n224\n354M\n60.0\n-\n45.2\n-\n-\n36.0\n30.9\n-\nQwen-VL-Chat 9.6B\n448\n1.4B\n78.2\n57.5\n56.6\n70.5\n68.2\n38.9\n61.5\n17.0 t/s\nFuyu-8B\n8B\n\u223c600 -\n74.2\n-\n60.6\n-\n-\n-\n-\n15.6 t/s\nmPLUG-Owl2\n8.2B\n448\n400M\n79.4\n56.1\n57.7\n-\n68.7\n54.5\n58.2\n19.6 t/s\nLLaVA-1.5\n7.2B\n336\n1.2M\n78.5\n62.0\n-\n-\n66.8\n50.0\n58.2\n23.8 t/s\nLLaVA-1.5\n13.2B\n336\n1.2M\n80.0\n63.3\n-\n-\n71.6\n53.6\n61.3\n-\nLLaVA-HR\n7.4B\n1024\n1.2M\n81.9\n64.2\n58.9\n68.4\n65.1\n48.7\n67.1\n19.7 t/s\nLLaVA-HR\n13.4B\n1024\n1.2M\n82.3\n64.8\n60.7\n67.7\n68.1\n57.9\n68.1\n15.0 t/s\nLLaVA-HR-X\n14B\n1024\n1.2M\n82.6\n65.2\n61.5\n69.0\n68.0\n56.6\n70.9\n12.9 t/s\n-61.3 on MME. Finally, we ablate the designs of the mixture-\nof-resolution adapter. Specifically, the best choices of map-\nping modules for the low- and high-resolution pathways are\nconvolution blocks and MLP blocks, respectively. Besides,\nthe choices of gating function also affect performance and\nthe tanh function perform the best. These ablations further\nconfirm the designs of MR-Adapter.\nComparison with existing MLLMs. In Tab. 4 - 5, we\ncompare LLaVA-HR with existing MLLMs on 11 VL tasks.\nOn the four MLLM benchmarks, we observe comprehen-\nsive advantages of LLaVA-HR against existing MLLMs.\nIn particular, LLaVA-HR achieves 1554.9 scores in MME\nbenchmark, outperforming LLaVA-1.5 by +23.6. On POPE,\na benchmark including video evaluations, LLaVA-HR-X\nstill outperforms existing MLLMs by a large margin, i.e.,\n+3.7% gains. Besides, LLaVA-HR achieves the best per-\nformance on the benchmark for visual hallucinations, i.e.,\nPOPE, suggesting that its visual hallucinations are greatly\nalleviated. Notably, Fuyu-8b (Fuyu-8B, 2023) is capable of\nhigh-resolution images, but its performance is much inferior\nto LLaVA-HR, e.g., 728.6 vs. 1554.9 on MME.\nTab. 5 gives the performance comparison on common VL\ntasks. On in-domain tasks, LLaVA-HR achieves the best\nresults on three tasks, e.g., 82.6 on VQAv2 and 61.5 on\nOKVQA. On OCRVQA, Qwen-VL-Chat collects more in-\ndomain data for training, so it performs better than LLaVA-\nHR. Under the zero-shot setting, we can observe more sig-\nnificant advantages of LLaVA-HR on the fine-grained tasks,\ne.g., VizWiz and TextVQA. Most notably, even Qwen-VL-\nChat is pre-trained with 24.8M OCR samples, it still per-\nforms worse than LLaVA-HR-X on TextVQA. These results\nsuggest the significance of high resolution for these tasks. In\n7\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nUser: What color is the tiniest \nboat in the forefront?\nLLaVA-HR (384 pix): Red\nLLaVA-HR (1024 pix): White\nUser: How many boats are fully \nvisible?\nLLaVA-HR (384 pix): 2\nLLaVA-HR (1024 pix): 3\nUser: what page is this book on?\nLLaVA-HR (384 pix): This is the first \npage of a book.\nLLaVA-HR (1024 pix): This book is on \npage 87.\nUser: what are the last three words on \nthis page?\nLLaVA-HR (384 pix): \"and other stories.\"\nLLaVA-HR (1024 pix): Wo ich wohne.\nLLaVA-HR (384 pix): \n[0.92, 0.69, 0.97, 0.72] \nLLaVA-HR (1024 pix): \n[0.66, 0.68, 0.74, 0.73]\nGround-truth: [0.66, 0.68, 0.74, 0.74]\nUser: Please provide the bounding \nbox coordinate of the region this \nsentence describes: The  child \nsliding down in red clothes.\nLLaVA-HR (384 pix): \n[0.4, 0.6, 0.51, 0.78] \nLLaVA-HR (1024 pix): \n[0.53, 0.57, 0.6, 0.71]\nGround-truth: [0.52, 0.56, 0.6, 0.7]\nUser: Please read the text in this image and return the information in the \nfollowing JSON format (note xxx is placeholder, if the information is not \navailable in the image, put \"N/A\" instead). {\"class\": XXX\uff0c\"DLN\": Xxx\uff0c\n\"DOB\": Xxx\uff0c\"Name\": XXX\uff0c\"Address\": XXX\uff0c\"EXP\": XXX\uff0c\"ISS\": XXX\uff0c\n\"SEX\": XXX\uff0c\"HGT\": XXX, WGT\": XXX\uff0c\"EYES\": XXX\uff0c\"HAIR\": XXX\uff0c\n\"DONOR\": XXX}\nGPT4-V:{\"class\": \"D\", \"DLN\": \"97542954****\", \"DOB\": \"15-03-1984\", \"Name\": \n\"GP VEERE*****\", \"Address\": \"52 KLINK RD ROCHESTER ** *****\", \"EXP\": \n\"10-01-16\", \"ISS\": \"09-30-08\", \"SEX\": \"M\", \"HGT\": \"5-09\", \"WGT\": \"N/A\", \n\"EYES\": \"BR\", \"HAIR\": \"N/A\", \"DONOR\": \"YES\"}\nLLaVA-1.5: {\"class\": Class D, \"DLN\": 97745434, \"DOB\": 03/15/1988, \"Name\": Gopal \nVeerendhar, \"Address\": 54 Klinck Rd, Rochester, NY 14607, \"EXP\": 10/01/2016 - \n10/01/2017, \"ISS\": New York State Department of Motor Vehicles, \"SEX\": Male, \n\"HGT\": 5'10\", \"WGT\": 170, \"EYES\": Brown, \"HAIR\": Black, \"DONOR\": N/A}\nLLaVA-HR-X: {\"class\": Class D, \"DLN\": 97542954****, \"DOB\": 15/03/1984, \n\"Name\": GP Veere*****, \"Address\": 52 Klink Rd, Rochester ** *****, \"EXP\": \n10/01/2016, \"ISS\": New York State, \"SEX\": Male, \"HGT\": N/A, \"WGT\": N/A, \n\"EYES\": Brown, \"HAIR\": Black, \"DONOR\": N/A}\n(a) Comparison of LLaVA-HR with different resolutions \n(b) Comparison of LLaVA-HR-X, LLaVA-1.5 and GPT4-V in visual information extraction\nUser: \nPlease \nprovide \nthe \nbounding box coordinate of the \nregion this sentence describes: \nThe deer on the far right.\nFigure 5. Visualizations of LLaVA-HR and existing MLLMs. Subfig-(a) shows that high image resolution greatly improves the\ncapability of MLLMs on fine-grained VL tasks. In Subfig-(b), LLaVA-HR-X demonstrates the comparable ability with GPT4-V in visual\ninformation extraction2. Correct and incorrect answers are colored in green and red, respectively.\ncontrast, most images of ScienceQA are synthetic and of low\nresolution, so the advantages of LLaVA-HR are not obvious.\nOverall, these results greatly confirm the effectiveness and\ngeneralization of LLaVA-HR and our MRA.\n5.3.2. QUALITATIVE EXPERIMENTS\nIn Fig 5 (a), we compare the predictions of LLaVA-HR\nwith different resolutions. The visualizations show that\nhigher image resolution obviously improves the capability\nof MLLMs on fine-grained tasks. For example, LLaVA-HR\nwith a resolution of 1,024 \u00d7 1,024 can well capture gran-\nular visual content, e.g., the tiny boat in the first example.\nBesides, high image resolution also enables LLaVA-HR a\nstronger ability of text recognition. For instance, the small\nand blurred phrase of \u201cwo ich wohne\u201d in the second example\nare correctly identified by the high-resolution LLaVA-HR.\nThese results greatly confirm the significance of high im-\nage resolution in addressing visual shortcoming. In Fig 5\n(b), we further compare the predictions of LLaVA-HR-X,\nLLaVA-1.5 (Liu et al., 2023a) and GPT4-V (OpenAI, 2023)\n2For privacy reasons, we blur some key personal information.\nin visual information extraction. Notably, LLaVA-HR-X\nshows a comparable ability with GPT4-V on this challeng-\ning task. As shown in Fig 5 (b), LLaVA-HR-X and GPT4-V\ncan correctly extract almost all visual content of the driver\nlicense and organize it in JSON format. Compared to GPT4-\nV, LLaVA-HR-X also correctly identifies the hair color of\nthe person, which requires fine-grained visual reasoning. In\ncontrast, LLaVA-1.5 can only recognize simple visual con-\ntent like \u201cclass\u201d and \u201cSEX\u201d, and fail to extract most visual\ninformation. These results further validate the effectiveness\nof MRA in addressing visual shortcoming of MLLMs.\n6. Conclusion\nIn this paper, we study the visual shortcoming of MLLMs\nfrom the perspective of image resolution, and propose a\nnovel and efficient method for high-resolution adaptations of\nMLLMs, namely mixture-of-resolution adaptation (MRA).\nMRA adopts dual visual pathways to process images of both\nhigh and low resolutions, where high-resolution informa-\ntion is embeded into the low-resolution modeling via the\nnovel mixture-of-resolution adapters (MR-Adapters). We\n8\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\napply MRA to a popular MLLM called LLaVA-1.5, and\nconstruct a new high-resolution MLLM, termed LLaVA-\nHR. Experimental results not only validate the effectiveness\nof LLaVA-HR in addressing visual shortcoming, but also\nconfirm its remarkable efficiency against existing MLLMs.\nAcknowledgements.\nThis work was supported by Na-\ntional Key R&D Program of China (No.2022ZD0118201) ,\nthe National Science Fund for Distinguished Young Scholars\n(No.62025603), the National Natural Science Foundation of\nChina (No. U21B2037, No. U22B2051, No. 62176222, No.\n62176223, No. 62176226, No. 62072386, No. 62072387,\nNo. 62072389, No. 62002305 and No. 62272401), the\nNatural Science Foundation of Fujian Province of China\n(No.2021J01002, No.2022J06001), and the China Funda-\nmental Research Funds for the Central Universities (Grant\nNo. 20720220068).\nReferences\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. arXiv preprint arXiv:2204.14198, 2022.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,\nZhou, C., and Zhou, J. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic. arXiv preprint arXiv:2306.15195, 2023a.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. E. Big self-supervised models are strong\nsemi-supervised learners. Advances in neural information\nprocessing systems (NeurIPS), 33:22243\u201322255, 2020.\nChen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,\nPadlewski, P., Salz, D., Goodman, S., Grycner, A.,\nMustafa, B., Beyer, L., et al.\nPali: A jointly-scaled\nmultilingual language-image model.\narXiv preprint\narXiv:2209.06794, 2022.\nChen, X., Wang, X., Beyer, L., Kolesnikov, A., Wu, J.,\nVoigtlaender, P., Mustafa, B., Goodman, S., Alabdul-\nmohsin, I., Padlewski, P., et al.\nPali-3 vision lan-\nguage models: Smaller, faster, stronger. arXiv preprint\narXiv:2310.09199, 2023b.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\ngeneral-purpose vision-language models with instruction\ntuning. arXiv preprint arXiv:2305.06500, 2023.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu,\nZ., Lin, W., Yang, J., Zheng, X., et al. Mme: A compre-\nhensive evaluation benchmark for multimodal large lan-\nguage models. arXiv preprint arXiv:2306.13394, 2023.\nFuyu-8B.\nhttps://www.adept.ai/blog/\nfuyu-8b, 2023.\nGilardi, F., Alizadeh, M., and Kubli, M. Chatgpt outper-\nforms crowd-workers for text-annotation tasks. arXiv\npreprint arXiv:2303.15056, 2023.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answer-\ning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 6904\u20136913, 2017.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\nAnswering visual questions from blind people. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3608\u20133617, 2018.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In CVPR, 2019.\nIlharco, G., Wortsman, M., Wightman, R., Gordon,\nC., Carlini, N., Taori, R., Dave, A., Shankar, V.,\nNamkoong, H., Miller, J., Hajishirzi, H., Farhadi, A.,\nand Schmidt, L.\nOpenclip.\nJuly 2021.\ndoi: 10.\n5281/zenodo.5143773. URL https://doi.org/10.\n5281/zenodo.5143773. If you use this software,\nplease cite it as below.\nJiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., and\nChen, X. In defense of grid features for visual question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 10267\u2013\n10276, 2020.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.\nSeed-bench: Benchmarking multimodal llms with gener-\native comprehension. arXiv preprint arXiv:2307.16125,\n2023a.\n9\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nLi, J., Li, D., Savarese, S., and Hoi, S.\nBlip-2: Boot-\nstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023b.\nLi, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen,\nJ.-R.\nEvaluating object hallucination in large vision-\nlanguage models.\narXiv preprint arXiv:2305.10355,\n2023c.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. In NeurIPS, 2023b.\nLiu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou,\nX., Yang, J., Su, H., Zhu, J., Zhang, L., Gao, J., and Li, C.\nLlava-plus: Learning to use tools for creating multimodal\nagents, 2023c.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,\nand Xie, S. A convnet for the 2020s. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11976\u201311986, 2022.\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. arXiv preprint arXiv:1908.02265,\n2019.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. Advances in Neural Infor-\nmation Processing Systems, 2022.\nLuo, G., Zhou, Y., Ren, T., Chen, S., Sun, X., and Ji, R.\nCheap and quick: Efficient vision-language instruction\ntuning for large language models. Advances in neural\ninformation processing systems (NeurIPS), 2023a.\nLuo, G., Zhou, Y., Sun, J., Sun, X., and Ji, R. A survivor\nin the era of large-scale pretraining: An empirical study\nof one-stage referring expression comprehension. IEEE\nTransactions on Multimedia, 2023b.\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-\nvqa: A visual question answering benchmark requiring\nexternal knowledge. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019.\nMerigan, W. H. and Maunsell, J. H. How parallel are the\nprimate visual pathways? Annual review of neuroscience,\n16(1):369\u2013402, 1993.\nMishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A.\nOcr-vqa: Visual question answering by reading text in\nimages. In 2019 international conference on document\nanalysis and recognition (ICDAR), pp. 947\u2013952. IEEE,\n2019.\nOpenAI.\nGpt-4v(ision)\nsystem\ncard.\nhttps:\n//cdn.openai.com/papers/GPTV_System_\nCard.pdf, 2023.\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F.\nKosmos-2: Grounding multimodal\nlarge language models to the world.\narXiv preprint\narXiv:2306.14824, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. arXiv preprint arXiv:2103.00020,\n2021.\nRen, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J.,\nHuang, X., Chen, Y., Yan, F., Zeng, Z., Zhang, H., Li, F.,\nYang, J., Li, H., Jiang, Q., and Zhang, L. Grounded sam:\nAssembling open-world models for diverse visual tasks,\n2024.\nRobertson, L. C. and Lamb, M. R. Neuropsychological\ncontributions to theories of part/whole organization. Cog-\nnitive psychology, 23(2):299\u2013330, 1991.\nRose, D., Himakunthala, V., Ouyang, A., He, R., Mei, A.,\nLu, Y., Saxon, M., Sonar, C., Mirza, D., and Wang, W. Y.\nVisual chain of thought: Bridging logical gaps with multi-\nmodal infillings. arXiv preprint arXiv:2305.02317, 2023.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 8317\u20138326, 2019.\nTong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie,\nS. Eyes wide shut? exploring the visual shortcomings\nof multimodal llms. arXiv preprint arXiv:2401.06209,\n2024.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nYu, J., Li, J., Yu, Z., and Huang, Q. Multimodal transformer\nwith multi-view visual representation for image caption-\ning. IEEE transactions on circuits and systems for video\ntechnology, 30(12):4467\u20134480, 2019.\n10\nFeast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L.\nMm-vet: Evaluating large multi-\nmodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023.\nZhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,\nChoi, Y., and Gao, J. Vinvl: Revisiting visual representa-\ntions in vision-language models. In CVPR, 2021.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n11\n"
  },
  {
    "title": "RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches",
    "link": "https://arxiv.org/pdf/2403.02709.pdf",
    "upvote": "6",
    "text": "RT-Sketch: Goal-Conditioned Imitation Learning\nfrom Hand-Drawn Sketches\nPriya Sundaresan1,3, Quan Vuong2, Jiayuan Gu2, Peng Xu2, Ted Xiao2, Sean Kirmani2, Tianhe Yu2,\nMichael Stark3, Ajinkya Jain3, Karol Hausman1,2, Dorsa Sadigh\u22171,2, Jeannette Bohg\u22172, Stefan Schaal\u22173\n\u2217Equal advising, alphabetical order\n1Stanford University, 2Google DeepMind, 3[Google] Intrinsic\nFig. 1: (Left) Qualitative rollouts comparing RT-Sketch, RT-1, and RT-Goal-Image, (right) highlighting RT-Sketch\u2019s robustness to (top) ambiguous language\nand (bottom) visual distractors.\nAbstract\u2014Natural language and images are commonly used as\ngoal representations in goal-conditioned imitation learning (IL).\nHowever, natural language can be ambiguous and images can\nbe over-specified. In this work, we study hand-drawn sketches\nas a modality for goal specification. Sketches are easy for users\nto provide on the fly like language, but similar to images they\ncan also help a downstream policy to be spatially-aware and\neven go beyond images to disambiguate task-relevant from task-\nirrelevant objects. We present RT-Sketch, a goal-conditioned\npolicy for manipulation that takes a hand-drawn sketch of\nthe desired scene as input, and outputs actions. We train RT-\nSketch\non a dataset of trajectories paired with synthetically\ngenerated goal sketches. We evaluate this approach on six\nmanipulation skills involving tabletop object rearrangements\non an articulated countertop. Experimentally we find that RT-\nSketch is able to perform on a similar level to image or language-\nconditioned agents in straightforward settings, while achieving\ngreater robustness when language goals are ambiguous or visual\ndistractors are present. Additionally, we show that RT-Sketch has\nthe capacity to interpret and act upon sketches with varied levels\nof specificity, ranging from minimal line drawings to detailed,\ncolored drawings. For supplementary material and videos, please\nrefer to our website.\nI. INTRODUCTION\nRobots operating alongside humans in households, work-\nplaces, or industrial environments have an immense potential\nfor assistance and autonomy, but careful consideration is\nneeded of what goal representations are easiest for humans\nto convey to robots, and for robots to interpret and act upon.\nInstruction-following robots attempt to address this problem\nusing the intuitive interface of natural language commands as\ninputs to language-conditioned imitation learning policies [8,\n9, 23, 28, 29]. For instance, imagine asking a household robot\nto set the dinner table. A language description such as \u201cput\nthe utensils, the napkin, and the plate on the table\u201d is under-\nspecified or ambiguous. It is unclear how exactly the utensils\nshould be positioned relative to the plate or the napkin, or\nwhether their distances to each other matter or not. To achieve\nthis higher level of precision, a user may need to give lengthier\ndescriptions such as \u201cput the fork 2cm to the right of the\nplate, and 5cm to the leftmost edge of the table.\u201d, or even\nonline corrections (\u201cno, you moved too far to the right, move\nback a bit!\u201d) [15, 29]. While language is an intuitive way to\nspecify goals, its qualitative nature and ambiguities can make\nit both inconvenient for humans to provide without lengthy\ninstructions or corrections, and for robot policies to interpret\nfor downstream precise manipulation.\nUsing goal images to specify objectives and training goal-\nconditioned imitation learning policies either paired with or\nwithout language instructions has shown to be quite successful\nin recent years [21, 22, 35]. In these settings, an image of the\narXiv:2403.02709v1  [cs.RO]  5 Mar 2024\nscene in its desired final state could fully specify the intended\ngoal. However, this has its own shortcomings: access to a\ngoal image is a strong prior assumption, and a pre-recorded\ngoal image can be tied to a particular environment, making it\ndifficult to reuse for generalization.\nTo summarize: while natural language is highly flexible, it\ncan also be highly ambiguous or require lengthy descriptions.\nThis quickly becomes difficult in long-horizon tasks or those\nrequiring spatial awareness. Meanwhile, goal images over-\nspecify goals in unnecessary detail, leading to the need for\ninternet-scale data for generalization.\nTo address these challenges, we study hand-drawn sketches\nas a convenient yet expressive modality for goal specification\nin visual imitation learning. By virtue of being minimal,\nsketches are still easy for users to provide on the fly like\nlanguage, but allow for more spatially-aware task specification.\nLike goal images, sketches readily integrate with off-the-shelf\npolicy architectures that take visual input, but provide an added\nlevel of goal abstraction that ignores unnecessary pixel-level\ndetails. Finally, the quality and selective inclusion/exclusion of\ndetails in a sketch can help a downstream policy distinguish\ntask relevant from irrelevant details.\nIn this work, we present RT-Sketch, a goal-conditioned\npolicy for manipulation that takes a user-provided hand-drawn\nsketch of the desired scene as input, and outputs actions.\nThe novel architecture of RT-Sketch modifies the original RT-\n1 language-to-action Transformer architecture [9] to consume\nvisual goals rather than language, allowing for flexible condi-\ntioning on sketches, images, or any other visually representable\ngoals. To enable this, we concatenate a goal sketch and\nhistory of observations as input before tokenization, omitting\nlanguage. We train RT-Sketch on a dataset of 80K trajectories\npaired with synthetic goal sketches, generated by an image-to-\nsketch stylization network trained from a few hundred image-\nsketch pairs.\nWe evaluate RT-Sketch across six manipulation skills on\nreal robots involving tabletop object rearrangements on a\ncountertop with drawers, subject to a wide range of scene\nvariations. These skills include moving objects near to one\nanother, knocking a can sideways, placing a can upright,\nclosing a drawer, and opening a drawer. Experimentally, we\nfind that RT-Sketch performs on a similar level to image or\nlanguage-conditioned agents in straightforward settings. When\nlanguage instructions are ambiguous, or in the presence of\nvisual distractors, we find that RT-Sketch achieves \u223c 2X more\nspatial precision and alignment scores, as assessed by human\nlabelers, over language or goal image-conditioned policies\n(see Fig. 1 (right)). Additionally, we show that RT-Sketch can\nhandle different levels of input specificity, ranging from rough\nsketches to more scene-preserving, colored drawings (see\nFig. 1 (left)).\nII. RELATED WORK\nIn this section, we discuss prior methods for goal-\nconditioned imitation learning. We also highlight ongoing\nefforts towards image-sketch conversion, which open new\npossibilities for goal-conditioning modalities which are under-\nexplored in robotics.\na) Goal-Conditioned Imitation Learning: Despite the\nsimilarity in name, our learning of manipulation policies\nconditioned on hand-drawn sketches of the desired scene is\ndifferent from the notion of policy sketches [1], symbolic\nrepresentations of task structure describing its subcomponents.\nReinforcement learning (RL) is not easily applicable in our\nscenario, as it is nontrivial to define a reward objective which\naccurately quantifies alignment between a provided scene\nsketch and states visited by an agent during training. We in-\nstead focus on imitation learning (IL) techniques, particularly\nthe goal-conditioned setting [17].\nGoal-conditioned IL has proven useful in settings where a\npolicy must be able to handle spatial or semantic variations\nfor the same task [3]. These settings include rearrangement\nof multiple objects [8, 9, 29, 30, 35], kitting [46], folding\nof deformable objects into different configurations [18], and\nsearch for different target objects in clutter [16]. However,\nthese approaches tend to either rely on language [9, 23, 28,\n29, 39], or goal images [16] to specify variations. Follow-\nup work enabled multimodal conditioning on either goal\nimages and language [21], in-prompt images [22], or image\nembeddings [18, 30, 46]. However, all of these representations\nare ultimately derived from raw images or language in some\nway, which overlooks the potential for more abstract goal\nrepresentations that are easy to specify but preserve spatial\nawareness, such as sketches.\nIn addition to their inflexibility in terms of goal representa-\ntion, goal-conditioned IL tends to overfit to demonstration data\nand fails to handle even slight distribution shift in new sce-\nnarios [37]. For language-conditioning, distribution shift can\nencompass semantic or spatial ambiguity, novel instructions or\nphrasing, or unseen objects [9, 21]. Goal-image conditioning\nis similarly susceptible to out-of-distribution visual shift, such\nas variations in lighting or object appearances, or unseen\nbackground textures [5, 11]. We instead opt for sketches\nwhich are minimal enough to combat visual distractors, yet\nexpressive enough to provide unambiguous goals. Prior work,\nincluding\n[4] and\n[32], have shown the utility of sketches\nover pure language for navigation and limited manipulation\nsettings. However, the sketches explored in these works are\nlargely intended to guide low-level motion at the joint-level\nfor manipulation, or provide explicit directional cues for\nnavigation. [14] considers sketches amongst other modalities\nas an input for goal-conditioned manipulation, but does not\nexplicitly train a policy conditioned on sketches. They thus\ncame to the conclusion that the scene image is better than\nthe sketch image at goal specification. Our result is different\nand complementary, in that policies trained to take sketches\nas input outperform a scene image conditioned policy, by\n1.63x and 1.5x in terms of Likert ratings for perceived spatial\nand semantic alignment, subject to visual distractors. Most re-\ncently, Gu et al. [19] propose an approach to goal-conditioned\nmanipulation via hindsight-trajectory sketches. Here, sketches\nrepresent 2D paths drawn over an image to indicate the desired\ntrajectory for the robot to follow. While this work treats\nsketches as a motion-centric representation to specify intended\nrobot trajectories, the sketches in our work are scene-centric,\nrepresenting the desired visual goal state rather than the actions\nthe robot should explicitly take.\nb) Image-Sketch Conversion: In recent years, sketches\nhave gained increasing popularity within the computer vision\ncommunity for applications such as object detection [6, 7, 12],\nvisual question answering [25, 33], and scene understand-\ning [13], either in isolation or in addition to text and images.\nWhen considering how best to incorporate sketches in IL,\nan important design choice is whether to take sketches into\naccount (1) at test time (i.e., converting a sketch to another\ngoal modality compatible with a pre-trained policy), or (2) at\ntraining time (i.e., explicitly training an IL policy conditioned\non sketches). For (1), one could first convert a given sketch to a\ngoal image, and then roll out a vanilla goal-image conditioned\npolicy. This could be based on existing frameworks for sketch-\nto-image conversion, such as ControlNet [47], GAN-style\napproaches [24], or text-to-image synthesis, such as Instruct-\nPix2Pix [10] or Stable Diffusion [36]. While these models\nproduce photorealistic results under optimal conditions, they\ndo not jointly handle image generation and style transfer,\nmaking it unlikely for generated images to match the style\nof an agent observations. At the same time, these approaches\nare susceptible to producing hallucinated artifacts, introducing\ndistribution shifts [47].\nBased on these challenges, we instead opt for (2), and\nconsider image-to-sketch conversion techniques for hindsight\nrelabeling of terminal images in pre-recorded demonstration\ntrajectories. Recently, Vinker et al. [44, 45] proposes networks\nfor predicting Bezier curve-based sketches of input image\nobjects or scenes. Sketch quality is supervised by a CLIP-\nbased alignment metric. While these approaches generate\nsketches of high visual fidelity, test-time optimization takes\non the order of minutes, which does not scale to the typical\nsize of robot learning datasets (hundreds to thousands of\ndemonstration trajectories). Meanwhile, conditional generative\nadversarial networks (cGANs) such as Pix2Pix [20] have\nproven useful for scalable image-to-image translation. Most\nrelated to our work is that of Li et al. [26], which trains a\nPix2Pix model to produce sketches from given images on a\nlarge crowd-sourced dataset of 5K paired images and line\ndrawings. We build on this work to fine-tune an image-to-\nsketch model on robot trajectory data, and show its utility for\nenabling downstream manipulation from sketches.\nIII. SKETCH-CONDITIONED IMITATION LEARNING\nIn this section, we will first introduce our problem of learn-\ning a sketch-conditioned policy. We will then discuss our ap-\nproach to train an end-to-end sketch-to-action IL agent. First,\nin Section III-A, we discuss our instantiation of an auxiliary\nimage-to-sketch translation network which automatically gen-\nerates sketches from a reference image. In Section III-B, we\ndiscuss how we use such a model to automatically hindsight\nrelabel an existing dataset of demonstrations with synthetically\ngenerated goal sketches, and train a sketch-conditioned policy\non this dataset.\na) Problem Statement: Our goal is to learn a manipula-\ntion policy conditioned on a goal sketch of the desired scene\nstate and a history of interactions. Formally, we denote such a\npolicy by \u03c0sketch(at|g, {oj}t\nj=1), where at denotes an action\nat timestep t, g \u2208 RW \u00d7H\u00d73 is a given goal sketch with width\nW and height H, and ot \u2208 RW \u00d7H\u00d73 is an observation at\ntime t. At inference time, the policy takes a given goal sketch\nalong with a history of RGB image observations to infer\nan action to execute. In practice, we condition \u03c0sketch on a\nhistory of D previous observations rather than all observations\nfrom the initial state at t = 1. To train such a policy, we\nassume access to a dataset Dsketch = {gn, {(on\nt , an\nt )}T (n)\nt=1 }N\nn=1\nof N successful demonstrations, where T (n) refers to the\nlength of the nth trajectory in timesteps. Each episode of the\ndataset consists of a given goal sketch and a corresponding\ndemonstration trajectory, with image observations recorded at\neach timestep. Our goal is to thus learn the sketch-conditioned\nimitation policy \u03c0sketch(at|g, {oj}t\nj=1) trained on this dataset\nDsketch.\nA. Image-to-Sketch Translation\nTraining a sketch-conditioned policy requires a dataset of\nrobot trajectories that are each paired with a sketch of the\ngoal state achieved by the robot. Collecting such a dataset\nfrom scratch at scale, including the trajectories themselves\nand manually drawn sketches, can easily become impractical.\nThus, we instead aim to learn an image-to-sketch transla-\ntion network T (g|o) that takes an image observation o and\noutputs the corresponding goal sketch g. This network can\nbe used to post-process an existing dataset of demonstra-\ntions D = {{(on\nt , an\nt )}T (n)\nt=1 }N\nn=1 with image observations\nby appending a synthetically generated goal sketch to each\ndemonstration. This produces a dataset for sketch-based IL:\nDsketch = {gn, {(on\nt , an\nt )}T (n)\nt=1 }N\nn=1.\na) RT-1 Dataset: In this work, we rely on an existing\ndataset of visual demonstrations collected by prior work [9].\nRT-1 is a prior language-to-action imitation learning agent\ntrained on a large-scale dataset (80K trajectories) of VR-\nteleoperated demonstrations that include skills such as moving\nobjects near one another, placing cans and bottles upright or\nsideways, opening and closing cabinets, and performing pick\nand place on countertops and drawers [9]. Here, we repurpose\nthe RT-1 dataset and further adapt the RT-1 policy architecture\nto accommodate sketches, detailed in Section III-B.\nb) Assumptions on Sketches: We acknowledge that there\nare innumerable ways for a human to provide a sketch\ncorresponding to a given image of a scene. In this work,\nwe make the following assumptions about input sketches for\na controlled experimental validation procedure. In particular,\nwe first assume that a given sketch respects the task-relevant\ncontours of an associated image, such that tabletop edges,\ndrawer handles, and task-relevant objects are included and\ndiscernible in the sketch. We do not assume contours in the\nsketch to be edge-aligned or pixel-aligned with those in an\nFig. 2: Architecture of RT-Sketch allowing different kinds of visual input. RT-Sketch adopts the Transformer [43] architecture with EfficientNet [41] tokenization\nat the input, and outputs bucketized actions.\nimage. We do assume that the input sketch consists of black\noutlines at the very least, with shading in color being optional.\nWe further assume that sketches do not contain information not\npresent in the associated image, such as hallucinated objects,\nscribbles, or textual annotations, but may omit task-irrelevant\ndetails that appear in the original image.\nc) Sketch Dataset Generation: To train an image-to-\nsketch translation network T , we collect a new dataset DT =\n{(oi, g1\ni , . . . , gL(i)\ni\n)}M\ni=1 consisting of M image observations\noi each paired with a set of goal sketches g1\ni , . . . , gL(i)\ni\n. Those\nrepresent L(i) different representations of the same image oi,\nin order to account for the fact that there are multiple, valid\nways of sketching the same scene. To collect DT , we take\n500 randomly sampled terminal images from demonstration\ntrajectories in the RT-1 dataset, and manually draw sketches\nwith black lines on a white background capturing the tabletop,\ndrawers, and relevant objects visible on the manipulation\nsurface. While we personally annotate each robot observation\nwith a single sketch only, we add this data to an existing, much\nlarger non-robotic dataset [26]. This dataset captures inter-\nsketch variation via multiple crowdsourced sketches per image.\nWe do not include the robot arm in our manual sketches, as we\nfind a minimal representation to be most natural. Empirically,\nwe find that our policy can handle such sketches despite actual\ngoal configurations likely having the arm in view. We collect\nthese drawings using a custom digital stylus drawing interface\nin which a user draws an edge-aligned sketch over the original\nimage (Appendix Fig. 16). The final recorded sketch includes\nthe user\u2019s strokes in black on a white canvas with the original\nimage dimensions.\nd) Image-to-Sketch Training: We implement the image-\nto-sketch translation network T with the Pix2Pix conditional\ngenerative adversarial network (cGAN) architecture, which is\ncomposed of a generator GT and a discriminator DT [20].\nThe generator GT takes an input image o, a random noise\nvector z, and outputs a goal sketch g. The discriminator DT is\ntrained to discriminate amongst artificially generated sketches\nand ground truth goal sketches. We utilize the standard cGAN\nsupervision loss to train both [20, 26]:\nLcGAN = min\nGT max\nDT Eo,g[log DT (o, g)] +\nEo,g[log(1 \u2212 DT (o, GT (o, g))]\n(1)\nWe also add the L1 loss to encourage the produced sketches\nto align with the ground truth sketches as in [26]. To account\nfor the fact that there may be multiple valid sketches for a\ngiven image, we only penalize the minimum L1 loss incurred\nacross all L(i) sketches provided for a given image as in Li\net al. [26]. This is to prevent wrongly penalizing T\nfor\nproducing a valid sketch that aligns well with one example\nbut not another simply due to stylistic differences in the\nground truth sketches. The final objective is then a \u03bb-weighted\ncombination of the average cGAN loss and the minimum\nalignment loss:\nLT =\n\u03bb\nL(i)\nL(i)\nX\nk=1\nLcGAN(oi, g(k)\ni\n) +\nmin\nk\u2208{1,...,L(i)} L1(oi, g(k)\ni\n)\n(2)\nIn practice, we supplement the 500 manually drawn sketches\nfrom DT\nby leveraging the existing larger-scale Contour\nDrawing Dataset [26]. We refer to this dataset as DCD, which\ncontains 1000 examples of internet-scraped images contain-\ning objects, people, animals from Adobe Stock, paired with\nL(i) = 5 crowd-sourced black and white outline drawings per\nimage collected on Amazon Mechanical Turk. Visualizations\nof this dataset are provided in Appendix Fig. 5. We first take\na pre-trained image-to-sketch translation network TCD [26]\ntrained on DCD, with L(i) = 5 sketches per image. Then, we\nfine-tune TCD on DT , with only L(i) = 1 manually drawn\nsketch per robot observation, to obtain our final image-to-\nsketch network T . Visualizations of the sketches generated\nby T for different robot observations are available in Fig. 6.\nB. RT-Sketch\nWith a means of translating image observations to black and\nwhite sketches via T (Section III-A), we can automatically\naugment the existing RT-1 dataset with goal sketches. This\nresults in a dataset, which we refer to as Dsketch, which can\nbe used for training our algorithm, RT-Sketch.\na) RT-Sketch Dataset: The original RT-1 dataset Dlang =\n{in, {(on\nt , an\nt )}T (n)\nt=1 }N\nn=1 consists of N episodes with a paired\nnatural language instruction i and demonstration trajectory\n{(on\nt , an\nt )}T n\nt=1. We can automatically hindsight-relabel such\na dataset with goal images instead of language goals [2].\nLet us denote the last step of a trajectory n as T (n). Then\nthe new dataset with image goals instead of language goals\nis Dimg = {on\nT (n), {(on\nt , an\nt )}T (n)\nt=1 }N\nn=1, where we treat the\nlast observation of the trajectory on\nT (n) as the goal gn. To\nproduce a dataset for \u03c0sketch, we can simply replace on\nT (n) with\n\u02c6gn = T (on\nT (n)) such that Dsketch = {\u02c6gn, {(on\nt , an\nt )}T (n)\nt=1 }N\nn=1.\nTo encourage the policy to afford different levels of input\nsketch specificity, we in practice produce goals by \u02c6gn =\nA(on\nT (n)), where A is a randomized augmentation function.\nA chooses between simply applying T , T with colorization\nduring postprocessing (e.g., by superimposing a blurred ver-\nsion of the ground truth RGB image over the binary sketch),\na Sobel operator [40] for edge detection, or not applying\nany operators, which preserves the original ground truth goal\nimage (Fig. 2). By co-training on all representations, we intend\nfor RT-Sketch to handle a spectrum of specificity going from\nbinary sketches; colorized sketches; edge detected images; and\ngoal images (Appendix Fig. 6).\nb) RT-Sketch Model Architecture: In our setting, we\nconsider goals provided as sketches rather than language\ninstructions as was done in RT-1. This change in the input\nrepresentation necessitates a change in the model architecture.\nThe original RT-1 policy relies on a Transformer architecture\nbackbone [43]. RT-1 first passes a history of D = 6 im-\nages through an EfficientNet-B3 model [41] producing image\nembeddings, which are tokenized, and separately extracts\ntextual embeddings and tokens via FiLM [31] and a Token\nLearner [38]. The tokens are then fed into a Transformer which\noutputs bucketized actions. The output action dimensionality is\n7 for the end-effector (x, y, z, roll, pitch, yaw, gripper width),\n3 for the mobile base, (x, y, yaw), and 1 for a flag that can\nselect amongst base movement, arm movement, and episode\ntermination. To retrain the RT-1 architecture but accommodate\nthe change in input representation, we omit the FiLM language\ntokenization altogether. Instead, we concatenate a given goal\nimage or sketch with the history of images as input to\nEfficientNet, and extract tokens from its output, leaving the\nrest of the policy architecture unchanged. We visualize the\nRT-Sketch training inputs and policy architecture in Fig. 2.\nWe refer to this architecture when trained only on images\n(i.e., an image goal-conditioned RT1 policy) as RT-Goal-Image\nand refer to it as RT-Sketch when it is trained on sketches as\ndiscussed in this section.\nc) Training RT-Sketch: We can now train \u03c0sketch on\nD\u03c0sketch utilizing the same procedure as was used to train RT-\n1 [9], with the above architectural modifications. We fit \u03c0sketch\nusing the behavioral cloning objective function. This aims to\nminimize the negative log-likelihood of an action provided the\nhistory of observations and a given sketch goal [42]:\nJ(\u03c0sketch) =\nN\nX\nn=1\nT (n)\nX\nt=1\nlog \u03c0sketch(an\nt |gn, {oj}t\nj=1)\nIV. EXPERIMENTS\nWe seek to understand the ability of RT-Sketch to perform\ngoal-conditioned manipulation as compared to policies that\noperate from higher-level goal abstractions like language, or\nmore over-specified modalities, like goal images. To that end,\nwe test the following four hypotheses:\nH1: RT-Sketch is successful at goal-conditioned IL.\nWhile sketches are abstractions of real images, our hypothesis\nis that they are specific enough to provide manipulation goals\nto a policy. Therefore, we expect RT-Sketch to perform on\na similar level to language goals (RT-1) or goal images (RT-\nGoal-Image) in straighforward manipulation settings.\nH2: RT-Sketch is able to handle varying levels of\nspecificity. There are as many ways to sketch a scene as there\nare people. Because we have trained RT-Sketch on sketches of\nvarying levels of specificity, we expect it to be robust against\nvariations of the input sketch for the same scene.\nH3: Sketches enable better robustness to distractors\nthan goal images. Sketches focus on task-relevant details of a\nscene. Therefore, we expect RT-Sketch to provide robustness\nagainst distractors in the environment that are not included\nin the sketch compared to RT-Goal-Image that operates on\ndetailed image goals.\nH4: Sketches are favorable when language is ambigu-\nous. We expect RT-Sketch to provide a higher success rate\ncompared to ambiguous language inputs when using RT-1.\nA. Experimental Setup\na) Policies:\nWe compare RT-Sketch to the original\nlanguage-conditioned agent RT-1 [9], and RT-Goal-Image, a\npolicy identical in architecture to RT-Sketch, but taking a goal\nimage as input rather than a sketch. All policies are trained on\na multi-task dataset of \u223c 80K real-world trajectories manually\ncollected via VR teleoperation using the setup from Brohan\net al. [9]. These trajectories span a suite of common office\nand kitchen tasks such as picking and placing objects, re-\norienting cups and bottles upright or sideways, opening and\nclosing drawers, and rearranging objects between drawers or\na countertop.\nb) Evaluation protocol: To ensure fair comparison, we\ncontrol for the same initial and goal state of the environment\nacross different policy rollouts via a catalog of well-defined\nevaluation scenarios that serve as references for human robot\noperators. For each scenario, we record an initial image\n(RGB observation) of the scene, the goal image (with objects\nmanually rearranged as desired), a natural language task string\ndescribing the desired agent behavior to achieve the goal, and a\nset of hand-drawn sketches corresponding to the recorded goal\nimage. At test time, a human operator retrieves a particular\nevaluation scenario from the catalog, aligns the physical robot\nand scene according to a reference image using a custom\nFig. 3: Goal Alignment Results: Average Likert scores for different policies rating perceived semantic alignment (Q1) and spatial alignment (Q2) to a\nprovided goal. For straightforward benchmark manipulation tasks, RT-Sketch performs comparably and in some cases better than RT-1 and RT-Goal-Image in\nterms of both metrics, for 5 out of 6 skills (H1). RT-Sketch further exhibits the ability to handle sketches of different levels of detail (H2), while achieving\nbetter goal alignment than baselines when the visual scene is distracting (H3) or language would be ambiguous (H4). Error bars indicate standard error across\nlabeler ratings.\nvisualization utility, and places the relevant objects in their\nrespective locations. Finally, the robot selects one of the goal\nrepresentations (language, image, sketch, etc.) for the scenario\nas input to a policy. We record a video of the policy rollout\nfor downstream evaluation (see Section IV-B). We perform\nall experiments using the Everyday Robot, which contains a\nmobile base, an overhead camera, and a 7-DoF manipulator\narm with a parallel jaw gripper. All sketches for evaluation are\ncollected with a custom manual drawing interface by a single\nhuman annotator on a tablet with a digital stylus.\nc) Performance Metrics: Defining a standardized, auto-\nmated evaluation protocol for goal alignment is non-trivial.\nSince binary task success is too coarse-grained and image-\nsimilarity metrics like frame-differencing or CLIP [34] tend\nto be brittle, we measure performance with two more targeted\nmetrics. First, we quantify policy precision as the distance\n(in pixels) between object centroids in achieved and ground\ntruth goal states, using manual keypoint annotations. Although\nleveraging out-of-the box object detectors to detect object\ncentroids is a possibility, we want to avoid conflating errors in\nobject detection (imprecise bounding box, wrong object, etc.)\nfrom manipulation error of the policy itself. Second, we gather\nhuman-provided assessments of perceived goal alignment,\nfollowing the commonly-used Likert [27] rating scheme from\n1 (Strongly Disagree) to 7 (Strongly Agree), for:\n\u2022 (Q1) The robot achieves semantic alignment with the\ngiven goal during the rollout.\n\u2022 (Q2) The robot achieves spatial alignment with the given\ngoal during the rollout.\nFor Q1, we present labelers with the policy rollout video\nalong with the given ground-truth language task descrip-\ntion. We expect reasonably high ratings across all meth-\nods for straightforward manipulation scenarios (H1). Sketch-\nconditioned policies should yield higher scores than a\nlanguage-conditioned policy when a task string is ambiguous\n(H4). Q2 is instead geared at measuring to what degree a\npolicy can spatially arrange objects as desired. For instance, a\npolicy can achieve semantic alignment for the instruction place\ncan upright as long as the can ends up in the right orientation.\nFor Q2, we visualize a policy rollout side-by-side with a\ngiven visual goal (ground truth image, sketch, etc.) to assess\nperceived spatial alignment. We posit that all policies should\nreceive high ratings for straightforward scenarios (H1), with\na slight edge for visual-conditioned policies which implicitly\nhave stronger spatial priors encoded in goals. We further\nexpect that as the visual complexity of a scene increases,\nsketches may be able to better attend to pertinent aspects\nof a goal and achieve better spatial alignment than image-\nconditioned agents (H3), even for different levels of sketch\nspecificity (H4). We provide a visualization of the assessment\ninterface for Q1 and Q2 in Appendix Fig. 17. We note that we\nperform these human assessment surveys across 62 individuals\n(non-expert, unfamiliar with our system), where we assign\nbetween 8 and 12 people to evaluate each of the 6 different\nSpatial Precision (RMSE in px.)\nFailure Occurrence (Excessive Retrying)\nSkill\nRT-1\nRT-Sketch\nRT-Goal-Image\nRT-1\nRT-Sketch\nRT-Goal-Image\nMove Near\n5.43 \u00b1 2.15\n3.49 \u00b1 1.38\n3.89 \u00b1 1.16\n0.00\n0.06\n0.33\nPick Drawer\n5.69 \u00b1 2.90\n4.77 \u00b1 2.78\n4.74 \u00b1 2.01\n0.00\n0.13\n0.20\nDrawer Open\n4.51 \u00b1 1.55\n3.34 \u00b1 1.08\n4.98 \u00b1 1.16\n0.00\n0.00\n0.07\nDrawer Close\n2.69 \u00b1 0.93\n3.02 \u00b1 1.35\n3.71 \u00b1 1.67\n0.00\n0.00\n0.07\nKnock\n7.39 \u00b1 1.77\n5.36 \u00b1 2.74\n5.63 \u00b1 2.60\n0.00\n0.13\n0.40\nUpright\n7.84 \u00b1 2.37\n5.08 \u00b1 2.08\n4.18 \u00b1 1.54\n0.06\n0.00\n0.27\nVisual Distractors\n-\n4.78 \u00b1 2.17\n7.95 \u00b1 2.86\n-\n0.13\n0.67\nLanguage Ambiguity\n8.03 \u00b1 2.52\n4.45 \u00b1 1.54\n-\n0.40\n0.13\n-\nTABLE I: Spatial Precision and Failure Occurrence : Left: We report the level of spatial precision achieved across policies, measured in terms of RMSE of\nthe centroids of manipulated objects in achieved vs. given reference goal images. Darker shading indicates higher precision (lower centroid distance). Fig. 8\ncontains visualizations illustrating the degree of visual alignment that different RMSE values correspond to. Right: We report the proportion of rollouts in\nwhich different policies exhibit excessive retrying behavior. Bolded numbers indicate the most precise and least failure-prone policy for each skill.\nskills considered below.\nB. Experimental Results\nIn this section, we present our findings related to the\nhypotheses of Section IV. Tables I and II measure the spatial\nprecision achieved by policies in terms of pixelwise distance,\nwhile Fig. 3 shows the results of human-perceived semantic\nand spatial alignment, based on a 7-point Likert scale rating.\nH1: We evaluate 6 skills from the RT-1 benchmark [9]:\nmove X near Y, place X upright, knock X over, open the X\ndrawer, close the X drawer, and pick X from Y. For each skill,\nwe record 15 different catalog scenarios, varying both objects\n(16 unique in total) and their placements.\nIn general, we find that RT-Sketch performs on a compara-\nble level to RT-1 and RT-Goal-Image for both semantic (Q1)\nand spatial alignment (Q2), achieving ratings in the \u2018Agree\u2019\nto \u2018Strongly Agree\u2019 range on average for nearly all skills\n(Fig. 3 (top)). A notable exception is upright, where RT-\nSketch essentially fails to accomplish the goal semantically\n(Q1), albeit with some degree of spatial alignment (Q2). Both\nRT-Sketch and RT-Goal-Image tend to position cans or bottles\nappropriately and then terminate, without realizing the need for\nreorientation (Appendix Fig. 9). This behavior results in low\ncentroid-distance to the goal (darker gray in Table I (left)). RT-\n1, on the other hand, reorients cans and bottles successfully,\nbut at the expense of higher error (Appendix Fig. 9, light color\nin Table I (left)). In our experiments, we also observe the\noccurrence of excessive retrying behavior, in which a policy\nattempts to align the current scene with a given goal with\nretrying actions such as grasping and placing. However, per-\nforming these low-level actions with a high degree of precision\nis challenging, and thus excessive retrying can actually disturb\nthe scene leading to knocking objects off the table or undoing\ntask progress. In Table I, we report the proportion of rollouts\nin which we observe this behavior across all policies. We\nnote that RT-Goal-Image is most susceptible to this failure\nmode, as a result of over-attending to pixel-level details and\ntrying in excess to match a given goal exactly. Meanwhile, RT-\nSketch and RT-1 are far less vulnerable, since both sketches\nand language provide a higher level of goal abstraction.\nH2: We next assess RT-Sketch\u2019s ability to handle input\nsketches of varied levels of detail (free-hand, edge-aligned\nline sketch, colorized line sketch, and a Sobel edge-detected\nimage as an upper bound). Free-hand sketches are drawn\nwith a reference image next to a blank canvas, while line\nsketches are drawn on a semi-transparent canvas overlaid on\nthe image (see Appendix Fig. 16). We find such a UI to be\nconvenient and practical, as an agent\u2019s current observations\nare typically available and provide helpful guides for sketching\nlines and edges. Across 5 trials each of the move near and open\ndrawer skills, we see in Table II that all types of sketches\nproduce reasonable levels of spatial precision. As expected,\nSobel edges incur the least error, but even free-hand sketches,\nwhich do not necessarily preserve perspective projection, and\nline sketches, which are far sparser in detail, are not far\nbehind. This is also reflected in the corresponding Likert\nratings (Fig. 3 (left, bottom)). Free-hand sketches already\ngarner moderate ratings (around 4) of perceived spatial and\nsemantic alignment, but line sketches result in a marked\nperformance improvement to nearly 7, on par with the upper\nbound of providing an edge-detected goal image. Adding color\ndoes not improve performance further, but leads to interesting\nqualitative differences in behavior (see Appendix Fig. 10).\nWe also evaluate whether RT-Sketch can generalize to\nsketches drawn by different individuals and handle stylistic\nvariations. We first collect 30 sketches drawn by 6 different\nannotators using line sketching (tracing) on 5 goal images from\nthe move near evaluation scenarios. We obtain the resulting\nrollouts produced by RT-Sketch with these sketches as input.\nAcross 22 human evaluators who report their perceived spatial\nalignment via Likert ratings, we find that RT-Sketch achieves\nhigh spatial alignment on sketches drawn by other annotators.\nNotably, there is no significant dropoff in performance be-\ntween sketches drawn by different annotators, or in the policy\nperformance as compared to when our original sketches are\nthe input (Fig. 4).\nH3: Next, we compare the robustness of RT-Sketch and RT-\nGoal-Image to the presence of visual distractors. We re-use 15\nmove X near Y trials from the catalog, but introducing 5 \u2212 9\ndistractor objects into the initial visual scene after alignment.\nFig. 4: Perceived Spatial Alignment for Sketches Drawn by Other\nAnnotators (H2): Across line sketches drawn by 6 annotators who are not\nrepresented in the training dataset for RT-Sketch, we record policy rollouts\nwith these sketches as input for the move near skill. We evaluate the resulting\nrollouts across 22 human evaluators who provide Likert ratings measuring\nspatial alignment between the achieved goal state and given sketch. RT-\nSketch\u2019s performance on these new input sketches is on par with policy\nperformance on our original sketches (OURS), and with no significant dropoff\nbetween sketches drawn by different annotators.\nSkill\nFree-Hand\nLine Sketch\nColor Sketch\nSobel Edges\nMove Near\n7.21 \u00b1 2.76\n3.49 \u00b1 1.38\n3.45 \u00b1 1.03\n3.36 \u00b1 0.66\nDrawer Open\n3.75 \u00b1 1.63\n3.34 \u00b1 1.08\n2.48 \u00b1 0.50\n2.13 \u00b1 0.25\nTABLE II: RT-Sketch\nSpatial Precision across Sketch Types (RMSE\n(centroid-distance) in px. We report the spatial precision achieved by RT-\nSketch subject to different input modalities. As expected, for less detailed and\nmore rough sketches, RT-Sketch achieves lower precision (lighter shading),\nand for richer representations RT-Sketch is more precise (bolded, darker\nshading). Still, there is a relatively small difference in performance between\nline, color, and edge-detected representations, indicating RT-Sketch\u2019s ability\nto afford different levels of input specificity.\nThis testing procedure is adapted from RT-1 generalization\nexperiments referred to as medium-high difficulty [9]. In\nTable I (left, bottom), we see that RT-Sketch exhibits far lower\nspatial errors on average, while producing higher semantic and\nspatial alignment scores over RT-Goal-Image( Fig. 3 (middle,\nbottom)). RT-Goal-Image is easily confused by the distribution\nshift introduced by distractor objects, and often cycles between\npicking up and putting down the wrong object. RT-Sketch, on\nthe other hand, ignores task-irrelevant objects not captured in\na sketch and completes the task in most cases (see Appendix\nFig. 11).\nH4: Finally, we evaluate whether sketches as a representa-\ntion are favorable when language goals alone are ambiguous.\nWe collect 15 scenarios encompassing 3 types of ambigu-\nity in language instructions: instance ambiguity (T1) (e.g.,\nmove apple near orange when multiple orange instances\nare present), somewhat out-of-distribution (OOD) language\n(T2) (e.g., move left apple near orange), and highly OOD\nlanguage (T3) (e.g., complete the rainbow) (see Appendix\nFig. 12). While the latter two qualifications should intuitively\nhelp resolve ambiguities, they were not explicitly made part of\nthe original RT-1 training [9], and hence only provide limited\nutility. In Table I (left, bottom), RT-Sketch achieves nearly half\nthe error of RT-1, and a 2.39-fold and 2.79-fold score increase\nfor semantic and spatial alignment, respectively (Fig. 3 (right,\nbottom)). For T1 and T2 scenarios, RT-1 often tries to pick\nup an instance of any object mentioned in the task string, but\nfails to make progress beyond that (Appendix Fig. 13). This\nfurther suggests the utility of sketches to express new, unseen\ngoals with minimal overhead, when language could otherwise\nbe opaque or difficult to express with only in-distribution\nvocabulary (Appendix Fig. 14).\na) Limitations and Failure Modes: Firstly, the image-to-\nsketch generation network used in this work is fine-tuned on a\ndataset of sketches provided by a single human annotator. Al-\nthough we empirically show that despite this, RT-Sketch can\nhandle sketches drawn by other annotators, we have yet to\ninvestigate the effects of training RT-Sketch at scale with\nsketches produced by different people. Secondly, we note that\nRT-Sketch shows some inherent biases towards performing\ncertain skills it was trained on, and occasionally performs\nthe wrong skill. For a detailed breakdown of RT-Sketch\u2019s\nlimitations and failure modes, please see Appendix A).\nV. CONCLUSION\nWe propose RT-Sketch, a goal-conditioned policy for ma-\nnipulation that takes a hand-drawn sketch of the desired scene\nas input, and outputs actions. To enable such a policy, we first\ndevelop a scalable way to generate paired sketch-trajectory\ntraining data via an image-to-sketch translation network, and\nmodify the existing RT-1 architecture to take visual informa-\ntion as an input. Empirically, we show that RT-Sketch not\nonly performs on a comparable level to existing language or\ngoal-image conditioning policies for a number of manipulation\nskills, but is amenable to different degrees of sketch fidelity,\nand more robust to visual distractors or ambiguities. Future\nwork will focus on extending hand-drawn sketches to more\nstructured representations, like schematics or diagrams for\nassembly tasks. While powerful, sketches are not without their\nown limitations \u2013 namely ambiguity due to omitted details or\npoor quality sketches. In the future, we are excited by avenues\nfor multimodal goal specification that can leverage the benefits\nof language, sketches, and other modalities to jointly resolve\nambiguity from any single modality alone.\nREFERENCES\n[1] Jacob Andreas, Dan Klein, and Sergey Levine. Modular\nmultitask reinforcement learning with policy sketches. In\nDoina Precup and Yee Whye Teh, editors, Proceedings of\nthe 34th International Conference on Machine Learning,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 166\u2013175. PMLR, 06\u201311 Aug 2017. URL\nhttps://proceedings.mlr.press/v70/andreas17a.html.\n[2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas\nSchneider, Rachel Fong, Peter Welinder, Bob McGrew,\nJosh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hind-\nsight experience replay. In 31st Conference on Neural\nInformation Processing Systems (NIPS 2017), 2017.\n[3] Brenna D Argall, Sonia Chernova, Manuela Veloso,\nand Brett Browning. A survey of robot learning from\ndemonstration.\nRobotics and autonomous systems, 57\n(5):469\u2013483, 2009.\n[4] Christine M Barber, Robin J Shucksmith, Bruce Mac-\nDonald, and Burkhard C W\u00a8unsche. Sketch-based robot\nprogramming. In 2010 25th International Conference of\nImage and Vision Computing New Zealand, pages 1\u20138.\nIEEE, 2010.\n[5] Suneel\nBelkhale,\nYuchen\nCui,\nand\nDorsa\nSadigh.\nData quality in imitation learning.\narXiv preprint\narXiv:2306.02437, 2023.\n[6] Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Sub-\nhadeep Koley, Rohit Kundu, Aneeshan Sain, Tao Xiang,\nand Yi-Zhe Song. Doodle it yourself: Class incremental\nlearning by drawing a few sketches. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2293\u20132302, 2022.\n[7] Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Ku-\nmar, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang,\nand Yi-Zhe Song. Sketch2saliency: Learning to detect\nsalient objects from human drawings.\nIn Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2733\u20132743, 2023.\n[8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Flo-\nrence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana\nGopalakrishnan, Kehang Han, Karol Hausman, Alex\nHerzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nIsabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey\nLevine, Yao Lu, Henryk Michalewski, Igor Mordatch,\nKarl Pertsch, Kanishka Rao, Krista Reymann, Michael\nRyoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet,\nJaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran,\nVincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Ste-\nfan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted\nXiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\nZitkovich. Rt-2: Vision-language-action models transfer\nweb knowledge to robotic control.\nIn arXiv preprint\narXiv:2307.15818, 2023.\n[9] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alexander Herzog, Jas-\nmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas\nJackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-\nHuei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deek-\nsha Manjunath, Igor Mordatch, Ofir Nachum, Carolina\nParada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jor-\nnell Quiambao, Kanishka Rao, Michael S Ryoo, Grecia\nSalazar, Pannag R Sanketi, Kevin Sayed, Jaspiar Singh,\nSumedh Sontakke, Austin Stone, Clayton Tan, Huong\nTran, Vincent Vanhoucke, Steve Vega, Quan H Vuong,\nFei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and\nBrianna Zitkovich. RT-1: Robotics Transformer for Real-\nWorld Control at Scale.\nIn Proceedings of Robotics:\nScience and Systems, Daegu, Republic of Korea, July\n2023. doi: 10.15607/RSS.2023.XIX.025.\n[10] Tim Brooks, Aleksander Holynski, and Alexei A Efros.\nInstructpix2pix: Learning to follow image editing instruc-\ntions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18392\u2013\n18402, 2023.\n[11] Kaylee Burns, Tianhe Yu, Chelsea Finn, and Karol\nHausman. Robust manipulation with spatial features. In\nCoRL 2022 Workshop on Pre-training Robot Learning,\n2022.\n[12] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan\nSain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song.\nWhat can human sketches do for object detection? In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15083\u201315094,\n2023.\n[13] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan\nSain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song.\nScenetrilogy: On human scene-sketch and its comple-\nmentarity with photo and text.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10972\u201310983, 2023.\n[14] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Ku-\nmar, and Aravind Rajeswaran. Can foundation models\nperform zero-shot task specification for robot manipula-\ntion? In Learning for Dynamics and Control Conference,\npages 893\u2013905. PMLR, 2022.\n[15] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya\nShivakumar, Percy Liang, and Dorsa Sadigh. No, to the\nright: Online language corrections for robotic manipula-\ntion via shared autonomy. In Proceedings of the 2023\nACM/IEEE International Conference on Human-Robot\nInteraction, pages 93\u2013101, 2023.\n[16] Michael Danielczuk, Andrey Kurenkov, Ashwin Bal-\nakrishna, Matthew Matl, David Wang, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Animesh Garg, Silvio Savarese, and Ken Gold-\nberg. Mechanical search: Multi-step retrieval of a target\nobject occluded by clutter. In 2019 International Confer-\nence on Robotics and Automation (ICRA), pages 1614\u2013\n1621. IEEE, 2019.\n[17] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mar-\niano Phielipp. Goal-conditioned imitation learning. Ad-\nvances in neural information processing systems, 32,\n2019.\n[18] Aditya Ganapathi, Priya Sundaresan, Brijen Thanan-\njeyan,\nAshwin\nBalakrishna,\nDaniel\nSeita,\nJennifer\nGrannen, Minho Hwang, Ryan Hoque, Joseph E Gon-\nzalez, Nawid Jamali, et al. Learning dense visual corre-\nspondences in simulation to smooth and fold real fabrics.\nIn 2021 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 11515\u201311522. IEEE, 2021.\n[19] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu,\nMontserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu,\nChuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, et al.\nRt-trajectory: Robotic task generalization via hindsight\ntrajectory sketches.\narXiv preprint arXiv:2311.01977,\n2023.\n[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1125\u2013\n1134, 2017.\n[21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,\nFrederik Ebert, Corey Lynch, Sergey Levine, and Chelsea\nFinn. Bc-z: Zero-shot task generalization with robotic\nimitation learning.\nIn Conference on Robot Learning,\npages 991\u20131002. PMLR, 2022.\n[22] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi\nWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima\nAnandkumar, Yuke Zhu, and Linxi Fan. Vima: General\nrobot manipulation with multimodal prompts.\narXiv\npreprint arXiv:2210.03094, 2022.\n[23] Siddharth Karamcheti, Suraj Nair, Annie S Chen,\nThomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy\nLiang.\nLanguage-driven representation learning for\nrobotics. arXiv preprint arXiv:2302.12766, 2023.\n[24] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song.\nPicture that sketch: Photorealistic image generation from\nabstract sketches.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6850\u20136861, 2023.\n[25] Zixing Lei, Yiming Zhang, Yuxin Xiong, and Siheng\nChen.\nEmergent communication in interactive sketch\nquestion answering.\narXiv preprint arXiv:2310.15597,\n2023.\n[26] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and\nDeva Ramanan. Photo-sketching: Inferring contour draw-\nings from images. In 2019 IEEE Winter Conference on\nApplications of Computer Vision (WACV), pages 1403\u2013\n1412. IEEE, 2019.\n[27] Rensis Likert.\nA technique for the measurement of\nattitudes. Archives of Psychology, 1932.\n[28] Corey Lynch and Pierre Sermanet. Language conditioned\nimitation learning over unstructured data. arXiv preprint\narXiv:2005.07648, 2020.\n[29] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli\nDing, James Betker, Robert Baruch, Travis Armstrong,\nand Pete Florence. Interactive language: Talking to robots\nin real time.\nIEEE Robotics and Automation Letters,\n2023.\n[30] Lucas Manuelli, Wei Gao, Peter Florence, and Russ\nTedrake. kpam: Keypoint affordances for category-level\nrobotic manipulation. In The International Symposium\nof Robotics Research, pages 132\u2013157. Springer, 2019.\n[31] Ethan Perez, Florian Strub, Harm De Vries, Vincent\nDumoulin, and Aaron Courville. Film: Visual reasoning\nwith a general conditioning layer. In Proceedings of the\nAAAI conference on artificial intelligence, volume 32,\n2018.\n[32] David Porfirio, Laura Stegner, Maya Cakmak, Allison\nSaupp\u00b4e, Aws Albarghouthi, and Bilge Mutlu.\nSketch-\ning robot programs on the fly.\nIn Proceedings of the\n2023 ACM/IEEE International Conference on Human-\nRobot Interaction, HRI \u201923, page 584\u2013593, New York,\nNY, USA, 2023. Association for Computing Machinery.\nISBN 9781450399647. doi: 10.1145/3568162.3576991.\nURL https://doi.org/10.1145/3568162.3576991.\n[33] Shuwen Qiu, Sirui Xie, Lifeng Fan, Tao Gao, Jungseock\nJoo, Song-Chun Zhu, and Yixin Zhu. Emergent graphical\nconventions in a visual communication game. Advances\nin Neural Information Processing Systems, 35:13119\u2013\n13131, 2022.\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. In International conference on ma-\nchine learning, pages 8748\u20138763. PMLR, 2021.\n[35] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf\nLioutikov.\nGoal-conditioned imitation learning using\nscore-based diffusion policies.\nRobotics: Science and\nSystems (RSS), 2023.\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10684\u201310695, June\n2022.\n[37] St\u00b4ephane Ross, Geoffrey Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction\nto no-regret online learning.\nIn Proceedings of the\nfourteenth international conference on artificial intelli-\ngence and statistics, pages 627\u2013635. JMLR Workshop\nand Conference Proceedings, 2011.\n[38] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa\nDehghani, and Anelia Angelova. Tokenlearner: Adaptive\nspace-time tokenization for videos. Advances in Neural\nInformation Processing Systems, 34:12786\u201312797, 2021.\n[39] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang,\nand Jeannette Bohg. Concept2robot: Learning manipu-\nlation concepts from instructions and human demonstra-\ntions. In Proceedings of Robotics: Science and Systems\n(RSS), 2020.\n[40] Irwin Sobel. An isotropic 3x3 image gradient operator.\nPresentation at Stanford A.I. Project 1968, 1968.\n[41] Mingxing Tan and Quoc Le.\nEfficientnet: Rethinking\nmodel scaling for convolutional neural networks.\nIn\nInternational conference on machine learning, pages\n6105\u20136114. PMLR, 2019.\n[42] Faraz Torabi, Garrett Warnell, and Peter Stone.\nBe-\nhavioral cloning from observation.\narXiv preprint\narXiv:1805.01954, 2018.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances\nin neural information processing systems, 30, 2017.\n[44] Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel\nShamir.\nClipascene: Scene sketching with differ-\nent types and levels of abstraction.\narXiv preprint\narXiv:2211.17256, 2022.\n[45] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir.\nClipasso:\nSemantically-aware object sketching. ACM Transactions\non Graphics (TOG), 41(4):1\u201311, 2022.\n[46] Kevin Zakka, Andy Zeng, Johnny Lee, and Shuran\nSong. Form2fit: Learning shape priors for generalizable\nassembly from disassembly. In 2020 IEEE International\nConference on Robotics and Automation (ICRA), pages\n9404\u20139410. IEEE, 2020.\n[47] Lvmin Zhang and Maneesh Agrawala.\nAdding condi-\ntional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\nAPPENDIX\nIn this section, we provide further details on the visual\ngoal representations RT-Sketch sees at train and test time\n(Appendix A), qualitative visualizations of experimental roll-\nouts (Appendix A), limitations (Appendix A) of RT-Sketch,\nas well as the interfaces used for data annotation, evaluation,\nand human assessment (Appendix A).\nSince the main bottleneck to training a sketch-to-action\npolicy like RT-Sketch is collecting a dataset of paired tra-\njectories and goal sketches, we first train an image-to-sketch\ntranslation network T\nmapping image observations oi to\nsketch representations gi, discussed in Section III. To train\nT , we first take a pre-trained network for sketch-to-image\ntranslation [26] trained on the ContourDrawing dataset of\npaired images and edge-aligned sketches (Fig. 5). This dataset\ncontains L(i) = 5 crowdsourced sketches per image for 1000\nimages. By pre-training on this dataset, we hope to embed a\nstrong prior in T and accelerate learning on our much smaller\ndataset. Next, we finetune T on a dataset of 500 manually\ndrawn line sketches for RT-1 robot images. We visualize a\nfew examples of our manually sketched goals in Fig. 6 under\n\u2018Line Drawings\u2019.\nFig. 5: ContourDrawing Dataset: We visualize 6 samples from the Contour-\nDrawing Dataset from [26]. For each image, 5 separate annotators provide an\nedge-aligned sketch of the scene by outlining on top of the original image. As\ndepicted, annotators are encouraged to preserve main contours of the scene,\nbut background details or fine-grained geometric details are often omitted. Li\net al. [26] then train an image-to-sketch translation network T with a loss that\nencourages aligning with at least one of the given reference sketches (Eq. (2)).\nNotably, while we only train T to map an image to a\nblack-and-white line sketch \u02c6gi, we consider various augmen-\ntations A on top of generated goals to simulate sketches with\nvaried colors, affine and perspective distortions, and levels\nof detail. Fig. 6 visualizes a few of these augmentations,\nsuch as automatically colorizing black-and-white sketches by\nsuperimposing a blurred version of the original RGB image,\nand treating an edge-detected version of the original image as\na generated sketch to simulate sketches with a lot of details.\nWe generate a dataset for training RT-Sketch by \u2018sketchifying\u2019\nhind-sight relabeled goal images via T and A.\nAlthough RT-Sketch is only trained on generated line\nsketches, colorized line sketches, edge-detected images, and\ngoal images, we find that it is able to handle sketches of\neven greater diversity. This includes non-edge aligned free-\nhand sketches and sketches with color infills, like those shown\nin Fig. 6.\nA. Alternate Image-to-Sketch Techniques\nThe choice of image-to-sketch technique we use is critical to\nthe overall success of the RT-Sketch pipeline. We experiment\nwith various other techniques before converging on the above\napproach.\nRecently, two recent works, CLIPAsso [45] and CLI-\nPAScene [44] explore methods for automatically generating a\nsketch from an image. These works pose sketch generation\nas inferring the parameters of Bezier curves representing\n\u201dstrokes\u201d in order to produce a generated sketch with maximal\nCLIP-similarity to a given input image. These methods per-\nform a per-image optimization to generate a plausible sketch,\nrather than a global batched operation across many images,\nlimiting their scalability. Additionally, they are fundamentally\nmore concerned with producing high-quality, aesthetically\npleasing sketches which capture a lot of extraneous details.\nWe, on the other hand, care about producing a minimal but\nreasonable-quality sketch. The second technique we explore\nis trying the pre-trained Photosketching GAN [26] on internet\ndata of paired images and sketches. However, this model\noutput does not capture object details well, likely due to not\nhaving been trained on robot observations, and contains irrele-\nvant sketch details. Finally, by finetuning this PhotoSketching\nGAN on our own data, the outputs are much closer to real,\nhand-drawn human sketches that capture salient object details\nas minimally as possible. We visualize these differences in\nFig. 7.\nTo further interpret RT-Sketch\u2019s performance, we provide\nvisualizations of the precision metrics and experimental roll-\nouts. In Fig. 8, we visualize the degree of alignment RT-Sketch\nachieves, as quantified by the pixelwise distance of object\ncentroids in achieved vs. given goal images. In Fig. 9, Fig. 10,\nFig. 11, and Fig. 13, we visualize each policy\u2019s behavior for\nH1, H2, H3 and H4, respectively. Fig. 12 visualizes the four\ntiers of difficulty in language ambiguity that we analyze for\nH4.\nWhile RT-Sketch\nis performant at several manipulation\nbenchmark skills, capable of handling different levels of\nsketch detail, robust to visual distractors, and unaffected by\nambiguous language, it is not without failures and limitations.\nIn Fig. 15, we visualize the failure modes of RT-Sketch. One\nfailure mode we see with RT-Sketch is occasionally re-trying\nexcessively, as a result of trying to align the scene as closely\nas possible. For instance, in the top row, Rollout Image 3, the\nscene is already well-aligned, but RT-Sketch keeps shifting the\nchip bag which causes some misalignment in terms of the chip\nbag orientation. Still, this kind of failure is most common with\nRT-Goal-Image (Table I), and is not nearly as frequent for RT-\nSketch. We posit that this could be due to the fact that sketches\nenable high-level spatial reasoning without over-attending to\npixel-level details.\nFig. 6: Visual Goal Diversity: RT-Sketch is capable of handling a variety of visual goals at both train and test time. RT-Sketch is trained on generated and\naugmented images like those shown on the right below \u2019Generated Goals\u2019. But it can also interpret free-hand, line sketches, and colored sketches at test time\nsuch as those on the left below \u2019Manually Sketched Goals\u2019.\nFig. 7: Alternate Image-to-Sketch Techniques\nOne consequence of spatial reasoning at such a high level,\nthough, is an occasional lack of precision. This is noticeable\nwhen RT-Sketch orients items incorrectly (second row) or\npositions them slightly off, possibly disturbing other items\nin the scene (third row). This may be due to the fact that\nsketches are inherently imperfect, which makes it difficult to\nreason with such high precision.\nFinally, we see that RT-Sketch occasionally manipulates the\nwrong object (rows 4 and 5). Interestingly, we see that a\nfairly frequent pattern of behavior is to manipulate the wrong\nobject (orange in row 4) to the right target location (near\ngreen can in row 4). This may be due to the fact that the\nsketch-generating GAN has occasionally hallucinated artifacts\nor geometric details missing from the actual objects. Having\nbeen trained on some examples like these, RT-Sketch can\nmistakenly perceive the wrong object to be aligned with an\nobject drawn in the sketch. However, the sketch still indicates\nthe relative desired spatial positioning of objects in the scene,\nso in this case RT-Sketch still attempts to align the incorrect\nobject with the proper place.\nFinally, the least frequent failure mode is manipulating the\nwrong object to the wrong target location (i.e. opening the\nwrong drawer handle). This is most frequent when the input\nis a free-hand sketch, and could be mmitigated by increasing\nsketch detail (Table II).\nFig. 8: Spatial Precision Visualization: We visualize four trials of RT-Sketch on the Move Near skill, along with the measured spatial precision in terms of\nRMSE. To evaluate spatial precision, we have a human annotator annotate the frame that is visually most aligned, and then keypoints for the object that was\nmoved in this frame and in the provided reference goal image. For each of the four trials, we visualize the rollout frames until alignment is achieved, along\nwith the labeled object centroids and the offset in achieved vs. desired positions. The upper right example shows a failure of RT-Sketch in which the apple\nis moved instead of the chip bag, incurring a high RMSE. These visualizations are intended to better contextualize the numbers from Table I.\nFig. 9: H1 Rollout Visualization: We visualize the performance of RT-1, RT-Sketch, and RT-Goal-Image on two skills from the RT-1 benchmark (upright\nand knock). For each skill, we visualize the goal provided as input to each policy, along with the policy rollout. We see that for both skills, RT-1 obeys the\nsemantic task at hand by successfully placing the can upright or sideways, as intended. Meanwhile, RT-Sketch and RT-Goal-Image struggle with orienting the\ncan upright, but successfuly knock it sideways. Interestingly, both RT-Sketch and RT-Goal-Image are able to place the can in the desired location (disregarding\ncan orientation) whereas RT-1 does not pay attention to where in the scene the can should be placed. This is indicated by the discrepancy in position of the\ncan in the achieved versus goal images on the right. This trend best explains the anomalous performance of RT-Sketch and RT-Goal-Image in perceived Likert\nratings for the upright task (Fig. 3), but validates their comparably higher spatial precision compared to RT-1 across all benchmark skills (Table I).\nFig. 10: H2 Rollout Visualization: For the open drawer skill, we visualize four separate rollouts of RT-Sketch operating from different input types. Free-hand\nsketches are drawn without outlining over the original image, such that they can contain marked perspective differences, partially obscured objects (drawer\nhandle), and roughly drawn object outlines. Line sketches are drawn on top of the original image using the sketching interface we present in Appendix Fig. 16.\nColor sketches merely add color infills to the previous modality, and Sobel Edges represent an upper bound in terms of unrealistic sketch detail. We see that\nRT-Sketch is able to successfully open the correct drawer for any sketch input except the free-hand sketch, without a noticeable performance gain or drop.\nFor the free-hand sketch, RT-Sketch still recognizes the need for opening a drawer, but the differences in sketch perspective and scale can occasionally cause\nthe policy to attend to the wrong drawer, as depicted.\nFig. 11: H3 Rollout Visualization: We visualize qualitative rollouts for RT-Sketch and RT-Goal-Image for 3 separate trials of the move near skill subject to\ndistractor objects. In Column 2, we highlight the relevant non-distractor objects that the policy must manipulate in order to achieve the given goal. In Trial\n1, we see that RT-Sketch successfuly attends to the relevant objects and moves the blue chip bag near the coke can. Meanwhile, RT-Goal-Image is confused\nabout which blue object to manipulate, and picks up the blue pepsi can instead of the blue chip bag (A). In Trial 2, RT-Sketch successfully moves an apple\nnear the fruit on the left. A benefit of sketches is their ability to capture instance multimodality, as any of the fruits highlighted in Column 2 are valid options\nto move, whereas this does not hold for an overspecified goal image. RT-Goal-Image erroneously picks up the green chip bag (B) instead of a fruit. Finally,\nTrial 3 shows a failure for both policies. While RT-Sketch successfully infers that the green can must be moved near the red one, it accidentally knocks over\nthe red can (C) in the process. Meanwhile, RT-Goal-Image prematurely drops the green can and instead tries to pick the green chip bag (D).\nFig. 12: H4 Tiers of Difficulty: To test H4, we consider language instructions that are either ambiguous due the presence of multiple similar object instances\n(T1), are somewhat out-of-distribution for RT-1 (T2), or are far out-of-distribution and difficult to specify concretely without lengthier descriptions (T3). Each\nimage represents the ground truth goal image paired with the task description.\nFig. 13: H4 Rollout Visualization (T1 as visualized in Fig. 12): One source of ambiguity in language descriptions is mentioning an object for which\nthere are multiple instances present. For example, we can easily illustrate three different desired placements of an orange in the drawer via a sketch, but an\nambiguous instruction cannot easily specify which orange is relevant to pick and place. In all rollouts, RT-Sketch successfully places the correct orange in the\ndrawer, while RT-1 either picks up the wrong object (A), fails to move to the place location (B), or knocks off one of the oranges (C). Although in this case,\nthe correct orange to manipulate could easily be specified with a spatial relation like pick up the \u27e8 left/middle/right \u27e9 orange, we show below in Appendix\nFig. 14 that this type of language is still out of the realm of RT-1\u2019s semantic familiarity.\nFig. 14: H4 Rollout Visualization (T2-3 as visualized in Fig. 12): For T2, we consider language with spatial cues that intuitively should help the policy\ndisambiguate in scenarios like the oranges in Fig. 13. However, we find that RT-1 is not trained to handle such spatial references, and this kind of language\ncauses a large distribution shift leading to unwanted behavior. Thus, for the top rollout of trying to move the chip bag to the left where there is an existing\npile, RT-Sketch completes the skill without issues, but RT-1 attempts to open the drawer instead of even attempting to rearrange anything on the countertop\n(A). For T3, we consider language goals that are even more abstract in interpretation, without explicit objects mentioned or spatial cues. Here, sketches are\nadvantageous in their ability to succinctly communicate goals (i.e. visual representation of a rainbow), whereas the corresponding language task string is far\ntoo underspecified and OOD for the policy to handle (B).\nFig. 15: RT-Sketch Failure Modes\nFig. 16: Sketching UI: We design a custom sketching interface for manually collecting paired robot images and sketches with which to train T , and for\nsketching goals for evaluation. The interface visualizes the current robot observation, and provides the ability to draw on a digital screen with a stylus. The\ninterface supports different colors and erasure. We note that intuitively, drawing on top of the image is not an unreasonable assumption to make, since current\nagent observations are far more readily available than a goal image, for instance. Additionally, the overlay is intended to make the sketching interface easy\nfor the user to provide, without having to eyeball edges for the drawers or handles blindly.\nFig. 17: Assessment UI: For all skills and methods, we ask labelers to assess semantic and spatial alignment of the recorded rollout relative to the ground\ntruth semantic instruction and visual goal. We show the interface above, where labelers are randomly assigned to skills and methods (anonymized). The results\nof these surveys are reported in Fig. 3.\n"
  },
  {
    "title": "MagicClay: Sculpting Meshes With Generative Neural Fields",
    "link": "https://arxiv.org/pdf/2403.02460.pdf",
    "upvote": "6",
    "text": "MagicClay: Sculpting Meshes With Generative Neural Fields\nAMIR BARDA, Tel Aviv University, Israel\nVLADIMIR G. KIM, Adobe Research, USA\nNOAM AIGERMAN, Universit\u00e9 de Montr\u00e9al, Canada\nAMIT H. BERMANO, Tel Aviv University, Israel\nTHIBAULT GROUEIX, Adobe Research, USA\nRough Edit\n\u201cHead with Devil Horns\u201d\nRough Edit\n\u201cHead with Elf Ears\u201d\nFig. 1. MagicClay introduces a novel sculpting tool, employing a hybrid mesh-SDF representation. The user selects a region (middle) of an input mesh (left),\ninputs a text-prompt (bottom), and the region automatically grows to match the prompt (right), while the rest of the shape is unchanged, and the mesh\nremains topologically valid. This enables operations such as sequential semantic mesh editing.\nThe recent developments in neural fields have brought phenomenal capa-\nbilities to the field of shape generation, but they lack crucial properties,\nsuch as incremental control \u2014 a fundamental requirement for artistic work.\nTriangular meshes, on the other hand, are the representation of choice for\nmost geometry related tasks, offering efficiency and intuitive control, but do\nnot lend themselves to neural optimization. To support downstream tasks,\nprevious art typically proposes a two-step approach, where first a shape is\ngenerated using neural fields, and then a mesh is extracted for further pro-\ncessing. Instead, in this paper we introduce a hybrid approach that maintains\nboth a mesh and a Signed Distance Field (SDF) representations consistently.\nUsing this representation, we introduce MagicClay \u2014 an artist friendly tool\nfor sculpting regions of a mesh according to textual prompts while keeping\nother regions untouched. Our framework carefully and efficiently balances\nconsistency between the representations and regularizations in every step\nof the shape optimization; Relying on the mesh representation, we show\nhow to render the SDF at higher resolutions and faster. In addition, we\nemploy recent work in differentiable mesh reconstruction to adaptively allo-\ncate triangles in the mesh where required, as indicated by the SDF. Using\nan implemented prototype, we demonstrate superior generated geometry\ncompared to the state-of-the-art, and novel consistent control, allowing\nsequential prompt-based edits to the same mesh for the first time.\n1\nINTRODUCTION\nThe field of 3D shape generation has always been heavily depen-\ndent on the representations it uses for the shapes. Recent Neural\nField-based representations (i.e., NeRFs [Mildenhall et al. 2021] or\nSDFs [Park et al. 2019; Chen and Zhang 2019]), have shown remark-\nable progress to the task [Poole et al. 2022; Wang et al. 2023a] in a\nvery short time. These representations are robust to noisy losses,\nAuthors\u2019 addresses: Amir Barda, amirbarda@mail.tau.ac.il, Tel Aviv University, Israel;\nVladimir G. Kim, Adobe Research, USA; Noam Aigerman, Universit\u00e9 de Montr\u00e9al,\nCanada; Amit H. Bermano, Tel Aviv University, Israel; Thibault Groueix, Adobe Re-\nsearch, USA.\nand are naturally well-suited for neural frameworks, yielding im-\npressive results and avoiding local minima. On the other hand, these\nrepresentations are expensive to evaluate (limited by volumetric\nrendering resolutions), and lack acutely in control (such localized\nedits or even shape smoothing).\nIn contrast, triangular meshes are the dominant representation\nfor most 3D applications in industry. This is because meshes are\ninexpensive, consistent, and intuitive. For instance, when evolving\na mesh-based shape, an artist performs edits on geometry, textures,\nor even topology, and expects any additional updates to retain these\nattributes. This is not possible currently using Neural Field-based\nrepresentations. Unfortunately, while the adaptive, or non-uniform,\nnature of meshes is perhaps their greatest advantage, it is also the\nreason they are not preferred by current generative frameworks.\nThe sparse gradients induced by meshes tend to limit the ability of\noptimizations to achieve large deformations in a stable manner.\nFor this reason, many works turn to implicit functions for coarse\ngeneration as a first step, and to meshes in a second step, for the\npurpose of finer details, or downstream editability. However, as\nwe demonstrate, two-stage pipelines are prone to local minima,\nand additionally cannot be extended to edit an existing mesh with\npre-computed UVs for example.\nIn this paper, we present MagicClay - a shape evolution and edit-\ning framework, based on a hybrid implicit-explicit representation,\nbenefiting from the best of both worlds. MagicClay optimizes a\nmesh and an SDF jointly in every step throughout the generation\nprocess, and introduces a novel sculpting tool for 3D generative mod-\neling. Sculpting is a common approach for 3D modeling, common\nto commercial 3D modeling software [Blender 2024; ZBrush 2024;\nSubstanceModeler 2024]. While sculpting currently requires a lot of\ntime and expertise, our new tool allows artists to select a region on\narXiv:2403.02460v1  [cs.GR]  4 Mar 2024\n2\n\u2022\nAmir Barda, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, and Thibault Groueix\na mesh to be modified, provide a textual prompt, and hallucinate\nan updated region (See Figure 1). As we demonstrate, in addition\nto the contribution to control, our hybrid representation also bene-\nfits computational efforts and overall geometric quality, as various\npriors can be placed more intuitively on the two representations.\nThe key technical challenge of the hybrid approach is keeping the\ntwo representations synced efficiently. To achieve this, we differen-\ntiably render both representations from various angles, and require\nconsistency in RGB renders, opacity and normal maps. Furthermore,\nwe rely on the in-sync-mesh representation to render the SDF at\nhigher resolutions and faster; Instead of the hundreds of samples\nper rays, we localize the SDF sampling around the mesh surface,\nand use as little as three samples. Critically, evolving a mesh consis-\ntently and stably is an additional challenge. In terms of resolution, a\ncoarse mesh would not be expressive enough for novel details, and\na fine mesh is expensive and unstable. Hence, an adaptive tessella-\ntion is required, that evolves along with the shape where required.\nWe rely on recent developments in differentiable mesh reconstruc-\ntion [Barda et al. 2023] to achieve dynamic mesh topology updates,\nincluding face splitting, edge collapse, and edge flips. To texture the\nmesh despite changes in its topology, we contribute a new strategy\nbased on triangle supersampling. Importantly, using this layer, we\ncan maintain mesh properties throughout the optimization.\nAs we demonstrate, our hybrid approach allows localized and\nsequential mesh editing operations, preserving mesh topology and\ninformation on one hand, while allowing radical and semantic evolu-\ntion on the other. In addition, we show overall higher generated geo-\nmetric quality, thanks to the priors the two representations impose\non each other. The hybrid approach and sculpting tool demonstrate\nhow the merits of both leading representations can be combined. In\nessence, our framework brings the recent and future breakthroughs\nin neural shape generation closer to artistic workflows, where work\nis in incremental steps, giving the artist precision and control over\nthe end result, but with unprecedented expressiveness.\n2\nRELATED WORK\n3D generative models. In their seminal work, DreamFusion,\npool et al. [Poole et al. 2022] show that Text-to-Image diffusion\nmodels can be used to provide gradients to optimize a Neural Radi-\nance Field (NeRF) via Score Distillation Sampling (SDS). Magic3D\n[Lin et al. 2023] achieves better quality by using a two stage ap-\nproach: the first stage is similar to DreamFusion, and they note that\nthe quality of the generated object is limited by the high cost of\nperforming volumetric rendering for high resolutions images. The\nsecond stage uses a differentiable mesh representation to further\nrefine the generated object, as differentiably rendering meshes in\nhigh resolution is significantly cheaper in both time and memory.\nMagic123 [Qian et al. 2023] further improves upon Magic3d by using\nboth 3d and 2d diffusion priors. ProlificDreamer [Wang et al. 2023a]\nproposes an improvement over SDS, the VSD loss, to drive 3D gen-\neration from 2d diffusion priors. Fantasia3d [Chen et al. 2023] and\nTextMesh [Tsalicoglou et al. 2023] decouple the appearance from\nthe geometry by replacing the NeRF with an SDF, and optimizing a\ncolor network separately. TextDeformer [Gao et al. 2023] uses CLIP\nas prior together with a novel gradient smoothing technique based\non mesh Jacobians to deform meshes according to a text prompt.\nThe choice of using two stages in Magic3D [Lin et al. 2023] high-\nlights the tradeoffs involved in choosing the right 3D representation\nfor 3D generative models. While implicit functions are well suited\nfor coarse generation because they allow topology updates, meshes\ncan be rasterized very efficiently at a high resolution to get fine\ndetails in a second step. However, two-stage pipelines are prone to\nlocal minima and crucially, cannot be extended to edit an existing\nmesh with pre-computed UVs. In contrast, we jointly optimise a\nhybrid SDF and mesh representation, that can be initialized from an\nexisting mesh and maintain all of its properties during optimization,\nbenefiting from the best of both worlds in a 1-step pipeline : topology\nupdates from the SDF part and fine details from the mesh part.\nFocus on local editing: Vox-E [Sella et al. 2023] edit a voxel grid\nvia SDS and use the attention layers to encourage localized edits.\nHowever, they can not guarantee where the edit will happen because\nthe localization mechanism is based on soft attention.\nConcurrent work: DreamCraft3D [Sun et al. 2023] builds on the\nrecent SDS works by fine-tuning the diffusion model during the\ngenerative process using DreamBooth [Ruiz et al. 2022]. Of note,\nInstant3D [Li et al. 2023] generates a 3D shape in a single forward\npass, without require any costly optimization. While it does not\nallow for local artistic controls similar to the sculpting application\nwe present, we are excited to leverage ideas from this research\ndirection to accelerate our method in the future.\nHybrid Representations. There is no ubiquitous representation\nin 3D, as there exist in 2D for images, thus several representations\nexist and have been combined for diverse 3D tasks. The plethora\nof representations and combination show that there is no one-fit-\nfor-all solution. In this work, we introduce a hybrid representation\nspecialized for generative modeling and focus the related work on\nhybrids most relevant to this paper. Poursaeed et al. [Poursaeed et al.\n2020] uses a coupling of implicit and explicit surface representation\nfor generative 3D modeling, kept in sync by 3D losses. NerfMeshing\n[Rakotosaona et al. 2023] proposes a improved meshing pipeline\nfor NeRFs. In contrast to [Poursaeed et al. 2020], the coupling is not\nachieved via coupled regularization losses, but explicitely enforced\nby projection layers from the SDF to the mesh. Finally, DmTeT [Shen\net al. 2021] proposes deep marching Tetrahedra as a hybrid repre-\nsentation for high-resolution 3D Shape synthesis, notably used in\nthe concurrent work Magic3D [Lin et al. 2023]. Our method uses\nboth a set of regularization losses, as well as a dynamic projection\nlayer based on ROAR [Barda et al. 2023] to keep the SDF and mesh\npart in sync.\nTraditional approaches for sculpting meshes. Many commer-\ncial tools employ digital sculpting metaphor for 3D modeling, such\nas Zbrush [ZBrush 2024], Mudbox [Autodesk 2024], or Substance-\nModeler [SubstanceModeler 2024]. Motivated by these workflows,\ngeometry processing research focused on improving interactive\ntechniques such as mesh deformation [Jacobson et al. 2014], mesh\nblending for cut-and-paste [Biermann et al. 2002], local parameteri-\nzation for adding surface details [Schmidt et al. 2006], symmetry-\nguided autocompletion [Peng et al. 2018], and version control for\ncollaborative editing [Salvati et al. 2015]. Despite these advances, 3D\nMagicClay: Sculpting Meshes With Generative Neural Fields\n\u2022\n3\nMagicClay Hybrid Representation\nSDF\nMesh\n\ud835\udc73\ud835\udfd0\nConsistency \nLosses \nRGB\nNormal\nOpacity\n\u201cHead with \ndevil horns\u201d\na\nb\nSDS & \nVSD \nLosses\nDynamic \nRemeshing\na\nb\nc\nd\nEikonal Loss\nSmoothness Loss\nNormal Loss\nOpacity Loss\nx\ny\nz\nr\ng\nb\nAppearance\nc   d\nFreeze Loss\n2D \ndiffusion prior\nTo \nDynamic \nRemeshing\nCurvature\nFig. 2. Overview of the hybrid optimization. We jointly optimize a mesh, an SDF and a shared appearance MLP according to an input prompt. We can either\noptimize the full geometry, or only a user-selected portion of the mesh for an iterative 3D modeling workflow. We start by differentiably rendering both\nrepresentations, and enforce their consistency. As they are kept in sync, we use the mesh to efficiently sample volumetric rays to render hi-res maps from the\nSDF in a memory-efficient manner. In addition to the standard SDS loss, the high-res renderings allows using high quality VSD losses [Wang et al. 2023a]\nto evolve the SDF. We keep the mesh surface and the SDF in sync via multi-view consistency constraints on the RGB pixels, the image opacity and the\nsurface normals. The mesh local topology is updated according to the SDF using ROAR [Barda et al. 2023], splitting triangles where geometry is created,\nand collapsing edges where needed. Additionally, we leverage representation-specific losses to regularize the optimization: an Eikonal loss on the SDF and a\nsmoothness loss on the mesh.\nmodeling remains to be only accessible to experts. As an alternative,\nexample-based approaches have been proposed to democratize 3D\nmodeling tools by using existing geometry from a database of stock\n3D models to assemble new shapes from parts [Funkhouser et al.\n2004]. Subsequent methods have built statistical models over part\nassemblies [Kalogerakis et al. 2012], and allow high-level semantic\ncontrol for deformations [Yumer et al. 2015]. Despite their accessi-\nbility, these tools are often restricted in their domain, and often rely\non heavy annotation of stock assets, and thus have received limited\nuse by professional modelers. In this paper, we utilize pretrained\n2D generative data prior to enable semantic controls for local and\niterative modeling workflow without the need of preannotated 3D\nstock data.\n3\nMETHOD\nGiven a mesh, a user-highlighted surface region, and a text prompt\nthat describes the desired target, MagicClay optimizes the shape of\nthe selected region so that the resulting mesh matches the target.\nTo drive the shape optimization, we follow current literature and\nuse the Score Distillation Sampling (SDS) technique [Poole et al.\n2022] with differentiable rendering to leverage on text-conditioned\n2D diffusion and guide the shape optimization. This approach, how-\never, does not perform well when operated on meshes. Meshes\nare driven by sparse and irregular samples (vertices), and their\nconnectivity mandates a stable and smooth deformation, avoiding\nself-intersections and flip-overs. For this reason, we employ a neural\nSigned Distance Field (SDF) to drive the mesh shape optimization\nand topology updates. We thus propose a hybrid representation that\ncaptures both a Signed Distance Field (SDF) and the surface, gaining\nfrom the advantages of both worlds. While the SDF allows guiding\nthe shape towards larger-scale complex changes, the mesh allows\nto capture fine details, and localize control to the user-highlighted\nsurface region.\nIn this section we provide details on the hybrid SDF/Mesh repre-\nsentations (Sec. 3.1), how it can be efficiently optimized with SDS\nguidance (Sec. 3.2), how to effectively use surface and volumet-\nric priors (Sec. 3.3), and how to update the mesh topology during\noptimization (Sec. 3.4). Figure 2 overviews the full pipeline.\n3.1\nHybrid Representation\nOur hybrid representation consists of a surface (a mesh), a volume\n(an SDF), and a shared appearance network encoding RGB colors\nfor an input 3D coordinate. Both the surface and the volumetric\nrepresentations can be differentiably rendered, leveraging the shared\nappearance network to output images with color, normals, and\nopacity channels. We now detail the three elements of our hybrid\nshape representation.\nSurface Representation. We represent the surface of the shape as a\n2-manifold triangular mesh. Mesh topology, or sampling resolution,\nis locally adapted according to the SDF (see Sec. 3.4 for details). We\n4\n\u2022\nAmir Barda, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, and Thibault Groueix\nencode colors for the mesh using an auxiliary appearance network,\nderived from the SDF itself (see below). We found this approach\nsimpler and more natural than traditional mesh coloring techniques;\nUsing per-vertex colors is sensitive to triangulation, and would re-\nquire a large number of vertices to match the resolution of the SDF.\nUsing a texture image requires a complex UV parameterization,\nusually done a priori on a fixed shape. In addition, our surface is\ncontinuously optimized and undergoes through topological changes,\nre-tessellation, and large-scale deformations, which makes it com-\nputationally infeasible to apply traditional UV parameterization\ntechniques during this optimization.\nInstead, our hybrid approach offers a simpler approach to shape\ncoloring. To apply the colors from the appearance network to the\nmesh, we propose to adaptively subdivide each face of the base\nmesh according to triangle area. Since we only use these subdivided\ntriangles to represent colors they do not have to form a connected\nmesh unlike traditional subdivision techniques. Thus, we employ\nMeshColors scheme that was originally proposed for UV-less tex-\nturing [Yuksel et al. 2010], and has an efficient GPU implementation.\nIn the inset we illustrate the example subdivision, note how sub-\ntriangles on two adjacent faces do not share the vertices along the\nedge. During rendering we assign a\ncolor to each sub-triangle, by using\nthe coordinate of the three associ-\nated supersampled vertices to sam-\nple the appearance network.\nSigned Distance Functions. Our\nvolumetric shape representation is\nchosen off-the-shelf, and conceptu-\nally serves as a regularization guid-\ning the mesh evolution using existing state-of-the-art text2shape\ntools. We use a neural SDF, or an implicit continuous scalar field that\ncan be sampled anywhere in R3, returning a signed shortest distance\nto the surface (negative on the inside, positive on the outside). We\nencode the SDF using a multiresolution hash encoding of features\ndefined over a grid which are then mapped to distance value by a\nsmall MLP, following instant-NGP [M\u00fcller et al. 2022]. As in the\nmesh case, the shared appearance network is sampled to obtain\ncolors during rendering.\nAppearance Network. The shared appearance network encodes\ncolors implicitly as a map over R3. It shares the same hash grid as\nthe SDF, but has a smaller MLP head, with a single hidden layer that\ntake hash grid features as input and outputs RGB values.\n3.2\nHybrid Shape Guidance\nIn essence, our shape optimization is based on Score-Distillation\nSampling (SDS) to distill gradients from a text prompt. The primary\nmotivation to maintain an SDF representation in addition to the\nmesh is because SDFs are more robust noisy guidance, which is an\ninherent property of the multi-view SDS approach (see Figure 5).\nWe thus choose to inject the text guidance only to the auxiliary\nSDF representation, and propagate the changes to the mesh via the\nconsistency losses (Sec. 3.3) and the topology updates (Sec. 3.4).\nTo apply the text guidance and the consistency losses, we need to\nrender both representation differentiably. We use Nvdiffrast [Laine\net al. 2020] to render meshes and VolSDF [Yariv et al. 2021] for volu-\nmetric rendering of our SDF. Clearly, as mesh rasterization is much\ncheaper than volumetric rendering, the process is bottlenecked by\nthe resolution at which we can render the SDF, both in terms of speed\nand memory. Our hybrid representation uniquely enables a strategy\nto render SDF faster and cheaper, at a higher resolution of 512x512.\nMesh\nMesh\nSDF\nThis is achieved thanks to the con-\nsistency between the mesh and SDF\nrepresentations throughout the op-\ntimization. We can significantly re-\nduce the typical 512 samples per ray\nnecessary for rendering the SDF by\nusing the intersection of the ray with\nthe mesh representation (efficiently\ncalculated by the differentiable mesh renderer). Using the intersec-\ntion as the center of a small spread of samples (typically 3), this\nallows for high resolution renders of the SDF (i.e. 512x512 and larger),\nwhich are otherwise memory prohibitive. The idea of leveraging the\nsurface to reduce the number of network queries per ray emerged\nin concurrent works, namely Adaptive Shell [Wang et al. 2023b]\nand HybridNerf [Turki et al. 2023], which shows its generality and\nsuccess in other settings than ours.\nUsing this strategy, we render the SDF in 512x512 and apply the\nVSD loss of those high-res renderings. We also aply regular SDS on\nlower-res 128x128 renderings by regular VolSDF as we find that this\nimproves the results slightly.\n3.3\nRepresentation Priors\nIn addition to the text guidance, we apply representation-specific\nregularizations and consistency losses that keep both representa-\ntions in sync.\nConsistency Loss. The SDF and the mesh are consistent if their\nimages are in 1 to 1 correspondences from any camera angle. We thus\nsupervise the L2 difference between their RGB renderings, normal\nmaps and opacity maps. If the renderings are made at different\nresolution, we downsize to the lower resolution before computing\nthe L2 loss.\nEnforcing Localization and Freeze Loss. To localize changes to the\nuser-selected area we first fix the mesh vertices in all non-selected\nregions during optimization by zeroing out gradients outside of user\nselection. While localization is harder to achieve for SDF, we add\na sampling-based freeze loss, which favors regions around fixed\nvertices to remain unchanged:\n\ud835\udc60(\ud835\udc63sampled) = 0\n(1)\nwhere \ud835\udc63sampled are vertices sampled uniformly over the faces\nwhich are not part of the optimization region selected by the user.\nLaplacian (Smoothness) Loss. While it is harder to regularize the\nsurface of an implicit function to be smooth, the explicit representa-\ntion of the mesh allows to easily define a smoothness term using\nthe Laplacian of the mesh, defined for each vertex:\nMagicClay: Sculpting Meshes With Generative Neural Fields\n\u2022\n5\n\ud835\udeff(\ud835\udc65\ud835\udc56) = \ud835\udc65\ud835\udc56 \u2212 \u03a3\ud835\udc57\ud835\udc65\ud835\udc57\n\ud835\udc41\n,\n(2)\nwhere \ud835\udc65\ud835\udc57 are neighbors of \ud835\udc65\ud835\udc56. The Laplacian vector encodes local\ngeometry changes, with a smooth mesh is defined by low Laplacian\nvectors, and we use a global smoothness loss:\n\ud835\udc3fsmooth = \u03a3\ud835\udc56 ||\ud835\udeff\ud835\udc56 ||.\n(3)\nSDF Eikonal Loss. To encourage the implicit function to learn a\nvalid SDF representation we use the Eikonal term as a loss. The SDF\n\ud835\udc60 is valid if and only if the loss in Eqn 4 is 0:\n\ud835\udc3fEik = \u03a3\ud835\udc65 (||\u2207\ud835\udc60(\ud835\udc65)|| \u2212 1)2\n(4)\nSDF opacity and normal Loss. Inspired by TextMesh [Tsalicoglou\net al. 2023], we also binarize the SDF opacity and apply a Binary\nCross Entropy loss to encourage discrete 0 or 1 values. To penalize\nbadly oriented normals of the implicit surface, we apply an L2\npenalty to the dot product between the normal and the camera\ndirection if it is negative.\n3.4\nUpdating the mesh topology\nTo maintain consistency between the mesh and SDF, it is neces-\nsary to perform local topology edits on the mesh in that increase\nor decrease mesh resolution where required. Continuous Remesh-\ning [Palfinger 2022] pioneered such a local topology update ap-\nproach, by using the Adam optimizer state as a signal. While this\napproach works well in a multi-view reconstruction scenario, where\nthe images are sharp, and the camera parameters known, the noise\ninvolved in SDS makes the gradients, and by extension the Adam\nstate, very noisy and an unstable signal to trigger those operations.\nWe turn to another work, ROAR [Barda et al. 2023], particularly\nwell-tailored to our hydrid representation. Within this framework,\nwe use the SDF as the signal to trigger mesh triangle splits.\nIn a nutshell, for each triangle on the mesh, ROAR starts by\nsupersampling the triangle into K sub-faces, and projects each sub-\nvertices on the 0-level set of the SDF \ud835\udc60 using a projection operator:\n\ud835\udc43(\ud835\udc65) = \u2212\ud835\udc60(\ud835\udc65) \u00b7 \u2207\ud835\udc60(\ud835\udc65)\n(5)\nThis projection results in a piece-wise linear surface that approx-\nimate the implicit surface closest to the initial triangle. The decision\nto split this triangle is based on the curvature score of this piece of\nprojected surface. If the surface is very curved, then the triangle is\nsplit using\n\u221a\n3-subdivision [Kobbelt 2000]. Similarly, each edge is\nassigned a score based on the quadratic distance of its vertices to\nall the planes in the 1-ring of the edge, which intuitively represents\nhow important is the edge to the geometry. If the score is low, then\nthe edge can be collapsed with Qslim [Garland and Heckbert 2023].\nWe refer the interested reader to the ROAR paper [Barda et al.\n2023] for more details, but the important point to note that ROAR\noffers a principled way to perform edge collapses and face splits in\nthe sense that each iteration of ROAR strictly decrease an energy -\nthe difference between the highest face score and the lowest edge\nscore. It thus exhibits a convergence behavior after enough itera-\ntions. We also note that manifoldness is guaranteed to be preserved\nthroughout the iterations.\n\u201ca delicious hamburger\u201d \u201ca chow chow puppy\u201d\n\u201ca delicious croissant\u201d\nProlificDreamer\nTextMesh\nOurs\nFantasia3d\nFig. 3. Comparison on text-to-3D from scratch. We compare the quality\nof the triangular meshes extracted from various state-of-the-art generative\nmethods: Fantasia3d [Chen et al. 2023], ProlificDreamer [Wang et al. 2023a]\nand TextMesh [Tsalicoglou et al. 2023]. While all methods produce realistic\nRGB renderings, only our hybrid representation generates smooth geometry.\n4\nEXPERIMENTS\nWe implement our pipeline in Threestudio [Guo et al. 2023], and use\nthe implementations of other methods provided in the framework\nfor comparisons with Stable Diffusion v1.5 as the backbone diffusion\nmodel. All experiments presented in this work were executed on a\nsingle A100-40GB GPU.\nIn the rest of this section we compare our representation to prior\nwork on text-conditioned 3D generation (Sec. 4.1), demonstrate its\nutility in a mesh sculpting application (Sec. 4.2), and compare to a\ntext-driven mesh deformation baseline (Sec. 4.3). We then provide a\nsimple illustrative experiment to motivate the hybrid representation\nwhen using SDS guidance with noisy gradients (Sec 4.4), and finally\nablate our method (Sec 4.5).\n4.1\nComparison with Generative Methods\nSince MagicClay is a modeling tool, we are primarily interested\nin evaluating the quality of the geometry and thus focus on mesh\nrenderings without texture. Note that existing 3D generative tech-\nniques are not designed to edit a part of an existing mesh, and thus\nwe compare performance of our hybrid approach on the task of\ntext-to-3D generation. We compare against three recent approaches:\nFantasia3d [Chen et al. 2023], ProlificDreamer [Wang et al. 2023a]\nand TextMesh [Tsalicoglou et al. 2023]. The representation used in\nTextMesh [Tsalicoglou et al. 2023] is an SDF, so we run marching\ncube on their output to evaluate the quality of the mesh. Prolific-\nDreamer uses a two stage approach: first a NeRF-base generation,\nthen a refinement using an explicit representation. Fantasia3d uses\nDMtet [Shen et al. 2021] to extract a mesh from an SDF.\nWe present our results in Figure 3. Even though each approach\ngenerates a representation that can produce realistic RGB renders,\n6\n\u2022\nAmir Barda, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, and Thibault Groueix\nTextDeformer\nOurs\n\u201chead \nwith \nhorns\u201d\n\u201cairplane \nwings\u201d\nInitial Mesh\nFig. 4. Comparisons to TextDeformer [Gao et al. 2023]. Given the user-\nmodified input mesh, we either run TextDeformer or our method to modify\nit towards the target prompt.\nextracted meshes often exhibit significant surface artifacts, which\nmake them hardly recognizable without texture (see \u201cChow Chow\npuppy\u201d by ProlificDreamer or \u201cCroissant\u201d by TextMesh). By com-\nparison, our geometries are recognizable and smooth thanks to\nthe fact that our hybrid representation enables an explicit regular-\nization of the surface. This validates that MagicClay successfully\nbridges the generative capabilities of implicit radiance fields with\nthe surface-level controls of meshes.\n4.2\nMesh Sculpting\nWe further demonstrate that our method can be used to enable novel\niterative 3D sculpting workflows. Starting with an initial mesh, an\nartist can select region of interest (and optionally sculpt a coarse\nadjustment to that region) along with a textual prompt describing\nthe desired updated shape. MagicClay generates a modified mesh,\nwhich could be iteratively refined with new elements. Note that\nhybrid representation is essential to this application. First, selecting\nand adjusting the region of interest is easily accomplished using\nstandard mesh editing tools [Blender 2024]. Second, the generated\nresult tends to be more expressive with additional SDF representa-\ntion guidance. Third, using the mesh allows us to keep non-selected\nsurface regions intact by zeroing out their deformation gradients,\nwhich guarantees that the change will only affect the user-selected\nregion. Refer to Figures 1, 9 and supplemental video for example\nresults. MagicClay generates a high quality edits, that matche the\nrough local edit and adhere to the user\u2019s text prompt.\n4.3\nComparison with TextDeformer\nOur method is the first to tackle the problem of interactive, localized\nmesh sculpting via text-based prompts. A naive alternative would\nbe to simply use the existing text-driven mesh deformation tech-\nnique [Gao et al. 2023] on the user-modified mesh, still aiming to\nachieve desired changes in the geometry. In Figure 4 we compare\nour method to this baseline. Note how our method is able to add geo-\nmetrically complex large-scale details due to guidance from SDF and\ntopological updates. Our method\u2019s changes are also restricted only\nL2 Reprojection Error\nWhite Noise Standard Deviation\nFig. 5. Mesh and SDF robustness to noisy gradients. We optimize a\nmesh, an SDF our our hybrid representation with multi-view reconstruction\nlosses after applying various noise levels to the ground truth renderings. We\nreport the L2 reprojection error against the ground truth renders. The SDF\nexhibit more robustness than the mesh to high noise regime, and our hybrid\noutperforms both.\nto user-modified region, and do not lead to large-scale deformations\nin the other parts of the input.\n4.4\nAnalysis of Mesh and SDF robustness to noise\nWe now illustrate the motivation for our hybrid representation\nby a simple controlled experiment, where we aim to reconstruct a\nfixed 3D target with different levels of noise in the guidance. We\nformulate it as a reconstruction problem to have a clear ground truth\nand eliminate ambiguity arising due to text-based objectives. Even\nthough we use synthetic noise, we expect these findings to apply\nin an SDS setting, where gradients are also noisy due to random\nnoising step performed at each SDS iteration [Poole et al. 2022].\nGiven multi-view renderings of a fixed (true) 3D model, we add\nuncorrelated per-pixel Gaussian noise to each image, and compute\nL2 pixel-wise loss to guide our shape representation towards the tar-\nget. As we increase the noise level (by increasing standard deviation)\nwe find that different representations are more prone to errors in\nreconstructing the target. We use L2 re-projection error with respect\nto the ground truth shape as our evaluation metric, and compare\nvanilla Mesh, SDF representations to our hybrid approach (using\nour mesh rendering), and show results in Figure 5. Note that as the\namount of noise increases, the quality of the mesh reconstruction\ndegrades sharply. At the highest noise regimes (standard deviation\nof 5), the mesh reconstruction degenerates to a blob, while the SDF\nreconstruction is still recognizable despite surface irregularities.\nImportantly, the hybrid representation performs better than both\nindividual representation at all levels of noise, and the benefits are\nthe strongest at higher noise level.\n4.5\nAblations\nWe perform several ablations to justify the design of our system.\nNo face supersampling for mesh colors. Figure 6 illustrates the\nneed for the mesh-driven super-sampling of the appearance net-\nwork based on Mesh Colors schema (Sec 3.1). We optimize our\nMagicClay: Sculpting Meshes With Generative Neural Fields\n\u2022\n7\nAppearance (SDF)\nMeshColors + Appearance \nNetwork (Ours)\nNo color supersampling\nFig. 6. Ablation: no color supersampling. Using mesh-guided super-\nsampling in conjunction with the appearance network allows to decouple\ngeometry and appearance. Using this approach (top right), large faces are\nused for the mesh, even though the rendering still presents high frequency\ncolors (bottom). When using a color-per-vertex scheme (top middle), signifi-\ncantly larger mesh resolution is required to achieve similar appearance.\nWithout freeze loss\nWith freeze loss\nSDF\nMesh\nOutput\nRenders\nRenders\nOutput\nSDF\nMesh\nFig. 7. Ablation: not enforcing localization. Without localization and\nfreeze losses we observe that shape changes can propagate beyond the user-\nhighlighted area, potentially destroying the original content. Here armadillo\nwas erased by \u201cangel wings.\u201d\nrepresentation with respect to a target image (first column), either\nsampling colors per-vertex (second column), or using our adaptive\nsampling using MeshColors (third column). Note that without Mesh-\nColors sampling a much higher resolution is needed, which leads to\npoorer reconstruction and an over-tesellated mesh.\nNot enforcing localization, no freeze loss. We remove the mech-\nanism for enforcing localization via fixing non-selected surface\nregions and nearby SDF values as discussed in Sec. 3.3. In Figure 7\nwe show that without this feature, the shapes undergo unintended\nglobal changes, potentially erasing the original shape.\nNo topology updates. The topological updates (Sec. 3.4) during\nthe optimization significantly improve the results as they allow to\ngradually add resolution. Optimizing a fixed-resolution mesh would\neither result in a shape that only marginally differs from input if\nthe initial resolution is too high (Fig 8, left), or lacks fine details if\nthe initial resolution is too coarse (Fig 8, right).\n5\nCONCLUSION\nWe presented MagicClay, a generative sculpting tool, backed by our\nnew hybrid SDF and mesh representation. We demonstrated the\nimportance of the hybrid representation through careful analysis\nand baselines. Key to the success of the generative process is our\nnew rendering strategy that leverages the mesh part of the hybrid\nrepresentation to localize ray sampled in the volumetric rendering\nof the SDF. We believe MagicClay is an important step towards\n\u201ca delicious hamburger\u201d\n\u201ca man holding a sword\u201d\nFig. 8. Ablation: no topology updates. Optimizing the mesh without\ntopology update results in the final generated object being limited by the\ninitial resolution. Left When starting with a fine mesh the optimization\nwill often get stuck since each vertex has tiny effect on the objective. Right\nWhen starting from a coarse mesh, no fine details can be created.\nturning the recent advancements in text-to-image-from-scratch into\nan actual modeling tool usable by artists in an iterative workflow.\nLimitations. Our method is inherently constrained by the quality\nof the SDS gradients. This stems from the inability of the current\ngenerative model to generate consistent multi-view images during\nscore-distillation sampling. Each view tracts the optimization in\na different direction which results in a noisy process, rendering\nthe emergence of fine details more difficult. That said, MagicClay\nemploys current text-to-shape methods without adaptation, and\ncan easily integrate and benefit from the unavoidable further devel-\nopment of the field. Second, MagicClay is not interactive, as run-\nning MagicClay takes \u02dc1 hr per prompt on an A100 GPU. We know\nfrom the NeRF literature that a couple of dozens of multi-view im-\nages are sufficient to reconstruct 3D shapes very efficiently. Hence,\nwe believe that as diffusion models become better at generating\nconsistent images, the number of iterations required by MagicClay\nwill drop significantly, making it much faster.\nFuture work. We see several venues for future research. First,\nwe see opportunities to leverage inpainting and depth-conditioned\ndiffusion models. Indeed, this generative process transforms the full\nobject in each rendering, whereas it is clear that some part of the\ngenerated image should stay the same as the 3D edit is localized.\nWe think that leveraging this insight would reduce the amount of\nnoise in the 3D optimization helping the optimization reach a better\ngeometry and faster. Second, we would like to explore using image\ntargets with our system. We believe this could help making edits\nmore specific, and also could allow more control, where users can\nhighlight which part of the image should affect the generated shape.\nFinally, embellishing the surface representation to capture high\nresolution geometric details (such as normal and bump maps) and\nconnecting it to SDF representation with appropriate consistency\nlosses, can further improve the quality of the resulting shapes.\nREFERENCES\nAutodesk. 2024. Mudbox. https://www.autodesk.com/products/mudbox.\nAmir Barda, Yotam Erel, Yoni Kasten, and Amit H. Bermano. 2023. ROAR: Robust Adap-\ntive Reconstruction of Shapes Using Planar Projections. arXiv:2307.00690 [cs.GR]\nHenning Biermann, Ioana Martin, Fausto Bernardini, and Denis Zorin. 2002. Cut-and-\nPaste Editing of Multiresolution Surfaces.\nBlender. 2024. http://www.blender.org.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentan-\ngling Geometry and Appearance for High-quality Text-to-3D Content Creation.\narXiv:2303.13873 [cs.CV]\n8\n\u2022\nAmir Barda, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, and Thibault Groueix\nZhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape\nModeling.\nThomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer,\nAyellet Tal, Szymon Rusinkiewicz, and David Dobkin. 2004. Modeling by Example.\nACM Transactions on Graphics (2004).\nWilliam Gao, Noam Aigerman, Thibault Groueix, Vladimir G. Kim, and Rana\nHanocka. 2023.\nTextDeformer: Geometry Manipulation using Text Guidance.\narXiv:2304.13348 [cs.CV]\nMichael Garland and Paul S. Heckbert. 2023. Surface Simplification Using Quadric\nError Metrics. , 8 pages. https://doi.org/10.1145/3596711.3596727\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan\nLuo, Chia-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\n2023. threestudio: A unified framework for 3D content generation. https://github.\ncom/threestudio-project/threestudio.\nAlec Jacobson, Zhigang Deng, Ladislav Kavan, and J.P. Lewis. 2014. Skinning: Real-time\nShape Deformation.\nEvangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, and Vladlen Koltun. 2012.\nA Probabilistic Model of Component-Based Shape Synthesis. ACM Transactions on\nGraphics 31, 4 (2012).\nLeif Kobbelt. 2000. Sqrt(3)-Subdivision. ACM SIGGRAPH 2000 (05 2000).\nSamuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo\nAila. 2020. Modular Primitives for High-Performance Differentiable Rendering.\nACM Transactions on Graphics 39, 6 (2020).\nJiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong,\nKalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. 2023. Instant3d: Fast text-to-3d\nwith sparse-view generation and large reconstruction model.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-\nResolution Text-to-3D Content Creation. arXiv:2211.10440 [cs.CV]\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields\nfor view synthesis. , 99\u2013106 pages.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4, Article 102 (July 2022), 15 pages.\nhttps://doi.org/10.1145/3528223.\n3530127\nWerner Palfinger. 2022. Continuous remeshing for inverse rendering. Computer\nAnimation and Virtual Worlds 33 (07 2022). https://doi.org/10.1002/cav.2101\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-\ngrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape\nRepresentation.\nMengqi Peng, Jun Xing, and Li-Yi Wei. 2018. Autocomplete 3D Sculpting.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion:\nText-to-3D using 2D Diffusion. arXiv:2209.14988 [cs.CV]\nOmid Poursaeed, Matthew Fisher, Noam Aigerman, and Vladimir G. Kim. 2020. Cou-\npling Explicit and Implicit Surface Representations for Generative 3D Modeling. In\nComputer Vision \u2013 ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and\nJan-Michael Frahm (Eds.). Springer International Publishing, Cham, 667\u2013683.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li,\nHsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. 2023. Magic123: One Image to High-Quality 3D Object Generation Using\nBoth 2D and 3D Diffusion Priors. arXiv:2306.17843 [cs.CV]\nMarie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer,\nAbhijit Kundu, and Federico Tombari. 2023. NeRFMeshing: Distilling Neural Radi-\nance Fields into Geometrically-Accurate 3D Meshes. arXiv:2303.09431 [cs.CV]\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models for\nSubject-Driven Generation.\nGabriele Salvati, Christian Santoni, Valentina Tibaldo, and Fabio Pellacini. 2015. Mesh-\nHisto: Collaborative Modeling by Sharing and Retargeting Editing Histories. ACM\nTrans. Graph. (2015).\nR Schmidt, C Grimm, and B Wyvill. 2006. Interactive decal compositing with discrete\nexponential maps.\nEtai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. 2023. Vox-E:\nText-guided Voxel Editing of 3D Objects. arXiv:2303.12048 [cs.CV]\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep\nMarching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Syn-\nthesis.\nSubstanceModeler. 2024. Substance Modeler. https://www.adobe.com/ie/products/\nsubstance3d-modeler.html.\nJingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin\nLiu. 2023. DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion\nPrior. arXiv:2310.16818 [cs.CV]\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Fed-\nerico Tombari. 2023. TextMesh: Generation of Realistic 3D Meshes From Text\nPrompts. arXiv:2304.12439 [cs.CV]\nHaithem Turki, Vasu Agrawal, Samuel Rota Bul\u00f2, Lorenzo Porzi, Peter Kontschieder,\nDeva Ramanan, Michael Zollh\u00f6fer, and Christian Richardt. 2023. HybridNeRF: Effi-\ncient Neural Rendering via Adaptive Volumetric Surfaces. arXiv:2312.03160 [cs.CV]\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun\nZhu. 2023a. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with\nVariational Score Distillation. arXiv:2305.16213 [cs.LG]\nZian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander\nKeller, Sanja Fidler, Thomas M\u00fcller, and Zan Gojcic. 2023b. Adaptive Shells for\nEfficient Neural Radiance Field Rendering. , 15 pages.\nhttps://doi.org/10.1145/\n3618390\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural\nimplicit surfaces.\nCem Yuksel, John Keyser, and Donald House. 2010. Mesh Colors. ACM Trans. Graph.\n29 (03 2010). https://doi.org/10.1145/1731047.1731053\nErsin Yumer, Siddhartha Chaudhuri, Jessica Hodgins, and Levent Burak Kara. 2015.\nSemantic Shape Editing Using Deformation Handles.\nZBrush. 2024. https://www.maxon.net/en/zbrush.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\nMagicClay: Sculpting Meshes With Generative Neural Fields\n\u2022\n9\n\u201cHead with Horns\u201d\n\u201cMermaid\u201d\n\u201cHorse with saddle\u201d\n\u201cHead with Elf ears\u201d\n\u201cMan holding a\u2026\n\u2026knight\u2019s sword\u201d\n\u2026Wizard staff\u201d\n\u2026maraca\u201d\n\u201cMan with\u2026\n\u2026Angel wings\u201d\n\u2026Airplane wings\u201d\n\u2026bat wings\u201d\nFig. 9. Sculpting gallery. Left: from a source mesh, the user performs a rough edit in under two minutes, highlighted in yellow. Right: MagicClay refines it to\nmatch the provided prompt.\n"
  },
  {
    "title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation",
    "link": "https://arxiv.org/pdf/2403.02827.pdf",
    "upvote": "5",
    "text": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation\nWeijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng\nAlimama Tech, Alibaba Group\nBeijing, China\n{weijie.lwj0, gonglitong.glt, yizhu.zyr, fanda.ffd,\neric.wb, tiezheng.gtz, bozheng}@alibaba-inc.com\nAbstract\nImage-to-video (I2V) generation tasks always suffer\nfrom keeping high fidelity in the open domains. Traditional\nimage animation techniques primarily focus on specific do-\nmains such as faces or human poses, making them difficult\nto generalize to open domains. Several recent I2V frame-\nworks based on diffusion models can generate dynamic con-\ntent for open domain images but fail to maintain fidelity. We\nfound that two main factors of low fidelity are the loss of\nimage details and the noise prediction biases during the de-\nnoising process. To this end, we propose an effective method\nthat can be applied to mainstream video diffusion models.\nThis method achieves high fidelity based on supplement-\ning more precise image information and noise rectification.\nSpecifically, given a specified image, our method first adds\nnoise to the input image latent to keep more details, then\ndenoises the noisy latent with proper rectification to allevi-\nate the noise prediction biases. Our method is tuning-free\nand plug-and-play. The experimental results demonstrate\nthe effectiveness of our approach in improving the fidelity\nof generated videos. For more image-to-video generated\nresults, please refer to the project website: https://noise-\nrectification.github.io/.\n1. Introduction\nWith the remarkable breakthroughs of diffusion mod-\nels in generating exquisite images [7, 36, 37, 39, 40], re-\nsearchers are exploring the further potential of diffusion\nmodels to achieve more coherent video generation. Some\nrecent works [15, 16, 43, 49, 51] have made incremental\nprogress in the text-to-video (T2V) task to generate videos\nthat align with the input text. However, a textual description\ncan correspond to various imaginable videos, which may\nnot necessarily meet people\u2019s specific expectations. There-\nfore, the reference image is proposed to guide the video\ngeneration process, aiming to generate videos that closely\nalign with the given image or even strictly start from the\nstill image, which is called the image-to-video (I2V) task.\nThe concept of image-to-video is not novel and has long\nexisted in traditional computer vision tasks, such as facial\nanimation [42, 50], body motion synthesis [33], nature-\ndriven animation [17, 28], and video prediction [9, 18, 24],\nwhich can all be considered as I2V tasks. However, these\ntasks were either limited to specific domains (such as faces,\nhuman poses, and simple natural scenes) or focused on\nrelatively simple scenarios (such as animating fonts [9],\ndrawing [44] or moving rigid objects [19]). The proposed\nsolutions for these specific tasks are difficult to be ap-\nplied to open-domain images.\nMoreover, previous stud-\nies [18, 19, 23, 28, 48, 53] adopted the autoregressive ap-\nproach to generate the video sequences, which is compu-\ntationally expensive and still faces challenges in complex\nopen-domain scenarios.\nRecently, the emerged diffusion\nmodels have demonstrated strong generative capabilities\nand significant extensibility by learning the data distribu-\ntion from noise. As a video can be considered as a temporal\nsequence (batch) of highly correlated images, it is feasible\nto process videos in batches using diffusion models. Con-\nsequently, there is a growing focus on leveraging diffusion\nmodels for image-to-video task, attracting significant atten-\ntion from both research and industry.\nHowever, current I2V research [4, 11, 51, 59] primar-\nily relies on enhancing the supervision of image signals to\nguide the video diffusion model. As a result, the gener-\nated videos are only able to resemble the given image. In\nour view, existing video diffusion models in these works\nalready exhibit strong capabilities for generating dynamic\nmotions, but they struggle to maintain fidelity, which can be\nattributed to two main factors. One is the loss of image de-\ntails, such as adopting IP-Adapter [54] or ControlNet [58]\nonly extracts partial image representation. Another is the\nnoise prediction biases during the denoise process, due to\nthe unattainable perfect zero loss in training the video dif-\nfusion model, even when the entire image information has\nbeen injected or concatenated. Inspired by the transition re-\nfinement with the pivotal noise vector in recent image edit-\n1\narXiv:2403.02827v1  [cs.CV]  5 Mar 2024\ning work [6, 29, 30], we propose to set the direction of ini-\ntial noise as the pivotal reference in the denoising process.\nSpecifically, we design an image-to-video pipeline, which\nadopts the \u201cnoising and rectified denoising\u201d reverse process\nto improve the fidelity of generated video. Our method uti-\nlizes pre-trained video latent diffusion models (VLDM) to\ngenerate fluent motion between frames. During the infer-\nence, we first add initial noise to the given image to alle-\nviate the loss of detail information, denoted as \u201cnoising\u201d\nstage. Then we properly rectify the predicted noise using\nthe pivotal reference noise in the reverse denoise timesteps\nto alleviate the noise prediction biases, denote as \u201crectified-\ndenoising\u201d stage. Additionally, in order to control the reten-\ntion degree of the reference image, we further introduce a\npractical step-adaptive intervention strategy based on noise\nrectification.\nIn general, we propose an effective method to utilize the\nexisting pre-trained video diffusion models for image-to-\nvideo tasks. The comparison experiments with current pub-\nlic I2V works and several I2V attempts in the active com-\nmunity have demonstrated the effectiveness of our meth-\nods in generating videos with higher fidelity. Moreover, our\nmethod does not require extra training and is simple to im-\nplement, which can be seamlessly integrated with current\npre-trained open-domain video diffusion models in a plug-\nand-play manner, enabling high-fidelity I2V generation in\nopen domains.\n2. Related Work\nThe diffusion models have achieved great success in gen-\nerative tasks in recent years. Due to the high correlation be-\ntween image and video modalities, many ideas and insights\nin current video generation work have been inspired by ex-\ntensive image generation work. Therefore, we introduce the\nrelated work on image and video generation here.\n2.1. Image Generation with Diffusion Model\nCompared to the traditional GAN [10] and VAE [22]\nbased methods, the diffusion models [7, 14, 36, 40, 45\u2013\n47] have demonstrated more powerful capabilities to pro-\nduce high-quality images with realistic textures and fine\ndetails.\nThe U-Net [38] with the attention layer is the\nwidely adopted structure in image diffusion models to pre-\ndict noise.\nTo save computation costs, Stable Diffusion\n(SD) [37] proposed the latent diffusion model (LDM),\nwhich utilized VAE [22] to encode the image into a la-\ntent space and perform the diffusion process on the latent\nspace. To enhance the controllability and support various\ncontrol conditions such as depth, reference image, normal\nmap and canny map, ControlNet [58] and T2I-Adapter [31]\nintroduced a flexible adapter based on the SD [37] for con-\ntrolled generation. Recently, IP-Adapter [54] also proposed\nan image prompt adapter for T2I models to guide the image\ngeneration with the reference image. Besides, SDEdit [29]\nadded noise to the input stroke image and progressively de-\nnoised the resulting image to increase the realism of the\nsynthesized image. In our image-to-video work, we adopt\nan inflated 3D U-Net similar to the T2I task and our noise\nrectification also takes inspiration from the transition refine-\nment in the image editing works [6, 29, 30].\n2.2. Video Generation with Diffusion Model\nThanks to the significant progress of text-to-image gen-\neration, video generation has also started to develop from\nthe text-to-video (T2V) task. VDM [16] introduced a pi-\noneering video diffusion model that extends the 2D U-\nNet to a 3D U-Net structure, jointly training both im-\nages and videos in the pixel space.\nSubsequent meth-\nods [1, 3, 12, 13, 26, 49, 52, 57, 60] mostly adopted the\nlatent space to reduce memory requirements and speed\nup the training and inference.\nTo optimize the running\ntime required for video generation, most works (Make-A-\nVideo [43], ModelScopeT2V [49], Latent-Shift [1], Ani-\nmateDiff [12]) were built upon the pre-trained T2I models\nand incorporated temporal modules, enabling batch genera-\ntion of all video frames simultaneously. Particularly, An-\nimateDiff [12] only trains a motion module that can be\nadapted to various personalized T2I models. Text2Video-\nZero [21] proposed a training-free sampling method to en-\nrich motion dynamics and maintain temporal consistency\nwith cross-frame attention. Besides, the cascade framework\nof video diffusion models is also used to generate high-\nresolution [3, 15, 52] and longer videos [13, 56].\nSimilar to image generation, introducing more control\nconditions in video generation is also crucial. Recently, to\nmake the generated videos more controllable, recent work\nhas introduced various conditions into the video diffusion\nmodels, including depth [8, 43], pose [20, 27], guided mo-\ntion from trajectory [55], stroke painting [5] or frequency\nanalysis [25]. As to the image condition, existing video\ngeneration work mainly draws on experiences from the im-\nage generation field, e.g., enhancing the image guidance us-\ning ControlNet [58] and IP-Adapter [54]. Besides, Seer [11]\nconcatenated the conditional image latent with the noisy la-\ntent and employed causal attention in the temporal module\nof 3D U-Net for image-to-video tasks. VideoComposer [51]\nproposed to concatenate the image embedding with the\nnoisy latent along the feature channel, as well as support\nforwarding the style of the given image into the video la-\ntent diffusion model (VLDM). Recently, VideoCrafter [4]\nextracted the image feature into the VLDM for the image-\nto-video task. Similarly, I2VGen-XL [59] both added the\nimage latent with the noisy latent in the input layer and built\na global encoder to extract the image CLIP feature into the\nVLDM. However, these image-to-video works either have\nlimited fidelity or require fine-tuning the whole VLDM. In\n2\nzt\nzt\u22121\nz0:L\u22121\nt\nz0:L\u22121\nt\u22121\nImage Diffusion Model\nVideo Diffusion Model\n(a)\nSpatial Module\nTemporal Module\n\u211dB\u00d7C\u00d7L\u00d7H\u2032 \u00d7W\u2032 \n\u211d(B\u00d7L)\u00d7C\u00d7H\u2032 \u00d7W\u2032 \n\u211d(B\u00d7H\u2032 \u00d7W\u2032 )\u00d7L\u00d7C\n\u211dB\u00d7C\u00d7L\u00d7H\u2032 \u00d7W\u2032 \n\u211dB\u00d7C\u00d7L\u00d7H\u2032 \u00d7W\u2032 \n(b)\nInflated 3D U-Net\n2D U-Net \nFigure 1. The general framework of image diffusion model and\nvideo diffusion model with inflated 3D U-Net structure.\ncomparison, our noise rectification method is tuning-free\nand maintains high fidelity.\n3. Preliminary\n3.1. Image-to-Video Task Definition\nAll video generation tasks require generating coherent\nframes that maintain both visual consistency and logical\nmotion. Specifically, image-to-video (I2V) task is defined\nas generating a video from a specified reference image. Its\ngoal is to transform the static nature of an image into the dy-\nnamic visual representation, adding motion and fluidity to\nthe content. Compared to the text-to-video (T2V) task, I2V\nprioritizes high fidelity with the conditional image, while\ndynamic motion in the video can be learned through com-\nmon prior knowledge or driven by the given conditions like\nthe text description or other data forms. Here we focus on\nthe text-conditioned image-to-video task, and this definition\ncan be formulated as, given a still image I and a text de-\nscription c, the generative system outputs a predicted video\nV 0:L\u22121 =\n\b\u00afI0, . . . , \u00afIL\u22121\t\n, where L represents the video\nlength. The objective is to keep appearance consistent with\nthe given initial image I, as well as ensure the generated\nvideo aligns with the text description c.\n3.2. Video Latent Diffusion Models\nThe diffusion models [14, 45\u201347] are a class of gener-\native models inspired by non-equilibrium thermodynamics,\nwhich define the Markov chain to perturb data to noise in\nthe diffusion process and then learn to convert noise back\nto data in the reverse process. Formally, in the diffusion\nprocess, given a data distribution z0 \u223c q(z0), the forward\nMarkov chain gradually adds Gaussian noise to the data\nsample in the T timesteps, thus obtaining a sequence of\nnoisy data {z1, z2, . . . , zT } conditioned on z0, following\nz0:L\u22121\n0\nz0:L\u22121\nT\nz0:L\u22121\n0\nz0:L\u22121\nT\nAdd Noise\nInflated 3D U-Net\nInflated 3D U-Net\nInflated 3D U-Net\nz0:L\u22121\n0\nz0:L\u22121\nT\nConcate\nCondition \nEmbedding\nEncoder\n(a)\n(b)\nEncoder\nFigure 2. Two basic approaches in existing research and commu-\nnity regarding image-to-video generation.\nthe transition formula, which can be denoted as:\nq(z1:T |z0) =\nT\nY\nt=1\nq(zt|zt\u22121),\n(1)\nq(zt|zt\u22121) = N(zt; \u221a\u03b1tzt\u22121, (1 \u2212 \u03b1t)I),\n(2)\nwhere {\u03b1t \u2208 (0, 1)}T\nt=1 is a variance schedule to control the\nstep size. In the reverse process, a model p\u03b8 is learned to\ndenoise from the noisy prior zT \u223c N(0, I) to gradually\ngenerate the desired data iteratively following:\np\u03b8(zt\u22121|zt) = N(zt\u22121; \u00b5\u03b8(zt, t), \u03a3\u03b8(zt, t)),\n(3)\nwhere \u03b8 is the model parameters, \u00b5\u03b8(zt, t) and \u03a3\u03b8(zt, t) de-\nnote the predicted mean and variance by the model.\nIn the image generative tasks, the denoising model is\nusually designed as the U-Net network architecture and\nlearned with the objective function\nmin\n\u03b8\nEz0\u223cpdata,t,\u03f5\u223cN (0,I)[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, c, t)\u22252\n2],\n(4)\nwhere \u03f5 and \u03f5\u03b8 are the actual and predicted noise respec-\ntively, c represents various conditions like text, image, or\nother control signals. Furthermore, to reduce the computa-\ntional complexity, diffusion models are utilized in a lower-\ndimensional latent space rather than the pixel space, which\nis denoted as the latent diffusion model [37].\nSimilar to image diffusion generation, video diffusion\ngeneration can be regarded as dealing with a batch of im-\nages together (see Fig.1).\nRecently, video latent diffu-\nsion models (VLDM) were also developed upon the text-\nto-image generation and followed the aforementioned diffu-\nsion process, aiming to model the video data from Gaussian\nnoise. Formally, a given video data V 0:L\u22121 \u2208 RL\u00d73\u00d7H\u00d7W\nwill be converted to the latent representation z0:L\u22121\n0\n\u2208\nRL\u00d7C\u00d7H\n\u2032\u00d7W\n\u2032\nthrough a VAE encoder [22], where C is\nthe number of feature channels. Besides, due to the tem-\nporal consistency and content relevance requirements in\nthe video, the VLDM often involves the additional tem-\nporal module [1, 12, 16, 49], thus inflating the denoising\n3\nAdd noise\nI2V (low fidelity)\nOurs (high fidelity)\nNoise Bias\n frame\n1st\n frame\n8th\n frame\n16th\nX\n\u03c98 \u22c5\n\u20d7\n\u03941\nt1\n(1 \u2212 \u03c98) \u22c5\n\u20d7\n\u03948\nt1\n\u03c916 \u22c5\n\u20d7\n\u03941\nt1\n(1 \u2212 \u03c916) \u22c5\n\u20d7\n\u039416\nt1\n400 timesteps\n200 timesteps\n960 timesteps\n1 timestep\nOurs\nBaseline \nI2V\nNoise Rectification\nX\nInput Image\n\u20d7\n\u03941\nt1\n(1 \u2212 \u03c98) \u22c5\n\u20d7\n\u03948\nt2\n\u03c98 \u22c5\n\u20d7\n\u03941\nt2\n(1 \u2212 \u03c916) \u22c5\n\u20d7\n\u039416\nt2\n\u03c916 \u22c5\n\u20d7\n\u03941\nt2\n\u20d7\n\u03941\nt2\n1st frame\n8th frame\n frame\n16th\nn0:L\u22121\nStep Forward\nEncoder\nz0\nAdd Noise \n(T steps)\nz0:L\u22121\nT\nz0:L\u22121\n\u2026 t\nIn\ufb02ated \n3D U-Net\n\u02dcn0:L\u22121\nt\n\u02dc\u02dcn0:L\u22121\nt\nNoise \nRectification\nz0:L\u22121\nt\u22121\nDenoising Step\n\u2026\nz0:L\u22121\n0\n\u00d7 L\nDecoder\nV0:L\u22121\n(a) Pipeline of our noise-rectification I2V method\nCLIP\n\u201cA bottle on the      \n     beach at sunset\u201d\n(b) Illustration for the pipeline and visualization of intermediate steps\nFigure 3. The framework of our tuning-free image-to-video method. (a) represents the inference pipeline, where the input image is noised\ninto the initial latent and the predicted noise of the inflated 3D U-Net will be rectified during the denoising process. (b) illustrates the\ndetailed generation process of our method. The visualization of intermediate steps shows our method can effectively refine the denoising\ndirection, making intermediate results closer to the given image.\nmodel from 2D U-Net to the 3D U-Net. Through the dif-\nfusion process z0:L\u22121\nt\n= q(z0:L\u22121\n0\n, t) and reverse process\nz0:L\u22121\nt\u22121\n= p\u03b8(z0:L\u22121\nt\n, t), the finally denoised video latent\n\u00afz0:L\u22121\n0\nwill be processed via the VAE decoder to generate\nthe video.\nInspired by the mainstream text-to-video framework, to\ngenerate a video from a still image, we also model the video\nmotion with temporal attention in the inflated 3D U-Net\n[12]. As shown in Fig. 1(b), to improve the computation ef-\nficiency, the video\u2019s frame dimension is treated as the batch\naxis in the forward of the spatial modules, and the video\u2019s\nspatial dimensions are treated as the batch axis in the for-\nward of the temporal modules.\n4. Method\n4.1. Enhance Image Condition Analysis\nAlthough the text-to-video framework can generate a\nvideo clip with relatively coherent motion, the semantic\ncontent of the generated video is mainly aligned with the\ngiven text description at a coarse-grained level. To control\nthe content consistency between the generated video and the\nreference image, the mainstream I2V works in existing re-\nsearch and the community can be summarized into two ba-\nsic types (see Fig.2): One is to incorporate the image condi-\ntion at the beginning of the reverse process. This approach\nis mainly inspired by the image generation field like the\nimg2img tasks, such as the image editing task [6, 29, 30],\nwhose idea is to inject the image latent into the initial noise\nlatent. In this way, the reverse denoising process could be\nimplicitly guided towards the direction of the image latent\nin the latent space. However, this approach can only achieve\na resemblance to the given image and there is still a certain\ngap to high fidelity. A different method involves concate-\nnating the full clean image with the initial noise to intro-\nduce more fine-grained details [11, 51, 59]. While this ap-\nproach improves fidelity, the entire generation framework\nmust be retrained, leading to low scalability and challenges\n4\nin integrating with existing pre-trained modules like Con-\ntrolNet [58].\nAnother method to enhance image fidelity\nintroduces more image feature signals and conditions into\nthe internal computation of the diffusion model at each\ntimestep [4], such as using various ControlNets [58] and IP-\nAdapter [54]. The image features act as strong supervision\nto improve the fidelity. However, since feature extraction in-\nevitably loses image details, these approaches tend to learn\nthe overall style or general layout from the original image,\nmaking it difficult to achieve high fidelity in terms of fine\ndetails.\nAll the above methods aim to enhance the guidance and\ncontrol of the initial image in video generation to improve\nfidelity. However, as shown in Fig.3(b), the denoising pro-\ncess (represented in gray arrow) can not restore the given\nimage even when the initial noisy latents are obtained by\nadding noise to the given image (represented in dashed\nblue arrow), we analyzed that the reason why these meth-\nods [4, 11, 51, 59] fail to achieve perfect fidelity lies in\nthe accumulated noise biases during the denoising process,\ncausing the generated frame latents to deviate from the\ngiven image latent. In the training process, although the\nMSE loss function is utilized to make the predicted noise\nclose to the initial input noise, the training process cannot\ncompletely achieve a perfect loss of 0. Therefore, there will\nalways be a discrepancy between the predicted noise and\nthe true noise. To further improve fidelity, we draw inspira-\ntion from the noise latent and aim to alleviate the noise gap\nduring the denoising process.\n4.2. Noise Rectification Strategy\nOur method includes the \u201cnoising and rectified denois-\ning\u201d process. Similar to [29], our approach starts by inject-\ning the image latent into the initial noise. Without introduc-\ning any additional operations, such a setting could gener-\nate a coherent video that resembles the given image in the\nwhole style and layout. Taking a different perspective, if\nthe denoising process adopts the known initial noise rather\nthan the predicted biased noise at each timestep, it would\nresult in a video sequence that is entirely faithful but also\nlacks any motion or dynamics. Therefore, to strike a bal-\nance between complete fidelity and dynamics, we propose\na noise rectification method. The pipeline of our inference\nprocess is shown in Fig.3(a), in some intermediate steps of\nthe denoising process, we rectify the predicted noises by\nadaptively compensating them with the known initial noise,\nwhich is formulated as\neen0:L\u22121\nt\n= Rectify(en0:L\u22121\nt\n, n0:L\u22121, t, \u03c90:L\u22121, \u03c4),\n(5)\nwhere een0:L\u22121\nt\ndenotes the rectified noise at tth timestep,\nen0:L\u22121\nt\ndenotes the predicted noise of 3D-UNet, n0:L\u22121\ndenotes the initial sampled noise that is added to a given\nAlgorithm 1 Noise Rectification for Image-to-Video\nInput: The given image latent z0, optional text embed-\nding c, video length L, rectification weight \u03c90:L\u22121 and\ntimestep period \u03c4.\nOutput: The generated video latent z0:L\u22121\n0\n.\n1: n0:L\u22121 \u223c N(0, I)\n2: z0:L\u22121\nT\n\u2190 AddNoise(Repeat(z0), n0:L\u22121, T)\n3: for t = T, . . . , 1 do\n4:\nPredict noise en0:L\u22121\nt\n= \u03f5\u03b8(z0:L\u22121\nt\n, c, t)\n5:\nCompute noise gap \u22060:L\u22121\nt\n= n0:L\u22121 \u2212 en0:L\u22121\nt\n6:\nif t in \u03c4 then\n7:\nRectify een0:L\u22121\nt\n= en0:L\u22121\nt\n+ \u03c90:L\u22121 \u00b7 Repeat(\u22060\nt)\n+(1 \u2212 \u03c90:L\u22121) \u00b7 \u22060:L\u22121\nt\n8:\nelse\n9:\neen0:L\u22121\nt\n= en0:L\u22121\nt\n10:\nend if\n11:\nz0:L\u22121\nt\u22121\n\u2190 Sample(z0:L\u22121\nt\n, een0:L\u22121\nt\n)\n12: end for\n13: return z0:L\u22121\n0\nimage, \u03c90:L\u22121 and \u03c4 denote the rectification weight and\ntimestep period.\nConcretely, in our noise rectification strategy, the noise\nen0:L\u22121\nt\npredicted by U-Net at each step t is first obtained:\nen0:L\u22121\nt\n= \u03f5\u03b8(z0:L\u22121\nt\n, c, t),\n(6)\nwhere z0:L\u22121\nt\nis the input latent map at step t and \u03f5\u03b8(\u00b7) is the\ndenoise model (an inflated 3D U-Net). c and L are the text\nembedding and video length respectively. Then, we can nat-\nurally calculate the noise gap (dubbed \u22060:L\u22121\nt\n) between the\ninitial sampled noise in our noising process and the noise\npredicted during the denoising process.\n\u22060:L\u22121\nt\n= n0:L\u22121 \u2212 en0:L\u22121\nt\n.\n(7)\nWe further calibrate the predicted biased noise, which is\nthe key procedure of our method. By introducing the rec-\ntification weight factor \u03c90:L\u22121, we balance the first frame\nnoise gap and the subsequent frames\u2019 noise gap to obtain the\nweighted rectification offset, which is then used to frame-\nwise update the originally predicted noise.\neen0:L\u22121\nt\n= en0:L\u22121\nt\n+ \u03c90:L\u22121 \u00b7 Repeat(\u22060\nt)\n+ (1 \u2212 \u03c90:L\u22121) \u00b7 \u22060:L\u22121\nt\n,\n(8)\nwhere Repeat(\u00b7) is the broadcasting operation to align the\ntemporal dimension.\nThe whole process of our image-to-video method is de-\ntailed in the Algorithm 1. Such a noise rectification method\nis simple but effective. As illustrated in Fig.3(b), through\nnoise rectification (represented in the green arrow), the ac-\ncumulation noise gap could be effectively alleviated and\n5\nVLDM \n+SDEdit\nVLDM \n+ConcateImage\nVLDM \n+Ctrl.R.O.\nVLDM \n+IP-Adapter\nOurs\nOurs \n+IP-Adapter\n\u201cNorthern \nlights over a \nwaterfall\u201d\n\u201cGolden Gate \nBridge in San \nFrancisco\u201d\n\u201cSnowfall in the \nstreet\u201d\nI2VGen-XL\nVideoComposer\nVideoCrafter1-I2V\nInput Image\nFigure 4. Visual comparison with current image-to-video methods. We use AnimateDiff [12] as the VLDM. \u201cCtrl.R.O.\u201d means ControlNet\nReference-Only method [58]. Our method achieves higher fidelity in the video sequences with the given image.\nthus the noisy latent of generated frames are closer to the\nimage latent. In this way, the fine-grained content details\nof the reference image can be well preserved in the gener-\nated video. In addition, to control the retention degree of\nthe reference image, we further introduce a step-adaptive\nintervention strategy based on noise rectification. Specifi-\ncally, by adjusting the parameter of rectification steps \u03c4 and\nweight \u03c90:L\u22121, our method could control the fidelity de-\ngree of the generated video. It is worth mentioning that our\nmethod is tuning-free and can be applied to most current\nvideo diffusion models.\n5. Experiments\n5.1. Experimental Setup\nDataset.\nWe utilized two public datasets Web-\nVid10M [2] and LAION-Aesthetic in [41] to evaluate our\nmethod. As for the WebVid10M dataset, we randomly sam-\npled 1000 video-text pairs in proportion to the different cat-\negories in its validation subset. For quantitative evaluation,\nto avoid buffering frames at the beginning of the videos,\nwe selected the 10th frame as the image input for the video\ngeneration, along with the video\u2019s text description. As for\nthe LAION-Aesthetic dataset, we also randomly chose 1000\nimage-text pairs for the qualitative evaluation.\nEvaluation Metrics. For the image-to-video generation\n6\nMethods\nImage fidelity\u2191\nTemporal coherence\u2191\nVideo-text alignment\u2191\nVLDM [12] + SDEdit [29]\n0.7425\n0.9888\n0.2548\nVLDM [12] + ConcateImage\n0.6944\n0.9427\n0.2084\nVLDM [12] + Ctrl.R.O. [58]\n0.7689\n0.9919\n0.2466\nVLDM [12] + IP-Adapter [54]\n0.7650\n0.9918\n0.2287\nVideoComposer [51]\n0.7483\n0.9352\n0.2447\nVideoCrafter1-I2V [4]\n0.7695\n0.9689\n0.2206\nI2VGen-XL [59]\n0.7717\n0.9560\n0.2208\nOurs\n0.7907\n0.9882\n0.2517\nOurs + IP-Adapter [54]\n0.8042\n0.9934\n0.2405\nTable 1. Quantitative comparison results on the WebVid dataset [2].\ntask, the focus lies on the fidelity and smoothness of the\ngenerated videos. Therefore, we assess the generated video\nfrom three aspects: image fidelity, temporal coherence, and\nvideo-text alignment. Specifically, to evaluate the fidelity\nbetween the generated video and the given image, we cal-\nculate the CLIP [35] image similarity for each generated\nframe and the given image. Considering the temporal con-\nsistency in the video, we evaluate the CLIP score between\nthe generated frames. Besides, since the text description is\ninput as a condition, we also calculate the CLIP text-image\nsimilarity to evaluate the semantic relevance between the\ngenerated video and the text description.\n5.2. Comparisons\nComparison Methods. We categorized the comparison\nmethods into these two types as Fig.2. One is to incorporate\nthe image condition into the input layer. (1) SDEdit [29],\na semantic image translation method, which can also be\nused for I2V tasks by simply adding noises to the given im-\nage and then denoising. (2) ConcateImage, another simple\nbaseline to concatenate the image condition on initializa-\ntion noises, which needs to be finetuned to learn the struc-\ntural information of the given image. Another type of ap-\nproach is to perform image condition injection at each layer\nof VLDM. (3) ControlNet Reference-Only [58], an effective\nway to directly link the attention layer of the VLDM to the\nreference image. (4) IP-Adapter [54], using an additional\ncross-attention layer for image prompts to achieve seman-\ntic and structural preservation. (5) VideoCrafter1-I2V [4],\nsimilar to IP-Adapter, is another implementation of image\nprompts injection into the VLDM. Besides, (6) VideoCom-\nposer [51] and (7) I2VGen-XL [59] combine the above two\ntypes of ideas for image injection both at the input and mid-\ndle layers of VLDM.\nBenefiting from plug-and-play and tuning-free proper-\nties, our method can combine with other image-condition\nenhancing modules mentioned above. In order to make an\nintuitive and fair comparison, we conduct our method on\nboth the two above types, denoted as Ours and Ours+IP-\n\u201cPeople in the \nsquare with \nchanging light\u201d\n60%\n=\n\u03c4\nw\nframe\nL \u2212 1\n0\n0\n1\n0.5\nL \u2212 1\n2\nw\nframe\nL \u2212 1\n0\n0\n1\n0.5\nL \u2212 1\n2\nw\nframe\nL \u2212 1\n0\n0\n1\n0.5\nL \u2212 1\n2\nw\nframe\nL \u2212 1\n0\n0\n1\n0.5\nL \u2212 1\n2\nFigure 5. Ablation study on the weight of noise rectification. We\nfix the rectification timestep \u03c4 and change the rectification weights\n\u03c9 for different frames.\nAdapter [54]. For fairness, we select AnimateDiff [12] as\nthe pre-trained VLDM.\nQualitative Comparison.\nAs shown in Fig.4, the\nmethod [29] and ConcateImage which only incorporate the\nimage condition with the noisy latent at the beginning of the\nreverse stage are only able to maintain a similar style of the\ngiven image. In contrast, those methods [4, 51, 54, 58, 59]\nthat iteratively utilize the image information in the model\u2019s\nintermediate computation process can preserve more visual\nfeatures of the given image. In comparison, our method\nmaintains more visual details and achieves high fidelity to\nthe input image. For clearer video samples please refer to\nthe project website.\nQuantitative Comparison.\nAs shown in Tab.1, our\nnoise rectification method effectively improves the fidelity.\nCombined with the additional image prompt module [54],\nour method can obtain the highest video-image fidelity\nvalue of 0.8042 and temporal coherence value of 0.9934.\nBesides, our method still obtains acceptable video-text\n7\n=\n\u03c4\n\u201cAn astronaut \nwith a helmet\u201d\n0%\n20%\n40%\n60%\n80%\n100%\n30% 70%\n100%\n60%\nw\nframe\nL \u2212 1\n0\n0\n1\n0.5\nL \u2212 1\n2\nFigure 6. Ablation study on the timestep of noise rectification. We fix the rectification weight \u03c9 and the green panels show the the\nrectification start and end timesteps \u03c4.\nAnimateDiff ModelScope\nAnimateDiff \n+Ours\nModelScope \n+Ours\nHotshot-XL\nHotshot-XL \n+Ours\n(a) Text-to-Video Generation\n(b) Image-to-Video Generation\nInput Image\n\u201cSunny seaside   \n  with blue sky\u201d\n\u201cSunny seaside   \n  with blue sky\u201d\nFigure 7. A plug-and-play extension of our method on current T2V frameworks to realize I2V. (a) Text-to-video generation results for\ndifferent T2V models. (b) Different T2V frameworks combined with our method for high-fidelity image-to-video generation.\nalignment although we mainly focus on the high fidelity\nimage-to-video task.\n5.3. Ablation Study\nOur method rectifies the predicted noise in the reverse\nsteps and contains two adjustable parameters: rectification\nweight \u03c9 and rectification timestep \u03c4 as introduced in Algo-\nrithm 1. Therefore, we take the ablation study on these two\nparameters, respectively. Specifically, \u03c4 = [s, e] indicates\nthat noise rectification is performed from s ratio to e ratio\nof the total timestep.\nRectification Weight. We fix the rectification timestep\n\u03c4 = [0%, 60%] and change the rectification weights \u03c9 for\ndifferent frames. The ablation results on \u03c9i are shown in\nFig.5, where the plots above the video sequences indicate\nthe rectification weights \u03c9i for the ith frame. It can be ob-\nserved that \u03c9i could affect the fidelity and temporal consis-\ntency of subsequent frames. For example, the results in the\nthird or fourth column may result in abrupt changes in the\nimage detail or motion effects. Therefore, we empirically\nselect the setting of the second column for maintaining high\nfidelity and natural motion.\nRectification Timestep. The rectification timestep pe-\nriod \u03c4 determines in which denoising steps the predicted\nnoise needs to be corrected. As shown in Fig.6, we fix the\nrectification weights \u03c9 and the green panels show the recti-\nfication start and end timestep. If noise rectification is not\nperformed, i.e. the first column \u03c4 = [0%, 0%], the fidelity\n8\nof the generated video will be poor. Starting from the initial\ndenoising, as the noise rectification period increases (from\n\u03c4 = [0%, 20%] to \u03c4 = [0%, 100%]), the fidelity will grad-\nually be improved. However, if the rectification only hap-\npens on the latter denoising process (i.e.,\u03c4 = [30%, 70%]\nor \u03c4 = [60%, 100%]), the generated video will still get poor\nfidelity. These results indicate that accurately predicting\nnoise at the start of the reverse process is crucial for main-\ntaining image fidelity. Considering that a perfect fidelity\nwill scarify the motion intensity, we strike a balance to set\n\u03c4 = [0%, 60%] for all experiments.\nExtension to More VLDMs. Our method utilizes the\nmotion prior of VLDM to model the dynamic motion,\nwhich is actually tuning-free and can be adapted to other\nvideo diffusion models. To evaluate the extension perfor-\nmance of our method, we selected several recent T2V mod-\nels and applied our noise rectification method to implement\nI2V. Besides AnimateDiff [12], ModelScopeT2V [49] is a\ndiffusion-based text-to-video model that utilizes the spatio-\ntemporal block to model dynamics. Hotshot-XL [32] is an\nopen-sourced text-to-GIF model developed to work along-\nside the Stable Diffusion XL (SD-XL) model [34]. We eval-\nuate these three T2V models and extend them to I2V using\nour plug-and-play noise rectification method. As shown in\nFig.7, based on pre-trained motion priors, our method can\nmaintain high fidelity and consistent animation.\n6. Conclusion\nIn this work, we propose a simple but effective noise\nrectification method for image-to-video generation in open\ndomains.\nWe deeply analyze the challenges in I2V and\npropose a tuning-free approach to ensure high fidelity\nthrough a noising and rectified denoising process.\nOur\nmethod is plug-and-play and can be applied to other video\nlatent diffusion models to realize I2V. Experimental results\ndemonstrate the effectiveness of our method. We hope that\nour method provides a new idea to improve fidelity in the\nvideo synthesis field. Notably, our method achieves higher\nfidelity while losing some motion intensity. Therefore, in\nfuture exploration, we will continue to focus on increas-\ning the motion intensity while maintaining high fidelity.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 2, 3\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 6, 7\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575. IEEE,\n2023. 2\n[4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 1, 2, 5, 7\n[5] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffu-\nsion model for controllable video synthesis. arXiv preprint\narXiv:2304.14404, 2023. 2\n[6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. ILVR: conditioning method for\ndenoising diffusion probabilistic models.\nIn ICCV, pages\n14347\u201314356. IEEE, 2021. 2, 4\n[7] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion\nmodels beat gans on image synthesis. In NIPS, pages 8780\u2013\n8794, 2021. 1, 2\n[8] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, pages 7346\u20137356, 2023. 2\n[9] Jean-Yves Franceschi, Edouard Delasalles, Micka\u00a8el Chen,\nSylvain Lamprier, and Patrick Gallinari.\nStochastic la-\ntent residual video prediction. In ICML, pages 3233\u20133246.\nPMLR, 2020. 1\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NIPS, 27, 2014.\n2\n[11] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer:\nLanguage instructed video prediction with latent diffusion\nmodels. arXiv preprint arXiv:2303.14897, 2023. 1, 2, 4, 5\n[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 4, 6, 7,\n9\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation. arXiv preprint arXiv:2211.13221, 2022. 2\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NIPS, 2020. 2, 3\n[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 2\n[16] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video diffu-\nsion models. In NIPS, 2022. 1, 2, 3\n[17] Aleksander Holynski, Brian L Curless, Steven M Seitz, and\nRichard Szeliski. Animating pictures with eulerian motion\nfields. In CVPR, pages 5810\u20135819, 2021. 1\n[18] Tobias H\u00a8oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling. arXiv preprint arXiv:2206.07696, 2022. 1\n9\n[19] Yaosi Hu, Chong Luo, and Zhenzhong Chen.\nMake it\nmove: controllable image-to-video generation with text de-\nscriptions. In CVPR, pages 18219\u201318228, 2022. 1\n[20] Johanna Karras, Aleksander Holynski, Ting-Chun Wang,\nand Ira Kemelmacher-Shlizerman.\nDreampose: Fashion\nimage-to-video synthesis via stable diffusion. arXiv preprint\narXiv:2304.06025, 2023. 2\n[21] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2\n[22] Diederik P. Kingma and Max Welling. Auto-encoding vari-\national bayes. In ICLR, 2014. 2, 3\n[23] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid.\nCcvs: context-aware controllable video synthesis. NIPS, 34:\n14042\u201314055, 2021. 1\n[24] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin\nLu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal\nvideo prediction from still images. In ECCV, pages 600\u2013615,\n2018. 1\n[25] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander\nHolynski.\nGenerative image dynamics.\narXiv preprint\narXiv:2309.07906, 2023. 2\n[26] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\narXiv preprint\narXiv:2303.08320, 2023. 2\n[27] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 2\n[28] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable\nanimation of fluid elements in still images. In CVPR, pages\n3667\u20133676, 2022. 1\n[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR. OpenReview.net, 2022. 2, 4, 5, 7\n[30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038\u20136047.\nIEEE, 2023. 2, 4\n[31] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[32] John Mullan,\nDuncan Crawbuck,\nand Aakash Sastry.\nHotshot-XL, 2023. 9\n[33] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models.\nIn CVPR, pages\n18444\u201318455, 2023. 1\n[34] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 9\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, pages\n8748\u20138763. PMLR, 2021. 7\n[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 2\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10674\u2013\n10685. IEEE, 2022. 1, 2, 3\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, pages 234\u2013241. Springer, 2015. 2\n[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 1\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In NIPS, 2022. 1, 2\n[41] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 6\n[42] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. NIPS, 32, 2019. 1\n[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In ICLR. OpenReview.net, 2023. 1, 2\n[44] Harrison Jesse Smith, Qingyuan Zheng, Yifei Li, Somya\nJain, and Jessica K Hodgins. A method for automatically\nanimating children\u2019s drawings of the human figure. arXiv\npreprint arXiv:2303.12741, 2023. 1\n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn ICML, pages 2256\u2013\n2265. PMLR, 2015. 2, 3\n[46] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 3\n10\n[48] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.\nMcvd-masked conditional video diffusion for prediction,\ngeneration, and interpolation. NIPS, 35:23371\u201323385, 2022.\n1\n[49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n2, 3, 9\n[50] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In CVPR, pages 10039\u201310049, 2021. 1\n[51] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 1, 2, 4, 5, 7\n[52] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2\n[53] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation. Entropy,\n25(10):1469, 2023. 1\n[54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arxiv:2308.06721,\n2023. 1, 2, 5, 7\n[55] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 2\n[56] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li,\nShuguang Liu, Fan Yang, Jianlong Fu, Ming Gong, Lijuan\nWang, Zicheng Liu, Houqiang Li, and Nan Duan. NUWA-\nXL: diffusion over diffusion for extremely long video gener-\nation. In ACL, pages 1309\u20131320. Association for Computa-\ntional Linguistics, 2023. 2\n[57] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 2\n[58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, pages 3836\u20133847, 2023. 1, 2, 5, 6, 7\n[59] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 1, 2, 4, 5, 7\n[60] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2\n11\n"
  }
]