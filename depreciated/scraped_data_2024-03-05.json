[
  {
    "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
    "link": "https://arxiv.org/pdf/2403.01422.pdf",
    "upvote": "21",
    "text": "MovieLLM: Enhancing Long Video Understanding with AI-Generated\nMovies\nZhende Song1\u2217\nChenchen Wang1\u2217\nJiamu Sheng1\u2217\nChi Zhang2\u2020\nGang Yu2\nJiayuan Fan1\u2021\nTao Chen1\n1 Fudan University\n2 Tencent PCG\nzdsong23@m.fudan.edu.cn\n{20307130244, jmsheng18, jyfan, eetchen}@fudan.edu.cn\n{johnczhang, skicyyu}@tencent.com\nhttps://deaddawn.github.io/MovieLLM/\nLine:\nI wonder what \nmysteries I \nwill find in \nthese old pages.\nLine:\nCould this... \ncould this be \nfor me?\nLine:\nWords penned \nwith such \npassion... Could \nit be Thomas?\nLine:\nHow did he \nknow of my \nfeelings for \nhim?\nLine:\nI must let \nThomas know, \nhe must know \nhow I feel.\nQuestion: Why is Rosamund so surprised and \nemotional after reading the letter? \nAnswer: Because she believes that the love letter \nreflects the same feelings she has for Thomas.\nFrame 386\nFrame 387\nFrame 388\nFrame 389\nFrame 390\nFrame 391\nFrame 392\nFrame 393\nFrame 394\nFrame 395\nFilm name: The Duchess and the Commoner\n...\n...\n. . . . . .\n. . . . . .\n...\nFrame 1-10\nFrame 386-395\nFrame 1299-1309\nLine:\nThomas,\nwe must be \ncareful.\nLine:\nIndeed, my love. \nWe have to \nensure our love \nremains a secret.\nLine:\nStay safe, \nmy love.\nLine:\nThe Duchess and \nthe commoner... \nThis surely \ncannot remain \nhidden.\nLine:\nI wonder, if \nI must ever \ngive him up.\nQuestion: Why are Rosamund and Thomas trying to \nkeep their relationship a secret?\nAnswer: They are attempting to protect their \nrelationship from societal mores and prejudices due to \ntheir differing social ranks.\nFrame 474\nFrame 475\nFrame 476\nFrame 477\nFrame 478\nFrame 479\nFrame 480\nFrame 481\nFrame 482\nFrame 483\n. . . . . .\n...\nFrame 474-483\nFigure 1: Examples of generated long video instruction data. We use GPT-4 and guided text-to-image generation\nmodels to generate consistent key frames of move-level video with reasonable lines and corresponding question-\nanswer pairs. These data are used to train multimodal large language models on video understanding.\nAbstract\nThe development of multimodal models has\nmarked a significant step forward in how ma-\nchines understand videos. These models have\nshown promise in analyzing short video clips.\nHowever, when it comes to longer formats like\nmovies, they often fall short. The main hurdles\nare the lack of high-quality, diverse video data\nand the intensive work required to collect or\nannotate such data. In the face of these chal-\nlenges, we propose MovieLLM, a novel frame-\nwork designed to create synthetic, high-quality\ndata for long videos. This framework leverages\nthe power of GPT-4 and text-to-image mod-\nels to generate detailed scripts and correspond-\ning visuals. Our approach stands out for its\nflexibility and scalability, making it a superior\nalternative to traditional data collection meth-\nods. Our extensive experiments validate that\nthe data produced by MovieLLM significantly\n*The first three authors contributed equally to this work.\n\u2020Project Leader.\n\u2021Corresponding Author.\nimproves the performance of multimodal mod-\nels in understanding complex video narratives,\novercoming the limitations of existing datasets\nregarding scarcity and bias.\n1\nIntroduction\nIn recent years, the field of artificial intelligence\nhas witnessed significant progress in the develop-\nment of large language models (LLMs) such as\nGPT-4 (OpenAI, 2023) and LLaMA (Touvron et al.,\n2023). These models have shown exceptional abil-\nity in generating coherent and contextually relevant\ntext. Further extending the capabilities of LLMs\ninto the multimodal domain, vision language mod-\nels (VLMs) like LLaVA (Liu et al., 2023a) have\ndemonstrated remarkable proficiency in tasks that\nrequire an understanding of both visual and lin-\nguistic information, such as image captioning and\nvisual question-answering (QA). The evolution of\nVLMs has now begun to include the dynamic realm\nof video understanding, with models like Video-\nLLaMA (Zhang et al., 2023) and VideoChat (Li\narXiv:2403.01422v1  [cs.CV]  3 Mar 2024\net al., 2023b) highlighting the potential of VLMs\nin processing and interpreting video content.\nHowever, a notable gap remains in the abil-\nity of these models to understand long-duration\nvideos, such as full-length movies. This limita-\ntion is primarily attributed to the lack of exten-\nsive long video datasets necessary for tuning these\nmodels. The LLaMA-VID (Li et al., 2023c) initia-\ntive represents a step forward in this direction by\ncreating long-video tuning datasets derived from\nMovieNet (Huang et al., 2020), indicating that fine-\ntuning LLMs on multimodal long video datasets\ncan enhance their movie-level video comprehen-\nsion. Nevertheless, constructing such benchmarks\nfrom existing datasets is both cost-intensive and\nchallenging, particularly due to the manual effort\nrequired to collect or annotate additional informa-\ntion like movie dialogues. Moreover, despite show-\ning promise, the QA dataset built upon MovieNet\nconfronts several hurdles. These include the la-\nborious nature of manual data collection and the\nlimited diversity in content, which can impede the\ngeneralization capabilities of models due to inher-\nent dataset biases.\nMotivated by these challenges, our paper intro-\nduces a novel and flexible approach for generating\ncomprehensive datasets for long video instruction\ntuning. Our methodology leverages the linguistic\nprowess of GPT-4 and the generative power of sta-\nble diffusion models, offering a unique solution to\nthe problem of dataset diversity and richness. Our\npipeline comprises three primary stages:\n1) Movie Plot Generation. Rather than limiting\nplot generation to conventional data sources such as\nthe web or existing datasets, we harness the power\nof GPT-4 to produce synthesized data. By provid-\ning specific elements such as themes, overview, and\nstyles, we guide GPT-4 to produce movie-level key\nframe descriptions tailored to the latter generation\nprocess.\n2) Style Immobilization Process. By adeptly em-\nploying textual inversion (Gal et al., 2022), we im-\nmobilize the style descriptions generated from the\nscript onto the latent space of the diffusion model.\nThis approach guides the model to generate scenes\nin a fixed style while maintaining diversity under a\nunified aesthetic.\n3) Video instruction data generation. By integrat-\ning the powerful generative capabilities of GPT-4\nwith the developed style-guided diffusion model,\nwe produce style-consistent key frames and corre-\nsponding QA pairs, resulting in a comprehensive\ninstruction tuning corpus, combining the visual\ndata with QA pairs.\nOur approach not only addresses the limitations\nof current datasets for long videos but also paves\nthe way for integrating advanced diffusion mod-\nels to generate video data at higher frame rates.\nMoreover, our methodology allows for the genera-\ntion of datasets without constraints on data volume,\nensuring a high degree of diversity within the gen-\nerated content. Additionally, it facilitates automatic\nannotation, significantly reducing the need for man-\nual labor and associated costs. These advantages\nenhance the scalability, richness, and efficiency\nof dataset creation for long video understanding,\nmarking a significant leap forward in the field. Ex-\namples from our generated long video instruction\ndata are shown in Fig. 1.\nOur contributions are summarized as follows:\n\u2022 We develop a novel pipeline for generating\nmovie-level video instruction tuning datasets\nby combining GPT-4 and diffusion models.\n\u2022 Leveraging our generative approach, we have\ndeveloped and will publicly release a com-\nprehensive dataset for movie-level video un-\nderstanding, alongside a sophisticated model\ntrained for enhanced understanding of long\nvideos.\n\u2022 Based on a real movie dataset, we propose\na benchmark for evaluating long video com-\nprehension capabilities. Experiments on the\nbenchmark show the effectiveness of the pro-\nposed method, significantly outperforming the\nbaseline.\n2\nPreliminary\nTo generate key frames with coherent scenes, we\nemployed the technique, textual inversion (Gal\net al., 2022). This preliminary section provides an\noverview of the textual inversion to lay the ground-\nwork for a detailed exposition of our pipeline.\nTextual Inversion\nThis technique aims to enable\nlanguage-guided image generation of new, user-\nspecified concepts in a Text-to-Image (T2I) model.\nTo achieve this objective, two main steps are re-\nquired: 1) text-embedding, this process first con-\nverts a string incorporating placeholder words into\ntokens. Then, these tokens are converted to a con-\ntinuous vector representation. 2) inversion process,\nMovie Plot Generation\nTheme\nOverview\nMovie style\nFrame-Level Description\nCharacters\n\u201cThe Duchess and the Commoner is a \nbeautifully poignant chronicle of a \nscandalous romance that shakes Victorian \nEngland to its core. The story unfolds \naround Rosamund ......\u201d\n{Gothic}, ((Victorian elegance)), \n((intricate detailing)), soft sepia, \nvoluminous gowns, stiff collars, \ndimly lit rooms, lavish ballrooms, \nelaborative architecture, ......\nArbitrary Style Scene\ngenerate an image \nin <Gothic> style\nTextural \nInversion Process\nImmobilized Style \nEmbedding\nKey Frame\nQA Pairs\nVideo Instruction Data\nFrame Description \nModified\nPrefix+In the ballroom, \nRosamund in lavender, wearing \na lavender gown, stands at \nthe entrance with a confident \nsmile.\nGeneration Guiding\nframe2\nframe3\n......\nframe4\nframe5\nframe6\nframe7\nframe8\nframe9\n......\nframe1\nQ1: Can you write a brief summary of \nthis movie?\nA1: The story unfolds...\nQ2: Where do Rosamund and Thomas \nconfess their love?\nA2: In the Church and the adjacent \ncourtyard.\n......\nStable Diffusion\nRosamund: a woman looks \nlike Keira Knightley in \nlavender\nThomas: a man looks like \nHugh Jackman in grey\nframe-1: In the ballroom, \nRosamund in lavender, wearing a \nlavender gown, stands at the \nentrance with a confident smile.\ndialogue-1: \"Let the show \nbegin.\"\nframe-2: ...\nframe-3\n...\nStyle Immobilization Process\n\u201cA love story\u201d\nPrefix\ngenerate an image in Gothic \nstyle \nStable Diffusion\nFigure 2: Our proposed pipeline for generating long video instruction tuning datasets. With merely a simple\nthematic description, our pipeline is capable of generating key frames of an entire film. The pipeline can be\nroughly divided into three stages: (a) movie plot generation, where we generate the whole movie plot based on a\ntheme phrase using GPT-4. (b) style immobilization process, where we leverage the textual inversion technique\nto immobilize the style descriptions generated from the plot into the latent space of the diffusion model, guiding\nit to generate frames with fixed style. (c) video instruction data generation, where we integrate all the previously\nobtained information to ultimately generate consistent key frames and corresponding question-and-answer pairs.\nthe embedding is then transformed into a single\nconditioning code that guides the generative model.\nThe embedding vector associated with the so-called\npseudo-word will be optimized using a reconstruc-\ntion objective. As such, the embedding is motivated\nto capture visual details unique to the concept. The\nlearned embedding is then used as a condition to\nguide the T2I model to generate customized im-\nages.\n3\nMethod\nThis section elucidates our novel approach for gen-\nerating movie-level video instruction tuning data.\nOur method compromises three interrelated steps:\nmovie plot generation, style immobilization pro-\ncess, and video instruction data generation. The\ncomplete process is depicted in Fig. 2. The de-\ntails of these steps are provided in the following\nsubsections.\n3.1\nMovie Plot Generation\nThe primary objective of this step is to harness the\npowerful capabilities of GPT-4 to generate diverse\nand compelling movie plots. Each movie plot com-\nprises basic elements, including overview, charac-\nters, and frame descriptions. This aims to construct\na high-quality movie plot text for the subsequent\nstyle immobilization and generation process. We\npropose three main strategies for this purpose:\n1) We first employ GPT-4 to generate a set of plau-\nsible movie scripts. Subsequently, we construct\nreusable prompts based on these scripts to gener-\nate a variety of movie plots in bulk. During this\nprocess, phrases describing movie themes, like \"a\ntragic film,\" serve as variables enabling us to direct\nGPT-4 in creating movie plots based on specific\nthemes.\n2) Within the movie plot text, we incorporate two\nkey elements: character and style. Utilizing the\ncharacter and style descriptions generated by GPT-\n4, we aim to ensure great consistency in both the\nportrayal of characters and the stylistic attributes\nacross key frames in the video.\n3) The substantial number of key frames in movie\nplots poses a challenge, as LLMs are prone to for-\ngetting plot details during continuous generation\nprocesses. Therefore, we propose a story expan-\nsion strategy to obtain the continuous key frame\ndescriptions and maximally mitigate the forgetting\nissue inherent in LLMs. We divide the description\nInput\nTheme: Romantic Comedy / Historical Drama / Action \nAdventure / \n......\nOutput\nStory Title: The Symphony of Love\nOverview: 'The Symphony of Love' is a heartwarming tale set in the \nbustling city of New York......\nStory tone and style: Romantic, pastel palette......\nStyle-keyword: Romantic\nLocations:......\nCharacter:......\nEpoch Chapters:\nChapter1: The Melody of Chance\nScene: Subway, Street\nContent: Amelia and Ethan have a chance encounter......\nQA: Question: ...... Answer: ......\n......\nInput\n[The generated Epoch Chapters:]Story Title, Overview, ...... \nNow, expand Chapter 1.\nOutput\nChapter1: The Melody of Chance\nThread1-1: The Unexpected Performance\nScene: Subway\nContent: Amelia is performing a soulful violin piece......\n......\nThread1-11: The Beginning of Something New\nScene: Street\nContent: Amelia, walking down the street, receives Ethan's \ninvitation......\nQA: Question: ...... Answer: ......\nLevel 1: Epoch Chapters Generation\nLevel 2: Narrative Threads Generation\nInput\nOverview: 'The Symphony of Love' is a heartwarming tale......\n[The generated Narrative Thread] Now, expand Thread1-1: ......\nOutput\nThread1-1: The Unexpected Performance\nframe1: In the subway station, Amelia is standing in the crowd, \nplaying the violin\n......  \ndialgue1: Only music can heal me\n......  \nframe11: In the subway station, Amelia is sitting on a bench\ndialgue6: I need some rest\nQA: Question: ...... Answer: ......\nLevel 3: Frame Description Generation\nFigure 3: Illustration of the story expansion strategy.\nTo obtain frame descriptions of a movie-level video\nand maximally mitigate the forgetting issue, we adopt a three-level story expansion strategy. The strategy can be\ndivided into three levels: (a) epoch chapter generation, where each chapter represents a significant period in story\ndevelopment. (b) narrative threads generation, where each thread refines each major chapter into more specific\nnarrative sub-stories. (c) frame description generation, where each frame description elaborates on these narrative\nsub-stories, depicting the critical visual frames.\ngeneration process into three levels.\nWe now elucidate how we construct the prompt\nto obtain high-quality movie plot text. Details can\nbe found in appendix B.1. Our prompt is comprised\nof the following elements:\nMovie Theme: As mentioned above, we treat the\nphrases used to describe a movie theme as vari-\nables. Therefore, various movie themes are first\ngenerated to ensure the diversity of movie data.\nOverview: To provide a referable foundation for\nthe subsequent generation of the movie plot, we\ninitially generate an overview based on the movie\ntheme. This will also facilitate the consistency\nof the whole movie plot and the generation of\nsummary-type QA.\nStyle: Another significant element is the movie\nstyle which serves as a groundwork for the style in-\nversion process. Appropriate, theme-specific style\ndescriptions can assist in generating scene images\nrelated to the movie theme. Through textual inver-\nsion, this process facilitates the training of style\nembeddings. This, in turn, contributes to the con-\nsistency of all scenes throughout the movie.\nCharacters: In movie-level video instruction data\ngeneration, ensuring consistency in character por-\ntrayal is crucial. Therefore, in the movie plot gener-\nation process, we require GPT-4 to play the director\nrole in selecting an appropriate character for movie\nplots and generating character descriptions. Subse-\nquently, we design to leverage the powerful prior\nknowledge of celebrity faces from pretrained stable\ndiffusion, aiming to maintain excellent character\ncoherence in the generated key frame portraits.\nStory Expanding to Key Frame Description: To\nguarantee the quantity and quality of key frames,\nwe propose to utilize the aforementioned story ex-\npansion strategy, dividing the movie plot into three\nlevels: Epoch Chapters, Narrative Threads, and\nFrame Description. Each chapter within Epoch\nChapters represents a significant period or phase\nin story development. Narrative Threads further\nrefine each major chapter into more specific nar-\nrative strands and sub-stories, showcasing the co-\nherence of character development and main events.\nFrame Description then elaborates on these narra-\ntive strands, depicting the critical visual frames.\nThen we iteratively expand the story from the pre-\nceding level to generate three-level movie plot texts,\nultimately yielding key frame descriptions, as illus-\ntrated in Fig. 3.\n3.2\nStyle Immobilization Process\nThe next step is designed to convert the style de-\nscriptions from the generated movie plots into in-\nformation that can guide a T2I model to generate\nscenes with a consistent style. To achieve this goal,\nwe utilize the previously introduced technique, tex-\ntual inversion. The specific steps are as follows:\n1) Initially, we utilize the stable diffusion to gen-\nerate arbitrary style scenes based on the specific\nstyle descriptions. 2) Subsequently, we employ tex-\ntual inversion to convert these scene images into\nembeddings through an optimization step. These\nembeddings can guide the diffusion model in gen-\nerating images with specific styles. We refer to\nthis entire process as the style immobilization pro-\ncess, which essentially fixes a certain movie style\ngenerated by GPT-4 into the latent space as an em-\nbedding, corresponding to a specific token. Thus,\nin the generation of continuous key frame images,\nusing the specific token solely triggers the diffu-\nsion model to generate images in a particular style,\nensuring the consistency of scenes across frames.\n3.3\nVideo Instruction Data Generation\nAfter completing the previous two steps, we\nnow have obtained immobilized style embeddings,\ncelebrity-wise characters, and key frame descrip-\ntions. Based on these, we utilize the style embed-\ndings to guide stable diffusion in generating key\nframes according to the key frame descriptions.\nFurthermore, we progressively generate various in-\nstruction QA pairs based on the movie plot. Here\nare the details.\nConsistent Key Frame Generation\nIt is ob-\nserved in (Feichtenhofer et al., 2019; Li et al.,\n2023c; Zhang et al., 2023; Maaz et al., 2023) that\nkey frames at a rather low frame rate have already\nbeen able to meet the models\u2019 requirements for\nunderstanding video content, thus our method gen-\nerates key frames based on corresponding descrip-\ntions. To generate key frames that are consistent\nin both characters and scenes, we initially replace\ncharacter names in the frame description with corre-\nsponding celebrities (chosen by GPT-4). Following\nthat, as previously mentioned, a style-immobilized\nembedding linked to a special token is utilized.\nThis style embedding can serve as a condition\nto guide the stable diffusion model in generating\nscenes in a fixed style. This process is triggered by\na special token, which, in this paper, denotes the\nname of a specific style, such as \"Gothic\" Hence,\nby incorporating a sentence like \"generate an im-\nage in Gothic style:\" at the beginning of our frame\ndescription, combined with celebrity-wise charac-\nter names, we can generate consistent key frames\nwhile maintaining the visual diversity.\nQA Pairs Generation\nBeyond the textual and\nvisual information inherent to the story itself, en-\nriched QA pairs can well instruct the learning of the\nmodel by multimodal information, thereby achiev-\ning a superior understanding of movie-level nar-\nratives. We utilize GPT-4 to progressively gen-\nDataset\nVideo QA\nQA per Image\nVideo Type\nQA per Video\nLLaMA-VID\n(MovieNet base)\n9k\n0.0054\n11\n21\nOurs\n13k\n0.0947\n15\n125\nTable 1: Comparison between our proposed movie-level\nvideo instruction dataset and the dataset in LLaMA-VID\nin terms of data.\nFigure 4: Movie type distribution of movie-level video\ninstruction tuning data.\nerate rich QA pairs based on all the contents of\nthe movie plot, encompassing aspects of overview,\nwhat, where, and why.\n4\nExperiment\nIn this section, we delineate the experiments con-\nducted to substantiate the effectiveness of our in-\nnovative pipeline for data generation tailored to\nmovie-level video instruction tuning. We will first\nintroduce our implementation details and dataset\nstatistics, followed by the evaluation metrics, and\nfinally demonstrate our main results and qualitative\nresults.\n4.1\nImplementation Details and Dataset\nStatistics\nImplementation Details.\nWe conduct our video-\nrelated experiments on LLaMA-VID. The training\nprocedure of the model is divided into three stages:\nmodality alignment, instruction tuning, and long\nvideo tuning. The training conducted during the\nfirst two stages endowed the model with the capa-\nbility to comprehend both images and short videos.\nInitially, we conducted fine-tuning of LLaMA-VID\nduring the third stage, using their raw long video\ndataset. We use this trained LLaMA-VID as our\nbase model. Subsequently, we fine-tune the model\non our dataset for comparison. Details can be re-\nferred to in our appendix.\nMethod\nConsistency \u2191\nAlignment \u2191\nBRISQUE \u2193\nCustom diffusion\n0.7318\n0.3278\n22.7554\nCones-V2\n0.7781\n0.2671\n23.4823\nOurs\n0.8080\n0.3325\n22.5648\nTable 2: Quantitative comparison with other existing\nmethods on quality of generated key frames.\nMethod\nMSVD-QA\nMSRVTT-QA\nAcc \u2191\nScore \u2191\nAcc \u2191\nScore \u2191\nLLaMA-VID\n0.493\n3.169\n0.435\n2.865\n+Ours\n0.567 (+15%)\n3.460 (+9%)\n0.513 (+18%)\n3.141 (+10%)\nTable 3: Quantitative comparison with baseline model\non two zero-shot video QA datasets.\nDataset Statistics.\nWe show the statistics of our\ngenerated dataset in Table. 1 and Fig. 4. Our in-\nstruction tuning data encompasses rich QA pairs\nthat extract in-depth information about video con-\ntent. We support 15 different genres of movie-level\nvideos, significantly enhancing generalization ca-\npability. This is primarily attributed to the robust\nflexibility of our data construction methodology.\nNotably, our approach can generate videos of any\ntheme and length, and we plan to continue expand-\ning the richness of our data in the future.\n4.2\nEvaluation Metrics\nWe primarily evaluate our approach in two main\naspects: the quality of generated video key frames\nand video understanding.\nQuality of Video Key Frames\nWe consider these\ntwo metrics to evaluate the generated results: 1)\nkey frame consistency, which is calculated by\naverage cosine similarity between adjacent embed-\ndings of key frames in CLIP image space. 2) text-\nimage alignment, which is calculated by the av-\nerage cosine similarity between each key frame\ndescription and key frame itself. Besides, we use\nthe non-reference image quality assessment metric\nBRISQUE score (Mittal et al., 2012) to evaluate\nthe image quality.\nVideo Undersatnding\nFor video understanding,\nwe conduct evaluations separately for short and\nlong videos. Given that there is no benchmark\nspecifically tailored for movie-level video under-\nstanding in existing works, we draw inspiration\nfrom the design of metrics for short video under-\nstanding and meticulously devise our long video\nunderstanding benchmark.\nFor short video understanding, we evaluate mod-\nels on two benchmarks: a) Video-based Genera-\ntive Performance Benchmarking, which includes\nfive aspects to estimate model performance. b)\nMetric\nLLaMA-VID\n+Ours\nCorrectness \u2191\n1.94\n2.154 (+11%)\nDetail \u2191\n2.431\n2.549 (+5%)\nContext \u2191\n2.701\n2.880 (+7%)\nTemporal \u2191\n1.585\n1.832 (+16%)\nConsistency \u2191\n1.699\n1.976 (+16%)\nTable 4: Quantitative comparison with baseline model\non the video-based generative performance benchmark\n(Maaz et al., 2023).\nZero-shot Question-Answer Evaluation, which is\na comprehensive quantitative evaluation using sev-\neral commonly used open-ended QA datasets, in-\ncluding MSVD-QA (Chen and Dolan, 2011) and\nMSRVTT-QA (Xu et al., 2016).\nFor long video understanding, we present a\nbenchmark tailored to this task. Our test dataset\nis derived from approximately 100 movies within\nthe MovieNet database, and associated question-\nanswer pairs are generated from both GPT-4 and\nhuman efforts. Inspired by Maaz et al. (2023), we\nredesign the evaluation pipeline using GPT-4. The\nevaluation prompt can be found in our appendix\nB.2. This pipeline assesses various capabilities\nof the model by comparing different predicted an-\nswers from different models based on the ground\ntruth and scoring the predicted answers on a scale\nof 1-5 in the following three aspects:\n1) Overview Understanding:\nWe assess the\nmodel\u2019s comprehension of global video informa-\ntion through questions that involve summarizing\nvideo content.\n2) Plot Understanding: We assess the model\u2019s rea-\nsoning capabilities regarding the plot, its under-\nstanding of details, and its ability to recognize char-\nacters through a set of plot-related questions.\n3) Temporal Understanding: We assess the model\u2019s\nunderstanding of video content from a temporal\nperspective by having the model arrange the events\noccurring in the video in temporal order.\n4.3\nMain Results\nWe first compare the quality of key frames gener-\nated using our pipeline with similar existing meth-\nods. Then, we evaluate the performance on short\nvideo understanding and long video understanding.\nQuality of Key Frames\nWe compare our method\nbased on immobilized style embedding with two\nsimilar existing methods: 1) Custom diffusion (Ku-\nmari et al., 2023), a representative multi-concept\ncustomization method. 2) Cones-V2 (Liu et al.,\nMethod\nOverview\nPlot\nTemporal\nCompare ratio \u2191\nScore \u2191\nCompare ratio \u2191\nScore \u2191\nCompare ratio \u2191\nScore \u2191\nLLaMA-VID\n0.39\n2.862\n0.4\n2.657\n0.25\n1.953\n+Ours\n0.61 (+56%)\n2.881 (+1%)\n0.6 (+50%)\n2.717 (+2%)\n0.75 (+200%)\n2.572 (+32%)\nTable 5: Quantitative comparison with baseline model on our long video understanding benchmark.\nCones-v2         Custom Diffusion          Ours\nInput style\ngenerate an image in Dramatic \nstyle: In the blacksmith's shop, a \nman looks like Gerard Butler in \nbrown is standing in front of the \nanvil with a tired yet satisfied \nsmile, looking at Rowan who is \nleaving the shop with the mended \nshield.\ngenerate an image in Dramatic \nstyle: In the blacksmith's shop, a \nman looks like Gerard Butler in \nbrown is sitting on the stool near \nthe anvil, his hands covered with \nsoot. He looks exhausted but with \na sense of satisfaction.\ngenerate an image in Dramatic \nstyle: In the blacksmith's shop, a \nman looks like Gerard Butler in \nbrown is cleaning up, ready to end \nthe day's work, his tired face lit \nby the last sparks jumping from \nthe cooling forge.\ngenerate an image in Dramatic \nstyle: At the Town Square, a man \nlooks like Gerard Butler in brown \nis standing near a stone fountain, \nholding a newly mended shield, \nwith a resolved expression.\ngenerate an image in Dramatic \nstyle: At the Town Square, a man \nlooks like Gerard Butler in brown \ncontinues to stand holding the \nshield, looking around at the busy \nactivities, with a serious face.\ngenerate an image in Dramatic \nstyle: At the Town Square, a man \nlooks like Gerard Butler in brown \nwalking through the crowd, with \nthe shield on his shoulder, his \nexpression is firm.\n.......\nFigure 5: Comparison of the key frames generated by ours and other methods. Our method generates rather\nmore consistent and high-quality frames.\n2023b), a multi-subject customizable image syn-\nthesis method utilizing layouts. Table. 2 that our\nmethod outperforms the existing methods across\nall three metrics, indicating our method generates\nconsistent and high-quality key frames.\nShort Video Understanding\nThe model trained\non our datasets gains significant performance im-\nprovement over baseline as listed in Table. 3 and\nTable. 4. The results indicate that although our data\nis focused on long video instruction tuning, it can\nstill enhance the model\u2019s understanding of short\nvideos, demonstrating the robustness of our data.\nLong Video Understanding\nResults on Long\nVideo Understanding are demonstrated in Table. 5.\nOur method outperforms the baseline by a signifi-\ncant margin in aspects of overview, plot, and tempo-\nral understanding. This showcases the richness of\nour dataset, proving their significant effectiveness\nin enhancing the model\u2019s understanding in long\nvideos across various aspects.\n4.4\nQualitative Results\nFig. 5 visualizes qualitative results on the quality of\ngenerated key frames. Our method generates rather\nmore consistent and high-quality frames. Fig. 6\nvisualizes video understanding results of the base-\nline model and ours. Our method demonstrates a\nmore reasonable and detailed answer in a movie\nunderstanding.\n5\nRelated Work\nVision Language Models\nWith the achievements\nof large language models (LLMs) such as GPT-\n4 (OpenAI, 2023) along with their open-source\nalternatives like LLaMA (Touvron et al., 2023),\nresearchers focus on leveraging the advanced lan-\nguage abilities of LLMs and developing the vision\nlanguage models (VLMs) that integrate vision mod-\nels with LLMs for cross-modality understanding.\nPrevious representative VLMs, miniGPT-4 (Zhu\net al., 2023) and LLaVA (Liu et al., 2023a), have\nshown great capabilities in visual chat by construct-\ning high-quality image-instruction pairs to align\nthe image and text dimensions. Further, VLMs\nare developed for video understanding (Jin et al.,\n2024; Li et al., 2023b; Lin et al., 2023; Bai et al.,\n2024; Ma et al., 2023). Video-LLaMA (Zhang\net al., 2023) utilizes BLIP-2 (Li et al., 2023a) to\nCan you provide a brief overview of the movie in just few sentences?\nReference:\u2019 Splice\u2018 is a science\nfiction horror film that delves into\nthe ethical and moral dilemmas of\ngenetic engineering. The story\nfollows two rebellious scientists,\nClive and Elsa, who, in their quest\nto push the boundaries of medical\nscience, create a hybrid creature\nby splicing together human and\nanimal DNA. The creature, named\nDren, rapidly develops from a\ndeformed female infant into a\nbeautiful but dangerous winged\nhuman-chimera. The film explores\nthemes of parental responsibility,\nthe nature of humanity, and the\nconsequences of playing God.\nOurs:\nIn\n\u2018Splice\u2019,\nscientists\nClive\nNicoli\nand\nElsa\nKast\nattempt\nto\ncreate a hybrid creature by splicing\nhuman\nDNA\ninto\npig\nembryos.\nHowever, the experiment goes awry\nwhen the pig embryos undergo rapid\ngrowth and evolution. Elsa's brother\nGavin, who is part of the experiment,\nbecomes obsessed with rescuing his\nbrother Clive from the experiment.\nAs the hybrid creature Dren evolves,\nit\nbegins\nto\ngrow\nrapidly\nand\ndevelops\na\ncomplex,\nunpredictable\npersonality.\nDren's\nbehavior\nescalates, leading to a confrontation\nwhere\nshe\nkills\nElsa,\nand\nshe\nis\nforced to euthanize her. The film\nconcludes\nwith\nClive\nand\nGavin\nreflecting\non\nthe\nmoral\ndilemmas\nthey have created and the ethical\nimplications\nof\ntheir\nscientific\nachievement.\nLLaMA-VID:\nIn\n\\\"The\nFantasy\nWorld,\\\"\na\nscientist\nnamed\nClive\nNicoli and his assistant Elsa Kast\ndevelop\nan\nadvanced\nartificial\nintelligence named Dren, who they\nbelieve can cure diseases. However,\nDren's violent behavior leads them to\nlock her away, and the experiment\ngoes awry when she escapes and kills.\nAfter\na\nseries\nof\nevents,\nDren's\nprotector, Gavin, discovers the truth,\nand she is brought back to the lab,\nwhere she is synthesized into a more\nstable form. The scientists attempt\nto destroy Dren, but she escapes\nagain, leading to a series of events\nthat threaten to expose the project.\nThe\nfilm\nends\nwith\nDren\nseeking\nrevenge against the scientists who\ncreated her.\nFigure 6: Video understanding results of baseline model and ours. Our model demonstrates a more reasonable\nand detailed answer in the video understanding.\nextract video embedding for each frame and fuse\nvideo embedding of all frames by video Q-Former.\nVideo-ChatGPT (Maaz et al., 2023) designs to ex-\ntract video embedding by averaging frame-level\nfeatures across temporal and spatial dimensions\nrespectively. However, previous works require gen-\nerating a large number of tokens for each frame for\nrepresentation. For hour-long videos, these VLMs\nare unable to process extensive video sequences.\nTo tackle this problem, LLaMA-VID (Li et al.,\n2023c) is proposed for long video understanding\nby encoding each frame with only two tokens.\nVideo Instruction Tuning Datasets\nPreparing\nvideo-text pairs to construct video instruction tun-\ning datasets is pivotal in facilitating the training\nof VLMs for video understanding. Most exist-\ning methods (Maaz et al., 2023; Tang et al., 2023;\nZhong et al., 2022; Gao et al., 2021; Castro et al.,\n2022; Yang et al., 2023) construct a video instruc-\ntion tuning dataset based on benchmark datasets\n(Han et al., 2023), utilizing rich annotation informa-\ntion. The Video-ChatGPT, for example, constructs\nVideoInstruct100K sourced from the ActivityNet\ndataset (Caba Heilbron et al., 2015). However,\nmost existing methods lack long video data in the\nconstructed instruction tuning datasets, resulting in\npoor long-video understanding capabilities of the\ntrained models. The recently proposed LLaMA-\nVID addresses this by extracting data from the\nMovieNet dataset (Huang et al., 2020) to build\nan instruction dataset containing movie-level long\nvideos. However, such datasets are limited by the\ntypes of annotations, leading to insufficient diver-\nsity in the data. Leveraging our novel pipeline,\nwe propose a diverse movie-level video instruction\ntuning dataset for enhancing long video understand-\ning.\n6\nConclusion\nIn this paper, we propose an effective method for\ngenerating key frames of movie-level videos. Our\ngeneration process consists of three main steps:\nmovie plot generation, style immobilization pro-\ncess, and video instruction data generation. The\nproposed data generation workflow significantly\neases the challenge of producing movie-level video\ndata for models, enhancing both the control and\ndiversity of the generated content. Experiments\nconducted on existing datasets, as well as on our\nnewly generated dataset, validate the effectiveness\nof our approach. Thanks to the richness of our\ndataset, models demonstrate capabilities previously\nunattainable with training on earlier datasets. We\nbelieve our data generation pipeline will make a\nsignificant contribution to the advancement of mul-\ntimodal models\u2019 understanding of long videos. Fur-\nthermore, it paves the way for the adoption of simi-\nlar data generation processes across other domains.\n7\nLimitations\nThe forgetting issue associated with large language\nmodels may lead to inconsistencies in the frame\ndescriptions generated during the first stage of our\npipeline, subsequently causing discontinuities in\nthe video scenes. To address this problem, we plan\nto refine the text generation component within our\npipeline.\n8\nEthics Statement\nThis Ethics Statement reflects on the broader im-\npacts and ethical considerations associated with the\ndevelopment of our MovieLLM framework. Our\nframework, designed for synthesizing synthetic,\nhigh-quality data for long videos, engages deeply\nwith ethical concerns, particularly in the realms\nof privacy, security, accessibility, the impact on\nemployment, sustainability, and the potential for\nmisuse.\nPrivacy and Security: MovieLLM\u2019s methodol-\nogy emphasizes the protection of privacy and secu-\nrity by generating synthetic video content, thereby\neliminating the risk of exploiting real individuals\u2019\ndata. This strategy significantly minimizes the po-\ntential for unauthorized data access and breaches,\nwith a commitment to ongoing improvement of\nthese protective measures.\nAccessibility and Inclusivity: Our framework\naims to enhance the accessibility of video un-\nderstanding technologies, making it easier for re-\nsearchers across different demographics, includ-\ning those with disabilities, to engage in AI de-\nvelopment.\nWe are dedicated to ensuring that\nMovieLLM supports a wide array of users by en-\nhancing its inclusivity and accessibility.\nImpact on Employment: While the automa-\ntion inherent in MovieLLM may prompt concerns\nregarding job displacement, our intention is to aug-\nment human capacity rather than replace it. By\nautomating the generation of video data, we hope\nto shift human focus to more creative and intel-\nlectually stimulating tasks, potentially enriching\nprofessional satisfaction and productivity.\nSustainability: Acknowledging the environmen-\ntal costs associated with training complex AI mod-\nels, MovieLLM is designed to be resource-efficient,\nthereby reducing the need for extensive computa-\ntional power and minimizing its ecological foot-\nprint as part of our commitment to environmental\nresponsibility.\nPotential Misuse: We recognize the potential\nfor MovieLLM\u2019s synthetic data to be misused, for\nexample, in creating deceptive content. To address\nthis, we advocate for responsible use, which in-\ncludes:\n\u2022 Monitoring and Detection: Developing mech-\nanisms to identify and prevent misuse of syn-\nthetic data.\n\u2022 Ethical Guidelines and Governance: Promot-\ning ethical standards in AI research and appli-\ncation to ensure awareness and compliance.\n\u2022 Collaboration with Platforms: Engaging with\ndigital platforms to align the use of synthetic\ndata with ethical and legal standards.\n\u2022 Transparency and Accountability: Maintain-\ning clear communication about the capabili-\nties and intended use of MovieLLM to ensure\ninformed and ethical application.\nThrough this statement, we emphasize our com-\nmitment to ethical research and innovation, high-\nlighting the importance of addressing these critical\nconsiderations in advancing AI technology respon-\nsibly.\nReferences\nZiyi Bai, Ruiping Wang, and Xilin Chen. 2024.\nGlance and focus: Memory prompting for multi-\nevent video question answering.\narXiv preprint\narXiv:2401.01529.\nFabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles. 2015. Activitynet:\nA large-scale video benchmark for human activity\nunderstanding. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 961\u2013970.\nSantiago Castro, Ruoyao Wang, Pingxuan Huang, Ian\nStewart, Oana Ignat, Nan Liu, Jonathan Stroud, and\nRada Mihalcea. 2022. Fiber: Fill-in-the-blanks as a\nchallenging video understanding evaluation frame-\nwork. In Association for Computational Linguistics,\npages 2925\u20132940.\nDavid Chen and William B Dolan. 2011. Collecting\nhighly parallel data for paraphrase evaluation. In As-\nsociation for Computational Linguistics, pages 190\u2013\n200.\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik,\nand Kaiming He. 2019. Slowfast networks for video\nrecognition.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-\nOr. 2022. An image is worth one word: Personalizing\ntext-to-image generation using textual inversion.\nDifei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen.\n2021. Env-qa: A video question answering bench-\nmark for comprehensive understanding of dynamic\nenvironments. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages\n1675\u20131685.\nYucheng Han, Chi Zhang, Xin Chen, Xu Yang,\nZhibin Wang, Gang Yu, Bin Fu, and Hanwang\nZhang. 2023. Chartllama: A multimodal llm for\nchart understanding and generation. arXiv preprint\narXiv:2311.16483.\nQingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and\nDahua Lin. 2020. Movienet: A holistic dataset for\nmovie understanding. In European Conference on\nComputer Vision, pages 709\u2013727. Springer.\nYang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao\nJiang, Quzhe Huang, Chengru Song, Yuliang Liu,\nDi Zhang, Yang Song, et al. 2024.\nVideo-lavit:\nUnified video-language pre-training with decou-\npled visual-motional tokenization. arXiv preprint\narXiv:2402.03161.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. 2023. Multi-concept\ncustomization of text-to-image diffusion. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1931\u20131941.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023a. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\nhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. 2023b. Videochat: Chat-centric video un-\nderstanding. arXiv preprint arXiv:2305.06355.\nYanwei Li, Chengyao Wang, and Jiaya Jia. 2023c.\nLlama-vid: An image is worth 2 tokens in large lan-\nguage models. arXiv preprint arXiv:2311.17043.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\nLi Yuan. 2023. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv\npreprint arXiv:2311.10122.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023a. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nZhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng,\nKai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren\nZhou, and Yang Cao. 2023b. Cones 2: Customiz-\nable image synthesis with multiple subjects. arXiv\npreprint arXiv:2305.19327.\nFan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi\nFeng, and Yi Yang. 2023.\nVista-llama: Reliable\nvideo narrator via equal distance to visual tokens.\narXiv preprint arXiv:2312.08870.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan,\nand Fahad Shahbaz Khan. 2023.\nVideo-chatgpt:\nTowards detailed video understanding via large\nvision and language models.\narXiv preprint\narXiv:2306.05424.\nAnish Mittal, Anush Krishna Moorthy, and Alan Conrad\nBovik. 2012. No-reference image quality assessment\nin the spatial domain. IEEE Transactions on image\nprocessing, 21(12):4695\u20134708.\nOpenAI. 2023. Gpt-4 technical report.\nYunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan\nLiang, Teng Wang, Daoan Zhang, Jie An, Jingyang\nLin, Rongyi Zhu, et al. 2023. Video understanding\nwith large language models: A survey. arXiv preprint\narXiv:2312.17432.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 5288\u20135296.\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Ze-\nbiao Huang, Bin Fu, and Gang Yu. 2023. Appa-\ngent: Multimodal agents as smartphone users. arXiv\npreprint arXiv:2312.13771.\nHang Zhang, Xin Li, and Lidong Bing. 2023. Video-\nllama: An instruction-tuned audio-visual language\nmodel for video understanding.\narXiv preprint\narXiv:2306.02858.\nYaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Wei-\nhong Deng, and Tat-Seng Chua. 2022. Video ques-\ntion answering: Datasets, algorithms and challenges.\nIn Empirical Methods in Natural Language Process-\ning, pages 6439\u20136455.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nA\nExperiment Details\nA.1\nTraining Details\nDuring the model training phase, we employed the\noriginal LLaMA-VID (Li et al., 2023c) configu-\nration as the foundation for our training process.\nWe utilized 4 NVIDIA A100 GPUs. To conserve\nGPU memory, we employed deepspeed with zero3\nduring model training, disabling tf32 and opting for\nfp16. The remaining parameters, including epochs\nand learning rates, were set according to the origi-\nnal LLaMA-VID configuration. For specific param-\neter details, please refer to the the original paper\n(Li et al., 2023c).\nA.2\nImplementation Details of Experiments\non Quality of Key Frames\nFor textual inversion (Gal et al., 2022), we follow\nthe implementation*, training is conducted on 2\nNVIDIA A100 GPUs. For custom diffusion (Ku-\nmari et al., 2023), we employed the same configura-\ntion on its version with Stable Diffusion v1.4, train-\ning is conducted on 2 NVIDIA A100 GPUs. For\nCones-V2 (Liu et al., 2023b), we also employed\nthe same configuration on its version with Stable\nDiffusion v2.1, training is conducted on 2 NVIDIA\nA100 GPUs.\nB\nPrompt Details\nB.1\nMovie Plot Generation Prompt\nWe have illustrated the movie plot generation\nprompt in Fig 11. Rather than limiting plot gen-\neration to conventional data sources such as the\nweb or existing datasets, we harness the power of\nGPT-4 to produce synthesized data. By providing\nspecific elements such as themes, overview, and\nstyles, we guide GPT-4 to produce movie-level key\nframe descriptions tailored to the latter generation\nprocess.\nB.2\nLong Video Evaluation prompt\nWe have illustrated Long Video Evaluation prompt\nin Fig 10. This pipeline assesses various capabili-\nties of the model by comparing different predicted\nanswers from different models based on the ground\ntruth and scoring the predicted answers on a scale\nof 1-5 in the following three aspects:\n1) Overview Understanding:\nWe assess the\n*https://github.com/oss-roettger/\nXL-Textual-Inversion\nmodel\u2019s comprehension of global video informa-\ntion through questions that involve summarizing\nvideo content.\n2) Plot Understanding: We assess the model\u2019s rea-\nsoning capabilities regarding the plot, its under-\nstanding of details, and its ability to recognize char-\nacters through a set of plot-related questions.\n3) Temporal Understanding: We assess the model\u2019s\nunderstanding of video content from a temporal\nperspective by having the model arrange the events\noccurring in the video in temporal order.\nC\nMore Results of our Generated Data\nWe illustrate more results of our generated data in\nFig 12.\nD\nMore Comparison Results of Long\nVideo Understanding\nWe demonstrate more comparison results of long\nvideo understanding in Fig 7, Fig 8, Fig 9. Words\nin blue color shows the alignment with ground truth\nwhile words in red color mean wrong prediction\npart. Model trained on our dataset demonstrates a\nmore reasonable and detailed answer in the video\nunderstanding.\nCan you provide a brief overview of the movie in just few sentences?\nReference: ' T h e\nP u r s u i t\no f\nHappyness' is an inspiring drama\nbased on the true story of Chris\nGardner, a struggling salesman who\nbecomes homeless with his young\nson. Faced with financial instability\nand life's adversities, Gardner\nperseveres through immense\nchallenges to secure a better\nfuture for himself and his son. The\nfilm explores themes of resilience,\ndetermination, and the American\ndream, as Gardner embarks on a\ngrueling journey to change their\nlives, ultimately landing a coveted\nposition as a stockbroker after\ncompleting a highly competitive\ninternship.\nOurs: \u2018The Pursuit of Happyness\u2019\nfollows the life of Chris Gardner, a\nstruggling\nstockbroker\nin\nSan\nFrancisco, who is fired from his job\ndue\nto\nhis\nfamily\nproblems\nand\nfinancial\nstruggles.\nDespite\nhis\nsetback, Chris perseveres by taking\na\njob\nas\na\nbroker,\nwhere\nhe\ndiscovers a passion for the financial\nindustry and begins to build his own\nwealth.\nHis\nlife\nbecomes\nmore\ncomplicated when he becomes the\ntarget of a con artist named Jay\nTwistle, who convinces him to invest\nin a scanner, which turns out to be a\ntime machine. The story culminates\nin\na\nseries\nof\nevents\nthat\ntest\nChris's sanity, and he is forced to\nconfront the reality of his situation,\nleading him to confront the pursuit\nof happiness and the importance of\nliving a simple life\nLLaMA-VID:In \\\"Riding the Bus,\\\"\nChris\nGardner,\na\nman\nwith\nbig\ndreams, is forced to take a lower-\npaid job to support his family after\nhis\nfather's\narrest.\nHe\nis\ndetermined to make it big in the\nfinancial world and eventually gets\na job at a brokerage firm. Along\nthe\nway,\nhe\nfaces\npersonal\nchallenges like a broken leg and a\ncar accident that forces him to\nwalk\nto\nwork,\nbut\nhe\nremains\ndetermined and eventually succeeds\nin his goal. The film ends with Chris\nand his son, Christopher, embracing\nthe happier aspects of life, as they\nride the bus together.\nWhat leads Chris Gardner and his son to become homeless?\nReference:Chris Gardner and his\nson become homeless due to a\ncombination of financial difficulties\nstemming from unsold inventory of\nbone density scanners, unpaid taxes,\nand increasing living expenses.\nGardner's struggle is compounded\nwhen his wife leaves him, and he is\nunable to afford their apartment\nrent, leading to their eviction. This\nseries of events forces Chris and\nhis son to seek temporary shelter\nwherever possible, including in\nhomeless shelters, public restrooms,\nand subway stations.\nOurs:Chris\nGardner\nand\nhis\nson\nbecome homeless due to a series of\nevents. First, they have to leave the\nmotel where they were living. Then\nChris gets fired from his job at the\nbrokerage firm due to the high cost\nof the scanner he bought. Next, his\nwife Linda leaves him, causing them\nto lose the motel's rent. Finally,\ntheir car breaks down, leaving them\nstranded\nin\nthe\nparking\nlot.\nAll\nthese events lead them to live in a\nhomemade\ncamp\nin\na\nwarehouse,\nwhere they are forced to scavenge\nfor food and struggle to survive in a\nharsh environment.\nLLaMA-VID:Chris Gardner and his\nson become homeless after being\nevicted from their motel.\nCan you outline the chronological sequence of events in the movie?\nReference:The\nfilm\nstarts\nwith\nChris\nGardner's\nstruggle\nto\nsell\nmedical equipment while grappling\nwith financial instability. His wife\nleaves\ndue\nto\nthe\nmounting\npressures, making Gardner a single\nparent. They are evicted from their\napartment,\nleading\nto\na\nlife\nof\nhomelessness. Gardner learns of a\ncompetitive\ninternship\nat\na\nbrokerage\nfirm, seeing it as an\nopportunity\nfor\na\nbetter\nlife.\nDespite\nthe\nchallenges\nof\nhomelessness, Gardner commits to\nthe\nunpaid\ninternship,\nbalancing\nfatherhood,\nfinancial\ndifficulties,\nand the rigorous demands of the\njob. His tenacity is tested through\nvarious\ntrials,\nincluding\nspending\nnights\nin\nhomeless\nshelters\nand\npublic\nrestrooms.\nThe\nmovie\nculminates in the emotional moment\nwhen Gardner is informed that he\nhas\nsecured\na\nposition\nas\na\nstockbroker,\na\nvictory\nachieved\nthrough\nhis\nresilience\nand\nhard\nwork.\nThe\nfilm\nconcludes\nwith\nGardner walking hand in hand with\nhis son, stepping into a future filled\nwith\nhope\nand\npossibilities,\nreflecting on their journey from\ndespair to happiness.\nOurs:The movie follows the life of\nChris Gardner, a young man who is\nstruggling to escape poverty and\ngain wealth through his work at a\nbrokerage firm. He first encounters\nLinda\nand\nher\nson,\nand\nlater\nbecomes an intern at a brokerage\nfirm where he is fired. Chris is\narrested for running away from a\nhitman, and he is forced to take his\nson, Tim, to a motel. After a series\nof failed attempts, including a failed\ninterview with a football team, Chris\ngets\nan\ninternship\nwith\nan\ninvestment firm where he is fired\nfor\nbeing\ntoo\nunreliable.\nIn\na\ndesperate\nmove,\nChris\nsteals\na\nscanner from a pawn shop and sells\nit to a medical equipment company.\nHe\nthen\nbecomes\nan\ninsanely\nsuccessful\nbroker,\nselling\nthe\nscanner for a fortune and buying a\none-way ticket to New York. Chris's\nwealthy past, his struggles with his\nson's upbringing, and his own inner\ndemons are explored through his\ninteractions with his son and his\nbrokerage colleagues.\nLLaMA-VID:\n1. Chris Gardner starts a new life in San\nFrancisco\nafter\ndiscovering\nhis\nwife\nLinda is pregnant.\n2. He takes up a job at a daycare and\nstruggles to balance his responsibilities\nwith his son.\n3. Chris's father, a homeless man, comes\nto San Francisco, and Chris's life is\nfurther disrupted when he has to take\ncare of his son after Linda's sudden\ndeath.\n4. Chris is arrested for failing to pay\nparking tickets and is forced to spend a\nnight in jail.\n5. Chris's father is homeless, and he is\narrested for loitering.\n6. Chris is hired by a brokerage firm and\nbegins\nan\ninternship,\nwhere\nhe\nis\nridiculed by his peers.\n7. Chris is hired by a rival firm, where he\nis ridiculed but determined to make a\nname for himself.\n8. Chris's father is arrested again, and\nChris is forced to take care of his son\nfor the second time.\n9. Chris's father is arrested again, and\nChris is forced to take care of his son\nagain.\n10. Chris's father is arrested again, and\nChris is forced to take care of his son\nagain.\n11. Chris is arrested for the third time,\nand this time he is forced to take care\nof his son again.\n12. Chris is hired by a rival brokerage\nfirm, where he is ridiculed again but\ndetermined to make a name for himself.\n13. Chris's father is arrested again, and\nChris is forced to take care of his son\nagain.\nFigure 7: Video understanding results of baseline model and ours. Our model demonstrates a more reasonable\nand detailed answer in the video understanding.\nCan you provide a brief overview of the movie in just few sentences?\nReference:'Holding the Man' is a\npoignant drama based on the true\nstory of Tim Conigrave and John\nCaleo's relationship that spans over\nfifteen years. From their high\nschool beginnings in the 1970s to\nthe challenges they face as adults,\nincluding discrimination and the\nAIDS crisis, the film explores the\ndepths of love, loss, and the impact\nof societal pressures on their lives.\nIt is a heartfelt tribute to their\nenduring love against the odds,\nhighlighting the struggles and\ntriumphs of the LGBTQ+ community\nduring a tumultuous period\nOurs:It is a poignant tale of two\nyoung men navigating the intricate\nwaters of love and life. The story\ncenters around Tim Conigrave, a\npopular student with a mysterious\npast, and John Caleo, a quiet and\nreserved student who is secretly in\nlove with Tim. Their lives become\nintertwined\nin\na\ncomplex\nand\nunpredictable relationship as they\nnavigate\nthe\nchallenges\nof\nhigh\nschool life, family issues, and their\nown internal struggles.\nLLaMA-VID:\\\"Romeo and Juliet\\\"\nis a 19963 adaptation of William\nShakespeare's tragedy, set against\nthe\nbackdrop\nof\nthe\nVerona\ncarnival. The film follows the lives\nof two young lovers from feuding\nfamilies,\nRomeo\nMontague\nand\nJuliet Capulet, who are doomed by\nfate and their own actions to meet\na tragic end. Their story is told\nthrough\nthe\nlens\nof\n1980s\nAustralia, with John Caleo and his\nlover Tim Conigrave portraying the\nlovers. The film ends with a sense\nof hope, as the play's final words\nare\nread,\nsuggesting\na\nfuture\nbeyond the tragedy.\nHow does the AIDS crisis affect Tim and John's life together?\nReference:The AIDS crisis deeply\naffects Tim and John's life\ntogether, as they both navigate the\nfear, stigma, and health challenges\nassociated with the disease. The\ncrisis puts a significant strain on\ntheir relationship, with both\npartners eventually diagnosed with\nHIV. It tests their commitment\na n d\nl o v e\nf o r\ne a c h\no t h e r ,\nhighlighting the broader impact of\nthe epidemic on the LGBTQ+\ncommunity during the 1980s and\n1990s.\nOurs:The AIDS crisis leads to a\nsignificant shift in Tim and John's\nrelationship, as they both realize\nthat they need to focus on their\nsurvival\nand\nJohn's\ncondition\nis\ndeteriorating, prompting a moment\nof\nunderstanding\nand\nempathy\nbetween them.\nLLaMA-VID:The AIDS crisis has a\nmajor impact on Tim and John's life\ntogether,\nwith\nJohn\ndeveloping\nAIDS\nand\nbeing\ntreated\nwith\nchemotherapy.\nCan you outline the chronological sequence of events in the movie?\nReference:The movie begins in the\nlate 1970s, with Tim Conigrave and\nJohn Caleo meeting and falling in\nlove while attending high school in\nMelbourne, Australia. Despite\nfacing opposition from their\nfamilies and society, they embark\non a relationship. As they graduate\nand move into adulthood, their love\nis tested by separations, career\npursuits, and the burgeoning AIDS\ncrisis. Tim pursues his passion for\nacting, while John becomes a\nchiropractor. Their commitment is\nfurther challenged when both are\ndiagnosed with HIV in the midst of\nthe AIDS epidemic. Throughout\ntheir struggles, they fight to\nmaintain their love and dignity,\ns e e k ing\na c c e p t a n c e\na n d\nunderstanding. The film chronicles\ntheir journey through the highs and\nl o w s\no f\nt h e i r\nr e l a t i o n s h i p ,\nculminating in John's death in 1992\nand Tim's death shortly after,\nsymbolizing the tragic impact of\nAIDS on their love story and the\nbroader LGBTQ+ community. The\nnarrative is a moving tribute to\ntheir relationship, illustrating the\npower of love in the face of\nadversity.\nOurs:The\nmovie\nfollows\nthe\nchronological sequence of events in\nthe lives of Tim Conigrave and John\nCaleo.\nIt\nbegins\nwith\nTim\nand\nJohn's high school years, where\nTim and John become close friends.\nThey\nconfess\ntheir\nfeelings\nfor\neach other, but Tim's parents are\nunsupportive and try to separate\nthem.\nTim\nand\nJohn\nstart\na\nromantic relationship, which Tim's\nparents attempt to disrupt. Tim\nand John's relationship is tested\nwhen John is infected with HIV,\nleading to a series of events that\nculminates in John's death. Tim and\nLois,\nJohn's\nmother,\nare\nleft\nbehind\nby\nthe\ngrief-stricken\nparents.\nTim\nand\nPeter,\nJohn's\nboyfriend, confront John's parents\nand make their own vows to each\nother, promising to be together.\nTim and Peter visit John's grave,\nreminiscing about the times they\nspent\ntogether.\nTim\nand\nPeter\nspend time together in a local bar,\nreminiscing their shared memories.\nTim and Peter spend time at the\nbeach, hoping to be together again.\nTim\nand\nLois\nhave\na\nheartfelt\nfarewell, promising to reunite. The\nmovie ends with Tim and Peter at a\nlocal bar, promising to be together\nagain.\nLLaMA-VID:\n1. 1981: John and Tim meet in high\nschool. They become engaged to be\nmarried.\n2. 1985: John and Tim are engaged\nto be married.\n3.\n19986:\nJohn\nand\nTim\nare\nmarried. They have a child named\nMichael.\n4. 19993: John is diagnosed with\nAIDS, leading to a crisis in their\nmarriage.\n5.\n19996:\nJohn's\nhealth\ndeteriorates, and he is placed in a\nhospice.\n6. 19999: John dies, and his funeral\nis held. His funeral is attended by\nhis friends and family.\n7. 20000: Tim and Peter are in a\nhotel,\nwriting\nout\ntheir\nfeelings\nabout John's death.\n8. 20003: The movie concludes with\nTim and Peter on a romantic beach,\nwriting\nout\ntheir\nfeelings\nabout\nJohn's death and their love for\neach other.\nFigure 8: Video understanding results of baseline model and ours. Our model demonstrates a more reasonable\nand detailed answer in the video understanding.\nCan you provide a brief overview of the movie in just few sentences?\nReference:\u2019Splice\u2018 is a science\nfiction horror film that delves into\nthe ethical and moral dilemmas of\ngenetic engineering. The story\nfollows two rebellious scientists,\nClive and Elsa, who, in their quest\nto push the boundaries of medical\nscience, create a hybrid creature\nby splicing together human and\nanimal DNA. The creature, named\nDren, rapidly develops from a\ndeformed female infant into a\nbeautiful but dangerous winged\nhuman-chimera. The film explores\nthemes of parental responsibility,\nthe nature of humanity, and the\nconsequences of playing God.\nOurs:In \u2018 S p l i c e \u2019, scientists Clive\nNicoli\nand\nElsa\nKast\nattempt\nto\ncreate a hybrid creature by splicing\nhuman\nDNA\ninto\npig\nembryos.\nHowever, the experiment goes awry\nwhen the pig embryos undergo rapid\ngrowth and evolution. Elsa's brother\nGavin, who is part of the experiment,\nbecomes obsessed with rescuing his\nbrother Clive from the experiment.\nAs the hybrid creature Dren evolves,\nit\nbegins\nto\ngrow\nrapidly\nand\ndevelops\na\ncomplex,\nunpredictable\npersonality.\nDren's\nbehavior\nescalates, leading to a confrontation\nwhere\nshe\nkills\nElsa,\nand\nshe\nis\nforced to euthanize her. The film\nconcludes\nwith\nClive\nand\nGavin\nreflecting\non\nthe\nmoral\ndilemmas\nthey have created and the ethical\nimplications\nof\ntheir\nscientific\nachievement.\nLLaMA-VID:In\n\\\"The\nFantasy\nWorld,\\\"\na\nscientist\nnamed\nClive\nNicoli and his assistant Elsa Kast\ndevelop\nan\nadvanced\nartificial\nintelligence named Dren, who they\nbelieve can cure diseases. However,\nDren's violent behavior leads them to\nlock her away, and the experiment\ngoes awry when she escapes and kills.\nAfter\na\nseries\nof\nevents,\nDren's\nprotector, Gavin, discovers the truth,\nand she is brought back to the lab,\nwhere she is synthesized into a more\nstable form. The scientists attempt\nto destroy Dren, but she escapes\nagain, leading to a series of events\nthat threaten to expose the project.\nThe\nfilm\nends\nwith\nDren\nseeking\nrevenge against the scientists who\ncreated her.\nWhat ethical boundaries do Clive and Elsa cross in their scientific research?\nReference:Clive and Elsa cross\nsignificant ethical boundaries by\nsecretly using human DNA in their\nexperiments to create a new\norganism, defying legal and ethical\nnorms. Their creation of Dren, a\nsentient being with human and\nanimal traits, challenges the\nboundaries of scientific ethics and\nthe definition of humanity.\nOurs:Clive and Elsa cross ethical\nboundaries by creating a human-like\ncreature\ncalled\nDren,\nwho\nis\ncapable of experiencing emotions\nand becoming sexually active, which\nis\nbeyond\ntheir\ninitial\nscientific\ngoals.\nLLaMA-VID:Clive\nand\nElsa\ncross\nethical boundaries by creating a\nnew species of organic life that is\ncapable\nof\nchanging\nsex\nand\nis\npotentially\nmore\ndangerous\nthan\nthe first species.\nCan you outline the chronological sequence of events in the movie?\nReference:The movie begins with\nscientists Clive and Elsa achieving\nsuccess in splicing animal DNA to\ncreate new hybrid organisms for\nmedical research. Seeking further\nadvancements,\nthey\nsecretly\nincorporate human DNA into their\nnext experiment, resulting in the\nbirth of Dren, a creature with rapid\ngrowth and evolving intelligence. As\nDren matures, she exhibits a blend\nof animalistic traits and human-like\nemotions, leading Clive and Elsa to\ndevelop\ncomplex\nfeelings\nof\nattachment\nand\nresponsibility\ntowards\nher.\nTheir\nexperiment\ntakes\na\ndark\nturn\nwhen\nDren's\naggression and predatory instincts\nemerge,\nleading\nto\nviolent\nconfrontations. The climax of the\nfilm\nreveals\nDren's\nability\nto\nchange\nsex,\nafter\nwhich\nshe\nbecomes\nmore\naggressive\nand\nfatally\ninjures\nClive.\nElsa,\nin\na\ndesperate move to protect herself,\nkills\nDren.\nThe\nmovie\nconcludes\nwith Elsa agreeing to a lucrative\ndeal\nto\ncontinue\nher\ngenetic\nresearch, hinting at the ongoing\ncycle of ethical compromise and the\npotential for further horror.\nOurs:The\nfilm\nbegins\nwith\nscientists\nClive\nNicoli\nand\nElsa\nKastner\nconducting\na\ngenetic\nexperiment that creates a hybrid\ncreature called Dren. Dren becomes\nincreasingly\nviolent\nand\nunpredictable,\nleading\nto\nthe\ndeaths of her creators and other\nscientists. Dren is then taken to a\nfarm where she is locked away,\nwhere\nshe\nbegins\nto\ndevelop\na\nromantic relationship with a man\nnamed\nGavin.\nDren\nescapes\nand\nbecomes\npregnant\nwith\nGavin's\nchild. Dren is discovered by the\npolice, who are unable to control\nher, leading to her death by a gas\nattack. Gavin and his son Clive are\nkilled by Dren's soldiers, who then\nattack the police station, leading to\na confrontation where Dren is killed\nby the police. The film ends with\nthe scientists reflecting on their\nexperiment,\nacknowledging\nthe\ntragic consequences of their work.\nLLaMA-VID:\n1.\nThe\nopening\nscene\nshows\na\nscientist\nnamed\nElsa\nKastle\nconducting an experiment with a\ndog named Fred and a cat named\nGinger.\n2. Elsa and Clive are fired from\ntheir high-paying jobs and start a\nbiotechnology company.\n3. The company is forced to close\ndown its operations, leading to a\ngovernment investigation.\n4. Elsa and Clive are arrested and\nsent to jail.\n5. While in prison, Elsa and Clive\nare introduced to a secret of the\nhuman genetic code, which they use\nto\ncreate\na\nnew\nspecies\nof\norganism.\n6.\nThe\nnewly\nformed\nspecies,\nnamed Dren, is discovered by the\ngovernment and becomes a subject\nof scientific study.\n7. Elsa and Clive are released from\njail and continue their research,\nwhich leads to the discovery of the\nprotein CD356.\n8. The government is concerned\nabout\nthe\npotential\nof\nthe\nnew\nspecies and orders the destruction\nof the company's labs.\n9. The government's actions lead to\nthe discovery of the protein, which\nis synthesized into a stable form.\n10. Dren is discovered dead, and\nthe government sends a team to\ninvestigate.\n11.\nElsa\nand\nClive\nare\narrested\nagain, but are released, and they\ncontinue their research.\n12.\nElsa\nand\nClive\nare\narrested\nagain, and the government is called\nupon to destroy the labs.\n13. The government is called upon\nto destroy the labs again.\nFigure 9: Video understanding results of baseline model and ours. Our model demonstrates a more reasonable\nand detailed answer in the video understanding.\nCHATGPT PROMPT FOR LONG VIDEO EVALUATION\n\"You are an intelligent chatbot designed for comparing two video-based question-\nanswer pairs to decide which one is better and score them(use score from 0 to 5, float \nis allowed) based on the reference answer.\\n\\n\"\n\"##Instructions:\\n\"\n\"1. consider the detailed information involved in the answer (more \ndetailed, better)\\n\"\n\"2. consider the character relationship in the answer\\n\"\n\"3. consider the conclusion or ending involved in the answer\\n\"\n\"4. nonsense like repeated sentences are not allowed, should be \nconsidered as a very bad answer\\n\"\n\"5. if mark number is used in the answer, the order of the number \nshould be right\\n\\n\"\n\"Note that your answer should be like this: {'better one':'first one', \n'score':{'first one':3.5,'second one':1}}, only choose between first and second answer,DO \nNOT PROVIDE ANY OTHER TEXT OR EXPLANATION, only provide the python \ndictionary string like above.\\n\\n\"\nf\"Question: {question}\\n\\n\"\nf\"Reference answer: {gt_answer}\\n\\n\"\nf\"First answer: {our_answer}\\n\\n\"\nf\"Second answer: {llama_answer}\\n\\n\"\n\"Now, give me your answer.\"\nFigure 10: Prompt for Long Video Evaluation. Our prompt for long video evaluation assesses various capabilities\nof the model by comparing different predicted answers from different models based on the ground truth and scoring\nthe predicted answers on a scale of 1-5.\nCHATGPT PROMPT FOR MOVIE PLOT GENERATION\nYou are a specialist in creating stories. Generate a story which theme is \"[theme]\".\nRequirements: First, generate the title, overview and \"story tone and the style\" of the \nstory. I will use the story to generate images, so \"Story tone and style\" should describe \nthe artistic style of the pictures, and should use only adjectives and short phrases, not \nsentences, with the most important phrases included in \"((\"\"))\", and a single one-word \nstyle-keyword included in \"{\"\"}\"(Note that you must provide the style-keyword like the \nexample). Second, list all the locations that the story will take place in, at least 12 \nlocations. Only common places are allowed and use at most 2 words for the location. \nDo not mention character names in the locations. Do not use location like \"Mary's\" or \n\"Mary's house\" to indicate someone's home. Third, generate all the characters' name, \nand use the name of a Western celebrity to describe them, with clothes color, in the \nformat of \"<xxx: a man/woman looks like xxx in xxx>\". For clothes color, a simple one-\nword color is enough. If a character appears in the story for at least twice, put it in \n\"Character\" part. A main charater appears most frequently, and a supporting character \nappears in part of the story. Except for the characters listed in \"Character\" part, all the \nother characters and objects that will appear in the story should only appear once. For \nthe new story, I need 1 or 2 main characters, and 0 or 1 supporting charater. Fourth, \ngenerate 11 substories based on the overview. \"Scene\" part should pick 1,2 or 3 \nlocations in the \"Locations\" part, and the whole substory will only take place in these \nlocations.  \"QA\" part is a question-answer pair based on each substory. You can only \nask one of the four types of questions\"what\", \"why\", \"how\" and \"where\" after each \nsubstory. The answer should be one or two sentences, no need to be too long. Below is \nan example:\n-------------------------------------\nStory Title: The Symphony of Love\nOverview: 'The Symphony of Love' is a heartwarming tale set in the bustling city of New \nYork, painted in a vibrant, contemporary style. The story revolves around Amelia, a \ntalented violinist with a passion for music, and Ethan, a successful businessman with a \nhidden love for the arts. Their lives intersect as they navigate the complexities of love, \nambition, and the pursuit of dreams.\nStory tone and style: {Romantic}, ((Urban chic)), ((modern elegance, bohemian charm)), \npastel palette, minimalist aesthetic, soft lighting, candid moments, architectural beauty, \ncityscape, dynamic, emotional, intimate, dreamy, nostalgic\nStyle-keyword: Romantic\nLocations: ['Concert Hall', 'Office', 'Cafe', 'Art Gallery', 'Park', 'Restaurant', 'Apartment', \n'Subway', 'Rooftop', 'Street', 'Lake', 'Classroom', 'Cinema']\nCharacter:\nMain characters:\n<Amelia: a woman looks like Anne Hathaway in red>\n<Ethan: a man looks like Chris Hemsworth in blue>\nSupporting character:\n<Oliver: a man looks like Robert Downey Jr. in brown>\nSubstories:\nSubstory1: The Melody of Chance\nScene: Subway, Street\nContent: Amelia and Ethan have a chance encounter in the subway, and walk on the \nstreet.\nQA: Question: Where did Amelia and Ethan meet each other? Answer: In the subway\nSubstory2: The Harmony of Friendship\nScene: Cafe\nContent: Amelia and Ethan start to build a friendship over shared coffees and \nconversations at their favorite cafe.\nQA: Question: How did Amelia and Ethan know each other? Answer: They meet each \nother in cafe, drinking coffee.\nI didn't list all the substories, but you should generate the complete version.\nFigure 11: Prompt for Movie Plot Generation. Our prompt for movie plot generation comprises basic elements,\nincluding overview, characters, style and so on.\nLine:\nJack, it's Eva. \nWe need to \nmeet. I've got \nthe proof.\nLine:\nWe can still \nprevent this \ncatastrophe.\nLine:\nIt's time to \nunveil the truth.\nLine:\nThese patterns... \nit's alarming.\nLine:\nThe toxins are \nstronger than \nexpected.\nQuestion: Why is Dr. Eva determined to validate her findings?\nAnswer: She believes her results that a corporation's toxic \nactivities may lead to an ecological disaster, and she wants to \nexpose this truth before it's too late.\nFrame 73\nFrame 74\nFrame 75\nFrame 76\nFrame 77\nFrame 78\nFrame 79\nFrame 80\nFilm name: The Green Conundrum\nFigure 12: Examples of generated long video instruction data We use GPT-4 and guided text-to-image generation\nmodels to generate consistent key frames of move-level video with reasonable lines and corresponding question-\nanswer pairs. These data are used to train multimodal large language models on video understanding.\n"
  },
  {
    "title": "OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on",
    "link": "https://arxiv.org/pdf/2403.01779.pdf",
    "upvote": "20",
    "text": "OOTDiffusion: Outfitting Fusion based Latent\nDiffusion for Controllable Virtual Try-on\nYuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen\nXiao-i Research\n{yuhao.xu,tao.gu,weifeng.chen,arlenecc}@xiaoi.com\nFig. 1: Outfitted images (1024 \u00d7 768) generated by our OOTDiffusion trained on the\nVITON-HD [6] (1st row; supporting upper-body garments) and Dress Code [33] (2nd\nrow; supporting upper-body garments, lower-body garments and dresses) datasets, with\nvarious input human and garment images. Please zoom in for more details.\nAbstract. We present OOTDiffusion, a novel network architecture for\nrealistic and controllable image-based virtual try-on (VTON). We lever-\nage the power of pretrained latent diffusion models, designing an out-\nfitting UNet to learn the garment detail features. Without a redundant\nwarping process, the garment features are precisely aligned with the tar-\nget human body via the proposed outfitting fusion in the self-attention\nlayers of the denoising UNet. In order to further enhance the controllabil-\nity, we introduce outfitting dropout to the training process, which enables\nus to adjust the strength of the garment features through classifier-free\nguidance. Our comprehensive experiments on the VITON-HD and Dress\nCode datasets demonstrate that OOTDiffusion efficiently generates high-\nquality try-on results for arbitrary human and garment images, which\noutperforms other VTON methods in both realism and controllability,\nindicating an impressive breakthrough in virtual try-on. Our source code\nis available at https://github.com/levihsu/OOTDiffusion.\nKeywords: Virtual try-on \u00b7 Latent diffusion \u00b7 Outfitting fusion\narXiv:2403.01779v2  [cs.CV]  7 Mar 2024\n2\nXu et al.\n1\nIntroduction\nImage-based virtual try-on (VTON) is a popular and promising image-synthesis\ntechnology for the current e-commerce industry, which is able to dramatically\nimprove the shopping experience of consumers and reduce the advertising cost\nof clothing merchants. As its name suggests, the VTON task aims to generate\nan outfitted image of a target human wearing a given garment. It has taken\ntremendous efforts from numerous researchers [8, 11, 12, 15\u201317, 24, 30, 49, 52, 54]\nfor more natural and accurate virtual try-on results in the past few years.\nImage-based VTON is currently facing two main challenges. First, the gener-\nated images should be realistic and natural enough to avoid dissonance. Most of\nrecent researches on virtual try-on leverage generative adversarial networks [13]\n(GANs) or latent diffusion models [40] (LDMs) for image generation. Previous\nGAN-based methods [6, 15\u201317, 27, 52] usually have difficulty in generating cor-\nrect garment folds, natural light and shadow, or realistic human bodies. Hence\nmore recent work favors LDM-based methods [14, 24, 32, 60], which effectively\nimprove the realism of outfitted images. The second critical challenge is how to\npreserve as much as possible the garment detail features, such as complicated\ntext, textures, colors, patterns and lines, etc. Previous researches [6,14,27,32,52]\nusually perform an explicit warping process to align the garment features with\nthe target human body, and then feed the warped garment into generative mod-\nels (i.e., GANs and LDMs, etc.). Thus the performance of such VTON meth-\nods is extremely dependent on the efficacy of the independent warping process\nwhich is prone to overfitting the training data. On the other hand, some LDM-\nbased methods [14,24,32] attempt to learn garment features via CLIP textual-\ninversion [10], which fail to preserve fine-grained garment details.\nMotivated by the aforementioned prospects and challenges of image-based\nVTON, we present a novel LDM-based virtual try-on method, namely Outfit-\nting over Try-on Diffusion (OOTDiffusion; see Fig. 2). First, we make full use of\nthe advantages of pretrained latent diffusion models [40] to ensure high realism\nof generated images and natural try-on effects, and design an outfitting UNet to\nlearn the detail features of garments in the latent space in a single step. Then, we\npropose an outfitting fusion process to precisely align the garment features with\nthe noisy human body in the self-attention layers [48] of the denoising UNet. In\nthis way, the garment features are smoothly adapted to various target human\nbody types and postures, without suffering information loss or feature distor-\ntion caused by an independent warping process. Furthermore, we perform an\noutfitting dropout operation, randomly dropping a handful of garment latents\nin training to enable classifier-free guidance [20] with respect to the garment\nfeatures. Through this approach, the strength of garment control over the gener-\nated result can be simply adjusted by a guidance scale, which further enhances\nthe controllability of our VTON method.\nOur contributions are summarized as follows:\n\u2013 We present OOTDiffusion, an LDM-based network architecture with a novel\noutfitting UNet for realistic and controllable virtual try-on.\nOOTDiffusion for Controllable Virtual Try-on\n3\n\u2013 We propose outfitting fusion to efficiently align the garment features with the\ntarget human body in the self-attention layers without redundant warping.\n\u2013 We introduce outfitting dropout to the training process, which further im-\nproves the controllability of the outfitting UNet.\n\u2013 We train our OOTDiffusion on two broadly-used high-resolution benchmark\ndatasets, i.e., VITON-HD [6] and Dress Code [33], respectively. Extensive\nqualitative and quantitative evaluations demonstrate our superiority over the\nstate-of-the-art VTON methods in both realism and controllability for vari-\nous target human and garment images (see Fig. 1), implying an impressive\nbreakthrough in image-based virtual try-on.\n2\nRelated Work\nImage-based Virtual Try-on. Image-based virtual try-on has been investi-\ngated for many years as a promising and challenging task [6,9,14,16,22,24,27,\n32,33,49,52,60]. Aiming at more natural and accurate results, recent researches\nare mainly based on generative adversarial networks [13] (GANs) or latent dif-\nfusion models [40] (LDMs) for image generation. Among the GAN-based VTON\nmethods [6,27,52], VITON-HD [6] collected a high-resolution dataset and pro-\nposed ALIAS normalization and generator to address the misalignment between\nwarped clothes and target regions. HR-VITON [27] simultaneously performed\nwarping and segmentation to handle the body occlusion and garment misalign-\nment. GP-VTON [52] proposed an LFGP warping module to generate deformed\ngarments and introduced a DGT training strategy for the warping network.\nAs introduced above, GAN-based methods usually rely on an explicit warping\nprocess neglecting realistic garment folds and natural light and shadow, which\nseriously degrades the fidelity and realism of outfitted images. Meanwhile, GAN-\nbased methods are prone to overfitting the training data and causing severe\nperformance degradation on out-of-distribution images.\nWith respect to the LDM-based approaches [14, 24, 32], LaDI-VTON [32]\nand DCI-VTON [14] also require an explicit warping process. In specific, LaDI-\nVTON [32] performed textual-inversion to map the visual garment features to\nthe CLIP [39] token embedding space and condition the latent diffusion model\nalong with the warped input. DCI-VTON [14] directly combined the warped\nclothes with the masked person image to get a coarse result, and then refined\nit by the diffusion model. Neither of these methods succeeded in fully preserv-\ning garment details like complicated patterns and text due to the information\nloss caused by the CLIP encoder. More recently, StableVITON [24] discarded\nindependent warping and proposed a zero cross-attention block to learn seman-\ntic correlation between the clothes and human body. However, information loss\nremains in the cross-attention layers, and the extra zero-initialized blocks heav-\nily increase the training and inference cost. In contrast, again without warping,\nour LDM-based OOTDiffusion finetunes the pretrained outfitting UNet to learn\ngarment details in one step and efficiently incorporates them into the denoising\nUNet via our outfitting fusion with negligible information loss.\n4\nXu et al.\nLDM-based Controllable Image Generation. Latent diffusion models [40]\nhave achieved great success in text-to-image [1, 26, 38, 42, 44] and image-to-\nimage [23,36,43,45,47] generation in recent years. For the purpose of more con-\ntrollable generated results, Prompt-to-Prompt [18] and Null-text Inversion [31]\ncontrolled the cross-attention layers to finely edit images by modifying the in-\nput captions without extra model training. InstructPix2Pix [3] created paired\ndata to train diffusion models that generate the edited image given an input\nimage and a text instruction. Paint-by-Example [53] trained image-conditioned\ndiffusion models in a self-supervised manner to offer fine-grained image control.\nControlNet [57] and T2I-Adapter [34] incorporated additional blocks into pre-\ntrained diffusion models to enable spatial conditioning controls. IP-Adapter [55]\nadopted a decoupled cross-attention mechanism for text and image features to\nenable controllable generation with image prompt and additional structural con-\nditions. In this paper, we focus on the image-based VTON task, employing out-\nfitting fusion in the self-attention layers of the denoising UNet and performing\noutfitting dropout at training time to enable latent diffusion models to generate\nmore controllable outfitted images with respect to the garment features.\n3\nMethod\n3.1\nPreliminary\nStable Diffusion. Our OOTDiffusion is an extension of Stable Diffusion [40],\nwhich is one of the most commonly-used latent diffusion models. Stable Diffusion\nemploys a variational autoencoder [25] (VAE) that consists of an encoder E and\na decoder D to enable image representations in the latent space. And a UNet [41]\n\u03f5\u03b8 is trained to denoise a Gaussian noise \u03f5 with a conditioning input encoded by\na CLIP text encoder [39] \u03c4\u03b8. Given an image x and a text prompt y, the training\nof the denoising UNet \u03f5\u03b8 is performed by minimizing the following loss function:\nLLDM = EE(x),y,\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4\u03b8(y))\u22252\n2\n\u0003\n,\n(1)\nwhere t \u2208 {1, ..., T} denotes the time step of the forward diffusion process, and\nzt is the encoded image E(x) with the added Gaussian noise \u03f5 \u223c N(0, 1) (i.e.,\nthe noise latent). Note that the conditioning input \u03c4\u03b8(y) is correlated with the\ndenoising UNet by the cross-attention mechanism [48].\n3.2\nOOTDiffusion\nOverview. Fig. 2 illustrates the overview of our method. Given a target human\nimage x \u2208 R3\u00d7H\u00d7W and an input garment image g \u2208 R3\u00d7H\u00d7W , OOTDiffusion\nis capable of generating a realistic outfitted image xg \u2208 R3\u00d7H\u00d7W . We employ\nOpenPose [4,5,46,51] and HumanParsing [28] to generate a masked human image\nxm \u2208 R3\u00d7H\u00d7W , and use a VAE encoder E to transform it into the latent space as\nE(xm) \u2208 R4\u00d7h\u00d7w, where h = H\n8 and w = W\n8 . Then we concatenate E(xm) with\na Gaussian noise \u03f5 \u2208 R4\u00d7h\u00d7w as the input latent zT \u2208 R8\u00d7h\u00d7w for the denoising\nOOTDiffusion for Controllable Virtual Try-on\n5\nVAE\nencoder\nVAE\nencoder\nC\nNoise\nCLIP image\nencoder\nLinear layer\nTokenizer\nCLIP text\nencoder\nC\nGarment image\n\"upperbody\"\nGarment label\n(optional)\nVAE\ndecoder\nMask\ngenerator\nOutfitted image\nTarget image\nMasked image\nOutfitting fusion\nOutfitting fusion\nOutfitting UNet\nDenoising UNet\n(single step)\n(multiple steps)\nOutfitting dropout\n(in training)\nC\n= Concatenate\nFig. 2: Overview of our proposed OOTDiffusion model. On the left side, the garment\nimage is encoded into the latent space and fed into the outfitting UNet for a single\nstep process. Along with the auxiliary conditioning input generated by CLIP encoders,\nthe garment features are incorporated into the denoising UNet via outfitting fusion.\nOutfitting dropout is performed for the garment latents particularly in training to\nenable classifier-free guidance. On the right side, the input human image is masked\nwith respect to the target region and concatenated with a Gaussian noise as the input\nto the denoising UNet for multiple sampling steps. After denoising, the feature map is\ndecoded back into the image space as our try-on result.\nUNet. Note that we add 4 zero-initialized channels to the first convolutional\nlayer of the denoising UNet to support our input with 8 channels.\nOn the other side, we feed the encoded garment latent E(g) \u2208 R4\u00d7h\u00d7w into\nan (i) outfitting UNet to learn the garment features in a single step, and in-\ntegrate them into the denoising UNet via our (ii) outfitting fusion. And we\nperform (iii) outfitting dropout for E(g) particularly in the training process. In\naddition, we also conduct CLIP textual-inversion [10] for the garment image\ng, and optionally concatenate it with a text embedding of the garment label\ny \u2208 {\u201cupperbody\u201d, \u201clowerbody\u201d, \u201cdress\u201d} as an auxiliary conditioning input,\nwhich is fed into both outfitting and denoising UNets via the cross-attention\n6\nXu et al.\nmechanism [48]. Finally, after multiple steps of the denoising process, we use a\nVAE decoder D to transform the denoised latent z0 \u2208 R4\u00d7h\u00d7w back into the im-\nage space as the output image xg = D(z0) \u2208 R3\u00d7H\u00d7W . We will elaborate the key\ntechnologies (i.e., (i) outfitting UNet, (ii) outfitting fusion, and (iii) outfitting\ndropout) of our OOTDiffusion in the following sections.\nOutfitting UNet. As introduced above, we propose an outfitting UNet to ef-\nficiently learn the detail features of the garment image g. The left side of Fig. 2\nshows the architecture of our outfitting UNet, which is essentially identical to the\ndenoising UNet of Stable Diffusion. The encoded garment latent E(g) \u2208 R4\u00d7h\u00d7w\nis fed into the outfitting UNet \u03c9\u03b8\u2032, and then incoporated into the denoising\nUNet \u03f5\u03b8 via our outfitting fusion (see the next section). Along with the afore-\nmentioned auxiliary conditioning input, the outfitting and denoising UNets are\njointly trained by minimizing the following loss function:\nLOOT D = EE(xm),E(g),\u03c8,\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c9\u03b8\u2032(E(g), \u03c8), \u03c8)\u22252\n2\n\u0003\n,\n(2)\nwhere \u03c8 = \u03c4g(g) c\u25cb \u03c4y(y) represents the auxiliary conditioning input for both\n\u03c9\u03b8\u2032 and \u03f5\u03b8. While \u03c4g and \u03c4y refer to the pretrained CLIP image encoder and text\nencoder respectively, and c\u25cb denotes concatenation.\nIn practice, we directly duplicate the pretrained UNet weights of Stable Dif-\nfusion [40] for the initialization of both our outfitting and denoising UNets (ex-\ncept for the zero-initialized channels added to the first convolutional layer), and\njointly finetune them on the high-resolution VTON datasets [6, 33]. Note that\n\u03c9\u03b8\u2032 and \u03f5\u03b8 do not share any weights in the training process. We claim that our\ntactical utilization of the pretrained models dramatically improves the training\nefficiency and reduces the training cost. Moreover, compared with the denoising\nUNet, a significant difference in our outfitting UNet is that it requires only one\nstep forward process before the multiple denoising steps in inference, causing a\nminimal amount of extra computational cost to the original Stable Diffusion [40].\nOutfitting Fusion. Based on our proposed outfitting UNet and inspired by\nthe spatial-attention mechanism [21,48], we propose an outfitting fusion process\nto incorporate the learned garment features into the denoising UNet. First, we\ndive into the transformer blocks [48] of two UNets, finding each pair of feature\nmaps used as the input to the corresponding self-attention layers [48]. Given the\nnth pair of the feature maps gn, xn \u2208 Rcn\u00d7hn\u00d7wn, we concatenate them in the\nspatial domain as:\nxgn = xn c\u25cb gn \u2208 Rcn\u00d7hn\u00d72wn.\n(3)\nAnd we replace xn with the concatenated feature map xgn as the input to the\nself-attention layer of the denoising UNet. Then we crop out the fist half of the\noutput feature map as the final output of the self-attention layer. Fig. 3 visualizes\nthe attention maps learned in our modified self-attention. We observe that the\nunmasked region focuses attention on the human body (1st row), and the masked\npixels are attracted to the garment features (2nd row). Meanwhile, during the\nOOTDiffusion for Controllable Virtual Try-on\n7\nInput\nFig. 3: Visualization of the attention maps with respect to the human body (1st row)\nand garment features (2nd row) aligned by our outfitting fusion.\ndenoising process, the attention to the human body gradually includes part of\nthe masked region like the neck and arms, and the attention to the garment\nfeatures gradually increases in the region of the complicated patterns. Through\noutfitting fusion in the self-attention layers, the garment features are implicitly\nwarped and effectively correlated with the target human body with negligible\ninformation loss. Hence the denoising UNet is made capable of learning the\nprecise features from the outfitting UNet for preserving garment details and\nnaturally adapting them to the target human body in the generated image.\nOutfitting Dropout. In order to further enhance the controllability of our\nVTON method, we employ an outfitting dropout operation in training to enable\nclassifier-free guidance [20] with respect to the garment features. Classifier-free\nguidance has been broadly used in conditional image generation [3, 35, 44, 56]\nfor trading off the quality and diversity of images generated by latent diffusion\nmodels. Specifically in the training process of our outfitting UNet, we randomly\ndrop the input garment latent as E(g) = \u2205, where \u2205 \u2208 R4\u00d7h\u00d7w refers to an\nall-zero latent. In this way, the denoising UNet is trained both conditionally and\nunconditionally, i.e., with and without the outfitting fusion. Then at inference\ntime, we simply use a guidance scale sg \u2265 1 to adjust the strength of conditional\ncontrol over the predicted noise \u02c6\u03f5\u03b8 as:\n\u02c6\u03f5\u03b8(zt, \u03c9\u03b8\u2032(E(g))) = \u03f5\u03b8(zt, \u2205) + sg \u00b7 (\u03f5\u03b8(zt, \u03c9\u03b8\u2032(E(g))) \u2212 \u03f5\u03b8(zt, \u2205)),\n(4)\nwhere we omit some minor terms compared with Eq. (2) for the sake of brevity.\nIn practice, we empirically set the outfitting dropout ratio to 10% in training,\ni.e., 10% of garment latents E(g) are set to \u2205. And the optimal value of the\nguidance scale sg is usually around 1.5 \u223c 2.0 according to our ablation study\n(see Sec. 4.3). Fig. 4 and Tab. 1 demonstrate the effects of our outfitting dropout\nand different guidance scale values.\n8\nXu et al.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets. Our experiments are performed on two high-resolution (1024 \u00d7 768)\nvirtual try-on datasets, i.e., VITON-HD [6] and Dress Code [33]. The VITON-\nHD dataset consists of 13,679 image pairs of frontal half-body models and cor-\nresponding upper-body garments, where 2032 pairs are used as the test set.\nThe Dress Code dataset consists of 15,363/8,951/2,947 image pairs of full-body\nmodels and corresponding upper-body garments/lower-body garments/dresses,\nwhere 1,800 pairs for each garment category are used as the test set.\nCompared Methods. On the VITON-HD dataset [6], we compare our OOTD-\niffusion with multiple state-of-the-art VTON methods, including the GAN-based\nVITON-HD [6], HR-VITON [27] and GP-VTON [52], as well as the LDM-based\nLaDI-VTON [32] and StableVITON [24].\nWhile for the evaluation on the Dress Code dataset [33], since VITON-HD [6],\nHR-VITON [27] and StableVITON [24] are not designed for the entire dataset be-\nyond upper-body garments, we select two VTON methods (i.e., GP-VTON [52]\nand LaDI-VTON [32]) and another LDM-based inpainting method (i.e., Paint-\nby-Example [53]) for fair comparison.\nEvaluation Metrics. We evaluate the results in both the paired and unpaired\nsettings, where the paired setting provides the target human and the corre-\nsponding garment images for reconstruction, and the unpaired setting provides\nthe different garment images for virtual try-on. Specifically for Dress Code [33],\nwe note that the evaluation is performed on the entire dataset rather than being\nlimited to upper-body garments. This more effectively validates the feasibility\nof each method in real-world applications with various garment types.\nIn the quantitative evaluation, though our OOTDiffusion supports higher-\nresolution (1024 \u00d7 768) virtual try-on, all the experiments are conducted at the\nresolution of 512 \u00d7 384 for fair comparison with previous VTON methods. For\nthe paired setting, we use LPIPS [58] and SSIM [50] to measure the quality of\nthe generated image in terms of restoring the original image. For the unpaired\nsetting, we employ FID [19] and KID [2] for realism and fidelity assessment. We\nfollow the previous work [7,32,37] to implement all of these metrics.\n4.2\nImplementation Details\nIn our experiments, we initialize the OOTDiffusion models by inheriting the\npretrained weights of Stable Diffusion v1.5 [40]. Then we finetune the outfitting\nand denoising UNets using an AdamW optimizer [29] with a fixed learning rate\nof 5e-5. Note that we train four types of models on VITON-HD [6] and Dress\nCode [33] datasets at resolutions of 512\u00d7384 and 1024\u00d7768, separately. All the\nmodels are trained for 36,000 iterations on a single NVIDIA A100 GPU, with a\nOOTDiffusion for Controllable Virtual Try-on\n9\nw/o Outfitting Dropout\nw/ Outfitting Dropout\nInput\nFig. 4: Qualitative comparison of outfitted images generated by OOTDiffusion models\ntrained without/with outfitting dropout and using different values of the guidance scale\nsg. Please zoom in for more details.\nTable 1: Ablation study of outfitting dropout and different guidance scale values on\nthe VITON-HD dataset [6]. The best and second best results are reported in bold and\nunderline, respectively.\nOutfitting Guidance LPIPS \u2193 SSIM \u2191 FID \u2193 KID \u2193\nDropout\nScale\n\u2717\n-\n0.0750\n0.8699\n8.91\n0.89\n\u2713\n1.0\n0.0749\n0.8705\n8.99\n0.89\n\u2713\n1.5\n0.0705\n0.8775\n8.81\n0.82\n\u2713\n2.0\n0.0708\n0.8766\n8.80\n0.86\n\u2713\n2.5\n0.0746\n0.8691\n8.84\n0.89\n\u2713\n3.0\n0.0753\n0.8684\n8.95\n0.96\n\u2713\n5.0\n0.0788\n0.8640\n9.28\n1.22\nbatch size of 64 for the 512\u00d7384 resolution and 16 for the 1024\u00d7768 resolution.\nAt inference time, we run our OOTDiffusion on a single NVIDIA RTX 4090\nGPU for 20 sampling steps using the UniPC sampler [59].\n4.3\nAblation Study\nWe investigate the effects of our proposed outfitting dropout as well as the dif-\nferent values of the guidance scale sg on the VITON-HD dataset [6]. First, we\n10\nXu et al.\nInput\nOurs\nStableVITON\nGP-VTON\nVITON-HD\nHR-VITON\nLaDI-VTON\nFig. 5: Qualitative comparison on the VITON-HD dataset [6] (half-body models with\nupper-body garments). Please zoom in for more details.\ntrain two variants of our OOTDiffusion models without/with outfitting dropout,\nrespectively. Then for the model trained with outfitting dropout, we set sg =\n1.0, 1.5, 2.0, 2.5, 3.0, 5.0 for classifier-free guidance. At inference time, we guar-\nantee all of other parameters (including the random seed) are consistent for fair\ncomparison. As Fig. 4 shows, without outfitting dropout, classifier-free guidance\nis not supported and the generated result is obviously the worst. While for the\nmodel trained with outfitting dropout, when sg = 1.0, the inference process\nis identical to the model without outfitting dropout (see Eq. (4)), which gets\na similarly bad result. When sg > 1.0, we see that the fine-grained garment\nfeatures become clearer as sg increases. However, color distortion occurs when\nsg \u2265 2.5 and becomes extremely significant when sg = 5.0 (see the flower pat-\nterns). Furthermore, Tab. 1 quantitatively proves the efficacy of our outfitting\ndropout which enables classifier-free guidance with respect to the garment fea-\ntures, and finds the optimal guidance scale value is around 1.5 \u223c 2.0 in most\ncases. According to this study, we consistently conduct outfitting dropout for\nOOTDiffusion, and empirically set sg = 1.5 for the VITON-HD dataset [6] and\nsg = 2.0 for the Dress Code dataset [33] in the following experiments.\nOOTDiffusion for Controllable Virtual Try-on\n11\nOurs\nLaDI-VTON\nGP-VTON\nPaint-by-Example\nInput\nFig. 6: Qualitative comparison on the Dress Code dataset [33] (full-body models with\nupper-body garments/lower-body garments/dresses). Please zoom in for more details.\n4.4\nExperimental Results\nQualitative Results. Fig. 5 visually shows some example results of our method\nand other VTON methods on the test set of VITON-HD [6]. We observe that\ncompared with other methods, our OOTDiffusion consistently achieves the best\ntry-on effects for various upper-body garments. More specifically, GAN-based\nmethods like GP-VTON [52] often fail to generate realistic human bodies (1st\n12\nXu et al.\nOurs\nLaDI-VTON\nGP-VTON\nStableVITON\nInput\nFig. 7: Qualitative results of the cross-dataset evaluation. The models are trained on\nthe VITON-HD dataset [6] and tested on the Dress Code dataset [33]. Please zoom in\nfor more details.\nand 4th rows) or natural garment folds (2nd and 3rd rows), making the outfit-\nted images look unrealistic. While other LDM-based methods including LaDI-\nVTON [32] and StableVITON [24] tend to lose some garment details such as\ncomplicated text (2nd and 4th rows) or patterns (1st and 3rd rows). In contrast,\nour OOTDiffusion not only generates realistic images but also preserves most of\nthe fine-grained garment details.\nRegarding the more complicated Dress Code dataset [33], which consists\nof full-body models and various garment categories, our OOTDiffusion still vi-\nsually outperforms other VTON methods. As illustrated in Fig. 6, Paint-by-\nExample [53] and LaDI-VTON [32] fail to preserve the garment features, and\nGP-VTON [52] tends to cause severe body and background distortion. On the\ncontrary, our OOTDiffusion consistently shows very stable performance on dif-\nferent garment categories including upper-body garments (1st row), lower-body\ngarments (2nd row) and dresses (3rd and 4th rows).\nIn order to evaluate the generalization ability of our method, we conduct\nan additional cross-dataset experiment, i.e., training on one dataset and testing\non the other. Fig. 7 demonstrates that among all the models trained on the\nVITON-HD dataset [6], our OOTDiffusion is optimally adapted to the test ex-\namples in the Dress Code dataset [33], generating more realistic outfitted images\nand preserving much more garment details. In summary, the observations above\n(Figs. 5 to 7) qualitatively prove the superiority and generalization capability of\nour OOTDiffusion in generating natural and accurate try-on results for various\nhuman and garment images.\nOOTDiffusion for Controllable Virtual Try-on\n13\nTable 2: Quantitative results on the VITON-HD dataset [6]. The best and second\nbest results are reported in bold and underline, respectively.\nMethod\nLPIPS \u2193 SSIM \u2191 FID \u2193 KID \u2193\nVITON-HD [6]\n0.116\n0.863\n12.13\n3.22\nHR-VITON [27]\n0.097\n0.878\n12.30\n3.82\nLaDI-VTON [32]\n0.091\n0.875\n9.31\n1.53\nGP-VTON [52]\n0.083\n0.892\n9.17\n0.93\nStableVITON [24]\n0.084\n0.862\n9.13\n1.20\nOOTDiffusion (Ours)\n0.071\n0.878\n8.81\n0.82\nTable 3: Quantitative results on the Dress Code dataset [33]. The best and second\nbest results are reported in bold and underline, respectively. The * marker refers to\nthe results reported in previous work.\nMethod\nAll\nUpper-body\nLower-body\nDresses\nLPIPS \u2193 SSIM \u2191 FID \u2193 KID \u2193 FID \u2193 KID \u2193 FID \u2193 KID \u2193 FID \u2193 KID \u2193\nPSAD* [33]\n0.058\n0.918\n10.61\n6.17\n17.51\n7.15\n19.68\n8.90\n17.07\n6.66\nPaint-by-Example [53]\n0.142\n0.851\n9.57\n3.63\n18.63\n4.81\n15.89\n4.12\n19.15\n5.88\nLaDI-VTON [32]\n0.067\n0.910\n5.66\n1.21\n12.30\n1.30\n13.38\n1.98\n13.12\n1.85\nGP-VTON [52]\n0.051\n0.921\n5.88\n1.28\n12.20\n1.22\n16.65\n2.86\n12.65\n1.84\nOOTDiffusion (Ours)\n0.045\n0.927\n4.20\n0.37\n11.03\n0.29\n9.72\n0.64\n10.65\n0.54\nTable 4: Quantitative results of the cross-dataset evaluation. Each model is trained\non one of the VITON-HD [6] and Dress Code [33] datasets, and evaluated on the other.\nThe best and second best results are reported in bold and underline, respectively. The\n* marker refers to the results reported in previous work.\nTrain/Test\nVITON-HD/Dress Code\nDress Code/VITON-HD\nMethod\nLPIPS \u2193 SSIM \u2191 FID \u2193 KID \u2193 LPIPS \u2193 SSIM \u2191 FID \u2193 KID \u2193\nVITON-HD* [6]\n0.187\n0.853\n44.26\n28.82\n-\n-\n-\n-\nHR-VITON* [27]\n0.108\n0.909\n19.97\n7.35\n-\n-\n-\n-\nLaDI-VTON [32]\n0.154\n0.908\n14.58\n3.59\n0.235\n0.812\n29.66\n20.58\nGP-VTON [52]\n0.291\n0.820\n74.36\n80.49\n0.266\n0.811\n52.69\n49.14\nStableVITON [24]\n0.065\n0.914\n13.18\n2.26\n-\n-\n-\n-\nOOTDiffusion (Ours)\n0.061\n0.915\n11.96\n1.21\n0.123\n0.839\n11.22\n2.72\nQuantitative Results. Tab. 2 presents the quantitative evaluation results on\nthe VITON-HD dataset [6]. We find that some GAN-based models like HR-\nVITON [27] and GP-VTON [52] achieve relatively high SSIM scores, indicating\nthat they are able to retain the structural information of the original images.\nHowever, their generated images lack detail fidelity, and thus drop behind ours\non LPIPS. The previous LDM-based methods including LaDI-VTON [32] and\nStableVITON [24] generate more realistic images according to their FID and\nKID scores, but they fail to restore the detail features due to their lossy feature\nfusion. In comparison, our OOTDiffusion not only generates realistic outfitted\n14\nXu et al.\nimages but also preserves the precise details, and thus substantially outperforms\nother methods on the other three metrics (LPIPS, FID and KID) while obtaining\ncomparable SSIM scores to the GAN-based methods.\nTab. 3 demonstrates the state-of-the-art performance of our method on the\nDress Code dataset [33], which outperforms others on all the metrics for all the\ngarment categories (upper-body/lower-body/dresses), confirming our feasibility\nin more complicated cases. Note that GP-VTON [52] applies extra data mod-\nifications such as background removal and pose normalization to Dress Code,\nand only provides part of their test data. Despite this, our OOTDiffusion still\nachieves the best results on the more challenging original test dataset.\nFurthermore, the generalization capability of our method is quantitatively\nverified by the results of the cross-dataset evaluation listed in Tab. 4. We find that\nGP-VTON [52] falls far behind other methods on all the metrics since its warping\nmodule severely overfits the training data. While our method leads again on all\nthe metrics for the out-of-distribution test dataset. Overall, the observations\nabove (Tabs. 2 to 4) further demonstrate that our OOTDiffusion significantly\noutperforms previous VTON methods in both realism and controllability in all\nkinds of scenarios and conditions.\n4.5\nLimitations\nDespite the state-of-the-art performance achieved in the image-based virtual\ntry-on task, limitations still exist in our OOTDiffusion which demand further\nimprovement. First, since our models are trained on paired human and garment\nimages, it may fail to get perfect results for cross-category virtual try-on, e.g., to\nput a T-shirt on a woman in a long dress, or to let a man in pants wear a skirt.\nThis issue can be partially solved in the future by collecting datasets of each\nperson wearing different clothes in the same pose. Another limitation is that\nsome details in the original human image might be altered after virtual try-on,\nsuch as muscles, watches or tattoos, etc. The reason is that the relevant body\narea is masked and repainted by the diffusion model. Thus more practical pre-\nand post-processing methods are required for addressing such problems.\n5\nConclusion\nIn this paper, we present OOTDiffusion, a novel LDM-based network architec-\nture for image-based vitrual try-on. The proposed outfitting UNet efficiently\nlearns the garment features and incorporates them into the denoising UNet via\nthe proposed outfitting fusion process with negligible information loss. Classifier-\nfree guidance for the garment features is enabled by the proposed outfitting\ndropout in training, which further enhances the controllability of our method.\nExtensive experiments on high-resolution datasets show our superiority over\nother VTON methods in both realism and controllability, indicating that our\nOOTDiffusion has broad application prospects for virtual try-on.\nOOTDiffusion for Controllable Virtual Try-on\n15\nAcknowledgements\nWe sincerely thank our colleagues including Yilan Ye, Bin Fu, Wei Du, Xuping\nSu, and Chi Zhang, etc., for kindly supporting and promoting our work. Special\nthanks to Minh-Duc Vo for his helpful advice.\nReferences\n1. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J.,\nLee, J., Guo, Y., et al.: Improving image generation with better captions. Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf 2(3), 8 (2023)\n2. Bi\u0144kowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying mmd gans.\narXiv preprint arXiv:1801.01401 (2018)\n3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image\nediting instructions. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 18392\u201318402 (2023)\n4. Cao, Z., Hidalgo Martinez, G., Simon, T., Wei, S., Sheikh, Y.A.: Openpose: Real-\ntime multi-person 2d pose estimation using part affinity fields. IEEE Transactions\non Pattern Analysis and Machine Intelligence (2019)\n5. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation\nusing part affinity fields. In: CVPR (2017)\n6. Choi, S., Park, S., Lee, M., Choo, J.: Viton-hd: High-resolution virtual try-on via\nmisalignment-aware normalization. In: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition. pp. 14131\u201314140 (2021)\n7. Detlefsen, N.S., Borovec, J., Schock, J., Jha, A.H., Koker, T., Di Liello, L., Stancl,\nD., Quan, C., Grechkin, M., Falcon, W.: Torchmetrics-measuring reproducibility\nin pytorch. Journal of Open Source Software 7(70), 4101 (2022)\n8. Dong, H., Liang, X., Shen, X., Wang, B., Lai, H., Zhu, J., Hu, Z., Yin, J.: Towards\nmulti-pose guided virtual try-on network. In: Proceedings of the IEEE/CVF inter-\nnational conference on computer vision. pp. 9026\u20139035 (2019)\n9. Fenocchi, E., Morelli, D., Cornia, M., Baraldi, L., Cesari, F., Cucchiara, R.:\nDual-branch collaborative transformer for virtual try-on. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2247\u2013\n2251 (2022)\n10. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,\nCohen-Or, D.: An image is worth one word: Personalizing text-to-image gener-\nation using textual inversion. arXiv preprint arXiv:2208.01618 (2022)\n11. Ge, C., Song, Y., Ge, Y., Yang, H., Liu, W., Luo, P.: Disentangled cycle consistency\nfor highly-realistic virtual try-on. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 16928\u201316937 (2021)\n12. Ge, Y., Song, Y., Zhang, R., Ge, C., Liu, W., Luo, P.: Parser-free virtual try-on\nvia distilling appearance flows. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 8485\u20138493 (2021)\n13. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial networks. Communications of the\nACM 63(11), 139\u2013144 (2020)\n14. Gou, J., Sun, S., Zhang, J., Si, J., Qian, C., Zhang, L.: Taming the power of diffusion\nmodels for high-quality virtual try-on with appearance flow. In: Proceedings of the\n31st ACM International Conference on Multimedia. pp. 7599\u20137607 (2023)\n16\nXu et al.\n15. Han, X., Hu, X., Huang, W., Scott, M.R.: Clothflow: A flow-based model for clothed\nperson generation. In: Proceedings of the IEEE/CVF international conference on\ncomputer vision. pp. 10471\u201310480 (2019)\n16. Han, X., Wu, Z., Wu, Z., Yu, R., Davis, L.S.: Viton: An image-based virtual try-on\nnetwork. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. pp. 7543\u20137552 (2018)\n17. He, S., Song, Y.Z., Xiang, T.: Style-based global appearance flow for virtual try-on.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 3470\u20133479 (2022)\n18. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or,\nD.: Prompt-to-prompt image editing with cross attention control. arXiv preprint\narXiv:2208.01626 (2022)\n19. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems 30 (2017)\n20. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022)\n21. Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent\nand controllable image-to-video synthesis for character animation. arXiv preprint\narXiv:2311.17117 (2023)\n22. Issenhuth, T., Mary, J., Calauzenes, C.: Do not mask what you do not need to\nmask: a parser-free virtual try-on. In: Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16. pp.\n619\u2013635. Springer (2020)\n23. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,\nM.: Imagic: Text-based real image editing with diffusion models. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n6007\u20136017 (2023)\n24. Kim, J., Gu, G., Park, M., Park, S., Choo, J.: Stableviton: Learning seman-\ntic correspondence with latent diffusion model for virtual try-on. arXiv preprint\narXiv:2312.01725 (2023)\n25. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013)\n26. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept cus-\ntomization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition. pp. 1931\u20131941 (2023)\n27. Lee, S., Gu, G., Park, S., Choi, S., Choo, J.: High-resolution virtual try-on with\nmisalignment and occlusion-handled conditions. In: European Conference on Com-\nputer Vision. pp. 204\u2013219. Springer (2022)\n28. Li, P., Xu, Y., Wei, Y., Yang, Y.: Self-correction for human parsing. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence 44(6), 3260\u20133271 (2020)\n29. Loshchilov, I., Hutter, F.: Fixing weight decay regularization in adam (2018)\n30. Minar, M.R., Tuan, T.T., Ahn, H., Rosin, P., Lai, Y.K.: Cp-vton+: Clothing shape\nand texture preserving image-based virtual try-on. In: CVPR Workshops. vol. 3,\npp. 10\u201314 (2020)\n31. Mokady, R., Hertz, A., Aberman, K., Pritch, Y., Cohen-Or, D.: Null-text inver-\nsion for editing real images using guided diffusion models. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6038\u2013\n6047 (2023)\nOOTDiffusion for Controllable Virtual Try-on\n17\n32. Morelli, D., Baldrati, A., Cartella, G., Cornia, M., Bertini, M., Cucchiara, R.: Ladi-\nvton: Latent diffusion textual-inversion enhanced virtual try-on. arXiv preprint\narXiv:2305.13501 (2023)\n33. Morelli, D., Fincato, M., Cornia, M., Landi, F., Cesari, F., Cucchiara, R.:\nDress code: High-resolution multi-category virtual try-on. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2231\u2013\n2235 (2022)\n34. Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453 (2023)\n35. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n36. Parmar, G., Kumar Singh, K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot\nimage-to-image translation. In: ACM SIGGRAPH 2023 Conference Proceedings.\npp. 1\u201311 (2023)\n37. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in\ngan evaluation. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 11410\u201311420 (2022)\n38. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M\u00fcller, J., Penna,\nJ., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952 (2023)\n39. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\nnatural language supervision. In: International conference on machine learning. pp.\n8748\u20138763. PMLR (2021)\n40. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. pp. 10684\u201310695 (2022)\n41. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015)\n42. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 22500\u201322510 (2023)\n43. Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., Norouzi,\nM.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH 2022 Confer-\nence Proceedings. pp. 1\u201310 (2022)\n44. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. Advances in Neural\nInformation Processing Systems 35, 36479\u201336494 (2022)\n45. Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-\nresolution via iterative refinement. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 45(4), 4713\u20134726 (2022)\n46. Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single\nimages using multiview bootstrapping. In: CVPR (2017)\n18\nXu et al.\n47. Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features\nfor text-driven image-to-image translation. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition. pp. 1921\u20131930 (2023)\n48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n\u0141., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n49. Wang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., Yang, M.: Toward characteristic-\npreserving image-based virtual try-on network. In: Proceedings of the European\nconference on computer vision (ECCV). pp. 589\u2013604 (2018)\n50. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. IEEE transactions on image processing\n13(4), 600\u2013612 (2004)\n51. Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines.\nIn: CVPR (2016)\n52. Xie, Z., Huang, Z., Dong, X., Zhao, F., Dong, H., Zhang, X., Zhu, F., Liang,\nX.: Gp-vton: Towards general purpose virtual try-on via collaborative local-flow\nglobal-parsing learning. In: Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition. pp. 23550\u201323559 (2023)\n53. Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., Wen, F.: Paint\nby example: Exemplar-based image editing with diffusion models. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n18381\u201318391 (2023)\n54. Yang, H., Zhang, R., Guo, X., Liu, W., Zuo, W., Luo, P.: Towards photo-realistic\nvirtual try-on by adaptively generating-preserving image content. In: Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition. pp.\n7850\u20137859 (2020)\n55. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-\nble image prompt adapter for text-to-image diffusion models. arXiv preprint\narXiv:2308.06721 (2023)\n56. Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,\nA., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789 2(3), 5 (2022)\n57. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 3836\u20133847 (2023)\n58. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable\neffectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp. 586\u2013595 (2018)\n59. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector\nframework for fast sampling of diffusion models. Advances in Neural Information\nProcessing Systems 36 (2024)\n60. Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M.,\nKemelmacher-Shlizerman, I.: Tryondiffusion: A tale of two unets. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n4606\u20134615 (2023)\n"
  },
  {
    "title": "AtomoVideo: High Fidelity Image-to-Video Generation",
    "link": "https://arxiv.org/pdf/2403.01800.pdf",
    "upvote": "17",
    "text": "AtomoVideo: High Fidelity Image-to-Video Generation\nLitong Gong\u2217, Yiran Zhu\u2217, Weijie Li\u2217, Xiaoyang Kang\u2217, Biao Wang, Tiezheng Ge, Bo Zheng\nAlimama Tech, Alibaba Group\nBeijing, China\n{gonglitong.glt, yizhu.zyr, weijie.lwj0, kangxiaoyang.kxy,\neric.wb, tiezheng.gtz, bozheng}@alibaba-inc.com\nInput image\nGenerated videos\nPrompt: Lonely cowboy, strumming, old west street, vivid, vintage. \nPrompt: Astronaut, wading through ocean waves, digital painting.\nPrompt: Volcanic eruption, tropical landscape, fiery skies, palm trees. \nFigure 1. Given a reference image and prompt, AtomoVideo can generates vivid videos while maintaining high fidelity detail with the\ngiven image.\nAbstract\nRecently, video generation has achieved significant rapid\ndevelopment based on superior text-to-image generation\ntechniques. In this work, we propose a high fidelity frame-\nwork for image-to-video generation, named AtomoVideo.\nBased on multi-granularity image injection, we achieve\nhigher fidelity of the generated video to the given image.\nIn addition, thanks to high quality datasets and training\n\u2217These authors contributed equally to this work.\nstrategies, we achieve greater motion intensity while main-\ntaining superior temporal consistency and stability. Our\narchitecture extends flexibly to the video frame prediction\ntask, enabling long sequence prediction through iterative\ngeneration.\nFurthermore, due to the design of adapter\ntraining, our approach can be well combined with existing\npersonalised models and controllable modules. By quanti-\ntatively and qualitatively evaluation, AtomoVideo achieves\nsuperior results compared to popular methods, more exam-\nples can be found on our project website: https://atomo-\nvideo.github.io/.\n1\narXiv:2403.01800v2  [cs.CV]  5 Mar 2024\n1. Introduction\nRecently, video generation based on diffusion models[3,\n4, 7, 15, 32, 36], have shown a growing interest and re-\nmarkable progress with impressive performance.\nIn this\npaper, we introduce AtomoVideo, a novel framework for\nhigh-fidelity image-to-video(I2V) generation. AtomoVideo\ncan generate high-fidelity videos from input image, achiev-\ning superior motion intensity and consistency compared to\nexisting works.\nIn combination with the advanced text-\nto-image(T2I) model[24, 28\u201330], AtomoVideo also can\nachieve text-to-video(T2V) generation.\nIn addition, our\napproach can be flexibly combined with personalised T2I\nmodels and controlled generative models[23, 42] for more\ncustomised and controllable generation, and we hope that\nAtomoVideo will contribute to the development of the video\ngeneration community.\nImage-to-video generation is different from text-to-\nvideo generation because it requires to ensure as much as\npossible the style, content, and more fine-grained details\nof the given image, which greatly increases the challenge\nof the image-to-video generation task.\nRecently, an in-\ncreasing number of researchers[3, 6, 13, 14, 43, 45] have\nfocused on the area of image-to-video generation. In or-\nder to improve the consistency with the given image, some\nmethods[3, 14, 43] encode the image as high-level image\nprompts to inject into the model with cross-attention, such\nmethods are difficult to achieve consistency of fine-grained\ndetails due to the utilisation of only higher-order seman-\ntics. In addition to this, a simpler idea is the concatenation\nof additional channels at the input, which although inputs\nmore fine-grained low-level information, is harder to con-\nverge and generates poorer stability of the video. Therefore,\na increasing number of works[3, 6] use both of the above\nmethods for image information injection.However, some of\nthese methods[6, 8, 14] use a noisy prior instead of start-\ning with pure Gaussian noise during inference, in order to\ncompensate for the artifacts of model instability. Since the\nnoise prior contains information of the given image, such\nas the inversion of the reference latent, the fidelity of the\nfine-grained details can be significantly enhanced. How-\never, such methods significantly reduce the motion inten-\nsity, due to the fact that each frame contains exactly the\nsame given image prior in the noise, making the initial noise\nrandom component decrease, which results in a reduction of\nthe motion intensity.\nIn this work, to address the challenges presented above,\nour work presents an image-to-video generation model that\nachieves high fidelity and coherent motion without relying\non noise priors. Specifically, we concatenate the given im-\nage at the input, while also injecting high-level semantic\ncues through cross-attention to improve the consistency of\nthe video generation with the given image. During training,\nwe employ zero terminal Signal-to-Noise Ratio[13, 21] and\nv-prediction strategies[31], which we analyse can signifi-\ncantly improve the stability of generation without a noisy\nprior. Moreover, our framework can be easily adapted to\nthe video frame prediction task by predicting the follow-\ning video frames, given the preceding frames, and through\niterative generation, which enables the generation of long\nvideos.\nFinally, we maintain a fixed T2I model during\ntraining, only adjusting the added temporal layer and input\nlayer parameters, so it can be combined with the commu-\nnity\u2019s personalised T2I model and the controllable models\nfor more flexible video generation.\n2. Related Work\nDiffusion Models. Due to the outstanding generative ca-\npabilities and controllability, Diffusion Probabilistic Model\n(DPM) [17] and its variants have recently ascended to a\ndominant status within the field of generative modeling.\nDiffusion models [9, 17, 33] accomplish the iterative re-\nfinement process by learning to progressively denoise sam-\nples from the normal distribution, while subsequent works\n[29, 34] reduce the computational burden by further lever-\naging learned representations in the latent space . For text-\nto-image generation models [7, 24, 28\u201330], it is common\nto use a language model such as CLIP [25] and T5 [27] as\na text encoder and introduce it by means of cross-attention\n[35] to improve the alignment of text and images. Beyond\nnatural language inputs, the use of additional image condi-\ntions to guide the layout of the generated images [20, 23, 42]\nalso becomes an active area of research.\nText-to-Video Synthesis with Diffusion Models. As\ndiffusion models have prospered in image generation tasks,\nthe use of diffusion models for video generation has re-\nceived increasing attention. Early attempts [4, 32, 36] fo-\ncused on generating videos from text by adding a time di-\nmension to text-to-image models, allowing them to capture\ntemporal information. AnimateDiff [15] learns a plug-and-\nplay motion module from large-scale video data by keep-\ning the original weights of the text-to-image model fixed.\nTo enhance the usability of the results, some works have\nimproved the quality of generated videos by leveraging the\ndiffusion noise prior [12] or cascading models [18, 38]. Ad-\nditionally, controllable video generation is also an important\narea. Some work have incorporated additional control sig-\nnals like depth maps [10], human poses [22], or a combi-\nnation of multiple conditions [37, 44] to create videos that\nmore accurately meet user needs.\nImage-to-Video Synthesis with Diffusion Models. Re-\ncently, image-to-video generation has been emerging as an\nactive area of research. This field not only focuses on the\noverall quality of generated content but also pays attention\nto the fidelity of the input image and the plausibility of the\nmotion effects. I2VGen-XL [43] achieves high-resolution\nimage-to-video generation by decoupling the tasks of se-\n2\nSL\nTL\n\ud835\udc39!\n\ud835\udc39\"\n\ud835\udc4b#\nE\nImage\nEncoder\nImage Latent\nFrame Mask\nNoise Latent\nC\nConv In\nSL\nTL\nSL\nTL\nSL\nTL\nPrompt: Waves crash against the lighthouse.\nDenoising UNet\n\ud835\udc37\nTL\nTemporal Layer\nSL\nSpatial Layer\nC\nConcat Operation\n\ud835\udc38\n\ud835\udc37\nVAE Encoder\nVAE Decoder\n\ud835\udc6a = \ud835\udfd7\nAdd noise\nFigure 2. The framework of our image-to-video method. During training, only the temporal and input layers are trained, and during testing,\nthe noise latent is a sampled from Gaussian distribution without any reference image prior.\nmantic scene creation and detail enhancement through two\ncascaded models. Stable Video Diffusion [3] leverages text-\nto-video pre-training on a carefully curated dataset to enable\nthe model to learn strong motion priors, which are further\napplied to downstream tasks like image-to-video and multi-\nview synthesis. Emu Video [13] learns to directly generate\nhigh-quality outputs by adjusting the noise scheduling and\nemploying multi-stage training. Some works [6, 45] incor-\nporates additional input channels to bolster control over the\noverall intensity of the video\u2019s motion effects. In our work,\nwe focus not only on the high-fidelity consistency of the\ngiven image but also achieve high-quality motion effects.\nMore importantly, as we have fixed the weights of the spa-\ntial layers, our work can seamlessly integrate with existing\nplugins such as ControlNet [42], LoRAs [19], and stylized\nbase models.\n3. Method\n3.1. Overall Pipeline\nOur overall process is shown in Fig.2, We use the pre-\ntrained T2I model, newly added 1D temporal convolution\nand temporal attention modules after every spatial convo-\nlution and attention layer, with fixed T2I model parameters\nand only training the added temporal layer. Meanwhile, in\norder to inject the image information, we modify the input\nchannel to 9 channels, add the image condition latent and\nbinary mask. Since the input concatenate image informa-\ntion is only encoded by VAE, it represents low-level infor-\nmation, which contributes to the enhancement of fidelity of\nthe video with respect to the given image. Meanwhile, we\nalso inject high-level image semantic in the form of cross-\nattention to achieve more semantic image controllability.\n3.2. Image Information Injection\nCurrently, with the rapid development of diffusion mod-\nels, text-to-image generation has enabled the generation of\nhighly aesthetic creative images. Therefore, achieving high-\nquality image-to-video video generation based on a given\nimage is a popular research topic. In which, consistency\npreservation with a given image and video motion coher-\nence in I2V tasks are usually trade-offs. In our approach,\nimages are injected at two separate positions. As shown in\nFig.2, we encode the image through VAE encoder to obtain\nthe low-level representation, formulated as Fi, and the cor-\nresponding input frame mask Fm, Fi and Fm are concate-\nnated with the Gaussian noise Xt in the channel dimension,\ndescribed by the formula:\nX\n\u2032\nt = Concat(Xt, Fm, Fi),\nWhere X\n\u2032\nt is the final input to the UNet with channel dimen-\nsion C = 9. The image condition Fi contains such infor-\nmation that can recover fine-grained image details, which is\nextremely important for the fidelity of the generated video\nto the given image.\nIn addition, we simultaneously encode the input image\nwith a CLIP image encoder[26] to yield the high-level se-\nmantic representation in patch granularity, which is fol-\nlowed by a linear projection layer for dimension transforma-\ntion and injected through the added cross-attention layer. In\nthe detailed implementation, we used IP-Adapter[41] based\non SD1.5[29] pre-trained model weights for training.\n3\n\u2026\n\u2026\n\u2026\n\u2026\nGiven Frames\nPredicted Frames\nImage\nLatents\nBinary\nMasks\n\u2026\n\u2026\nNoise\nLatents\nL\nT - L\nUNet\nFigure 3. Illustration of video prediction. Given a length L se-\nquence of video frames, predicting the subsequent frames of T \u2212L\nis performed by making adaptation only at the input layer, with no\nadditional adjustment of the model. And T denotes the maximum\nsequence of frames supported by the model.\n3.3. Video Frames Prediction\nLong video generation is a significant challenge in video\ndiffusion models due to the constraints of GPU memory.\nWe extend our approach to the task of video frame predic-\ntion by implementing long video generation in an iterative\nmanner by predicting subsequent frames given the preced-\ning frames. Specifically, the input image conditions, image\nlatents Fi and frame mask Fm in Fig.2, can be flexibly re-\nplaced with any several frames from a given video, as il-\nlustrated in Fig.3. Typically, for video frame prediction,\nwe input the first L = 8 frames to the model and predict\nthe subsequent T \u2212 L = 16 frames. Apart from that, the\nmodel structure does not require any other changes. We use\nthe well-trained I2V model as an initialisation, and train it\nwith only a little number of videos to converge quickly and\nachieve relatively stable long video generation.\n3.4. Training and Inference\nWe employ Stable Diffusion 1.5 as our foundational\nText-to-Image (T2I) model and initialize the temporal at-\ntention layers with AnimateDiff. We use our 15M internal\ndataset for training, where each video is about 10-30 sec-\nonds in length and the textual description of the video is\nalso fed into the model. In addition, we employ zero termi-\nnal Signal-to-Noise Ratio (SNR)[21] and v-prediction[31]\nwhen training, which in our practice proved that they are\neffective on the stability of video generation. The input size\nof our model is 512 \u00d7 512 and contains 24 frames.\nDuring\ninference,\nWe\nperform\nClassifier-Free\nGuidance[16] with both image and text conditional\ninjection.\nOur empirical validation indicates that image\nconditions\ncombined\nwith\ntext\nprompts\nsignificantly\nincrease the stability of the generated output.\n4. Experiments\n4.1. Quantitative Comparisons\nEvaluation Setting. We follow the AIGCBench[11] set-\nting for evaluation, which provides more comprehensive\nevaluation criterions in the I2V task. We compare recent ex-\ncellent methods in the I2V domain such as VideoCraft[5],\nI2VGEN-XL[43], SVD[3], and also commercial methods\nsuch as Pika[2] and Gen-2[1].\nWe calculate metrics on\nmultiple dimensions commonly used in the field, includ-\ning 1).Image Consistency. We calculate Structural Similar-\nity Index Measure(SSIM)[39] between the first frame of the\ngenerated video and the reference image to evaluate the gen-\neration fidelity with the given image. 2).Temporal Consis-\ntency. We compute the image CLIP[26] score(ICS) between\nadjacent frames of the generated video to measure temporal\ncoherence and consistency. 3). Video-Text Alignment. We\nuse the CLIP[26] score of the video frames to the prompt\nto measure the degree of video-text alignments. 4). Mo-\ntion Intensity. To avoid over-optimising the image fidelity\nin preference to generating static videos, we use RAFT cal-\nculate the flow score between adjacent frames of the gener-\nated video to represent the magnitude of the motion inten-\nsity. 5). Video Quality. We utilize disentangled objective\nvideo quality evaluator(DOVER)[40] to evaluate the video\nquality.\nQuantitative Results. The quantitative evaluation re-\nsults are shown in Table 1, comparing with other excellent\nopen source methods, including VideoCrafter[5], I2VGEN-\nXL[43] and SVD[3], we achieve the best scores in all eval-\nuation dimensions, especially in image consistency.\nBe-\nsides, comparing with the commercial methods, we also\nshow advantages in several dimensions, especially the mo-\ntion intensity score. AtomoVideo shows greater motion in-\ntensity(RAFT) with competitive temporal consistency com-\npared to Pika[2] and Gen-2[1], while they tend to generate\nstatic videos. Further, it is worth noting that we are slightly\nlower than commercial methods in image consistency and\nvideo quality, we analyse two reasons for this, one is the\ninfluence of the resolution of the generated video, and the\nother is that they may employ a better base model, whereas\nwe utilize SD-1.5 and fix the parameters, and we believe\nthat we can obtain a superior video by employing more ad-\nvanced base models.\n4.2. Qualitative Samples\nIn this section, we show some qualitative samples in\nFig.4.\nWe compare our method with SVD[3], the com-\nmercial methods Pika[2] and Gen-2[1], which all achieve\n4\nMethods\nImage\nConsistency\nTemporal\nConsistency\nVideo-Text\nAlignment\nMotion\nEffects\nVideo\nQuality\nSSIM\u2191\nICS\u2191\nCLIP Score\u2191\nRAFT\u2191\nDOVER\u2191\nVideoCrafter[5]\n0.417\n0.9906\n0.259\n0.384\n0.601\nI2VGEN-XL[43]\n0.417\n0.9795\n0.248\n1.271\n0.552\nSVD[3]\n0.615\n0.9830\n0.273\n2.778\n0.726\nPika[2]\n0.739\n0.9974\n0.274\n0.192\n0.747\nGen-2[1]\n0.835\n0.9972\n0.274\n0.497\n0.824\nOurs\n0.759\n0.9938\n0.279\n3.124\n0.804\nTable 1. Quantitative comparison of AtomoVideo with other methods.\nInput\nOurs\nGen-2\nPika\nSVD\n(a)\n(a)\nPrompt:\nA small, plucky robot \nnamed WALL-E is \ncompacting trash into neat \ncubes on a deserted, \ntrash-covered Earth, his \neyes gleaming with \ncuriosity at any treasures \nhe finds.\nPrompt:\nBehold an adventurous \nastronaut in the throes of \ngracefully dancing under \nthe moonlight surrounded \nby a blooming enchanted \ngarden, envisioned as a \nstunning 3D render.\nFigure 4. Samples comparison with other methods. We compare the SVD[3], Pika[2] and Gen-2[1], where AtomoVideo maintains better\nstability and greater motion intensity.\nrelatively high fidelity with the given image. However, in\nour experiments, pika is more preferred to generate static\nvideos, while the results generated by Gen-2[1] and SVD[3]\nare susceptible to artifacts when the subject undergoes a\nlarge motion change.\nOverall, compared to other meth-\nods, we achieve more coherent and stable temporal con-\nsistency when generating videos with greater motion inten-\nsity. We train our model on 512 \u00d7 512 size and more exam-\nples are shown in Fig 5, but we find also great generalisa-\ntions on other resolutions, e.g. some samples generated on\n5\nInput image\nGenerated videos\nPrompt: A panda standing on a surfboard, in the ocean in sunset, 4k.\nPrompt: Flying through an intense battle between pirate ships in a stormy ocean.\nPrompt: fireworks display in the sky at night.\nPrompt: a stack of dried leaves burning in a forest. \nPrompt: living room with party decoration.\nPrompt: close up video of strawberry plant.\nPrompt: ferry under bridge on river near city in malaysia.\nFigure 5. More samples with 512 \u00d7 512 size.\n6\nInput image\nPrompt: Pixel art of a knight doing magic in a hidden lab.\nPrompt: Oil painting-style image of a panda experimenting in a magical garden.\nPrompt: Waves are crashing against a rocky shoreline.\nPrompt: A lighthouse is beaming over turbulent seas at night.\nPrompt: 3D rendered scene of an alien exploring a cave in a dense jungle.\nPrompt: Cartoon of Lily giggling and blowing bubbles in a sunny meadow.\nPrompt: A fawn's first steps in a forest clearing, watched by its mother.\nPrompt: Crowds are watching spellbound as fireworks explode overhead, in cartoon style.\nGenerated videos\nFigure 6. More samples with 1280 \u00d7 720 size.\n7\n0 s\n2 s\n4 s\n6 s\nInput image\nPrompt: jetliner breaking through clouds\nPrompt: pug, in skateboard, on the road\nFigure 7. Samples of long video generation. The left is the input\nimage and the right is the generated video of 7s length.\nInput image\nPrompt:\nSimba is standing in \nfront of the frame\n(a)\n(b)\nFigure 8. Comparison using SD-1.5 and epiCRealism models. (a).\nResults generated using the SD-1.5 model, consistent with train-\ning. (b). Results generated using the epiCRealism model, with\nimage-to-video generation injected with more light elements.\n1280 \u00d7 720 size are shown in the Fig 6.\nBesides, as shown in Fig.7, demonstrating the results of\nvideo frame prediction, we achieve longer video generation\nby iterative video frame prediction.\n4.3. Personelized Video Generation\nSince our method freezes the parameters of the base 2D\nUNet and trains only the added parameters, it can be com-\nbined with the popular personalised models in the commu-\nnity. As shown in Figure8, we show the results of combin-\ning our model with epiCRealism, a T2I model that is excel-\nlent for light and shadow generation, and utilizing it for I2V\ngeneration prefers to generate videos with light elements.\nIn this work, since we emphasise more on the fidelity of\nthe generated video with respect to the given image, it is\nnot easy to work in combination with many stylistic models\nsuch as cartoon style.\n5. Conclusion\nIn this work, we present AtomoVideo, a high-fidelity\nimage-to-video generation framework.\nOur method\ngreatly exploits the generative capabilities of the T2I\nmodel and is trained only on the parameters of the added\ntemporal and input layers.\nQualitative and quantitative\nevaluations indicate that our method achieves excellent\nperformance,\nmaintaining\nsuperior\ntemporal\nconsis-\ntency and stability in the case of generating video with\ngreater motion intensity.\nIn the future, we will work\ntowards more controllable image-to-video generation,\nas well as expanding to more powerful T2I base models.\nReferences\n[1] Gen-2. https://research.runwayml.com/gen2.\nAccessed 2023-12-15. 4, 5\n[2] Pika 1.0. https://pika.art/. Accessed 2023-12-15.\n4, 5\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 4,\n5\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2\n[5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 4, 5\n[6] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu\nLiu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real\nimage animation with text-guided motion control.\narXiv\npreprint arXiv:2312.02928, 2023. 2, 3\n[7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 2\n[8] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Fine-grained open do-\nmain image animation with motion guidance. arXiv preprint\narXiv:2311.12886, 2023. 2\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[10] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 2\n[11] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan.\nAigcbench: Comprehensive evaluation of image-to-video\ncontent generated by ai.\nBenchCouncil Transactions on\nBenchmarks, Standards and Evaluations, page 100152,\n2024. 4\n8\n[12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 22930\u201322941, 2023. 2\n[13] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709, 2023. 2, 3\n[14] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan\nDeng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin\nHuang, Pengfei Wan, et al. I2v-adapter: A general image-\nto-video adapter for video diffusion models. arXiv preprint\narXiv:2312.16693, 2023. 2\n[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2\n[16] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[19] Edward J Hu, Yelong Chen, Hanyu He, Chen Li, Weizhu\nHuang, Shuxin Wang, Zhi Liu, Jianfeng Gao, and Heung-\nYeung Shum. Lora: Low-rank adaptation of large language\nmodels. In International Conference on Learning Represen-\ntations, 2022. 3\n[20] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 2\n[21] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed.\nIn Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 5404\u20135411,\n2024. 2, 4\n[22] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 2\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[24] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 4\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 2\n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 2, 3\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[31] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 2, 4\n[32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[36] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2\n[37] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\n9\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2\n[38] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2\n[39] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 4\n[40] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi\nLin.\nExploring video quality assessment on user gener-\nated contents from aesthetic and technical perspectives. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 20144\u201320154, 2023. 4\n[41] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 3\n[42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 3\n[43] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2, 4, 5\n[44] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 2\n[45] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing\nFang, and Kai Chen. Pia: Your personalized image animator\nvia plug-and-play modules in text-to-image models. arXiv\npreprint arXiv:2312.13964, 2023. 2, 3\n10\n"
  },
  {
    "title": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding",
    "link": "https://arxiv.org/pdf/2403.01487.pdf",
    "upvote": "14",
    "text": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nHaogeng Liu * 1 2 Quanzeng You * 3 Xiaotian Han 3 Yiqi Wang 3 Bohan Zhai 3 Yongfei Liu 3 Yunzhe Tao 3\nHuaibo Huang 1 2 Ran He * 1 2 Hongxia Yang * 3\nAbstract\nMultimodal Large Language Models (MLLMs)\nhave experienced significant advancements re-\ncently. Nevertheless, challenges persist in the ac-\ncurate recognition and comprehension of intricate\ndetails within high-resolution images. Despite\nbeing indispensable for the development of ro-\nbust MLLMs, this area remains underinvestigated.\nTo tackle this challenge, our work introduces\nInfiMM-HD, a novel architecture specifically de-\nsigned for processing images of different resolu-\ntions with low computational overhead. This in-\nnovation facilitates the enlargement of MLLMs to\nhigher-resolution capabilities. InfiMM-HD incor-\nporates a cross-attention module and visual win-\ndows to reduce computation costs. By integrating\nthis architectural design with a four-stage training\npipeline, our model attains improved visual per-\nception efficiently and cost-effectively. Empirical\nstudy underscores the robustness and effective-\nness of InfiMM-HD, opening new avenues for ex-\nploration in related areas. Codes and models can\nbe found https://huggingface.co/Infi-MM/infimm-\nhd\n1. Introduction\nThe landscape of Multimodal Large Language Models\n(MLLMs) has been revolutionized by integrating pretrained\nvision encoders with Large Language Models (LLMs) (Han\net al., 2023; Wang et al., 2024; Han et al., 2024), a trend\nexemplified by developments in Flamingo (Alayrac et al.,\n2022), BLIP-2 (Li et al., 2023b), LLaVA (Liu et al., 2023b),\nMiniGPT-4 (Zhu et al., 2023a) and etc. MLLMs can exhibit\nemergent vision-language capabilities (Yang et al., 2023).\nFor example, they can write codes according to images, con-\nvert plots in images into Markdown format tables, and per-\n*Equal contribution 1MAIS & CRIPAC, Institute of Automa-\ntion, Chinese Academy of Sciences, China. 2School of Artificial\nIntelligence, University of Chinese Academy of Sciences, Bei-\njing, China. 3ByteDance, Inc.. Correspondence to: Haogeng Liu\n<liuhaogeng22@mails.ucas.ac.cn>.\n67.7\n67.0\n51.3\n50.5\n63.5\n63.3\n63.1\n65.5\n62.6\n61.3\n58.9\n83.6\n71.6\n70.6\n69.4\n70.7\n68.1\n67.6\n61.3\n83.4\n82.0\n80.7\n87.9\n87.2\n86.2\n85.9\n71.6\n67.7\n65.9\n37.6\n36.4\n32.9\n37.4\n37.1\nTextVQA\nSQA-IMG\nOK-VQA\nGQA\nIconQA\nSTVQA\nInfiMM\nPOPE\nMMMU\nMMbench\nVQAv2\nFigure 1. InfiMM-HD showcases superior performance across var-\nious tasks, thanks to its enhanced training pipeline and high-\nresolution inputs, consistently outperforming recent methods in\ndownstream tasks.\nform web browsing, by leveraging combined single-modal\ncapabilities from pretrained vision encoders and LLMs.\nEffective integration of pretrained vision encoders with\nLarge Language Models in MLLMs relies on carefully de-\nsigned vision-language bridging modules. These modules\naddress two critical aspects: the transformation and align-\nment of visual tokens to LLM-compatible formats, and the\nsubsequent utilization of these transformed tokens.\nFor the transformation and alignment of visual tokens, mod-\nels such as Flamingo (Alayrac et al., 2022) and BLIP-2 (Li\net al., 2023b) employ Perceiver-Resampler/Q-Former tech-\nniques for transforming visual tokens, offering flexibility\nand nuanced alignment with language counterparts but at\nthe cost of higher computational demands and potential in-\nformation loss (Cha et al., 2023). Conversely, models such\nas LLaVA and MiniGPT-v2 take a different route, using\nsimpler Multi-Layer Perceptron (MLP) approaches. While\nthis reduces computational complexity and the number of\nlearnable parameters, it might not capture the full intricacies\nof visual data.\n1\narXiv:2403.01487v1  [cs.CV]  3 Mar 2024\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nInfiMM-HD: The price of the drink \nGloki is 5.00.\nUser: What is the price of Gloki?\nInfiMM-HD: 30.07.2007\nInfimm-HD: Yellow flowers.\nInfimm-HD: A green street sign.\nUser: What is the Entspricht in \nEuro in the recipe?\nInfiMM-HD: The picture shows a small action figure of a female \ncharacter holding a gun, standing in front of an advertisement for a \nvideo game called \"I-KOHL Pheromonic Make-Up.\" The advertisment \nfeatures a woman in a red and white outfit, holding a weapon, and \nthe text \"In War and Love.\" The background is a blue gradient.\nDetailed description\nUser: Please provide a short description for this region: \n[0.06,0.7,0.27,0.79].\nRef-Region \nDescription\nUser: Please provide a short description for this region: \n[0.17,0.07,0.29,0.11].\nDetailed info extraction\nInfiMM-HD: The Entsprich in \nEuro is 36.33.\nUser: What is the day in the \nrecipe? Answer in format of \nxx.xx.xxxx.\nFigure 2. Example outputs by InfiMM-HD, highlighting the model\u2019s adeptness in fine-grained visual perception.\nThe integration phase of visual and language tokens is\nequally vital. Flamingo-style architectures employ cross-\nattention mechanisms, facilitating intricate interactions be-\ntween token types without expanding the token sequence\nlength. This approach efficiently manages the computational\nload. LLaVA-style models, however, use a direct concatena-\ntion method, which, while straightforward, increases token\nsequence length and computational complexity.\nThough different, both architecture utilized low image res-\nolution due to pretrained Vision Transformer (ViT) en-\ncoders (Jiang et al., 2023; Radford et al., 2021; He et al.,\n2021). Low resolution suffices for basic image-level seman-\ntic understanding but falls short for detailed, region-level\nanalysis. Recent efforts (Wang et al., 2023; Li et al., 2023c;\nLin et al., 2023) aim to enable MLLMs to handle higher-\nresolution images. However, significant challenges remain,\nprimarily because the computational demands tend to in-\ncrease quadratically in relation to the sequence length for\nlarger images. For instance, increasing the image resolution\nfrom 224 \u00d7 224 (Dosovitskiy et al., 2021) to 448 \u00d7 448\nmultiplies the self-attention computation by 16 times.\nTo address these challenges, we introduce InfiMM-HD, an\ninnovative MLLM architecture designed for processing high-\nresolution image. InfiMM-HD innovatively merges method-\nologies from both Flamingo and LLaVA styles in MLLMs.\nFor the transformation and alignment phase, it adopts an\nMLP-based approach, akin to LLaVA, effectively transform-\ning and aligning visual tokens into formats compatible with\nLLMs. This strategy balances computational efficiency\nwith precise processing. In the integration phase, InfiMM-\nHD utilizes a cross-attention mechanism, reminiscent of\nFlamingo-style MLLMs, to seamlessly incorporate visual\ntoken features with language tokens. This approach miti-\ngates the computational challenges associated with longer\ntoken sequences as previously mentioned. Notably, while\nthe exploration of high-resolution image input capabilities in\nFlamingo-style MLLMs is still an emerging area of research,\nInfiMM-HD marks a significant, pioneering advancement\nin this domain, blending the best of both worlds to enhance\nMLLM performance with high-resolution visual inputs.\nTo overcome the resolution constraints of pretrained vision\nencoders, InfiMM-HD is strategically trained in four stages,\nenhancing resolution handling while maintaining vision-\nlanguage alignment. Initially, the model is pretrained with\n2\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\n224 \u00d7 224 resolution images for efficient visual-language\nalignment. Subsequently, it continues pretraining with inter-\npolated positional embeddings for 448 \u00d7 448 images from\nmultiple datasets, keeping the LLMs frozen. This is fol-\nlowed by training with full-resolution images, resized to the\nnearest multiple of 448\u00d7448, added with 2D positional em-\nbeddings and crop to multiple subimages. In the final stage,\nthe model undergoes visual instruction fine-tuning, freezing\nthe vision encoder and making LLM trainable to enhance\ninstruction-following capability. This structured training\napproach is crucial for the model\u2019s adaptability and perfor-\nmance across various input resolutions. The contributions\nof our work can be summarized as follows:\n\u2022 We present InfiMM-HD, a pioneering MLLM that em-\nploys an MLP-based approach for visual token transfor-\nmation and alignment, coupled with a Flamingo-style\ncross-attention mechanism for enhanced and efficient\nintegration of transformed visual and language tokens.\nIt is uniquely designed to seamlessly process high-\nresolution image inputs.\n\u2022 We present a four-stage training pipeline that effec-\ntively achieves a high-resolution Multimodal Large\nLanguage Model with reduced training cost, from ini-\ntial low-resolution pretraining stage, to continue pre-\ntraining stage for knowledge injection and alignment,\nto dynamic resolution adaption stage for high resolu-\ntion adoption and finally go through visual instruction\nfine-tuning stage.\n\u2022 Experiments conducted across diverse benchmarks\nshowcase the remarkable proficiency of our model in\nthe realm of visual perception. Additionally, com-\nprehensive ablation studies underscore the distinctive\nsuperiority of our design within the context of cross-\nattention-style Multimodal Language Model architec-\ntures.\n2. Related Work\nThe advent of Large Language Models (LLMs) has cat-\nalyzed the development of MLLMs. Flamingo (Alayrac\net al., 2022) integrates pretrained language models into the\nMLLM paradigm, employing a gated-cross attention mech-\nanism to fuse visual information into textual sequences. In\ncontrast, BLIP-2 (Li et al., 2023b), MiniGPT4 (Zhu et al.,\n2023a), and LLaVA (Liu et al., 2023b) propose a paradigm\nshift, transforming visual signals into soft tokens and di-\nrectly incorporating them into language models. Shikra\n(Chen et al., 2023) concentrates on referential dialogue.\nOtterHD (Li et al., 2023a) fine-tunes Fuyu-8B (Bavishi\net al., 2023) with instructional guidance, enabling ViT-free\nMLLMs.\nDespite the progress we have seen, some problems still\nexist. (Zhai et al., 2023) points out that misalignment be-\ntween visual representation and language causes halluci-\nnation. (Zhang et al., 2023) reveals that enhancing the\ninput resolution will significantly increase MLLM\u2019s Optical\nCharacter Recognition (OCR) ability. More and more ex-\nperiments suggest the presence of an information bottleneck\nin contemporary vision encoders (Tong et al., 2024; Zhai\net al., 2023). The resolution of the image stands out as a crit-\nical factor that constrains the capacity for visual processing.\nThe study by (Tong et al., 2024) highlights that contempo-\nrary MLLMs still face systematic challenges, particularly in\naligning visual and textual modalities effectively.\nThere are some works trying to solve the problem. SPHINX\n(Lin et al., 2023), introduced by Lin et al., employs a multi-\nlayer perception (MLP) to establish connections between\nvisual signals and language. This model leverages mul-\ntiple vision encoders, amalgamating their output features\nto construct a robust visual representation. To deal with\nhigh resolution image input, SPHINX breaks down input\nhigh-resolution images into smaller sub-images and then\nconcatenate visual tokens directly with text tokens. It in-\ntroduces limitations related to increasing input sequence\nlengths for the Large Language Model (LLM).\nThe Monkey model (Li et al., 2023c) addresses this chal-\nlenge by incorporating a shared resampler. This approach\ninvolves compressing each input subimage using the resam-\npling technique from Flamingo (Alayrac et al., 2022) and\ndirectly concatenate the visual tokens with text sequence, ef-\nfectively upscaling the input image resolution to 1344\u00d7896.\nHowever, the reliance on learnable queries within the per-\nceiver architecture for extracting and compressing infor-\nmation from the raw output of the vision encoder raises\nconcerns about the model\u2019s adaptability across diverse ap-\nplication scenarios.\nWe assert that the information compression process should\nintricately intertwine with contextual instructions, allowing\nfor discernment of pertinent details essential for instruction\ncompletion. We introduce InfiMM-HD, which establishes\nconnections between vision and language through cross at-\ntention mechanism. This departure from the reliance on\nlearnable queries aims to enhance the adaptability and appli-\ncability of the model across a broader spectrum of scenarios-\nmore detailed vision perception. Besides, it enables the\nmodel to consume high-resolution images at lower cost than\npreviously proposed methods.\n3. Methods\nIn this section, we introduce InfiMM architecture and pro-\npose a training pipeline for elevating MLLM\u2019s input image\nresolution with reduced cost. To the best of our knowl-\n3\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nDistribution of Image Sizes\nFrequency (log scale)\n17\n8411004\n1830\n8130\n261k 325k\n310\n5644\n146\n28k\n24\n247\n0\n200\n400\n600\n800\n1000\n1200\nImage Size\nFigure 3. Visualization of the distribution of image sizes from the\nLLaVA 665k dataset indicates a predominant clustering of resolu-\ntions between 500-700, mixed with some high-resolution examples.\nDynamic resolution utilization during training is key for efficient\nresource management.\nedge, we are the pioneers in achieving HD MLLM using the\nFlamingo-style architecture.\n3.1. Model Architecture\nThe proposed model consists of three components: a Vision\nTransformer Encoder, a Gated Cross Attention Module, and\na Large Language Model. The comprehensive architecture\nis elucidated in Figure 4. While the illustration showcases a\nsingle image, it is imperative to note that follow flamingo\u2019s\ndesign, our module can also deal with multiple image as\ninput. Following prior work (Li et al., 2023b; Wang et al.,\n2023), for the Vision Transformer, we employ EVA2-CLIP2-\nE (Sun et al., 2023), utilizing the output from the penulti-\nmate layer as the extracted vision features. The Gated Cross\nAttention Module leverages text hidden states as queries,\nand vision features as keys and values. Different from the\ngating methodology introduced in Flamingo (Alayrac et al.,\n2022), we incorporate an element-wise tanh gating mech-\nanism for activation. The language model in this study is\ninstantiated using Vicuna (Chiang et al., 2023).\nTo ensure an effective assimilation of visual information, the\nGated Cross Attention Module is strategically inserted every\nfour layers between the decoder layers of Large Language\nModel. This decision stems from our empirical observation\nthat inserting the module every two layers results in ap-\nproximately 50% of the gates weights near 0, rendering the\ncross-attention module ineffective. The model showcases\nan impressive aggregate of 18 billion parameters, intricately\nallocated among three key components: the Vision Trans-\nformer (4.4 billion), the Large Language Model (12.9 bil-\nlion), and the Gated Cross Attention Module (approximately\n0.7 billion).\nDuring this investigation, we depart from the conventional\nparadigm of the LLaVA-style structure. This deviation is\nessential due to its compromised compatibility with high-\nresolution images, as demonstrated in previous studies (Li\net al., 2023c; Lin et al., 2023). Notably, the processing of\nan image with dimensions 1344 \u00d7 1344 yields an extensive\ntoken sequence comprising 9217 tokens when employing\na patch size of 14. Despite the capability of Large Lan-\nguage Models (LLMs) to accommodate sequence lengths\nup to 32k, the utilization of 9k tokens per image inherently\nimposes constraints on the performance of Multimodal Lan-\nguage Models (MLLMs), particularly in scenarios involving\nmultiple images. This limitation is consequential, underscor-\ning the necessity for alternative architectural considerations.\nThese considerations aim to tackle the challenges posed\nby high-resolution image processing within the context of\ncontemporary language models. We adopt a cross-attention\nmodule for the integration of visual information at a re-\nduced dimensionality of 768. This method, in contrast to the\nLLaVA-style architecture, incurs significantly lower com-\nputational costs while accommodating extended sequences.\nMeanwhile, our experiments demonstrate its effective of\nextracting visual information.\nTable 1. Details on the training data of CPT and DRA.\nTask\nDataset\nSamples\nImage Caption\nCOCO Caption (Chen et al., 2015)\n205k\nTextCaps (Sidorov et al., 2020)\n55k\nVizWiz Caption (Gurari et al., 2020)\n55k\nGeneral VQA\nVQAV2 (Antol et al., 2015)\n443k\nOKVQA (Marino et al., 2019)\n9k\nVizWiz VQA (Gurari et al., 2018)\n20k\nGQA (Hudson & Manning, 2019)\n471k\nA-OKQA (Schwenk et al., 2022)\n17k\nText-oriented VQA\nTextVQA (Singh et al., 2019)\n34k\nOCRVQA (Mishra et al., 2019)\n166k\nSTVQA (Biten et al., 2019)\n26k\nDocVQA (Mathew et al., 2021)\n63k\nLLaVAR (Zhang et al., 2023)\n16k\nRegion Description\nVG (Krishna et al., 2017)\n429k\nTotal\n-\n2.00m\n3.2. Training Details\nWe have established a four-stage training procedures for im-\nproving MLLMs\u2019 capability of processing high-resolution\nimages, as shown in Figure 5. These stages are denoted as\nthe Pretraining (PT), Continue Pretraining (CPT), Dynamic\nResolution Adaption (DRA), and Instruction Finetuning\n(IFT).\nPretraining Stage (PT): This stage is mainly for initially\n4\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\n Mike Ehrmann.\nPOS(0, 0)\nPOS(0, 1)\nVision \nEncoder\n448\u00d7448\nPOS(1, 0)\nPOS(1, 1)\nWhat is the text in the image?\nLM Block 1\nGated Cross-Attn 1\n...\nVision \nEncoder\nVision \nEncoder\nVision \nEncoder\nVision \nEncoder\nLM Block n\nGated Cross-Attn n\nGlobal Image\nLocal Images\nFigure 4. The architectural framework of InfiMM-HD is outlined, where POS(i, j) represents the positional embedding of local patches,\nwith (i, j) indicating their position within the overall image. The model progresses through various training stages, each characterized by\nselectively training different modules. A detailed explanation of this strategic approach will be elaborated in the following sections.\naligning vision features and language features. During this\nstage, both the Vision Transformer (ViT) and Large Lan-\nguage Model (LLM) are frozen, only the Gated Cross At-\ntention module is trainable. In this stage, all of the images\nare resized to 224 \u00d7 224 to keep the low training cost.\nContinue Pretraining Stage (CPT): In this stage, we em-\nploy bilinear interpolation of positional embedding to extend\nthe ViT\u2019s capability to process image of resolution 448\u00d7448.\nThe ViT and Gated Cross Attention modules are trainable.\nTraining datasets mainly focus on image captioning and vi-\nsual question-answering tasks. Detailed information about\ntraining datasets is listed in Table 1.\nDynamic Resolution Adaption (DRA): In Figure 3, the\noutcomes illustrating the sizes of images in the LLaVA-665k\ndataset (Liu et al., 2023a). Upon meticulous examination of\nthe dataset, it becomes apparent that not all images exhibit\nresolutions reaching up to 1344. In contrast to conventional\npractices of uniformly resizing images to a fixed resolu-\ntion, such an approach incurs unnecessary computational\ncosts, while dynamic image resolution may be cost friendly.\nTo facilitate dynamic resolution inputs, we incorporate the\n2D position embedding method proposed in (Wang & Liu,\n2021) for individual sub-images. We adopts dynamic input\nimage resolution, ranging from 448 \u00d7 448 to 1344 \u00d7 1344,\nduring training. Subsequently, the resized image is divided\ninto sub-images of 448 \u00d7 448. We also keep an original\nimage thumbnail of 448\u00d7448. Finally we use ViT to extract\nfeatures from each sub-image and original image thumbnail,\nconcatenated directly to form the final vision feature. We\nuse the same training datasets as CPT stage, and keep both\nthe ViT and Gated Cross Attention modules trainable.\nInstruction Finetuning Stage (IFT): In this final stage, our\ngoal is to make the model better follow user instructions\nwithout losing high-resolution visual perception capabil-\nity. Thus, we keep the ViT frozen, but let the Gated Cross\nAttention modules and LLM trainable.\nThe proposed four-stage training pipeline is key to stabilize\ntraining while elevate input image resolution gradually.\n4. Experiments\nIn this section, we first discuss about experiment setup.\nThen we show main results of InfiMM-HD and list series\nof ablation studies to prove the importance of our proposed\nmodules in InfiMM-HD.\n4.1. Setup\nTraining Dataset. For Pretrainin (PT) stage, training data\nincludes both image-text pairs and interleaved image-text.\nImage-text pair data includes 140M samples filtered from\nLAION-2B (Li et al., 2023b), COYO (Byeon et al., 2022),\nand Laion-coco (Schuhmann et al.). Interleaved image-\ntext data is sampled from MMC4 (Zhu et al., 2023b) and\nOBELISIC (Laurenc\u00b8on et al., 2023) randomly with 50% for\ntraining.\nThe datasets utilized in the Continue Pretraining (CPT) and\nDynamic Resolution Adaption (DRA) stages are enumer-\nated in Table 1. During the Instruction Finetuning (IFT)\nstage, we amalgamate datasets from LLaVA-665k (Liu et al.,\n2023a), LLaVAR (Zhang et al., 2023), TextVQA (Singh\net al., 2019), and ScienceQA (Lu et al., 2022). This fusion is\nmotivated by the fact that the original LLaVA-665k dataset\n5\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nViT\nText\nDecoder \nLayer1\nGated Cross \nAttn1\nDecoder \nLayern\nGated Cross \nAttnn\n...\nStage1: Pretraining\nViT\nResized \nImage (448)\nText\nDecoder \nLayer1\nGated Cross \nAttn1\nDecoder \nLayern\nGated Cross \nAttnn\n...\nStage2: Continue Pretraining\nViT\nText\nDecoder \nLayer1\nGated Cross \nAttn1\nDecoder \nLayern\nGated Cross \nAttnn\n...\nStage3: Dynamic Resolution\nAdaption\nViT\nText\nDecoder \nLayer1\nGated Cross \nAttn1\nDecoder \nLayern\nGated Cross \nAttnn\n...\nStage4: Instruction Finetuning \nResized \nImage (224)\nResized Image and \nsubimages (448)\nResized Image and \nsubimages (448)\nFigure 5. The four stages of InfiMM-HD training pipeline. Each stage is characterized by distinct trainable modules, datasets, and\nthe resolution of images input to ViT. Our experimental findings confirm the efficacy of our approach, demonstrating the benefits of\nprogressively transitioning from 224x224 to high-resolution images.\nQ: What is the characters on the left top of the image?\nA: ijd2sa5d\nQ: What is the color of the characters on the left top?\nA: Black\nFigure 6. Illustration of data augmentation: Introducing randomly\ngenerated characters with diverse colors into arbitrary regions of\nthe image. Corresponding questions are then generated to comple-\nment the original query.\nlacks text-oriented samples. Consequently, we supplement\nthis deficiency by incorporating additional data from these\ndiverse sources.\nIn the IFT stage, we primarily utilize the LLaVA-665k\ndataset (Liu et al., 2023a) by default. Alongside, we in-\ncorporate additional datasets such as TextVQA, LLAVAR,\nand ScienceQA to enrich our instruction tuning dataset.\nText-oriented Data Augmentation. Due to a scarcity of\ntext-oriented data, we employ a straightforward but effective\ndata augmentation methodology. Concretely, this process\ninvolves the incorporation of scene-irrelevant characters into\nimages, followed by the generation of corresponding ques-\ntions. In this context, we randomly introduce two sets of\ncharacters, each positioned at the left-top and right-bottom\ncorners of the image, respectively, with distinct colors. The\nquestions generated include inquiries such as \u201cWhat char-\nacter is situated at the left-top of the image?\u201d, \u201cWhat is the\ncolor of the characters located at the right-bottom of the\nimage?\u201d, and \u201cHow many characters are present in the left-\ntop region of the image?\u201d. It is imperative to note that this\ndata augmentation technique is exclusively applied to the\nGQA dataset, resulting in the generation of approximately\n100,000 question-answer pairs. Remarkably, we observe the\neffectiveness of this augmentation approach in enhancing\nour model\u2019s proficiency in text recognition. Examples can\nbe found in Figure 6.\nTraining Details.\nThe training process was facilitated\nthrough the utilization of deepspeed (Aminabadi et al.,\n2022), and the FusedAdam optimizer was selected to or-\nchestrate optimization tasks. Additional intricacies related\nto the experimental configurations are thoroughly delineated\nin the accompanying Appendix 8.\nEvaluation. We evaluate InfiMM-HD across a diverse ar-\nray of VQA tasks. For general VQA tasks, we leverage\nbenchmarks such as OKVQA (Marino et al., 2019), VQAV2\n(Antol et al., 2015), GQA (Hudson & Manning, 2019), and\nScienceQA (Lu et al., 2022). These datasets, while not de-\nmanding advanced detail visual perception capabilities from\nthe model, effectively gauge models\u2019 ability to understand\ngeneral scenes and follow user instructions. Moreover, to\nscrutinize our model\u2019s fine-grained detail perception capa-\nbility, we incorporate text-oriented VQA datasets, including\nTextVQA (Singh et al., 2019), STVQA (Biten et al., 2019),\n6\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nTable 2. Results on general VQA task. The table exclusively presents the performance of our generalist model, showcasing its superiority\ncompared with various models.\nModel\nLLM\nIn-house data\nOKVQA\nIconVQA\nGQA\nVQAv2\nScienceQAimg\nFlamingo-80B (Alayrac et al., 2022)\n-\n\u2713\n50.6\n-\n-\n56.3\n-\nPalm-E-12B (Driess et al., 2023)\n-\n\u2713\n60.1\n-\n-\n77.7\n-\nQwen-VL (Bai et al., 2023)\nQwen-7B\n\u2713\n58.6\n-\n59.3\n79.5\n67.1\nQwen-VL-Chat (Bai et al., 2023)\nQwen-7B\n\u2713\n56.6\n-\n57.5\n78.2\n68.2\nCogVLM (Wang et al., 2023)\nVicuna-7B\n\u2713\n58.9\n-\n-\n83.4\n-\nMonkey (Li et al., 2023c)\nQwen-7B\n\u2713\n61.3\n-\n60.7\n80.3\n69.4\nBLIP-2 (Li et al., 2023b)\nVicuna-13B\n\u00d7\n45.9\n40.6\n41.0\n-\n-\nShikra (Chen et al., 2023)\nVicuna-13B\n\u00d7\n47.2\n-\n-\n77.4\n-\nmPLUG-Owl2(Ye et al., 2023)\nLLaMA2-7B\n\u00d7\n57.7\n-\n56.1\n79.4\n68.7\nLLaVA 1.5 (Liu et al., 2023a)\nVicuna-13B\n\u00d7\n-\n-\n63.3\n80.0\n71.6\nSphinx-2K (Lin et al., 2023)\nLLaMA2-13B\n\u00d7\n62.6\n50.5\n63.1\n80.7\n70.6\nInfiMM-HD\nVicuna-13B\n\u00d7\n65.5\n51.3\n63.5\n82.0\n83.6\nTable 3. Evaluation results for text-oriented Visual Question An-\nswering (VQA) task. For STVQA, Monkey randomly samples\ndata from the train set for evaluation.\nModel\nRes\nIn-house data\nTextVQA\nOCRVQA\nSTVQA\nQwen-VL-Chat(Bai et al., 2023)\n448 \u00d7 448\n\u2713\n61.5\n70.5\n-\nMonkey (Li et al., 2023c)\n1344 \u00d7 768\n\u2713\n67.6\n-\n67.7\nUniDoc (Gu et al., 2022)\n224 \u00d7 224\n\u00d7\n40.7\n34.5\n30.8\nDocPedia (Feng et al., 2023)\n2560 \u00d7 2560\n\u00d7\n60.2\n57.2\n45.5\nBLIP-2 (Li et al., 2023b)\n224 \u00d7 224\n\u00d7\n-\n40.6\n-\nLLaVA1.5 (Liu et al., 2023a)\n336 \u00d7 336\n\u00d7\n48.5\n-\n-\nSphinx-2K (Lin et al., 2023)\n768 \u00d7 768\n\u00d7\n61.2\n67.8\n-\nInfiMM-HD (all are piexl-only)\ndynamic\n\u00d7\n70.7\n66.0\n67.0\nand OCRVQA (Mishra et al., 2019). We assess the logical\nreasoning capabilities of our model by employing newly\nintroduced benchmarks, including MM-VET (Yu et al.,\n2023), MME (Fu et al., 2023), MMbench (Liu et al., 2023c),\nInfiMM-Eval (Han et al., 2023), and MMMU (Yue et al.,\n2023). Notably, the MMMU (Yue et al., 2023) presents chal-\nlenging tasks that demand advanced subject knowledge and\ndeliberate reasoning at a collegiate level. These tasks span\ndiverse fields such as physics, chemistry, and biology. The\nMM-VET benchmark assesses the integrated capabilities of\nmodels.\n4.2. Main Results\nWe present evaluation results of general VQA and text-\noriented VQA tasks in this section.\nTable 3 presents results for general VQA tasks. It is essen-\ntial to underscore that the scope of evaluations in OKVQA,\nGQA, and VQAv2 extends beyond the mere assessment\nof models\u2019 visual perception capabilities, they also criti-\ncally examine the models\u2019 ability to utilize prior knowl-\nedge effectively, thereby providing a more comprehensive\nevaluation of models\u2019 overall functionality. Additionally,\nScienceQA (Lu et al., 2022), which comprises 21,000 mul-\ntimodal multiple-choice questions covering a wide range of\nscientific subjects, represents a significant expansion of the\nbenchmarking landscape. In these varied tasks, our model\nhas shown remarkable effectiveness, indicating significant\nperformance improvements. By outperforming its closest\ncompetitor by an average margin of 3.88%, our model not\nonly showcases its superior capacity for integrating mul-\ntimodal information but also its proficiency in leveraging\nextensive prior knowledge to navigate through a diverse\narray of questions successfully.\nIn addition to the general VQA assessment, we further ex-\nplore our model\u2019s detailed visual perception capability by\nevaluating on text-oriented datasets, including TextVQA,\nOCRVQA and STVQA, as demonstrated in Figure A. Quan-\ntitative results are outlined in Table 2. These results un-\nderscore the effectiveness of our proposed high-resolution\nmodel in comprehending intricate textual details within im-\nages.\nWe also evaluate InfiMM-HD on recently proposed\nMLLMs evaluation benchmarks, including MMMU, MM-\nVet, InfiMM-Eval, MMB, MME, and POPE. Compared with\nprevious VQA datasets, these datasets include more com-\nprehensive evaluation aspects of MLLMs, requiring more\ncomplex reasoning capabilities. Evaluation results are out-\nlined in Table 4. It is noteworthy that no single model excels\nacross all benchmarks, with each model exhibiting its unique\nstrengths and limitations. Our proposed model demonstrates\ncommendable overall performance, highlighting its adapt-\nability and competence across diverse disciplines.\n4.3. Ablation Study\nTo elucidate the direct impact of input image resolution on\nmodel performance, we conducted an ablation study. In\nthis investigation, different resolution inputs underwent an\n7\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nTable 4. Results obtained from benchmarks intricately designed for MLLMs with heightened complexity.\nModel\nPOPE\nMMEP\nMMEC\nMMB\nMM-VET\nInfiMM-Eval\nMMMU (val)\nBLIP-2 (Li et al., 2023b)\n85.3\n1293.8\n-\n-\n22.4\n-\n-\nShikra (Chen et al., 2023)\n-\n-\n-\n58.8\n-\n-\n-\nLLaVA 1.5 (Liu et al., 2023a)\n85.9\n1531.3\n295.4\n67.7\n35.4\n32.62\n36.4\nQwen-VL-Chat (Bai et al., 2023)\n-\n1487.5\n360.7\n60.6\n-\n37.39\n35.9\nSphinx-2K (Lin et al., 2023)\n87.2\n1470.6\n326.8\n65.9\n40.2\n-\n32.9\nOurs\n87.9\n1472.3\n329.4\n71.6\n38.9\n37.42\n37.6\nTable 5. Evaluation Results for models trained with different input\nresolutions. Here dynamic means the model supports resolution\nranging from 448 \u00d7 448 to 1344 \u00d7 1344. During inference, we\ndon\u2019t limit resolution to 1344.\nResolution\nGQA\nVQAv2\nOCRVQA\nDocVQA\nTextVQA\n224 \u00d7 224\n60.7\n78.7\n57.6\n25.6\n50.0\n448 \u00d7 448\n61.3\n80.5\n58.7\n44.9\n64.1\ndynamic\n63.5\n82.0\n66.0\n55.1\n70.7\nTable 6. Ablation study Results for position embedding. w/o PE\nmeans removing the positional embedding.\nResolution\nGQA\nVQAv2\nOCRVQA\nDocVQA\nTextVQA\ndynamic (w/o PE)\n63.3\n81.6\n65.4\n53.0\n70.3\ndynamic\n63.5\n82.0\n66.0\n55.1\n70.7\nidentical set of four training stages. To ensure experimental\nequitability, we conduct the same model training without\nDRA by two epochs on the multi-task datasets. The results\nare presented in Table 5. An observable pattern suggests that\nelevating the input images\u2019 resolution boosts the efficacy\nof the model, especially in tasks necessitating an under-\nstanding of textual nuances, such as TextVQA (Singh et al.,\n2019) and DocVQA (Mathew et al., 2021). In contrast, for\ngeneral VQA tasks, a coarse scene overview is often suf-\nficient for accurate question-answering, underscoring the\ncontext-dependent influence of resolution on model efficacy.\nIntuitively, as we cropped the input image into subimages,\nto maintain the spatial information, it is important adding\na position embedding for each subimage. To figure out its\nimpact, we carried out ablation study on 2D position embed-\nding, with the results listed in Table 6. The findings suggest\nthat removing the position embedding slightly influences\nmodel performance. But on DocVQA, it faces apparently\ndegradation. This phenomenon may be attributed to the fact\nthat DocVQA predominantly comprises documents, where\nthe correspondence between various components holds sig-\nnificance, directly reflected through spatial information.\nIn our model, the perceiver resampler is removed compared\nTable 7. Evaluation Results for models trained with different reso-\nlution. PC here means the model has perceriver resampler.\nConfiguration\nGQA\nVQAv2\nDocVQA\nTextVQA\n224 \u00d7 224\n60.7\n78.7\n25.6\n50.0\n224 \u00d7 224 (PC)\n57.7\n79.0\n25.2\n48.9\n448 \u00d7 448\n61.3\n80.5\n44.9\n64.1\n448 \u00d7 448 (PC)\n56.9\n79.5\n30.7\n56.0\nwith the origin Flamingo. To figure out its impact, we in-\nvestigated the significance of the perceiver resampler with\nablation study. A comparison with models incorporating\nthe perceiver resampler is presented in Table 7. As the table\nindicates, the perceiver resampler would become an infor-\nmation bottleneck, constraining the model\u2019s performance\nimprovement with increased resolution.\n4.4. Limitations\nThis study introduces an enhancement to MLLMs to ef-\nfectively process high-resolution images. Results marks\nsignificant advancements in various dimensions. Despite\nthese achievements, certain limitations persist. In practice,\nthe model exhibits deficiencies in tasks oriented towards\ntext comprehension. Our ongoing efforts involve exploring\nmore effective modal alignment strategies while augmenting\nthe dataset to enhance overall model performance.\n5. Conclusions\nIn this work, we present InfiMM-HD, an improvement\nover Flamingo-style MLLM designed for processing high-\nresolution input images. Our approach leverages a cross-\nattention mechanism to seamlessly integrate visual informa-\ntion with language model in a low-dimensional space. To\naddress the formidable computational demands associated\nwith high-resolution images, we partition the input high-\nresolution image into smaller sub-images, each subjected\nto individual processing using a shared Vision Transformer\n(ViT) specifically tailored for relatively lower resolutions.\nAdditionally, we establish a four-stage training pipeline to\n8\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nconstruct the proposed model, ensuring low computational\ncosts are incurred. The proposed model is thus characterized\nby its comprehensive design and an emphasis on minimizing\ncomputational resources.\n6. Broader Impact\nOur model, despite its capabilities, may encounter chal-\nlenges, including the generation of inaccurate information\nand susceptibility to perceptual illusions. Furthermore, akin\nto many machine learning models, it may manifest biases\ninfluenced by underlying value systems. Recognizing these\npotential issues is crucial for ensuring the responsible and\nethical deployment of such technologies.\nReferences\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\nAminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A.,\nLi, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase,\nO., and He, Y. Deepspeed inference: Enabling efficient\ninference of transformer models at unprecedented scale,\n2022.\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,\nZitnick, C. L., and Parikh, D. Vqa: Visual question\nanswering. In International Conference on Computer\nVision (ICCV), 2015.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P.,\nLin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile\nvision-language model for understanding, localization,\ntext reading, and beyond, 2023.\nBavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena,\nA., Somani, A., and Tas\u00b8\u0131rlar, S. Introducing our mul-\ntimodal models, 2023. URL https://www.adept.\nai/blog/fuyu-8b.\nBiten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M.,\nValveny, E., Jawahar, C., and Karatzas, D. Scene text vi-\nsual question answering. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pp. 4291\u2013\n4301, 2019.\nByeon,\nM.,\nPark,\nB.,\nKim,\nH.,\nLee,\nS.,\nBaek,\nW., and Kim, S.\nCoyo-700m:\nImage-text pair\ndataset.\nhttps://github.com/kakaobrain/\ncoyo-dataset, 2022.\nCha, J., Kang, W., Mun, J., and Roh, B. Honeybee: Locality-\nenhanced projector for multimodal llm, 2023.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic, 2023.\nChen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,\nDollar, P., and Zitnick, C. L. Microsoft coco captions:\nData collection and evaluation server, 2015.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale, 2021.\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,\nA., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu,\nT., Huang, W., Chebotar, Y., Sermanet, P., Duckworth,\nD., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,\nM., Greff, K., Zeng, A., Mordatch, I., and Florence, P.\nPalm-e: An embodied multimodal language model, 2023.\nFeng, H., Liu, Q., Liu, H., Zhou, W., Li, H., and Huang,\nC. Docpedia: Unleashing the power of large multimodal\nmodel in the frequency domain for versatile document\nunderstanding, 2023.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang,\nJ., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme:\nA comprehensive evaluation benchmark for multimodal\nlarge language models, 2023.\nGu, J., Kuen, J., Morariu, V. I., Zhao, H., Barmpalios, N.,\nJain, R., Nenkova, A., and Sun, T. Unified pretraining\nframework for document understanding, 2022.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\nAnswering visual questions from blind people. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3608\u20133617, 2018.\nGurari, D., Zhao, Y., Zhang, M., and Bhattacharya, N. Cap-\ntioning images taken by people who are blind. In Com-\nputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part\nXVII 16, pp. 417\u2013434. Springer, 2020.\nHan, X., You, Q., Liu, Y., Chen, W., Zheng, H., Mrini,\nK., Lin, X., Wang, Y., Zhai, B., Yuan, J., Wang, H., and\nYang, H. InfiMM-Eval: Complex Open-Ended Reasoning\nEvaluation For Multi-Modal Large Language Models.\n9\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\narXiv e-prints, art. arXiv:2311.11567, November 2023.\ndoi: 10.48550/arXiv.2311.11567.\nHan, X., You, Q., Liu, Y., Chen, W., Zheng, H., Mrini, K.,\nLin, X., Wang, Y., Zhai, B., Yuan, J., Wang, H., and\nYang, H. Infimm-eval: Complex open-ended reasoning\nevaluation for multi-modal large language models, 2023.\nHan, X., Wang, Y., Zhai, B., You, Q., and Yang, H. Coco is\n\u201dall\u201d you need for visual instruction fine-tuning, 2024.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00b4ar, P., and Girshick, R.\nMasked autoencoders are scalable vision learners, 2021.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 6700\u2013\n6709, 2019.\nJiang, D., Liu, Y., Liu, S., Zhang, X., Li, J., Xiong, H., and\nTian, Q. From clip to dino: Visual encoders shout in\nmulti-modal large language models, 2023.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 123:32\u201373, 2017.\nLaurenc\u00b8on, H., Saulnier, L., Tronchon, L., Bekman, S.,\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\nA. M., Kiela, D., Cord, M., and Sanh, V. Obelics: An\nopen web-scale filtered dataset of interleaved image-text\ndocuments, 2023.\nLi, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., and Liu, Z.\nOtterhd: A high-resolution multi-modality model, 2023a.\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders\nand large language models, 2023b.\nLi, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y.,\nLiu, Y., and Bai, X. Monkey: Image resolution and text\nlabel are important things for large multi-modal models,\n2023c.\nLin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H.,\nQiu, H., Lin, C., Shao, W., Chen, K., Han, J., Huang,\nS., Zhang, Y., He, X., Li, H., and Qiao, Y. Sphinx: The\njoint mixing of weights, tasks, and visual embeddings for\nmulti-modal large language models, 2023.\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines\nwith visual instruction tuning, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning, 2023b.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin,\nD. Mmbench: Is your multi-modal model an all-around\nplayer?, 2023c.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. In The 36th Conference on\nNeural Information Processing Systems (NeurIPS), 2022.\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.\nOk-vqa: A visual question answering benchmark requir-\ning external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition,\npp. 3195\u20133204, 2019.\nMathew, M., Karatzas, D., and Jawahar, C. Docvqa: A\ndataset for vqa on document images. In Proceedings\nof the IEEE/CVF winter conference on applications of\ncomputer vision, pp. 2200\u20132209, 2021.\nMishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A.\nOcr-vqa: Visual question answering by reading text in\nimages. In 2019 international conference on document\nanalysis and recognition (ICDAR), pp. 947\u2013952. IEEE,\n2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nSchuhmann, C., K\u00a8opf, A., Vencu, R., Coombes, T.,\nand Beaumont, R.\nLaion coco: 600m synthetic cap-\ntions from laion2b-en. https://laion.ai/blog/\nlaion-coco/.\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\nMottaghi, R. A-okvqa: A benchmark for visual ques-\ntion answering using world knowledge. In European\nConference on Computer Vision, pp. 146\u2013162. Springer,\n2022.\nSidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps:\na dataset for image captioning with reading comprehen-\nsion. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part II 16, pp. 742\u2013758. Springer, 2020.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 8317\u20138326, 2019.\nSun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-clip:\nImproved training techniques for clip at scale, 2023.\n10\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nTong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S.\nEyes wide shut? exploring the visual shortcomings of\nmultimodal llms, 2024.\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,\nJ., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J.,\nDong, Y., Ding, M., and Tang, J. Cogvlm: Visual expert\nfor pretrained language models, 2023.\nWang, Y., Chen, W., Han, X., Lin, X., Zhao, H., Liu, Y.,\nZhai, B., Yuan, J., You, Q., and Yang, H. Exploring the\nreasoning abilities of multimodal large language models\n(mllms): A comprehensive survey on emerging trends in\nmultimodal reasoning, 2024.\nWang, Z. and Liu, J.-C.\nTranslating math formula im-\nages to latex sequences using deep neural networks with\nsequence-level training. International Journal on Docu-\nment Analysis and Recognition (IJDAR), 24(1-2):63\u201375,\n2021.\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\nWang, L. The dawn of lmms: Preliminary explorations\nwith gpt-4v(ision), 2023.\nYe, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian,\nQ., Zhang, J., Huang, F., and Zhou, J.\nmplug-owl2:\nRevolutionizing multi-modal large language model with\nmodality collaboration, 2023.\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L. Mm-vet: Evaluating large multimodal\nmodels for integrated capabilities, 2023.\nYue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G.,\nStevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B.,\nYuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y.,\nHuang, W., Sun, H., Su, Y., and Chen, W. Mmmu: A\nmassive multi-discipline multimodal understanding and\nreasoning benchmark for expert agi, 2023.\nZhai, B., Yang, S., Xu, C., Shen, S., Keutzer, K., and Li, M.\nHalle-switch: Controlling object hallucination in large\nvision language models, 2023.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D.,\nand Sun, T. Llavar: Enhanced visual instruction tuning\nfor text-rich image understanding, 2023.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models, 2023a.\nZhu, W., Hessel, J., Awadalla, A., Gadre, S. Y., Dodge, J.,\nFang, A., Yu, Y., Schmidt, L., Wang, W. Y., and Choi, Y.\nMultimodal C4: An open, billion-scale corpus of images\ninterleaved with text. arXiv preprint arXiv:2304.06939,\n2023b.\n11\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nA. Training Configuration\nFigure 7. Example of a TextVQA sample, exemplifying its intricate scrutiny of nuanced visual perception.\nWe report the detailed training hyper-parameters and settings of InfiMM-HD in Table 8. Our model was trained with 64\nNVIDIA A100-SXM-80GB. The pretraining stage cost about 72 hours. (If only use the image-text pair, it costs 30 hours.)\nThe remaining three stages cost less than 21 hours.\nTable 8. Details of the training Configuration. For the second stage, we utilize bilinear interpolation to extend the origin ViT to support\nresolution 448. IT means image text pair. And IIT means interleaved image text sequence.\nConfiguration\nPretraining\nContinue Pretraining\nDynamic Resolution Adaption\nInstruction Finetuning\nViT init.\nEVA2-CLIP2-E (res 224)\nEVA2-CLIP2-E (res 448)\nViT from 2nd-stage\nViT from 3rd-stage\nLLM init.\nVicuna-13b\nVicuna-13b\nVicuna-13b\nVicuna-13b\nGated cross-attention init.\nrandom\nInfiMM-HD 1st stage\nInfiMM-HD 2nd stage\nInfiMM-HD 3rd stage\nImage resolution\n224\n448\ndynamic(448-1344)\ndynamic(448-1344)\nViT sequence length\n257\n1024\n1025\n1025\nLLM sequence length\n32 (IT);384 (IIT)\n128\n128\n1024\nOptimizer\nAdamW\nOptimizer hyperparameter\n\u03b21 = 0.9, \u03b22 = 0.95, eps = 1e\u22128\n\u03b21 = 0.9, \u03b22 = 0.999, eps = 1e\u22125\nPeak learning rate\n1e\u22124\n1e\u22125\n1e\u22125\n5e\u22126\nMinimum learning rate\n1e\u22124\n1e\u22126\n1e\u22126\n5e\u22127\nLearning rate schedule\ncosine decay\nWeight decay\n0.1\nGradient clip\n1.0\nTraining steps\n120k\n8k\n8k\n11k\nwarm steps\n6k\n400\n400\n550\nGlobal batch size\n10240 (IT);768 (IIT)\n256\n256\n64\nGradient accumulation steps\n2\n1\n1\n2\nGradient ACC.\n2\n1\n1\n2\nNumerical precision\nbfloat16\nGradient checkpointing\n\u00d7\n\u00d7\n\u2713\n\u00d7\nDeepspeed Zero Stage\n2\nTraining resource\n64 NVIDIA A100-SXM-80GB\nB. Summary of Evaluation Benchmarks\nWe provided a detailed summary of evaluation benchmarks we used and their corresponding metrics in Table 9. Note that\nANLS means Average Normalized Levenshtein Similarity.\n12\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\nTable 9. Details on the test dataset.\nTask\nDataset\nDescription\nSplit\nMetric\nGeneral VQA\nVQAV2\nVQA on natural images\ntest-dev\nVQA Score(\u2191)\nOKVQA\nVQA on natural images but need world knowledge\nval\nVQA Score(\u2191)\nIconQA\nAbstract diagram understanding and visual language reasoning\ntest\nEM(\u2191)\nGQA\nVQA on scene understandig and reasoning\ntest-dev\nEM(\u2191)\nScienceQA\nMultimodal multi choice VQA on science filed\ntest\nAccuracy(\u2191)\nText-oriented VQA\nTextVQA\nVQA about text in natural scene\nval\nVQA Score(\u2191)\nOCRVQA\nVQA on images of book covers\nval\nEM(\u2191)\nSTVQA\nVQA covering reading and reasoning about text\ntest\nANLS(\u2191)\nDocVQA\nVQA on images from documents\ntest\nANLS(\u2191)\nOther Benchmarks\nMME\nEvalutaion for MLLM on perception and cognition\nPerception and Cognition\nAccuracy(\u2191)\nMM-VET\nDialog style VQA on integrated ability\ntest\nGPT-4 score(\u2191)\nMMbench\nComprehensive evalutaion with multi choice VQA\ntest\nAccuracy(\u2191) score(\u2191)\nPOPE\nObject hallucination in MLLM\nadversial\nF1 score(\u2191)\nInfiMM\nComplex Open-ended Reasoning\ntest\nGPT-4 score(\u2191)\nMMMU\nCollege-level multi choice VQA\nval\nAccuracy(\u2191)\n13\n"
  },
  {
    "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
    "link": "https://arxiv.org/pdf/2403.00818.pdf",
    "upvote": "10",
    "text": "DenseMamba: State Space Models with Dense Hidden Connection\nfor Efficient Large Language Models\nWei He * 1 Kai Han * 1 Yehui Tang 1 Chengcheng Wang 1 Yujie Yang 1 Tianyu Guo 1 Yunhe Wang 1\nAbstract\nLarge language models (LLMs) face a daunt-\ning challenge due to the excessive computational\nand memory requirements of the commonly used\nTransformer architecture.\nWhile state space\nmodel (SSM) is a new type of foundational net-\nwork architecture offering lower computational\ncomplexity, their performance has yet to fully\nrival that of Transformers.\nThis paper intro-\nduces DenseSSM, a novel approach to enhance\nthe flow of hidden information between layers\nin SSMs.\nBy selectively integrating shallow-\nlayer hidden states into deeper layers, Dens-\neSSM retains fine-grained information crucial for\nthe final output. Dense connections enhanced\nDenseSSM still maintains the training paralleliz-\nability and inference efficiency. The proposed\nmethod can be widely applicable to various SSM\ntypes like RetNet and Mamba.\nWith similar\nmodel size, DenseSSM achieves significant im-\nprovements, exemplified by DenseRetNet out-\nperforming the original RetNet with up to 5%\naccuracy improvement on public benchmarks.\ncode is avalaible at : https://github.com/\nWailordHe/DenseSSM .\n1. Introduction\nSince the release of ChatGPT (OpenAI, 2023), large lan-\nguage models have entered a new epoch, showcasing out-\nstanding abilities in language comprehension, dialogue, and\nlogical reasoning. Over the past year, the industry has wit-\nnessed the emergence of numerous large language models,\nsuch as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng\net al., 2023). These large language models have given rise\nto a plethora of practical applications, including conversa-\ntional bots, code assistants, and AI agents. The foundation\nof large language models lies in the Transformer network\n*Equal contribution\n1Huawei Noah\u2019s Ark Lab. Correspon-\ndence to:\nKai Han <kai.han@huawei.com>, Yunhe Wang\n<yunhe.wang@huawei.com>.\nstructure (Vaswani et al., 2017), primarily utilizing a multi-\nhead self-attention module for modeling relationships be-\ntween tokens and a Feed-forward network for non-linear\nfeature transformations. The scaling law (Kaplan et al.,\n2020) based on the Transformer structure has propelled the\ncontinuous development and expansion of large language\nmodels.\nIn the Transformer network, multi-head self-attention\n(MHSA) plays a crucial role, but it comes with signif-\nicant computational demands and memory requirements\nduring inference. In terms of computational complexity,\nfor an input sentence of length N, the calculation of self-\nattention has a complexity of O(N 2) during training and\ninference. Regarding memory usage, previously encoun-\ntered keys and values are stored, leading to a memory oc-\ncupation of O(ND). As a result, recent efforts on network\narchitectures have focused on simplifying Transformer by\nreducing its computation and space complexity. This in-\ncludes various approaches, notably convolutional language\nmodels (Poli et al., 2023), recurrent unit (Lei, 2021), long\ncontext models (Ding et al., 2023), and state space models\n(SSMs) (Gu et al., 2021; Gu & Dao, 2023). These new\nmodels have provided strong alternatives to Transformer for\nbuilding efficient LLMs.\nSSMs propose modeling sequences by introducing an ap-\npropriate design of hidden states for handling long-range\ndependencies with both training parallelizability and in-\nference efficiency. Starting from the continuous mapping\nsystem, SSMs are discretized to process discrete inputs in\ndeep learning such as language sequence. The discretized\nSSMs can be computed in both linear recurrence and global\nconvolution modes. Commonly, convolution mode is used\nduring training to achieve parallel acceleration, while recur-\nrence mode is used during autoregressive inference because\nit has lower computational complexity.\nThe core distinction of SSMs from other neural networks,\nsuch as fully-connected neural networks, lies in the design\nof hidden states. Hidden states enable information to be\npropagated along the temporal dimension, while avoiding\nthe computation complexity of accessing historical tokens\nat each step. Through state transition parameters A, hidden\nstates transfer the hidden information from the previous time\n1\narXiv:2403.00818v2  [cs.CL]  5 Mar 2024\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nsteps to the current time step, allowing for autoregressive\nprediction of the next token. Hidden states play a crucial role\nin SSMs, but have not received sufficient investigation in the\npast. Weights and hidden features in different layers contain\ninformation at various levels from fine-grained to coarse-\ngrained (Gu et al., 2021). However, in previous versions of\nSSMs, hidden states only flowed within the current layer\nand could not transmit more information to deeper layers,\nthus failing to capture more hierarchical information.\nIn this paper, we propose DenseSSM to facilitate a more\ncomprehensive flow of hidden information between layers in\nstate space models. We first analyze the hidden state degra-\ndation in conventional SSMs which will prevent hidden\ninformation flow from low levels to high levels. By selec-\ntively integrating shallow-layer hidden states into deeper\nlayers, DenseSSM retains fine-grained information that is\nuseful for the final output. The proposed method is applica-\nble to different types of SSMs, such as RetNet (Sun et al.,\n2023) and Mamba (Gu & Dao, 2023). Our approach main-\ntains the training parallelizability and inference efficiency\nof SSMs, while achieving a significant improvement with\nonly a slight increase in the number of parameters. For\ninstance, our DenseRetNet model outperforms traditional\nRetNet with up to 5% accuracy improvement on public\nbenchmarks.\n2. Related Works\n2.1. Large Language Models\nLarge language models (LLMs) have seen transformative\nadvancements, enabling them to excel in a diverse array of\nnatural language processing (NLP) tasks, including machine\ntranslation, text summarization, and emergent abilities like\nincontext learning, which were previously unattainable by\nearlier language models (Devlin et al., 2019; Raffel et al.,\n2023). The evolution of LLMs has been marked by a mon-\numental shift in scale, exemplified by models like GPT-\n3 (Brown et al., 2020), with its 175 billion parameters, and\nthe even more expansive PaLM (Chowdhery et al., 2022),\npacking in a astounding 540 billion parameters. These mod-\nels have empirically validated the scaling law (Kaplan et al.,\n2020), which posits that increasing model size leads to im-\nproved performance.\nThe rapid expansion in model size has underscored the criti-\ncal need for the development of efficient Transformer algo-\nrithms, where FlashAttention (Dao et al., 2022; Dao, 2023)\nhas emerged as a significant innovation. This approach\nenhances the pivotal attention mechanism within Transform-\ners by optimizing softmax computations using a technique\nknown as tiling. By minimizing memory transactions be-\ntween the GPU\u2019s HBM and on-chip SRAM, FlashAttention\ncompute exact attention with fewer memory accesses, result-\ning in both faster execution and a lower memory footprint\ncompared to standard attention implementations.\n2.2. State Space Models\nWhile the Transformer is currently the de facto architec-\nture for large language models (LLMs), providing efficient\nparallel GPU training, the inference time for single-token in-\nference increases significantly with longer sequence lengths,\nposing challenges for deployment due to the O(N) complex-\nity per step even with accelerating algorithms like FlashAt-\ntention (Dao et al., 2022; Dao, 2023). Efforts have been\ndedicated to researching the Transformer-Next architecture,\naiming to achieve state-of-the-art (SOTA) performance with\nefficient parallel training and effective inference, particu-\nlarly for long sequence lengths.\nState Space Sequence Models (SSMs) have recently\nemerged as promising architectures for sequence modeling.\nHiPPO (Gu et al., 2020) streamlines sequence modeling\nby compressing lengthy inputs into a dynamic, polynomial-\nbased representation using orthogonal polynomials. S4 (Gu\net al., 2021) introduced a novel parameterization through the\napplication of a low-rank structured correction, enabling sta-\nble diagonalization and simplifying the process into Cauchy\nkernel operations. S5 (Smith et al., 2023) further simplifies\nthe S4 layer by employing a single multi-input, multi-output\nSSM and introducing efficient parallel scan algorithms into\nthe S4 layers. H3 (Fu et al., 2023) narrows the performance\ngap between SSMs and Transformer language models by\ndesigning three projections (Q, K, V) to simulate the atten-\ntion mechanism and adopting a fast Fourier transform (FFT)\nto reduce computation and memory consumption further.\nGSS (Mehta et al., 2022) was the first gated neural network\narchitecture incorporating SSMs, it builds upon (Hua et al.,\n2022) and introducing a compact SSM architecture that con-\ntracts model dimensions. Unlike GSS, which emphasizes\ncompressing context into a smaller state, Mamba (Gu &\nDao, 2023) diverges by focusing on enhancing the selectiv-\nity of the state representation, aiming to balance the tradeoff\nbetween efficiency and effectiveness without compromising\nthe model\u2019s ability to capture essential information from the\ncontext. It achieves this by integrating a selection mecha-\nnism which enabling the model to selectively prioritize rel-\nevant information while concurrently utilizing a hardware-\noptimized algorithm that ensures efficient computation.\n2.3. Linear Attention\nLinear attentions (Katharopoulos et al., 2020; Zhai et al.,\n2021), which remove the softmax operation from traditional\nattention, can be seen as a derivative of State Space Models\n(SSMs). They replace SSMs\u2019 convolutions with a variation\nof Multi-Head Attention (MHA) and eliminate the softmax\nof the traditional attention mechanism by utilizing a kernel\n2\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nfunction that operates independently on the queries (Q) and\nkeys (K). These mechanisms also have a parallel form for\nefficient training and a recurrent form with O(1) complexity.\nRetNet (Sun et al., 2023), TransNormerLLM (Qin et al.,\n2024), and RWKV (Peng et al., 2023) implement a fixed\ndecay factor to update the previous key-value (KV) states\nat each recurrent step. This decay mechanism seamlessly\nintegrates with the causal attention mask for efficient paral-\nlel computation. However, since this decay factor is preset\nand independent of the data, it may not be universally ap-\nplicable across all tasks, especially when prompts or long-\nrange information is particularly important. To address\nthis challenge, GLA (Gated Linear Attention) (Yang et al.,\n2023) introduces data-dependent gating mechanisms that\nare practical for both parallel and block-parallel forms. It\nperforms competitively against strong baselines, including\nthe LLaMA-architecture Transformer (Touvron et al., 2023)\nand Mamba (Gu & Dao, 2023).\n3. DenseSSM\nIn this section, we analyze the hidden state degradation\nin the deeper layers of SSMs and further introduce dense\nconnection of hidden states to preserve richer information\nfor deeper layers.\n3.1. Prelimineries\nTransformer\nTransformer is the widely-used network ar-\nchitecture of large language models which is based on the\nself-attention mechanism. The self-attention performs as\nfollows:\not = Wo\nPT\ni=1 eqT\nt kivi\nPT\ni=1 eqT\nt ki l,\n(1)\nwhere q, k and v are obtained by fully-connected layers, Wo\nis the linear transformation weight for the output token ot\nat the t-th timestep. Each token will merge information of\nthe other tokens by relationship weights calculated by the\nself-attention. In addition to self-attention module, the fee-\nforward network (FFN) module is another key component\nto transform the token representation and introduces more\nnon-linearity. FFN module is usually composed by two\nstacked linear layers and non-linear activation function:\nyt = Wdown\u03c3(Wupot),\n(2)\nwhere Wup and Wdown are the weight matrices of up pro-\njection and down projection layers, and \u03c3(\u00b7) is the activation\nfunction such as GELU (Hendrycks & Gimpel, 2016).\nSSM\nState space models (SSM) in the literature of deep\nlearning refer to the class of structured SSMs (Gu et al.,\n2021) and the derivatives such as RWKV (Peng et al., 2023)\nand RetNet (Sun et al., 2023). Here we briefly describe\nthe structured SSMs as a representative. Structured SSMs\ndefine a sequence-to-sequence transformation x(t) \u2192 y(t)\nwith an implicit latent state h(t). The continuous form is\nformulated as\nh\u2032(t) = Ah(t) + Bx(t),\n(3)\ny(t) = Ch(t),\n(4)\nwhere A, B and C are the parameters. To apply SSM to\nthe real discrete data, we discretize the continuous case and\nobtain the recurrence formulation and convolution formu-\nlation of it. The parameters A and B are transformed to\nthe discrete parameters A and B with the discretization rule\nsuch as zero-order hold (Gu et al., 2021). The recurrence\nformulation is\nht = Aht\u22121 + Bxt,\n(5)\nyt = Cht.\n(6)\nThe convolution formulation is\nK = (CB, CAB, \u00b7 \u00b7 \u00b7 , CA\ntB),\n(7)\ny = x \u2217 K,\n(8)\nwhere \u2217 is convolution operation, and t+1 is the convolution\nkernel size. The recurrence mode is usually used for efficient\nautoregressive inference, while the convolution mode is used\nfor efficient parallelizable training.\n3.2. Hidden State Degradation\nHere we analyze the hidden information flow from shallow\nlayers to deep layers. In the following, we use the super-\nscript \u201cl\u201d to represent the l-th block.\nhl\nt =Ahl\nt\u22121 + Bxl\nt\n=Ahl\nt\u22121 + B\u0398(yl\u22121\nt\n)\n=Ahl\nt\u22121 + B\u0398(Chl\u22121\nt\n)\n=Ahl\nt\u22121 + B\u0398(CAhl\u22121\nt\u22121 + CB\u0398(Chl\u22122\nt\n))\n=Ahl\nt\u22121 + B\u0398(CAhl\u22121\nt\u22121 + \u00b7 \u00b7 \u00b7\n+ CB\u0398(CAhl\u2212m+1\nt\u22121\n+ CB\u0398(Chl\u2212m\nt\n)) \u00b7 \u00b7 \u00b7 )\n| {z }\nm\n,\n(9)\nwhere \u0398(\u00b7) is the transformations from the last output to\nthe input of SSM module, such as convolution and FFN.\nFrom Eq. 9, we can see that the transmission of hidden in-\nformation from the (l\u2212m)-th layer to the l-th layer requires\npassing through m transformation blocks and m BC matrix\nmultiplications. Such a complex computational process can\nlead to significant information loss, meaning that attempting\nto retrieve certain information from the (l \u2212 m)-th layer at\nthe l-th layer becomes very challenging and unclear.\n3\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\n(a) DenseSSM in autoregressive mode.\n(b) DenseSSM in parallelizable convolution mode.\nFigure 1. Illustrations of DenseSSM framework, where \u03d5 is the selective transition module and \u2018Fusion\u2019 is the hidden fusion module.\n3.3. Dense Hidden Connection\nThrough the above analysis, we have identified a crucial\nissue in SSM, which is the decay of important hidden states\nas the layer depth increases. Therefore, we propose a dense\nconnection for hidden states to better preserve fine-grained\ninformation from shallow layers, enhancing the ability of\ndeep layers to perceive the original textual information. For\nthe l-th block, we densely connect the hidden states in its pre-\nvious m blocks. First, we collect the shallow hidden states\nand introduce a selective transition module \u03d5 to project them\nto the subspace of the target layer and select useful parts\nsimultaneously:\nHl\nt = [\u03d5(hl\u22121\nt\n); \u03d5(hl\u22122\nt\n); \u00b7 \u00b7 \u00b7 ; \u03d5(hl\u2212m\nt\n)],\n(10)\nThen, the intermediate hidden vectors are injected into the\noriginal hidden state of this layer:\nh\u2032l\nt = Fuse(hl\nt, Hl\nt).\n(11)\nThe operation Fuse() is the function to fuse the intermedi-\nate hidden vectors and the current hidden state. The SSMs\nwith the proposed dense hidden connection is named as\nDenseSSM (Figure 1(a)). The DenseSSM scheme can be\nused in any SSM variant such as Mamba (Gu & Dao, 2023).\nCompared to DenseNet (Huang et al., 2017) for convolu-\ntional networks, the proposed DenseSSM densely connect\nthe hidden states in SSMs, and the selective mechanism and\nfusion manner are more efficient for language modeling.\nThe above analysis is based on the recurrence mode, in the\nfollowing we introduce the convolution mode of DenseSSM\nfor efficient training. From Eq. 5, we have\nhl\nt = Ahl\nt\u22121 + Bxl\nt\n= A(Ahl\nt\u22122 + Bxl\nt\u22121) + Bxl\nt\n= A\n2hl\nt\u22122 + ABxl\nt\u22121 + Bxl\nt\n= A\nthl\n0 + A\nt\u22121Bxl\n1 + \u00b7 \u00b7 \u00b7 + ABxl\nt\u22121 + Bxl\nt\n= A\ntBxl\n0 + A\nt\u22121Bxl\n1 + \u00b7 \u00b7 \u00b7 + ABxl\nt\u22121 + Bxl\nt.\n(12)\nThis process can be conducted by a convolution on the input\nsequence (xl\n0, xl\n1, \u00b7 \u00b7 \u00b7 , xl\nt):\nhl\nt = A\ntBxl\n0 + A\nt\u22121Bxl\n1 + \u00b7 \u00b7 \u00b7 + ABxl\nt\u22121 + Bxl\nt\n= (xl\n0, xl\n1, \u00b7 \u00b7 \u00b7 , xl\nt) \u2217 (B, AB, \u00b7 \u00b7 \u00b7 , A\ntB).\n(13)\nIn the proposed DenseSSM, we enhance the hidden states\nby Eq. 11 and then obtain the outputs of SSM:\nyl\nt = Ch\u2032l\nt\n= CFuse((xl\n0, xl\n1, \u00b7 \u00b7 \u00b7 , xl\nt) \u2217 (B, AB, \u00b7 \u00b7 \u00b7 , A\ntB), Hl\nt).\n(14)\nAs shown in Figure 1(b), DenseSSM can be trained in paral-\nlelizable convolution mode.\nSelective Transition Module\nThe selective transition\nmodule \u03d5(\u00b7) is to project inputs to the target subspace and\nselect the useful part of hidden information simultaneously.\nWe implement the selective transition module with projec-\ntion layer and gate selection mechanism, as shown in Fig-\nure 2. First, we project the hidden states in the previous m\n4\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nSSM blocks to the same space:\nh\u2032l\u2212m\nt\n= Proj(hl\u2212m\nt\n).\n(15)\nThen we generate the gate weights based on the input xl\nt\nand use them to select useful hidden states:\n\u03d5(hl\u2212m\nt\n) = h\u2032l\u2212m\nt\n\u2299 Gate(xl\nt).\n(16)\nPlease note that the newly introduced modules must not\ncompromise the training parallelizability and inference effi-\nciency of the original SSM framework. Therefore, we main-\ntain a simple and efficient implementation in practice. The\nprojection layer is implemented using a linear transforma-\ntion, while the gate module is implemented with a two-layer\nMLP with a SiLU activation (Elfwing et al., 2018).\nFigure 2. Selective Transition Module.\nHidden Fusion Module\nAfter the selective transition\nmodule, we obtain the selected hidden states from shal-\nlow layers, i.e., HL\nt\n= [\u03d5(h1\nt); \u03d5(h2\nt); \u00b7 \u00b7 \u00b7 ; \u03d5(hL\u22121\nt\n)]. A\nhidden fusion module is utilized to integrate shallow hidden\nstates with the current hidden states. Similarly, we keep the\nimplementation simple for efficiency. We add the selected\nhidden states since they have been projected to the same\nspace:\nhL\nt = Fuse(hL\nt , HL\nt ) = hL\nt +\nm\nX\ni=1\nhl\u2212i\nt\n.\n(17)\nHere, we provide a basic implementation, but of course,\nthere are other implementation approaches such as con-\ncatenation and cross-attention. We will compare different\nimplementation methods in later experiments.\nExtension to RetNet\nRetNet (Sun et al., 2023) can be\nviewed as a kind of state space models which uses a variant\nof self-attention rather than convolution in Eq. 7. Com-\npared to the standard Transformer, RetNet is a RNN-style\nlanguage model with fast inference and parallelized train-\ning. It utilizes linear attention to simplify the computation\ncomplexity of self-attention.\nSt = \u03b3St\u22121 + kT\nt vt,\n(18)\nyt = qtSt,\n(19)\nwhere St is the recurrent state, and 0 < \u03b3 < 1. The dense\nKV connection for RetNet is performed as follows. The\nlow-level keys and values are first concatenated:\nKl\nt = [\u03d5(kl\u22121\nt\n); \u03d5(kl\u22122\nt\n); \u00b7 \u00b7 \u00b7 ; \u03d5(kl\u2212m\nt\n)],\n(20)\nVl\nt = [\u03d5(vl\u22121\nt\n); \u03d5(vl\u22122\nt\n); \u00b7 \u00b7 \u00b7 ; \u03d5(vl\u2212m\nt\n)].\n(21)\nThen, the intermediate key (or value) vectors are injected\ninto the original keys (or values) of this layer:\nk\u2032L\nt = kL\nt +\nm\nX\ni=1\nkl\u2212i\nt\n,\n(22)\nv\u2032L\nt = vL\nt +\nm\nX\ni=1\nvl\u2212i\nt\n.\n(23)\nThe RetNet equiped with the proposed dense key-value\n(KV) connections is named as DenseRetNet, as illustrated\nas shown in the figure 3. In addition, the paralleizable mode\nof DenseRetNet is formulated as follows:\nyt = qt\nt\nX\ni=1\n\u03b3t\u2212ik\u2032T\ni v\u2032\ni.\n(24)\nOur DenseRetNet can be implemented in parallelizable\nmode as well, that is, can be trained in parallel on GPUs or\nNPUs.\nFigure 3. DenseRetNet in autoregressive mode.\n4. Experiments\nIn this section, we conducted comprehensive experiments\nto validate the effectiveness of the proposed DenseSSM.\nThe verification was carried out on different architectures,\nincluding RetNet and Mamba.\n5\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\n4.1. Data and Experimental Settings\nPretraining\nData\nFollowing\nthe\ncommon\nsettings\nin (Yang et al., 2023), we trained all models from scratch\nutilizing a corpus comprising 56GB of raw data extracted\nfrom The Pile (Gao et al., 2020), a commonly used di-\nverse and high-quality datasets. Excluding data from the\nDM Mathematics and Github subsets, we performed a ran-\ndom shuffle and sampled from all remaining corpus. The\ndata was tokenized using the LLaMA tokenizer, which has a\nvocabulary size of 32,000 tokens. <bos> token was used as\nthe start-of-sequence marker. The resulting cached dataset\ncontained a total of 15 billion tokens.\nEvaluation Datasets\nIn our experiment, we investigate\nmodels performance across a spectrum of downstream\ntasks, focusing on zero-shot and 4-shot learning capabil-\nities. The tasks, presented in Table 4 and 6, encompass\na range of datasets designed to test common-sense reason-\ning and question-answering, such as HellaSwag (Zellers\net al., 2019), BoolQ (Clark et al., 2019), COPA (Ponti et al.,\n2020), PIQA (Bisk et al., 2019), Winograd (Muennighoff\net al., 2022), Winogrande (Sakaguchi et al., 2019), Sto-\nryCloze (Lin et al., 2021), OpenBookQA (Mihaylov et al.,\n2018), SciQ (Welbl et al., 2017), ARC E(ARC-easy) and\nARC C(ARC-challenge) (Clark et al., 2018). Words Per-\nplexity results of WikiText (Merity et al., 2016) and LAM-\nBADA (LAMBADA OPENAI) (Paperno et al., 2016) are\nalso reported. All evaluations are executed using the LM\nevaluation harness (Gao et al., 2023), ensuring a standard-\nized approach to assessing the models\u2019 capabilities.\n4.2. Training Setup and Model\u2019s Architectures\nWe selected the 350M and 1.3B model specifications to\nverify the validity of our proposed dense mechanism. All\nmodels were trained from scratch for one epoch on 15 billion\ntokens. The training batch size was set to 0.5 million to-\nkens with a training length setting of 2048 tokens. AdamW\n(Loshchilov & Hutter, 2019) optimizer was used for train-\ning, with a polynomial learning rate decay, and warm-up\nratio is set to 1.5% of total training steps. Weight decay is\nset to 0.01, and gradient clipping is set to 1. We tailored the\nhyper-parameters of the model to ensure comparability with\nmodels of same scale. Additionally, we designed our Dense\nRetNet model to be fully comprised of GAU-like blocks,\nthis will be explicitly detailed in the subsequent paragraph.\nTransformer-based language models\nWe evaluate our\nproposed select dense mechanism against popular large\nlanguage models like LLaMA (Touvron et al., 2023) and\nOPT (Zhang et al., 2022), comparing with LLaMA for 350M\nsize models and with OPT for 1.3B size models. Table 1\nreports their hyperparameters.\nHyperparameters\nLLaMA 350M\nOPT 1.3B\nlayers\n18\n24\nhidden size\n1024\n2048\nffn size\n4096\n8192\nheads\n8\n32\nlearning rate\n6 \u00d7 10\u22124\nAdam \u03b2\n(0.9, 0.98)\ndropout\n0.0\n0.1\nTable 1. Hyperparamters used for LLaMA and OPT models.\nMamba\nAs shwon in Table 2, since our tokenizer is\nsmaller than the GPT-NeoX (Black et al., 2022) tokenzier\nwhich Mamba (Gu & Dao, 2023) uses, we have added two\nadditional layers to match the parameters. Besides this, we\nhave adhered to Mamba\u2019s model structure and other train-\ning settings described in their paper. Specifically, we have\nset the learning rates to 3e-4 for the 360M model and 2e-\n4 for the 1.3M model, and we have not applied dropout\nin either case. The obtained new architecture is named as\nDenseMamba.\nDenseMamba Hyperparameters\n360M\n1.3B\nn layers\n50\n50\nd model\n1024\n2048\ndense fusion layers\n4\n4\nlearning rate\n3 \u00d7 10\u22124\n2 \u00d7 10\u22124\nAdam \u03b2\n(0.9, 0.95)\ndropout\n0.0\nTable 2. Hyperparamters used for DenseMamba models.\nRetNet\nModel sizes and hyperparameters for our\nDenseRetNet is shown in Table 3.\nWe further utilize\nGated Attention Unit (GAU) (Hua et al., 2022) in our\nDenseRetNet. GAU combine Attention and FFN block into\none, so a single block can perform both channel mixing and\ntoken mixing: Y = (XWu \u2299 A \u02c6V )Wo, where A is attention\nweight cauculated though Eq. 24. Also, multiple attention\nDenseRetNet Hyperparameters\n360M\n1.3B\nlayers\n16\n25\nhidden size\n1536\n2560\nq & k size\n768\n1280\nv & gate size\n3072\n5120\nheads\n2\n4\ndense fusion layers\n2\n2\nlearning rate\n6 \u00d7 10\u22124\nAdam \u03b2\n(0.9, 0.98)\ndropout\n0.1\nTable 3. Hyperparamters used for DenseRetNet models.\n6\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nModels / Tasks\nWikitext LAMBADA ARC C ARC E BoolQ COPA HellaSwag PIQA WinoGrande StoryCloze Winograd OpenBookQA SciQ Avg.\nZero-Shot\nLLaMa-350M\n26.79\n22.50\n22.95\n46.13\n59.27\n64\n33.19\n64.36\n49.09\n57.64\n62.02\n29.6\n75.3 51.23\nRetNet-350M\n36.88\n35.53\n21.25\n40.99\n48.35\n61\n29.86\n62.30\n51.07\n55.59\n59.05\n28.4\n75.8 48.51\nDenseRetNet-350M\n31.35\n19.92\n23.72\n45.03\n58.50\n69\n32.31\n64.04\n52.09\n58.04\n60.82\n30.4\n76.6 51.87\nFour-Shot\nLLaMa-350M\n-\n-\n23.81\n47.26\n53.00\n65\n33.71\n64.15\n51.14\n57.38\n64.25\n28.2\n81.2 51.73\nRetNet-350M\n-\n-\n23.04\n40.91\n50.37\n63\n29.49\n62.08\n51.78\n55.66\n59.61\n27.4\n77.4 49.16\nDenseRetNet-350M\n-\n-\n24.74\n45.66\n54.89\n69\n32.14\n63.70\n52.01\n57.58\n59.23\n28.2\n78.3 51.41\nZero-Shot\nOPT-1.3B\n22.04\n13.79\n24.66\n48.65\n58.07\n63\n37.00\n65.89\n52.80\n61.02\n65.51\n29.6\n81.1 53.39\nRetNet-1.3B\n27.90\n23.41\n22.61\n46.34\n48.75\n58\n32.25\n63.44\n49.96\n57.71\n60.65\n23.4\n77.3 49.13\nDenseRetNet-1.3B\n21.55\n10.88\n24.49\n50.88\n58.62\n63\n38.72\n67.25\n49.96\n60.82\n65.85\n31.8\n82.7 54.01\nFour-Shot\nOPT-1.3B\n-\n-\n25.94\n50.46\n52.35\n63\n36.97\n64.64\n52.33\n60.09\n66.58\n28.2\n89.4 53.63\nRetNet-1.3B\n-\n-\n24.66\n46.30\n47.49\n67\n31.96\n63.22\n52.09\n57.51\n61.42\n26.6\n80.3 50.78\nDenseRetNet-1.3B\n-\n-\n25.68\n53.07\n56.3\n67\n38.56\n66.97\n53.59\n62.08\n65.12\n27.8\n86.7 54.81\nTable 4. Benchmarking results of DenseRetNet are compared against the original RetNet (Sun et al., 2023) and Transformer-based\nmodels, specifically LLaMA-350M (Touvron et al., 2023) and OPT-1.3B (Zhang et al., 2022). Our DenseRetNet architecture has lower\nperplexity and higher accuracy, effectively enhances the performance of Linear Attention, e.g., in RetNet, and surpasses the performance\nof Transformer-based Models.\nheads with different exponential decay rates are utilized to\nperform multi-scale decay instead of GAU\u2019s single-head\nstrategy. In our experiments, we have observed that our\narchitecture surpasses the RetNet structure with FFN layers\nin terms of training stability and performance. The obtained\nnew architecture is named as DenseRetNet.\n4.3. Main Results for DenseRetNet\nWe evalute our models on both common corpus, and\ndownstream tasks including common-sense reasoning and\nquestion-answering. Table 4 presents the experimental re-\nsults comparing DenseRetNet with LLaMA-350M (Touvron\net al., 2023), OPT-1.3B (Zhang et al., 2022) and RetNet (Sun\net al., 2023). Our DenseRetNet obtains lower perplexity on\nWikitext and LAMBADA corpus and shows clear advan-\ntages in the downstream tasks in both 0-shot and few-shot\nsettings. Especially, our model significantly improves the\nperformance of RetNet, and achieves superior performance\ncompared to the transformer large language models.\n4.4. Main Results for DenseMamba\nTable 6 compares the performance of DenseMamba with\nLLaMA-350M (Touvron et al., 2023), OPT-1.3B (Zhang\net al., 2022), and Mamba (Gu & Dao, 2023). DenseMamba\ndemonstrates superior perplexity and accuracy on the test\nset, outperforming Mamba and other Transformer-based\nmodels.\n4.5. Ablation Studies\nIn this section, we conduct an ablation study to evaluate the\nimpact of various design choices in our Selective Transition\nModule and Hidden Fusion Module. Perplexity results are\npresented for both in-domain evaluation sets and out-of-\ndomain corpora (Merity et al., 2016). For fair comparison,\nthe baseline for all ablation studies is DenseRetNet-350M,\nwith parameter adjustments to facilitate comparisons under\nsimilar computational constraints when necessary. We fol-\nlow the default training settings outlined in Table 3 for our\nmodels, except for the model trained on 1B tokens.\nAblations on Selective Transition Module\nThe proposed\nselective transition module is to project the shallow hidden\nstates to the same subspace and select the useful parts of\nthem. The selective transition module can be implemented\nin different manners.\nTable 5 investigates the impact of various Projection and\nSelect configurations. The experiment\u2019s other parameters\nwere held constant: the number of dense layers(m) was\nset to 2, and the Fusion operation following the selective\ntransition module was an \u201dAdd\u201d operation. The findings\nsuggest that the combination of Identity projection with\nMLP strikes an optimal balance between parameter count\nand performance.\nProjection\nSelect\n#Param\nIn domain\nWikitext\nNone\nNone\n346M\n2.565\n2.359\nIdentity\nMLP\n353M\n2.546\n2.348\nIdentity\nLinear\n357M\n2.572\n2.369\nLinear\nMLP\n353M\n2.579\n2.372\nLinear\nLinear\n356M\n2.582\n2.378\nTable 5. In-domain evaluation cross-entropy loss and out-of-\ndomain byte perplexity results for DenseRetNet-350M with var-\nious implementations of the selective transition module are pre-\nsented.\nAblations on Dense Layers\nIn this experiment, we con-\nducted an ablation analysis on the depth of fusion layers\n(denoted as m) . We employed a fusion strategy based on\nIdentity projection and generate the gate using MLP. Our\nexperimental results, as presented in Table 7, both two-\n7\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nModels / Tasks\nWikitext LAMBADA ARC C ARC E BoolQ COPA HellaSwag PIQA WinoGrande StoryCloze Winograd OpenBookQA SciQ Avg.\nZero-Shot\nLlaMa-350M\n26.79\n22.50\n22.95\n46.13\n59.27\n64\n33.19\n64.36\n49.09\n57.64\n62.02\n29.6\n75.3 51.23\nMamba-360M\n26.60\n17.55\n23.98\n45.83\n55.78\n61\n34.89\n64.31\n52.88\n58.90\n62.92\n29.2\n79.8 51.77\nDenseMamba-360M\n26.41\n17.03\n24.32\n46.0\n59.20\n66\n34.68\n64.80\n51.14\n59.03\n63.23\n29.8\n79.8 52.55\nFour-Shot\nLLaMa-350M\n-\n-\n23.81\n47.26\n53.00\n65\n33.71\n64.15\n51.14\n57.38\n64.25\n28.2\n81.2 51.73\nMamba-360M\n-\n-\n25.26\n46.51\n45.41\n63\n34.25\n65.13\n52.80\n58.97\n62.88\n29.0\n81.0 51.29\nDenseMamba-360M\n-\n-\n24.83\n46.97\n58.26\n66\n34.74\n64.69\n52.01\n58.37\n63.44\n28.6\n80.3 52.56\nZero-Shot\nOPT-1.3B\n22.04\n13.79\n24.66\n48.65\n58.07\n63\n37.00\n65.89\n52.80\n61.02\n65.51\n29.6\n81.1 53.39\nMamba-1.3B\n21.79\n12.46\n25.09\n50.84\n53.15\n67\n38.34\n67.19\n50.59\n60.29\n65.25\n30.0\n79.8 53.41\nDenseMamba-1.3B\n21.39\n12.47\n25.09\n51.89\n58.59\n67\n39.26\n67.90\n52.01\n61.28\n66.11\n30.6\n79.9 54.51\nFour-Shot\nOPT-1.3B\n-\n-\n25.94\n50.46\n52.35\n63\n36.97\n64.64\n52.33\n60.09\n66.58\n28.2\n89.4 53.63\nMamba-1.3B\n-\n-\n26.96\n52.69\n49.56\n69\n39.25\n66.27\n52.96\n61.15\n66.06\n30.4\n82.3 54.24\nDenseMamba-1.3B\n-\n-\n26.54\n52.99\n58.59\n67\n39.26\n67.08\n53.67\n61.48\n65.89\n31.0\n82.1 55.05\nTable 6. Benchmarking results of DenseMamba are compared against LLaMA-350M (Touvron et al., 2023), OPT-1.3B (Zhang et al.,\n2022), and Mamba (Gu & Dao, 2023). The results show that DenseMamba achieves a certain improvement achieving lower perplexity\nand higher accuracy on the test set compared to Mamba, and surpassing the capabilities of Transformer-based architectures.\nlayer (m=2) and four-layer (m=4) fusion architectures have\nperformance gains. Considering the computational cost as-\nsociated with training and inference, the two-layer fusion\napproach is considered to be more optimal.\nIn addition, we have explored the necessity of employing\ndistinct gate generation layers for different dense layers. Our\nexperimental results indicate that varying this configuration\ndo not has a positive impact on the model\u2019s performance,\nwhich is beneficial for the development of lightweight dense\nconnection architectures.\nLayers\nDiff. gates\n#Param\nIn domain\nWikitext\n1\n%\n353M\n2.570\n2.363\n2\n%\n353M\n2.546\n2.348\n2\n!\n360M\n2.547\n2.351\n4\n%\n353M\n2.542\n2.348\n4\n!\n374M\n2.557\n2.371\nTable 7. In-domain evaluation cross-entropy loss and out-of-\ndomain byte perplexity results for DenseRetNet-350M with differ-\nent dense layer numbers and different gate strategies. Diff. gates\ndonates if different gate is applied to different dense features.\nAblations on Hidden Fusion Module\nThe hidden fusion\nmodule is to fuse the transited hidden states and the current\nhidden states. A popular way of feature fusion is by Con-\ncat followed by dimensionality reduction, which adds more\nnumber of parameters compared to our way. By fine-tuning\nthe model structure, we compare it at the same magnitude,\nand Table 8 finds that our proposed lightweight Dense hid-\nden connection achieves a better result.\nAnother study investigates the impact of fusing dense fea-\ntures either every m layers or at each individual layer. To\nmaintain a consistent parameter count, we adjusted the di-\nmensions of the MLP intermediate layer and trained the\nmodel with the full 15B tokens. The results in Table 9 in-\nFusion\n#Param\nIn domain\nWikitext\nConcat\n354M\n2.551\n2.370\nAdd\n353M\n2.546\n2.348\nTable 8. In-domain evaluation cross-entropy loss and out-of-\ndomain byte perplexity of DenseRetNet-350M with different im-\nplementations of hidden fusion module.\ndicate that fusing at each layer more effectively facilitates\ninformation transfer from lower to higher layers.\nDense frequency\n#Param\nIn domain\nWikitext\nEvery layer\n353M\n2.303\n1.845\nEvery 2 layers\n353M\n2.331\n1.866\nEvery 4 layers\n353M\n2.387\n1.923\nTable 9. In-domain evaluation cross-entropy loss and out-of-\ndomain byte-perplexity for DenseRetNet-350M with varying dense\nintervention frequency strategies.\n5. Conclusion\nIn this paper, we propose a new DenseSSM framework for\nenhancing the hidden information flow cross different layers.\nThe hidden states are crucial information storage units in\nthe SSMs. Utilizing the hidden states from each layer more\neffectively would greatly benefit the fundamental capabili-\nties of SSMs. Therefore, we propose to collect the hidden\nstates from shallow layers and selectively fusing them into\nthe hidden states of deeper layers to enhance the SSM\u2019s\nperception of low-level textual information. The proposed\nDenseSSM method does not affect the excellent character-\nistics of SSM, i.e., efficient autoregressive inference and\nefficient parallelizable training. We apply the DenseSSM\nmethod to widely used architectures like RetNet and Mamba,\nresulting in new architectures with stronger foundational\nlanguage abilities and achieving higher accuracy in public\nbenchmark evaluations.\n8\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\n6. Impact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\nPiqa: Reasoning about physical commonsense in natural\nlanguage, 2019.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. Gpt-neox-20b: An\nopen-source autoregressive language model, 2022.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners, 2020.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,\nJ., Petrov, S., and Fiedel, N. Palm: Scaling language\nmodeling with pathways, 2022.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\nArXiv, abs/1803.05457, 2018.\nDao, T. Flashattention-2: Faster attention with better paral-\nlelism and work partitioning, 2023.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness, 2022.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2019.\nDing, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W.,\nZheng, N., and Wei, F. Longnet: Scaling transformers to\n1,000,000,000 tokens. arXiv preprint arXiv:2307.02486,\n2023.\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\nlinear units for neural network function approximation\nin reinforcement learning. Neural networks, 107:3\u201311,\n2018.\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,\nand R\u00b4e, C. Hungry hungry hippos: Towards language\nmodeling with state space models, 2023.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\nPresser, S., and Leahy, C. The pile: An 800gb dataset of\ndiverse text for language modeling, 2020.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n12 2023. URL https://zenodo.org/records/\n10256836.\nGu, A. and Dao, T.\nMamba:\nLinear-time sequence\nmodeling with selective state spaces.\narXiv preprint\narXiv:2312.00752, 2023.\nGu, A., Dao, T., Ermon, S., Rudra, A., and R\u00b4e, C. Hippo:\nRecurrent memory with optimal polynomial projections.\nAdvances in neural information processing systems, 33:\n1474\u20131487, 2020.\nGu, A., Goel, K., and Re, C. Efficiently modeling long\nsequences with structured state spaces. In International\nConference on Learning Representations, 2021.\nHendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\nHua, W., Dai, Z., Liu, H., and Le, Q. V. Transformer quality\nin linear time, 2022.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 4700\u20134708, 2017.\n9\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models,\n2020.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention, 2020.\nLei, T. When attention meets fast recurrence: Training\nlanguage models with reduced compute. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 7633\u20137648, 2021.\nLin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen,\nS., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du,\nJ., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary,\nV., O\u2019Horo, B., Wang, J., Zettlemoyer, L., Kozareva,\nZ., Diab, M. T., Stoyanov, V., and Li, X.\nFew-shot\nlearning with multilingual language models.\nCoRR,\nabs/2112.10668, 2021. URL https://arxiv.org/\nabs/2112.10668.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019.\nMehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B.\nLong range language modeling via gated state spaces,\n2022.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In EMNLP, 2018.\nMuennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-\nderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong,\nZ.-X., Schoelkopf, H., Tang, X., Radev, D., Aji, A. F., Al-\nmubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff,\nE., and Raffel, C. Crosslingual generalization through\nmultitask finetuning, 2022.\nOpenAI.\nChatgpt (mar 14 version).\nhttps://chat.\nopenai.com/chat, 2023.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\nFern\u00b4andez, R. The lambada dataset, Aug 2016.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\net al. Rwkv: Reinventing rnns for the transformer era. In\nFindings of EMNLP 2023, 2023.\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T.,\nBaccus, S., Bengio, Y., Ermon, S., and R\u00b4e, C. Hyena\nhierarchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866, 2023.\nPonti, E. M., Glava\u02c7s, G., Majewska, O., Liu, Q., Vuli\u00b4c,\nI., and Korhonen, A. XCOPA: A multilingual dataset\nfor causal commonsense reasoning. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2020. URL https:\n//ducdauge.github.io/files/xcopa.pdf.\nQin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y.,\nLv, B., Luo, X., Qiao, Y., and Zhong, Y. Transnormerllm:\nA faster and better large language model with improved\ntransnormer, 2024.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer, 2023.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. arXiv preprint arXiv:1907.10641, 2019.\nSmith, J. T. H., Warrington, A., and Linderman, S. W. Sim-\nplified state space layers for sequence modeling, 2023.\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,\nWang, J., and Wei, F. Retentive network: A successor to\ntransformer for large language models, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998\u20136008, 2017.\nWelbl, J., Liu, N. F., and Gardner, M.\nCrowdsourcing\nmultiple choice science questions. In NUT@EMNLP,\n2017.\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated\nlinear attention transformers with hardware-efficient train-\ning. arXiv preprint arXiv:2312.06635, 2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\nHellaswag: Can a machine really finish your sentence?,\n2019.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., Tam, W. L., Ma, Z.,\nXue, Y., Zhai, J., Chen, W., Liu, Z., Zhang, P., Dong, Y.,\nand Tang, J. GLM-130b: An open bilingual pre-trained\n10\nDenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nmodel. In The Eleventh International Conference on\nLearning Representations (ICLR), 2023. URL https:\n//openreview.net/forum?id=-Aw0rrrPUF.\nZhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H.,\nZhang, R., and Susskind, J. An attention free transformer,\n2021.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-\nhaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,\nKoura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,\nL. Opt: Open pre-trained transformer language models,\n2022.\n11\n"
  },
  {
    "title": "ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models",
    "link": "https://arxiv.org/pdf/2403.02084.pdf",
    "upvote": "10",
    "text": "ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models\nJiaxiang Cheng\nPan Xie\u2020\nXin Xia\nJiashi Li\nJie Wu\nYuxi Ren\nHuixia Li\nXuefeng Xiao\nMin Zheng\nLean Fu\nByteDance Inc\n{chengjiaxiang,xiepan.01,xiaxin.97,lijiashi,wujie.10}@bytedance.com\n{renyuxi.20190622,lihuixia,xiaoxuefeng.ailab,zhengmin.666,fulean}@bytedance.com\nResolution Interpolation: generation images with resolutions below training resolution.\nResolution Extrapolation: generation images with resolutions above training resolution.\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\nInsert\nRealistic Vision (SD1.5)\nDreamlike (SD1.5)\nAnimeArtXL (SDXL)\nDreamshaperXL (SDXL)\n1024x1024\n(Realism)\nResAdapter\n768x1024\n(Realism)\n1024x768\n(Realism)\n1024x768\n1024x1024\n(Dreamlike)\n768x768\n(Dreamlike)\n1280x1280\n(AnimeArtXL)\n1280x1536\n(AnimeArtXL)\n1280x1024\n1536x1536\n(DreamshaperXL)\n512x512\n768x1024\n(Dreamlike)\n1536x1280\n(DreamshaperXL)\n512x512\n256x256\n256x512\n384x512\n256x512\n256x512\n384x512\n384x512\n768x512\n768x512\n128x128 128x128\n128x128\n128x128\nFigure 1. We propose ResAdapter, a plug-and-play adapter for enabling the resolution extrapolation and interpolation of diffusion models\nof arbitrary style domain. Resolution Extrapolation: generate images with resolutions above their trained resolution (e.g., 1536 \u00d7 1536).\nResolution Interpolation: generate images with resolutions below their trained resolution (e.g., 128 \u00d7 128). Once learning resolution\npriors, ResAdapter can be integrated into diffusion models to generate images with unrestricted resolutions and aspect ratio while preserving\ntheir original style domain. More results are in Appendix 8.\nAbstract\nRecent advancement in text-to-image models (e.g., Sta-\nble Diffusion) and corresponding personalized technologies\n(e.g., DreamBooth and LoRA) enables individuals to gen-\n\u2020Corresponding author\nerate high-quality and imaginative images. However, they\noften suffer from limitations when generating images with\nresolutions outside of their trained domain. To overcome this\nlimitation, we present the Resolution Adapter (ResAdapter),\na domain-consistent adapter designed for diffusion models\nto generate images with unrestricted resolutions and aspect\n1\narXiv:2403.02084v1  [cs.CV]  4 Mar 2024\nratios. Unlike other multi-resolution generation methods\nthat process images of static resolution with complex post-\nprocess operations, ResAdapter directly generates images\nwith the dynamical resolution. Especially, after learning a\ndeep understanding of pure resolution priors, ResAdapter\ntrained on the general dataset, generates resolution-free im-\nages with personalized diffusion models while preserving\ntheir original style domain. Comprehensive experiments\ndemonstrate that ResAdapter with only 0.5M can process im-\nages with flexible resolutions for arbitrary diffusion models.\nMore extended experiments demonstrate that ResAdapter\nis compatible with other modules (e.g., ControlNet, IP-\nAdapter and LCM-LoRA) for image generation across a\nbroad range of resolutions, and can be integrated into other\nmulti-resolution model (e.g., ElasticDiffusion) for efficiently\ngenerating higher-resolution images. Project link https://res-\nadapter.github.io/.\n1. Introduction\nDiffusion models [13,36,38] have experienced a remark-\nable surge in their capabilities and applications [22,25,29].\nAmong them, Stable Diffusion (SD) [30] and SDXL [27]\nare pre-trained models on the large-scale dataset LAION-\n5B [33], having emerged as powerful generative models.\nAdditionally, the open-source community has been enriched\nby numerous personalized diffusion models from CivitAI [4],\ntrained with DreamBooth [32] or Low-rank Adaptation\n(LoRA) [15]. They are capable of generating imaginative\nhigh-quality images at the training resolution (e.g., 512\u00d7512\nfor SD-based models and 1024 \u00d7 1024 for SDXL-based\nmodels) using the given prompts. However, they often suffer\nfrom limitations when generating images with resolutions\noutside of their trained domain. As shown in Fig.2, the SD-\nbased model and the personalized diffusion model generate\nlower-resolution images (e.g., 256 \u00d7 256) with the poor fi-\ndelity and higher-resolution images (e.g., 1024 \u00d7 1024) with\nthe poor framing and composition. As a result, we can name\nthis phenomena as the resolution domain inconsistent.\nExisting work is categorized into two main research direc-\ntions to address this limitation. The first research line is post-\nprocessing [1,9,16], represented by MultiDiffusion [1] and\nElasticDiffusion [9], where images with resolutions in their\ntrained domain are repeatedly processed and then stitched\ntogether to generate images with flexible resolutions through\noverlap. However, this approach often takes longer infer-\nence time with complex post-process operations. The second\nresearch line is straightforward. Fine-tuning on a broader\nrange of resolutions to empower diffusion models to generate\nresolution-free images with LoRA [15]. However, most per-\nsonalized models in CivitAI [4] do not provide details about\ntheir training datasets. Fine-tuning on the general dataset\nlike LAION-5B [33] inevitably influences their original style\ndomain, which is shown in Fig.2. We name this phenomena\nTable 1. Comparison of ResAdapter and other methods. Do-\nmain consistent: resolution and style domain of generation image\nmaintain consistent for arbitrary diffusion models. Module com-\npatible: compatible with other modules except diffusion models.\nTraining inexpensive: low-cost training. Inference efficient: pro-\ncess images without repeated denoising steps and complex post-\nprocess operations.\nMethod\nDomain\nConsistent\nModule\nCompatible\nTraining\nInexpensive\nInference\nEfficient\nSDXL [27]\n\u00d7\n\u00d7\n\u00d7\n\u2713\nLoRA [15]\n\u00d7\n\u2713\n\u2713\n\u2713\nDiffFit [41]\n\u00d7\n\u00d7\n\u2713\n\u2713\nMixture-of-Diffuser [16]\n\u00d7\n\u00d7\n\u2713\n\u00d7\nMultiDiffusion [1]\n\u00d7\n\u00d7\n\u2713\n\u00d7\nAny-Size-Diffusion [46]\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nElasticDiffusion [9]\n\u00d7\n\u00d7\n\u2713\n\u00d7\nResAdapter\n\u2713\n\u2713\n\u2713\n\u2713\nas the style domain inconsistent.\nCan we train a plug-and-play resolution adapter to\ngenerate images with unrestricted resolutions and as-\npect ratio for arbitrary diffusion models? To answer this\nquestion, we decompose it into three dimensions: (1) resolu-\ntion interpolation: generate images with resolutions below\nthe trained resolution of diffusion models. (2) resolution\nextrapolation: process images with resolutions above the\ntrained resolution of diffusion models. (3) style domain con-\nsistency: generate images without transforming the original\nstyle domain of the diffusion model.\nWe analyze the structure of UNet\u2019s blocks [31], finding\nthat the attention and MLP are both content-sensitive lay-\ners, which are sensitive to the style information of images\ncompared to resolution. But the convolution layers with\nfixed receptive file are resolution-sensitive, meaning they\nare easily influenced by the resolution of generation images.\nLeveraging these finds, we present the resolution convolution\nLoRA (ResCLoRA) for dynamically matching the recep-\ntive filed of convolution and the feature map size of images\nwith flexible resolutions. However, we find that as the res-\nolution increases, the gap about the quality of generation\nimages increases between LoRA [15] and full fine-tuing,\nwhich is as shown in Fig.4. We attribute it into the inability\nof normalization in UNet\u2019s blocks to adapt the statistical\ndistribution of images in resolution extrapolation. According\nto this, we present the resolution extrapolation normalization\n(ResENorm) for reducing the gap between LoRA and full\nfine-tuning in resolution extrapolation. To enable the style\ndomain consistency, we optimize the position of ResCLoRA\nand ResENorm insertions on UNet\u2019s blocks to guide them to\nlearn resolution priors ignoring the style information from\nthe general datasets.\nBy integrating these two optimized methods, we can ob-\ntain a plug-and-play domain-consistent resolution adapter\n(ResAdapter), which expands the range of resolution do-\n2\nDomain Consistent\nDomain Distribution of Multi-Resolution Generation Images\nDomain Consistent\nDreamlike\nLoRA(Dreamlike)\nResAdapter(Dreamlike)\nSD1.5\nResAdapter\n(Dreamlike)\nBaseline\n(Dreamlike)\nLoRA\n(Dreamlike)\nResAdapter\n(SD1.5)\nBaseline\n(SD1.5)\n1024x1024\n(Duplicated Layout)\n1024x1024\n(Style Conflict)\n1024x1024\n(Duplicated Layout)\n512x512\n(SD1.5 Domain)\n256x256\n(Poor Fidelity)\n1024x1024\n512x512\n256x256\n1024x1024\n1024x1024\nDreamlike Domain\nSD1.5 Domain\nResAdapter(SD1.5)\nDomain Consistent\nDomain Consistent\n256x256\n512x512\n1024x1024\n256x256\n512x512\n1024x1024\nDomain Inconsistent\nDomain Inconsistent\nMean and Variance of Generation Images\nDomain \nConsistent\nDomain Distribution of Multi-Resolution Generation Images\nDomain \nConsistent\nResAdapter\n(SD1.5)\nBaseline\n(SD1.5)\nBaseline\n(Dreamlike)\nResAdapter\n(Dreamlike)\nVanilla LoRA\n(Dreamlike)\nSD1.5 \nDomain\nDreamlike \nDomain\n256x256\n512x512\n1024x1024\n256x256\n512x512\n1024x1024\nMean and Variance of Generation Images\n256x256\n512x512\n1024x1024\n256x256\n512x512\n1024x1024\n256x256\n512x512\n1024x1024\nGroups\nSD1.5\n1024x1024\n(Duplicated Composition)\n512x512\n(SD1.5 Domain)\n256x256\n(Poor Fidelity)\n1024x1024\n512x512\n256x256\nResAdapter (SD1.5)\nDomain Consistent\nDomain Inconsistent\nDreamlike\nVanilla LoRA (Dreamlike)\nResAdapter (Dreamlike)\n1024x1024\n(Duplicated Composition)\n1024x1024\n(Style Conflict)\n512x512\n1024x1024\nDomain Consistent\nDomain Inconsistent\nFigure 2. Motivation. We explore the domain of images generated by SD1.5 (Left) [30] and Dreamlike (Right) at resolutions of 256 \u00d7 256,\n512 \u00d7 512 and 1024 \u00d7 1024. Dreamlike is the personalized diffusion model based on SD1.5. ResAdapter and vanilla LoRA are both\ntrained on LAION-5B [33], their structures are shown in Fig.3. Baselines transform domains at resolutions of 256 \u00d7 256 and 1024 \u00d7 1024.\nResAdapter keep domain consistent at different resolutions. Vanilla LoRA [15] injects style priors from LAION-5B and influences the\nDreamlike domain, resulting to low-quality images with the style conflict.\nmain from the diffusion models without transforming their\noriginal style domains. Our main experiments demonstrate\nthat after learning resolution priors, ResAdapter with only\n0.5M can expand the generation resolution of arbitrary dif-\nfusion models based SD from 128 \u00d7 128 to 1024 \u00d7 1024\nand scale the SDXL-based diffusion models from 256 \u00d7 256\nto 1536 \u00d7 1536, as shown in Fig.1. Our extensive experi-\nments demonstrate that ResAdapter is compatible with other\nmodules (e.g., ControlNet [44] for conditional generation,\nIP-Adapter for image generation based on the image prompt\nand LCM-LoRA [23] for accelerating generation), and even\ncan be integrated into other multi-resolution models (e.g.,\nElasticDiffusion [9]) for efficiently generating 2048 \u00d7 2048\nhigh-resolution images. Detailed comparison with other\nrelated work is summarized in Table 1.\nOur contributions can be summarized as follows:\n\u2022 We present a plug-and-play domain-consistent Re-\nsAdapter for generating images of resolution interpola-\ntion and extrapolation with diffusion models.\n\u2022 ResAdapter enables diffusion models of arbitrary style\ndomain to generate images of unrestricted resolution\nand aspect ratio without transforming their style do-\nmain.\n\u2022 ResAdapter is lightweight and without complex post-\nprocess operations.\nWe can train it for only 0.5M\nwith low-cost consumption and efficiently inference\nresolution-free images.\n\u2022 ResAdapter is compatible with other modules to gen-\nerate images with flexible resolution, such as Con-\ntrolNet [44], IP-Adapter [43] and LCM-LoRA [23],\neven can optimize generation efficiency of other multi-\nresolution models such as ElasticDiffusion [9].\n2. Related Work\n2.1. Text-to-Image Generation\nThe rapid development of Artificial Intelligence Gener-\native Component (AIGC) has attracted growing interest in\nText-to-Image Generation (TTI). Early GAN-based methods\n[8,26,28] employ small-scale data for training but encounter\nchallenges in adapting to large-scale data due to the instabil-\nity of the adversarial training process. Autoregressive-based\nmethods [6,19,39] learn the latent distribution of discrete\nlatent spaces but take more inference costs. Recently, diffu-\nsion models [5,13,17,38] has emerged as the state-of-the-art\n(SOTA) model in TTI field. The diffusion models repre-\nsented by Stable Diffusion [30] and SDXL [27] contribute\nto high-resolution image generation. With the advent of\npersonalized techniques [15, 32], they are capable of gen-\nerating imaginative images. However, they still encounter\nlimitations in resolution extrapolation and interpolation.\n2.2. Multi-Resolution Image Generation\nExisting work of multi-resolution image generation\nmainly utilizes post-processing to generate images beyond\nthe training resolution. Mixture-of-Diffusers [16] and Mul-\ntiDiffusion [1] utilize pre-trained Stable Diffusion [30] to\ngenerate 512x512 images multiple times, and overlap to gen-\nerate high-resolution landscape images. But this also lead\nto duplicated objects. Any-Size-Diffusion [46] fine-tunes\non multi-aspect ratio images, and generates high-resolution\nimages throught implicit overlap. ElasticDiffusion [9] opti-\nmizes the post-processing process and can generate lower\nresolution images. Compared to these work, our ResAdapter\ndoes not require post-processing that take more inference\ntime and can be integrated into any personalized model. Re-\nsAdapter can even be combined with these work to optimize\ninference time of generating higher resolution images.\n3\n2.3. Parameters-Efficient Fine-Tuning\nWith the emergence of large-scale datasets [33], the pa-\nrameter count of diffusion models has reached the billion\nlevel. Full-parameter fine-tuning [27] takes high training\ncosts in specific downstream tasks, while also leading to\ncatastrophic forgetting [7, 24, 35]. There are two kind ap-\nproaches of PEFT applied to diffusion models: One is\nAdapter [2, 14, 15]. Inserting extra modules on the base\nmodel. LoRA [15] inserts two low-rank matrices on the\nquery and value of the attention to learn the knowledge of\nthe downstream task. Another is Partial Parameter Tun-\ning [20,41,42]. DiffFit [41] unfreezes partial weights and\nbiases of the base model to learn the new domain. However,\nthey can not integrated into any personalized model. They\nboth transform the style domain of the personalized model,\nleading to the style conflict, shown in Fig.2 and Fig.16.\n3. Background\nIn this section, we provide a brief review of the definition\nand principles of Denoise Diffusion Model [13] (Sec.3.1)\nand Stable Diffusion Model [30] (Sec.3.2).\n3.1. Denoise Diffusion Model\nThe generation process of the diffusion model involves\nboth forward diffusion and reverse denoise processes. Given\na data sample x0 \u223c qdata(x), diffusion model gradually in-\njects small gaussian noise into data and generates samples\nthrough reverse denoise. Specifically, the forward diffusion\nprocess of the diffusion model is controlled by a Markov\nchain as q(xt|xt\u22121) = N(xt; \u221a1 \u2212 \u03b2txt\u22121, \u03b2tI), where\n\u03b2t is a variance schedule between 0 and 1. The data dis-\ntribution qdata(x) can be transformed into a marginal dis-\ntribution q(xt|x0) by employing reparameterization tricks.\nUsing notation \u03b1t := 1 \u2212 \u03b2t and \u00af\u03b1t := Qt\ns=1 \u03b1s, we have\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I).\nConsidering the reverse process, the diffusion model\nlearns to progressively reduce small gaussian noise as:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03c32\nt I), where \u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t (xt \u2212\n\u03b2t\n\u221a1\u2212\u00af\u03b1t \u03f5\u03b8(xt, t).\nThe corresponding objec-\ntive function is the variational lower bound of the neg-\native log-likelihood.\nWe get the equation as L(\u03b8) =\nP\nt DKL(q(xt\u22121|xt, x0)|p\u03b8(xt\u22121|xt)) \u2212 p\u03b8(x0|x1), where\nDKL denotes KL divergence between distribution p and q.\nFurthermore, through the parameterization \u00b5\u03b8(xt, t), the loss\nfunction can be simplified as:\nLsimple = Ex0,\u03f5,t[||\u03f5 \u2212 \u03f5\u03b8(\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, t)||2]\n(1)\nThe training objective in this formulation is minimizing the\nsquared error between the gaussian noise and the estimated\nnoise of the noise-added samples.\n3.2. Stable Diffusion Model\nTo reduce the training cost of the diffusion model and\ngenerate high-resolution images, Stable Diffusion (SD) [30]\nencodes the image using variational autoencoder [39]. SD\nperforms forward diffusion and reverse denoise in the latent\nspace. Specifically, given a data x0 \u223c qdata(x), the encoder\nE encodes the image into latent representation z0 = E(x0).\nFor the latent representation \u02dcz generated by the diffusion\nmodel in the latent space, the decoder D can reconstruct\nit into an image \u02dcx. The encoder usually downsamples the\nimage by a factor f = 8 in SD. According to Sec.3.1, we\nhave a simple loss function for SD:\nLsimple = Ez0,\u03f5,t[||\u03f5 \u2212 \u03f5\u03b8(\u221a\u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, t)||2]\n(2)\nThe architecture of UNet\u2019s blocks is shown in Fig.3. Each\nblock catains three components: resnet layer Wr [10], atten-\ntion layer Wa [40], downsampler Wd or upsampler Wu.\n4. Method\nIn this section, we delve into our proposed plug-and-\nplay domain-consistent ResAdapter, which enables diffusion\nmodels of arbitrary style domain to generate images with\nunrestricted resolutions and aspect ratio. In Sec.4.1, we\nintroduce ResCLoRA, which enables diffusion models to\ngenerate images with resolution interpolation. In Sec.4.2,\nwe introduce ResENorm, which compensates for the lack\nof capability about ResCLoRA in resolution extrapolation.\nIn Sec.4.3, we present a simple multi-resolution training\nstrategy, which can effectively make diffusion models gen-\nerate images with flexible resolutions through only one Re-\nsAdapter.\n4.1. Resolution Interpolation\nVanilla LoRA [15] enables the base model (e.g., SD [30]\nand SDXL [27]) to generate high-quality style images. As\nshown in Fig.3(b), vanilla LoRA is inserted into the query,\nvalue, key and output layers of the attention block to learn\nthe style domain distribution of images. It is defined as\nW\u2032\na = Wa + \u2206W = Wa + ABT , where A \u2208 Rm\u00d7r,\nB \u2208 Rn\u00d7r are two rank-decomposition matrices, r rep-\nresents the rank of matrices. However, the vanilla LoRA\ntrained on the general datasets can not be integrated into\nother personalized diffusion models, which influences their\noriginal style domain. As shown in Fig.2, vanilla LoRA\ntrained on LAION-5B [33] transforms the domain of Dream-\nlike to the domain of SD1.5 and generates bad quality images\nwith style conflicts.\nResCLoRA can be integrated into any personalized model\nto enable resolution interpolation for high-quality images\nwithout transforming the style domain. As described in\nSec.1, the reason that leads to the poor fidelity of images\nwith resolution interpolation is that the convolution with\n4\nE\nMixed\nResolutions\nMSE Loss\nIteratively Denoise\nD\nGroup Norm\nConv2D\nResNet \nBlock\nLayer Norm\nMulti-Head\nAttention\nLayer Norm\nFFN\nDown(Up)\nSampler\nLoRA\n\ud83d\udd25\n\u2744\n\u2744\n\u2744\n\u2744\n\u2744\n\u2744\n\u2744\nAttention\nBlock\nBase Model (Frozen)\nResAdapter\nPersonalized  Model\nTraining\nInference\n(a) Pipeline of  ResAdapter\n(b) Vanilla LoRA\n\ud835\udc65!\n\ud835\udc67\"\n\ud835\udf16\"\n\ud835\udf16\n\ud835\udc67#\n\ud835\udc65\nLoRA\n\ud83d\udd25\nGroup Norm\nConv2D\nResNet \nBlock\nLayer Norm\nMulti-Head\nAttention\nLayer Norm\nFFN\nDown(Up)\nSampler\n\u2744\n\u2744\n\u2744\n\u2744\n\ud83d\udd25\n\u2744\n\u2744\nAttention\nBlock\n(c) ResAdapter\nLoRA\n\ud83d\udd25\nFigure 3. Overview of ResAdapter. Left: Pipeline of ResAdapter. ResAdapter based on the frozen model (e.g., SD [30] or SDXL [27])\nlearns resolution priors from mixed-resolution general datasets, and can be integrated into arbitrary personalized models to generate\nmulti-resolution images. Right: Architecture comparison between ResAdapter and the vanilla LoRA [15]. Compared to vanilla LoRA,\nResAdapter is only inserted to downsampler and upsampler, and it unfreezes the group normalization of resnet blocks.\nGap\nGap\nFigure 4. High-quality images represent without poor fidelity or\ncomposition. We train SD1.5 with LoRA [15] and full fine-tuning\non LAION-5B [33] ranging from 128\u00d7128 to 1024\u00d71024 resolu-\ntion. We sample 100 images using for each resolution, respectively.\nWe find that as the resolution increases, the gap between LoRA and\nfull fine-tuning becomes larger.\nthe fixed receptive field is sensitive to the resolution of im-\nages. According to this, ResCLoRA is inserted into the\nconvolution layers of UNet\u2019s blocks to learn resolution pri-\nors. To prevent as much as possible ResCLoRA from cap-\nturing the style domain of the general datasets, it is only\ninserted into the convolution layers in downsampler and up-\nsampler. We define ResCLoRA as W\u2032\nd = Wd + ABT and\nW\u2032\nu = Wu + ABT , which is shown in Fig.3(c). Compared\nto the style information, the resolution information is low-\nlevel knowledge. Thus, ResCLoRA with only 0.4M can\nprovide the rich resolution prior for the personalized model,\nadaptively adjusting the receptive field of the convolution in\nUNet\u2019s blocks to match the feature map size of generation\nimages while preserving their style domain.\n4.2. Resolution Extrapolation\nOur initial experiment finds that there is a large gap as\nthe resolution increases between LoRA and full fine-tuning,\nas shown in Fig.4. This means that only ResCLoRA does\nnot enable the resolution extrapolation ability of the per-\nsonalized model. For example, ResCLoRA integrated into\nthe diffusion model still generates higher-resolution images\nwith poor framing and composition. Inspired by [3], we find\nthat the failure of the resolution extrapolation is limited by\nthe ability of normalization layers. Existing normalization\nlayers can not adapt to the statistical distribution of feature\nmaps of higher-resolution images.\nHowever, we find that if all the normalization layers in\nUNet\u2019s blocks are trained on the general dataset, the trained\nweights are incompatible with other personalized diffusion\nmodels, which leads to low-quality images with poor style\ncolor, as shown in Appendix.9. However, we find that all\nnormalization layers of UNet\u2019s blcoks trained on LAION-\n5B [34] are not compatible with the other parameters of the\npersonalized model, which still leads to low-quality images\nwith poor style color, as shown in Fig.16. In order to keep\nthe original style domain of generation images, we need\nto maintain partial normalization layers of the personalized\nmodel. As shown in Fig.3(c), we only open group normal-\nization of resnet layer, which is named as ResENorm. It\nnot only reduce the gap about the resolution prior between\nResCLoRA and full fine-tuning, but also helps retain the\nstyle domain of the personalized model. Additionally, we\nonly train ResENorm in resolution extrapolation for better\nadapting to the statistical distribution of the feature map\n5\nof higher-resolution images. After that, ResENorm with\nResCLoRA can improve the poor framing and composition\nof generation images. Especially, ResENorm only occupies\n0.1M parameters but make effects for reducing the gap in\nresolution extrapolation.\n4.3. Multi-Resolution Training\nTo enable multi-resolution image generation for single\nResAdapter, we propose a simple mixed-resolution training\nstrategy, as shown in Fig.3(a). For SD [30], we train on the\nmixed datasets with common resolutions from 128 \u00d7 128 to\n1024 \u00d7 1024 with unrestricted aspect ratio. For SDXL [27],\nwe train on the mixed datasets with common resolutions\nfrom 256 \u00d7 256 to 1536 \u00d7 1536 with unrestricted aspect ra-\ntio. The multi-resolution training strategy allows ResAdapter\nto learn multi-resolution knowledge simultaneously and pre-\nvents catastrophic forgetting [7,24,35].\nOur experiments find that lower and higher resolution im-\nages (e.g., 128 \u00d7 128 and 1024 \u00d7 1024 for SD) are more dif-\nficult to train. To alleviate this phenomenon, we use a simple\nprobability function to sample images at the different train-\ning resolution. It is defined as p(x) = |x\u2212r|2/ PN\ni |xi\u2212r|2,\nwhere r represents the standard resolution (e.g., 512 \u00d7 512\nfor SD). This can improve the probability of selecting lower\nor higher resolution images resolution during the multi-\nresolution training process. More training details are pro-\nvided in Sec.5.1.\n5. Experiment\nIn this section, we present the experimental setup and\nexperimental results of ResAdapter. In Sec.5.1, we describe\nthe experimental setup in detail, including training details,\nevaluation metrics, and the selection of personalized mod-\nels. In Sec.5.2, we show the main experimental results. We\ncompare ResAdapter with other multi-resolution image gen-\neration models as well as the original personalized model.\nIn Sec.5.3, we show the extended experimental results. That\nis the application of ResAdapter in combination with other\nmodules. In Sec.5.4, we perform ablation experiments about\nmodules and alpha of ResAdapter.\n5.1. Experiment Setup\nTraining Details. We train ResAdapter using the large-\nscale dataset LAION-5B [33]. Considering most structures\nof personalized models in the open-source community, we\nchoose SD1.5 [30] and SDXL [27] as the base models. For\nSD1.5, we train on images with 128\u00d7128, 256\u00d7256, 384\u00d7\n384, 768\u00d7768 and 1024\u00d71024 resolutions. For SDXL, we\nexpanded the training resolution range to 256 \u00d7 256, 384 \u00d7\n384, 512 \u00d7 512, 768 \u00d7 768, 1280 \u00d7 1280, 1408 \u00d7 1408, and\n1536 \u00d7 1536. Meanwhile, the training dataset contains im-\nages with different ratios such as 4:3, 3:4, 3:2, 2:3, 16:9 and\n9:16, etc. For SD1.5 and SDXL, we both use a batch size\nTable 2. Personalized models used for evaluation. We selected\nrepresentative personalize models from CivitAI [4] for evaluation\nin domain-consistent experiments, which cover domains ranging\nfrom 2D anime to real photographs.\nModel Name\nDomain\nModel Type\nTraining Type\nDreamshaper\nStylistic\nSD1.5 [30]\nDreamBooth\nDreamlike\nIllustration\nDreamBooth\nCuteyukimix\nCute\nDreamBooth\nRealisticVision\nRealist\nDreamBooth\nSamaritanXL\n3D Cartoon\nSDXL1.0 [27]\nLoRA\nAnimeArtXL\nAnime\nLoRA\nJuggernautXL\nRealistic\nDreamBooth\nDreamshaperXL\nStylistic\nDreamBooth\nof 32 and a learning rate of 1e-4 for training. We use the\nAdamW optimizer [18] with \u03b21 = 0.95, \u03b22 = 0.99. The\ntotal number of training steps is 20,000. Since ResAdapter\nis only 0.5M of trainable parameters, we train it for less than\nan hours on 8xA100 GPUs.\nEvaluation Metrics. For experiments comparing Re-\nsAdapter with the personalized model, we hire 5 humans to\nparticipating in the qualitative evaluation. For experiments\ncomparing ResAdapter and the other multi-resolution image\ngeneration models, we refer to ElasticDiffusion [9] and use\nFr\u00b4echet Inception Distance (FID) [12] and CLIP Score [11]\nas evaluation metrics. They evaluate the quality of the gen-\nerated images and the degree of alignment between the\ngenerated images and prompts. For other multi-resolution\ngeneration models, we chose MultiDiffusion (MD) [1] and\nElasticDiffusion (ED) [9] as baselines.\nPersonalized Models. In order to demonstrate the effec-\ntiveness of our ResAdapter, we choose multiple personalized\nmodels from Civitai [4], which cover a wide domains range\nfrom 2D animation to realistic photography. More detailed\ninformation is provided in Table 2.\n5.2. Main Results\nComparison with Multi-Resolution Generation Mod-\nels. For qualitative results, see Fig.5. We compare the image\nperformance of MultiDiffusion (MD) [1] and ElasticDif-\nfusion (ED) [9] with our ResAdapter at the resolutions of\n256 \u00d7 256, 768 \u00d7 768 and 1024 \u00d7 1024.\nThe qualitative results demonstrate that ResAdapter gen-\nerates multi-resolution images of better quality compared\nwith MD and ED. MD generates higher-resolution images\nwith the poor framing and composition, and can not generate\nlower-resolution images than the training resolution. ED\ngenerates the images with more inference time. ResAdapter\nsignificantly improves the quality of multi-resolution im-\nages. For quantitative results, see Table 3. The results show\nthat ResAdapter outperforms MD and ED in terms of FID\nand CLIP Score. About the latency time, ResAdapter with-\nout post-processing is, on average, 9 times faster compared\n6\n1024 x 1024\nResAdapter\n(SD1.5)\nSD1.5\nElasticDiffusion\n(SD1.5)\nMultiDiffusion\n(SD1.5)\n768x768\n256x256\nPrompt: A young badger delicately sniffing a yellow rose.\nFigure 5. Qualitative results of ResAdapter and other multi-resolution generation models. Blank images indicate that the model can not\ngenerate images with that resolution.\nTable 3. Quantitative results on LAION-COCO [34] at resolutions\nof 256 \u00d7 256 and 1024 \u00d7 1024. #Calls represents the number of\ninference iterations at each noise reduction step. We measure the\nlatency time of one step on an A100-80G.\nSize\nMethod\nFID(\u2193)\nCLIP(\u2191)\n#Call\nLatency Time/(s)\n256x256\nSD1.4 [30]\n54.06\n21.43\n2\n0.025\nSDXL [27]\n175.87\n14.60\n2\n0.038\nElasticDiffusion1.4 [9]\n23.77\n26.30\n2\n0.288\nResAdapter1.4\n23.01\n26.98\n2\n0.025\n512x512\nSD1.4 [30]\n20.50\n27.33\n2\n0.0714\nResAdapter1.4\n20.53\n27.32\n2\n0.0714\n1024x1024\nSD [30]\n47.01\n25.70\n2\n0.1322\nSDXL [27]\n25.58\n28.06\n2\n0.1322\nMultiDiffusion1.4 [1]\n37.70\n26.96\n162\n2.50\nElasticDiffusion [9]\n27.76\n26.07\n33\n1.16\nResAdapter1.4\n26.89\n27.26\n2\n0.1322\nto ED. We measure the latency time of one step on single\nA100-80G.\nComparison with Personalized Models. For qualita-\ntive results, see Fig.6. To ensure the fairness of the experi-\nments, we generate multi-resolution images using prompts\nTable 4. Human side-by-side evaluations about text-to-image\ngeneration of ResAdapter and the personalized model.\nModel Name\nSize\nRatio\nGood(G)\nSame(S)\nBad(B)\n(G+S)/(B+S)\nRealisticVision\n256\n4:3\n3321\n1090\n589\n2.63\n3:4\n3302\n1102\n596\n2.59\n1024\n16:9\n2877\n1563\n560\n2.09\n9:16\n2978\n1531\n491\n2.23\nSamaritanXL\n384\n4:3\n4613\n338\n49\n12.79\n3:4\n4684\n249\n67\n15.61\n1536\n16:9\n2863\n1566\n571\n2.07\n9:16\n3009\n1071\n920\n2.05\nfrom Civitai [4]. These images are generated by ResAdapter\nand the personalized model. Lower-resolution images (e.g.,\n256 \u00d7 256, 384 \u00d7 384) generated by the personalized model\nare significantly lower in terms of the images of the fidelity,\nwhile higher-resolution images (e.g., 1536 \u00d7 1536) suffer\nfrom the poor framing and composition. After integrating\nour ResAdapter into the personalized model, the fidelity and\nthe composition of generation images are significantly im-\n7\n1024 x 1024\n384 x 384\n256 x 256\n1536 x 1536\n384 x 384\n256 x 256\n288 x 512\n288 x 384\nOurs (ResAdapter)\nBaseline (Personalized Models)\n1024 x 1024\n384 x 384\n256 x 256\n288 x 384\n1536 x 1536\n384 x 384\n256 x 256\n288 x 512\n(RealisticVision, Realism Style) Edgy pinterest woman, 20 years old, wearing a oversized sweater, happy\u2026\n(Dreamshaper, Multiple Style) Portrait of a girl, the most beautiful in the world\u2026\n(Cuteyukimix, Cute Style) solo, 1girl, platinum-blonde hair, long hair, flowing hair, wind, winter coat\u2026\n(Dreamlike, Inbetweening Style) dreamlikeart a painting of illustration of Goku wearing a skirt cinematic lighting.\n(JuggernautXL, Realism Style) beautiful lady, (freckles), big smile, ruby eyes, pigtails hair, dark makeup\u2026\n(DreamshaperXL, Multiple Style) Portrait photo of muscular bearded guy in a worn mech suit\u2026\n(SamathertalXL, 3DCartoon Style) Handsome young businessman smiling and gesturing a double thumbs up\u2026\n(AnimeArtXL, Animation Style) girl sitting on the hill looking at sky, with her hair blowing in clear day, with cat\u2026\nFigure 6. Qualitative results. We compare the multi-resolution images generated by ResAdapter and the personalized models. Left:\nGeneration images from ResAdapter integrated into the personalized model. Right: Generation images from the original personalized model.\nSome prompts are edited for clarity. XL represent the personalize model based on SDXL [27]. More information about the personalized\ndiffusion models is in Table 2 .\n8\n(c) ResAdapter with LCM-LoRA Experiments\n512 x 512\nBaseline\n282 x 423\n672x1008\n(a) ResAdapter with ControlNet Experiments\n256 x 256\nR-Adapter\nR-Adapter\nBaseline\nBaseline\nResAdapter\nResAdapter\nBaseline\nDuplicated Composition\nPoor Fidelity\nOurs\nOurs\nInput\nResAdapter\nBaseline\n256x256\n256x256\n256x256\nResAdapter\nBaseline\nResAdapter\nBaseline\n(b) ResAdapter with IP-Adapter Experiments\nFigure 7. Qualitative results of extended experiments with ResAdapter. Top: Image-to-image tasks with ConrtolNet [44]. The condition is\ncanny images at different resolution. Middle: Image variation tasks with IP-Adapter [43]. We resize the input image from 1024x1024 to\n256x256. Bottom: Accelerating text-to-image tasks with LCM-LoRA [23]. We generate images in 4 steps. More results in Appendix 8.\nproved. ResAdapter enables the resolution extrapolation and\ninterpolation of the personalized model. For quantitative\nresults, see Table 2. We evaluate the image quality by four\ncriteria, which are the fidelity, the composition, the prompt\nalignment and the style domain consistency. The quan-\ntitative results demonstrate that ResAdapter significantly\noutperforms the personalized model, particularly in lower-\nresolution images.\n5.3. Extended Results\nResAdapter with ControlNet. ControlNet [44] is a con-\nditional control module that can utilize conditional images to\ncontrol the generation of layout-specific images for SD [30].\nAs shown in Fig.7, ControlNet generates low-quality images\nwith poor fidelity and composition in the image-to-image\ntask. While ResAdapter is compatible with ControlNet to\nenable the resolution extrapolation and interpolation, im-\nproving the quality of images.\nResAdapter with IP-Adapter. IP-Adapter [43] is a\nadapter for image generation with the image prompt. As\nshown in Fig.7, ResAdapter with IP-Adapter can generate\nhigh-quality images in the image variation task.\nResAdapter with LCM-LoRA. LCM-LoRA [23] is a\nmodule for accelerated image generation capable of gener-\nating high-quality images in 4 steps. As shown in Fig.7,\nResAdapter is integrated into the personalized model and\ncompatible with LCM-LoRA. ResAdapter improves the fi-\ndelity of lower-resolution image while not degrades the qual-\nity of 512 \u00d7 512 images.\nResAdapter with ElasticDiffusion. ResAdapter com-\nbined with other multi-resolution image generation models\nof post-processing can optimize the inference time. Specif-\n9\n2048 x 2048\n(ED)\n2048 x 2048\n(ED)\n2048 x 2048\n(ED)\n2048 x 2048\n(Speed up 44%)\n(ResAdapter with ED)\n2048 x 2048\n(Speed up 44%)\n(ResAdapter with ED)\n2048 x 2048\n(Speed up 44%)\n(ResAdapter with ED)\nFigure 8. Qualitative results about ResAdapter with ElasticDiffusion (ED) [9]. Top: DreamshaperXL (ED) generates 2048 \u00d7 2048\nimages by the post-process technology and uses 1024 \u00d7 1024 as the standard resolution. Bottom: DreamshaperXL (ResAdapter with ED),\ngenerates 2048 \u00d7 2048 images by the post-process technology and uses 768 \u00d7 768 as the standard resolution, which efficiently optimizes\nthe inference time.\nically, ElasticDiffusion (ED) [9] requires to inference the\n1024\u00d71024 images multiple times, and overlaps them to get\nthe 2048 \u00d7 2048 images with the post-process technology,\nwhich takes much inference time. In order to optimize the in-\nference time, we combine ResAdapter with ED to inference\nthe 768 \u00d7 768 images same times and overlap them to get\n2048 \u00d7 2048 images. As shown in Fig.8, ResAdapter with\nED can generate 2048 \u00d7 2048 images while no degradation\nof image quality compared with ED. Our ResAdapter can\nspeed up the inference time by 44%. This demonstrates that\nResAdapter can be flexibly applied to multiple scenarios.\n5.4. Ablation Study\nFor the modules of ResAdapter, we ablate on ResCLoRA\nand ResENorm, as shown in Fig.9(a). Without ResCLoRA or\nResENorm, the duplicated composition of generation images\nstill exists, which demonstrate the importance of their simul-\ntaneous presence. Compared with baseline, ResAdapter can\ngenerate images without transforming the original style do-\nmain. For alpha \u03b1r for ResAdapter, we make the ablation\nstudy on \u03b1r, as shown in Fig.9(b). We find the quality of\ngeneration images increases as \u03b1r from 0 to 1.\n6. Conclusion\nIn this paper, we present a plug-and-play domain-\nconsitent ResAdapter for diffusion models of arbitrary style\ndomain, which enables the resolution extrapolation and inter-\npolation of generation images. Our experiments demonstrate\nthat after a low-cost training, ResAdapter with only 0.5M\ncan be integrated into diffusion models to generate high-\nquality images of unrestricted resolutions and aspect without\ntransforming the original style domain. Our extended ex-\nperiments also demonstrate ResAdapter is compatible with\nother modules (e.g., ControlNet [44], IP-Adapter [43] and\nLCM-LoRA [23]). In addition, ResAdapter can be com-\nbined with other multi-resolution image generation models\n(e.g., ElasticDiffusion [9]) to optimize inference time for\nhigh-resolution 2048 \u00d7 2048 images.\n10\nBaseline\nBaseline\nw/o ResENorm\nResAdapter\n256x256\n1024x1024\n(a) Ablation Study for Modules of ResAdapter\n(b) Ablation Study for ResAdapter Alpha\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAlpha\n1024x1024\n(Duplicated Composition)\n1024x1024\n(Duplicated Composition)\n1024x1024\n(Domain Consistent)\n1024x1024\n(Duplicated Composition)\n512x512\nw/o ResCLoRA\nFigure 9. Ablation study. Top: We ablate on the modules of ResAdapter. Baseline represents Dreamshaper, which is a personalized\ndiffusion model based on SD1.5 [30]. The third column represents only ResCLoRA integrated into the model. The fourth column represents\nonly ResENorm integrated into the model. The fifth column represents both them integrated into the model. Bottom: We ablate on the alpha\nof ResAdapter \u03b1r from 0 to 1 at lower and higher resolutions.\nReferences\n[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. 2023. 2, 3, 6, 7\n[2] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib-\ning Song, Jue Wang, and Ping Luo. Adaptformer: Adapting\nvision transformers for scalable visual recognition. Advances\nin Neural Information Processing Systems, 35:16664\u201316678,\n2022. 4\n[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhi-\njian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-\ntuning of long-context large language models. arXiv preprint\narXiv:2309.12307, 2023. 5\n[4] Civitai. Civitai, 2022. 2, 6, 7, 14\n[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. 3\n[6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 12873\u201312883, 2021. 3\n[7] Rui Gao and Weiwei Liu. Ddgr: continual learning with\ndeep diffusion-based generative replay. In International Con-\nference on Machine Learning, pages 10744\u201310763. PMLR,\n2023. 4, 6\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Communi-\ncations of the ACM, 63(11):139\u2013144, 2020. 3\n[9] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez.\nElasticdiffusion: Training-free arbitrary size image genera-\ntion. arXiv preprint arXiv:2311.18822, 2023. 2, 3, 6, 7, 10,\n14\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4\n[11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation metric\nfor image captioning. arXiv preprint arXiv:2104.08718, 2021.\n6\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n6\n11\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3, 4\n[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for nlp. In International Conference on Machine\nLearning, pages 2790\u20132799. PMLR, 2019. 4\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021. 2, 3, 4, 5\n[16] \u00b4Alvaro Barbero Jim\u00b4enez. Mixture of diffusers for scene com-\nposition and high resolution image generation. arXiv preprint\narXiv:2302.02412, 2023. 2, 3\n[17] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in Neural Information Processing Systems,\n35:26565\u201326577, 2022. 3\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[19] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\nWook-Shin Han. Autoregressive image generation using resid-\nual quantization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11523\u2013\n11532, 2022. 3\n[20] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.\nScaling & shifting your features: A new baseline for efficient\nmodel tuning. Advances in Neural Information Processing\nSystems, 35:109\u2013123, 2022. 4\n[21] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances in\nNeural Information Processing Systems, 35:5775\u20135787, 2022.\n14\n[22] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11461\u201311471, 2022. 2\n[23] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von\nPlaten, Apolin\u00b4ario Passos, Longbo Huang, Jian Li, and Hang\nZhao. Lcm-lora: A universal stable-diffusion acceleration\nmodule. arXiv preprint arXiv:2311.05556, 2023. 3, 9, 10, 14,\n20\n[24] Sergi Masip, Pau Rodriguez, Tinne Tuytelaars, and Gido M\nvan de Ven. Continual learning of diffusion models with gen-\nerative distillation. arXiv preprint arXiv:2311.14028, 2023.\n4, 6\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n2\n[26] Augustus Odena, Christopher Olah, and Jonathon Shlens.\nConditional image synthesis with auxiliary classifier gans. In\nInternational conference on machine learning, pages 2642\u2013\n2651. PMLR, 2017. 3\n[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 2,\n3, 4, 5, 6, 7, 8, 14\n[28] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\nvised representation learning with deep convolutional genera-\ntive adversarial networks. 2015. 3\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022. 2\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 4, 5, 6, 7, 9, 11,\n14\n[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 2\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 22500\u201322510,\n2023. 2, 3\n[33] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b: An open large-scale dataset for training next gen-\neration image-text models. Advances in Neural Information\nProcessing Systems, 35:25278\u201325294, 2022. 2, 3, 4, 5, 6, 21\n[34] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 5, 7\n[35] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting\nHua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual\ndiffusion: Continual customization of text-to-image diffusion\nwith c-lora. arXiv preprint arXiv:2304.06027, 2023. 4, 6\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2020. 2\n[37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 14\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2, 3\n12\n[39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 3, 4\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[41] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,\nZhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Unlocking\ntransferability of large diffusion models via simple parameter-\nefficient fine-tuning. arXiv preprint arXiv:2304.06648, 2023.\n2, 4\n[42] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao\nChang, Songfang Huang, and Fei Huang. Raise a child in\nlarge language model: Towards effective and generalizable\nfine-tuning. arXiv preprint arXiv:2109.05687, 2021. 4\n[43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3, 9, 10, 14, 19\n[44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3, 9, 10, 14, 18\n[45] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu. Unipc: A unified predictor-corrector framework\nfor fast sampling of diffusion models. Advances in Neural\nInformation Processing Systems, 36, 2024. 14\n[46] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han,\nYing Li, Songcen Xu, and Hang Xu. Any-size-diffusion:\nToward efficient text-driven synthesis for any-size hd images.\narXiv preprint arXiv:2308.16582, 2023. 2, 3\n13\nAppendix\n7. Detailed Inference Setup\nWe set up detailed inference settings about ResAdapter\nintegrated into the base model (e.g., SD [30] and SDXL [27]),\nthe personalized models [4], ControlNet [44], IP-Adapter\n[43], LCM-LoRA [23], and ElasticDiffusion (ED) [9].\n1. For the base model, we select the corresponding prompt\ngeneration images from SD and SDXL. We reference\nSD to generate images in 50 steps with CFG=7.5 and\nchoose DDIM [37] as the scheduler.\n2. For the personalized model, we select the correspond-\ning prompt from CivitAI [4] to generate imaginative\nimages to stimulate the creativity of the personalized\nmodel. We generate images in 25 steps with CFG=10\nand choose DPM Solver [21] as the scheduler.\n3. For ControlNet, we select the appropriate images and\nprompts from [44]. For the canny condition, for ex-\nample, we extract the canny condition from the input\nimage, where the thresholds are set to 100 and 200.\nWe generate the images in 25 steps with CFG=7.5 and\nchoose UniPC [45] as the scheduler.\n4. For IP-Adapter, we select the input image and prompt\nfrom [43]. we complete the image variation and in-\npainting tasks. We generate the images in 50 steps with\nCFG=7.5 and choose DDIM as the scheduler.\n5. For LCM-LoRA, we generate images based on the per-\nsonalized model. We generate images in 4 steps with\nCFG=1, and choose LCM scheduler [23].\n6. For ElasticDiffusion, we generate images with the per-\nsonalized model. For example, Dreamshaper-XL gener-\nates a 2048x2048 image and we generate a 1024x1024\nimage for post-processing.\nAfter inserting the Re-\nsAdapter, we generated a 768 \u00d7 768 image for post-\nprocessing. We follow the default post-processing pa-\nrameters of the ED [9], we generated the image in\n25 steps with CFG=7.5, and we choose DDIM as the\nscheduler.\n8. Additional Qualitative Results\nResAdapter with Personalized Models. We present\nadditional qualitative results of the personalized diffusion\nmodel in Fig.10-12. Each comparison set consists of two\nimages: the left image shows the personalized model with\nthe ResAdapter, and the right image shows the personalized\nmodel without the ResAdapter. ResAdapter significantly\nenhances the quality of images with unrestricted resolutions\nand aspect ratio without transforming their original style\ndomain.\nResAdapter with Other Modules. We provide more\nqualitative results for ControlNet in Fig.13, IP-Adapter in\nFig.14 and LCM-LoRA in Fig.15. ResAdapter can enhance\nthe quality of resolution-free generation images with other\nmodules.\n9. Ablation Study for Trained Modules\nThe ablation study for the trainable modules of Re-\nsAdapter is shown in Fig. 16. We visualize the three groups\nof LoRA insertions on the personalized diffusion models.\nResAdapter maintains style domain consistency (Realism\nstyle), while both the ResNet LoRA and the vanilla LoRA\ninfluence its original style domain. Meanwhile, we visualize\nthe two sets of normalization loaded on the personalized dif-\nfusion model (Cute style). ResAdapter does not compromise\nthe image domain, while all normalization layers severely\ndegrade the image quality.\n10. Limitation and Future Work\nIn our experiments, we find that most failure cases occur\nwhen the general prompts are input into the personalized\ndiffusion models. It leads that the image generation capabil-\nity in personalized models is not activated correctly. In this\ncase, the role of ResAdapter in resolution interpolation and\nextrapolation is difficult to highlight. A potential solution\nis to correct the user prompts with a large language model,\nwhich we leave for future work. Additionally, we consider\nto use super-resolution models to speed up generation of\nhigher-resolution images.\n14\n1536x1536\n672x864\n336x432\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nText-to-image, Dreamshaper-XL (SDXL)\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nFigure 10. Qualitative results about the text-to-image generation task. The baseline represents DreamshaperXL. For each pair of images,\nthe left side is from ResAdapter with baseline and the right side is from baseline.\n15\n1024x1024\n336x432\nText-to-image, Dreamlike (SD1.5)\n224x288\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\n1024x1024\n336x432\n224x288\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nFigure 11. Qualitative results about the text-to-image generation task. The baseline represents Dreamlike. For each pair of images, the left\nside is from ResAdapter with baseline and the right side is from baseline.\n16\n1024x1024\n224x288\nResAdapter\nBaseline\nText-to-image, Dreamshaper (SD1.5)\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\n1608x1368\n416x608\n336x432\nText-to-image, AnimeArtXL (SDXL)\nFigure 12. Qualitative results about the text-to-image generation task. The baseline represents Dreamshaper or AnimeArtXL. For each pair\nof images, the left side is from ResAdapter with baseline and the right side is from baseline.\n17\n675x1012\n843x1060\nControlNet, Image-to-image, SD1.5\nControlNet, Image-to-image, SDXL\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\n504x672\n448x672\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nCondition\nOutput\nCondition\nOutput\nFigure 13. Qualitative results about the image-to-image generation task. The baseline represents ControlNet [44]. For each pair of images,\nthe left side is from ResAdapter with baseline and the right side is from baseline.\n18\n256x256\n672x864\n256x384\n+\nIP-Adapter, Image varation, SD1.5\nIP-Adapter, Inpainting, SD1.5\nIP-Adapter, Face consistent, SD1.5\nIP-Adapter, Image varation, SDXL\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nInput\nInput\nMask\nInput\nInput\nOutput\nOutput\nOutput\nOutput\n512x512\nFigure 14. Qualitative results. The baseline represents IP-Adapter [43]. For each pair of images, the left side is from ResAdapter with\nbaseline and the right side is from baseline.\n19\n1104x1360\nLCM-LoRA, Text-to-image, Samaritan3dCartoon (SDXL)\nLCM-LoRA, Text-to-image, Dreamshaper-XL (SDXL)\n384x384\n256x256\n768x768\n384x384\n256x256\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nResAdapter\nBaseline\nFigure 15. Qualitative results about the accelerating text-to-image task. The baseline represents Samaritan3dXL with LCM-LoRA [23]\nor DreamshaperXL with LCM-LoRA. For each pair of images, the left side is from ResAdapter with baseline and the right side is from\nbaseline.\n20\nResAdapter\nResNet LoRA\nVanilla LoRA\nResAdapter\nAll Normalization\n768x768\n1024x1024\nRealisticVision (SD1.5), Realism Style\nCuteyukimix (SD1.5), Cute Syle\nPrompt: monster Baba yaga house with in a forest, dark horror style, black and white.\nPrompt: a cat drinking a pint of beer.\nFigure 16. Left: ResNet-LoRA represents LoRA weights on the resnet block. All-LoRA represents LoRA weights on the resnet layer and\nthe attention layer. Right: All-Normalization represents the layer normalization of attention layer and the group normalization of resnet\nlayer. We train them on LAION-5B [33].\n21\n"
  },
  {
    "title": "TripoSR: Fast 3D Object Reconstruction from a Single Image",
    "link": "https://arxiv.org/pdf/2403.02151.pdf",
    "upvote": "8",
    "text": "TripoSR: Fast 3D Object Reconstruction from a Single Image\nDmitry Tochilkin1\nDavid Pankratz1\nZexiang Liu2\nZixuan Huang1\nAdam Letts1\nYangguang Li2\nDing Liang2\nChristian Laforte1\nVarun Jampani1\u2217\nYan-Pei Cao2\u2217\n1Stability AI, 2Tripo AI\nInput Image\nTripoSR Mesh Output\nTripoSR Mesh Output\nInput Image\nFigure 1. We present TripoSR, a 3D reconstruction model that reconstructs high-quality 3D from single images in under 0.5 seconds. Our\nmodel achieves state-of-the-art performance and generalizes to objects of various types and input images across different domains.\nAbstract\nThis technical report introduces TripoSR, a 3D recon-\nstruction model leveraging transformer architecture for fast\nfeed-forward 3D generation, producing 3D mesh from a\nsingle image in under 0.5 seconds.\nBuilding upon the\nLRM [11] network architecture, TripoSR integrates sub-\nstantial improvements in data processing, model design,\nand training techniques.\nEvaluations on public datasets\nshow that TripoSR exhibits superior performance, both\nquantitatively and qualitatively, compared to other open-\nsource alternatives. Released under the MIT license, Tri-\npoSR is intended to empower researchers, developers, and\ncreatives with the latest advancements in 3D generative AI.\n*Equal advising.\nModel: https://huggingface.co/stabilityai/Tr\nipoSR\nCode: https://github.com/VAST-AI-Research/Tr\nipoSR\nDemo: https://huggingface.co/spaces/stability\nai/TripoSR\n1. Introduction\nThe landscape of 3D Generative AI has witnessed a con-\nfluence of developments in recent years, blurring the lines\nbetween 3D reconstruction from single or few views and\n3D generation [3, 9, 11, 13, 17, 29, 33\u201335]. This conver-\ngence has been significantly accelerated by the introduc-\ntion of large-scale public 3D datasets [4, 5] and advances\nin generative model architectures. Comprehensive reviews\n1\narXiv:2403.02151v1  [cs.CV]  4 Mar 2024\nof these technologies can be found in the literature such as\n[15] and [22].\nTo overcome the scarcity of 3D training data, recent ef-\nforts have explored utilizing 2D diffusion models to cre-\nate 3D assets from text prompts [20, 21, 27] or input im-\nages [17, 23]. DreamFusion [20], a notable example, intro-\nduced score distillation sampling (SDS), employing a 2D\ndiffusion model to guide the optimization of 3D models.\nThis approach represents a pivotal strategy in leveraging 2D\npriors for 3D generation, achieving breakthroughs in gener-\nating detailed 3D objects. However, these methods typically\nface limitations with slow generation speed, due to the ex-\ntensive optimization and computational demands, and the\nchallenge of precisely controlling the output models.\nOn the contrary, feed-forward 3D reconstruction models\nachieve significantly higher computational efficiency [7, 8,\n11\u201314, 17, 19, 24\u201326, 28, 31, 32, 35]. Several recent ap-\nproaches [11, 13, 14, 17, 24, 26, 28, 31, 35] along this di-\nrection have shown promise in scalable training on diverse\n3D datasets. These approaches facilitate rapid 3D model\ngeneration through fast feed-forward inference and are po-\ntentially more capable of providing precise control over the\ngenerated outputs, marking a notable shift in the efficiency\nand applicability of these models.\nIn this work, we introduce TripoSR model for fast feed-\nforward 3D generation from a single image that takes less\nthan 0.5 seconds on an A100 GPU. Building upon the\nLRM [11] architecture, we introduce several improvements\nin terms of data curation and rendering, model design and\ntraining techniques. Experimental results demonstrate su-\nperior performance, both quantitatively and qualitatively,\ncompared to other open-source alternatives. Figure 1 shows\nsome sample results of the TripoSR. TripoSR is made avail-\nable under the MIT license, accompanied by source code,\nthe pretrained model, and an interactive online demo. The\nrelease aims to enable researchers, developers, and cre-\natives to advance their work with the latest advancements in\n3D generative AI, promoting progress within the wider do-\nmains of AI, computer vision, and computer graphics. Next,\nwe introduce the technical advances in our TripoSR model,\nfollowed by the quantitative and qualitative results on two\npublic datasets.\n2. TripoSR: Data and Model Improvements\nThe design of TripoSR is based on the LRM [11], with a se-\nries of technical advancements in data curation, model and\ntraining strategy. We now give an overview of the model\nfollowed by our technical improvements.\n2.1. Model Overview\nSimilar to LRM [11], TripoSR leverages the transformer ar-\nchitecture and is specifically designed for single-image 3D\nreconstruction. It takes a single RGB image as input and\noutputs a 3D representation of the object in the image. The\ncore of TripoSR includes components: an image encoder,\nan image-to-triplane decoder, and a triplane-based neural\nradiance field (NeRF).\nThe image encoder is initialized with a pre-trained vision\ntransformer model, DINOv1 [1], which projects an RGB\nimage into a set of latent vectors. These vectors encode\nthe global and local features of the image and include the\nnecessary information to reconstruct the 3D object.\nThe subsequent image-to-triplane decoder transforms\nthe latent vectors onto the triplane-NeRF representation [2].\nThe triplane-NeRF representation is a compact and expres-\nsive 3D representation, well-suited for representing objects\nwith complex shapes and textures. Our decoder consists\nof a stack of transformer layers, each with a self-attention\nlayer and a cross-attention layer. The self-attention layer al-\nlows the decoder to attend to different parts of the triplane\nrepresentation and learn relationships between them. The\ncross-attention layer allows the decoder to attend to the la-\ntent vectors from the image encoder and incorporate global\nand local image features into the triplane representation. Fi-\nnally, the NeRF model consists of a stack of multilayer per-\nceptrons (MLPs), which are responsible for predicting the\ncolor and density of a 3D point in space.\nInstead of conditioning the image-to-triplane projection\non camera parameters, we have opted to allow the model to\n\u201cguess\u201d the camera parameters (both extrinsics and intrin-\nsics) during training and inference. This is to enhance the\nmodel\u2019s robustness to in-the-wild input images at inference\ntime. By foregoing explicit camera parameter condition-\ning, our approach aims to cultivate a more adaptable and re-\nsilient model capable of handling a wide range of real-world\nscenarios without the need for precise camera information.\nThe architecture\u2019s main parameters, such as the number\nof layers in the transformer, the dimensions of the triplanes,\nthe specifics of the NeRF model, and the main training con-\nfigurations, are detailed in Table 1. Compared to LRM [11],\nTripoSR introduces several technical improvements which\nwe discuss next.\n2.2. Data Improvements\nRecognizing the critical importance of data, we have incor-\nporated two improvements in our training data collection:\n\u2022 Data Curation: By selecting a carefully curated subset\nof the Objaverse [4] dataset, which is available under the\nCC-BY license, we have enhanced the quality of training\ndata.\n\u2022 Data Rendering: We have adopted a diverse array of\ndata rendering techniques that more closely emulate the\ndistribution of real-world images, thereby enhancing the\nmodel\u2019s ability to generalize, even when trained exclu-\nsively with the Objaverse dataset.\n2\nParameter\nValue\nImage Tokenizer\nimage resolution\n512 \u00d7 512\npatch size\n16\n# attention layers\n12\n# feature channels\n768\nTriplane Tokenizer\n# tokens\n32 \u00d7 32 \u00d7 3\n# channels\n16\nBackbone\n# channels\n1024\nattention layers\n16\n# attention heads\n16\nattention head dim\n64\ncross attention dim\n768\nTriplane Upsampler\nfactor\n2\n# input channels\n1024\n# output channels\n40\noutput shape\n64 \u00d7 64 \u00d7 40\nNeRF MLP\nwidth\n64\n# layers\n10\nactivation\nSiLU\nRenderer\n# samples per ray\n128\nradius\n0.87\ndensity activation\nexp\ndensity bias\n\u22121.0\nTraining\nlearning rate\n4e\u22124\noptimizer\nAdamW\nlr scheduler\nCosineAnnealingLR\n# warm-up steps\n2, 000\n\u03bbLPIPS\n2.0\n\u03bbmask\n0.05\nTable 1. Model configuration of TripoSR.\n2.3. Model and Training Improvements\nOur adjustments aim to boost both the model\u2019s efficiency\nand its performance.\nTriplane Channel Optimization.\nThe configuration of\nchannels within the triplane-NeRF representation plays an\nimportant role in managing the GPU memory footprint dur-\ning both training and inference, due to the high computa-\ntional cost of volume rendering.\nMoreover, the channel\ncount significantly influences the model\u2019s capacity for de-\ntailed and high-fidelity reconstruction. In pursuit of an op-\ntimal balance between reconstruction quality and computa-\ntional efficiency, experimental evaluations led us to adopt a\nconfiguration of 40 channels. This choice enables the use of\nlarger batch sizes and higher resolutions during the training\nphase, while concurrently minimizing the memory require-\nments during inference.\nMask Loss. We incorporated a mask loss function during\ntraining that significantly reduces \u201cfloater\u201d artifacts and im-\nproves the fidelity of reconstructions:\nLmask( \u02c6\nM v, M GT\nv\n) = BCE( \u02c6\nM v, M GT\nv\n),\n(1)\nwhere \u02c6\nM v and M GT\nv\nare rendered and ground-truth mask\nimages of the v-th supervision view, respectively. The full\ntraining loss we minimized during training is:\nLrecon(I) = 1\nV\nV\nX\nv=1\n\u0010\nLMSE(\u02c6Iv, IGT\nv\n)\n+ \u03bbLPIPSLLPIPS(\u02c6Iv, IGT\nv\n)\n+\u03bbmaskLmask( \u02c6\nM v, M GT\nv\n)\n\u0011\n(2)\nLocal Rendering Supervision. Our model fully relies on\nrendering losses for supervision, thereby imposing a need\nfor high-resolution rendering for our model to learn detailed\nshape and texture reconstructions. However, rendering and\nsupervising at high resolutions (e.g., 512 \u00d7 512 or higher)\n3\nFigure 2. We outperform SOTA methods for 3D reconstruction\nwhile achieving fast inference time. In the figure, F-Score with\nthreshold 0.1 is averaged over GSO [6] and OmniObject3D [30].\ncan overwhelm computational and GPU memory loads. To\ncircumvent this issue, we render 128 \u00d7 128-sized random\npatches from the original 512 \u00d7 512 resolution images dur-\ning training. Crucially, we increase the likelihood of se-\nlecting crops that cover foreground regions, thereby plac-\ning greater emphasis on the areas of interest. This impor-\ntance sampling strategy ensures faithful reconstructions of\nobject surface details, effectively balancing computational\nefficiency and reconstruction granularity.\n3. Results\nWe quantitatively and qualitatively compare TripoSR\nto previous state-of-the-art methods using two different\ndatasets with 3D reconstruction metrics.\nEvaluation Datasets.\nWe curate two public datasets,\nGSO [6] and OmniObject3D [30], for evaluations. We iden-\ntify that both datasets include many simple-shaped objects\n(e.g., box, sphere or cylinder) and can thus cause high vali-\ndation bias towards these simple shapes. Therefore we man-\nually filter the datasets and select around 300 objects from\neach dataset to make sure they form a diverse and represen-\ntative collection of common objects.\n3D Shape Metrics. We extract the isosurface using March-\ning Cubes [18] to convert implicit 3D representations (such\nas NeRF) into meshes. We sample 10K points from these\nsurfaces to calculate the Chamfer Distance (CD) and F-\nscore (FS). Considering that some methods are not capa-\nble of predicting view-centric shapes, we use a brute-force\nsearch approach to align the predictions with the ground\ntruth shapes. We linearly search the rotation angle by op-\ntimizing for the lowest CD and further employ the Iterative\nMethod\nCD\u2193\nFS@0.1\u2191\nFS@0.2\u2191\nFS@0.5\u2191\nOne-2-3-45 [16]\n0.227\n0.382\n0.630\n0.878\nZeroShape [13]\n0.160\n0.489\n0.757\n0.952\nTGS [35]\n0.122\n0.637\n0.846\n0.968\nOpenLRM [10]\n0.180\n0.430\n0.698\n0.938\nTripoSR (ours)\n0.111\n0.651\n0.871\n0.980\nTable 2.\nQuantitative comparison of different techniques on\nGSO [6] validation set, where CD and FS refer to Chamfer Dis-\ntance and F-score respectively.\nMethod\nCD\u2193\nFS@0.1\u2191\nFS@0.2\u2191\nFS@0.5\u2191\nOne-2-3-45 [16]\n0.197\n0.445\n0.698\n0.907\nZeroShape [13]\n0.144\n0.507\n0.786\n0.968\nTGS [35]\n0.142\n0.602\n0.818\n0.949\nOpenLRM [10]\n0.155\n0.486\n0.759\n0.959\nTripoSR (ours)\n0.102\n0.677\n0.890\n0.986\nTable 3. Quantitative comparison of different techniques on Om-\nniObject3D [30] validation set, where CD and FS refers to Cham-\nfer Distance and F-score respectively.\nClosest Point (ICP) method to refine the alignment.\nQuantitative Comparisons.\nWe compare TripoSR with\nthe existing state-of-the-art baselines on 3D reconstruc-\ntion that use feed-forward techniques, including One-2-3-\n45 [16], TriplaneGaussian (TGS) [35], ZeroShape [13] and\nOpenLRM [10]1. As shown in Table 2 and Table 3, our\nTripoSR significantly outperforms all the baselines, both in\nterms of CD and FS metrics, achieving the new state-of-the-\nart performance on this task.\nPerformance vs. Runtime. Another key advantage of Tri-\npoSR is its inference speed. It takes around 0.5 seconds to\nproduce a 3D mesh from a single image on an NVIDIA\nA100 GPU. Figure 2 shows a 2D plot of different tech-\nniques with inference times along the x-axis and the av-\neraged F-Score along the y-axis. The plot shows that Tri-\npoSR is among the fastest networks, while also being the\nbest-performing feed-forward 3D reconstruction model.\nVisual Results. We further show the qualitative results of\ndifferent approaches in Figure 3. Because some methods do\nnot reconstruct textured meshes, we render TripoSR recon-\nstructions both with and without vertex color for a better\ncomparison. As shown in the figure, ZeroShape tends to\npredict over-smoothed shapes. TGS reconstructs more sur-\nface details but these details sometimes do not align with\nthe input. Moreover, both ZeroShape and TGS cannot out-\nput textured meshes directly 2. On the other hand, One-2-\n3-45 and OpenLRM predict textured meshes, but their esti-\n1We use the openlrm-large-obj-1.0 model.\n2TGS leverages 3DGS to represent 3D objects. We follow the paper\nand utilize their auxiliary point cloud outputs to reconstruct the surface.\nHowever, it is non-trivial to reconstruct textures on meshes, (e.g., directly\ntaking vertex colors from the nearest Gaussian leads to noisy textures).\n4\nZeroShape\nTGS\nOne-2-3-45\nOpenLRM\nTripoSR\nw/o texture\nTripoSR\nw/ texture\nInput\nimage\nFigure 3. Qualitative results. We compare TripoSR output meshes to other SOTA methods on GSO and OmniObject3D (first four columns\nare from GSO [6], last two are from OmniObject3D [30]). Our reconstructed 3D shapes and textures achieve significantly higher quality\nand better details than previous state-of-the-art methods.\nmated shapes are often inaccurate. Compared to these base-\nlines, TripoSR demonstrates a high reconstruction quality\nfor both shape and texture. Our model not only captures a\nbetter overall 3D structure of the object, but also excels at\nmodeling several intricate details.\n4. Conclusion\nIn this report, we present an open-source feedforward 3D\nreconstruction model, TripoSR. The core of our model is\na transformer-based architecture developed upon the LRM\nnetwork [11], together with substantial technical improve-\nments along multiple axes. Evaluated on two public bench-\nmarks, our model demonstrates state-of-the-art reconstruc-\ntion performance with high computational efficiency. We\nhope TripoSR empowers researchers and developers in de-\nveloping more advanced 3D generative AI models.\nAcknowledgements\nTripo AI. We extend our sincere gratitude to Yuan-Chen\nGuo and Zi-Xin Zou for their critical roles in coding, demo\ndevelopment, and experimentation.\nWe also thank Peng\nWang for his insightful discussions, Dehu Wang for prepar-\ning the datasets for model validation, and Sienna Huang for\nmanaging communication within our collaboration.\nStability AI. We thank Emad Mostaque, Anel Islam-\novic, Bryce Wilson, Ana Guillen, Adam Chen, Chris-\ntian Dowell and Ella Irwin for their help in various as-\npects of the model development, collaboration and re-\nlease.\nWe also thank Vikram Voleti for helpful discus-\nsions and Eric Courtemanche for his help with visual re-\nsults.\nReferences\n[1] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the International Conference on Computer Vi-\nsion (ICCV), 2021. 2\n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2\n[3] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In arXiv, 2023. 1\n[4] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 1, 2\n5\n[5] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A\nuniverse of 10m+ 3d objects. Advances in Neural Informa-\ntion Processing Systems, 36, 2024. 1\n[6] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In 2022 In-\nternational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560. IEEE, 2022. 4, 5\n[7] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh\nr-cnn. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 9785\u20139795, 2019. 2\n[8] Thibault Groueix,\nMatthew Fisher,\nVladimir G Kim,\nBryan C Russell, and Mathieu Aubry. A papier-m\u02c6ach\u00b4e ap-\nproach to learning 3d surface generation. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 216\u2013224, 2018. 2\n[9] Yuan-Chen Guo, Ying-Tian Liu, Chen Wang, Zi-Xin Zou,\nGuan Luo, Chia-Hao Chen, Yan-Pei Cao, and Song-Hai\nZhang. threestudio: A unified framework for 3d content gen-\neration, 2023. 1\n[10] Zexin He and Tengfei Wang. Openlrm: Open-source large\nreconstruction models. https://github.com/3DTopi\na/OpenLRM, 2023. 4\n[11] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d. arXiv preprint arXiv:2311.04400, 2023. 1, 2, 5\n[12] Zixuan Huang, Varun Jampani, Anh Thai, Yuanzhen Li, Ste-\nfan Stojanov, and James M. Rehg. Shapeclipper: Scalable 3d\nshape learning from single-view images via geometric and\nclip-based consistency.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 12912\u201312922, 2023.\n[13] Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani,\nand James M Rehg.\nZeroshape: Regression-based zero-\nshot shape reconstruction. arXiv preprint arXiv:2312.14198,\n2023. 1, 2, 4\n[14] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model.\narXiv preprint arXiv:2311.06214, 2023. 2\n[15] Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao,\nJingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, and\nYing Shan. Advances in 3d generation: A survey. arXiv\npreprint arXiv:2401.17807, 2024. 2\n[16] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund\nVarma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single\nimage to 3d mesh in 45 seconds without per-shape optimiza-\ntion. Advances in Neural Information Processing Systems,\n36, 2024. 4\n[17] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 1, 2\n[18] William E. Lorensen and Harvey E. Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. SIG-\nGRAPH Comput. Graph., 21(4):163\u2013169, 1987. 4\n[19] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4460\u20134470, 2019. 2\n[20] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2\n[21] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2\n[22] Zifan Shi, Sida Peng, Yinghao Xu, Andreas Geiger, Yiyi\nLiao, and Yujun Shen. Deep generative models on 3d rep-\nresentations: A survey. arXiv preprint arXiv:2210.15663,\n2022. 2\n[23] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 2\n[24] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang,\nGang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian\nmodel for high-resolution 3d content creation. arXiv preprint\narXiv:2402.05054, 2024. 2\n[25] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single rgb images. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages 52\u201367,\n2018.\n[26] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan,\nKalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai\nZhang.\nPf-lrm:\nPose-free large reconstruction model\nfor joint pose and shape prediction.\narXiv preprint\narXiv:2311.12024, 2023. 2\n[27] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. Advances in Neural Information Processing Systems,\n36, 2024. 2\n[28] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph\nFeichtenhofer, and Georgia Gkioxari. Multiview compres-\nsive coding for 3d reconstruction.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9065\u20139075, 2023. 2\n[29] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong\nPark, Ruiqi Gao, Daniel Watson, Pratul P Srinivasan, Dor\nVerbin, Jonathan T Barron, Ben Poole, et al.\nReconfu-\nsion: 3d reconstruction with diffusion priors. arXiv preprint\narXiv:2312.02981, 2023. 1\n[30] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\net al. Omniobject3d: Large-vocabulary 3d object dataset for\nrealistic perception, reconstruction and generation. In Pro-\n6\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 803\u2013814, 2023. 4, 5\n[31] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji-\nahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein,\nZexiang Xu, et al.\nDmv3d: Denoising multi-view diffu-\nsion using 3d large reconstruction model.\narXiv preprint\narXiv:2311.09217, 2023. 2\n[32] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh\nTenenbaum, Bill Freeman, and Jiajun Wu. Learning to re-\nconstruct shapes from unseen classes. Advances in neural\ninformation processing systems, 31, 2018. 2\n[33] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12588\u201312597, 2023. 1\n[34] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang,\nYing Shan, and Song-Hai Zhang.\nSparse3d:\nDistill-\ning multiview-consistent diffusion for object reconstruction\nfrom sparse views. arXiv preprint arXiv:2308.14078, 2023.\n[35] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li,\nDing Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane\nmeets gaussian splatting:\nFast and generalizable single-\nview 3d reconstruction with transformers.\narXiv preprint\narXiv:2312.09147, 2023. 1, 2, 4\n7\n"
  },
  {
    "title": "RT-H: Action Hierarchies Using Language",
    "link": "https://arxiv.org/pdf/2403.01823.pdf",
    "upvote": "7",
    "text": "RT-H: Action Hierarchies Using Language\nSuneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson,\nYevgen Chebotar\u2217, Debidatta Dwibedi\u2217, Dorsa Sadigh\u2217\nGoogle DeepMind\nFig. 1: Given a task in language like \u201cclose the pistachio jar\u201d and an image of the scene, RT-H utilizes a Vision Language Model (VLM)\nto predict language motions like \u201cmove arm forward\u201d and \u201crotate arm right\u201d, and then conditioned on these language motions, it predicts\nactions for the robot (purple box). This action hierarchy teaches the model the shared structure across tasks with many semantically different\ndescriptions (see bottom task examples). These language motions enable better data sharing across diverse multi-task datasets as compared\nto mapping directly from the task to actions. This hierarchy also enables humans to optionally provide language motion corrections to the\nrobot to prevent task failure, and then to use these new language motions to predict better actions (peach box). Once the human is done\nintervening, RT-H resumes predicting language motions like before.\nAbstract\u2014Language provides a way to break down complex\nconcepts into digestible pieces. Recent works in robot imitation\nlearning have proposed learning language-conditioned policies\nthat predict actions given visual observations and the high-level\ntask specified in language. These methods leverage the structure\nof natural language to share data between semantically similar\ntasks (e.g., \u201cpick coke can\u201d and \u201cpick an apple\u201d) in multi-task\ndatasets. However, as tasks become more semantically diverse\n(e.g., \u201cpick coke can\u201d and \u201cpour cup\u201d), sharing data between\ntasks becomes harder and thus learning to map high-level tasks\nto actions requires substantially more demonstration data. To\nbridge this divide between tasks and actions, our insight is\nto teach the robot the language of actions, describing low-\nlevel motions with more fine-grained phrases like \u201cmove arm\nforward\u201d or \u201cclose gripper\u201d. Predicting these language motions\nas an intermediate step between high-level tasks and actions\nforces the policy to learn the shared structure of low-level\nmotions across seemingly disparate tasks. Furthermore, a policy\nthat is conditioned on language motions can easily be corrected\nduring execution through human-specified language motions.\nThis enables a new paradigm for flexible policies that can learn\nfrom human intervention in language. Our method RT-H builds\nan action hierarchy using language motions: it first learns to\npredict language motions, and conditioned on this along with\nthe high-level task, it then predicts actions, using visual context\nat all stages. Experimentally we show that RT-H leverages this\nlanguage-action hierarchy to learn policies that are more robust\nand flexible by effectively tapping into multi-task datasets. We\nshow that these policies not only allow for responding to language\ninterventions, but can also learn from such interventions and\noutperform methods that learn from teleoperated interventions.\nOur website and videos are found at rt-hierarchy.github.io.\nI. INTRODUCTION\nLanguage is the engine of human reasoning, empowering us\nto break complex concepts into simpler ones, to correct our\nmisunderstandings, and to generalize concepts in new settings.\nIn recent years, robots too have begun to leverage language\u2019s\nefficient, compositional structure for breaking down high-level\narXiv:2403.01823v1  [cs.RO]  4 Mar 2024\nconcepts [1], providing language corrections [2, 3], or enabling\ngeneralization to new settings [4]. These works often share\na common paradigm: given a high-level task described in\nlanguage like \u201cpick coke can\u201d, they learn policies that map\nobservations and task descriptions in language to low-level\nrobot actions across large multi-task datasets. The advantage\nof language in these settings is to encode the shared structure\nbetween similar tasks (e.g., \u201cpick coke can\u201d vs. \u201cpick an\napple\u201d), reducing the data needed to learn the mapping from\ntasks to actions. However as tasks become more diverse, so too\ndoes the language describing each task (e.g., \u201cpick coke can\u201d\nvs. \u201cpour a cup\u201d), making it harder to learn the shared structure\nbetween different tasks from only the high-level language.\nTo learn diverse tasks, our aim is to better capture the\nsimilarities between these tasks. We observe that language\nis capable of expressing much more than just the high-level\ntask: we can also express how to do the task \u2013 a more fine-\ngrained representation that lies closer to the low-level actions.\nFor example, we can decompose the \u201cpick coke can\u201d task\ninto a sequence of fine-grained behaviors, which we denote\nas language motions: \u201cmove arm forward\u201d, then \u201cgrasp the\ncan\u201d, and then \u201cmove the arm up\u201d. Our key insight is to\nleverage language motions as an intermediate prediction layer\nbetween high-level task descriptions and low-level actions\n\u2013 thus building an action hierarchy via language motions.\nCreating such an action hierarchy leads to several benefits:\n(1) It enables much better data sharing between different tasks\nat the level of language motions, leading to better language\nmotion composition and generalization in diverse multi-task\ndatasets. For example, even though \u201cpour a cup\u201d and \u201cpick up\na coke can\u201d are semantically different, they entirely overlap\nat the language motion level until the object is picked. (2)\nLanguage motions are not merely fixed primitives, but rather\nlearned in the context of the current task and scene using\nthe instruction and visual observation. For example, \u201cmove\narm forward\u201d alone does not convey how fast to move or\nin what exact direction vector; that depends on the task and\nthe observation. The contextuality and flexibility of learned\nlanguage motions introduce a new set of capabilities: it allows\nhumans to provide their own corrections to language motions\nwhen the policy is not 100% successful (see center orange\nbox in Fig. 1). Further, the robot can even learn from these\nhuman corrections, entirely in the realm of language motions.\nFor example, with \u201cpick coke can\u201d, if the robot closes its\ngripper early, we can instead tell it to \u201cmove arm forward\u201d\nfor longer, which RT-H interprets in context of the current\nscene. This slight change in language motions is not only easy\nfor a human to provide, but also much easier to learn from\ncompared to correcting individual robot actions.\nMotivated by the benefits of language motions, we propose\nan end-to-end framework, RT-H (Robot Transformer with\nAction Hierarchies), for learning these action hierarchies: at\neach step, RT-H conditions on the observation and the high-\nlevel task description to predict the current language motion\n(language motion query), enabling the model to reason about\nhow to do the task at a fine-grained level. Then RT-H uses\nthe observation, the task, and the inferred language motion\nto predict the action for that step (action query), where the\nlanguage motion provides additional context to improve the\nprediction of precise actions (see purple box in Fig. 1).\nTo extract language motions from our data, we develop an\nautomated approach to extract a simplified set of language\nmotions from robot proprioception, yielding a rich library of\nover 2500 language motions without any manual annotation\neffort. We base our model architecture on RT-2, a large Vision-\nLanguage Model (VLM) which co-trains on internet-scale\nvision and language data to improve policy learning [4]. RT-\nH uses a single model for both language motion and action\nqueries to leverage this broad internet-scale knowledge for all\nlevels of the action hierarchy.\nExperimentally, we find that using a language motion hier-\narchy yields substantial improvements when ingesting diverse\nmulti-task datasets, outperforming RT-2 by 15% on a wide\nrange of tasks. We also find that correcting language motions\nreaches near perfect success rates on the same tasks, demon-\nstrating the flexibility and contextuality of learned language\nmotions. Additionally, fine-tuning our model with language\nmotion interventions outperforms state-of-the-art interactive\nimitation learning methods such as IWR [5] by 50%. Finally,\nwe show that language motions in RT-H generalize to varia-\ntions in scene and objects better than RT-2.\nII. RELATED WORK\nIn this section, we discuss the role of language in policy\nlearning, how hierarchy has been used in imitation learning,\nand previous approaches for providing and learning from\nhuman corrections on robot policies.\nLanguage-Conditioned Policies. In recent years, language\nhas emerged as a powerful goal representation for robotic\ntasks. In imitation learning (IL), many approaches encode\ntasks described in language into embeddings using pretrained\nlanguage models, which are then inputted to a policy that is\ntrained on multi-task robot datasets [6\u201311]. These pretrained\nlanguage embeddings lack any visual understanding, so other\nworks jointly train visual and language representations from\nthe ground up, often using large internet-scale datasets and\nsometimes including robot data [12\u201315]. The resulting goal\nrepresentations can then be inputted into a policy to provide\nboth visual and semantic context. More recently, policies built\non vision language model (VLM) backbones have become ca-\npable of learning actions directly from visual observations and\nlanguage without the need for pretrained embeddings [4, 16].\nAll these approaches leverage language to represent the high-\nlevel task and often directly predict low-level actions \u2013 but as\nboth language task descriptions become semantically diverse,\nsharing data between different tasks becomes challenging, so\nsignificantly more data is required.\nHierarchical Action Representation in Imitation Learning.\nAn alternative approach to boost performance is to impose\nstructure on the multi-task learning problem through the use\nof hierarchical action representations. Several works have\nexplored learning general \u201cskill\u201d representations as parame-\nterized primitives [17, 18] or embeddings to describe short\nsequences of actions or interactions with objects, often from\ndiverse multi-task datasets [10, 19\u201328]. While they generally\nimprove performance, they are often quite computationally\ncomplex and sensitive to hyperparameters. Another line of\nwork shows the benefits of separating coarse and fine action\nabstractions in IL, but requiring both coarse and fine action\nannotations [29, 30].\nLanguage has also been used to create hierarchy in multi-\ntask learning. When tackling long horizon instructions, many\nrecent approaches use LLMs or VLMs to decompose long\nhorizon instructions into a sequence of tasks specified in lan-\nguage [1, 16, 31\u201334]. Usually, scripted or individually trained\npolicies are used to execute these tasks, limiting scalability. To\nlearn long horizon policies end-to-end, Hu and Clune train a\nmodel to first predict language tasks and then predict actions\nconditioned on those tasks [35]. This approach is similar to\nRT-H but exists one level higher in the action hierarchy: they\ndo not label or learn from fine-grained language motions. A\nfew works explore the usage of more fine-grained language.\nSharma et al. use an LLM to decompose tasks into a sequence\nof motion primitives [36]. Yet, these motion primitives are\nhard-coded and lack the contextuality that is required in more\ncomplex settings. RT-H learns language motions in the context\nof both the task and the scene, enabling better policies and\nmore contextual corrections.\nInteractive Imitation Learning and Correction. Interactive\nIL methods learn from human feedback during robot exe-\ncution [37]. Ross et al. proposed DAgger, which iteratively\naggregates expert annotated actions for online rollouts [38].\nWhile effective, providing such expert annotation is costly,\nso later works used more selective interventions, for example\nletting either the human decide when to intervene [5, 39] or\nletting the policy decide [40\u201343]. These methods all require\nintervention in the action space of the robot, i.e., robot\nteleoperation [5] or kinesthetic teaching [44, 45], which can\nbe challenging for non-experts and hard to scale.\nTo make intervention more intuitive and scalable, several\nworks have studied language as an intervention medium, for\nexample intervening on incorrect task predictions with human\nguidance [32, 46]. Correcting language at a more fine-grained\nlevel is challenging. Several works define a fixed set of fine-\ngrained language corrections and then map natural language\nutterances to these correction primitives [47]. Later work\nremoves these brittle primitives and replaces them with com-\nposable cost functions, but they assume privileged environ-\nment knowledge [3]. Data driven approaches have also been\nexplored, requiring large datasets of language corrections [48\u2013\n51] or shared autonomy during deployment [2]. RT-H instead\nlearns to predict language motions end-to-end with actions,\nenabling not just correction in the space of language motions\nbut also efficient learning from those corrections.\nIII. RT-H: ACTION HIERARCHIES USING LANGUAGE\nTo effectively capture the shared structure across multi-task\ndatasets \u2013 that is not represented by high-level task descrip-\ntions \u2013 our goal is to learn policies that explicitly leverage\naction hierarchies. Specifically, we introduce an intermediate\nlanguage motion prediction layer into policy learning. The\nlanguage motions describing fine-grained behavior of the robot\ncan capture useful information from multi-task datasets and\ncan lead to performant policies.\nWhen the learned policies struggle to perform, language\nmotions can again come to rescue: they enable an intuitive\ninterface for online human corrections that are contextual to\nthe given scene. A policy trained with language motions can\nnaturally follow low-level human corrections and successfully\nachieve the task given the correction data. Additionally, the\npolicy can even be trained on the language correction data\nand further improve its performance.\nRT-H Model Overview. RT-H, shown in Fig. 2, has two\nkey phases: It first predicts language motions from the task\ndescription and visual observation (language motion query, top\nleft of Fig. 2), and then conditions on the predicted language\nmotion, the task, and the observation to infer the precise\nactions (action query, bottom left of Fig. 2). We instantiate RT-\nH using a VLM backbone and following the training procedure\nfrom RT-2 [4]. Similar to RT-2, we leverage the immense\nprior knowledge in natural language and image processing\nin internet-scale data through co-training. To incorporate this\nprior knowledge into all levels of the action hierarchy, a single\nmodel learns both the language motion and action queries.\nA. Formalizing Action Hierarchies\nWe are given a dataset D = {(\u03c41, g1), . . . , (\u03c4N, gN)} of N\nexpert demonstrations (\u03c4) paired with task descriptions in\nnatural language (g \u2208 G), where each g describes exactly\none task from a set of m high-level tasks {Ti}m\ni=1. Each\ndemonstration \u03c4i consists of a sequence of observations and\naction hierarchies of length Li. We define an action hierarchy\nto consist of an intermediate action representation specified in\nnatural language z \u2208 Z, and the low-level action a \u2208 A. Here,\nthe intermediate action is more fine-grained than the high-\nlevel task, but more coarse-grained than the low-level action.\nThus we write \u03c4i = {(o1, z, a1), . . . , (oLi, zLi, aLi)}, with\nobservations o \u2208 O. Our goal is to learn a sequence of policies:\na high-level policy \u03c0h : O \u00d7G \u2192 Z which maps observations\nand task descriptions to intermediate actions, and a low-level\npolicy \u03c0l : O\u00d7G \u00d7Z \u2192 A which maps observations, task de-\nscriptions, and the intermediate action to the low-level action.\nThen we define the action hierarchy policy as the composition\nof these two policies: \u03c0(a, z|o, g) = \u03c0h(z|o, g)\u03c0l(a|o, g, z).\nIn this work, we model the intermediate action represen-\ntation z using language motions like \u201cmove arm forward\u201d or\n\u201crotate arm right\u201d. Note that an action hierarchy can easily be\nextended to more than just a single level (i.e., z1 . . . zK, in\norder of how fine-grained they are).\nB. RT-H: Model and Training Details\nTo model this action hierarchy and acquire the benefits of\nlanguage motions, our method RT-H, shown in Fig. 2, learns\n\u03c0h and \u03c0l using a single VLM co-trained with internet-scale\ndata. We instantiate this VLM with the same PaLI-X 55B [52]\narchitecture as RT-2 [4] \u2013 RT-H uses a ViT encoder model to\nprocess images into tokens, and then uses an Encoder-Decoder\ntransformer to convert streams of image and natural language\nFig. 2: RT-H Overview. Left: Our method leverages language to create an action hierarchy for policy learning. We separate the action\nprediction problem into a language motion query (\u03c0h), which predicts a fine-grained language motion like \u201cmove arm forward\u201d using the\nimage tokens and task description tokens, and an action query (\u03c0l), which flexibly decodes this language motion into actions using the\ncontext of the task and the scene. We leverage a single VLM for both queries based on RT-2 [4] that encapsulate the broad prior knowledge\nin internet-scale data at each level of the action hierarchy. Right: a user can intervene directly on the action query to provide language\nmotion corrections to robot behavior, for example \u201cmove arm left\u201d instead of \u201cmove arm forward\u201d here (top). To learn from corrections,\nwe can update only the language motion query with the newly labeled language motion corrections (bottom). Then we deploy the updated\nmodel back to the action hierarchy (orange block).\ntokens into action tokens. These action tokens are produced\nin the same fashion as RT-2, by discretizing each action\ndimension into 256 bins and encoding these bins as integer\nvalues. Each action a is comprised of delta positions of the end\neffector, delta axis-angle rotations of the end effector, actions\nto close or open the gripper, and a termination flag. RT-H\nconstructs two queries to the VLM. First, a language motion\nquery models \u03c0h, mapping tasks described in language g and\nthe image observations o to language motions z (Encoder sees\ng and o, Decoder predicts z). This first stage teaches RT-H to\nfirst predict correct behavior (language motion) in a coarser\nand more compressed action space than the low-level robot\nactions, enabling better modeling of the structure of each task\nand thus better sharing of sub-trajectories across diverse tasks.\nSecond, the action query models \u03c0l, mapping the image o,\ntask g, and the language motion z to action tokens, which\nthen get detokenized into robot actions a (Encoder sees g,\no, and z, Decoder predicts a). This second stage teaches\nRT-H to be contextual (both with the scene and the task\ndescription) in how it decodes language motion z into precise\nactions to be executed. This extra context is often critical\nto complete the task successfully, and it is also important\nfor performing and learning from correction, as we discuss\nin Section IV-A. Compared to training both language motion\nand action autoregressively in one query, using two queries\nenables (1) specialized prompts for each query, and (2) the\nlanguage motion z is passed into the Transformer Encoder\nrather than the Decoder when predicting actions.\nRT-H is then co-trained using the same PaLI-X [52] training\nmixture that is used in RT-2, starting from a pre-trained check-\npoint. The ViT encoder is frozen for this co-training. RT-H\nreplaces the action prediction query in RT-2 with the language\nmotion and action queries at equal sampling rates. Using\na single model simplifies the training process, and enables\nboth language motion and action queries to benefit from the\nbroad prior knowledge in the PaLI-X training mixture. See\nAppendix A for model and training details.\nC. Extracting Language Motions\nWhile in principle, humans can label the full spectrum of\nfine-grained language motions, we found that having humans\nprovide these labels offline leads to language inconsistency\nacross the dataset and even inaccuracy in the labeled skills. For\nexample, humans would often mislabel the transitions between\nskills, or misjudge the direction of motion of the robot due\nto camera angles. Thus to cheaply extract reliable language\nmotions z at each time step in each episode, we develop an\nautomated labeling scheme relying on robot proprioception\ninformation. First, we connect each dimension of the change\nin robot end effector pose to a spatial dimension (e.g., the z-\naxis of the position change maps to up and down). Doing this\nfor all 9 action dimensions (3 dimensions for delta position,\n3 dimensions for delta orientation, 2 dimensions for base\nmovement, 1 dimension for gripper) we determine a list of\nthe current dominant spatial movements of the robot, for\nexample \u201cmove arm up and right\u201d, \u201cclose gripper\u201d, \u201crotate\narm counterclockwise\u201d, or \u201cturn base left\u201d. Then, we can\nfilter out the dimensions that are below a chosen \u201csmall\naction\u201d threshold, and then compose the resulting actions in\norder of the action magnitude. For example, if the robot is\npredominantly moving the arm forward but also beginning\nto close gripper, we would extract \u201dmove arm forward and\nclose gripper.\u201d In this manner, the combinatorial nature of\nlanguage enables over 2500 language motions to be extracted\nfrom a simple set of known motions. Furthermore, since these\nlanguage motions are derived directly from actions, they hold\nhigh predictive power for the actions themselves when running\nthe action query in RT-H. Importantly, we fix the details of this\nprocedure for all our experiments and datasets irrespective of\nthe task, and so designing this procedure is a one-time fixed\ncost for the developer.\nOf course, this procedure represents one simple way to\ndefine and extract language motions, and many others could\nexist. For example, one can imagine developing a higher-level\nobject-referential language motion space, for example \u201creach\nthe object\u201d or \u201cgrasp the object handle\u201d, but this likely requires\nhuman annotation or robust object detection and tracking. One\ncould also label at an even more fine-grained level, for example\ndescribing the rate of motion like \u201cmove arm forward slowly.\u201d\nHowever, these examples highlight a fundamental trade-off\nthat exists in the chosen abstraction level for language motions:\nthe more fine-grained they are, the harder they would be to\npredict for the language motion query, but the more guidance\nthey provide to the action query, and vice versa. As we show\nin Section V, our choice of language motions strikes a good\nbalance in both language motion and action query accuracy,\nwhile also being cheap to label.\nIV. RT-H: INFERENCE & CORRECTION\nAt test time, RT-H first runs the language motion query to infer\nthe skill, and then uses this inferred skill in the action query to\ncompute the action. However, this process doubles inference\ntime since the two queries must be run sequentially at each\ntime step. While not a problem for smaller models, with larger\nmodel sizes such as the 55B model used in RT-H, we will\nexperience unavoidable querying lag. To handle this challenge,\nwe discuss two modes of language motion inference: (1)\nasynchronous querying: we train just the language motion\nquery in RT-H to predict the skill one step into the future. Then\nat test time, we query the action using the inferred language\nmotion of the previous time step, while also predicting the\nlanguage motion for the next time step. This enables us to\nbatch the queries and thus achieve nearly identical querying\nlag as RT-2. (2) fixed frequency: we can evaluate skill queries\nonce every H steps, also reducing the amortized lag. In our\nexperiments we opt for asynchronous querying, since skills\noften need to change at precise time steps that may not align\nwith fixed frequencies.\nA. Correction via Language Motions\nEven when an RT-H policy encounters new manipulation\nsettings or fail at a given task, the action hierarchy in RT-\nH makes it possible to correct the policy: users can directly\nintervene on the learned language motions. While the policy\nmight struggle at performing the high-level task, following the\nlower-level language motions is much easier.\nIntervening on the model is simple in RT-H (see top right\nin Fig. 2), and since it is text-based, all you need is a key-\nboard or a microphone. Similar to prior interactive imitation\nlearning approaches such as Intervention Weighted Regression\n(IWR) [5], we let the human operator decide when to intervene\non the model, e.g., by pressing a key on the keyboard. Once\nthey have entered the correction mode, they can type a new\nlanguage motion correction on the keyboard or use hotkeys\nfor common language motions. This new language motion will\ndirectly be passed into the action query in RT-H (see Fig. 2) to\nproduce a contextual action that aligns with the user\u2019s intent.\nWe can also display the current predicted language motion\nfor transparency and providing the user additional context,\nso they know what the robot was planning to do and can\nbetter choose their corrections. Then, at a fixed frequency,\nwe requery the user to either enter a new language mo-\ntion correction, keep running the previously entered language\nmotion correction, or exit correction mode. Fixed frequency\nrequerying gives the user time to update their correction or to\ndecide to let the model take over once again.\nLearning from Correction Data. To learn from these lan-\nguage motion corrections, we collect the language motion\nlabels that were provided and the associated images for\ncorrections from successful episodes. Then, we do not need\nto re-train the action query in RT-H, since the model already\nknows how to map the language motion correction to actions\nthat succeed at the task; instead, we only need to update the\nlanguage motion query to produce the correct higher level\nmotions (see Fig. 2; bottom right). This significantly reduces\nthe complexity of learning from corrections, since we only\nneed to learn minor changes in the smaller language motion\nspace rather than the large action space. Like IWR [5], we\nco-train with a mixture of the original dataset (both action\nand language motion queries) and the dataset of corrections\n(just language motion query). See Appendix B for more\nmixture details. Of course, since we use a single model for\nboth language motion and action queries, updating only one\nwill likely still update the other \u2013 co-training RT-H on both\nqueries in the original dataset helps maintain action prediction\nperformance while still learning the minor changes in language\nmotion space through the intervention dataset.\nV. EXPERIMENTS\nTo comprehensively evaluate the performance of RT-H, we\nstudy four key experimental questions:\n\u2022 Q1 (Performance): Do action hierarchies with lan-\nguage improve policy performance on diverse multi-task\ndatasets?\n\u2022 Q2 (Contextuality): Are learned language motions in RT-\nH contextual to the task and scene?\n\u2022 Q3 (Corrections): Is training on language motion cor-\nrections better than teleoperated corrections?\n\u2022 Q4 (Generalization): Do action hierarchies improve ro-\nbustness to out-of-distribution settings?\nDataset: We utilize a large multi-task dataset consisting of\n100K demonstrations with randomized object poses and back-\ngrounds. This dataset combines the following datasets:\n\u2022 Kitchen: The dataset used in RT-1 [6] and RT-2 [4],\nconsisting of 6 semantic task categories in 70K demon-\nstrations.\n\u2022 Diverse: A new dataset consisting of more complex range\nof tasks, with over 24 semantic task categories, but just\n30K demonstrations (see Appendix C for more details).\nWe call this combined dataset the Diverse+Kitchen (D+K)\ndataset, and it is labeled with language motions using our\nautomated procedure described in Section III-C. We evaluate\nour method trained on the full Diverse+Kitchen dataset on\neight tasks that are a representative sample of its hardest tasks:\n1) \u201cflip bowl upright on the counter\u201d\n2) \u201copen pistachio jar\u201d\n3) \u201cclose pistachio jar\u201d\n4) \u201cmove bowl away from cereal dispenser\u201d\n5) \u201cput bowl under cereal dispenser\u201d\n6) \u201cplace oatmeal packet in the bowl\u201d\n7) \u201cgrab scooper from basket\u201d\n8) \u201cpull napkin from dispenser\u201d\nThese eight tasks, shown in Fig. 3, were chosen because they\nrequire complex sequences of motions and high precision.\nMethods: We study and compare the following methods,\nincluding ablating a number of choices in RT-H:\n\u2022 RT-H is our proposed method in this work, and we use\nthe asynchronous querying variant for these experiments\n(see Section IV).\n\u2022 RT-H-Joint is also our method but using a single au-\ntoregressive query to produce both language motion and\naction, rather than querying the VLM twice with two\ndifferent prompts for each query. RT-H-Joint first out-\nputs language motion then action (where action is still\nconditioned on language motion). While both RT-H and\nRT-H-Joint are autoregressive on the language motion,\nRT-H uses distinct queries for action and language motion\n(\u201cWhat motion ...\u201d vs. \u201cWhat action ..., given motion\n. . . \u201d), whereas RT-H-Joint has just one query (\u201cWhat\nmotion and action ...\u201d). More specifically, RT-H passes\nthe language motion to the Encoder in the action query,\nwhile RT-H-Joint treats the language motion as a Decoder\ninput when predicting the action. Thus, we expect this to\nperform comparably to RT-H.\n\u2022 RT-H-Cluster is an ablation of the automated language\nmotion labeling procedure, which instead clusters actions\ndirectly using K-means [53] into a set of classes with\ninteger labels. These class labels are used in place of the\nautomatically labeled language motions in RT-H.\n\u2022 RT-H-OneHot ablates the use of language to represent\nmotions in RT-H by replacing each unique language\nmotion with an integer class label.\n\u2022 RT-2 is a flat model that does not use any action\nhierarchy [4].\n\u2022 RT-H + Human Intervention involves having a human\ncorrect only the language motions during execution, but\nstill using the action query from RT-H (top right of\nFig. 2). RT-H + Human Intervention is a variant of our\nmethod that enables humans to intervene so we expect it\nto be an upperbound for the other non-intervention-based\nmethods. We discuss this in Section V-C.\n\u2022 RT-H-Intervene is an extension of RT-H method addi-\ntionally trained on human intervention data using lan-\nguage motion corrections. We discuss this in Section V-C.\n\u2022 RT-H-InterveneAction is an ablation of RT-H-Intervene\nmethod that trains on both the action corrections and\nlanguage motion corrections from human intervention\ndata. We discuss this in Section V-C.\n\u2022 RT-2-IWR is the interactive version of RT-2, which is\nadditionally trained with human interventions in the form\nof teleoperated demonstrations \u2013 in contrast to language\nmotion corrections \u2013 and is compared to RT-H-Intervene\nin Section V-C.\nNote that RT-H-Joint, RT-H-Cluster, and RT-H-OneHot are\nvariants of RT-H that still utilize an action hierarchy. See\nAppendix A for exact queries and a deeper dive into each\nRT-H variant implementation.\nIn Section V-A, we first train and evaluate the performance\nof RT-H on a diverse multi-task dataset (Q1). In Section V-B,\nwe qualitatively analyze the learned language motions across\nvarious tasks to see how language motions adapt to the\ncontext (Q2). In Section V-C, we collect and train on language\nmotion corrections on top of RT-H, demonstrating that training\nFig. 3: Results on Diverse+Kitchen multi-task dataset, consisting of eight challenging evaluation tasks. RT-H outperforms RT-2 by 15% on\naverage, getting higher performance on 6/8 of the tasks. Replacing language with class labels (RT-H-OneHot) drops performance significantly.\nUsing action clusters via K-Means [53] instead of the automated motion labeling procedure leads to a minor drop in performance as well\n(RT-H-Cluster), demonstrating the utility of language motions as the intermediate action layer.\non language motion corrections improves policy performance\n(Q3). Finally, in Section V-D we test the robustness of RT-H\nto variations in scenes, objects, and tasks (Q4).\nA. RT-H on Diverse Multi-Task Datasets\nHere, we discuss how action hierarchies can improve policy\nperformance addressing Q1. We will first discuss the online\nperformance of RT-H and its variants when trained on Di-\nverse+Kitchen dataset. We then present offline performance\nmetrics to further analyze the role of language motions.\nOn-Robot Performance: Fig. 3 illustrates the performance of\neach method when trained on the Diverse+Kitchen dataset and\nevaluated on the 8 selected tasks within this dataset discussed\nearlier. Checkpoints are chosen using validation action MSE,\nand then run for 10 controlled trials for each task (80 total\ntrials per method). RT-H outperforms RT-2 on most of the\ntasks, surpassing RT-2 by 15% on average, which strongly\nsupports the benefit of action hierarchies (Q1), despite using\nno additional human annotation. See Appendix D for the\nsuccess rates for each stage of each task, where we see that\nRT-H makes more progress towards success in 7/8 tasks.\nFurthermore, whereas RT-2 achieves nonzero performance on\nonly 4/8 tasks, RT-H is nonzero on 6/8 tasks and RT-H-Joint is\nnonzero on all the tasks, suggesting that RT-H and RT-H-Joint\nare better at managing the diversity of tasks in the dataset.\nAblations: RT-H-Joint does comparably to RT-H, showing\nthat RT-H is robust to the exact querying mechanism. RT-\nH-Cluster replaces the automating labeling procedure with\naction clustering, and without language it performs slightly\nworse than RT-H on average. Interestingly, RT-H-Cluster does\nbetter on the hardest tasks in the evaluation set (open and\nclose pistachio jar). We hypothesize that since RT-H-Cluster\nuses clusters derived from the dataset, its clusters provide even\nmore fine-grained action context than our labeling procedure,\nallowing it to outperform RT-H in precise tasks; however, the\nlack of language makes predicting clusters harder than pre-\ndicting language motions when using broad datasets, leading\nto worse performance for RT-H-Cluster on the broader set of\ntasks. RT-H-OneHot replaces language motions with onehot\nclass labels, and it performs much worse than RT-H despite\nbeing derived from the same underlying language motions.\nThus, while action hierarchy itself gets us part of the way, the\nstructure of language greatly improves language motion and\naction prediction.\nOffline Performance: We investigate if language motions as\nan intermediate layer for action prediction has any noticeable\neffect by comparing the offline validation mean squared error\n(MSE) for end-to-end action prediction across RT-H and its\njoint variant RT-H-Joint vs. the flat RT-2 model (Q1). The\nend-to-end MSE reflects how well each model learns action\nprediction. For RT-H, we also study the action validation\nMSE when using the ground truth (GT) language motion that\nwas labeled in the data as input to the action query (\u03c0l),\nrather than inputting the inferred language motion from the\nlanguage motion query (\u03c0h). This ground truth MSE reflects\nhow informative the true language motion is for predicting the\nactions. In Table I, we report the minimum MSE across train-\ning checkpoints for RT-H, RT-H-Joint, and RT-2 when trained\non either the Diverse+Kitchen dataset or the Kitchen dataset.\nRT-H has roughly a 20% lower MSE than RT-2, and RT-\nH-Joint has a 5-10% lower MSE than RT-2, demonstrating\nthat action hierarchies help improve action prediction offline\nin large multi-task datasets. Using two queries (RT-H) instead\nof one (RT-H-Joint) also seems to improve action prediction,\nwhich could stem from how the language motion gets passed\ninto the model (through the encoder for RT-H vs. through the\ndecoder for RT-H-Joint). RT-H (GT) uses the ground truth\nMSE metric, and we find the gap with the end to end MSE\nis 40%, illustrating that the correct labeled language motions\nare highly informative for predicting actions.\nTrain\nDataset\nEval\nDataset\nRT-2\nRT-H-Joint\nRT-H\nRT-H (GT)\nKitchen\nKitchen\n30.2\n28.22\n24.9\n17.9\nD+K\nDiverse\n27.7\n25.44\n23.6\n17.8\nTABLE I: Best checkpoint Mean Squared Error (MSE) for end-\nto-end action prediction on the validation set for models (columns)\ntrained on different multi-task datasets (rows). Kitchen refers to the\ndata used to train RT-1 [6] and RT-2 [4] (70K demonstrations),\nDiverse+Kitchen (D+K) refers to a combination of Kitchen and the\nmore complex set of tasks (30K demonstrations). We also report\nthe MSE of using the ground truth language motion (the labeled\nlanguage motion) for the action query in RT-H (GT) rather than the\ninferred language motion from the language motion query. RT-H and\nRT-H-Joint achieve lower MSE on both datasets compared to RT-2,\nillustrating the benefits of action hierarchies for ingesting multi-task\ndatasets compared to flat models like RT-2. Also, RT-H has lower\nMSE than RT-H-Joint.\nB. Contextual & Flexible Language Motions\nIn this section, we analyze (1) contextuality: how well the\nactions for a single in-distribution language motion adapt\nto the context of the scene and the task instruction, and\n(2) flexibility: how well RT-H responds to out-of-distribution\nlanguage motions.\nContextuality: We illustrate several examples of contextual\nmotions taken from online evaluations of RT-H in Fig. 4.\nWe see that the same language motions often lead to subtle\nchanges in actions to complete the task, while still respecting\nthe higher level language motion (Q2). For example, for \u201cmove\narm forward\u201d in the top left example in Fig. 4, the arm moves\ngenerally forward but also towards the object of interest, the\nnapkin dispenser. It also slightly tilts the arm to more easily\ngrasp the napkin. In the top right, we see that the same\ncommand leads to a slight downward angle and a rotation\nof the gripper to avoid colliding with the cereal dispenser. For\n\u201cmove arm left\u201d in the middle left of Fig. 4, we similarly see\nthat left implies moving the oatmeal packet precisely above the\nbowl, while in the middle right, left implies precise motion of\nthe lid to latch onto the jar. It would be immensely challenging\nto design a single \u201cmove arm left\u201d primitive to capture this\ncontextuality. We see a similar behavior for \u201crotate arm right\u201d\nin the bottom row of Fig. 4, where in the left case the arm\nrotates and stays up to sit on top of the closed jar, while in\nthe right case the arm rotates and moves down to reach the\nlid on the table. See Appendix D for a quantitative analysis\nof language motion contextuality.\nFig. 4: Examples showing how language motions depend on the\ncontext of the scene and task, taken from online evaluations of RT-\nH trained on the Diverse+Kitchen dataset. For each row, the given\nlanguage motions (\u201cmove arm forward\u201d, \u201cmove arm left\u201d, \u201crotate arm\nright\u201d) manifest with different variations (columns) depending on the\ntask and observation, such as subtle changes in speed, non-dominant\naxes of movement, e.g., rotation for \u201cmove arm forward\u201d, and even\ngripper positions.\nFlexibility: In Fig. 5, we demonstrate the flexibility of RT-\nH by intervening on language motions in RT-H online to\ninstead perform out-of-distribution language motions for in-\ndistribution tasks. In the first row (a), RT-H is tested with two\nvalid ways of completing the \u201cpull napkin\u201d task, and we find\nit responds correctly to both. Despite each of the language\nmotions demonstrated in Fig. 5 being out-of-distribution for\nthe task, RT-H is capable of following these new language\nmotions with ease (Q2). In the bottom two rows (b) of Fig. 5,\nwe find that RT-H is also flexible to more general language\nmotions that are not specific to the task and thus not seen in\nthe training data. For example, in the middle right example,\nmoving the arm away from the jar is not a common language\nmotion for \u201cclose pistachio jar\u201d, but RT-H is still able to act\ncorrectly in response to this language motion. Being flexible\nto more general language motions is critical for responding\nto a wide variety of language motion corrections, especially\nwhen the task or scene are out-of-distribution and require novel\nFig. 5: Examples of the flexibility of learned language motions. In\nthe top row (a) we correct RT-H using two different task-completing\nlanguage motions for pulling the napkin out of the dispenser, either\n\u201cright and down\u201d or \u201cup and backward\u201d, showing RT-H performs\nboth correctly. For the bottom two rows (b), we still correct language\nmotions but ask RT-H to perform a more general set of language\nmotions for each task, demonstrating that RT-H is often flexible even\nto completely out-of-distribution language motions for a given task.\nsequences of language motions.\nOverall, we see that RT-H is able to maintain the flexibility\nand contextuality of actions while learning the high-level\nstructure of each task through language motions (Q2). See\nAppendix D for quantitative analysis of contextuality and a\nqualitative look at language motion multimodality in RT-H,\nalong with staged success rates for each method for each\ntask. Next, we leverage these properties to collect and train\non language motion corrections to improve RT-H.\nC. Training on Online Corrections\nIn this section we are interested in how well RT-H can learn\nfrom language motion corrections compared to methods with-\nout action hierarchy that use teleoperated correction data (Q3).\nWe collect a multi-task language motion correction dataset and\na teleoperated correction dataset for each of the eight tasks in\nSection V-A. As in prior interactive IL methods [5, 39], the\nhuman decides when to correct in both datasets, usually either\nanticipating or responding to a task failure. Next we describe\nthe collection and training pipelines for each method.\nFig. 6: Results for Corrections on models trained on the Diverse+Kitchen multi-task dataset, for the same eight evaluation tasks as in Fig. 3.\nRT-2-IWR is trained on teleoperation corrections from rolling out RT-2, while RT-H-Intervene is trained on language motion corrections\nfrom rolling out RT-H. RT-H-InterveneAction is trained on both language motion and action correction data. We see RT-H-Intervene both\nimproves upon RT-H and substantially outperforms RT-2-IWR, suggesting that language motions are a much more sample efficient space\nto learn corrections than teleoperated actions. RT-H-InterveneAction performs better than RT-H, but fine-tuning actions sometimes leads to\npolicy degeneration, since actions produced by RT-H during intervention can be suboptimal.\nRT-H-Intervene and RT-H-InterveneAction: We collect 30\nepisodes (failed episodes filtered out) of language motion\ncorrections for each of the eight tasks, using the correction\nprocedure described in Section IV-A. The base policy used\nfor collection is RT-H trained on the Diverse+Kitchen dataset\n(same as Section V-A). Then, we train RT-H on these on-policy\ncorrections in the manner described in Section IV-A to produce\nRT-H-Intervene (only training the language motion query on\nthe intervention data). To train RT-H-InterveneAction, we\ninclude both the language motion and action queries when\ntraining on the intervention data, at equal sampling rates.\nRT-2-IWR: We collect 30 episodes (failed episodes filtered\nout) of teleoperated corrections for the same eight tasks,\nusing VR-based teleoperation instead of language motion\ncorrections. Since we only care about learning to correct\nthe failure modes of RT-2, we must use RT-2 trained on\nthe Diverse+Kitchen dataset (same as RT-H-Intervene) as the\nbase policy for collection to ensure fair comparison to RT-H-\nIntervene. We then train RT-2 on these on-policy corrections\nusing the Intervention Weighted Regression (IWR) method [5]\nto produce RT-2-IWR.\nOf course, the base policy for RT-2 performs worse than\nDiverse+Kitchen on these tasks than RT-H, so to ensure a fair\ncomparison we focus on the change in success rates before\nand after training on correction data for each method.\nResults: We evaluate both methods in the same eight eval-\nuation tasks as in Section V-A. In Fig. 6, we compare the\nperformance of each method to the pre-correction models,\nRT-H and RT-2, respectively (duplicated from Fig. 3). We\nalso compare RT-H + Human Intervention as an upperbound\nmethod that uses online human-in-the-loop language motion\ncorrections when necessary, but still uses the action query in\nRT-H conditioned on these language motion corrections.\nFirst, we see how amenable RT-H is to language motion\ncorrections with RT-H + Human Intervention, which gets\nvery high success rates even for the most precise tasks. This\nshows that RT-H actually does change its behavior in task-\nrelevant ways with language motion corrections at test time.\nThis further supports the claim in Section V-B that language\nmotions are both flexible and contextual. In addition, this high-\nlights that language motion prediction is often the bottleneck\nfor performance, so we expect that refining language motion\nprediction through intervention will yield clear improvements.\nRT-2-IWR, a state-of-the-art online imitation learning\nmethod, sees a degradation in performance from 25% to 13%\non average, likely due to a combination of relatively small\namounts of data per task and the use of only a single round\nof correction. In addition, we suspect teleoperation-based\ncorrections are more likely to introduce action distributions\nthat are too different from the training data (and thus the base\npolicy). The language motion corrections in RT-H on the other\nhand are much more consistent with the training data because\nactions come from the base policy itself (under slight changes\nin language motion space) and thus easier to learn from.\nRT-H-Intervene, on the other hand, substantially outper-\nforms RT-2-IWR in this setting despite using the same amount\nof data, improving by 60-70% on the harder precise tasks\n(open and close pistachio jar). RT-H-Intervene regresses on\njust one task, \u201cplace oatmeal packet in bowl\u201d, where we\nobserved that the robot would successfully grasp the packet\nvery often, but would get stuck predicting \u201cclose gripper\u201d.\nWe suspect there is a slight bias towards that language\nmotion correction in the dataset, and it could be resolved\nby specifying \u201cmove arm up\u201d as a follow up correction, or\nby running more rounds of correction. The oatmeal example\nalso highlights how language motion corrections can make\nthe policy\u2019s behavior interpretable and thus more intuitive to\ndebug \u2013 more effectively allowing the designer to identify or\ncorrect the failure points.\nRT-H-InterveneAction also improves upon RT-H, outper-\nforming it by 9% on average. We suspect that compared\nto RT-H-Intervene, RT-H-InterveneAction suffers from policy\ndegeneration, where new actions in the interventions bias the\naction distribution toward model generated actions (since we\nuse the action query in RT-H to collect language motion\ncorrection data) rather than the true expert distribution. These\nmodel generated actions can be sub-optimal and thus can\nimpact performance: for example, we see that close pistachio\njar task does not improve as much with RT-H-InterveneAction\nas with RT-H-Intervene, because the policy starts producing\nnear-zero actions at states where the intervention-produced\nactions were small (e.g. after grasping the jar lid).\nFig. 7: Results when models trained on Kitchen data [6] are deployed on the same tasks, but in a new building with novel backgrounds,\nlighting, and flooring. RT-H and RT-H-Joint each outperform RT-2, suggesting that the use of action hierarchy helps the policy generalize\nto novel scenes. RT-2 struggles particularly with placing upright and opening and closing the drawers in these new scenes.\nOverall, we see that language motion corrections bring the\naverage success rates of RT-H from 40% to 63% with just\n30 episodes of correction per task. By abstracting actions into\nthe more condensed language motion space, RT-H can more\nquickly learn to improve itself from feedback from language\nmotion corrections than from teleoperation corrections (Q3).\nD. Generalization\nTo evaluate Q4, we study three types of generalization:\ngeneralization to new scenes (with similar objects but new\nbackgrounds and lighting), to novel objects, and to novel\ntasks. We use RT-H trained on only the Kitchen dataset [6]\nunless otherwise noted (i.e., not including the Diverse data),\nwhich consists of the following training and evaluation tasks\non various objects:\n1) \u201cknock over\u201d\n2) \u201cdrawer place\u201d\n3) \u201cmove\u201d\n4) \u201cpick\u201d\n5) \u201copen / close drawer\u201d\n6) \u201cplace upright\u201d\nGeneralization to New Scenes: We evaluate each task in\nthe Kitchen dataset in new environments, specifically in a\nnew building consisting of varying lighting, and diverse back-\ngrounds and floors. In Fig. 7, we see that RT-H and RT-H-Joint\nare more robust to changes in scenes, with especially large\ndeltas for the hardest tasks of \u201cplace upright\u201d task and \u201copen\n/ close drawer\u201d tasks (Q4).\nGeneralization to New Objects: We evaluate \u201cpick\u201d and\n\u201cmove\u201d under object generalization, using 50 evaluations of\nobjects unseen during training such as pears, coconut water,\nand oreos. In Table II, we find that RT-H achieves 65% on\nthese tasks, whereas RT-2 gets 55%. As shown in Appendix D,\nRT-H also progresses farther in each task (in terms of stages\nof each task) compared to RT-2 on average (Q4).\nGeneralization to New Tasks with Limited Corrections:\nWhile zero-shot success on out-of-distribution tasks is quite\ndifficult, in Fig. 8, we qualitatively demonstrate that even for\npick\nmove\nAverage\nRT-2\n60\n50\n55\nRT-H\n70\n60\n65\nTABLE II: We evaluate RT-2 and RT-H trained on Kitchen data [6]\non the \u201cpick\u201d and \u201cmove\u201d tasks but under novel objects for 50\nscenarios total. RT-H outperforms RT-2, demonstrating that action\nhierarchy helps the policy generalize to novel objects.\nunseen tasks, RT-H requires just a few well-timed corrections\nto succeed at the task. For these examples, we use the version\nof RT-H trained on the Diverse+Kitchen dataset to provide\nRT-H with language motions demonstrated in a wide variety\nof contexts. Fig. 8 also shows the shared structure between\nseemingly diverse tasks: each of these tasks require some\npicking behavior to begin the task, and by learning the shared\nstructure of language motions across many diverse tasks, RT-H\ncan complete the picking stage without any correction (Q4).\nEven when RT-H is no longer able to generalize its language\nmotion prediction, we see that language motion corrections\noften do generalize, allowing us to successfully complete the\ntask with just a few corrections (Q2, Q4). This demonstrates\nthe potential of language motions for scaling up data collection\nfor novel tasks.\nVI. CONCLUSION\nIn this work, we introduce RT-H, which leverages language\nmotions like \u201cmove arm forward\u201d as an intermediate pre-\ndiction layer between the high-level task and the low-level\naction. RT-H trains to map tasks described in language into\nlanguage motions, and then uses the inferred language motion\nto predict the action, where both steps are conditioned on\nvisual input and the task. We label language motions using\nan automated procedure that scales to a wide variety of tasks\nat no human labeling cost. We instantiate RT-H using a single\ntransformer model like RT-2 [4], where both the action and\nlanguage motion queries are co-trained with a vast amount\nof internet-scale data. RT-H (1) enables more data sharing\nbetween different tasks by learning the shared task structure\nacross seemingly disparate tasks, and thus is more capable\nFig. 8: We show the generalization capabilities of RT-H with completely unseen tasks with minimal correction. By breaking down tasks into\nlanguage motions, RT-H learns the shared structure between seemingly diverse tasks. This allows it to generalize language motions to new\ntasks, as shown in the first part of each task, where RT-H performs the picking phases easily. We also show that when RT-H cannot zero-shot\ngeneralize, language motion corrections often do generalize, allowing it to complete these tasks with just a few well-timed corrections.\nof ingesting multi-task datasets at scale, and (2) is amenable\nto language motion corrections that change the underlying\nbehaviors within the context of the scene and task. In our\nexperiments, we show that RT-H outperforms RT-2 and action\nhierarchy ablations on diverse multi-task data. Then we show\nthat RT-2 is highly correctable in language motion space\neven for unseen language motions, and that learning from\nthese language motion corrections outperforms learning from\nteleoperation-based corrections. Finally, we show that RT-H is\nmore robust to scene and object variations compared to RT-\n2. These results show the promise of action hierarchies using\nlanguage, and we believe RT-H provides a strong foundation\non which to scale up data collection and robot learning.\nLimitations & Future Work: RT-H opens several exciting\navenues for future work. First, we test RT-H on a large and di-\nverse datasets, achieving state-of-the-art performance, but the\nabsolute success rates still leave room for improvement, even\nafter training on corrections. We believe that, as evidenced by\nthe more sample efficient language motion corrections of RT-\nH, future work should scale up both the offline datasets and\ncorrection pipeline \u2013 language motions could even be used\nto help bridge datasets with many different embodiments like\nOXE [54], or even to learn from human videos with actions\ndescribed only in language.\nSecond, although we ablate different action hierarchies\nin Section V-A, future work is needed determine the best\nabstraction level for the intermediate layers (e.g., using object-\nreferential language vs. our language motions). Additionally,\nwe primarily test only one intermediate action layer in this\nwork, or just one step of action reasoning, but future work\nmight define multiple steps of action reasoning instead, where\nthe user can intervene at any level of abstraction. For example,\nwe might add a task prediction level to go from long horizon\ninstructions like \u201cclean the room\u201d to individual tasks like \u201dpick\ncoke can\u201d, before mapping the task to language motions and\nthen actions. To decompose corrections at any level of the\nhierarchy, one might even teach the model to automatically\nlocate a correction in the hierarchy, and then autoregressively\npredict lower level actions until it deems it has reached the\nrobot action level.\nThird, language motions represent a contextual and com-\npressed space in which to predict actions. One might leverage\nthis motion contextuality in RT-H to greatly compress the\naction space for reinforcement learning methods and policy\nexploration, possibly leveraging language motion prediction\nas a prior. We suspect language motions will provide more\nmeaningful degrees of exploration and more sample efficient\npolicy learning, while also being highly interpretable and\ncorrectable to humans. Even in imitation learning, several\nworks have shown the important of action consistency across\ndemonstrators [55, 56], and we posit that using language\nmotions as a compressed action space could lead to more con-\nsistent actions and thus more sample-efficient policy learning.\nREFERENCES\n[1] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz,\nAlex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. In\nConference on Robot Learning, pages 287\u2013318. PMLR,\n2023.\n[2] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya\nShivakumar, Percy Liang, and Dorsa Sadigh.\nNo, to\nthe right: Online language corrections for robotic ma-\nnipulation via shared autonomy. In Proceedings of the\n2023 ACM/IEEE International Conference on Human-\nRobot Interaction, HRI \u201923, page 93\u2013101, New York,\nNY, USA, 2023. Association for Computing Machinery.\nISBN 9781450399647. doi: 10.1145/3568162.3578623.\nURL https://doi.org/10.1145/3568162.3578623.\n[3] Pratyusha Sharma, Balakumar Sundaralingam, Valts\nBlukis, Chris Paxton, Tucker Hermans, Antonio Torralba,\nJacob Andreas, and Dieter Fox. Correcting robot plans\nwith natural language feedback. ArXiv, abs/2204.05186,\n2022.\nURL https://api.semanticscholar.org/CorpusID:\n248085271.\n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Flo-\nrence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana\nGopalakrishnan, Kehang Han, Karol Hausman, Alex\nHerzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nIsabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey\nLevine, Yao Lu, Henryk Michalewski, Igor Mordatch,\nKarl Pertsch, Kanishka Rao, Krista Reymann, Michael\nRyoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet,\nJaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran,\nVincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Ste-\nfan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted\nXiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\nZitkovich. Rt-2: Vision-language-action models transfer\nweb knowledge to robotic control.\nIn arXiv preprint\narXiv:2307.15818, 2023.\n[5] Ajay Mandlekar, Danfei Xu, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nYuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-\nthe-loop imitation learning using remote teleoperation.\nCoRR, abs/2012.06733, 2020. URL https://arxiv.org/abs/\n2012.06733.\n[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine\nHsu, et al.\nRt-1: Robotics transformer for real-world\ncontrol at scale. arXiv preprint arXiv:2212.06817, 2022.\n[7] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,\nFrederik Ebert, Corey Lynch, Sergey Levine, and Chelsea\nFinn. BC-z: Zero-shot task generalization with robotic\nimitation learning. In 5th Annual Conference on Robot\nLearning, 2021. URL https://openreview.net/forum?id=\n8kbp23tSGYv.\n[8] Simon Stepputtis, Joseph Campbell, Mariano Phielipp,\nStefan Lee, Chitta Baral, and Heni Ben Amor. Language-\nconditioned imitation learning for robot manipulation\ntasks.\nIn H. Larochelle, M. Ranzato, R. Hadsell,\nM.F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages\n13139\u201313150. Curran Associates, Inc., 2020.\nURL\nhttps://proceedings.neurips.cc/paper files/paper/2020/\nfile/9909794d52985cbc5d95c26e31125d1a-Paper.pdf.\n[9] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport:\nWhat and where pathways for robotic manipulation. In\nProceedings of the 5th Conference on Robot Learning\n(CoRL), 2021.\n[10] Oier Mees, Lukas Hermann, and Wolfram Burgard.\nWhat matters in language conditioned robotic imitation\nlearning over unstructured data.\nIEEE Robotics and\nAutomation Letters, 7(4):11205\u201311212, 2022.\n[11] Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, and\nJeannette Bohg.\nKITE: Keypoint-conditioned policies\nfor semantic manipulation.\nIn 7th Annual Conference\non Robot Learning, 2023. URL https://openreview.net/\nforum?id=veGdf4L4Xz.\n[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision.\nIn Marina\nMeila and Tong Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research,\npages 8748\u20138763. PMLR, 18\u201324 Jul 2021. URL https:\n//proceedings.mlr.press/v139/radford21a.html.\n[13] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea\nFinn, and Abhinav Gupta.\nR3m: A universal visual\nrepresentation for robot manipulation. In Conference on\nRobot Learning, pages 892\u2013909. PMLR, 2023.\n[14] Siddharth Karamcheti, Suraj Nair, Annie S. Chen,\nThomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy\nLiang.\nLanguage-driven representation learning for\nrobotics. In Robotics: Science and Systems (RSS), 2023.\n[15] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman,\nOsbert Bastani, Vikash Kumar, and Amy Zhang. Vip:\nTowards universal visual reward and representation via\nvalue-implicit pre-training. In The Eleventh International\nConference on Learning Representations, 2022.\n[16] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet,\nDaniel Duckworth, Sergey Levine, Vincent Vanhoucke,\nKarol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence.\nPalm-e: An\nembodied multimodal language model. In arXiv preprint\narXiv:2303.03378, 2023.\n[17] George Konidaris, Scott Kuindersma, Roderic Grupen,\nand Andrew Barto. Robot learning from demonstration\nby constructing skill trees.\nThe International Journal\nof Robotics Research, 31(3):360\u2013375, 2012.\ndoi: 10.\n1177/0278364911428653. URL https://doi.org/10.1177/\n0278364911428653.\n[18] Scott Niekum, Sarah Osentoski, George Konidaris, and\nAndrew G. Barto.\nLearning and generalization of\ncomplex tasks from unstructured demonstrations.\nIn\n2012 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 5239\u20135246, 2012.\ndoi:\n10.1109/IROS.2012.6386006.\n[19] Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Gold-\nberg. Ddco: Discovery of deep continuous options for\nrobot learning from demonstrations. In Conference on\nrobot learning, pages 418\u2013437. PMLR, 2017.\n[20] Tanmay Shankar and Abhinav Gupta.\nLearning robot\nskills with temporal variational inference.\nIn Interna-\ntional Conference on Machine Learning, pages 8624\u2013\n8633. PMLR, 2020.\n[21] Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi,\nAlvaro Sanchez-Gonzalez, Edward Grefenstette, Push-\nmeet Kohli, and Peter Battaglia. Compile: Compositional\nimitation learning and execution. In International Con-\nference on Machine Learning, pages 3418\u20133428. PMLR,\n2019.\n[22] Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, and\nAbhinav Gupta.\nDiscovering motor programs by re-\ncomposing demonstrations.\nIn International Confer-\nence on Learning Representations, 2020.\nURL https:\n//openreview.net/forum?id=rkgHY0NYwr.\n[23] Daniel Tanneberg, Kai Ploeger, Elmar Rueckert, and Jan\nPeters. Skid raw: Skill discovery from raw trajectories.\nIEEE robotics and automation letters, 6(3):4696\u20134703,\n2021.\n[24] Yifeng Zhu, Peter Stone, and Yuke Zhu.\nBottom-up\nskill discovery from unsegmented demonstrations for\nlong-horizon robot manipulation.\nIEEE Robotics and\nAutomation Letters, 7(2):4126\u20134133, 2022.\n[25] Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan,\nPieter Abbeel, and Michael Laskin. Hierarchical few-shot\nimitation with skill transition models. In International\nConference on Learning Representations, 2021.\n[26] Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Fre-\nitas, Gregory Wayne, and Nicolas Heess. Robust imita-\ntion of diverse behaviors. Advances in Neural Informa-\ntion Processing Systems, 30, 2017.\n[27] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Ku-\nmar, Jonathan Tompson, Sergey Levine, and Pierre Ser-\nmanet. Learning latent plans from play. In Leslie Pack\nKaelbling, Danica Kragic, and Komei Sugiura, editors,\nProceedings of the Conference on Robot Learning, vol-\nume 100 of Proceedings of Machine Learning Research,\npages 1113\u20131132. PMLR, 30 Oct\u201301 Nov 2020. URL\nhttps://proceedings.mlr.press/v100/lynch20a.html.\n[28] Suneel Belkhale and Dorsa Sadigh. PLATO: Predicting\nlatent affordances through object-centric play.\nIn 6th\nAnnual Conference on Robot Learning, 2022.\nURL\nhttps://openreview.net/forum?id=UAA5bNospA0.\n[29] Edward Johns. Coarse-to-fine imitation learning: Robot\nmanipulation from a single demonstration. In 2021 IEEE\ninternational conference on robotics and automation\n(ICRA), pages 4613\u20134619. IEEE, 2021.\n[30] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Hydra:\nHybrid robot actions for imitation learning. In Confer-\nence on Robot Learning, pages 2113\u20132133. PMLR, 2023.\n[31] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, et al. Inner monologue:\nEmbodied reasoning through planning with language\nmodels. In Conference on Robot Learning, pages 1769\u2013\n1782. PMLR, 2023.\n[32] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia,\nDebidatta Dwibedi, Keerthana Gopalakrishnan, Chris-\ntine Chan, Gabriel Dulac-Arnold, Sharath Maddineni,\nNikhil J Joshi, Pete Florence, Wei Han, Robert Baruch,\nYao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi,\nKarol Hausman, Izhak Shafran, Brian Ichter, and Yuan\nCao. Robovqa: Multimodal long-horizon reasoning for\nrobotics. In arXiv preprint arXiv:2311.00899, 2023.\n[33] Suvir Mirchandani, Siddharth Karamcheti, and Dorsa\nSadigh.\nELLA: Exploration through learned language\nabstraction. In A. Beygelzimer, Y. Dauphin, P. Liang,\nand J. Wortman Vaughan, editors, Advances in Neural\nInformation Processing Systems, 2021.\nURL https://\nopenreview.net/forum?id=VvUldGZ3izR.\n[34] Joey Hejna, Pieter Abbeel, and Lerrel Pinto. Improving\nlong-horizon imitation through instruction prediction. In\nProceedings of the AAAI Conference on Artificial Intel-\nligence, volume 37, pages 7857\u20137865, 2023.\n[35] Shengran Hu and Jeff Clune. Thought Cloning: Learning\nto think while acting by imitating human thinking. Ad-\nvances in Neural Information Processing Systems, 2023.\n[36] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas.\nSkill induction and planning with latent language. In Pro-\nceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 1713\u20131726, 2022.\n[37] Xinjie Liu.\nInteractive imitation learning in robotics\nbased on simulations, 2022.\n[38] St\u00b4ephane Ross, Geoffrey J. Gordon, and J. Andrew\nBagnell. No-regret reductions for imitation learning and\nstructured prediction. CoRR, abs/1011.0686, 2010. URL\nhttp://arxiv.org/abs/1011.0686.\n[39] Michael Kelly, Chelsea Sidrane, Katherine Driggs-\nCampbell, and Mykel J Kochenderfer. Hg-dagger: Inter-\nactive imitation learning with human experts. In 2019\nInternational Conference on Robotics and Automation\n(ICRA), pages 8077\u20138083. IEEE, 2019.\n[40] Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller,\nAlbert Wilcox, Daniel S Brown, and Ken Goldberg.\nThriftydagger: Budget-aware novelty and risk gating for\ninteractive imitation learning. In Conference on Robot\nLearning, pages 598\u2013608. PMLR, 2022.\n[41] Ryan Hoque, Ashwin Balakrishna, Carl Putterman,\nMichael Luo, Daniel S. Brown, Daniel Seita, Brijen\nThananjeyan, Ellen R. Novoseller, and Ken Goldberg.\nLazydagger: Reducing context switching in interactive\nimitation learning. In CASE, pages 502\u2013509, 2021. URL\nhttps://doi.org/10.1109/CASE49439.2021.9551469.\n[42] Jiakai Zhang and Kyunghyun Cho. Query-efficient im-\nitation learning for end-to-end simulated driving.\nIn\nProceedings of the Thirty-First AAAI Conference on\nArtificial Intelligence, AAAI\u201917, page 2891\u20132897. AAAI\nPress, 2017.\n[43] Kunal Menda, Katherine Driggs-Campbell, and Mykel J.\nKochenderfer. Ensembledagger: A bayesian approach to\nsafe imitation learning. In 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 5041\u20135048, 2019. doi: 10.1109/IROS40897.2019.\n8968287.\n[44] Mengxi Li, Alper Canberk, Dylan P Losey, and Dorsa\nSadigh.\nLearning human objectives from sequences\nof physical corrections.\nIn 2021 IEEE International\nConference on Robotics and Automation (ICRA), pages\n2877\u20132883. IEEE, 2021.\n[45] Dylan P Losey, Andrea Bajcsy, Marcia K O\u2019Malley, and\nAnca D Dragan. Physical interaction as communication:\nLearning robot objectives online from human corrections.\nThe International Journal of Robotics Research, 41(1):\n20\u201344, 2022.\n[46] Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon,\nMontserrat Gonzalez Arenas, Andy Zeng, Fei Xia, and\nDorsa Sadigh.\nDistilling and retrieving generalizable\nknowledge for robot manipulation via language cor-\nrections.\nIn 2nd Workshop on Language and Robot\nLearning: Language as Grounding, 2023.\n[47] Alexander Broad, Jacob Arkin, Nathan Ratliff, Thomas\nHoward, and Brenna Argall. Real-time natural language\ncorrections for assistive robotic manipulators. The Inter-\nnational Journal of Robotics Research, 36(5-7):684\u2013698,\n2017.\n[48] Arthur Bucker, Luis Figueredo, Sami Haddadinl, Ashish\nKapoor, Shuang Ma, and Rogerio Bonatti.\nReshaping\nrobot trajectories using natural language commands: A\nstudy of multi-modal data alignment using transformers.\nIn 2022 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), pages 978\u2013984. IEEE,\n2022.\n[49] Arthur Bucker, Luis Figueredo, Sami Haddadin, Ashish\nKapoor, Shuang Ma, Sai Vemprala, and Rogerio Bonatti.\nLatte: Language trajectory transformer, 2022.\n[50] John D Co-Reyes, Abhishek Gupta, Suvansh San-\njeev, Nick Altieri, Jacob Andreas, John DeNero, Pieter\nAbbeel, and Sergey Levine.\nGuiding policies with\nlanguage via meta-learning. In International Conference\non Learning Representations, 2018.\n[51] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli\nDing, James Betker, Robert Baruch, Travis Armstrong,\nand Pete Florence. Interactive language: Talking to robots\nin real time.\nIEEE Robotics and Automation Letters,\npages 1\u20138, 2023. doi: 10.1109/LRA.2023.3295255.\n[52] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x:\nOn scaling up a multilingual vision and language model.\narXiv preprint arXiv:2305.18565, 2023.\n[53] S. Lloyd. Least squares quantization in pcm. IEEE Trans-\nactions on Information Theory, 28(2):129\u2013137, 1982.\ndoi: 10.1109/TIT.1982.1056489.\n[54] Open X-Embodiment Collaboration, Abhishek Padalkar,\nAcorn Pooley, Ajinkya Jain, Alex Bewley, Alex Her-\nzog, Alex Irpan, Alexander Khazatsky, Anant Rai,\nAnikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan\nWahid, Ben Burgess-Limerick, Beomjoon Kim, Bern-\nhard Sch\u00a8olkopf, Brian Ichter, Cewu Lu, Charles Xu,\nChelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang\nHuang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline\nDevin, Danny Driess, Deepak Pathak, Dhruv Shah, Di-\neter B\u00a8uchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward\nJohns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue\nZhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan,\nGiulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi,\nHeni Ben Amor, Henrik I Christensen, Hiroki Furuta,\nHomer Walke, Hongjie Fang, Igor Mordatch, Ilija Ra-\ndosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim,\nJan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey\nBingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan\nGu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tomp-\nson, Jonathan Yang, Joseph J. Lim, Jo\u02dcao Silv\u00b4erio, Jun-\nhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman,\nKeegan Go, Keerthana Gopalakrishnan, Ken Goldberg,\nKendra Byrne, Kenneth Oslund, Kento Kawaharazuka,\nKevin Zhang, Keyvan Majd, Krishan Rana, Krishnan\nSrinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam\nTan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max-\nimilian Du, Michael Ahn, Mingtong Zhang, Mingyu\nDing, Mohan Kumar Srirama, Mohit Sharma, Moo Jin\nKim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess,\nNikhil J Joshi, Niko Suenderhauf, Norman Di Palo,\nNur Muhammad Mahi Shafiullah, Oier Mees, Oliver\nKroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu,\nPierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael\nRafailov, Ran Tian, Ria Doshi, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nRussell Mendonca, Rutav Shah, Ryan Hoque, Ryan Ju-\nlian, Samuel Bustamante, Sean Kirmani, Sergey Levine,\nSherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song,\nSichun Xu, Siddhant Haldar, Simeon Adebola, Simon\nGuist, Soroush Nasiriany, Stefan Schaal, Stefan Welker,\nStephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki\nOsa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao,\nTianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao,\nTravis Armstrong, Trevor Darrell, Vidhi Jain, Vincent\nVanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Bur-\ngard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin\nLi, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu,\nYing Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho,\nYoungwoon Lee, Yuchen Cui, Yueh hua Wu, Yujin Tang,\nYuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo,\nZhuo Xu, and Zichen Jeff Cui. Open X-Embodiment:\nRobotic learning datasets and RT-X models.\nhttps:\n//arxiv.org/abs/2310.08864, 2023.\n[55] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\nNasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,\nSilvio Savarese, Yuke Zhu, and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n.\nWhat matters in learning from offline human demonstra-\ntions for robot manipulation. In 5th Annual Conference\non Robot Learning, 2021. URL https://openreview.net/\nforum?id=JrsfBJtDFdI.\n[56] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data\nquality in imitation learning. In Thirty-seventh Confer-\nence on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=FwmvbuDiMk.\nAPPENDIX\nWe first outline the implementation of RT-H and ablations\nin Appendix A, along with the training recipes. Then we\ndiscuss implementations and training protocols for methods\nusing corrections in Appendix B. After, we detail each of the\ndatasets used in Appendix C. Finally, we show more detailed\nresults in Appendix D, including success rates for different\nstages of each task for each method, quantitative analysis of\ncontextuality in RT-H, and qualitative analysis of the language\nmotion multimodality of RT-H.\nA. Method Implementations\nAs described in Section III, RT-H and RT-2 [4] are imple-\nmented using a Pali-X 55B Multimodal Encoder Decoder\nTransformer architecture [52]. Images are encoded using a\n22B ViT architecture, which is learned during the Pali-X\npre-training phase but fixed during robot demonstration data\nco-training. The encoded images and the prompt are passed\nthrough the Encoder, and the output for each query is autore-\ngressively decoded with the Decoder. Next we describe each\nmethod (offline only) in detail.\nRT-H: RT-H first predicts language motions from the task\nusing the following language motion query: Q: What skill\nshould the robot do to [task]? A: , where the task is specified in\nlanguage, and the resulting language motion (skill) is returned\nin language. Then, it uses the predicted language motion to\nbetter inform action prediction using the following action\nquery: Q: What action should the robot do to [task], with\ncurrent skill: [motion]? A: , where the output is the tokenized\naction string. RT-H trains in the same fashion as RT-2 [4].\nFirst, we pretrain RT-H on the same large vision language\ndataset as RT-2, and we then co-train the language motion and\naction queries using 50% of the overall training samples (25%\neach), along with 50% using the original pre-training mixture.\nSimilar to RT-2, we use a learning rate of 1e-3 and with a\nconstant linear warmup and square root normalize decay, and\na batch size of 1024.\nRT-H-Joint: Unlike RT-H, RT-H-Joint uses a single query to\npredict both language motion and action. While both methods\nare autoregressive on the language motion, RT-H has two\nqueries which using different wordings to indicate language\nmotion or action prediction, and RT-H also passes in the\nlanguage motion to the encoder for the action query since\nit is part of the prompt string. The prompt for RT-H-Joint\nis as follows: Q: What skill and action should the robot do\nto [task]? A: . Then the output is a concatenation of first\nlanguage motion (skill) in language, and then the tokenized\naction string, in the form skill: [skill], action: [action]. RT-H-\nJoint is trained identically to RT-2 and RT-H as well, but with\nthe joint language motion and action query instead of action\nprediction from RT-2.\nRT-H-Cluster: RT-H-Cluster follows the same two query\nprocedure and training implementation as RT-H. In order to\ndetermine the action clusters, we first normalize the actions in\nthe dataset using dataset statistics. Then, we cluster the actions\nusing K-means [53] using 256 cluster centers. We chose this\nnumber to be on par with the number of actively used language\nmotions in the dataset from our automated labeling procedure.\nThen, the cluster centers are replaced with integers from 0\nto 255, and used in place of language motions in the action\nhierarchy. This ablation tests the utility of language motions\ncompared to embeddings tuned to the specific actions in the\ndatasets.\nRT-H-OneHot: RT-H-OneHot also follows the same two\nquery procedure and training implementation as RT-H. The\nonly change is to replace unique language motions with\nintegers. We first enumerate skills in order of how common\nthey are, and then assign a unique integer value to each.\nImportantly, this formulation does not capture the inherent\nstructure of language: for example, \u201cmove arm forward\u201d and\n\u201cmove arm forward and left\u201d are similar in many ways and\nshould be treated as such, but their replaced one-hot labels will\nlikely be as equidistant as any other two random language\nmotions. Thus, RT-H-OneHot tests the importance of the\nstructure of language when predicting language motions.\nB. Corrections\nAs described in Section IV-A, RT-H enables humans to inter-\nvene with new language motions, and then these corrections\ncan be deployed on robot. To train RT-H on corrections, we\ncan directly type or say language motion corrections that will\nbe passed directly into the action query in place of the inferred\nlanguage motion from the language motion query. This shifts\nthe burden of correction up one level in the action hierarchy,\nfrom actions to language motions. We collect the dataset of\nlanguage motion corrections, recording the observations, task,\nand language motion corrections, and then co-train our model\nwith the original pre-training dataset, the robot demonstration\ndataset, and upweighted language motion corrections. For such\na large demonstration dataset, we aim for each correction sam-\nple to be seen 50x as often as a corresponding demonstration\ndataset example. Thus the sampling weights during training\non the Diverse+Kitchen dataset are as follows:\n\u2022 Pre-training Queries: 50%\n\u2022 Demonstration Data Language Motion Query: 23%\n\u2022 Demonstration Data Action Query: 23%\n\u2022 Correction Data Language Motion Query: 4%\nGiven that the ratio of the demonstration dataset size to the\nlanguage motion correction dataset size is roughly 300:1, this\ncorresponds to upweighting each language motion correction\nsample by 50:1.\nWe use the same recipe for training from teleoperated\ncorrections with RT-2-IWR. The only difference is that the\nlanguage motion training queries are replaced with action\nqueries like in IWR.\nC. Datasets\nWe use two datasets in this work, the Kitchen dataset from\nRT-1 [6] and RT-2 [4], and the new Diverse+Kitchen dataset\n(which is an extended version of Kitchen). Kitchen consists\nof the 6 semantic tasks used for evaluation in 70K\ndemonstrations, across several common object categories like\ncans, bottles, and fruits, forming 542 unique instructions. The\nsemantic task instructions are as follows:\n\u2022 knock over: Knock Object Over.\n\u2022 drawer place: Pick Object from drawer and place it\non counter.\n\u2022 move: Move Object Near Object.\n\u2022 pick: Pick Object.\n\u2022 open / close drawer: [Open / Close] [Top / Middle /\nBottom] Drawer.\n\u2022 place upright: Place Object Upright.\nDiverse+Kitchen consists of all the demonstrations from\nKitchen, but with 24 more semantic tasks in only 30K\nadditional demonstrations, with 165 unique instructions. The\nnew task instructions in the dataset are as follows, sorted by\nfrequency (most to least):\n\u2022 pull napkin out of dispenser and place napkin flat on\ncounter\n\u2022 pull napkin out of dispenser\n\u2022 pick a bowl and place the bowl upright on counter\n\u2022 close the large glass jar containing pistachios using\nthe lid on counter\n\u2022 pick a cup and place the cup upright on counter\n\u2022 open the large glass jar with pistachios\n\u2022 grab a scooper\n\u2022 pick up the scoop from the basket\n\u2022 open the large glass jar with pistachios and place the lid\non counter\n\u2022 place the scoop inside the basket\n\u2022 put a bowl under the cereal dispenser spout\n\u2022 move the bowl away from underneath the spout\n\u2022 pick an oatmeal packet and place the oatmeal packet\nin the bowl\n\u2022 pick up spoon and place spoon in bowl with cereal\n\u2022 swivel the cereal dispenser until the bowl is half full\n\u2022 pick up the tong from the basket\n\u2022 place the tong inside the basket\n\u2022 pour the snack from the scoop into the cup\n\u2022 scoop the snack from the jar\n\u2022 pick object\n\u2022 move object near object\n\u2022 knock object over\n\u2022 place object upright\n\u2022 squeeze honey into the bowl\nThis represents a diverse range of behaviors for the robot, often\nwith huge data imbalances between tasks. Note that there are\nadditional demonstrations for the knock over, move, pick, and\nplace tasks in this dataset, although these comprise a small\nfraction of the overall data. The tasks used for evaluation in\nSection V-A and Section V-C are bolded.\nD. Detailed Results\nDiverse Evaluations: Next we show the cumulative success\nrates for different stages of each task in the Diverse+Kitchen\nevaluations from Section V-A. Fig. 9 shows the \u201cPlace Bowl\nUpright\u201d task, and RT-H and RT-H-Joint are able to pick up\nthe bowl 50% of the time (compared to 20% for RT-2), but\nRT-H struggles to rotate the bowl afterwards. Fig. 10 shows\nthe \u201cOpen Pistachio Jar\u201d task, where we see that methods\nwith action hierarchy get substantially farther than RT-2 on\nthis task. Fig. 11 shows the \u201cClose Pistachio Jar\u201d task, where\nonce again RT-2 rarely exhibits the correct behavior compared\nto methods with action hierarchy. Thus even though success\nrates for all methods are fairly low on the open and close jar\ntasks, we see that RT-H and its variants are able to progress\nmuch farther. Fig. 12 shows the \u201cMove Bowl Away\u201d task,\nwhere we see once again that methods with action hierarchy\nget much farther in the task than RT-2. Here, we can see that\nRT-H struggles to grasp the thin rim of the bowl, compared\nto RT-H-Joint which has high success with grasping. Fig. 13\nshows the \u201cPut Bowl Under\u201d task, where once again RT-H and\nother action hierarchy methods do better on each stage of the\ntask, with RT-H getting the highest final success rate. Fig. 14\nshows the \u201cPlace oatmeal in bowl\u201d task, and RT-H and RT-H-\nJoint get much farther in the task compared to RT-2, RT-H-\nOneHot, and Fig. 15 shows the \u201cGrab Scooper\u201d task, and it is\none of the few tasks where RT-2 does better than some action\nhierarchy methods (RT-H-Cluster and RT-H-OneHot), but RT-\nH outperforms RT-2 on all stages of the task. In Fig. 16, we\nshow the \u201cPull napkin out\u201d task, and RT-H, RT-H-Joint, and\nRT-2 all get very high success rates.\nOverall, we see that in many cases, there are only one or\ntwo stages of the task that require correction. This often only\nrequires a few language corrections, which provides insight as\nto why RT-H-Intervene can improve task performance with so\nlittle new data.\nGeneralization: Next we show the staged cumulative success\nrates for RT-H generalizing to novel objects, as shown in\nSection V-D and Table II. Fig. 17 shows the pick task and\nFig. 18 shows the move task, and in both tasks we see that\nRT-H does better not just in final success rate but also in each\nindividual stage of each task.\nContextuality: To highlight the contextuality of RT-H quan-\ntitatively, we compute in Table III the mean (and standard\ndeviation) of each action dimension for actions that belong to\nthe same language motion group. We use the validation set of\nthe Diverse+Kitchen dataset (using the automated language\nmotion labeling procedure from Section III-C) to compute\nthese statistics. We find that even though the dominant action\ndimension for each language motion has the largest mean and\naction variance (bold in Table III), other action dimensions\nalso have nontrivial variance, suggesting that the interpretation\nof each language motion changes with the scene and the task.\nIn other words, translating language motion to action (action\nquery) is a contextual process. Sometimes, the mean of the\nnon-dominant action axis is also nontrivial (e.g., for rotate\narm right, the arm has some arm (x) and (y) bias), which is\nlikely due to bias from the chosen set of tasks in the dataset.\nMultimodality: Next, we study if the language motion ab-\nstraction has enabled RT-H to learn not just the correct\nlanguage motion at each step, but also diverse ways of\naccomplishing the same task. To analyze this qualitatively,\nwe run the language motion query on offline validation data\nFig. 9: Place Bowl Upright on Counter: Cumulative success rates for each method.\nFig. 10: Open Pistachio Jar: Cumulative success rates for each method.\nFig. 11: Close Pistachio Jar: Cumulative success rates for each method.\nwith beam search to output the top three language motions\nfor images in the dataset. We show four examples of this\nin Fig. 19. In the first row (examples (a) and (b)), RT-\nH predicts language motions that differ slightly from each\nother but in task-contextual ways (e.g., move arm forward\nvs. move arm down and forward, both are accurate for the\ntask). In the second row (examples (c) and (d)), RT-H predicts\nlanguage motions that are quite different from each other (e.g.,\nmove arm left vs. close gripper), but despite the variety, each\nlanguage motion is reasonable given the context of the scene\nand task. This shows that RT-H can capture the multimodality\nof behaviors for tasks from the data. In fact, this ability to\nrepresent multiple high level behaviors could be how RT-H\nis so efficient in learning from language motion intervention\ndata \u2013 an intervened language motion might be quite likely\nalready in the model, and so updating the model to predict the\nnew language motion might be a trivial change to the model.\nAdditionally, this language motion multimodality could be\nquite useful for exploration in a reinforcement learning context\n(e.g., sampling language motions from the model instead of\nactions from some uniform distribution).\nFig. 12: Move Bowl Away from Cereal Dispenser: Cumulative success rates for each method.\nFig. 13: Put Bowl Under Cereal Dispenser: Cumulative success rates for each method.\nFig. 14: Place Oatmeal Packet in Bowl: Cumulative success rates for each method.\nFig. 15: Grab a Scooper: Cumulative success rates for each method.\nFig. 16: Pull Napkin out of Dispenser: Cumulative success rates for each method.\nFig. 17: Pick (novel objects): Cumulative success rates for RT-2 and RT-H. RT-H not only has higher final success rates compared to RT-2,\nbut also success at each stage of the task.\nFig. 18: Move (novel objects): Cumulative success rates for RT-2 and RT-H. RT-H not only has higher final success rates compared to\nRT-2, but also success at each stage of the task.\nLanguage Motion\narm (x)\narm (y)\narm (z)\narm (rx)\narm (ry)\narm (rz)\ngripper\nmove arm forward\n7.7 (6.2)\n-0.5 (3.3)\n-1.4 (3.2)\n3.4 (10.9)\n-1.1 (8.9)\n-4.0 (10.3)\n1.0 (3.3)\nmove arm backward\n-10.9 (10.1)\n-2.4 (4.6)\n-2.3 (5.2)\n0.5 (9.2)\n7.8 (11.6)\n0.0 (13.1)\n0.5 (6.6)\nmove arm left\n-0.4 (3.2)\n8.2 (6.3)\n-2.1 (3.4)\n6.1 (12.0)\n2.5 (7.7)\n7.9 (11.5)\n1.0 (2.1)\nmove arm right\n-1.4 (4.1)\n-6.9 (7.3)\n-1.5 (4.0)\n-1.9 (11.7)\n2.2 (7.7)\n-6.6 (11.6)\n1.1 (0.9)\nmove arm up\n-1.7 (4.6)\n-2.1 (4.9)\n11.1 (8.6)\n1.7 (11.7)\n-7.4 (10.9)\n0.5 (10.5)\n0.9 (4.6)\nmove arm down\n-0.6 (4.0)\n-0.3 (3.1)\n-8.1 (9.0)\n1.4 (10.0)\n5.6 (10.5)\n-1.0 (8.2)\n1.1 (3.5)\nrotate arm right\n2.1 (4.6)\n3.0 (5.5)\n0.3 (5.1)\n29.3 (24.5)\n-2.1 (12.2)\n0.0 (13.0)\n1.0 (1.5)\nrotate arm left\n0.4 (4.2)\n-0.4 (4.2)\n-0.5 (3.9)\n-24.3 (21.9)\n-4.1 (14.8)\n0.9 (10.6)\n1.2 (1.6)\nrotate arm up\n0.0 (4.5)\n0.3 (3.3)\n-2.3 (4.4)\n2.5 (12.4)\n20.5 (18.2)\n1.7 (8.8)\n1.0 (1.4)\nrotate arm down\n0.3 (4.5)\n0.8 (3.8)\n1.9 (5.0)\n-7.4 (15.1)\n-28.7 (26.6)\n-0.1 (13.6)\n1.0 (1.6)\nrotate arm counterclockwise\n3.3 (4.3)\n-1.1 (4.8)\n-0.3 (4.5)\n4.2 (12.7)\n-3.1 (12.0)\n-24.1 (21.5)\n1.1 (3.4)\nrotate arm clockwise\n-0.5 (5.4)\n3.1 (5.1)\n0.0 (4.8)\n-0.9 (12.3)\n-0.1 (11.8)\n24.0 (20.8)\n0.9 (5.2)\nopen gripper\n0.6 (0.9)\n0.7 (1.3)\n0.7 (1.3)\n0.7 (2.7)\n0.6 (2.0)\n0.8 (2.3)\n-67.6 (42.9)\nclose gripper\n0.7 (1.8)\n0.8 (1.3)\n0.8 (2.0)\n0.9 (4.3)\n0.5 (3.8)\n0.9 (3.6)\n65.0 (43.5)\nTABLE III: Action Means (and Standard Deviations) for basic language motions (cardinal directions) for each action dimension (arm delta\nx,y,z; rotate arm delta x,y,z; and gripper) in the Diverse+Kitchen dataset, computed over the validation set. The bolded numbers correspond\nto the dominant axis which the language motion refers to. Note that positions and rotations are not scaled to match each other. We find that\nwhile the dominant axis has the largest mean and variance for each skill (bolded numbers), other axes also have nontrivial variation (but\noften close to zero mean). This demonstrates that a given skill is not merely a fixed primitive, but maps to a wide variety of potential actions\ndepending on the actual state and the task (i.e., context).\nFig. 19: Four examples of multimodal language motion prediction in RT-H using beam search on the language motion query. We show the\ntop three language motion predictions (P1, P2, and P3) for each example. In (a) and (b) (top row), we see that RT-H is capable of representing\nmultiple valid ways of doing the task that are all very contextual. The language motions outputted in (a) and (b) are quite similar to each\nother, but differ in subtle but task-relevant ways. In (c) and (d), we see that RT-H predicts even more diverse ways to accomplish the task,\nonce again contextual to the task and scene.\n"
  },
  {
    "title": "ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models",
    "link": "https://arxiv.org/pdf/2403.01807.pdf",
    "upvote": "7",
    "text": "ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models\nLukas H\u00a8ollein1,2\nAlja\u02c7z Bo\u02c7zi\u02c7c2\nNorman M\u00a8uller2\nDavid Novotny2\nHung-Yu Tseng2\nChristian Richardt2\nMichael Zollh\u00a8ofer2\nMatthias Nie\u00dfner1\n1Technical University of Munich\n2Meta\nhttps://lukashoel.github.io/ViewDiff/\na stuffed \nbear sitting \non a \nwooden box\nInput\nMulti-view generated images\nFigure 1. Multi-view consistent image generation. Our method takes as input a text description, or any number of posed input images, and\ngenerates high-quality, multi-view consistent images of a real-world 3D object in authentic surroundings from any desired camera poses.\nAbstract\n3D asset generation is getting massive amounts of attention,\ninspired by the recent success of text-guided 2D content\ncreation. Existing text-to-3D methods use pretrained text-\nto-image diffusion models in an optimization problem or\nfine-tune them on synthetic data, which often results in non-\nphotorealistic 3D objects without backgrounds. In this paper,\nwe present a method that leverages pretrained text-to-image\nmodels as a prior, and learn to generate multi-view images in\na single denoising process from real-world data. Concretely,\nwe propose to integrate 3D volume-rendering and cross-\nframe-attention layers into each block of the existing U-Net\nnetwork of the text-to-image model. Moreover, we design an\nautoregressive generation that renders more 3D-consistent\nimages at any viewpoint. We train our model on real-world\ndatasets of objects and showcase its capabilities to generate\ninstances with a variety of high-quality shapes and textures\nin authentic surroundings. Compared to the existing methods,\nthe results generated by our method are consistent, and have\nfavorable visual quality (\u221230% FID, \u221237% KID).\n1. Introduction\nIn recent years, text-to-image (T2I) diffusion models [31, 35]\nhave emerged as cutting-edge technologies, revolutionizing\nhigh-quality and imaginative 2D content creation guided by\ntext descriptions. These frameworks have found widespread\napplications, including extensions such as ControlNet [59]\nand DreamBooth [34], showcasing their versatility and poten-\ntial. An intriguing direction in this domain is to use T2I mod-\nels as powerful 2D priors for generating three-dimensional\n(3D) assets. How can we effectively use these models to\ncreate photo-realistic and diverse 3D assets?\nExisting methods like DreamFusion [29], Fantasia3D [5],\nand ProlificDreamer [52] have demonstrated exciting results\nby optimizing a 3D representation through score distillation\nsampling [29] from pretrained T2I diffusion models. The\n1\narXiv:2403.01807v1  [cs.CV]  4 Mar 2024\n3D assets generated by these methods exhibit compelling\ndiversity. However, their visual quality is not consistently\nas high as that of the images generated by T2I models. A\nkey step to obtaining 3D assets is the ability to generate\nconsistent multi-view images of the desired objects and their\nsurroundings. These images can then be fitted to 3D represen-\ntations like NeRF [26] or NeuS [50]. HoloDiffusion [19] and\nViewsetDiffusion [42] train a diffusion model from scratch\nusing multi-view images and output 3D-consistent images.\nGeNVS [4] and DFM [46] additionally produce object sur-\nroundings, thereby increasing the realism of the generation.\nThese methods ensure (photo)-realistic results by training\non real-world 3D datasets [33, 62]. However, these datasets\nare orders of magnitude smaller than the 2D dataset used to\ntrain T2I diffusion models. As a result, these approaches pro-\nduce realistic but less-diverse 3D assets. Alternatively, recent\nworks like Zero-1-to-3 [24] and One-2-3-45 [23] leverage\na pretrained T2I model and fine-tune it for 3D consistency.\nThese methods successfully preserve the diversity of gener-\nated results by training on a large synthetic 3D dataset [7].\nNonetheless, the produced objects can be less photo-realistic\nand are without surroundings.\nIn this paper, we propose a method that leverages the\n2D priors of the pretrained T2I diffusion models to produce\nphoto-realistic and 3D-consistent 3D asset renderings. As\nshown in the first two rows of Fig. 1, the input is a text\ndescription or an image of an object, along with the cam-\nera poses of the desired rendered images. The proposed\napproach produces multiple images of the same object in\na single forward pass. Moreover, we design an autoregres-\nsive generation scheme that allows to render more images\nat any novel viewpoint (Fig. 1, third row). Concretely, we\nintroduce projection and cross-frame-attention layers, that\nare strategically placed into the existing U-Net architecture,\nto encode explicit 3D knowledge about the generated object\n(see Fig. 2). By doing so, our approach paves the way to fine-\ntune T2I models on real-world 3D datasets, such as CO3D\n[33], while benefiting from the large 2D prior encoded in\nthe pretrained weights. Our generated images are consistent,\ndiverse, and realistic renderings of objects.\nTo summarize, our contributions are:\n\u2022 a method that utilizes the pretrained 2D prior of text-to-\nimage models and turns them into 3D-consistent image\ngenerators. We train our approach on real-world multi-\nview datasets, allowing us to produce realistic and high-\nquality images of objects and their surroundings (Sec. 3.1).\n\u2022 a novel U-Net architecture that combines commonly used\n2D layers with 3D-aware layers. Our projection and cross-\nframe-attention layers encode explicit 3D knowledge into\neach block of the U-Net architecture (Sec. 3.2).\n\u2022 an autoregressive generation scheme that renders images\nof a 3D object from any desired viewpoint directly with\nour model in a 3D-consistent way (Sec. 3.3).\n2. Related Work\nText-To-2D.\nDenoising diffusion probabilistic models\n(DDPM) [14] model a data distribution by learning to invert\na Gaussian noising process with a deep network. Recently,\nDDPMs were shown to be superior to generative adversarial\nnetworks [8], becoming the state-of-the-art framework for\nimage generation. Soon after, large text-conditioned models\ntrained on billion-scale data were proposed in Imagen [35]\nor Dall-E 2 [31]. While [8] achieved conditional generation\nvia guidance with a classifier, [13] proposed classifier-free\nguidance. ControlNet [59] proposed a way to tune the diffu-\nsion outputs by conditioning on various modalities, such as\nimage segmentation or normal maps. Similar to ControlNet,\nour method builds on the strong 2D prior of a pretrained\ntext-to-image (T2I) model. We further demonstrate how to\nadjust this prior to generate 3D-consistent images of objects.\nText-To-3D.\n2D DDPMs were applied to the generation of\n3D shapes [30, 37, 44, 47, 49, 55, 64] or scenes [10, 15, 45]\nfrom text descriptions. DreamFusion [29] proposed score dis-\ntillation sampling (SDS) which optimizes a 3D shape whose\nrenders match the belief of the DDPM. Improved sample\nquality was achieved by a second-stage mesh optimization\n[5, 22], and smoother SDS convergence [38, 52]. Several\nmethods use 3D data to train a novel-view synthesis model\nwhose multi-view samples can be later converted to 3D, e.g.\nconditioning a 2D DDPM on an image and a relative cam-\nera motion to generate novel views [24, 53]. However, due\nto no explicit modelling of geometry, the outputs are view-\ninconsistent. Consistency can be improved with epipolar\nattention [48, 63], or optimizing a 3D shape from multi-view\nproposals [23]. Our work fine-tunes a 2D T2I model to gen-\nerate renders of a 3D object; however, we propose explicit\n3D unprojection and rendering operators to improve view-\nconsistency. Concurrently, SyncDreamer [25] also add 3D\nlayers in their 2D DDPM. We differ by training on real data\nwith backgrounds and by showing that autoregressive gener-\nation is sufficient to generate consistent images, making the\nsecond 3D reconstruction stage expendable.\nDiffusion on 3D Representations.\nSeveral works model\nthe distribution of 3D representations. While DiffRF [28]\nleverages ground-truth 3D shapes, HoloDiffusion [19] is\nsupervised only with 2D images. HoloFusion [18] extends\nthis work with a 2D diffusion render post-processor. Im-\nages can also be denoised by rendering a reconstructing 3D\nshape [1, 42]. Unfortunately, the limited scale of existing 3D\ndatasets prevents these 3D diffusion models from extrapo-\nlating beyond the training distribution. Instead, we exploit\na large 2D pretrained DDPM and add 3D components that\nare tuned on smaller-scale multi-view data. This leads to im-\nproved multi-view consistency while maintaining the expres-\nsivity brought by pretraining on billion-scale image data.\n2\nMSE\nMSE\npredicted \nnoise\nsampled multi-view \nnoise & images\nup/downsample\nCross-Attention\nCross-Frame-Attn\nProjection Layer\nResNet Block\nmulti-branch U-Net (shared weights)\nRT, K, I,\u201ctext\u201c\nt~U[0,1000]\nFigure 2. Method Overview. We augment the U-Net architecture of pretrained text-to-image models with new layers in every U-Net\nblock. These layers facilitate communication between multi-view images in a batch, resulting in a denoising process that jointly produces\n3D-consistent images. First, we replace self-attention with cross-frame-attention (yellow) which compares the spatial features of all views.\nWe condition all attention layers on pose (RT), intrinsics (K), and intensity (I) of each image. Second, we add a projection layer (green)\ninto the inner blocks of the U-Net. It creates a 3D representation from multi-view features and renders them into 3D-consistent features. We\nfine-tune the U-Net using the diffusion denoising objective (Eq. 3) at timestep t, supervised from captioned multi-view images.\n3. Method\nWe propose a method that produces 3D-consistent images\nfrom a given text or posed image input (see Fig. 1 top/mid).\nConcretely, given desired output poses, we jointly generate\nall images corresponding to the condition. We leverage pre-\ntrained text-to-image (T2I) models [31, 35] and fine-tune\nthem on multi-view data [33]. We propose to augment the\nexisting U-Net architecture by adding new layers into each\nblock (see Fig. 2). At test time, we can condition our method\non multiple images (see Fig. 1 bottom), which allows us to\nautoregressively render the same object from any viewpoint\ndirectly with the diffusion model (see Sec. 3.3).\n3.1. 3D-Consistent Diffusion\nDiffusion\nmodels\n[14,\n39]\nare\na\nclass\nof\ngener-\native models that learn the probability distribution\np\u03b8(x0)=\nR\np\u03b8(x0:T )dx1:T over data x0\u223cq(x0) and latent\nvariables x1:T =x1, . . . , xT . Our method is based on pre-\ntrained text-to-image models, which are diffusion models\np\u03b8(x0 | c) with an additional text condition c. For clarity, we\ndrop the condition c for the remainder of this section.\nTo produce multiple images x0:N\n0\nat once, which are 3D-\nconsistent with each other, we seek to model their joint\nprobability distribution p\u03b8(x0:N\n0\n)=\nR\np\u03b8(x0:N\n0:T )dx0:N\n1:T . Simi-\nlarly to concurrent work by Liu et al. [25], we generate one\nset of images p\u03b8(x0:N\n0\n) by adapting the reverse process of\nDDPMs [14] as a Markov chain over all images jointly:\np\u03b8(x0:N\n0:T ) := p(x0:N\nT\n)\nT\nY\nt=1\nN\nY\nn=0\np\u03b8(xn\nt\u22121 | x0:N\nt\n),\n(1)\nwhere we start the generation from Gaussian noise sam-\npled separately per image p(xn\nT ) = N(xn\nT ; 0, I), \u2200n \u2208\n[0, N]. We gradually denoise samples p\u03b8(xn\nt\u22121 | x0:N\nt\n) =\nN(xt\u22121; \u00b5n\n\u03b8 (x0:N\nt\n, t), \u03c32\nt I) by predicting the per-image\nmean \u00b5n\n\u03b8 (x0:N\nt\n, t) through a neural network \u00b5\u03b8 that is shared\nbetween all images. Importantly, at each step, the model uses\nthe previous states x0:N\nt\nof all images, i.e., there is commu-\nnication between images during the model prediction. We\nrefer to Sec. 3.2 for details on how this is implemented. To\ntrain \u00b5\u03b8, we define the forward process as a Markov chain:\nq(x0:N\n1:T | x0:N\n0\n) =\nT\nY\nt=1\nN\nY\nn=0\nq(xn\nt | xn\nt\u22121),\n(2)\nwhere q(xn\nt\n| xn\nt\u22121) = N(xn\nt ; \u221a1 \u2212 \u03b2txn\nt\u22121, \u03b2tI) and\n\u03b21, . . . , \u03b2T define a constant variance schedule, i.e., we ap-\nply separate noise per image to produce training samples.\nWe follow Ho et al. [14] by learning a noise predictor \u03f5\u03b8\ninstead of \u00b5\u03b8. This allows to train \u03f5\u03b8 with an L2 loss:\nEx0:N\n0\n,\u03f50:N\u223cN (0,I),n\nh\r\r\u03f5n \u2212 \u03f5n\n\u03b8 (x0:N\nt\n, t)\n\r\r2i\n.\n(3)\n3.2. Augmentation of the U-Net architecture\nTo model a 3D-consistent denoising process over all im-\nages, we predict per-image noise \u03f5n\n\u03b8 (x0:N\nt\n, t) through a neu-\nral network \u03f5\u03b8. This neural network is initialized from the\npretrained weights of existing text-to-image models, and is\nusually defined as a U-Net architecture [31, 35]. We seek to\nleverage the previous states x0:N\nt\nof all images to arrive at\na 3D-consistent denoising step. To this end, we propose to\nadd two layers into the U-Net architecture, namely a cross-\nframe-attention layer and a projection layer. We note that the\npredicted per-image noise needs to be image specific, since\n3\nall images are generated starting from separate Gaussian\nnoise. It is therefore important to keep around 2D layers\nthat act separately on each image, which we achieve by\nfinetuning the existing ResNet [11] and ViT [9] blocks. We\nsummarize our architecture in Fig. 2. In the following, we\ndiscuss our two proposed layers in more detail.\nCross-Frame Attention.\nInspired by video diffusion [54,\n56], we add cross-frame-attention layers into the U-Net archi-\ntecture. Concretely, we modify the existing self-attention lay-\ners to calculate CFAttn(Q, K, V )=softmax\n\u0000 QKT\n\u221a\nd\n\u0001\nV with\nQ = W Qhi, K = W K[hj]j\u0338=i, V = W V [hj]j\u0338=i,\n(4)\nwhere W Q, W K, W V are the pretrained weights for feature\nprojection, and hi\u2208RC\u00d7H\u00d7W is the input spatial feature of\neach image i\u2208[1, N]. Intuitively, this matches features across\nall frames, which allows generating the same global style.\nAdditionally, we add a conditioning vector to all cross-\nframe and cross-attention layers to inform the network about\nthe viewpoint of each image. First, we add pose information\nby encoding each image\u2019s camera matrix p \u2208 R4\u00d74 into an\nembedding z1 \u2208 R4 similar to Zero-1-to-3 [24]. Addition-\nally, we concatenate the focal length and principal point of\neach camera into an embedding z2 \u2208 R4. Finally, we provide\nan intensity encoding z3 \u2208 R2, which stores the mean and\nvariance of the image RGB values. At training time, we set\nz3 to the true values of each input image, and at test time,\nwe set z3=[0.5, 0] for all images. This helps to reduce the\nview-dependent lighting differences contained in the dataset\n(e.g., due to different camera exposure). We construct the\nconditioning vector as z=[z1, z2, z3], and add it through a\nLoRA-linear-layer [16] W \u2032Q to the feature projection matrix\nQ. Concretely, we compute the projected features as:\nQ = W Qhi + s \u00b7 W \u2032Q[hi; z],\n(5)\nwhere we set s=1. Similarly, we add the condition via W \u2032K\nto K, and W \u2032V to V .\nProjection Layer.\nCross-frame attention layers are helpful\nto produce globally 3D-consistent images. However, the\nobjects do not precisely follow the specified poses, which\nleads to view-inconsistencies (see Fig. 5 and Tab. 3). To this\nend, we add a projection layer into the U-Net architecture\n(Fig. 3). The idea of this layer is to create 3D-consistent\nfeatures that are then further processed by the next U-Net\nlayers (e.g. ResNet blocks). By repeating this layer across all\nstages of the U-Net, we ensure that the per-image features\nare in a 3D-consistent space. We do not add the projection\nlayer to the first and last U-Net blocks, as we saw no benefit\nfrom them at these locations. We reason that the network\nprocesses image-specific information at those stages and\nthus does not need a 3D-consistent feature space.\nInspired by multi-view stereo literature [3, 17, 41], we\ncreate a 3D feature voxel grid from all input spatial features\nAggregator MLP\nVolume \nRenderer\ninput posed \nfeatures\noutput \nfeatures\nScaleNet \n(1x1 CNN)\n3D CNN\nExpandNet\n(1x1 CNN)\nCompressNet\n(1x1 CNN)\nFigure 3. Architecture of the projection layer. We produce 3D-\nconsistent output features from posed input features. First, we\nunproject the compressed image features into 3D and aggregate\nthem into a joint voxel grid with an MLP. Then we refine the voxel\ngrid with a 3D CNN. A volume renderer similar to NeRF [26]\nrenders 3D-consistent features from the grid. Finally, we apply a\nlearned scale function and expand the feature dimension.\nh0:N\nin\n\u2208 RC\u00d7H\u00d7W by projecting each voxel into each image\nplane. First, we compress h0:N\nin\nwith a 1\u00d71 convolution to\na reduced feature dimension C\u2032=16. We then take the bilin-\nearly interpolated feature at the image plane location and\nplace it into the voxel. This way, we create a separate voxel\ngrid per view, and merge them into a single grid through an\naggregator MLP. Inspired by IBRNet [51], the MLP predicts\nper-view weights followed by a weighted feature average.\nWe then run a small 3D CNN on the voxel grid to refine\nthe 3D feature space. Afterwards, we render the voxel grid\ninto output features h0:N\nout \u2208 RC\u2032\u00d7H\u00d7W with volumetric ren-\ndering similar to NeRF [26]. We dedicate half of the voxel\ngrid to foreground and half to background and apply the\nbackground model from MERF [32] during ray-marching.\nWe found it is necessary to add a scale function after the\nvolume rendering output. The volume renderer typically uses\na sigmoid activation function as the final layer during ray-\nmarching [26]. However, the input features are defined in an\narbitrary floating-point range. To convert h0:N\nout back into the\nsame range, we non-linearly scale the features with 1\u00d71 con-\nvolutions and ReLU activations. Finally, we expand h0:N\nout to\nthe input feature dimension C. We refer to the supplemental\nmaterial for details about each component\u2019s architecture.\n3.3. Autoregressive Generation\nOur method takes as input multiple samples x0:N\nt\nat once\nand denoises them 3D-consistently. During training, we set\nN=5, but can increase it at inference time up to memory\nconstraints, e.g., N=30. However, we want to render an\n4\nobject from any possible viewpoint directly with our network.\nTo this end, we propose an autoregressive image generation\nscheme, i.e., we condition the generation of next viewpoints\non previously generated images. We provide the timesteps\nt0:N of each image as input to the U-Net. By varying t0:N,\nwe can achieve different types of conditioning.\nUnconditional Generation.\nAll samples are initialized to\nGaussian noise and are denoised jointly. The timesteps t0:N\nare kept identical for all samples throughout the reverse pro-\ncess. We provide different cameras per image and a single\ntext prompt. The generated images are 3D-consistent, show-\ning the object from the desired viewpoints (Figs. 4 and 5).\nImage-Conditional Generation.\nWe divide the total num-\nber of samples N=nc+ng into a conditional part nc and\ngenerative part ng. The first nc samples correspond to im-\nages and cameras that are provided as input. The other ng\nsamples should generate novel views that are similar to the\nconditioning images. We start the generation from Gaussian\nnoise for the ng samples and provide the un-noised images\nfor the other samples. Similarly, we set t0:nc=0 for all de-\nnoising steps, while gradually decreasing tng:N.\nWhen nc=1, our method performs single-image recon-\nstruction (Fig. 6). Setting nc>1 allows to autoregressively\ngenerate novel views from previous images (Fig. 1 bottom).\nIn practice, we first generate one batch of images uncondi-\ntionally and then condition the next batches on a subset of\nprevious images. This allows us to render smooth trajectories\naround 3D objects (see the supplemental material).\n3.4. Implementation Details\nDataset.\nWe train our method on the large-scale CO3Dv2\n[33] dataset, which consists of posed multi-view images of\nreal-world objects. Concretely, we choose the categories\nTeddybear, Hydrant, Apple, and Donut. Per cate-\ngory, we train on 500\u20131000 objects with each 200 images\nat resolution 256\u00d7256. We generate text captions with the\nBLIP-2 model [21] and sample one of 5 proposals per object.\nTraining.\nWe base our model on a pretrained latent-\ndiffusion text-to-image model. We only fine-tune the U-Net\nand keep the VAE encoder and decoder frozen. In each itera-\ntion, we select N=5 images and their poses. We sample one\ndenoising timestep t\u223c[0, 1000], add noise to the images ac-\ncording to Eq. 2, and compute the loss according to Eq. 3. In\nthe projection layers, we skip the last image when building\nthe voxel grid, which enforces to learn a 3D representation\nthat can be rendered from novel views. We train our method\nby varying between unconditional and image-conditional\ngeneration (Sec. 3.3). Concretely, with probabilities p1=0.25\nand p2=0.25 we provide the first and/or second image as\ninput and set the respective timestep to zero. Similar to Ruiz\net al. [34], we create a prior dataset with the pretrained text-\nto-image model and use it during training to maintain the 2D\nTable 1. Quantitative comparison of unconditional image gener-\nation. We report average FID [12] and KID [2] per category and\nimprove by a significant margin. This signals that our images are\nmore similar to the distribution of real images in the dataset. We\nmask away the background for our method and the real images to\nensure comparability of numbers with the baselines.\nCategory\nHF [18]\nVD [42]\nOurs\nFID\u2193 KID\u2193\nFID\u2193 KID\u2193 FID\u2193 KID\u2193\nTeddybear\n81.93 0.072 201.71 0.169 49.39 0.036\nHydrant\n61.19 0.042 138.45 0.118 46.45 0.033\nDonut\n105.97 0.091 199.14 0.136 68.86 0.054\nApple\n62.19 0.056 183.67 0.149 56.85 0.043\nprior (see supplemental material for details).\nWe fine-tune the model on 2\u00d7 A100 GPUs for 60K it-\nerations (7 days) with a total batch size of 64. We set the\nlearning rate for the volume renderer to 0.005 and for all\nother layers to 5\u00d710\u22125, and use the AdamW optimizer [35].\nDuring inference, we can increase N and generate up to 30\nimages/batch on an RTX 3090 GPU. We use the UniPC [61]\nsampler with 10 denoising steps, which takes 15 seconds.\n4. Results\nBaselines.\nWe compare against recent state-of-the-art\nworks for 3D generative modeling. Our goal is to create\nmulti-view consistent images from real-world, realistic ob-\njects with authentic surroundings. Therefore, we consider\nmethods that are trained on real-world datasets and select\nHoloFusion (HF) [18], ViewsetDiffusion (VD) [42], and\nDFM [46]. We show results on two tasks: unconditional gen-\neration (Sec. 4.1) and single-image reconstruction (Sec. 4.2).\nMetrics.\nWe report FID [12] and KID [2] as common met-\nrics for 2D/3D generation and measure the multi-view con-\nsistency of generated images with peak signal-to-noise ratio\n(PSNR), structural similarity index (SSIM), and LPIPS [60].\nTo ensure comparability, we evaluate all metrics on images\nwithout backgrounds, as not every baseline models them.\n4.1. Unconditional Generation\nOur method can be used to generate 3D-consistent views of\nan object from any pose with only text as input by using our\nautoregressive generation (Sec. 3.3). Concretely, we sample\nan (unobserved) image caption from the test set for the first\nbatch and generate N=10 images with a guidance scale [13]\nof \u03bbcfg=7.5. We then set \u03bbcfg=0 for subsequent batches, and\ncreate a total of 100 images per object.\nWe evaluate against HoloFusion (HF) [18] and Viewset-\nDiffusion (VD) [42]. We report quantitative results in Tab. 1\nand qualitative results in Figs. 4 and 5. HF [18] creates di-\nverse images that sometimes show view-dependent floating\n5\nHoloFusion (HF) [18]\nViewsetDiffusion (VD) [42]\nOurs\nTeddybear\nHydrant\nApple\nDonut\nFigure 4. Unconditional image generation of our method and baselines. We show renderings from different viewpoints for multiple\nobjects and categories. Our method produces consistent objects and backgrounds. Our textures are sharper in comparison to baselines. Please\nsee the supplemental material for more examples and animations.\nartifacts (see Fig. 5). VD [42] creates consistent but blurry\nimages. In contrast, our method produces images with back-\ngrounds and higher-resolution object details. Please see the\nsuppl. material for more examples and animated results.\n4.2. Single-Image Reconstruction\nOur method can be conditioned on multiple images in or-\nder to render any novel view in an autoregressive fashion\n(Sec. 3.3). To measure the 3D-consistency of our generated\nimages, we compare single-image reconstruction against\nViewsetDiffusion (VD) [42] and DFM [46]. Concretely, we\nsample one image from the dataset and generate 20 images\nat novel views also sampled from the dataset. We follow\nSzymanowicz et al. [42] and report the per-view maximum\nPSNR/SSIM and average LPIPS across multiple objects and\nviewpoints for all methods. We report quantitative results in\n6\n0\u00b0\n340\u00b0\nHF [18]\nVD [42]\nOurs\n(no proj)\nOurs\n(no cfa)\nOurs\nFigure 5. Multi-view consistency of unconditional image generation. HoloFusion (HF) [18] has view-dependent floating artifacts (the\nbase in first row). ViewsetDiffusion (VD) [42] has blurrier renderings (second row). Without the projection layer, our method has no precise\ncontrol over viewpoints (third row). Without cross-frame-attention, our method suffers from identity changes of the object (fourth row). Our\nfull method produces detailed images that are 3D-consistent (fifth row).\nTable 2. Quantitative comparison of single-image reconstruction. Given a single image as input, we measure the quality of novel views\nthrough average PSNR, SSIM, and LPIPS [60] per category. We mask away the generated backgrounds to ensure comparability across all\nmethods. We improve over VD [42] while being on-par with DFM [46].\nMethod\nTeddybear\nHydrant\nDonut\nApple\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nVD [42]\n19.68\n0.70\n0.30\n22.36\n0.80\n0.19\n18.27\n0.68\n0.14\n19.54\n0.64\n0.31\nDFM [46]\n21.81\n0.82\n0.16\n22.67\n0.83\n0.12\n23.91\n0.86\n0.10\n25.79\n0.91\n0.07\nOurs\n21.98\n0.84\n0.13\n22.49\n0.85\n0.11\n21.50\n0.85\n0.18\n25.94\n0.91\n0.11\nTab. 2 and show qualitative results in Fig. 6. VD [42] creates\nplausible results without backgrounds. DFM [46] creates\nconsistent results with backgrounds at a lower image reso-\nlution (128\u00d7128). Our method produces higher resolution\nimages with similar reconstruction results and backgrounds\nas DFM [46]. Please see the supplemental material for more\nexamples and animated results.\n4.3. Ablations\nThe key ingredients of our method are the cross-frame-\nattention and projection layers that we add to the U-Net\n(Sec. 3.2). We highlight their importance in Tab. 3 and Fig. 5.\nHow important are the projection layers?\nThey are nec-\nessary to allow precise control over the image viewpoints\n(e.g., Fig. 5 row 3 does not follow the specified rotation).\nOur goal is to generate a consistent set of images from any\nviewpoint directly with our model (Sec. 3.3). Being able\nto control the pose of the object is therefore an essential\npart of our contribution. The projection layers build up a\n3D representation of the object that is explicitly rendered\ninto 3D-consistent features through volume rendering. This\nallows us to achieve viewpoint consistency, as also demon-\nstrated through single-image reconstruction (Tab. 3).\n7\nInput\nVD [42]\nDFM [46]\nOurs\nReal Image\nFigure 6. Single-image reconstruction of our method and base-\nlines. Given one image/pose as input, our method produces plausi-\nble novel views that are consistent with the real shape and texture.\nWe can also produce detailed backgrounds that match the input.\nsample 1\nsample 2\n[C] wearing a green hat\n[C] sits on a green blanket\nsample 1\nsample 2\nrusty [C] in the grass\nyellow and white [C]\nFigure 7. Diversity of generated results. We condition our method\non text input, which allows to create objects in a desired style.\nWe show samples for hand-crafted text descriptions that combine\nattributes (e.g., color, shape, background) in a novel way. Each\nrow shows a different generation proposal from our method and we\ndenote the object category (Teddybear, Hydrant) as [C]. This\nshowcases the diversity of generated results, i.e., multiple different\nobjects are generated for the same description.\nHow important are cross-frame-attention layers?\nThey\nare necessary to create images of the same object. Without\nthem, the teddybear in Fig. 5 (row 4) has the same general\ncolor scheme and follows the specified poses. However, dif-\nferences in shape and texture lead to an inconsistent set of\nimages. We reason that the cross-frame-attention layers are\nTable 3. Quantitative comparison of our method and ablations.\nWe report average PSNR, SSIM, LPIPS [60], FID [12], and KID\n[2] over the Teddybear and Hydrant categories. We compare\nagainst dropping the projection layer (\u201cOurs no proj\u201d) and the cross-\nframe-attention layer (\u201cOurs no cfa\u201d) from the U-Net (see Sec. 3.2).\nWhile still producing high-quality images with similar FID/KID\nscores, this demonstrates that our proposed layers are necessary to\nobtain 3D-consistent images.\nMethod\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 FID\u2193 KID\u2193\nOurs (no proj)\n16.55\n0.71\n0.29\n47.95 0.034\nOurs (no cfa)\n18.15\n0.76\n0.25\n47.93 0.034\nOurs\n22.24\n0.84\n0.11\n47.92 0.034\nessential for defining a consistent object identity.\nDoes the 2D prior help?\nWe utilize a 2D prior in form\nof the pretrained text-to-image model that we fine-tune in a\n3D-consistent fashion (Sec. 3.1). This enables our method to\nproduce sharp and detailed images of objects from different\nviewpoints. Also, we train our method on captioned images\nto retain the controllable generation through text descriptions\n(Sec. 3.4). We show the diversity and controllability of our\ngenerations in Fig. 7 with hand-crafted text prompts. This\nhighlights that, after finetuning, our model is still faithful to\ntext input, and can combine attributes in a novel way, i.e.,\nour model learns to extrapolate from the training set.\n4.4. Limitations\nOur method generates 3D-consistent, high-quality images of\ndiverse objects according to text descriptions or input images.\nNevertheless, there are several limitations. First, our method\nsometimes produces images with slight inconsistency, as\nshown in the supplement. Since the model is fine-tuned on a\nreal-world dataset consisting of view-dependent effects (e.g.,\nexposure changes), our framework learns to generate such\nvariations across different viewpoints. A potential solution\nis to add lighting condition through a ControlNet [59]. Sec-\nond, our work focuses on objects, but similarly scene-scale\ngeneration on large datasets [6, 57] can be explored.\n5. Conclusion\nWe presented ViewDiff, a method that, given text or image\ninput, generates 3D-consistent images of real-world objects\nthat are placed in authentic surroundings. Our method lever-\nages the expressivity of large 2D text-to-image models and\nfine-tunes this 2D prior on real-world 3D datasets to pro-\nduce diverse multi-view images in a joint denoising process.\nThe core insight of our work are two novel layers, namely\ncross-frame-attention and the projection layer (Sec. 3.2).\nOur autoregressive generation scheme (Sec. 3.3) allows to\ndirectly render high-quality and realistic novel views of a\ngenerated 3D object.\n8\n6. Acknowledgements\nThis work was done during Lukas\u2019 internship at Meta\nReality Labs Zurich as well as at TU Munich, funded by\na Meta sponsored research agreement. Matthias Nie\u00dfner\nwas also supported by the ERC Starting Grant Scan2CAD\n(804724). We also thank Angela Dai for the video voice-over.\nReferences\n[1] Titas Anciukevi\u02c7cius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-\nderdiffusion: Image diffusion for 3D reconstruction, inpaint-\ning and generation. In CVPR, 2023. 2\n[2] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and\nArthur Gretton. Demystifying MMD GANs. In ICLR, 2018.\n5, 8, 12\n[3] Alja\u02c7z Bo\u02c7zi\u02c7c, Pablo Palafox, Justus Thies, Angela Dai, and\nMatthias Nie\u00dfner. TransformerFusion: Monocular RGB scene\nreconstruction using transformers. In NeurIPS, 2021. 4\n[4] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala,\nShalini De Mello, Tero Karras, and Gordon Wetzstein. Gen-\nerative novel view synthesis with 3D-aware diffusion models.\narXiv:2304.02602, 2023. 2\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3D: Disentangling geometry and appearance for high-\nquality text-to-3D content creation. In ICCV, 2023. 1, 2\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-\nannotated 3D reconstructions of indoor scenes. In CVPR,\n2017. 8\n[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,\nAniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3D objects. In CVPR, 2023. 2, 15\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. In NeurIPS, 2021. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16\u00d716 words: Transformers for image recognition at\nscale. In ICLR, 2021. 4\n[10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Edit-\ning 3D scenes with instructions. In ICCV, 2023. 2\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n4, 13, 15\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. GANs trained by a two\ntime-scale update rule converge to a local Nash equilibrium.\nIn NeurIPS, 2017. 5, 8, 12\n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS Workshops, 2021. 2, 5, 12, 15\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2, 3\n[15] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner. Text2Room: Extracting textured 3D\nmeshes from 2D text-to-image models. In ICCV, 2023. 2\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA:\nLow-rank adaptation of large language models. In ICLR,\n2022. 4\n[17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hos-\nseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot\nlearning with retrieval augmented language models. JMLR,\n24(251):1\u201343, 2023. 4\n[18] Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, and\nDavid Novotny. HoloFusion: Towards photo-realistic 3D\ngenerative modeling. In ICCV, 2023. 2, 5, 6, 7, 12, 15\n[19] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J. Mitra. HoloDiffusion: Training a 3D diffusion model\nusing 2D images. In CVPR, 2023. 2, 12\n[20] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In European Conference on Computer\nVision, 2018. 15\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-\n2: Bootstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv:2301.12597,\n2023. 5, 12\n[22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-\n3D content creation. In CVPR, 2023. 2\n[23] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexi-\nang Xu, Hao Su, et al.\nOne-2-3-45: Any single image\nto 3D mesh in 45 seconds without per-shape optimization.\narXiv:2306.16928, 2023. 2\n[24] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n9298\u20139309, 2023. 2, 4, 15\n[25] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gener-\nating multiview-consistent images from a single-view image.\nIn The Twelfth International Conference on Learning Repre-\nsentations, 2024. 2, 3, 15\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. Communications of the ACM, 65(1):99\u2013106, 2021. 2, 4,\n15, 20, 21, 22\n[27] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multiresolu-\ntion hash encoding. ACM Transactions on Graphics (ToG),\n41(4), 2022. 20, 21, 22\n[28] Norman M\u00a8uller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota\nBul`o, Peter Kontschieder, and Matthias Nie\u00dfner. DiffRF:\n9\nRendering-guided 3D radiance field diffusion.\nIn CVPR,\n2023. 2\n[29] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamFusion: Text-to-3D using 2D diffusion. In ICLR, 2023.\n1, 2, 15\n[30] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-\naksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,\nPeter Wonka, Sergey Tulyakov, et al. Magic123: One image\nto high-quality 3D object generation using both 2D and 3D\ndiffusion priors. arXiv:2306.17843, 2023. 2\n[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with CLIP latents. arXiv:2204.06125, 2022. 1, 2,\n3\n[32] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan,\nBen Mildenhall, Andreas Geiger, Jon Barron, and Peter Hed-\nman. MERF: Memory-efficient radiance fields for real-time\nview synthesis in unbounded scenes. ACM Transactions on\nGraphics (TOG), 42(4):1\u201312, 2023. 4, 15\n[33] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3D: Large-scale learning and evaluation of\nreal-life 3D category reconstruction. In ICCV, 2021. 2, 3, 5,\n12\n[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In CVPR, 2023. 1, 5, 12\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In NeurIPS, 2022. 1, 2, 3, 5\n[36] Nikita Selin. CarveKit. github.com/OPHoperHPO/image-\nbackground-remove-tool, 2023. 13, 20, 22\n[37] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung\nLee, and Seungryong Kim.\nLet 2D diffusion model\nknow 3D-consistency for robust text-to-3D generation.\narXiv:2303.07937, 2023. 2\n[38] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. MVDream: Multi-view diffusion for 3D\ngeneration. arXiv:2308.16512, 2023. 2\n[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 3\n[40] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 15\n[41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and\nHujun Bao. NeuralRecon: Real-time coherent 3D reconstruc-\ntion from monocular video. In CVPR, 2021. 4\n[42] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. ViewSet diffusion: (0-)image-conditioned 3D gener-\native models from 2D data. In ICCV, 2023. 2, 5, 6, 7, 8, 12,\n13, 15\n[43] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, David McAllister, and\nAngjoo Kanazawa. Nerfstudio: A modular framework for\nneural radiance field development. In SIGGRAPH, 2023. 20\n[44] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D\ncreation from a single image with diffusion prior. In ICCV,\n2023. 2\n[45] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. MVDiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\nIn NeurIPS, 2023. 2\n[46] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B. Tenenbaum, Fr\u00b4edo Durand, William T.\nFreeman, and Vincent Sitzmann. Diffusion with forward\nmodels: Solving stochastic inverse problems without direct\nsupervision. In NeurIPS, 2023. 2, 5, 6, 7, 8, 13, 15\n[47] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. TextMesh: Gener-\nation of realistic 3D meshes from text prompts. In 3DV, 2024.\n2\n[48] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-\nBin Huang, and Johannes Kopf. Consistent view synthesis\nwith pose-guided diffusion models. In CVPR, 2023. 2\n[49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score Jacobian chaining: Lifting\npretrained 2D diffusion models for 3D generation. In CVPR,\n2023. 2\n[50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nIn NeurIPS, 2021. 2, 20, 22\n[51] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-\nvasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-\nBrualla, Noah Snavely, and Thomas Funkhouser. IBRNet:\nLearning multi-view image-based rendering. In CVPR, 2021.\n4, 14\n[52] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and\ndiverse text-to-3D generation with variational score distilla-\ntion. arXiv:2305.16213, 2023. 1, 2, 15\n[53] Daniel Watson, William Chan, Ricardo Martin Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models. In ICLR, 2023.\n2\n[54] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of im-\nage diffusion models for text-to-video generation. In ICCV,\n2023. 4\n[55] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong.\n3D-aware image generation using 2D diffusion models. In\nICCV, 2023. 2\n[56] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy.\nRerender a video: Zero-shot text-guided video-to-video trans-\nlation. In SIGGRAPH Asia, 2023. 4\n10\n[57] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and\nAngela Dai. ScanNet++: A high-fidelity dataset of 3D indoor\nscenes. In ICCV, 2023. 8\n[58] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng, Apra-\ntim Bhattacharyya, Michael Niemeyer, Siyu Tang, Torsten\nSattler, and Andreas Geiger. SDFStudio: A unified framework\nfor surface reconstruction, 2023. 20\n[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV, 2023. 1, 2, 8, 13\n[60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 5, 7, 8, 13\n[61] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu. UniPC: A unified predictor-corrector framework\nfor fast sampling of diffusion models. In NeurIPS, 2023. 5\n[62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view syn-\nthesis using multiplane images. ACM Transactions on Graph-\nics, 37(4):65:1\u201312, 2018. 2\n[63] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Dis-\ntilling view-conditioned diffusion for 3D reconstruction. In\nCVPR, 2023. 2\n[64] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D\nwith advanced diffusion guidance. arXiv:2305.18766, 2023.\n2\n11\nViewDiff: 3D-Consistent Image Generation with Text-to-Image Models\nSupplementary Material\nA. Supplemental Video\nPlease watch our attached video 1 for a comprehensive evalu-\nation of the proposed method. We include rendered videos of\nmultiple generated objects from novel trajectories at differ-\nent camera elevations (showcasing unconditional generation\nas in Fig. 4). We also show animated results for single-image\nreconstruction (Fig. 6) and sample diversity (Fig. 7).\nB. Training Details\nB.1. Data Preprocessing\nWe train our method on the large-scale CO3Dv2 [33] dataset,\nwhich consists of posed multi-view images of real-world\nobjects. Concretely, we choose the categories Teddybear,\nHydrant, Apple, and Donut. Per category, we train on\n500\u20131000 objects, with 200 images at resolution 256\u00d7256\nfor each object. We generate text captions with the BLIP-2\nmodel [21] and sample one of 5 proposals per object during\neach training iteration. With probability p1=0.5 we select\nthe training images randomly per object and with probability\np2=0.5 we select consecutive images from the captured\ntrajectory that goes around the object. We randomly crop\nthe images to a resolution of 256\u00d7256 and normalize the\ncamera poses such that the captured object lies in an axis-\naligned unit cube. Specifically, we follow Szymanowicz et al.\n[42] and calculate a rotation transformation such that all\ncameras align on an axis-aligned plane. Then, we translate\nand scale the camera positions, such that their bounding box\nis contained in the unit cube.\nB.2. Prior Preservation Loss\nInspired by Ruiz et al. [34], we create a prior preservation\ndataset of 300 images and random poses per category with\nthe pretrained text-to-image model. We use it during training\nto maintain the image generation prior. This has been shown\nto be successful when fine-tuning a large 2D diffusion model\non smaller-scale data [34]. For each of the 300 images we\nrandomly sample a text description from the training set\nof CO3Dv2 [33]. We then generate an image with the pre-\ntrained text-to-image model given that text description as\ninput. During each training iteration we first calculate the\ndiffusion objective (Eq. 3) on the N=5 multi-view images\nsampled from the dataset and obtain Ld. Then, we sample\none image of the prior preservation dataset and apply noise\nto it (Eq. 2). Additionally, we sample a camera (pose and\nintrinsics) that lies within the distribution of cameras for\neach object category. We then similarly calculate the loss\n1https://youtu.be/SdjoCqHzMMk\n(Eq. 3) on the prediction of our model and obtain Lp. Since\nwe only sample a single image instead of multiple, this does\nnot train the diffusion model on 3D-consistency. Instead,\nit trains the model to maintain its image generation prior.\nConcretely, the cross-frame-attention layers are treated again\nas self-attention layers and the projection layers perform\nunprojection and rendering normally, but only from a single\nimage as input. In practice, we scale the prior preservation\nloss with factor 0.1 and add it to the dataset loss to obtain\nthe final loss: L=Ld + 0.1Lp.\nC. Evaluation Details\nC.1. Autoregressive Generation\nWe showcase unconditional generation of our method in\nSec. 4.1. To obtain these results, we employ our autoregres-\nsive generation scheme (Sec. 3.3). Concretely, we sample\nan (unobserved) image caption from the test set for the first\nbatch and generate N=10 images with a guidance scale [13]\nof \u03bbcfg=7.5. Then we set \u03bbcfg=0 for subsequent batches\nand create a total of 100 images per object. We found that\nthe results are most consistent, if the first batch generates\nN images in a 360\u00b0 rotation around the object. This way,\nwe globally define the object shape and texture in a single\ndenoising forward pass. All subsequent batches are condi-\ntioned on all N images of the first batch. To render a smooth\ntrajectory, we sample the camera poses in other batches in a\nsequence. That is, the next N images are close to each other\nwith only a small rotation between them. We visualize this\nprinciple in our supplemental video.\nC.2. Metric Computation\nWe give additional details on how we computed the metrics\nas shown in Tabs. 1 to 3. To ensure comparability, we evalu-\nate all metrics on images without backgrounds as not every\nbaseline models them.\nFID/KID.\nWe report FID [12] and KID [2] as common\nmetrics for 2D/3D generation. We calculate these metrics\nto compare unconditional image generation against Holo-\nFusion [18] and ViewsetDiffusion [42]. This quantifies the\nsimilarity of the generated images to the dataset and thereby\nprovides insight about their quality (e.g., texture details and\nsharpness) and diversity (e.g., different shapes and colors).\nFollowing the baselines [18, 19], we sample 20,000 images\nfrom the CO3Dv2 [33] dataset for each object category. We\nremove the background from each object by using the fore-\nground mask probabilities contained in the dataset. Similarly,\nwe generate 20,000 images with each method and remove the\n12\ndifferent sharpness\ndifferent lighting\nFigure 8. Limitations. Our method generates consistent images at\ndifferent camera poses. However, there can be slight inconsistencies\nlike different sharpness and lighting between images. Since our\nmodel is fine-tuned on a real-world dataset consisting of view-\ndependent effects (e.g., exposure changes), our framework learns\nto generate such variations across different viewpoints.\nbackground from our generated images with CarveKit [36].\nFor our method, we set the text prompt to an empty string\nduring the generation to facilitate complete unconditional\ngeneration.\nPSNR/SSIM/LPIPS.\nWe measure the multi-view consis-\ntency of generated images with peak signal-to-noise ratio\n(PSNR), structural similarity index (SSIM), and LPIPS [60].\nWe calculate these metrics to compare single-image recon-\nstruction against ViewsetDiffusion [42] and DFM [46]. We\nresize all images to the resolution 256\u00d7256 to obtain com-\nparable numbers. First, we obtain all objects that were not\nused during training for every method (hereby obtaining a\nunified test set across all methods). Then, we randomly sam-\nple 20 posed image pairs from each object. We use the first\nimage/pose as input and predict the novel view at the second\npose. We then calculate the metrics as the similarity of the\nprediction to the ground-truth second image. We remove the\nbackground from the prediction and ground-truth images by\nobtaining the foreground mask with CarveKit [36] from the\nprediction image. We use the same mask to remove back-\nground from both images. This way, we calculate the metrics\nonly on similarly masked images. If the method puts the\npredicted object at a wrong position, we would thus quantify\nthis as a penalty by comparing the segmented object to the\nbackground of the ground-truth image at that location.\nD. Limitations\nOur method generates 3D-consistent, high-quality images of\ndiverse objects according to text descriptions or input images.\nNevertheless, there are several limitations. Our method some-\ntimes produces images with slight inconsistency, as shown\nin Fig. 8. Since the model is fine-tuned on a real-world\ndataset consisting of view-dependent effects (e.g., exposure\nConv_3x3 (C, C)\nReLU\nConv_3x3 (C, C\u2019)\nReLU\nConv_1x1 (C\u2019, C\u2019)\nin_features (NxCxI)\nout_features (NxC\u2019xI)\nConv_1x1 (C\u2019, C\u2019)\nReLU\nConv_1x1 (C\u2019, C)\nReLU\nConv_1x1 (C, C)\nin_features (NxC\u2019xI)\nout_features (NxCxI)\nCompressNet\nScaleNet &\nExpandNet\nFigure 9. Architecture of projection layer components. The\nprojection layer contains the components CompressNet, ScaleNet\n(green), and ExpandNet. We implement these networks as small\nCNN networks.\nGroupNorm (16, C\u2019)\nSiLU\nConv_3x3 (C\u2019, C\u2019)\nLinear (T, C\u2019)\nin_features (C\u2019xG)\ntime_embedding (T)\nGroupNorm (16, C\u2019)\nSiLU\nConv_3x3 (C\u2019, C\u2019)\nout_features (C\u2019xG)\nResNet block of the 3D CNN\nFigure 10. Architecture of projection layer components. The\nprojection layer contains the component 3D CNN. We implement\nthis networks as a series of 5 3D ResNet [11] blocks with timestep\nembeddings.\nchanges), our framework learns to generate such variations\nacross different viewpoints. This can lead to flickering arti-\nfacts when rendering a smooth video from a generated set\nof images (e.g., see the supplemental video). A potential\nsolution is to (i) filter blurry frames from the dataset, and (ii)\nadd lighting-condition through a ControlNet [59].\nE. Projection Layer Architecture\nWe add a projection layer into the U-Net architecture of pre-\ntrained text-to-image models (see Fig. 3 and Sec. 3.2). The\nidea of this layer is to create 3D-consistent features that are\n13\nvoxel features \n(NxC\u2019xG)\nray dir (Nx3xG),\ndepth (Nx1xG)\nConv_3x3 (4, C\u2019 / 2)\nELU\nConv_3x3 (C\u2019 / 2, C\u2019)\nELU\ntime embedding \n(NxT)\nLinear (T, T / 2)\nLinear (T / 2, C\u2019)\nELU\n(NxC\u2019xG)\nmean (C\u2019xG)\nvar (C\u2019xG)\nC\n(Nx3C\u2019xG)\nMLP\n(3C\u2019, 2C\u2019)\n(2C\u2019, C\u2019)\n(NxC\u2019xG)\nMLP\n(C\u2019, C\u2019)\n(C\u2019, 1)\nSigmoid()\nMLP\n(C\u2019, C\u2019)\n(C\u2019, C\u2019)\nMLP\n(C\u2019, C\u2019)\n(C\u2019, 1)\nSigmoid()\n(NxC\u2019xG)\n(Nx1xG)\n(Nx1xG)\nmean (C\u2019xG)\nvar (C\u2019xG)\nweighted pooling\nConv_3x3 (2C\u2019+1,C\u2019)\nELU\naggregated voxel \nfeatures (C\u2019xG)\nELU\nC\nconcatenation\nelement-wise addition\nelement-wise multiplication\nmean (1xG)\nAggregator MLP\nFigure 11. Architecture of projection layer components. The projection layer contains the component Aggregator MLP. First, we combine\nper-view voxel grids with their ray-direction/depth encodings (blue) and the temporal embedding (green). Inspired by IBRNet [51], the\nMLPs (pink) then predict per-view weights followed by a weighted feature average. Finally, we combine the per-voxel weights with the\nmean and variance grids (yellow) to obtain the aggregated feature grid.\nthen further processed by the next U-Net layers (e.g. ResNet\nblocks). Concretely, we create a 3D representation from all\ninput features in form of a voxel grid, that is defined inside\nof the axis-aligned unit cube. We set the 3D feature dimen-\nsion as C\u2032 = 16 and define the base resolution of the voxel\ngrid as 128\u00d7128\u00d7128. Throughout the U-Net, we apply\nthe same up/downsampling as for the 2D features, i.e., the\nresolution decreases to 8\u00d78\u00d78 in the bottleneck layer. The\nprojection layer consists of multiple network components.\nWe show detailed network architectures of these components\nin Figs. 9 to 11.\nE.1. CompressNet and ExpandNet\nWe apply the 3D layers on features that are defined in a\nunified dimensionality of C\u2032=16. Since our 3D layers act\non dense voxel grids this helps to lower the memory require-\nments. To convert to/from this compressed feature space, we\nemploy small CNNs, as depicted in Fig. 9. In these schemat-\nics, we define N as the number of images in a batch, C as\nthe uncompressed feature dimension and I as the spatial\ndimension of the features.\nE.2. Aggregator MLP\nAfter creating per-view voxel grids via raymarching (see\nSec. 3.2), we combine N voxel grids into one voxel grid that\nrepresents the features for all viewpoints. To this end, we\nemploy a series of networks, as depicted in Fig. 11. In these\nschematics, we define N as the number of images in a batch,\nC\u2032 as the compressed feature dimension, T as the dimension\nof the timestep embedding, G as the 3D voxel grid resolution,\nand I as the spatial dimension of the features. The MLPs are\ndefined as a sequence of linear layers of specified input and\noutput dimensionality with ELU activations in between.\nFirst, we concatenate the voxel features with an encod-\ning of the ray-direction and depth that was used to project\nthe image features into each voxel. We also concatenate the\ntimestep embedding to each voxel. This allows to combine\nper-view voxel grids of different timesteps (e.g., as proposed\nin image conditional generation in Sec. 3.3). It is also use-\nful to inform the subsequent networks about the denoising\ntimestep, which allows to perform the aggregation differ-\nently throughout the denoising process. Inspired by IBRNet\n[51], a set of MLPs then predict per-view weights followed\nby a weighted feature average. We perform this averaging\noperation elementwise: since all voxel grids are defined in\n14\nthe same unit cube, we can combine the same voxel across\nall views. Finally, we combine the per-voxel weights with\nthe mean and variance grids to obtain the final aggregated\nfeature grid.\nE.3. 3D CNN\nAfter aggregating the per-view voxel grids into a joint grid,\nwe further refine that grid. The goal of this network is to add\nadditional details to the feature representation such as the\nglobal orientation of the shape. To achieve this, we employ a\nseries of 5 3D ResNet [11] blocks with timestep embeddings,\nas depicted in Fig. 10. In these schematics, we define C\u2032 as\nthe compressed feature dimension, T as the dimension of the\ntimestep embedding, and G as the 3D voxel grid resolution.\nE.4. Volume Renderer and ScaleNet\nAfter we obtain a refined 3D feature representation in form\nof the voxel grid, we render that grid back into per-view\nimage features (see Fig. 3). Concretely, we employ a vol-\nume renderer similar to NeRF [26] and implement it as a\ngrid-based renderer similar to DVGO [40]. This allows to\nrender features in an efficient way that is not a bottleneck\nfor the forward pass of the network. In contrast to NeRF,\nwe render down features instead of rgb colors. Concretely,\nwe sample 128 points along a ray and for each point we\ntrilinearly interpolate the voxel grid features to obtain a fea-\nture vector f \u2208 RC\u2032. Then, we employ a small 3-layer MLP\nthat transforms f into the density d \u2208 R and a sampled\nfeature s \u2208 RC\u2032. Using alpha-compositing, we accumulate\nall pairs (d0, s0), ..., (d127, s127) along a ray into a final ren-\ndered feature r \u2208 RC\u2032. We dedicate half of the voxel grid to\nforeground and half to background and apply the background\nmodel from MERF [32] during ray-marching.\nWe found it is necessary to add a scale function after the\nvolume rendering output. The volume renderer typically uses\na sigmoid activation function as the final layer during ray-\nmarching [26]. However, the input features are defined in\nan arbitrary floating-point range. To convert r back into the\nsame range, we non-linearly scale the features with 1\u00d71 con-\nvolutions and ReLU activations. We depict the architecture\nof this ScaleNet as the green layers in Fig. 9.\nF. Additional Results\nF.1. Comparison To Additional Baselines\nWe compare against additional text-to-3D baselines that\nalso utilize a pretrained text-to-image model in Fig. 12. We\nchoose ProlificDreamer [52] as representative of score distil-\nlation [29] methods. Rendered images are less photorealistic\nsince the optimization may create noisy surroundings and\nover-saturated textures. Similar to us, Zero123-XL [24] and\nSyncDreamer [25] circumvent this problem by generating\n3D-consistent images directly. However, they finetune on a\nInput Image\nZero123-XL [24] SyncDreamer [25]\nOurs\nteddy sitting on a wooden box\ndonut on top of a white plate\nProlificDreamer [52]\nOurs\nProlificDreamer [52]\nOurs\nFigure 12. Comparison to other text-to-3D baselines from image-\n(top) and text-input (bottom). Our method produces images with\nhigher photorealism and authentic surroundings.\nTable 4. Comparison of consistency (mid) and photorealism\n(FID). Our method shows similar 3D-consistency as baselines,\nwhile producing more photorealistic images.\nMethod\nEwarp \u2193 #Points\u2191 PSNR\u2191\nFID\u2193\nDFM [46]\n0.0034\n17,470\n32.32\n\u2014\nVD [42]\n0.0021\n\u2014\n\u2014\n\u2014\nHF [18]\n0.0031\n\u2014\n\u2014\n\u2014\nSyncDreamer [25]\n0.0042\n4,126\n33.81\n135.78\nZero123-XL (SDS) [24] 0.0039\n\u2014\n\u2014\n126.83\nOurs\n0.0036\n18,358\n33.65\n85.08\nlarge synthetic dataset [7] instead of real-world images. As\na result, their images have synthetic textures and lighting\neffects and no backgrounds. We quantify this in Tab. 4 with\nthe FID between sets of generated images (conditioned on\nan input view), and real images of the same object (with-\nout backgrounds). Our method has better scores since the\ngenerated images are more photorealistic.\nWe calculate temporal stability (Ewarp) of video render-\nings with optical flow warping following [20]. Also, we mea-\nsure the consistency of generated images for methods that\ndo not directly produce a 3D representation. Concretely, we\nreport the number of point correspondences following [25]\nand the PSNR between NeRF [26] re-renderings and input\nimages. Table 4 shows that our method is on-par with base-\nlines in terms of 3D consistency, while generating higher\nquality images.\nF.2. Unconditional Generation\nWe generate images in a similar fashion as in Sec. 4.1. Con-\ncretely, we sample an (unobserved) image caption from the\ntest set for the first batch and generate N=10 images with\na guidance scale [13] of \u03bbcfg=7.5. Then we set \u03bbcfg=0\nfor subsequent batches and create a total of 100 images per\nobject. We show additional results in Figs. 13 to 16.\n15\na teddy bear sitting on a colorful rug\na stuffed panda bear with a heart on its chest\na stuffed animal sitting on a tile floor\na black and white teddybear with blue feet\na teddy bear laying on a bed\na stuffed animal sitting on a chair\na teddy bear sitting on the ground in the dark\na stuffed bear wearing a red hat and a cloak\nFigure 13. Additional examples of our method. Given a text prompt as input, we generate a smooth trajectory around an object with our\nautoregressive generation scheme (Sec. 3.3). Please see the supplemental video for animations of the generated samples.\n16\na red and white fire hydrant on a brick floor\na yellow and green fire hydrant sitting on the ground\na yellow fire hydrant sitting on the sidewalk\na red fire hydrant in the snow\na fire hydrant on the sidewalk\na red and blue fire hydrant\na blue and white fire hydrant sitting in the grass\na green fire hydrant with a tag on it\nFigure 14. Additional examples of our method. Given a text prompt as input, we generate a smooth trajectory around an object with our\nautoregressive generation scheme (Sec. 3.3). Please see the supplemental video for animations of the generated samples.\n17\na glazed donut sitting on a marble counter\na donut on a clear plate\na chocolate donut with sprinkles on it\na donut on a plate with a hole in it\na large donut on a plate on a table\na yellow plate with a donut on it\na white plate with a donut on it\na donut sitting on a cloth\nFigure 15. Additional examples of our method. Given a text prompt as input, we generate a smooth trajectory around an object with our\nautoregressive generation scheme (Sec. 3.3). Please see the supplemental video for animations of the generated samples.\n18\na red apple on a white counter top\na green apple sitting on a counter\na red and yellow apple\na red and yellow apple\na single apple on a table cloth with a floral pattern\na red and yellow apple on a wooden floor\na red apple sitting on a black leather couch\na red apple on a blue and white patterned table cloth\nFigure 16. Additional examples of our method. Given a text prompt as input, we generate a smooth trajectory around an object with our\nautoregressive generation scheme (Sec. 3.3). Please see the supplemental video for animations of the generated samples.\n19\nG. Optimizing a NeRF/NeuS\nOur method is capable of directly rendering images from\nnovel camera positions in an autoregressive generation\nscheme (see Sec. 3.3). This allows to render smooth trajecto-\nries around the same 3D object at arbitrary camera positions.\nDepending on the use case, it might be desirable to obtain an\nexplicit 3D representation of a generated 3D object (instead\nof using our method to autoregressively render new images).\nWe demonstrate that our generated images can be used di-\nrectly to optimize a NeRF [26] or NeuS [50]. Concretely, we\noptimize a NeRF with the Instant-NGP [27] implementation\nfrom our generated images for 10K iterations (2 minutes).\nAlso, we extract a mesh by optimizing a NeuS with the neus-\nfacto implementation from SDFStudio [43, 58] for 20K it-\nerations (15 minutes). First, we remove the background of\nour generated images by applying Carvekit [36] and then\nstart the optimization with these images. We show results in\nFigs. 17 to 19.\n20\nFigure 17. NeRF [26] optimization from our generated images. Left: given a text prompt as input, we generate a smooth trajectory around\nan object with our autoregressive generation scheme (Sec. 3.3). In total, we generate 100 images at different camera positions. Right: we\ncreate a NeRF using Instant-NGP [27] from the generated images. We show the camera positions of the generated images on top of the\noptimized radiance field.\n21\nFigure 18. NeRF [26] optimization from our generated images. Left: given a text prompt as input, we generate a smooth trajectory around\nan object with our autoregressive generation scheme (Sec. 3.3). In total, we generate 100 images at different camera positions. Right: we\ncreate a NeRF using Instant-NGP [27] from the generated images. We show the camera positions of the generated images on top of the\noptimized radiance field.\nFigure 19. Mesh extraction from our generated images. Given a text prompt as input, we generate a smooth trajectory around an object\nwith our autoregressive generation scheme (Sec. 3.3). In total, we generate 100 images at different camera positions and mask-out the\nbackground with Carvekit [36]. We then optimize a NeuS [50] and extract the mesh from it (last 4 images per row).\n22\n"
  },
  {
    "title": "Twisting Lids Off with Two Hands",
    "link": "https://arxiv.org/pdf/2403.02338.pdf",
    "upvote": "5",
    "text": "Twisting Lids Off with Two Hands\nToru Lin\u2217, Zhao-Heng Yin\u2217, Haozhi Qi, Pieter Abbeel, Jitendra Malik\nUC Berkeley\nhttps://toruowo.github.io/bimanual-twist\nFigure 1.\nWe train two anthropomorphic robot hands to twist (off) lids of various articulated objects. The control policy is first trained\nin simulation with deep reinforcement learning, then zero-shot transferred to a real-world setup. A single policy trained with simplistic,\nsimulated bottle-like objects generalizes to real objects with drastically different physical properties (e.g. shape, size, color, material, mass).\nThe length, diameter (or diagonal length), and mass of each object are annotated at the bottom. See our video for more visualizations.\nAbstract\u2014Manipulating objects with two multi-fingered hands\nhas been a long-standing challenge in robotics, attributed to\nthe contact-rich nature of many manipulation tasks and the\ncomplexity inherent in coordinating a high-dimensional bimanual\nsystem. In this work, we consider the problem of twisting lids of\nvarious bottle-like objects with two hands, and demonstrate that\npolicies trained in simulation using deep reinforcement learning\ncan be effectively transferred to the real world. With novel\nengineering insights into physical modeling, real-time perception,\nand reward design, the policy demonstrates generalization capa-\nbilities across a diverse set of unseen objects, showcasing dynamic\nand dexterous behaviors. Our findings serve as compelling\nevidence that deep reinforcement learning combined with sim-\nto-real transfer remains a promising approach for addressing\nmanipulation problems of unprecedented complexity.\nI. INTRODUCTION\nWe study the task of twisting or removing lids with\ntwo multi-fingered robot hands. This task is both practically\nimportant and profoundly interesting. For one, the ability\nto twist or remove lids from containers is a crucial motor\nskill that toddlers acquire during their early developmental\nstages [36, 54]. For another, the manipulation skills required\n* Equal Contribution.\nfor this task, such as the coordination of fingers to manipulate\na multi-part object, can be generally useful across a large\ncollection of practical tasks.\nOur work demonstrates the feasibility of learning a dex-\nterous and dynamic bimanual manipulation policy purely in\nsimulation and zero-shot transferring it to the real world. By\nabstracting a class of articulated objects into a simplistic two-\npart object model with randomized physical properties, we\nsuccessfully train a policy that can generalize to a diverse set of\nnovel objects with different shapes, sizes, visual appearances,\nand other physical properties (Figure 1). Our method does\nnot require precise modeling of any individual object, or\nhardcoding prior knowledge on object properties. Instead, it\nallows for stable and natural twisting behaviors to emerge\nthrough large-scale reinforcement learning (RL) training.\nIn developing a system with these capabilities, we uncover a\nnumber of novel engineering insights, which we outline below.\nPhysical Modeling. Our work features a novel class of objects\nfor in-hand manipulation: articulated objects defined as two\nrigid bodies connected via a revolute joint with threaded\nstructure. Accurately modeling friction and contact with rev-\nolute joints and threaded structure has traditionally been a\narXiv:2403.02338v1  [cs.RO]  4 Mar 2024\nhard challenge in robotics simulation [33]. To address this,\nwe introduce a brake-based design to model the interaction\nbetween lid and body of bottle-like objects. This design is\nfast to simulate while maintaining a high fidelity to real-\nworld physical dynamics, enabling efficient policy learning\nand successful sim-to-real transfer.\nPerception. We initially hypothesize that a fine-grained,\ncontact-rich manipulation task like lid-twisting must require\nprecise perceptual information on object states. To our sur-\nprise, a two-point sparse object representation, extracted from\noff-the-shelf object segmentation and tracking tools, is suffi-\ncient to solve the perception problem. With simple domain\nrandomization techniques, we train policies that are robust\nagainst occlusion and camera noise. This discovery suggests\nthat a minimal amount of object perception information can\nbe adequate for complicated bimanual manipulation tasks.\nReward Design. Previously performant reward designs for\ntasks like in-hand reorientation cannot be straightforwardly\napplied to our task, since those tasks focus on manipulating\nsingle-part rigid bodies with one hand rather than multi-\npart articulated bodies with two hands. Solving this task is\nmore challenging since it involves more complex and precise\ncontact (e.g. using two hands to hold a lid). In addressing this\nchallenge, we discover a simple keypoint-based contact reward\nthat yields natural lid-twisting behavior on the robot fingers.\nWe conduct several controlled experiments in both sim-\nulation and the real world. Through empirical analysis, we\nverify that our simulation modeling, perception module, and\nreward design can reliably lead to the desired behavior of lid\ntwisting. Our final successful policy manifests natural behavior\nacross test objects with various physical properties such as\nshapes, sizes, and masses in simulation. Moreover, the learned\npolicy can be zero-shot transferred to a wide range of novel\nhousehold objects whose lids can be removed (Figure 1), and\nit is robust against perturbations.\nII. BACKGROUND\nA. Classic Approaches for Bimanual Manipulation\nFor decades, the problem of bimanual manipulation has\nremained a fascinating unsolved challenge in robotics [6, 23,\n35, 48, 49, 53]. While multi-fingered robot hands seem to be a\nnatural choice for bimanual robot systems in theory, designing\ncontrollers for high-dimensional action spaces remains an open\nproblem due to difficulties in mechanical design and reliable\nactuation. Most previous works in bimanual manipulation thus\nuse simple and durable parallel jaw-grippers as end-effectors.\nFor example, Caccavale et al. [5] demonstrate carrying and\nmoving objects using two arms with impedance control. Sarkar\net al. [45] show controlling object states using rolling contact.\nUnlike most previous works, our work focuses on bimanual\nmanipulation with two multi-fingered hands.\nWhile there exist prior works that utilize two multi-fingered\nhand for manipulation, they differ substantially from our work\nin terms of approach and results. Vahrenkamp et al. [53]\npresents a bimanual grasp planner that could be used to build\ngrasps for large objects using multi-fingered hands, but without\ndemonstrating any real-world results. Platt et al. [39] achieve\nobject reconfiguration using two three-fingered robot hands,\nbut do so by planning a sequence of low-level controllers.\nSteffen et al. [50] study a screwing task similar to our task\nof interest but synthesize the control motion sequence with a\nkernel approach applied to human motion data captured with\na data glove. Our work fills a missing space in the bimanual\nmulti-fingered manipulation literature, as we do not rely on\nany planner, accurate object model, or human data. Instead,\nwe directly work on a high-dimensional action space with RL\non randomized environments.\nB. Learning Approaches for Bimanual Manipulation\nIn recent years, bimanual manipulation has been more\nactively studied with learning-based methods, as a result of\nprogress in learning algorithms and compute infrastructure.\nThese learning-based approaches can generally be categorized\ninto two types: 1) learning from real-world data; 2) learning\nin simulation, then transferring to the real world (sim-to-real).\nLearning from Real-World Data. Rapid progress has been\nmade in reinforcement learning (RL) in the real world. Zhang\net al. [58] learn to chain motor primitives for vegetable cutting,\nwith relatively motion primitives; much of the task difficulty\nis bypassed via the use of specialized end-effectors [1, 14].\nChiu et al. [12] learn precise needle manipulation with two\ngrippers by integrating RL with a sampling-based planner.\nWhile impressive, these works cannot easily scale to higher\ndimensional action space due to their sample inefficiency or\nthe need to define heuristic-based action primitives.\nMost\nrecent\nsuccesses\nin\nbimanual\nmanipulation\nare\nachieved by learning from demonstrations [47, 51, 59]. How-\never, successes so far are largely limited to simple end-\neffectors like parallel jaw grippers. One reason is the lack\nof high-quality demonstration data from multi-fingered robot\nhands [24]; the availability of such hand hardware is itself lim-\nited, not to mention the sophisticated data collection infrastruc-\nture. Although several works aiming to improve demonstration\ndata collection with two arms [15, 26, 28] or multi-fingered\nhands exist [2, 3, 16, 42, 43], their latency and retargeting\nerrors limit their practical applicability and scalability. Our\nmethod uses RL in simulation and is thus not limited by the\nhardware and data collection infrastructure problems faced by\nlearning from demonstration approaches.\nSim-to-Real. There has been growing interest in sim-to-real\napproaches for robotics \u2013 i.e. learning policies in simulation\nand transferring them to the real world \u2013 stimulated by\nseveral notable successes in recent years ranging from loco-\nmotion [19, 25, 32] to manipulation [8, 17, 34, 40]. Existing\nworks in manipulation, however, are mostly done with either a\nsingle multi-fingered hand [7, 10, 21, 38, 41, 44, 52, 55, 56], or\ntwo arms with simpler end-effectors [20, 27, 30]. While Chen\net al. [9] and Zakka et al. [57] feature bimanual tasks with\ndexterous hands, only simulation results are shown. Perhaps\nthe work most related to ours is Huang et al. [18], where\nthe authors demonstrate throwing and catching objects using\nBase Link\nLid Link\nBrake Link\nRevolute Joint\nPrismatic Joint\nA\nB\nC\nA-B\nA-C\nFigure 2. Bottle URDF model. A key design in the bottle model is the\nbrake link and the prismatic \u201cbrake\u201d joint. The brake link constantly\npresses against the lid link to simulate the static friction between the\nbase link and the lid link. Without the brake, the lid link can easily\nstart to rotate even when it is not in contact with the fingers.\ntwo dexterous hands. However, our task is significantly more\ncontact-rich and requires substantially more challenging bi-\nmanual coordination to maintain object stability at all times.\nOur work distinguishes itself from the literature by considering\nmanipulation for an extremely contact-rich task using two\nmulti-fingered hands; notably, the objects that we work with\nare two-part articulated bodies, while previous work focuses\non single-part rigid bodies.\nIII. TASK FORMULATION\nTwisting lids of container objects is a complex in-hand ma-\nnipulation process that requires dynamic dexterity of multiple\nfingers and precise coordination between two hands. Below,\nwe define the specific task being considered in this work.\nTask Initialization. Each object of interest consists of two\nrigid, near-cylindrical parts (a \u201cbody\u201d and a \u201clid\u201d); the two\nparts are connected via a continuous revolute joint, allowing\nthem to rotate about each other. To better benchmark the\nbimanual twisting capability, we consider a class of articulated\nbottles with lids that can be twisted infinitely (see Section IV-B\nfor more details). The two robotic hands are initialized in a\nstatic pose with upward-facing palms. At the beginning of\neach episode, a bottle-like object is gently dropped or placed\nonto the fingers. The initial pose of the object is randomized\nboth in translation and rotation to a fixed default pose; the\ninitial joint positions of the hands are randomized about a\npredefined canonical pose by adding Gaussian noise. Note\nthat since we do not assume a stable grasp configuration at\ntask initialization, the control policy needs to learn in-grasp\nreorientation to place the object in a stable location to perform\nsuccessive manipulation.\nTask Objective. The goal of this task is to twist the lid\nabout the object\u2019s axis of rotation in one direction as much\nas possible; during this process, the object should always\nstay in hand. Achieving this involves a sequence of delicate\nmovements: 1) after initialization, the robot hand should firmly\ngrasp and slightly rotate the bottle to a suitable pose; 2) the\nhand that is closer to the object lid should place its finger\nFigure 3. Real-time perception system. Top: scene overview. Bottom:\nwe segment and track object parts from the RGB frames (left), take\nmask centers as object part centers (middle), and estimate 3D object\nkeypoints using noisy depth information from the camera (right).\naround the lid to initiate twisting motion; 3) the two hands\nshould coordinate to avoid dropping the object while one hand\ntwists the lid. Motor skills that arise from this task could serve\nas generic abstractions for skills necessary to manipulate many\nother household objects, especially those with revolute joints\nsuch as Rubik\u2019s cubes, light bulbs, and jars.\nTask Complexity. The complexity inherent in this task can be\nsummarized into the following three aspects.\n1) In-Hand Perception Difficulty. Heavy occlusion can occur\nduring manipulation due to fingers surrounding the object in-\nhand. How to build a real robot system that can perform robust\nbimanual manipulation against such occlusion and perceptual\nnoises remains an open problem.\n2) Fine-grained Contact. This task requires both hands to\ncoordinate to maintain stability. Incorrect contact points or\nforces could result in the bottle dropping.\n3) High-Dimensional Control. Our system has 32 degrees of\nfreedom, and almost all the fingers are engaged throughout\nthe manipulation process. This requires extremely precise\ncoordination among a large number of rigid bodies, which\nis a known challenge for existing learning-based methods.\nIV. SYSTEM SETUP\nTo address the aforementioned challenges in bimanual ma-\nnipulation and our specific task choice, we develop a sim-to-\nreal pipeline based on a deep RL approach. In this section, we\ndescribe our overall system setup.\nA. Real-world System\nHardware Setup. As shown in Figure 1, we use two 16-DoF\nAllegro Hands from Wonik Robotics for our experiments. Each\nAllegro Hand is mounted on a fixed UR5e arm. We employ\na single RealSense D435 depth camera to provide visual\ninformation, from which we extract object state information.\nWe send control commands to the robot at a frequency of\n10 Hz via a Linux workstation.\nPerception. Figure 3 shows an overview of our perception\npipeline. Instead of directly using pixels as the RL agent\u2019s\npolicy input, we propose to extract object keypoints from\nimages for control through object segmentation and tracking.\nSpecifically, we utilize the Segment Anything model [22] to\ngenerate two separate masks for the bottle body and the lid on\nthe first frame, and XMem [11] to track the masks throughout\nall remaining frames. To approximate the 3D center-of-mass\ncoordinates of the bottle body and lid, we calculate the center\nposition of their masks in the image plane, then obtain noisy\ndepth readings from a depth camera to recover a corresponding\n3D position. The perception pipeline runs at 10 Hz to match\nthe neural network policy\u2019s control frequency.\nB. Bottle Models\nSimulated Bottles. A central challenge in simulating the lid-\ntwisting task is how to properly model friction between the\nbottle body and the lid, particularly static friction. Simulating\nthis type of physical force has been a long-standing problem\nin graphics [33]. We design a simple modeling approximation\nthat strikes a balance between fidelity and speed during\nphysical simulation. As a result, a successful policy can be\nlearned in simulation within a reasonable amount of wall-\nclock time while being realistic enough for transfer to the real\nworld. We illustrate our bottle-like object model in Figure 2.\nOur design features a special Brake Link that constantly\npresses against the bottle lid (through a prismatic joint). This\nartificially generates frictional forces between the bottle body\n(Base Link) and the lid (Lid Link), preventing relative rotation\nbetween them\u2014similar to a bottle with its lid screwed on.\nReal-World Articulated Bottles. To easily vary the shapes\nand sizes of our objects and encourage reproducibility, we\ndesign a set of articulated bottle-like objects in CAD software\nand 3D print them in a range of different colors and materials.\nThe lids can be rotated infinitely, simulating an ideal revolute\njoint. We show examples of our custom bottles in Figure 5\n(middle). We also collect household bottle-like objects with\nvarying physical properties to demonstrate the generalizability\nof our method, as depicted in Figure 5 (bottom).\nV. LEARNING TO TWIST LIDS\nBimanual in-hand dexterous manipulation involves highly\ncomplex hand-object contacts, and remains challenging to\nsolve with traditional methods. In this work, we address\nthe control challenge through RL. We formulate our control\nproblem as a partially observable Markov Decision Process\nM = (S, O, A, R, P), where S is the state space, A is the\naction space, O is the observation space, R is the reward\nfunction, and P is the environment dynamics. The control\npolicy generates an action at \u2208 A to maximize the cumulative\nreward given an observation ot \u2208 O.\nA. Observation and Action\nObservation Space. At each time step t, the control policy\nobserves the following information from the environment: the\nproprioceptive hand joint positions qt, the estimated center-of-\nmass 3D positions of the bottle base and lid, and previously\ncommanded target joint positions \u02dcqt.\nFigure 4. Illustration of our reward design. Our task-specific reward\ncontains three terms: finger contact reward (yellow arrows), twisting\nreward (white arrow), and pose reward (blue arrow). In particular,\nour keypoint-based finger reward is crucial for learning desired\nscrewing behavior, which encourages the robot to minimize the\ndistance between fingertips and reference contact keypoints.\nAction Space. We use an impedance PD controller to drive\nthe robot hand. The control policy produces a relative target\njoint position as the action at, which is added to the current\ntarget joint position qt to produce the next target: \u02dcqt+1 =\n\u02dcqt +\u03b7EMA(at). \u03b7 is a scaling factor. Note that we smooth the\naction with its exponential moving average (EMA) to produce\nsmooth motion. The next target position is sent to the PD\ncontroller to generate torque on each joint.\nB. Reward Design\nThe reward function plays a critical role in defining the\nrobot\u2019s behavior. The bimanual in-hand manipulation system\nhas a higher action space dimension and much more complex\ncontact patterns compared to single-hand scenarios. This mag-\nnifies the challenges inherent in the RL exploration process,\nunderlining the need for more sophisticated reward designs.\nWhile one way to approach hard exploration problems is\nto add intrinsic rewards [4, 29], we propose a fine-grained,\nbehavior-aware reward design for this task. Our reward func-\ntion contains the following terms:\nTwisting Reward. We define the twisting reward as\nrtwisting = \u2206\u03b8 = qt+1\nbottle \u2212 qt\nbottle,\n(1)\nwhich is the rotation angle of the lid during one-step execution.\nFinger Contact Reward. We encourage natural object ma-\nnipulation behavior by defining a set of keypoints positioned\non the object to guide the fingers\u2019 interactions (Figure 4). We\ndefine two set of keypoints XL \u2208 Rn\u00d73 and XR \u2208 Rm\u00d73\nattached on the bottle base and lid respectively. Then, we\ndefine the finger contact reward as\nrcontact =\nX\ni\n\u0014\n1\n1 + \u03b1d(XL, FL\ni ) +\n1\n1 + \u03b1d(XR, FR\ni )\n\u0015\n,\n(2)\nwhere FL \u2208 R4\u00d73 and FR \u2208 R4\u00d73 are the position of left\nand right fingertips, \u03b1 is a scaling hyperparameter, and d is a\ndistance function defined as\nd(A, x) = min\ni\n\u2225Ai \u2212 x\u22252.\n(3)\nTherefore, we encourage each fingertip to stay as close to\none of the keypoints as possible. By choosing the keypoint\nproperly around the bottle base and lid, we can elicit natural\ngrasping and lid-twisting behavior.\nPose Reward. We also introduce a pose matching reward\nterm to encourage the bottle main axis xaxis aligned with\na predefined direction v. This term is defined as\nrpose = \u2212 arccos(\u27e8xaxis, v\u27e9).\n(4)\nOther Regularizations. The above three reward terms specify\nthe objective of our task. Besides these three rewards, we\nalso introduce another few regularization terms as in previous\nworks [55], including work penalty and action penalty to\npenalize large, jerky motions:\nraction = \u2212\u2225at\u22252,\n(5)\nrwork = \u2212\u27e8|\u03c4|, | \u02d9qt|\u27e9.\n(6)\nThe reward function is a weighted sum of the above terms:\nr = \u03b11rcontact+\u03b12rtwist+\u03b13rpose+\u03b14raction+\u03b15rwork. (7)\nReset Strategy. Due to the high dimensionality of our learning\nproblem, there exist many possible ways how the robot interact\nwith the object; among these, most modes lead to failures such\nas dropping the object beyond recovery. Exploring these modes\nrarely leads to meaningful learning progress. To circumvent\nthis issue, we introduce several early termination criteria. Most\nimportantly, we reset an episode if the robot hands fail to rotate\nthe bottle into a desired pose for bimanual twisting within a\nshort time limit. Additionally, we also reset when the bottle\u2019s\nz-position is below a certain threshold, as the fingertips of the\ntwo hands can pinch the bottle at a low position without being\nable to reposition it into the palm.\nC. Domain Randomization\nWe apply a wide range of domain randomizations to ensure\nzero-shot sim-to-real transfer, including both physical and non-\nphysical randomizations. Physical randomizations include the\nrandomization of object friction, mass, and scale. We also\napply random forces to the object to simulate the physical\neffects that are not implemented by the simulator. Non-\nphysical randomizations model the noise in observation (e.g.\njoint position measurement and detected object positions)\nand action. A summary of our randomization attributes and\nparameters is shown in Table I.\nD. Training\nWe use the proximal policy optimization (PPO) algorithm\nto learn RL policies. We use an advantage clipping coefficient\n\u03f5 = 0.2; a horizon length of 16, with \u03b3 = 0.99, and general-\nized advantage estimator (GAE) [46] coefficient \u03c4 = 0.95.\nThe policy network is a three-layer MLP with ELU [13]\nTable I. Domain Randomization Setup.\nObject: Mass (kg)\n[0.03, 0.1]\nObject: Friction\n[0.5, 1.5]\nObject: Shape\n\u00d7U(0.95, 1.05)\nObject: Initial Position (cm)\n+U(\u22120.02, 0.02)\nObject: Initial z-orientation\n+U(\u22120.75, 0.75)\nHand: Friction\n[0.5, 1.5]\nPD Controller: P Gain\n\u00d7U(0.8, 1.1)\nPD Controller: D Gain\n\u00d7U(0.7, 1.2)\nRandom Force: Scale\n2.0\nRandom Force: Probability\n0.2\nRandom Force: Decay Coeff. and Interval\n0.99 every 0.1s\nBottle Pos Observation: Noise\n0.02\nJoint Observation Noise.\n+N(0, 0.4)\nAction Noise.\n+N(0, 0.1)\nFrame Lag Probability\n0.1\nAction Lag Probability\n0.1\nactivation, whose hidden layer is [256, 256, 128]. The policy\nnetwork outputs a Gaussian distribution with a learnable state-\nindependent standard deviation. The value network is also an\nMLP with ELU activation, whose hidden layer is [512, 512,\n512]. We use an adaptive learning rate with KL threshold\nof 0.016 [19]. During training, we normalize the state input,\nvalue, and advantage. The gradient norm is set to 1.0 and the\nminibatch size is set to 8192. We use asymmetric observa-\ntion [37] for the policy and value network, adding privileged\ninformation (randomized physical parameters, cube velocity\nand angular velocity, joint velocity and angular velocity) to\nthe value network inputs. This privileged information is not\naccessible by the policy network.\nVI. SIMULATED EXPERIMENTS\nWe study the following questions in simulation:\nReward Design. How important is the keypoint-based reward\nfor eliciting desired twisting behavior in bimanual manip-\nulation? How effective is it compared to the other reward\nfunctions used in recent works?\nPerception. How important is the visual information in solving\nthis task? Is a sparse, keypoint representation enough for\nlearning a policy that can handle multiple objects?\nA. Setup\nObject Set. In the simulated experiments, we utilize a collec-\ntion of simulated cylindrical bottles with varying aspect ratios\nfor both training and evaluation. Some samples are visualized\nin Figure 5. We consider two setups in simulation: 1) Multi-\nobject setup, in which all the objects are used, and 2) Single-\nobject setup, in which we only use a medium-sized bottle that\nrepresents the mean of the dataset.\nEvaluation Metric. We introduce the following metrics for\nevaluating the performance:\nAngular Displacement (AD) is the total number of degrees\nthrough which the lid has been twisted.\nTraining Objects in Simulation\nCustom-Made Bottles \n(within Training Distribution except for the Square Bottle)\nReal World Household Objects (Out-of-Distribution)\nFigure 5. Used bottles in our experiments. Top: Simulated bottles.\nMiddle: Custom-made bottles (in-distribution except for the rightmost\nsquare bottle). Bottom: Household object bottles (out-of-distribution).\nTime-to-Fail (TTF) is the period measured from the moment\nthe bottle is held to the point when it either slips from the\nhand or becomes lodged.\nBaselines. We compare our policy with the following base-\nlines:\nPolicy without Vision. This is a neural network policy without\nvisual information (bottle keypoints). We use this to evaluate\nthe importance of vision.\nPolicy with Reduced Contact Reward. In training this policy,\nwe reduce the intensity of our proposed finger contact reward.\nWe use this to study the role of our contact reward in policy\nlearning and shaping the policy\u2019s behavior.\nPolicy with Gait Reward. In training this policy, we replace our\ncontact reward with a gait constraint reward function similar\nto ones used for in-hand reorientation tasks (e.g. [40]). This\nbaseline is only used for qualitative analysis.\nB. Main Results\nReward Design. We first compare our approach with the\nreduced finger reward baseline (Figure 6). After decreasing\nthe scale of finger contact reward, learned policies fail to\nmaster the desired lid-twisting skill and have low performance\nin general. We hypothesize that this is because the motion of\nlid-twisting requires a very specific pose pattern for holding\nthe object; without explicitly encouraging such a pose pattern\n(e.g., via its contact modes), RL exploration becomes so hard\nthat it is unsolvable within the available training time. We also\nobserve a positive correlation between the intensity of finger\ncontact reward and both 1) sample efficiency during learning\nand 2) performance of learned policies (as reflected by the AD\nscore).\nVision vs No Vision. We also study the importance of vision\nmodality in solving our considered task. Existing works show\nthat certain rotation behavior can be achieved through implicit\ntactile sensing (joint proprioception data) [40], but it is unclear\nwhether the same conclusion can be drawn for the bimanual\nlid-twisting task. Our empirical results show that, in both\nsingle and multi-object setups, the no-vision baseline performs\nsubstantially worse than our full method. This suggests that\nknowledge of the position of bottle keypoints is essential for\nsuccessful lid-twisting.\nSingle Object vs Multi Object. We run RL training with\ntwo object settings: (1) using a single bottle-like object and\n(2) using multiple bottle-like objects with more variation in\nthe ratio between the bottle base and lid. The two settings\npose a trade-off between specialization and generalization: in\nthe single-object scenario, the policy might learn successful\nbehaviors more easily but find it harder to generalize to unseen\nobjects, and vice versa. To our surprise, we observe that multi-\nobject training yields slightly better performance compared\nto single-object training. We hypothesize that multi-object\nmakes exploring lid-twisting behavior an easier process by\nintroducing an object curriculum that covers both easy and\nhard object instances during training.\nC. Qualitative Results\nIn this section, we study the reward design by examining\ntheir induced behavior. Specifically, we compare our method\nto the reduced contact reward baseline and the gaining reward\nbaseline. Visualization of trajectories from policies trained\nwith each method is shown in Figure 7. Policies trained\nwithout the finger contact reward exhibit extremely unstable\ngrasping or screwing motions; the behaviors not only appear\nunnatural in simulation, but also completely fail to transfer to\na real-world setup. We hypothesize that the use of two hands,\neach with a high degree of freedom, leads to a massive search\nspace for in-hand object manipulation; introducing task priors\nlike behavior-aware contact reward is therefore essential for\nreducing the search space and eliciting natural behavior.\nVII. REAL-WORLD EXPERIMENTS\nIn this section, we first demonstrate that our final learned\npolicy can be directly transferred to the real world in a zero-\nshot manner. Then, we investigate the effect of several key\ndesign choices on the success of sim-to-real transfer. Finally,\nwe examine our policy\u2019s ability to generalize, as well as its\nrobustness against various kinds of perturbations.\nA. Setup\nFor quantitative evaluation, we evaluate the sim-to-real\ntransfer capabilities of our policies on five different articulated\nbottle objects (Figure 5). Among them, four are in-distribution\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nSingle Object (AD)\ndisabled (0%)\nreduced (50%)\nours\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0\n100\n200\n300\n400\n500\nSingle Object (TTF)\ndisabled (0%)\nreduced (50%)\nours\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nSingle Object (AD)\nw/o vision\nours\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0\n100\n200\n300\n400\n500\nSingle Object (TTF)\nw/o vision\nours\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nMulti Object (AD)\ndisabled (0%)\nreduced (50%)\nours (100%)\n0.0\n0.5\n1.0\n1.5\n2.0\n1e8\n0\n100\n200\n300\n400\n500\nMulti Object (TTF)\ndisabled (0%)\nreduced (50%)\nours (100%)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e8\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nMulti Object (AD)\nw/o vision\nours\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e8\n0\n100\n200\n300\n400\n500\nMulti Object (TTF)\nw/o vision\nours\nFigure 6. Training curves in different settings. Top: Results in single-object setup. Bottom: Results in multi-object setup. Left half: Comparisons\nof different reward setups. Right half: Ablations on the use of vision. The results are averaged on 5 seeds. The shaded area shows the standard\ndeviation. The AD score is averaged by the total execution steps.\nTable II. Comparison with baselines on real setup. For each method, we deploy 3 policies trained on 3 different seeds and average the results.\nEach deployment trial is conducted for 30 seconds. The Angular Displacement (AD) metric refers to number of degrees that object lid is\ntwisted by hands in a single trial. The Time-to-Fail (TTF) metric refers to number of seconds that has passed before the object drops out of\nrobot hands; this number is only counted when there is such a failure. Our success policy outperforms all baselines on both metrics.\nBlueBottle\nWoodBottle\nRedBottle\nGoldBottle\nSqaureBottle\nMethod\nAD (deg) \u2191\nTTF (s) \u2191\nAD (deg) \u2191\nTTF (s) \u2191\nAD (deg) \u2191\nTTF (s) \u2191 AD (deg) \u2191\nTTF (s) \u2191 AD (deg) \u2191\nTTF (s) \u2191\nReplay 128.33\u00b1217.96\n7.67\u00b14.93\n2.67\u00b14.62\n7.67\u00b15.86\n15.00\u00b125.98\n4.67\u00b14.62\n28.33\u00b143.04\n7.67\u00b14.04\n29.67\u00b18.62\n10.00\u00b10.00\nNo-Vis\n1.33\u00b12.31 21.67\u00b114.43\n1.07\u00b11.85 14.67\u00b113.61\n1.90\u00b13.29\n8.33\u00b16.11\n0.67\u00b11.15 16.33\u00b113.05\n5.00\u00b16.24 20.33\u00b111.24\nNo-Asym\n18.67\u00b128.94\n30.00\u00b10.00\n0.67\u00b11.15 19.33\u00b115.14\n8.33\u00b114.43 13.00\u00b115.13\n4.3\u00b17.51\n3.67\u00b13.79\n0.00\u00b10.00\n2.33\u00b11.53\nLarge\n2.00\u00b12.00 22.33\u00b113.28\n0.00\u00b10.00 24.00\u00b110.39\n2.33\u00b12.52\n9.33\u00b14.73\n2.67\u00b13.06 22.33\u00b113.28\n1.67\u00b12.89\n30.00\u00b10.00\nOurs 946.33\u00b1383.81 23.67\u00b110.97 499.50\u00b1578.23\n30.0\u00b10.0 150.67\u00b1113.47\n30.00\u00b10.00\n98.67\u00b166.91\n30.00\u00b10.00\n43.00\u00b112.12\n30.00\u00b10.00\n(round-body bottles) and one is outside of the training distri-\nbution (square-body bottle). We measure both AD and TTF in\n20 trials, with each trial lasting for a maximum of 30 seconds.\nFor each evaluated method, we select the three best policies\nout of ten policies trained on ten different random seeds. We\nend a trial if the bottle falls off the palm.\nBaselines. We compare our final policy with the following\nbaselines to study the effect several key design choices in our\npipeline.\nOpen-loop Replay Policy (Replay). We record successful trials\nof our learned policy in the simulation and randomly select a\ntrajectory to replay on the real robot. This baseline is used\nto evaluate whether the task can be solved by a deterministic\nmotion pattern.\nPolicy without Vision (No-Vis). This baseline policy only takes\nproprioceptive state information as input, without information\nabout the object state.\nPolicy without Asymmetric Training (No-Asym). We compare\nwith a baseline where policy is trained without asymmetric\nPPO, and evaluate whether introducing additional privielged\ninformation into the value network will affect the transfer\nperformance.\nLarger Policy Network Size (Large). We increase the size of\nour actor-network and train a reduced-size policy. We use\nthis to evaluate whether over-parameterization harms policy\nperformance.\nB. Sim-to-Real Results\nWe show quantitative results comparing our policy with\nbaseline policies in Table II. For both metrics, our policy\noutperforms all baselines across all evaluated objects. Our\nmethod also achieves a stable grasp and can rotate 3 out of\n5 objects at a reasonable speed. In particular, for the blue\nbottle, one of the deployed policies can achieve 4 full turns\n(360 degrees) in 30 seconds on average. In contrast, almost\nall the baselines fail to achieve any effective rotations, either\ngetting stuck or dropping the bottle to the ground. We find that\nTime\nFigure 7. Behavior of different reward functions. Top: Our full reward\nfunction achieves a stable grasp, as well as a smooth, natural, and\nhuman-like twisting motion. Middle: A naive gait constraint reward\nwithout any contact hints leads to erratic finger motion and unnatural\ngrasps. Bottom: A reduced contact reward yields somewhat natural\nbehavior, but the grasp is loose and ungainly compared to the full\ncontact reward case.\nthe open-loop policy has the lowest TTF score. Replaying a\nsuccessful trajectory will not lead to a stable grasp for most\nof the time, and the bottle will directly roll on the fingers\nand then drop off the palm. This suggests that the considered\ntask involves very fine-grained contacts and requires the policy\nto act very precisely according to the object state. Another\ninteresting observation is that the large policy does not transfer\nto the real world, although we confirm that it can achieve\nsimilar performance to our full policy in simulation. This\nsuggests that some overfitting occurs, and controlling the size\nof the policy network is very important for the successful sim-\nto-real transfer of our considered contact-rich task.\nC. Generalization to Novel Objects\nWe further test our policy\u2019s ability to generalize by testing\nit on an additional 10 novel objects commonly found in\nhouseholds (Figure 1). These objects substantially differ from\nour training objects in terms of shape, size, mass, material,\nand color. Besides, they are also fundamentally different in\nterms of mechanical design. While the lids of the synthetic\nbottles that we use for both simulation training and real-world\ntesting can be twisted infinitely, the lids of these household\nobjects cannot. Therefore, to evaluate our policy\u2019s ability to\ngeneralize the lid-twisting skill to these novel objects, we use\nlid removal as a success criterion. We define lid removal as\nthe object lid being completely detached from the object body,\nas can be seen in the case when the lids fall from the robot\nhands in Figure 1. We find that our policy can achieve about\na 30% success rate. The results can be found in the video\nsupplementary materials.\nD. Robustness against Perturbation\nFinally, we also evaluate our policy\u2019s robustness against\nforce perturbation. Specifically, we perturb the object during\nFigure 8. Perturbing a learned policy with random external force. Our\npolicy is resilient to these external forces and recovers to normal. See\nour website for video visualizations.\ndeployment at random times by poking or pushing it along\nrandom directions using a picker tool (see the left of Figure 8).\nWe find that our policy can reorient and translate the object\nback to the center for continual manipulation, indicating that it\nhas some robustness to external forces and can adapt to these\nunexpected changes. Note that we use a marker-based object\ndetection system in this experiment to disentangle the visual\nocclusion effect.\nVIII. CONCLUSION\nIn this paper, we consider the task of twisting or removing\nlids of bottle-like objects with two hands. We present a RL-\nbased sim-to-real system to solve this problem, and propose\nvarious techniques to handle the challenges that arise \u2013 includ-\ning a novel reward design, a sparse object representation for\nreal-time perception, and an efficient yet high-fidelity method\nto simulate twisting bottle caps. We conduct experiments in\nboth simulation and real world to demonstrate the effectiveness\nof our approach. Our real-world results show generalization\nacross a wide range of seen and unseen objects. We hope\nour system can inspire future researchers to tackle bimanual\ndexterous hands challenges with RL and sim-to-real.\nACKNOWLEDGMENTS\nWe thank Chen Wang and Yuzhe Qin for helpful discus-\nsions on hardware setup and simulation of the Allegro Hand.\nTL is supported by fellowships from the National Science\nFoundation and UC Berkeley. ZY is supported by funding\nfrom InnoHK Centre for Logistics Robotics and ONR MURI\nN00014-22-1-2773. HQ is supported by the DARPA Machine\nCommon Sense and ONR MURI N00014-21-1-2801.\nREFERENCES\n[1] Alexandre Amice, Peter Werner, and Russ Tedrake.\nCertifying bimanual rrt motion plans in a second.\narXiv:2310.16603, 2023. 2\n[2] Sridhar Pandian Arunachalam, Irmak G\u00a8uzey, Soumith\nChintala, and Lerrel Pinto. Holo-dex: Teaching dexterity\nwith immersive mixed reality. In ICRA, 2023. 2\n[3] Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans,\nand Lerrel Pinto.\nDexterous imitation made easy: A\nlearning-based framework for efficient dexterous manip-\nulation. In ICRA, 2023. 2\n[4] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg\nKlimov. Exploration by random network distillation. In\nICLR, 2019. 4\n[5] Fabrizio Caccavale, Pasquale Chiacchio, Alessandro\nMarino, and Luigi Villani.\nSix-dof impedance control\nof dual-arm cooperative manipulators. Transactions on\nMechatronics, 2008. 2\n[6] Konstantinos Chatzilygeroudis, Bernardo Fichera, Ilaria\nLauzana, Fanjun Bu, Kunpeng Yao, Farshad Khadivar,\nand Aude Billard.\nBenchmark for bimanual robotic\nmanipulation of semi-deformable objects. RA-L, 2020.\n2\n[7] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for\ngeneral in-hand object re-orientation. In CoRL, 2021. 2\n[8] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Ed-\nward Adelson, and Pulkit Agrawal. Visual dexterity: In-\nhand reorientation of novel and complex object shapes.\nScience Robotics, 2023. 2\n[9] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong\nFeng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer,\nHao Dong, Song-Chun Zhu, and Yaodong Yang. Towards\nhuman-level bimanual dexterous manipulation with rein-\nforcement learning. In NeurIPS, 2022. 2\n[10] Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen\nLiu.\nSequential dexterity: Chaining dexterous policies\nfor long-horizon manipulation. In CoRL, 2023. 2\n[11] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, 2022. 4\n[12] Zih-Yun Chiu, Florian Richter, Emily K Funk, Ryan K\nOrosco, and Michael C Yip.\nBimanual regrasping for\nsuture needles using reinforcement learning for rapid\nmotion planning. In ICRA, 2021. 2\n[13] Djork-Arn\u00b4e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learning by\nexponential linear units. arXiv:1511.07289, 2015. 5\n[14] Thomas Cohn, Seiji Shaw, Max Simchowitz, and Russ\nTedrake. Constrained bimanual planning with analytic\ninverse kinematics. arXiv:2309.08770, 2023. 2\n[15] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren,\nJingjing Chen, Ruo Zhang, Weiming Wang, and Cewu\nLu. Low-cost exoskeletons for learning whole-arm ma-\nnipulation in the wild. arXiv:2309.14975, 2023. 2\n[16] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang,\nYu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff,\nand Dieter Fox. Dexpilot: Vision-based teleoperation of\ndexterous robotic hand-arm system. In ICRA, 2020. 2\n[17] Ankur Handa, Arthur Allshire, Viktor Makoviychuk,\nAleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys\nMakoviichuk,\nKarl\nVan\nWyk,\nAlexander\nZhurke-\nvich, Balakumar Sundaralingam, Yashraj Narang, Jean-\nFrancois Lafleche, Dieter Fox, and Gavriel State. Dex-\ntreme: Transfer of agile in-hand manipulation from sim-\nulation to reality. In ICRA, 2023. 2\n[18] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe\nQin, Yaodong Yang, Nikolay Atanasov, and Xiaolong\nWang.\nDynamic handover: Throw and catch with bi-\nmanual hands. In CoRL, 2023. 2\n[19] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario\nBellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco\nHutter.\nLearning agile and dynamic motor skills for\nlegged robots. Science Robotics, 2019. 2, 5\n[20] Satoshi Kataoka, Seyed Kamyar Seyed Ghasemipour,\nDaniel Freeman, and Igor Mordatch.\nBi-manual ma-\nnipulation and attachment via sim-to-real reinforcement\nlearning. arXiv:2203.08277, 2022. 2\n[21] Gagan Khandate, Siqi Shang, Eric T Chang, Tristan Luca\nSaidi, Johnson Adams, and Matei Ciocarlie. Sampling-\nbased exploration for reinforcement learning of dexterous\nmanipulation. In RSS, 2023. 2\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo,\nPiotr Doll\u00b4ar, and Ross Girshick. Segment anything. In\nICCV, 2023. 4\n[23] Franziska Krebs and Tamim Asfour. A bimanual manip-\nulation taxonomy. RA-L, 2022. 2\n[24] Franziska Krebs, Andre Meixner, Isabel Patzer, and\nTamim Asfour. The kit bimanual manipulation dataset.\nIn Humanoids, 2021. 2\n[25] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra\nMalik. Rma: Rapid motor adaptation for legged robots.\nIn RSS, 2021. 2\n[26] Marco Laghi, Michele Maimeri, Mathieu Marchand,\nClara Leparoux, Manuel Catalano, Arash Ajoudani, and\nAntonio Bicchi. Shared-autonomy control for intuitive\nbimanual tele-manipulation. In Humanoids, 2018. 2\n[27] Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, and\nYi Wu. Efficient bimanual handover and rearrangement\nvia symmetry-aware actor-critic learning. In ICRA, 2023.\n2\n[28] Zhijun Li, Bo Huang, Arash Ajoudani, Chenguang Yang,\nChun-Yi Su, and Antonio Bicchi. Asymmetric bimanual\ncontrol of dual-arm exoskeletons for human-cooperative\nmanipulations. Transactions on Robotics, 2017. 2\n[29] Toru Lin and Allan Jabri. Mimex: Intrinsic rewards from\nmasked input modeling. In NeurIPS, 2023. 4\n[30] Yijiong Lin, Alex Church, Max Yang, Haoran Li, John\nLloyd, Dandan Zhang, and Nathan F Lepora. Bi-touch:\nBimanual tactile manipulation with sim-to-real deep re-\ninforcement learning. RA-L, 2023. 2\n[31] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong\nGuo, Michelle Lu, Kier Storey, Miles Macklin, David\nHoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and\nGavriel State. Isaac gym: High performance gpu-based\nphysics simulation for robot learning. arXiv:2108.10470,\n2021. 11\n[32] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz\nWellhausen, Vladlen Koltun, and Marco Hutter. Learning\nrobust perceptive locomotion for quadrupedal robots in\nthe wild. Science Robotics, 2022. 2\n[33] Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles\nMacklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong\nGuo, Adam Moravanszky, Gavriel State, Michelle Lu,\nAnkur Handa, and Dieter Fox. Factory: Fast contact for\nrobotic assembly. In RSS, 2022. 2, 4\n[34] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek\nChociej, Mateusz Litwin, Bob McGrew, Arthur Petron,\nAlex Paino, Matthias Plappert, Glenn Powell, Raphael\nRibas, Jonas Schneider, Nikolas Tezak, Jerry Tworek,\nPeter Welinder, Lilian Weng, Qiming Yuan, Wojciech\nZaremba, and Lei Zhang. Solving rubik\u2019s cube with a\nrobot hand. arXiv:1910.07113, 2019. 2\n[35] Christian Ott, Oliver Eiberger, Werner Friedl, Berthold\nBauml, Ulrich Hillenbrand, Christoph Borst, Alin Albu-\nSchaffer, Bernhard Brunner, Heiko Hirschmuller, Simon\nKielhofer, Rainer Konietschke, Michael Suppa, Thomas\nWimbock, Franziska Zacharias, and Gerhard Hirzinger.\nA humanoid two-arm system for dexterous manipulation.\nIn Humanoids, 2006. 2\n[36] American Academy Of Pediatrics. Caring for Your Baby\nand Young Child: Birth to Age 5. American Academy\nOf Pediatrics, 2019. 1\n[37] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wo-\njciech Zaremba, and Pieter Abbeel. Asymmetric actor\ncritic for image-based robot learning. In RSS, 2018. 5\n[38] Johannes\nPitz,\nLennart\nRostel,\nLeon\nSievers,\nand\nBerthold Bauml. Dextrous tactile in-hand manipulation\nusing a modular reinforcement learning architecture. In\nICRA, 2023. 2\n[39] Robert Platt, Andrew H. Fagg, and Roderic A. Grupen.\nManipulation Gaits: Sequences of Grasp Control Tasks.\nIn ICRA, 2004. 2\n[40] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and\nJitendra Malik. In-hand object rotation via rapid motor\nadaptation. In CoRL, 2022. 2, 6\n[41] Haozhi Qi, Brent Yi, Yi Ma, Sudharshan Suresh, Mike\nLambeta, Roberto Calandra, and Jitendra Malik. General\nin-hand object rotation with vision and touch. In CoRL,\n2023. 2\n[42] Yuzhe Qin, Hao Su, and Xiaolong Wang.\nFrom one\nhand to multiple hands: Imitation learning for dexterous\nmanipulation from single-camera teleoperation.\nRA-L,\n2022. 2\n[43] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk,\nHao Su, Xiaolong Wang, Yu-Wei Chao, and Dietor Fox.\nAnyteleop: A general vision-based dexterous robot arm-\nhand teleoperation system. In RSS, 2023. 2\n[44] Lennart\nRostel,\nJohannes\nPitz,\nLeon\nSievers,\nand\nBerthold Bauml. Estimator-coupled reinforcement learn-\ning for robust purely tactile in-hand manipulation.\nIn\nHumanoids, 2023. 2\n[45] Nilanjan Sarkar, Xiaoping Yun, and Vijay R. Kumar.\nDynamic control of 3-d rolling contacts in two-arm\nmanipulation. In ICRA, 1993. 2\n[46] John Schulman, Philipp Moritz, Sergey Levine, Michael\nJordan, and Pieter Abbeel.\nHigh-dimensional contin-\nuous control using generalized advantage estimation.\narXiv:1506.02438, 2015. 5\n[47] Lucy Xiaoyang Shi, Archit Sharma, Tony Z Zhao, and\nChelsea Finn.\nWaypoint-based imitation learning for\nrobotic manipulation. In CoRL, 2023. 2\n[48] Christian Smith, Yiannis Karayiannidis, Lazaros Nalpan-\ntidis, Xavi Gratal, Peng Qi, Dimos V. Dimarogonas,\nand Danica Kragic. Dual arm manipulation - a survey.\nRobotics and Autonomous Systems, 2012. 2\n[49] Nicolas Sommer, Miao Li, and Aude Billard. Biman-\nual compliant tactile exploration for grasping unknown\nobjects. In ICRA, 2014. 2\n[50] Jan Steffen, Christof Elbrechter, Robert Haschke, and\nHelge Ritter.\nBio-inspired motion strategies for a bi-\nmanual manipulation task. In Humanoids, 2010. 2\n[51] Simon Stepputtis, Maryam Bandari, Stefan Schaal, and\nHeni Ben Amor.\nA system for imitation learning of\ncontact-rich bimanual manipulation policies.\nIn IROS,\n2022. 2\n[52] Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha\nFan, Luis Pineda, Mike Lambeta, Jitendra Malik, Mrinal\nKalakrishnan, Roberto Calandra, Michael Kaess, Joe\nOrtiz, and Mustafa Mukadam. Neural feels with neural\nfields: Visuo-tactile perception for in-hand manipulation.\narXiv:2312.13469, 2023. 2\n[53] Nikolaus Vahrenkamp, Markus Przybylski, Tamim As-\nfour, and R\u00a8udiger Dillmann. Bimanual grasp planning.\nIn Humanoids, 2011. 2\n[54] Renee Watling. Peabody Developmental Motor Scales.\nSpringer, 2013. 1\n[55] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng\nChen, and Xiaolong Wang.\nRotating without seeing:\nTowards in-hand dexterity through touch. In RSS, 2023.\n2, 5\n[56] Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang,\nZhao-Heng Yin, Kang-Won Lee, Yi Wu, Soo-Chul Lim,\nand Xiaolong Wang. Robot synesthesia: In-hand manip-\nulation with visuotactile sensing. In ICRA, 2024. 2\n[57] Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi,\nTaylor Howell, Xue Bin Peng, Sumeet Singh, Yuval\nTassa, Pete Florence, Andy Zeng, et al.\nRobopianist:\nDexterous piano playing with deep reinforcement learn-\ning. In CoRL, 2023. 2\n[58] Kevin Zhang, Mohit Sharma, Manuela Veloso, and Oliver\nKroemer. Leveraging multimodal haptic sensory data for\nrobust cutting. In Humanoids, 2019. 2\n[59] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea\nFinn. Learning fine-grained bimanual manipulation with\nlow-cost hardware. In RSS, 2023. 2\nAPPENDIX\nA. Object Details\nSimulated Bottles. We use Isaac Gym [31] to model the\nsimulated learning environments.\nFor the multi-object environment, we use bottles whose\nbodies range from 82cm to 86cm in diameter and 55cm to\n67cm in height, and whose caps range from 62cm to 70cm in\ndiameter and 20cm to 33cm in height.\nFor the single-object environment, we use a bottle whose\nbody is 84cm in diameter and 60cm in height, and whose cap\nis 67cm in diameter and 26cm in height.\nReal-World Bottles. We show details of our real-world bottle\ndesign in Figure 9.\nFigure 9. Real-world bottle design. Each bottle is consisted of three\nparts: the cap (top left), the pin (top right), and the body (bottom\nleft). The parts can be 3D printed and assembled by inserting the pin\ninto the cap and fixing the cap onto the body. With the pin holding\nthe cap and body in place, the cap can be infinitely twisted about the\nbody.\nB. Implementation Details\nAction Hyperparameters. To generate action commands, we\nclip neural network policy output to [\u22121, 1] range. We then\napply an action scale of 0.1 and a moving average parameter\nof 0.75 to the actions.\nReward Hyperparameters. We use \u03b11 = 2.5, \u03b12 = 500.0,\n\u03b13 = 20, \u03b14 = \u22120.001, and \u03b15 = \u22121.0 as reward weights.\nAsymmetric States. In addition to the policy inputs, we\nprovide the following privilege state inputs to the value net-\nwork of asymmetric PPO: hand joint velocities, all fingertip\npositions, all contact keypoint positions, object orientation,\nobject velocity, object angular velocity, random forces applied\nto object, object brake torque, object mass randomization\nscale, object friction randomization scale, and object shape\nrandomization scale.\nC. Real-World Experiment Details\nCamera Calibration. We use a novel marker-based approach\nto calibrate the extrinsics matrix of our camera. Specifically,\nwe add the marker tag used in our real-world setup into\nthe simulation environment, such that pair coordinates of the\nmarker corners can be obtained easily in both the camera\nframe and the world frame (Figure 10). We then use the\npaired coordinates to solve for camera extrinsics. Doing so\ngreatly reduces the manual labor required by other camera\ncalibration approaches, such as capturing checkerboard images\nand solving for multiple extrinsic matrices.\nFigure 10. Modeling real-world marker tag in simulation for easy\ncamera calibration.\nHardware Communication. To keep our control loop running\nreliably at 10Hz, we use ZeroMQ to manage communication\nbetween robot hands, camera, and Linux workstation.\nTask Initialization. We illustrate details of how we initialize\nthe task sequence in the real world in Figure 11. The canonical\njoint positions of each finger is documented in Table III.\nFigure 11. Real-world task initialization. At the beginning of each\ntask sequence, we initialize the robot hands about a canonical position\nwith upward-facing palms. Then, we lightly place an object onto the\nfingers.\nTable III. Initial joint positions of robot hands. Both hands are\ninitialized with the same joint positions.\nFinger\nInitial Joint Positions\nIndex\n[-0.0080, 0.9478, 0.6420, -0.0330]\nMiddle\n[0.0530, 0.7163, 0.9609, 0.0000]\nRing\n[0.0000, 0.7811, 0.7868, 0.3454]\nThumb\n[1.0670, 1.1670, 0.7500, 0.4500]\n"
  },
  {
    "title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos",
    "link": "https://arxiv.org/pdf/2403.01444.pdf",
    "upvote": "4",
    "text": "3DGStream: On-the-fly Training of 3D Gaussians for Efficient Streaming of\nPhoto-Realistic Free-Viewpoint Videos\nJiakai Sun,\nHan Jiao,\nGuangyuan Li,\nZhanjie Zhang,\nLei Zhao*,\nWei Xing*\nZhejiang University\n{csjk, csjh, cslgy, cszzj, cszhl, wxing}@zju.edu.cn\nhttps://sjojok.github.io/3dgstream\nAbstract\nConstructing\nphoto-realistic\nFree-Viewpoint\nVideos\n(FVVs) of dynamic scenes from multi-view videos remains\na challenging endeavor. Despite the remarkable advance-\nments achieved by current neural rendering techniques,\nthese methods generally require complete video sequences\nfor offline training and are not capable of real-time render-\ning. To address these constraints, we introduce 3DGStream,\na method designed for efficient FVV streaming of real-world\ndynamic scenes. Our method achieves fast on-the-fly per-\nframe reconstruction within 12 seconds and real-time ren-\ndering at 200 FPS. Specifically, we utilize 3D Gaussians\n(3DGs) to represent the scene.\nInstead of the na\u00a8\u0131ve ap-\nproach of directly optimizing 3DGs per-frame, we employ\na compact Neural Transformation Cache (NTC) to model\nthe translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame.\nFurthermore, we propose an adaptive 3DG addition strat-\negy to handle emerging objects in dynamic scenes. Exper-\niments demonstrate that 3DGStream achieves competitive\nperformance in terms of rendering speed, image quality,\ntraining time, and model storage when compared with state-\nof-the-art methods.\n1. Introduction\nConstructing Free-Viewpoint Videos (FVVs) from videos\ncaptured by a set of known-poses cameras from multiple\nviews remains a frontier challenge within the domains of\ncomputer vision and graphics.\nThe potential value and\napplication prospects of this task in the VR/AR/XR do-\nmains have attracted much research. Traditional approaches\npredominantly fall into two categories:\ngeometry-based\nmethods that explicitly reconstruct dynamic graphics prim-\nitives [15, 17], and image-based methods that obtain new\nviews through interpolation [7, 76]. However, these conven-\ntional methods struggle to handle real-world scenes charac-\n*Corresponding authors.\n(a) I-NGP [40]: Per-frame training\n(b) HyperReel [1]: Offline training\n(c) StreamRF [29]: Online training\n(d) Ours: Online training\nFigure 1. Comparison on the flame steak scene of the N3DV\ndataset [31]. The training time, requisite storage, and PSNR are\ncomputed as averages over the whole video. Our method stands\nout by the ability of fast online training and real-time rendering,\nstanding competitive in both model storage and image quality.\nterized by complex geometries and appearance.\nIn recent years, Neural Radiance Fields (NeRFs) [36] has\ngarnered significant attention due to its potent capabilities\nin synthesizing novel views as a 3D volumetric representa-\ntion. A succession of NeRF-like works [19, 29, 31\u201333, 43\u2013\n46, 48, 61, 68] further propelled advancements in construct-\ning FVVs on dynamic scenes. Nonetheless, the vast major-\nity of NeRF-like FVV construction methods encountered\ntwo primary limitations: (1) they typically necessitate com-\nplete video sequences for time-consuming offline training,\nmeaning they can replay dynamic scenes but are unable to\nstream them, and (2) they generally fail to achieve real-time\nrendering, thereby hindering practical applications.\nRecently, Kerbl et al. [26] have achieved real-time radi-\nance field rendering using 3D Gaussians (3DGs), thus en-\nabling the instant synthesis of novel views in static scenes\n1\narXiv:2403.01444v2  [cs.CV]  5 Mar 2024\n10\n2\n10\n3\n10\n4\n10\n5\nTraining Time (Minutes)\n10\n2\n10\n1\n10\n0\n10\n1\n10\n2\nRendering Speed (FPS)\nPlenoxels\nI-NGP\nDyNeRF\nNeRFPlayer\nHexPlane\nK-Planes\nHyperReel\nMixVoxels\nStreamRF\nOurs\nFigure 2. Comparison of our method with other methods on\nthe N3DV dataset [31].\n\u25a1 denotes training from scratch per\nframe, \u25b3 represents offline training on complete video sequences,\nand \u20dd signifies online training on video streams. While achieving\nonline training , our method reaches state-of-the-art performance\nin both rendering speed and overall training time.\nwith just minutes of training.\nInspired by this break-\nthrough, we propose 3DGStream, a method that utilizes\n3DGs to construct Free-Viewpoint Videos (FVVs) of dy-\nnamic scenes. Specifically, we first train the initial 3DGs on\nthe multi-view frames at timestep 0. Then, for each timestep\ni, we use the 3DGs of previous timestep i \u2212 1 as initial-\nization and pass it to a two-stage pipeline. (1) In Stage 1,\nwe train a Neural Transformation Cache (NTC) to model\nthe transformations of 3DGs. (2) Then in the Stage 2, we\nuse an adaptive 3DG addition strategy to handle emerging\nobjects by spawning frame-specific additional 3DGs near\nthese objects and optimize them along with periodic split-\nting and pruning. After the two-stage pipeline concludes,\nwe use both the 3DGs transformed by the NTC and the ad-\nditional 3DGs for rendering at the current timestep i, with\nonly the former carrying over for initialization of the subse-\nquent timestep. This design significantly reduces the stor-\nage requirements for the FVV, as we only need to store the\nper-frame NTCs and frame-specific additions, rather than\nall 3DGs for each frame.\n3DGStream is capable of rendering photo-realistic FVVs\nat megapixel resolution in real-time, boasting exceptionally\nrapid per-frame training speeds and limited model storage\nrequirements.\nAs illustrated in Figs. 1 and 2, compared\nwith static reconstruction methods that train from scratch\nper-frame and dynamic reconstruction methods that neces-\nsitate offline training across the complete video sequences,\nour approach excels in both training speed and rendering\nspeed, maintaining a competitive edge in image quality\nand model storage. Furthermore, our method outperforms\nStreamRF [29], a state-of-the-art technique tackling the ex-\nactly same task, in all the relevant aspects.\nTo summarize, our contributions include:\n\u2022 We propose 3DGStream, a method for on-the-fly con-\nstruction of photo-realistic, real-time renderable FVV on\nvideo streams, eliminating the necessity for lengthy of-\nfline training on the entire video sequences.\n\u2022 We utilize NTC for modeling the transformations of\n3DGs, in conjunction with an adaptive 3DG addi-\ntion strategy to tackle emerging objects within dynamic\nscenes. This combination permits meticulous manipula-\ntion of 3DGs, accommodating scene alterations with lim-\nited performance overhead.\n\u2022 We\nconduct\nextensive\nexperiments\nto\ndemonstrate\n3DGStream\u2019s competitive edge in rendering quality,\ntraining time, and requisite storage, as well as its supe-\nrior rendering speed, compared to existing state-of-the-art\ndynamic scene reconstruction methods.\n2. Related Work\n2.1. Novel View Synthesis for Static Scenes\nSynthesizing novel views from a set of images of static\nscenes is a time-honored problem in the domains of com-\nputer vision and graphics. Traditional methods such as Lu-\nmigraph [8, 22] or Light-Field [10, 16, 28, 50] achieve new\nview synthesis through interpolation. In recent years, Neu-\nral Radiance Fields (NeRF) [36] has achieved photorealistic\nsynthesizing results by representing the radiance field us-\ning a multi-layer perceptron (MLP). A series of subsequent\nworks enhance NeRF\u2019s performance in various aspects,\nsuch as accelerating training speeds [12, 13, 20, 25, 40, 53],\nachieving real-time rendering [14, 21, 23, 47, 65, 73],\nand improving synthesis quality on challenging scenes [2\u2013\n4, 35, 37, 57] or sparse inputs [11, 41, 54, 64, 67, 70, 74].\nSince the vanilla NeRF employs costly volume rendering,\nnecessitating neural network queries for rendering, subse-\nquent approaches faced trade-offs in training time, render-\ning speed, model storage, image quality, and applicability.\nTo address these challenges, Kerbl et al. [26] propose 3D\nGaussian Splatting (3DG-S), which integrates of 3DGs with\ndifferentiable point-based rendering. 3DG-S enables real-\ntime high-fidelity view synthesis in large-scale unbounded\nscenes after brief training periods with modest storage re-\nquirements. Inspired by this work, we extend its application\nto the task of constructing FVVs of dynamic scenes. Taking\nit a step further, we design a on-the-fly training framework\nto achieve efficient FVV streaming.\n2.2. Free-Viewpoint Videos of Dynamic Scenes\nConstructing FVVs from a set of videos of dynamic scenes\nis a more challenging and applicable task in the domains of\ncomputer vision and graphics. Earlier attempts to address\nthis task pivoted around the construction of dynamic primi-\ntives [15, 17] or resorting to interpolation [7, 76]. With the\n2\nStage 1\nStage 2\nStage 1\nStage 2\nFrames at t=0\nInitial 3DGs\nStage 1 \nRenderings\nNeural Transformation Cache\nDifferentiable \nTile Rasterizer\nSpawn 3DGs\nQuantity \nControl\nStage 2\nRenderings\nGround Truth\nRotations\nTranslations\nTransform\nLoss\nLoss\nStage 1: Train the Neural Transformation Cache\nStage 2: Spawn, Optimize and Prune Additional 3DGs\n: Where Additional 3DGs Shall Be\n: Where Previous 3DGs Shall Be Transformed\n: Previous 3DGs\n: Additional 3DGs\n3DGs from t=i-1\n3DGs to t=i+1\n3DGs at t=i\nDifferentiable \nTile Rasterizer\n: Transformed 3DGs\nFrames at t=i\nRenderings at t=i\nFrames at t=2\nStage 1\nStage2\n3DGs\n3DGs\n3DGs\n3DGs\nRenderings at t=2\nFrames at t=1\nRenderings at t=1\nFigure 3. Overview of 3DGStream. Given a set of multi-view video streams, 3DGStream aims to construct high-quality FVV stream\nof the captured dynamic scene on-the-fly. Initially, we optimize a set of 3DGs to represent the scene at timestep 0. For each subsequent\ntimestep i, we use the 3DGs from timestep i \u2212 1 as an initialization and then engage in a two-stage training process: Stage 1: We train\nthe Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs. After training, the NTC transforms the 3DGs,\npreparing them for the next timestep and the next stage in the current timestep. Stage 2: We spawn frame-specific additional 3DGs at\npotential locations and optimize them along with periodic splitting and pruning. After the two-stage process concludes, both transformed\nand additional 3DGs are used to render at the current timestep i, with only the transformed ones carried into the next timestep.\nsuccess of NeRF-like methods in novel view synthesis for\nstatic scenes, a series of works [1, 9, 19, 29\u201334, 42, 44\u2013\n46, 48, 52, 56, 58, 60, 62, 63, 69, 75] attempt to use NeRF\nfor constructing FVVs in dynamic scenes. These works can\ntypically be categorized into five types: prior-driven, flow-\nbased, warp-based, those using spatio-temporal inputs, and\nper-frame training.\nPrior-driven methods [27, 30, 63, 69, 75] leverage para-\nmetric models or incorporate additional priors, such as\nhuman-pose skeletons, to bolster performance on the recon-\nstruction of specific dynamic objects, e.g., humans. How-\never, their application is limited and not generalizable to\nbroader scenes.\nFlow-based methods [32, 33] primarily focus on con-\nstructing FVVs from monocular videos.\nBy estimating\nthe correspondence of 3D points in consecutive frames,\nthey achieve impressive results. Nonetheless, the intrinsic\nill-posedness of monocular reconstructions in intricate dy-\nnamic scenes frequently calls for supplementary priors like\ndepth, optical flow, and motion segmentation masks.\nWarp-based methods [1, 42, 44, 46, 52, 56, 62] assumpt\nthat the dynamics of the scene arise from the deformation of\nstatic structures. These techniques warp the radiance field\nof each frame onto one or several canonical frames, achiev-\ning notable results. However, the strong assumptions they\nrely on often prevent them from handling topological varia-\ntions.\nMethods that use spatio-temporal inputs [9, 19, 31, 45,\n48, 58, 59] enhance radiance fields by adding a temporal di-\nmension, enabling the querying of the radiance field using\nspatio-temporal coordinates. While these techniques show-\ncase a remarkable ability to synthesize new viewpoints in\ndynamic scenes, the entangled scene parameters can con-\nstrain their adaptability for downstream applications.\nPer-frame training methods [29, 34, 60] adapt to\nchanges in the scene online by leveraging per-frame train-\ning, a paradigm we have also adopted.\nTo be specific,\nStreamRF [29] employs Plenoxels [20] for scene repre-\nsentation and achieves rapid on-the-fly training with min-\nimal storage requirements through techniques like narrow\nband tuning and difference-based compression. ReRF [60]\nuses DVGO [53] for scene representation and optimize mo-\ntion grid and residual grid frame by frame to model inter-\nframe discrepancies, enabling high-quality FVV stream-\ning and rendering.\nDynamic3DG [34] optimizes simpli-\nfied 3DGs and integrates physically-based priors for high-\nquality novel view synthesis on dynamic scenes.\nAmong\nthe\naforementioned\nworks,\nonly\nNeRF-\nPlayer\n[52],\nReRF\n[60],\nStreamRF\n[29],\nand\nDy-\nnamic3DG [34] are able to stream FVVs.\nNeRFPlayer\nachieves FVV streaming through a decomposition module\nand a feature streaming module, but it is only able to\nstream pre-trained models.\nReRF and Dynamic3DG\nare limited to processing inward-facing scenes that are\nequipped with foreground masks and contain only a few\nobjects, necessitating several minutes to train each frame.\nStreamRF stands out by requiring only a few seconds for\neach frame\u2019s training to construct high-fidelity FVVs on\nchallenging real-world dynamic scenes with compressed\nmodel storage. However, it falls short in rendering speed.\nContrarily, our approach matches or surpasses StreamRF in\ntraining speed, model storage, and image quality, all while\nachieving real-time rendering at 200 FPS.\n3\n2.3. Concurrent Works\nExcept for Dynamic3DG, several concurrent works have\nextended 3DG-S to represent dynamic scenes.\nDe-\nformable3DG [71] employs an MLP to model the deforma-\ntion of 3DGs, while [66] introduces a hexplane-based en-\ncoder to enhance the efficency of deformation query. Mean-\nwhile, [18, 72] lift 3DG to 4DG primitives for dynamic\nscene representation. However, these approaches are lim-\nited to offline reconstruction and lack streamable capabili-\nties, whereas our work aims to achieve efficient streaming\nof FFVs with an online training paradigm.\n3. Background: 3D Gaussian Splatting\n3D Gaussian Splatting (3DG-S) [26] employs anisotropic\n3D Gaussians as an explicit scene representation. Paired\nwith a fast differentiable rasterizer, 3DGs achieves real-time\nnovel view synthesis with only minutes of training.\n3.1. 3D Gaussians as Scene Representation\nA 3DG is defined by a covariance matrix \u03a3 centered at point\n(i.e., mean) \u00b5:\nG(x; \u00b5, \u03a3) = e\u2212 1\n2 (x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5).\n(1)\nTo ensure positive semi-definiteness during optimization,\nthe covariance matrix \u03a3 is decomposed into a rotation ma-\ntrix R and a scaling matrix S:\n\u03a3 = RSST RT .\n(2)\nRotation is conveniently represented by a unit quaternion,\nwhile scaling uses a 3D vector. Additionally, each 3DG\ncontains a set of spherical harmonics (SH) coefficients of\nto represent view-dependent colors, along with an opacity\nvalue \u03b1, which is used in \u03b1-blending (Eq. (4)).\n3.2. Splatting for Differentiable Rasterization\nFor novel view synthesis, 3DG-S [26] project 3DGs to 2D\nGaussian (2DG) splats [77]:\n\u03a3\u2032 = JW\u03a3W T JT .\n(3)\nHere, \u03a3\u2032 is the covariance matrix in camera coordinate. J\nis the Jacobian of the affine approximation of the projective\ntransformation, and W is the viewing transformation ma-\ntrix. By skipping the third row and third column of \u03a3\u2032, we\ncan derive a 2\u00d72 matrix denoted as \u03a32d. Furthermore, pro-\njecting the 3DG\u2019s mean, \u00b5, into the image space results in\na 2D mean, \u00b52d. Consequently, this allows us to define the\n2DG in the image space as G2d(x; \u00b52d, \u03a32d).\nUsing \u03a3\u2032, the color C of a pixel can be computed by\nblending the N ordered points overlapping the pixel:\nC =\nX\ni\u2208N\nci\u03b1\u2032\ni\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1\u2032\nj).\n(4)\nHere, ci denotes the view-dependent color of the i-th 3DG.\n\u03b1\u2032\ni is determined by multiplying the opacity \u03b1i of the i-th\n3DG G with the evaluation of the corresponding 2DG G2d.\nLeveraging a highly-optimized rasterization pipeline\ncoupled with custom CUDA kernels, the training and ren-\ndering of 3DG-S are remarkably fast.\nFor instance, for\nmegapixel-scale real-world scenes, just a few minutes of\noptimization allows 3DGs to achieve photo-realistic visual\nquality and rendering speeds surpassing 100 FPS.\n4. Method\n3DGStream constructs photo-realistic FVV streams from\nmulti-view video streams on-the-fly using a per-frame train-\ning paradigm. We initiate the process by training 3DGs [26]\nat timestep 0. For subsequent timesteps, we employ the pre-\nvious timestep\u2019s 3DGs as an initialization and pass them to\na two-stage pipeline. Firstly (Sec. 4.1), a Neural Transfor-\nmation Cache (NTC) is trained to model the transforma-\ntion for each 3DG. Once the training is finished, we trans-\nform the 3DGs and carry the transformed 3DGs to the next\ntimestep. Secondly (Sec. 4.2), we employ an adaptive 3DG\naddition strategy to handle emerging objects. For each FVV\nframe, we render views at the current timestep using both\nthe transformed 3DGs and additional 3DGs, while the latter\nare not passed to the next timestep. Note that we only need\nto train and store the parameters of the NTC and the addi-\ntional 3DGs for each subsequent timestep, not all the 3DGs.\nWe depict an overview of our approach in Fig. 3.\n4.1. Neural Transformation Cache\nFor NTC, we seek a structure that is compact, efficient, and\nadaptive to model the transformations of 3DGs. Compact-\nness is essential to reduce the model storage. Efficiency\nenhances training and inference speeds. Adaptivity ensures\nthe model focuses more on dynamic regions. Additionally,\nit would be beneficial if the structure could consider certain\npriors of dynamic scenes [5, 24, 55], such as the tendency\nfor neighboring parts of an object to have similar motion.\nInspired by Neural Radiance Caching [39] and I-\nNGP [40], we employ multi-resolution hash encoding com-\nbined with a shallow fully-fused MLP [38] as the NTC.\nSpecifically, following I-NGP, we use multi-resolution\nvoxel grids to represent the scene. Voxel grids at each res-\nolution are mapped to a hash table storing a d-dimensional\nlearnable feature vector. For a given 3D position x \u2208 R3,\nits hash encoding at resolution l, denoted as h(x; l) \u2208\nRd, is the linear interpolation of the feature vectors cor-\nresponding to the eight corners of the surrounding grid.\nConsequently, its multi-resolution hash encoding h(x) =\n[h(x; 0), h(x; 1), ..., h(x; L \u2212 1)] \u2208 RLd, where L repre-\nsents the number of resolution levels. The multi-resolution\nhash encoding addresses all our requirements for the NTC:\n4\n\u2022 Compactness: Hashing effectively reduces the storage\nspace needed for encoding the whole scene.\n\u2022 Efficiency: Hash table lookup operates in O(1), and is\nhighly compatible with modern GPUs.\n\u2022 Adaptivity:\nHash collisions occur in hash tables at\nfiner resolutions, allowing regions with larger gradi-\nents\u2014representing dynamic regions in our context\u2014to\ndrive the optimization.\n\u2022 Priors:\nThe combination of linear interpolation and\nthe voxel-grid structure ensures the local smoothness of\ntransformations.\nAdditionally, the multi-resolution ap-\nproach adeptly merges global and local information.\nFurthermore, to enhance the performance of NTC with min-\nimal overhead, we utilize a highly-optimized shallow fully-\nfused MLP [38].\nThis maps the hash encoding to a 7-\ndimensional output: the first three dimensions indicate the\ntranslation of the 3DG; the remaining dimensions represent\nthe rotation of the 3DG using quaternions. Given multi-\nresolution hash encoding coupled with MLP, our NTC is\nformalized as:\nd\u00b5, dq = MLP(h(\u00b5)),\n(5)\nwhere \u00b5 denotes the mean of the input 3DG. We transform\nthe 3DGs based on d\u00b5 and dq. Specifically, the following\nparameters of the transformed 3DGs are given as:\n\u2022 Mean: \u00b5\u2032 = \u00b5 + d\u00b5, where \u00b5\u2032 is the new mean and +\nrepresents vector addition.\n\u2022 Rotation: q\u2032 = norm(q) \u00d7 norm(dq), where q\u2032 is the\nnew roation, \u00d7 indicates quaternion multiplication and\nnorm denotes normalization.\n\u2022 SH Coefficients: Upon rotating the 3DG, the SH coeffi-\ncients should also be adjusted to align with the rotation\nof the 3DG. Leveraging the rotation invariance of SH, we\ndirectly employ SH Rotation to update SHs. Please refer\nto the supplementary materials (Suppl.) for details.\nIn Stage 1, we transform the 3DGs from the previous frame\nby NTC and then render with them. The parameters of the\nNTC is optimized by the loss between the rendered image\nand the ground truth. Following 3DG-S [26], the loss func-\ntion is L1 combined with a D-SSIM term:\nL = (1 \u2212 \u03bb)L1 + \u03bbLD\u2212SSIM,\n(6)\nwhere \u03bb = 0.2 in all our experiments. It should be noted\nthat during the training process, the 3DGs from the previous\nframe remain frozen and do not undergo any updates. This\nimplies that the input to the NTC remains consistent.\nAdditionally, to ensure training stability, we initialize the\nNTC with warm-up parameters. The loss employed during\nthe warm-up is defined as:\nLwarm\u2212up = ||d\u00b5||1 \u2212 cos2(norm(dq), Q),\n(7)\nwhere Q is the identity quaternion.\nThe first term uses\nthe L1 norm to ensure the estimated translation approaches\nzero, while the second term, leveraging cosine similarity,\nensures the estimated rotation approximates no rotation.\nHowever, given the double-covering property of the unit\nquaternions, we use the square of the cosine similarity. For\neach scene, we execute the warm-up solely after the training\nat timestep 0, using noise-augmented means of the initial\n3DGs as input. After 3000 iterations of training (roughly\n20 seconds), the parameters are stored and used to initialize\nthe NTCs for all the following timesteps.\n4.2. Adaptive 3DG Addition\nRelying solely on 3DGs transformations adequately cover\na significant portion of real-world dynamic scenes, with\ntranslations effectively managing occlusions and disappear-\nances in subsequent timesteps. However, this approach fal-\nters when faced with objects not present in the initial frame,\nsuch as transient objects like flames or smoke, and new per-\nsistent objects like the liquid poured out of a bottle. Since\n3DG is an unstructured explicit representation, it\u2019s essential\nto add new 3DGs to model these emerging objects. Consid-\nering constraints related to model storage requirements and\ntraining complexities, it\u2019s not feasible to generate an exten-\nsive number of additional 3DGs nor allow them to be used\nin subsequent frames, as this would cause 3DGs to accumu-\nlate over time. This necessitates a strategy for swiftly gen-\nerating a limited number of frame-specific 3DGs to model\nthese emerging objects precisely and thereby enhance the\ncompleteness of the scene at the current timestep.\nFirstly, we need to ascertain the locations for the emerg-\ning objects. Inspired by 3DG-S [26], we recognized the\nview-space positional gradients of 3DGs as a key indicator.\nWe observed that for emerging objects, the 3DGs in prox-\nimity exhibited large view-space positional gradients. This\nis attributed to the optimization attempting to \u2018masquerade\u2019\nthe emerging object by transforming the 3DGs. However,\nsince we prevent the colors of the 3DGs from being updated\nin Stage 1, this attempt falls short. Nonetheless, they are\nstill transformed to appropriate positions, with large view-\nspace positional gradients.\nBased on the aforementioned observations, we deem\nit appropriate to introduce additional 3DGs around these\nhigh-gradient regions. Moreover, to exhaustively capture\nevery potential location where new objects might emerge,\nwe adopt an adaptive 3DG spawn strategy. Specifically, we\ntrack view-space positional gradient during the final train-\ning epoch of Stage 1. Once this stage concludes, we select\n3DGs that have an average magnitude of view-space posi-\ntion gradients exceeding a relatively low threshold \u03c4grad =\n0.00015. For each selected 3DG, the position of the addi-\ntional 3DG is sampled from X \u223c N(\u00b5, 2\u03a3), where \u00b5 and \u03a3\nis the mean and the convariance matrix of the selected 3DG.\nWhile we avoid assumptions about the other attributes of the\nadditional 3DGs, improper initializations of SH coefficients\n5\n(a) I-NGP [40]\n(b) HyperReel [1]\n(c) StreamRF [29]\n(d) 3DGStream\n(e) Ground Truth\nFigure 4. Qualitative comparisons on the discussion scene of the Meet Room dataset and the sear steak scene of the N3DV dataset.\nand scaling vectors tend to result in an optimization prefer-\nence for reducing opacity over adjusting these parameters.\nThis causes additional 3DGs to quickly become transparent,\nthereby failing to capture the emerging objects. To mitigate\nthis issue, the SH coefficients and scaling vectors of these\n3DGs are derived from the selected ones, with rotations set\nto the identity quaternion q = [1, 0, 0, 0] and opacity initial-\nized at 0.1. After spawning, the 3DGs undergo optimization\nutilizing the same loss function (Eq. (6)) as Stage 1. Note\nthat only the parameters of the additional 3DGs are opti-\nmized, while those of the transformed 3DGs remain fixed.\nTo guard against local minima and manage the number of\nadditional 3DGs, we implement an adaptive 3DG quantity\ncontrol strategy. Specifically, in Stage 2, we set a relatively\nhigh threshold, \u03c4\u03b1 = 0.01, for the opacity value. At the\nend of each training epoch, for 3DGs with view-space posi-\ntion gradients exceeding \u03c4grad, we spawn additional 3DGs\nnearby to address under-reconstructed regions. These addi-\ntional 3DGs inherit their rotations and SH coefficients from\nthe original 3DG, but their scaling is adjusted to 80% of the\noriginal, mirroring the \u2018split\u2019 operation described by Kerbl\net al. [26]. Subsequently, we discard any additional 3DGs\nwith opacity values below \u03c4\u03b1 to suppress the growth in the\nquantity of 3DGs.\n5. Experiments\n5.1. Datasets\nWe conduct experiments on two real-world dynamic scene\ndatasets: N3DV dataset [31] and Meet Room dataset [29].\nN3DV dataset [31] is captured using a multi-view sys-\ntem of 21 cameras, comprises dynamic scenes recorded at a\nresolution of 2704\u00d72028 and 30 FPS. Following previous\nworks [9, 29, 31, 48, 52, 58], we downsample the videos by\na factor of two and follow the training and validation camera\nCategory\nMethod\nPSNR\u2191\nStorage\u2193\nTrain\u2193\nRender\u2191\nStreamable\n(dB)\n(MB)\n(mins)\n(FPS)\nStatic\nPlenoxels [20]\n30.77\n4106\n23\n8.3\n\u2713\nI-NGP [40]\n28.62\n48.2\n1.3\n2.9\n\u2713\n3DG-S [26]\n32.08\n47.1\n8.3\n390\n\u2713\nOffline\nDyNeRF [31]\n29.58\u2020\n0.1\n260\n0.02\n\u00d7\nNeRFPlayer [52]\n30.69\n17.1\n1.2\n0.05\n\u2713\nHexPlane [9]\n31.70\n0.8\n2.4\n0.21\n\u00d7\nK-Planes [48]\n31.63\n1.0\n0.8\n0.15\n\u00d7\nHyperReel [1]\n31.10\n1.2\n1.8\n2.00\n\u00d7\nMixVoxels [58]\n30.80\n1.7\n0.27\n16.7\n\u00d7\nOnline\nStreamRF [29]\n30.68\n17.7/31.4\u22c6\n0.25\n8.3\n\u2713\nOurs\n31.67\n7.6/7.8\u22c6\n0.20\n215\n\u2713\nTable 1. Quantitative comparison on the N3DV dataset. The\ntraining time, required storage and PSNR are averaged over the\nwhole 300 frames for each scene. \u2020DyNeRF [31] only report met-\nrics on the flame salmon scene. \u22c6Considering the initial model.\nMethod\nPSNR\u2191\nStorage\u2193\nTrain\u2193\nRender\u2191\n(dB)\n(MB)\n(mins)\n(FPS)\nPlenoxels [20]\n27.15\n1015\n14\n10\nI-NGP [40]\n28.10\n48.2\n1.1\n4.1\n3DG-S [26]\n31.31\n21.1\n2.6\n571\nStreamRF [29]\n26.72\n5.7/9.0\u22c6\n0.17\n10\nOurs\n30.79\n4.0/4.1\u22c6\n0.10\n288\nTable 2. Quantitative comparison on the Meet Room dataset.\nNote that the training time, required storage and PSNR are aver-\naged over the whole 300 frames. \u22c6Considering the initial model.\nsplit provided by [31].\nMeet Room dataset [29] is captured using a 13-camera\nmulti-view system, comprises dynamic scenes recorded at\na resolution of 1280\u00d7720 and 30 FPS. Following [29], we\nutilize 13 views for training and reserved 1 for testing.\n6\n1\n200 400 600 800 100012001400160018002000\nIterations\n22\n24\n26\n28\nPSNR\nNTC\nw/o Hash enc.\nw/o Warm-up\nDirect Opt.\nFigure 5. Comparison of different approaches for modeling the\ntransformation of 3DGs. Conducted on the second frame of the\nflame salmon video, utilizing identical initial 3DGs.\n1\n50\n100\n150\n200\n250\n299\nFrame Index\n18\n20\n22\n24\n26\n28\nPSNR\nNTC\nw/o Hash enc.\nw/o Warm-up\nDirect Opt.\nFigure 6.\nComparison of different approaches on the flame\nsalmon scene.\n5.2. Implementation\nWe implement 3DGStream upon the codes of 3D Gaussian\nSplatting (3DG-S) [26], and implement the Neural Trans-\nformation Cache (NTC) using tiny-cuda-nn [38]. For the\ntraining of initial 3DGs, we fine-tune the learning rates on\nthe N3DV dataset based on the default settings of 3DG-S,\nand apply them to the Meet Room dataset. For all scenes,\nwe train the NTC for 150 iterations in Stage 1. and train the\nadditional 3DGs for 100 iterations in Stage 2. Please refer\nto Suppl. for more details.\n5.3. Comparisons\nQuantitative comparisons. Our quantitative analysis in-\nvolves benchmarking 3DGStream on the N3DV dataset and\nMeet Room dataset, comparing it with a range of repre-\nsentative methods. We take Plenoxels [20], I-NGP [40],\nVariant\nPSNR\u2191 (dB)\n#Additional 3DGs\u2193\nBaseline\n28.39\n0\nRnd. Spawn\n28.39\n971.9\nw/o Quant. Ctrl.\n28.43\n8710.8\nFull Model\n28.42\n477.7\nTable 3. Ablation study of the Adaptive 3DG Addition strategy\non the flame salmon scene. The metrics are averaged over the\nwhole sequence.\n(a) Result of Stage 1\n(b) Result of Stage 2\n(c) Ground Truth\nFigure 7. Quantitative results of the ablation study conducted\non the flame steak scene and the coffee martini scene.\nand 3DG-S [26] as representatives of fast static scene re-\nconstruction methods, training them from scratch for each\nframe. StreamRF [29], Dynamic3DG [34], and ReRF [61]\nare designed for online training in dynamic scenes. Ow-\ning to the limitations of Dynamic3DG and ReRF, which\nnecessitate foreground masks and are confined to scenes\nwith fewer objects, and their minute-level per-frame train-\ning times, we select StreamRF selected as the representa-\ntive for online training methods due to its adaptability and\ntraining feasibility on the N3DV and MeetRoom datasets.\nTo demonstrate 3DGStream\u2019s competitive image quality,\nwe drew comparisons with the quantitative results reported\nfor the N3DV dataset in the respective papers of DyN-\neRF [31], NeRFPlayer [52], HexPlane [9], K-Planes [48],\nHyperReel [1], and MixVoxels [58], all of which are meth-\nods for reconstructing dynamic scenes through offline train-\ning on entire video sequences.\nIn Tab. 1, we present the averaged rendering speed, train-\ning time, required storage, and peak signal-to-noise ratio\n(PSNR) over all scenes of the N3DV dataset.\nFor each\nscene, the latter three metrics are computed as averages over\nthe whole 300 frames. Besides, we provide a breakdown of\ncomparisons across all scenes within the N3DV dataset in\nthe Suppl. To demonstrate the generality of our method, we\nconducted experiments on the MeetRoom dataset, as intro-\nduced by StreamRF [29], and performed a quantitative com-\nparison against Plenoxels [20], I-NGP [40], 3DG-S [26],\nand StreamRF [29]. The results are presented in Tab. 2. As\npresented in Tabs. 1 and 2, our method demonstrates su-\nperiority through fast online training and real-time render-\n7\nStep\nOverhead (ms)\nFPS\nRender w/o NTC\n2.56\n390\n+ Query NTC\n0.62\n+ Transformation\n0.02\n+ SH Rotation\n1.46\nTotal\n4.66\n215\nTable 4.\nRendering profilling for the flame salmon scene at\nmegapixel resolution. Note that flame salmon is the most time-\nconsuming to render of all scenes in our experiments.\ning, concurrently maintaining a competitive edge in terms\nof model storage and image quality. Furthermore, among\nthe methods capable of streaming FVVs, our model requires\nthe minimal model storage.\nQualitative comparisons. While our approach primarily\naims to enhance the efficiency of online FVV construction,\nas illustrated in Tabs. 1 and 2, it still achieves competitive\nimage quality. In Fig. 4, we present a qualitative compar-\nison with I-NGP [40], HyperReel [1], and StreamRF [29]\nacross scenes on the N3DV dataset [31] and the Meet Room\ndataset [29], with a special emphasis on dynamic objects\nsuch as faces, hands, and tongs, as well as intricate objects\nlike labels and statues. It is evident that our method faith-\nfully captures the dynamics of the scene without sacrificing\nthe ability to reconstruct intricate objects. Please refer to\nour project page for more video results.\n5.4. Evaluations\nNeural Transformation Cache.\nWe utilize distinct ap-\nproaches to model the transformations of 3DGs from the\nfirst to the second frame within the flame salmon video of\nthe N3DV dataset to show the effectiveness of NTC. Fig. 5\nshows that, without multi-resolution hash encoding (w/o\nHash enc.), the MLP faces challenges in modeling transfor-\nmations effectively. Additionally, without the warm-up (w/o\nWarm-up), it takes more iterations for convergence. Be-\nsides, even when compared with the direct optimization of\nthe previous frame\u2019s 3DGs (Direct Opt.), NTC demonstrate\non-par performance. In Fig. 6, We present the results of\ndifferent approaches applied across the entire flame salmon\nvideo, excluding the first frame (i.e., Frame 0). w/o Hash\nenc. and w/o Warm-up. are not able to converge swiftly, re-\nsulting in accumulating errors as the sequence progresses.\nDirect Opt. yields the best outcomes but at the cost of in-\nflated storage. Utilizing NTC, in contrast, delivers compa-\nrable results with substantially lower storage overhead by\neliminating the need for saving all the 3DGs.\nAdaptive 3DG Addition. Tab. 3 presents the quantita-\ntive results of the ablation study conducted on the flame\nsalmon scene, and more results are presented in Suppl. The\nbase model without Stage 2, and a set of randomly spawned\n3DGs (Rnd. Spawn) in equivalent quantities to our spawn\nstrategy, both fail to capture emerging objects. The vari-\nant without our quantity control strategy (w/o Quant. Ctrl.)\nmanages to model emerging objects but requires a signifi-\ncantly larger number of additional 3DGs. In contrast, our\nfull model proficiently reconstructs emerging objects using\na minimal addition of 3DGs. The ablation study illustrated\nin Fig. 7 qualitatively showcases the effect of the Adap-\ntive 3DG Addition strategy, highlighting its ability to re-\nconstruct the objects not present in the initial frame, such as\ncoffee in a pot, a dog\u2019s tongue, and flames.\nReal-time Rendering. Following 3DG-S [26], we em-\nploy the SIBR framework [6] to measure the rendering\nspeed. Once all resources required are loaded onto the GPU,\nthe additional overhead of our approach is primarily the\ntime taken to query the NTC and transform the 3DGs. As\ndetailed in Tab. 4, our method benefits from the efficiency\nof the multi-resolution hash encoding and the fully-fused\nMLP [38], which facilitate rapid NTC query. Notably, the\nmost time-consuming step is the SH Rotation. However, our\nexperiments indicate that the SH rotation has a minimal im-\npact on the reconstruction quality, which may be attributed\nto the 3DGs modeling view-dependent colors through alter-\nnative mechanisms (e.g., small 3DGs of varying colors sur-\nrounding the object) rather than SH coefficients. Nonethe-\nless, we maintain SH rotation for theoretical soundness.\n6. Discussion\nThe quality of 3DG-S [26] on the initial frame is crucial to\n3DGStream. Therefore, we inherit the limitations of 3DG-\nS, such as high dependence on the initial point cloud. As\nillustrated in Fig. 7, there are obvious artifacts beyond the\nwindows, attributable to COLMAP\u2019s [49] inability to re-\nconstruct distant landscapes. Hence, our method stands to\nbenefit directly from future enhancements to 3DG-S. More-\nover, for efficient on-the-fly training, we limit the number of\ntraining iterations, which restricts modeling of drastic mo-\ntion in Stage 1 and complex emerging objects in Stage 2.\n7. Conclusion\nWe propose 3DGStream, an novel method for efficient\nFree-Viewpoint Video streaming. Based on 3DG-S [26],\nwe utilizes an effective Neural Transformation Cache to\ncapture the motion of objects.\nIn addition, we pro-\npose an Adaptive 3DG Addition strategy to accurately\nmodel emerging objects in dynamic scenes. The two-stage\npipeline of 3DGStream enables the on-the-fly reconstruc-\ntion of dynamic scenes in video streams.\nWhile ensur-\ning photo-realistic image quality, 3DGStream achieves on-\nthe-fly training (\u223c10s per-frame) and real-time rendering\n(\u223c200FPS) at megapixel resolution with moderate requisite\nstorage. Extensive experiments demonstrate the efficiency\nand effectiveness of 3DGStream.\n8\n8. Acknowledgement\nThis work was supported in part by Zhejiang Province Pro-\ngram (2022C01222, 2023C03199, 2023C03201), the Na-\ntional Program of China (62172365, 2021YFF0900604,\n19ZDA197), Ningbo Science and Technology Plan Project\n(022Z167, 2023Z137), and MOE Frontier Science Center\nfor Brain Science & Brain-Machine Integration (Zhejiang\nUniversity).\n9\n3DGStream: On-the-fly Training of 3D Gaussians for Efficient Streaming of\nPhoto-Realistic Free-Viewpoint Videos\nSupplementary Material\n9. Implementation Details\nWe implement 3DGStream upon the codebase of 3D Gaus-\nsian Splatting (3DG-S) [26] and use tiny-cuda-nn [38] to\nimplement Neural Transformation Cache (NTC). All exper-\niments were conducted on an NVIDIA RTX 3090 GPU.\nIn training of the initial frame, we let the densification of\n3DG-S end at iteration 5000. For the scenes in the N3DV\ndataset, we use the 3DGs of iteration 15000 as the initial\n3DGs, while for the scenes in the Meet Room dataset, we\nuse the results of iteration 10000. For convenience, we set\nthe maximum degree of spherical harmonics (SH) to 1, and\nall other hyperparameters are consistent with the 3DG-S.\nTraining NTC. We set the learning rate of NTC to 0.002.\nFor the scenes in the N3DV dataset [31], the hash table size\nof the multi-resolution hash encoding is 215, the feature vec-\ntor dimension is 4, and there are 16 resolution levels. For\nthe Meet Room dataset [29], the hash table length is 214,\nwith all other hyperparameters matching those specified for\nthe N3DV dataset.\nFor all scenes, our fully-fused MLP\ncomprises 2 hidden layers with 64 neurons each, employ-\ning ReLu as the activation function. Given that the N3DV\ndataset and the Meet Room dataset both record indoor dy-\nnamic scenes, and multi-resolution hash encoding requires\nnormalized coordinates for input, we create an axis-aligned\nbounding box that roughly encloses the house to normalize\nthe 3D points and discard any points outside the bounding\nbox to prevent distant landscapes from influencing the train-\ning.\nTraining the additional 3DGs. Compared to training on\nthe initial frame, we increase the learning rate in the sec-\nond stage for faster convergence. Specifically, the learn-\ning rates for the mean, SH coefficient, opacity value, scal-\ning vector, and rotation quaternion of the 3DGs are set to\n0.0024, 0.0375, 0.75, 0.075, and 0.015, respectively. Note\nthat these learning rates were not individually fine-tuned;\ninstead, their proportions are following the default settings\nof 3DG-S.\n10. SH Rotation\nIn order to preserve theoretical soundness, we also rotate\nthe SH after transforming the 3DGs. The zeroth-degree SH\ndoes not require rotation; therefore, we only need to rotate\nthe first-degree SH coefficients.\nWe utilize the projection function [51] to project normal\nvectors onto the first-order SH. Given a rotation matrix R,\nwe seek a matrix M that can rotate the first-degree SH. Be-\nMethod\nCoffee\nCook\nCut\nFlame\nFlame\nSear\nMean\nMartini\nSpinach\nBeef\nSalmon\nSteak\nSteak\nPlenoxels [20]\u2020\n27.65\n31.73\n32.01\n28.68\n32.24\n32.33\n30.77\nI-NGP [40]\u2020\n25.19\n29.84\n30.73\n25.51\n30.04\n30.40\n28.62\n3DG-S [26]\u2020\n27.78\n34.10\n34.03\n28.66\n34.41\n33.48\n32.08\nDyNeRF [31]\n\u2013\n\u2013\n\u2013\n29.58\n\u2013\n\u2013\n29.58\nNeRFPlayer [52]\n31.53\n30.58\n29.35\n31.65\n31.93\n29.13\n30.69\nHexPlane [9]\n\u2013\n32.04\n32.55\n29.47\n32.08\n32.39\n31.70\nK-Planes [48]\n29.99\n32.60\n31.82\n30.44\n32.38\n32.52\n31.63\nHyperReel [1]\n28.37\n32.30\n32.92\n28.26\n32.20\n32.57\n31.10\nMixVoxels [58]\n29.36\n31.61\n31.30\n29.92\n31.43\n31.21\n30.80\nStreamRF [29]\u2020\n27.84\n31.59\n31.81\n28.26\n32.24\n32.36\n28.26\nOurs\n27.75\n33.31\n33.21\n28.42\n34.30\n33.01\n31.67\nTable 5. Quantitative comparison of PSNR values across all\nscenes in the N3DV dataset, with the metric for each scene cal-\nculated as the average over 300 frames.\n\u2020Obtained in our own\nexperiments with the official codes.\nMethod\nCoffee\nCook\nCut\nFlame\nFlame\nSear\nMean\nMartini\nSpinach\nBeef\nSalmon\nSteak\nSteak\nBaseline\n27.68\n33.19\n33.10\n28.39\n33.54\n32.79\n31.45\nFull Model\n27.75\n33.31\n33.21\n28.42\n34.30\n33.01\n31.67\nTable 6. Ablation Study of the Adaptive 3DG Addition strategy\nacross all scenes in the N3DV dataset, with the metric for each\nscene calculated as the average over 300 frames. We take PSNR\nto measure the image quality.\ncause rotating a vector before projecting it to SH produces\nthe same outcome as projecting the vector first and then ro-\ntating the SH, we have the following relationship:\nMP(N) = P(RN),\n(8)\nwhere N is a normal vector. For any three normal vectors\nN0, N1, and N2 we denote A = [P(N0), P(N1), P(N2)].\nConsequently, we obtain:\nMA = [P(RN0), P(RN1), P(RN2)].\n(9)\nAnd hence:\nM = [P(RN0), P(RN1), P(RN2)]A\u22121\n(10)\nFor\ncomputational\nconvenience,\nwe\nchoose\nN0\n=\n[1, 0, 0]T , N1 = [0, 1, 0]T , and N2 = [0, 0, 1]T .\n11. More Results\n11.1. Quantitative Results\nWe provide a quantitative comparison of image quality,\nmeasured by PSNR, across all scenes in the N3DV dataset\n10\nStep\nOverhead (ms)\nFPS\nRender w/o NTC\n1.75\n571\n+ Query NTC\n+0.46\n+ Transformation\n+0.02\n+ SH Rotation\n+1.24\nTotal\n3.47\n288\nTable 7. Rendering profilling on the Meet Room dataset.\nDataset\nNTC (KB)\nNew 3DGs (KB)\nTotal (KB)\nN3DV\n7781.5\n49.1\n7830.6\nMeetRoom\n3941.5\n195.3\n4136.8\nTable 8. Detailed \u201cStorage\u201d entry of our method in Tabs. 1 and\n2.\n1\n50\n100\n150\n200\n250\n299\nFrame Index\n200\n300\n400\n500\n600\n700\n800\n900\nNumber of Additional 3DGs\n20.93\n31.08\n41.24\n51.40\n61.55\n71.71\n81.86\n92.02\nStorage (KB)\nFigure 8. Number of additional 3DGs and corresponding stor-\nage requirement of each frame on the flame salmon scene.\nin Tab. 5. Furthermore, we provide the quantitative result\nof the ablation study across all scenes in the N3DV dataset\nin Tab. 6, Additionally, we provide rendering profilling on\nthe Meet Room Dataset in Tab. 7.\n11.2. Qualitative Results\nWe provide videos to show the free view synthesis results\non various scenes from the N3DV dataset in https://\nsjojok.github.io/3dgstream.\n12. More Evaluations\n12.1. Storage Requirements\nExcept the initial 3DGs, we only need to store per-frame\nNTCs and per-frame additional 3DGs for each FFV frame,\nas detailed in Tab. 8.\n12.2. Quantity of 3DGs\nIn our experiments on the N3DV datasets, the quantity of\ninitial 3DGs (i.e., the transformed ones) is on the order of\n105, while the quantity of frame-specific additional 3DGs\nis on the order of 102. We show how the number of the\nScene\nStage 1\nStage 2\n150\n250\n100\n200\nFlame Salmon\n28.39\n28.44\n28.46\n28.46\nFlame Steak\n33.54\n33.81\n34.44\n34.46\nSear Steak\n32.79\n33.02\n33.18\n33.19\nCook Spinach\n33.19\n33.50\n33.56\n33.57\nCut Roasted Beef\n33.10\n33.39\n33.44\n33.44\nCoffee Martini\n27.68\n27.77\n27.83\n27.83\nTable 9. Evaluation on the impact of training iterations con-\nducted on the N3DV dataset. The result of Stage 2 is is obtained\nafter 250 iterations of optimization at Stage 1. We take PSNR to\nmeasure the image quality.\nframe-specific additional 3DGs changes as the frame num-\nber increases in Fig. 8.\n12.3. Impact of Training Iterations\nIn Sec. 6, we discuss the trade-off between training ef-\nficiency and reconstruction quality, noting that limiting\nthe number of training iterations enables efficient on-the-\nfly training at the expense of reduced quality.\nTo show\nthis trade-off, we conduct experiments to evaluate the im-\npact of training iterations, and show the quantitative results\nin Tab. 9. As shown in Tab. 9, increasing training itera-\ntions in Stage 1 significantly enhances the reconstruction\nquality. However, an additional 100 iterations result in an\nincrement of 3 seconds in the per-frame training duration.\nIncrementing training iterations in the second stage has a\nminimal impact on quality, which can be attributed to the\nhigher learning rate employed in this phase and the smaller\nnumber of additional 3DGs, facilitating rapid convergence.\nReferences\n[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16610\u201316620, 2023. 1, 3, 6, 7, 8, 10\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 2\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022.\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\n11\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 2\n[5] Michael J Black and Paul Anandan. The robust estimation\nof multiple motions: Parametric and piecewise-smooth flow\nfields. Computer vision and image understanding, 63(1):75\u2013\n104, 1996. 4\n[6] Sebastien Bonopera, Jerome Esnault, Siddhant Prakash,\nSimon Rodriguez, Theo Thonat, Mehdi Benadel, Gaurav\nChaurasia, Julien Philip, and George Drettakis. sibr: A sys-\ntem for image based rendering, 2020. 8\n[7] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-\nson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay\nBusch, Matt Whalen, and Paul Debevec. Immersive light\nfield video with a layered mesh representation. ACM Trans-\nactions on Graphics (TOG), 39(4):86\u20131, 2020. 1, 2\n[8] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\nGortler, and Michael Cohen. Unstructured lumigraph ren-\ndering. In SIGGRAPH, pages 425\u2013432, 2001. 2\n[9] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. CVPR, 2023. 3, 6, 7, 10\n[10] Jin-Xiang Chai, Xin Tong, Shing-Chow Chan, and Heung-\nYeung Shum. Plenoptic sampling. In Proceedings of the\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 307\u2013318, 2000. 2\n[11] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 14124\u201314133, 2021. 2\n[12] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 2\n[13] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Dictionary fields: Learning a neural\nbasis decomposition. ACM Trans. Graph., 2023. 2\n[14] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures. In The Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[15] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo.\nACM Transactions on Graphics (TOG), 34(4):69,\n2015. 1, 2\n[16] Abe Davis, Marc Levoy, and Fredo Durand. Unstructured\nlight fields.\nComput. Graph. Forum, 31(2pt1):305\u2013314,\n2012. 2\n[17] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh\nKhamis, Adarsh Kowdle, Christoph Rhemann, Vladimir\nTankovich, and Shahram Izadi. Motion2fusion: Real-time\nvolumetric performance capture. ACM Trans. Graph., 36(6):\n246:1\u2013246:16, 2017. 1, 2\n[18] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wen-\nzheng Chen, and Baoquan Chen.\n4d gaussian splatting:\nTowards efficient novel view synthesis for dynamic scenes.\narXiv preprint arXiv:2402.03307, 2024. 4\n[19] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nIn SIGGRAPH Asia 2022 Conference Papers, 2022. 1, 3\n[20] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2, 3, 6, 7, 10\n[21] Stephan J. Garbin, Marek Kowalski, Matthew Johnson,\nJamie Shotton, and Julien Valentin. Fastnerf: High-fidelity\nneural rendering at 200fps. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n14346\u201314355, 2021. 2\n[22] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F. Cohen. The lumigraph. In Proceedings of the\n23rd Annual Conference on Computer Graphics and Inter-\nactive Techniques, page 43\u201354, New York, NY, USA, 1996.\nAssociation for Computing Machinery. 2\n[23] Peter Hedman,\nPratul P. Srinivasan,\nBen Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural radi-\nance fields for real-time view synthesis. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n5855\u20135864, 2021. 2\n[24] Berthold KP Horn and Brian G Schunck. Determining opti-\ncal flow. Artificial intelligence, 17(1-3):185\u2013203, 1981. 4\n[25] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,\nXiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation\nfor efficient anti-aliasing neural radiance fields. In ICCV,\n2023. 2\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 1, 2, 4, 5, 6, 7, 8, 10\n[27] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim\nWalter, and Matthias Nie\u00dfner. Nersemble: Multi-view ra-\ndiance field reconstruction of human heads. arXiv preprint\narXiv:2305.03027, 2023. 3\n[28] Marc Levoy and Pat Hanrahan. Light field rendering. In\nSIGGRAPH, pages 31\u201342, 1996. 2\n[29] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping\nTan. Streaming radiance fields for 3d video synthesis. In\nNeurIPS, 2022. 1, 2, 3, 6, 7, 8, 10\n[30] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollh\u00a8ofer,\nJ\u00a8urgen Gall, Angjoo Kanazawa, and Christoph Lassner.\nTava: Template-free animatable volumetric actors. In Eu-\nropean Conference on Computer Vision, pages 419\u2013436.\nSpringer, 2022. 3\n[31] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5521\u20135531, 2022. 1, 2,\n3, 6, 7, 8, 10\n[32] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\n12\non Computer Vision and Pattern Recognition (CVPR), pages\n6498\u20136508, 2021. 3\n[33] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 1,\n3\n[34] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis. In 3DV, 2024. 3, 7\n[35] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,\nJonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. NeRF in the Wild: Neural Radiance Fields for Un-\nconstrained Photo Collections. In CVPR, 2021. 2\n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European conference on computer vision, pages\n405\u2013421. Springer, 2020. 1, 2\n[37] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\nPratul P Srinivasan, and Jonathan T Barron. Nerf in the dark:\nHigh dynamic range view synthesis from noisy raw images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16190\u201316199, 2022.\n2\n[38] Thomas M\u00a8uller. tiny-cuda-nn, 2021. 4, 5, 7, 8, 10\n[39] Thomas M\u00a8uller, Fabrice Rousselle, Jan Nov\u00b4ak, and Alexan-\nder Keller. Real-time neural radiance caching for path trac-\ning.\nACM Transactions on Graphics (TOG), 40(4):1\u201316,\n2021. 4\n[40] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 1, 2, 4, 6, 7, 8, 10\n[41] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5480\u20135490, 2022. 2\n[42] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 5865\u20135874, 2021. 3\n[43] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 1\n[44] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), 2021. 3\n[45] Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun\nAhn, Ji-Yeon Kim, and Nahyup Kang. Temporal interpola-\ntion is all you need for dynamic neural radiance fields. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4212\u20134221, 2023. 3\n[46] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 1, 3\n[47] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Pe-\nter Hedman. Merf: Memory-efficient radiance fields for real-\ntime view synthesis in unbounded scenes. ACM Transactions\non Graphics (TOG), 42(4):1\u201312, 2023. 2\n[48] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR, 2023. 1, 3, 6, 7, 10\n[49] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 8\n[50] Heung-Yeung Shum and Li-Wei He. Rendering with con-\ncentric mosaics.\nIn Proceedings of the 26th annual con-\nference on Computer graphics and interactive techniques,\npages 299\u2013306, 1999. 2\n[51] Peter-Pike Sloan. Stupid spherical harmonics (sh) tricks. In\nGame developers conference, page 42, 2008. 10\n[52] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n3, 6, 7, 10\n[53] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5449\u20135459,\n2022. 2, 3\n[54] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li,\nBoyan Ji, Lei Zhao, and Wei Xing. Vgos: Voxel grid opti-\nmization for view synthesis from sparse inputs. In Proceed-\nings of the Thirty-Second International Joint Conference on\nArtificial Intelligence, IJCAI-23, pages 1414\u20131422. Interna-\ntional Joint Conferences on Artificial Intelligence Organiza-\ntion, 2023. Main Track. 2\n[55] Carlo Tomasi and Takeo Kanade. Shape and motion from\nimage streams under orthography: a factorization method.\nInternational journal of computer vision, 9:137\u2013154, 1992.\n4\n[56] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 12959\u201312970, 2021. 3\n[57] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. CVPR, 2022. 2\n13\n[58] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei\nSong, and Huaping Liu. Mixed neural voxels for fast multi-\nview video synthesis. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 19706\u2013\n19716, 2023. 3, 6, 7, 10\n[59] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13524\u201313534, 2022. 3\n[60] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu,\nTinne Tuytelaars, Lan Xu, and Minye Wu. Neural residual\nradiance fields for streamably free-viewpoint videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 76\u201387, 2023. 3\n[61] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu,\nTinne Tuytelaars, Lan Xu, and Minye Wu. Neural residual\nradiance fields for streamably free-viewpoint videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 76\u201387, 2023. 1, 7\n[62] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah Snavely.\nTracking everything everywhere all at once. In International\nConference on Computer Vision, 2023. 3\n[63] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n16210\u201316220, 2022. 3\n[64] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel\nCremers. Behind the scenes: Density fields for single view\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9076\u2013\n9086, 2023. 2\n[65] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8534\u20138543, 2021. 2\n[66] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 4\n[67] Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf:\nRegularizing neural radiance fields with denoising diffu-\nsion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4180\u2013\n4189, 2023. 2\n[68] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 9421\u2013\n9431, 2021. 1\n[69] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ra-\nmanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building\nanimatable 3d neural models from many casual videos. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2863\u20132873, 2022. 3\n[70] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-\nproving few-shot neural rendering with free frequency reg-\nularization.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8254\u2013\n8263, 2023. 2\n[71] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 4\n[72] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-\ntime photorealistic dynamic scene representation and render-\ning with 4d gaussian splatting. 2024. 4\n[73] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[74] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021. 2\n[75] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang\nZhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently gen-\nerated human radiance field from sparse inputs. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 7743\u20137753, 2022. 3\n[76] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\nSimon Winder, and Richard Szeliski.\nHigh-quality video\nview interpolation using a layered representation.\nACM\ntransactions on graphics (TOG), 23(3):600\u2013608, 2004. 1,\n2\n[77] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa volume splatting. In Proceedings Visu-\nalization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001. 4\n14\n"
  }
]