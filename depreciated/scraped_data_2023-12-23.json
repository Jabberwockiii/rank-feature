[
  {
    "title": "AppAgent: Multimodal Agents as Smartphone Users",
    "link": "https://arxiv.org/pdf/2312.13771.pdf",
    "upvote": "49",
    "text": "AppAgent: Multimodal Agents as Smartphone Users\nChi Zhang\u2217\nZhao Yang\u2217\nJiaxuan Liu\u2217\nYucheng Han\nXin Chen\nZebiao Huang\nBin Fu\nGang Yu\u2020\nTencent\n{johnczhang, jayzyang, jiaxuanliu, yuchenghan, shingxchen, zebiaohuang, brianfu, skicyyu}@tencent.com\nhttps://appagent-official.github.io/\nClock\nTEMU\nObservation: The screenshot shows a photo editing app on \na smartphone with an image displayed on the screen...\nThought: To complete this task, I should adjust the noise \nreduction level using the slider. \nAction: Action: swipe(23, \"right\", \"medium\")\nObservation: The image shows a screenshot of an \nalarm application on a smartphone...\nThought: To complete this task, I should disable the \nvibration feature by tapping on the UI element. \nAction: tap(17)\nObservation: The screenshot displays a shopping app \nwith a variety of headsets listed ...\nThought: To complete this task, I should add the \nselected headset into my shopping cart.\nAction: tap(14)\nImprove the quality of \nthis image until it looks \ngood to you\nObservation: The screenshot shows the Telegram app, \na conversation from \"Agent Liu\" with a message...\nThought: To complete this task, I should input the text with \nthis answer into the input field labeled with the tag \u20187\u2019\nAction: text(\"The Toronto Raptors won the 2019 NBA ... \")\nSet an alarm at 12:30 pm \nevery Friday and Sunday, \nand disable the vibration\nSearch for a gaming \nheadset and add it to my \nshopping cart.\nSend an email to \njanedoe@email.com to \nask her about her new job\nSearch music video song \nWonderful Tonight and \nleave a praising comment \nChat with the user \nAgent Liu and \nanswer his question\nObservation: The screenshot shows a YouTube search \nresult for \"Wonderful Tonight\" ...\nThought: To complete this task, I should insert a \npraising comment into the text input field labeled \u20182\u2019.\nAction: text(\u201cThis is such a timeless piece...\")\nObservation: The screenshot shows a smartphone's \nemail application ...\nThought: To complete this task, I should type the \ncontent of the email asking Jane Doe about her new job.\nAction: text(\u201cDear Jane Doe,\\n\\nI am writing to ...\")\nFigure 1: Diverse applications of our multimodal agent framework for smartphone App operation. We\nevaluate the effectiveness of our agent model on 50 tasks across 10 different Apps, highlighting its adaptability and\neffectiveness in a real-world context.\nAbstract\nRecent advancements in large language models\n(LLMs) have led to the creation of intelligent\nagents capable of performing complex tasks.\nThis paper introduces a novel LLM-based mul-\ntimodal agent framework designed to operate\nsmartphone applications. Our framework en-\nables the agent to operate smartphone applica-\n*Equal contributions.\n\u2020Corresponding Author.\ntions through a simplified action space, mim-\nicking human-like interactions such as tapping\nand swiping. This novel approach bypasses\nthe need for system back-end access, thereby\nbroadening its applicability across diverse apps.\nCentral to our agent\u2019s functionality is its in-\nnovative learning method. The agent learns\nto navigate and use new apps either through\nautonomous exploration or by observing hu-\nman demonstrations. This process generates\n1\narXiv:2312.13771v2  [cs.CV]  22 Dec 2023\na knowledge base that the agent refers to for\nexecuting complex tasks across different appli-\ncations. To demonstrate the practicality of our\nagent, we conducted extensive testing over 50\ntasks in 10 different applications, including so-\ncial media, email, maps, shopping, and sophis-\nticated image editing tools. The results affirm\nour agent\u2019s proficiency in handling a diverse\narray of high-level tasks.\n1\nIntroduction\nThe emergence of large language models (LLMs),\nsuch as ChatGPT (OpenAI, 2021) and GPT-4 (Ope-\nnAI, 2023), marks a significant milestone in the\nfield of artificial intelligence and natural language\nprocessing. These advanced models represent a\nfundamental change in how machines understand\nand generate human language, exhibiting a level of\nsophistication and versatility previously unattain-\nable. One of the most exciting developments in\nthis field is the capability of LLMs to function not\njust as language processors, but as agents capable\nof performing complex tasks. This evolution is ev-\nident in initiatives such as AutoGPT (Yang et al.,\n2023a) and MetaGPT (Hong et al., 2023), which\nshowcase the practical applications of LLMs in\ntasks requiring advanced cognitive functions like\nreasoning, planning, and collaboration. The signifi-\ncance of these developments cannot be overstated,\nas they extend the utility of LLMs beyond simple\nlanguage tasks, revolutionizing various aspects of\ntechnology and daily life.\nHowever, a key limitation of these LLM-based\nagents has been their reliance solely on text-based\ninformation. This restriction has historically cur-\ntailed their perception and interaction with their\nenvironment. The introduction of models equipped\nwith vision capabilities, such as the latest itera-\ntion of GPT-4, marks a pivotal breakthrough. By\nintegrating the ability to process and interpret vi-\nsual information, these models can now understand\naspects of their surroundings that are difficult or\nimpossible to convey through text alone. This ex-\ntended capability enables LLMs to interpret con-\ntext, recognize patterns, and respond to visual cues,\nthus providing a more holistic and interactive expe-\nrience with the world.\nIn our work, we focus on building a multimodal\nagent leveraging the vision capabilities of multi-\nmodal large language models to undertake tasks\npreviously unachievable by text-only agents. In\nparticular, we explore an interesting but challeng-\ning application that builds an agent to operate any\nsmartphone application (App) in the mobile oper-\nating system. Our approach differs significantly\nfrom existing intelligent phone assistants like Siri,\nwhich operate through system back-end access and\nfunction calls. Instead, our agent interacts with\nsmartphone apps in a human-like manner, using\nlow-level operations such as tapping and swiping\non the graphical user interface (GUI). The proposed\nagent offers multiple advantages. Firstly, it elimi-\nnates the need for system back-end access, making\nour agent universally applicable across various ap-\nplications. Additionally, this approach enhances\nsecurity and privacy, as the agent does not require\ndeep system integration. Furthermore, by operating\non the GUI level, our agent can adapt to changes\nin app interfaces and updates, ensuring long-term\napplicability and flexibility.\nHowever, creating a multimodal agent capable\nof operating diverse smartphone apps presents sig-\nnificant challenges. Existing research indicates that\nadapting current models for embodied tasks ne-\ncessitates extensive training data, and collecting a\nlarge dataset of app demonstrations for training is\na formidable task. Moreover, different apps have\nunique GUIs with varying icon meanings and op-\nerational logic, and it remains uncertain whether\nthese adapted models can effectively generalize to\nunseen apps.\nIn this paper, we introduce a multimodal agent\nframework aimed at operating any smartphone app\nlike human users. The learning of our framework\ninvolves an exploration phase where the agent in-\nteracts autonomously with apps through a set of\npre-defined actions and learns from their outcomes.\nThese interactions are documented, which assists\nthe agent in navigating and operating the apps. This\nlearning process can be accelerated by observing\na few human demonstrations. Following this ex-\nploratory phase, the agent can operate the app by\nconsulting the constructed document based on its\ncurrent state, eliminating the need to adapt the pa-\nrameters of the LLMs or collect extensive training\ndata for each app.\nTo validate its effectiveness, we tested our agent\non 50 tasks across 10 different apps, ranging from\nsocial media and messaging to email, maps, shop-\nping, and even complex image editing apps. Both\nquantitative results and user studies underscore the\nadvantages of our design, particularly its adapt-\nability, user-friendliness, and efficient learning and\n2\noperating capabilities across a wide range of appli-\ncations. This underlines the potential of our agent\nas a versatile and effective tool in the realm of\nsmartphone app operation.\nIn summary, this paper makes the following con-\ntributions:\n\u2022 We open-source a multimodal agent frame-\nwork, focusing on operating smartphone ap-\nplications with our developed action space.\n\u2022 We propose an innovative exploration strategy,\nwhich enables the agent to learn to use novel\napps.\n\u2022 Through extensive experiments across multi-\nple apps, we validate the advantages of our\nframework, demonstrating its potential in the\nrealm of AI-assisted smartphone app opera-\ntion.\n2\nRelated Work\n2.1\nLarge language models\nThe development of ChatGPT (OpenAI, 2021) and\nGPT-4 (OpenAI, 2023) represents a crucial ad-\nvancement in natural language processing. Unlike\nearlier large language models (LLMs), these new\nmodels (Touvron et al., 2023a,b; Zeng et al., 2022;\nTaori et al., 2023; Zheng et al., 2023) enable multi-\nround conversations and have the impressive ability\nto follow complex instructions. The integration of\nvision capabilities in GPT-4V (Yang et al., 2023b)\nis a further milestone, enabling the language model\nto process and interpret visual data. This addition\nhas broadened the scope of potential AI applica-\ntions, allowing GPT-4 to undertake diverse tasks\nsuch as problem-solving, logical reasoning, tool\nusage, API calls, and coding. Recent studies (Yang\net al., 2023c; Yan et al., 2023) have shown that\nGPT-4V can understand various types of images,\nincluding simple user interfaces (UIs) in popular\nsmartphone apps. However, challenges arise when\nthe apps are new and their UIs are less typical,\nwhich highlights a major problem that our work\naims to address. Among open-source efforts from\nthe industry and research community, the LLaMA\nseries (Touvron et al., 2023a,b) are the most pop-\nular equivalents and have been fine-tuned to ac-\nquire conversational abilities, employing a decoder-\nonly architecture similar to ChatGPT (Taori et al.,\n2023; Zheng et al., 2023). Building upon LLaMA,\nmany multimodal LLMs, such as LLaVA (Liu et al.,\n2023b,a), ChartLlama (Han et al., 2023), and Sta-\nbleLLaVA (Li et al., 2023), also demonstrate vision\nunderstanding capabilities akin to those of GPT-4V.\nNevertheless, a performance gap persists between\nthese open-source models and GPT-4V, suggesting\npotential areas for further development.\n2.2\nLLMs as agents\nThe use of LLMs as agents for executing com-\nplex tasks has gained increasing attention. Ini-\ntiatives like AutoGPT (Yang et al., 2023a), Hug-\ngingGPT (Shen et al., 2023), and MetaGPT (Hong\net al., 2023) illustrate this trend, and many projects\ndemonstrate impressive capabilities, moving be-\nyond basic language tasks to engaging in activi-\nties requiring higher cognitive functions, such as\nsoftware development (Qian et al., 2023; Chen\net al., 2021) and gaming (FAIR et al., 2022; Park\net al., 2023; Xu et al., 2023).\nIn this context,\nYao et al. (Yao et al., 2023) introduce an innova-\ntive approach that synergizes reasoning and acting\nin LLMs, significantly enhancing their decision-\nmaking and interactive capabilities. LLM-based\nagents are designed to utilize the advanced lan-\nguage and reasoning skills of LLMs to interact\nwith and manipulate their environment (Liu et al.,\n2023c; Gur et al., 2023; Xie et al., 2023). This in-\ncludes performing tasks that require understanding\ncontext, making decisions, and learning from inter-\nactions (Xi et al., 2023; Hu and Shu, 2023). Such\nagents are pivotal in applications where human-like\ncognitive abilities are essential.\nThe\nemergence\nof\nmultimodal\nLLM\nagents (Wang et al., 2023; Furuta et al., 2023;\nBrohan et al., 2022, 2023; Reed et al., 2022),\ncapable of processing various inputs including text,\nimages, audio, and video, has further broadened\nthe scope of LLM applications. This versatility\nis particularly beneficial for LLM-based agents,\nenabling them to interact more effectively with\ntheir environment and complete more complex\ntasks, be it completing household tasks in a\nphysical world (Ahn et al., 2022), generating 3D\nassets via procedural tool use (Sun et al., 2023), or\nmastering over 600 tasks across different domains\nat the same time (Reed et al., 2022). Our research\ncontributes to this area by focusing on an agent\ndesigned to operate smartphone applications. This\nagent\u2019s ability to interpret screenshots from the\noperating system demonstrates its flexibility and\nadaptability, making it a valuable tool in a wide\n3\nTap\n\ud83d\udc40\n\ud83d\udc40\n\u201cTapping this UI element opens \nthe color settings to adjust the \ncolor of the image.\u201d\n\u201cSwiping this UI element allows the \nuser to adjust the color \ntemperature of the image.\u201d\n\ud83e\udd14\n\u2026\n\u2026\nExploration Phase\nDeployment Phase: \u201cbeautify this photo for me\u2026\u201d\n\u2026\n\u2026\nObserve, Think, Act, and Summarize\nSwipe\nDoc for App\nFigure 2: Overview of our multimodal agent framework designed to operate smartphone applications. The\nfigure illustrates the two-phase approach of our framework. In the exploration phase, the agent interacts with a\nsmartphone application and learns from their outcomes to create a comprehensive reference document. In the\ndeployment phase, the agent utilizes the information compiled in this document to operate and navigate the apps\neffectively.\nrange of applications.\n3\nMethod\nThis section details the methodology behind our in-\nnovative multimodal agent framework. This frame-\nwork enables an agent to interact with smartphone\napplications in a manner akin to human behavior.\nWe first describe the experimental environment and\naction space, which are foundational elements of\nour system. Next, we discuss the exploration phase,\nwhere the agent learns app functionalities either\nthrough autonomous interactions or by observing\nhuman demonstrations. Finally, we outline the de-\nployment phase, explaining how the agent applies\nits acquired knowledge to execute high-level tasks.\n3.1\nEnvironment and Action Space\nExperimental Environment: Our experimental\nenvironment is built on a command-line interface\n(CLI), allowing the agent to interact with smart-\nphone apps. We chose the Android operating sys-\ntem for our experiments. The agent receives two\nkey inputs: a real-time screenshot showing the\napp\u2019s interface and an XML file detailing the in-\nteractive elements. To enhance the agent\u2019s ability\nto identify and interact with these elements seam-\nlessly, we assign each element a unique identifier.\nThese identifiers are derived either from the re-\nsource ID in the XML file (if provided) or are con-\nstructed by combining the class name, size, and\ncontent of the element. These elements are over-\nlaid as semi-transparent numbers on the screenshot.\nThis helps the agent to interact accurately without\nneeding to specify exact positions on the screen\nand enhances the agent\u2019s precision in controlling\nthe phone.\nAction Space: Our agent\u2019s action space mir-\nrors common human interactions with smartphones:\ntaps and swipes. We designed four basic functions:\n\u2022 Tap(element : int) : This function simu-\nlates a tap on the UI element numbered on\nthe screen. For example, tap(5) would tap\nthe element labeled \u20185\u2019.\n\u2022 Long_press(element : int) : This func-\ntion emulates a long press (for 1 second) on a\nUI element.\n\u2022 Swipe ( element : int, direction : str,\ndist : str): It allows the agent to swipe on\nan element in a specified direction (up, down,\nleft, right) and distance (short, medium, long).\nFor instance,\nswipe(21, \u201cup\u201d, \u201cmedium\u201d)\nwould swipe up on element \u201821\u2019 for a medium\ndistance.\n\u2022 Text(text : str) : To bypass inefficient vir-\ntual keyboard typing, this function inputs\ntext directly into an input field when a\nvirtual keyboard is visible.\nFor exam-\nple, text(\u201cHello, world!\u201d) inputs the string\n\u201cHello, world!\".\n\u2022 Back() : A system-level function that helps\nthe agent return to the previous UI page, espe-\ncially useful for exiting irrelevant pages.\n4\n\u2022 Exit() : A specialized function is employed\nto conclude processes, typically invoked upon\nsuccessful task completion.\nThese predefined actions are designed to sim-\nplify the agent\u2019s interactions, particularly by elim-\ninating the need for precise screen coordinates,\nwhich can pose challenges for language models\nin accurately predicting.\n3.2\nExploration Phase\nExploring by autonomous interactions. The Ex-\nploration Phase is central to our framework. Here,\nthe agent learns about the functionalities and fea-\ntures of smartphone apps through trial and error. In\nthis phase, the agent is assigned a task and starts\ninteracting autonomously with the UI elements. It\nuses different actions and observes the resulting\nchanges in the app interface to understand how\nit works. The agent, driven by a large language\nmodel, attempts to figure out the functions of UI\nelements and the effects of specific actions by an-\nalyzing screenshots before and after each action.\nThis information is compiled into a document that\nrecords the effects of actions applied to different el-\nements. When a UI element is acted upon multiple\ntimes, the agent will update the document based\non past documents and current observations to im-\nprove quality. To make exploration more efficient,\nthe agent stops further exploring UI elements if the\ncurrent UI page seems unrelated to the main tasks\nof the app, like advertisement pages. In such cases,\nit uses the Android system\u2019s Back() function to\nreturn to the previous UI page. Compared with\nrandom exploration, such as Depth-First Search\nand Breadth-First Search, this goal-oriented explo-\nration approach ensures that the agent focuses on\nelements crucial for the effective operation of the\napp. The agent also utilizes the LLM\u2019s existing\nknowledge about user interfaces to improve explo-\nration efficiency. The exploration stops when the\nagent completes the assigned task.\nExploring by watching demos. An alternative\nand often more effective exploration method in-\nvolves the agent observing human demonstrations.\nThese demonstrations provide the agent with ex-\namples of efficient app usage, especially for un-\nderstanding complex functionalities that might be\nchallenging to discover through autonomous inter-\nactions. In this method, a human user operates\nthe apps while the agent observes, recording only\nthe elements and actions employed by the human.\nThis strategy narrows down the exploration space\nand prevents the agent from engaging with irrele-\nvant app pages, making it a more streamlined and\nefficient approach compared to autonomous inter-\nactions.\n3.3\nDeployment Phase\nFollowing the exploration phase, the agent is well-\nequipped to execute complex tasks based on its\naccrued experience. The agent adheres to a step-\nby-step approach when given a task, with each step\nencompassing access to a screenshot of the current\nUI and a dynamically generated document detail-\ning the functions of UI elements and the actions\u2019\neffects on the current UI page. The prompts also\nprovide detailed explanations of all available ac-\ntions. In each step, the agent is first tasked with\nproviding its observations of the current UI, fol-\nlowed by articulating its thought process concern-\ning the task and current observations. Subsequently,\nthe agent proceeds to execute actions by invoking\navailable functions. After each action, the agent\nsummarizes the interaction history and the actions\ntaken during the current step. This information is\nincorporated into the next prompt, which provides\nthe agent with a form of memory. This meticulous\napproach enhances the reliability and interpretabil-\nity of the agent\u2019s actions, thereby facilitating more\ninformed decision-making. The deployment phase\nstops when the agent determines that the task has\nbeen accomplished, at which point it can exit the\nprocess by taking the Exit() action.\n4\nExperiments\nIn this section, we will present our evaluation of\nthe multimodal agent framework through a combi-\nnation of quantitative and qualitative experiments.\nOur primary goal is to assess the agent\u2019s perfor-\nmance and its ability to operate a diverse set of\nsmartphone applications effectively.\n4.1\nExperimental Setup\nTo comprehensively evaluate our method, we con-\nstruct a benchmark that includes 10 popular appli-\ncations, each serving various purposes. These ap-\nplications include Google Maps, Twitter, Telegram,\nYouTube, Spotify, Yelp, Gmail, TEMU, Clock, and\nLightroom. We have intentionally chosen this di-\nverse set of apps to test the agent\u2019s adaptability\nacross various functions and interfaces. In particu-\nlar, to gain a more comprehensive insight into the\n5\nThe search field is focused and a keyboard is \npresent on the lower half of the screen. UI \nelement labeled '6' appears to be the search input \nfield where text can be input to find locations, \naddresses, or points of interest.\nObservation\nThought\nAction\ntext(\"Tencent Shanghai Branch\")\nTo complete the given task, I should enter the \ntext \"Tencent Shanghai Branch\" in the search \nfield to proceed with the search for the \ndestination.\nA keyboard and a search result for \"Tencent Shanghai \nBranch\" with address details and a distance measurement \nare displayed.\nObservation\nThought\nAction\ntap(1)\nTo complete the given task, I should confirm the search \nfor \"Tencent Shanghai Branch\". In this screenshot, the \nsearch confirm button is represented by the blue button\nwith the text \u201c\\u641c\\u7d22\u201d (\u201cSearch\u201d in Chinese) on it.\nThe screenshot shows a map application with the \nlocation \"Tencent Shanghai Branch\" pinpointed \nwith a red marker. There are two buttons at the \nbottom of the screen, one labeled \"Directions\" and \nthe other labeled \"Start\", with numeric tags 1 and \n3 respectively.\nObservation\nThought\nAction\ntap(2)\nTo complete the given task, which is to \nnavigate to Tencent Shanghai Branch, I should \ntap the \"Directions\" button to initiate the \nroute finding process.\nObservation\nThe recipient's email address is already filled \nin, and the subject line reads \"Inquiry About \nYour New Position.\" The body of the email is\nnot visible, and there's a UI element labeled \n'11' which is used to collapse the keyboard.\nThought\nAction\ntap(11)\nTo complete the task, the next step should be \nto collapse the keyboard so that I can access \nthe body of the email to type the message to \nJane Doe asking about her new job.\nObservation\nThe recipient's email address is already filled in as \njanedoe@email.com and the subject line is \"Inquiry \nAbout Your New Position.\" The cursor is blinking in \nthe main body area of the email, indicating that it is \nready to receive text input. The on-screen keyboard is \nalso visible.\nThought\nAction\ntext(\"Dear Jane Doe,\\n\\nI am writing to inquire \nabout your new position that you recently started\u2026\")\nTo complete the given task, the next step I should do is \nto type the content of the email asking Jane Doe about \nher new job.\nObservation\nThe email body is typed with a message asking \nJane Doe about her recent job and expressing \neagerness to hear from her. There is a send \nbutton labeled with the numeric tag '3\u2019.\nThought\nAction\ntap(3)\nTo complete the given task, the next step I \nshould take is to send the email by tapping \nthe send button.\nObservation\nThe screenshot shows a photo editing application \ninterface with various adjustment settings for \nexposure, contrast, highlights, and shadows. \nThese adjustments have numeric tags 20, 21, 22, \nand 23 respectively. The settings are currently at \ntheir default values, and the image appears \nunderexposed, needing improvement in lighting.\nThought\nAction\nswipe(20, \"right\", \"medium\")\nTo improve the image quality and correct the \nunderexposure, I need to increase the \nexposure. The slider for the exposure setting \nis labeled with numeric tag 20, which I can \ninteract with to adjust the exposure level. I will \nswipe right on this slider to increase the \nexposure.\nObservation\nThe exposure level has been increased, as indicated by the \nslider tagged with number 20 being to the right. The \ncontrast, highlights, and shadows are yet to be adjusted, \nwith their sliders at the neutral position. There are also \nother adjustment options available, such as color and detail, \nwhich could further improve the image quality if needed.\nThought\nAction\nswipe(21, \"right\", \"short\")\nThe next step to improve the quality of the image could be \nto adjust the contrast, as increasing contrast can often \nmake an image appear more vivid and can complement \nthe increased exposure. Based on the documentation, \ninteracting with UI element labeled 21 will adjust the \ncontrast level of the image.\nObservation\nThe exposure and contrast sliders are visible, \nwith the exposure increased to +2.20 and the \ncontrast increased to +24. The highlights and\nshadows sliders are at 0, indicating no \nadjustment has been made to them.\nThought\nAction\ntap(22)\nSince I have previously increased the exposure \nand adjusted the contrast slightly, the next step \ncould be to adjust the highlights or shadows to \nfurther improve the image quality. Adjusting \nthe highlights might help in bringing out more \ndetail in the brighter parts of the image \nwithout affecting the overall exposure too much.\nGoogle Maps: navigate to Tencent Shanghai Branch\nGmail: send an email to janedoe@email.com to ask her about her new job\nLightroom: improve the quality of this image until it looks good to you\nFigure 3: Qualitative Task Evaluation Across Three Apps.\nThis figure presents qualitative results for three\ndistinct tasks conducted on Google Maps, Gmail, and Lightroom. It showcases AppAgent\u2019s ability to accurately\nperceive, reason, and execute tasks, demonstrating its competence in various application contexts. Due to space\nconstraints, some less critical details have been omitted from the description.\nvision capabilities of our agent, we conducted an\nin-depth case study using Adobe Lightroom, an\nimage-editing application. This specific case study\nallowed us to evaluate the agent\u2019s proficiency in\nhandling visual tasks and its ability to interpret\nand manipulate images within the app. For the\nexploration phase, we capped the maximum num-\nber of steps at 40. During testing, we limited the\nmaximum number of steps to 10. For these experi-\nments, we utilized the state-of-the-art multimodal\nlarge language model, GPT-4. GPT-4 is equipped\nto process interleaved image-and-text inputs effec-\ntively. This unique capability enables our agent to\ninterpret and interact with both visual and textual\ninformation seamlessly within the applications.\n4.2\nDesign and Analysis\nBaselines. To comprehensively evaluate our mul-\ntimodal agent framework, we considered various\ndesign choices and their impact on performance.\nWe conducted experiments using different configu-\nrations to provide valuable insights into the agent\u2019s\nbehavior. We started with GPT-4 without any ref-\nerence documents during testing and examined its\nperformance both with the raw action API and\nour simplified action space. Next, we explored\ndifferent ways to generate guiding documents for\nthe agent. These included documents generated\nthrough autonomous exploration, watching human\ndemonstrations, and the manually crafted docu-\nment as an oracle benchmark.\nTo effectively compare the performance of dif-\n6\nMethod\nDocument\nAction Space\nSR \u2191\nReward \u2191\nAvg. Steps\nGPT4 (Baseline)\nNone\nRaw\n2.2%\n0.6\n4.0\nNone\nOurs\n48.9%\n3.5\n6.9\nAppAgent\nAuto. Exploration\nOurs\n73.3%\n5.1\n4.4\nWatching Demos\nOurs\n84.4%\n4.7\n5.1\nManually Crafted\nOurs\n95.6%\n5.5\n5.5\nTable 1: Evaluating Design Choices in AppAgent Performance. This table contrasts different design elements\nwithin AppAgent. Key findings include: our custom-developed action space surpasses the raw action space in effi-\nciency; the exploration phase, incorporating both autonomous interaction and observation of human demonstrations,\nsignificantly enhances agent performance; and the auto-generated documentation yields outcomes on par with those\nderived from manually crafted documents.\nMethod\nDocument\nAction Space\nAvg. Rank \u2193\nNum. Tools\nGPT4 (Baseline)\nNone\nOurs\n2.30\n2.4\nAppAgent\nWatching Demos\nOurs\n1.95\n5.8\nManually Crafted\nOurs\n1.75\n4.0\nTable 2: Case study on image editing tasks with Lightroom App. We conduct a user study to rank the image\nediting results of different methods. Our agents produce better results than the GPT-4 baseline.\nferent methods, we employed three key metrics:\nSuccessful Rate (SR): This metric measures the\naverage rate at which the agent successfully com-\npletes tasks within an app. If the agent fails to\nfinish the task in 10 steps, it is considered a failure.\nReward: To provide a more fine-grained measure-\nment, we developed a reward model to assess per-\nformance. For each task within an app, we scored\ndifferent UI pages. The closer the UI page was to\nthe objective, the higher the score received. This\nmeans that even if the agent failed to complete the\ntask, it would still receive credit based on its final\nstate.\nAverage Steps: We also reported the average num-\nber of steps required to successfully finish tasks\nacross the selected applications.\nResults. The comparison of our experimental re-\nsults is presented in Table 1. We report the average\nperformance of 45 tasks on 9 of the 10 previously\ndescribed apps. Notably, we excluded Lightroom\nfrom this evaluation, as assessing task completion\nin this application presented inherent ambiguities.\nAs demonstrated, our simplified action space sig-\nnificantly improves the performance of the GPT-4\nbaseline. Our observations indicate that LLM strug-\ngles with producing accurate xy coordinates, while\nour simplified action space eliminates this challeng-\ning requirement. Additionally, documents gener-\nated through autonomous exploration and observ-\ning human demonstrations proved to be highly ef-\nfective. Their results consistently outperformed the\nGPT-4 baseline and are comparable to the results\nof human-written documents, which highlights the\nefficacy of our design in enhancing the agent\u2019s per-\nformance across a diverse set of applications.\nQualitative results. In Fig. 3, we provide ex-\namples showcasing the agent\u2019s execution process\nfor various tasks. This qualitative analysis serves\nto demonstrate the agent\u2019s capacity to accurately\nperceive, reason, and act in response to given tasks.\nFor a more comprehensive understanding of our\nagent\u2019s capabilities, please refer to our project page,\nwhich includes additional demonstration videos.\n4.3\nCase Study\nTo gain deeper insights into the vision capabili-\nties of our agent, we conducted an extensive case\nstudy using Adobe Lightroom, an image-editing\napplication. This specific case study allowed us\nto evaluate the agent\u2019s proficiency in handling vi-\nsual tasks, which was previously impossible for\ntext-only agent models. Lightroom, as an image-\nediting app with various editing tools, demands a\nwide range of operations, such as selecting appro-\npriate tools and manipulating image parameters.\nThis case study provides a robust evaluation of the\nagent\u2019s overall capabilities. Additionally, the open-\nended nature of image editing tasks allows us to\n7\nassess the agent\u2019s problem-solving abilities. We\nprepared five images with visual issues, such as\nlow contrast and overexposure. Various variants\nof our model, as previously illustrated, were used\nto edit these images. A user study was conducted\nto rank the editing results produced by different\nmethods. We also reported the average number of\ntools used for image editing, providing an addi-\ntional reference to the editing process\u2019s complexity.\nAll models were assigned the task of \u201cfix this image\nuntil it looks good to you\u201d without specifying the\nimage\u2019s problems. The comparison of the results\nis presented in Table 2. As we can see, our agent\nmodel with documents yields consistently better\nresults than the GPT-4 baseline, which emphasizes\nthe influence of documents in our design. The gen-\nerated documents by watching the demonstration\nproduced comparable results with the results of\nmanually crafted documents, which suggests the\neffectiveness of the exploration phase. We also\nfind that with a document, the agent tends to use\nvarious tools to improve the image quality, while\nthe GPT-4 baseline uses fewer tools.\n5\nConclusion\nIn this paper, we have introduced a novel multi-\nmodal agent framework that leverages the vision\ncapabilities of large language models to operate\nsmartphone applications in a human-like manner.\nOur approach eliminates the need for system back-\nend access and offers security, adaptability, and\nflexibility advantages. Our exploration-based learn-\ning strategy allows the agent to quickly adapt to\nnew applications with unfamiliar user interfaces,\nmaking it a versatile tool for various tasks. Our\nextensive experiments across various apps high-\nlight our agent\u2019s ability to handle diverse high-level\ntasks and underscore its adaptability and learning\nefficiency.\nLimitation. We have adopted a simplified action\nspace for smartphone operations, which means that\nadvanced controls such as multi-touch and irregu-\nlar gestures are not supported. This limitation may\nrestrict the agent\u2019s applicability in some challeng-\ning scenarios. Nevertheless, we recognize this as\nan avenue for future research and development.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can\nand not as i say: Grounding language in robotic af-\nfordances. In arXiv preprint arXiv:2204.01691.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli\nDing, Danny Driess, Avinava Dubey, Chelsea Finn,\net al. 2023. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. arXiv\npreprint arXiv:2307.15818.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jas-\nmine Hsu, et al. 2022. Rt-1: Robotics transformer\nfor real-world control at scale.\narXiv preprint\narXiv:2212.06817.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nMeta FAIR, Anton Bakhtin, Noam Brown, Emily Di-\nnan, Gabriele Farina, Colin Flaherty, Daniel Fried,\nAndrew Goff, Jonathan Gray, Hengyuan Hu, et al.\n2022. Human-level play in the game of diplomacy\nby combining language models with strategic reason-\ning. Science, 378(6624):1067\u20131074.\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka\nMatsuo, Aleksandra Faust, Shixiang Shane Gu, and\nIzzeddin Gur. 2023. Multimodal web navigation with\ninstruction-finetuned foundation models.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\ndra Faust. 2023. A real-world webagent with plan-\nning, long context understanding, and program syn-\nthesis.\nYucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin\nWang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023.\nChartllama: A multimodal llm for chart understand-\ning and generation.\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\nZheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\nand J\u00fcrgen Schmidhuber. 2023. Metagpt: Meta pro-\ngramming for a multi-agent collaborative framework.\n8\nZhiting Hu and Tianmin Shu. 2023. Language mod-\nels, agent models, and world models: The law for\nmachine reasoning and planning.\narXiv preprint\narXiv:2312.05230.\nYanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin\nFu, Guosheng Lin, Chunhua Shen, Ling Chen, and\nYunchao Wei. 2023. Stablellava: Enhanced visual\ninstruction tuning with synthesized image-dialogue\ndata.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved baselines with visual instruc-\ntion tuning.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-\nhan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\nShen, Tianjun Zhang, Yu Su, Huan Sun, Minlie\nHuang, Yuxiao Dong, and Jie Tang. 2023c. Agent-\nBench: Evaluating LLMs as agents. arXiv preprint\narXiv: 2308.03688.\nOpenAI. 2021.\nChatgpt.\nhttps://openai.com/\nresearch/chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th An-\nnual ACM Symposium on User Interface Software\nand Technology, pages 1\u201322.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen,\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. 2023. Communicative agents for software de-\nvelopment. arXiv preprint arXiv:2307.07924.\nScott Reed, Konrad Zolna, Emilio Parisotto, Ser-\ngio Gomez Colmenarejo,\nAlexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, et al. 2022. A\ngeneralist agent. arXiv preprint arXiv:2205.06175.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends\nin huggingface. In Advances in Neural Information\nProcessing Systems.\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang,\nZishan Qin, and Stephen Gould. 2023. 3d-gpt: Proce-\ndural 3d modeling with large language models. arXiv\npreprint arXiv:2310.12945.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He,\nZilong Zheng, Yaodong Yang, Xiaojian Ma, and\nYitao Liang. 2023.\nJarvis-1: Open-world multi-\ntask agents with memory-augmented multimodal lan-\nguage models.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\npotential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864.\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-\noxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,\nQian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin\nSu, Dongchan Shin, Caiming Xiong, and Tao Yu.\n2023. Openagents: An open platform for language\nagents in the wild.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\naolong Wang, Weidong Liu, and Yang Liu. 2023.\nExploring large language models for communica-\ntion games: An empirical study on werewolf. arXiv\npreprint arXiv:2309.04658.\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\nJulian McAuley, Jianfeng Gao, Zicheng Liu, and\nLijuan Wang. 2023. Gpt-4v in wonderland: Large\nmultimodal models for zero-shot smartphone gui nav-\nigation. arXiv preprint arXiv: 2311.07562.\n9\nHui Yang, Sifu Yue, and Yunzhong He. 2023a. Auto-\ngpt for online decision making: Benchmarks and\nadditional opinions.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. 2023b.\nThe dawn of lmms: Preliminary\nexplorations with gpt-4v (ision).\narXiv preprint\narXiv:2309.17421.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\n2023c.\nThe dawn of lmms: Preliminary explo-\nrations with gpt-4v(ision).\narXiv preprint arXiv:\n2309.17421.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In ICLR.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\n10\n"
  },
  {
    "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
    "link": "https://arxiv.org/pdf/2312.14125.pdf",
    "upvote": "41",
    "text": "VideoPoet: A Large Language Model for Zero-Shot Video Generation\nDan Kondratyuk * 1 Lijun Yu * 1 2 Xiuye Gu * 1 Jos\u00b4e Lezama * 1 Jonathan Huang * 1 Grant Schindler 1\nRachel Hornung 1 Vighnesh Birodkar 1 Jimmy Yan 1 Ming-Chang Chiu 1 Krishna Somandepalli 1\nHassan Akbari 1 Yair Alon 1 Yong Cheng 3 Josh Dillon 1 Agrim Gupta 1 Meera Hahn 1 Anja Hauth 1\nDavid Hendon 1 Alonso Martinez 1 David Minnen 1 Mikhail Sirotenko 1 Kihyuk Sohn 1 Xuan Yang 1\nHartwig Adam 1 Ming-Hsuan Yang 1 Irfan Essa 1 Huisheng Wang 1 David A. Ross 1 Bryan Seybold * 1\nLu Jiang * 1\nAbstract\nWe present VideoPoet, a model for synthesizing\nhigh-quality videos from a large variety of con-\nditioning signals. VideoPoet employs a decoder-\nonly transformer architecture that processes mul-\ntimodal inputs \u2013 including images, videos, text,\nand audio. The training protocol follows that of\nLarge Language Models (LLMs), consisting of\ntwo stages: Pretraining and task-specific adapta-\ntion. During pretraining, VideoPoet incorporates\na mixture of multimodal generative objectives\nwithin an autoregressive Transformer framework.\nThe pretrained LLM serves as a foundation that is\nadapted to a range of video generation tasks. We\npresent results demonstrating the model\u2019s state-of-\nthe-art capabilities in zero-shot video generation,\nspecifically highlighting the generation of high-\nfidelity motions.\n1. Introduction\nRecently, there has been a surge of generative video mod-\nels capable of a variety of video creation tasks. These in-\nclude text-to-video (Zhang et al., 2023a; Singer et al., 2022),\nimage-to-video (Yu et al., 2023d), video-to-video styliza-\ntion (Chen et al., 2023b; Chai et al., 2023; Voleti et al.,\n2022), and video editing (Ceylan et al., 2023; Wang et al.,\n2023b; Geyer et al., 2023) among other video applications.\nMost existing models employ diffusion-based methods for\nvideo generation. These video models typically start with a\npretrained image model, such as Stable Diffusion (Rombach\net al., 2022; Podell et al., 2023), that produces high-fidelity\nimages for individual frames, and then fine-tune the model\nto improve temporal consistency across video frames.\n*Equal contribution\n1Google Research\n2Carnegie Mel-\nlon University\n3Google DeepMind.\nCorrespondence to:\nDan Kondratyuk <dankondratyuk@google.com>,\nJonathan\nHuang <jonathanhuang@google.com>, Bryan Seybold <sey-\nbold@google.com>.\nPreliminary work, Copyright 2024 by the authors(s).\nWhile Large Language Models (LLMs) are commonly used\nas foundation models across various modalities including\nlanguage (Brown et al., 2020), code (Li et al., 2023; OpenAI,\n2023), audio (Rubenstein et al., 2023), speech (Agostinelli\net al., 2023), and robotics (Driess et al., 2023; Zitkovich\net al., 2023), the diffusion model remains the predominant\napproach for video generation. Although early research has\ndemonstrated the effectiveness of LLMs in text-to-image\ngeneration, e.g., DALL-E (Ramesh et al., 2022), Parti (Yu\net al., 2022) and (Ding et al., 2021), and text-to-video, e.g.,\nCogVideo (Hong et al., 2022)), language models have not\nreached a level of quality on par with video diffusion models\nin tasks like text-to-video generation as shown in previous\nstudies (Nash et al., 2022; Villegas et al., 2022). In contrast\nto training exclusively for text-to-video tasks, the generative\nmodel of LLMs in the language domain emphasizes a large\npretraining stage to learn a foundation (Bommasani et al.,\n2021) by examining pretraining tasks that extend beyond\ntext-to-video generation.\nA notable advantage of employing LLMs in video genera-\ntion lies in the ease of integrating existing LLM frameworks.\nThis integration allows for reusing LLM infrastructure and\nleverages the optimizations our community has developed\nover many years for LLMs, including optimizations in learn-\ning recipes for model scaling (Brown et al., 2020; Chowdh-\nery et al., 2022), training and inference infrastructure (Du\net al., 2022), hardware, among other advancements. This\ncouples with their flexibility in encoding many diverse tasks\nin the same model (Raffel et al., 2020), which stands in con-\ntrast to most diffusion models where architectural changes\nand adapter modules are the dominant approach used to\nadapt the model to more diverse tasks (Zhang et al., 2023b).\nIn this paper, we exploit language models for video genera-\ntion, following the canonical training protocols of LLMs in\nthe language domain. We introduce VideoPoet, a language\nmodel for video generation. VideoPoet employs a decoder-\nonly LLM architecture (Anil et al., 2023; OpenAI, 2023)\nthat admits image, video, and audio modalities as discrete\ntokens, each produced by their respective tokenizer.\n1\narXiv:2312.14125v2  [cs.CV]  14 Mar 2024\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nFigure 1: VideoPoet Overview: a versatile video generator that conditions on multiple types of inputs and performs a\nvariety of video generation tasks.\nThe training process of VideoPoet consists of two stages:\n(1) pretraining and (2) task-adaptation. During pretraining,\nVideoPoet incorporates a mixture of multimodal pretraining\nobjectives within an autoregressive transformer framework.\nAfter pretraining, the model functions as a versatile multi-\ntask video generation model such as text-to-video, image-to-\nvideo, video editing and video-to-video stylization. These\ncapabilities are inherently integrated into a single LLM,\nrather than relying on a separate generative model controlled\nby text prompts (Tang et al., 2023). During subsequent task-\nadaptation, the pretrained model can be further fine-tuned\neither to enhance its generation quality on the training tasks\nor to perform new tasks.\nExperiments show VideoPoet\u2019s state-of-the-art capabilities\nin generating videos with large and high-fidelity motions.\nThrough the powerful capabilities of the transformer archi-\ntecture, VideoPoet can be straightforwardly trained on a\nmulti-task, multimodal generative objective, allowing for\ngenerating consistent and realistic motion driven by text or\nother prompts. Furthermore, VideoPoet can synthesize co-\nherent long videos of up to 10 seconds by autoregressively\nextending the content, conditioned on the last second of the\ngenerated video.\nWe also demonstrate that VideoPoet is capable of zero-shot\nvideo generation. We use the term \u201czero-shot video gen-\neration\u201d as VideoPoet processes new text, image, or video\ninputs that diverge from the training data distribution. Fur-\nthermore, VideoPoet handles new tasks not included in its\ntraining. For example, VideoPoet is able to perform new\nediting tasks by sequentially chaining training tasks together.\nThe main contributions of this work are:\n\u2022 A method for training a Large Language Model (LLM)\nspecifically for video generation tasks, utilizing tokenized\nvideo data that incorporates both text-paired and unpaired\nvideo data.\n\u2022 An approach to video super-resolution that increases spa-\ntial resolution within the latent token space using a bidirec-\ntional transformer with efficient windowed local attention.\n\u2022 Evaluations and demonstrations to highlight VideoPoet\u2019s\ncompetitive and state-of-the-art performance, especially\nin generating realistic and interesting videos with motion.\n2. Related Work\nVideo diffusion models.\nRecently, numerous video gen-\neration methods use diffusion-based methods for text-to-\nvideo (Ho et al., 2022a; Blattmann et al., 2023b; Zhang\net al., 2023a; Blattmann et al., 2023a; He et al., 2023; Zhou\net al., 2022; Wang et al., 2023a; Ge et al., 2023; Wang et al.,\n2023d;c; Singer et al., 2022; Zhang et al., 2023a; Zeng et al.,\n2023) and video-to-video editing (Liew et al., 2023; Feng\net al., 2023; Esser et al., 2023; Chen et al., 2023b). As\nvideo diffusion models are usually derived from text-to-\nimage diffusion models (Ramesh et al., 2021; Saharia et al.,\n2022), additional tasks and modalities are added via infer-\nence tricks (Meng et al., 2021), architectural changes (Esser\net al., 2023; Liew et al., 2023) and adapter layers (Zhang\net al., 2023b; Guo et al., 2023). Although these models are\ncomposable after training, they are not trained end-to-end in\na unified framework. Our multitask pretraining strategy in a\nsingle model improves performance and provides zero-shot\nvideo generation capabilities.\nLanguage models for video and image generation.\nVideo language models are typically derived from the gen-\neral family of transformer-based language models (Vaswani\net al., 2017; Raffel et al., 2020) that easily combine multi-\nple tasks in pretraining and demonstrate powerful zero-shot\ncapabilities. Image generation language models can gener-\nate images autoregressively (Yu et al., 2022) or via masked\nprediction (Chang et al., 2022; 2023). Both families have\n2\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nFigure 2: Sequence layout for VideoPoet. We encode all modalities into the discrete token space, so that we can directly\nuse large language model architectures for video generation. We denote special tokens in <> (see Table 4 for definitions).\nThe modality agnostic tokens are in darker red; the text related components are in blue; the vision related components are in\nyellow; the audio related components are in green. The left portion of the layout on light yellow represents the bidirectional\nprefix inputs. The right portion on darker red represents the autoregressively generated outputs with causal attention.\nbeen extended to text-to-video (Hong et al., 2022; Villegas\net al., 2022; Hu et al., 2023; Yan et al., 2021) using paired\ndata. While other text-to-video work with transformers only\nleverages video-text pairs for training, we also leverage un-\npaired videos (without text) and the same video for different\ntasks. Since video language models can flexibly incorpo-\nrate numerous tasks (Yu et al., 2023a; Nash et al., 2022),\nincluding video-to-video, we extend this family of work to\ntext- and multimodal-conditioned tasks in this work with a\nsynergistic pretraining strategy across various tasks.\nPretraining task design in LLMs.\nAs language models\ncan easily incorporate multiple training tasks, task selection\nis an important area of research. GPT-3 (Brown et al., 2020)\nand PaLM (Chowdhery et al., 2022) demonstrate that train-\ning LLMs on diverse tasks leads to positive scaling effects\non zero- and few-shot tasks. Other approaches show that\nmasking approaches are a valuable learning target (Hoff-\nmann et al., 2022; Yu et al., 2023a;c). As the model size\ngrows, training data must grow as well (Hoffmann et al.,\n2022) to maintain similar performance. Our pretraining\nstrategy enables using the same video for multiple train-\ning tasks even without paired text. This design facilitates\ntraining on a large quantity of video-only examples, thereby\ndecreasing the demand for video-text pairs.\n3. Model Overview\nWe propose an effective method for video generation and\nrelated tasks from different input signals by leveraging large\nlanguage models. Our model consists of three components:\n(1) modality-specific tokenizers, (2) a language model back-\nbone (Fig. 2), and (3) a super-resolution module (Fig. 3).\nThe tokenizers map input data \u2013 i.e. image pixels, video\nframes, and audio waveforms \u2013 into discrete tokens in a\nunified vocabulary. The visual and audio tokens are flat-\ntened into a sequence of integers. Next, the LLM accepts\nthese tokens as input along with text embeddings, and is\nresponsible for generative multi-task and multimodal mod-\neling. As illustrated in Fig. 2, VideoPoet conditions on text\nembeddings, visual tokens, and audio tokens, and autore-\ngressively predicts visual and audio tokens. Subsequently,\nthe super-resolution module increases the resolution of the\nvideo outputs while refining visual details for higher quality.\n3.1. Tokenization\nWe employ the MAGVIT-v2 (Yu et al., 2023c) tokenizer\nfor joint image and video tokenization, and the Sound-\nStream (Zeghidour et al., 2021) tokenizer for audio. Visual\nand audio vocabularies are concatenated into a unified vo-\ncabulary. The text modality is represented by embeddings.\nImage and video tokenizer.\nVisual tokenizer is key to\ngenerating high-quality video content, often determining\nthe upper limit of achievable video generation quality (Yu\net al., 2023c). After analyzing existing tokenizers (Esser\net al., 2020; Villegas et al., 2022; Yu et al., 2023a;b), we\nchoose the MAGVIT-v2 (Yu et al., 2023c) tokenizer due\nto its performance in visual quality and high compression\ncapabilities, which effectively reduce the sequence length\nrequired by the LLM, thereby facilitating more efficient and\neffective learning. Specifically, a video clip is encoded and\nquantized into a sequence of integers, with a decoder map-\nping them back to the pixel space. MAGVIT-v2 tokenizes\n17-frame 2.125-second 128\u00d7128 resolution videos sampled\nat 8 fps to produce a latent shape of (5, 16, 16), which is\nthen flattened into 1280 tokens, with a vocabulary size of\n218. We also tokenize videos into a portrait aspect ratio at\n128\u00d7224 resolution, producing a latent shape of (5, 28, 16),\nor 2240 tokens.\nWe enforce causal temporal dependency, which facilitates\nthe generation of longer videos. To jointly represent images\nand videos, we encode the initial frame of a video or a static\nimage into tokens with a consistent shape of (1, 16, 16). We\nuse the COMMIT (Yu et al., 2023a) encoding scheme to\ntokenize the inpainting and outpainting tasks.\nAudio tokenizer.\nWe tokenize audio clips with a pre-\ntrained SoundStream (Zeghidour et al., 2021) tokenizer. We\nembed 2.125 seconds of audio to produce 106 latent frames\n3\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nxN\nmulti-axis transformer block\nself-attn\ncross-attn\nlow-res\nspatial\nvertical\nspatial\nhorizontal\ntemporal\nhigh-res\ntoken factorization (k=2)\nlow-res \ntokens\nhigh-res  \nmasked tokens\nembedding\nT5X \nembeddings\nmulti-head \nclassification and \nmerging (k=2)\nhigh-res\n output tokens\nFigure 3: Custom transformer architecture for video\nsuper-resolution.\nwith a residual vector quantizer (RVQ) of four levels. To im-\nprove audio generation performance, we transpose the clip\nbefore flattening so that the model predicts the full audio\nclip at each RVQ granularity level before moving on to the\nfiner grained levels. Finally, each RVQ level has a disjoint\nvocabulary with each level containing 1,024 codes. This\nresults in a combined audio vocabulary size of 4,096 codes.\nText embedding as input.\nPretrained text representations,\nin general, outperform training our model by learning text\ntokens from scratch. We use pretrained language embed-\ndings from a frozen T5 XL encoder (Raffel et al., 2020).\nFor tasks with text guidance, such as text-to-video, T5 XL\nembeddings are projected into the transformer\u2019s embedding\nspace with a linear layer.\n3.2. Language Model Backbone\nAfter converting the image, video, and audio modalities into\ndiscrete tokens within a shared vocabulary, we can directly\nleverage a language model to generate videos and audios\nin the token space. We use a prefix language model with a\ndecoder-only architecture as the backbone. By constructing\ndifferent patterns of input tokens to output tokens during\ntraining, we can control the tasks the model is able to per-\nform as explained in Section 4.\n3.3. Super-Resolution\nGenerating high-resolution (HR) videos with an autoregres-\nsive transformer entails heavy computational costs due to\nthe increase in sequence length. To illustrate this with an\nexample, the video tokenizer of Section 3.1 operating on\na 17 \u00d7 896 \u00d7 512 video produces a sequence of 35, 840\ntokens, making autoregressive sampling highly impractical.\nAiming at efficient and high-quality generative video upsam-\npling, we develop a custom spatial super-resolution (SR)\nnon-autoregressive video transformer (Yu et al., 2023a) to\noperate in token space on top of the language model out-\nput. To mitigate the computational requirements of the very\nlong sequences involved, and in particular the quadratic\nmemory of the self-attention layers, our design incorporates\nwindowed local attention (Gupta et al., 2022). Specifically,\nour SR transformer is composed of blocks of three trans-\nformer layers, each of which performs self-attention in a\nlocal window aligned with one of three axes (Tu et al., 2022):\nspatial vertical, spatial horizontal and temporal. The cross-\nattention layers attend to the low-resolution (LR) token se-\nquence and are also divided into local windows, isomorphic\nto those of the self-attention layers. All blocks also include\ncross-attention to T5 XL text embeddings. See Fig. 3 for a\nschematic representation of the custom transformer archi-\ntecture.\nSimilar to (Yu et al., 2023c), we train the SR transformer\nusing token factorization (with k = 2 factors) to account for\nthe large vocabulary size. The LR token sequences are ob-\ntained by tokenizing bicubic-downsampled versions of the\nground truth videos and applying noise augmentation (Ho\net al., 2022a) in the discrete latent space. Specifically, we\nrandomly resample the value of a random subset of the LR\ntokens and independently drop the LR condition and text\nembeddings for 10% of the training samples. During in-\nference, we use non-autoregressive sampling (Chang et al.,\n2022; Yu et al., 2023a) with classifier-free guidance inde-\npendently on both the LR condition and the text embeddings\n(Brooks et al., 2023). We use a cascade of two 2\u00d7 stages to\ngenerate videos of 896 \u00d7 512 resolution from the 224 \u00d7 128\nbase output of VideoPoet. More implementaiton details can\nbe found in the appendix.\n4. LLM Pretraining for Generation\n4.1. Task Prompt Design\nWe design a pretraining task mixture, each with a defined\nprefix input and output. The model conditions on the prefix,\napplying the loss solely to the output. Fig. 2 shows a typi-\ncal input-output sequence layout. For each task, the input\nsequence may include three types of values: text embed-\ndings (T5), visual tokens(MAGVIT-v2), and audio tokens\n(SoundStream). The model outputs two types of tokens:\nvisual and audio tokens. To facilitate training, VideoPoet\nemploys special tokens, as listed in Appendix Table 4. In\nthe following, we describe key designs for the task prompts.\nPretraining tasks.\nWe consider the following tasks. Un-\nconditioned video generation: Generate video frames with-\nout conditioning on an input. Text-to-video (T2V): Gen-\nerate video from a text prompt. Video future prediction\n4\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\n(FP): Given an input video of variable length, predict future\nframes. Image-to-video (I2V): Given the first frame of a\nvideo as an input image, predict the future frames. Video\ninpainting/outpainting (Painting): Given a masked video,\npredict the video with the masked contents filled in. Video\nstylization: Given text, optical flow, and depth, predict the\nvideo frames (Section 4.1). Audio-to-video: Given an input\naudio waveform, predict the corresponding video. Video-\nto-audio: Given an input video, predict the corresponding\naudio waveform. Audio-video continuation (AVCont) given\nan input frame and its audio, predict the rest of the video\nand audio.\nTo indicate the type of task, we condition on the <task>\ntoken, which has a unique value for each unique output.\nWe note that not all input variations need a new <task>;\nthe model adapts to different context signals for identical\noutputs. For instance, text-to-video, image-to-video, and\nunconditioned video generation share the same <task>. If\na modality is absent in a task, related input/output tokens\nand special tokens are excluded, shortening the sequence.\nRepresenting an image as a video.\nIn text-to-image pre-\ntraining, we omit the <eos> and <eov o> tokens from the\ninput sequence, enabling continuous token generation for\ninference of longer videos. This approach blurs the bound-\nary between video and image generation tasks, enhancing\ncross-modality information sharing. This design leads to\nthe prediction of higher-quality initial frames and reduces\nerrors and artifacts in subsequent frames.\nVideo token format.\nWe generate video tokens at two\nresolutions, 128\u00d7128 and 128\u00d7224, each available in two\nlengths: 17 frames and 41 frames, both encoded at 8 frames\nper second. Special conditioning tokens are used to signal\nthe desired resolutions and durations for video generation.\nImages are a special case of a 1-frame video, which we\ntokenize at 128\u00d7128 resolution.\nVideo stylization.\nFor video stylization, we adopt a\nmethod motivated by (Zhang et al., 2023b; Chen et al.,\n2023b; Esser et al., 2023), predicting videos from text, opti-\ncal flow, and depth signals. The training task for stylization\nis to reconstruct the ground truth video from the given opti-\ncal flow, depth, and text information, but during inference,\nwe apply optical flow and depth estimation on an input\nvideo but then vary the text prompt to generate a new style,\ne.g. \u201ccartoon.\u201d Similar to (Esser et al., 2023), text dictates\nthe output \u201ccontent\u201d or appearance, while optical flow and\ndepth guide its \u201cstructure.\u201d\n4.2. Training Strategy\nFor multi-task training, we use the Alternating Gradient\nDescent (AGD) method (Akbari et al., 2023) to train videos\nof varying lengths. We design the tasks in the AGD format\nresulting in a near 0% padding ratio, lower than that of the\npacking approach (Raffel et al., 2020). This is accomplished\nby grouping tasks by sequence length and alternately sam-\npling one group at each iteration. Since sequence lengths\nare fixed and vary significantly across tasks, e.g., first frame\nand long video generation, we achieve efficient training with\nminimal padding.\nWe find that sampling from image and video datasets uni-\nformly across time can lead to suboptimal results, as training\non images can enhance the model\u2019s understanding of ob-\njects but does not capture any motions that are represented\nin video data. Thus, we devise a two-stage pretraining strat-\negy, where we augment our sampling weights to sample\nimage data 90% of the time and video data 10% of the time\nfor the first 25% iterations of training. We then switch to\ntraining on video 90% and image 10% for the remaining\niterations.\nWe fine-tune our pretrained model for enhanced perfor-\nmance on specific tasks or for new task adaptation, such as\ntext-to-video and image-to-video tasks, using a high-quality\ndata subset. This results in improved generation quality,\nconsistent with Zhou et al. (2023), and addresses decoding\ncollapse issues, characterized by repetitive token predictions.\nSuch fine-tuning not only diversifies outputs but also allows\nfor a higher classifier-free guidance scale (Ho & Salimans,\n2022), boosting overall quality.\n5. Experiments\n5.1. Experimental Setup\nTraining tasks.\nWe train the model on a mixture of pre-\ntraining tasks as detailed in Section 4.1. We finetune a\nmodel on a high-quality training subset for text-to-video\nevaluations, as discussed in Section 4.2. Unless explicitly\nstated, we do not finetune on specific tasks for evaluations.\nDatasets.\nWe train on a total of 1B image-text pairs and\n\u223c270M videos (\u223c100M with paired text, of which \u223c50M\nare used for high-quality finetuning, and \u223c170M with paired\naudio) from the public internet and other sources, i.e. around\n2 trillion tokens across all modalities. The data has been\nfiltered to remove egregious content and sampled to improve\ncontextual and demographic diversity.\nEvaluation protocol.\nWe employ a zero-shot generation\nevaluation protocol, as the model has not been trained on\nthe training data of target benchmarks. Specifically, the\nevaluation benchmark includes two text-to-video generation\ndatasets, MSR-VTT (Xu et al., 2016) and UCF-101 (Soomro\net al., 2012), as well as the frame prediction task on Kinetics\n600 (K600) (Carreira et al., 2018), in which the first 5 frames\nare provided as the condition to predict the next 11 frames.\nWe also include inpainting and outpainting tasks (Yu et al.,\n2023a) on Something-Something V2 (SSv2) (Goyal et al.,\n2017).\nWe adopt widely used metrics such as Fr\u00b4echet Video Dis-\ntance (FVD) (Unterthiner et al., 2018), CLIP similarity\n5\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nTable 1: Pretraining task analysis on 300M models. The top rows list models with 300M parameters, trained on a\nsubset of the data, and are comparable to each other. The last row shows an 8B model trained on the entire dataset. T2I\n(text-to-image), T2V (text-to-video), FP (frame prediction), Painting (inpainting/outpainting), Uncond (unconditional\ngeneration), AVCont (audio-video continuation), and SSL (self-supervised learning).\nMethod\nPretraining Tasks\nZero-shot Evaluation Benchmark\nT2I\nT2V\nUncond\nFP\nPainting\nAVCont\nT2V\nFP\nInpainting\nOutpainting\nMSR-VTT\nUCF101\nK600\nSSv2\nSSv2\nCLIPSIM \u2191\nFVD \u2193\nFVD \u2193\nFVD \u2193\nFVD \u2193\nT2V\n\u2713\n0.244\n822\n759\n2,333\n2,310\nT2V+I\n\u2713\n\u2713\n0.247\n1,025\n794\n2,118\n1,916\nSSL\n\u2713\n\u2713\n\u2713\n\u2713\n0.226\n1,742\n700\n1,093\n1,500\nNO T2I\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.235\n1,008\n755\n95\n389\nALL\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.240\n1,085\n729\n127\n636\nALL (8B)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.305\n355\n687\n4.7\n13.76\nscore (Wu et al., 2021), and Inception Score (IS) (Saito\net al., 2020) for evaluation. Note that the specific met-\nrics and evaluation methods vary across different datasets.\nDetailed information on these variations can be found in\nAppendix A.5.4. We include examples of the generated\nvideos in the supplementary materials.\n5.2. Pretraining Task Analysis\nWe investigate the learning capabilities of different combi-\nnations of pretraining tasks using a model with 300 million\nparameters. All task combinations are trained using a learn-\ning rate of 10\u22123 for the same number of steps (300k) with a\nbatch size of 1024.\nFor the analysis of pretraining tasks, we consider text-to-\nvideo (T2V), text-to-image (T2I), and four self-supervised\nlearning (SSL) tasks: frame prediction (FP), central inpaint-\ning and central outpainting (Painting) (Yu et al., 2023a)\nand audio-video continuation (AVCont) where the model is\nprovided with the first frame and its corresponding audio\nto predict the subsequent 16 frames and matching audio.\nFor each video task, we uniformly select 20% of training\nsamples from a random subset of 50 million videos. For\nthe text-to-image task, we randomly sample 50 million text-\nimage pairs from our training dataset. For tasks involving\naudio, our sampling is exclusive to videos that contain an\naudio track.\nThe evaluation results are presented in Table 1. We assess a\nmodel across the four tasks within the zero-shot evaluation\nbenchmark: the T2V task on MSR-VTT (Xu et al., 2016)\nand UCF 101 (Soomro et al., 2012), the FP on K600 (Car-\nreira et al., 2018), and central inpainting and outpainting on\nSSv2 (Goyal et al., 2017). In these experiments, we employ\na single model to perform all the tasks. The model is not\ntrained on the training data of these evaluation datasets, and\nthus it is a zero-shot evaluation.\nThe top rows of Table 1 depict each pretraining task con-\nfiguration of the 300 million parameter model, which are\ncomparable in their setups. Our evaluation benchmarks\nspan diverse visual domains, posing a challenge to achiev-\nTable 2: Comparison on zero-shot text-to-video bench-\nmarks. See Appendix A.5.4 for evaluation details.\nModel\nMSR-VTT\nUCF-101\nCLIPSIM\nFVD\nFVD\nIS\nCogVideo (EN) (2022)\n0.2631\n1294\n702\n25.27\nMagicVideo (2022)\n-\n998\n655\n-\nVideo LDM (2023b)\n0.2929\n-\n551\n33.45\nModelScopeT2V (2023a)\n0.2930\n550\n-\n-\nInternVid (2023d)\n0.2951\n-\n617\n21.04\nVideoFactory (2023c)\n0.3005\n-\n410\n-\nMake-A-Video (2022)\n0.3049\n-\n367\n33.00\nShow-1 (2023a)\n0.3072\n538\n394\n35.42\nVideoPoet (Pretrain)\n0.3049\n213\n355\n38.44\nVideoPoet (Task adapt)\n0.3123\n-\n-\n-\ning consistent improvement across all of them. Nevertheless,\nincorporating all pretraining tasks results in the best overall\nperformance, on average, across all evaluated tasks. Ad-\nditionally, the significant disparity observed in the \u201cSSL\u201d\nrow suggests the limitations of self-supervised training and\nunderscores the necessity for text-paired data during train-\ning. The last row, \u201cALL (8B)\u201d, is the model with 8 billion\nparameters, trained on the pretraining tasks as discussed in\nSection 3 and utilized significantly more compute.\n5.3. Comparison with the State-of-the-Art\nText-to-Video (T2V).\nTable 2 shows zero-shot text-to-\nvideo evaluation results on the common MSR-VTT (Xu\net al., 2016) and UCF-101 (Soomro et al., 2012) datasets.\nOur model performs favorably in terms of CLIP similarity\nand FVD scores on MSR-VTT and UCF-101. The pre-\ntrained foundation model already achieves competitive per-\nformance on all metrics. After finetuned on high-quality\nsubset of text-video pairs, VideoPoet achieves even better\nCLIPSIM on MSR-VTT. More details on the evaluation\nsettings can be found in Appendix A.5.4.\nHuman Evaluations with Text-to-Video (T2V).\nWe an-\nalyze VideoPoet using human raters and compare with\nother recent models:\nShow-1 (Zhang et al., 2023a),\n6\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nVideo Quality\nPhenaki\nVideoCrafter\nShow-1\nPika\nGen2\nOther model preferred\nVideoPoet preferred\n29%\n71%\n42%\n58%\n34%\n66%\n32%\n68%\n44%\n56%\nLumiere\n64%\n36%\nText Fidelity\nPhenaki\nVideoCrafter\nShow-1\nPika\nGen2\nOther model preferred\nVideoPoet preferred\n25%\n75%\n33%\n67%\n37%\n63%\n18%\n82%\n28%\n72%\nLumiere\n28%\n72%\nMotion Interestingness\nPhenaki\nVideoCrafter\nShow-1\nPika\nGen2\nOther model preferred\nVideoPoet preferred\n31%\n69%\n37%\n63%\n34%\n65%\n16%\n84%\n90%\nLumiere\n18%\n82%\n10%\nMotion Realism\nPhenaki\nVideoCrafter\nShow-1\nPika\nGen2\nOther model preferred\nVideoPoet preferred\n22%\n78%\n26%\n74%\n32%\n68%\n92%\n30%\n70%\nLumiere\n29%\n71%\n8%\nTemporal Consistency\nPhenaki\nVideoCrafter\nShow-1\nPika\nGen2\nOther model preferred\nVideoPoet preferred\n32%\n68%\n40%\n60%\n41%\n59%\n30%\n70%\n34%\n66%\nLumiere\n40%\n60%\nFigure 4: Human evaluation results on text-to-video\n(T2V) generation. Green and pink bars represent the pro-\nportion of trials where VideoPoet was preferred over or less\npreferred to an alternative, respectively.\nVideoCrafter (Chen et al., 2023a), Phenaki (Villegas et al.,\n2022), Pika (Pika, 2023), Gen2 (Runway, 2023) and\nLumiere (Bar-Tal et al., 2024).\nShow-1, VideoCrafter,\nPika, Gen2 and Lumiere are video diffusion models while\nPhenaki is a token-based model using masked token model-\ning (Chang et al., 2022). We ran the most up-to-date model\nversions as of January 2024.\nWe first develop a unified evaluation prompt bank consisting\nof \u223c 250 selected prompts from a variety of categories and\nstyles. Our prompts are sourced from published prompt sets\n(e.g., Show-1, Video LDM (Blattmann et al., 2023b)). We\nselect the prompts prior to generating videos and fix these\nchoices after initial selection. We also select preferentially\nfor prompts that contain an explicit mention of motion so\nthat the evaluation would not be biased for models that gen-\nerate high quality videos that are almost still (e.g., \u201cperson\njumping off of a chair\u201d over \u201cperson standing on a chair\u201d).\nNote that due to time constraints, our experiments for Pika\nand Gen2 were run on a subset of 50 prompts due to having\nto submit these manually via their web interface. These 50\nprompts were pre-selected (before any evaluations were run)\nso as to be representative of the entire set.\nFor this user study we use the fine-tuned version of\nVideoPoet as discussed in Section 4.2 and compare against\nalternative models in a side-by-side fashion for each prompt.\nRaters are shown videos generated by two models at a time\n(in randomized order so as to not bias raters). Not all meth-\nods generate videos at the same size or aspect ratio, and\nwe resize each video to a fixed area while maintaining its\noriginal aspect ratio. Raters are then asked to compare the\nvideos along 5 dimensions and for each dimension to report\nwhich video is better. The 5 dimensions are: (1) text fidelity\n(which video follows the text prompt most faithfully), (2)\nvideo quality, (3) motion \u201cinterestingness\u201d, (4) motion real-\nism, and (5) temporal consistency. Raters are required to go\nthrough a collection of training examples for each of these\n5 dimensions before they begin.\nOur findings are summarized in Fig. 4, where green and\npink bars represent the proportion of trials where VideoPoet\nwas preferred or less preferred over an alternative, respec-\ntively. We observe that VideoPoet outperforms all baseline\nmodels along almost all of the dimensions. More specifi-\ncally, VideoPoet achieves significant wins along the motion\ncategories (motion interestingness and realism, temporal\nconsistency) and Lumiere (Bar-Tal et al., 2024) which is dif-\nfusion based and concurrent to our work, is the only model\nthat outperforms VideoPoet on Video Quality.\n5.4. LLM\u2019s Diverse Capabilities in Video Generation\nThis subsection presents several capabilities we discover\nfrom the pretrained VideoPoet, shedding light on the LLM\u2019s\npromising potential in video generation. By combining the\n7\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nFigure 5: 10-Second long video generation example. By\npredicting 1-second video segments from an initial 1-second\nclip, VideoPoet can iteratively generate videos of extended\nlengths.\nAnimated from historical photo\nAnimated from painting\nFigure 6: Examples of videos animated from still images\nplus text prompts tailored to each initial image.\nflexibility of our autoregressive language model to perform\ndiverse tasks such as extending video in time, inpainting,\noutpainting, and stylization, VideoPoet accomplishes multi-\nple tasks in a unified model.\nCoherent long video generation and image-to-video.\nA\nbenefit of an decoder-based language model is that it pairs\nwell with autoregressively extending generation in time. We\npresent two different variants: Generating longer videos\nand converting images to videos. Encoding the first frame\nindependently allows us to convert any image into the initial\nframe of a video without padding. Subsequent frames are\ngenerated by predicting remaining tokens, transforming the\nimage into a video as shown in Fig. 61.\nThis results in the capability to generate videos longer than\n10 seconds or to allow users to iteratively extend video clips\n1For image-to-video examples we source images from Wikime-\ndia Commons: https://commons.wikimedia.org/wiki/Main Page\nAnimated from still image\nStylized video\nPrompt: An oil painting of a snowman with a red hat\nopening their mouth to yawn\nFigure 7: Example of zero-shot video editing via task\nchaining (text conditioned image-to-video and stylization)\nbased on previously generated video, and produces tempo-\nrally consistent videos without significant distortion. Such\ncapabilities are rarely observed in contemporary diffusion\nmodels.\nZero-shot video editing and task chaining.\nWith the\nmulti-task pretraining, VideoPoet exhibits task generaliza-\ntion that can be chained together to perform novel tasks.\nWe show the model can apply image-to-video animation\nfollowed by video-to-video stylization in Fig. 7. In the Ap-\npendix, Fig. 10 shows another example applying video-to-\nvideo outpainting, followed by editing them with additional\nvideo-to-video effects. At each stage, the quality of the out-\nput is sufficient to remain in-distribution (i.e. teacher forc-\ning) for the next stage without noticeable artifacts. These\ncapabilities can be attributed to our multimodal task de-\nsign within an LLM transformer framework that allows for\nmodeling multimodal content using a single transformer\narchitecture over a unified vocabulary.\nZero-shot video stylization.\nStylization results are pre-\nsented in Appendix A.4 where the structure and text are used\nas prefixes to guide the language model. Unlike other styl-\nization methods that employ adapter modules such as cross-\nattention networks (Zhang et al., 2023b) or latent blend-\ning (Meng et al., 2021), our approach stylizes videos within\nan LLM as one of several generative tasks.\n3D structure, camera motion, visual styles.\nEven though\nwe do not add specific training data or losses to encourage\n3D consistency, our model can rotate around objects and\npredict reasonable visualizations of the backside of objects.\n8\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nAdditionally, with only a small proportion of input videos\nand texts describing camera motion, our model can use\nshort text prompts to apply a range of camera motions to\nimage-to-video and text-to-video generations (see Fig. 11).\n5.5. Limitations\nDespite VideoPoet demonstrating highly competitive perfor-\nmance of LLMs relative to state-of-the-art models, certain\nlimitations are still observed. For example, the RGB frame\nreconstruction from compressed and quantized tokens place\nan upper bound on the generative model\u2019s visual fidelity.\nSecond, the per-frame aesthetic biases in static scenes does\nnot match the best baseline. This difference is largely due\nto a choice of training data, where we focus our training\non more natural aesthetics and excluded some sources con-\ntaining copyrighted images, such as LAION (Schuhmann\net al., 2022), which is commonly used in other work. Finally,\nsmall objects and fine-grained details, especially when cou-\npled with significant motions, remains difficult within the\ntoken-based modeling.\n6. Conclusion\nVideoPoet demonstrates the potential of a large language\nmodel that is trained on discrete visual, text and audio to-\nkens, in generating videos of compelling state-of-the-art\nquality. A particular strength of our model lies in its ability\nto generate high-fidelity, large, and complex motions. Our\nlarge language model formulation benefits from training\nacross a variety of multimodal tasks with a unified architec-\nture and vocabulary. Consequently, the pretrained model is\nadept at multi-task video creation, and serves as a foundation\nfor a diverse variety of video generation related capabilities,\nincluding multiple forms of editing.\nAcknowledgements\nWe give special thanks to Alex Siegman, Victor Gomes, and\nBrendan Jou for managing computing resources. We also\ngive thanks to Aren Jansen, Marco Tagliasacchi, Neil Zeghi-\ndour, John Hershey for audio tokenization and processing,\nAngad Singh for storyboarding in \u201cRookie the Raccoon\u201d,\nCordelia Schmid for research discussions, David Salesin,\nTomas Izo, and Rahul Sukthankar for their support, and Jay\nYagnik for the initial concept.\nReferences\nAgostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M.,\nCaillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi,\nM., et al. Musiclm: Generating music from text. arXiv preprint\narXiv:2301.11325, 2023.\nAkbari, H., Kondratyuk, D., Cui, Y., Hornung, R., Wang, H.,\nand Adam, H. Alternating gradient descent and mixture-of-\nexperts for integrated multimodal perception. arXiv preprint\narXiv:2305.06324, 2023.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos,\nA., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\nBar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada,\nS., Ephrat, A., Hur, J., Li, Y., Michaeli, T., et al. Lumiere: A\nspace-time diffusion model for video generation. arXiv preprint\narXiv:2401.12945, 2024.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian,\nM., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.\nStable video diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023a.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W.,\nFidler, S., and Kreis, K. Align your latents: High-resolution\nvideo synthesis with latent diffusion models. In CVPR, pp.\n22563\u201322575, 2023b.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S.,\nvon Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill,\nE., et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258, 2021.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learn-\ning to follow image editing instructions. In CVPR, pp. 18392\u2013\n18402, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\nA., et al. Language models are few-shot learners. NeurIPS, 33:\n1877\u20131901, 2020.\nCarreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zis-\nserman, A. A short note about kinetics-600. arXiv preprint\narXiv:1808.01340, 2018.\nCeylan, D., Huang, C.-H. P., and Mitra, N. J. Pix2video: Video\nediting using image diffusion. In CVPR, pp. 23206\u201323217,\n2023.\nChai, W., Guo, X., Wang, G., and Lu, Y. Stablevideo: Text-driven\nconsistency-aware diffusion video editing. In CVPR, pp. 23040\u2013\n23050, 2023.\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T.\nMaskgit: Masked generative image transformer. In CVPR, pp.\n11315\u201311325, 2022.\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang,\nL., Yang, M.-H., Murphy, K., Freeman, W. T., Rubinstein, M.,\net al. Muse: Text-to-image generation via masked generative\ntransformers. arXiv preprint arXiv:2301.00704, 2023.\nChen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J.,\nLiu, Y., Chen, Q., Wang, X., et al. Videocrafter1: Open diffu-\nsion models for high-quality video generation. arXiv preprint\narXiv:2310.19512, 2023a.\nChen, W., Wu, J., Xie, P., Wu, H., Li, J., Xia, X., Xiao, X., and Lin,\nL. Control-a-video: Controllable text-to-video generation with\ndiffusion models. arXiv preprint arXiv:2305.13840, 2023b.\nChiu, M.-C., Chen, P.-Y., and Ma, X. Better may not be fairer: A\nstudy on subgroup discrepancy in image classification. In ICCV,\npp. 4956\u20134966, 2023.\n9\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G.,\nRoberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann,\nS., et al. PaLM: Scaling language modeling with pathways.\narXiv:2204.02311, 2022.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin,\nJ., Zou, X., Shao, Z., Yang, H., et al. Cogview: Mastering\ntext-to-image generation via transformers. NeurIPS, pp. 19822\u2013\n19835, 2021.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter,\nB., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y.,\nKrikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. GLaMs:\nEfficient scaling of language models with mixture-of-experts.\nIn ICML, 2022.\nEsser, P., Rombach, R., and Ommer, B. Taming transformers for\nhigh-resolution image synthesis. In CVPR, pp. 12868\u201312878,\n2020.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Germanidis,\nA. Structure and content-guided video synthesis with diffusion\nmodels. In CVPR, pp. 7346\u20137356, 2023.\nFeng, R., Weng, W., Wang, Y., Yuan, Y., Bao, J., Luo, C., Chen,\nZ., and Guo, B. Ccedit: Creative and controllable video editing\nvia diffusion models. arXiv preprint arXiv:2309.16496, 2023.\nGe, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs,\nD., Huang, J.-B., Liu, M.-Y., and Balaji, Y. Preserve your own\ncorrelation: A noise prior for video diffusion models. In CVPR,\npp. 22930\u201322941, 2023.\nGeyer, M., Bar-Tal, O., Bagon, S., and Dekel, T. Tokenflow:\nConsistent diffusion features for consistent video editing. arXiv\npreprint arXiv:2307.10373, 2023.\nGoyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J.,\nWestphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P.,\nMueller-Freitag, M., et al. The \u201csomething something\u201d video\ndatabase for learning and evaluating visual common sense. In\nICCV, 2017.\nGuo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and\nDai, B.\nAnimatediff: Animate your personalized text-to-\nimage diffusion models without specific tuning. arXiv preprint\narXiv:2307.04725, 2023.\nGupta, A., Tian, S., Zhang, Y., Wu, J., Mart\u00b4\u0131n-Mart\u00b4\u0131n, R., and Fei-\nFei, L. Maskvit: Masked visual pre-training for video prediction.\narXiv preprint arXiv:2206.11894, 2022.\nHe, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. Latent video\ndiffusion models for high-fidelity long video generation. arXiv\npreprint arXiv:2211.13221, 2(3):4, 2023.\nHershey, S., Chaudhuri, S., Ellis, D. P. W., Gemmeke, J. F., Jansen,\nA., Moore, C., Plakal, M., Platt, D., Saurous, R. A., Seybold,\nB., Slaney, M., Weiss, R., and Wilson, K. Cnn architectures\nfor large-scale audio classification. In ICASSP, 2017. URL\nhttps://arxiv.org/abs/1609.09430.\nHo, J. and Salimans, T. Classifier-free diffusion guidance. arXiv\npreprint arXiv:2207.12598, 2022.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A.,\nKingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen\nvideo: High definition video generation with diffusion models.\narXiv preprint arXiv:2210.02303, 2022a.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and\nFleet, D. J. Video diffusion models. arXiv:2204.03458, 2022b.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai,\nT., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl,\nJ., Clark, A., et al. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo:\nLarge-scale pretraining for text-to-video generation via trans-\nformers. arXiv preprint arXiv:2205.15868, 2022.\nHu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A.,\nShotton, J., and Corrado, G. Gaia-1: A generative world model\nfor autonomous driving.\narXiv preprint arXiv:2309.17080,\n2023.\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou,\nC., Marone, M., Akiki, C., Li, J., Chim, J., et al. StarCoder:\nmay the source be with you! arXiv:2305.06161, 2023.\nLiew, J. H., Yan, H., Zhang, J., Xu, Z., and Feng, J. Magicedit:\nHigh-fidelity and temporally coherent video editing. arXiv\npreprint arXiv:2308.14749, 2023.\nMeng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS. Sdedit: Guided image synthesis and editing with stochastic\ndifferential equations. arXiv preprint arXiv:2108.01073, 2021.\nNash, C., Carreira, J., Walker, J., Barr, I., Jaegle, A., Malinowski,\nM., and Battaglia, P. Transframer: Arbitrary frame prediction\nwith generative models. arXiv preprint arXiv:2203.09494, 2022.\nOpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.\nPerazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross,\nM., and Sorkine-Hornung, A. A benchmark dataset and eval-\nuation methodology for video object segmentation. In CVPR,\n2016.\nPika. Pika 1.0, 2023. URL https://pika.art/launch.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T.,\nM\u00a8uller, J., Penna, J., and Rombach, R. Sdxl: Improving latent\ndiffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,\nM., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. Journal\nof Machine Learning Research, 21(1):5485\u20135551, 2020.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A.,\nChen, M., and Sutskever, I. Zero-shot text-to-image generation.\narXiv preprint arXiv:2102.12092, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\nHierarchical text-conditional image generation with clip latents.\narXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRanftl, R., Lasinger, K., Hafner, D., Schindler, K., and Koltun, V.\nTowards robust monocular depth estimation: Mixing datasets\nfor zero-shot cross-dataset transfer. IEEE TPAMI, 44(3):1623\u2013\n1637, 2020.\n10\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.\nHigh-resolution image synthesis with latent diffusion models.\nIn CVPR, pp. 10684\u201310695, 2022.\nRubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., Bapna, A.,\nBorsos, Z., Quitry, F. d. C., Chen, P., Badawy, D. E., Han, W.,\nKharitonov, E., et al. Audiopalm: A large language model that\ncan speak and listen. arXiv preprint arXiv:2306.12925, 2023.\nRunway. Gen2, 2023. URL https://runwayml.com/.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B.,\nSalimans, T., et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. NeurIPS, 35:36479\u201336494,\n2022.\nSaito, M., Saito, S., Koyama, M., and Kobayashi, S. Train sparsely,\ngenerate densely: Memory-efficient unsupervised training of\nhigh-resolution temporal gan. IJCV, 128(10):2586\u20132606, 2020.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman,\nR., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman,\nM., et al. Laion-5b: An open large-scale dataset for training next\ngeneration image-text models. Advances in Neural Information\nProcessing Systems, 35:25278\u201325294, 2022.\nSchumann, C., Ricco, S., Prabhu, U., Ferrari, V., and Pantofaru, C.\nA step toward more inclusive people annotations for fairness. In\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics,\nand Society, pp. 916\u2013925, 2021.\nSchumann, C., Olanubi, G. O., Wright, A., Monk, E., Heldreth,\nC., and Ricco, S. Consensus and subjectivity of skin tone anno-\ntation for ML fairness. In Thirty-seventh Conference on Neu-\nral Information Processing Systems Datasets and Benchmarks\nTrack, 2023. URL https://openreview.net/forum?\nid=L9I9FhHfS3.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu,\nQ., Yang, H., Ashual, O., Gafni, O., et al. Make-a-video: Text-\nto-video generation without text-video data. arXiv preprint\narXiv:2209.14792, 2022.\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101\nhuman actions classes from videos in the wild. arXiv preprint\narXiv:1212.0402, 2012.\nSun, D., Herrmann, C., Reda, F., Rubinstein, M., Fleet, D. J., and\nFreeman, W. T. Disentangling architecture and training for\noptical flow. In ECCV, 2022.\nTang, Z., Yang, Z., Zhu, C., Zeng, M., and Bansal, M. Any-\nto-any generation via composable diffusion. arXiv preprint\narXiv:2305.11846, 2023.\nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A.,\nand Li, Y. Maxvit: Multi-axis vision transformer. In ECCV, pp.\n459\u2013479, 2022.\nUnterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R.,\nMichalski, M., and Gelly, S.\nTowards accurate generative\nmodels of video: A new metric & challenges. arXiv preprint\narXiv:1812.01717, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all\nyou need. NeurIPS, 30, 2017.\nVillegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H.,\nZhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\nVoleti, V., Jolicoeur-Martineau, A., and Pal, C. Mcvd-masked\nconditional video diffusion for prediction, generation, and inter-\npolation. NeurIPS, 35:23371\u201323385, 2022.\nWang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., and Zhang,\nS. Modelscope text-to-video technical report. arXiv preprint\narXiv:2308.06571, 2023a.\nWang, W., Xie, K., Liu, Z., Chen, H., Cao, Y., Wang, X., and Shen,\nC. Zero-shot video editing using off-the-shelf image diffusion\nmodels. arXiv preprint arXiv:2303.17599, 2023b.\nWang, W., Yang, H., Tuo, Z., He, H., Zhu, J., Fu, J., and Liu,\nJ. Videofactory: Swap attention in spatiotemporal diffusions\nfor text-to-video generation. arXiv preprint arXiv:2305.10874,\n2023c.\nWang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y.,\nLuo, P., Liu, Z., et al. Internvid: A large-scale video-text dataset\nfor multimodal understanding and generation. arXiv preprint\narXiv:2307.06942, 2023d.\nWu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G.,\nand Duan, N. Godiva: Generating open-domain videos from\nnatural descriptions. arXiv preprint arXiv:2104.14806, 2021.\nXu, J., Mei, T., Yao, T., and Rui, Y. Msr-vtt: A large video\ndescription dataset for bridging video and language. In CVPR,\npp. 5288\u20135296, 2016.\nYan, W., Zhang, Y., Abbeel, P., and Srinivas, A.\nVideogpt:\nVideo generation using vq-vae and transformers. arXiv preprint\narXiv:2104.10157, 2021.\nYu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan,\nV., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789, 2022.\nYu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H.,\nHauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al. Magvit:\nMasked generative video transformer. In CVPR, pp. 10459\u2013\n10469, 2023a.\nYu, L., Cheng, Y., Wang, Z., Kumar, V., Macherey, W., Huang,\nY., Ross, D. A., Essa, I., Bisk, Y., Yang, M.-H., et al. Spae:\nSemantic pyramid autoencoder for multimodal generation with\nfrozen llms. arXiv preprint arXiv:2306.17842, 2023b.\nYu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K.,\nMinnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G.,\net al. Language model beats diffusion\u2013tokenizer is key to visual\ngeneration. arXiv preprint arXiv:2310.05737, 2023c.\nYu, S., Sohn, K., Kim, S., and Shin, J. Video probabilistic diffusion\nmodels in projected latent space. In CVPR, pp. 18456\u201318466,\n2023d.\nZeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasac-\nchi, M.\nSoundstream: An end-to-end neural audio codec.\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, 30:495\u2013507, 2021.\n11\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nZeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y., and Li,\nH. Make pixels dance: High-dynamic video generation. arXiv\npreprint arXiv:2311.10982, 2023.\nZhang, D. J., Wu, J. Z., Liu, J.-W., Zhao, R., Ran, L., Gu, Y.,\nGao, D., and Shou, M. Z. Show-1: Marrying pixel and latent\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023a.\nZhang, L., Rao, A., and Agrawala, M. Adding conditional control\nto text-to-image diffusion models. In CVPR, pp. 3836\u20133847,\n2023b.\nZhang, Y., Jiang, L., Turk, G., and Yang, D. Auditing gender\npresentation differences in text-to-image models. arXiv preprint\narXiv:2302.03675, 2023c.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat,\nA., Yu, P., Yu, L., et al. Lima: Less is more for alignment. arXiv\npreprint arXiv:2305.11206, 2023.\nZhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., and Feng, J. Mag-\nicvideo: Efficient video generation with latent diffusion models.\narXiv preprint arXiv:2211.11018, 2022.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J.,\nWohlhart, P., Welker, S., Wahid, A., et al.\nRT-2: Vision-\nlanguage-action models transfer web knowledge to robotic con-\ntrol. In CoRL, 2023.\n12\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nA. Appendix\nA.1. Responsible AI and Fairness Analysis\nWe evaluate whether the generated outputs of our model are fair regarding protected attributes such as (1) Perceived Age (2)\nPerceived Gender Expression (3) Perceived Skin Tone. We construct 306 prompts with template \u2014 \u201ca {profession or people\ndescriptor} looking {adverb} at the camera\u201d with \u201cprofession\u201d being crawled from the US Bureau of Labor and Statistics\nand \u201cpeople descriptors\u201d including emotion state, socioeconomic class, etc. The \u201cadverb\u201d is used to generate semantically\nunchanged prompt templates such as \u201cstraightly\u201d or \u201cdirectly\u201d. We generate 8 videos for each prompt and for each generated\nvideo we infer an approximation of the expressed attribute regarding the 3 protected attributes. Across 10 prompts that have\nthe same semantic meaning but different \u201cadverbs\u201d, we observe our outputs generally introduced a stronger distribution shift\ntoward \u201cYoung Adults\u201d (age 18-35), \u201cMale\u201d and \u201cLight Skin Tone\u201d. However, we observe changing the \u201cadverb\u201d in the\nprompt template can significantly alter the output distributions. Therefore, our model can be prompted to produce outputs\nwith non-uniform distributions across these groups, but also possess the ability of being prompted to enhance uniformity,\nthough prompts are semantically unchanged. While research has been conducted in the image generation and recognition\ndomain (Zhang et al., 2023c; Schumann et al., 2021; 2023; Chiu et al., 2023), this finding highlights the importance of\ncontinued research to develop strategies to mitigate issues and improve fairness for video generation.\nA.2. Model Scale and Performance\nTo analyze model performance versus model scale, we use a subset of the training set without text-paired data and a\nslightly different task prompt design. We evaluate the video generation quality using FVD (Unterthiner et al., 2018)\nand audio generation quality using the Fr\u00b4echet Audio Distance (FAD), which uses the VGGish model as the embedding\nfunction (Hershey et al., 2017). Both FVD and FAD metrics are calculated using a held-out subset of 25 thousand videos.\nFig. 8 shows that as the model size grows and the amount of training data increases, performance improves across visual and\naudiovisual tasks. After obtaining the above results, we retrain our 1B and 8B models using the task design and text-paired\ntraining data discussed in Section 3. Appendix A.2.1 shows a qualitative comparison of the 1B and 8B pretrained models.\nIncreasing the model size improved temporal consistency, prompt fidelity, and motion dynamics while adding capabilities\nfor limited text rendering, spatial understanding, and counting.\n(a) Video generation quality in FVD (\u2193).\n(b) Audio generation quality in FAD (\u2193).\nFigure 8: Effects of model and data scale on video and audio generation quality. The performance, depicted on a log-log\nscale, improves significantly when we scale up the model and training data. Language models with 300 million, 1 billion,\nand 8 billion parameters are trained on datasets comprising 10, 37, and 58 billion visual and audio tokens, respectively.\nA.2.1. QUALITATIVE COMPARISON OF 1B AND 8B MODELS\nIn Figure 9, we show outputs of 1B and 8B parameter models on the same prompts. Four frames from the best video output\nof each model in a batch of four text-to-video samples were selected to represent the model. In the first row, the 1B model\nis unstable with large changes to the subject over time and misses elements from the complex prompt. This prompt was\noriginally used for scaling comparisons in (Yu et al., 2022), and compared to a dedicated image-only model, our model does\n13\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nprompt: A portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the\nSydney Opera House holding a sign on the chest that says Welcome Friends!\nprompt: A kangaroo holding a sign with the letter A on it\nprompt: A photo of an astronaut riding a horse in the forest. There is a river in front of them with water lilies\nprompt: A zoomed out map of the United States made out of sushi. It is on a table next to a glass of red wine. Pieces of\nsushi disappear one by one\nprompt: Rotating around a vase holding a dozen roses\nFigure 9: A comparison of a 1B (left) and 8B (right) parameter models on the same prompt and settings.\n14\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nOriginal Video\nOutpainted Video\nStylized Video\nPrompt: A gingerbread and candy train on a\ntrack\nFigure 10: Example of zero-shot video editing via task chaining (outpainting and stylization) \u2013 the original video is first\noutpainted and then stylized via a text prompt.\nnot preserve text as well given the training data used. In the second row, we use a simpler text task and show that the 8B\nmodel can represent a single letter clearly, but the 1B model still produces artifacts. In the third row, we show that the 8B\nmodel learns spatial positioning such as the river being in front of the astronaut and horse. In the fourth row, we show that\nthe 8B parameter model learned a stop motion style to have items disappear \u201cone by one\u201d and can follow a complicated\nlayout from a long prompt. In contrast, the 1B model includes all of the nouns, but is unstable over time and does not follow\nthe layout indicated in the prompt. In the bottom row, we show that the 8B model understands counts of objects in that it\ndisplays a full bouquet (though 12 roses are not explicitly in frame) and smooth consistent motion as opposed to the 5 roses\nand distorting objects produced by the 1B model. Overall, scaling the model improved temporal consistency, prompt fidelity,\nand motion dynamics while adding capabilities for limited text rendering, spatial understanding, and counting.\nA.3. Additional Generated Examples\nWe include most generated videos in the supplementary materials for an enhanced visualization of motion and visual quality,\nin addition to Fig. 10 and Fig. 11.\nA.4. Video Stylization\nTo perform video stylization, we follow an approach inspired by (Zhang et al., 2023b; Chen et al., 2023b; Esser et al., 2023)\nto predict videos from the combination of text, optical flow, and depth signals. On a subset of steps, we also condition on the\nfirst video frame. As described in (Esser et al., 2023), the text will generally define the \u201ccontent\u201d or appearance of the output\nand the optical flow and depth control the \u201cstructure.\u201d In contrast to the diffusion-based approaches that usually use external\ncross-attention networks (Zhang et al., 2023b) or latent blending (Meng et al., 2021) for stylization, our approach is more\nclosely related to machine translation using large language models in that we only need to provide the structure and text as a\nprefix to a language model.\nTo perform the task, we estimate optical flow from RAFT (Sun et al., 2022) and produce monocular depth maps from\nMIDAS (Ranftl et al., 2020), and then normalize and concatenate on the channel dimension. This conveniently produces the\n15\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nCamera Motion: Arc shot\nCamera Motion: FPV drone shot\nFigure 11: Examples of directed camera movement from the same initial frame.\nStylization\nControl-a-video preferred\nVideoPoet preferred\nVideo Quality\nText Fidelity\n22.5%\n77.5%\n30%\n70%\nFigure 12: Human side-by-side evaluations comparing VideoPoet with the video stylization model Control-a-\nvideo (Chen et al., 2023b). Raters prefer VideoPoet on both text fidelity and video quality. Green and pink bars\nrepresent the proportion of trials where VideoPoet was preferred over an alternative, or preferred less than an alternative,\nrespectively.\nsame number of channels as the RGB ground truth and so can be tokenized in the same fashion as RGB videos with the\nMAGVIT-v2 tokenizer without retraining the tokenizer. The task of stylization is to reconstruct the ground truth video from\nthe given optical flow, depth, and text information. During inference, we apply optical flow and depth estimation on an input\nvideo but then vary the text prompt to generate a new style, e.g. \u201ccartoon\u201d.\nTable 3: Comparison on video stylization. VideoPoet outperforms Control-A-Video by a large margin.\nModel\nCLIPSIM\nControl-A-Video (Chen et al., 2023b)[depth]\n0.3246\nVideoPoet (Ours)\n0.3417\nTo evaluate stylization capabilities, we choose 20 videos from the public DAVIS 20162 (Perazzi et al., 2016) dataset and\nprovide 2 style prompts for each video. For more details, please refer to Appendix A.5.6. Following (Esser et al., 2023), we\nevaluated the CLIP-embedding consistency between each frame and the text prompt to determine if the stylization results\nmatches the text. As shown in Table 3, VideoPoet outperforms Control-A-Video conditioned on depth by a large margin. We\nalso conduct human evaluations as discussed above comparing with Control-A-Video (Chen et al., 2023b). Human raters\nconsistently prefer our text fidelity and video quality as shown in Fig. 12.\n2DAVIS license: https://creativecommons.org/licenses/by-nc/4.0/deed.en\n16\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nSpecial Token\nUsage\n<bos>\nBeginning of sequence\n<task>\nTask to perform for this sequence\n<bot i>\nBeginning of the text input.\n<eot i>\nEnd of the text input.\n<bov i>\nBeginning of the visual input.\n<eov i>\nEnd of the video input.\n<boa i>\nBeginning of the audio input.\n<eoa i>\nEnd of the audio input.\n<source>\nThe source of the video to generate.\n<res>\nOutput resolution for the video.\n<bov o>\nBeginning of the video output.\n<eov o>\nEnd of the video output.\n<boa o>\nBeginning of the audio output.\n<eoa o>\nEnd of the audio output.\n<eos>\nEnd of the entire sequence.\nTable 4: List of representative special tokens used in training and inference.\nA.5. Additional Implementation and Evaluation Details\nA.5.1. ADDITIONAL IMPLEMENTATION DETAILS\nThe unified vocabulary is constructed as follows: the initial 256 codes are reserved for special tokens and task prompts.\nTable 4 lists some examples of special tokens. Subsequently, the next 262,144 codes are allocated for image and video\ntokenization. This is followed by 4,096 audio codes. We also include a small text vocabulary of English words. Overall, this\nproduces a total vocabulary size of approximately 300,000.\nSince the first frame is tokenized separately, MAGVIT-v2 allows images to be represented in the same vocabulary as video.\nIn addition to being more compact, images provide many learnable characteristics that are not typically represented in\nvideos, such as strong visual styles (e.g., art paintings), objects which are infrequently seen in video, rich captions, and\nsignificantly more text-image paired training data. When training on images, we resize the images to 128\u00d7128 which are\nthen tokenized to a latent shape of (1, 16, 16), or 256 tokens. We scale the MAGVIT-v2 model\u2019s size and train it on the\ndatasets discussed in Section 5.1. The training follows two steps: image training, inflation (Yu et al., 2023c) and video\ntraining. Due to images requiring fewer tokens, we can include roughly 5\u00d7 more images per batch than videos, i.e. 256\nimage tokens vs. 1280 video tokens. We use up to a maximum of 64 text tokens for all of our experiments. For the <res>\ntoken, the resolution is only specified for 128 \u00d7 224 output, 128 \u00d7 128 resolution is assumed otherwise.\nThe video-to-video tasks use the COMMIT encoding (Yu et al., 2023a) to obtain the tokens for the tasks such as inpainting\nand outpainting. Text is encoded as T5 XL embeddings (Raffel et al., 2020) and are inserted into reserved sequence positions\nright after the <bot i> token as shown in Fig. 2.\nA.5.2. SUPER-RESOLUTION IMPLEMENTATION DETAILS\nWe use a 1B model for the first 2\u00d7 spatial super-resolution stage and a 500M model for the second 2\u00d7 stage. The first super-\nresolution stage models videos of 17\u00d7448\u00d7256 pixels with a token sequence of shape (5, 56, 32). The second stage models\nvideos of 17 \u00d7 896 \u00d7 512 pixels with a token sequence of shape (5, 112, 64). The token sequences are obtained with the\nsame MAGVIT-v2 (Yu et al., 2023c) tokenizer used for the base language model. The custom super-resolution transformer\nhas local self-attention windows for vertical, horizontal and temporal layers of shape (1, 56, 4), (1, 8, 32), (5, 8, 8) in the\nfirst stage and (1, 112, 2), (1, 4, 64), (5, 8, 8) in the second stage, respectively (Fig. 3). The cross-attention layers attend to\nlocal windows in the low-resolution sequence isomorphic to self-attention windows but with half the spatial size.\nWe train the super-resolution stages on a dataset of 64M high-quality text-video pairs using the masked modeling objective\nof MAGVIT (Yu et al., 2023a), with token factorization into k = 2 groups (Yu et al., 2023c). During inference, we use the\nsampling algorithm of MAGVIT-v2 (Yu et al., 2023c) with 24 sampling steps for each stage and classifier-free guidance\n17\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nscale (Ho & Salimans, 2022; Brooks et al., 2023) of 4.0/8.0 for the text condition and 1.0/2.0 for the low-resolution\ncondition, in the first/second stage.\nA.5.3. ADDITIONAL EVALUATION DETAILS\nWe measure CLIP similarity scores (Wu et al., 2021) following an implementation given by Villegas et al. (2022), measure\nFVD (Unterthiner et al., 2018) following Yu et al. (2023a) on UCF101 dataset and following Zhang et al. (2023a) on\nMSR-VTT, and measure Inception Score (IS) (Saito et al., 2020). When the evaluation protocol is on 16 frames, we discard\nthe generated last frame to make a 16-frame video.\nA.5.4. ZERO-SHOT TEXT-TO-VIDEO EVALUATION SETTINGS\nWe report the details of our zero-shot text-to-video settings here. We note that some details are missing in previous papers\nand different papers use different settings. Hence, we provide all the details and hope this evaluation setting can serve\nas a standard text-to-video generation benchmark. Our results are reported on the 8B model and we adopt classifier-free\nguidance (Ho & Salimans, 2022).\nAll metrics are evaluated on generated videos containing 16 frames with a resolution of 256 x 256. We first generate videos\nof 128 x 128 resolution and then resize to 256 x 256 via bicubic upsampling.\nZero-shot MSR-VTT. For CLIP score, we used all 59,794 captions from the MSR-VTT test set. We use CLIP ViT-B/16 model\nfollowing Phenaki (Villegas et al., 2022). We note that some papers use other CLIP models, e.g., VideoLDM (Blattmann\net al., 2023b) uses ViT-B/32. Our CLIP score evaluated on the ViT-B/32 backbone for MSR-VTT is 30.01. For the FVD\nmetric, to evaluate on a wide range of captions as well as to be comparable with previous papers that evaluate on 2,048\nvideos, we evaluate on the first 40,960 captions in the MSR-VTT test set. More specifically, we report the FVD metrics on\n2048 videos with 20 repeats. The FVD real features are extracted from 2,048 videos sampled from the MSR-VTT test set.\nWe sample the central 16 frames of each real video, without any temporal downsampling, i.e., we use the original fps in the\nMSR-VTT dataset (30 fps as reported in Xu et al. (2016)). The FVD is evaluated with an I3D model trained on Kinetics-400.\nZero-shot UCF-101. Following VDM (Ho et al., 2022b), we sample 10,000 videos from the UCF-101 test set and use their\ncategories as the text prompts to generate 10,000 videos. We use the class text prompts provided in PYoCo (Ge et al., 2023)\nto represent the 101 categories. To compute the FVD real features, we sample 10K videos from the training set, following\nTGAN2 (Saito et al., 2020). We sample the central 16 frames for each real video , without any temporal downsampling, i.e.,\nwe use the original fps in the UCF-101 dataset (25 fps as reported in (Soomro et al., 2012)). The FVD metric is evaluated\nwith an I3D model trained on Kinetics-400 and the IS metric is evaluated with a C3D model trained on UCF-101.\nA.5.5. SELF-SUPERVISED TASKS EVALUATION SETTINGS\nSelf-supervised learning tasks include frame prediction on K600 with 5 frames as condition, as well as inpainting and\noutpainting on SSv2. FVD (Unterthiner et al., 2018) is used as the primary metric, calculated with 16 frames at 128\u00d7128\nresolution. We follow MAGVIT (Yu et al., 2023a) in evaluating these tasks against the respective real distribution, using\n50000\u00d74 samples for K600 and 50000 samples for SSv2.\nA.5.6. STYLIZATION EVALUATION ON DAVIS\nTo evaluate the CLIP similarity score and human preference on video stylization, we use the following set of videos and\nprompts. We select 20 videos from DAVIS 2016 (Perazzi et al., 2016), and for each video we take 16 frames starting from the\ninitial frame specified below and evaluate stylization on the two text prompts specified below. To be easily reproducible, we\nuse a central square crop at the height of the video and evaluate the output videos at 256x256 resolution. We use CLIP-B/16\nfor the similarity score. Several prompts below are used in or inspired by previous work (Esser et al., 2023; Chen et al.,\n2023b; Liew et al., 2023).\n18\nVideoPoet: A Large Language Model for Zero-Shot Video Generation\nvideo name\nstarting frame\nfirst text prompt\nelephant\n10\noil painting of an elephant walking away\nelephant\n10\ncartoon animation of an elephant walking through dirt surrounded by boulders\ncar-turn\n40\ncar on a snowcovered road in the countryside\ncar-turn\n40\n8-bit pixelated car driving down the road\ndog-agility\n0\na dog in the style of a comic book\ndog-agility\n0\na dog running through a field of poles in the style of cyberpunk\nbmx-bumps\n10\nriding a bicycle on a rainbow track in space with stars and planets in the background\nbmx-bumps\n10\nriding a bicycle on a dirt track in the style of a graphic novel\ntrain\n0\na gingerbread steam train made of candy\ntrain\n0\na train in lava\nbus\n0\na black and white drawing of a bus\nbus\n0\na bus in cyberpunk style\nlucia\n0\nan astronaut walking on mars\nlucia\n0\na claymation animation of a woman walking\ntennis\n15\na robot throwing a laser ball\ntennis\n15\nastronaut playing tennis on the surface of the moon\nbear\n60\na polar bear exploring on an iceberg\nbear\n60\na space bear walking beneath the stars\nflamingo\n0\n2D vector animation of a group of flamingos standing near some rocks and water\nflamingo\n0\noil painting of pink flamingos wading\nhike\n0\na green alien explorer hiking in the mountains\nhike\n0\npaper cut-out mountains with a paper cut-out hiker\ngoat\n59\na tiger prowling along the ridge above a jungle\ngoat\n59\na dragon prowling over a crater on the moon\nparkour\n60\na man jumping over rocks in a red sandstone canyon\nparkour\n60\na robot dodging through an obstacle course\ncows\n10\na pig standing in the mud\ncows\n10\na robotic cow walking along a muddy road\ncamel\n10\na camel robot on a snowy day\ncamel\n10\ntoy camel standing on dirt near a fence\nblackswan\n0\na watercolor painting of a white swan\nblackswan\n0\na crochet black swan swims in a pond with rocks and vegetation\ndog\n20\na cat walking\ndog\n20\na dalmatian dog walking\nkite-surf\n10\na sand surfer kicking up sand in the desert\nkite-surf\n10\nkite surfer in the ocean at sunset\nlibby\n0\nchinese ink painting of a dog running\nlibby\n0\n3D animation of a small dog running through grass\nhorsejump-high\n0\na cartoon of a magical flying horse jumping over an obstacle\nhorsejump-high\n0\nperson rides on a horse while jumping over an obstacle with an aurora borealis in the background\nTable 5: DAVIS stylization evaluation settings.\n19\n"
  },
  {
    "title": "Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis",
    "link": "https://arxiv.org/pdf/2312.13834.pdf",
    "upvote": "25",
    "text": "Fairy: Fast Parallellized Instruction-Guided Video-to-Video Synthesis\nBichen Wu\nChing-Yao Chuang\nXiaoyan Wang\nYichen Jia\nKapil Krishnakumar\nTong Xiao\nFeng Liang\nLicheng Yu\nPeter Vajda\nGenAI, Meta\nProject page: https://fairy-video2video.github.io\n[ Input Video ]\n[ Input Video ]\nIn Van Gogh Style\nIn Picasso Style\nTurn into a wood sculpture\nTurn into a metal knight sculpture\nFigure 1. Fairy for Instruction-Guided Video Editing. Given a video and an instruction for editing, Fairy performs accurate edits while\nensuring temporal coherence. Remarkably efficient, 120 frames of 512\u00d7384 video can be generated in just 14 seconds. We refer readers\nto our supplementary material to check the results in video format.\nAbstract\nIn this paper, we introduce Fairy, a minimalist yet ro-\nbust adaptation of image-editing diffusion models, enhanc-\ning them for video editing applications. Our approach cen-\nters on the concept of anchor-based cross-frame attention,\na mechanism that implicitly propagates diffusion features\nacross frames, ensuring superior temporal coherence and\nhigh-fidelity synthesis.\nFairy not only addresses limita-\ntions of previous models, including memory and process-\ning speed. It also improves temporal consistency through\na unique data augmentation strategy.\nThis strategy ren-\nders the model equivariant to affine transformations in both\nsource and target images. Remarkably efficient, Fairy gen-\nerates 120-frame 512\u00d7384 videos (4-second duration at 30\nFPS) in just 14 seconds, outpacing prior works by at least\n44\u00d7. A comprehensive user study, involving 1000 gener-\nated samples, confirms that our approach delivers superior\nquality, decisively outperforming established methods.\n1. Introduction\nThe advent of generative artificial intelligence has heralded\na new era of creative potential, characterized by the abil-\nity to create or modify content in an effortless manner. In\nparticular, image editing has undergone significant evolu-\ntion, driven by text-to-image diffusion models pretrained on\nbillion-scale datasets. This surge has catalyzed a vast array\narXiv:2312.13834v1  [cs.CV]  20 Dec 2023\nof applications in image editing and content creation.\nBuilding on the accomplishments of image-based mod-\nels, the next natural frontier is transitioning these capabili-\nties to the temporal dimension to enable effortless and cre-\native video editing. A direct strategy to leap from image\nto video, is to simply process a video on a frame-by-frame\nbasis using an image model. Nonetheless, generative im-\nage editing is inherently high-variance \u2013 there are countless\nways to edit a given image based on the same text prompt.\nAs a result, it is difficult to maintain temporal coherence if\neach frame is edited independently [30].\nPrevious and concurrent studies have proposed several\nways to improve the temporal consistency, and one promis-\ning paradigm is what we call tracking-and-propagation:\none first apply an image editing model on one or a few\nframes, then tracks pixels across all frames and propagate\nthe edit to the entire video. Existing works [4, 6, 12, 14, 16,\n19, 24, 33] track pixels mainly through optical flow or by\nreconstructing videos as some canonical layered representa-\ntions. Despite some successful applications, this paradigm\nis not robust, since tracking is an unsolved computer vision\nchallenge. Existing methods, including optical flow or lay-\nered video representation, often struggle with videos with\nlarge motion and complex dynamics.\nIn this work, we introduce Fairy, a versatile and ef-\nficient video-to-video synthesis framework that generates\nhigh-quality videos with remarkable speed (Figure 1). Our\nwork re-examines the tracking-and-propagation paradigm\nunder the context of diffusion model features. In partic-\nular, we bridge cross-frame attention with correspondence\nestimation, showing that it temporally tracks and propagates\nintermediate features inside a diffusion model. The cross-\nframe attention map can be interpreted as a similarity met-\nric assessing the correspondence between tokens through-\nout various frames, where features from one semantic re-\ngion will assign higher attention to similar semantic regions\nin other frames, as shown in Figure 3. Consequently, the\ncurrent feature representations are refined and propagated\nthrough a weighted sum of similar regions across frames via\nattention, effectively minimizing feature disparity between\nframes, which translates to improved temporal consistency.\nThe analysis gives rise to our anchor-based model, the\ncentral component of Fairy.\nTo ensure temporal consis-\ntency, we sample K anchor frames from which we ex-\ntract diffusion features, and the extracted features define a\nset global features to be propagated to successive frames.\nWhen generating each new frame, we replace the self-\nattention layer with cross-frame attention with respect to the\ncached features of anchor frames. With cross-frame atten-\ntion, the tokens in each frame take the features in anchor\nframes that exhibit analogous semantic content, thereby en-\nhancing consistency. In addition, by sampling K anchor\nframes instead of computing cross-attention with respect\nto all frames, Fairy achieves several advantages: (1) it en-\nsures temporal consistency by sharing the same global fea-\ntures, (2) it overcomes the memory issue due to extensive\nframe number, (3) it enhances processing speed through\nthe caching of anchor frame features, and (4) it streamlines\nparallel computation, thereby facilitating remarkably rapid\ngeneration on multiple GPUs.\nDespite the improvement from anchor-based cross-frame\nattention, the model is still sensitive to minor variations\nwithin the input frames, even with the same text prompt\nand initial latent noise.\nSuch small changes could stem\nfrom natural movements within a video sequence or from\naffine transformations applied to the input. The gold stan-\ndard solution is to train the model with pairs of original\nand edited videos, thereby accommodate it to recognize and\nadapt to such variations. However, collecting such a dataset\nis far from straightforward. To emulate these transforma-\ntions, we employ a data augmentation strategy. Starting\nwith an input image and its edited counterpart, we apply a\nsequence of affine transformations to both, generating suc-\ncessive frames. The assumption is that the affine transfor-\nmations applied to input images should correspondingly af-\nfect the edited images. This method of equivariant finetun-\ning leads to notable enhancements in temporal consistency.\nTo verify the effectiveness of Fairy, we conduct a large-\nscale evaluation consists of 1000 generated videos. Both\nhuman evaluation and quantitative metrics confirm that\nour model achieves significantly better quality. Moreover,\nthanks to the simplicity of the design and the parallelizable\narchitecture, Fairy achieves >44x speedup over baselines.\nIn short, this work makes the following contributions:\n(1) We adopt a series of simple yet effective adaptions that\ntransform an image-editing model for video-to-video syn-\nthesis. (2) We evaluate our approach via extensive human\nstudy with 1000 generated videos, confirming that Fairy de-\nlivers superior quality over prior state-of-the art methods.\n(3) Fairy is blazing fast, achieving >44x speedup over pre-\nvious methods when utilizing 8-gpu parallel generation.\n2. Related works\nConditional video generation: Following the success of\ndiffusion models in text-to-image generation [7, 22, 23, 25],\nthere has been a surge in video generation. Based on a text-\nto-video model, video-to-video generation can be achieved\nby conditioning the model on attributes extracted from a\nsource video. For example, Gen-1 [9] conditions on the\nestimated depth while VideoComposer [31] integrates addi-\ntional cues, such as depth, motion vectors, sketches, among\nothers. Building such models requires training on video\ndatasets, which are much more scarce than image datasets\n[26]. Training such models also imposes considerable com-\nputational demands. Consequently, these constraints con-\nfine video models to reduced resolution, shorter duration,\nand smaller model size, leading to a decline in visual quality\nwhen contrasted with contemporary image generation mod-\nels. In comparison, our model is adapted from a pretrained\nimage-to-image model. Our finetuning only requires image\ndata, and the training cost (30 hours on 8 A100 GPUs) is\nsubstantially smaller than video models.\nTracking and propagation: this paradigm involves ini-\ntiating edits on a single image, identifying pixel correspon-\ndences across the video sequence, then propagating the edit.\nThe key in this approach lies in tracking. Numerous ef-\nforts [12, 24, 33] have adopted optical flow, keypoint track-\ning, or other motion cues to tackle this. Another stream\nof efforts [4, 6, 14, 16, 19] reconstruct the video using a\nmulti-layer canonical representation, associating pixels to\ncanonical points on the representation.\nHowever, video\ntracking is an unsolved computer vision challenge and of-\nten fails on complex videos. Additionally, tracking-and-\npropagation does not allow edits to alter object contours,\nwhich breaks the pixel correspondence. Instead of tracking\nin pixel space, our model leverages cross-frame attention to\nimplicitly track corresponding regions and propagate fea-\ntures to reduce frame discrepancy. Owing to the robust-\nness and versatility of diffusion features, as also observed\nin Tang et al. [27], our approach accommodates a broader\nspectrum of videos and offers enhanced editing flexibility.\nImage model adaptation:\nMany works also adapt\nimage-to-image models to video. For example, [15] mod-\nifies self-attention in diffusion models. [32] performs per-\nvideo finetuning and utilizes a inversion-denoising proce-\ndure for editing.\n[10, 17, 21, 29] adapt image-to-image\npipelines [5, 11, 28] to edit videos, by modifying/adding\ncross-frame attention modules, null-text inversion, etc.\nMost of these methods can only generate video clips with\na small number of frames, while [10] leverages a nearest-\nneighbor field on diffusion features to propagate key frame\nfeatures to more frames. Our model improves the design of\nspatial temporal attention [15, 17, 21, 29] to anchor-based\ncross-frame attention, which enables generating long videos\nwith arbitrarily many frames. We further improves its tem-\nporal consistency by equivariant finetuning. Our work bears\nresemblance to the concurrent work [10]. To edit a video,\n[10] first performs a latent inversion on the original video,\nextract a nearest-neighbor field, which is then used for fea-\nture propagation to generate the target video. Our pipeline\nis much simpler and more efficient. We do not require latent\ninversion; and the feature propagation is achieved through\nattention; our architectures naturally allows parallel gener-\nation. As a result, our model is 53 times faster than [10].\n3. Preliminaries\nVideo-to-Video Diffusion Models\nIn this work, we pri-\nmarily focus on instruction-guided video editing. Given an\ninput video with N frames I = {I1, . . . , IN} \u2208 IN, the\ngoal is to edit it into a new video I\u2032 = {I1\u2032, . . . , IN \u2032} \u2208 IN\naccording to an natural language instruction c \u2208 T that\npreserves the semantic of the original video. A straight-\nforward baseline is to adopt an image-based editting model\nf : (I, T ) \u2192 I to edit the video frame by frame: I\u2032 =\n{f(I1, c), . . . , f(IN, c)}. In this work, we build upon this\nline of work and improve the consistency with a variant of\ncross-frame attention.\nSelf-attention and Cross-frame attention\nSelf-attention\nhas played a crucial role in the diffusion networks. In a self-\nattention block, features of tokens are projected into queries\nQ \u2208 Rn\u00d7d, keys K \u2208 Rn\u00d7d, and values V \u2208 Rn\u00d7d, where\nthe output is defined as\nSelfAttention(Q, K, V ) = softmax\n\u0012QKT\n\u221a\nd\n\u0013\nV .\nThe output from the softmax is commonly referred to as\nthe attention score or attention map. Given N frames, to\nextend the self-attention to cross-frame attention, one can\nsimply concat the keys and values from all frames, e.g.,\nK\u2217 = [K1, \u00b7 \u00b7 \u00b7 , KN], and compute the self-attention as\nSelf-Attention(Q, K\u2217, V \u2217). In particular, cross-frame at-\ntention provides temporal modeling capability by attending\nto other frames and shows encouraging results in improving\ntemporal consistency [17, 30].\n4. Implicit Tracking via Cross-frame Attention\nWe first bridge cross-frame attention with correspondence\nestimation, fostering a straightforward yet effective feature\npropagation mechanism for video-to-video generation.\nThe primary objective of self-attention is to select ap-\npropriate values V with the attention scores determined by\nQKT . In the case of cross-frame attention, given a token\nlocation p in a frame, the attention score is computed by\nthe cosine similarity between Qp,: and each token in K\u2217,\nwhere the key values V \u2217 are the features of tokens across\nboth spatial and temporal dimension.\nIt is noteworthy that the mathematical formulation ex-\nhibits profound similarities to feature propagation mecha-\nnisms. Specifically, the attention score serves as the esti-\nmated correspondence, and the output of attention module\ncould be interpreted as a fused representation of warped fea-\ntures derived from successive frames. We will empirically\nsubstantiate this hypothesis through analyses of the tracking\nbehavior inherent in the attention score.\n4.1. Temporal Tracking with Attention Score\nIn this section, we provide evidences that the attention\nscores in cross-frame attention implicitly serve as a corre-\nspondence estimation across frames. In particular, we adopt\na conditional image-to-image diffusion model and examine\n(a) \u03b4x = 16\n(b) \u03b4x = 32\nPosition Accuracy\nLayers\nLayers\nFigure 2. Position Accuracy \u03b4x on DAVIS. The cross-frame at-\ntention score demonstrates significant tracking proficiency, partic-\nularly evident in the initial and final stages of the UNet.\nthe attention map between two frames of a video clip. Con-\nsider Qt and Kt as the respective query and key represen-\ntations corresponding to the frame at time t. To corrobo-\nrate our conjecture regarding the role of attention scores,\nwe designate a specific query point p at time t and endeavor\nto ascertain its corresponding coordinate q at a subsequent\ntime t\u2032 through the expression:\nq = arg max\np\u2032\nAp,p\u2032, where A = softmax(QtKt\u2032T\n\u221a\nd\n),\nwhere Ap,p\u2032 denotes the element of the matrix A located at\nthe row index p and column index p\u2032. The correspondence\nis estimated by selecting the location p\u2032 with the highest at-\ntention score with respect to p. For multi-head attention, we\naverage the attention scores from all heads. By evaluating\nthe tracking ability of the proposed estimator, we can verify\nwhether the attention scores are good correspondence esti-\nmator for feature propagation.\n4.2. Video Tracking Experiments: TAP-Vid\nIn our evaluation, we utilize the DAVIS datasets from the\nTAP-Vid [8, 20], with 30 videos clips ranging from 34-104\nframes. The frames are resize to 256 \u00d7 256 for evaluation.\nWe measure the < \u03b4x position accuracy proposed in TAP-\nVid, which calculates the fraction of points that are within\n\u03b4x pixels of their ground truth position. The dimensions of\nthe attention map inherently impose a constraint on the pre-\ncision achievable in point tracking. Since diffusion UNets\nadopts spatial downsampling, we configure \u03b4x at the values\nof 16 and 32 for our experiments. We set the number of\ndiffusion step to 10 with Euler ancestral sampler [13].\nFigure 2 shows the position accuracy for attention scores\nacross different layers and diffusion step. We can see that\nthe first and last few layers demonstrate a strong tracking re-\nsults, achieving over 60%/70% accuracy for \u03b4x = 16/32.\nInterestingly, the results are consistent across different dif-\nfusion step, demonstrating the strong tracking ability of\ncross-frame attention. The observed degradation in accu-\nracy at the central layers of the UNet architecture can be\nFigure 3. Visualization of Attention Score. The left image shows\nthe query point p within the current frame, and the right image is\nthe target frame. Cross-frame attention performs accurate tempo-\nral correspondence estimation without any finetuning.\nattributed primarily to the reduction in the spatial resolution\nof the feature maps. For instance, within the seventh layer\nof the network, the feature map dimensions are constrained\nto 4 \u00d7 4. Figure 3 visualizes the attention score on a target\nframe given a query point. We can see that the attention\nmap locate the corresponding position in target frame.\nCross-frame Attention \u2248 Tracking and Feature Propa-\ngation\nOur experimental findings disclose an unexpect-\nedly potent tracking capability associated with the attention\nscore. These results robustly validate our hypothesis: even\nin the absence of explicit finetuning, cross-frame attention\nimplicitly executes a formidable feature propagation mech-\nanism. In particular, features V \u2217 from alternative frames\nare transmitted to the current frame based on the correspon-\ndence determined through the attention scores.\n5. Fairy: Fast Video-to-Video Synthesis\nBuilding on the analyses, we present Fairy, an efficacious\nvideo-to-video framework that leverages the inherent fea-\nture propagation of cross-frame attention. In particular, we\npropose to propagate the value features from a collection of\nanchor frames to a candidate frame using cross-frame atten-\ntion. The performance can be further enhanced through the\nproposed equivariant finetuning method. We also demon-\nstrate that Fairy is easily parallelizable, facilitating fast gen-\neration of arbitrarily long videos.\n5.1. Anchor-Based Model\nInspired by prior research in tracking-and-propagation,\nwhere the edits to one or a few frames are propagated to\nthe entire video, we sample a set of anchor frames and\nedit them with an image-based model f : (I, T ) \u2192 I.\nSimilarly, our objective is to extend the edits in the an-\nchor frames to the successive frames, but utilizing cross-\nframe attention mechanisms instead of optical flow or ex-\nplicit point tracking. In particular, given a set of anchor\nframes Ianc = {\u02dcI1, . . . , \u02dcIK} \u2286 I = {I1, . . . , IN}, we\ntreat them as a batch and feed them to the diffusion model\nf, where the self-attention in the model is replaced with\ncross-frame attention in a zero-shot manner. Throughout\nthe diffusion process, for each anchor frame \u02dcIn, we store its\nAnchor\nAnchor\nQ\nK\nV\nQ\nK\nV\n(a) Extract and cache anchor features\n(b) Anchor-based Cross-frame Attention\nanc\nanc\nanc\nInput\nAnchor\nFigure 4. Illustration of Attention Blocks (a) Given a set of an-\nchor frames, we extract and cache the attention feature Kanc and\nVanc. (b) Given an input frame, we perform cross-frame attention\nwith respect to the cached features of anchor frames.\nkey and value vectors Kn,l,t, V n,l,t for every cross-frame\nattention layer l and every diffusion step t in cache. Intu-\nitively, V n,l,t defines a set global features to be propagated\nto successive frames. To simplify the notation, we will drop\nthe subscript l and t in the following sections.\nLet Kanc = [K1, . . . , KK] and Vanc = [V 1, . . . , V K]\nbe the concatenated anchor key and value vectors. To edit\nany frame It \u2208 I, we modify the self-attention module to\nthe cross-frame attention with respect to the key and value\nvectors of anchor frames as follows:\nsoftmax\n\u0012Q[K, Kanc]T\n\u221a\nd\n\u0013\n[V , Vanc],\nwhere Q, K and V are the self-attention vectors of the in-\nput frame It. The idea is that the attention score generated\nby the softmax facilitates cross-frame tracking by estimat-\ning the temporal correspondence between the input frame\nand anchor frames. The global value vectors then be propa-\ngated to input frame by multiplying the attention score with\nVanc.\nBy substituting the self-attention module with an\nanchor-based cross-frame attention mechanism, we found\nthat the model could generate highly consistent video edits.\nIn the default setting, we choose anchor frames uniformly\nacross the video, and we did not notice consistent perfor-\nmance improvement or degradation when adopting different\nanchor-frame selection strategies.\nFast Generation via Parallelization\nNote that editing\nframe It does not require other frames as input except the\ncached features Kanc and Vanc from anchor frames. There-\nfore, we can edit arbitrary long videos by splitting them into\nsegments and leverage multi-GPUs to parallize the gener-\nation, while the computation remains numerically identi-\ncal. As a result, our method achieves significant speedup\ncompared to previous works. Moreover, it delivers supe-\nrior quality outputs without succumbing to memory-related\nconstraints. This efficiency underscores our approach\u2019s en-\nhanced scalability and practicality, setting a new benchmark\nfor performance in the realm of video editing.\n5.2. Equivariant Finetuning\nWhile anchor-based attention greatly improves the qual-\nity, we still occasionally observed temporal inconsistency.\nIn particular, we found that for generated contents that do\nnot have semantic correspondence with the input, small\nchanges in input frames can cause significant variances in\nthe output frames.\nTo improve the consistency, we leverage the following\nintuition to design a data augmentation strategy. In particu-\nlar, if an input frame It differs from It\u22121 only in the camera\nposition, then the output frame \u02c6It and \u02c6It\u22121 should only be\ndifferent in the camera position as well. This inspires us to\ncome up with a data augmented strategy that can be applied\nto any image editing dataset to imporve the temporal con-\nsistency. Given a pair of images, the original and the edited,\ndenoted as (I, I\u2032), we randomly sample an affine transfor-\nmation g : I \u2192 I and apply them to both images to obtain\n(g(I), g(I\u2032)). We implement this using torchvision\u2019s ran-\ndom affine transformation [2], setting random rotations de-\ngrees to < 5\u25e6, random translation to [\u22120.05, 0.05], random\nscaling factor to [0.95, 1.05], and random shear degrees to\n[\u22125\u25e6, 5\u25e6] on both axis. We also apply random resized crop,\nscaling the original image to 288pix, and randomly crop a\nsquare image with 256 pix. We then fine-tuned the base\nimage-to-image model to generate the transformed g(I\u2032)\ngiven the transformed g(I). The proposed fine-tuning pro-\ncess makes the model equivariant to affine transformations,\nleading us to denote our approach as equivariant finetuning.\nEmpirically, we observe a notable enhancements in tempo-\nral consistency after finetuning (Section 6.3).\n6. Results\nWe implement Fairy based on an instruction-based image\nediting model, similar to [5], and replace the model\u2019s self-\nattention with cross-frame attention. We set the number of\nanchor frames to 3. Anchor frames are uniformly selected\nwith equal intervals among all frames. The model can ac-\ncept input with different aspect ratios, and we rescale the\ninput resolution with the longer size to be 512, and keep\nthe aspect ratio unchanged. We edit all frames of the input\nvideo, without temporal downsampling. We distribute the\ncomputation to 8 A100 GPUs. We use the Euler Ancestral\nsampler with 10 diffusion steps.\nFor equivariant finetuning, we use the same dataset that\nwas used to train the image editing model, and apply the\ndata augmentation discussed in Section 5.2. We load the\nimage editing model\u2019s pretrained checkpoint, and resumed\n[ Input Video ]\nTurn into lion\n[ Input Video ]\nMake it Tokyo\n[ Input Video ]\nMake it a wood sculpture\n[ Input Video ]\nTurn into a vintage car\nFigure 5. Diverse Video Editing via Fairy. Fairy enables a wide range of video edits with different types of subjects.\ntraining for 50,000 steps with a batch size of 128, costing\n30 hours on 8 A100 GPUs with 80GB memories.\n6.1. Qualitative Evaluation\nWe first show qualitative results of Fairy. Since most of the\nPDF readers do not render videos properly, we only show\na small number of frames for each video. We strongly rec-\nommend readers to checkout our supplementary materials\nto watch the complete videos. In Figure 5, we show that\nour model is capable conducting edits on different subjects.\nIn Figure 6, we show that our model is able to conduct\ndifferent types of editing, including stylization, character\nswap, local editing, and attribute editing, following textual\ninstructions. In Figure 9, we show that our model can trans-\nform the source character into different target characters\nbased on instructions. Note that our model can adapt to dif-\nferent input aspect ratios without need for re-training. Our\ninput videos contain large motions, occlusions, and other\ncomplex dynamics. Despite those challenges, videos gen-\nerated by our model are temporally consistent and visually\nappealing. We also show our model\u2019s capabilities to gener-\nate long videos in the supplementary material.\n6.2. Quantitative Evaluation\nQuantitatively evaluating video generative models is chal-\nlenging.\nFirst, the generation task is intrinsically high-\nvariance \u2013 there are countless ways to edit an video given\nthe instruction. Second, previous works have adopted met-\nrics such as CLIP scores [9, 10] to evaluate the generation\nquality. However, these metrics are not necessarily aligned\nwith human perception [18]. Lastly, human evaluation is\nstill the golden standard to judge the quality. Yet, due to the\ncost of human evaluation, previous works have only con-\nducted small scale human evaluations (< 100 samples).\nIn this paper, we conduct a large-scale user study on an\nevaluation set consists of 1000 video-instruction samples.\nThe evaluation set is divided into two parts: first, to test a\nmodel\u2019s robustness across different videos, we construct the\nevaluation set of 50 videos \u00d7 10 instructions. And to test\na model\u2019s robustness across different instructions, we con-\nstruct a dual evaluation set of 10 videos \u00d7 50 instructions.\nThe videos are accessible from ShutterStock [3]. To our\nbest knowledge, this is the largest evaluation in the video-\nto-video generation literature so far.\nWe conduct a A/B comparison to compare our method\nwith three previous works, Rerender [33] (tracking and\npropagation), Tokenflow [2] (image model adaptation),\nand Gen-1 [9] (conditional video model), which are the\nstrongest representative of the three paradigms for video-\nto-video generation today. Results from baselines are col-\nlected from [1]. Prompts for baselines are descriptive, e.g.,\n\u201da dog running on grass, in Van Gogh style\u201d. We re-write\nthe prompt for our method as an edit instruction, e.g., \u201din\nVan Gogh Style\u201d. Since Gen-1 is not open-sourced, the\nevaluation is done on a smaller evaluation set of 100 videos.\nIn each evaluation tuple, we show the input video, the edit-\ning instruction or prompt, and the output videos generated\nby Fairy and a baseline. We ask human evaluators to choose\nthe better video in terms of their single frame quality, tem-\nporal consistency, prompt faithfulness, input faithfulness,\nand overall quality. Each comparison is rated by 3 different\nannotators and the decision is determined by the majority\nvote. We report the overall quality comparison in Figure 7,\nwhich demonstrates that videos produced by Fairy are more\npreferable, with a win rate of 41% vs 36% against Rerender,\n73% vs. 16% against TokenFlow, and 72% vs 26% against\nGen-1. More details in the supplementary material.\ncharacter swap\nstylization\nattribute edit\nlocal edit\nIn Monet style\nAdd sunglasses\nMake it black\nTurn into a cartoon lion cub\nFigure 6. Different type of editing. Fairy is able to handle a\ndiverse set of instructions and perform appropriate editing.\n(a) vs. Rerender\n(b) vs. TokenFlow\n(c) vs. Gen-1\n10%\n1%\n16%\n73%\nFairy (Ours)\nBaseline\nBoth Good\nBoth Bad\n17%\n6%\n36%\n41%\n2%\n26%\n72%\nFigure 7. A/B Comparison with Baselines. Fairy significantly\nsurpassed baseline models, demonstrating its effectivity.\nLatency (sec) \u2193 Frame-Acc \u2191 Tem-Con \u2191\nTokenFlow\n744\n0.537\n0.973\nRerender\n608\n0.775\n0.972\nOurs\n13.8\n0.819\n0.974\nTable 1. We assess our method\u2019s temporal consistency and fidelity\nto the target text prompt using CLIP similarity metrics.\nFigure 10 shows the visual comparison with the base-\nlines. We observe that both Tokenflow [10] and Rerender\n[33] do not adhere closely to the provided instructions, re-\nsulting in evident inconsistencies. Outputs from Gen-1 of-\nten over-modify the entire scene and do not retain the orig-\ninal content effectively. In contrast, Fairy meticulously fol-\n+ Anchor   Equi-Finetune\n+\n+ Anchor   Equi-Finetune\n\u2212\n\u2212 Anchor   Equi-Finetune\n\u2212\nInput (Prompt: Turn into lion)\n(Tem-Con: 0.959)\n(Tem-Con: 0.968)\n(Tem-Con: 0.974)\nFigure 8. Ablation Study. Without equivariant fine-tuning and\nanchor-based attention, we observed inconsistencies, particularly\nin the changing patterns of body and earscostumes over time. This\ninconsistency is further exacerbated upon the removal of anchor-\nbased attention, leading to lower temporal consistency score.\nlows the instruction, delivering high-quality, temporal con-\nsistent, and authentic generations.\nLastly, we compute metrics adopted by previous works,\nTem-Con and Frame-Acc [21, 33]. Tem-Con assesses tem-\nporal consistency by calculating the cosine similarity of\nCLIP feature across successive frame pairs, and Frame-Acc\nmeasures the percentage of frames where the edited image\nexhibits greater CLIP similarity to the target prompt than\nto the source prompt. The results in Table 1 demonstrates\nthat Fairy achieves the best temporal consistency and frame-\nwise editing accuracy against the baselines.\nSpeed Comparison\nIn Table 1, we also compare the la-\ntency of different models. In particular, we calculate the\ninference time of editing a 4-seconds, 30 FPS, 512p\u00d7384p\nvideo on a server with 8 A100 GPUs. The key-frame inter-\nval of Rerender is set to 4 instead of the default 10, since the\ntest videos contain faster motion. This leads to improved\nquality for Rerender. All other parameters were default.\nDue to its architecture simplicity, Fairy is already signif-\nicantly faster than baselines using 1 GPU. Using a single\nGPU, Fairy completes inference in just 78 seconds, achiev-\ning 9.5\u00d7 faster than TokenFlow and 7.5\u00d7 faster than Reren-\nder. When utilizing all 8 GPUs on the node, Fairy is 53\u00d7\nfaster than TokenFlow and 44\u00d7 faster than Rerender.\n6.3. Ablation Studies\nWe conduct an ablation study to verify the effectiveness of\nour model\u2019s component. We gradually remove equivariant\nfine-tuning and anchor-based attention, ultimately leading\nto the adoption of a standard frame-by-frame editing ap-\nproach.\nThe results are shown in Figure 8.\nThe model\nbecomes sensitive to the camera motion without equivari-\nant finetuning, rendering inconsistency in the details. The\nsubsequent removal of anchor-based attention, transitioning\nto a frame-based model, introduces further inconsistencies\nin the generated video. We compute the Tem-Con metric\n[ Input Video ]\nMake it marble roman sculpture\nTurn into a bronze statue\nTurn into a robot\nTurn into a yeti\nTurn into a metal knight sculpture\nFigure 9. Diverse Character Swap: Fairy possesses the capability to interchange the individual with a diverse array of characters.\nTokenFlow\nRenderer\nGen-1\nFairy (ours)\nTokenFlow\nRenderer\nGen-1\nFairy (ours)\nIn Van Gogh style\nTurn into a wood \nsculpture\nFigure 10. Comparison with Baselines. Fairy consistently outperform baselines in terms of consistency and instruction-faithfulness.\nbased on 150 videos and report in Figure 8. It confirms our\nobservation that the proposed methodology effectively im-\nproves the temporal consistency, lifting the Tem-Con from\n0.959 (baseline) to 0.968 (w/ anchors) to 0.974 (w/ anchor\nand equivariant finetuning).\n6.4. Limitations\nThe efficacy of Fairy is intrinsically tied to the underlying\nimage-editing model. This means that any inherent con-\nstraints of this underlying model, e.g., face and text distor-\ntion, etc., will naturally manifest in the video editing capac-\nities of Fairy . In our observations, a notable side effect of\nequivariant finetuning is the diminished ability to accurately\nrender dynamic visual effects, such as lightning or flames.\nThe process seems to overly focus on maintaining temporal\nconsistency, which often results in the depiction of lightning\nas static or stagnate, rather than dynamic and fluid. See the\nsupplementary material for visualization.\n7. Conclusion\nFairy offers a transformative approach to video editing,\nbuilding on the strengths of image-editing diffusion mod-\nels. By leveraging anchor-based cross-frame attention and\nequivariant finetuning, Fairy guarantees temporal consis-\ntency and superior video synthesis. Moreover, it tackles\nthe memory and processing speed constraints observed in\npreceding models.\nWith the capability to produce high-\nresolution videos at a blazing speed, Fairy firmly establishes\nits superiority in terms of quality and efficiency, as further\ncorroborated by our extensive user study.\nReferences\n[1] Exploring\nvideo-to-video\nsynthesis:\nA\ncompara-\ntive\nanalysis\nof\nrerender,\ntokenflow,\nand\ngen-1.\nhttps://medium.com/@lwen9595/exploring-\nvideo-to-video-synthesis-a-comparative-\nanalysis-of-rerender-tokenflow-and-gen-\n1-9a63f281c4e1. 6\n[2] pytorch documentation.\nhttps://pytorch.org/\nvision/main/transforms.html. Accessed: 2023-\n11-02. 5, 6\n[3] Stock footage video, royalty-free hd, 4k video clips, 2023. 6\n[4] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In European conference on computer vi-\nsion, pages 707\u2013723. Springer, 2022. 2, 3\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n3, 5\n[6] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 23040\u201323050, 2023. 2, 3\n[7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 2\n[8] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri`a Re-\ncasens, Lucas Smaira, Yusuf Aytar, Jo\u02dcao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for track-\ning any point in a video. Advances in Neural Information\nProcessing Systems, 35:13610\u201313626, 2022. 4\n[9] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 2, 6\n[10] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3, 6, 7\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[12] Ond\u02c7rej Jamri\u02c7ska, \u02c7S\u00b4arka Sochorov\u00b4a, Ond\u02c7rej Texler, Michal\nLuk\u00b4a\u02c7c, Jakub Fi\u02c7ser, Jingwan Lu, Eli Shechtman, and Daniel\nS`ykora. Stylizing video by example. ACM Transactions on\nGraphics (TOG), 38(4):1\u201311, 2019. 2, 3\n[13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 4\n[14] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 2, 3\n[15] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3\n[16] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Eliza-\nbeth Qiu, and Jia-Bin Huang. Shape-aware text-driven lay-\nered video editing. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n14317\u201314326, 2023. 2, 3\n[17] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[18] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,\nYong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Ray-\nmond Chan, and Ying Shan. Evalcrafter: Benchmarking and\nevaluating large video generation models.\narXiv preprint\narXiv:2310.11440, 2023. 6\n[19] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 2, 3\n[20] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 4\n[21] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 3, 7\n[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[24] Alexander S. Disco diffusion v5.2 - warp fusion. https:\n//github.com/Sxela/DiscoDiffusion-Warp. 2,\n3\n[25] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[26] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 2\n[27] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 3\n[28] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 3\n[29] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[30] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 2, 3\n[31] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2\n[32] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 3\n[33] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 2, 3, 6,\n7\nA. Webpage Demo\nThe videos in the main paper and appendix can be viewed\nwith our demo webpage by opening the webpage/\nindex.html in the supplementary material using a web\nbrowser.\nB. Additional Results from Human Evaluation\nAs mentioned in Section 6.2, we conduct A/B comparison\nbetween Fairy with baselines. We ask annotators to com-\npare our generated video with a baseline method\u2019s result,\nand decide which one is better.Each video pair is evaluated\nby three independent annotators, and the majority vote is\nconsidered as the final rating. We ask raters to evaluate\nby four attributes: frame quality \u2013 visual quality of single\nframe; temporal consistency \u2013 whether the frames are co-\nherent or flickering; prompt faithfulness \u2013 whether the out-\nput followed the editing instruction or target prompt; input\nfaithfulness \u2013 whether the output video followed the con-\ntents of the original video.\nWe reported the overall rat-\ning in Section 6.2, Figure 10. Here, we report a more de-\ntailed comparison along each attributes in Figure 11, 12, 13.\nCompared with Rerender (Figure 11), Fairy loses in terms\nof single frame visual quality. This is mainly due to the\nlimitation of the foundational image editing model, while\nRerender utilizes LoRA to enhance frame quality.\nYet,\nFairy significantly outperforms Rerender in terms of tem-\nporal consistency, achieves better prompt faithfulness, and\nperforms similarly in terms of input faithfulness. Compared\nwith TokenFlow, Fairy outperforms significantly in terms\nof frame quality, temporal consistency, and prompt faithful-\nness. They performs similarly in terms of input faithfulness.\nCompared with Gen-1, Fairy significantly outperforms in\nall attributes.\nIn addition to the A/B comparison, in which we ask hu-\nman annotators to compare our method with a baseline,\nwe also conduct a standalone evaluation to examine output\nvideo\u2019s quality. Each time we show an annotator the orig-\ninal video, an editing instruction, and the result video. We\nthen ask the annotator to rate the output as good or bad by\nthe same four attributes. We ask 3 annotators to rate each\nvideo, and the decision is determined by their majority vote.\nWe report the success rate by each attributes in Figure 14.\nC. More Results\nC.1. Character Swap\nIn Figure 15, we demonstrate more results of character\nswap, where Fairy is able to interchange individuals with\nvarious characters. Note that our model can adapt to differ-\nent input aspect ratios without need for re-training.\n7%\n28%\n28%\n37%\nPrompt faithfulness\n82%\n7%\n11%\nInput faithfulness\n9%\n21%\n15%\n55%\nFairy (Ours)\nBaseline\nBoth Good\nBoth Bad\nTemporal consistency\n4%\n25%\n51%\n20%\nFrame visual quality\nFigure 11. Comparison with Rerender.\n6%\n14%\n13%\n67%\nPrompt faithfulness\n93%\n5%\n2%\nInput faithfulness\n6%\n24%\n14%\n55%\nFairy (Ours)\nBaseline\nBoth Good\nBoth Bad\nTemporal consistency\n5%\n11%\n24%\n60%\nFrame visual quality\nFigure 12. Comparison with TokenFlow.\n26%\n16%\n59%\nPrompt faithfulness\n43%\n1%\n56%\nInput faithfulness\n3%\n15%\n13%\n69%\nFairy (Ours)\nBaseline\nBoth Good\nBoth Bad\nTemporal consistency\n2%\n9%\n30%\n59%\nFrame visual quality\nFigure 13. Comparison with Gen-1.\nFrame quality\nTemporal consistency\nPrompt faithfulness\nInput faithfulness\nEvaluation Attributes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n0.59\n0.62\n0.78\n0.89\nSuccess Rate of Fairy by Attributes\nFigure 14. Standalone success rate by attributes. We report\nFairy\u2019s success rate in terms of frame quality, temporal consis-\ntency, prompt faithfulness, and input faithfulness.\nC.2. Stylization\nFigure 16 demonstrates more stylization results of Fairy. In\nparticular, our model is able to recognize various styles,\nwhile perform high quality and temporal consistent edit\nbased on the stylization instructions.\nC.3. Arbitrary Long Videos\nFairy is able to scale to arbitrary long video without mem-\nory issue due to the proposed anchor-based attention. In\nFigure 17, we show that our model is able to generate a\n27 second long video with high quality, while the latency\nis less than 71.89 seconds via 6 A100 GPUs. In particu-\nlar, the Fairy manage to retain decent temporal consistency\neven number of frames (664 frames) is way more than the\nnumber of anchor frames (3 frames).\nC.4. Ablation Study\nFigure 18 shows more ablation results by removing equiv-\nariant finetuning and anchor-based attention. We can see\nthat without equivariant finetuning, the model is sensitive\nto local motion and movement of the subject and therefore\ndegenerate the frame quality and temporal consistency. For\ninstance, in the first video, the tail of the cat becomes the\nhead of the lion in some of the frames, and the face of the cat\nin second video vary significantly between frames. Without\nanchor-based attention, the edit of each frame is completely\nindependent, rendering in significant worse temporal con-\nsistency.\nFigure 19 demonstrates results generated with different\nnumber of anchor frames. When number of anchor frames\nequals to 1, the global features model can leverage are too\nrestricted, which lead to suboptimal edits. In contrast, we\nobserve that when the number of anchor frames is greater\nthan 7, the quality also gradually degrades, losing some vi-\nsual details.\nIn Figure 20, we perform ablation study on the number\nof diffusion steps during generation. The model perform\nreasonably well when the number of diffusion step is above\n10. We therefore set the diffusion step to 10 for all of our\nexperiments to optimize the latency.\nC.5. Limitations\nFinally, Figure 21 demonstrates some limitations we point\nout in section 6.4. Since the model is never trained on video\ndata, it does learn to generate concepts containing motion\nsuch as raining, lightning, or flames. Fairy also inherits the\nlimit of the image editing model, where it is not able to\nfollow the instructions that involve camera motion, such as\nzoom in or zoom out.\n[ Input Video ]\nTurn into a baby lion cub\n[ Input Video ]\n[ Input Video ]\nTurn into a marble Roman sculpture\nTurn into a bronze statue\n[ Input Video ]\nTurn into a wood sculpture\n[ Input Video ]\nTurn into a fox\nFigure 15. Additional Results on Character Swap: Fairy is able to interchange the characters for videos with arbitrary ratios.\n[ Input Video ]\nIn Van Gogh style\nIn Monet style\n[ Input Video ]\n[ Input Video ]\nIn low poly art style\n[ Input Video ]\nIn cubism style\nFigure 16. Additional Results on Stylization: Fairy enables a wide range of style editing.\n27 sec (664 frames)\n26 sec (633 frames)\n25 sec (610 frames)\nIn Van Gogh style\nTurn into a knight\nIn low poly art style\n[ Input Video ]\n[ Input Video ]\n[ Input Video ]\nFigure 17. Any-length Video Editing. Fairy is able to scale to arbitrary long video without memory issue.\n Anchor \n+\n Equi-Finetune\n+\n Anchor \n+\n Equi-Finetune\n\u2212\n Anchor \n\u2212\n Equi-Finetune\n\u2212\nTurn into a baby lion cub\nIn Van Gogh style\nTurn into a fox\nFigure 18. Additional Results on Ablation Study. We demonstrate that both equivariant finetuning and anchor-based attention are crucial\nto Fairy .\n# Anchor = 1\n# Anchor = 3\n# Anchor = 5\n# Anchor = 7\nIn cubism style\nTurn into a Roman marble sculpture\nFigure 19. Ablation study on number of anchor frames. We found that setting number of anchor frames to 3 yields the best results.\nIn Van Gogh style\nTurn into a bronze statue\n# Step = 5\n# Step = 10\n# Step = 20\n# Step = 50\nFigure 20. Ablation study on number of diffusion steps. We found that diffusion steps above 5 generally yield good results.\n[ Input Video ]\n[ Input Video ]\n[ Input Video ]\n[ Input Video ]\nAdd lightning\nAdd flames\nMake it rain\nZoom out\nFigure 21. Limitations of Fairy. Our model cannot accurately render dynamic visual effects, such as lightning, flames, or rain.\n"
  },
  {
    "title": "DreamTuner: Single Image is Enough for Subject-Driven Generation",
    "link": "https://arxiv.org/pdf/2312.13691.pdf",
    "upvote": "22",
    "text": "DreamTuner: Single Image is Enough for Subject-Driven Generation\nMiao Hua\u2217\nJiawei Liu\u2217\nFei Ding\u2217\nWei Liu\nJie Wu\nQian He\nByteDance Inc.\nBeiJing, China\n{huamiao,liujiawei.cc22,dingfei.212,liuwei.jikun,wujie.10,heqian}@bytedance.com\nhttps://dreamtuner-diffusion.github.io/\n* Equal Contribution\nUser Input Image, [S*]\n(Single Image is enough)\n[S*], swimming in the pool\n[S*], wearing sunglasses, \nsitting on the park bench\n[S*], lying on the grass, \neating watermelon\n[S*], driving a convertible \nsports car\nUser Input Image, [S*]\n(Single Image is enough)\n[S*], carrying a handbag, \nwalking on the street\n[S*], sitting on the sofa, \nhand on head\n[S*], sitting on the grass, \nunder the tree\n[S*], on the road, with\nbuildings in the background\n[S*], with white wings\n[S*]\nUser Input Image, [S*]\nUser Input Pose Condition\n[S*], closed eyes\n[S*], in the forest,\nwith butterflies\nFigure 1. Subject-driven image generation results of our method. Our proposed DreamTuner could generate high-fidelity images of user\ninput subject, guided by complex texts (the first two rows) or other conditions as pose (the last row), while maintaining the identity\nappearance of the specific subject. We found that a single image is enough for surprising subject-driven image generation.\nAbstract\nDiffusion-based models have demonstrated impressive\ncapabilities for text-to-image generation and are expected\nfor personalized applications of subject-driven generation,\nwhich require the generation of customized concepts with\none or a few reference images. However, existing methods\nbased on fine-tuning fail to balance the trade-off between\n1\narXiv:2312.13691v1  [cs.CV]  21 Dec 2023\nsubject learning and the maintenance of the generation ca-\npabilities of pretrained models. Moreover, other methods\nthat utilize additional image encoders tend to lose impor-\ntant details of the subject due to encoding compression.\nTo address these challenges, we propose DreamTurner1, a\nnovel method that injects reference information from coarse\nto fine to achieve subject-driven image generation more ef-\nfectively.\nDreamTurner introduces a subject-encoder for\ncoarse subject identity preservation, where the compressed\ngeneral subject features are introduced through an atten-\ntion layer before visual-text cross-attention. We then mod-\nify the self-attention layers within pretrained text-to-image\nmodels to self-subject-attention layers to refine the details\nof the target subject. The generated image queries detailed\nfeatures from both the reference image and itself in self-\nsubject-attention. It is worth emphasizing that self-subject-\nattention is an effective, elegant, and training-free method\nfor maintaining the detailed features of customized subjects\nand can serve as a plug-and-play solution during inference.\nFinally, with additional subject-driven fine-tuning, Dream-\nTurner achieves remarkable performance in subject-driven\nimage generation, which can be controlled by a text or other\nconditions such as pose.\n1. Introduction\nLatest developments in diffusion models have shown aston-\nishing achievements in both text-to-image (T2I) generation\n[24, 30, 32, 34] and editing [3, 11, 45]. Further researches\nare expected for personalized applications of subject-driven\nimage generation [9, 17, 23, 33, 41]. The goal of subject-\ndriven image generation is to synthesis various high fidelity\nimages of customized concepts, as shown in Fig. 1. This is a\npractical task with a wide range of multimedia applications.\nFor example, merchants can generate advertising images for\nspecific products, and designers can create storybooks by\nsimply drawing an initial character image. When generat-\ning images with only a single reference image, it becomes\nmore practical as it reduces user operations and allows the\ninitial reference image to be a generated one.\nHowever, achieving single-reference image subject-\ndriven generation remains a challenging task due to the dif-\nficulty in balancing detailed identity preservation and model\ncontrollability.\nCurrent subject-driven image generation\nmethods are primarily based on fine-tuning or additional\nimage encoders.\nFine-tuning-based methods [9, 17, 33]\ntrain pretrained T2I models on specific reference images\nto learn the identities of the target subjects. Though more\ntraining steps may improve identity preservation, they may\nalso undermine the generation capacity of the pretrained\nmodels. Image-encoder-based methods [16, 23, 41] train\n1This work was done in May 2023.\nReference image\nGenerated images\nw/\nreference features\nGenerated images\nw/o\nreference features\n\u201cA small brown and white dog \nsitting on an orange background.\u201d\n\u201cA photo of dog\u201d\nFigure 2. Exploration experiment of self-attention. It makes the\ngenerated images more similar to the reference one to use the ref-\nerence image features for self-attention. Detailed text can better\nserve its purpose.\nadditional image encoders to inject features of the reference\nimage into the pretrained T2I models. However, these meth-\nods require a trade-off of encoding compression. Strong\ncompression may result in the loss of details of the target\nsubject, while weak compression can easily result in the\ngenerated image collapsing to a simple reconstruction of the\nreference image.\nIn fact, as shown in some video generation methods\n[18, 42], self-attention layers in T2I models are detailed\nspatial contextual association modules that could be ex-\ntended to multiple images for temporal consistency. We\nperformed a simple experiment to demonstrate that self-\nattention layers could also preserve the detailed identity fea-\ntures of the subject, as shown in Fig. 2. At each diffu-\nsion time step, the reference image is noised through the\ndiffusion process and the reference features could be ex-\ntracted from the noised image. Then the reference features\nand those of the generated image are concatenated and used\nas the key and value of the self-attention layers. This ap-\nproach resembles image outpainting, but it does not modify\nthe convolutional layers and cross-attention layers. Con-\nsequently, we found that the generated images with refer-\nence features were significantly more similar to the refer-\nence image in terms of detailed subject identity preserva-\ntion, indicating that self-attention plays a role in this aspect.\nHowever, as shown in the comparison of generated images\nguided by detailed and concise text, it is still necessary to\nintroduce coarse identity preservation methods so that the\nmodel could generate a rough appearance to better utilize\nthe self-attention layers.\nBased on the above motivations, we propose Dream-\nTuner, a novel subject-driven image generation method\nthat injects the reference information from coarse to fine.\nFirstly, a subject-encoder is proposed for coarse iden-\ntity preservation.\nThe main object of the reference im-\n2\nStage 1: Subject Encoder Pre-training\nSubject Encoder\nText\nImage\n\u2112!\"#\nControlNet\ncontent\nlayout\nSE\nSelf-Subject-Attention\nTransformer Block\ndiffusion\ndiffusion\n\u00d7T\n\u201cA dog [S*]\u201d\n\u201cA dog\u201d\nSE\nSE\n\u2112$%&\n\u2112'()\nStage 2: Subject-Driven Fine-tuning\nStage 3: Subject-Driven Inference\n\u00d7T\nReference\nText\ndiffusion\nSE\nText\n[S*]\nSubject-Driven Inference with Text Condition.\nSubject-Driven Inference with Text and Layout Condition.\n\u00d7T\ndiffusion\nText [S*]\nReference\nText\nSE\nControlNet\nEdit Layout\nFrozen\nRegular Data\nPartial Frozen\nFigure 3. Overview of the proposed DreamTuner framework. Firstly, a subject-encoder (SE) is trained for coarse identity preservation,\nwhere a frozen ControlNet is utilized to maintain the layout. Then an additional fine-tuning stage like existing methods is conducted with\nproposed subject-encoder and self-subject-attention for fine identity preservation. Finally a refined subject driven image generation model\nis obtained which could synthesis high-fidelity images of the specific subject controlled by text or other layout conditions. It is worth\nnoting that both of the subject-driven fine-tuning stage and inference stage require only a single reference image.\nage is compressed by a pretrained CLIP [27] image en-\ncoder. And the compressed features are injected to the T2I\nmodel through an additional attention layer before visual-\ntext cross-attention. Furthermore, we introduce a training\ntechnique that decouples content and layout for the subject-\nencoder. To achieve this, we add a pretrained depth-based\nControlNet [45] to avoid the subject-encoder focusing on\nthe layout of reference image. During training, ControlNet\nis frozen to control the layout of generated images so that\nthe subject-encoder could only need to learn the content of\nthe reference image. Next, we alter the self-attention lay-\ners to self-subject-attention to preserve fine identity. Based\non the above-mentioned method to inject reference features\ninto self-attention layers, self-subject-attention further in-\ntroduces weight and mask strategies for improved control.\nNotably, self-subject-attention is a graceful, efficient, and\ntraining-free technique for preserving identity that can be\nused plug-and-play during inference. Lastly, an extra phase\nof fine-tuning can enhance identity preservation. With the\naid of subject-encoder and self-subject-attention, the fine-\ntuning procedure only needs a few training steps.\nThe main contributions are summarized as follows:\n\u2022 We propose a novel image encoder and fine-tuning based\nsubject-driven image generation method that could gen-\nerate high-fidelity images of a specific subject with only\na single reference image.\n\u2022 A subject-encoder is proposed for coarse identity preser-\nvation, which could also reduce the fine-tuning time\ncost, along with a content and layout decoupled training\nmethod.\n\u2022 A novel self-subject-attention is proposed as an elegant,\neffective, and plug-and-play method for refined subject\nidentity preservation.\n2. Related Works\n2.1. Text-to-Image Generation\nThe development of text-to-image generation has evolved\nfrom Generative Adversarial Networks (GANs) [10] based\nmethods [1, 26, 40, 43] in early years to Vector Quanti-\nzation Variational AutoEncoders (VQVAEs) [31, 38] and\nTransformers [39] based methods [7, 8, 29, 44], and more\nrecently to diffusion based methods [13, 15, 30, 32, 34, 37].\nDiffusion models are probabilistic generative models\nthat learn to restore the data distribution perturbed by the\nforward diffusion process. At the training stage, the data\ndistribution is perturbed by a certain scale of Gaussian noise\nin different time steps, and the diffusion models are trained\n3\nto predict the denoised data. The data distribution with the\nlargest scale of noise tends to standard normal distribution.\nThus at the inference stage, diffusion models could sample\nhigh-fidelity data from Gaussian noise step by step.\nLarge-scale diffusion models [30, 32, 34] have achieved\nstate-of-the-art performance on text-to-image generation.\nDALLE-2 [30] builds a prior model to transform CLIP [27]\ntext embeddings to CLIP image embeddings and further\ngenerates high-resolution images using large-scale hierar-\nchical U-Nets which consists of a image generation decoder\nand some image super-resolution models. IMAGEN [34]\nadopt a larger pretrained text encoder [28] for better image-\ntext consistency. LDM [32] compresses the images through\na pretrained auto-encoder and perform diffusion in the la-\ntent space, which leads to efficient high-resolution image\ngeneration. With the help of classifier-free guidance [12],\nhigh-fidelity images can be generated by these large-scale\ndiffusion based text-to-image models. However, these mod-\nels are all designed for general scenarios that are controlled\nonly by text. For subject driven generation that requires\nsubject identity preservation, text is far from enough.\n2.2. Subject-Driven Generation\nSubject-driven image generation requires the model to un-\nderstand the visual subject contained in the initial reference\nimages and synthesize totally unseen scene of the demon-\nstrated subjects. Most existing works use a pretrained syn-\nthesis network and perform test-time training for each sub-\nject. For instance, MyStyle [25] adopts a pretrained Style-\nGAN for personalized face generation.\nLater on, some\nworks fine-tune the pretrained T2I models to adapt image\ngeneration to a specific unseen subject. Dreambooth [33]\nfine-tunes all the parameters, Texture Inversion [9] intro-\nduces and optimizes a word vector for the new concept,\nCustom-Diffusion [21] only fine-tunes a subset of cross-\nattention layer parameters.\nFor those fine-tuning based\nmethods, more training steps leads to better identity preser-\nvation but undermines the generation capacity of the pre-\ntrained model. Another line of works train additional im-\nage encoders to inject the reference image information for\nsubject generation. ELITE [41] and Instantbooth [36] use\ntwo mapping networks to convert the reference images to\na textual embedding and additional cross-attention layers\nfor subject generation. Taming Encoder[16] adopts an en-\ncoder to capture high-level identifiable semantics of sub-\njects. SuTI [6] achieves personalized image generation by\nlearning from massive amount of paired images generated\nby subject-driven expert models. However, these methods\nare trained with weakly-supervised data and performs with-\nout any test-time fine-tuning, thus leading to much worse\nfaithfulness than DreamBooth [33]. Recently, MasaCtrl [4]\nproposes a mask-guided mutual self-attention strategy simi-\nlar as our self-subject-attention for text-based non-rigid im-\nResBlock\nSelf\nAttention\nS-E\nAttention\nCross\nAttention\nFFN\nImage\nCLIP\nText\nCLIP\nPaired\nText\nSubject Encoder\n\u2026\n\u2026\nFrozen\nTrainable\nConv\nBlock\nTransformer\nBlock\n\ud835\udefd\nText-to-Image U-Net Model\nFigure 4. Illustration of the text-to-image generation U-Net model\nwith proposed subject-encoder.\nage synthesis and editing without fine-tuning.\nOur pro-\nposed method combines the strengths of those works, it\nfirst coarsely injects the subject information using a subject-\nencoder, then fine-tunes the model with the novel self-\nsubject-attention for better identity preservation.\n3. Method\nIn this paper, we propose DreamTuner as a novel frame-\nwork for subject driven image generation based on both\nfine-tuning and image encoder, which maintains the sub-\nject identity from coarse to fine.\nAs shown in Fig.\n3,\nDreamTuner consists of three stages: subject-encoder pre-\ntraining, subject-driven fine-tuning and subject-driven in-\nference. Firstly, a subject-encoder is trained for coarse iden-\ntity preservation. Subject-encoder is a kind of image en-\ncoder that provides compressed image features to the gen-\neration model.\nA frozen ControlNet [45] is utilized for\ndecoupling of content and layout. Then we fine-tune the\nwhole model on the reference image and some generated\nregular images as in DreamBooth [33]. Note that subject-\nencoder and self-subject-attention are used for regular im-\nages generation to refine the regular data. At the inference\nstage, the subject-encoder, self-subject-attention, and sub-\nject word [S*] obtained through fine-tuning, are used for\nsubject identity preservation from coarse to fine. Pretrained\nControlNet [45] could also used for layout controlled gen-\neration.\n3.1. Subject-Encoder\nImage encoder based subject driven image generation meth-\nods [36, 41] have proved that the compressed reference im-\nage features can guide the pretrained text-to-image model\nto generate a general appearance of the subject. Thus we\n4\npropose subject-encoder as a kind of image encoder that\nprovides a coarse reference for subject driven generation.\nAs shown in Fig. 4, a frozen CLIP [27] image encoder\nis used to extract the compressed features of reference im-\nage. Salient Object Detection (SOD) model or segmenta-\ntion model are used to remove the background of the in-\nput image and emphasize the subject. Then some residual\nblocks (ResBlock) are introduced for domain shift. Multi-\nlayer features extracted by CLIP are concatenated in the\nchannel dimension and then adjust to the same dimension\nas the generated features through the residual blocks.\nThe encoded reference features of subject-encoder are\ninjected to the text-to-image model using additional subject-\nencoder-attention (S-E-A) layers.\nThe subject-encoder-\nattention layers are added before the visual-text cross-\nattention, because the cross-attention layers are the mod-\nules that control the general appearance of generated im-\nages. We build the subject-encoder attention according to\nthe same settings as cross-attention and zero initial the out-\nput layers. An additional coefficient \u03b2 is introduced to ad-\njust the influence of subject-encoder, as in Eq. 1.\nS-E-A(z, SE(r))\u2032 = \u03b2 \u2217 S-E-A(z, SE(r))\n(1)\nwhere z is the generated image features and r is the ref-\nerence image.\nIt should be noted that only the addi-\ntional residual blocks and subject-encoder-attention layers\nare trainable during training to maintain the generation ca-\npacity of the text-to-image model.\nBesides, we found that the subject-encoder will provide\nboth the content and the layout of the reference image for\ntext-to-image generation. However, in most cases, layout\nis not required in subject driven generation. Thus we fur-\nther introduce ControlNet [45] to help decouple the con-\ntent and layout. Specifically, we train the subject-encoder\nalong with a frozen depth ControlNet, as in Fig. 3. As the\nControlNet has provided the layout of reference image, the\nsubject-encoder can focus more on the subject content.\n3.2. Self-Subject-Attention\nAs discussed in some text-to-video works [18, 42], self-\nattention layers in pretrained text-to-image models could\nbe used for content consistency across-frames. Since the\nsubject-encoder has provided general appearance of the\nspecific subject, we further propose self-subject-attention\nbased on the self-attention layers for fine subject identity\npreservation.\nThe features of reference image extracted\nby the text-to-image U-Net model are injected to the self-\nattention layers, which can provide refined and detailed ref-\nerence because they share the same resolution with the gen-\nerated features. Specifically, as shown in Fig. 3 Stage-2\nand Stage-3, the reference image are noised through dif-\nfusion forward process at time step t. Then the reference\nfeatures before self-attention layers are extracted from the\nS-A\nQuery\nS-A\nValue\nS-A\nKey\nGenerated Image\nReference Image\n\u2217 \ud835\udf14!\"#\nlog\nSelf-Subject-Attention\nSoftmax\nFigure 5. Illustration of the proposed self-subject-attention. S-A\nindicates self-attention.\nnoised reference image, which share the same data distribu-\ntion with the generated features at time step t. The original\nself-attention layers are modified to self-subject-attention\nlayers by utilizing reference features. As demonstrated in\nFig. 5, the features of generated image are taken as the\nquery and the concatenation of generated features and refer-\nence features is taken as the key and value. To eliminate the\ninfluence of reference image background, SOD model or\nsegmentation model are used to create a foreground mask,\nwhich uses 0 and 1 to indicate the background and fore-\nground. Besides, the mask can also be used to adjust the\nscale of the impact of reference image through a weight\nstrategy, i.e., multiply the mask by an adjustment coefficient\n\u03c9ref. The mask works as an attention bias, thus a log func-\ntion is used as a preprocessing. It is worth noting that the\nextraction of reference features is unrelated to the generated\nimage, so additional reference text can be used to enhance\nconsistency with the reference image.\nThe original Self-Attention (S-A) is formulated as Eq. 2.\nS-A(z) = Softmax(q \u00b7 kT\n\u221a\nd\n) \u00b7 v\nw.r.t.\nq = Query(z), k = Key(z), v = V alue(z)\n(2)\nwhere Query, Key, V alue are linear layers in self-\nattention, d is the dimension of q, k, v. The proposed Self-\nSubject-Attention (S-S-A) is formulated as Eq. 3.\nS-S-A(z, zr) = Softmax(q \u00b7 [k, kr]T\n\u221a\nd\n+ log m) \u00b7 [v, vr]\nw.r.t.\nkr = Key(zr), vr = V alue(zr)\nm = [J, \u03c9ref \u2217 mr]\n(3)\nwhere zr is the reference image features, [] indicates con-\ncatenation, mr is the foreground mask of reference image,\nJ is a all-ones matrix. As in Re-Imagen [5], there are more\nthan one conditions in DreamTuner, thus we modify the\noriginal classifier free guidence to a interleaved version:\n\u02c6\u03f5r\nt = \u03c9r \u2217 \u03f5\u03b8(xt, c, xr\nt\u2212\u2206t, t) \u2212 (\u03c9r \u2212 1) \u2217 \u03f5\u03b8(xt, c, \u2205, t)\n(4)\n\u02c6\u03f5c\nt = \u03c9c \u2217 \u03f5\u03b8(xt, c, xr\nt\u2212\u2206t, t) \u2212 (\u03c9c \u2212 1) \u2217 \u03f5\u03b8(xt, uc, xr\nt+\u2206t\u2032, t)\n(5)\n5\n\u02c6\u03f5t =\n(\n\u02c6\u03f5r\nt,\n\u03bbt \u2264 pr\n\u02c6\u03f5c\nt,\n\u03bbt > pr\n\u03bbt \u223c U(0, 1)\n(6)\nwhere xt is the generated image at time step t, c is the\ncondition, uc is the undesired condition, \u03c9r and \u03c9c are\nthe guidance scales of reference image and text condition.\nxr\nt\u2212\u2206t and xr\nt+\u2206t\u2032 are the diffusion noised reference images\nat time step t \u2212 \u2206t and t + \u2206t\u2032, \u2206t and \u2206t\u2032 are small time\nstep biases which are used for the adjustment of noise in-\ntensity of reference image. \u02c6\u03f5\u03b8 is the diffusion model and\n\u03b8 represents the parameters. \u02c6\u03f5t is the final output at time\nstep t. Eq. 4 emphasizes the guidance of reference image\nand Eq. 5 emphasizes the guidance of condition, where pr\ncontrols the possibility of selecting Eq. 4.\n3.3. Subject Driven Fine-tuning\nWhile excellent performance can be achieved through SE\nand S-S-A, a few additional fine-tuning steps leads to better\nsubject identity preservation. As shown in Fig. 3 Stage-\n2, We conduct the fine-tuning stage similar to DreamBooth\n[33], while other fine-tuning methods, such as LoRa [14],\nalso works. Firstly, some regular images are generated to\nhelp the model learn the specific identity of the subject\nand maintain the generation capacity. The paired data are\nbuilt as {subject reference image, \u201dA class word [S*]\u201d} and\n{regular image, \u201dA class word\u201d}. Then the paired data is\nused to fine-tune the pretrained text-to-image model. All\nof the parameters including CLIP text encoder are trainable\nin this stage as in DreamBooth. In order to achieve more\nrefined subject learning, we make four improvements:\n1. The background of reference image is replaced by a sim-\nple white background to focus on the subject itself.\n2. The word [S*] is an additional trainable embedding as in\nTextual Inversion [9] rather than a rare word [33].\n3. The subject-encoder is trained with the text-to-image\ngeneration model for better subject identity preservation.\n4. To learn the detailed identity of the subject, we generate\nregular images that are more similar to the reference im-\nage by using subject-encoder, self-subject-attention and\ndetailed caption of the reference image. The model are\nrequired to learn the subject details to distinguish \u201dA\nclass word\u201d and \u201dA class word [S*]\u201d\n4. Experiments\n4.1. Implementation Details\nWe build our DreamTuner based on the Stable Diffusion\nmodel [32]. For the subject-encoder, CLIP features from\nlayers with layer indexes {25, 4, 8, 12, 16} are selected\nas in ELITE [41]. Elastic transform and some other data\naugmentation methods are used to enhance the difference\nbetween input reference images and the generated images.\nInSPyReNet [20] is adopted as the SOD model for natu-\nral images and a pretrained anime segmentation model is\nTable 1. Subject fidelity (CLIP-I) and prompt fidelity (CLIP-T)\nquantitative metric comparison. The top-2 results of each metric\nhave been emphasized.\nMethod\nCLIP-T\nCLIP-I\nTextual Inversion\n0.251\n0.722\nMasactrl\n0.296\n0.735\nELITE\n0.271\n0.726\nDreamBooth\n0.267\n0.788\nDreamTuner\n0.281\n0.767\nused to separate the foreground and background for anime\ncharacter images. For self-subject-attention, pr is set to 0.9,\n\u03c9ref is set to 3.0 for natural images and 2.5 for anime im-\nages. The training steps of subject-driven fine-tuning is set\nto 800-1200. BLIP-2 [22] and deepdanbooru [19] are used\nas the caption models for natural images and anime charac-\nter images. More details could be found in Appendix-B.\n4.2. Subject Driven Generation Results\nSubject driven generation results could be found in Fig. 6,\nconsisting of static objects, animals, and anime characters.\nIt could be found that the subject identities have been pre-\nserved well and the generated images are consistent with the\ntext condition. The subject appearance is maintained even\nwith some complex texts. It is worth noting that Dream-\nTuner could achieve excellent identity preservation on de-\ntails, e.g., the words on the can in the first row, the white\nstripes of the dog in the second raw, the eyes and clothes of\nthe anime characters, etc.\n4.3. Comparison with Other Methods\nThe comparison results between our proposed DreamTuner\nand other subject driven image generation methods are\nshwon in Fig. 7. The corresponding quantitative metric\ncomparison could be found in Table. 1. We use CLIP-\nT, the average cosine similarity between CLIP image em-\nbeddings and CLIP text embeddings, as the metric to eval-\nuate the consistency between generated images and input\ntext prompts.\nCLIP-I, the average cosine similarity be-\ntween CLIP embeddings of generated images and refer-\nence images, is adopted as a metric that indicates the sub-\nject fidelity. Only a single image are used as the reference\nimage for each method. It could be found that our pro-\nposed DreamTuner outperforms the fine-tuning based meth-\nods (DreamBooth [33] and Textual Inversion [9]), image\nencoder based method (ELITE [41]) and improved self-\nattention based method (MasaCtrl [4]). DreamBooth [33]\nperforms best in CLIP-I but not well on CLIP-T, as it is easy\nto overfit on such a single reference image with only 1200\nsteps, and less steps leads to worse subject fidelity. Besides,\nit could be found in Fig. 7 that DreamTuner can generate\n6\nInput Image\nIn the snow\nWith a mountain in \nthe background\nInput Image\nIn the snow\nFloating on top of water\nInput Image\nRunning on the grass\nwith autumn leaves\nSitting on a\ncobblestone street\nInput Image\nJumping into the air\nand biting a frisbee\nSleeping in a cardboard box\nInput Image\nOn the beach\nIn the snow\nInput Image\nIn the jungle\nOn top of a wooden floor\nInput Image\nSitting at the table with a \ncup of tea in hands, sunlight \nstreaming through the window\nSitting on the park bench\nSitting on the sofa\nInput Image\nSleep in bed, closed eyes\nFigure 6. Subject driven generation results of DreamTuner.\nbetter subject details benefiting from subject-encoder and\nself-subject-attention. Similar to DreamBooth, Textual In-\nversion [9] trained with only a single image also has diffi-\nculties in the trade-off between avoid overfitting and iden-\ntity preservation. Compared with ELITE [41], DreamTuner\nuse additional self-subject-attention and subject-driven fine-\ntuning that leads to better identity preservation. MasaC-\ntrl [4] shows the best CLIP-T because it does not train\nthe text-to-image model and inherits the creative power of\nthe pretrained model.\nHowever, it only use a improved\nself-attention for identity preservation while our proposed\nDreamTuner introduces subject-encoder and subject-driven\nfine-tuning, thus achieves better subject fidelity.\n7\nA can [S*] on the beach\nA vase [S*] with a wheat field in the background\nA dog [S*] with a mountain in the background, red hat and sunglasses\nA dog [S*] swimming in the pool\nInput\nOurs\nDreamBooth\nTextual Inversion\nELITE\nMasaCtrl\nFigure 7. Comparison between our proposed DreamTuner and state-of-the-art subject driven image generation methods.\n4.4. Ablation Study\nWe conduct ablation studies to evaluate the effects of var-\nious components in our method, including the subject-\nencoder and the self-subject-attention. Fig. 10 shows the\ninput reference image, corresponding prompt, and results\nof different settings.\nEffect of Subject-Encoder.\nFor subject-encoder, \u03b2 is in-\ntroduced to control how much subject information is in-\njected into the model. To evaluate its effect, we vary its\nvalue from 0.0 to 0.5, and the generated results are shown\nin the first row of Fig. 10. From the figure, we see that\nwith the increasing of \u03b2, the consistency between the syn-\nthesized image and generated image is improved. However,\nwhen the value of \u03b2 is too large(i.e., 0.5), it may lead to de-\ngenerated editing results. As a result, for a tradeoff between\ninversion and editability, we set \u03b2 = 0.2. We found that this\nsetting works well for most cases. We also conduct studies\nof training subject-encoder without ControlNet [45]. In this\ncase, the subject-encoder cannot decouple the content and\nlayout properly, therefore the generated images may have\nsimilar layout as the reference images, rather than matching\nthe input prompt.\nEffect of Self-Subject-Attention.\nWe further analyze the\neffect of the proposed self-subject-attention. From the im-\nages in the last row of Fig. 10, we observe a similar phe-\nnomenon when increasing the \u03c9ref from 0.0 to 10.0. When\n\u03c9ref is too small, such as close to 0.0, the generated image\nis not similar to the input image, and a large \u03c9ref value may\ndegrade the quality of the generated image. When using S-\nS-A without mask, i.e., mr is set to an all-ones matrix, the\nwhite background of reference image will also play a sig-\nnificant role and do harm to the generated image. More\nablations are provided in Appendix-C.\n8\nInput\nInput\n\u03b2=0.0\n\u03c9ref =0.0\n\u03b2=0.1\n\u03c9ref =1.0\n\u03b2=0.2\n\u03c9ref =2.5\n\u03b2=0.5\n\u03c9ref =10.0\nSE without ControlNet\nS-S-A without mask\nFigure 8. Ablation study of subject-encoder and self-subject-attention. The input text is \u2019a photo of girl [S*], sitting at a desk, typing on a\ncomputer\u2019\n5. Conclusion\nIn this paper, we focus on the task of subject driven image\ngeneration with only a single reference image. Our method,\nDreamTuner, combines the strengths of existing fine-tuning\nand additional image encoder-based methods. Firstly, we\nuse a subject-encoder for coarse subject identity preserva-\ntion. Secondly, we fine-tune the model with self-subject-\nattention layers to refine the details of the target subject.\nThis allows us to generate high-fidelity images based on\nonly one reference image, with fewer training steps. Fur-\nthermore, our method is highly flexible in editing learned\nconcepts into new scenes, guided by complex texts or addi-\ntional conditions such as pose. This makes it an invaluable\ntool for personalized text-to-image generation.\nReferences\n[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy J. Mitra,\nand Peter Wonka. Clip2stylegan: Unsupervised extraction\nof stylegan edit directions.\nIn Special Interest Group on\nComputer Graphics and Interactive Techniques Conference,\npages 48:1\u201348:9, 2022. 3\n[2] Anonymous, Danbooru community, and Gwern Branwen.\nDanbooru2021:\nA large-scale crowdsourced and tagged\nanime illustration dataset.\nhttps://gwern.net/\ndanbooru2021, January 2022. Accessed: DATE. 11\n[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\n2022. 2\n[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 4, 6, 7\n[5] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W.\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 5\n[6] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,\nXuhui Jia, Ming-Wei Chang, and William W. Cohen.\nSubject-driven text-to-image generation via apprenticeship\nlearning. 2023. 4\n[7] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, and Jie Tang. Cogview: Mastering text-to-\nimage generation via transformers. In Advances in Neural\nInformation Processing Systems, pages 19822\u201319835, 2021.\n3\n[8] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In Con-\nference on Computer Vision and Pattern Recognition, pages\n12873\u201312883, 2021. 3\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. 2022. 2, 4, 6, 7\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 3\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. 2022. 2\n[12] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 3\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 6\n9\n[15] Nisha Huang, Fan Tang, Weiming Dong, and Changsheng\nXu. Draw your art dream: Diverse digital art synthesis with\nmultimodal guided diffusion.\nIn Proceedings of the 30th\nACM International Conference on Multimedia, pages 1085\u2013\n1094, 2022. 3\n[16] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-\nChuan Su. Taming encoder for zero fine-tuning image cus-\ntomization with text-to-image diffusion models. 2023. 2,\n4\n[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. 2022.\n2\n[18] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. 2023.\n2, 5\n[19] Kichang Kim, Rachmadani Haryono, and Arthur Guo. Deep-\ndanbooru. 2019. 6\n[20] Taehun Kim, Kunhee Kim, Joonyeong Lee, Dongmin Cha,\nJiho Lee, and Daijin Kim. Revisiting image pyramid struc-\nture for high resolution salient object detection.\nIn Asian\nConference on Computer Vision, pages 257\u2013273, 2022. 6\n[21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. 2023. 4\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi.\nBLIP-2: bootstrapping language-image pre-training\nwith frozen image encoders and large language models.\n2023. 6\n[23] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and\nJiaying Liu. Unified multi-modal latent diffusion for joint\nsubject and text conditional image generation. 2023. 2\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[25] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal\nYarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and\nDaniel Cohen-Or. Mystyle: A personalized generative prior.\narXiv preprint arXiv:2203.17272, 2022. 4\n[26] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In International Conference on Computer\nVision, pages 2065\u20132074, 2021. 3\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In International\nConference on Machine Learning, pages 8748\u20138763, 2021.\n3, 4, 5, 11\n[28] Colin Raffel, Minh-Thang Luong, Peter J Liu, Ron J Weiss,\nand Douglas Eck. Online and linear-time attention by en-\nforcing monotonic alignments. In International Conference\non Machine Learning, pages 2837\u20132846, 2017. 4\n[29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In Proceedings of the In-\nternational Conference on Machine Learning, pages 8821\u2013\n8831, 2021. 3\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3, 4\n[31] Ali Razavi, A\u00a8aron van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with VQ-VAE-2.\nIn\nAdvances in Neural Information Processing Systems, pages\n14837\u201314847, 2019. 3\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Conference on Com-\nputer Vision and Pattern Recognition, pages 10684\u201310695,\n2022. 2, 3, 4, 6\n[33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 2, 4, 6, 11\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 2, 3, 4\n[35] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. 2022. 11\n[36] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. 2023. 4\n[37] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3\n[38] A\u00a8aron\nvan\nden\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu.\nNeural discrete representation learning.\nIn Advances in Neural Information Processing Systems,\npages 6306\u20136315, 2017. 3\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998\u20136008, 2017. 3\n[40] Hao Wang, Guosheng Lin, Steven C. H. Hoi, and Chunyan\nMiao. Cycle-consistent inverse gan for text-to-image synthe-\nsis. In ACM International Conference on Multimedia, page\n630\u2013638, 2021. 3\n[41] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\n10\ncepts into textual embeddings for customized text-to-image\ngeneration. 2023. 2, 4, 6, 7\n[42] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of im-\nage diffusion models for text-to-video generation. 2022. 2,\n5\n[43] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Conference on Computer Vision and\nPattern Recognition, pages 1316\u20131324, 2018. 3\n[44] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. 2022. 3\n[45] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. 2023. 2, 3, 4, 5,\n8, 11\nA. More Implementation Details\nIn this section, we will provide more implementation de-\ntails.\nFor the pretraining of subject-encoder, we use the pre-\ntrained OpenCLIP 2 model to extract multilayer image fea-\ntures. We selected the features from indices {25, 4, 8, 12,\n16}. We then train our encoder on two datasets: the natu-\nral images dataset, which is a sub-set of LAION-5B [35],\nand the anime character images dataset, which is selected\nfrom Danbooru [2]. For natural images, we use the V1-\n5 version of Stable Diffusion, and for anime images, we\nuse the Anything V3 model 3. We add a pretrained depth-\nbased ControlNet [45] to make the subject-encoder focus\non content since layout is controlled by the ControlNet. To\nenhance the robustness of the subject-encoder, we use data\naugmentation techniques such as elastic transform, random\nflip, blur, and random scale and rotation. We set \u03b2 = 1.0\nduring the training process and use a batch size of 16 with\na learning rate of 1e \u2212 5. We conduct all experiments on\n4 A100 GPUs, and the training time for each experiment is\napproximately one week.\nAt the subject-driven fine-tuning stage, we generate 32\nregular images for each subject. Subject-encoder and self-\nsubject-attention are used to generate regular images that\nare similar to the reference image, where \u03b2 is set to 0.2,\n\u03c9ref is set to 3.0 for natural images and 2.5 for anime im-\nages.\nMask strategy is not used in self-subject-attention\nwhen generating regular images, as the background is also\nexpected to be maintained. The U-Net and CLIP [27] text\n2https://github.com/mlfoundations/open_clip\n3https://huggingface.co/Linaqruf/anything-v3.0\nencoder in the Stable Diffusion model , and the ResBlocks\nin the subject-encoder are trainable during fine-tuning. The\nCLIP image encoder is still frozen. To use a specific word\nto represent the subject, we train an additional word em-\nbedding [S*], which is initialized by the class word. The\nlearning rate of the [S*] word embedding is set to 5e-3 and\nthe learning rate of other parameters is set to 1e-6. The\nbatch size is set to 2, consisting of a reference image and\na regular image. Flip with a probability of 0.5 is adopted\nas a data augmentation method. We fine-tune the model for\n800-1200 steps. It is worth noting that 800 steps are enough\nfor most subjects.\nB. More Generated Results\nIn the paper, we have shown the generated results guided\nby text descriptions. And the pretrained ControlNets [45]\ncould also be used for DreamTuner through a simple model\ntransfer process [45]. As shown in Fig. ??, with an addi-\ntional pose version ControlNet, DreamTuner can generate\nhigh fidelity images of the reference anime character, which\nshare the same pose as the input conditions. Besides, the\ntext prompts can still play its role. For example, the back-\nground could be controlled by the text descriptions, such as\nsimple white background, road, etc. The expression of the\ngenerated characters could also be controlled by the text,\nsuch as closed eyes. Additional objects like flowers and\nwhite wings could also be added through text prompts. Be-\nsides, DreamTuner could also be used for pose controlled\nsubject-driven video generation, as shwon in Fig. 9. We\nalso use the first frame and the previous frame as the ref-\nerence images in self-subject-attention for cross-frame co-\nherence. The generated video could be found in the project\npage4.\nC. Ablation Study of Fine-tuning\nWe conduct ablation studies on fine-tuning stage to evalu-\nate the effects of subject-encoder and self-subject-attention,\nas depicted in Fig. 10. The figure displays the input ref-\nerence image, corresponding prompt, and results of vari-\nous settings. The first row presents the results of original\nDreamBooth [33] achieved at different training steps. The\nsecond row exhibits results of DreamTuner without subject-\nencoder at the fine-tuning stage, while the last row shows\nthe results of integral DreamTuner. The figure elucidates\nthat our method generates images that resemble the input\nreference image and comply well with the target prompt.\nAdditionally, our method converges faster than the original\nDreamBooth, which benefits from the subject-encoder. At\napproximately 300 fine-tuning steps our method produces\nsatisfactory results.\n4https://dreamtuner-diffusion.github.io/\n11\nReference Image\nFigure 9. Text and pose controlled subject-driven video generation results.\na photo of girl [S*], \nsleep in bed, closed eyes.\nDreamBooth\nDreamTuner\nw/o\nsubject-encoder\nDreamTuner\n0 step\n300 steps\n600 steps\n900 steps\n1200 steps\nInput\nFigure 10. Ablation study at the subject-driven fine-tuning stage.\n12\n"
  },
  {
    "title": "DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation",
    "link": "https://arxiv.org/pdf/2312.13578.pdf",
    "upvote": "22",
    "text": "DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for\nSingle Image Talking Face Generation\nChenxu Zhang1*, Chao Wang1*, Jianfeng Zhang1, Hongyi Xu1, Guoxian Song1, You Xie1,\nLinjie Luo1, Yapeng Tian2, Xiaohu Guo2, Jiashi Feng1\n1ByteDance Inc.\n2The University of Texas at Dallas\nFigure 1. DREAM-Talk takes as input a driving audio sequence, a given portrait image, and an example of emotion style (a clip of an\nemotional talking face), and generates a photorealistic, lip-synchronized talking face video that features high-quality emotional expressions.\nThe results include both real human images and images generated by AIGC. Please refer to our Project Page for more results.\nAbstract\nThe generation of emotional talking faces from a single\nportrait image remains a significant challenge. The simulta-\nneous achievement of expressive emotional talking and ac-\ncurate lip-sync is particularly difficult, as expressiveness is\noften compromised for the accuracy of lip-sync. As widely\nadopted by many prior works, the LSTM network often fails\nto capture the subtleties and variations of emotional expres-\nsions. To address these challenges, we introduce DREAM-\nTalk, a two-stage diffusion-based audio-driven framework,\ntailored for generating diverse expressions and accurate\nlip-sync concurrently. In the first stage, we propose EmoD-\n*Equal contribution\niff, a novel diffusion module that generates diverse highly\ndynamic emotional expressions and head poses in accor-\ndance with the audio and the referenced emotion style.\nGiven the strong correlation between lip motion and audio,\nwe then refine the dynamics with enhanced lip-sync accu-\nracy using audio features and emotion style. To this end, we\ndeploy a video-to-video rendering module to transfer the\nexpressions and lip motions from our proxy 3D avatar to\nan arbitrary portrait. Both quantitatively and qualitatively,\nDREAM-Talk outperforms state-of-the-art methods in terms\nof expressiveness, lip-sync accuracy and perceptual quality.\n1\narXiv:2312.13578v1  [cs.CV]  21 Dec 2023\n1. Introduction\nThe field of talking face generation has seen significant ad-\nvancements in recent years, becoming a key area of re-\nsearch with a wide range of applications, including video\nconferencing, virtual assistants, and entertainment, among\nothers. Recently, researchers have commenced incorporat-\ning emotion-conditioned facial expressions into talking face\ngeneration [10, 13, 14], leveraging emotional annotations in\ntalking video datasets like MEAD [33]. However, none of\nthese methods have yet succeeded in generating expressive\nand lifelike expressions in their talking faces.\nSeveral major challenges exist in current emotional talk-\ning face generation methods.\nFirstly, it is difficult to\nachieve expressive emotion and accurate lip-sync simulta-\nneously. Emotional expressions in datasets like MEAD [33]\nshow significant exaggeration in the movements of eye-\nbrows, eye-blinking, and mouth shapes.\nNonetheless,\nthe sentences and audio content used in these datasets\nlack sufficient length to effectively train a precise lip-\nsync model. To address this issue, SPACE [10] employed\ntwo supplementary non-emotional datasets, namely Vox-\nCeleb2 [5] and RAVDESS [20], alongside MEAD, to train\ntheir model. However, integrating non-emotional and emo-\ntional datasets results in synthesized emotional expressions\nthat may lack the desired level of expressiveness and dy-\nnamism.\nEAMM [14] tackles this problem by integrat-\ning two modules: one dedicated to learning non-emotional\naudio-driven face synthesis and another focused on cap-\nturing emotional displacements of expressions.\nTo pre-\nvent emotional displacements from distorting lip-sync, it\naugmented the training data by obscuring the mouth re-\ngion of the speakers.\nUnfortunately, employing mouth-\ncovering data augmentation compromises the expressive-\nness of mouth shapes during emotional speech.\nSecondly, modeling the subtleties and variations of emo-\ntional expressions is challenging. Emotional expressions in-\nvolve the activation of numerous facial muscles and exhibit\nsignificant diversity throughout a speech. Existing meth-\nods [10, 13, 14, 21] typically utilize LSTM or CNN net-\nworks as generators to transform audio into facial repre-\nsentations. While these models are adequate for capturing\nthe movements of the mouth and lips during regular speech,\nthey face challenges when it comes to faithfully portraying\nthe nuances and variations of emotional expressions. Con-\nsequently, their generated emotional depictions often ap-\npear bland and artificial. For a detailed comparison of these\napproaches, please refer to the supplementary video.\nTo overcome these challenges, we propose a diffusion-\nbased realistic emotional audio-driven method for talking\nface generation (DREAM-Talk) from a single portrait im-\nage. At its core is a carefully designed two-stage pipeline,\nwhich achieves both expressive emotion and precise lip-\nsync, as shown in Fig. 2. The first stage, EmoDiff Module, is\ntailored to capture the dynamic nature of emotional expres-\nsions.\nSpecifically, we designed an emotion-conditioned\ndiffusion model to transform input audio to the facial ex-\npressions of the ARKit model [19]. The second stage, Lip\nRefinement, focuses on ensuring the precision of lip-sync in\nthe generated talking faces. To enhance the synchronization\nof mouth movements with audio signals while preserving\nthe richness of emotional expressions, we have developed a\nnovel lip refinement network to re-optimize the parameters\nof the mouth based on audio signals and specific emotional\nstyles. Unlike traditional face model [17] where mouth pa-\nrameters are integrated with other facial parameters, using\n3D ARKit model enables explicitly optimizing lip motion,\nensuring that the intensity of other facial expressions re-\nmains unaffected. This design choice in our lip refinement\nnetwork guarantees that the expressiveness of emotions is\nnot compromised by lip-sync refinement, offering a more\ntargeted and effective approach for emotion-rich facial ani-\nmation.\nThe sequential two-stage process employed in DREAM-\nTalk effectively addresses the challenges mentioned earlier,\nallowing for the simultaneous achievement of expressive\nemotions and precise lip-sync in the generated talking faces.\nOur experimental results convincingly showcase its excep-\ntional ability to model the intricacies and variations of emo-\ntional expressions from the input audio. This includes re-\nalistically capturing emotional movements in areas such as\neyebrows, eye blinks, mouth shapes, and beyond. Specifi-\ncally, our diffusion model adeptly captures high-frequency\nfacial details, while lip refinement further elevates the pre-\ncision of mouth motion. The contributions of this paper can\nbe summarized as follows:\n\u2022 A novel two-stage generative framework DREAM-Talk\nthat achieves expressive emotional talking faces with\naudio-synchronized lip motion.\n\u2022 A diffusion module EmoDiff that is capable of generating\ndiverse highly-dynamic emotional expressions and head\nposes in accordance with the audio and the referenced\nemotion style.\n\u2022 An emotional ARKit dataset with precise isolation of\nmouth parameters from other facial attributes, well tai-\nlored for the task of lip motion refinement.\n2. Related Work\nAudio-driven talking face. The task of generating a talk-\ning face driven by audio involves producing a realistic and\ncohesive video of a person\u2019s speaking face, utilizing audio\ninput and, occasionally, an image or video of the speaker.\nEarly efforts by Taylor et al. [31] focused on converting au-\ndio sequences into phoneme sequences to generate adapt-\nable talking avatars for multiple languages. [29] curated\na dataset of Obama videos and introduced an end-to-end\nframework to synthesize corresponding talking faces with\n2\nFigure 2. Pipeline of our DREAM-Talk framework. Starting with the input audio, initial state, and emotion style as conditions, we\nfirst employ EmoDiff for learning to denoise 3D expressions over time, utilizing a transformer-based architecture for sequence modeling.\nThe initial state corresponds to the expression in the first frame, and the emotion style is defined by a randomly selected expression clip,\nindependent of the input audio. Then, utilizing the conditioned audio and emotional expressions, the lip refinement model further optimizes\nthe mouth without altering the intensity of emotions. This is followed by producing corresponding 3D rendering faces on a blendshape rig.\nFinally, we employ a fine-tuned Face-Vid2Vid model [34] to generate emotional talking videos.\narbitrary voices. [4] and [41] pioneered the generation of\ntalking face videos, requiring only a single facial and au-\ndio sequence. ATVGnet [2] and [40] proposed two-stage\ntalking face synthesis methods guided by landmarks. They\ninitially generated landmarks from a single identity image\nand an audio sequence, which were then combined with the\nidentity image for the second stage of talking face synthe-\nsis. [22] decomposed talking head synthesis into spatial and\nstyle components, demonstrating improved performance in\nfew-shot novel view synthesis. FACIAL [37] and [27] used\naudio to regress parameters in 3D face models, resulting\nin more realistic synthesis. To enhance video quality, AD-\nNeRF [9] and Geneface [36] employed an audio-driven neu-\nral radiance fields (NeRF) model to synthesize high-quality\ntalking-head videos from audio input, surpassing existing\nGAN-based methods.\nSadTalker [38] adeptly generates\nemotive speech content by mapping audio inputs to 3DMM\nmotion coefficients, but challenges remain in achieving both\nrealistic expression and accurate lip movement.\nOn the\nother hand, [28] and DiffTalk [24] applied diffusion models\nto avoid challenges of GAN-based methods, such as train-\ning difficulties, mode collapse, and facial distortion. How-\never, those methods require extra motion sequence of the\ntarget individual to guide the video generation and avoid\nunnatural-looking motions. Moreover, utilizing diffusion\nmodel as foundational framework, DiffTalk also encoun-\ntered challenges in maintaining temporal coherence within\nthe mouth region. On the other hand, the works discussed\nabove still lack emotional information guidance, leading to\nmonotonous expressions in generated talking faces.\nEmotional audio-driven talking face. In recent research\nendeavors, there has been a growing focus on the develop-\nment of emotionally expressive talking faces [8, 14, 30, 35]\nor emotional talking mesh [23]. ExprGAN [6] introduced\nan expression control module that enables the synthesis of\nfaces with diverse expressions, allowing for continuous ad-\njustment of expression intensity.\n[7] presented a neural\nnetwork system conditioned on categorical emotions, pro-\nviding direct and flexible control over visual emotion ex-\npression. MEAD [33] enhanced the vividness of talking\nfaces by curating the MEAD dataset, offering a baseline for\nemotional talking face generation. [13] proposed a ground-\nbreaking method for emotional control in video-based talk-\ning face generation, incorporating a component for distill-\ning content-agnostic emotion features. Addressing the chal-\nlenge of the timbre gap, [16] introduced a framework for\ntalking-head synthesis that generates facial expressions and\ncorresponding head animations from textual inputs. EAMM\n[14] and EMMN [30] both utilized 2D keypoints displace-\nment to synthesize the final emotional video, which can de-\ngrade the quality of generation. [18] presented a method for\ngenerating expressive talking heads with meticulous con-\ntrol over mouth shape, head pose, and emotional expres-\nsion. [26] proposed an optical flow-guided texture gener-\nation network capable of rendering emotional talking face\nanimations from a single image, regardless of the initial\nneutral emotion. SPACE [10] introduced a method decom-\nposing the task into facial landmark prediction and emotion\nconditioning, resulting in talking face videos with elevated\nresolution and fine-grained controllability. In our work, we\n3\nemploy a diffusion model to predict expression sequences,\nyielding more expressive outcomes.\n3. Method\n3.1. Preliminaries\nContrary to 2D landmark-based methods, which are sus-\nceptible to head pose variations and often face challenges\nin maintaining consistent facial shape representation [14,\n41], 3D modeling techniques offer shape-invariant infor-\nmation, thereby facilitating more realistic renderings that\nalign with the actual three-dimensional structure of human\nfaces. Traditional 3D models, such as 3D Morphable Mod-\nels (3DMM) or FLAME, predominantly utilize Principal\nComponent Analysis (PCA) to encapsulate facial features.\nWhile these parameters provide control over general facial\nappearance, they fall short in isolating specific facial at-\ntributes, such as eye blinking or lip movements.\nGiven\nour objective to enhance the mouth region while concur-\nrently preserving the expressiveness of other facial features,\nwe have elected to employ ARKit blendshapes. This tech-\nnology distinctly separates mouth-related parameters from\nother facial elements, thus enabling targeted optimization.\nThe ARKit facial model comprises 52 distinct parameters,\neach representing unique facial features. It utilizes blend-\nshapes based on the Facial Action Coding System (FACS),\nallowing each facial expression to activate specific facial\nregions (e.g., mouth area, eyes, eyebrows) independently\nand in a manner consistent with human facial anatomy [19].\nThis approach offers precise control over and optimization\nof various facial attributes, rendering it particularly well-\nsuited for our specialized optimization requirements.\nSubsequently, we conduct a comprehensive analysis of\nARKit parameters on each frame within the MEAD emo-\ntion dataset, thereby extracting corresponding parameters.\nThis process facilitates the creation of an ARKit-specific\nfacial dataset, meticulously tailored to align with the emo-\ntional nuances of the MEAD dataset. To the best of our\nknowledge, we are the first to develop an emotion dataset\nthat features fully disentangled 3D facial parameters. Such\na development significantly amplifies the practical utility\nand applicability of emotion-based datasets in the field.\n3.2. EmoDiff Module\nOur goal is to generate 3D emotional expressions from au-\ndio. However, this task presents significant challenges that\nrequire innovative solutions. Firstly, mapping audio to ex-\npressions is a one-to-many problem, making it difficult to\nobtain dynamic and realistic expressions. Secondly, gener-\nating a sequence of 3D face expression parameters involves\nnumerous issues, such as continuity and diversity. To ad-\ndress these challenges, we propose the first diffusion-based\nmodel for generating 3D face expression sequences.\nFigure 3. Time position-aware embedding of conditions with N\nframes per sequence.\nThe initial state corresponds to the first\nframe of the audio, and the middle three frames correspond to the\nemotion style. An additional bit is used to indicate the effective\ninformation. Finally, initial state and emotion style conditions are\nmerged frame-by-frame with audio features.\nForward diffusion and reverse process.\nWe adopt\nthe definition of denoising diffusion probabilistic models\n(DDPM) [12] as a forward noising process with latents\n{xt}T\nt=1 follow a Markov process q(xt|x0), where x0 is a\ndata point sampled from a real data distribution x0 \u223c q(x).\nThe forward noising process can be defined as:\nq(xt|x0) \u223c N(\u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I),\n(1)\nwhere \u00af\u03b1t \u2208 (0, 1) follow a monotonically decreasing sched-\nule, such that as \u00af\u03b1t approaches 0, xT can be approximated\nas N(0, I). The data point x0 gradually loses its distin-\nguishable features as step t becomes larger, generating a\nsequence a sequence of latent variables x1, . . . , xT .\nThe reverse diffusion process estimates the joint distri-\nbution of p\u03b8(x0:T ). The true sample could be generated\nfrom a noise input xT \u223c N(0, I). To adapt to the condi-\ntional synthesis, we provide additional conditions c to the\nmodel, including audio, initial state, and emotion style. The\noverview of our conditional reverse diffusion process is il-\nlustrated in Figure 3. The reverse process of each timestep\ncan be updated as follows:\np\u03b8(xt\u22121|xt, c) = N(xt\u22121; \u00b5\u03b8(xt, t, c), \u03b2tI).\n(2)\nIn the reverse diffusion process, given a Gaussian noise\nxT \u223c N(0, I), we use Eq. 2 to iteratively obtain the fi-\nnal results. Following [42], we set the variances to \u03b2tI with\nuntrained time-dependent constants.\nTraining objective. Since xt is available as the input to the\nmodel, we predict the gaussian noise \u03f5 instead of \u00b5 at time\nstep t:\n\u00b5\u03b8(xt, t, c) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t, c)\n\u0013\n,\n(3)\n4\nwhere \u03f5\u03b8 is a function approximator intended to predict \u03f5\nfrom xt. We optimize \u03b8 with the following objective, which\nworks better by ignoring the weighting term introduced in\nHo et al. [12]:\nLsimple(\u03b8) =Et,x0,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t, c)\u22252\n2\n\u0003\n=Et,x0,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, t, c)\u22252\n2\n\u0003\n,\n(4)\nwhere t is uniformly sampled between 1 and T.\nClassifier-free guidance. We train our conditional diffu-\nsion model by applying classifier-free guidance [11], which\nis widely used in recent diffusion-based works [32, 42].\nSpecifically, the condition c gets discarded periodically at\nrandom by setting c = \u03d5. The guided inference is then\nexpressed as the weighted sum of unconditionally and con-\nditionally generated samples:\n\u02c6\u03f5\u03b8 = (w + 1)\u03f5\u03b8(xt, t, c) \u2212 w(\u03f5\u03b8(xt, t, \u03d5),\n(5)\nwhere w is the scale parameter to trade off uncondition-\nally and conditionally generated samples. The classifier-\nfree guidance can achieve a good balance between quality\nand diversity.\nDiffusion model. For the selection of the network architec-\nture, we primarily considered two issues: 1) how to incor-\nporate different modality data, such as audio and 3D expres-\nsions, and 2) how to generate the N-frame 3D face expres-\nsion sequence different from images. As shown in Fig. 2,\nwe choose to use the transformer structure, which fuses the\nrepresentation of different modalities and captures the long-\nterm dependency with a cross-attention mechanism follow-\ning [42]. Please refer to the supplementary materials for\ndetailed information on the network architecture.\nTime-aware positional embedding. Generating facial ex-\npressions with temporal continuity requires consideration\nof two issues. Firstly, our training dataset only includes T-\nframe input data. However, during testing, we may require\nlonger testing audio, so we need to consider the continuity\nbetween generated sequences. To address this issue, we use\nthe first frame of the generated sequence as the input fea-\nture condition and add it to the network, which constrains\nthe initial state of the generated sequence.\nAdditionally, we need to guide the network to capture\nthe style of emotion expression. To achieve this, we use\nthree frames of expression as a representation of emotion\nstyle. To prevent the style from representing audio informa-\ntion, we randomly sample three frames of expression during\ntraining. To incorporate the initial state condition and style\ninformation into the network, we use a time position-aware\napproach as illustrated in Fig. 3 Specifically, we first create\na matrix with the same length as the audio frames and set\nthe first column of the matrix to the initial state value, a 50-\ndimensional vector with the last element set to 1. For style\nAlgorithm 1 Long-term Dynamic Sampling\n1: Trained diffusion model M, Input audio A, Emotion Style S,\nFrame length N.\n2: LA = length(A)\n3: Lexp = length(S[0])\n4: Output o = zeros(L, Lexp)\n5: Condition c = zeros(N, Nexp + 1)\n6: c[:, 0, : \u22121] = S[random(1), :]\n7: c[:, N/2 \u2212 1 : N/2 + 2, : \u22121] = S[random(3), :]\n8: c[:, 0, \u22121] = 1\n9: c[:, N/2 \u2212 1 : N/2 + 2, \u22121] = 1\n10: for i = 0 :: N \u2212 1 :: LA do\n11:\ntemp=M.sampling(cat[c, A[i : i + N]])\n12:\nc[:, 0, : \u22121] = temp[\u22121]\n13:\nc[:, N/2 \u2212 1 : N/2 + 2, : \u22121] = S[random(3), :]\n14:\no[i : i + N] = temp\n15: end for\n16: return o\ninformation, we select the middle three positions of the ma-\ntrix and set them to the style values, with the last element\nalso set to 1. We then combine this matrix with the audio\nfeature frame by frame during training, which completes the\nsetting of conditions.\nLong-term and dynamic sampling. During testing, we\nfirst select an emotional clip of a character from the dataset,\nsuch as \"M003 Angry clip001\", along with input audio. As\nshown in Alg. 1, to ensure continuity between sequences,\nwe randomly select one frame as the initial state, and sub-\nsequently use the last frame of the previous sequence as the\ninitial state for the next sequence. This ensures the con-\ntinuity of long sequences. To introduce diversity in each\nsequence generation, we randomly select 3 frames as the\nstyle each time, which allows for the generation of dynamic\nfacial expressions within the overall sequence.\n3.3. Lip Refinement\nAfter obtaining dynamic emotional expressions denoted as\nx0 from the diffusion model, we observed an unintended\nconsequence in which the diffusion network inadvertently\nreduced the influence of audio, resulting in a noticeable\nmisalignment between the audio and mouth shape. This\nphenomenon is likely attributed to the diffusion network\u2019s\ninherent inclination toward generating realistic sequences,\nwhich, in turn, diminishes the impact of the audio. To rec-\ntify this issue, we introduce a lip-sync refinement network\nthat utilizes the same audio and emotional data to recalibrate\nand generate refined mouth parameters. Our Lip-sync net-\nwork incorporates an LSTM structure as the audio encoder\nand a CNN structure as the emotion encoder. This design\neffectively generates mouth-related parameters that closely\nalign with the input audio and emotional reference style.\nFor a comprehensive understanding of our lip refinement\n5\nTable 1. Comparison with state-of-the-art one-shot methods on MEAD and HDTF datasets. MakeitTalk and SadTalker maintain lip-sync\nand image quality without considering emotion. However, adding emotion complicates lip-sync and rendering. EAMM struggles with\nthese challenges. Our method achieves both emotional expression and maintains lip-sync and image quality.\nMEAD\nHDTF\nMethod\nLPIPS\u2193\nCPBD\u2191\nF-LMD \u2193\nLSE-D \u2193\nLSE-C\u2191\nLPIPS\u2193\nCPBD\u2191\nF-LMD \u2193\nLSE-D \u2193\nLSE-C\u2191\nGround Truth\n0\n0.316\n0\n7.420\n7.486\n0\n0.303\n0\n7.413\n7.487\nMakeitTalk\n0.295\n0.213\n4.178\n10.151\n5.012\n0.289\n0.247\n5.026\n10.334\n4.823\nSadTalker\n0.189\n0.256\n3.960\n9.634\n6.095\n0.195\n0.269\n4.006\n9.958\n5.050\nEAMM\n0.295\n0.172\n6.053\n10.890\n4.328\n0.304\n0.161\n6.941\n10.686\n4.448\nOurs\n0.169\n0.299\n3.845\n9.868\n5.915\n0.176\n0.280\n3.948\n9.233\n5.263\nnetwork, we direct readers to the supplementary materials.\nSubsequently, we employ these refined facial parameters\nand the generated head poses to animate a 3D blend shape\nrig. Utilizing GPU rendering, we obtain corresponding 3D\nrendered avatar images denoted as It,t\u2208{0,..,N}. Following\nthis, we employ a video-to-video approach to generate talk-\ning face videos for arbitrary characters. Our ablation studies\ndemonstrated a notable improvement in the synchronization\nbetween mouth movements and audio upon the implemen-\ntation of lip refinement.\n3.4. Face Neural Rendering\nUpon acquiring synthesized images It,t\u2208{0,..,N} from an\nexternal GPU renderer, we employ motion transfer tech-\nniques to achieve a realistic talking head effect for dif-\nferent subjects. Specifically, we utilize the Face-Vid2Vid\nmethod proposed by Wang et al [34] as the fundamental\nneural rendering pipeline R. Furthermore, we conduct a\nfine-tuning process on the model using carefully selected\nhigh-resolution expressive talking videos from TalkHead-\n1HK dataset [34], aiming to enhance both expressiveness\nand rendering quality. In addition to fine-tuning, we aug-\nment the final image resolution to 512x512 using the face\nsuper-resolution method outlined in [1]. To ensure effective\nidentity preservation throughout the process, we implement\nthe relative mode technique developed by Siarohin et al.\n[25] for neural motion transfer. Specifically, we first render\na reference frame In with neural expression and then apply\nthe relative motion MIn\u2192It, which represents the transfor-\nmation between talking frames and the neural frame, onto\nthe source image T. Consequently, ultimate rendered out-\nputs R(T, MIn\u2192It) is generated.\n4. Experiments\n4.1. Implementation Details\nAll experiments were conducted on a single V100 GPU\nutilizing the Adam optimizer [15].\nThe frame rate for\nall training datasets was set at 25 FPS. In the EmoDiff\nTable 2. Comparisons of state-of-the-art methods and our pro-\nposed method. SadTalker and MakeItTalk do not generate emo-\ntional speech. EAMM produces emotional videos but loses iden-\ntity and dynamic facial expressions. EVP is a video-based method\nthat generates emotional speech but lacks emotion dynamics. In\ncontrast, DreamTalk offers dynamic emotional expression with\ngenerated eye blinks and identity preservation.\nMethods\nEmotional\nGenerated\nIdentity\ntalking\neye blinks\npreservation\nMakeItTalk\n\u2717\n\u2717\n\u2713\nSadTalker\n\u2717\n\u2713\n\u2713\nEAMM\n\u2713\n\u2717\n\u2717\nEVP\n\u2713\n\u2717\n\u2713\nOurs\n\u2713\n\u2713\n\u2713\nModule, our training primarily leveraged two datasets: the\nMEAD emotion dataset [33] and the HDTF multi-character\ndataset [39]. Each sequence generated during training con-\nsisted of a fixed length of 32 frames. We trained for a total\nof 1000 epochs with a batch size of 64 and a learning rate of\n0.0004. For the Lip refinement model, we employed a slid-\ning window of size T = 8 to extract training samples of au-\ndio features. The training process encompassed 50 epochs\nwith a batch size of 32 and a learning rate of 0.0001.\n4.2. Comparison with State-of-the-Arts\nIn Tab. 2, we offer an intuitive comparison of various meth-\nods\u2019 capabilities. It\u2019s clear that SadTalker and MakeItTalk\nlack the ability to generate emotional speech.\nAlthough\nEAMM can produce emotional videos, it does so at the ex-\npense of maintaining identity and fails to incorporate dy-\nnamic facial expressions, such as eye blinking. While EVP\ncan generate emotional speech (Fig. 5) , it also sacrifices\nthe dynamism of emotions and cannot drive from a single\nimage, limiting its applicability.\nIn contrast, DreamTalk\nnot only guarantees dynamic emotional expressions, such\nas generating high-frequency expressions like eye blinks,\nbut also delivers high-quality videos with accurate lip-sync.\n6\nFigure 4. Comparison of state-of-the-art models with our approach: In the first two comparisons, we conduct evaluations on the MEAD\nand HDTF datasets, respectively. For the third comparison, we utilize one AIGC-generated face. We also visualize our rig model re-\nsults as intermediate representations. Our method consistently yields significantly superior results in terms of emotional expression, lip\nsynchronization, identity preservation, and image quality. Please refer to our supplementary video for better comparison.\nFigure 5. Comparison with the video sequences provided by EVP\non emotion \"angry\". Since EVP requires separate training for each\nvideo, we cannot test it with arbitrary characters.\n4.2.1\nQualitative Evaluation\nAs shown in Fig. 4, we compare our work with three state-\nof-the-art methods. MakeItTalk and SadTalker can\u2019t gen-\nerate desired emotions from a single image. MakeItTalk\u2019s\n2D approach lowers image quality, while SadTalker pro-\nvides only neutral conversation. EAMM generates emo-\ntional speech but sacrifices emotion dynamics and video\nquality. Our method excels in emotional expression, lip-\nsync, identity preservation, and image quality.\nFigure 6. Here we evaluate the control parameters of Brow Down\non the right side. There will be a big jump between generated\nsequences, such as the results in the red box. By employing au-\ntoregressive feedback to generate long sequences, we can ensure\ncontinuous generation results.\n4.2.2\nQuantitative Evaluation\nFollowing established standards in the field, we utilize met-\nrics for lip sync and image quality in our comparative anal-\nyses. For assessing the synchronization between lip move-\n7\nments and input audio, we employ SyncNet [3], which\nmeasures he distance score (LSE-D) and confidence score\n(LSE-C) to evaluate lip-sync precision. We also employ the\nLandmark Distance on the entire face (F-LMD) for a com-\nprehensive evaluation of facial expression accuracy. For im-\nage quality, we assess image quality using learned percep-\ntual image patch similarity (LPIPS) and cumulative proba-\nbility blur detection (CPBD).\nOur model exhibits enhanced performance over current\nleading methods, as demonstrated in Tab.\n1.\nCompar-\ning state-of-the-art methods on MEAD and HDTF datasets,\nMakeItTalk and SadTalker maintain lip-sync and image\nquality without considering emotion.\nHowever, adding\nemotion complicates lip-sync and rendering. EAMM faces\nchallenges in achieving both emotion and maintaining lip-\nsync and image quality. Our method successfully balances\nemotional expression with lip-sync and image quality.\n4.3. Ablation Study\nOur ablation study primarily investigates three key ques-\ntions, including whether to employ an auto-regressive gen-\neration approach based on the initial state, whether to utilize\na style encoding method with positional embedding, and\nwhether to incorporate lip refinement for lip-sync results.\nAutoregressive generation Since our goal is to generate\nindefinitely long sequences, we require an autoregressive\nmethod to achieve continuous talking results. In this ap-\nproach, we use the last state of the previous sequence as\na condition to generate the next sequence, thereby ensur-\ning continuity between sequences. As illustrated in Fig. 6,\nwhere we depict the Brow Down value sequence, it is evi-\ndent that without the autoregressive constraint, there will be\na big jump between generated sequences.\nEmotional style embedding Unlike the approach used in\nEAMM, our emotional style employs positional embedding\nwith diffusion model. Since EAMM applies the same style\nfor each frame, which diminishes the dynamism of the emo-\ntional style, resulting in overly smooth emotions that fail\nto capture high-frequency information, such as eye blink-\ning. Here, we compare the use of a uniform emotional code\nagainst our method of positional embedding. It is observed\nthat our diffusion method combined with positional embed-\nding effectively captures high-frequency information. Refer\nto our supplementary video for comparison.\nLip refinement While the diffusion network aids in gen-\nerating dynamic emotions, we observed that it struggles to\nproduce sequences that fully align with the audio. Hence,\nwe employed a Lip Refinement Model to further optimize\nlip motion based on the audio. As shown in Tab. 3, we\nmeasured the results using SyncNet and found that the use\nof Lip Refinement leads to more synchronized lip motion.\nTable 3. Ablation for lip refinement. Without lip refinement, the\nquality of the generated mouth region cannot be guaranteed. With\nlip refinement, the synchronization between the generated mouth\nand the audio improves, and mouth movements becomes closer to\nthe ground truth (GT).\nMethod\nF-LMD \u2193\nLSE-D \u2193\nLSE-C\u2191\nw/o Lip refinement\n4.167\n10.583\n4.338\nOurs\n3.927\n9.648\n5.507\nFigure 7. User study results show ratings for emotion preservation\n(on the left) and overall quality (on the right).\n4.4. User Study\nDue to the subjective nature of emotion, quantitative evalu-\nation is challenging. Therefore, we employed a subjective\nassessment method, involving 20 users who compared the\nresults of different speech generation techniques. We pro-\nvided reference images and emotional information for eval-\nuation. The evaluation results depicted in Figure 7 demon-\nstrate that while EAMM can generate emotion, it comes at\nthe expense of video quality, leading to lower user ratings.\nMakeitalk and SadTalker, while lacking in emotion gen-\neration, achieved better overall quality than EAMM. Our\nmethod, on the other hand, successfully maintains emo-\ntional intensity while attaining high-quality generation.\n5. Conclusion\nIn this paper, we present DREAM-Talk, an innovative\nframework designed for the generation of emotionally ex-\npressive talking faces with precise lip synchronization.\nOur two-stage approach, comprising the EmoDiff Mod-\nule and Lip Refinement, effectively captures emotional\nnuances and ensures accurate lip-syncing.\nLeveraging\nan emotion-conditioned diffusion model and a lip re-\nfinement network, our method outperforms existing tech-\nniques.\nOur results demonstrate improved facial emo-\ntional expressiveness while maintaining high video qual-\nity.\nDREAM-Talk represents a significant leap forward\nin the domain of emotional talking face generation, en-\nabling the creation of realistic and emotionally engaging\ndigital human representations across a wide range of appli-\ncations.\n8\nReferences\n[1] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and\nChen Change Loy. Glean: Generative latent bank for large-\nfactor image super-resolution. In CVPR, 2021. 6\n[2] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang\nXu. Hierarchical cross-modal talking face generation with\ndynamic pixel-wise loss. In CVPR, 2019. 3\n[3] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In ACCV, 2016. 8\n[4] Joon Son Chung, Amir Jamaludin, and Andrew Zisserman.\nYou said that? arXiv, 2017. 3\n[5] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.\nVoxCeleb2: Deep Speaker Recognition. In Proceedings of\nInterspeech, 2018. 2\n[6] Hui Ding, Kumar Sricharan, and Rama Chellappa.\nEx-\nprGAN: Facial expression editing with controllable expres-\nsion intensity. In AAAI, 2018. 3\n[7] Sefik Emre Eskimez, You Zhang, and Zhiyao Duan. Speech\ndriven talking face generation from a single image and an\nemotion condition. IEEE TMM, 2021. 3\n[8] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, and\nYi Yang.\nEfficient emotional adaptation for audio-driven\ntalking-head generation. In ICCV, 2023. 3\n[9] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. AD-NERF: Audio driven neural\nradiance fields for talking head synthesis. In ICCV, 2021. 3\n[10] Siddharth Gururani, Arun Mallya, Ting-Chun Wang, Rafael\nValle, and Ming-Yu Liu. SPACE: Speech-driven portrait an-\nimation with controllable expression. arXiv, 2022. 2, 3\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv, 2022. 5\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 4, 5\n[13] Xinya Ji,\nHang Zhou,\nKaisiyuan Wang,\nWayne Wu,\nChen Change Loy, Xun Cao, and Feng Xu. Audio-driven\nemotional video portraits. In CVPR, 2021. 2, 3\n[14] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne\nWu, Feng Xu, and Xun Cao. EAMM: One-shot emotional\ntalking face via audio-based emotion-aware motion model.\nIn SIGGRAPH Conference, 2022. 2, 3, 4\n[15] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[16] Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yix-\ning Zheng, Xin Yu, and Changjie Fan.\nWrite-a-speaker:\nText-based emotional and rhythmic talking-head generation.\nIn AAAI, 2021. 3\n[17] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4d scans. ACM TOG, 2017. 2\n[18] Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin\nHong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding,\nand Jingdong Wang. Expressive talking head generation with\ngranular audio-visual control. In CVPR, 2022. 3\n[19] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng,\nZhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Beat:\nA large-scale semantic and emotional multi-modal dataset\nfor conversational gestures synthesis. In ECCV, 2022. 2, 4\n[20] Steven R. Livingstone and Frank A. Russo.\nThe Ryer-\nson Audio-Visual Database of Emotional Speech and Song\n(RAVDESS): A dynamic, multimodal set of facial and vocal\nexpressions in north american english. 2018. 2\n[21] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan,\nTangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. Styletalk:\nOne-shot talking head generation with controllable speaking\nstyles. arXiv, 2023. 2\n[22] Moustafa Meshry, Saksham Suri, Larry S Davis, and Abhi-\nnav Shrivastava. Learned spatial representations for few-shot\ntalking-head synthesis. In ICCV, 2021. 3\n[23] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu\nZhu, Jun He, Hongyan Liu, and Zhaoxin Fan.\nEmotalk:\nSpeech-driven emotional disentanglement for 3d face anima-\ntion. In ICCV, 2023. 3\n[24] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng\nZhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion\nmodels for generalized audio-driven portraits animation. In\nCVPR, 2023. 3\n[25] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. In NeurIPS, 2019. 6\n[26] Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Bro-\njeshwar Bhowmick. Emotion-controllable generalized talk-\ning face generation. arXiv, 2022. 3\n[27] Linsen Song, Wayne Wu, Chen Qian, Ran He, and\nChen Change Loy. Everybody\u2019s talkin\u2019: Let me talk as you\nwant. IEEE Transactions on Information Forensics and Se-\ncurity, 2022. 3\n[28] Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Sen He,\nMaciej Zi\u02dbeba, Stavros Petridis, and Maja Pantic. Diffused\nheads: Diffusion models beat gans on talking-face genera-\ntion. arXiv, 2023. 3\n[29] Supasorn\nSuwajanakorn,\nSteven\nM\nSeitz,\nand\nIra\nKemelmacher-Shlizerman. Synthesizing obama: Learning\nlip sync from audio. ACM TOG, 2017. 2\n[30] Shuai Tan, Bin Ji, and Ye Pan. Emmn: Emotional motion\nmemory network for audio-driven emotional talking face\ngeneration. In ICCV, 2023. 3\n[31] Sarah Taylor, Taehwan Kim, Yisong Yue, Moshe Mahler,\nJames Krahe, Anastasio Garcia Rodriguez, Jessica Hodgins,\nand Iain Matthews. A deep learning approach for generalized\nspeech animation. ACM TOG, 2017. 2\n[32] Jonathan Tseng, Rodrigo Castellon, and C Karen Liu.\nEDGE: Editable dance generation from music. arXiv, 2022.\n5\n[33] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,\nWayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change\nLoy.\nMEAD: A large-scale audio-visual dataset for emo-\ntional talking-face generation. In ECCV, 2020. 2, 3, 6\n[34] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In CVPR, 2021. 3, 6\n[35] Chao Xu, Junwei Zhu, Jiangning Zhang, Yue Han, Wenqing\nChu, Ying Tai, Chengjie Wang, Zhifeng Xie, and Yong Liu.\nHigh-fidelity generalized emotional talking face generation\nwith multi-modal emotion space learning. In CVPR, 2023. 3\n9\n[36] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, JinZheng He,\nand Zhou Zhao.\nGeneface: Generalized and high-fidelity\naudio-driven 3D talking face synthesis. arXiv, 2023. 3\n[37] Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng,\nSaifeng Ni, Madhukar Budagavi, and Xiaohu Guo.\nFA-\nCIAL: Synthesizing dynamic talking face with implicit at-\ntribute learning. In ICCV, 2021. 3\n[38] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In CVPR, 2023.\n3\n[39] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan.\nFlow-guided one-shot talking face generation with a high-\nresolution audio-visual dataset. In CVPR, 2021. 6\n[40] Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei,\nGangming Zhao, Liang Lin, and Guanbin Li.\nIdentity-\npreserving talking face generation with landmark and ap-\npearance priors. arXiv, 2023. 3\n[41] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar-\nria, Evangelos Kalogerakis, and Dingzeyu Li. MakeItTalk:\nSpeaker-aware talking-head animation. ACM TOG, 2020. 3,\n4\n[42] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu,\nand Lequan Yu. Taming diffusion models for audio-driven\nco-speech gesture generation. arXiv, 2023. 4, 5\n10\n"
  },
  {
    "title": "Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.13913.pdf",
    "upvote": "21",
    "text": "Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models\nXianfang Zeng1*\nXin Chen1*\nZhongqi Qi1*\nWen Liu1\nZibo Zhao1,3\nZhibin Wang1\nBIN FU1\nYong Liu2\nGang Yu1\u2020\n1Tencent PCG\n2Zhejiang University\n3 ShanghaiTech University\nhttps://github.com/OpenTexture/Paint3D\nFigure 1.\nA gallery of generated texture results by Paint3D. Our method is capable of generating lighting-less, high-quality, and high-\nfidelity textures across diverse objects from numerous categories.\nAbstract\nThis paper presents Paint3D, a novel coarse-to-fine\ngenerative framework that is capable of producing high-\nresolution, lighting-less, and diverse 2K UV texture maps\nfor untextured 3D meshes conditioned on text or image\ninputs. The key challenge addressed is generating high-\nquality textures without embedded illumination informa-\ntion, which allows the textures to be re-lighted or re-edited\nwithin modern graphics pipelines.\nTo achieve this, our\nmethod first leverages a pre-trained depth-aware 2D diffu-\nsion model to generate view-conditional images and per-\nform multi-view texture fusion, producing an initial coarse\ntexture map. However, as 2D models cannot fully repre-\nsent 3D shapes and disable lighting effects, the coarse tex-\nture map exhibits incomplete areas and illumination arti-\n*These authors contributed equally to this work.\n\u2020Corresponding author (email: skicyyu@tencent.com).\nfacts. To resolve this, we train separate UV Inpainting and\nUVHD diffusion models specialized for the shape-aware re-\nfinement of incomplete areas and the removal of illumina-\ntion artifacts. Through this coarse-to-fine process, Paint3D\ncan produce high-quality 2K UV textures that maintain se-\nmantic consistency while being lighting-less, significantly\nadvancing the state-of-the-art in texturing 3D objects.\n1. Introduction\nThe rise of deep generative models has ushered the era\nof Artificial Intelligence Generated Content, catalyzing ad-\nvancements in natural language generation [47, 59, 72],\nimage synthesis [43, 49, 51, 52], and 3D generation [32,\n44, 62].\nThese 3D generative technologies have signif-\nicantly impacted various applications, revolutionizing the\nlandscape of current 3D productions. However, the gener-\nated meshes, characterized by chaotic lighting textures and\narXiv:2312.13913v2  [cs.CV]  22 Dec 2023\ncomplex wiring, are often incompatible with traditional ren-\ndering pipelines, such as physically based rendering (PBR).\nThe lighting-less texture diffusion model, capable of gen-\nerating diverse appearances of 3D assets, should augment\nthese pre-existing 3D productions for the gaming industry,\nfilm industry, virtual reality, and so on.\nRecent advancements in texture synthesis have shown\nsignificant progress, particularly in the utilization of 2D\ndiffusion models such as TEXTure [50] and Text2tex [5].\nThese models effectively employ pre-trained depth-to-\nimage diffusion models to generate high-quality textures\nthrough text conditions. However, these methods have is-\nsues with pre-illuminated textures. This can damage the\nquality of final renderings in 3D environments and cause\nlighting errors when changing lighting within common\ngraphics workflows, as shown in the bottom of Fig. 2. Con-\nversely, texture generation methods trained from 3D data\noffer an alternative approach such as PointUV [69] and\nMesh2tex [2], which typically generate textures by com-\nprehending the entire geometries for specific 3D objects.\nHowever, they are often hindered by a lack of generaliza-\ntion, struggling to apply their models to a broad range of 3D\nobjects beyond their training datasets, as well as generate\nvarious textures through different textual or visual prompts.\nTwo challenges are crucial for texture generation. The\nfirst is achieving broad generalization across various objects\nusing diverse prompts or image guidance, and the second is\neliminating the coupled illumination on the generated re-\nsults obtained from pre-training. Recent advancement of\nconditioned image synthesis works [51, 70] using billion-\nlevel images, capable of \u201crendering\u201d diverse image results\nfrom 3D views, can help overcome the size limitation of\n3D data in texture generation. However, the pre-illuminated\ntextures can interfere with the final visual outcomes of these\ntextured objects within rendering engines.\nFurthermore,\nsince the pre-trained image diffusion models only provide\n2D results in the view domain, they struggle to maintain\nview consistency for 3D objects due to the lack of compre-\nhensive understanding of their shapes. Therefore, our focus\nis on developing a two-stage texture diffusion model for 3D\nobjects. This model should be able to generalize to various\npre-trained image generative models and learn lighting-less\ntexture generation while preserving view consistency.\nIn this work, we propose a coarse-to-fine texture gener-\nation framework, namely Paint3D, that leverages the strong\nimage generation and prompt guidance abilities of pre-\ntrained image generative models for texturing 3D objects.\nTo enable the generalization of rich and high-quality texture\nresults from diverse prompts, we first progressively sam-\nple multi-view images from a pre-trained view depth-aware\n2D image diffusion model and then back-project these im-\nages onto the surface of the 3D mesh to generate an ini-\ntial texture map. In the second stage, Paint3D focuses on\nfree illumination\npre illumination\n(a) textured meshes with ambient lighting (b) textured meshes with three lighting conditions\nFigure 2. Illustration of the pre-illumination problem. The texture\nmap with free illumination is compatible with traditional rendering\npipelines, while there are inappropriate shadows when relighting\nis applied on the pre-illumination texture.\ngenerating lighting-less textures. To achieve this, we con-\ntribute separate UV Inpainting and UVHD diffusion models\nspecialized in the shape-aware refinement of incomplete re-\ngions and removal of lighting influences. We train these\ndiffusion models on UV texture space, using feasible 3D\nobjects and their high-quality illumination-free textures as\nsupervision. Through this coarse-to-fine process, Paint3D\ncan generate semantically consistent high-quality 2K tex-\ntures devoid of intrinsic illumination effects. Extensive ex-\nperiments demonstrate that Paint3D achieves state-of-the-\nart performance in texturing 3D objects with different texts\nor images as conditional inputs and offers compelling ad-\nvantages for graphics editing and synthesis tasks.\nWe summarize our contributions as follows: 1) We pro-\npose a novel coarse-to-fine generative framework that is ca-\npable of producing high-resolution, lighting-less, and di-\nverse 2K UV texture maps for untextured 3D meshes; 2)\nWe separately design a shape-aware UV Inpainting diffu-\nsion model and a shape-aware UVHD diffusion model as\nthe refinement of incomplete regions and removal of light-\ning influences; 3) Our proposed Paint3D supports both tex-\ntual and visual prompts as conditional inputs and achieves\nstate-of-the-art performance on texturing 3D objects. The\ncode will be released later.\n2. Related Work\nTraditional methods [19, 25, 26, 61, 63, 64, 73] of synthe-\nsizing texture to 3D assets concentrated on placing sim-\nple exemplar patterns on a surface or levering global op-\ntimization for painting the 3D shape. However, the recent\nlearning-based approaches [8, 21, 29, 41, 45, 48, 57, 65, 74]\nhave succeeded in generating plausible textures for more\ncomplex 3D shapes. The following discusses the related\nlearning-based methods.\nIteratively Texturing via 2D Diffusion Models. The\nrapidly expanding large-scale 2D text-to-image (T2I) dif-\nfusion models [49, 51, 52] have yielded remarkable out-\ncomes, and subsequently, [28, 32, 33, 53, 58] harness the\ncapabilities of T2I models to facilitate texture synthesis on\nNext Viewpoint - Depth-aware Inpainting\nInitial Viewpoint - Depth-aware Generation\n\u2026\nCoarse Stage\nRefinement Stage \nCoarse Texture Map\nCoarse Textured Mesh\nDepth Maps\n\ud835\udf0f\ud835\udf0f\ud835\udc5d\ud835\udc5d\n\ud835\udf0f\ud835\udf0f\ud835\udc56\ud835\udc56\n\ud835\udf0f\ud835\udf0f\ud835\udc50\ud835\udc50\nRefined \nTexture Map\nRendering\nTexturing\nData Flow\nUV Unwarping\nUV Warping\nFrozen Module\nTrainable Module\nTEXTURE REFINEMENT IN UV SPACE (SEC. 3.2)\nUntextured Mesh\nConditional Inputs\nimage\ntext\na brown armadillo\nPosition Map\n\ud835\udc9f\ud835\udc9f(\u0209; \ud835\udf0f\ud835\udf0f\ud835\udc56\ud835\udc56, \ud835\udf0f\ud835\udf0f\ud835\udc50\ud835\udc50, \ud835\udf0f\ud835\udf0f\ud835\udc51\ud835\udc51)\nRendered Images\nCoarse \nTextured Mesh\nPROGRESSIVE COARSE TEXTURE GNERATION (SEC. 3.1)\n\ud835\udf0f\ud835\udf0f\ud835\udc61\ud835\udc61\nFinal\nTextured Mesh\nSwitch\n\u2026\nIterative Process\n\ud835\udc9f\ud835\udc9f(\u0209; \ud835\udf0f\ud835\udf0f\ud835\udc50\ud835\udc50, \ud835\udf0f\ud835\udf0f\ud835\udc51\ud835\udc51)\nPosition\nEncoder\nInpainting\nEncoder\nMask\nHD \nEncoder\nXYZ\nRGB\nUV Inpainting and UVHD\nFigure 3. The overview of our coarse-to-fine framework. The coarse stage (Sec. 3.1) samples multi-view images from the pre-trained\n2D image diffusion models, then back-projects these images onto the mesh surface to create initial texture maps. The refinement stage\n(Sec. 3.2) generates high-quality textures with a diffusion model in UV space, conditioned on the position map and the coarse texture map.\n3D assets. TEXTure [50] devises an iteratively texturing\nscheme and succeeds in synthesizing high-quality texture.\nIt leverages a pretrained depth-to-image diffusion model\nand gradually paints the texture map of a 3D model from\nmultiple viewpoints. Although TEXTure [50] samples a\npartial texture map under each viewpoint conditioned on\nprevious results, the generative process still lacks global in-\nformation modeling, leading to the view-inconsistency re-\nsults. Later, TexFusion [3] proposes to aggregate texture\ninformation from different viewpoints during the denois-\ning process and synthesize the entire texture map, which\nimproves the view consistency. Besides, Text2tex [5] de-\nveloped an automatic method to select viewpoints for sav-\ning human efforts. These methods improve the global tex-\nture modeling but still suffer from the inherited lighting bias\nfrom 2D Priors, leading to inconsistent results. In contrast,\nour framework involves a texture refinement model trained\nwith illumination-free texture data, significantly alleviating\nthe illumination artifacts.\nOptimization-based 3D Generation via 2D diffusion\nmodel. Prior to the emergence of large-scale text-to-image\nmodels, early optimization-based texturing approaches [18,\n27, 35, 37, 38] endeavored to utilize the large-scale vision-\nlanguage model, CLIP [46], for optimizing texture map of\n3D models. Subsequently, the introduction of Score Dis-\ntillation Sampling (SDS) in DreamFusion [44] has paved\nthe way for numerous text-to-3D approaches [7, 9, 31, 36,\n55, 56, 60, 62]. Latent-nerf [36] and Fantasia3D [7] extend\nSDS for optimizing the texture map with texture-less 3D\nshapes as input. Those methods consider inputting an ini-\ntial shape and simultaneously optimize the texture map and\ngeometry. They could produce multi-view consistent tex-\nture but cannot guarantee geometry fidelity. Moreover, they\nstruggle with the Janus problem due to the semantically am-\nbiguous. Different from these methods, our model learns on\nthe whole texture map, preserving the 3D geometry.\nGenerative Texturing from 3D Data. Various learning-\nbased approaches usually train generative texturing models\nbased on the 3D data [12, 13, 20, 30, 34, 39] from scratch.\nEarly methods [6, 15, 16, 40] learn implicit texture fields to\nassign a color to each pixel on the surface of the 3D shape.\nHowever, since the texture on the surface of 3D shapes is\ncontinuous, discrete supervision is unlikely to train a model\nfor synthesizing high-quality textures. Texturify [54] de-\nfines texture maps on the surface of polygon meshes and de-\nvises a convolution operator for mesh structures by incorpo-\nrating SytleGAN [22\u201324] architecture for predicting texture\non each face. Such methods are limited by the mesh resolu-\ntion and the lack of global information modeling, although\nthe recent Mesh2tex [2] further integrates an implicit tex-\nture field branch for improvements. Moreover, some meth-\nods (AUV-net [10], LTG [68], TUVF [11], PointUV [69])\nlearn to synthesize UV-Maps for 3D shapes, avoiding the\nabovementioned limitations. Unfortunately, these methods\nusually struggle when handling more general objects due to\nthe variations between 3D objects in different categories.\n3. Method\nTo synthesize high-quality and diverse texture maps for 3D\nmodels based on desired conditional inputs like prompts or\nimages, we propose a coarse-to-fine framework, Paint3D,\nwhich progressively generates and refines texture maps, as\nshown in Fig. 3. In the coarse stage (see Sec. 3.1), we sam-\nple multi-view images from the pre-trained 2D image diffu-\nsion models, then back-project these images onto the mesh\nsurface to create initial texture maps. In the refinement\nstage (see Sec. 3.2), we enhance coarse texture maps by\nperforming a diffusion process in the UV space, achieving\nlighting-less, inpainting, High Definition (HD) functions to\nensure the final texture\u2019s completeness and visual appeal.\nGiven an uncolored 3D model M and an appearance\ncondition c, such as text prompts [5, 50] or an appearance\nreference image [2], our Paint3D aims to generate the tex-\nture map T for the 3D model. Here, we represent the 3D\nmodel\u2019s geometry using a surfaced mesh, denoted as M =\n(V, F), with vertices V = {vi}, vi \u2208 R3 and triangular\nfaces F = {fi}, where each fi is a triplet of vertices. The\ntexture map is represented by a multi-channel image in UV\nspace, denoted as T \u2208 RH\u00d7W \u00d7C. The proposed Paint3D\nframework P consists of two stages: the coarse texture gen-\neration stage C : (M, c) 7\u2192 \u02c6T and the texture refinement\nstage F : \u02c6T 7\u2192 T, that is T = P(M, c) = F(C(M, c)).\nFurthermore, we define a conditional diffusion model as\nD(\u00b7; \u03c4\u03b8), where \u03c4\u03b8 is a domain-specific encoder and can be\nsubstituted for varying conditions.\n3.1. Progressive Coarse Texture Generation\nIn this state, we generate a coarse UV texture map for untex-\ntured 3D meshes based on a pre-trained view depth-aware\n2D diffusion model. Specifically, we first render the depth\nmap from different camera views, then sample images from\nthe image diffusion model with depth conditions, and finally\nback-project these images onto the mesh surface. To im-\nprove the consistency of textured meshes in each view, we\nalternately perform the three processes of rendering, sam-\npling, and back-projection, progressively generating the en-\ntire texture map [5, 50].\nInitial Viewpoint. With the set of camera views {pi}n\ni=1\nfocusing on the 3D mesh, we start to generate the texture of\nthe visible region. We first render the 3D mesh to a depth\nmap d1 from the first view p1, where this rendering process\nis denoted as R : (M, p1) 7\u2192 d1. We then sample a tex-\nture image I1 given an appearance condition c and a depth\ncondition d1, denoted as\nI1 = D(z, c, d1; \u03c4c, \u03c4d),\n(1)\nwhere z \u2208 Rh\u00d7w\u00d7e is a random initialized latent, \u03c4c is ap-\npearance encoder, and \u03c4d is depth encoder. Subsequently,\nwe back-project this image onto the 3D mesh from the first\nview, generating the initial texture map \u02c6T1, where this back-\nprojecting process is denoted as R\u22121 : (M, I1, p1) 7\u2192 \u02c6T1.\nNext Non-initial Viewpoint. For these viewpoints pk,\nwe execute a similar process as mentioned above but the\ntexture sampling process is performed in an image in-\npainting manner. Specifically, taking into account the tex-\ntured region from all previous viewpoints \u02c6T{1,k\u22121}, the\nrendering process outputs not only a depth image dk but\nalso a partially colored RGB image \u02c6Ik and an uncol-\nored area mask mk in the current view, denoted as R :\n(M, pk, \u02c6T{1,k\u22121}) 7\u2192 (dk, \u02c6Ik, mk). We use a depth-aware\nimage inpainting model, with a new inpainting encoder \u03c4i,\nto fill the uncolored area within the rendered RGB image,\ndenoted as\nIk = D(\u02c6Ik, mk, c, dk; \u03c4i, \u03c4c, \u03c4d).\n(2)\nThe inpainted image is back-projected onto the 3D mesh\nunder the current view, generating the current texture map\n\u02c6Tk from the view pk, denoted as R\u22121 : (M, Ik, pk) 7\u2192 \u02c6Tk.\nThe textured region from previous viewpoints \u02c6T{1,k\u22121} is\nkept and the uncolored area is updated by the current texture\nmap \u02c6Tk, formatted as\n\u02c6T{1,k} = mUV\nk\u22121 \u2299 \u02c6T{1,k\u22121} + (1 \u2212 mUV\nk\u22121) \u2299 \u02c6Tk,\n(3)\nwhere mUV\nk\u22121 is the colored area mask in the UV plane and\ncan be calculated from the texture map \u02c6T{1,k\u22121}. Therefore,\nthe texture map is progressively generated view-by-view\nand arrives at the entire coarse texture map \u02c6T = \u02c6T{1,n}.\nMulti-view Texture Sampling. We extend the texture\nsampling process mentioned above (Eq. (1) and Eq. (2))\nto the multi-view scene. Specifically, in the initial texture\nsampling, we utilize a pair of cameras to capture two depth\nmaps {d1, d2} from symmetric viewpoints. We then con-\ncatenate those two depth maps horizontally (in width) and\ncompose a depth grid with a size of 1 \u00d7 2, denoted as d1.\nTo perform multi-view depth-aware texture sampling, we\nreplace the single depth image d1 with the depth grid d1\nin Eq. (1). Similarly, in the non-initial texturing, we hor-\nizontally concatenate renders, composing depth grid dk,\nRGB image grid \u02c6Ik, and mask grid mk. To perform multi-\nview depth-aware texture inpainting, we replace the inputs\nin Eq. (2) with those grids. As evaluated in Sec. 4.4, we also\nexplore the effectiveness of the number of viewpoints.\n3.2. Texture Refinement in UV Space\nAlthough the appearance of the coarse texture map is co-\nherent, it still has some issues like lighting shadows in-\nvolved by the 2D image diffusion model, or the texture\nholes caused by self-occlusion during the rendering process.\nWe propose to perform a diffusion process in the UV space\nbased on the coarse texture map, aiming to mitigate these\nissues and further enhance the visual aesthetics of the tex-\nture map during texture refinement. However, refining tex-\nture maps in the UV space with mainstream image diffusion\nmodels [51, 71] presents the challenge of texture disconti-\nnuity [69]. The texture map is derived through UV mapping\nof the 3D surface texture, which cuts the continuous texture\non the 3D mesh into a series of individual texture fragments\nin the UV plane. This fragmentation complicates the learn-\ning of the 3D adjacency relationships among the fragments\nin the UV plane, leading to texture discontinuity issues.\nPosition Encoder.\nTo refine the texture map in UV\nspace, we perform the diffusion process guided by adja-\ncency information of texture fragments. Here, the 3D adja-\ncency information of texture fragments is represented as the\nposition map in UV space O \u2208 RH\u00d7W \u00d73, where each non-\nbackground element is a 3D point coordinate. Similar to the\ntexture map, the position map can be obtained through UV\nmapping of the 3D point coordinates. To fuse the 3D adja-\ncency information during the diffusion process, we add an\nindividual position map encoder \u03c4p to the pretrained image\ndiffusion model. Following the design principle of Control-\nNet [71], the new encoder has the same architecture as the\nencoder in the image diffusion model and is connected to it\nthrough zero-convolution layer.\nOur texture diffusion model is trained using a dataset\nconsisting of paired position maps and texture maps\n{Oi, Ti}n\ni=1. Given a set of conditions including time step\nt, appearance condition c, as well as a position map O , our\ntexture diffusion model learns to predict the noise added to\nthe noisy latent zt with\nL = Ez0,t,c,O,\u03f5\u223cN (0,1)\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, c, \u03c4p(O))\u22252\n2\ni\n. (4)\nFor an image diffusion model with a trained denoiser \u03f5\u03b8,\nwe freeze \u03f5\u03b8 as suggested by [71] and only optimize the\nposition encoder \u03c4p with Eq. (4). Since texture maps in UV\nspace are lighting-less, our model can learn this prior from\ndata distribution, generating lighting-less texture.\nUV Inpainting. We can simultaneously use the position\nencoder and other conditional encoders to perform various\nrefinement tasks in UV space. Here we introduce two spe-\ncific refinement capabilities, namely UV inpainting and UV\nHigh Definition (UVHD). The UV inpainting is used to fill\ntexture holes within the UV plane, which can avoid self-\nocclusion problems during rendering. To achieve UV in-\npainting, we add the position map encoder \u03c4p on an image\ninpainting diffusion model as\nTinpainting = D( \u02c6T, mUV , c, O; \u03c4i, \u03c4c, \u03c4p),\n(5)\nwhich takes as input a coarse texture map \u02c6T, texture map\nmask mUV , appearance condition c, and position map O,\nand produces as output an inpainted texture map Tinpaint.\nUV High Definition (UVHD) is designed to enhance the\nvisual aesthetics of the texture map. We use the position en-\ncoder \u03c4p and an image enhance encoder \u03c4t with a diffusion\nmodel D(\u00b7; \u03c4c) to achieve UVHD, denoted as\nTtiling = D( \u02c6T, c, O; \u03c4t, \u03c4c, \u03c4p).\n(6)\nIn our refinement stage, we perform UV inpainting followed\nby UVHD to get the final refined texture map T. By inte-\ngrating the UV inpainting and UVHD, Paint3D is capable\nof producing lighting-less (Fig. 7), complete (Fig. 8), high-\nresolution, and diverse UV texture maps (Fig. 9).\n4. Experiments\nWe provide extensive comparisons to evaluate our models\non both quality and diversity in the following. Firstly, we in-\ntroduce the datasets settings, evaluation metrics and imple-\nmentation details Sec. 4.1. Importantly, we show the com-\nparisons on two texture generation tasks, including text-to-\ntexture (Sec. 4.2), image-to-texture (Sec. 4.3). Lastly, we\nconduct ablation studies to demonstrate the effectiveness of\neach module in our Paint3D (Sec. 4.4). More qualitative re-\nsults, comparisons, and details are provided in supplements.\n4.1. Implementation Details\nWe apply the text2image model from Stable Diffusion\nv1.5 [51] as our texture generation backbone. To handle\nthe image condition, we employ the image encoder intro-\nduced in IP-Adapter [66]. For additional conditional con-\ntrols such as depth, image inpainting, and image high defi-\nnition, we utilize the domain encoders provided in Control-\nNet [71]. In the coarse texture generation, we define six\naxis-aligned principal viewpoints, and sample two texture\nimages from a pair of symmetric viewpoints during a sin-\ngle diffusion progress. The denoising strengths are set as 1\nand 0.75 for the coarse and refinement stages, respectively.\nOur implementation uses the PyTorch [42] framework, with\nKaolin [14] used for rendering and texture projection. For\nthe UV unwarping process, we utilize the original UV map\nif the mesh contains texture coordinates, or we use an open-\nsource UV-Atlas tool [67] to perform UV unwarping.\nDatasets. We conduct experiments on a subset of tex-\ntured meshes from the Objaverse [13] dataset. We exclude\nmeshes devoid of textures, those with monochromatic tex-\nture, and 3D scene objects composed of multiple meshes.\nThe filtered subset contains 105,301 texture meshes, with\n105,000 meshes utilized for training the position encoder\nand 301 meshes employed for evaluating our model. Ad-\nditionally, we gather 30 meshes in the wild to assess our\nmodel. This brings the total to 331 high-quality textured\nmeshes for evaluation.\nEvaluation metrics. We access the generated textures\nwith commonly used metrics for image quality and diver-\nsity. Specifically, we report the Frechet Inception Distance\n(FID) [17] and Kernel Inception Distance (KID \u00d710\u22123) [1].\nTo calculate the generated image distribution, we render\n512 \u00d7 512 images of each mesh with the synthesized tex-\ntures, captured from 20 fixed viewpoints. The real distri-\nbution is made up of renders of the meshes under identical\nsettings, but using their original textures.\n4.2. Comparisons on Text-to-Texture\nWe first evaluate the texture generation effect of Paint3D\nconditioned on the text prompt.\nWe compare our\nmethod with state-of-the-art approaches, including Latent-\nPaint [36], TEXTure [50], and Text2Tex [5]. Latent-Paint\nTEXTure Latent-Paint\nText2Tex\nOurs\n\u201ca brown armadillo\u201d\n\u201chanfu-style clothing\u201d\n\u201cteapot, blue and white porcelain\u201d\n\u201ca next gen nascar\u201d\nFigure 4. Qualitative comparisons on texture generation conditioned on text prompt. We compare our textured mesh against Latent-\nPaint [36], TEXTure [50], and Text2Tex [5]. Compared to the baselines, our method generates an illumination-free texture map, as well as\nmore exquisite texture details (cf. supplements for more our results).\nMethods\nFID\u2193\nKID \u2193\nUser Study\nOverall Quality\u2191\nText Fidelity\u2191\nLatent-Paint [36]\n62.22\n15.81\n2.83\n3.29\nTEXTure [50]\n43.13\n11.13\n3.36\n4.12\nText2Tex [5]\n38.93\n7.94\n3.57\n4.27\nOurs\n27.28\n4.81\n4.45\n4.74\nTable 1. Quantitative comparisons on text-to-texture task. Ours\noutperforms other approaches on both FID and KID (\u00d710\u22123).\nis a texture generation variant of the NeRF-based 3D object\ngeneration framework, explicitly manipulating the texture\nmap via the text2image model from Stable Diffusion. TEX-\nTure devises an iterative texture generation scheme to ma-\nnipulate the texture map, and successfully synthesizes high-\nquality textures. Following a similar principle, Text2Tex\ndevelops an automatic viewpoint selection strategy in the\niterative process, representing the current state-of-the-art in\nthe field of text-conditioned texture generation.\nFor the\ncategory-specific texture generation approaches [2, 54, 69],\nwe provide more comparisons in the supplements.\nQualitative comparisons. As shown in Fig. 4, our ap-\nproach is able to generate an illumination-free texture map\nwhile excelling at synthesizing high-quality texture details.\nFirstly, Latent-Paint [36] tends to generate blurry textures,\nwhich can lead to suboptimal visual effects. Additionally,\nwhile TEXTure [50] is capable of generating clear textures,\nthe generated textures may lack smoothness and exhibit no-\nticeable seams or splicing(e.g., the teapot in Fig. 4). Lastly,\neven though Text2Tex [5] demonstrates the ability to gener-\nate smoother textures, it may compromise in generating fine\ntextures with intricate details. Notably, all baselines gener-\nate pre-illumination texture maps that led to inappropriate\nshadows when relighting was applied.\nQuantitative comparisons. In Tab. 1, we present the\nquantitative comparisons with the previous SOTA methods\nin text-driven texture synthesis. Following [5, 69], we re-\nport the FID [17] and KID [1] to access the quality and\ndiversity of the generated texture maps. Our method out-\nperforms all baselines by a significant margin (29.93% im-\nprovement in FID and 39.42% improvement in KID). These\nimprovements demonstrate the superior capability of our\nmethod in generating high-quality textures across diverse\nobjects from numerous categories.\nUser study. We further conduct a user study to ana-\nlyze the overall quality of the generated textures and their\nfidelity to the input text prompts. We randomly select 60\nmeshes and corresponding text prompts to perform the user\nstudy. Those meshes are textured by both Paint3d and base-\nline models, and displayed to users in random sequence.\nEach object displays full-view texture details in the form\nof 360-degree rotation. Each respondent is asked to eval-\nuate the results based on two aspects: (1) overall quality\nand (2) fidelity to the text prompt, using a scale of 1 to\n5. We collected the evaluation results of 30 users, as pre-\nsented in Tab. 1, where we show the average results across\nall prompts for each method. As can be seen, our approach\noutperforms all baselines in terms of both overall quality\nand text fidelity by a significant margin.\n4.3. Comparisons on Image-to-Texture\nWe then evaluate the texture generation capability of\nPaint3D conditioned on the image prompt.\nHere, we\nprovide TEXTure [50] as our comparison baseline.\nWe\nuse the texture transfer capability of TEXTure to generate\nits image-to-texture results.\nTo handle the image condi-\nTEXTure\nOurs\nTEXTure\nOurs\nFigure 5. Qualitative comparisons on texture generation condi-\ntioned on image prompt. Compared to TEXTure, our method can\nbetter represent texture details contained in the image condition.\nMethods\nFID\u2193\nKID \u2193\nUser Study\nOverall Quality\u2191\nImage Fidelity\u2191\nTEXTure [50]\n40.83\n9.76\n3.56\n3.73\nOurs\n26.86\n4.94\n4.71\n4.89\nTable 2. Quantitative comparisons on image-to-texture task. Our\nmethod achieves a significant improvement over the baseline.\ntion, our Paint3D employs the image encoder introduced\nin [66] based on the txt2image model from Stable Diffu-\nsion v1.5 [51]. As depicted in Fig. 5, our approach excels\nin synthesizing exquisite texture while maintaining high fi-\ndelity with respect to the image condition. TEXTure [50]\nis capable of generating a similar texture as the input im-\nage, but it struggles to accurately represent texture details\nin the image condition. For instance, in the samurai case,\nTEXTure generates a golden armor texture but fails to syn-\nthesize high-frequency line details present on the armor.\nAs shown in Tab. 2, we also report the FID [17] and\nKID [1] scores under the image condition.\nOur method\ndemonstrates a significant improvement over the baseline,\nas evidenced by the FID score decreasing from 40.83 to\n26.86 and the KID score decreasing from 9.76 to 4.94. For\nthe user study, we follow a similar evaluation setting as de-\nscribed in Sec. 4.2, but replace the text prompt with the\nimage prompt. Each participant needs to assess the gener-\nated texture based on its overall quality and fidelity to the\nimage prompt, using a rating scale ranging from 1 to 5. The\naverage scores of all users are reported in Tab. 2. Notably,\nPaint3D gets a 4.89 average score on image fidelity, indicat-\nCoarse Stage\nRefinement Stage\nFID\u2193\nKID \u2193\nUV inpainting\nUVHD\n\u2713\n41.84\n10.91\n\u2713\n\u2713\n48.81\n11.98\n\u2713\n\u2713\n37.84\n7.13\n\u2713\n\u2713\n33.42\n6.19\n\u2713\n\u2713\n\u2713\n27.28\n4.81\nTable 3. Evaluation of modules in the Paint3D framework. This\ndemonstrates the effectiveness of each component, including the\ncoarse stage, UV inpainting, and UVHD. By integrating the gen-\neration prior in the coarse stage and the illumination-free prior in\nthe refinement stage, our full model achieves the optimal result.\n(a) without our coarse stage\n(b) with our coarse stage\nFigure 6. Illustration of the effect of the coarse stage. The absence\nof our coarse stage may result in semantic confusion in the texture.\n(a) without our refinement stage\n(b) with our refinement stage\nFigure 7. Visualization of the effect of the refinement stage. With\nour refinement stage, the generated textures are illumination-free.\ning our method is able to accurately represent texture details\ncontained in the image condition.\n4.4. Ablation Studies\nEvaluation of Coarse-to-fine Framework.\nTo demon-\nstrate the effectiveness of our coarse-to-fine texture gen-\neration framework, we conduct experiments on two base-\nlines \u201cw/o coarse stage\u201d and \u201cw/o refinement stage\u201d. The\n\u201cw/o coarse stage\u201d configuration refers to directly generat-\ning the texture map using the texture refinement modules in\nUV space, performing UV inpainting followed by UVHD\nwithout initialization from the coarse stage. The \u201cw/o re-\nfinement stage\u201d configuration represents the outcome of the\ncoarse stage, where the uncolored area is assigned a color\n(a) without our UV inpainting module\n(b) with our UV inpainting module\nFigure 8. Illustration of the effect of UV inpainting. UV inpainting\ncan effectively fill texture holes that are located in projecting blind\nspots (e.g. the inner side of a pleated skirt).\n(a) without our UVHD module\n(b) with our UVHD module\nFigure 9. Illustration of the effect of UVHD module. This displays\nthe capability of UVHD to enhance existing texture details and can\neven generate new textures in monochromatic areas.\nusing bilinear interpolation. In both scenarios, the model\nproduces inferior results compared to our full model, as re-\nported in Tab. 3. We visualize the results of \u201cw/o coarse\nstage\u201d in Fig. 6. Absent the coarse stage, the generated\ntextures may display noticeable semantic problems, as the\ntexture map in UV space consists of separate texture frag-\nments. As shown in in Fig. 7, without the refinement stage,\nthe generated textures are pre-illuminated.\nEvaluation of UV inpainting and UVHD. To demon-\nstrate the effectiveness of two texture refinement modules,\nUV inpainting and UVHD, we further conduct experiments\non two baselines \u201cw/o UV inpainting\u201d and \u201cw/o UVHD\u201d.\nThe \u201cw/o UV inpainting\u201d configuration refers to filling the\nuncolored area with the bilinear interpolation instead of\nUV inpainting, followed by the UVHD module. The \u201cw/o\nUVHD\u201d configuration represents the inpainted result of the\ncoarse stage with the UV inpainting module. As indicated\nin Tab. 3, the performance shows a significant decrease\nwhen UV inpainting or UVHD is not utilized, indicating\ntheir irreplaceable function during texture refinement pro-\ncessing. We visualize the results of \u201cw/o UV inpainting\u201d\nin Fig. 8. UV inpainting can effectively fill texture holes\nthat are located in blind spots, as this inpainting process-\n#Viewpoint\nFID\u2193\nKID \u2193\n#Viewpoint\nFID\u2193\nKID \u2193\nTotal\nOne Iter\nTotal\nOne Iter\n2\n1\n42.31\n11.67\n2\n2\n41.74\n10.19\n4\n1\n36.07\n7.85\n4\n2\n32.60\n6.37\n6\n1\n29.02\n5.10\n6\n2\n27.28\n4.81\n8\n1\n30.15\n5.65\n8\n2\n27.71\n4.93\nTable 4. Evaluation of the number of viewpoints in the coarse\nstage. The viewpoints are not the more the better, as the pretrained\n2D image diffusion model may involve illumination artifacts.\ning is performed within the UV plane, without occlusion\nproblems. As depicted in Figure 9, UVHD demonstrates\nits capability to enhance exsiting texture details and even\ngenerate new textures on monochromatic areas.\nEvaluation of the Number of Viewpoints. The selec-\ntion of viewpoints has shown a significant influence on the\ntexture generation result in the coarse stage [5]. We con-\nduct ablation studies to analyze the impact of the number\nof viewpoints on both overall coarse texture generation and\nsingle diffusion process. As shown in Tab. 4, we can see\nthat increasing the number of viewpoints can improve the\nquality of generated textures, but it is not that the more the\nviewpoints the better the results. We achieve the best result\nwhen the viewpoint is set to 6. The result is further im-\nproved when we sample two texture images from a pair of\nsymmetric viewpoints during a single diffusion progress.\n5. Disscusion\nThis paper presents Paint3D, a novel coarse-to-fine gener-\native framework that is capable of generating high-quality\n2K UV textures that maintain semantic consistency while\nbeing lighting-less, conditioned on text or image inputs. To\nachieve this, our method first leverages a pre-trained depth-\naware 2D diffusion model to generate view-conditional im-\nages and perform multi-view texture fusion, producing an\ninitial coarse texture map. Subsequently, we train distinct\nUV Inpainting and UVHD diffusion models, specifically\ndesigned for shape-aware refinement of incomplete areas\nand the removal of illumination artifacts.\nThrough this\ncoarse-to-fine process, Paint3D can produce high-quality,\nlighting-less, and diverse texture maps, significantly ad-\nvancing the state-of-the-art in texturing 3D objects.\nOur method has inherent limitations as follows.\nOur\napproach still suffers from the multi-faces problem in the\ncoarse stage which will result in a failure case. This issue\nprimarily arises from the inconsistency of multi-view tex-\nture images sampled by the pre-trained 2D diffusion model,\nas it is not explicitly trained on multi-view datasets. It re-\nmains a challenge for Paint3D to generate material maps,\nwhich are commonly used in modern physically based ren-\ndering pipelines. Furthermore, unlike optimization-based\n3D generation methods [7, 31, 36, 62], Paint3D is not capa-\nble to generate or edit the geometry of 3D assets.\nReferences\n[1] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton. Demystifying MMD gans. In 6th In-\nternational Conference on Learning Representations, ICLR\n2018. 5, 6, 7\n[2] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai.\nMesh2tex: Generating mesh textures from image queries. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 8918\u20138928, October 2023.\n2, 3, 4, 6\n[3] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,\nand Kangxue Yin. Texfusion: Synthesizing 3d textures with\ntext-guided image diffusion models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4169\u20134181, 2023. 3\n[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 15\n[5] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner. Text2tex: Text-driven tex-\nture synthesis via diffusion models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 18558\u201318568, October 2023. 2, 3, 4, 5, 6,\n8\n[6] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang.\nShaddr: Real-time example-based geometry and texture gen-\neration via 3d shape detailization and differentiable render-\ning. arXiv preprint arXiv:2306.04889, 2023. 3\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3, 8\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 2\n[9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n3\n[10] Zhiqin Chen, Kangxue Yin, and Sanja Fidler.\nAuv-net:\nLearning aligned uv maps for texture transfer and synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1465\u20131474, 2022. 3\n[11] An-Chieh Cheng, Xueting Li, Sifei Liu, and Xiaolong Wang.\nTuvf:\nLearning generalizable texture uv radiance fields.\narXiv preprint arXiv:2305.03040, 2023. 3\n[12] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-\nwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas\nF Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al.\nAbo: Dataset and benchmarks for real-world 3d object un-\nderstanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 21126\u2013\n21136, 2022. 3\n[13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 3, 5\n[14] Clement Fuji Tsang,\nMaria Shugrina,\nJean Francois\nLafleche, Towaki Takikawa, Jiehan Wang, Charles Loop,\nWenzheng Chen, Krishna Murthy Jatavallabhula, Edward\nSmith, Artem Rozantsev, Or Perel, Tianchang Shen, Jun\nGao, Sanja Fidler, Gavriel State, Jason Gorski, Tommy Xi-\nang, Jianing Li, Michael Li, and Rev Lebaredian. Kaolin:\nA pytorch library for accelerating 3d deep learning re-\nsearch. https://github.com/NVIDIAGameWorks/\nkaolin, 2022. 5\n[15] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 3\n[16] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 3\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 5, 6, 7\n[18] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022. 3\n[19] Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu,\nChiyu Jiang, Leonidas J Guibas, Matthias Nie\u00dfner, Thomas\nFunkhouser, et al.\nAdversarial texture optimization from\nrgb-d scans. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1559\u2013\n1568, 2020. 2\n[20] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 3\n[21] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and\nDavid Novotny. Holofusion: Towards photo-realistic 3d gen-\nerative modeling. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 22976\u201322985,\n2023. 2\n[22] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. Advances in Neural Infor-\nmation Processing Systems, 34:852\u2013863, 2021. 3\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019.\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 3\n[25] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver\nDeussen, Dani Lischinski, and Tien-Tsin Wong. Solid tex-\nture synthesis from 2d exemplars. In ACM SIGGRAPH 2007\npapers, pages 2\u2013es. 2007. 2\n[26] Sylvain Lefebvre and Hugues Hoppe.\nAppearance-space\ntexture synthesis. ACM Transactions on Graphics (TOG),\n25(3):541\u2013548, 2006. 2\n[27] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. Advances in Neural Information Processing Sys-\ntems, 35:30923\u201330936, 2022. 3\n[28] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n2\n[29] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi\nZhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-\ndriven 3d editing via focal-fusion assembly. arXiv preprint\narXiv:2308.10608, 2023. 2\n[30] Yuchen Li, Ujjwal Upadhyay, Habib Slim, Ahmed Abdelre-\nheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, and\nMohamed Elhoseiny. 3d compat: Composition of materials\non parts of 3d things. In European Conference on Computer\nVision, pages 110\u2013127. Springer, 2022. 3\n[31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 3, 8\n[32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 1, 2\n[33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 2\n[34] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 3\n[35] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei\nWang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji.\nX-mesh: Towards fast and accurate text-driven 3d styliza-\ntion via dynamic textual guidance.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2749\u20132760, 2023. 3\n[36] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 3, 5, 6, 8\n[37] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492\u2013\n13502, 2022. 3\n[38] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 3\n[39] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3\n[40] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo\nStrauss, and Andreas Geiger. Texture fields: Learning tex-\nture representations in function space. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4531\u20134540, 2019. 3\n[41] Zijie Pan, Jiachen Lu, Xiatian Zhu, and Li Zhang. Enhanc-\ning high-resolution 3d generation through pixel-wise gradi-\nent clipping. arXiv preprint arXiv:2310.12474, 2023. 2\n[42] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 5\n[43] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1\n[44] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 1, 3\n[45] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 2\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from nat-\nural language supervision. In ICML, 2021. 3\n[47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learn-\ning Research, 21(140):1\u201367, 2020. 1\n[48] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2\n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022. 1, 2\n[50] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. In Erik Brunvand, Alla Sheffer, and Michael Wim-\nmer, editors, ACM SIGGRAPH 2023 Conference Proceed-\nings, SIGGRAPH 2023, Los Angeles, CA, USA, August 6-10,\n2023, pages 54:1\u201354:11. ACM, 2023. 2, 3, 4, 5, 6, 7\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2, 4, 5, 7\n[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 2\n[53] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2\n[54] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,\nMatthias Nie\u00dfner, and Angela Dai. Texturify: Generating\ntextures on 3d shape surfaces. In European Conference on\nComputer Vision, pages 72\u201388. Springer, 2022. 3, 6\n[55] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023. 3\n[56] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3\n[57] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2\n[58] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\narXiv preprint arXiv:2307.01097, 2023. 2\n[59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 1\n[60] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 3\n[61] Greg Turk. Texture synthesis on surfaces. In Proceedings of\nthe 28th annual conference on Computer graphics and inter-\nactive techniques, pages 347\u2013354, 2001. 2\n[62] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 1, 3, 8\n[63] Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk.\nState of the art in example-based texture synthesis. Euro-\ngraphics 2009, State of the Art Report, EG-STAR, pages 93\u2013\n117, 2009. 2\n[64] Li-Yi Wei and Marc Levoy. Texture synthesis over arbitrary\nmanifold surfaces. In Proceedings of the 28th annual con-\nference on Computer graphics and interactive techniques,\npages 355\u2013360, 2001. 2\n[65] Bangbang Yang, Wenqi Dong, Lin Ma, Wenbo Hu, Xiao Liu,\nZhaopeng Cui, and Yuewen Ma.\nDreamspace: Dreaming\nyour room space with text-driven panoramic texture propa-\ngation. arXiv preprint arXiv:2310.13119, 2023. 2\n[66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 5, 7\n[67] Jonathan\nYoung.\nxatlas,\n2018.\nhttps://github.com/jpcy/xatlas. 5\n[68] Rui Yu, Yue Dong, Pieter Peers, and Xin Tong. Learning tex-\nture generators for 3d shape collections from internet photo\nsets. In British Machine Vision Conference, 2021. 3\n[69] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and\nXiaojuan Qi. Texture generation on 3d meshes with point-\nuv diffusion. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 4206\u20134216, 2023. 2,\n3, 4, 6, 14, 15\n[70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2\n[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 3836\u20133847, October 2023.\n4, 5\n[72] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-\nlez, and Ion Stoica. Judging llm-as-a-judge with mt-bench\nand chatbot arena, 2023. 1\n[73] Qian-Yi Zhou and Vladlen Koltun. Color map optimization\nfor 3d reconstruction with consumer depth cameras. ACM\nTransactions on Graphics (ToG), 33(4):1\u201310, 2014. 2\n[74] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. arXiv preprint arXiv:2306.13455, 2023. 2\nAppendix\nThis appendix provides more qualitative results (Sec. A), several additional experiments (Sec. B), and discussion on the\nfailure cases of our proposed texture generation approach (Sec. C).\nA. Qualitative Results\nLighting-less texture\nTextured meshes illuminated from various light source directions\nFigure 10. Lighting-less texture maps generated by Paint3D. These lighting-less textures produce appropriate shadows when the textured\nmeshes are illuminated from different directions of light sources.\n\u201cJim's pistol, disney\u201d under various seeds\n\u201ca dragon with wings and claws\u201d under various seeds\n\u201cclothing, Chinese landscape style\u201d under various seeds\n\u201ctoy plane, airplane\u201d under various seeds\nFigure 11. More samples from our best model for text-to-texture generation. Samples are generated with text prompts of the test set under\nvarious seeds. We recommend the supplemental video to see more results.\n\u201cknife CSGO, Sci-Fi digital painting\u201d\n\u201cspray can\u201d\n\u201clow poly space shuttle\u201d\n\u201cwasteland robot, tracks\u201d\n\u201ca dog, Sci-Fi digital painting\u201d\n\u201cbinocular microscope\u201d\n\u201ca purple clothing\u201d\n\u201csamurai helmet, metal\u201d\nFigure 12. Additional texturing results generated by Paint3D on text-to-texture task. Each textured mesh is shown from three viewpoints.\nInput image\nInput mesh\nTextured meshes\nInput image\nInput mesh\nTextured meshes\nFigure 13. Additional samples from Paint3D for image-to-texture generation and each textured mesh is shown from two viewpoints. The\ninput image conditions are collected in the wild. We recommend the supplemental video to see more results.\nB. Additional Experiments\nWe first study the effectiveness of the position map in the UV Inpaint and UVHD modules. Then, we provide more compar-\nisons with category-specific texture generation approaches [69].\nB.1. Evaluation of Position Map\nTo demonstrate the effectiveness of position map in two texture refinement modules, UV inpainting and UVHD, we further\nconduct experiments on two baselines \u201cUV inpainting w/o position map\u201d and \u201cUVHD w/o position map\u201d. The \u201cUV in-\npainting w/o position map\u201d configuration refers to inpainting the uncolored area without the guidance of the position map\nThe \u201cUVHD w/o position map\u201d configuration represents the result of enhancing the texture map in UV space, without the\nposition map. As indicated in Tab. 5, the performance shows a significant decrease when the position map is not utilized in\nUV inpainting or UVHD, indicating its irreplaceable function during texture refinement processing. We visualize the results\nof two baselines in Fig. 14 and Fig. 15. In both scenarios, the model produces inferior results compared to our full model.\nInput texture map\nUV Inpainting without position map\nUV Inpainting with position map\nFigure 14. Visualization of the effect of the position map in the UV inpainting module. Without the position map, the inpainted texture is\nsemantically confused. The purple area indicates the uncolored area.\nInput texture map\nUVHD without position map\nUVHD with position map\nFigure 15. Visualization of the effect of the position map in the UVHD module. In the absence of the position map, the enhanced texture\nappears distorted (top) or lacks semantic coherence (bottom).\nMethod\nFID\u2193\nKID \u2193\nUV inpainting w/o position map\n39.29\n8.36\nUVHD w/o position map\n37.62\n7.96\nFull model\n27.28\n4.81\nTable 5. Evaluation of the effectiveness of the position map in the UV Inpaint and UVHD modules. This demonstrates the crucial role of\nthe position map during the diffusion process in UV space.\nB.2. Comparisons with Category-Specific Model\nIn addition, we conduct comparison experiments with a category-specific approach on the chair and table categories of\nShapeNet [4]. We choose Point-UV [69] as the baseline because 1) it represents the current state-of-the-art for category-\nspecific texture generation, and 2) it has the conditional texture generation capability under both text and image conditions.\nFor the input conditions, we utilize text and images as provided in [69]. As shown in Fig. 16, Paint3D achieves comparable\nresults with Point-UV under both text and image conditions.\n\u201ca coffee table \nmade of \nconcrete bricks\u201d\n\u201can aluminium\nlounge chair \nwith purple \nfabric\u201d\nInput text\nPoint-UV\nOurs\nInput image\nPoint-UV\nOurs\nFigure 16. Qualitative comparisons on texture generation conditioned under text prompt (left) and image condition (right) on ShapeNet\ndataset [4]. We compare our textured mesh against those generated by the state-of-the-art category-specific approach, Point-UV [69]. In\nthe categories of table and chair, Paint3D achieves comparable results with Point-UV under both text and image conditions.\nC. Discussion on failure case\nOur approach still suffers from the multi-faces problem in the coarse stage which will result in a failure case. This issue\nprimarily arises from the inconsistency of multi-view texture images sampled by the pre-trained 2D diffusion model, as it\nis not explicitly trained on multi-view datasets. We believe that fine-tuning or retraining 2D diffusion models on large-scale\nmulti-view datasets will improve the multi-view consistency of textures.\n\u201ca gray mouse\u201d\n\u201cT-shirt with a lion in the front\u201d\nFigure 17. Visualization of our failure cases. Paint3D still suffers from the multi-faces problem in the coarse stage which will result in a\nfailure case. Here, Paint3D generates duplicate mouse or lion faces in both the front and back views\n"
  },
  {
    "title": "Time is Encoded in the Weights of Finetuned Language Models",
    "link": "https://arxiv.org/pdf/2312.13401.pdf",
    "upvote": "18",
    "text": "Time is Encoded in the Weights of Finetuned Language Models\nKai Nylund1\nSuchin Gururangan1\nNoah A. Smith1,2\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Allen Institute for AI\nknylund@cs.washington.edu\nAbstract\nWe present time vectors, a simple tool to cus-\ntomize language models to new time periods.\nTime vectors are created by finetuning a lan-\nguage model on data from a single time (e.g.,\na year or month), and then subtracting the\nweights of the original pretrained model. This\nvector specifies a direction in weight space\nthat, as our experiments show, improves perfor-\nmance on text from that time period. Time vec-\ntors specialized to adjacent time periods appear\nto be positioned closer together in a manifold.\nUsing this structure, we interpolate between\ntime vectors to induce new models that perform\nbetter on intervening and future time periods,\nwithout any additional training. We demon-\nstrate the consistency of our findings across\ndifferent tasks, domains, model sizes, and time\nscales. Our results suggest that time is encoded\nin the weight space of finetuned models.\n1\nIntroduction\nTemporal variation is a fundamental characteristic\nof language. As we show in \u00a73, it manifests in\nlanguage model development as temporal misalign-\nment, where deviations in train and test data lead\nto large performance degradation across different\ntime periods (Luu et al., 2022; Lazaridou et al.,\n2021; Jaidka et al., 2018, inter alia). This necessi-\ntates adaptation techniques for customizing models\nto specific time periods as needed. Designing such\ntechniques is difficult, however, due to the mul-\ntitude of time scales and the possibility that data\nfrom a target time period might be unavailable.\nRecent work has shown that the behavior of neu-\nral networks can be edited through closed-form\ninterpolation between parameters of finetuned mod-\nels (Ilharco et al., 2023; Ortiz-Jim\u00e9nez et al., 2023;\nLi et al., 2022; Wortsman et al., 2021, inter alia).\nIn this work, we demonstrate that weight-space\ninterpolation can also be used to cheaply edit lan-\nguage model behavior over time. To this end, we\nintroduce time vectors (\u00a74), an extension of task\nFigure 1: We present time vectors, a simple tool to cus-\ntomize language models to new time periods. Time\nvectors (\u03c4i) specify a direction in weight space that im-\nproves performance on text from a time period i. They\nare computed by subtracting the pretrained weights (\u03b8pre;\nleft panel) from those finetuned to a target time period\n(\u03b8i). We can customize model behavior to new time\nperiods (e.g., intervening months or years) by interpo-\nlating between time vectors and adding the result to the\npretrained model (middle panel). We can also gener-\nalize to a future time period j with analogy arithmetic\n(right panel). This involves combining a task-specific\ntime vector with analogous time vectors derived from\nfinetuned language models (\u03c4 LM\nj\n).\nvectors (Ilharco et al., 2023). We finetune a pre-\ntrained language model on text from a single time\nperiod, and then subtract the pretrained weights.\nThis vector represents a direction of movement in\nweight space that improves performance on text\nfrom the target time period.\nWe analyze the structure of time vectors with\ntemporally organized datasets for language mod-\neling, classification, and summarization (\u00a72). Our\nresults consistently suggest that time vectors are in-\ntuitively organized on a manifold; years or months\nthat are closer together in time yield time vectors\nthat are also closer together in weight space. Simi-\nlarly, we show that temporal degradation in yearly\nand monthly settings is strongly correlated with the\nangles between time vectors (\u00a74.2).\nWe use this structure of time vectors to induce\narXiv:2312.13401v2  [cs.CL]  30 Dec 2023\nFigure 2: Model performance degrades linearly year-to-year. We evaluate language model perplexity (WMT),\nROUGE-L (news summarization), and macro F1 (political affiliation classification). Each cell indicates the monthly\nperformance of T5-3B finetuned and evaluated on a single year from that task. We report the percentage difference\nfrom the average performance for each year, and find linear degradation as finetuning and evaluation years become\nmore misaligned regardless of task. We display similar trends for T5-small and medium, as well as for other domains\nand tasks, in \u00a7A.1. We measure the linearity of these degradations in Appendix Table 4.\nmodels that generalize better to data from new time\nperiods. By interpolating between two time vectors,\nwe discover vectors that, when applied to the pre-\ntrained model, improve performance on intervening\nmonths or years (\u00a74.3). The structure can also be\nused to generalize task-specific models across time\nperiods with analogous time vectors specialized to\nunlabeled data (\u00a74.4).\nOur results show that temporal variation is to\nsome extent encoded in the weight space of fine-\ntuned models, and that weight interpolation can\nhelp customize language models to new time peri-\nods. We publicly release our code, data, and over\n500 models finetuned on specific time periods.1\n2\nData and Finetuning\nIn this section, we describe our datasets and fine-\ntuning techniques, which serve as the basis for all\nsubsequent experiments. We finetune language\nmodels on multiple time-stratified datasets, which\nwe use to analyze temporal misalignment and build\ntime vectors. Then, we explore different ways of\ninterpolating between time vectors to generalize\nto new times. See \u00a74.3-4.5 for more details on\ninterpolation strategies.\n2.1\nDatasets\nLanguage Modeling\nWe create two new time-\nspecific language modeling datasets from unla-\nbeled text in news and Twitter domains. For these\n1https://github.com/KaiNylund/\nlm-weights-encode-time\ndatasets, we measure perplexity of the model on\nthe test set:\n\u2022 WMT Language Modeling:\nWe randomly\nsample 67K \u00b1 5K articles (47M BPE tokens)\nof training articles and 3K \u00b1 0.3K test articles\n(2.3\u20132.4M tokens of) from each year 2012\u2013\n2021 in the English subset of the WMT news\ndataset (Barrault et al., 2021), from 2012\u2013\n2016. From the same time range, we also\nsample 7.1M tokens of training articles and\n700\u2013720K tokens of test articles from each\nmonth. We are missing WMT train and test\nsplits for August 2012 and May 2016.\n\u2022 Twitter Language Modeling:\nWe randomly\nsample 2M \u00b1 105K training tweets (72\u201378M\ntokens BPE tokens) and 100K \u00b1 5.4K test\ntweets (3.6-3.9M BPE tokens) from each year\nin the Internet Archive Twitter Stream Grab ,2\nfrom 2015\u20132020. We only use this dataset to\nstudy the domain-specificity of time vectors\nin \u00a74.4.\nTo understand the level of contamination in our\ndatasets, we measure the overlap between yearly\ntrain and test splits in both tasks using a Bloom fil-\nter.3 We find that less than two percent and 0.1 per-\ncent of examples in the Twitter and WMT LM test\nsets, respectively, contain contaminated n-grams.\n2https://archive.org/details/\ntwitterstream\n3https://github.com/allenai/bff\nDownstream Tasks\nFor downstream tasks, we\ndraw from Luu et al. (2022). We measure each\nmodel\u2019s performance on the test set in ROUGE-L\nfor NewsSum and macro F1 for PoliAff.\n\u2022 NewsSum: We use Luu et al. (2022) postpro-\ncessing of Grusky et al. (2018) news summa-\nrization task. To align with out WMT dataset,\nwe do not bin adjacent years together, creating\nuniformly sized splits for each year from 2012\nto 2016.\n\u2022 PoliAff: We use the Political Affiliation task\nfrom Luu et al. (2022), with uniformly sized\ndatasets for each year from 2015 to 2020.\n2.2\nFinetuning\nTo compare the same weight space across tasks,\nwe use pretrained T5 (Raffel et al., 2023) check-\npoints for all our experiments. We finetune T5-\nsmall, T5-large, and T5-3b on each of our time-\nstratified datsets. For language modeling, we use\nthe \u201cLM adaptation\u201d objective (Lester et al., 2021).\nTo reduce the computational burden, we fine-\ntune T5-large and T5-3B with Low-Rank Adap-\ntation (LoRA; Hu et al., 2021) and default hy-\nperparameters (q and v attention target modules,\nr = 8, \u03b1 = 32, dropout = 0.1). When creating\ntime vectors, we merge LoRA weights back into\nthe base model before subtracting the pretrained\nmodel.\nAcross all settings, we use a batch size of 2 with\n8 gradient accumulation steps. We finetune for\na single epoch on LM splits and three epochs on\ndownstream task splits. Our learning rates across\nall tasks are 8 \u00d7 10\u22124 for T5-small and T5-large,\nand 2 \u00d7 10\u22124 for T5-3b. We finetuned models con-\ncurrently with a single GPU each; we used 8 2080ti,\n4 Titan, and 8 A40 GPUs. In experiments for sec-\ntions \u00a74.4 and \u00a74.5, we ran evaluations in parallel\nusing available Titan, A40, and A100 GPUs.\n3\nRevealing Temporal Misalignment at\nMultiple Time Scales\nWe begin with an analysis of temporal misalign-\nment using the new set of models and tasks that we\nconsider in this work (\u00a72). These findings set the\nstage for our creation of time vectors in \u00a74.\n3.1\nYearly Degradation is Linear\nPrevious work on temporal misalignment shows\nthat models degrade over time on a yearly basis.\nFigure 3: Monthly temporal degradation has sea-\nsonal patterns. Each cell indicates the monthly perfor-\nmance of T5-small finetuned and evaluated on a single\nmonth of the WMT dataset. We report the percentage\ndifference in test perplexity from the average on the eval-\nuation month over all finetuned T5-small models (darker\nis better). The diagonal indicates that each model does\nbest on its finetuning month. Models also do relatively\nbetter on the same month in other years, visible as the\nstripes radiating out from the diagonal every 12 months.\nTo confirm these results, we finetune T5-small,\nT5-large, and T5-3b on each yearly split from ev-\nery dataset. We then evaluate each of these year-\nfinetuned models on every other time split of the\ntest data.\nWe display heatmaps of temporal misalignment\nat a yearly scale in Figure 2. We report percent\nperplexity change from the average on each year to\navoid inherent year performance differences. Con-\nsistent with past work (Lazaridou et al., 2021; Luu\net al., 2022; Longpre et al., 2023), we observe linear\npatterns of degradation in each task for all model\nsizes (see Table 4 in the Appendix for more details).\nLike Luu et al. (2022) show, some tasks, like politi-\ncal affiliation classification, exhibit clearer degra-\ndation than others. We quantify these variations in\n\u00a7A.2.\n3.2\nMonthly Degradation is Seasonal\nNext, we turn to month-by-month temporal mis-\nalignment, which, to the best of our knowledge,\nis unexplored. We train T5-small on each WMT\nLM month split from 2012\u20132016, resulting in 58\nmonth-finetuned models. We then test every 2012\u2013\n2016 month model on each month test split for a\ntotal of 3,364 evaluations.\nAs seen in Figure 3, finetuning and evaluating\nmodels on specific months in the WMT dataset re-\nFigure 4: Time vectors are organized in a mani-\nfold that reflects temporal variation. Each point is a\nUMAP projection of the last feedforward layer of a T5-\nsmall time vector finetuned on single month of WMT.\nPoints and edges between adjacent months are colored\nby year. Distances between the weights of time vectors\ncorrelate with temporal misalignment (\u00a74.2).\nveals non-linear patterns in temporal misalignment,\nwhich correspond to the cycle of months in each\nyear. This pattern is captured by the stripes that oc-\ncur parallel to the diagonal every 12 months, which\nindicate that the model for a particular month tends\nto do better on the same month in other years. We\nquantify these differences in perplexity in appendix\nFigure 12. We also report degradation patterns in\nonline training settings in \u00a7A.4.\n3.3\nSummary\nWe measure temporal misalignment across a va-\nriety of domains, tasks and time scales. While\nperformance decays linearly on a yearly scale, we\ndiscover seasonal trends in month-to-month mis-\nalignment. Next, we analyze how these phenomena\nrelate to the weights of time-specific models, and\nthen use that relationship to present techniques for\nadapting LMs to new times.\n4\nTemporal Adaptation with Time\nVectors\nThe collection of year and month-finetuned mod-\nels from \u00a73 presents a new source of data to study\ntemporal misalignment: model weights. In this\nsection, we analyze these weights through the lens\nof time vectors, formed by taking the difference of\na model finetuned on a specific time and the pre-\ntrained model. First, we show that the weights of\ntwo time vectors become less similar as the times\nthey were finetuned on become more misaligned\n(\u00a74.2). Then, we attempt to use the reverse relation-\nPearson r\nT5 size\nWMT LM\nNewsSum\nPoliAff\nsmall\n-0.867\n0.663\n0.654\nlarge\n-0.737\n0.628\n0.672\n3b\n-0.795\n0.626\n0.668\nTable 1: The similarity between time vectors corre-\nlates with temporal degradation. Pearson correlation\nbetween cosine similarity of yearly time vectors and %\ndegradation from the mean performance of all yearly\nmodels on each evaluation time period. All p-values are\n< 8 \u00d7 10\u22124.\nship to update models to unseen times: reducing\nmisalignment on intervening (\u00a74.3), future (\u00a74.4),\nand multiple time periods (\u00a74.5) by interpolating\ntime vectors.\n4.1\nBackground and Definition\nTask vectors (Ilharco et al., 2023) are the differ-\nence of the weights of a pretrained model from\nthe weights of the same model after finetuning on\na task. Adding and subtracting task vectors from\nfinetuned models is a simple and effective way to\nimprove performance on other settings, or reduce\nunwanted behavior without further training. Like\nword embeddings, if there are tasks with the anal-\nogous relationship \u201cA is to B as C is to D,\u201d then\ntask vectors can be used to improve performance\non D with the approximation D \u2248 C + (B \u2212 A).\nTime vectors are an extension of task vectors\nto the time domain. Given the weights of the pre-\ntrained model, \u03b8pre and those of the model fine-\ntuned on data from only a single time period t, \u03b8t,\na time vector \u03c4t = \u03b8t \u2212 \u03b8pre . Like their task-based\ncounterparts, we add back the pretrained weights at\ninference time and evaluate \u03b8pre + \u03c4t (Ilharco et al.,\n2023). We call time vectors from models finetuned\non individual years and months \u201cyear-vectors\u201d and\n\u201cmonth-vectors.\u201d\n4.2\nCorrelation of Time Vector Similarity and\nTemporal Degradation\nWe visualize time vectors with a UMAP in Figure\n4, which suggests that time vectors closer together\nin weight space are also closer together in time.\nTo verify this hypothesis, we measure the cosine\nsimilarity between model weights from each pair\nof time vectors trained on different time periods\n(visualized in \u00a7A.1).\nWe find that this similarity metric and perfor-\nmance (Figure 11) decay similarly over time. Table\n1 shows that the correlation between cosine simi-\nlarity and relative performance change on different\nyears is highest in WMT language modeling. Cor-\nrelations are generally similar across T5 sizes, with\na higher score for T5-small in the WMT LM setting\nthan T5-large and T5-3b, and no absolute values\nless than 0.6.\nThis relationship also extends to the monthly\nscale. Seasonal stripes are visible in the cosine sim-\nilarities between each pair of monthly WMT time\nvectors (visualized in Appendix Figure 9). The\nmonthly performance degradation from the mean\n(Figure 3) and cosine similarity matrices (Figure 9)\nhave a negative correlation (Pearson r = \u22120.667;\np < 10\u221216). We analyze cosine similarities to\nsingle-year time vectors throughout online training\nin Appendix \u00a7A.5.\nThese results indicate that time vectors are or-\nganized in way that is predictive of their perfor-\nmance on corresponding time periods. Next, we\nexplore how we can use this structure to improve\nperformance on new time periods by interpolating\nbetween time vectors.\n4.3\nGeneralizing to Intervening Time Periods\nArchiving issues or a low sampling rate can lead\nto gaps in datasets between the oldest and newest\nexamples. Without data, we expect models to per-\nform worse on these \u201cgap\u201d times due to temporal\nmisalignment. In this section, we find that we can\ngeneralize better to these intervening time periods\nby interpolating between models finetuned on the\noldest and newest times.\nMethod\nFor two time vectors \u03c4j, \u03c4k, we compute\ntheir interpolation \u03b1 \u00b7 \u03c4j + (1 \u2212 \u03b1) \u00b7 \u03c4k with \u03b1 \u2208\n[0, 1]. In this section, we interpolate between the\nearliest year time vector \u03c40 and latest year time\nvector \u03c4n and evaluate on times t0, ..., tn for each\n\u03b1 \u2208 [0.1, 0.2, ..., 1.0].\nResults\nFigure 5 shows that interpolating be-\ntween start and end-year finetuned models im-\nproves performance on intervening years in both\nWMT LM and PoliAff tasks. Improvement is gen-\nerally greatest on the exact middle years (2014\nfor WMT LM, 2017 for PoliAff) and decreases\non years closer to start and end times. Patterns of\nimprovement also vary depending on setting, with\nflatter changes in performance near \u03b1 = 1.0, 0.0\nin PoliAff compared to WMT LM, and minimal\nimprovements in NewsSum across \u03b1s compared\nPerplexity (\u2193)\nRouge (\u2191)\nF1 (\u2191)\nMethod\nWMT LM\nNewsSum\nPoliAff\nStart-year finetuned (\u03c40)\n13.92\n38.56\n0.6886\nEnd-year finetuned (\u03c4n)\n13.84\n35.09\n0.6967\n1\n2(\u03c40 + \u03c4n)\n13.77\n38.86\n0.7765\nBest interpolations\n13.75\n40.11\n0.7941\nEval-year finetuned (\u03c4i)\n13.65\n42.36\n0.8341\nTable 2: Interpolation between start and end-year\nfinetuned models reduces temporal misalignment\non intervening years. T5-3b average performance on\neach year between start and end (non-inclusive). \u201cBest\ninterpolations\" use the best performing \u03b1 values for\neach year.\nto the difference in performance between evalua-\ntion years. Table 2 quantifies these changes, show-\ning that interpolation closes the gap on intervening\nyears between temporally aligned and misaligned\nmodels. Improvements are particularly large for\nPoliAff, nearly eight macro-F1 points just by aver-\naging the start and end-year time vectors.\nFigure 6 shows that these results extend to the\nmonthly scale for WMT LM; we can interpolate\nbetween time vectors finetuned on January and\nDecember in a year to improve performance on\nthe months between them. The best interpolations\nfor each month follow an intuitive pattern, with\na higher percentage of the January model leading\nto better performance on earlier months and vice\nversa.\n4.4\nGeneralizing to the Future\nThe creation of labeled datasets lags behind corpera\nof raw text, which can be scraped automatically. As\na result, language models that rely on supervision\nfor finetuning are quickly outdated. Updating these\nmodels can be expensive, involving extra finetuning\nand creating labeled datasets from more recent ex-\namples. In this section, we present a new technique\nfor updating task models finetuned on a source time\nperiod j to a target time period k with only unla-\nbeled data from j, using task analogies (Ilharco\net al., 2023).\nMethod\nGiven language models with weights\n\u03b8LM\nj\n, \u03b8LM\nk\nfinetuned on unlabeled text from times\nj, k, and a task-specific model with weights \u03b8j fine-\ntuned on labeled data from time j, we perform the\nFigure 5: Interpolating between two year vectors improves performance on the years between them. These\nperformance improvements follow an intuitive structure, e.g. when interpolating between 2012 and 2016, the best\nresult on 2013 occurs with a higher percentage of 2012 and vice versa for 2015. Improvement from interpolation\nvaries across settings.\nFigure 6: Interpolating between two month vectors improves performance on the months between them.\nWe interpolate between January and December month vectors and evaluate on all other months within the same\nfinetuning year. Like at the yearly scale, early months do better with a higher percentage of the January model\nand vice versa while middle months do best with a 50% split between the models. The stars in the upper plots\ncorrespond to the best performing interpolations for each evaluation month; these optimums are mirrored in the\nlower line plots.\nfollowing arithmetic on the vectors:\n\u03c4j = \u03b8j \u2212 \u03b8pre\n\u03c4 LM\nj\n= \u03b8LM\nj\n\u2212 \u03b8pre\n\u03c4 LM\nk\n= \u03b8LM\nk\n\u2212 \u03b8pre\n\u03c4k \u2248 \u03b11 \u00b7 \u03c4j + (\u03b12 \u00b7 \u03c4 LM\nk\n\u2212 \u03b13 \u00b7 \u03c4 LM\nj\n)\n\u03b8k = \u03c4k + \u03b8pre\nWe evaluate our estimated \u03b8k on each target\ntime tk, sweeping over all combinations of \u03b11 \u2208\n[0.6, 0.8, . . . 2.2], \u03b12, \u03b13 \u2208 [0.1, . . . 0.6] and re-\nporting the best result compared to the original\nmodel \u03b8j. In this section, we update a 2012 News-\nSum model to 2013\u20132016, and a 2015 PoliAff\nmodel to 2016\u20132020 using WMT LM and Twit-\nter LM time vectors respectively.\nResults\nTask analogies improve performance on\nfuture years in both PoliAff and NewsSum tasks.\nFigure 7 shows that improvement compared to fine-\ntuning on the start year increases as the target and\nstart years become more misaligned. Model size\nalso affects performance, with T5-large and T5-3b\nshowing greater improvements. In PoliAff, T5-\nFigure 7: Task analogies can offset downstream tem-\nporal misalignment without labeled data from the\ntarget time. We report the performance of NewsSum\nand PoliAff T5 models updated using WMT LM and\nTwitter LM vectors for each target evaluation time. We\nreport the percent improvement of the best updated\nmodel over 2012 NewsSum and 2015 PoliAff models\non each target time for all model sizes.\nsmall has no improvement over the baseline and\nT5-large task analogies perform worse than the\nbaseline on 2016 and 2017 before improving on\n2019 and 2020. Strangely, we find that only scal-\ning \u03b11 can also improve performance on future\nyears. We report these \u03b1 ablations and our results\non two other classification tasks in Appendix \u00a7A.6.\nWe observe mostly similar results on these tasks,\nalthough there are task-specific inconsistencies.\n4.5\nGeneralizing to Multiple Time Periods\nBecause interpolations prove useful for generaliz-\ning to intervening and future time periods, we next\ntest if we can build models that perform well on\nmultiple time periods by interpolating between all\ntime vectors for a task.\nMethod\nWe approach this problem with the\nmodel soup technique (Wortsman et al., 2022). One\nof the key practical advantages of soups is that con-\nstituent time-specific models can be trained inde-\npendently (on smaller compute budgets) and com-\nbined at any time. Furthermore, the multi-year\nmodel does not need to be retrained to include new\ntime periods; new time periods can be incorporated\nby merely growing the soup with additional fine-\ntuned models.\nWe attempt to create a multi-year model by\nfollowing the recipe outlined by Wortsman et al.\n(2022). They introduce two soup variants: the uni-\nform soup and greedy soup. The uniform soup\napplies a uniform weight among all constituent\nmodels in the interpolation, while the greedy soup\nis an iterative procedure that only includes models\nin the soup that improves validation performance.\nWe assess both variants here.\nOur \u201cuniform time soup\u201d is \u03b8pre +\n1\n|T|\nP\nt\u2208T \u03c4t\nwhere T is the set of all years for a given task. For\nour \u201cgreedy time soup,\u201d we implement a similar\nalgorithm to Wortsman et al. (2022) which samples\ntime vectors (with replacement) from each year in\norder of decreasing performance and adds them\nto the average model soup if they improve perfor-\nmance.\nTo evaluate our ability to build models that gen-\neralize to multiple time periods, we measure the\naverage performance across all evaluation years for\neach task. We compare our model soups against\ntwo baselines: 1) a model trained on all shuffled\navailable data at once and 2) the best-performing\nmodel finetuned on only a single year of data. The\nall-year model is the most compute-intensive ap-\nproach.\nResults\nOverwhelmingly, time soups perform\nworse than the model finetuned on all shuffled avail-\nable data. For WMT LM and NewsSum, the uni-\nform time soup performs worse than even the best\nsingle year model, despite having access to five\ntimes the amount of finetuning data. The greedy\ntime soup only improves over the best single-year\nmodel on PoliAff with a single macro F1 point gain.\nThese findings suggest that a model which general-\nizes to multiple time periods does not lie in a region\nof weight space bounded by models finetuned on\nsingle years of data. Future work may explore more\nsophisticated methods of merging which to induce\nbetter performing multi-year models.\n4.6\nSummary\nWe propose methods for updating models to in-\ntervening, future, and multiple time periods using\ntime vector arithmetic. We find that interpolating\nbetween two time vectors improves performance\non unseen intervening times at both yearly and\nmonthly scales. Similarly, we can improve per-\nformance on the future with unlabeled data from\ntarget times using time vector analogies. Building\na multi-year model with a \u201csoup\u201d of time vectors,\nPerplexity (\u2193)\nRouge (\u2191)\nF1 (\u2191)\nMethod\nWMT LM\nNewsSum\nPoliAff\nBest single-year model\n34.45\n38.95\n0.7101\nUniform time soup\n34.70\n33.05\n0.6078\nGreedy time soup\n34.45\n38.95\n0.7202\nTraining on all years\n29.17\n40.07\n0.7853\nTable 3: Interpolation does not enable generalization\nto multiple time periods simultaneously. Here, we\nmeasure the average performance of models on all years.\nWe compare multiple ways of building multi-year mod-\nels; T5-small models finetuned to individual years or all\nyears, and \u201ctime soups\u201d created by averaging together\nall year time vectors for a task.\nhowever, does not approach the performance of a\nmodel finetuned on all times at once. These results\nsuggest that task arithmetic can be a simple way to\nupdate models to new times, but it does not help to\nimprove genearlization across the board within a\nsingle model.\n5\nRelated Work\nSemantic Drift\nAlthough changes in the full\nweight spaces of models over time have not been\npreviously explored, semantic changes in word em-\nbeddings over time are well-documented (Hamil-\nton et al., 2016). Temporal misalignment (Bamler\nand Mandt, 2017; Gonen et al., 2021) and word\nanalogies over time (Szymanski, 2017) have also\nbeen studied in embeddings. Our work extends\nthese analyses to the full set of language model\nparameters.\nTemporal Misalignment\nThe phenomenon of\ntemporal misalignment in language models has\ngained attention in the last three years. Moving\nfrom semantic drift to model misalignment in the\ntwitter domain, Jaidka et al. (2018) measure gender\nand age classifier degradation over time, Rijhwani\nand Preo\u00b8tiuc-Pietro (2020) demonstrate the effect\nof temporal drift on named entity recognition, and\nLoureiro et al. (2022) find decay on a variety of\nlanguage modeling and downstream tasks. Lazari-\ndou et al. (2021) extend these analyses to language\nmodeling on News and Science domains and show\nthat increasing model size does not help mitigate\ntemporal misalignment. Luu et al. (2022) compare\ntemporal misalignment across a variety of down-\nstream tasks, finding that degradation varies greatly\nover both domain and task. Using the same suite\nof tasks, Longpre et al. (2023) report similar degra-\ndation over time in pretraining regardless of model\nsize.\nUpdating LMs\nRecent attempts at updating lan-\nguage models to new time periods have used a\nrange of techniques. Luu et al. (2022) find limited\nimprovement with continued pretraining (R\u00f6ttger\nand Pierrehumbert, 2021) on target times. Similar\nto the sequential updating setting, however, Lazari-\ndou et al. (2021) show that dynamic evaluation\n(Gururangan et al., 2020) can improve language\nmodeling performance on new times, but results in\nforgetting the past. More recent techniques have\nbeen proposed for keeping models up to date in\nthe QA domain by adding flags with the year for\neach example (Dhingra et al., 2022) or by discard-\ning outdated facts (Zhang and Choi, 2023). Unlike\nthese methods, we consider the problem of updat-\ning models to new time periods without data in the\ntarget time and without additional finetuning.\nInterpolation\nOur work draws heavily on recent\ntechniques for editing models directly with inter-\npolation and task analogies. Time vectors are an\napplication of task vectors (Ilharco et al., 2023)\nto the time domain, our interpolation experiments\nare inspired by previous work on patching mod-\nels for multiple tasks (Ilharco et al., 2022), and\nour time soups are an application of models soups\n(averaging multiple models trained with different\ninitializations) (Wortsman et al., 2022).\n6\nConclusion\nWe connect studies of temporal misalignment and\nweight arithmetic with time vectors, formed by\nfinetuning a model on a specific time period and\nthen subtracting its pretrained weights. We show\nthat the weights of time vectors are more similar if\ntheir corresponding times are closer and vice versa.\nThese similarities are highly correlated to temporal\nmisalignment at both yearly and monthly scales\n(which exhibit seasonal patterns). Leveraging this\ntemporal structure in weight space, we induce new\nmodels that perform better on intervening years by\ninterpolating between adjacent time vectors. Simi-\nlarly, we use task analogies to improve downstream\nperformance on future time periods using only un-\nlabeled data from those times. These results show\nthat task arithmetic can be a simple tool for com-\nbating temporal misalignment.\nReferences\nRobert Bamler and Stephan Mandt. 2017. Dynamic\nword embeddings. In International conference on\nMachine learning, pages 380\u2013389. PMLR.\nLoic Barrault, Ondrej Bojar, Fethi Bougares, Rajen\nChatterjee, Marta R. Costa-jussa, Christian Feder-\nmann, Mark Fishel, Alexander Fraser, Markus Fre-\nitag, Yvette Graham, Roman Grundkiewicz, Paco\nGuzman, Barry Haddow, Matthias Huck, Antonio Ji-\nmeno Yepes, Philipp Koehn, Tom Kocmi, Andre\nMartins, Makoto Morishita, and Christof Monz, edi-\ntors. 2021. Proceedings of the Sixth Conference on\nMachine Translation. Association for Computational\nLinguistics, Online.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics, 10:257\u2013\n273.\nHila Gonen, Ganesh Jawahar, Djam\u00e9 Seddah, and Yoav\nGoldberg. 2021. Simple, interpretable and stable\nmethod for detecting words with usage change across\ncorpora. arXiv preprint arXiv:2112.14330.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries\nwith diverse extractive strategies.\narXiv preprint\narXiv:1804.11283.\nSuchin\nGururangan,\nAna\nMarasovi\u00b4c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don\u2019t stop pretraining:\nAdapt language models to domains and tasks. ArXiv,\nabs/2004.10964.\nWilliam L Hamilton, Jure Leskovec, and Dan Juraf-\nsky. 2016. Diachronic word embeddings reveal sta-\ntistical laws of semantic change.\narXiv preprint\narXiv:1605.09096.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-\nman, Suchin Gururangan, Ludwig Schmidt, Han-\nnaneh Hajishirzi, and Ali Farhadi. 2023. Editing\nmodels with task arithmetic.\nGabriel Ilharco, Mitchell Wortsman, Samir Yitzhak\nGadre, Shuran Song, Hannaneh Hajishirzi, Simon\nKornblith, Ali Farhadi, and Ludwig Schmidt. 2022.\nPatching open-vocabulary models by interpolating\nweights. Advances in Neural Information Processing\nSystems, 35:29262\u201329277.\nKokil Jaidka, Niyati Chhaya, and Lyle Ungar. 2018. Di-\nachronic degradation of language models: Insights\nfrom social media. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 195\u2013200.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d\u2019Autume,\nTomas Kocisky, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization in\nneural language models.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A. Smith, and Luke Zettle-\nmoyer. 2022. Branch-train-merge: Embarrassingly\nparallel training of expert language models.\nShayne Longpre, Gregory Yauney, Emily Reif, Kather-\nine Lee, Adam Roberts, Barret Zoph, Denny Zhou,\nJason Wei, Kevin Robinson, David Mimno, et al.\n2023. A pretrainer\u2019s guide to training data: Measur-\ning the effects of data age, domain coverage, quality,\n& toxicity. arXiv preprint arXiv:2305.13169.\nDaniel Loureiro, Francesco Barbieri, Leonardo Neves,\nLuis Espinosa Anke, and Jose Camacho-Collados.\n2022. Timelms: Diachronic language models from\ntwitter. arXiv preprint arXiv:2202.03829.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. 2022. Time\nwaits for no one! analysis and challenges of tem-\nporal misalignment.\nIn Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5944\u20135958, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nGuillermo Ortiz-Jim\u00e9nez, Alessandro Favero, and Pas-\ncal Frossard. 2023.\nTask arithmetic in the tan-\ngent space: Improved editing of pre-trained models.\nArXiv, abs/2305.12827.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2023. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nShruti Rijhwani and Daniel Preo\u00b8tiuc-Pietro. 2020.\nTemporally-informed analysis of named entity recog-\nnition. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7605\u20137617.\nPaul R\u00f6ttger and Janet B Pierrehumbert. 2021. Tem-\nporal adaptation of bert and performance on down-\nstream document classification: Insights from social\nmedia. arXiv preprint arXiv:2104.08116.\nTerrence Szymanski. 2017. Temporal word analogies:\nIdentifying lexical replacement with diachronic word\nembeddings. In Proceedings of the 55th annual meet-\ning of the association for computational linguistics\n(volume 2: short papers), pages 448\u2013453.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,\nRebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-\ncos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,\nSimon Kornblith, et al. 2022. Model soups: averag-\ning weights of multiple fine-tuned models improves\naccuracy without increasing inference time. In In-\nternational Conference on Machine Learning, pages\n23965\u201323998. PMLR.\nMitchell\nWortsman,\nGabriel\nIlharco,\nMike\nLi,\nJong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. 2021.\nRobust fine-tuning of zero-shot models.\n2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 7949\u20137961.\nMichael JQ Zhang and Eunsol Choi. 2023. Mitigating\ntemporal misalignment by discarding outdated facts.\narXiv preprint arXiv:2305.14824.\nA\nAppendix\nA.1\nYearly Misalignment with Other Tasks\nand T5 Sizes\nIn this section, we report raw performance degra-\ndation over time on four downstream and three\nlanguage modeling tasks with three sizes of T5.\nWe evaluate on all tasks in the main paper plus\nNewsroom Source Classification (NewsCls) and AI\nVenue Classification (AIC) from Luu et al. (2022).\nWe also create a third science domain language\nmodeling task from abstracts in the Kaggle arXiv\ndataset4. For each group of three years from 2006-\n2008 to 2018-2020 we randomly sample 26-38M\nand 2.6-3.9M BPE tokens (150MB and 15MB of\ntext) of arXiv paper abstracts for train and test splits\nrespectively.\nFigures 8 and 11 are yearly degradation\nheatmaps for each model size and task. These\nresults show that normalizing performance by the\naverage on each evaluation time helps account for\nvariations in test splits. ArXiv language modeling\nand NewsSum, for example, have large differences\nin performance on evaluation years regardless of\nfinetuning year.\nA.2\nTask Variations in Linear Yearly\nDegradation\nLike Luu et al. (2022), we find differences across\ndomain and task in the rate and linearity of year-to-\nyear decay. TD scores measure the average rate of\nperformance degredation for each year of misalign-\nment between train and test time periods (Luu et al.,\n2022). We find the rate of decay using a linear least\n4https://www.kaggle.com/datasets/Cornell-\nUniversity/arxiv/data\nsquares regression and average rates for each task\nover all evaluations. Table 4 shows TD scores (Luu\net al., 2022) for all tasks and T5-sizes. We also com-\npare TD scores calculated from raw performance to\nTD scores calculated from performance normalized\nby the average on each evaluation year. In general,\npercent performance difference from the mean on\nan evaluation year decays more linearly than raw\nperformance.\nA.3\nYearly and Monthly Cosine Similarities\nIn this section, we report cosine similarity between\neach pair of yearly and monthly time vectors. Fig-\nure 10 shows cosine similarity between every pair\nof year vectors for each T5-size and task. Figure 9\nshows cosine similarity between each pair of T5-\nsmall monthly WMT LM time vectors. Similar to\nperformance, year-to-year degradation in cosine\nsimilarity between task vectors appears to be linear\nregardless of setting. Like figure 3, we observe sea-\nsonal \"stripes\" every 12 months from the diagonal\n9.\nA.4\nTemporal Degradation in Online Settings\nOur work so far illustrates temporal misalignment\non static time splits. However, in practice, we\nusually deploy language models in online settings,\nmeaning that they are continually updated with the\nlatest data, and we do not have access to data from\nall training years simultaneously.\nTo show how temporal misalignment manifests\nin these settings, we first sort all the training data\nfrom the PoliAff and WMT tasks by month, and\nfinetune T5-small on each task separately. We dis-\nplay the performance of the LM on every year\nthroughout training in Figure 13. As expected, for\nPoliAff, we see that the performance of models on\na particular year peak at the final month of that year,\nand then gradually degrade as the model continues\ntraining.\nFor language modeling on WMT data, perfor-\nmance consistently improves during training, re-\ngardless of the evaluation year. However, perplex-\nity reduces more slowly in earlier years as we con-\ntinue training. These results suggest that temporal\nmisalignment may manifest differently in online\nsettings based on the training setup and task.\nA.5\nOnline Cosine Similarities\nWe study the relationship between performance\ndegradation and cosine similarity during online\nFigure 8: Yearly downstream performance degradation on four tasks and three T5 sizes.\nNormalized?\nT5 Size\nWMT LM\nNewsSum\nNewsCls\nTwitter LM\nPoliAff\nArXiv LM\nAIC\nsmall\n-0.67 (0.81)\n2.21 (0.51)\n0.05 (0.67)\n-0.35 (0.97)\n0.08 (0.98)\n-0.59 (0.65)\n0.03 (0.55)\nNo\nlarge\n-0.10 (0.34)\n2.07 (0.53)\n0.04 (0.61)\n-0.20 (0.97)\n0.07 (0.97)\n-0.20 (0.67)\n0.03 (0.50)\n3b\n-0.07 (0.34)\n2.12 (0.53)\n0.04 (0.67)\n-0.20 (0.97)\n0.07 (0.95)\n-0.13 (0.66)\n0.03 (0.40)\nsmall\n-1.70 (0.90)\n6.99 (0.87)\n6.43 (0.74)\n-4.52 (0.89)\n10.47 (0.95)\n-2.61 (0.94)\n2.93 (0.57)\nYes\nlarge\n-0.56 (0.92)\n6.27 (0.89)\n5.33 (0.84)\n-2.64 (0.91)\n9.57 (0.94)\n-1.24 (0.93)\n2.53 (0.51)\n3b\n-0.52 (0.93)\n6.44 (0.88)\n4.72 (0.84)\n-2.90 (0.91)\n7.66 (0.91)\n-0.96 (0.94)\n3.12 (0.61)\nTable 4: TD scores for all tasks and T5 sizes for raw performance and performance divide by the average on each\neval. year. Variance explained by the TD score linear fit in parentheses. TD scores calculated with normalized\nperformance decay have generally higher R2 scores, except on Twitter LM and PoliAff, and are easier to compare.\nFigure 9: Cosine similarity between monthly time\nvectors also exhibits seasonality. We observe simi-\nlar \"stripes\" every 12 months when measuring the co-\nsine similarity between each pair of T5-small WMT\nmonth vectors. The correlation between this heatmap\n(including the diagonal) and figure 3 is \u22120.667 with\np < 1 \u00d7 10\u221216.\ntraining. Recall that in the online setting, we per-\nform a single finetuning run on the Poliaff and\nWMT tasks (after ordering their training data by\nmonth), and measure performance on each year\nthroughout training. To study how time vectors\nmove throughout space in this setting, we measure\nthe cosine similarity between the time vector of the\nmodel trained up to month m and each yearly time\nvector for the PoliAff and WMT tasks.\nWe find that the cosine similarity to each time\nvector decreases as the online model is updated\npast the first 12 months of data. This means that\nonline models\u2019 peak similarity to earlier years tends\nto be higher than those to later years since the they\nmake up a smaller part of its total finetuning set.\nLike our experiments with soups of time vectors in\nsection \u00a74.5, this indicates that models trained on\nmultiple years of data lay outside a region defined\nby single-year models.\nTo account for these decreases, we normalize the\nsimilarity to each year time vector by its average\nafter updating on all months in Figure 13. Our\nresults reveal that the vector for our online model\nis relatively most similar to each year vector after\nfinetuning on the months in that year.\nA.6\nTime Vector Analogy Ablations\nIn this section, we ablate our time vector analogy\nexperiment to determine the effects of only adding\nthe LM vector from the target time, and only scal-\ning the weights of the initial time vector. For \u03c4k \u2248\n\u03b11 \u00b7\u03c4j +(\u03b12 \u00b7\u03c4 LM\nk\n\u2212\u03b13 \u00b7\u03c4 LM\nj\n), we define our \"task\naddition\" ablation for \u03b13 = 0, \u03b11, \u03b12 \u0338= 0, and our\n\"scaling only\" ablation for \u03b11 \u0338= 0, \u03b12, \u03b13 = 0\nWe report the best results after sweeping over\nthe same \u03b1 ranges from \u00a74.4 with the added con-\nstraints in figure 15. While task analogies generally\nperform best across tasks and T5-sizes (especially\nas \u03c4j and \u03c4k become more misaligned), we find that\nablating \u03c4 LM\nk\nand \u03c4 LM\nj\ncan still improve over the\nbase \u03c4j model. Surprisingly, only scaling \u03c4j also\nimproves over the initial model on many tasks.\nA.7\nTemporal Misalignment Affects Some\nParameters More than Others\nIn this section, we explore whether we can re-\nduce temporal misalignment by swapping parame-\nter weights from a model trained on a misaligned\nyear with those of the model trained on the target\nyear. For example, we substitute the QKV attention\nlayers from a model finetuned on 2015 PoliAff with\nthose finetuned on 2020 PoliAff and evaluate on\n2020 data. In table 5 we evaluate the start-year fine-\ntuned models for each task on the end times (e.g.\nstart = 2012 for WMT LM, end = 2016) with vari-\nous parameter weights swapped with the end-year\nfinetuned model.\nFrom these experiments, we find that we can\nimprove performance on a target time by swapping\nout weights with a time vector finetuned on\nthat time.\nSurprisingly, swapping embeddings\nwith the target time vector makes very little\ndifference, except in language modeling tasks,\nand swapping all non-embedding weights with a\ntarget time almost reaches the performance the\ntarget time-specific models for downstream tasks.\nSwapping only feed-forward or attention layers\nalso improves performance on the target time,\nsuggesting temporal misalignment is somewhat\nisolated to those model regions in downstream\ntasks.\nFigure 10: Cosine similarities between all pairs of year time vectors for all tasks and model sizes.\nFigure 11: Yearly language modeling perplexity decay on three tasks and three T5 sizes.\nSwapped Params\nWMT LM\nNewsSum\nNewsCls\nTwitter LM\nPoliAff\nArXiv LM\nAIC\nNone\n35.72\n35.11\n0.7232\n6.69\n0.5903\n18.18\n0.8224\nFeed Forward\n35.31\n35.17\n0.8162\n13.25\n0.6174\n18.21\n0.8500\nAttention\n36.23\n34.49\n0.7986\n14.95\n0.6095\n19.24\n0.8644\nEmbeddings\n36.13\n34.30\n0.7232\n16.65\n0.5902\n19.29\n0.8192\nNon-Embedding\n34.57\n37.24\n0.8760\n13.46\n0.7991\n17.37\n0.8845\nAll\n33.51\n38.89\n0.8759\n5.79\n0.7999\n15.75\n0.8845\nTable 5: We can improve performance on a target time by swapping out weights with a time vector finetuned\non that time. T5-small start-year finetuned model performance on the end-year split for each task (e.g. finetuning\non 2015 for PoliAff and evaluating on 2020). We compare the baseline start-year model (none swapped) to versions\nwith various parameter weights from the target-year model, and the target-year model itself (all swapped).\nFigure 12: Seasonality makes a small, but noticeable\nimpact on monthly misalignment. Distribution of per-\nplexity change from the mean for aligned finetuning and\nevaluation months (left, mean=-4.36), seasonal \"stripes\"\n(middle, mean=0.04), and all finetuning and evaluation\ncombinations which share neither the same month nor\nyear (right, mean=0.77).\nFigure 13: In online settings, language model performance degrades on earlier time periods. We show macro\nF1 and perplexity on each year split of PoliAff and WMT LM respectively after sequentially finetuning T5-small on\neach new month of task data. PoliAff performance over all years plateaus after finetuning on months up to 2018.\nWMT performance continues to improve with more data, but perplexity decrease slows on earlier years. Starred\npoints are where performance on a year is best relative to the average performance on all years.\nFigure 14: Cosine similarity between an online time vector and a year vector peaks relative to other years\nafter updating on data for that year. We show cosine similarity between each monthly checkpoint of online\nT5-small time vectors and yearly vectors for PoliAff and WMT LM. To account for overall decreases in similarity\nas online time vectors are updated, we normalize similarities to each year vector by the mean similarity to that year\nover all checkpoints. We star the point for each year vector where its cosine similarity to the online model is largest\nrelative to the average on all years.\nFigure 15: Time vector analogy ablations for three sizes of T5. Given the time vector analogy \u03c4k \u2248 \u03b11 \u00b7 \u03c4j + (\u03b12 \u00b7\n\u03c4 LM\nk\n\u2212 \u03b13 \u00b7 \u03c4 LM\nj\n), \u03b11, \u03b12, \u03b13 \u0338= 0, we define \"task addition\" to be only adding the language modeling vector (i.e\n\u03b11, \u03b12 \u0338= 0, \u03b13 = 0), and \"scaling only\" to be only scaling the base \u03c4j model (i.e \u03b11 \u0338= 0, \u03b12, \u03b13 = 0). We sweep\nover the same alpha combinations as in \u00a74.4 and report the best results for each target year, task, and T5-size.\nFigure 16: Year-to-year, T5-small feed forward layers change the most across all tasks and domains, and attention\nchanges more in the language modeling setting. For our T5-large and T5-3b models trained with LoRA, the V\nattention layers change more than the Q layers, with most of the changes (regardless of model size) concentrated\nin the last layers. Like our param swapping experiment, this suggests that some parameters play a larger role in\ntemporal misalignment than others.\n"
  },
  {
    "title": "PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models",
    "link": "https://arxiv.org/pdf/2312.13964.pdf",
    "upvote": "16",
    "text": "PIA: Your Personalized Image Animator\nvia Plug-and-Play Modules in Text-to-Image Models\nYiming Zhang1\u2020 Zhening Xing1\u2020 Yanhong Zeng1\nYouqing Fang1\nKai Chen1*\n1Shanghai Artificial Intelligence Laboratory\nhttps://PI-Animator.github.io\nFigure 1. Given an elaborated image generated by a personalized text-to-image model, the proposed Personalized Image Animator (PIA)\nanimates it with realistic motions according to different text prompts while preserving the original distinct styles and high-fidelity details.\nWe recommend using Abode Arobat and clicking the images to play the animation clips. [Best viewed in color with zoom-in]\nAbstract\nRecent advancements in personalized text-to-image\n(T2I) models have revolutionized content creation, empow-\nering non-experts to generate stunning images with unique\nstyles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in\npreserving distinct styles, high-fidelity details, and achiev-\ning motion controllability by text. In this paper, we present\nPIA, a Personalized Image Animator that excels in align-\ning with condition images, achieving motion controllabil-\n*Corresponding author.\n\u2020Equal contribution.\nity by text, and the compatibility with various personalized\nT2I models without specific tuning. To achieve these goals,\nPIA builds upon a base T2I model with well-trained tempo-\nral alignment layers, allowing for the seamless transforma-\ntion of any personalized T2I model into an image animation\nmodel. A key component of PIA is the introduction of the\ncondition module, which utilizes the condition frame and\ninter-frame affinity as input to transfer appearance informa-\ntion guided by the affinity hint for individual frame synthe-\nsis in the latent space. This design mitigates the challenges\nof appearance-related image alignment within PIA and al-\nlows for a stronger focus on aligning with motion-related\nguidance. To address the lack of a benchmark for this field,\nwe introduce AnimateBench, a comprehensive benchmark\narXiv:2312.13964v1  [cs.CV]  21 Dec 2023\ncomprising diverse personalized T2I models, curated im-\nages, and motion-related prompts. We show extensive ex-\nperiments on AnimateBench to verify the superiority of PIA.\n1. Introduction\nRecent democratization of text-to-image (T2I) generation\nhas seen incredible progress by the growing Stable Diffu-\nsion community [1, 31] and the explosion of personalized\nmodels [19, 32]. Numerous AI artists and enthusiasts cre-\nate, share, and use personalized models on model-sharing\nplatforms such as Civitai [6] and Hugging Face [20], turn-\ning imagination into images at an affordable cost (e.g., a\nlaptop with an RTX3080). However, there is still limited\nresearch exploring the realm of infusing these unique per-\nsonalized images with dynamic motions [4, 11, 39].\nWe are intrigued by the potential of animating person-\nalized images, given their distinctive styles and impressive\nvisual quality. Moreover, the incorporation of text prompts\nto guide the animation process can significantly streamline\nvarious creative endeavors within the vibrant personalized\ncommunity, minimizing the required effort. While promis-\ning, this task presents two distinct challenges: image align-\nment with the detailed personalized image and achieving\nmotion controllability through text prompt. While recent\nadvancements in training large-scale text-to-video (T2V)\nmodels have enabled users to generate videos based on text\nor images [39], these approaches struggle to preserve the\nunique styles and high-fidelity details of conditional images\ndue to the absence of personalized domain knowledge. To\naddress this limitation, researchers have developed temporal\nalignment layers [4, 11]. These plug-and-play modules ef-\nfectively transform personalized T2I models into T2V mod-\nels while maintaining their distinct styles [4, 11]. However,\ndespite numerous attempts to adapt these personalized T2V\nmodels for image animation, the primary focus of the tem-\nporal alignment layers remains on aligning the appearance\nof individually generated frames in latent space to achieve\nsmooth video results. As a result, their ability to effectively\nrespond to motion-related text guidance is limited.\nTo address the aforementioned limitations, we intro-\nduce PIA, a Personalized Image Animator that excels in\nboth image alignment and motion controllability.\nIni-\ntially, we choose to build our framework upon a base T2I\nmodel (i.e., Stable Diffusion [31]) and incorporate well-\nestablished temporal alignment layers following previous\nworks [11]. This approach allows PIA to effectively lever-\nage personalized domain knowledge by seamlessly replac-\ning the base T2I model with any personalized T2I model\nduring inference. Next, to empower our framework with\nimage animation ability, we introduce a trainable condition\nmodule into the input layer of the T2I model. This mod-\nule plays a crucial role in generating each frame of the\nanimation by taking the conditional image and the inter-\nframe affinity between the current frame and the condi-\ntional frame as inputs.\nThrough this design, the condi-\ntion module effectively borrows appearance features from\nthe conditional frame, guided by the affinity hint, result-\ning in improved alignment with the condition image. Fi-\nnally, we fine-tune the condition module and the tempo-\nral alignment layers while keeping the T2I model fixed to\npreserve its open-domain knowledge. Leveraging the en-\nhanced appearance-related alignment facilitated by the con-\ndition module within the input layer, the temporal align-\nment layers can focus more on motion-related alignment,\nleading to improved motion controllability. Importantly, we\ndemonstrate the effortless generalizability of PIA\u2019s anima-\ntion capability by replacing the T2I model with any other\npersonalized model. This flexibility empowers users to an-\nimate their elaborated personal images using text prompts\nwhile preserving distinct features and high-fidelity details.\nIn summary, PIA presents a powerful solution for per-\nsonalized image animation, offering superior image align-\nment, motion controllability by text, and the flexibility to\nintegrate various personalized models. This comprehensive\napproach ensures an engaging and customizable animation\nexperience for users. To address the lack of a benchmark in\npersonalized image animation, we introduce a new bench-\nmark called AnimateBench, which comprises various per-\nsonalized T2Is, curated images, and tailored motion-related\nprompts. To validate the effectiveness of PIA, we conducted\nextensive quantitative and qualitative evaluations using An-\nimateBench. These evaluations provide a robust assessment\nof the capabilities and performance of our approach.\n2. Related Work\nSingle Image Animation.\nEnhancing still pictures with\nmotions has attracted decades of attention and efforts in the\nresearch field [3, 5, 8, 18, 35, 36, 41]. Chuang et al. present\na pioneer work for animating a variety of photographs and\npaintings [5]. Specifically, the still scene is segmented into\na series of layers by a human user and each layer is ren-\ndered with a stochastic motion texture to form the final an-\nimated video. With the success of deep learning, more re-\ncent works get rid of the manual segmentation and can syn-\nthesize more natural motion textures [3, 8, 18, 35, 36, 41].\nEndo et al. propose learning and predicting the motion and\nappearance separately using two types of DNN models for\nanimating landscape [8]. Holynski et al. learns an image-to-\nimage translation network to encode motion priors of natu-\nral scenes with continuous fluid [18]. Bertiche et al. fo-\ncus on learning garment motion dynamics for blowing the\ndressed humans under the wind [3]. Xiao et al. propose au-\ntomatically segmenting hair wisps and animating hair wisps\nwith a wisp-aware animation module with pleasing motions\nRes \n+ \nSA\nCA\nTA\nPrompt: \u201cA dog running\u201d\nAligned \nframes\n(b) U-Net Block and Temporal Alignment layer(TA)\n\ud835\udc67!\nPrompt: \u201cA dog running\u201d\nIterative denoising: \ud835\udc67! \u2192 \ud835\udc67!\"#\nU-Net Block\nTA\nU-Net Block\nTA\nU-Net Block\nTA\n\ud835\udc64$%&'\n\ud835\udc64\n\ud835\udc67!\n(& = \u211d )\u00d7(,-#-,)\u00d7/\u00d70\n\ud835\udc60#:)\n\ud835\udc60#, \u2026 \ud835\udc60)\n\u00d7\ud835\udc39\n\u2130 \ud835\udc3c\n(a) The overview of Personalized Image Animator(PIA)\nIndividual\nframes\nFrozen weights\nRes: ResBlock CA: Cross-Attention\nSA: Self-Attention\nTrainable weights\nFigure 2. Personalized Image Animator (PIA). As shown in (a), PIA consists of a text-to-image (T2I) model, well-trained temporal\nalignment layers (TA), and a new condition module Wcond responsible for encoding the condition image zI and inter-frame affinity s1:F .\nIn particular, the T2I model consists of U-Net blocks, including a ResBlock (Res) [13], a self-attention layer (SA), and a cross-attention\nlayer (CA), as depicted in (b). During training, the condition module learns to leverage the affinity hints and incorporate appearance\ninformation from the condition images, facilitating image alignment and enabling a stronger emphasis on motion-related alignment.\n[41]. Some works focus on synthesizing natural lip motion\nfor talking heads based on audio [35, 36]. Recent works\nalso have been significantly prompted by the powerful dif-\nfusion models [21, 23, 31]. Mahapatra et al. transfer the\nestimated optical flow from real images to artistic paintings\nby the concept of twin image synthesis with a pre-trained\ntext-to-image diffusion model [23]. Li et al. leverages a\nlatent diffusion model for modeling natural oscillating mo-\ntion such as flowers and candles blowing in the wind [21].\nDespite the significant progress, these animation techniques\ncan only synthesize specific types of content and motion\nsuch as time-lapse videos, and facial and body animation.\nIn contrast, our model, PIA, can animate the image without\nsegmenting the region of interest and drive the image with\nany motion through a text prompt.\nText-to-video\nSynthesis.\nRecent\nlarge\ntext-to-image\nmodels have shown remarkable progress and attracted sig-\nnificant attention, by enabling diverse and high-fidelity im-\nage synthesis based on a text prompt written in natural lan-\nguage [26, 29\u201331, 34]. The video latent diffusion model\n(VLDM) has demonstrated a strong ability to capture mo-\ntion priors [4, 9, 11, 12, 17, 22, 40, 42]. Some text-to-video\ngeneration models[4, 11] are trained to synthesize natural\nvideos. However, most of them can not take an image as\ninput. Current approaches[33] have tried to improve image\nanimation based on VLDM, but they suffer from issues such\nas lack of fidelity to the conditioning frames and poor con-\ntrol over generated content. Some works[39, 45] finetune\nmodels to take images as condition, however, most image\ndetails are lost. Besides, most of the prompts are used to\ndescribe the image content, users can not animate the image\naccording to their intent. AnimateDiff [11] is a pipeline that\naims at training a plug-and-play motion module from large-\nscale video datasets and transferring the learned reasonable\nmotion priors to new domains by inserting the motion mod-\nule into customized T2I models. Although it has shown\npromising results, AnimateDiff fails to animate a given im-\nage with a text-to-video model.\n3. PIA: Your Personalized Image Animator\nFor an image I generated from a personalized text-to-image\nmodel, our target is to generate a F frame video clip V =\n{v1, . . . , vF }, where the first frame v1 should resemble\ncondition image I, and the motion should follow the in-\nput prompt c. As mentioned in the previous section, since\nthe appearance information between frames is independent\nof each other, the current temporal alignment layers need\nto align appearance and motion information across frames\nsimultaneously. This prevents temporal alignment layers\nfrom learning motion prior effectively. To address this is-\nsue, we design PIA, as shown in Fig. 2. We propose two\nframe-wise inputs: the conditional image I and the inter-\nframe affinity s1:F . These inputs are encoded through a\ncondition module, which provides the necessary appearance\ninformation for each frame.\nIn this section, Sec. 3.1 introduces preliminary informa-\ntion about Stable Diffusion model and temporal alignment\ntechniques in text-to-video methods. Then, we describe the\ndesign of our proposed PIA in Sec. 3.2. Finally, in Sec. 3.3,\nwe present the training and inference details.\n3.1. Preliminaries\nStable Diffusion.\nStable Diffusion (SD) [31] is one of the\nmost popular large-scale open-source text-to-image models\nand has a well-developed community. SD has 4 main com-\nponents: a encoder E, a decoder D, a U-Net denoiser \u03f5\u03b8,\nand a pretrained CLIP-based [27] text encoder T . In the\ntraining stage, the input image x is first encoded to the la-\ntent code z0 = E(x). Then z0 is perturbed via pre-defined\ndiffusion process [7, 16, 24]:\nzt = \u221a \u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5t, \u03f5t \u223c N(0, I)\n(1)\nwhere timestep t \u223c U[1, T], and \u00af\u03b1t is the noise strength at\nt. Then the denoising U-Net \u03f5 is trained to predict the added\nnoise with the following loss:\nL = Ex,\u03f5\u223cN(0,1),t,c [\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, T (c))\u2225]\n(2)\nwhere c is the corresponding prompt for x0.\nTemporal Alignment Layer.\nThe recent T2V methods\n[4, 11, 40] have discovered that incorporating temporal\ntransformers[38] into SD enables the alignment of appear-\nance and motion information across frames, thereby trans-\nforming the T2I model into a T2V model. Taking feature\nwith a shape of f \u2208 RF,C,H,W as an example, where F\nrepresents the frame length, C denotes the number of chan-\nnels, and H and W represent the spatial dimensions. We\nbegin by permuting its shape to ((H, W), F, C), and then\nthe transformers layers for temporal alignment can be de-\nfined as follows:\nfout = Softmax(QK\u22ba\n\u221ac ) \u00b7 V,\n(3)\nwhere Q = WQz, K = WKz, V = WV z are projection\noperation. Due to the temporal alignment layer needing to\nconsider both appearance and motion information simulta-\nneously, a trade-off arises between appearance consistency\nand motion controllability in the T2V task. In our work,\nwe introduce explicitly appearance information and corre-\nsponding encoding module to make the temporal alignment\nlayer focus more on motion-related information.\n3.2. Plug-and-Play Modules for Animation\nWe introduce the additional conditions and how conditional\nmodule can borrow appearance information from them to\nimprove the motion controllability. In the T2V model, the\ninitial latent for each frame is independently sampled from\nGaussian Distribution, this results in inconsistent appear-\nance identities for each frame, as illustrated in the upper\nrow of Fig. 3. As the only frame-aware modules, tempo-\nral alignment layers need to devote effort to simultaneously\nlearning motion prior and aligning appearance consistency.\nTo address this problem, we encode the conditional image\nI to the latent space, i.e. zI = E(I), and utilize it as the\nexplicit appearance information for each frame. Further-\nmore, we propose inter-frame affinity s to quantify the de-\ngree of motion for each frame in relation to the conditional\n(a) Vanilla Personalized \nT2V: w/o CM \n(b) PIA: w/ CM \nU-Net \nBlocks\nConvIN + CM\n\ud835\udc64!\"#$\n\ud835\udc64\nConvIN\n\ud835\udc64\nU-Net \nBlocks\nTA\n\ud835\udc67%\n\ud835\udc67%\n\ud835\udc67& \u2295 \ud835\udc60\nTA\nIndividual frames\nAligned \nframes\nIndividual frames\nAligned \nframes\nFigure 3. Illustration of the condition module (CM). A vanilla\npersonalized T2V model (shown in (a)) needs to align both the\nappearance and motion of individual frames simultaneously. PIA\nwith CM (shown in (b)) can borrow appearance information from\ncondition image zI with affinity hints s, easing the challenge of\nboth appearance and motion alignment. We use the color and strip\nto denote appearance and motion, respectively.\nimage. In the training stage, we calculate the affinity score\nbased on the training data. Given a video clip v1:F , we\nset the first frame v1 as the conditional image. For frame\nvi, we first calculate the L1 distance between it and the\nconditional frame v1 in HSV space, and denote this dis-\ntance as di. Subsequently, we apply this operation to all\nframes of video clips in the dataset, resulting in the maxi-\nmum distance value dmax. Then we normalize the distance\ndi to [0, 1] via dmax. The affinity score can be calculated\nas si = 1 \u2212 di/dmax \u00d7 (smax \u2212 smin). Here, smax and\nsmin are hyperparameters to scale affinity scores to specific\nranges. In order to align si with zI, we expand it to a sin-\ngle channel feature map with the same size as latent code\n(i.e. [1 \u00d7 h \u00d7 w]). During the inference stage, we allow the\nuser to provide a user-defined input for si. This enables the\ngeneration of animation results with varying motion scales,\nproviding customization in the output, as shown in Fig. 7.\nTo expand the capabilities of the U-Net without compro-\nmising its original functionality, we introduce a lightweight\nsingle-layer convolution as the conditional module.\nFor\ncondition zI \u2295 si, where \u2295 denotes concatenation opera-\ntion, we encode it with Wcond and add it to the output of the\nfirst convolution layer. As shown in the lower row of Fig. 3,\nthe inclusion of the conditional module results in enhanced\nconsistency of the appearance information in the output of\nthe first convolutional layer compared to the original model.\nThis makes subsequent temporal alignment layers focus on\naligning motion guidance.\nThis operation is equivalent to concatenating Wcond and\nW, the weight of the original convolution block, at the first\ndimension. To ensure the newly added weight does not hurt\nthe performance of the pretrained model, we apply zero-\ninitialization [44] to Wcond.\n3.3. Training and inference\nPrior to the training of PIA, following AnimateDiff [11],\nwe initiate the process by pretraining a domain adapter and\nthe temporal alignment layers on a single frame dataset and\na video dataset, respectively. Next, we expand the number\nof channels in the first convolution block by initializing the\nadditional channels with zero weights, as shown in Fig. 3.\nIn the training stage, for sampled video clip v1:F , we des-\nignate the first frame as the conditional image, i.e. I := v0.\nWe only update the Wcond and the temporal alignment lay-\ners the following loss function, while keeping the other pa-\nrameters unchanged.\nL = Ev1:F ,\u03f5\u223cN (0,I),t,c[\u2225\u03f5 \u2212 \u03f5\u03b8(z1:F\nt\n, z0\n0, s1:F , t, T (c))||2\n2],\n(4)\nwhere c is the corresponding prompt, z0\n0 is the clean latent\ncode for the first frame and z1:F\nt\nis perturbed latent code\nfollow Eq. (1). To retain the model\u2019s text-to-video capabil-\nity, we randomly drop inputs with a 20% probability dur-\ning training, effectively transforming the training task into\na T2V task. Specifically, we replace both s1:F and zI with\nzeros. When calculating the affinity score, we set smax and\nsmin to be 1 and 0.2, respectively. Both single frame and\nvideo data training are conducted on WebVid dataset [2].\nThe number of frames F is fixed as 16 during training. In\nthe inference stage, for image I from personalized models,\nwe replace the domain adapter with personalized models\nand keep the temporal alignment layers and extended con-\nvolution block unchanged. Affinity score s serves as user\ninput and plays a role in adjusting the magnitude of motion.\n4. Experiments\n4.1. AnimateBench\nAnimateBench.\nExisting benchmarks are restricted to\nspecific domains like human faces, fluid elements, etc. To\nthis end, we introduce AnimateBench for comparisons in\nthe field of personalized image animation. AnimateBench\ncontains images in different domains and multiple prompts\nto test the text-based image animation ability. Specifically,\nwe evaluated 105 personalized cases, which contain dif-\nferent contents, styles, and concepts. These personalized\ncases are generated from 7 different personalized text-to-\nimage models, which are used to evaluate the domain gen-\neralization ability. We use five images generated by each\nmodel for a comprehensive comparison. In addition, we\ncarefully elaborate three motion-related prompts for each\nimage to evaluate the motion controllability by the text of\ndifferent approaches. Specifically, the motion-related text\nprompts typically describe the following motions that the\nimage probably happens within a single shot shot.\nEvaluation Metrics.\nOur quantitative comparison evalu-\nates animation quality including image alignment and text\nFigure 4. An example of AnimateBench. The images in Ani-\nmateBench are carefully crafted using a set of collected person-\nalized text-to-image models. Each image has three carefully de-\nsigned prompts, describing the following motions that the image\nprobably happens within a single short shot.\nalignment, following previous works [11, 40]. Specifically,\nthe CLIP score is calculated by computing the cosine sim-\nilarity between different embeddings, e.g., text and image.\nTherefore, we can use the CLIP score to evaluate both text\nand input image similarity with videos.\n\u2022 Image Alignment We compute the cosine similarity be-\ntween input image embeddings and each video frame em-\nbeddings to evaluate the image-video alignment.\n\u2022 Text Alignment We also compute the cosine similarity\nbetween text embeddings and frame embeddings to eval-\nuate the text-video alignment.\nBaselines.\nWe carefully choose the most recent and com-\npetitive approaches for personalized image animation with\ntheir brief introduction as follows,\n\u2022 VideoComposer [39] can generate controllable videos\nusing different conditions such as text, image, motion\nvector, or a combination of them.\n\u2022 AnimateDiff [11] is a milestone work that largely facil-\nitates the development of personalized video generation\nby learning motion prior. We extend it for the application\nof image animation following previous best practices by\nusing ControlNet [44] and image prompt adapter [43].\n\u2022 Pika Labs [25] and Gen2 [10] are two of the most pop-\nular commercial tools for video generation, which are\nwidely recognized as state-of-the-art in image animation.\n4.2. Comparisons with State-of-the-Art\nWe compare our method with state-of-the-art for personal-\nized image animation on AnimateBench. The performance\nof these models was evaluated in various terms of text align-\nment, and image alignment.\nQualitative Comparison.\nWe qualitatively compared\nPIA with the most recent animation methods, including\nAnimateDiff-ControlNet, Gen-2, Pika Labs, and Video-\nComposer. Visual results can be found in Fig. 5. It can\nbe observed that the videos generated by PIA exhibit better\nAnimateDiff\nPika\nGen-2\nPIA(ours)\n\"A teddy \nbear is \nwalking\n...\"\n\"A rabbit \nis on fire\"\nInput image\nVideoComposer\nInput image\nFigure 5. Qualitative comparison with state-of-the art approaches. Compared with other methods, PIA shows excellent motion con-\ntrollability and strong image alignment. Specifically, in the first case, PIA generates a \u201cwalking\u201d motion for the toy bear (in its feet), while\nother methods can only remain static frames, showing a lack of motion controllability. In the second case, PIA adds a new element, i.e.,\nfire, with realistic motion. We show more video cases in supplementary materials due to the file size limit of the main paper.\nresponses to motion-related keywords in the prompts and\nbetter preservation of details from the input image content.\nVideocomposer produces video clips with poor text align-\nment and incoherent motion. Animatediff-ControlNet, on\nthe other hand, generates videos with smooth motion but\nloses image details. Gen2 and Pika Labs are prone to gener-\nate videos with high identity but respond less to the prompt.\nQuantitative Evaluation.\nWith the personalized cases,\nwe computed the CLIP Score to compare text-video align-\nment and image-video alignment.\nDetailed results are\nshown in Table 1. The results demonstrate that PIA achieves\nthe highest CLIP Scores, indicating strong text-video align-\nMethods\nCLIP Score\nUser Study\nimage\ntext\nimage\ntext\nVideoComposer[39]\n225.3\n62.85\n0.180\n0.110\nAnimateDiff[11]\n218.0\n63.31\n0.295\n0.220\nPIA(ours)\n225.9\n63.68\n0.525\n0.670\nTable 1. Quantitative comparison on AnimateBench.\nment and high image fidelity.\nWe generated videos with different methods on Ani-\nmateBench and solicited ratings from participants for the\nranking in text alignment and image alignment. Specifi-\ncally, we set up twenty questions and asked which of the\nvideos best matched the text or image. Note that all the\ncases are selected randomly. We then compute the prefer-\nence rate to evaluate the performance of all the methods.\nResults can be found in Table 1.\n4.3. Analysis\nIn this section, we present some interesting applications and\nconduct ablative study. Specifically, we first introduce three\nabilities of PIA, which are motion control by text prompt,\nmotion magnitude controllability and style transfer.\nWe\nshow visual results for each application. Then we com-\npare two different frame affinities to clarify the effective-\nness. Details of different constructions of frame affinity will\nbe explained in the following.\nMotion Control by Text Prompt.\nThe condition module\nenables PIA to borrow appearance features from the con-\nditional frame so that temporal alignment layers can fo-\ncus more on motion generation. These designs enable PIA\nto perform better image and text alignment in generated\nvideos. The strong prompt control ability of PIA can be\nseen in Fig. 6. During the test, we observed that a simple\nprompt design produces great visual effects, as shown in the\nfigure. PIA is able to generate fancy results by adding new\nelements to the generated videos with smooth motion qual-\nity based on text prompts. Unlike Pika Labs or Gen-2 prone\nto generate small motion within 3 seconds, PIA generates\nvideo with more text alignment motion.\nMotion Magnitude Controllability.\nPIA can control the\nmagnitude of motion by adjusting the frame affinity. In the\ninference stage, we constructed three types of frame affin-\nity patterns based on real video data, and the test results\nare shown in Fig. 7. Specifically, we use the statistics in\nthe training stage, to construct the similarity affinity pat-\ntern. We broadly divide the range of motion into three cat-\negories and provide the corresponding affinity in inference\ntime. In Fig. 7, the frame affinity gradually decreases from\ntop to bottom, indicating an increasing magnitude of mo-\ntion. From the results, it can be observed that the generated\nPrompt: Fireworks, 1castle\nPrompt: 1castle is on fire\nPrompt: Lightning struck the castle\nFigure 6. Motion Control by Text Prompt. PIA can effectively\ncapture the motion-related guidance in text prompt and add realis-\ntic related motion in the results.\nLight\nMiddle\nLarge\nGenerated Videos\nPrompt: Jumping, a golden labrador, \u2026\u2026\nFigure 7. Motion Magnitude Controllability. PIA enables users\nto adjust the motion magnitude as light, middle, or large by setting\nthe input affinity information as different values.\nmotion aligns with our expectations, demonstrating the ef-\nfectiveness of the similarity channel in controlling motion\nmagnitude. Meanwhile, PIA can also control the position\nof the input image in the video by adjusting the Similarity\nchannel. In short, by using different frame affinity during\nthe inference stage, users can flexibly control the motion\nmagnitude and conditional frame position in the generated\nvideo, leading to a more controllable animation.\nPrompt: The sea lapping behind 1man, \u2026\nInput Images\nGenerated Videos\nFigure 8. Style transfer. PIA is able to achieve style transfer in\ngenerated videos by applying a personalized text-to-image model\nwith a different domain style to the given image.\nStyle Transfer.\nSo far, we have discussed using models\nand input images in the same domain to generate videos.\nHowever, we have found interesting facts that if the two\nhave different domains, such as providing a real-style image\nand using an anime-style model to generate motion, PIA can\nachieve style transfer effects. As shown in Fig. 8. Frame\naffinity helps generate videos with smooth style transfer be-\ntween image style and model style. This feature of PIA\nallows users to have a better-personalized experience.\nEffectiveness of Frame Affinity.\nIn the training stage, we\ncalculate the similarity score for affinity hints as mentioned\nbefore. We perform an ablation study with one-hot frame\naffinity to prove the effectiveness of similarity frame affin-\nity. One-hot frame affinity is constructed by 1 in the condi-\ntion frame and 0 in the other frames which is able to indicate\nthe position of the condition frame. We evaluate two meth-\nods on the AnimateBench. One-hot frame affinity obtains\nthe image CLIP score of 210.9, while the similarity frame\naffinity reaches 225.9. Moreover, we also show the visual\nresult of video frames generated by the two constructions in\nFig. 9. We can find that frame affinity constructed by simi-\nlarity allows the model to learn identity better. Note that in\ndifferent constructions, we use the same pattern of affinity\nin training and inference.\nLimitation.\nPIA is capable of animating personalized im-\nages with high text and image alignment with text condi-\ntions. However, we have observed that videos generated\nby PIA exhibit color discrepancy when applied to images\nwith significantly different styles from the training data, as\nshown in Fig. 10. We hypothesize the color discrepancy\nis caused by the WebVid [2] domain. PIA learns the color\ndistribution of WebVid, and when there is a notable differ-\nence between the personalized image during inference, the\ncolor discrepancy may arise. Additionally, when generat-\ning images, specific trigger words such as a certain style\nOne-hot\nAffinity\nPrompt: Strong waves hitting the lighthouse, \u2026\nFigure 9. Effectiveness of frame affinity. With frame affinity,\nPIA is able to generate videos with stronger image alignment.\nFigure 10. Limitation. When the input image is far from the\ntraining dataset domain, PIA is prone to generate videos with sig-\nnificant shifts of color.\nmay be required. However, when generating videos, if the\nprompts do not include these words, it can also result in\ncolor discrepancy. We further speculate that training on a\nmore diverse range of video data with various styles and\ncontent and providing complete trigger words during infer-\nence could mitigate this phenomenon.\n5. Conclusions\nIn this paper, we introduce PIA a powerful solution for\npersonalized image animation. Our method shows excel-\nlent image alignment and motion controllability, ensuring\ntext-based customized animation for users.\nTo achieve\nhigh alignment, we introduce condition module and inter-\nframe affinity. They endow PIA borrow appearance features\nfrom the conditional frame, leading temporal layers to fo-\ncus more on motion generation. Furthermore, we construct\nan animation benchmark named AnimateBench to evaluate\nthe animation performance of PIA and other methods. Ex-\nperimental results demonstrate that PIA performs excellent\non image animation tasks and shows various interesting ex-\ntended applications.\nIn future work, we first plan to extend our method to a\nmore powerful base text-to-image model, e.g., SDXL [26],\nwhich is becoming the new trend in the Stable Diffusion\ncommunity. Second, we plan to study the limitation of color\nshift caused by the low-quality training dataset.\nReferences\n[1] AUTOMATIC1111. https://github.com/AUTOMATIC1111/stable-\ndiffusion-webui.\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021.\n[3] Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, Chun-\nHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio\nEscalera, and Duygu Ceylan. Blowing in the wind: Cyclenet\nfor human cinemagraphs from still images. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 459\u2013468, 2023.\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023.\n[5] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian\nCurless, David H Salesin, and Richard Szeliski.\nAnimat-\ning pictures with stochastic motion textures. In ACM SIG-\nGRAPH 2005 Papers, pages 853\u2013860. 2005.\n[6] CIVITAI, 2022. https://civitai.com/.\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021.\n[8] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. An-\nimating landscape: self-supervised learning of decoupled\nmotion and appearance for single-image video synthesis.\nACM Transactions on Graphics (TOG), 38(6):1\u201319, 2019.\n[9] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023.\n[10] Gen2, 2023. https://research.runwayml.com/gen2.\n[11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023.\n[12] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion modeling\nof long videos. Advances in Neural Information Processing\nSystems, 35:27953\u201327965, 2022.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016.\n[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. CLIPScore: a reference-free evaluation met-\nric for image captioning. In EMNLP, 2021.\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020.\n[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video diffu-\nsion models, 2022.\n[18] Aleksander Holynski, Brian L Curless, Steven M Seitz, and\nRichard Szeliski.\nAnimating pictures with eulerian mo-\ntion fields.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5810\u2013\n5819, 2021.\n[19] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\nrank adaptation of large language models. In International\nConference on Learning Representations, 2021.\n[20] Huggingface, 2022. https://huggingface.co/.\n[21] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander\nHolynski.\nGenerative image dynamics.\narXiv preprint\narXiv:2309.07906, 2023.\n[22] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023.\n[23] Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee,\nSergey Tulyakov, and Jun-Yan Zhu. Text-guided synthesis\nof eulerian cinemagraphs. SIGGRAPH ASIA, 2023.\n[24] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021.\n[25] Pika Labs, 2023. https://www.pika.art/.\n[26] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023.\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022.\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023.\n[33] s9roll7.\nhttps://github.com/s9roll7/animatediff-cli-prompt-\ntravel.\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022.\n[35] Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou,\nand Jiwen Lu. Learning dynamic facial radiance fields for\nfew-shot talking head synthesis. In European Conference on\nComputer Vision, pages 666\u2013682. Springer, 2022.\n[36] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng\nZhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion\nmodels for generalized audio-driven portraits animation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1982\u20131991, 2023.\n[37] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n[39] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023.\n[40] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023.\n[41] Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem,\nand Bing Li. Automatic animation of hair blowing in still\nportrait photos. In ICCV, 2023.\n[42] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation. Entropy,\n25(10):1469, 2023.\n[43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023.\n[44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023.\n[45] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023.\n6. Implementation Details\n6.1. Training\nWe train PIA on WebVid10M [2] with only condition mod-\nule and temporal alignment layers trainable. Specifically,\nwe use the motion module in AnimateDiff [11] as a pre-\ntrained model for temporal alignment layers.\nWe com-\npute the L1 distance between the condition frame and other\nframes in HSV space. Subsequently, we utilize this distance\nto calculate the affinity score. Specifically, we consider the\ntop 2.5th percentile of the samples as the minimum and the\n97.5th as the maximum value to linearly scale the affinity\nscore to [0.2, 1]. In addition, we set a probability of 20%\nto zero out the entire input of the condition module. This\nensures that PIA retains text-to-video capabilities and pro-\nmotes the training of the condition module. We train condi-\ntion module and temporal alignment layers on 16 NVIDIA\nA100s for 4.5k steps and use a learning rate of 1 \u00d7 10\u22125.\n6.2. Inference\nDuring the inference stage, users can replace the base model\nwith the personalized T2I model to realize the image anima-\ntion. Besides, we construct the inter-frame affinity accord-\ning to the affinity score obtained from the training stage.\nWe design three affinity ranges for three different amplitude\nmotions. The maximum value of all three affinity ranges is\n1, achieved at the conditional frame. The minimum val-\nues are 0.2, 0.4, and 0.8, respectively, with corresponding\ndecreases in motion magnitude. We use classifier-free guid-\nance during the DDIM process [37] and set the classifier-\nfree guidance [15] scale as 7.5.\n7. AnimateBench\nAnimateBench is a comprehensive benchmark, which con-\nsists of 105 image and prompt pairs. To cover a wide va-\nriety of contents, styles, and concepts, we choose seven\nbase models [32] and LoRA [19]. An example case of Ani-\nmateBench is depicted in Fig. 11.\n7.1. Images in AnimateBench\nWe carefully choose seven of the most popular base models\n[32] and LoRAs [19] in Cvitai [6]. Each personalized model\nhas very distinct styles and we use them to curate images\nwith impressive high quality by tailored text prompts for\nFigure 11. AnimateBench case. Each curated personalized im-\nage corresponds to a personalized text-to-image model and three\ntailored motion-related text prompts.\nimage generation. Specifically, these images differ in styles,\ncontents, and concepts and ensure that AnimateBench cov-\ners three categories: people, animals, and landscapes.\n7.2. Prompts in AnimateBench\nFor each generated image, we design three prompts describ-\ning different motions to test the text alignment ability of\nmodels. Prompts are mainly composed of three parts: the\nsubject, the motion descriptor, and the trigger words.\nSubject and motion descriptors specify the content of mo-\ntion in the generated videos. The trigger word is a well-\nknown technique that is able to activate the DreamBooth\nor LoRA to generate personalized effects [6]. Only when\nthese prompts are included during inference, DreamBooth\nor LoRA can achieve optimal performance. Then we can\nget the complete prompt in AnimateBench. For example,\nwe use \u20181girl is smiling, white hair by atey ghailan, by greg\nrutkowski, by greg tocchini.\u2019 to generate a personalized im-\nage, and then we can get the complete prompt as \u20181girl is\nsmiling, white hair by atey ghailan, by greg rutkowski, by\ngreg tocchini\u2019. In this case, \u20181gril\u2019 represents the subject,\nsmiling represents the motion descriptor, and \u2018white hair\nby atey ghailan, by greg rutkowski, by greg tocchini\u2019 rep-\nresents the trigger word. We also distinguish motion be-\ntween different types of subjects. For example, the prompt\nof people or animals contains more descriptors such as smil-\ning, crying, etc, while the prompt of landscapes or scenes\ncontains more like raining, lightning, etc.\n8. Evaluation Details\n8.1. CLIP Score\nFollowing previous work [4, 11, 39], we compute CLIP\nscore to quantitatively evaluate the alignment in generated\nvideos. In addition to calculating text alignment, we mea-\nsure image alignment by computing the similarity between\nthe embeddings of the generated video frames and the in-\nput images. The two average CLIP scores are calculated on\nAnimateBench which contains 1680 frames. We leverage\nthe code provided by [14] and use ViT-B/32 [28] model to\nextract the embedding of images and prompts.\nFigure 12. User Study. Example of user study questionnaires.\n8.2. User Study\nFor user study, we randomly select input image and prompt\npairs in AnimateBench and then generate videos by using\nPIA, VideoComposer [39] and AnimateDiff [11, 33] with\nControlNet [44] and IP-Adapter [43]. We ask the partic-\nipants to choose from the three generated videos with the\nbest image alignment or text alignment in each question.\nWe show an illustration of the question cases in Fig. 12.\nThere are 20 questions in total, and the order of options is\nshuffled. A total of 20 participants were involved in the\nsurvey. Following the previous work [4], we calculated the\npreference rate, and the results are shown in the main paper.\n9. Ablation\nIn this section, we introduce more ablation studies to ver-\nify the effectiveness of the inter-frame affinity and the fine-\ntuning of temporal alignment layers.\n9.1. Inter-frame Affinity\nTo further verify the effectiveness of inter-frame affinity, we\ntrain a model without affinity hints for ablation study. We\nremove the affinity channel from the input of the condition\nmodule and the result is shown in Fig. 13. Compared to our\nmethod, videos generated by the model without inter-frame\naffinity are more incoherent and change suddenly.\n9.2. Fine-tuning Temporal Alignment Layers\nIn the training stage, we train both the condition module and\ntemporal alignment layers. We now show the result of only\ntraining the condition module with temporal alignment lay-\ners frozen in Fig. 14. The result proves that the frozen tem-\nporary alignment layers failed to align the condition frame\nwith other frames.\nFigure 13. Ablation study for Inter-frame Affinity. Without\nan affinity hint, the generated videos become incoherent and may\nchange significantly after the first given frame. With the inter-\nframe affinity as inputs, PIA is able to animate images that are\nfaithful to the condition frame.\nFigure 14. Ablation study for fine-tuning the Temporal Align-\nment Layers. Pre-trained temporal alignment layers fail to align\nthe condition frame in generated videos. PIA fine-tunes both the\ncondition module and the temporal alignment layers, leading to\nbetter preservation of the information in the condition frames.\n10. Visualization Results\n10.1. Visualization of Attention Map\nTo demonstrate that the motion alignment of PIA is better\nthan other methods, we visualize the average cross attention\nmap of motion descriptor token. We use prompt \u2018the rab-\nbit on is on fire\u2019 as an example and visualize the cross atten-\ntion map corresponding to token \u2018fire\u2019, as shown in Fig. 15.\nWe can observe that in our method, the region attended by\nthe \u2018fire\u2019 matches the region of flames. In contrast, the mo-\ntion descriptor token in the baseline method randomly at-\ntends to the entire context and cannot form a meaningful\nstructure. This phenomenon demonstrates that our method\nexhibits better motion alignment performance.\nFigure 15. Visualization of Cross-attention map. We generate\nthe video using prompt \u2018a rabbit is on fire\u2019 and visualize the cross-\nattention map corresponding to the token \u2018fire\u2019 for both Animate-\nDiff [11] and our own method. In PIA, token \u2018fire\u2019 shows more ac-\ncurate attention to the shape of flames, while in AnimateDiff, the\ntoken randomly attends to the entire context. This demonstrates\nthe superior motion alignment performance of our method.\nFigure 16. Using PIA to animate open-domain images.Without\nproviding personalized T2I models, PIA is able to animate the\nopen-domain images with realistic motions by text while preserv-\ning the details and identity in condition frame with IP-Adapter[43]\n10.2. PIA for Open-Domain Images\nIn this section, we further explore animating open-domain\nimages with PIA without using personalized T2I mod-\nels. To further enhance the preservation of the information\nand details of the condition frame, we combine the Image\nPrompt Adapter (IP-Adapter) [43] with PIA. Specifically,\nwe use a CLIP image encoder to extract features from the\ninput images. Then, these image features are incorporated\ninto each frame through the cross-attention mechanism in\nthe UNet. As shown in Fig. 16, without using personalized\nmodels, our model successfully animates an open-domain\nimage with realistic motion by text while preserving the\nidentity of the given image.\n"
  },
  {
    "title": "HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.14091.pdf",
    "upvote": "13",
    "text": "HD-Painter: High-Resolution and Prompt-Faithful\nText-Guided Image Inpainting with Diffusion Models\nHayk Manukyan1\u2217 Andranik Sargsyan1\u2217 Barsegh Atanyan1 Zhangyang Wang1,2\nShant Navasardyan1 Humphrey Shi1,3\n1Picsart AI Research (PAIR)\n2UT Austin\n3Georgia Tech\nhttps://github.com/Picsart-AI-Research/HD-Painter\nshirt\nflowers\nlion\ncastle\nFigure 1. High-resolution (the large side is 2048 in all these examples) text-guided image inpainting results with our approach. The method\nis able to faithfully fill the masked region according to the prompt even if the combination of the prompt and the known region is highly\nunlikely. Zoom in to view high-resolution details.\nAbstract\nRecent progress in text-guided image inpainting, based\non the unprecedented success of text-to-image diffusion\nmodels, has led to exceptionally realistic and visually plau-\nsible results. However, there is still significant potential\nfor improvement in current text-to-image inpainting models,\nparticularly in better aligning the inpainted area with user\nprompts and performing high-resolution inpainting. There-\nfore, we introduce HD-Painter, a training-free approach\n*Equal contribution.\nthat accurately follows prompts and coherently scales to\nhigh resolution image inpainting. To this end, we design\nthe Prompt-Aware Introverted Attention (PAIntA) layer en-\nhancing self-attention scores by prompt information result-\ning in better text aligned generations. To further improve\nthe prompt coherence we introduce the Reweighting Atten-\ntion Score Guidance (RASG) mechanism seamlessly inte-\ngrating a post-hoc sampling strategy into the general form\nof DDIM to prevent out-of-distribution latent shifts. More-\nover, HD-Painter allows extension to larger scales by intro-\nducing a specialized super-resolution technique customized\nfor inpainting, enabling the completion of missing regions\n1\narXiv:2312.14091v3  [cs.CV]  18 Mar 2024\nin images of up to 2K resolution. Our experiments demon-\nstrate that HD-Painter surpasses existing state-of-the-art\napproaches quantitatively and qualitatively across multiple\nmetrics and a user study. Code is publicly available at:\nhttps://github.com/Picsart-AI-Research/\nHD-Painter .\n1. Introduction\nThe recent wave of diffusion models [11, 34] has taken the\nworld by storm, becoming an increasingly integral part of\nour everyday lives. After the unprecedented success of text-\nto-image models [26, 27, 30, 37] diffusion-based image ma-\nnipulations such as prompt-conditioned editing [3, 10], con-\ntrollable generation [19, 43], personalized and specialized\nimage synthesis [9, 17, 29] became hot topics in computer\nvision leading to a huge amount of applications. Particu-\nlarly, text-guided image completion or inpainting [1, 36, 37]\nallows users to generate new content in user-specified re-\ngions of given images based on textual prompts (see Fig.\n1), leading to use cases like retouching specific areas of an\nimage, replacing or adding objects, and modifying subject\nattributes such as clothes, colors, or emotion.\nPretrained text-to-image generation models such as Sta-\nble Diffusion [27], Imagen [30], and Dall-E 2 [26] can be\nadapted for image completion by blending diffused known\nregions with generated (denoised) unknown regions during\nthe backward diffusion process. Although such approaches\n[1, 2] produce visually plausible completions, they are not\nwell harmonized and lack global scene understanding, es-\npecially when denoising in high diffusion timesteps.\nTo address this, existing methods [21, 23, 27, 30], mod-\nify pretrained text-to-image models to take additional con-\ntext information and fine-tune specifically for text-guided\nimage completion. GLIDE [21] and Stable Inpainting [27]\nconcatenate the mask and the masked image as additional\nchannels to the input of the diffusion UNet, initializing the\nnew convolutional weights with zeros, then fine tune the\nmodified model using random masks together with the ini-\ntial prompt.\nHowever, SmartBrush [38] and Imagen Editor [36] men-\ntion the weak image-text alignment of such models, attribut-\ning it to the random masking strategies, and the misalign-\nment of the global prompts used during training with the\nlocal context of the masked region. In this paper, we will ad-\ndress this issue as prompt neglect. To alleviate this problem,\nboth papers introduce novel, object-aware masking strate-\ngies. Additionally SmartBrush proposes BLIP captioning\napproach, to ensure a better alignment of the inpainting\nprompt with the masked region. Nonetheless, we find that\nwhile this approach reduces the amount of prompt neglect,\nit also decreases the generation quality Tab. 1.\nWe noticed that prompt neglect is commonly expressed\nin two ways: either the model fills in the masked region\nwith background (background dominance, Fig. 4, columns\n1, 3, 5), or the model completes a nearby object partially\noccluded by the mask (nearby object dominance, Fig. 4,\ncolumns 2, 4, 6). In both cases the issue seems to be caused\nby the model preferring the local context of the known re-\ngion to the textual information provided by the prompt.\nTo address the mentioned problems we introduce\nPrompt-Aware Introverted Attention (PAIntA) block without\nany training or fine-tuning requirements. PAIntA enhances\nthe self-attention scores according to the given textual con-\ndition aiming to decrease the impact of non-prompt-relevant\ninformation from the image known region while increasing\nthe contribution of the prompt-aligned known pixels.\nTo improve the text-alignment of the generation results\neven further we apply a post-hoc guidance mechanism by\nleveraging the cross-attention scores. However the vanilla\npost-hoc guidance mechanism used by seminal works such\nas [6, 7], etc.\nmay lead to generation quality degrada-\ntion due to out-of-distribution shifts caused by the addi-\ntional gradient term in the backward diffusion equation (see\nEq. (4)).\nTo this end we propose Reweighting Attention\nScore Guidance (RASG), a post-hoc mechanism seamlessly\nintegrating the gradient component in the general form of\nDDIM process. This allows to simultaneously guide the\nsampling towards more prompt-aligned latents and keep\nthem in their trained domain leading to visually plausible\ninpainting results.\nWith the combination of PAIntA and RASG our method\ngains a significant advantage over the current state-of-the-\nart approaches by solving the issue of prompt neglect. In ad-\ndition, by leveraging high-resolution diffusion models and\ntime-iterative blending technology we design a simple yet\neffective pipeline for up to 2048 \u00d7 2048 resolution inpaint-\ning.\nTo summarize, our main contributions are as follows:\n\u2022 We introduce the Prompt-Aware Introverted Attention\n(PAIntA) layer to alleviate the prompt neglect issues of\nbackground and nearby object dominance in text-guided\nimage inpainting.\n\u2022 To further improve the text-alignment of generation\nwe present the Reweighting Attention Score Guid-\nance (RASG) strategy which enables to prevent out-of-\ndistribution shifts while performing post-hoc guided sam-\npling.\n\u2022 Our designed pipeline for text-guided image comple-\ntion is training-free and demonstrates a significant ad-\nvantage over current state-of-the-art approaches quanti-\ntatively and qualitatively. Moreover, with the additional\nhelp of our simple yet effective inpainting-specialized\nsuper-resolution framework we make high-resolution (up\nto 2048 \u00d7 2048) image completion possible.\n2\n2. Related Work\n2.1. Image Inpainting\nImage inpainting is the task of filling missing regions of\nthe image in a visually plausible manner. Early deep learn-\ning approaches such as [20, 40, 41] introduce mechanisms\nto propagate deep features from known regions.\nLater\n[31, 39, 44, 45] utilize StyleGAN-v2-like [13] decoder and\ndiscriminative training for better image detail generation.\nImage inpainting also benefited from diffusion models,\nparticularly with the emergence of text-guided inpainting.\nGiven a pre-trained text-to-image diffusion model [1, 2]\nreplace the unmasked region of the latent by the noised\nversion of the known region during sampling. However,\nas noted by [21], this leads to poor generation quality, as\nthe denoising network only sees the noised version of the\nknown region. [21, 23, 36, 38] fine-tune pretrained text-\nto-image models for text-guided image inpainting by con-\nditioning the denoising model on the inpainting mask and\nthe known region, concatenating them with the input la-\ntents.\n[36, 38], in particular, use object-aware masking\nstrategies, to improve image-text alignment of training sam-\nples. Alternatively, [43] obtains an inpainting model by at-\ntaching trainable modules to the UNet, while keeping the\nbase model unchanged.\nWe propose a training-free ap-\nproach leveraging plug-and-play components PAIntA and\nRASG, improving text-prompt alignment. Moreover, our\napproach allows inpainting on high-resolution images (up\nto 2048 \u00d7 2048).\n2.2. Inpainting-Specific Architectural Blocks\nEarly deep learning approaches were designing special\nlayers for better/more efficient inpainting.\nParticularly,\n[16, 20, 42] introduce special convolutional layers dealing\nwith the known region of the image to effectively extract\nthe information useful for visually plausible image comple-\ntion. [40] introduces the contextual attention layer reduc-\ning the unnecessarily heavy computations of all-to-all self-\nattention for high-quality inpainting. In this work we pro-\npose Prompt-Aware Introverted Attention (PAIntA) layer,\nspecifically designed for text-guided image inpainting. It\naims to decrease (increase) the prompt-irrelevant (-relevant)\ninformation from the known region for better text aligned\ninpainting generation.\n2.3. Post-Hoc Guidance in Diffusion Process\nPost-hoc guidance methods are backward diffusion sam-\npling techniques which guide the next step latent predic-\ntion towards a specific objective function minimization.\nSuch approaches appear to be extremely helpful when gen-\nerating visual content especially with an additional con-\nstraint. Particularly [6] introduced classifier-guidance aim-\ning to generate images of a specific class.\nLater CLIP-\nguidance was introduced by [21] leveraging CLIP [25] as\nan open-vocabulary classification method. LDM [27] fur-\nther extends the concept to guide the diffusion sampling\nprocess by any image-to-image translation method, partic-\nularly guiding a low-resolution trained model to generate\n\u00d72 larger images. [4] guides image generation by maxi-\nmizing the maximal cross-attention score relying on multi-\niterative optimization process resulting in more text aligned\nresults. [7] goes even further by utilizing the cross-attention\nscores for object position, size, shape, and appearance guid-\nances. All the mentioned post-hoc guidance methods shift\nthe latent generation process by a gradient term (see Eq. (6))\nsometimes leading to image quality degradations.\nTo this end we propose the Reweighting Attention Score\nGuidance (RASG) mechanism allowing to perform post-\nhoc guidance with any objective function while preserv-\ning the diffusion latent domain. Specifically for inpaint-\ning task, to alleviate the issue of prompt neglect, we ben-\nefit from a guidance objective function based on the open-\nvocabulary segmentation properties of cross-attentions.\n3. Method\nWe first formulate the text-guided image completion prob-\nlem followed by an introduction to diffusion models, par-\nticularly Stable Diffusion ([27]) and Stable Inpainting. We\nthen discuss the overview of our method and its compo-\nnents.\nAfterwards we present our Prompt-Aware Intro-\nverted Attention (PAIntA) block and Reweighting Atten-\ntion Score Guidance (RASG) mechanism in detail. Lastly\nour inpainting-specific super-resolution technique is intro-\nduced.\nLet I \u2208 RH\u00d7W \u00d73 be an RGB image, M \u2208 {0, 1}H\u00d7W\nbe a binary mask indicating the region in I one wants to\ninpaint with a textual prompt \u03c4. The goal of text-guided\nimage inpainting is to output an image Ic \u2208 RH\u00d7W \u00d73 such\nthat Ic contains the objects described by the prompt \u03c4 in the\nregion M while outside M it coincides with I, i.e. Ic\u2299(1\u2212\nM) = I \u2299 (1 \u2212 M).\n3.1. Stable Diffusion and Stable Inpainting\nStable Diffusion (SD) is a diffusion model that functions\nwithin the latent space of an autoencoder D(E(\u00b7)) (VQ-\nGAN [8] or VQ-VAE [35]) where E denotes the encoder\nand D the corresponding decoder.\nSpecifically, let I \u2208\nRH\u00d7W \u00d73 be an image and x0\n= E(I), consider the\nfollowing forward diffusion process with hyperparameters\n{\u03b2t}T\nt=1 \u2282 [0, 1]:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI), t = 1, .., T (1)\nwhere q(xt|xt\u22121) is the conditional density of xt given\nxt\u22121, and {xt}T\nt=0 is a Markov chain.\nHere T is large\nenough to allow an assumption xT \u223c N(0, 1). Then SD\n3\nlearns a backward process (below similarly, {xt}0\nt=T is a\nMarkov chain)\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03c3t1), t = T, .., 1, (2)\nand hyperparameters {\u03c3t}T\nt=1, allowing the generation\nof a signal x0 from the standard Gaussian noise xT .\nHere \u00b5\u03b8(xt, t) is defined by the predicted noise \u03f5t\n\u03b8(xt)\nmodeled as a neural network (see [11]):\n\u00b5\u03b8(xt, t)\n=\n1\n\u221a\u03b2t\n\u0010\nxt \u2212\n\u03b2t\n\u221a1\u2212\u03b1t \u03f5t\n\u03b8(xt)\n\u0011\n. Then \u02c6I = D(x0) is returned.\nThe following claim can be derived from the main DDIM\nprinciple, Theorem 1 in [34].\nCLAIM 1 After training the diffusion backward process\n(Eq.\n2) the following {\u03c3t}T\nt=1-parametrized family of\nDDIM sampling processes can be applied to generate high-\nquality images:\nxt\u22121 = \u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(xt)\n\u221a\u03b1t\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u03f5t\n\u03b8(xt) + \u03c3t\u03f5t,\n(3)\nwhere \u03f5t \u223c N(0, 1), \u03b1t = Qt\ni=1(1 \u2212 \u03b2i), and 0 \u2264 \u03c3t \u2264\n\u221a1 \u2212 \u03b1t\u22121 can be arbitrary parameters.\nUsually (e.g. in SD or Stable Inpainting described be-\nlow) \u03c3t = 0 is taken to get a deterministic process:\nxt\u22121 = \u221a\u03b1t\u22121\n\u0012xt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(xt)\n\u221a\u03b1t\n\u0013\n+\np\n1 \u2212 \u03b1t\u22121\u03f5t\n\u03b8(xt), t = T, . . . , 1.\n(4)\nFor text-to-image synthesis, SD guides the processes with a\ntextual prompt \u03c4. Hence the function \u03f5t\n\u03b8(xt) = \u03f5t\n\u03b8(xt, \u03c4),\nmodeled by a UNet-like ([28]) architecture, is also con-\nditioned on \u03c4 by its cross-attention layers. For simplicity\nsometimes we skip \u03c4 in writing \u03f5t\n\u03b8(xt, \u03c4).\nAs mentioned earlier, Stable DIffusion can be modi-\nfied and fine-tuned for text-guided image inpainting. To\ndo so [27] concatenate the features of the masked image\nIM = I \u2299 (1 \u2212 M) obtained by the encoder E, and the\n(downscaled) binary mask M to the latents xt and feed\nthe resulting tensor to the UNet to get the estimated noise\n\u03f5t\n\u03b8([xt, E(IM), down(M)], \u03c4), where down is the down-\nscaling operation to match the shape of the latent xt. Newly\nadded convolutional filters are initialized with zeros while\nthe rest of the UNet from a pretrained checkpoint of Sta-\nble Diffusion. Training is done by randomly masking im-\nages and optimizing the model to reconstruct them based\non image captions from the LAION-5B ([32]) dataset. The\nresulting model shows visually plausible image completion\nand we refer to it as Stable Inpainting.\n3.2. HD-Painter: Overview\nThe overview of our method is presented in Fig. 2. The\nproposed pipeline is composed of two stages: text-guided\nimage inpainting on the resolution H/4 \u00d7 W/4 is applied\nfollowed by the inpainting-specific \u00d74 super-resolution of\nthe generated content.\nTo complete the missing region M according to the given\nprompt \u03c4 we take a pre-trained inpainting diffusion model\nlike Stable Inpainting, replace the self-attention layers by\nPAIntA layers, and perform a diffusion backward process\nby applying our RASG mechanism. After getting the final\nestimated latent x0, it is decoded resulting in an inpainted\nimage Ic\nlow = D(x0) \u2208 R\nH\n4 \u00d7 W\n4 .\nTo inpaint the original size image I \u2208 RH\u00d7W we uti-\nlize the super-resolution stable diffusion from [27].\nWe\napply the diffusion backward process of SD starting from\nXT \u223c N(0, 1) and conditioned on the low resolution in-\npainted image Ic\nlow.\nAfter each step we blend the de-\nnoised Xpred\n0\nwith the original image\u2019s encoding E(I) in the\nknown region indicated by the mask (1\u2212M) \u2208 {0, 1}H\u00d7W\nand derive the next latent Xt\u22121 by Eq. 4. After the final step\nwe decode the latent by D(X0) and use Poisson blending\n([22]) with I to avoid edge artifacts.\n3.3. Prompt-Aware Introverted Attention (PAIntA)\nThroughout our experiments we noticed that existing ap-\nproaches, such as Stable Inpainting, tend to ignore the user-\nprovided prompt relying more on the visual context around\nthe inpainting area. In the introdution we categorized this is-\nsue into two classes based on user experience: background\ndominance and nearby object dominance. Indeed, for ex-\nample in Fig. 4, rows 1, 3, 4, the existing solutions (besides\nBLD) fill the region with background, and in rows 5, 6, they\nprefer to continue the animal and the car instead of generat-\ning a boat and flames respectively. We hypothesize that the\nvisual context dominance over the prompt is attributed to\nthe prompt-free, only-spatial nature of self-attention layers.\nTo support this we visualize the self-attention scores (see\nAppendix) and observe a high similarity between the in-\npainted tokens and such known tokens of the image which\nhave low similarity with the prompt (for more details see\nAppendix). Therefore, to alleviate the issue, we introduce a\nplug-in replacement for self-attention, Prompt-Aware Intro-\nverted Attention (PAIntA, see Fig. 3 (a)) which utilizes the\ninpainting mask M and cross-attention matrices to control\nthe self-attention output in the unknown region. Below we\ndiscuss PAIntA in detail.\nLet X \u2208 R(h\u00d7w)\u00d7d be the input tensor of PAIntA.\nSimilar to self-attention, PAIntA first applies projection\nlayers to get the queries, keys, and values we denote by\nQs, Ks, Vs \u2208 R(h\u00d7w)\u00d7d respectively, and the similarity\nmatrix Aself =\nQsKT\ns\n\u221a\nd\n\u2208 Rhw\u00d7hw.\nThen we mitigate\n4\nFigure 2. Our method has two stages: image completiton, and inpainting-specialized super-resolution (\u00d74). For image completion\nin each diffusion step we denoise the latent xt by conditioning on the inpainting mask M and the masked downscaled image IM =\ndown(I) \u2299 (1 \u2212 M) \u2208 R\nH\n4 \u00d7 W\n4 \u00d73 (encoded with the VAE encoder E). To make better alignement with the given prompt our PAIntA\nblock is applied instead of self-attention layers. After predicting the denoised xpred\n0\nin each step t, we provide it to our RASG guidance\nmechanism to estimate the next latent xt\u22121. For inpainting-specific super resolution we condition the high-resolution latent Xt denoising\nprocess by the lower resolution inpainted result Ic\nlow, followed by blending Xpred\n0\n\u2299 M + E(I) \u2299 (1 \u2212 M). Finally we get Ic by Poisson\nblending the decoded output with the original image I.\nthe too strong influence of the known region over the un-\nknown by adjusting the attention scores of known pixels\ncontributing to the inpainted region. Specifically, leverag-\ning the prompt \u03c4, PAIntA defines a new similarity matrix:\n\u02dcAself \u2208 Rhw\u00d7hw,\n( \u02dcAself)ij =\n(\ncj \u00b7 (Aself)ij\nMi = 1 and Mj = 0,\n(Aself)ij\notherwise,\n(5)\nwhere cj shows the alignment of the jth feature token\n(pixel) with the given textual prompt \u03c4.\nWe define {cj}hw\nj=1 using the cross-attention spatio-\ntextual similarity matrix Scross = SoftMax(QcKT\nc /\n\u221a\nd),\nwhere Qc \u2208 R(h\u00d7w)\u00d7d, Kc \u2208 Rl\u00d7d are query and key\ntensors of corresponding cross-attention layers, and l is\nthe number of tokens of the prompt \u03c4.\nSpecifically, we\nconsider CLIP text embeddings of the prompt \u03c4 and sep-\narate the ones which correspond to the words of \u03c4 and\nEnd of Text (EOT) token (in essence we just disregard the\nSOT token and the null-token embeddings), and denote the\nset of chosen indices by ind(\u03c4) \u2282 {1, 2, . . . , l}. We in-\nclude EOT since (in contrast with SOT) it contains infor-\nmation about the prompt \u03c4 according to the architecture\nof CLIP text encoder.\nFor each jth pixel we define its\nsimilarity with the prompt \u03c4 by summing up it\u2019s similarity\nscores with the embeddings indexed from ind(\u03c4), i.e. cj =\nP\nk\u2208ind(\u03c4)(Scross)jk. Also, we found beneficial to normal-\nize the scores cj = clip\n\u0010\ncj\u2212median(ck; k=1,...,hw)\nmax(ck; k=1,...,hw)\n, 0, 1\n\u0011\n,\nwhere clip is the clipping operation between [0, 1].\nNote that in vanilla SD cross-attention layers come after\nself-attention layers, hence in PAIntA to get query and key\ntensors Qc, Kc we borrow the projection layer weights from\nthe next cross-attention module (see Fig. 2). Finally we get\nthe output of the PAIntA layer with the residual connection\nwith the input: Out = X + SoftMax( \u02dcAself) \u00b7 Vs.\n3.4.\nReweighting\nAttention\nScore\nGuidance\n(RASG)\nTo further enhance the generation alignment with the\nprompt \u03c4 we adopt a post-hoc sampling guidance mech-\nanism [6] with an objective function S(x) leveraging the\nopen-vocabulary segmentation properties of cross-attention\nlayers. Specifically1 at each step the following update rule\nis used after predicting the noise \u03f5t\n\u03b8(xt): \u02c6\u03f5t\n\u03b8(xt) \u2190 \u03f5t\n\u03b8(xt)+\n\u221a1 \u2212 \u03b1t \u00b7s\u2207xtS(xt), where s is a hyperparameter control-\nling the amount of the guidance. However, as also noted by\n[4], vanilla post-hoc guidance may shift the domain of dif-\nfusion latents xt\u22121 resulting in image quality degradations.\nIndeed, according to the (deterministic) DDIM process (Eq.\n4) after substituting \u03f5t\n\u03b8(xt) with \u02c6\u03f5t\n\u03b8(xt) we get\nxt\u22121 = \u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(xt)\n\u221a\u03b1t\n+\np\n1 \u2212 \u03b1t\u22121\u03f5t\n\u03b8(xt) \u2212 \u03bet\u2207xtS(xt),\n\u03bet =\n\u221a\n1 \u2212 \u03b1t \u00b7 s\n\u0012\u221a1 \u2212 \u03b1t\u221a\u03b1t\u22121\n\u221a\u03b1t\n\u2212\np\n1 \u2212 \u03b1t\u22121\n\u0013\n,\n(6)\nhence in Eq. 4 we get the additional term \u2212\u03bet\u2207xtS(xt)\nwhich may shift the original distribution of xt\u22121.\nTo this end we introduce the Reweighting Attention Score\nGuidance (RASG) strategy which benefits from the general\nDDIM backward process (Eq. 3) and introduces a gradient\nreweighting mechanism resulting in latent domain preserva-\n1for brevity: \u03f5t\n\u03b8(xt) = \u03f5t\n\u03b8([xt, E(IM), down(M)], \u03c4).\n5\nFigure 3. (a) PAIntA block takes an input tensor X \u2208 Rh\u00d7w\u00d73 and the CLIP embeddings of \u03c4. After computing the self- and cross-\nattention scores Aself and Across, we update the former (Eq. 5) by scaling with the normalized values {cj}hw\nj=1 obtained from Scross =\nSoftMax(Across). Finally the the updated attention scores \u02dcAself are used for the convex combination of the values Vs to get the residual\nof PAIntA\u2019s output. (b) RASG mechanism takes the predicted scaled denoised latent \u221a\u03b1t\u22121xpred\n0\n=\n\u221a\u03b1t\u22121\n\u221a\u03b1t\n\u0000xt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt)\n\u0001\nand\nguides the xt\u22121 estimation process towards minimization of S(xt) defined by Eq. 9. Gradient reweighting makes the gradient term close\nto being sampled from N(0, 1) (green area) by so ensuring the domain preservation (blue area).\ntion. Specifically, according to Claim 1, xt\u22121 obtained ei-\nther by Eq. 4 or by Eq. 3 will be in the required domain (see\nFig. 3). Hence if in Eq. 3 we replace the stochastic compo-\nnent \u03f5t by the rescaled version of the gradient \u2207xtS(xt) (to\nmake it closer to a sampling from N(0, 1)), we will keep\nxt\u22121 in the required domain and at the same time will guide\nits sampling towards minimization of S(xt). Rescaling of\nthe gradient \u2207xtS(xt) is done by dividing it on its stan-\ndard deviation (we do not change the mean to keep the di-\nrection of the S(xt) minimization, for more discussion see\nAppendix). Thus, RASG sampling is done by the formula\nxt\u22121 = \u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt)\n\u221a\u03b1t\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u03f5\u03b8(xt) + \u03c3t\n\u2207xtS(xt)\nstd(\u2207xtS(xt)).\n(7)\nNow let us define the function S(xt) (for more discussion\non its choice see Appendix). First we consider all cross-\nattention maps Across with the output resolution of H\n32 \u00d7\nW\n32 : A1\ncross, . . . , Am\ncross \u2208 R(H/32\u00b7W/32)\u00d7l, where m is the\nnumber of such cross-attention layers, and l is the number of\ntoken embeddings. Then for each k \u2208 ind(\u03c4) \u2282 {1, . . . , l}\nwe average the attention maps and reshape to H\n32 \u00d7 W\n32 :\nA\nk\ncross(xt) = 1\nm\nm\nX\ni=1\nAi\ncross[:, k] \u2208 R\nH\n32 \u00d7 W\n32 .\n(8)\nUsing post-hoc guidance with S(xt) we aim to maximize\nthe attention scores in the unknown region determined by\nthe binary mask M \u2208 {0, 1}H\u00d7W , hence we take the av-\nerage binary cross entropy between A\nk(xt) and M (M is\ndownscaled with NN interpolation, \u03c3 here is sigmoid):\nS(xt) = \u2212\nX\nk\u2208ind(\u03c4)\nH\n32 \u00b7 W\n32\nX\ni=1\n[Mi log \u03c3(A\nk\ncross(xt)i)+\n(1 \u2212 Mi) log(1 \u2212 \u03c3(A\nk\ncross(xt)i))].\n(9)\n3.5.\nInpainting-Specialized\nConditional\nSuper-\nResolution\nHere we discuss our method for high-resolution inpaint-\ning utilizing a pre-trained diffusion-based super-resolution\nmodel. We leverage the fine-grained information from the\nknown region to upscale the inpainted region (see Fig. 2.).\nRecall that I \u2208 RH\u00d7W \u00d73 is the original high-resolution\nimage we want to inpaint, and E is the encoder of VQ-\nGAN [8]. We consider X0 = E(I) and take a standard\nGaussian noise XT \u2208 R\nH\n4 \u00d7 W\n4 \u00d74. Then we apply a back-\nward diffusion process (Eq. 4) on XT by using the upscale-\nspecialized SD model and conditioning it on the low res-\nolution inpainted image Ic\nlow.\nAfter each diffusion step\nwe blend the estimated denoised latent Xpred\n0\n= (Xt \u2212\n\u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(Xt))/\u221a\u03b1t with X0 by using M:\nXpred\n0\n\u2190 M \u2299 Xpred\n0\n+ (1 \u2212 M) \u2299 X0,\n(10)\nand use the new Xpred\n0\nto determine the latent Xt\u22121 (by\nEq. 4). After the last diffusion step Xpred\n0\nis decoded and\n6\nblended (Poisson blending) with the original image I.\nIt\u2019s worth noting that our blending approach is inspired\nby seminal works [1, 33] blending Xt with the noisy la-\ntents of the forward diffusion. In contrast, we blend high-\nfrequencies from X0 with the denoised prediction Xpred\n0\nal-\nlowing noise-free image details propagate from the known\nregion to the missing one during all diffusion steps.\n4. Experiments\n4.1. User Study\nWe also performed a user study for a qualitative comparison\nwith the competitor state-of-the art methods. The 12 partic-\nipants were shown 20 (image, mask, prompt) triplets and the\ninpainting results of all methods in random order. For each\nsample image we asked to select the best results based on\n(i) prompt alignment and (ii) overall quality, allowing the\nchoice of no methods when all methods were bad, or multi-\nple methods when the quality was similar. We calculate the\ntotal votes for all methods for each question. The results are\npresented in Fig. 5 demonstrating a clear advantage of our\nmethod in both aspects over all competitor methods.\n4.2. Implementation Details\nWe use DreamShaper 8 [18] version of Stable Inpaint-\ning and Stable Super-Resolution 2.0 as image comple-\ntion and inpainting-specialized super-resolution baselines\nrespectively. PAIntA is used to replace the self attention lay-\ners on the H/32\u00d7W/32 and H/16\u00d7W/16 resolutions for\nthe first half of generation steps. For RASG we select only\ncross-attention similarity matrices of the H/32\u00d7W/32 res-\nolution since utilizing higher resolutions did not offer sig-\nnificant improvements.\nFor hyperparameters {\u03c3t}T\nt=1 we chose\n\u03c3t = \u03b7\np\n(1 \u2212 \u03b1t\u22121)/(1 \u2212 \u03b1t)\np\n1 \u2212 \u03b1t/\u03b1t\u22121, \u03b7 = 0.1\n4.3. Experimental Setup\nHere we compare with existing state-of-the-art meth-\nods such as GLIDE [21], Stable 2.0 Inpainting2 [27],\nDreamShaper Inpainting [18], Blended Latent Diffusion\n(BLD) [2], ControlNet-Inpainting3 [43] (with DreamShaper\n4 base), SDXL-Inpainting5 [23] and SmartBrush [38]. As\nauthors of the SmartBrush paper don\u2019t provide code and\nmodel, we reproduce it according to paper and refer to\nit as SmartBrush reprod..\nSpecifically, we build Smart-\nBrush reprod. based on DreamShaper text-to-image model\n2https : / / huggingface . co / stabilityai / stable -\ndiffusion-2-inpainting\n3https : / / huggingface . co / lllyasviel / control _\nv11p_sd15_inpaint\n4https://huggingface.co/Lykon/dreamshaper-8\n5https : / / huggingface . co / spaces / diffusers /\nstable-diffusion-xl-inpainting\nin order to have fair comparison with our method, which\nuses DreamShaper Inpainting as a baseline. We evaluate\nthe methods on a random sample of 10000 (image, mask,\nprompt) triplets from the validation set of MSCOCO 2017\n[15], where the prompt is chosen as the label of the selected\ninstance mask. We noticed that when a precise mask of a\nrecognizable shape is given to Stable Inpainting, it tends to\nignore the prompt and inpaint based on the shape. To pre-\nvent this, we use the convex hulls of the object segmentation\nmasks and compute the metrics accordingly.\nWe evaluate the CLIP score on a cropped region of\nthe image using the bounding box of the input mask. As\nCLIP score can still assign high scores to adversarial ex-\namples, we additionally compute the generation class accu-\nracy. So, we utilize a pre-trained instance detection model\nfor MSCOCO: MMDetection [5]. We run it on the cropped\narea of the generated image, and, as there might be more\nthan one objects included in the crop, we treat the example\nas positive if the prompt label is in the detected object list.\nTo measure the visual fidelity of the results we employ\nthe LAION aesthetic score.6 The aesthetic score is com-\nputed by an MLP trained on 5000 image-rating pairs from\nthe Simulacra Aesthetic Captions dataset [24], and can be\nused to assign a value from the [0, 10] range to images based\non their aesthetic appeal.\nFinally, we employ PickScore [14] as a combined met-\nric of text-alignment and visual fidelity. Being trained on\nreal user feedback PickScore is able to not only assess the\nprompt-faithfulness of inpainting methods but also the gen-\neration quality, while reflecting the complex requirements\nof users. In our setting we apply PickScore between our vs\nother methods results and compute the percentage when it\ngives the advantage to our.\n4.4. Quantitative and Qualitative Analysis\nTable 1 shows that our method outperforms the competi-\ntors in all metrics.\nIt can be noticed that while Smart-\nBrush trained over DreamShaper Inpainting improves the\naccuracy over the baseline, the CLIP score improvement is\nmarginal and the overall quality is significantly dropped ac-\ncording to aesthetic score. On the other hand, our method\nsignificantly improves the prompt-alignment as measured\nby both CLIP score and accuracy while also maintaining\nthe quality.\nThe examples in Fig. 4 demonstrate qualitative compari-\nson between our method and the other state-of-the-art ap-\nproaches. In many cases the baseline DreamShaper Inp.\ngenerates a background (Fig. 4, columns 1, 3, 5) or recon-\nstructs the missing regions as continuation of the known re-\ngion objects disregarding the prompt (Fig. 4, columns 4, 6,\n7), while our method, thanks to the combination of PAIntA\n6https : / / github . com / christophschuhmann /\nimproved-aesthetic-predictor\n7\nSmartBrush\nreprod.\nDreamShaper\nInpainting\nOurs\nMasked\nwooden boat\nflames\nwhite duck\nswimming\nin water\nantique greek\nvase\nsunglasses\nGLIDE\nBLD\nStable 2.0\nInpainting\nSDXL\nInpainting\nDreamshaper-\nControlNet\nInpainting\nflowers\nshirt\nFigure 4. Comparison with state-of-the-art text-guided inpainting methods. Zoom in for details. For more comparison see Appendix.\n8\nModel Name\nCLIP score \u2191\nAccuracy \u2191\nAesthetic score \u2191\nPickScore\n(Ours vs Baselines) \u2193\nGLIDE [21]\n25.14\n43.39 %\n4.48\n57.81 %\nBLD [2]\n24.23\n49.12 %\n4.81\n55.35 %\nSDXL Inpainting [23]\n24.79\n53.94 %\n4.69\n58.69 %\nStable 2.0 Inpainting [27]\n24.86\n51.38 %\n4.88\n55.64 %\nDreamShaper-ControlNet Inp. [43]\n25.73\n58.92 %\n4.95\n54.69 %\nSmartBrush reprod. [38]\n25.79\n66.36 %\n4.85\n54.23 %\nDreamShaper Inpainting [18]\n25.62\n59.02 %\n4.96\n51.98 %\nOurs\n26.25\n67.59 %\n5.00\n50.0 %\nTable 1. Quantitative comparison.\nOurs\nDreamShaper \nInpainting\nSmartBrush\nreprod.\nBLD\nDreamShaper\nControlNet\nInpainting\nStable 2.0\nInpainting\nGLIDE\nSDXL-\nInpainting\n0\n50\n100\n150\n200\nTotal Votes\n193\n153\n146\n120\n115\n58\n16\n9\n146\n110\n34\n25\n59\n35\n4\n5\nprompt alignment\noverall quality\nFigure 5. Total votes of each method based on our user study for prompt alignment and overall quality. Our method HD-Painter has a clear\nadvantage over all competitors.\nand RASG, successfully generates the target objects. No-\ntice that even though DreamShaper-ControlNet-Inpainting\nand SmartBrush reprod. may also generate the required ob-\nject, the quality of the generation is poor compared to ours.\nAdditionally, Fig. 1 demonstrates how effective our\ninpainting-specialized super-resolution is in seamlessly\nleveraging known region details for upscaling the generated\nregion. We show more results in our Appendix, as well as\ncomparison with vanilla Stable Super-Resolution approach\n[27] used as an upscaling method after inpainting.\n4.5. Ablation Study\nIn Tab. 2 we show that PAIntA and RASG separately on\ntheir own provide substantial improvements to the model\nquantitatively. We also provide more discussion on each\nof them in our supplementary material, including thorough\nanalyses on their impact, demonstrated by visuals.\n5. Conclusion\nIn this paper, we introduced a training-free approach to\ntext-guided high-resollution image inpainting, addressing\nthe prevalent challenges of prompt neglect: background\nand nearby object dominance.\nOur contributions, the\nPrompt-Aware Introverted Attention (PAIntA) layer and the\nModel Name\nCLIP\nscore \u2191\nAccuracy \u2191\nAesthetic\nscore\n\u2191\nbase (DreamShaper Inp.)\n25.62\n59.02 %\n4.96\nonly RASG\n25.82\n62.67 %\n4.97\nonly PAIntA\n26.06\n64.17 %\n4.99\nRASG & PAIntA\n26.25\n67.59 %\n5.00\nTable 2. Ablation study for PAIntA and RASG.\nReweighting Attention Score Guidance (RASG) mecha-\nnism, effectively mitigate the mentioned issues leading our\nmethod to surpass the existing state-of-the-art approaches\nqualitatively and quantitatively. Additionally, our unique\ninpainting-specific super-resolution technique offers seam-\nless completion in high-resolution images, distinguishing\nour method from existing solutions.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022.\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics (TOG), 42\n(4):1\u201311, 2023.\n9\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n[4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023.\n[5] K Chen, J Wang, J Pang, Y Cao, Y Xiong, X Li, S Sun,\nW Feng, Z Liu, J Xu, et al. Mmdetection: Open mmlab\ndetection toolbox and benchmark. arxiv 2019. arXiv preprint\narXiv:1906.07155, 2019.\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021.\n[7] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023.\n[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021.\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022.\n[10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022.\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020.\n[12] Itseez. Open source computer vision library. https://\ngithub.com/itseez/opencv, 2015.\n[13] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020.\n[14] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023.\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014.\n[16] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n85\u2013100, 2018.\n[17] Haoming Lu,\nHazarapet Tunanyan,\nKai Wang,\nShant\nNavasardyan, Zhangyang Wang, and Humphrey Shi. Spe-\ncialist diffusion: Plug-and-play sample-efficient fine-tuning\nof text-to-image diffusion models to learn any unseen style.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14267\u201314276, 2023.\n[18] Lykon. Dreamshaper-8 inpainting. https://civitai.\ncom / models / 4384 ? modelVersionId = 131004,\n2023.\n[19] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023.\n[20] Shant Navasardyan and Marianna Ohanyan. Image inpaint-\ning with onion convolutions.\nIn proceedings of the asian\nconference on computer vision, 2020.\n[21] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021.\n[22] Patrick P\u00b4erez, Michel Gangnet, and Andrew Blake. Poisson\nImage Editing. Association for Computing Machinery, New\nYork, NY, USA, 1 edition, 2023.\n[23] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023.\n[24] John David Pressman, Katherine Crowson, and Simu-\nlacra Captions Contributors. Simulacra aesthetic captions.\nTechnical Report Version 1.0, Stability AI, 2022.\nurl\nhttps://github.com/JD-P/simulacra-aesthetic-captions .\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022.\n[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022.\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015.\n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\n10\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023.\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022.\n[31] Andranik Sargsyan, Shant Navasardyan, Xingqian Xu, and\nHumphrey Shi. Mi-gan: A simple baseline for image in-\npainting on mobile devices. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n7335\u20137345, 2023.\n[32] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022.\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021.\n[35] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017.\n[36] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-\nTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah\nLaszlo, David J Fleet, Radu Soricut, et al. Imagen editor\nand editbench: Advancing and evaluating text-guided im-\nage inpainting. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 18359\u2013\n18369, 2023.\n[37] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In European con-\nference on computer vision, pages 720\u2013736. Springer, 2022.\n[38] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun\nZhang. Smartbrush: Text and shape guided object inpainting\nwith diffusion model. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22428\u201322437, 2023.\n[39] Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, An-\ndranik Sargsyan, Yadong Mu, and Humphrey Shi. Image\ncompletion with heterogeneously filtered spectral hints. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pages 4591\u20134601, 2023.\n[40] Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, and Zhan\nXu. Contextual residual aggregation for ultra high-resolution\nimage inpainting.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n7508\u20137517, 2020.\n[41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505\u20135514,\n2018.\n[42] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 4471\u20134480, 2019.\n[43] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023.\n[44] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image comple-\ntion via co-modulated generative adversarial networks. arXiv\npreprint arXiv:2103.10428, 2021.\n[45] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli\nShechtman, Connelly Barnes, Jianming Zhang, Ning Xu,\nSohrab Amirghodsi, and Jiebo Luo. Cm-gan: Image inpaint-\ning with cascaded modulation gan and object-aware training.\narXiv preprint arXiv:2203.11947, 2022.\n11\nAppendix A. Extended Qualitative Compari-\nson\nHere in Fig. 15 we show more visual comparison with the\nother state-of-the-art methods. Fig. 16 includes more com-\nparison on the validation set of MSCOCO 2017 [15]. The\nresults show the advantage of our method over the baselines.\nIn Fig. 6 we compare our inpainting-specialized super-\nresolution method with vanilla approaches of Bicubic or\nStable Super-Resolution-based upscaling of the inpainting\nresults followed by Poisson blending in the unknown re-\ngion. We can clearly see that our method, leveraging the\nknown region fine-grained information, can seamlessly fill\nin with high quality. In Figures 17 and 18 we show more\nvisual comparison between our method and the approach of\nStable Super-Resolution.\nAppendix B. Discussion on PAIntA\nIn this section we discuss the effectiveness of the proposed\nPAIntA module as a plug-in replacement for self-attention\n(SA) layers. To that end, first we visualize SA similarity\nmaps averaged across masked locations from resolutions\nH/16 \u00d7 W/16 and H/32 \u00d7 W/32 where PAIntA is ap-\nplied (see Fig.\n8).\nThen, we see that PAIntA success-\nfully scales down the similarities of masked locations with\nprompt-unrelated locations from the known region, and, as\na result, a prompt-specified object is generated inside the\nmask.\nFor a given resolution (H/16\u00d7W/16 or H/32\u00d7W/32),\nin order to visualize the average SA similarity map across\nmasked pixels, first we resize the input mask to match the\ndimensions of the corresponding resolution (we use near-\nest interpolation in resize operation). Then, for each SA\nlayer in the given resolution, we form a 2D similarity map\nby reshaping and averaging the similarity matrix rows cor-\nresponding to the masked region. Further, we average ob-\ntained 2D similarity maps across all SA layers (of the given\nresolution) and diffusion timesteps. More specifically, if\nA1\nself, . . . , AL\nself \u2208 Rhw\u00d7hw (h\u00d7w is either H/16\u00d7W/16\nor H/32 \u00d7 W/32) are the self-attention matrices of Stable\nInpainting layers of the given resolution, and, respectively,\nare being updated by PAIntA to the matrices \u02dcAiself (see Eq.\n5), then we consider the following similarity maps:\nA =\n1\n|M| \u00b7 L\nX\ni,Mi=1\nL\nX\nl=1\n(Al\nself)i \u2208 Rhw,\n\u02dcA =\n1\n|M| \u00b7 L\nX\ni,Mi=1\nL\nX\nl=1\n( \u02dcAl\nself)i \u2208 Rhw,\nand reshape them to 2D matrices of size h\u00d7w. So, Aij and\n\u02dcAij show the average amount in which masked pixels attend\nto to other locations in the cases of the vanilla self-attention\nand PAIntA respectively. Finally, in order to visualize the\nsimilarity maps, we use bicubic resize operation to match it\nwith the image dimensions and plot the similarity heatmap\nusing JET colormap from OpenCV [12].\nNext, we compare the generation results and correspond-\ning similarity maps obtained from above procedure when\nPAIntA\u2019s SA scaling is (the case of \u02dcA) or is not (the case\nof A) used. Because PAIntA\u2019s scaling is only applied on\nH/32 \u00d7 W/32 and H/16 \u00d7 W/16 resolutions, we are in-\nterested in those similarity maps.\nRows 1-3 in Fig.\n8\ndemonstrate visualizations on nearby object dominance is-\nsue (when known objects are continued to the inpainted re-\ngion while ignoring the prompt) of the vanilla diffusion in-\npainting, while rows 4-6 demonstrate those of with back-\nground dominance issue (when nothing is generated, just\nthe background is coherently filled in).\nFor example, on row 1, Fig. 8 in case of DreamShaper\nInpainting without PAIntA generation, the average similar-\nity of the masked region is dominated by the known regions\nof the car on both 16 and 32 resolutions. Whereas, as a re-\nsult of PAIntA scaling application, the average similarity of\nthe masked region with the car is effectively reduced, and\nthe masked region is generated in accordance to the input\nprompt.\nRow 4, Fig. 8 demonstrates an example where the re-\nsult without PAIntA continues the background based on vi-\nsual context instead of following the user prompt. In this\ncase, visualization shows that usage of PAIntA success-\nfully reduces the similarity of the masked region with the\nunrelated background. As a result, by reducing the simi-\nlarity of masked region with the unrelated known regions\nPAIntA enables prompt-faithful generation. You can find\nadditional examples of PAIntA\u2019s effect on the final genera-\ntion in Fig. 7.\nAppendix C. Discussion on RASG\nIn this section we discuss the choice of RASG objective\nguidance function S(x), then demonstrate the effect of\nRASG and motivate the part of gradient reweighting by its\nstandard deviation. Finally, we present additional examples\nof RASG\u2019s effect on the final generation in Fig. 9.\nAppendix C.1. The Objective Function S(x)\nAs we already mentioned in the main paper, Stable Inpaint-\ning may fail to generate certain objects in the prompt, com-\npletely neglecting them in the process. We categorized these\ncases into two types, namely background and nearby ob-\nject dominance issues. [4] also mentions these issues but\nfor text-to-image generation task, and refers them as catas-\ntrophic neglect problem. To alleviate this problem [4] pro-\npose a mechanism called generative semantic nursing, al-\nlowing the users to \u201cboost\u201d certain tokens in the prompt,\n12\nBicubic\nStable Super-Resolution\nOurs\nInput\nbench\nsailor hat\nFigure 6. Comparison of our inpainting-specialized super-resolution approach with vanilla upscaling methods for inpainting. Best viewed\nwhen zoomed in.\ninput\nDreamShaper\nInpainting\nwith PAIntA\nwhite duck\nswimming\nin water\nwine botttle\nclock\nboots\nmoon\nFigure 7. Visual ablation of PAIntA. Generated images use the same seed. In row 3 only PAIntA is used.\nensuring their generation. In essence the mechanism is a\npost-hoc guidance with a chosen objective function maxi-\nmizing the maximal cross-attention score of the image with\nthe token which should be \u201cboosted\u201d. This approach can be\neasily adapted to the inpainting task by just restricting the\nmaximum to be taken in an unknown region so that the ob-\nject is generated there, and averaging the objectives across\nall tokens, since we don\u2019t have specific tokens to \u201cboost\u201d,\nbut rather care about all of them. In other words, by our\nnotations from the main paper, the following guidance ob-\n13\nwith PAIntA\nwithout PAIntA \n(vanilla DreamShaper Inpainting)\ninput\nflames\nresult\nresult\nshirt\nsailor hat\nantique greek vase\nwhite robot\nSA map\nSA map\nSA map\nSA map\nflowers\nFigure 8. Comparison of self-attention similarity maps averaged across masked pixels for generations without/with PAIntA\u2019s scaling of the\noriginal self-attention scores. Images are generated from the same seed.\njective funciton can be used:\nS(xt) = \u2212\n1\n|ind(\u03c4)|\nX\nk\u2208ind(\u03c4)\nmax\ni: Mi=1{A\nk(xt)i}.\n(11)\nHowever\nwe\nnoticed\nthat\nwith\nthis\napproach\nthe\nshapes/sizes of generated objects might not be suffi-\nciently aligned with the shape/size of the input mask,\nwhich is often desirable for text-guided inpainting (see Fig.\n11).\nTherefore, we utilize the segmentation property of\ncross-attention similarity maps, by so using Binary Cross\nEntropy as the energy function for guidance (see Eq. 9 in\nthe main paper). As can be noticed from Fig. 11 the results\nwith the binary cross-entropy better fit the shape of the\ninpaining mask.\nAppendix C.2. Effect of RASG Strategy\nAlthough the objective function S(x) defined by Eq.\n9\n(main paper) results in better mask shape/size aligned in-\n14\ninput\nDreamShaper\nInpainting\nwith RASG\nwooden boat\nclock\nwhite fox\nspace helmet\nantique\ngreek vase\nFigure 9. Visual ablation of RASG. Generated images use the same seed. In row 3 only RASG is used.\npainting, the vanilla post-hoc guidance may lead the latents\nto become out of their trained domain as also noted by [4]:\n\u201cmany updates of xt may lead to the latent becoming out-\nof-distribution, resulting in incoherent images\u201d. Due to this\nthe post-hoc guidance mechanism (semantic nursing) by\n[4] is done using multiple iterations of very small, iterative\nperturbations of xt, which makes the process considerably\nslow. In addition, the generation can still fail if the itera-\ntive process exceeds the maximum iteration limit without\nreaching the necessary thresholds.\nThanks to RASG\u2019s seamless integration of the \u2207xtS(xt)\ngradient component into the general form of DDIM diffu-\nsion sampling, our RASG mechanism keeps the modified\nlatents xt within the expected distribution, while introduc-\ning large enough perturbations to xt with only one iteration\nof guidance per time-step. This allows to generate the ob-\njects described in the prompts coherently with the known\nregion without extra-cost of time.\nFig. 10 demonstrates the advantage of RASG\u2019s strategy\nover the vanilla guidance mechanism. Indeed, in the vanilla\npost-hoc guidance there is a hyperparameter s controlling\nthe amount of guidance. When s is too small (e.g. close to\n0 or for some cases s = 100) the vanilla guidance mech-\nanism does not show much effect due to too small guid-\nance from s\u2207xtS(xt). Then with increasing the hyperpa-\nrameter (s = 1000, 10000) one can notice more and more\ntext/shape alignment with prompt/inpainting mask, how-\never the generated results are unnatural and incoherent with\nthe known region. This is made particularly challenging\nby the fact, that different images, or even different starting\nseeds with the same input image might require different val-\nues of the perturbation strength to achieve the best result. In\ncontrast, RASG approach is hyperparameter-free allowing\nboth: prompt/mask-aligned and naturally looking results.\nAppendix C.3. Rescaling with Standard Deviation\nThe core idea of RASG is to automatically scale perturba-\ntion using certain heuristics, such that the guidance process\nhas a consistent effect on the output, without harming the\nquality of the image. Our main heuristic relies on the fact\nthat [34] have defined a parametric family of stochastic de-\nnoising processes, which can all be trained using the same\ntraining objective as DDPM [11]. Recall the general form\nof parametric family of DDIM sampling processes:\nxt\u22121 = \u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(xt)\n\u221a\u03b1t\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u03f5t\n\u03b8(xt) + \u03c3t\u03f5t,\n(12)\nwhere \u03f5t \u223c N(0, 1). Particularly \u03f5t can be taken to be\ncollinear with the gradient \u2207xtS(xt) which will result in\nxt\u22121 distribution preservation by at the same time guiding\nthe generation process towards minimization of S(xt).\n15\nInput\nDreamshaper\nInpainting\n(w/o Guidance)\nGuidance:\ns = 100\nGuidance:\ns = 1000\nGuidance:\ns = 10000\nRASG\nFigure 10. Comparison of RASG strategy with default Stable Inpainting and vanilla guidance mechanism with different guidance scales.\nIn contrast to vanilla guidance, where the generation highly depends on the guidance scale, RASG consistently produces naturally looking\nand prompt-aligned results.\nTherefore we propose to scale the gradient \u2207xtS(xt)\nwith a value \u03bb and use instead of \u03f5t in the general form\nof DDIM. To determine \u03bb we analyse the distribution of\n\u2207xtS(xt) and found out that the values of the gradients\nhave a distribution very close to a gaussian distribution,\nwith 0 mean and some arbitary \u03c3, which changes over time-\nstep/image (Fig. 12). Therefore, computing the standard de-\nviation of the values of \u2207xtS(xt), and normalizing it by\n\u03bb =\n1\nstd(\u2207xtS(xt)) results in the standard normal distribu-\ntion (see Fig. 13). So the final form of RASG guidance\nstrategy is\nxt\u22121 = \u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8(xt)\n\u221a\u03b1t\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u03f5t\n\u03b8(xt) + \u03c3t\n\u2207xtS(xt)\nstd(\u2207xtS(xt)).\n(13)\nAppendix D. Limitations\nAlthough our method improves the prompt-alignment of ex-\nisting text-guided inpainting approaches, it still has a depen-\ndency on the backbone model, hence inherits some quality\n16\ndog\nelephant\nclock\nzebra\ninput\nAttend&Excite\nObjective\nBinary Cross\nEntropy\nFigure 11. Comparison of the Binary Cross Entropy engery function to modifed version of Attend & Excite. Images generated from the\nsame seed.\nFigure 12. Histogram of \u2207xtS(xt) values (i.e. before gradient standardization)\nFigure 13. Histogram of\n\u2207xt S(xt)\nstd(\u2207xt S(xt)) values (i.e. after gradient standardization)\nlimitations. Particularly it may generate extra limbs (the\nelephant in Fig. 14 has 5 legs) or illogical appearances (the\nsheep appears to have two bodies in Fig. 14 after the in-\npainting).\nepephant\nsheep\nFigure 14. Failure examples produced by our approach.\nAppendix E. Potential negative impacts\nOur research strives to enhance the accuracy of object gen-\neration within the scope of text-guided image inpainting.\nHowever, it is crucial to acknowledge the potential negative\nimpacts. The technology could be exploited to create decep-\ntive imagery or disseminate misinformation, raising ethical\nconcerns. While our method is training-free and does not\nintroduce new biases, it is imperative to consider the poten-\ntial propagation of biases from the base models we build\nupon. These biases could lead to the generation of content\nthat inadvertently reflects societal or historical prejudices.\n17\nTo counter these issues, it is essential for the broader\nresearch community to establish ethical standards and de-\nvelop robust methods to detect AI-generated content. Fur-\nthermore, efforts should be made to diversify training\ndatasets to reduce inherent biases. While these challenges\nare significant, the positive implications of our work in ar-\neas such as creative arts, design and content creation, when\nused responsibly, have the potential to surpass the negative\nrepercussions.\nAppendix F. More Examples of Our Method\nWe present more results of our method both for low-\nresolution (512 for the long side) images (Fig. 19), as well\nas high-resoltuion (2048 for the long side) (Figures 20, 21,\n22).\n18\nSmartBrush\nreprod.\nDreamShaper\nInpainting\nOurs\nInput\nboots\nhamster\nmoon\nwine bottle\nGLIDE\nBLD\nStable 2.0\nInpainting\nSDXL\nInpainting\nDreamShaper-\nControlNet\nInpainting\nclock\nbig horns\nFigure 15. More qualitative comparison results. Zoom in to view high-resolution details.\n19\ncup\nlaptop\nchair\ntruck\nbear\nremote\nSmartBrush\nreprod.\nDreamShaper\nInpainting\nOurs\nInput\nGLIDE\nBLD\nStable 2.0\nInpainting\nSDXL\nInpainting\nDreamShaper-\nControlNet\nInpainting\nFigure 16. More qualitative comparison results on MSCOCO 2017.\n20\nStable Super-Resolution\nOurs\nInput\nyellow headphones\nlake\nteddy bear\nFigure 17. Comparison between vanilla SD 2.0 upscale and our approach. In all examples the large side is 2048px. The cropped region is\n256x256px. Best viewed when zoomed in.\n21\nlion\nStable Super-Resolution\nOurs\nInput\nleather couch\nice cream\nFigure 18. Comparison between vanilla SD 2.0 upscale and our approach. In all examples the large side is 2048px. The cropped region is\n256x256px. Best viewed when zoomed in.\n22\ntractor\nfedora\nleather couch\nantique greek vase\nspacesuit\nlaptop\nhoodie\nspace helmet\nbowtie\nFigure 19. More results of our method.\n23\nboots\nparrot\nFigure 20. More high-resolution results of our method. Zoom in to view high-resolution details.\n24\npotted roses\nraccoon\nFigure 21. More high-resolution results of our method. Zoom in to view high-resolution details.\n25\ncastle\nsalad\nFigure 22. More high-resolution results of our method. Zoom in to view high-resolution details.\n26\n"
  },
  {
    "title": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model",
    "link": "https://arxiv.org/pdf/2312.13789.pdf",
    "upvote": "13",
    "text": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model\nHan Shu1,2, Wenshuo Li2, Yehui Tang2, Yiman Zhang2, Yihao Chen2, Houqiang Li1, Yunhe Wang2 \u0000, Xinghao Chen2 \u00001 University of Science and Technology of China. 2 Huawei Noah\u2019s Ark Lab.\n{xinghao.chen, yunhe.wang}@huawei.com\nAbstract\nRecently segment anything model (SAM) has shown pow-\nerful segmentation capability and has drawn great attention\nin computer vision fields. Massive following works have de-\nveloped various applications based on the pretrained SAM\nand achieved impressive performance on downstream vi-\nsion tasks. However, SAM consists of heavy architectures\nand requires massive computational capacity, which hin-\nders the further application of SAM on computation con-\nstrained edge devices. To this end, in this paper we pro-\npose a framework to obtain a tiny segment anything model\n(TinySAM) while maintaining the strong zero-shot perfor-\nmance. We first propose a full-stage knowledge distillation\nmethod with hard prompt sampling and hard mask weight-\ning strategy to distill a lightweight student model. We also\nadapt the post-training quantization to the promptable seg-\nmentation task and further reduce the computational cost.\nMoreover, a hierarchical segmenting everything strategy\nis proposed to accelerate the everything inference by 2\u00d7\nwith almost no performance degradation. With all these\nproposed methods, our TinySAM leads to orders of mag-\nnitude computational reduction and pushes the envelope\nfor efficient segment anything task. Extensive experiments\non various zero-shot transfer tasks demonstrate the signifi-\ncantly advantageous performance of our TinySAM against\ncounterpart methods.\nPre-trained models and codes are\navailable at https://github.com/xinghaochen/\nTinySAM and https://gitee.com/mindspore/\nmodels/tree/master/research/cv/TinySAM.\n1. Introduction\nObject segmentation is an important and foundational task\nin computer vision fields.\nExtensive visual applications\nsuch as object localization and verification rely on accu-\nrate and fast object segmentation. Tremendous prior works\nhave focused on segmentation tasks which include seman-\ntic segmentation [35, 43], instance segmentation [1, 31] and\n\u0000 Corresponding authors.\npanoptic segmentation [6, 20]. Recently, Kirillov et al. [21]\nintroduced a powerful segment anything model (SAM), to-\ngether with a massive segmentation dataset SA-1B that con-\ntains over 1 billion masks on 11 million images. With the\nstrong capability to segment objects with arbitrary shapes\nand categories, SAM has become a foundation framework\nfor many downstream tasks such as object tracking [7], im-\nage inpainting [48] and 3D vision [3] etc. Moreover, the\npowerful zero-shot segmentation ability of SAM has bene-\nfited research area with less data like medical imaging [36].\nAlthough SAM has achieved impressive performance on\ndownstream vision tasks, complicated architecture and huge\ncomputational cost make SAM difficult to be deployed on\nresource constrained devices. The inference time of SAM\nmodel for a 1024\u00d71024 image could take up to 2 seconds\non a modern GPU [51]. Some recent attempts have tried\nto obtain a more computation efficient segment anything\nmodel. For example, MobileSAM [50] tries to replace the\nheavy component of image encoder with a light-weight ar-\nchitecture of TinyViT [46]. However, it only accesses the\nimage encoder network with a decoupled knowledge distil-\nlation strategy by training the compact image encoder net-\nwork with the supervision of image embeddings from the\nteacher network. This partially training strategy inevitably\ncauses performance decay without the supervision of final\nmask prediction. FastSAM [51] transfers the segment any-\nthing task to an instance segmentation task with only one\nforeground category with YOLOv8 [19]. To fulfill the func-\ntion of promptable segmentation, FastSAM applies a post-\nprocess strategy together with the instance segmentation\nnetwork. However, this reformulated framework could not\nachieve comparable performance as SAM on downstream\nzero-shot tasks.\nTo further push the envelope for efficient segment any-\nthing model, in this paper we propose a full framework\nto obtain TinySAM that greatly reduces the computational\ncost while maintaining the zero-shot segmentation ability\nto maximum extent. Specifically, we propose a hard min-\ning full-stage knowledge distillation method to improve the\ncapability of the compact student network. The student net-\nwork is distilled in an end-to-end manner with the supervi-\n1\narXiv:2312.13789v2  [cs.CV]  9 Mar 2024\nTinySAM\nHard Mining \nFull-stage Distillation\nQuantization\nQ-TinySAM\nHierarchical\nEverything Mode\nSAM\n2976G\n1.78s\n0.93s\nEverything\nMode\nFLOPs\n(a) The proposed framework.\n(b) Zero-Shot Instance Segmentation.\nCOCO\nLVIS\n20.3G\n42.0G\nFigure 1. (a) The overall framework of our proposed method. Consisting the modules of the hard mining full-stage knowledge distillation,\nthe post training quantization and the hierarchical everything inference, the computation cost is down-scaled by magnitudes. (b) The\nproposed TinySAM can save considerable computation cost while maintaining the performance. The latency is tested with TensorRT on\nNVIDIA T4 GPU.\nsion of teacher network from different network stages. A\nmask-weighted distillation loss is proposed to efficiently\ntransfer the information from teacher to student through\nmassive various SA-1B masks.\nBesides, an online hard\nprompt sampling strategy is proposed to make the distil-\nlation process attend more to hard examples and thus im-\nproves the final performance.\nWe also adapt the post-\ntraining quantization to the promptable segmentation task\nand further reduce the computational cost. Moreover, we\nfind that it takes tremendous computational cost for seg-\nmenting everything in an image since massive masks have\nto be generated from grid prompt points. To this end, a\nhierarchical segmenting everything strategy is proposed to\naccelerate the everything inference by 2\u00d7 with almost no\nperformance degradation. With all these proposed methods,\nour TinySAM leads to orders of magnitude computational\nreduction and pushes the envelope for efficient segment\nanything task. For example, TinySAM can achieve 100\u00d7\nacceleration for segment anything task compared with the\noriginal SAM. Extensive experiments on various zero-shot\ntransfer tasks demonstrate the significantly advantageous\nperformance of our TinySAM against counterparts.\n2. Related Work\n2.1. Segment Anything Model\nRecently proposed segment anything model (SAM) [21]\nproves its generalization and versatility in object segmen-\ntation and downstream vision tasks. SAM consists of three\nsubnetworks, i.e., image encoder, prompt encoder and mask\ndecoder. The image encoder is a heavy vision transformer-\nbased network [12], which extracts the input image into im-\nage embeddings. The prompt encoder is designed to encode\ninput points, boxes, arbitrary-shaped masks and free-form\ntext with positional information.\nThe geometric prompt\nand text prompt are processed with different networks. The\nmask decoder, which contains a two-way transformer, takes\nthe output of image encoder and prompt encoder to gener-\nate the final mask prediction. Together with the proposed\nSA-1B dataset, which contains 11 million high-resolution\nimages and more than 1 billion high-quality segmentation\nmasks, SAM shows impressive high quality segmentation\nability for objects of any category and shape. Moreover,\nSAM demonstrates powerful generality on zero-shot down-\nstream vision tasks including edge detection, object pro-\nposal, instance segmentation and text-to-mask prediction.\nDue to the flexible prompts mode and high quality segmen-\ntation capability, SAM has been regarded as a foundation\nmodel for vision applications. However, SAM, especially\nthe image encoder network, consists of large parameters and\nrequires high computation capacity for deployment. There-\nfore, it is not easy to apply SAM on edge devices with con-\nstrained resources. The compression and acceleration of\nSAM is becoming an important research topic [50, 51].\n2.2. Knowledge Distillation\nHinton et al. [18] propose the knowledge distillation\nmethod to supervise the training of lightweight student net-\nwork via the output of teacher network. Since then knowl-\nedge distillation has been an important approach to improve\nthe performance of compact networks during training pro-\ncess. Knowledge distillation methods can be roughly di-\nvided into two categories,i.e. distillation for network out-\nputs [18] and for intermediate features [42]. Majority of\nresearch of knowledge distillation methods have focused\non image classification task [11, 24, 39, 40]. Subsequent\nworks [4, 5, 10, 16, 32] propose knowledge distillation\nmethods for high-level computer vision tasks such as ob-\n2\n\ud835\udc38\ud835\udc56\ud835\udc5a\ud835\udc54\n\ud835\udc47\n\ud835\udc38\ud835\udc56\ud835\udc5a\ud835\udc54\nS\n\ud835\udc37\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\n\ud835\udc46\n\ud835\udc37\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\n\ud835\udc47\n\ud835\udcaf \ud835\udc46\n\ud835\udcaf \ud835\udc47\n\u2112\ud835\udc52\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\n0.2\n0.8\n0.5\n0.1\nHard Prompt Sampling\n\ud835\udc400\n\ud835\udc47\n\ud835\udc401\n\ud835\udc47 \ud835\udc402\n\ud835\udc47\n\ud835\udc40\ud835\udc41\n\ud835\udc47\n\ud835\udc400\n\ud835\udc46\n\ud835\udc401\n\ud835\udc46 \ud835\udc402\n\ud835\udc46\n\ud835\udc40\ud835\udc41\n\ud835\udc46\n\u2026\n\u2026\n\u2026\n\u2112\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b\n\u2026\n0.8\n0.5\n0.2\n0.1\nHard Mask Weighting\n\u2112\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61\nIter. 0\nIter. 1\nIter. t\n\u2217 \ud835\udc64\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\nGT\nFigure 2. The framework of the hard mining full-stage knowledge distillation. For the massive masks of SA-1B dataset, we design the\nhard prompt sampling for prompts and hard mask weighting for distillation loss. For hard prompt sampling process, the stars represent\nsampling point with different iterations. With iterations of prompt sampling, the sampling region is more closed to the edge of the target\nmask, which makes the prompt relatively harder for student network to learn. Moreover, according to the gap between student and teacher\nnetwork, different weight is assigned to each mask when calculating the distillation loss.\nject detection and semantic segmentation. Zhang et al. [50]\npropose to use the distillation method to obtain an efficient\nsegment anything model (MobileSAM). However, Mobile-\nSAM only accesses the image encoder network with the su-\npervision of image and corresponding image embeddings of\noriginal SAM. This partial distillation strategy could cause\nconsiderable performance decay since there is no guidance\nof mask-level information for light-weight student network\nfrom either teacher network or labeled data.\n2.3. Quantization\nModel quantization is also one of the commonly used model\ncompression methods, which quantizes weights or activa-\ntions from higher bit-width to lower bit-width to reduce\nboth storage requirements and computational complexity\nwith limited accuracy degradation.\nThere are two types\nof model quantization methods, quantization-aware train-\ning (QAT) [8, 13] and post-training quantization (PTQ) [9].\nQAT methods require a labeled training dataset and exten-\nsive training cost, while PTQ methods only need a small\nunlabeled calibration dataset and thus are more efficient.\nMany prior PTQ methods [30, 38] have been proposed to\nsearch for appropriate quantization parameters for convo-\nlutional neural networks. As vision transformers [12, 33]\nachieved remarkable performance on various visual tasks,\nrecent works [26, 34, 44, 49] investigate how to apply\npost-training quantization for ViT-based models and have\nachieved strong performance with 8-bit quantization config-\nuration. However, there are still no exploration for quanti-\nzation of promptable segmentation task, especially for seg-\nment anything models.\n3. Methodology\n3.1. Overview of TinySAM\nThis paper proposes a framework to get an highly efficient\nSAM, as described in Figure 1.\nFirstly, we introduce a\nhard mining full-stage knowledge distillation specifically\ndesigned for SAM in Section 3.2. To further activate the\ndistillation process, the proposed hard mask weighting and\nhard prompt sampling strategy are both utilized to mine the\nessential knowledge from the teacher network to the student\nnetwork. Secondly, a post-training quantization method de-\nscribed in Section 3.3 is adapted to promptable segmenta-\ntion task and applied to the light-weight student network.\nThirdly, a hierarchical everything inference mode described\nin 3.4 is designed for segmenting anything task, which can\navoid massive redundant computation only with negligible\naccuracy loss and speedup the inference time by 2\u00d7.\n3.2. Hard Mining Full-Stage Knowledge Distillation\nSAM consists of three sub-networks, i.e. image encoder,\nprompt encoder and mask decoder.\nThe image encoder\nnetwork is based on vision transformer [12] and consumes\ngreat computation cost. Inspired by MobileSAM [50], we\nuse the lightweight TinyViT [46] to replace the original\nheavy image encoder network. Considerable performance\ndecay exists for this simple substitution. Therefore, we pro-\n3\npose a hard mining full-stage knowledge distillation strat-\negy to guide the lightweight image encoder during learning\nprocedure from multiple knowledge levels.\nBesides the conventional loss between the predicted re-\nsults and ground-truth labels, we introduce multiple distil-\nlation losses on different stages as described in Figure 2.\nSpecifically, we select several nodes of teacher network to\nguide the learning of student network from multiple level of\nknowledge. Firstly, we choose the output feature of image\nencoder, i.e. image embedding, as a distillation informa-\ntion. Image embedding concentrates the information from\ninput image, which is the fundamental knowledge during\nthe prediction. For an input image of I , the distillation loss\nfunction for image embedding can be expressed as,\nLembedding = L(E T\nimg(I ), E S\nimg(I )),\n(1)\nwhere E S\nimg and E T\nimg denote the image encoder for stu-\ndent and teacher network, respectively. Since image level\ninformation does not directly relate to the mask prediction,\nfeatures more close to the final output are essential for this\nsegmentation task. Naturally, the final output of the teacher\nnetwork is chosen to be a distillation point. The output dis-\ntillation loss Loutput can be described as,\nLoutput = L(DT\nmask(E T\nimg(I ), q), DS\nmask(E S\nimg(I ), q)),\n(2)\nwhere DS\nmask and DT\nmask are mask decoders for student and\nteacher, respectively. q denotes the query of the mask de-\ncoder, which is the concatenation of prompt embedding and\noutput tokens. Since the structure of SAM is rather com-\nplicated, the previously mentioned two distillation could be\ninconsistent and hard for lightweight student to learn. We\nfurther propose to distill the output tokens from the two-\nway transformer of the mask decoder, which interacts in-\nformation from prompt embedding and image embedding.\nIt captures the target mask information in a more abstract\nway. The corresponding distillation losses Ltoken can be\ndescribed as,\nLtoken = L(T T (E T\nimg(I ), q), T S(E S\nimg(I ), q)),\n(3)\nwhere T S and T T are the two-way transformer module of\nmask decoder and L denotes the loss function. We em-\npirically find that the numerical values of feature differ-\nence could make the conventionally used MSE loss (\u21132 dis-\ntance) too small to be well optimized. Thus we use \u21131 dis-\ntance function instead. The overall distillation loss function\nLdistill can be expressed as,\nLdistill = \u03b1 \u2217 Lembedding + \u03b2 \u2217 Ltoken + \u03b3 \u2217 Loutput,\n(4)\nwhere \u03b1, \u03b2, \u03b3 represent the hyper-parameters for each dis-\ntillation loss. The total training loss is a linear combination\nof distillation loss and groundtruth loss for mask prediction\nLmask and IoU prediction Lious, where Lmask is a combi-\nnation of focal loss [29] and dice loss [37], Lious is L1 loss\nfunction between predicted IoUs and calculated IoUs.\nLtotal = Ldistill + Lmask + Lious.\n(5)\nHard Mask Weighting. To make the knowledge distilla-\ntion more effective, we design a hard mask weighting strat-\negy when calculating the losses. There is an observation\nthat masks could be extremely various in a single image of\nSA-1B dataset since the fine-grained granularity and no se-\nmantic constraints. As shown in Figure 2, segmenting the\nflag with complex boundary could be difficult while seg-\nmenting the rectangular window with high contrast color\ncould be easy. The hard mask should reasonably be as-\nsigned with larger weight for student to learn. Specifically,\nwe calculate the gap of student and teacher network output\nto indicate the mask hardness Hi.\nHi = sigmoid(\nIoU(M T\ni , M GT\ni\n)\nIoU(M S\ni , M GT\ni\n) + \u03f5 \u2212 1),\n(6)\nwhere M T\ni , M S\ni , M GT\ni\nrepresent the mask prediction of\nstudent network, the mask prediction of teacher network\nand the groundtruth for ith mask, respectively. Thus the\ndistillation loss could be updated with\nL\u2217\ndistill = \u03b1\u2217Lembedding+\u03b2\u2217Ltoken+\u03b3\u2217\nN\nX\ni=1\nHi\u2217Li\noutput. (7)\nHard Prompt Sampling.\nGenerally, random sampling\nfrom labeled training data could be adopted to generate\nthe prompts to drive the end-to-end training of prompt-\nbased mask prediction network as SAM. To further ease\nthe learning process of the distillation between teacher and\nlightweight student network, we propose a hard prompt\nsampling strategy, which makes the training samples con-\ncentrate in the difficult area for prediction. Taking points\nprompt as an example, points P0 are initially sampled in-\nside the labeled mask region Mgt. These initial points are\nfed into the network with input image to get the predicted\nmask region M0. Then we sample the prompt points from\nthe difference set of Mgt and M0, and we conduct the pro-\ncedure interactively. The (i + 1)-th round sampling points\nPi are sampled from the difference set of Mgt and Mi, i.e.\nPi+1 \u2208 Mgt \u2212 Mi, i = 0, 1, 2, ...\n(8)\nwhere\nMi = Dmask(Eprompt(Pi), Eimg(I )).\n(9)\nWhen applied on the training process, the i-th iteration is\nrandom sampled from 0 to 9, which makes the difficulty\nof sampled prompts in a constrained range. The bottom of\nFigure 2 shows the location change of the sampling prompts\nwith iterations, the green stars denote the new sampled point\nprompt with online hard prompt sampling strategy. With\nmore iterations, the sampling points are more close to the\nedge region of the groundtruth mask.\n4\nRemove points in high \nconfidence regions\nMerge & Post-process\nPost-process\n(a)\n(c)\n(d)\n(b)\n(e)\nOriginal Everything Mode\nOurs Hierarchical Everything Mode\nFigure 3. Comparison between our hierarchical strategy and the original strategy. (a) Points sampling (take points per side=16 as an\nexample) of original everything mode. (b) Segmentation results of original strategy. (c) First step of our hierarchical strategy, only 1/16\npoints are sampled. (d) Get high confidence area from (c) and ignore points in this area. The high confidence area is shown as white mask.\n(e) Segmentation results of our hierarchical strategy.\n3.3. Quantization\nQuantization aims to project floating point tensor x to b-\nbit integer tensor xq with a scaling factor s. The uniform\nsymmetric quantization could be formulated as follows,\nxq = Q(b, s) = clip(round(x\ns ), \u22122b\u22121, 2b\u22121 \u2212 1).\n(10)\nFor a matrix multiplication O = AB, it can be quan-\ntized with two scaling factors sA and sB, and the quan-\ntized matrix is denoted as \u02c6O = \u02c6A \u02c6B. The metric for mea-\nsuring the distance between \u02c6O and O is vitally important\nfor optimizing Aq and Bq. Following the successful prac-\ntice of quantization methods in image classification mod-\nels [15, 44, 45, 49], we perform hessian guided metric as\nthe distance to solve the scaling factors, which is more con-\nsistent with task loss. Different from classification tasks,\nthe promptable segmentation task of SAM outputs segmen-\ntation predictions which contains fine-grained masks. Thus\nwe use the Kullback-Leible (KL) divergence of masks and\nIoUs as the task loss and use some calibration data to calcu-\nlate the hessian matrix, the task loss is formulated as:\nL = KL(\u02c6ypred, ypred) + KL(\u02c6yiou, yiou),\n(11)\nwhere ypred and yiou are the outputs of the floating point\nmodel, \u02c6ypred and \u02c6yiou are the outputs after quantization.\nAfter specifying the distance metric, we could solve sA\nand sB as an alternate iterative grid search problem. With\ncalibration data we get the maximum value of A and B,\nwhich is Amax and Bmax respectively, and use two param-\neters \u03b1 and \u03b2 to specify the search range for sA and sB,\n[\u03b1 Amax\n2b\u22121 , \u03b2 Amax\n2b\u22121 ] and [\u03b1 Bmax\n2b\u22121 , \u03b2 Bmax\n2b\u22121 ]. These two search\nranges are linearly divided into n candidate options sep-\narately. Aq and Bq are alternately optimized for several\nrounds.\nThe input of matrix multiplication after softmax is un-\nevenly distributed at both ends of the interval [0,1], while\nthe feature after GELU varies greatly between the positive\nand negative ranges. These two circumstances go far from\nthe assumption of uniform quantization, i.e., the activation\nin neural networks obeys Gaussian distribution. The vio-\nlation will result in high quantization error. Thus we split\nfeature into two groups and use two scaling factors to re-\nduce the quantization error.\n3.4. Hierarchical Segmenting Everything\nSAM proposes an automatic mask generator which samples\npoints as a grid to segment everything. However, we find\nthat dense points grid leads to over-finegrained segmenta-\ntion results and also occupies massive computing resources.\nOn the one hand, for a complete object, too many sampling\npoints may cause slightly different parts of the object to\nbe incorrectly segmented as separate masks. On the other\nhand, since the image encoder has been largely shrinked,\nthe time cost of everything mode inference is mainly in the\nmask decoder part. For the default settings of SAM auto-\nmatic mask generator, it samples 32 \u00d7 32 = 1024 points as\nthe prompts, which means the mask decoder is inferred by\n1024 times. It costs 16ms for image encoder and 894ms for\nmask decoder on a single V100 GPU.\nTo reduce the time cost of everything mode, we pro-\npose a hierarchical mask generating method. The compar-\nison between our hierarchical strategy and the original one\nis shown in Figure 3. Different from original everything\n5\nCOCO [27]\nLVIS v1 [17]\nMethod\nFLOPs\nLat.(ms)\nAP\nAPS\nAPM\nAPL\nAP\nAPS\nAPM\nAPL\nViTDet-H [23]\n-\n-\n51.0\n32.0\n54.3\n68.9\n46.6\n35.0\n58.0\n66.3\nzero-shot transfer methods (segmentation module only):\nSAM-H [22]\n2976G\n2392\n46.6\n30.8\n51.0\n61.7\n44.7\n32.5\n57.6\n65.5\nSAM-L [22]\n1491G\n1146\n46.2\n30.2\n50.1\n60.5\n43.5\n31.1\n56.3\n65.1\nSAM-B [22]\n487G\n368.8\n43.4\n28.5\n45.5\n53.4\n40.8\n29.1\n52.8\n60.7\nFastSAM [51]\n344G\n153.6\n37.9\n23.9\n43.4\n50.0\n34.5\n24.6\n46.2\n50.8\nEfficientSAM-Ti [47]\n106G\n81.0\n42.3\n26.7\n46.2\n57.4\n39.9\n28.9\n51.0\n59.9\nMobileSAM [50]\n42.0G\n38.4\n41.0\n24.4\n44.5\n58.6\n37.0\n24.7\n47.8\n59.1\nTinySAM (Ours)\n42.0G\n38.4\n42.3\n26.3\n45.8\n58.8\n38.9\n27.0\n50.3\n60.2\nQ-TinySAM (Ours)\n20.3G\n24.0\n41.4\n25.6\n45.1\n57.9\n38.5\n26.6\n49.8\n59.8\nTable 1. Zero-shot instance segmentation results on COCO and LVIS v1 dataset. Zero-shot transfer methods are prompted with the\ndetection boxes from fully-supervised ViTDet model. TinySAM and quantized Q-TinySAM demonstrate advantageous performance on\naverage precision. The latency is tested on NVIDIA T4 GPU.\nmode, in the first step we only use 25% points in each side\nso the total points is 1/16 of the original settings, as shown\nin Figure 3(c). Then we infer the prompt encoder and mask\ndecoder with these prompts and get the results.\nWe then filter out some masks with confidence exceed-\ning a threshold \u03c4, and mark the corresponding locations as\nareas that could be considered as final predictions. For these\nareas, since they are considered as the segmentation results\nof instances with high confidences, there is no need to re-\ngenerate point prompts. Then we sample points as the same\ndensity with original settings but ignore points in the above\narea. As shown in Figure 3(d), most points on the grass and\nbody of the first cow are ignored by our strategy. Mean-\nwhile, the points on the second cow and the sky are kept\nto further segment. Specifically, the second cow is incor-\nrectly segmented as the same object with the first cow in\nthe initial round. This strategy can avoid both the cost of\ninference time and the over-finegrained segmentation of the\nobject. Then we utilize the point prompts sampled in the\nsecond round to get the mask predictions. Finally, the re-\nsults of these two round are merged and post-processed to\nget the final masks. More than 50% points are ignored by\nour method thus brings in great latency reduction.\n4. Experiments\n4.1. Implementation Details\nWe utilize the TinyViT-5M [46] as the lightweight student\nimage encoder and SAM-H as the teacher model, follow-\ning prior work [50]. Partial SA-1B dataset is used as the\ntraining data for full-stage distillation. We adopt Adam op-\ntimizer and train the student network for 8 epochs.\nFor\neach iteration, we sample 64 prompts according to hard\nprompt sampling strategy described in Section 3.2. To ac-\ncelerate the distillation process, the image embeddings from\nthe teacher network have been computed and stored in ad-\nvance. Therefore, the heavy image encoder of teacher net-\nwork is not necessary to compute repeatedly during training\ntime. For post training quantization, we set \u03b1 = 0.01, \u03b2 =\n1.2, n = 100, rounds = 3 for iterative search. We cal-\nibrate quantized model on SA-1B dataset using 8 images.\nWe conduct zero-shot evaluation on downstream tasks like\ninstance segmentation and point prompt segmentation. Fol-\nlowing the suggestions by SAM [21], the multi-output mode\nis adopted and the final mask prediction is the one with\nhighest IoU prediction.\n4.2. Zero-Shot Instance Segmentation\nFor zero-shot instance segmentation task, we strictly fol-\nlow the experimental settings of SAM and use the ob-\nject detection results of ViTDet-H [23] as the box prompt\nfor instance segmentation. We evaluate the zero-shot in-\nstance segmentation task for models on the benchmark\nof COCO [28] dataset and LVIS v1 [17].\nWe com-\npare our TinySAM with different variants of SAM [22],\nand also with prior efficient models like FastSAM [51],\nMobileSAM [50] and EfficientSAM [47].\nAs shown in\nTable 1, the proposed TinySAM obtained superior per-\nformance when compared with prior methods.\nSpecifi-\ncally, our TinySAM outperforms FastSAM [51] in terms\nof FLOPs and instance segmentation accuracy, i.e., about\n4% AP improvement with only 12.2% FLOPs and 25% la-\ntency. With the same computational cost, our TinySAM\nalso achieves 1.3%+ AP on COCO dataset than Mobile-\n6\n2\n4\n6\n8\n0.55\n0.60\n0.65\n0.70\n0.75\nLVIS\nMobileSAM\nSAM-B\nSAM-L\nSAM-H\nTinySAM\n2\n4\n6\n8\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\nDOORS\nMobileSAM\nSAM-B\nSAM-L\nSAM-H\nTinySAM\n2\n4\n6\n8\n0.725\n0.750\n0.775\n0.800\n0.825\nBBBC038v1\nMobileSAM\nSAM-B\nSAM-L\nSAM-H\nTinySAM\n2\n4\n6\n8\n0.4\n0.5\n0.6\n0.7\n0.8\nTimberSeg 1.0\nMobileSAM\nSAM-B\nSAM-L\nSAM-H\nTinySAM\n2\n4\n6\n8\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\nMean\nMobileSAM\nSAM-B\nSAM-L\nSAM-H\nTinySAM\nFigure 4. Evaluation results of zero-shot points valid mask. X-axis represents the number of points used in prompts and Y-axis represents\nthe mIoU across all masks evaluated. The proposed TinySAM outperforms MobileSAM and achieves results close to SAM ViT-B.\nSAM [50] and 1.9%+ AP on LVIS v1 dataset, respectively.\nWith similar performance on COCO dataset, TinySAM is\n2\u00d7 faster than EfficientSAM [47]. Our W8A8 quantized\nvariant of TinySAM (Q-TinySAM) also obtains competi-\ntive performance across different methods. Specifically, Q-\nTinySAM achieves 0.4%+ AP on COCO dataset than Mo-\nbileSAM [50], with only 48% FLOPs and 62.5% latency.\nSome visual results on COCO validation set are shown in\nFigure 5. Our proposed TinySAM captures more clear and\nsmooth boundaries compared with other efficient variants\nof SAM.\n4.3. Zero-shot Points Valid Mask Evaluation\nIn this section we also evaluate the performance of our\nTinySAM for segmenting an object from several points as\nthe prompts. We use the same points selection metric as pre-\nvious work [17, 21], which calculates the distance transform\nof false positive and false negative masks, and then sample\npoints at a maximal value. We calculate the mIoU of each\ndataset to evaluate the performance of different models.\nWe choose a subset of total 23 datasets used in [21]\nfor efficient evaluation, which contains BBBC038v1 [2],\nDOORS [41], TimberSeg [14] and LVIS [17]. To make fair\ncomparisons, we follow the settings of Table 7 in SAM [21]\npaper to sample the images and masks, and the first N\nmasks in the corresponding split are used in the evaluation.\nThe evaluation results are shown in Figure 4.\nOur\nTinySAM outperforms MobileSAM [50] significantly on\nLVIS and TimberSeg dataset and obtains similar perfor-\nmance on DOORS dataset.\nMoreover, our TinySAM\nStrategy\nModel\nmIoU\nTime (s)\nOriginal\nMobileSAM\n0.5963\n1.6719\nHierarchical (Ours)\nMobileSAM\n0.5958\n0.8462\nOriginal\nSAM-H\n0.7047\n2.4549\nHierarchical (Ours)\nSAM-H\n0.7055\n1.3537\nOriginal\nTinySAM\n0.6137\n1.7790\nHierarchical (Ours)\nTinySAM\n0.6061\n0.9303\nTable 2. Comparison of original points grid strategy and our hi-\nerarchical strategy. Evaluation on the first 100 images of COCO\nval2017 set.\nachieves better results on BBBC038v1 when fewer number\nof points is utilized as prompts. We also report the mean\nIoU of all four datasets, as shown in the right figure in Fig-\nure 4. The proposed TinySAM is better than MobileSAM\nand obtains close performance to that of SAM-B.\n4.4. Everything Mode Acceleration\nWe evaluate our proposed hierarchical everything inference\nstrategy on COCO validation set. Latency benchmarks are\nconducted on a single NVIDIA V100 GPU for everything\nmode. We sample 100 images with the least img id from\nval2017 and conduct everything mode inference on these\nsamples. The threshold values used in the everything mode\nare all kept the same as default. The results are shown in\nTable 2. We apply the same threshold and stability score on\nthe same model evaluated with different strategies to make\n7\nTinySAM\nMobileSAM\nInput Image\nSAM-H\nFastSAM\nFigure 5. Visualization results of COCO validation dataset for zero-shot instance segmentation. The green box marks the box prompt. Our\nTinySAM captures more clear and smooth boundaries especially for hard targets of small size or similar texture feature.\n(a)\n(b)\nFigure 6. Visual comparison of everything mode segmentation\nresults on COCO val2017 set.\n(a) Points grid strategy result,\nmIoU 0.5292. (b) Hierarchical strategy result, mIoU 0.6429. Our\nmethod avoids the over-fining segmentation of objects in this case.\na fair comparison. But they can be different between these\nmodels. Our hierarchical strategy achieves comparable re-\nsults compared with original 32 \u00d7 32 points grid strategy\nwhile the cost of inference time is reduced by about 50%.\nFigure 6 shows the visual comparison between points grid\nstrategy and the proposed hierarchical strategy.\nWe can\nfind that the proposed hierarchical strategy avoids the over-\nfinegrained segmentation of objects in some cases which\nmakes up for some losses of IoU. More visual results of the\nefficient everything mode inference is shown in Figure 7.\n4.5. Ablation Studies\nIn this section, we conduct ablation studies of the proposed\nmethod on zero-shot instance segmentation task on COCO\nvalidation dataset. The experimental setting is the same as\ndescribed in Section 4.2.\nImpacts of different modules. We first evaluate the effects\nof different modules, i.e., full-stage knowledge distillation\nloss, online hard prompt sampling and post quantization, re-\nInd.\nSettings\nAP (%)\n0\nBaseline\n40.7\n1\n+ Knowledge Distillation Loss\n41.4\n2\n+ Hard Prompt Sampling\n41.9\n3\n+ Hard Mask Weighting\n42.3\n4\n+ Quantization\n41.4\nTable 3. Effect of distillation loss, online hard prompt sampling\nand quantization respectively, evaluated on zero-shot instance seg-\nmentation on COCO validation dataset.\nspectively. As shown in Table 3, utilizing our proposed full-\nstage distillation strategy improve the performance from\n40.7% to 41.4%. Incorporated with the online hard prompt\nsampling strategy, our method could obtain 0.5% AP gain.\nWith the hard mask weighting loss, the performance can\nfurther increase to 42.3%. Using post-training quantization\nresults in 0.9% AP degradation but also greatly reduces the\ncomputational cost, which is also discussed in the main ex-\nperiments.\nImpacts of different distillation losses. For detailed full-\nstage knowledge distillation process, we investigate the\nnecessity of the proposed three-level distillation from the\nteacher network. Table 4 shows the ablation results with\ndifferent combinations of distillation losses. The output dis-\ntillation loss takes important part since it is close to the su-\npervision information and the similarity with teacher net-\nwork directly reflects in the evaluation metric. Token loss\nand embedding loss both prove to be beneficial since they\nare related to key nodes of teacher network, which reflects\nthe image-level information and the interaction of prompts\n8\nFigure 7. Visualization results of hierarchical everything inference with TinySAM model.\nEmbedding Loss\nToken Loss\nOutput Loss\nAP (%)\n-\n-\n\u2713\n41.6\n\u2713\n-\n\u2713\n41.7\n\u2713\n\u2713\n\u2713\n41.9\n\u2713\n\u2713\n\u2713(HMW)\n42.3\nTable 4. Ablation study on combinations of knowledge distillation\nlosses for zero-shot instance segmentation on COCO val set.\nPoints per side 1st/2nd\nThresh. \u03c4\nmIoU\nTime (s)\n4/16\n8.5\n0.5521\n0.3571\n8/32\n8.5\n0.6061\n0.9303\n10/32\n8.5\n0.6078\n1.2774\n8/32\n7.0\n0.6018\n0.8154\n8/32\n10.0\n0.6067\n1.1819\n32/-\n\u2013\n0.6137\n1.7790\nTable 5. Ablation on point density and threshold for hierarchical\nstrategy.\nwith the image, respectively. Hard mask weighting for out-\nput loss can further boost the performance.\nPoint density and threshold for hierarchical strategy.\nIn Table. 5, we conduct ablation study with different set-\ntings of point density and high-confidence mask threshold\n\u03c4. More points and higher threshold \u03c4 lead to more precise\nresults but longer inference time. The point density of 2nd\nround is more sensitive compared to the 1st one. Consid-\nering both accuracy and efficiency, the setting in bold is a\ngood balance and used for other experiments of everything\ninference mode.\nDifferent bits for quantization. We here explore the in-\nfluence of different bit width. Table 6 reports the average\nprecision on COCO dataset. From the results, we can con-\nModel\nAP (%)\nFLOPS (G)\nMobileSAM\n41.0\n42.0\n+ W8A8\n39.8\n20.28\n+ W6A6\n36.3\n18.45\nTinySAM (Ours)\n42.3\n42.0\n+ W8A8\n41.4\n20.28\n+ W6A6\n39.0\n18.45\nTable 6. Ablation study for different bit width of quantization for\nzero-shot instance segmentation on COCO validation dataset.\nclude that quantization to 8 bits results in only slight per-\nformance drop. We also demonstrate the performance by\nfurther reducing the quantization bit width to 6 bits.\n5. Conclusion\nIn this paper, we propose a framework to push the enve-\nlope for segment anything task and obtain a highly effi-\ncient model named TinySAM. We first propose a full-stage\nknowledge distillation method with hard mask weighting\nand hard prompt sampling strategy to distill a lightweight\nstudent model. We also adapt the post-training quantization\nto the promptable segmentation task and further reducing\nthe computational cost. Moreover, a hierarchical segment-\ning everything strategy is proposed to accelerate the every-\nthing inference by 2\u00d7 with almost no performance degrada-\ntion. With all these proposed methods, our TinySAM leads\nto orders of magnitude computational reduction and push\nthe envelope for efficient segment anything task. Exten-\nsive experiments on various zero-shot transfer tasks demon-\nstrate the significantly advantageous performance of our\nTinySAM against counterparts methods. We hope the pro-\nposed TinySAM brings beneficial perspective for designing\na highly efficient segment anything model.\n9\nTinySAM(Ours)\nMobileSAM\nInput Image\nSAM-H\nFastSAM\nFigure 8. Visualization results of LVIS v1 dataset for zero-shot instance segmentation. The green box denotes the box prompt. TinySAM\ngenerates more accurate and smooth masks compared with FastSAM and MobileSAM.\nAppendix\nWe provide more visualization results for supplementary\nmaterial. Figure 8 shows zero-shot instance segmentation\non LVIS v1 [17] dataset. For clear presentation, only de-\ntected boxes by VitDet-H [25] with scores higher than 0.8\nare prompted into models.\nLVIS dataset has more fine-\ngrained labels than COCO dataset [28], on which the pro-\nposed TinySAM demonstrates greater advantage consider-\ning both accuracy and efficiency.\nFigure 9 shows the everything inference results by the\nproposed TinySAM model with hierarchical everything in-\nference and its counterpart algorithms. TinySAM captures\nclear boundaries and produce more fine-grained masks,\nwhereas MobileSAM [50] and FastSAM [51] sometimes\ngenerate fabricated boundaries and masks. TinySAM shows\nmore close performance to the original SAM [21], while\nconsuming significantly less computation cost.\n10\nTinySAM(Ours)\nMobileSAM\nInput Image\nSAM-H\nFastSAM\nFigure 9. Visualization results of TinySAM model with hierarchical everything inference and its counterpart algorithms. Compared to\nFastSAM and MobileSAM, TinySAM captures fine-grained boundaries and masks, demonstrating similar performance with the computa-\ntional expensive SAM-H model.\n11\nReferences\n[1] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae\nLee. Yolact: Real-time instance segmentation. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision, pages 9157\u20139166, 2019. 1\n[2] Juan C Caicedo, Allen Goodman, Kyle W Karhohs,\nBeth\nA\nCimini,\nJeanelle\nAckerman,\nMarzieh\nHaghighi, CherKeng Heng, Tim Becker, Minh Doan,\nClaire McQuin, et al. Nucleus segmentation across\nimaging experiments: the 2018 data science bowl.\nNature methods, 16(12):1247\u20131253, 2019. 7\n[3] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen,\nLingxi Xie, Xiaopeng Zhang, and Qi Tian.\nSeg-\nment anything in 3d with nerfs.\narXiv preprint\narXiv:2304.12308, 2023. 1\n[4] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han,\nand Manmohan Chandraker.\nLearning efficient ob-\nject detection models with knowledge distillation. Ad-\nvances in neural information processing systems, 30,\n2017. 2\n[5] Xinghao Chen, Yiman Zhang, Yunhe Wang, Han Shu,\nChunjing Xu, and Chang Xu. Optical flow distilla-\ntion: Towards efficient and stable video style trans-\nfer. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part VI 16, pages 614\u2013630. Springer, 2020.\n2\n[6] Bowen Cheng, Ishan Misra, Alexander G Schwing,\nAlexander Kirillov, and Rohit Girdhar.\nMasked-\nattention mask transformer for universal image seg-\nmentation.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 1290\u20131299, 2022. 1\n[7] Yangming Cheng,\nLiulei Li,\nYuanyou Xu,\nXi-\naodi Li, Zongxin Yang, Wenguan Wang, and Yi\nYang. Segment and track anything. arXiv preprint\narXiv:2305.06558, 2023. 1\n[8] Jungwook Choi, Zhuo Wang, Swagath Venkatara-\nmani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan,\nand Kailash Gopalakrishnan.\nPact: Parameterized\nclipping activation for quantized neural networks.\narXiv preprint arXiv:1805.06085, 2018. 3\n[9] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. Low-bit quantization of neural networks for\nefficient inference. In 2019 IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW),\npages 3009\u20133018. IEEE, 2019. 3\n[10] Ziwei Deng, Quan Kong, and Tomokazu Murakami.\nTowards efficient instance segmentation with hierar-\nchical distillation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV)\nWorkshops, 2019. 2\n[11] Minjing Dong, Xinghao Chen, Yunhe Wang, and\nChang Xu. Improving lightweight addernet via dis-\ntillation from \u21132 to \u21131-norm.\nIEEE transactions on\nimage processing: a publication of the IEEE Signal\nProcessing Society, 32:5524\u20135536, 2023. 2\n[12] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer,\nGeorg Heigold,\nSylvain Gelly,\net al.\nAn image is worth 16x16 words:\nTransformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[13] Steven K Esser,\nJeffrey L McKinstry,\nDeepika\nBablani, Rathinakumar Appuswamy, and Dharmen-\ndra S Modha. Learned step size quantization. arXiv\npreprint arXiv:1902.08153, 2019. 3\n[14] Jean-Michel\nFortin,\nOlivier\nGamache,\nVincent\nGrondin, Franc\u00b8ois Pomerleau, and Philippe Gigu`ere.\nInstance segmentation for autonomous log grasping in\nforestry operations. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 6064\u20136071. IEEE, 2022. 7\n[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh.\nGptq: Accurate post-training quanti-\nzation for generative pre-trained transformers. arXiv\npreprint arXiv:2210.17323, 2022. 5\n[16] Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xing-\nhao Chen, Chunjing Xu, and Chang Xu. Distilling ob-\nject detectors via decoupled features. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2154\u20132164, 2021. 2\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis:\nA dataset for large vocabulary instance segmentation.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 5356\u2013\n5364, 2019. 6, 7, 10\n[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Dis-\ntilling the knowledge in a neural network.\narXiv\npreprint arXiv:1503.02531, 2(7), 2015. 2\n[19] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo\nby ultralytics, 2023.\nhttps://github.com/\nultralytics/ultralytics. 1\n[20] Alexander Kirillov, Kaiming He, Ross Girshick,\nCarsten Rother, and Piotr Doll\u00b4ar. Panoptic segmen-\ntation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n9404\u20139413, 2019. 1\n[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen\nLo, et al.\nSegment anything.\narXiv preprint\narXiv:2304.02643, 2023. 1, 2, 6, 7, 10\n12\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen\nLo, et al.\nSegment anything.\narXiv preprint\narXiv:2304.02643, 2023. 6\n[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li\nCheng. Exploring denoised cross-video contrast for\nweakly-supervised temporal action localization.\nIn\nCVPR, pages 19914\u201319924, 2022. 6\n[24] Yanxi Li, Xinghao Chen, Minjing Dong, Yehui Tang,\nYunhe Wang, and Chang Xu. Spatial-channel token\ndistillation for vision mlps.\nIn International Con-\nference on Machine Learning, pages 12685\u201312695.\nPMLR, 2022. 2\n[25] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming\nHe. Exploring plain vision transformer backbones for\nobject detection, 2022. 10\n[26] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao,\nPeng Gao, and Guodong Guo. Q-vit: Accurate and\nfully quantized low-bit vision transformer. Advances\nin Neural Information Processing Systems, 35:34451\u2013\n34463, 2022. 3\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar,\nand C Lawrence Zitnick.\nMicrosoft coco:\nCom-\nmon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13,\npages 740\u2013755. Springer, 2014. 6\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar,\nand C Lawrence Zitnick.\nMicrosoft coco:\nCom-\nmon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13,\npages 740\u2013755. Springer, 2014. 6, 10\n[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\nHe, and Piotr Doll\u00b4ar. Focal loss for dense object de-\ntection. In Proceedings of the IEEE international con-\nference on computer vision, pages 2980\u20132988, 2017.\n4\n[30] Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang,\nXinggang Wang, and Wenyu Liu.\nPd-quant: Post-\ntraining quantization based on prediction difference\nmetric. In CVPR, pages 24427\u201324437, 2023. 3\n[31] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Ji-\naya Jia. Path aggregation network for instance seg-\nmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8759\u2013\n8768, 2018. 1\n[32] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin,\nZhenbo Luo, and Jingdong Wang. Structured knowl-\nedge distillation for semantic segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 2604\u20132613, 2019.\n2\n[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer:\nHierarchical vision transformer using\nshifted windows. In ICCV, pages 10012\u201310022, 2021.\n3\n[34] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Si-\nwei Ma, and Wen Gao. Post-training quantization for\nvision transformer. Advances in Neural Information\nProcessing Systems, 34:28092\u201328103, 2021. 3\n[35] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmenta-\ntion. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2015.\n1\n[36] Jun Ma and Bo Wang. Segment anything in medical\nimages. arXiv preprint arXiv:2304.12306, 2023. 1\n[37] Fausto Milletari, Nassir Navab, and Seyed-Ahmad\nAhmadi. V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In 2016\nfourth international conference on 3D vision (3DV),\npages 565\u2013571. Ieee, 2016. 4\n[38] Markus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort.\nUp or\ndown? adaptive rounding for post-training quantiza-\ntion. In International Conference on Machine Learn-\ning, pages 7197\u20137206. PMLR, 2020. 3\n[39] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.\nRelational knowledge distillation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 2\n[40] Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li,\nYichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning\nZhang. Correlation congruence for knowledge distil-\nlation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5007\u20135016,\n2019. 2\n[41] Mattia Pugliatti and Francesco Topputo.\nDoors:\nDataset for boulders segmentation.\nZenodo, 9:20,\n2022. 7\n[42] Adriana Romero, Nicolas Ballas, Samira Ebrahimi\nKahou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio.\nFitnets: Hints for thin deep nets.\narXiv\npreprint arXiv:1412.6550, 2014. 2\n[43] Robin Strudel, Ricardo Garcia, Ivan Laptev, and\nCordelia Schmid. Segmenter: Transformer for seman-\ntic segmentation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 7262\u20137272, 2021. 1\n[44] Yu-Shan Tai, Ming-Guang Lin, and An-Yeu Andy Wu.\nTsptq-vit: Two-scaled post-training quantization for\n13\nvision transformer. In ICASSP 2023-2023 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 1\u20135. IEEE, 2023. 3, 5\n[45] Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying\nFu, and Debing Zhang.\nEasyquant: Post-training\nquantization via scale optimization.\narXiv preprint\narXiv:2006.16669, 2020. 5\n[46] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen\nLiu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast\npretraining distillation for small vision transformers.\nIn European Conference on Computer Vision, pages\n68\u201385. Springer, 2022. 1, 3, 6\n[47] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xi-\naoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang\nDai, Dilin Wang, Fei Sun, Forrest Iandola, et al.\nEfficientsam:\nLeveraged masked image pretrain-\ning for efficient segment anything.\narXiv preprint\narXiv:2312.00863, 2023. 6, 7\n[48] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu,\nXin Jin, Wenjun Zeng, and Zhibo Chen.\nInpaint\nanything: Segment anything meets image inpainting.\narXiv preprint arXiv:2304.06790, 2023. 1\n[49] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu,\nand Guangyu Sun. Ptq4vit: Post-training quantization\nfor vision transformers with twin uniform quantiza-\ntion. In ECCV, pages 191\u2013207. Springer, 2022. 3,\n5\n[50] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk\nKim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon\nHong. Faster segment anything: Towards lightweight\nsam for mobile applications, 2023. 1, 2, 3, 6, 7, 10\n[51] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du,\nTao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast\nsegment anything, 2023. 1, 2, 6, 10\n14\n"
  },
  {
    "title": "Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning",
    "link": "https://arxiv.org/pdf/2312.13980.pdf",
    "upvote": "11",
    "text": "Carve3D: Improving Multi-view Reconstruction Consistency\nfor Diffusion Models with RL Finetuning\nDesai Xie1, 2\nJiahao Li1, 3\nHao Tan1\nXin Sun1\nZhixin Shu1\nYi Zhou1\nSai Bi1\nS\u00a8oren Pirk4\nArie E. Kaufman2\n1Adobe Research\n2Stony Brook University\n3TTIC\n4Kiel University\nAbstract\nRecent advancements in the text-to-3D task leverage\nfinetuned text-to-image diffusion models to generate multi-\nview images, followed by NeRF reconstruction. Yet, exist-\ning supervised finetuned (SFT) diffusion models still suffer\nfrom multi-view inconsistency and the resulting NeRF arti-\nfacts. Although training longer with SFT improves consis-\ntency, it also causes distribution shift, which reduces diver-\nsity and realistic details. We argue that the SFT of multi-\nview diffusion models resembles the instruction finetuning\nstage of the LLM alignment pipeline and can benefit from\nRL finetuning (RLFT) methods. Essentially, RLFT meth-\nods optimize models beyond their SFT data distribution by\nusing their own outputs, effectively mitigating distribution\nshift. To this end, we introduce Carve3D, a RLFT method\ncoupled with the Multi-view Reconstruction Consistency\n(MRC) metric, to improve the consistency of multi-view dif-\nfusion models. To compute MRC on a set of multi-view im-\nages, we compare them with their corresponding renderings\nof the reconstructed NeRF at the same viewpoints. We vali-\ndate the robustness of MRC with extensive experiments con-\nducted under controlled inconsistency levels. We enhance\nthe base RLFT algorithm to stabilize the training process,\nreduce distribution shift, and identify scaling laws. Through\nqualitative and quantitative experiments, along with a user\nstudy, we demonstrate Carve3D\u2019s improved multi-view con-\nsistency, the resulting superior NeRF reconstruction qual-\nity, and minimal distribution shift compared to longer SFT.\nProject webpage: https://desaixie.github.io/\ncarve-3d.\n1. Introduction\nIn recent times, notable advancements have been made\nin the text-to-3D domain, driven by lifting images gener-\nated by 2D diffusion models [9, 40, 42, 48, 51, 55] to 3D.\nNumerous methods [24, 28, 46, 58] have demonstrated that\na set of multi-view images is adequate for generating di-\nverse and detailed 3D models, effectively mitigating the\nJanus (two-face) problem. Thus, ensuring the 3D consis-\ntency across these multi-view images is crucial for 3D gen-\neration, as inconsistencies can inevitably introduce artifacts,\nsuch as broken geometries, blurring, or floaters, in the NeRF\nreconstruction. However, the lack of an established multi-\nview consistency metric has led researchers to rely on quali-\ntative inspections, which are both inefficient and unreliable,\nhighlighting the ongoing issues with inconsistency in cur-\nrent methods.\nExisting multi-view diffusion models [24, 27, 28, 46, 58]\nprimarily utilize supervised finetuning (SFT) with datasets\nderived from rendering perfectly consistent multi-view im-\nages of 3D assets [12, 13]. While SFT can achieve some\ndegree of multi-view consistency, it presents a dilemma:\nprolonged SFT enhances this consistency but also induces\na distribution shift that diminishes diversity and realism of\nthe results [24]. Such dilemma has been observed in the\nresearch of large language models (LLMs).\nWhile SFT\nchanges the output distribution of pre-trained LLMs from\ntext completion to answering instructions, such distribu-\ntion shift to the instruction dataset also introduces halluci-\nnation [44], preventing longer SFT. InstructGPT [36], the\npaper behind ChatGPT 3.5 [35], introduces RL finetuning\n(RLFT) to further align the SFT model without causing\nadditional distribution shift. Drawing an analogy between\ninstruction-finetuned LLMs and multi-view diffusion mod-\nels, RLFT emerges as an essential step following the SFT\nstage. By adopting RLFT, we aim to enhance the consis-\ntency of multi-view diffusion models without introducing\nthe biases from a SFT dataset (Figure 1).\nWe introduce Carve3D, a RLFT framework paired with\nour Multi-view Reconstruction Consistency (MRC) metric,\nto improve the consistency of multi-view diffusion models\n(Figure 2). MRC compares the output multi-view images\nfrom a diffusion model, which serve as inputs for NeRF re-\nconstruction, with images rendered from the NeRF at iden-\ntical camera viewpoints. We use the sparse-view Large Re-\n1\narXiv:2312.13980v1  [cs.CV]  21 Dec 2023\na teddy bear pushing a shopping cart full of fruits and vegetables\nEpoch 0\nEpoch 28\nEpoch 55\na ceramic upside down yellow octopus \nholding a blue green ceramic cup\na cup full of pens and pencils\nFigure 1. Carve3D steadily improves the 3D consistency of the\nmulti-view diffusion model and the resulting quality of the NeRF\nand the mesh, without sacrificing its diversity or realism. Here\nwe show results of the finetuned multi-view diffusion model over\nthree epochs on three testing prompts (three blocks separated by\ndotted line), including the generated multi-view images (top), the\nreconstructed NeRF and extracted mesh (bottom) and the prompt\n(middle). The inconsistencies in the multi-view images, e.g. the\nfacing direction of the shopping cart, the position of the octopus\narms, and the position of the pencils, lead to artifacts in the NeRF\nand the mesh, highlighted by the red boxes.\nconstruction Model (LRM) [17, 24] to achieve fast, feed-\nforward NeRF reconstruction from a few multi-view im-\nages. To quantify image similarity, we adopt LPIPS [56] as\nit is more effective and robust for MRC. We further normal-\nize LPIPS with respect to the bounding boxes of foreground\nobjects to prevent trivial reward hacking through size reduc-\ntion of the foreground object. To validate the reliability of\nMRC, we conduct extensive experiments with controlled in-\nconsistency levels; starting from a set of perfectly consistent\nmulti-view images rendered from a 3D asset [12], we man-\nually introduce distortion to one of the views to create in-\nconsistency. Our MRC metric provides robust evaluation of\nconsistency of multi-view images, offers a valuable tool for\nassessing current multi-view generation methods and guid-\ning future developments in the field.\nWith MRC, we employ RLFT for multi-view diffu-\nsion models.\nIn the RLFT process, we use a set of cu-\nrated, creative text prompts to repeatedly generate diverse\nmulti-view images with random initial noises and use their\nMRC reward to update the diffusion model (Figure 2).\nSuch diversity- and quality-preserving finetuning cannot be\nachieved with SFT, as it is infeasibly expensive to cre-\nate a dataset of diverse ground-truth multi-view images for\nthese prompts. We make the following improvements to\nthe RLFT algorithm [5]. In addressing the common issue\nof training instability in RL, we opt for a purely on-policy\npolicy gradient algorithm [53], diverging from the widely\nadopted, partially on-policy PPO [45] algorithm. We in-\ncorporate KL divergence regularization [15, 36] to maintain\nproximity to the base model and prevent distribution shift.\nMoreover, we scale up the amount of compute to achieve\noptimal rewards by applying the scaling laws for diffu-\nsion model RLFT, identified from extensive experiments \u2013\na topic that has not yet been extensively covered in existing\nstudies [5, 15].\nThrough quantitative and qualitative experiments, as\nwell as human evaluation, we demonstrate that Carve3D:\n(1) achieves improved multi-view consistency and NeRF\nreconstruction quality over the base multi-view diffusion\nmodel, Instant3D-10K [24], as well as Instant3D-20K and\nInstant3D-100K, which utilize more SFT steps, and (2)\nmaintains similar prompt alignment, diversity, and realistic\ndetails from the base Instant3D-10k, preventing the degra-\ndation in Instant3D-20k and Instant3D-100k. We extend\nour consistency evaluation to additional multi-view diffu-\nsion models using MRC, revealing the universal presence of\nmulti-view inconsistency when relying solely on SFT. Our\nwork is the first application of RLFT to the text-to-3D task,\nespecially with diffusion models at the SDXL [39] scale us-\ning a 2.6B-parameter UNet. We hope this work will bolster\nthe research on RLFT for alignment in the computer vision\ncommunity.\n2. Related Works\n2.1. 3D Generation with 2D Diffusion Models\nNeRF is a neural representation of 3D assets [8, 31, 33].\nIt infers the direction-dependent radiance at an arbitrary\n2\na completely \ndestroyed car\nMulti-view Diffusion Model\n\u2026\nt=100\nt=50\nt=0\nMRC \nreward\nStop gradient\nForward Pass\nPolicy Gradient\nNeRF Reconstruct\nNeRF Render\nInput\nBackward Pass\n\u2026\n(a)\n(b)\n(c)\nFigure 2. Overview of Carve3D. Given a prompt sampled from the curated prompt set, we run the denoising process to obtain the final\ndenoised image, which contains four multi-view images tiled in a 2-by-2 grid. MRC reward is computed by comparing (a) the generated\nmulti-view images with (c) the corresponding multi-view images rendered at the same camera viewpoints from (b) the reconstructed NeRF.\nThen, we train the model with policy gradient loss function, where the loss is derived from the reward and log probabilities of the model\u2019s\npredicted noise, accumulated across all denoising timesteps. Using only a set of prompts, this RLFT process finetunes the diffusion model\nwith its own outputs, without relying on ground truth multi-view images.\nvolumetric position with neural models. Many text-to-3D\nmethods rely on NeRF to produce 3D objects.\nWhile text-to-image diffusion models are trained on 5\nbillion data [43], the largest public 3D dataset only contains\n10 million 3D assets [12, 13] with little text annotation. This\ngap in the diversity and quality of 3D data has restricted the\nquality of current 3D diffusion models and their ability in\nhandling complex prompts [19, 34]. To circumvent this lim-\nitation, another line of work focuses on lifting 2D images\nto 3D, thus leveraging the remarkable semantic understand-\ning and high-quality generation capabilities of 2D diffusion\nmodels [39, 42]. These methods [9, 40, 48, 51, 55] typ-\nically employ 2D diffusion models to provide supervision\nat the novel views for optimizing 3D objects represented as\nNeRF or by 3D Gaussian Splatting [21]. Building on this\nconcept, multiple works [24, 26\u201328, 46, 58] have proposed\ngenerating multi-view images using a finetuned 2D diffu-\nsion model, providing a more comprehensive visual prior\nand preventing the multi-face (Janus) problem. However,\nas the finetuning datasets of multi-view images are rendered\nfrom the same 3D dataset [12, 13], the limited quality and\ndiversity remains a challenge, preventing running SFT to\nconvergence [24]. By adopting RLFT, we do not depend\non ground truth multi-view images and thus optimize the\nmodel beyond the distribution of their SFT dataset.\nA key challenge in utilizing multi-view images is achiev-\ning 3D consistency, ensuring that the geometry and ap-\npearance of an object is uniform across different views.\nWhile numerous methods have attained notable multi-view\nconsistency by supervised finetuning 2D diffusion mod-\nels [24, 27, 28, 46, 58], their evaluation has been empirical,\nlacking explicit metrics. An approach known as 3D consis-\ntency scoring [52] measures the consistency of output views\nby optimizing a NeRF trained on these views. However,\nthis method requires dense input view sampling for cross-\nvalidation, making it unsuitable for evaluating sparse views.\nTo overcome this limitation, we propose the MRC metric.\nMRC evaluates multi-view consistency by comparing input\nimages with renderings of the generated 3D objects from\ncorresponding views. We employ LPIPS, a perceptual im-\nage distance metric [56], to quantify the image differences.\nAdditionally, we validate the reliability of MRC by con-\nducting extensive experiments under controlled inconsis-\ntency levels. The robustness of MRC allows us to conduct\nRLFT on multi-view diffusion models, which significantly\nenhances their multi-view consistency.\n3D models can be derived from either single or multi-\nview images by optimizing the SDS distillation loss [40,\n51]. However, the optimization process is notably time-\nconsuming, requiring multiple hours to generate a single\n3D asset.\nIn contrast, LRM [17], trained on the exten-\nsive 3D dataset Objaverse [12], can efficiently reconstruct\nNeRF models from a single image in a feed-forward man-\nner. In this work, we focus exclusively on text-to-3D using\nfeed-forward sparse-view NeRF reconstruction, specifically\nemploying sparse-view LRM [24]. This choice is driven\nby its significantly faster performance compared to SDS-\nbased optimization methods and its superior quality rela-\ntive to feed-forward text-to-3D diffusion models [19, 34].\nWe choose Instant3D [24] as our base multi-view diffu-\nsion model, owing to its light-weight SFT that preserves the\nstrong semantic understanding and high-quality image gen-\neration capabilities of SDXL [39], similar to the instruction\nfinetuning stage in InstructGPT [36].\n3\n2.2. RLFT of LLMs and Diffusion Models\nRL has been widely used to finetune large pre-trained\nmodels in NLP [2, 3, 22, 36] and CV [5, 11, 15, 38, 41, 57],\ndue to its advantage over SFT. SFT directly fits a model\nto the distribution of the SFT dataset containing inputs and\nground-truth target data, which unavoidably causes some\ndegree of distribution shift [44]. On the contrary, based on\nan objective function and a dataset containing only inputs,\nRLFT optimizes a model beyond the limitation of a SFT\ndataset by using its own outputs and effectively mitigates\ndistribution shift [7].\nRLFT of LLMs\nLLMs like GPT-3 [6] are pre-trained on\nthe next-word prediction task on an internet-scale corpus.\nWhile the autoregressive pre-training is a powerful self-\nsupervised objective that allows LLMs to extract substantial\nknowledge from the internet-scale unlabeled dataset, pre-\ntrained LLMs can only perform the corresponding text com-\npletion task. The pre-training lacks an objective that allows\nLLMs to respond to text prompts. In InstructGPT [36], the\npaper behind ChatGPT 3.5, a two-stage finetuning solution\nis proposed to align GPT-3 to answer instructions accord-\ning to human preferences. In the first stage, InstructGPT\nemploys SFT with a small dataset of hand-crafted prompt-\nanswer pairs. While SFT changes the model\u2019s output dis-\ntribution from text completion to answering instructions,\nit also introduces hallucination [44]. This is because the\noutput distribution drifts too much towards the instruction-\nfollowing dataset, and the model tries to imitate the be-\nhavior in the data and always provide plausible answers\neven when the model is uncertain about the answer [44].\nTo address this issue, InstructGPT opts for a light-weight\nSFT stage and relies on RLFT in the second stage, using a\nhuman-preference reward model. This approach provides\ngeneral alignment to human values and causes minimal hal-\nlucination [44], because RLFT does not rely on a poten-\ntially biased dataset containing fixed ground-truth answers,\nbut instead learns the general concept of human-preference\nthrough the reward model. The success of InstructGPT [36]\nand its analogy to the distribution shift problem in multi-\nview SFT [24] motivate us to pursue RLFT for 2D diffusion\nmodels.\nRLFT of Diffusion Models\nWitnessing the success of\nRLFT methods in LLMs [2, 3, 22, 36], recently, a few\nRLFT algorithms have been proposed for text-to-image dif-\nfusion models. RWR [23] is the first work to bring the hu-\nman feedback reward finetuning idea to diffusion models.\nWhile RWR only finetunes stable diffusion [42] via a sin-\ngle log probability of the entire denoising process, multi-\nstep RLFT can be facilitated by treating the denoising pro-\ncess as a multi-step MDP, as demonstrated in DDPO [5] and\nDPOK [15]. Our RLFT is based on DDPO [5], while our\nKL-divergence regularization is similar to DPOK [15] and\nInstructGPT [36]. Furthermore, RWR, DDPO, and DPOK\nall finetune SD-1.5 [42], while we finetune a much larger\ndiffusion model based on SDXL. We also study training\nstability, a notorious challenge in both traditional RL and\nRLFT [7, 59], and scaling laws [20] for RLFT.\n3. Multi-view Reconstruction Consistency\nIn this section, we propose the Multi-view Reconstruc-\ntion Consistency (MRC) metric, for quantitative and robust\nevaluation of the consistency of multi-view images, which\nwe define to be the degree of geometry and appearance uni-\nformity of an object across the views.\n3.1. Evaluate Consistency via NeRF Reconstruction\nNeRF [31] is widely adopted as the 3D representation\nfor learning text-to-3D tasks. A 3D model represented by\nNeRF can be reconstructed from the view images of the ob-\nject and their corresponding camera poses. The quality of\na NeRF notably depends on the consistency of the provided\nimages images [31, 52] \u2013 inconsistent views lead to artifacts\nin the NeRF, which includes floaters, blurring, and broken\ngeometry. To address this challenge, we introduce a metric\nfor assessing the consistency among multiple views.\nThe intuition behind MRC comes from the relation-\nship between multi-view consistency and the reconstructed\nNeRF. As shown in Figure 3, when the multi-view images\nare consistent, they can produce a well reconstructed NeRF,\npreserving almost all the visual cues from the input im-\nages; therefore, the views rendered from the NeRF at the\nsame camera viewpoints will look the same as the original\nviews; conversely, when the multi-view images are incon-\nsistent (e.g., intentionally introduced inconsistency in Fig-\nure 3), they will produce a NeRF with broken geometry and\nfloater artifacts; thus, the NeRF rendered views will look\ndifferent from the original views. Building upon this ob-\nservation, we propose the MRC metric, defined as the im-\nage distances between the original multi-view images and\nthe views of the reconstructed NeRF rendered at the same\nviewpoints, as illustrated in Figure 2.\n3.2. Implementation\nWe formulate the implementation of MRC as three parts:\nfast sparse-view NeRF reconstruction, measuring image\ndistance between the input images and the rendered images,\nand a normalization technique for the image distance. The\npseudo code for our MRC implementation is shown in List-\ning 1 in Appendix.\nFast Sparse-view Reconstruction\nWe conduct NeRF re-\nconstruction with sparse-view Large Reconstruction Model\n4\nGT 4th view\n50x50 inpaint 4th view\n110x110 inpaint 4th view\nInput\nNeRF Rendered\nGT 4 views\nFigure 3. Qualitative correlation between MRC and inconsistency with increasing intensity, introduced by inpainting with increasing mask\nsizes. Left: the four ground truth views. Right: the 4th view is inpainted with increasing area sizes, i.e. 0\u00d70, 50\u00d750 and 110\u00d7110 pixels.\nThe top row is the image after inpainting and the bottom row is the image rendered from the NeRF reconstructed with the top inpainted 4th\nview and the other 3 original views. We mark the inpainting area with blue and red boxes. Since the lion\u2019s right paw in the inpainted 4th\nviews look different from the other three original views, its geometry is broken in the NeRF rendered views. This difference is captured in\nMRC\u2019s image dissimilarity metric.\nFigure 4. Quantitative correlation between MRC and inconsis-\ntency with increasing intensity, for the object shown in Figure 3.\nAs inconsistency intensity rises, MRC also monotonically in-\ncreases.\n(LRM) proposed in [17, 24]. Different from dense view\nNeRF reconstruction [8, 31, 33], sparse-view LRM recon-\nstructs a NeRF with only 4-6 view images. Also, with its\nfeed-forward reconstruction, it can achieve a speed two or-\nders of magnitude faster than previous optimization-based\nreconstruction methods. MRC leverages all multi-view im-\nages for both NeRF reconstruction and 3D consistency eval-\nuation. Although the NeRF is reconstructed based on the\nvisual prior of the input multi-views images, the render-\ning from the same views still exhibits notable differences\nif there is inconsistency inside the input, as shown in Fig-\nure 3.\nImage Distance Metric\nIn Section 3.1, the consistency\nproblem is reduced from 3D to a 2D image dissimilar-\nity problem. To measure the image dissimilarity between\nthe input views and their corresponding NeRF rendered\nviews, we utilize the perceptual image distance metric,\nLPIPS [56]. LPIPS exhibits smoother value changes with\nrespect to the consistency of multi-view images compared\nto PSNR, SSIM, L1, and L2, as shown in Figure 14 in Ap-\npendix). Such smoothness is derived from the non-pixel-\naligned computation in LPIPS, as opposed to the other im-\nage distance metrics that are more pixel-aligned. Also, the\nsmoothness is a crucial aspect for MRC to serve as the re-\nward function in RLFT, because non-smooth, high-variance\nreward functions makes the RLFT training more challeng-\ning.\nBounding-box Normalization\nCurrent multi-view diffu-\nsion models [24, 28, 46, 58] target single object generation\nwith background. Consequently, if computing LPIPS on\nthe entire image, trivially reducing the object\u2019s relative size\n(as illustrated in Appendix\nFigure 9\u2019s car example) can\nexploit MRC, as the majority of images will be the white\nbackground. Therefore, we propose normalizing our metric\nwith respect to the object\u2019s size. Specifically, we identify\nthe smallest square bounding box of the foreground object\nin the input view image. Then we crop both the input im-\nages and the rendered images with that bounding box, resize\nthem to a fixed resolution, and evaluate the LPIPS. This nor-\nmalization effectively prevents the reward hacking of MRC\nby diminishing foreground object sizes, as shown in Fig-\nure 9 in Appendix.\n3.3. Metric Experiment\nThe two key objectives for introducing the MRC met-\nric are (1) to assess the consistency of any multi-view gen-\nerative model and (2) to enable RLFT for improving the\n5\nconsistency of multi-view diffusion models. Thus, the pro-\nposed consistency metric should ideally present two respec-\ntive properties: (1) MRC should monotonically increase\nas inconsistency increases; (2) the MRC vs. inconsistency\ncurve should be smooth.\nTo validate the effectiveness and robustness of MRC, i.e.\nwhether it satisfies the two properties, we conduct evalua-\ntion on sets of multi-view images with controlled level of\ninconsistency. Starting from a set of perfectly-consistent\nground truth views rendered from a 3D asset from Obja-\nverse [12], we manually introduce inconsistency to one im-\nage. We select a portion of this image and inpaint it with an\nimage-to-image diffusion model1. Therefore, we get differ-\nent levels of distortion on one image, determined by the size\nof the inpainting area, that corresponds to different levels of\ninconsistency of the set of images.\nFigure 3 shows the qualitative result on one object of our\nMRC metric experiment. With increased inpainting area\nsize, the NeRF rendered view also shows larger image dif-\nference, which is then captured by MRC\u2019s image distance\nmetric, LPIPS. Figure 4 presents the quantitative curve of\nthe same experiment. MRC indeed shows a monotonically\nincreasing pattern as the views become more inconsistent.\nAs shown in Figure 14, MRC constantly exhibits mono-\ntonically increasing pattern, and it is also smoother than\nthe other MRC variants using PSNR, SSIM, L1, and L2.\nFor metric experiments on other distortion types, see Ap-\npendix D.\n4. RLFT for Multi-view Consistency\nIn the previous section, we proposed a fast and reliable\nmulti-view consistency metric, and in this section we de-\nscribe how it can be used to finetune a multi-view diffusion\nmodel. We propose RLFT for enhancing the consistency of\n2D multi-view diffusion models, using the negative MRC as\nthe reward function (Figure 2). Building upon DDPO [5],\nwe opt for its pure on-policy policy gradient algorithm over\nthe default partially on-policy version for substantially im-\nproved training stability. To maintain proximity to the base\nmodel, we incorporate KL divergence regularization similar\nto [15, 36]. In addition, we scale up the RLFT to achieve\nhigher rewards by studying the scaling laws [20] of diffu-\nsion model RLFT through extensive experiments.\n4.1. Preliminaries on DDPO\nMarkov Decision Process\nTo use RL for finetuning, we\nneed to formulate the task as a Markov Decision Process\n(MDP). In a MDP, an agent interacts with the environ-\nment at discrete timesteps; at each timestep t, the agent\nis at a state st, takes an action at according to its policy\n1We use Adobe Photoshop\u2019s Generative Fill [1] without text prompt to\nadd inpainting distortion, which is based on a diffusion model.\n\u03c0(at|st), receives a reward rt, and transitions to the next\nstate st+1. Following denoising diffusion policy optimiza-\ntion (DDPO) [5], the denoising process of a diffusion model\nis formulated as a multi-step MDP:\nst = (c, t, xt),\nat = xt\u22121,\n\u03c0(at|st) = p\u03b8(xt\u22121|c, t, xt),\nr(st, at) =\n(\nr(x0, c)\nif t = 0,\n0\notherwise,\nr(x0, c) = \u2212MRC(x0)\nwhere each denoising step is a timestep, c is the context, i.e.\nthe text prompt, xt is the image being denoised at step t,\np\u03b8 is the diffusion model being finetuned, xT is the initial\nnoisy image, x0 is the fully denoised image, and r(x0, c) is\nthe negative MRC (Listing 1 in Appendix) computed on the\nfully denoised image.\nPolicy Gradient\nIn order to optimize the model with re-\nspect to the reward function, a family of RL algorithms,\nknown as policy gradient methods, are commonly adopted,\nsuch as REINFORCE [53] and Proximal Policy Optimiza-\ntion (PPO) [45]. DDPOSF is based on the vanilla policy\ngradient algorithm, REINFORCE [53], also known as the\nScore Function (SF). On the other hand, DDPOIS builds\nupon PPO [45] and conducts multiple optimization steps per\nround of data using an importance sampling (IS) estimator\nand importance weight clipping.\nAs a common practice to reduce the variance of the pol-\nicy gradients [32], DDPO [5] uses the advantages, which are\nrewards normalized to have zero mean and unit variance, in-\nstead of directly using the rewards. Specifically, the mean\nand standard deviation statistics of the rewards are tracked\nfor each prompt c:\nAr(x0, c) = r(x0, c) \u2212 \u00b5r(c)\n\u03c3r(c)\n(1)\nDDPO\u2019s normalizing advantage replaces the value model\nthat is more widely adopted in PPO-based RLHF meth-\nods [36, 50, 54]. This is similar to the recent work [25],\nwhich shows that the value model creates unnecessary com-\nputation cost that can be replaced with a simpler advantage\nformulation.\nBy using the advantage term (Equation (1)) in place of\nthe reward, the DDPOSF policy gradient function is:\n\u02c6gSF = E\n\" T\nX\nt=0\n\u2207\u03b8 log p\u03b8(xt\u22121|c, t, xt)Ar(x0, c)\n#\n(2)\nwhere the expectation is taken over data generated by the\npolicy \u03c0\u03b8 with the parameters \u03b8.\nThe log probability\n6\nFigure 5. Reward curves on the testing set with 4 different random\nseeds for IS and SF versions, where negative MRC is used as the\nreward. The IS version produces reward curves with high variance,\nincluding two runs that fail to improve the reward and collapse. In\ncontrast, the SF version stably produces reward curves with low\nvariance.\na pug wearing a bee costume\nFigure 6. When we only plot KL divergence without incorporating\nKL regularization, we observe qualitative correlation between the\nKL value and the prompt alignment degradation. Despite being\ndistant in the finetuning process, epoch 15 and epoch 45, which\nhave lower KL divergence to the base model, generates prompts\nbetter aligned with the prompts. On the other hand, epoch 30,\nwhich has much higher KL divergence to the base model, gener-\nates results with broken identity, i.e. the body of the pug is miss-\ning.\nlog p\u03b8(xt\u22121|c, t, xt) can be easily obtained since the pol-\nicy is an isotropic Gaussian distribution when using the\nDDIM sampler [5, 47].\nThe DDPOIS (Equation (6) in\nAppendix) function has an additional importance sampling\nterm than Equation (2).\nBlack et al. [5] choose DDPOIS as the default policy\ngradient function, because it exhibits better sample effi-\nciency than DDPOSF (Fig. 4 of [5]). Such choice is con-\nsistent with the use of PPO [45] in LLM RLHF litera-\nture [2, 3, 36, 50, 54].\n4.2. Improvements over DDPO\nWhile RLFT using the default DDPOIS coupled with\nMRC can enhance the 3D consistency of multi-view diffu-\nsion models, it still faces challenges regarding training sta-\nbility, the shift of output distributions, and an unclear train-\ning scale setting to achieve optimal rewards with minimal\ndistribution shift. To address these issues, we propose three\nimprovements over DDPO [5] in this section. Given the\nuniversal nature of these challenges in RLFT, our enhance-\nments may offer broader applicability across various tasks.\n4.2.1\nPure On-policy Training\nTraining stability is a major challenge in both RLFT [7,\n59] and traditional RL [14].\nWith the default DDPOIS,\nour training process is evidently unstable, as shown in Fig-\nure 5. Training experiments with different random seeds\nor a slight change of hyperparameters can lead to different\nreward curves and qualitative results. This complicates the\ntraining result evaluation as we cannot distinguish mean-\ningful improvement or deterioration from the variance in-\ntroduced by random seed.\nWe argue that such high variance is derived from the\nmulti-step update in DDPOIS [5], originally proposed in\nPPO [45]. While it theoretically allows for better sample\nefficiency similar to off-policy methods [45], it also causes\nthe training to be more unstable and the reward curves to be\nmore variant, because it uses data collected with the older\npolicy to update the newer policy. Due to the undesirable\nconsequences of training instability, we adopt the pure on-\npolicy variant DDPOSF, discarding the multi-step update\nfrom PPO (Equation (6) in Appendix). As shown in Fig-\nure 5, DDPOSF significantly improves the training stability\nof our RLFT, while maintaining a comparable sample effi-\nciency as the default DDPOIS.\nDiverging from DDPO [5] and most LLM RLHF lit-\nerature [2, 3, 36, 50, 54], we choose REINFORCE [53]\n(DDPOSF) over PPO [45] (DDPOIS) for its superior training\nstability. We provide two hypotheses behind our surprising\nfinding in Appendix B.3, including the difficulty of the task\nreward function and the size of the model being finetuned.\nThe favored use of REINFORCE [53] over PPO [45] could\napply to broader scenarios that meet these two conditions.\nWe leave the verification of our hypotheses as future work.\n4.2.2\nKL Divergence Regularization\nIn RLFT methods, distribution shift (also known as re-\nward overoptimization) can lead to low-quality results, such\nas cartoon-like, less realistic style [5] or oversaturated col-\nors and unnatural shape [15], despite achieving high re-\nwards. In our case, we observe this as degradation of diver-\nsity, texture details and prompt alignment after prolonged\nRLFT with the MRC reward. Previous methods [15, 36]\nsuggest mitigating reward overoptimization by incorporat-\ning a penalty on the KL divergence between the log proba-\nbilities of the outputs from the base and the finetuned mod-\n7\nels.\nIn our case, the base model is Instant3D-10K [24]\nwithout any additional finetuning. By plotting the KL di-\nvergence values during finetuning, we also find that KL di-\nvergence correlates to the reward overoptimization problem\n(Figure 6), suggesting us to adopt KL divergence regular-\nization.\nFollowing the widely adopted implementation in LLM\nRLHF [36, 54], we incorporate KL penalty into the reward\nfunction. Subtraction of the log probabilities is commonly\nused to approximate the full KL divergence [50, 54]:\nKL (log p\u03b8(x0|c, T, xT )|| log p\u03b8base(x0|c, T, xT ))\n=\nT\nX\nt=0\nlog p\u03b8(xt\u22121|c, t, xt) \u2212 log p\u03b8base(xt\u22121|c, t, xt)\nT + 1\n(3)\nwhere p\u03b8base is the base model. We will denote this approx-\nimated KL divergence term as KL(x0|c, xT ) for clarity in\npresentation.\nKL divergence values starts at 0 and unavoidably in-\ncreases as finetuning proceeds, making it hard to determine\nan optimal coefficient for the penalty term. To enable a\nsteady KL divergence regularization throughout the finetun-\ning process, we propose to normalize the KL divergence\npenalty term. This normalization ensures that the gradient\nconsistently favors low-KL-divergence, high-reward sam-\nples, even in the early stages when KL divergence is still\nlow compared to the later stages. We extend DDPO\u2019s [5]\nper prompt stat tracking to also track the mean and standard\ndeviation statistics of the KL divergence term in order to to\nnormalize it:\nAKL(x0, c) = KL(x0|c, xT ) \u2212 \u00b5KL(c)\n\u03c3KL(c)\n.\n(4)\nOur advantage terms now consist of both the normalized\nreward and the normalized KL divergence. Our final policy\ngradient function, used in our experiments, is a combination\nof Equations (2) and (4)\n\u02c6gSF,KL = E\n\" T\nX\nt=0\n\u2207\u03b8 log p\u03b8(xt\u22121|c, t, xt)\n\u00b7 (\u03b1Ar(x0, c) \u2212 \u03b2AKL(x0, c))\n#\n(5)\nwhere \u03b1 and \u03b2 are the coefficients for the reward advantage\nand the KL advantage, respectively.\n4.2.3\nScaling Laws for RLFT\nThe training of RL is highly sensitive to the chosen scale\nsetting [14], impacting various results, including the final\nconverged reward. Through the scaling laws identified from\nextensive experiments, we scale up the amount of com-\npute (equivalent to scaling up the batch size in our case)\nFigure 7. Scaling law for Carve3D\u2019s diffusion model RLFT algo-\nrithm. When we scale up the amount of compute for RLFT, the\nmodel improves its reward smoothly under the optimal data size.\nThe amount of compute scales linearly with respect to the batch\nsize. The reward curves also become more stable (less variant)\nwith a larger batch size. The reward curves are reported up to\nepoch 50.\nto achieve the optimal reward. Although our scaling exper-\niments are only conducted with the multi-view consistency\ntask, our insights into the scaling laws of diffusion model\nRLFT are general and can be beneficial in broader scenar-\nios.\nCompute and Batch Size\nThe reward curves from our\nexperiments demonstrate a positive scaling law of the\nmodel\u2019s reward at epoch 50 with respect to the amount of\ncompute (Fig. 7); the scaled up compute brings smooth im-\nprovement to the model\u2019s reward, under the optimal data\nsizes at each batch size. Note that the amount of compute\nscales directly with respect to the batch size.\nData Size\nThe model\u2019s reward does not directly scale with\nthe data size but there exists a more complicated relation-\nship between them. As shown in Figure 7, the optimal data\nsize at each batch size grows as the batch size get larger,\nindicating that both factors need to be scaled up in tandem;\nafter the optimal data size, naively continuing to scale up the\ndata size actually reduces the model\u2019s reward. Surprisingly,\neven when trained on a prompt set as small as a size of 10,\nthe model still shows generalization to the testing prompts.\nWe choose data size of 30 with batch size 768 in our final\nexperiments as it achieves the highest reward in our analy-\nsis.\nTraining Epochs\nWith the pure on-policy DDPOSF (RE-\nINFORCE [53]), the model steadily and smoothly im-\nproves its rewards throughout the finetuning process, mean-\ning that more training epochs constantly lead to higher\n8\nFigure 8. We conducted a user study with 20 randomly selected\ntesting prompts and the corresponding outputs from both the base\nand fine-tuned model. 15 participants took part in the study, with\na majority favoring the 3D consistency of our fine-tuned model.\nOpinions are evenly split on which has better prompt alignment.\nreward.\nHowever, from our qualitative results, we also\nobserve worse distribution shift, e.g.\nthe degradation of\nprompt alignment and texture details, as training epoch in-\ncreases.\nDue to the correlation between KL divergence\nand the quality degradation (Figure 6), we stop the fine-\ntuning early when a predefined KL divergence threshold\nis reached. This threshold is empirically chosen based on\nqualitative results. For fair comparisons, we report the re-\nward curves up to epoch 50 in Figure 7. See Appendix B.1\nfor the definition of epoch in RLFT, which is different from\nits definition in supervised learning.\n5. Experiments\nIn this section, knowing that Carve3D\u2019s RLFT steadily\nimproves the MRC reward on the training set (Figure 5),\nwe aim to answer the following questions:\n1. Does Carve3D\u2019s improved MRC generalize to the\ntesting set?\n2. Qualitatively, is Carve3D more consistent than the\nbase model? And comparing to longer SFT?\n3. Qualitatively, does Carve3D sacrifice the diversity,\nprompt alignment, and texture details of the base\nmodel?\nWe quantitatively and qualitatively compare Carve3D with\nthree versions of Instant3D [24], with 10K, 20K, and 100K\nSFT steps respectively. Instant3D-10K is both the default\nmodel in [24] and also our base model. For fair qualitative\ncomparisons, results from each model are generated from\nthe the same initial noise. Since 20K and 100K versions of\nInstant3D and Carve3D are all finetuned from Instant3D-\n10K, their output tend to represent the same object when\ngiven the same initial noise (e.g. Figures 1 and 6).\nAvg MRC on Testing Set \u2193\nMVDream\n0.1222\nInstant3D-10k (Base)\n0.0892\nInstant3D-20k\n0.0795\nInstant3D-100k\n0.0685\nCarve3D (Ours)\n0.0606\nTable 1. Carve3D (RLFT with MRC reward on Instant3D-10k),\nachieves substantially better MRC than baselines, which corre-\nsponds to better multi-view 3D consistency. We evaluate these\ntext-to-multiview diffusion models on the DreamFusion testing\nset, containing 414 text prompts. We generate 4 outputs for each\nprompt, and the average MRC is computed over the 1656 outputs.\nFor each output, we use the same randomly sampled initial noise\nfor all models to ensure the comparison is fair.\n5.1. Comparison with Base Model and Longer SFT\nQuantitative\nComparison\nand\nGeneralization\nAs\nshown in Table 1, when evaluated on the testing set,\nCarve3D achieves substantially improved MRC over\nthe base model.\nMore SFT steps indeed provides better\nmulti-view consistency and achieves better MRC, with\nInstant3D\u2019s 100K version performing the best and 10K\nversion performing the worst.\nHowever, Carve3D still\noutperforms even the most consistent 100K version of\nInstant3D by a notable gap. This suggests that the explicit\nmulti-view consistency objective in MRC, paired with our\nRLFT algorithm, can improve the model\u2019s consistency\nmore efficiently than SFT. Furthermore, our RLFT provides\ngeneralization of the improved multi-view consistency, al-\nthough only finetuned on 30 prompts. Such generalization,\nalso observed in [5, 36], is likely derived from the strong\nknowledge from the base model.\nMulti-view Consistency and NeRF Artifacts\nFigure 12\nshows the improved multi-view consistency and the result-\ning NeRF reconstruction quality. While the multi-view im-\nages generated by the base model may be inconsistent, caus-\ning artifacts such as floaters and broken geometry, Carve3D\ncan fix such inconsistency in the multi-view images and\nproduce NeRF with clean geometry, free of artifacts. In\nFigure 11, Carve3D continues to show superior multi-view\nconsistency and reduced NeRF artifacts, but such improve-\nment is less and further less obvious when compared to the\n20K and 100K version of Instant3D [24], which aligns with\nthe qualitative results in Table 1.\nPrompt Alignment and Texture Details\nBy virtue of\nour RLFT with KL-divergence regularization (Section 4.2),\nwhich prevents distribution shift, and curated low-reward\nprompts, which describes complex objects (Appendix C.1),\nCarve3D preserves the prompt alignment and the texture\ndetails of the base model, as shown in Figure 12. On the\n9\nother hand, longer SFT causes additional distribution shift\nin Instant3D [24] from the base SDXL [39] towards the SFT\ntraining set [12]. As shown in Figure 11, Instant3D-20K\nand Instant3D-100K exhibits degradation in diversity, real-\nism, and level of detailed textures. This quality degradation\nwith longer SFT is also observed in [24].\nDiversity\nAs shown in Figure 13, Carve3D can preserve\nthe diversity of the base model. This owes to our RLFT pro-\ncess, which repeatedly samples different initial noises for\nthe diffusion model to generate diverse results (Figure 2).\n5.2. Evaluating Existing Methods\nAt the time of writing, MVDream [46] is the only text-to-\nmulti-view diffusion model with released code and model\nweights. As shown in Table 1 and Fig. 11, its outputs have\nnotably worse multi-view consistency, realism, and level of\ndetails than all three Instant3D variants and Carve3D. This\nsupports our claim in Section 1 that current multi-view dif-\nfusion models that solely rely on SFT still suffer from the\ninconsistency problem and could benefit from RL finetun-\ning.\n5.3. User Study\nIn addition to the quantitative and qualitative compar-\nisons in Section 5.1, we conducted a user study to further\nunderstand the qualitative results of Carve3D\u2019s RLFT when\nperceived by human. To run the study we randomly se-\nlected 20 unseen testing prompts. For each text prompt,\nwe generated a pair of data from both the base and the fine-\ntuned models with the same initial noise. Then, we pro-\nvided both the tiled 4-view image and the turntable video\nof the reconstructed NeRF to participants and asked them\nthe following two questions: (1) Which result is more 3D-\nconsistent? and (2) Which result is better aligned with the\nprompt? As shown in Figure 8, 68.33% of participants be-\nlieve that Carve3D\u2019s generated results are more 3D consis-\ntent than that of the base model [24]. Given that the multi-\nview consistency in the base model has already been much\nimproved with SFT 2, the nearly 37% gain in human prefer-\nence introduced by Carve3D on randomly selected testing\nprompts is impressive. Furthermore, the Carve3D finetuned\nmodel exhibits similar prompt alignment, as participants\u2019\nvotes are evenly distributed among ours, base model, and\n\u201csame\u201d. The preservation of alignment can be attributed to\nthe KL divergence regularization (Section 4.2) as well as\nearly stopping the RLFT according to KL divergence (Sec-\ntion 4.2.3).\n2Please see https://jiahao.ai/instant3d/ for base model\u2019s 3D consistency\n6. Conclusion\nIn this paper, we propose Carve3D, an RL finetuning\nmethod to improve the reconstruction consistency of 2D dif-\nfusion models. The reward of Carve3D relies on MRC, a\nnovel metric that measures the reconstruction consistency\nby comparing input multi-view images with the renderings\nof the reconstructed NeRF at the same viewpoints. The ef-\nfectiveness and robustness of MRC are also validated by\nshowing its correlation with intentional distortions. Lastly,\nwe conduct experiments and a user study to show that\nCarve3D significantly improves the reconstruction consis-\ntency of multi-view images and the resulting quality of the\nNeRF. These enhancements are achieved without sacrificing\nthe prompt alignment, texture details, or prompt alignment\nof the base model.\nOur MRC metric can serve as a valuable tool for eval-\nuating any multi-view generative methods and guiding fu-\nture developments in the field. Although we only demon-\nstrate our RLFT with MRC on one multi-view diffusion\nmodel [24], it can be directly adapted to other text-to-multi-\nview diffusion models; such adaptation only requires tun-\ning a few hyperparameters related to the scaling laws for\ndiffusion model RLFT (Section 4.2.3). Our surprising find-\ning behind the choice of REINFORCE [53] over PPO [45]\nfor better training stability could also be applied in broader\nRLFT scenarios.\nAs AI models grow more powerful, it becomes more im-\nportant to evaluate and improve their safety and reduce their\nbias. RLFT has been widely used for LLM alignment as it\nallows models to be finetuned with hard-to-specify objec-\ntives and its results are generalizable without undermining\nthe base model\u2019s knowledge. As the first work to use RLFT\nfor text-to-3D and on diffusion models at the SDXL scale,\nwe hope Carve3D can inspire more alignment research in\nthe computer vision community.\nReferences\n[1] Adobe.\nAdobe firefly.\nhttps://www.adobe.com/\nsensei/generative- ai/firefly.html, 2023.\nAccessed: 2023-11-15. 6\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,\nAnna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,\nDeep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Ka-\ndavath, Jackson Kernion, Tom Conerly, Sheer El-Showk,\nNelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tris-\ntan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,\nNeel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, Ben Mann, and\nJared Kaplan. Training a helpful and harmless assistant with\nreinforcement learning from human feedback, 2022. 4, 7, 1\n[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda\nAskell, Jackson Kernion, Andy Jones, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol\n10\nChen, Catherine Olsson, Christopher Olah, Danny Her-\nnandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\nJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey\nLadish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,\nLiane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,\nRobin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Tamera Lanham, Timo-\nthy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan\nHume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann,\nDario Amodei, Nicholas Joseph, Sam McCandlish, Tom\nBrown, and Jared Kaplan. Constitutional ai: Harmlessness\nfrom ai feedback, 2022. 4, 7, 1\n[4] Kevin Black. ddpo-pytorch. https://github.com/\nkvablack/ddpo-pytorch, 2023. Accessed: 2023-11-\n17. 2\n[5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine.\nTraining diffusion models with reinforce-\nment learning, 2023. 2, 4, 6, 7, 8, 9, 1\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 4\n[7] Stephen\nCasper,\nXander\nDavies,\nClaudia\nShi,\nThomas Krendl Gilbert, J\u00b4er\u00b4emy Scheurer, Javier Rando,\nRachel Freedman, Tomasz Korbak, David Lindner, Pe-\ndro Freire, Tony Wang, Samuel Marks, Charbel-Rapha\u00a8el\nSegerie, Micah Carroll, Andi Peng, Phillip Christoffersen,\nMehul Damani, Stewart Slocum, Usman Anwar, Anand\nSiththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau,\nDmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter\nHase, Erdem B\u0131y\u0131k, Anca Dragan, David Krueger, Dorsa\nSadigh, and Dylan Hadfield-Menell.\nOpen problems and\nfundamental limitations of reinforcement learning from\nhuman feedback, 2023. 4, 7\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 2, 5\n[9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting, 2023. 1, 3\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 1\n[11] Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.\nDirectly fine-tuning diffusion models on differentiable re-\nwards, 2023. 4\n[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects, 2022. 1, 2, 3, 6, 10\n[13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Anirud-\ndha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana\nEhsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A\nuniverse of 10m+ 3d objects, 2023. 1, 3\n[14] Theresa Eimer, Marius Lindauer, and Roberta Raileanu. Hy-\nperparameters in reinforcement learning and how to tune\nthem, 2023. 7, 8\n[15] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:\nReinforcement learning for fine-tuning text-to-image diffu-\nsion models, 2023. 2, 4, 6, 7\n[16] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[17] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d, 2023. 2, 3, 5\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2\n[19] Heewoo Jun and Alex Nichol. Shap-e: Generating condi-\ntional 3d implicit functions, 2023. 3\n[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models, 2020. 4, 6\n[21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 3\n[22] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,\nThomas Mesnard, Colton Bishop, Victor Carbune, and Ab-\nhinav Rastogi. Rlaif: Scaling reinforcement learning from\nhuman feedback with ai feedback, 2023. 4\n[23] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu.\nAligning text-to-\nimage models using human feedback, 2023. 4\n[24] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model,\n2023. 1, 2, 3, 4, 5, 8, 9, 10\n[25] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, and\nZhi-Quan Luo. Remax: A simple, effective, and efficient\nreinforcement learning method for aligning large language\nmodels, 2023. 6\n[26] Minghua Liu,\nChao Xu,\nHaian Jin,\nLinghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-\nshape optimization, 2023. 3\n[27] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 1, 3\n11\n[28] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n1, 3, 5\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 2\n[30] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von\nPlaten, Apolin\u00b4ario Passos, Longbo Huang, Jian Li, and Hang\nZhao. Lcm-lora: A universal stable-diffusion acceleration\nmodule, 2023. 4\n[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 4, 5\n[32] Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza,\nAlex Graves, Timothy P. Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning, 2016. 6\n[33] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2, 5\n[34] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts, 2022. 3\n[35] OpenAI.\nChatgpt.\nhttps://chat.openai.com/,\n2023. Accessed: 2023-11-15. 1, 2\n[36] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to follow instructions with\nhuman feedback, 2022. 1, 2, 3, 4, 6, 7, 8, 9\n[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n2\n[38] Andr\u00b4e Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas\nBeyer, and Xiaohua Zhai. Tuning computer vision models\nwith task rewards, 2023. 4\n[39] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 3, 10, 1, 4\n[40] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 1,\n3, 2\n[41] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Ka-\nterina Fragkiadaki. Aligning text-to-image diffusion models\nwith reward backpropagation, 2023. 4\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2022. 1, 3, 4\n[43] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models, 2022. 3\n[44] John Schulman.\nRl and truthfulness: Towards truthgpt.\nYouTube, 2023. 1, 4\n[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms, 2017. 2, 6, 7, 10, 1\n[46] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 1, 3, 5, 10\n[47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models, 2022. 7\n[48] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 1, 3\n[49] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 2\n[50] Leandro von Werra, Younes Belkada, Lewis Tunstall, Ed-\nward Beeching, Tristan Thrush, Nathan Lambert, and\nShengyi Huang. Trl: Transformer reinforcement learning.\nhttps://github.com/huggingface/trl, 2020. 6,\n7, 8, 1, 2\n[51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation, 2022. 1, 3\n[52] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models, 2022.\n3, 4\n[53] Ronald J Williams. Simple statistical gradient-following al-\ngorithms for connectionist reinforcement learning. Machine\nlearning, 8:229\u2013256, 1992. 2, 6, 7, 8, 10, 1\n[54] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase,\nSamyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan,\nJeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes,\nZhongzhu Zhou,\nMichael Wyatt,\nMolly Smith,\nLev\nKurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che,\nShuaiwen Leon Song, and Yuxiong He.\nDeepspeed-chat:\nEasy, fast and affordable rlhf training of chatgpt-like mod-\nels at all scales, 2023. 6, 7, 8, 1, 2\n[55] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng\nZhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-\ndreamer: Fast generation from text to 3d gaussian splatting\nwith point cloud priors. arxiv:2310.08529, 2023. 1, 3\n[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 2, 3, 5\n[57] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih\nChen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese,\n12\nStefano Ermon, Caiming Xiong, and Ran Xu. Hive: Har-\nnessing human feedback for instructional visual editing.\n2023. 4\n[58] Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li,\nZeng Zhao, Zhipeng Hu, Changjie Fan, and Xin Yu.\nEf-\nficientdreamer:\nHigh-fidelity and robust 3d creation via\northogonal-view diffusion prior, 2023. 1, 3, 5\n[59] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei\nShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao\nZhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin\nLai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang\nWeng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang\nYan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang.\nSecrets of rlhf in large language models part i: Ppo, 2023. 4,\n7\nA. Appendix Summary\nIn the appendix, we cover additional details of DDPO\nand RLFT (Appendix B, the training data details (Ap-\npendix C.1), training details (Appendix C.2), additional\nMRC metric experiments (Appendix D), ablation studies\n(Appendix E), and future work (Appendix F).\nB. Additional Details of DDPO and RLFT\nB.1. Definitions\nFollowing [5, 36, 45, 54], an epoch is defined as one\nround of data collection (sampling), which may consists\nmultiple PPO update steps (training), as discussed in Equa-\ntion (6) and Sec. 4.2. This definition of \u201cepoch\u201d is differ-\nent from the meaning in supervised learning which usually\nrefers to go through all data once. Since we opt for us-\ning pure on-policy training (vanilla policy gradient), as dis-\ncussed in Section 4.2, we only do one training step per sam-\npling step, and thus our sampling batch size and training\nbatch size are equal.\nB.2. DDPOIS Policy Gradient Function\nBy using the advantage term (Equation (1)) in place of\nthe reward, the DDPOIS policy gradient function is:\n\u02c6gIS = E\n\" T\nX\nt=0\np\u03b8(xt\u22121|c, t, xt)\np\u03b8old(xt\u22121|c, t, xt)\n\u00b7 \u2207\u03b8 log p\u03b8(xt\u22121|c, t, xt)Ar(x0, c)\n#\n(6)\nwhere the expectation is taken over data generated by the\npolicy \u03c0\u03b8old with the parameters \u03b8old.\nB.3. Hypotheses on Stability and Sample Efficiency\nDiverging from DDPO [5] and most LLM RLHF lit-\nerature [2, 3, 36, 50, 54], we choose REINFORCE [53]\n(DDPOSF) over PPO [45] (DDPOIS) for its superior training\nstability. We provide two hypotheses behind our surprising\nfinding.\n(1) Training stability is more vital than sample efficiency\nwhen the task reward function is more challenging. When a\nreward function is more variant with respect to the model\u2019s\noutput, it becomes more difficult for the model to discover\nthe pattern of high-reward outputs and to improve its re-\nwards. The high-variance prompt alignment reward curves\nin Fig. 5 of DDPO [5] indicates the challenging nature of\nthe prompt alignment task as opposed to the smooth reward\ncurves for the aesthetics and compressibility tasks in Fig. 4\nof DDPO [5].\n(2) The RLFT sample efficiency is less important for a\nlarge model which requires less finetuning steps, as demon-\nstrated in studies of LLM instruction finetuning [10]. Simi-\nlarly, our RLFT on a 2.6B-parameter UNet from SDXL [39]\nonly takes 55 epochs, as opposed to DDPO\u2019s [5] RLFT on a\n860M-parameter UNet from SD 1.4 [42] using 200 epochs.\nTherefore, the potential sample efficiency gain provided by\nthe multi-step update of PPO [45] gets outweighted by the\ntraining stability provided by REINFORCE [53].\nThe favorableness of REINFORCE [53] could apply to\nbroader scenarios that fits these two conditions. We leave\nthe verification of our hypotheses as future work.\nC. Implementation Details\nC.1. Training Data\nAn advantage of RLFT over SFT is that, we can man-\nually create a high-quality text prompts training set, while\ncreating a dataset of diverse ground truth multi-view im-\nages for these high-quality text prompts is prohibitively ex-\npensive for SFT. By relying on samples generated by the\nmodel itself to compute the reward and the loss, RLFT can\noptimize a model beyond the limitation of a dataset and\npreserves the diversity and the style of the base model. In\nCarve3D, our training prompts preparation process involves\ntwo strategies.\nTraining Data Curation\nInstead of randomly sampling\nprompts from a dataset, we employ a data curation strategy\nwhere prompts with lowest rewards are selected. Specif-\nically, we run inference of the base model on a prompt\ndataset, generating four results per prompt, compute the\nMRC rewards for each result, and sort the prompts accord-\ning to their average reward. This is derived from obser-\nvation that, for certain prompts, the model generates nearly\noptimal outputs with rewards close to the rewards of ground\ntruth views of a 3D asset [12] (Figure 4). Thus, the cu-\nrated lowest-reward prompts have substantial room for 3D\nconsistency improvement and prevent learning stagnation.\nThis approach not only brings more efficient training but\n1\ndef compute_mrc(ori_views, ori_cam_poses, lrm, lpips, resize_res):\nnerf = lrm(ori_views, ori_cam_poses)\nnerf_views = nerf.render(ori_cam_poses)\nsquare_bbox = compute_square_bbox(ori_views) # bounding box coordinates for each view\nx_min, y_min, x_max, y_max = square_bbox\nori_views_bbox = [resize(o[:, y_min:y_max + 1, x_min:x_max + 1], resize_res) for o in\nori_views]\n,\u2192\nnerf_views_bbox = [resize(n[:, y_min:y_max + 1, x_min:x_max + 1], resize_res) for n in\nnerf_views]\n,\u2192\nmrc = lpips(ori_views_bbox, nerf_views_bbox).mean()\nreturn mrc\nListing 1. Pseudo code for our MRC implementation. ori views and ori cam poses are the multi-view images to be evaluated and their\ncamera poses. lrm is the sparse-view LRM [17, 24]. lpips the the LPIPS [56] metric. resize res is a fixed resolution to which we resize the\nbounding box patches.\nalso provides a more generalized improvement in 3D con-\nsistency to the testing set.\nCreating New Training Prompt Set\nThe prompt dataset\nfrom DreamFusion [40], which contains 414 prompts and\nis commonly used as testing set. To employ the Dream-\nFusion prompt set also as our testing set, we create a new\nprompt dataset with ChatGPT4 [35]. Following our training\ndata curation strategy, we first sort the DreamFusion [40]\nprompts according to their rewards attained by the base\nInstant3D [24] model. We provide the sorted prompt set\nto ChatGPT4, and ask it to summarize the characteristics\nof the low-reward prompts by looking at the low-reward,\nmedian-reward, and high-reward prompts. ChatGPT4 sum-\nmarizes low-reward prompts to possess properties of \u201ccom-\nplex and creative\u201d. We then ask it to generate 100 low-\nreward prompts that are both complex and creative, and an-\nother 100 low-reward prompts that are \u201ccomplex but not too\ncreative\u201d. For each set, again, we sort the prompts accord-\ning to their rewards, and select those with the lowest re-\nwards to be our training prompt set. Our best results are\nobtained with the \u201ccomplex but not too creative\u201d set.\nC.2. Training Details\nAll of our RL finetuning experiments are run on 6\nAWS EC2 P4de nodes with 8 NVIDIA A100-SXM4-80GB\nGPUs, a total of 48 GPUs. We use batch size of 768, which\nis 2x compared to that of DDPO. One experiment takes 16.5\nhours to reach 55 epochs. The number of finetuning epochs\nis determined by our KL-divergence early-stopping mecha-\nnism, which we empirically choose to be 3.2e\u22124 according\nto the level of reward overoptimization shown on qualitative\nresults.\nWe use minibatches of size 8 during sampling and 4 dur-\ning training due to the limited GPU memory.\nThe total\nbatch size of 768 is evenly distributed among each GPU,\nso that the per GPU batch size is 16. The model samples\ntwo minibatches of size 8 on all GPUs to reach the total\nbatch size. Similarly, the model accumulates gradients over\nfour minibatches of size 4 on all GPUs, before synchroniz-\ning the gradients and performing an optimizer step. We use\na per prompt stat tracker with windows of size 76, so that\nit roughly tracks all the rewards per prompt ever 3 epochs.\nThis is much larger than DDPO\u2019s default tracking window\nof size 32 for better training stability. The coefficients for\nthe advantage terms in Equation (5) are \u03b1 = 1 and \u03b2 = 0.2.\nThe rest of our RL finetuning setup follows DDPO [4, 5].\nWe use the AdamW [29] optimizer with a fixed learning\nrate 3e \u2212 4, \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 1e \u2212 8 and a\nweight decay of 1e\u22124. The high learning rate is paired with\nLow Rank Adaptation (LoRA) [18] finetuning with rank 4,\nwhich significantly reduces the memory and computation\nrequirements for finetuning. We freeze all networks in the\nbase model and set their precision to fp16, and only fine-\ntune the LoRA weights of the unet under fp32 via mixed\nprecision training.\nOur base text-to-multiview diffusion model setup fol-\nlows Instant3D [24], which uses the same architecure as\nSDXL [39]. It produces images of resolution 1024x1024,\nwhich contains four images of resolution 512x512 tiled in\na 2-by-2 fashion. Instant3D requires 100 denoising steps\nduring inference, doubling the required computation than\nthe default 50 steps for SDXL. It uses Classifier Free Guid-\nance [16] with a scale of 5.0\nOur code is mainly based on DDPO\u2019s [5] official imple-\nmentation, the ddpo-pytorch [4] Github repository, which\nuses Hugginface diffusers [49] and PyTorch [37] libraries.\nOur KL divergence regularization implementation is in-\nspired by the codebases of DeepSpeedChat [54], TRL [50],\nand DPOK [15]. We thank the authors of these repositories\nfor releasing the high-quality implementations and promot-\ning open-sourced research. We are going to release the code\nfor computing MRC and our improved DDPO implementa-\ntion. However, due to the fact that Sparse View LRM and\n2\nBase\nFinetuned\nw/ bbox norm\nw/o bbox norm\nFigure 9.\nAlation study on the boundingbox normalization of\nLPIPS for MRC evaluation. Top: with normalization, the 3D ob-\njects keep similar size after finetuning. Bottom: without normal-\nization, the sizes of 3D objects are greatly reduced after RL fine-\ntuning.\nInstant3D do not plan to release their code, we have to leave\nthese as empty, abstract functions in our released code.\nD. Additional MRC Metric Experiments\nDistortion Types\nHere, we show the full results for the\nmetric experiments for the inpainting distortion (Figure 14)\ndiscussed in Section 3.3 and Figs. 3 and 4. We also conduct\nmetric experiments with other distortions types: azimuth ro-\ntation (Figure 15, and elevation rotation (Figure 16). In az-\nimuth and elevation rotation, for one out of the four views,\nwe rotate the object with an azimuth or elevation rotation\nby 3.6 or 4 degrees, before rendering that view, and also use\nthe original camera extrinsic matrix as the input to Sparse\nView LRM. The quantitative results matches our expecta-\ntions, where MRC (LPIPS) monotonically decreases as we\nintentionally add more distortion.\nLPIPS vs. Other Image Similarity Metrics\nHere, we\ncompare substituting LPIPS with L1, L2, PSNR, and SSIM\nin the metric experiments on all distortion types. In the in-\npainiting distortion experiments (Figure 14), which is the\nmost representative of diffusion model\u2019s inconsistencies,\nLPIPS is more linear than other pixel level image metrics.\nIn azimuth and elevation distortion experiments (Figures 15\nand 16), all image metrics shows monotonically decreas-\ning pattern, while pixel-level image metrics are more linear.\nThis is expected as the distortion is pixel-aligned and more\nstructured.\nFigure 10. Ablation study on KL divergence regularization. Top:\nKL Divergence between the base model and the finetuned model\non testing set. Bottom: mean MRC reward on testing set. Our\nKL divergence regularization does not sacrifice the model\u2019s effi-\nciency on improving the reward. Without KL divergence regular-\nization, the finetuned model\u2019s KL divergence to the base model on\nthe training and the testing set grows faster, which results in de-\ngraded object identity and reduced texture details.\nE. Ablation Studies\nBounding Box Normalization\nAs shown in Figure 9,\nwhen the bounding box normalization is removed from\nMRC, the model would trivially increase the reward by re-\nducing the size of the foreground object on the white back-\nground. This would lead to the model generating images\ncontaining only the white background, after longer finetun-\ning. With bounding box normalization, the model would\nlearn the harder task of improving the reconstruction con-\nsistency of the multiview images.\nKL Divergence Regularization\nAs shown in Figure 10\nOur KL divergence regularization does not sacrifice the\nmodel\u2019s efficiency on improving its reward. Without KL\ndivergence regularization, the KL divergence grows much\nfaster. As discussed in Section 4.2, this leads to degraded\nobject identity and loss of texture details.\nF. Limitations and Future Work\nCarve3D is limited by the reconstruction quality of\nSparse View LRM [17, 24]. Because its reconstruction is\nnot perfect, this leads to non-zero MRC metric on GT views\nas shown in Figures 14 to 16. Due to this limitation of\nSparse View LRM, Carve3D RL finetuned model can pro-\nduce less high-frequency details than the base model in or-\n3\nder to lower the image distance to the NeRF rendered views.\nThis might be solved by using a future sparse view recon-\nstructor that can preserve more details or training a dedi-\ncated model for computing MRC.\nFurther increasing data size and batch size to improve\ngeneralization of the improved reconstruction consistency\nis possible. However, in this work, we are limited by the\nhigh computation cost of SDXL [39], Instant3D\u2019s 100 de-\nnoising steps, and the high number of samples needed in\nDDPO. A few concurrent works could address this chal-\nlenge. It is possible to substantially reduce the computa-\ntion cost by switching to Consistency Models for one/few-\nstep inference (e.g., LCM-LoRA [30]). In addition, we can\nalso switch from DDPO to direct backpropagation of reward\n(e.g. Align-Prop [41], and DRaFT [11]) to reduce the num-\nber of samples needed. We leave these extensions as future\nwork.\n4\nA DMC Delorean car\nMVDream\nInstant3D-10k\nInstant3D-20k\nInstant3D-100k\nCarve3D\na beautifully carved wooden knight chess piece\nBlender failed\na bumblebee sitting on a pink flower\na drum set made of cheese\nFigure 11. Qualitative comparison of MVDream, Instant3D with 10k, 20k, and 100k SFT steps, and Carve3D (five columns) on four\nprompts (four blocks separated by dotted line). In each block, we show their generated multi-view images in the 2-by-2 grid (top), re-\nconstructed NeRF and extracted mesh (bottom) when given the prompt (middle). When compared to the base Instant3D-10K: Carve3D\nmaintains the detailed texture and provides improved multi-view consistency and higher quality NeRF; in contrast, the models with pro-\nlonged SFT of 20K and 100K steps exhibit worse level of details and realism, while only providing slightly improved consistency.\n5\na frazer nash super sport car\nInstant3D (Base)\nCarve3D (Finetuned)\na red eyed tree frog, low poly\na group of dogs eating pizza\na chihuahua wearing a tutu\na ficus planted in a pot\nan egg cracked open with a \nnewborn chick hatching out of it\na group of dogs playing poker\na group of squirrels rowing crew\na spanish galleon sailing on the open sea\na shiny red stand mixer\nInstant3D (Base)\nCarve3D (Finetuned)\nInstant3D (Base)\nCarve3D (Finetuned)\na green tractor\na wide angle zoomed out DSLR photo of A red \ndragon dressed in a tuxedo and playing chess. \nThe chess pieces are fashioned after robots\nFigure 12. Qualitative comparison of Instant3D (the base model) and Carve3D (the model finetuned from Instant3D) on 12 prompts (in 12\nblocks separated by dotted line). In each block, we show the their generated multi-view images in the 2-by-2 grid (top), the reconstructed\nNeRF and the extracted mesh (bottom) when given the prompt (middle). We draw red boxes on the NeRF and the extracted mesh to\nhighlight the artifacts in the NeRF and the mesh, resulting from the inconsistencies in the multi-view images. Carve3D maintains the\ndetailed texture and provides improved multi-view consistency and higher quality NeRF than the base Instant3D.\n6\nA completely destroyed car\nA dog made out of salad\nA plush dragon toy\nAn airplane made out of wood\nInstant3D (Base)\nCarve3D (Finetuned)\nFigure 13. Diverse results from original Instant3D (left) and our Carve3D (right) on 4 prompts (in 4 blocks separated by the dotted line).\nIn each block, we show the their generated multi-view images in the 2-by-2 grid (top), the reconstructed NeRF and the extracted mesh\n(bottom) when given the prompt (middle). Our RLFT does not compromise the diversity of the base Instant3D model, while improving the\nconsistency.\n7\nFigure 14. Quantitative correlation between five variants of MRC (our default LPIPS, as well as PSNR, SSIM, L1, and L2) and incon-\nsistency introduced by inpaint distortion with increasing intensity on four objects. We take negative of the similarity metrics (PSNR and\nSSIM) for easy comparisons to the distance metrics (LPIPS, L1, and L2). LPIPS constantly exhibits monotonically increasing pattern with\nrespect to the increased inconsistency, while other image metrics do not.\n8\nFigure 15. Quantitative correlation between five variants of MRC (our default LPIPS, as well as PSNR, SSIM, L1, and L2) and inconsis-\ntency introduced by azimuth rotation distortion with increasing intensity on four objects. We take negative of the similarity metrics (PSNR\nand SSIM) for easy comparisons to the distance metrics (LPIPS, L1, and L2). All metrics constantly exhibits monotonically, steadily\nincreasing pattern with respect to the increased inconsistency.\n9\nFigure 16. Quantitative correlation between five variants of MRC (our default LPIPS, as well as PSNR, SSIM, L1, and L2) and incon-\nsistency introduced by elevation rotation distortion with increasing intensity on four objects. We take negative of the similarity metrics\n(PSNR and SSIM) for easy comparisons to the distance metrics (LPIPS, L1, and L2). All metrics constantly exhibits monotonically,\nsteadily increasing pattern with respect to the increased inconsistency.\n10\n"
  },
  {
    "title": "Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation",
    "link": "https://arxiv.org/pdf/2312.13469.pdf",
    "upvote": "10",
    "text": "Neural feels with neural fields:\nVisuo-tactile perception for in-hand manipulation\nSudharshan Suresh,1,2\u2217 Haozhi Qi,2,3 Tingfan Wu,2 Taosha Fan,2\nLuis Pineda,2 Mike Lambeta,2 Jitendra Malik,2,3 Mrinal Kalakrishnan,2\nRoberto Calandra,4,5 Michael Kaess,1 Joseph Ortiz,2 Mustafa Mukadam2\n1CMU, 2FAIR, 3UC Berkeley, 4TU Dresden, 5CeTI\n\u2217To whom correspondence should be addressed; E-mail: suddhus@gmail.com.\nTo achieve human-level dexterity, robots must infer spatial awareness from\nmultimodal sensing to reason over contact interactions. During in-hand ma-\nnipulation of novel objects, such spatial awareness involves estimating the ob-\nject\u2019s pose and shape. The status quo for in-hand perception primarily em-\nploys vision, and restricts to tracking a priori known objects. Moreover, visual\nocclusion of objects in-hand is imminent during manipulation, preventing cur-\nrent systems to push beyond tasks without occlusion. We combine vision and\ntouch sensing on a multi-fingered hand to estimate an object\u2019s pose and shape\nduring in-hand manipulation. Our method, NeuralFeels encodes object geom-\netry by learning a neural field online and jointly tracks it by optimizing a pose\ngraph problem. We study multimodal in-hand perception in simulation and the\nreal-world, interacting with different objects via a proprioception-driven pol-\nicy. Our experiments show final reconstruction F-scores of 81% and average\npose drifts of 4.7 mm, further reduced to 2.3 mm with known CAD models. Ad-\nditionally, we observe that under heavy visual occlusion we can achieve up to\n94% improvements in tracking compared to vision-only methods. Our results\ndemonstrate that touch, at the very least, refines and, at the very best, disam-\nbiguates visual estimates during in-hand manipulation. We release our eval-\nuation dataset of 70 experiments, FeelSight, as a step towards benchmarking\nin this domain. Our neural representation driven by multimodal sensing can\nserve as a perception backbone towards advancing robot dexterity. Videos can\nbe found on our project website: https://suddhu.github.io/neural-feels.\n1\narXiv:2312.13469v1  [cs.RO]  20 Dec 2023\nSummary\nNeural perception with vision and touch yields robust tracking and reconstruction of novel ob-\njects for in-hand manipulation.\nFigure 1: Visuo-tactile perception with NeuralFeels. Our method estimates pose and shape of novel objects\n(right) during in-hand manipulation, by learning neural field models online from a stream of vision, touch, and\nproprioception (left).\n1\nIntroduction\nTo perceive deeply is to have sensed fully. Humans effortlessly combine their senses for everyday\ninteractions\u2014we can rummage through our pockets in search of our keys, and deftly insert them\nto unlock our front door. Currently, robots lack the cognition to replicate even a fraction of the\nmundane tasks we perform, a trend summarized by Moravec\u2019s Paradox [48]. For dexterity in\nunstructured environments, a robot must first understand its spatial relationship with respect to\nthe manipuland. Indeed, as robots move out of instrumented labs and factories to cohabit our\nspaces, there is a need for generalizable spatial AI [12].\nSpecific to in-hand dexterity, knowledge of object pose and geometry is crucial to policy gen-\neralization [51, 50, 24, 57]. As opposed to end-to-end supervision [89, 23, 10], these methods\nrequire a persistent 3D representation of the object. However, the status quo for in-hand per-\nception is currently restricted to the narrow scope of tracking known objects with vision as the\ndominant modality [24]. Further, it is common for practitioners to sidestep the perception prob-\nlem entirely, retrofitting objects and environments with fiducials [51, 50]. To further progress\ntowards general dexterity, it is clear that one of the missing pieces is general, robust perception.\nWith visual sensing, researchers tend to tolerate interaction rather than embrace it. This is\nat odds with contact-rich problems where self-occlusions is imminent, like rotating [56], re-\norienting [24, 10], and sliding [63, 72]. Additionally, vision often fails in the real-world due to\n2\npoor illumination, limited range, transparency, and specularity. Touch provides a direct window\ninto these dynamic interactions, and human cognitive studies have reinforced the complementar-\nity with vision [26].\nHardware advances have led to affordable vision-based touch sensors [93, 16, 81, 1, 40, 53,\n79] like the GelSight and DIGIT. Progress in touch simulation [78] enables practitioners to learn\ntactile observation models that transfer to real-world interactions [79, 67, 73]. With a fingertip\nform-factor, their illuminated gel deforms on contact and the physical interaction is captured by\nan internal camera. When chained with robot kinematics, we obtain dense, situated contact that\ncan be processed similar to natural camera images.\nNow given multimodal sensing, how best to represent the spatial information? Coordinate-\nbased learning, formalized as neural fields [87], has found great success in visual computing.\nWith neural fields, practitioners can create high-quality 3D assets offline given noisy visual data\nand pose annotation [46, 49, 43]. They are continuous representations with higher fidelity than\ntheir discrete counterparts like point clouds and meshes. While they are specialized towards batch\noptimization, lightweight SDF models [52, 69, 97, 82] have made online perception possible.\nResearchers have used this extensible architecture not just for continuous 3D quantities like\nsigned distance fields (SDFs) and radiance [54, 46, 49], but also for pose estimation [88, 82],\nplanning [22], and latent physics [41]. Moreover, the ease of imparting generative priors [90] and\ninitializing with pre-trained models [54] future-proofs them. While neural fields have emerged\nlittle by little in robot manipulation [96, 32, 83, 22], the optimization of multimodal data remains\nan open question.\nThe domain of our work\u2014an intersection of simultaneous localization and mapping (SLAM)\nand manipulation\u2014has been studied for over two decades. A first exemplar is from Moll and Erd-\nmann [47], who reconstruct the shape and motion of an object rolled between robot palms, later\nreproduced with specialized sensors [68, 42]. Tactile SLAM has been thoroughly investigated\nfor planar pushing due to its well-understood mechanics [91, 71]. The combination of vision and\ntouch has been explored for reconstructing fixed objects [80, 64, 73, 11] and tracking known ob-\njects [92, 39, 66]. Closest to our work is FingerSLAM [95], combining dense touch from a single\nfinger with vision, however we consider the more challenging case of in-hand manipulation.\nNeuralFeels presents an online solution to localize and reconstruct objects for in-hand ma-\nnipulation with multimodal sensing. We unify vision, touch, and proprioception into a neural rep-\nresentation and demonstrate SLAM for apriori unknown objects, and robust tracking of known\nobjects. In our experiments, we present our robot with a novel object, and it infers and tracks\nits geometry through just interaction. We use a dexterous hand [84] sensorized with commercial\nvision-based touch sensors [40] and a fixed RGB-D camera (Figure 1). With a proprioception-\ndriven policy [56] we explore the object\u2019s extents through in-hand rotation.\n3\nFigure 2: A visuo-tactile perception stack amidst interaction. An online representation of object shape and pose\nis built from vision, touch, and proprioception during in-hand manipulation. Raw sensor data is first fed into the\nfrontend, which extracts visuo-tactile depth with our pre-trained models. Following this, the backend samples from\nthe depth to train a neural signed distance field (SDF), while the pose graph tracks the posed neural field.\nThrough our experiments we study the role that vision and touch play in interactive per-\nception, the effects of occlusion, and visual sensing noise. To evaluate our work, we collect\na dataset of 70 in-hand rotation trials in both the real-world and simulation, with ground-truth\nobject meshes and tracking. Our results on novel objects show average reconstruction F-scores\nof 81% with pose drifts of just 4.7 mm, further reduced to 2.3 mm with known CAD models.\nUnder heavy occlusion, we demonstrate up to 94% improvements in pose tracking compared to\nvision-only methods. Our combination of rich sensing and spatial AI requires minimal hardware\ncompared to complex sensing cages, and is easier to interpret than end-to-end perception meth-\nods. The output of the neural SLAM pipeline\u2014pose and geometry\u2014can drive further research\nin general dexterity, broadening the capabilities of home robots.\n2\nResults\nOur multi-fingered robot hand is presented with a novel object, placed randomly between its\nfingertips. It rotates the object in-hand, through a proprioception-driven policy [56], which gives\nrise to a stream of visual and tactile signals. We combine the visual, tactile, and proprioceptive\nsensing into our online neural field, for a persistent, evolving 3D representation of the unknown\nobject. The full pipeline of our NeuralFeels perception stack is illustrated in Figure 2. We also\nsummarize our experiments and findings in our webpage.\nWe evaluate NeuralFeels over simulated and real-world interactions, totaling up to 70 exper-\niments over different object classes. Details of the dataset can be found in Section 4.3. First, we\n4\ndemonstrate SLAM results for novel objects, and highlight some qualitative examples. Next, we\ndemonstrate pose-tracking when we have a priori shape of the manipuland. Finally, we analyze\nthe role touch plays in improving perception under occlusion and visual sensing noise.\n2.1\nMetrics and baseline\nPose and reconstruction metrics. We use the symmetric average Euclidean distance metric\n(ADD-S) to evaluate the pose tracking error over time [77]. The ADD metric is commonly used\nin manipulation [86, 6, 76, 77] as a geometrically-interpretable distance metric for pose error. It\nis computed by sub-sampling the ground-truth object mesh and averaging the Euclidean distance\nbetween the point-set in the estimated and ground-truth object pose frames. Rather than pairwise\ndistance, ADD-S considers the closest point distance, which disambiguates symmetric objects.\nFor reconstruction, we compare how accurate (precision) and complete (recall) the neural\nSDF is in comparison to the ground-truth mesh. The F-score, an established metric in the multi-\nview reconstruction community [37, 75], combines these two criteria into an interpretable [0\u22121]\nvalue. To compute this, we first sub-sample the ground-truth and reconstructed meshes, and\ntransform both to the common object-centric reference frame. Given a distance threshold, in our\ncase \u03c4 =5 mm, precision measures the percentage of reconstructed points within \u03c4 distance from\nthe ground-truth points. Conversely, recall measures the percentage of ground-truth points within\n\u03c4 distance from the reconstructed points. The harmonic mean of these two quantities give us the\nF-score, which captures both surface reconstruction accuracy and shape completion. Broadly, a\nhigher F-score with tighter \u03c4 bounds implies better object reconstructions. For brevity, we refer\nto ADD-S and F-score as the pose metric and shape metric respectively.\nGround-truth shape and pose. We evaluate these metrics against the ground-truth estimates\nof object shape and pose. For each object, the ground-truth shape is obtained from offline scans\n(Figure S1). Ground-truth object pose is straightforward in simulation experiments, directly\nexposed by IsaacGym [45]. In the real-world, we estimate a pseudo ground-truth, via multi-\ncamera pose tracking of the experiment. Instrumented solutions, such as 3D motion capture, are\ninfeasible as it both visually and physically interferes with the experiments. We opt to install two\nadditional cameras (Section 4.3) and run NeuralFeels in pose tracking mode with the ground-\ntruth object shape. This represents the best tracking estimates given known shape and occlusion-\nfree vision. For further details, refer to Section S1.\n2.2\nNeural SLAM: object pose and shape estimation\nIn this section, we evaluate NeuralFeels\u2019 ability for embodied spatial reasoning from scratch. We\npresent the robot with a novel object, and the robot is tasked with building an object model on-\n5\nthe-fly. This is typical where robots continually learn from interaction, such as when deployed\nin unstructured household environments. We make no assumptions about the object geometry,\nwhich is built from scratch, or manipulation actions, which are decided at deployment. We\nprocess visuo-tactile data sequentially with no access to future information or category-level\npriors. This formulation aligns with other dexterous manipulation work [24, 56, 57, 10], and is\nless restrictive than that of FingerSLAM [95], where the object is always in contact with a single\ntactile sensor and the camera is unobstructed.\nWe evaluate over a combined 70 experiments in simulation and real-world across of 14 differ-\nent objects. The objects are placed in-hand, after which the policy collects 30 seconds of vision,\ntouch, and proprioception data. As each run is non-deterministic, we average our results across\n5 different seeds, resulting in a total of 350 trials. The first frame of each sequence only presents\nlimited visual knowledge: a single side of Rubik\u2019s cube or large dice; the underside of the rubber\nduck. Through the course of any 30 second sequence, in-hand rotation exposes previously unseen\ngeometries to vision and touch fills in the rest of the occluded surfaces. In Figure 3, we present\nthe main set of results, where we compare the multimodal fusion schemes against ground-truth.\nObject reconstructions. Figure 3 (a) shows the final shape metric at the end of each se-\nquence for a fixed threshold \u03c4. Here we pick \u03c4 = 5 mm for this evaluation, around 3% of the\nmaximum diagonal length of the objects. Greater the value of the shape metric, the closer the\nsurface reconstructions are to ground-truth. We observe large gains when incorporating touch,\nwith surface reconstructions on average 15.3% better in simulation and 14.6% better in the real-\nworld. Our final reconstructions, as seen in Figure 3 (e), have a median error of 2.1mm in simula-\ntion and 3.9mm in the real-world. Additionally, the second plot compares the final shape metrics\nagainst a range of \u03c4 thresholds. Here we observe that multimodal fusion leads to consistently\nbetter shape metrics across all \u03c4 values in simulation and the real-world.\nObject pose drift. As SLAM is the exemplar of a chicken and egg problem, there is a strong\ncorrelation between a low shape metric and high pose metric. Empirically, we observe larger\npose drift in the initial few seconds due to incomplete geometry, which levels off with further\nexploration. For fair comparisons we initialize the object\u2019s canonical pose to the ground-truth,\nbut this is not necessary otherwise. With this initialization, we ignore the pose metric over the\nfirst five seconds, as it is ill-defined.\nFigure 3 (b) plots the drift of the object\u2019s estimated pose with respect to the ground-truth,\nlower being more accurate. We observe better tracking with respect to the vision-only baseline,\nwith improvements of 21.3% in simulation and 26.6% in the real-world. Table 1 in Figure 3 (c)\nreports the number of failures in vision-only tracking compared to NeuralFeels. Here, a failed\nexperiment is defined as when the average pose drift exceeds an empirical threshold of 10 mm.\nQualitative results. Figure 3 (d) visualizes the rendered normals of the posed neural field\n6\nFigure 3: Summary of SLAM experiments. (a, b) We present aggregated statistics for SLAM over a combined\n70 experiments (40 in simulation and 30 in the real-world), with each trial run over 5 different seeds. We compare\nacross simulation and real-world to show low pose drift and high reconstruction accuracy. (c) Table 1 illustrates the\nnumber of trials that our method fails to track (and reconstruct) the object. (d) Representative examples of the final\nobject pose and neural field renderings from the experiments. (e) The final 3D objects generated by marching cubes\non our neural field. Here, we highlight the role tactile plays in both shape completion and shape refinement.\nat the end of each experiment, with the 3D coordinate axes superimposed. The final 3D recon-\nstructions, generated via marching cubes, are shown in Figure 3 (e) alongside the ground-truth\nmeshes. Below that, we highlight the gains with visuo-tactile integration, with examples of shape\ncompletion and refinements.\nIn Figure 4 we show the incremental pose tracking and reconstructions of objects across\ndifferent time slices of a few representative experiments. We present two results from the real-\nworld, bell pepper and large dice, and two from simulation, rubber duck and peach. At each\ntimestep, we highlight the input stream, frontend depth and output object model. The 3D visual-\nizations are generated by marching-cubes, in addition to the rendered normals of the neural field\nprojected onto the visual image. In each case, we partially reconstruct the object at the initial\nframe, and build the surfaces out progressively over time.\n7\nFigure 4: Representative SLAM results. In both real-world and simulation, we build an evolving neural SDF that\nintegrates vision and touch while simultaneously tracking the object. We illustrate the input stream of RGB-D and\ntactile images, paired with the posed reconstruction at that timestep.\n8\nFigure 5: Neural pose tracking of known objects. (a) With known ground-truth shape, we can robustly track\nobjects such as the Rubik\u2019s cube and potted meat can. (b) We observe reliable tracking performance, with average\npose errors of 2 mm through the sequence. (c) With a known object model and good visibility, touch plays the role\nof pose refinement.\n2.3\nNeural tracking: object pose estimation given shape\nAs a special case of NeuralFeels, we demonstrate superior multimodal pose tracking when\nprovided the CAD models of the objects at runtime. Tracking known geometries is an ac-\ntive area of research in visual SLAM [24, 38], with some work that incorporates touch as\nwell [92, 39, 66, 72, 5]. This is applicable in environments like warehouses and manufactur-\ning lines, where robots have intimate knowledge of the manipulands [5]. It is further useful in\nhousehold scenarios, where the robot has already generated an object model through interaction.\nIn implementation, the object\u2019s SDF is pre-computed from a given CAD model. During\nruntime, we freeze the weights of the neural field, and only perform visuo-tactile tracking with\nthe frontend estimates. Similar to the SLAM experiments, we run each of the 70 experiments\nover 5 seeds, and report the pose metrics with respect to ground-truth.\nResults from pose tracking. Figure 5 (a) shows some qualitative examples of tracking the\npose of the Rubik\u2019s cube and potted meat can with vision and touch. For the given examples, the\npose metric over the sequences are plotted in Figure 5 (b). We observe low, bounded pose error\neven with imprecise visual segmentation and sparse touch signals. In Figure 5 (c) we observe the\nrole touch plays in reducing the average pose error over all experiments to the range of 2.3 mm.\nGiven the CAD model, we observe that incorporating touch can refine our pose estimates, with a\ndecrease in average pose error by 22.29% in simulation and 3.9% in the real-world. As addresed\nin Section 3, the less-pronounced contacts in the real-world can explain this disparity. In the\nfollowing section, we highlight greater improvements with respect to the baseline when visual\nsensing is suboptimal.\n9\nFigure 6: Ablations on occlusions and sensing noise. (a) With occluded viewpoints, visuo-tactile fusion helps\nimprove tracking performance with an unobstructed local perspective. We quantify these gains across a sphere of\ncamera viewpoint to show improvements, particularly in occlusion-heavy points-of-view. (b) We observe that touch\nplays a larger role when vision is heavily occluded, and a refinement role when we there is negligible occlusion. (c)\nWith larger noise in visual depth, tactile help curb large pose tracking errors.\n2.4\nPerceiving under duress: occlusion and visual depth noise\nIn this section, we explore the broader benefits of fusing touch and vision through ablations on\nvisual sensing properties. The previous results were achieved through the iterative co-design\nof perception and hardware, such that we have favorable camera positioning and precise stereo\ndepth tuning. Indeed, this attention to detail is necessary for practitioners [24, 10], but can we\nalso harness touch to improve over sub-optimal visual data? We consider two such scenarios\nin simulation, where we can freely control these parameters, and evaluate on the pose tracking\nproblem from the previous section.\nThe effects of camera-robot occlusion. In an embodied problem, third-person and egocen-\ntric cameras are both susceptible to occlusion from robot motion and environment changes. For\nexample, if we were to retrieve a cup off the top shelf in the kitchen, we rely primarily on tactile\nsignals to complete the task. For the perception system, this translates to the object of interest\ndisappearing from the field of view, while local touch sensing is still unaffected. To emulate this\nwe consider tracking the pose of a known Rubik\u2019s cube. We simulate 200 different cameras in\n10\na sphere of radius 0.5 m, each facing towards the robot. As shown in Figure 6 (a), each camera\ncaptures a unique vantage point of the same in-hand sequence, with varying levels of robot-object\nocclusion. This serves as proxy for occlusion faced by an egocentric or fixed camera when either\nthe hand or environment occludes the object.\nTo simplify the experiment, we assume the upper-bound performance of the vison-only fron-\ntend by providing ground-truth object segmentation masks. We characterize the visibility in\nterms of an occlusion score by calculating the average segmentation mask area for each view-\npoint, and normalizing them to [0\u22121]. For example, scores closer to 0 correspond to viewpoints\nbeneath the hand (most occluded), while those closer to 1 correspond to cameras placed atop\n(least occluded). We run pose tracking experiments for each of the 200 cameras in two modes:\nvision-only and visuo-tactile and compare between them.\nIn Figure 6 (a) we colormap each camera view based on the pose tracking improvements from\nincorporating touch. On average the improvement across all cameras is 21.2%, and it peaks at\n94.1% at heavily occluded views. We inset frames from a few representative viewpoints and their\ncorresponding relative improvement with visuo-tactile fusion. In Figure 6 (b) the pose error for\neach modality is further plotted versus the [0\u22121] occlusion score. This corroborates the idea that\ntouch refines perception in low-occlusion regimes and robustifies it in high-occlusion regimes.\nThe effects of noisy visual depth. Depth from commodity RGB-D sensors are degraded as\na function of camera-robot distance, environment lighting, and object specularity. Even in ideal\nscenarios, the RealSense depth algorithm has 35 hyperparameters [34] that considerably affect\nthe frontend input to NeuralFeels. To simulate this, we corrupt the depth maps progressively\nwith a realistic RGB-D noise, and observe the tracking performance for a known geometry.\nAs implemented by Handa et al. [25], we simulate common sources of depth-map errors as\na sequence of pixel shuffling, quantization, and high frequency noise. The depth noise factor D\ndetermines the magnitude of these operations, with the depth-maps visualized in Figure 6 (c).\nWhile all prior simulation experiments have been collected with D=5, here we vary the magni-\ntude from 0\u221250 in intervals of 10. At each noise level, we run pose tracking across the 5 Rubik\u2019s\ncube experiments with 5 unique seeds, resulting in a total of 150 experiments. In Figure 6 (c)\nwe plot error against the noise factor D, showing an expected upward trend in error with noise.\nHowever, we see markedly better tracking when fusing touch, especially in high-noise regimes.\n3\nDiscussion\nNeuralFeels achieves robust object-centric SLAM through interaction. To the best of our\nknowledge, NeuralFeels is the first demonstration of full-SLAM for multimodal, multifinger\nmanipulation. We are inspired by computer vision systems that achieve high-fidelity neural re-\n11\nconstructions without pose annotation [69, 97, 82] through online learning. They highlight the\nbenefit of co-designed pose tracking and reconstruction, which has also shown promise in manip-\nulation systems [71, 95]. More broadly, our stack relies on recent progress in somewhat disparate\nfields: SLAM, neural rendering, tactile sensing, and reinforcement-learning.\nAs shown in the Figure 3 (a), we achieve average reconstruction F-scores of 81% across\nsimulation and real-world experiments on novel objects. Simultaneously, we stably track these\nobjects amidst interaction with minimal drift, an average of 4.7 mm. While the vision-only base-\nline may suffice for some scenarios, the results validate the utility of rich, multimodal sensing\nfor interactive tasks. This corroborates years of research in interactive perception from touch and\nvision [65, 73, 5], now applied on dexterous manipulation platforms.\nTouch and proprioception ground embodied perception. Interactive perception is far from\nideal, an embodiment can more often than not get in the way of sensing. As seen in Figure\n4, in-hand manipulation suffers from challenges such as frequent occlusions, limited field-of-\nview, noisy segmentation, and rapid object motion. To tackle this, proprioception helps focus\nthe perception problem: we can accurately singulate the object of interest through embodied\nprompting (Section 4.6.1). When combined with touch, we robustify our visual estimates by\ngiving us a window into local interactions. These are evident in simulated / real SLAM and\npose tracking experiments, where multimodal fusion leads to improvements of 15.3% / 14.6% in\nreconstruction and 21.3% / 26.6% in pose tracking.\nQualitatively, we see touch performs two key functions: (i) disambiguating noisy frontend\nestimates and (ii) providing context in the presence of occlusion. The former alleviates the\neffect of noisy visual segmentation and depth with co-located local information for mapping and\nlocalization. The latter provides important context hidden from visual sensing, like the occluded\nface of the large dice or back of the rubber duck. The final reconstructions in Figure 3 (e) support\nthese findings, with improved shape completion and refinement. This is important in the few-shot\ninteractions of everyday life, where the richer sensing can create better object models.\nThe largest gains from incorporating touch are in heavy-occlusion regimes (Figure 6 (a)),\nwhere we can observe up to 94.1% improvements at certain camera viewpoints. To our knowl-\nedge, this co-design of perception and hardware has not been explored by practitioners before.\nThis doesn\u2019t just demonstrate the complementary nature of the modalities, but further, the ideal\nconfigurations for occlusion-free manipulation. Finally, our results in tactile-only tracking (Fig-\nure 5 (c)) support the analysis of Smith et al. [64] that learning exclusively from touch leads to\npoor performance as it lacks any global context.\nModularity marries pre-training with online learning. As opposed to an end-to-end percep-\ntion, NeuralFeels is fully interpretable due to its modular construction. This allows us to combine\n12\nfoundational models trained on large-scale image and tactile data (frontend), with SLAM as on-\nline learning (backend). Furthermore, our backend is a combination of state-of-the-art neural\nmodels [49] with classical least-square optimization [55] that have found success in SLAM [8].\nChaining these systems together, we can achieve first-of-its-kind multimodal SLAM results with-\nout explicit training in the domain. This is crucial given the dearth of training data for in-hand\ntasks, and robot manipulation in general.\nThis modular design has benefits for future generalization of our system: (i) Other models of\ntactile sensors [93, 79, 1] can be easily integrated as long as they can be accurately simulated;\n(ii) alternate scene representations [4, 31] can supplant our neural field model, as required; (iii)\nadditional state knowledge can be seamlessly integrated as factor graph cost functions, e.g. tactile\nodometry [95] and force-constraints [71]; (iv) any combination of tactile and visual sensors can\nbe fused into our multimodal framework with appropriate calibration and kinematics.\nApplication towards perception-driven planning. NeuralFeels is relevant to manipulation re-\nsearchers and practitioners who require spatial perception with a single camera and affordable\ntactile sensing. It can be extended to not just in-hand rotation, but many other object-centric ma-\nnipulation tasks like in-hand reorientation [10], pick-and-place [5], insertion [42], nonprehensile\nsliding [33], and planar pushing [71]. In the future, we hope to generalize to these different tasks\nand varied robot morphologies. While not explored in this work, the direct benefit of an online\nSDF is the ability to seamlessly plan for dexterous interactions. Recent works demonstrate the\nbenefit of apriori-known object point-clouds [57] and SDFs [18] for goal-conditioned planning,\nand running our perception stack in-the-loop is the next natural step.\nSystem limitations. NeuralFeels shows the potential of a multimodal system for manipula-\ntion that leverages pre-training and online learning for high accuracy spatial understanding. We\npresent some of the limitations and promising directions for future work:\n\u2022 Generic 3D priors for object reconstruction. For each experiment with a novel object,\nour method learns a 3D geometry from scratch to best explain the visuo-tactile sensor\nstream. The pose tracker has a higher chance of failure in the initial few seconds, when the\nneural SDF is a poor-approximation of the full object due to limited sensor coverage. We\nfurther note that our rotation policy might not completely explore the object in the real-\nworld, resulting in a lower average final F-Score of 81%. Out-of-scope in our work, but\nof great interest in the visual learning community [54, 85, 29], is leveraging pre-trained\nmodels for an initial object prior. Given an initial occluded view, careful integration of\nthese large reconstruction models trained via category [54] or multi-view supervision [85,\n29] may yield an initial-guess SDF that we refine over time with vision and touch. In\nmanipulation, Wang et al. [80] have seen promising results in using shape priors for visuo-\n13\ntactile reconstruction of fixed objects.\n\u2022 Sim-to-real adaptation. Our findings indicate that while multimodal fusion performs\nwell both in simulation and the real-world, its benefits are less pronounced in real-world\ndeployment. This is a common problem in sim-to-real applications, and we qualitatively\nidentify several domain gaps that explain this: (i) the DIGIT elastomer is less sensitive in\nreal-world deployment, leading to sparser contact predictions; (ii) our RL policy is less\nstable in the real-world (sometimes requiring human intervention) and causes rapid jumps\nin object motion; (iii) noise in proprioception is only indirectly modelled as uncertainty\nterms in estimation. To tackle these, we must leverage work in sim-to-real generalization\nfor tactile simulation [27] and reinforcement-learning [57].\n\u2022 System design considerations. We identify viable engineering improvements that can\nbe made towards a general-purpose system. We are currently restricted to a fixed-camera\nsetup, with an online hand-eye calibration or egocentric vision, this can be relaxed. Depth\nuncertainty [15] is valuable information for our neural model to handle visually-adversarial\nobjects like glass and metal. To achieve true real-time frequencies, efficiency gains can be\nmade in the pose optimizer and frontend estimation. Finally, we can increase robustness\nby using the color information for feature-based tracking of objects [14].\n\u2022 Perceiving latent state. We consider geometry as just the starting point for neural mod-\nels: interaction reveals latent properties like texture [33], friction [41], and object dynam-\nics [70]. Neural fields can embed these latents as auxiliary optimization terms so as to\nbenefit tasks that go beyond just geometry and pose. Applications can range from learning\nto manipulate inertially-significant objects (e.g. a hammer), to identifying a grasp point\nfrom local texture (e.g. a saucepan handle).\n4\nMaterials and methods\nNeuralFeels ingests multimodal information to build a persistent 3D object representation. Sim-\nilar to classical SLAM frameworks, it first has a frontend, responsible for abstracting the vision\n(RGB-D) and touch (RGB) input stream into a format suitable for estimation (segmented depth).\nThereafter, the backend fuses this data into an optimization structure that infers the object model:\nan evolving posed object SDF. An illustration of the entire pipeline is found in Figure 2, which\nwe refer the reader back to throughout this section.\n4.1\nTask definition\nNeuralFeels incrementally builds an object model, simultaneously optimizing for the object SDF\nnetwork\u2019s weights \u03b8 and its corresponding pose xt at the current timestep t. For object explo-\n14\nration, we use a proprioception-driven policy \u03c0t that executes the optimal action to achieve stable\nrotation. The input stream of all sensors S consists of the following (left column of Figure 2):\n\u2022 RGB-D vision: image Ic\nt and depth Dc\nt from calibrated camera c \u2208 S\n\u2022 RGB touch: images Is\nt from four DIGITs [40]; s \u2208 {dindex, dmiddle, dring, dthumb} \u2208 S\n\u2022 Proprioception: joint-angles qt from robot encoders.\n4.2\nRobot hardware and simulation\nThe Allegro hand [84] is retrofit with four DIGIT vision-based tactile sensors [40], at each of\nthe distal ends. The DIGIT produces a 240 \u00d7 320 RGB image of the physical interaction at\n30 Hz. The Allegro publishes 16D joint-angles so as to situate the tactile sensors with respect\nto the base frame. The hand is rigidly mounted on a Franka Panda arm, with an Intel D435\nRGB-D camera placed at approximately 35 cm from it. The camera extrinsics are computed with\nrespect to the base frame of the Allegro through ArUco [20] hand-eye calibration. For our vision\npseudo-ground-truth we use three such cameras in the workspace (Figure 7), jointly calibrated via\nKalibr [19], to achieve \u2248 1 px reprojection error. Our simulator replicates the real-world setup:\na combination of the IsaacGym physics simulator [45] with the TACTO touch renderer [78]. In\nthis case, we can record and store the true ground-truth object pose directly from IsaacGym.\n4.3\nFeelSight: a visuo-tactile perception dataset\nVisuo-tactile perception lacks a standardized benchmark or dataset that has driven progress in\nadjacent fields like visual tracking [28], SLAM [21], and reinforcement learning [30]. Towards\nthis, we introduce our FeelSight dataset for visuo-tactile manipulation. We use the in-hand rota-\ntion policy (Section S4) to collect vision, touch, and proprioception for 30 seconds per trial.\nWhen we encounter a novel object, we tend to twirl it in our hand to get a better look from\ndifferent views, and regrasp it from different angles. The equivalent for a multi-fingered hand, in-\nhand rotation, is an ideal choice for the interactive perception problem. We adopt the method of\nQi et al. [56] where they train a proprioception-based policy in simulation, and directly transfer\nit to the real-world. Recent work has further shown in-hand object rotation using touch and\nproprioceptive history [57, 89], however our simpler abstraction proves sufficient for this task.\nIn our experiments, the rotation policy \u03c0t sends commands to the robot hand at 20 Hz via the\nROS Allegro controller. This achieves stable rotation of novel objects and interesting visuo-\ntactile stimuli; for further details refer to Section S4.\nThe dataset has 5 in-hand rotation trials each of 6 objects in the real-world and 8 objects\nin simulation; a total 35 minutes of interaction. As explained in Figure 7, we record a pseudo-\nground-truth in the real-world, and exact ground-truth poses in simulation. We ensure diversity\n15\nFigure 7: Robot setup in the real-world and simulation. (a) We capture diverse visuo-tactile interactions across\ndifferent object categories in the real-world and physics simulation. (b) The robot cell is made up of three realsense\nRGB-D cameras, an Allegro robot hand mounted on a Franka Panda, and four DIGIT tactile sensors. All real-world\nresults use the primary camera and DIGIT sensing, while the additional cameras are fused for our ground-truth pose\ntracking. In simulation, we use an identical primary camera in IsaacGym with touch simulated in TACTO. The\nsimulator provides ground-truth object pose, so multi-camera tracking is not necessary.\nin the class of objects: they vary in geometry and size from 6-18 cm in diagonal length. Ground-\ntruth meshes of each object are obtained with the Revopoint 3D scanner [59], which uses dual-\ncamera infrared for \u2248 0.05 mm scan accuracy. Additionally, the the simulated experiments have\nground-truth meshes from the YCB [9] and ContactDB [7] datasets.\n4.4\nMethod overview and key insights\nObject model (Section 4.5): We represent the object SDF as a neural network with weights \u03b8,\nwhose output is transformed by the current object pose xt. This continuous function F \u03b8\nxt(p) :\nR3 \u2192 R maps a 3D coordinate p to a scalar signed-distance from the object\u2019s closest surface.\nOnline updates are decomposed into alternating steps between refining the weights of the neural\nSDF \u03b8, and optimizing the object pose xt. Our bespoke object model is a representation of both\nthe pose and object geometry over time.\nFrontend (Section 4.6.1, 4.6.2): Given the RGB-D, RGB, and proprioception inputs, our fron-\ntend returns segmented depth measurements compatible with our backend optimizer. These mod-\nules are pre-trained with a large corpus of data.\nShape optimizer (Section 4.7.1): Takes in frontend output and optimizes for \u03b8 at fixed object\n16\npose \u00afxt via gradient descent [49]. Each shape iteration results in improved object SDF F \u03b8\n\u00afxt.\nPose optimizer (Section 4.7.2): Builds and optimizes an object pose-graph [55] for xt given\nfixed network weights \u00af\u03b8. Every pose iteration spatially aligns the evolving object SDF with the\ncurrent set of frontend output.\n4.4.1\nInsight 1: NeuralFeels is a posed neural field\nThe object model F \u03b8\nxt is estimated by a chicken-and-egg optimization of both the instant-NGP\nweights \u03b8, and the object pose xt. Prior work has estimated the pose of a sensor in fixed neural\nfield, either by freezing the network weights [88, 44], or joint-optimization [69, 97, 60]. In our\ncase, robot kinematics gives us the pose of the touch sensors, and extrinsics give us the pose of\nthe camera. So, we instead flip this paradigm to estimate the pose of the neural field with respect\nto known-pose sensors.\n4.4.2\nInsight 2: Touch is vision, albeit local\nWe extend neural fields to directly incorporate touch just as it would vision. Our key insight is\nthat vision-based touch can be approximated as a perspective camera model in tactile simulators\nlike TACTO [78]. There are, however, differences that must be accounted for in image formation\n(i) vision-based tactile sensor impose their own color and illumination to the scene, which makes\nit hard to get reliable visual cues, (ii) a tactile image stream has considerably smaller metric\nfield-of-view and depth-range is usually in centimeters rather than meters, (iii) tactile images\nhave depth discontinuities along all non-contact regions, while natural images only encounter\nthem along occlusion boundaries. Our method adapts each of these by (i) consistently using\ndepth rather than color for optimization, (ii) sampling at different scales (centimeter v.s. meter)\nbased on sensing source, (iii) sampling only surface points for touch, but both free-space and\nsurface points for vision. More details are described in Section 4.7.1. After accounting for these\ndifferences, we can sample touch consistent with vision, giving us a rich perspective of the object.\n4.5\nObject model\nOur object model is depicted in the right column of Figure 2. In general, a neural SDF [52, 3, 49]\nrepresents 3D surfaces as the zero level-set of a learnable function F(p) : R3 \u2192 R. The scalar\nfield\u2019s sign indicates if any query point p in the volume is inside (negative), outside (positive) or\non (\u2248 0) the reconstructed surface. p is first positionally-encoded [74] into a higher-dimensional\nspace, an important routine that helps networks better approximate high-frequency surfaces. This\nis followed by a multi-layer perceptron (MLP) that fits the encoding to a scalar field. Typically,\n17\nthis network is optimized with depth samples from a camera of known intrinsics, and annotated\nposes from structure-from-motion [61].\nA neural SDF is more compact than the more popular neural radiance fields [46], as they do\nnot model color and appearance properties of the scene. This is sufficient for manipulation, as we\ncare more about estimating geometry than generating novel-views. Recently, instant-NGP [49]\ndemonstrated a learnable multiresolution hash table as a positional encoding that greatly accel-\nerates SDF optimization with small MLP backbones. This has been successfully leveraged for\nreal-time SLAM in indoor scene [60].\nIn our work, F \u03b8\nxt represents the neural SDF of the object at a given pose xt. While xt is\ninitialized to be between the robot fingers, \u03b8 is randomly initialized. Both shape and pose are\nestimated via alternating optimization, which emulating the paradigm of tracking and mapping\nthat has found great success in robot vision [8]. The model is fully-differentiable, can be queried\narbitrarily in 3D space, and easily extensible to color, latent physics, and other properties.\n4.6\nFrontend\nThe frontend processes are shown in the center column of Figure 2. Its function is to robustly ex-\ntract depth measurements from raw vision and touch sensing. Depth is available as-is in an RGB-\nD camera, but the challenge is to robustly segment out object depth pixels in heavily-occluded\ninteractions. Towards this, we introduce a kinematics-aware segmentation strategy using power-\nful vision foundation models [36] (Section 4.6.1). Estimating depth from vision-based touch is\nan open research problem [6, 79, 67, 2, 73] where millimeter precision and generalization across\nsensors is important. Towards this, we present a transformer architecture that accurately predicts\nDIGIT contact patches from inputs images (Section 4.6.2). Unlike our backend that is optimized\nonline, the frontend networks are pre-trained from a large corpus of data. The output of our\nfrontend is a segmented depth image \u02c6Ds\nt for each sensor s \u2208 S.\n4.6.1\nSegmented visual depth\nDuring in-hand manipulation, finger-object occlusion is inevitable and the foreground-background\nis ambiguous. Robust segmentation of the image stream Ic\nt via prompts has successfully been\ndemonstrated by image foundation models, like the Segment Anything Model (SAM) [36].\nTrained with a vision transformer (ViT) in the data-rich natural image domain, SAM generalizes\nto novel scenes for state-of-the-art, zero-shot instance segmentation.\nEven with SAM, in-hand object segmentation requires appropriate prompts to guide the pre-\ntrained model. With an embodied agent, we can take advantage of robot kinematics to achieve\nthis. Given our camera c with known projection operation \u03a0c, we can obtain any 3D point p as\n18\nFigure 8: Frontend and backend description. (a) Segment-Anything [36] combined with embodied prompts,\ngives us robust object segmentation. Through reasoning about finger occlusion and object pose with respect to the\nfingers, we can accurately prompt the segmentation network for robust output masks. (b) Representative examples\nof the sim-to-real performance of the tactile transformer. Each RGB image is fed through the network to output a\npredicted depth, along with a contact mask. (c) Our sliding window nonlinear least squares optimizer estimates the\nobject pose xt from the outputs of the frontend. Each object pose xt is constrained by the SDF loss, frame-to-frame\nICP, and pose regularization to ensure tracking remains stable.\na pixel (u, v) = \u03a0c (p) on the image Ic\nt . Our insight is to use the 3D center of grasp and robot\nkinematics as prompts for SAM (refer to Section S3). This makes the reasonable assumption\nthat the object exists between the robot\u2019s fingers, which is almost always the case. In Figure 8\n19\n(a) we visualize the segmentation on real-world images, alongside the SAM prompts. In our\nexperiments we use the ViT-L model with 308M parameters. While this achieves a speed of\naround 4Hz, in practice, we can use efficient segmentation models [94] for speeds up to 40Hz.\n4.6.2\nTactile transformer\nIn contrast, vision-based touch images are out-of-distribution from images SAM is typically\ntrained on, and does not directly provide depth either. The embedded camera perceives an illu-\nminated gelpad, and contact depth is either obtained via photometric stereo [93], or supervised\nlearning [6, 79, 67, 2, 73]. Existing touch-to-depth relies on convolution, however recent work\nhas shown the benefit of a ViT for dense depth prediction [58] in natural images. We train a\ntactile transformer for predicting contact depth from vision-based touch to generalize across\nmultiple real-world DIGIT sensors.\nThe architecture is trained entirely in tactile simulation, using weights initialized from a pre-\ntrained image-to-depth model [58]. The tactile transformer represents the inverse sensor model\n\u2126 : Is\nt 7\u2192 \u02c6Ds\nt where s \u2208 {dindex, dmiddle, dring, dthumb} \u2208 S. This architecture is based on the dense\nvision transformer [58] and is lightweight (21.7M parameters) compared to its fully-convolution\ncounterparts [72].\nSimilar to prior work [73, 72], we generate a large corpus of tactile images and paired ground-\ntruth depthmaps in the optical touch simulator TACTO [78]. We collect 10K random tactile\ninteractions each on the surface of 40 unique YCB objects [9]. For sim-to-real transfer we\naugment the data with randomization in sensor LED lighting, indentation depth, and pixel noise.\nIn TACTO, image realism is achieved by compositing with template non-contact images from\nreal-world DIGITs. For details on the training and data, refer to Section S2.\nThese augmentations enable generalized performance across our multi-finger platform, where\neach sensor has differing image characteristics. Our tactile transformer is supervised on mean-\nsquare depth reconstruction loss against the ground-truth depthmaps from simulation. Based on\nthe predicted depthmaps, the output is thresholded to mask out non-contact regions. The tactile\ntransformer demonstrates an average prediction error of 0.042 mm on simulated test set. Figure\n8 (b) shows sim-to-real performance of the tactile transformer on real-world interactions.\n4.7\nBackend: shape and pose optimizer\nThe backend (right column of Figure 2) is responsible for taking in depth and sensor poses from\nthe frontend to build our object model online. This alternates between shape (Section 4.7.1)\nand pose optimization (Section 4.7.2) steps using samples from the visuo-tactile depth stream.\nSimilar to other neural SLAM methods [52], the modules maintain a bank of keyframes over\n20\ntime to generate these samples. Additional implementation details for the backend are found in\nSection S3.\n4.7.1\nShape optimizer\nFor online estimation it is intractable to optimize F \u03b8\n\u00afxt using all input frames as in neural radiance\nfields [46]. We opt for an online learning approach [69, 52], which builds a subset of keyframes\nK on-the-fly to optimize over. The backend must both (i) accept new keyframes based on a\ncriteria, and (ii) replay old keyframes in the optimization to prevent catastrophic forgetting [69].\nEach iteration of the shape optimizer replays a batch kt \u2208 K of size 10 per sensor to optimize our\nnetwork. This includes the latest two frames, and a weighted random sampling of past keyframes\nbased on average rendering loss.\nThe initial visuo-tactile frame is automatically added as a keyframe K0 = { \u02c6Ds\n0 | s \u2208 S}, and\nevery subsequent keyframe Kt is accepted using an information gain metric [69]. For this, the\naverage rendering loss is computed from the frozen network F \u03b8\n\u00afxt using the given keyframe pose\nand compared against a threshold dthresh = 0.01 m. Finally, if we have not added a keyframe for\nan interval tmax = 0.2 secs, we force one to be added.\nSampling and SDF loss. At each iteration, we sample coordinates in the neural volume from\nkt to optimize the neural weights \u03b8. The first step is to sample a batch of pixels ukt from kt\u2014a\nmix of surface and free-space pixels. While surface pixels directly supervise the SDF zero level-\nset, free-space pixels carve out the neural volume. In our implementation, we sample 50% of\ncamera pixels in free-space, while we only sample surface pixels for touch. Through each pixel\nu \u2208 ukt given their corresponding sensor pose, we project a ray into the neural volume. Similar\nto Ortiz et al. [52], we sample Pu points per ray, a mix of stratified and surface points.\nWith these samples, we compute an SDF prediction \u02c6du for each \u02c6Dt \u2208 kt, as the batch distance\nbound [52]. For each ray, we split the samples into P f\nu and P tr\nu based on \u02c6du lies within the\ntruncation distance dtr = 5 mm from the surface. Our shape loss Lshape = Lf + wtrLtr, with\nwtr = 10, resembles the truncated SDF loss of Azinovi\u00b4c et al. [3]:\nLf =\n1\n|ukt|\nX\nu\u2208ukt\n1\n|P f\nu||F \u03b8\n\u00afxt(P f\nu) \u2212 dtr|\nand\nLtr =\n1\n|ukt|\nX\nu\u2208ukt\n1\n|P tr\nu ||F \u03b8\n\u00afxt(P tr\nu ) \u2212 \u02c6du|\n4.7.2\nPose optimizer\nBefore each shape iteration, we use a pose graph [13] to refine the object pose xt with respect to\nthe frozen neural field F\n\u00af\u03b8\nxt. We achieve this by inverting the problem to instead optimize for the 6-\nDoF poses in a sliding window of size n. At timestep t, if we have accumulated N keyframes, this\nis represents poses Xt =(xi)N\u2212n\u2264i\u2264N and measurements Mt =\n\u0010\n\u02c6Ds\ni | s\u2208S\n\u0011\nN\u2212n\u2264i\u2264N. Similar to\n21\npose updates in visual SLAM [88, 69, 97], the network weights \u00af\u03b8 are frozen and we estimate the\nSE(3) poses Xt instead.\nWe formulate the problem as a nonlinear least squares optimization with custom measure-\nment factors in Theseus [55]. While prior work uses gradient descent [88], we instead use a\nsecond-order Levenberg\u2013Marquardt (LM) solver, which provides faster convergence [13]. The\npose graph, illustrated in Figure 8 (c), solves for:\n\u02c6Xt = argmin\nXt\nLpose(Xt | Mt, \u00af\u03b8)\nwhere\nLpose = wsdfLsdf + wregLreg + wicpLicp\n\u2022 SDF loss Lsdf. We use the shape loss Lshape, modified such that we sample only about\nsurface points of each ray. This works well for both visual and tactile sensing as we have\nhigher confidence in SDFs about the surface of the object than in free-space. For each\ndepth measurement in Mt, we sample surface points over M rays, and average the SDF\nloss along each ray. This results in an M \u00d7n SDF loss, which we use to update the se(3)\nlie algebra of Xt. We implement a custom Jacobian for this cost function, which is up to\n4\u00d7 more efficient than PyTorch automatic differentiation.\n\u2022 Pose regularizer Lreg. We apply a weak regularizer between consecutive keyframe poses\nin Xt to ensure the relative pose updates stay well-behaved. This is important for robustness\nto noisy frontend depth and incorrect segmentations.\n\u2022 ICP loss Licp. We further apply iterative closest point (ICP) between the current visuo-\ntactile pointcloud \u03a0\u22121(Mt) and previous pointcloud \u03a0\u22121(Mt\u22121). This gives us frame-to-\nframe constraints in addition to the frame-to-model Lsdf.\n22\nReferences\n[1] Alex Alspach, Kunimatsu Hashimoto, Naveen Kuppuswamy, and Russ Tedrake.\nSoft-\nbubble: A highly compliant dense geometry tactile sensor for robot manipulation. In Proc.\nIEEE Intl. Conf. on Soft Robotics (RoboSoft), pages 597\u2013604. IEEE, 2019. 3, 13\n[2] Rares Ambrus, Vitor Guizilini, Naveen Kuppuswamy, Andrew Beaulieu, Adrien Gaidon,\nand Alex Alspach. Monocular depth estimation for soft visuotactile sensors. In Proc. IEEE\nIntl. Conf. on Soft Robotics (RoboSoft), 2021. 18, 20\n[3] Dejan Azinovi\u00b4c, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus\nThies. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6290\u20136301, 2022. 17, 21\n[4] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing\nneural radiance fields. In Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5855\u20135864, 2021. 13\n[5] Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, and Alberto\nRodriguez. simple: a visuotactile method learned in simulation to precisely pick, localize,\nregrasp, and place objects. arXiv preprint arXiv:2307.13133, 2023. 9, 12, 13\n[6] Maria Bauza, Oleguer Canal, and Alberto Rodriguez. Tactile mapping and localization\nfrom high-resolution tactile imprints. In Proc. IEEE Intl. Conf. on Robotics and Automation\n(ICRA), pages 3811\u20133817. IEEE, 2019. 5, 18, 20\n[7] Samarth Brahmbhatt, Ankur Handa, James Hays, and Dieter Fox. ContactGrasp: Func-\ntional multi-finger grasp synthesis from contact. In Proc. IEEE/RSJ Intl. Conf. on Intelli-\ngent Robots and Systems (IROS), pages 2386\u20132393. IEEE, 2019. 16, 35\n[8] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jos\u00e9 Neira,\nIan Reid, and John J Leonard. Past, present, and future of Simultaneous Localization and\nMapping: Toward the robust-perception age. IEEE Trans. on Robotics (TRO), 32(6):1309\u2013\n1332, 2016. 13, 18\n[9] Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srini-\nvasa, Pieter Abbeel, and Aaron M Dollar. Yale-CMU-Berkeley dataset for robotic ma-\nnipulation research. Intl. J. of Robotics Research (IJRR), 36(3):261\u2013268, 2017. 16, 20,\n35\n23\n[10] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal.\nVisual dexterity: In-hand dexterous manipulation from depth. In ICML Workshop on New\nFrontiers in Learning, Control, and Dynamical Systems, 2023. 2, 6, 10, 13\n[11] Yiting Chen, Ahmet Ercan Tekden, Marc Peter Deisenroth, and Yasemin Bekiroglu. Sliding\ntouch-based exploration for modeling unknown object shape with multi-fingered hands.\narXiv preprint arXiv:2308.00576, 2023. 3\n[12] Andrew J Davison. FutureMapping: The computational structure of spatial AI systems.\narXiv preprint arXiv:1803.11288, 2018. 2\n[13] Frank Dellaert and Michael Kaess. Factor graphs for robot perception. Foundations and\nTrends in Robotics, 6(1-2):1\u2013139, 2017. 21, 22\n[14] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.\nSuperpoint:\nSelf-\nsupervised interest point detection and description. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 224\u2013236, 2018. 14\n[15] Eric Dexheimer and Andrew J Davison. Learning a depth covariance function. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n13122\u201313131, 2023. 14\n[16] Elliott Donlon, Siyuan Dong, Melody Liu, Jianhua Li, Edward Adelson, and Alberto Ro-\ndriguez. GelSlim: A high-resolution, compact, robust, and calibrated tactile-sensing finger.\nIn Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pages 1927\u20131934.\nIEEE, 2018. 3\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020. 37\n[18] Danny Driess, Jung-Su Ha, Marc Toussaint, and Russ Tedrake. Learning models as func-\ntionals of signed-distance fields for manipulation planning. In Conference on Robot Learn-\ning, pages 245\u2013255. PMLR, 2022. 13\n[19] Paul Furgale, Joern Rehder, and Roland Siegwart. Unified temporal and spatial calibration\nfor multi-sensor systems. In 2013 IEEE/RSJ International Conference on Intelligent Robots\nand Systems, pages 1280\u20131286. IEEE, 2013. 15\n24\n[20] Sergio Garrido-Jurado, Rafael Mu\u00f1oz-Salinas, Francisco Jos\u00e9 Madrid-Cuevas, and\nManuel Jes\u00fas Mar\u00edn-Jim\u00e9nez. Automatic generation and detection of highly reliable fidu-\ncial markers under occlusion. Pattern Recognition, 47(6):2280\u20132292, 2014. 15\n[21] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics:\nThe KITTI dataset. The International Journal of Robotics Research, 32(11):1231\u20131237,\n2013. 15\n[22] Phillip Grote, Joaquim Ortiz-Haro, Marc Toussaint, and Ozgur S Oguz.\nNeural field\nrepresentations of articulated objects for robotic manipulation planning. arXiv preprint\narXiv:2309.07620, 2023. 3\n[23] Irmak Guzey, Ben Evans, Soumith Chintala, and Lerrel Pinto.\nDexterity from touch:\nSelf-supervised pre-training of tactile representations with robotic play.\narXiv preprint\narXiv:2303.12076, 2023. 2\n[24] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh,\nJingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sun-\ndaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, and Gavriel State. DeX-\ntreme: Transfer of agile in-hand manipulation from simulation to reality. arXiv, 2022. 2, 6,\n9, 10\n[25] Ankur Handa, Thomas Whelan, John McDonald, and Andrew J Davison. A benchmark for\nrgb-d visual odometry, 3d reconstruction and slam. In 2014 IEEE international conference\non Robotics and automation (ICRA), pages 1524\u20131531. IEEE, 2014. 11\n[26] Hannah B Helbig and Marc O Ernst. Optimal integration of shape information from vision\nand touch. Experimental brain research, 179(4):595\u2013606, 2007. 3\n[27] Carolina Higuera, Byron Boots, and Mustafa Mukadam. Learning to read braille: Bridging\nthe tactile reality gap with diffusion models. arXiv preprint arXiv:2304.01182, 2023. 14\n[28] Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders GlentBuch, Dirk\nKraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, et al. BOP: Benchmark\nfor 6D object pose estimation. In Proceedings of the European conference on computer vi-\nsion (ECCV), pages 19\u201334, 2018. 15\n[29] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan\nSunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to\n3d. arXiv preprint arXiv:2311.04400, 2023. 13\n25\n[30] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLbench: The\nrobot learning benchmark and learning environment. IEEE Robotics and Automation Let-\nters, 5(2):3019\u20133026, 2020. 15\n[31] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3D Gaus-\nsian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG),\n42(4):1\u201314, 2023. 13\n[32] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski,\nAngjoo Kanazawa, and Ken Goldberg. Evo-NeRF: Evolving NeRF for sequential robot\ngrasping of transparent objects. In 6th Annual Conference on Robot Learning, 2022. 3\n[33] Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque, Jeffrey Ichnowski, Roberto Calan-\ndra, and Ken Goldberg. Learning self-supervised representations from vision and touch for\nactive sliding perception of deformable surfaces. arXiv preprint arXiv:2209.13042, 2022.\n13, 14\n[34] Leonid Keselman, Katherine Shih, Martial Hebert, and Aaron Steinfeld. Optimizing algo-\nrithms from pairwise user preferences. arXiv preprint arXiv:2308.04571, 2023. 11\n[35] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014. 38\n[36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Seg-\nment anything. arXiv preprint arXiv:2304.02643, 2023. 18, 19\n[37] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun.\nTanks and temples:\nBenchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG),\n36(4):1\u201313, 2017. 5\n[38] Yann Labb\u00e9, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan\nTremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d\npose estimation of novel objects via render & compare. arXiv preprint arXiv:2212.06870,\n2022. 9\n[39] Alexander Sasha Lambert, Mustafa Mukadam, Balakumar Sundaralingam, Nathan Ratliff,\nByron Boots, and Dieter Fox.\nJoint inference of kinematic and force trajectories with\nvisuo-tactile sensing. In Proc. IEEE Intl. Conf. on Robotics and Automation (ICRA), pages\n3165\u20133171. IEEE, 2019. 3, 9\n26\n[40] Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose\nMost, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, et al. DIGIT:\nA novel design for a low-cost compact high-resolution tactile sensor with application to in-\nhand manipulation. IEEE Robotics and Automation Letters (RA-L), 5(3):3838\u20133845, 2020.\n3, 15, 37\n[41] Simon Le Cleac\u2019h, Hong-Xing Yu, Michelle Guo, Taylor Howell, Ruohan Gao, Jiajun Wu,\nZachary Manchester, and Mac Schwager. Differentiable physics simulation of dynamics-\naugmented neural objects. IEEE Robotics and Automation Letters, 8(5):2780\u20132787, 2023.\n3, 14\n[42] Marion Lepert, Chaoyi Pan, Shenli Yuan, Rika Antonova, and Jeannette Bohg. In-hand\nmanipulation of unknown objects with tactile sensing for insertion. In Embracing Contacts-\nWorkshop at ICRA 2023, 2023. 3, 13\n[43] Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu\nLiu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 8456\u20138465, 2023. 3\n[44] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey.\nBARF: Bundle-\nadjusting neural radiance fields. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5741\u20135751, 2021. 17\n[45] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles\nMacklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac Gym:\nHigh performance GPU-based physics simulation for robot learning.\narXiv preprint\narXiv:2108.10470, 2021. 5, 15, 39\n[46] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoor-\nthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis.\nCommunications of the ACM, 65(1):99\u2013106, 2021. 3, 18, 21\n[47] Mark Moll and Michael A Erdmann. Reconstructing the shape and motion of unknown\nobjects with active tactile sensors. In Algorithmic Foundations of Robotics V, pages 293\u2013\n309. Springer, 2004. 3\n[48] Hans Moravec. Mind children: The future of robot and human intelligence. Harvard Uni-\nversity Press, 1988. 2\n27\n[49] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graph-\nics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG),\n41(4):1\u201315, 2022. 3, 13, 17, 18\n[50] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob Mc-\nGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al.\nSolving Rubik\u2019s Cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019. 2\n[51] OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafa\u0142 J\u00f3zefowicz, Bob\nMcGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas\nSchneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba.\nLearning dexterous in-hand manipulation. CoRR, 2018. 2\n[52] Joseph Ortiz, Alexander Clegg, Jing Dong, Edgar Sucar, David Novotny, Michael Zoll-\nhoefer, and Mustafa Mukadam. iSDF: Real-time neural signed distance fields for robot\nperception. arXiv preprint arXiv:2204.02296, 2022. 3, 17, 20, 21\n[53] Akhil Padmanabha, Frederik Ebert, Stephen Tian, Roberto Calandra, Chelsea Finn, and\nSergey Levine. OmniTact: A multi-directional high-resolution touch sensor. In Proc. IEEE\nIntl. Conf. on Robotics and Automation (ICRA), pages 618\u2013624. IEEE, 2020. 3\n[54] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\nDeepSDF: Learning continuous signed distance functions for shape representation. In Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n165\u2013174, 2019. 3, 13\n[55] Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi,\nRicky TQ Chen, Joseph Ortiz, Daniel DeTone, Austin Wang, Stuart Anderson, Jing Dong,\nBrandon Amos, and Mustafa Mukadam. Theseus: A Library for Differentiable Nonlinear\nOptimization. Advances in Neural Information Processing Systems, 2022. 13, 17, 22, 38\n[56] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-hand object\nrotation via rapid motor adaptation. In Conference on Robot Learning, pages 1722\u20131732.\nPMLR, 2022. 2, 3, 4, 6, 15, 39, 41\n[57] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and\nJitendra Malik. General in-hand object rotation with vision and touch. In Conference on\nRobot Learning, pages 1722\u20131732. PMLR, 2023. 2, 6, 13, 14, 15\n28\n[58] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.\nVision transformers for dense\nprediction. In Proceedings of the IEEE/CVF international conference on computer vision,\npages 12179\u201312188, 2021. 20, 36\n[59] Revopoint. Revopoint POP 3 3D Scanner, 2023. 16, 35\n[60] Antoni Rosinol, John J Leonard, and Luca Carlone. NeRF-SLAM: Real-time dense monoc-\nular SLAM with neural radiance fields. arXiv preprint arXiv:2210.13641, 2022. 17, 18\n[61] Johannes L Schonberger and Jan-Michael Frahm.\nStructure-from-Motion revisited.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\n4104\u20134113, 2016. 18\n[62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 39\n[63] Yu She, Shaoxiong Wang, Siyuan Dong, Neha Sunil, Alberto Rodriguez, and Edward Adel-\nson. Cable manipulation with a tactile-reactive gripper. Intl. J. of Robotics Research (IJRR),\n40(12-14):1385\u20131401, 2021. 2\n[64] Edward J Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari, David Meger,\nJitendra Malik, and Michal Drozdzal. 3D shape reconstruction from vision and touch. In\nProc. Conf. on Neural Information Processing Systems (NeurIPS), 2020. 3, 12\n[65] Edward J Smith, David Meger, Luis Pineda, Roberto Calandra, Jitendra Malik, Adriana\nRomero, and Michal Drozdzal. Active 3D shape reconstruction from vision and touch. In\nProc. Conf. on Neural Information Processing Systems (NeurIPS), 2021. 12\n[66] Paloma Sodhi, Michael Kaess, Mustafa Mukadam, and Stuart Anderson. Learning tac-\ntile models for factor graph-based estimation. In Proc. IEEE Intl. Conf. on Robotics and\nAutomation (ICRA), pages 13686\u201313692. IEEE, 2021. 3, 9\n[67] Paloma Sodhi, Michael Kaess, Mustafa Mukadam, and Stuart Anderson. Patchgraph: In-\nhand tactile tracking with learned surface normals. In Proc. IEEE Intl. Conf. on Robotics\nand Automation (ICRA), 2022. 3, 18, 20\n[68] Claudius Strub, Florentin W\u00f6rg\u00f6tter, Helge Ritter, and Yulia Sandamirskaya. Correcting\npose estimates during tactile exploration of object shape: a neuro-robotic study. In 4th\nInternational Conference on Development and Learning and on Epigenetic Robotics, pages\n26\u201333. IEEE, 2014. 3\n29\n[69] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison. iMAP: Implicit mapping\nand positioning in real-time. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6229\u20136238, 2021. 3, 12, 17, 21, 22, 42\n[70] Balakumar Sundaralingam and Tucker Hermans. In-hand object-dynamics inference using\ntactile fingertips. IEEE Transactions on Robotics, 37(4):1115\u20131126, 2021. 14\n[71] Sudharshan Suresh, Maria Bauza, Kuan-Ting Yu, Joshua G Mangelson, Alberto Rodriguez,\nand Michael Kaess. Tactile SLAM: Real-time inference of shape and pose from planar\npushing. In Proc. IEEE Intl. Conf. on Robotics and Automation (ICRA), May 2021. 3, 12,\n13\n[72] Sudharshan Suresh, Zilin Si, Stuart Anderson, Michael Kaess, and Mustafa Mukadam.\nMidastouch: Monte-carlo inference over distributions across sliding touch. In 6th Annual\nConference on Robot Learning, 2022. 2, 9, 20\n[73] Sudharshan Suresh, Zilin Si, Joshua G Mangelson, Wenzhen Yuan, and Michael Kaess.\nShapeMap 3-D: Efficient shape mapping through dense touch and vision. In Proc. IEEE\nIntl. Conf. on Robotics and Automation (ICRA), Philadelphia, PA, USA, May 2022. 3, 12,\n18, 20\n[74] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,\nUtkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let\nnetworks learn high frequency functions in low dimensional domains. Advances in Neural\nInformation Processing Systems, 33:7537\u20137547, 2020. 17\n[75] Maxim Tatarchenko, Stephan R Richter, Ren\u00e9 Ranftl, Zhuwen Li, Vladlen Koltun, and\nThomas Brox. What do single-view 3D reconstruction networks learn? In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages 3405\u20133414,\n2019. 5\n[76] Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan\nBirchfield. Deep object pose estimation for semantic robotic grasping of household objects.\narXiv preprint arXiv:1809.10790, 2018. 5\n[77] Jonathan Tremblay, Bowen Wen, Valts Blukis, Balakumar Sundaralingam, Stephen Tyree,\nand Stan Birchfield. Diff-dope: Differentiable deep object pose estimation. arXiv preprint\narXiv:2310.00463, 2023. 5\n30\n[78] Shaoxiong Wang, Mike Maroje Lambeta, Po-Wei Chou, and Roberto Calandra. TACTO:\nA fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors.\nIEEE Robotics and Automation Letters (RA-L), 2022. 3, 15, 17, 20, 37\n[79] Shaoxiong Wang, Yu She, Branden Romero, and Edward Adelson. GelSight Wedge: Mea-\nsuring high-resolution 3D contact geometry with a compact robot finger. In Proc. IEEE\nIntl. Conf. on Robotics and Automation (ICRA). IEEE, 2021. 3, 13, 18, 20\n[80] Shaoxiong Wang, Jiajun Wu, Xingyuan Sun, Wenzhen Yuan, William T Freeman, Joshua B\nTenenbaum, and Edward H Adelson. 3D shape perception from monocular vision, touch,\nand shape priors. In Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS),\npages 1606\u20131613. IEEE, 2018. 3, 13\n[81] Benjamin Ward-Cherrier, Nicholas Pestell, Luke Cramphorn, Benjamin Winstone,\nMaria Elena Giannaccini, Jonathan Rossiter, and Nathan F Lepora. The TacTip family: Soft\noptical tactile sensors with 3D-printed biomimetic morphologies. Soft robotics, 5(2):216\u2013\n227, 2018. 3\n[82] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas M\u00fcller, Alex Evans,\nDieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d recon-\nstruction of unknown objects. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 606\u2013617, 2023. 3, 12\n[83] Youngsun Wi, Andy Zeng, Pete Florence, and Nima Fazeli. Virdo++: Real-world, visuo-\ntactile dynamics and perception of deformable objects. arXiv preprint arXiv:2210.03701,\n2022. 3\n[84] Wonik Robotics. Allegro Hand, 2023. 3, 15\n[85] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia\nGkioxari. Multiview compressive coding for 3D reconstruction. arXiv:2301.08247, 2023.\n13\n[86] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. PoseCNN: A convo-\nlutional neural network for 6D object pose estimation in cluttered scenes. arXiv preprint\narXiv:1711.00199, 2017. 5\n[87] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Fed-\nerico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in\nvisual computing and beyond. In Computer Graphics Forum, volume 41, pages 641\u2013676.\nWiley Online Library, 2022. 3\n31\n[88] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and\nTsung-Yi Lin.\niNeRF: Inverting neural radiance fields for pose estimation.\nIn 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1323\u2013\n1330. IEEE, 2021. 3, 17, 22, 42\n[89] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang.\nRo-\ntating without seeing:\nTowards in-hand dexterity through touch.\narXiv preprint\narXiv:2303.10880, 2023. 2, 15\n[90] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance\nfields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 3\n[91] Kuan-Ting Yu, John Leonard, and Alberto Rodriguez. Shape and pose recovery from planar\npushing. In Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pages\n1208\u20131215. IEEE, 2015. 3\n[92] Kuan-Ting Yu and Alberto Rodriguez. Realtime state estimation with tactile and visual\nsensing: application to planar manipulation. In Proc. IEEE Intl. Conf. on Robotics and\nAutomation (ICRA), pages 7778\u20137785. IEEE, 2018. 3, 9\n[93] Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. GelSight: High-resolution robot\ntactile sensors for estimating geometry and force. Sensors, 17(12):2762, 2017. 3, 13, 20\n[94] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee,\nand Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile\napplications. arXiv preprint arXiv:2306.14289, 2023. 20\n[95] Jialiang Zhao, Maria Bauza, and Edward H Adelson.\nFingerSLAM: Closed-loop un-\nknown object localization and reconstruction from visuo-tactile feedback. arXiv preprint\narXiv:2303.07997, 2023. 3, 6, 12, 13\n[96] Shaohong Zhong, Alessandro Albini, Oiwi Parker Jones, Perla Maiolino, and Ingmar Pos-\nner. Touching a NeRF: Leveraging neural radiance fields for tactile sensory data generation.\nIn 6th Annual Conference on Robot Learning, 2022. 3\n[97] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Mar-\ntin R Oswald, and Marc Pollefeys. NICE-SLAM: Neural implicit scalable encoding for\nSLAM. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12786\u201312796, 2022. 3, 12, 17, 22, 42\n32\nAcknowledgments\nThe authors thank Dhruv Batra, Theophile Gervet, Akshara Rai for feedback on the writing, and\nWei Dong, Tess Hellebrekers, Carolina Higuera, Patrick Lancaster, Franziska Meier, Alberto\nRodriguez, Akash Sharma, Jessica Yin for helpful discussions on the research.\nAuthor contributions:\nSudharshan Suresh developed and implemented the core approach including tactile transformer,\nvisual depth segmentation, neural SDF reconstruction, pose-graph optimization, performed full-\nstack tuning, worked on Allegro and DIGIT integration, TACTO and IsaacGym integration, cam-\nera and robot calibration, data collection, ground truth object scans, and live visualizations, con-\nducted evaluations, made visuals, and wrote the paper.\nHaozhi Qi designed and implemented in-hand object rotation policies and sim-to-real policy\ntransfer, helped with Allegro and DIGIT integration, TACTO and IsaacGym integration, data\ncollection, did code reviews and bug fixes, and helped edit the paper.\nTingfan Wu coordinated hardware and software systems integration, performed profiling of soft-\nware stack, helped with Allegro and DIGIT integration, camera and robot calibration, ground\ntruth object scans, and advised on evaluations.\nTaosha Fan designed and implemented forward kinematics, helped implement visual depth seg-\nmentation, pose-graph cost functions and optimization, software systems integration, and advised\non evaluations.\nLuis Pineda implemented workflow for cluster deployment, streamlined development workflow,\nhelped with modules that use Theseus, did code reviews and bug fixes, and advised on evalua-\ntions.\nMike Lambeta helped with Allegro and DIGIT integration, TACTO and IsaacGym integration,\nand hardware systems integrations.\nJitendra Malik advised on the project, gave feedback on approach, evaluations, and the paper.\nMrinal Kalakrishnan advised on the project, managed and supported researchers, gave feedback\non approach, evaluations, and the paper.\nRoberto Calandra advised on the project, helped with Allegro and DIGIT integration, TACTO\nand IsaacGym integration, gave feedback on approach, evaluations, and the paper.\nMichael Kaess advised on the project, helped design pose-graph optimization, gave feedback on\napproach, evaluations, and the paper.\nJoseph Ortiz advised on the project, co-developed the core approach, implemented volumetric\nray sampling, SDF cost function, and 2D live visualizations, helped implement workflow for\ncluster deployment, streamlined development workflow, did code reviews and bug fixes, gave\nfeedback on evaluations, designed visuals, and edited the paper.\n33\nMustafa Mukadam set the vision and research direction, steered and aligned the team, provided\nguidance on all aspects of the project including core approach, systems, and evaluations, de-\nsigned visuals, and edited the paper.\nFunding: Sudharshan Suresh and Haozhi Qi acknowledge funding from Meta, and their work\nwas partially conducted while at FAIR, Meta. Sudharshan Suresh was further partially supported\nby NSF grant IIS-2008279 while at CMU. Roberto Calandra acknowledge support from the\nGerman Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany\u2019s\nExcellence Strategy \u2013 EXC 2050/1 \u2013 Project ID 390696704 \u2013 Cluster of Excellence \u201cCentre\nfor Tactile Internet with Human-in-the-Loop\u201d (CeTI) of Technische Universit\u00e4t Dresden, and\nfrom Bundesministerium f\u00fcr Bildung und Forschung (BMBF) and German Academic Exchange\nService (DAAD) in project 57616814 (School of Embedded and Composite AI, SECAI).\n34\nSupplementary materials\n\u2022 Section S1 to S7\n\u2022 Figure S1 to S12\n\u2022 Multimedia on our webpage at https://suddhu.github.io/neural-feels\nS1\nGround-truth shape and pose\nGround-truth object scans. Our results in Section 2 require ground-truth object shape to com-\npare against. For this, we use a commercial dual-camera infrared scanner, the Revopoint POP\n3 [59]. The hardware can scan objects from a close range with a minimum precision of 0.05 mm.\nEach real-world object is placed on a turntable and scanned while rotating about its axis (Fig-\nure S1 (a)). For object\u2019s that lack texture, an artificial dot pattern is tracked by adding stickers.\nAfter generating the scans, we perform hole-filling for unseen regions of the object, like the bot-\ntom. Figure S1 (b) shows all the scanned meshes\u2014a few meshes are directly sourced from the\nYCB [9] and ContactDB [7] datasets.\nPseudo ground-truth pose. In the real-world, we pass three RGB-D cameras as input into our\npose tracking pipeline to use as a pseudo ground-truth estimate. This consists of three unique\ncameras (front left, back right, top down) with complementary but overlapping fields-of-view\n(Figure S2 and Figure S3 (b)). With this broad perspective of the scene, known shape from\nground-truth scans, and the tracker running at 0.5Hz, we can obtain an accurate estimation of\nobject pose at each timestep.\nFigure S1: Object ground-truth with dual-camera infrared scanner. (a) Objects are placed on a turntable and\nscanned, followed by post-processing to ensure complete, accurate meshes. (b) Meshes visualized for the real and\nsimulated FeelSight objects.\n35\nFigure S2: Robot cell for pseudo-ground-truth tracking. Each of the three camera\u2019s captures a different field-of-\nview of the interaction (left). For a pseudo-ground-truth, we pass the RGB-D stream from all three cameras into our\npipeline, with known shape obtained from scanning. The output pose tracking (right) represents the ground-truth\nwe compare to in the real-world results.\nFigure S3: (a) As a proof-of-concept, we assembled a minimal robot cell for live demonstrations of our method\nwith one RGB-D camera and the robot policy deployed at 2Hz. (b) The three different RGB-D camera viewpoints in\nour full robot cell used to collect FeelSight evaluation dataset. (c) Average pose error for known shape experiments\nbased on camera viewpoint. We see that while the front and back cameras perform comparably, there are larger\nerrors in the top-down camera as it is further away.\nS2\nTactile transformer: data and training\nModel architecture. Our model architecture is based on a monocular depth network, the dense\nprediction transformer (DPT) [58]. It comprises of a vision transformer (ViT) backbone that out-\nputs bag-of-words features at different resolutions, finally combined into a dense prediction via\na convolutional decoder. Compared to fully-convolutional methods, DPT has a global receptive\nfield and the resulting embedding does not explicitly down-sample the image.\nTraining and loss metric. Our image-to-depth training dataset comprises of 10K simulated tac-\ntile interactions each on the surface of 40 YCB objects. We illustrate examples of the interactions\n36\nFigure S4: Our tactile transformer is trained in simulation with real-world augmentation. (a) The tactile\ntransformer is supervised from paired RGB-depth images rendered in TACTO [78]. (b) Each of these samples are\ngenerated from dense, random interactions with 40 different YCB objects. (c) In our training, we augment the data\nwith background images collected from 25 unique DIGIT sensors [40].\nin Figure S4 (b). We use the ADAM optimizer with momentum and a batch size of 100, trained\nwith mean-square depth reconstruction loss (Figure S4 (a)). We start with a pre-trained small\nViT [17], with an embedding dimension of 384 patch size of 16. The dataloader splits the train,\ntest, and validation data into 60%, 20%, and 20% respectively. To supplement our results in\nSection 4.6.2, we visualize additional simulation results in Figure S5.\nData augmentation. An important aspect of generalization and sim-to-real transfer is the aug-\nmentation applied during data collection and training. These include:\n\u2022 Real-world backgrounds. We compose simulated renderings with real-world background\nimages, collected from 25 different DIGIT sensors. These are shown in Figure S2 (c).\n\u2022 Pose variations. Before rendering a sensor pose, we apply noise in rotation/translation\nand sensing normal direction. Additionally, we randomly vary the distance of penetration\ninto the object surface.\n\u2022 Sensor lighting. We randomize position, direction and intensity of the three DIGIT LEDs.\n\u2022 Sensor pixel noise. We add Gaussian noise to RGB data, with a standard deviation of 7px.\n\u2022 Standard transforms. Randomized horizontal flipping, cropping, and rotations of the\ntactile images.\n37\nFigure S5: Image to depth predictions by the tactile transformer on simulated contacts. Our tactile transformer\nshows good performance in simulated interactions, capturing both large contact patches, as well as smaller edge\nfeatures. These objects are unseen during training\u2014as highlighted in Section 4.6.2, we demonstrate an average\nprediction error of 0.042 mm on simulated test images.\nS3\nAdditional implementation details\nSegmented visual depth. As discussed in Section 4.6.1, we use the 3D center of grasp, by\ncomputing the centroid of the end-effectors as a positive point prompt for SAM. However, in\npractice, this prompt alone doesn\u2019t suffice. First, the robot fingers frequently appear in these\nsegmentations, which is misleading to our shape optimizer. This is solved by adding negative\npoint prompts to fingertip pixels that we obtain by projecting the forward kinematics results. We\nfirst verify if the fingertips are unoccluded by the object, which we do by comparing against the\ncurrent rendered object model. Second, SAM tends to over segment objects with distinct parts\n(e.g. different faces of the Rubik\u2019s cube). In case of these ambiguities, SAM outputs multiple\nmasks, at different spatial scales. We apply a final pruning step to find the mask prediction closest\nto the average mask area we typically observe in simulation.\nShape optimizer. The neural field is optimized via Adam [35] with learning rate of 2e-4\nand weight decay of 1e-6. Instant-NGP uses a hash table of size 219 for positional encoding,\nfollowed by a 3-layer MLP with 64 dimensional width. We use a uniform random weights \u03b8init\nand initialize the SDF by running 500 shape iterations using the first keyframe K0.\nFor evaluating the neural field we freeze the network and query a 2003 feature grid. The\nfeature grid\u2019s extents are defined as a bounding box of 15 cm side, centered at the object\u2019s initial\npose x0. When training, we apply a series of bounding-box checks post hoc, to eliminate any\nray samples Pu found outside this bounding box. Mesh visualizations (Figure 4) are periodically\ngenerated via marching-cubes on the feature grid. We add color to the mesh by averaging the\ncolored object pointcloud with a Gaussian kernel.\nPose optimizer. We use the vectorized SE(3) pose graph optimizer in Theseus [55], with\n20 LM iterations of step size 1.0. The keyframe window size n=3 and we run 2 pose iterations\nfor each shape iteration. The weighting factors for each loss are wsdf = 0.01, wreg = 0.01, and\nwicp =1.0.\n38\nCompute and timings. All results in Section 2 are generated from playing-back the trials\nat a publishing rate of 1 Hz. Experimentally, however, we can run the pose optimizer at 10 Hz\nand full backend at 5 Hz. Figure S3 (a) has a minimal robot setup of an online SLAM system\nwith rotation policy in-the-loop. Experiments are run on an Nvidia GeForce RTX 4090, while\nthe aggregate results are evaluated on a cluster with Nvidia Tesla V100s.\nS4\nIn-hand exploration policy\nWe first train a policy in simulation with access to an embedding of physical properties such as\nobject position, size, mass, friction, and center-of-mass (denoted as zt). From the joint-angles\nqt and this embedding zt, the policy outputs a PD controller target at \u2208 R16. The policy is\ntrained in parallel simulated environments [45] using proximal policy optimization [62]. The\nreward function is a weighted combination of a rotational reward, joint-angle regularizer, torque\npenalty, and object velocity penalty. The resulting policy can adaptively rotate objects in-hand\naccording to different physical properties.\nDuring deployment, however, the policy does not have access to these physical properties.\nThe estimator is instead trained to infer zt from a history of proprioceptive states, which is in\nturn fed into the policy \u03c0t. A crucial change compared to Qi et al. [56] is that we train the policy\nto rotate objects with DIGIT sensors on the distal ends (Figure 1). This results in different gaits,\nas it (i) relies on finger-object friction instead of gravity, and (ii) learns to maintain contact with\nthe DIGIT gelpads.\nS5\nAdditional results\nShape and pose metrics over time. In Figure S6, we plot these metrics for each of the experi-\nments in Figure 4, instead against 0\u221230 sec timesteps. For shape, we observe gradual convergence\nto an asymptote close to 1.0, indicating evolution of both shape completion and refinement over\ntime. Also visualized here is the precision and recall metrics over time, whose harmonic mean\nrepresents the F-score. For pose, we observe stable drift over time, indicating the estimated\nobject pose lies close to the ground-truth estimate.\nEffect of camera viewpoint in the real-world. In Section 2.4, we establish the relationship\nbetween occlusion/sensing noise and pose error. Here, we run additional experiments, on a\nlimited set of viewpoints in the real-world. Figure S3 (b) shows the RGB-D data from three\ncameras front left, back right, top down, at distances of 27 cm, 28 cm, and 49 cm respectively\nfrom the robot. We run our vision-only pose tracker with known shape using each of three\ncameras over all 5 Rubik\u2019s cube rotation experiments and plot the average metrics in Figure S3\n39\nFigure S6: Shape and pose metrics over time for in-hand SLAM. Here, we plot the time-varying metrics for\nexperiments visualized in Figure 4. First, we note the gradual increase in F-score over time with further coverage.\nAdditionally, we have bounded pose drift over time\u2014for each experiment we omit the first five seconds as the metric\nis ill-defined then.\nFigure S7: Pose (left) and shape (right) metrics for each object class, sorted in best-to-worst performance.\n(c). We observe that the front left and back right viewpoints result in lowest average pose error\ndue to their closer proximity. The top down camera gives less reliable depth measurements and\nsegmentation output, leading to almost 2x greater pose error.\nClass-specific metrics. In Figure S7, we present our metrics for the SLAM results in Section 2.2,\ndividing based on object class. This helps us make some assessments on how object geometry\nand scale can affect our results. Some observations include:\n\u2022 Object symmetry. Objects with symmetries about their rotation axis are challenging for\nour depth-based estimator. This leads to higher pose errors for the peach and pear, for\nexample.\n\u2022 Object visibility. Partial visibility of the large objects, such as the pepper grinder, affect\nthe completeness of the reconstructions. Touch in this case is not advantageous since the\nfinger gait does not span the length of the object to provide coverage.\n\u2022 Object scale. Smaller-sized objects, such as the peach, may demonstrate better shape\nmetrics as their scale is closer to the F-score threshold of 5 mm.\n40\nFigure S8: A collage depicting the entirety of the FeelSight dataset. We collect (i) 5 sequences each (row) in\nthe real-world across 6 different objects (column), and (ii) 5 sequences each (row) in simulation across 8 different\nobjects (column).\nS6\nAdditional visualizations\nAll experiments from the FeelSight dataset. In Figure S8 we illustrate all of the 70 visuo-\ntactile experiments that comprise our dataset. While both simulation and real data collection use\nthe proprioception-driven policy [56], the policy generalizes better in simulation across the class\nof objects. Some objects in the real-world require a human-in-the-loop to assist with in-hand\nrotation; e.g. supporting cube-shaped objects from the bottom to occasionally prevent falling out\nof hand.\nAdditional neural tracking visualizations. Figure S9 shows rendering results from the exper-\niments in Section 2.3 along with the pose axes. We see good alignment of the renderings when\noverlaid on the RGB camera frame.\nFurther visual segmentation results. Figure S10 shows additional qualitative results of visual\nsegmentation for (a) real-world and (b) simulated rotations sequences.\nFigure S9: Further visualizations of neural tracking experiments. These qualitatively complement the results\nfrom Section 2.3 for both (a) simulated and (b) real-world experiments.\n41\nFigure S10: Additional results on visual segmentation. Our segmentation module can accurately singulate the\nin-hand object in both (a) real-world and (b) simulated image sequences.\nFigure S11: Sensor coverage illustrated in final mesh reconstructions of select objects\u2014indicating vision, touch,\nand hallucinated regions.\nS7\nIllustrating the role of touch\nSensor coverage visualized in SLAM. To illustrate the complementary nature of touch and\nvision, we color the reconstructed mesh regions based on their dominant sensing modality in\nFigure S11. After running the SLAM experiments in Section 2.2, we first run marching-cubes\non the final neural SDF. In the resultant mesh, we assign each vertices color based on if vision or\ntouch is the nearest pointcloud measurement to it. In the case where there is no vision or touch\npointcloud within a 5 mm radius, it is assigned as a hallucinated vertex. This is a demonstrable\nadvantage of neural SDFs, where the network can extrapolate well based on information in the\nneighborhood of the query point. From the meshes in Figure S11 we see that while vision gets\nbroad coverage of each object, there is considerable tactile signal from the interaction utilized\nfor shape estimation.\nTouch aligns local geometries with predicted depth. As described in Section 4.7.2, the pose\noptimizer inverts the neural field to back-propagate a loss in pose space [88, 69, 97]. This has\nbeen illustrated in work such as iNeRF [88], where the rendered neural field attempts to match the\nimage measurements via updates to the se(3) Lie algebra of the camera pose. As our framework\n42\nFigure S12: Six examples of tactile images compared against the neural field. We see that our tactile pose optimizer\nmatches the predicted local geometry with the neural surface rendering. Thus, patches and edges predicted by touch\nappear in the rendering as well.\nleverages the idea that vision-based touch is just another perspective camera, we show how the\nrendered neural field matches with tactile depth features in Figure S12.\nEach RGB image is first passed through the tactile transformer (Section 4.6.2) to output a\npredicted tactile depthmap. Our pose optimizer aligns the neural rendering of the surface with\nthe measured depthmap, based on 3D samples from the measured depthmap. Thus we can see\nthat both in simulation (Figure S12 (a)) and the real-world (Figure S12 (b)), the edge and patch\nfeatures predicted match well with the rendered object.\n43\n"
  },
  {
    "title": "Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.13763.pdf",
    "upvote": "9",
    "text": "Align Your Gaussians:\nText-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models\nHuan Ling1,2,3 *\nSeung Wook Kim1,2,3 *\nAntonio Torralba4\nSanja Fidler1,2,3\nKarsten Kreis1\n1NVIDIA\n2Vector Institute\n3University of Toronto\n4MIT\nProject page: https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/\nFigure 1.\nText-to-4D synthesis with Align Your Gaussians (AYG). Top: Different dynamic 4D sequences. Dotted lines represent\ndynamics of deformation field. Bottom: Multiple dynamic 4D objects are composed within a large dynamic scene; two time frames shown.\nAbstract\nText-guided diffusion models have revolutionized image\nand video generation and have also been successfully used\nfor optimization-based 3D object synthesis. Here, we in-\nstead focus on the underexplored text-to-4D setting and syn-\nthesize dynamic, animated 3D objects using score distilla-\ntion methods with an additional temporal dimension. Com-\npared to previous work, we pursue a novel compositional\ngeneration-based approach, and combine text-to-image,\ntext-to-video, and 3D-aware multiview diffusion models to\nprovide feedback during 4D object optimization, thereby si-\nmultaneously enforcing temporal consistency, high-quality\nvisual appearance and realistic geometry.\nOur method,\ncalled Align Your Gaussians (AYG), leverages dynamic 3D\nGaussian Splatting with deformation fields as 4D represen-\ntation. Crucial to AYG is a novel method to regularize the\ndistribution of the moving 3D Gaussians and thereby sta-\nbilize the optimization and induce motion. We also pro-\npose a motion amplification mechanism as well as a new\nautoregressive synthesis scheme to generate and combine\nmultiple 4D sequences for longer generation. These tech-\nniques allow us to synthesize vivid dynamic scenes, out-\nperform previous work qualitatively and quantitatively and\nachieve state-of-the-art text-to-4D performance. Due to the\nGaussian 4D representation, different 4D animations can\nbe seamlessly combined, as we demonstrate. AYG opens\nup promising avenues for animation, simulation and digital\ncontent creation as well as synthetic data generation.\n1. Introduction\nGenerative modeling of dynamic 3D scenes has the poten-\ntial to revolutionize how we create games, movies, simu-\n*Equal contribution.\n1\narXiv:2312.13763v2  [cs.CV]  3 Jan 2024\nFigure 2.\nText-to-4D synthesis with\nAYG. We generate dynamic 4D scenes\nvia score distillation. We initialize the 4D\nsequence from a static 3D scene (gener-\nated first, Fig. 3), which is represented\nby 3D Gaussians with means \u00b5i, scales\n\u03c3i, opacities \u03b7i and colors \u2113i. Consec-\nutive rendered frames x\ncj\n\u03c4j from the 4D\nsequence at times \u03c4j and camera posi-\ntions cj are diffused and fed to a text-\nto-video diffusion model [7] (green ar-\nrows), which provides a distillation gra-\ndient that is backpropagated through the\nrendering process into a deformation field\n\u2206(x, y, z, \u03c4) (dotted lines) that captures\nscene motion.\nSimultaneously, random\nframes \u02dcx\n\u02dccj\n\u03c4j are diffused and given to a\ntext-to-image diffusion model [70] (red\narrows) whose gradients ensure that high\nvisual quality is maintained frame-wise.\nlations, animations and entire virtual worlds. Many works\nhave shown how a wide variety of 3D objects can be synthe-\nsized via score distillation techniques [10, 11, 31, 41, 52, 62,\n79, 85, 88, 92, 109], but they typically only synthesize static\n3D scenes, although we live in a moving, dynamic world.\nWhile image diffusion models have been successfully ex-\ntended to video generation [1, 7, 22, 28, 78, 90, 91, 107],\nthere is little research on similarly extending 3D synthesis\nto 4D generation with an additional temporal dimension.\nWe propose Align Your Gaussians (AYG), a novel\nmethod for 4D content creation.\nIn contrast to previous\nwork [79], we leverage dynamic 3D Gaussians [36] as back-\nbone 4D representation, where a deformation field [59, 63]\ncaptures scene dynamics and transforms the collection of\n3D Gaussians to represent object motion.\nAYG takes a\ncompositional generation-based perspective and leverages\nthe combined gradients of latent text-to-image [70], text-to-\nvideo [7] and 3D-aware text-to-multiview-image [76] dif-\nfusion models in a score distillation-based synthesis frame-\nwork. A 3D-aware multiview diffusion model and a regu-\nlar text-to-image model are used to generate an initial high-\nquality 3D shape. Afterwards, we compose the gradients\nof a text-to-video and a text-to-image model; the gradients\nof the text-to-video model optimize the deformation field to\ncapture temporal dynamics, while the text-to-image model\nensures that high visual quality is maintained for all time\nframes (Fig. 2). To this end, we trained a dedicated text-to-\nvideo model; it is conditioned on the frame rate and can cre-\nate useful gradients both for short and long time intervals,\nwhich allows us to generate long and smooth 4D sequences.\nWe developed several techniques to ensure stable opti-\nmization and learn vivid dynamic 4D scenes in AYG: We\nemploy a novel regularization method that uses a modi-\nfied version of the Jensen-Shannon divergence to regular-\nize the locations of the 3D Gaussians such that the mean\nand variance of the set of 3D Gaussians is preserved as\nthey move. Furthermore, we use a motion amplification\nmethod that carefully scales the gradients from the text-to-\nvideo model and enhances motion. To extend the length\nof the 4D sequences or combine different dynamic scenes\nwith changing text guidance, we introduce an autoregres-\nsive generation scheme which interpolates the deformation\nfields of consecutive sequences. We also propose a new\nview-guidance method to generate consistent 3D scenes for\ninitialization of the 4D stage, and we leverage the concur-\nrent classifier score distillation method [102].\nWe find that AYG can generate diverse, vivid, detailed\nand 3D-consistent dynamic scenes (Fig. 1), achieving state-\nof-the-art text-to-4D performance.\nWe also show long,\nautoregressively extended 4D scenes, including ones with\nvarying text guidance, which has not been demonstrated be-\nfore. A crucial advantage of AYG\u2019s 4D Gaussian backbone\nrepresentation is that different 4D animations can trivially\nbe combined and composed together, which we also show.\nWe envision broad applications in digital content cre-\nation, where AYG takes a step beyond the literature on text-\nto-3D and captures our world\u2019s rich dynamics. Moreover,\nAYG can generate 4D scenes with exact tracking labels for\nfree, a promising feature for synthetic data generation.\nContributions. (i) We propose AYG, a system for text-\nto-4D content creation leveraging dynamic 3D Gaussians\nwith deformation fields as 4D representation. (ii) We show\nhow to tackle the text-to-4D task through score distillation\nwithin a new compositional generation framework, com-\nbining 2D, 3D, and video diffusion models. (iii) To scale\nAYG, we introduce a novel regularization method and a\nnew motion amplification technique. (iv) Experimentally,\nwe achieve state-of-the-art text-to-4D performance and gen-\nerate high-quality, diverse, and dynamic 4D scenes. (v) For\nthe first time, we also show how our 4D sequences can\nbe extended in time with a new autoregressive generation\nscheme and even creatively composed in large scenes.\n2\n2. Background\n3D Gaussian Splatting [36] represents 3D scenes by N\n3D Gaussians with positions \u00b5i, covariances \u03a3i, opacities\n\u03b7i and colors \u2113i (Fig. 2). Rendering corresponds to projec-\ntion of the 3D Gaussians onto the 2D camera\u2019s image plane,\nproducing 2D Gaussians with projected means \u02c6\u00b5i and co-\nvariances \u02c6\u03a3i. The color C(p) of image pixel p can be cal-\nculated through point-based volume rendering [111] as\nC(p) =\nN\nX\ni=1\n\u2113i\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j) ,\n(1)\n\u03b1i = \u03b7i exp\n\u0014\n\u22121\n2 (p \u2212 \u02c6\u00b5i)\u22a4 \u02c6\u03a3\u22121\ni\n(p \u2212 \u02c6\u00b5i)\n\u0015\n,\n(2)\nwhere j iterates over the Gaussians along the ray through\nthe scene from pixel p until Gaussian i. To accelerate ren-\ndering, the image plane can be divided into tiles, which\nare processed in parallel. Initially proposed for 3D scene\nreconstruction, 3D Gaussian Splatting uses gradient-based\nthresholding to densify areas that need more Gaussians to\ncapture fine details, and unnecessary Gaussians with low\nopacity are pruned every few thousand optimization steps.\nDiffusion Models and Score Distillation Sampling.\nDiffusion-based generative models (DMs) [18, 27, 57, 80,\n81] use a forward diffusion process that gradually perturbs\ndata, such as images or entire videos, towards entirely ran-\ndom noise, while a neural network is learnt to denoise and\nreconstruct the data. DMs have also been widely used for\nscore distillation-based generation of 3D objects [62]. In\nthat case, a 3D object, represented for instance by a neural\nradiance field (NeRF) [54] or 3D Gaussians [36], like here,\nwith parameters \u03b8 is rendered from different camera views\nand the renderings x are diffused and given to a text-to-\nimage DM. In the score distillation sampling (SDS) frame-\nwork, the DM\u2019s denoiser is then used to construct a gradient\nthat is backpropagated through the differentiable rendering\nprocess g into the 3D scene representation and updates the\nscene representation to make the scene rendering look more\nrealistic, like images modeled by the DM. Rendering and\nusing DM feedback from many different camera perspec-\ntives then encourages the scene representation to form a ge-\nometrically consistent 3D scene. The SDS gradient [62] is\n\u2207\u03b8LSDS(x = g(\u03b8)) = Et,\u03f5\n\u0014\nw(t) (\u02c6\u03f5\u03d5(zt, v, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n,\nwhere x denotes the 2D rendering, t is the time up to which\nthe diffusion is run to perturb x, w(t) is a weighting func-\ntion, and zt is the perturbed rendering. Further, \u02c6\u03f5\u03d5(zt, v, t)\nis the DM\u2019s denoiser neural network that predicts the dif-\nfusion noise \u03f5. It is conditioned on zt, the diffusion time t\nand a text prompt v for guidance. Classifier-free guidance\n(CFG) [26] typically amplifies the text conditioning.\nFigure 3. In AYG\u2019s initial 3D stage we synthesize a static 3D\nscene leveraging a text-guided multiview diffusion model [76] and\na regular text-to-image model [70]. The text-to-image model re-\nceives viewing angle-dependent text prompts and leverages view\nguidance (Sec. 3.4). See Fig. 2 for 4D stage and descriptions.\n2.1. Related Work\nSee Supp. Material for an extended discussion. Here, we\nonly briefly mention the most relevant related literature.\nAs\ndiscussed,\nAYG\nbuilds\non\ntext-driven\nim-\nage [5, 14, 21, 61, 67, 70, 72, 98],\nvideo [1, 7,\n22, 25, 28, 38, 78, 90, 91, 94, 107] and 3D-aware\nDMs [42, 44, 45, 56, 64, 75, 76, 104], uses score distillation\nsampling [10, 11, 17, 31, 41, 48, 52, 62, 85, 88, 92, 96, 109]\nand leverages 3D Gaussian Splatting [36] as well as defor-\nmation fields [8, 59, 60, 63, 84] for its 4D representation.\nThe concurrent works DreamGaussian [83], GSGEN [12]\nand GaussianDreamer [101] use 3D Gaussian Splatting to\nsynthesize static 3D scenes, but do not consider dynamics.\nDynamic 3D Gaussian Splatting has been used for 4D\nreconstruction [50, 93, 110], but not for 4D generation. The\nidea to compose the gradients of multiple DMs has been\nused before for controllable image generation [19, 43], but\nhas received little attention in 3D or 4D synthesis.\nMost related to AYG is Make-A-Video3D (MAV3D) [79],\nto the best of our knowledge the only previous work\nthat generates dynamic 4D scenes with score distillation.\nMAV3D uses NeRFs with HexPlane [9] features as 4D\nrepresentation, in contrast to AYG\u2019s dynamic 3D Gaus-\nsians, and it does not disentangle its 4D representation into\na static 3D representation and a deformation field model-\ning dynamics. MAV3D\u2019s representation prevents it from\ncomposing multiple 4D objects into large dynamic scenes,\nwhich our 3D Gaussian plus deformation field representa-\ntion easily enables, as we show. Moreover, MAV3D\u2019s se-\nquences are limited in time, while we show a novel au-\ntoregressive generation scheme to extend our 4D sequences.\n3\nFigure 4.\nAYG\u2019s JSD-based regular-\nization of the evolving 4D Gaussians\n(see Sec. 3.4) calculates the 3D mean \u03bd\u03c4\nand diagonal covariance matrix \u0393\u03c4 of the\nset of dynamic 3D Gaussians at different\ntimes \u03c4 of the 4D sequence and regular-\nizes them to not vary too much.\nAYG outperforms MAV3D qualitatively and quantitatively\nand synthesizes significantly higher-quality 4D scenes. Our\nnovel compositional generation-based approach contributes\nto this, which MAV3D does not pursue. Finally, instead of\nregular SDS, used by MAV3D, in practice AYG employs\nclassifier score distillation [102] (see Sec. 3.4).\n3. Align Your Gaussians\nIn Sec. 3.1, we present AYG\u2019s 4D representation, and in\nSec. 3.2, we introduce its compositional generation frame-\nwork with multiple DMs. In Sec. 3.3, we lay out AYG\u2019s\nscore distillation framework in practice, and in Sec. 3.4, we\ndiscuss several novel methods and extensions to scale AYG.\n3.1. AYG\u2019s 4D Representation\nAYG\u2019s 4D representation combines 3D Gaussian Splat-\nting [36] with deformation fields [59, 63] to capture the\n3D scene and its temporal dynamics in a disentangled man-\nner. Specifically, each 4D scene consists of a set of N 3D\nGaussians as in Sec. 2. Following Kerbl et al. [36], we\nalso use two degrees of spherical harmonics to model view-\ndependent effects, this is, directional color, and thereby im-\nprove the 3D Gaussians\u2019 expressivity. Moreover, we re-\nstrict the 3D Gaussians\u2019 covariance matrices to be isotropic\nwith scales \u03c3i. We made this choice as our 3D Gaussians\nmove as a function of time and learning expressive dynam-\nics is easier for spherical Gaussians. We denote the col-\nlection of learnable parameters of our 3D Gaussians as \u03b8.\nThe scene dynamics are modeled by a deformation field\n\u2206\u03a6(x, y, z, \u03c4) = (\u2206x, \u2206y, \u2206z), defined through a multi-\nlayer perceptron (MLP) with parameters \u03a6. Specifically,\nfor any 3D location (x, y, z) and time \u03c4, the deformation\nfield predicts a displacement (\u2206x, \u2206y, \u2206z). The 3D Gaus-\nsians smoothly follow these displacements to represent a\nmoving and deforming 4D scene (Fig. 2). Note that in prac-\ntice we preserve the initial 3D Gaussians for the first frame,\ni.e. \u2206\u03a6(x, y, z, 0) = (0, 0, 0), by setting \u2206\u03a6(x, y, z, \u03c4) =\n(\u03be(\u03c4)\u2206x, \u03be(\u03c4)\u2206y, \u03be(\u03c4)\u2206z) where \u03be(\u03c4) = \u03c4 0.35 such that\n\u03be(0) = 0 and \u03be(1) = 1. Following Luiten et al. [50], we\nregularize the deformation field so that nearby Gaussians\ndeform similarly (\u201crigidity regularization\u201d, see Supp. Mat.).\nApart from the intuitive decomposition into a backbone\n3D representation and a deformation field to model dynam-\nics, a crucial advantage of AYG\u2019s dynamic 3D Gaussian-\nbased representation is that different dynamic scenes, each\nwith its own set of Gaussians and deformation field, can be\nFigure 5. AYG\u2019s autoregressive extension scheme interpolates\nthe deformation fields of an initial and an extended 4D sequence\nwithin an overlap interval between the two sequences (Sec. 3.4).\neasily combined, thereby enabling promising 3D dynamic\ncontent creation applications (see Fig. 1). This is due to the\nexplicit nature of this representation, in contrast to typical\nNeRF-based representations. Moreover, learning 4D scenes\nwith score distillation requires many scene renderings. This\nalso makes 3D Gaussians ideal due to their rendering effi-\nciency [36]. Note that early on we also explored MAV3D\u2019s\nHexPlane- and NeRF-based 4D representation [79], but we\nwere not able to achieve satisfactory results.\n3.2. Text-to-4D as Compositional Generation\nWe would like AYG\u2019s synthesized dynamic 4D scenes to\nbe of high visual quality, be 3D-consistent and geometri-\ncally correct, and also feature expressive and realistic tem-\nporal dynamics. This suggests to compose different text-\ndriven DMs during the distillation-based generation to cap-\nture these different aspects. (i) We use the text-to-image\nmodel Stable Diffusion (SD) [70], which has been trained\non a broad set of imagery and provides a strong general im-\nage prior. (ii) We also utilize the 3D-aware text-conditioned\nmulti-view DM MVDream [76], which generates multi-\nview images of 3D objects, was fine-tuned from SD on\nthe object-centric 3D dataset Objaverse [15, 16] and pro-\nvides a strong 3D prior. It defines a distribution over four\nmultiview-consistent images corresponding to object ren-\nderings from four different camera perspectives c1, ..., c4.\nMoreover, we train a text-to-video DM, following Vide-\noLDM [7], but with a larger text-video dataset (HDVG-\n130M [90] and Webvid-10M [4]) and additional condi-\ntioning on the videos\u2019 frame rate (see Supp.\nMaterial\nfor details). This video DM provides temporal feedback\nwhen rendering 2D frame sequences from our dynamic 4D\nscenes. All used DMs are latent DMs [70, 86], which means\nthat in practice we first encode renderings of our 4D scenes\ninto the models\u2019 latent spaces, calculate score distillation\ngradients there, and backpropagate them through the mod-\nels\u2019 encoders. All DMs leverage the SD 2.1 backbone and\nshare the same encoder. To keep the notation simple, we do\n4\nFigure 6. Text-to-4D synthesis with AYG. Various samples shown in two views each. Dotted lines denote deformation field dynamics.\nnot explicitly incorporate the encoding into our mathemati-\ncal description below and the visualizations (Figs. 2 and 3).\nWe disentangle optimization into first synthesizing a\nstatic 3D Gaussian-based object \u03b8, and then learning the\ndeformation field \u03a6 to add scene dynamics.\nStage 1: 3D Synthesis (Fig. 3).\nWe first use MV-\nDream\u2019s multiview image prior to generate a static 3D scene\nvia score distillation (Supp. Mat. for details). Since MV-\nDream on its own would generate objects in random ori-\nentations, we enforce a canonical pose by combining MV-\nDream\u2019s gradients with those of regular SD, while augment-\ning the text-conditioning for SD with directional texts \u201cfront\nview\u201d, \u201cside view\u201d, \u201cback view\u201d and \u201coverhead view\u201d [62].\nFormally, we can derive a score distillation gradient (see\nSec. 3.3) by minimizing the reverse Kulback-Leibler diver-\ngence (KLD) from the rendering distribution to the product\nof the composed MVDream and SD model distributions\nKL\n\u0012\nq\u03b8\n\u0000{zci}4, {\u02dcz\u02dccj}K\n\u0001 \f\f\f\f\n\f\f\f\fp\u03b1\n3D ({zci}4)\nK\nY\nj=1\np\u03b2\nim\n\u0000\u02dcz\u02dccj\u0001\u0013\n,\nsimilar to Poole et al. [62] (App. A.4). Here, p3D({zci}4)\nrepresents the MVDream-defined multiview image distri-\nbution over four diffused renderings from camera views ci,\ndenoted as the set {zci}4 (we omit the diffusion time t sub-\nscript for brevity). Moreover, pim(\u02dcz\u02dccj) is the SD-based gen-\neral image prior and {\u02dcz\u02dccj}K is another set of K diffused\nscene renderings. In principle, the renderings for SD and\nMVDream can be from different camera angles ci and \u02dccj,\nbut in practice we choose K=4 and use the same render-\nings. Furthermore, \u03b1 and \u03b2 are adjustable temperatures of\nthe distributions p3D and pim, and q\u03b8 denotes the distribu-\ntion over diffused renderings defined by the underlying 3D\nscene representation \u03b8, which is optimized through the dif-\nferentiable rendering. We also use the Gaussian densifica-\ntion method discussed in Sec. 2 (see Supp. Material).\nStage 2: Adding Dynamics for 4D Synthesis (Fig. 2).\nWhile in stage 1, we only optimize the 3D Gaussians, in\nstage 2, the main 4D stage, we optimize (only) the defor-\nmation field \u03a6 to capture motion and extend the static 3D\nscene to a dynamic 4D scene with temporal dimension \u03c4.\nTo this end, we compose the text-to-image and text-to-video\nDMs and formally minimize a reverse KLD of the form\nKL\n\u0012\nq\u03a6\n\u0010\n{zci\n\u03c4i}F , {\u02dcz\u02dccj\n\u02dc\u03c4j}M\n\u0011 \f\f\f\f\n\f\f\f\fp\u03b3\nvid\n\u0000{zci\n\u03c4i}F\n\u0001 M\nY\nj=1\np\u03ba\nim\n\u0010\n\u02dcz\u02dccj\n\u02dc\u03c4j\n\u0011\u0013\n,\nwhere pvid({zci\n\u03c4i}F ) is the video DM-defined distribution\n5\nFigure 7. Autoregressively extended text-to-4D synthesis. AYG\nis able to autoregressively extend dynamic 4D sequences, combine\nsequences with different text-guidance, and create looping anima-\ntions, returning to the initial pose (also see Supp. Video).\nover F 4D scene renderings {zci\n\u03c4i}F taken at times \u03c4i and\ncamera angles ci (F=16 for our model). Similar to before,\nM additional renderings are given to the SD-based general\nimage prior, and \u03b3 and \u03ba are temperatures. The renderings\n{\u02dcz\u02dccj\n\u02dc\u03c4j}M fed to regular SD can be taken at different times \u02dc\u03c4j\nand cameras \u02dccj than the video model frames, but in practice\nM=4 and we use three random renderings as well as the\n8th middle frame among the ones given to the video model.\nq\u03a6 defines the distribution over renderings by the 4D scene\nwith the learnable deformation field parameters \u03a6.\nWe\ncould render videos from the 4D scene with a fixed cam-\nera, but in practice dynamic cameras, i.e. varying ci, help\nto learn more vivid 4D scenes, similar to Singer et al. [79].\nMoreover, following Singer et al. [79], our video DM\nis conditioned on the frame rate (fps) and we choose the\ntimes 0 \u2264 \u03c4i \u2264 1 accordingly by sampling fps \u2208 {4, 8, 12}\nand the starting time. We render videos from the 4D scene\nand condition the video DM with the sampled fps. This\nhelps generating not only sufficiently long but also tempo-\nrally smooth 4D animations, as different fps correspond to\nlong-term and short-term dynamics. Therefore, when ren-\ndering short but high fps videos they only span part of the\nentire length of the 4D sequence. Also see Supp. Material.\nOptimizing the deformation field while supervising both\nwith a video and image DM is crucial. The video DM gen-\nerates temporal dynamics, but text-to-video DMs are not as\nrobust as general text-to-image DMs. Including the image\nDM during this stage ensures stable optimization and that\nhigh visual frame quality is maintained (ablations in Sec. 4).\nA crucial advantage of the disentangled two stage design\nis that AYG\u2019s main 4D synthesis method\u2014the main inno-\nvation of this work\u2014could in the future in principle also\nbe applied to 3D objects originating from other generation\nsystems or even to synthetic assets created by digital artists.\n3.3. AYG\u2019s Score Distillation in Practice\nAbove, we have laid out AYG\u2019s general synthesis frame-\nwork. The full stage 2 score distillation gradient including\nCFG can be expressed as (stage 1 proceeds analogously)\n\u2207\u03a6LAYG\nSDS = Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03b3\n\u0012\n\u03c9vid\n\u0002\u02c6\u03f5vid(Z, v, t) \u2212 \u02c6\u03f5vid(Z, t)\n\u0003\n+ \u02c6\u03f5vid(Z, v, t) \u2212 \u03f5vid\n|\n{z\n}\n\u03b4vid\ngen\n\u0013\n+ \u03ba\n\u0012\n\u03c9im\n\u0002\u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u02c6\u03f5im(\u02dcZ, t)\n\u0003\n+ \u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u03f5im\n|\n{z\n}\n\u03b4im\ngen\n\u0013\u001b\u2202{x}\n\u2202\u03a6\n\u0015\n,\n(3)\nwhere Z := {zci\n\u03c4i}F , \u02dcZ := {\u02dcz\u02dccj\n\u02dc\u03c4j}M, \u03c9vid/im are the CFG\nscales for the video and image DMs, \u02c6\u03f5vid(Z, v, t) and\n\u02c6\u03f5im(\u02dcZ, v, t) are the corresponding denoiser networks and\n\u03f5vid and \u03f5im are the diffusion noises (an analogous SDS gra-\ndient can be written for stage 1). Moreover, {x} denotes\nthe set of all renderings from the 4D scene through which\nthe SDS gradient is backpropagated, and which are diffused\nto produce Z and \u02dcZ. Recently, ProlificDreamer [92] pro-\nposed a scheme where the control variates \u03f5vid/im above are\nreplaced by DMs that model the rendering distribution, are\ninitialized from the DMs guiding the synthesis (\u02c6\u03f5vid(Z, v, t)\nand \u02c6\u03f5im(\u02dcZ, v, t) here), and are then slowly fine-tuned on the\ndiffused renderings (Z or \u02dcZ here). This means that at the\nbeginning of optimization the terms \u03b4vid/im\ngen\nin Eq. (3) would\nbe zero. Inspired by this observation and aiming to avoid\nProlificDreamer\u2019s cumbersome fine-tuning, we instead pro-\npose to simply set \u03b4vid/im\ngen\n= 0 entirely and optimize with\n\u2207\u03a6LAYG\nCSD = Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03c9vid\n\u0002\u02c6\u03f5vid(Z, v, t) \u2212 \u02c6\u03f5vid(Z, t)\n|\n{z\n}\n\u03b4vid\ncls\n\u0003\n+ \u03c9im\n\u0002\u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u02c6\u03f5im(\u02dcZ, t)\n|\n{z\n}\n\u03b4im\ncls\n\u0003\u001b\u2202{x}\n\u2202\u03a6\n\u0015\n,\n(4)\nwhere we absorbed \u03b3 and \u03ba into \u03c9vid/im. Interestingly, this\nexactly corresponds to the concurrently proposed classifier\nscore distillation (CSD) [102], which points out that the\nabove two terms \u03b4vid/im\ncls\nin Eq. (4) correspond to implicit\nclassifiers predicting v from the video or images, respec-\ntively. CSD then uses only \u03b4vid/im\ncls\nfor score distillation, re-\nsulting in improved performance over SDS. We discovered\nthat scheme independently, while aiming to inherit Prolific-\nDreamer\u2019s strong performance. Supp. Material for details.\n6\nFigure 8. AYG (ours) vs. MAV3D [79]. We show four 4D frames\nfor different times and camera angles (also see Supp. Video).\n3.4. Scaling Align Your Gaussians\nTo scale AYG and achieve state-of-the-art text-to-4D per-\nformance, we introduce several further novel techniques.\nDistribution Regularization of 4D Gaussians. We de-\nveloped a method to stabilize optimization and ensure real-\nistic learnt motion. We calculate the means \u03bd\u03c4 and diagonal\ncovariances \u0393\u03c4 of the entire set of 3D Gaussians (using their\nmeans \u00b5i) at times \u03c4 along the 4D sequence (Fig. 4). Defin-\ning a Normal distribution N(\u03bd\u03c4, \u0393\u03c4) with these means and\ncovariances, we regularize with a modified version of the\nJensen-Shannon divergence JSD (N(\u03bd0, \u03930)||N(\u03bd\u03c4, \u0393\u03c4))\nbetween the 3D Gaussians at the initial and later frames\n\u03c4 (see Supp. Material). This ensures that the mean and\nthe diagonal covariance of the distribution of the Gaussians\nstay approximately constant and encourages AYG to gen-\nerate meaningful and complex dynamics instead of simple\nglobal translations and object size changes.\nExtended Autoregressive Generation.\nBy default,\nAYG produces relatively short 4D sequences, which is due\nto the guiding text-to-video model, which itself only gen-\nerates short video clips (see Blattmann et al. [7]). To over-\ncome this limitation, we developed a method to autoregres-\nsively extend the 4D sequences. We use the middle 4D\nframe from a first sequence as the initial frame of a second\nsequence, optimizing a second deformation field, optionally\nusing a different text prompt. As the second sequence is ini-\ntialized from the middle frame of the first sequence, there is\nan overlap interval with length 0.5 of the total length of each\nsequence.\nWhen optimizing for the second deformation\nfield, we smoothly interpolate between the first and second\nTable 1. Comparison to MAV3D [79] by user study on synthe-\nsized 4D scenes with 28 text prompts. Numbers are percentages.\nMethod\nAYG (ours)\nMAV3D [79]\nEqual\npreference\npreferred\npreferred\npreference\nOverall Quality\n53.6\n38.8\n7.6\n3D Appearance\n47.4\n37.2\n15.4\n3D Text Alignment\n45.9\n38.8\n15.3\nMotion Amount\n45.9\n38.8\n15.3\nMotion Text Alignment\n47.4\n33.7\n18.9\nMotion Realism\n44.4\n43.9\n11.7\ndeformation fields for the overlap region (Fig. 5). Specif-\nically, we define \u2206interpol\n\u03a612\n= (1 \u2212 \u03c7(\u03c4))\u2206\u03a61 + \u03c7(\u03c4)\u2206\u03a62\nwhere \u03c7 is a linear function with \u03c7(\u03c40.5) = 0 and \u03c7(\u03c41.0) =\n1, \u03c40.5 and \u03c41.0 represent the middle and last time frames of\nthe first sequence, \u2206interpol\n\u03a612\nis the interpolated deformation\nfield, and \u2206\u03a61(kept fixed) and \u2206\u03a62 are the deformation\nfields of the first and second sequence, respectively. We\nadditionally minimize LInterpol-Reg. = ||\u2206\u03a61 \u2212 \u2206interpol\n\u03a612\n||2\n2\nwithin the overlap region to regularize the optimization pro-\ncess of \u2206\u03a62. For the non-overlap regions, we just use the\ncorresponding \u2206\u03a6. With this careful interpolation tech-\nnique the deformation field smoothly transitions from the\nfirst sequence\u2019s into the second sequence\u2019s. Without it, we\nobtained abrupt, unrealistic transitions.\nMotion Amplification. When a set of 4D scene ren-\nderings is given to the text-to-video model, it produces a\n(classifier) score distillation gradient for each frame i. We\nexpect most motion when the gradient for each frame points\ninto a different direction. With that in mind, we propose a\nmotion amplification technique. We post-process the video\nmodel\u2019s individual frame scores \u03b4vid\ncls i (i \u2208 {1, ..., F}) as\n\u03b4vid\ncls i \u2192\n\n\u03b4vid\ncls i\n\u000b\n+ \u03c9ma\n\u0000\u03b4vid\ncls i \u2212\n\n\u03b4vid\ncls i\n\u000b\u0001\n, where\n\n\u03b4vid\ncls i\n\u000b\nis\nthe average score over the F video frames and \u03c9ma is the\nmotion amplifier scale. This scheme is inspired by CFG\nand reproduces regular video model scores for \u03c9ma=1. For\nlarger \u03c9ma, the difference between the individual frames\u2019\nscores and the average is amplified, thereby encouraging\nlarger frame differences and more motion.\nView Guidance. In AYG\u2019s 3D stage, for the text-to-\nimage model we use a new view guidance. We construct\nan additional implicit classifier term \u03c9vg\n\u0002\u02c6\u03f5im(z, vaug, t) \u2212\n\u02c6\u03f5im(z, v, t)\n\u0003\n, where vaug denotes the original text prompt v\naugmented with directional texts such as \u201cfront view\u201d (see\nSec. 3.2) and \u03c9vg is the guidance scale. View guidance am-\nplifies the effect of directional text prompt augmentation.\nNegative Prompting.\nWe also use negative prompt\nguidance during both the 3D and 4D stages. During the\n4D stage, we use \u201clow motion, static statue, not moving, no\nmotion\u201d to encourage AYG to generate more dynamic and\nvivid 4D scenes. Supp. Material for 3D stage and details.\n4. Experiments\nText-to-4D. In Fig. 6, we show text-to-4D sequences gen-\nerated by AYG (hyperparameters and details in Supp. Ma-\n7\nAlign Your Gaussians\nOverall\n3D\n3D Text\nMotion\nMotion Text\nMotion\n(full model)\nQuality\nAppearance\nAlignment\nAmount\nAlignment\nRealism\nv.s. w/o rigidity regularization\n45.8/13.3\n43.3/19.2\n38.3/15.0\n40.8/15.0\n42.5/18.3\n30.8/26.7\nv.s. w/o motion amplifier\n43.3/23.3\n37.5/28.3\n30.8/26.7\n45.8/10.8\n37.5/26.7\n33.3/31.7\nv.s. w/o initial 3D stage\n67.5/15.0\n57.5/21.7\n64.2/15.0\n60.8/21.7\n60.8/20.8\n59.2/24.2\nv.s. w/o JSD-based regularization\n40.0/25.0\n40.0/27.5\n36.7/27.5\n41.7/24.2\n39.2/29.2\n45.0/24.2\nv.s. w/o image DM score in 4D stage\n42.5/22.5\n39.2/27.5\n36.7/25.8\n33.3/25.9\n37.5/30.0\n27.5/40.0\nv.s. SDS instead of CSD\n44.2/35.8\n40.0/27.5\n35.8/35.0\n35.0/27.5\n35.0/34.2\n32.5/35.8\nv.s. 3D stage w/o MVDream\n66.7/21.7\n48.3/34.2\n38.3/34.2\n41.7/22.5\n40.0/27.5\n40.8/27.5\nv.s. 4D stage with MVDream\n50.8/27.5\n38.3/34.2\n41.6/29.2\n39.2/35.0\n44.2/30.0\n39.2/31.7\nv.s. video model with only fps 4\n46.7/15.8\n27.5/36.7\n30.0/23.3\n36.7/30.0\n31.7/26.7\n32.5/28.3\nv.s. video model with only fps 12\n48.3/29.2\n30.8/29.2\n29.2/28.3\n35.0/28.3\n35.0/30.0\n39.2/26.7\nv.s. w/o dynamic cameras\n32.5/25.0\n32.5/31.7\n35.0/33.3\n35.0/32.5\n35.8/33.3\n32.5/25.0\nv.s. w/o negative prompting\n44.2/28.3\n38.3/32.5\n31.7/29.2\n29.2/31.6\n33.3/30.0\n37.5/28.3\nTable 2.\nAblation study by user\nstudy on synthesized 4D scenes\nwith 30 text prompts. For each pair\nof numbers, the left number is the\npercentage that the full AYG model\nis preferred and the right number\nindicates preference percentage for\nablated model as described in left\ncolumn. The numbers do not add\nup to 100 and the difference is due\nto users voting \u201cno preference\u201d (de-\ntails in Supp. Material).\nterial). AYG can generate realistic, expressive, detailed and\nvivid dynamic 4D scenes (4D scenes can be rendered at\nvarying speeds and frame rates). Importantly, our method\ndemonstrates zero-shot generalization capabilities to cre-\native text prompts corresponding to scenes that are unlikely\nto be found in the diffusion models\u2019 training images and\nvideos. More results in Supp. Material and on project page.\nTo compare AYG to MAV3D [79], we performed a com-\nprehensive user study where we took the 28 rendered videos\nfrom MAV3D\u2019s project page2 and compared them to corre-\nsponding generations from AYG with the same text prompts\n(Table 1). We asked the users to rate overall quality, 3D\nappearance and text alignment, as well as motion amount,\nmotion text alignment and motion realism (user study de-\ntails in Supp. Material). AYG outperforms MAV3D on all\nmetrics, achieving state-of-the-art text-to-4D performance\n(we also evaluated R-Precision [32, 58] on a larger prompt\nset used by MAV3D [78, 79], performing on par, see Supp.\nMat.; however, R-Precision is a meaningless metric to eval-\nuate dynamic scenes). Qualitative comparisons are shown\nin Fig. 8 (more in Supp. Mat.). We see that AYG produces\nmore detailed 4D outputs. Note that MAV3D uses an extra\nbackground model, while AYG does not. Adding a similar\nbackground model would be easy but is left to future work.\nAblation Studies. Next, we performed an ablation study\non AYG\u2019s different components. We used a set of 30 text\nprompts and generated 4D scenes for versions of AYG with\nmissing or modified components, see Table 2. Using the\nsame categories as before, we asked users to rate preference\nof our full method vs. the ablated AYG variants. Some com-\nponents have different effects with respect to 3D appearance\nand motion, but we generally see that all components matter\nsignificantly in terms of overall quality, i.e., for all ablations\nour full method is strongly preferred over the ablated AYG\nversions. This justifies AYG\u2019s design. A thorough discus-\nsion is presented in the Supp. Material, but we highlight\nsome relevant observations. We see that our novel JSD-\nbased regularization makes a major difference, and we also\nobserve that the motion amplifier indeed has a strong effect\nfor \u201cMotion Amount\u201d. Moreover, our compositional ap-\nproach is crucial. Running the 4D stage without image DM\n2https://make-a-video3d.github.io/\nfeedback produces much worse 3D and overall quality. Also\nthe decomposition into two stages is important\u2014carrying\nout 4D synthesis without initial 3D stage performs poorly.\nTemporally Extended 4D Synthesis and Large Scene\nComposition.\nIn Fig. 7, we show autoregressively ex-\ntended text-to-4D results with changing text prompts (also\nsee Supp. Video). AYG can realistically connect differ-\nent 4D sequences and generate expressive animations with\nchanging dynamics and behavior. We can also create se-\nquences that loop endlessly by enforcing that the last frame\nof a later sequence matches the first frame of an earlier one\nand suppressing the deformation field there (similar to how\nwe enforce zero deformation at \u03c4=0 in Sec. 3.1). Finally,\ndue to the explicit nature of the dynamic 3D Gaussians,\nAYG\u2019s 4D representation, multiple animated 4D objects can\nbe easily composed into larger scenes, each shape with its\nown deformation field defining its dynamics. We show this\nin Fig. 1, where each dynamic object in the large scene is\ngenerated, except for the ground plane. These capabilities,\nnot shown by previous work [79], are particularly promising\nfor practical content creation applications.\n5. Conclusions\nWe presented Align Your Gaussians for expressive text-to-\n4D synthesis. AYG builds on dynamic 3D Gaussian Splat-\nting with deformation fields as well as score distillation with\nmultiple composed diffusion models. Novel regularization\nand guidance techniques allow us to achieve state-of-the-art\ndynamic scene generation and we also show temporally ex-\ntended 4D synthesis as well as the composition of multiple\ndynamic objects within a larger scene. AYG has many po-\ntential applications for creative content creation and it could\nalso be used in the context of synthetic data generation. For\nexample, AYG would enable synthesis of videos and 4D se-\nquences with exact tracking labels, useful for training dis-\ncriminative models. AYG currently cannot easily produce\ntopological changes of the dynamic objects. Overcoming\nthis limitation would be an exciting avenue for future work.\nOther directions include scaling AYG beyond object-centric\ngeneration and personalized 4D synthesis. The initial 3D\nobject could be generated from a personalized diffusion\nmodel (e.g. DreamBooth3D [66, 71]) or with image-to-3D\nmethods [29, 42, 44, 45, 64] and then animated with AYG.\n8\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-\nBin Huang, Jiebo Luo, and Xi Yin. Latent-Shift: Latent\nDiffusion with Temporal Shift for Efficient Text-to-Video\nGeneration. arXiv preprint arXiv:2304.08477, 2023. 2, 3,\n15\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer Normalization.\narXiv preprint arXiv:1607.06450,\n2016. 17\n[3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gor-\ndon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey\nTulyakov, Jeong Joon Park, Andrea Tagliasacchi, and\nDavid B. Lindell.\n4D-fy:\nText-to-4D Generation Us-\ning Hybrid Score Distillation Sampling.\narXiv preprint\narXiv:2311.17984, 2023. 16\n[4] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in Time: A Joint Video and Image Encoder for\nEnd-to-End Retrieval. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 2021.\n4, 24\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Ait-\ntala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Kar-\nras, and Ming-Yu Liu.\neDiff-I: Text-to-Image Diffusion\nModels with Ensemble of Expert Denoisers. arXiv preprint\narXiv:2211.01324, 2022. 3, 15\n[6] Miguel \u00b4Angel Bautista, Pengsheng Guo, Samira Abnar,\nWalter Talbott, Alexander T Toshev, Zhuoyuan Chen, Lau-\nrent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht,\nAfshin Dehghan, and Joshua M. Susskind.\nGAUDI: A\nNeural Architect for Immersive 3D Scene Generation.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 15\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and Karsten\nKreis.\nAlign your Latents: High-Resolution Video Syn-\nthesis with Latent Diffusion Models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2, 3, 4, 7, 15, 19, 24, 25\n[8] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang,\nand Juyong Zhang. Neural Surface Reconstruction of Dy-\nnamic Scenes with Monocular RGB-D Camera. In Thirty-\nsixth Conference on Neural Information Processing Sys-\ntems (NeurIPS), 2022. 3, 16\n[9] Ang Cao and Justin Johnson. HexPlane: A Fast Representa-\ntion for Dynamic Scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 3, 15\n[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fanta-\nsia3D: Disentangling Geometry and Appearance for High-\nquality Text-to-3D Content Creation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 2, 3, 15\n[11] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. IT3D: Improved\nText-to-3D Generation with Explicit View Synthesis. arXiv\npreprint arXiv:2308.11473, 2023. 2, 3, 15\n[12] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3D us-\ning Gaussian Splatting. arXiv preprint arXiv:2309.16585,\n2023. 3, 16\n[13] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin\nLee, and Kyoung Mu Lee.\nLucidDreamer:\nDomain-\nfree Generation of 3D Gaussian Splatting Scenes. arXiv\npreprint arXiv:2311.13384, 2023. 16\n[14] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek\nKadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue\nZhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Mot-\nwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ra-\nmanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu:\nEnhancing Image Generation Models Using Photogenic\nNeedles in a Haystack. arXiv preprint arXiv:2309.15807,\n2023. 3, 15\n[15] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-XL: A Universe of 10M+ 3D Objects.\narXiv\npreprint arXiv:2307.05663, 2023. 4\n[16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA Universe of Annotated 3D Objects. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 4\n[17] C. Deng, C. Jiang, C. R. Qi, X. Yan, Y. Zhou, L. Guibas,\nand D. Anguelov. NeRDi: Single-View NeRF Synthesis\nwith Language-Guided Diffusion as General Image Priors.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 3, 15\n[18] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion\nModels Beat GANs on Image Synthesis. In Advances in\nNeural Information Processing Systems, 2021. 3, 15\n[19] Yilun Du, Conor Durkan, Robin Strudel, Joshua B.\nTenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-\nDickstein, Arnaud Doucet, and Will Grathwohl. Reduce,\nReuse, Recycle: Compositional Generation with Energy-\nBased Diffusion Models and MCMC. In Proceedings of the\n40th International Conference on Machine Learning, 2023.\n3, 16, 20\n[20] Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, and\nXiaoli Liu. MetaDreamer: Efficient Text-to-3D Creation\nWith Disentangling Geometry and Texture. arXiv preprint\narXiv:2311.10123, 2023. 15\n[21] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,\nLanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, We-\nichong Yin, Shikun Feng, Yu Sun, Li Chen, Hao Tian,\nHua Wu, and Haifeng Wang.\nERNIE-ViLG 2.0:\nIm-\nproving Text-to-Image Diffusion Model With Knowledge-\nEnhanced Mixture-of-Denoising-Experts. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 3, 15\n9\n[22] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An-\ndrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,\nMing-Yu Liu, and Yogesh Balaji. Preserve Your Own Cor-\nrelation: A Noise Prior for Video Diffusion Models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023. 2, 3, 15\n[23] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah,\nXi Yin, Devi Parikh, and Ishan Misra. Emu Video: Fac-\ntorizing Text-to-Video Generation by Explicit Image Con-\nditioning. arXiv preprint arXiv:2311.10709, 2023. 15\n[24] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind,\nand Navdeep Jaitly. Matryoshka Diffusion Models. arXiv\npreprint arXiv:2310.15111, 2023. 15\n[25] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your\nPersonalized Text-to-Image Diffusion Models without Spe-\ncific Tuning. arXiv preprint arXiv:2307.04725, 2023. 3,\n15\n[26] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 3, 23\n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2020. 3, 15, 21, 22\n[28] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sal-\nimans. Imagen Video: High Definition Video Generation\nwith Diffusion Models. arXiv preprint arXiv:2210.02303,\n2022. 2, 3, 15\n[29] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and\nHao Tan. LRM: Large Reconstruction Model for Single\nImage to 3D. arXiv preprint arXiv:2311.04400, 2023. 8\n[30] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.\nSimple Diffusion: End-to-End Diffusion for High Resolu-\ntion Images. In Proceedings of the 40th International Con-\nference on Machine Learning (ICML), 2023. 15\n[31] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi,\nZheng-Jun Zha, and Lei Zhang.\nDreamTime: An Im-\nproved Optimization Strategy for Text-to-3D Content Cre-\nation. arXiv preprint arXiv:2306.12422, 2023. 2, 3, 15\n[32] A. Jain, B. Mildenhall, J. T. Barron, P. Abbeel, and B.\nPoole.\nZero-Shot Text-Guided Object Generation with\nDream Fields. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2022. 8, 28\n[33] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and\nYao Yao.\nConsistent4D: Consistent 360\u25e6 Dynamic Ob-\nject Generation from Monocular Video.\narXiv preprint\narxiv:2311.02848, 2023. 16\n[34] Nikolai Kalischek, Torben Peters, Jan D. Wegner, and Kon-\nrad Schindler. Tetrahedral Diffusion Models for 3D Shape\nGeneration. arXiv preprint arXiv:2211.13220, 2022. 15\n[35] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski. Noise-Free Score Distillation. arXiv preprint\narXiv:2310.17590, 2023. 15\n[36] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3D Gaussian Splatting for Real-Time\nRadiance Field Rendering. ACM Transactions on Graphics,\n42(4), 2023. 2, 3, 4, 16, 17\n[37] Isaac Kerlow. The Art of 3D Computer Animation and Ef-\nfects. Wiley Publishing, 4th edition, 2009. 16\n[38] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto Henschel,\nZhangyang Wang,\nShant\nNavasardyan, and Humphrey Shi. Text2Video-Zero: Text-\nto-Image Diffusion Models are Zero-Shot Video Genera-\ntors. arXiv preprint arXiv:2303.13439, 2023. 3, 15, 25\n[39] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten\nKreis, Katja Schwarz, Daiqing Li, Robin Rombach, Anto-\nnio Torralba, and Sanja Fidler. NeuralField-LDM: Scene\nGeneration with Hierarchical Latent Diffusion Models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2023. 15\n[40] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang\nXu, and Yingcong Chen. LucidDreamer: Towards High-\nFidelity Text-to-3D Generation via Interval Score Match-\ning. arXiv preprint arXiv:2311.11284, 2023. 15, 16\n[41] Chen-Hsuan\nLin,\nJun\nGao,\nLuming\nTang,\nTowaki\nTakikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-\nResolution Text-to-3D Content Creation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2, 3, 15\n[42] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\nayuan Gu, and Hao Su. One-2-3-45++: Fast Single Image\nto 3D Objects with Consistent Multi-View Generation and\n3D Diffusion. arXiv preprint arXiv:2311.07885, 2023. 3,\n8, 15\n[43] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B. Tenenbaum. Compositional Visual Generation\nwith Composable Diffusion Models. In Computer Vision \u2013\nECCV 2022, 2022. 3, 16, 20\n[44] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot One Image to 3D Object. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 3, 8, 15\n[45] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncDreamer:\nGenerating Multiview-consistent Images from a Single-\nview Image. arXiv preprint arXiv:2309.03453, 2023. 3,\n8, 15\n[46] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ.\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu. MeshD-\niffusion: Score-based Generative 3D Mesh Modeling. In\nInternational Conference on Learning Representations\n(ICLR), 2023. 15\n[47] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, and Wenping Wang.\nWonder3D: Single Image to 3D using Cross-Domain Dif-\nfusion. arXiv preprint arXiv:2310.15008, 2023. 15\n10\n[48] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin,\nMing-Yu Liu, Sanja Fidler, and James Lucas.\nATT3D:\nAmortized Text-to-3D Object Synthesis. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 2023. 3, 15\n[49] Bruce D Lucas and Takeo Kanade. An iterative image reg-\nistration technique with an application to stereo vision. In\nIJCAI\u201981: 7th international joint conference on Artificial\nintelligence, 1981. 24\n[50] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3D Gaussians:\nTracking\nby Persistent Dynamic View Synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 3, 4, 16, 17\n[51] Shitong Luo and Wei Hu.\nDiffusion Probabilistic Mod-\nels for 3D Point Cloud Generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 15\n[52] G. Metzer, E. Richardson, O. Patashnik, R. Giryes, and D.\nCohen-Or. Latent-NeRF for Shape-Guided Generation of\n3D Shapes and Textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2, 3, 15\n[53] Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, and\nSiyu Tang.\nResFields: Residual Neural Fields for Spa-\ntiotemporal Signals.\narXiv preprint arXiv:2309.03160,\n2023. 16\n[54] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\nNeRF: Representing Scenes as Neural Radiance Fields for\nView Synthesis. In ECCV, 2020. 3, 15\n[55] Vinod Nair and Geoffrey E Hinton. Rectified Linear Units\nImprove Restricted Boltzmann Machines. In Proceedings\nof the 27th international conference on machine learning\n(ICML-10), pages 807\u2013814, 2010. 17\n[56] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-E: A System for Gener-\nating 3D Point Clouds from Complex Prompts, 2022. 3,\n15\n[57] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\nDenoising Diffusion Probabilistic Models. In Proceedings\nof the 38th International Conference on Machine Learning\n(ICML), 2021. 3, 15\n[58] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,\nand Anna Rohrbach. Benchmark for Compositional Text-\nto-Image Synthesis. In NeurIPS Datasets and Benchmarks,\n2021. 8, 28\n[59] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla.\nNerfies:\nDeformable Neural Radiance\nFields. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 2021. 2, 3, 4, 16\n[60] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHyperNeRF: A Higher-\nDimensional Representation for Topologically Varying\nNeural Radiance Fields. ACM Trans. Graph., 40(6), 2021.\n3, 16\n[61] Dustin Podell,\nZion English,\nKyle Lacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. SDXL: Improving Latent Diffusion Mod-\nels for High-Resolution Image Synthesis. arXiv preprint\narXiv:2307.01952, 2023. 3, 15\n[62] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion. In The\nEleventh International Conference on Learning Represen-\ntations (ICLR), 2023. 2, 3, 5, 15, 16, 21, 22\n[63] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-NeRF: Neural Radiance\nFields for Dynamic Scenes.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020. 2, 3, 4, 16\n[64] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One Image to High-Quality 3D Object\nGeneration Using Both 2D and 3D Diffusion Priors. arXiv\npreprint arXiv:2306.17843, 2023. 3, 8, 15\n[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning Transferable Vi-\nsual Models From Natural Language Supervision. In Pro-\nceedings of the 38th International Conference on Machine\nLearning (ICML), 2021. 28\n[66] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aber-\nman, Michael Rubenstein, Jonathan Barron, Yuanzhen Li,\nand Varun Jampani. DreamBooth3D: Subject-Driven Text-\nto-3D Generation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2023. 8\n[67] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical Text-Conditional\nImage Generation with CLIP Latents.\narXiv preprint\narXiv:2204.06125, 2022. 3, 15\n[68] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris\nKitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace\nand Pace: Controllable Pedestrian Animation via Guided\nTrajectory Diffusion. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 16\n[69] Geoffrey Roeder, Yuhuai Wu, and David Duvenaud. Stick-\ning the Landing: Simple, Lower-Variance Gradient Estima-\ntors for Variational Inference. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2017. 22\n[70] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-Resolution Image\nSynthesis with Latent Diffusion Models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2, 3, 4, 15, 19, 20, 24, 29\n[71] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 8\n[72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n11\nGhasemipour,\nRaphael Gontijo-Lopes,\nBurcu Karagol\nAyan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mo-\nhammad Norouzi. Photorealistic Text-to-Image Diffusion\nModels with Deep Language Understanding. In Advances\nin Neural Information Processing Systems (NeurIPS),\n2022. 3, 15\n[73] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLAION-5B: An open large-scale dataset for\ntraining next generation image-text models. In Advances in\nNeural Information Processing Systems (NeurIPS), 2022.\n24\n[74] Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler,\nAndreas Geiger, and Karsten Kreis. WildFusion: Learning\n3D-Aware Latent Diffusion Models in View Space. arXiv\npreprint arXiv:2311.13570, 2023. 15\n[75] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua\nLiu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng,\nand Hao Su.\nZero123++:\na Single Image to Consis-\ntent Multi-view Diffusion Base Model.\narXiv preprint\narXiv:2310.15110, 2023. 3, 15\n[76] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. MVDream: Multi-view Diffusion for 3D\nGeneration. arXiv preprint arXiv:2308.16512, 2023. 2, 3,\n4, 15, 19, 20, 24, 25, 29\n[77] J. Shue, E. Chan, R. Po, Z. Ankner, J. Wu, and G. Wetzstein.\n3D Neural Field Generation Using Triplane Diffusion. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2023. 15\n[78] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taig-\nman.\nMake-A-Video: Text-to-Video Generation without\nText-Video Data. In The Eleventh International Conference\non Learning Representations (ICLR), 2023. 2, 3, 8, 15, 16,\n24, 27, 28\n[79] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman.\nText-to-4D Dynamic Scene Generation. In Proceedings of\nthe 40th International Conference on Machine Learning,\n2023. 2, 3, 4, 6, 7, 8, 15, 17, 20, 24, 26, 27, 30, 31, 36\n[80] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep Unsupervised Learning using\nNonequilibrium Thermodynamics. In International Con-\nference on Machine Learning (ICML), 2015. 3, 15\n[81] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nBased Generative Modeling through Stochastic Differential\nEquations. In International Conference on Learning Rep-\nresentations (ICLR), 2021. 3, 15, 21, 22\n[82] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. DreamCraft3D: Hierarchi-\ncal 3D Generation with Bootstrapped Diffusion Prior. arXiv\npreprint arXiv:2310.16818, 2023. 15\n[83] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and\nGang Zeng. DreamGaussian: Generative Gaussian Splat-\nting for Efficient 3D Content Creation.\narXiv preprint\narXiv:2309.16653, 2023. 3, 16, 25\n[84] Edgar Tretschk,\nAyush Tewari,\nVladislav Golyanik,\nMichael Zollh\u00a8ofer,\nChristoph Lassner,\nand Christian\nTheobalt. Non-Rigid Neural Radiance Fields: Reconstruc-\ntion and Novel View Synthesis of a Dynamic Scene From\nMonocular Video. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021. 3,\n16\n[85] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. TextMesh: Gen-\neration of Realistic 3D Meshes From Text Prompts. In In-\nternational conference on 3D vision (3DV), 2024. 2, 3, 15\n[86] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\nGenerative Modeling in Latent Space. In Neural Informa-\ntion Processing Systems (NeurIPS), 2021. 4, 15, 22, 24\n[87] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention Is All You Need. Advances\nin neural information processing systems, 30, 2017. 17\n[88] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lifting\nPretrained 2D Diffusion Models for 3D Generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 3, 15\n[89] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jian-\nmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen,\nFang Wen, Qifeng Chen, and Baining Guo.\nRODIN: A\nGenerative Model for Sculpting 3D Digital Avatars Using\nDiffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023.\n15\n[90] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. VideoFactory: Swap\nAttention in Spatiotemporal Diffusions for Text-to-Video\nGeneration. arXiv preprint arXiv:2305.10874, 2023. 2, 3,\n4, 15, 24\n[91] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si,\nYuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai,\nDahua Lin, Yu Qiao, and Ziwei Liu. LAVIE: High-Quality\nVideo Generation with Cascaded Latent Diffusion Models.\narXiv preprint arXiv:2309.15103, 2023. 2, 3, 15\n[92] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu.\nProlificDreamer: High-\nFidelity and Diverse Text-to-3D Generation with Varia-\ntional Score Distillation. In Thirty-seventh Conference on\nNeural Information Processing Systems (NeurIPS), 2023.\n2, 3, 6, 15, 23\n[93] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xi-\naopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xing-\ngang Wang. 4D Gaussian Splatting for Real-Time Dynamic\nScene Rendering. arXiv preprint arXiv:2310.08528, 2023.\n3, 16\n[94] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\n12\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023. 3, 15, 25\n[95] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao\nFeng, Yin Yang, and Chenfanfu Jiang.\nPhysGaussian:\nPhysics-Integrated 3D Gaussians for Generative Dynamics.\narXiv preprint arXiv:2311.12198, 2023. 16\n[96] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. NeuralLift-360: Lifting An In-the-\nwild 2D Photo to A 3D Object with 360\u00b0 Views. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 3, 15\n[97] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji-\nahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein,\nZexiang Xu, and Kai Zhang. DMV3D: Denoising Multi-\nView Diffusion using 3D Large Reconstruction Model.\narXiv preprint arXiv:2311.09217, 2023. 15\n[98] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,\nZhuofan Zong, Yu Liu, and Ping Luo. RAPHAEL: Text-\nto-Image Generation via Large Mixture of Diffusion Paths.\narXiv preprint arXiv:2305.18295, 2023. 3, 15\n[99] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ra-\nmanan, Andrea Vedaldi, and Hanbyul Joo. BANMo: Build-\ning Animatable 3D Neural Models from Many Casual\nVideos. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022.\n16\n[100] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin. Deformable 3D Gaussians for\nHigh-Fidelity Monocular Dynamic Scene Reconstruction.\narXiv preprint arXiv:2309.13101, 2023. 16\n[101] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang.\nGaussianDreamer: Fast Generation from Text to 3D Gaus-\nsian Splatting with Point Cloud Priors.\narXiv preprint\narxiv:2310.08529, 2023. 3, 16\n[102] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-\nHai Zhang, and Xiaojuan Qi. Text-to-3D with Classifier\nScore Distillation. arXiv preprint arXiv:2310.19415, 2023.\n2, 4, 6, 16, 23, 24\n[103] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan\nKautz. PhysDiff: Physics-Guided Human Motion Diffu-\nsion Model. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2023. 16\n[104] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Go-\njcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLION:\nLatent Point Diffusion Models for 3D Shape Generation.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 3, 15\n[105] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong,\nZhenguo Li, and Gim Hee Lee.\nAnimate124: Animat-\ning One Image to 4D Dynamic Scene.\narXiv preprint\narXiv:2311.14603, 2023. 16\n[106] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Karsten\nKreis, Otmar Hilliges, and Shalini De Mello. A Unified Ap-\nproach for Text- and Image-guided 4D Scene Generation.\narXiv preprint arXiv:2311.16854, 2023. 16\n[107] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. MagicVideo: Efficient Video\nGeneration With Latent Diffusion Models. arXiv preprint\narXiv:2211.11018, 2023. 2, 3, 15\n[108] Linqi Zhou, Yilun Du, and Jiajun Wu. 3D Shape Gener-\nation and Completion Through Point-Voxel Diffusion. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021. 15\n[109] Junzhe Zhu and Peiye Zhuang. HiFA: High-fidelity Text-\nto-3D with Advanced Diffusion Guidance. arXiv preprint\narXiv:2305.18766, 2023. 2, 3, 15\n[110] Wojciech\nZielonka,\nTimur\nBagautdinov,\nShunsuke\nSaito,\nMichael\nZollh\u00a8ofer,\nJustus\nThies,\nand\nJavier\nRomero. Drivable 3D Gaussian Avatars. arXiv preprint\narxiv:2311.08581, 2023. 3, 16\n[111] M. Zwicker, H. Pfister, J. van Baar, and M. Gross. EWA\nvolume splatting. In Proceedings Visualization, 2001. VIS\n\u201901., 2001. 3\n13\nContents\n1. Introduction\n1\n2. Background\n3\n2.1. Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n3. Align Your Gaussians\n4\n3.1. AYG\u2019s 4D Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2. Text-to-4D as Compositional Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.3. AYG\u2019s Score Distillation in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.4. Scaling Align Your Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4. Experiments\n7\n5. Conclusions\n8\nReferences\n8\nA. Supplementary Videos\n15\nB. Related Work\u2014Extended Version\n15\nC. Details of Align Your Gaussians\u2019 4D Representation and Optimization\n16\nC.1. 3D Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nC.2. Deformation Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.3. Frames-Per-Second (fps) Sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.4. Rigidity Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.5. JSD-based Regularization of the Evolving Distribution of the Dynamic 3D Gaussians . . . . . . . . . . . . . .\n17\nC.6. Camera Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.7. Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.8. Rendering Resolution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.9. Additional Fine-tuning and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nD. Align Your Gaussian\u2019s Synthesis Framework\n20\nD.1. AYG\u2019s Compositional Generation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nD.2. AYG\u2019s Score Distillation Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nD.3. AYG\u2019s Parameter Gradients\u2014Putting it All Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nE. Experiment Details\n24\nE.1. Video Diffusion Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nE.2. Text-to-4D Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nE.3. Evaluation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nE.4. User Study Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nF. Additional Quantitative Results\n27\nF.1. Comparisons to MAV3D and R-Precision Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nF.2. Extended Discussion of Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nF.3. View-guidance Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nG. Additional Qualitative Results\u2014More AYG Samples\n31\nG.1. Text-to-4D Samples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.2. Autoregressively Extended and Looping Text-to-4D Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.3. More Comparisons to Make-A-Video3D\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.4. Videos Generated by AYG\u2019s fps-conditioned Video Diffusion Model . . . . . . . . . . . . . . . . . . . . . . .\n31\n14\nA. Supplementary Videos\nFor fully rendered videos, we primarily refer the reader to our project page, https://research.nvidia.com/labs/\ntoronto-ai/AlignYourGaussians/, which shows all our results in best quality.\nMoreover, we include 3 videos in the following google drive folder: https://drive.google.com/drive/\nfolders/1I7e6aj-7BBrIVdePyHEQDGUxLEEls3-e:\n\u2022 ayg text to 4d.mp4: This video contains many text-to-4D generation results, comparisons to MAV3D [79] and au-\ntoregressively extended and looping 4D sequences. Moreover, we show how we compose different 4D sequences into a\nlarge scene as in Fig. 1.\n\u2022 ayg ablation study.mp4: This video contains different generated text-to-4D sequences for our ablation study, com-\nparing our main Align Your Gaussians model with the different modified versions in the ablations. Note that these ablation\nresults only use 4,000 optimization steps in the second dynamic 4D optimization stage for efficiency (see Appendix F.2).\n\u2022 ayg new video model.mp4: This video shows generated 2D video samples for different fps conditionings from our\nnewly trained latent video diffusion model for this project.\nNote that all videos shown on the project page leverage an additional fine-tuning stage with additional optimization steps\ncompared to the results shown in the paper and in ayg text to 4d.mp4. See Appendix C.9 for a discussion on additional\nfine-tuning and optimization.\nB. Related Work\u2014Extended Version\nDiffusion Models. Align Your Gaussians leverages multiple different diffusion models (DMs) [27, 80, 81]. DMs have\nrevolutionized deep generative modeling in the visual domain, most prominently for image synthesis [18, 30, 57, 70, 74].\nThey leverage a forward diffusion process that gradually perturbs input data towards entirely random noise, while a denoiser\nneural network is learnt to reconstruct the input. Specifically, AYG builds on large-scale text-to-image [5, 14, 21, 24, 61, 67,\n70, 72, 98], text-to-video [1, 7, 22, 23, 25, 28, 38, 78, 90, 91, 94, 107] and text-to-multiview-image DMs [42, 44, 45, 47,\n64, 75, 76, 97]. A popular framework for efficient yet expressive generative modeling with DMs is the latent DM approach,\nwhere data is mapped into a compressed latent space with an autoencoder and the DM is trained in a more efficient manner\nin this latent space [70, 86]. The most prominent model of this type is Stable Diffusion [70], and AYG uses exclusively\nlatent DMs which are based on Stable Diffusion. Specifically, apart from Stable Diffusion 2.1, we retrain a VideoLDM [7]\nfor fps-conditioned text-to-video synthesis, which starts from a Stable Diffusion image generator as base model. Moreover,\nAYG uses MVDream [76], a text-guided multiview latent DM similarly fine-tuned from Stable Diffusion.\nText-to-3D Generation with Score Distillation. Text-conditioned image DMs are often trained on large-scale datasets\nconsisting of hundreds of millions or billions of text-image pairs. However, such huge text-annotated datasets are not available\nfor 3D data. While there exists a rich literature on 3D DMs trained on small explicit 3D or multiview image datasets [6, 34,\n39, 46, 51, 56, 77, 89, 104, 108], the lack of large text-annotated 3D training datasets is a challenge for 3D generative\nmodeling. Aiming to overcome these limitations, score distillation methods, introduced in the seminal work by Poole et al.\n[62], use large-scale text-guided 2D diffusion models to distill 3D objects in a per-instance optimization process. A 3D\nscene, parametrized by learnable parameters, is rendered from different camera perspectives and the renderings are given\nto a 2D diffusion model, which can provide gradients to make the renderings look more realistic. These gradients can be\nbackpropagated through the rendering process and used to update the 3D scene representation. This has now become a\nflourishing research direction for 3D generative modeling [10, 11, 17, 20, 31, 35, 40, 41, 48, 52, 82, 85, 88, 92, 96, 109].\nAYG also builds on the score distillation framework.\nText-to-4D Generation. The vast majority of papers on score distillation tackles static 3D object synthesis. To the best\nof our knowledge, there is only one previous paper that leverages score distillation for text-guided generation of dynamic\n4D scenes, Make-A-Video3D (MAV3D) [79]. Hence, this is the most related work to AYG. However, MAV3D uses neural\nradiance fields [54] with HexPlane [9] features as 4D representation, in contrast to AYG\u2019s dynamic 3D Gaussians, and it does\nnot disentangle its 4D representation into a static 3D representation and a deformation field modeling dynamics. MAV3D\u2019s\nrepresentation prevents it from composing multiple 4D objects into large dynamic scenes, which our 3D Gaussian plus\ndeformation field representation easily enables, as we show. Moreover, MAV3D\u2019s sequences are limited in time, while we\nshow a novel autoregressive generation scheme to extend our 4D sequences. AYG outperforms MAV3D qualitatively and\nquantitatively and synthesizes significantly higher-quality 4D scenes. Our novel compositional generation-based approach\ncontributes to this, which MAV3D does not pursue. More specifically, in contrast to MAV3D, our AYG shows how to\nsimultaneously leverage image and video diffusion models for improved synthesis in the 4D generation stage, and moreover\n15\nleverages a 3D-aware multiview image diffusion model for improved 3D generation in the initial stage. Finally, instead of\nregular score distillation sampling [62], used by MAV3D, in practice AYG employs classifier score distillation [102]. Note\nthat MAV3D did not release any code or model checkpoints and its 4D score distillation leverages the large-scale Make-A-\nVideo [78] text-to-video diffusion model, which also is not available publicly.\n3D Gaussian Splatting and Deformation Fields. AYG leverages a 3D Gaussian Splatting-based 4D representation with\ndeformation fields to model dynamics. 3D Gaussian Splatting [36] has been introduced as an efficient 3D scene representation\nand, concurrently with our work, has also been employed for text-to-3D generation by DreamGaussian [83], GSGEN [12],\nGaussianDreamer [101], LucidDreamer [40] and another work also called LucidDreamer [13]. However, these works syn-\nthesize only static 3D scenes, but do not consider dynamics. Deformation fields have been widely used for dynamic 3D scene\nreconstruction [8, 33, 53, 59, 60, 63, 84]. Concurrently with our work, also several papers on dynamic 3D Gaussian Splatting\nemerged [50, 93, 95, 100, 110], similarly tackling the dynamic 3D scene reconstruction task. However, none of these works\naddress generative modeling.\nCompositional Generation. A crucial part of AYG is that it combines multiple diffusion models when performing\nscore distillation. Specifically, in the 4D stage we combine a text-to-image with a text-to-video diffusion model in the\nform of a product distribution between the two and we then leverage the gradients of this product distribution for score\ndistillation. This can be viewed as a form of compositional generation. Usually, in compositional generation, different\ndiffusion models, or the same diffusion model with different text-conditionings, are combined such that multiple conditions\nare fulfilled during generation. Formally, this can be achieved by forming the product distribution of different models, see,\nfor instance, Liu et al. [43] and Du et al. [19]. This has been used for controllable image generation. Analogously, we\nperform a form of compositional generation by composing an image and a video diffusion model to generate dynamic 4D\nassets whose renderings obey both the text-to-image and text-to-video model distributions simultaneously. Instead of aiming\nfor controllability our goal is to simultaneously enforce smooth temporal dynamics (video model) and high individual frame\nquality (image model).\nAnimation and Motion Synthesis. AYG is also broadly related to the rich literature on character animation and motion\nsynthesis with deep generative models; see, for example, the recent works by Rempe et al. [68] and Yuan et al. [103].\nHowever, this line of work usually only considers the generation of joint configurations of human skeletons or similar low-\ndimensional simplified representations. An interesting direction for future work would be to combine these types of models\nwith a method like our AYG. For instance, one could first synthesize a rough motion trajectory or motion path, and AYG\ncould then generate a detailed character animation following this trajectory. There is also work on reconstructing animatable\n3D assets directly from videos [99]. It would be interesting to extend AYG towards extracting assets from the synthesized\n4D scenes which are simulation-ready and fully animatable using standard graphics software. Finally, AYG is also related to\nthe broader literature on computer animation using classical methods without deep learning; see, for instance, Kerlow [37].\nFurther Concurrent Work. Concurrently with us, Dream-in-4D [106] also developed a new text-to-4D synthesis scheme.\nIn contrast to AYG, the work leverages a NeRF-based 4D representation and focuses on image-guided and personalized\ngeneration, instead of using dynamic 3D Gaussians and targeting temporally extended generation as well as the possibility to\neasily compose different 4D assets in large scenes (one of the goals of AYG). Hence, their direction is complementary to ours,\nand it would be interesting to also extend AYG to personalized and image-guided 4D generation. The image-to-4D task is also\ntackled by Animate124 [105], which similarly leverages a dynamic NeRF-based representation. Moreover, 4D-fy [3] also\naddresses text-to-4D synthesis, combining supervision signals from image, video and 3D-aware diffusion models. However,\nlike Dream-in-4D and Animate124, the work leverages a NeRF-based representation with a multi-resolution feature grid in\ncontrast to AYG\u2019s dynamic 3D Gaussians. Moreover, 4D-fy does not explicitly disentangle shape and dynamics into a 3D\ncomponent and a deformation field. Note that, in contrast to AYG, none of the mentioned works demonstrated temporally\nextended generation, the possibility to change the text prompt during synthesis or the ability to easily compose multiple 4D\nassets in large scenes.\nC. Details of Align Your Gaussians\u2019 4D Representation and Optimization\nC.1. 3D Representation\nWe initialize each scene with 1,000 3D Gaussians spread across a sphere with radius 0.3 located on the origin (0, 0, 0). Each\nGaussian is initialized with a random RGB color and spherical harmonics coefficients of 0. As we use isotropic covariances,\nwe only make use of a single scaling parameter per Gaussian, such that Gaussians are spherical. We follow Kerbl et al. [36]\nto initialize the scaling and opacity parameters. Similar to Kerbl et al. [36], we add and delete Gaussians during the initial 3D\noptimization steps to densify regions requiring more detail and prune Gaussians that are not used for rendering. Gaussians\n16\nwith an average magnitude of position gradients above 0.002 are densified, and Gaussians with opacity less than 0.005 are\npruned every 1,000 steps starting from the 500-th step. Additionally, we limit the number of Gaussians such that we only\nperform the densification step if the total number of Gaussians is less than 50,000, and we also reset the opacities to have the\nmaximum value of 0.01 (after sigmoid) every 3,000 steps.\nC.2. Deformation Field\nAfter the 3D Gaussians are optimized into a 3D scene in the first stage, the scene dynamics are modeled by a deformation field\n\u2206\u03a6(x, y, z, \u03c4) = (\u2206x, \u2206y, \u2206z), defined through a multi-layer perceptron (MLP) with parameters \u03a6. We encode (x, y, z, \u03c4)\nwith the sinusoidal positional encoding [87]. We use 4 frequencies (each with sine and cosine components), resulting in a\n32-dimensional input. The MLP with parameters \u03a6 consists of linear layers with ReLU [55] activation functions. We include\nLayer Normalization [2] before the ReLU every second layer, as we found it helped stabilize optimization. The last layer of\n\u03a6 contains a linear layer that is initialized with zero weights, followed by a soft clamping layer (f(x) = tanh(x/0.5)\u22170.5) so\nthat it produces the 3-dimensional output (\u2206x, \u2206y, \u2206z) clamped between (\u22120.5, 0.5). Furthermore, we preserve the initial\n3D Gaussians for the first frame, i.e. \u2206\u03a6(x, y, z, 0) = (0, 0, 0), by multipying the output with \u03be(\u03c4), and the final output is\ngiven as (\u03be(\u03c4)\u2206x, \u03be(\u03c4)\u2206y, \u03be(\u03c4)\u2206z) where \u03be(\u03c4) = \u03c4 0.35 such that \u03be(0) = 0 and \u03be(1) = 1. Given the deformation offsets\n(\u2206x, \u2206y, \u2206z), we can visualize the dynamic 4D scene defined by Gaussians at a given time \u03c4 by running \u03a6 for all Gaussians\nand rendering them using the renderer from Kerbl et al. [36].\nC.3. Frames-Per-Second (fps) Sampling\nAt each 4D optimization step, we sample a fps \u2208 {4, 8, 12} from the distribution p(fps = 4) = 0.81, p(fps = 8) =\n0.14, p(fps = 12) = 0.05. This scheme samples lower fps more (faster video speed) to encourage our optimization process\nto converge to a 4D scene with larger motion. Our time interval ranges from 0 to 1, and we set 0.75 as the length of the time\ninterval covering a 16 frame 4 fps video (i.e. \u03c4 \u2208 [0, 1] and 0.75 corresponds to a 4 seconds video). After an fps value is\nsampled, we sample the starting time \u03c4s \u223c U(0, 1 \u2212 3.0/fps), and calculate the frame times for the 16 frames from \u03c4s and\nfps. This strategy is inspired by Singer et al. [79].\nC.4. Rigidity Regularization\nFollowing Luiten et al. [50], we regularize the deformation field such that nearby Gaussians deform similarly. From the 3D\noptimized scene after the first stage, we pre-calculate 40 nearest neighbor Gaussians for each Gaussian. Then, at each 4D\noptimization step, we reduce the following loss:\nLRigidity-Reg. = 1\n40\n40\nX\ni=1\n||\u2206\u03a6 \u2212 \u2206\u03a6NNi ||2\n2\n(5)\nwhere the term inside the summation denotes the L2 distance between the deformation values of Gaussians and their i-th\nnearest neighbor.\nC.5. JSD-based Regularization of the Evolving Distribution of the Dynamic 3D Gaussians\nHere, we explicitly write out the regularization term introduced in Sec. 3.4, which is based on the Jensen-Shannon divergence\nand used to regularize the evolving distribution of the dynamic 3D Gaussians. It is used during the optimization of the\ndeformation field when generating the temporal dynamics of the 4D sequences. The idea is to regularize the mean and the\ndiagonal covariance of the distribution of the 3D Gaussians to stay approximately constant during the optimization of the\ntemporal dynamics. The mean and the diagonal covariance capture an object\u2019s position (mean of the distribution of the\nGaussians) and its size (variances of the distribution of the Gaussians in each xyz-direction, i.e., diagonal entries of the\ncovariance matrix). Hence, keeping the mean and the diagonal covariance approximately constant ensures that the temporal\ndynamics of the 4D assets induced by the video model do not simply move the objects around or change them in size, but\ninstead produce more complex and meaningful deformations and motions, while maintaining object size and position. The\ntechnique stabilizes the optimization of the deformation field and helps ensuring that the learnt motion is realistic. Note that\nwe also do not necessarily want the mean and covariances to be exactly the same across time, because some motion may nat-\nurally require them to vary (e.g. a running horse would potentially be more realistic with an oscillating center of mass), hence\nwe use a \u201csoft\u201d regularization strategy instead of, for instance, a hard re-centering and re-scaling during the optimization.\nSince we only want to regularize the mean and the variances in each xyz-direction, we can model the distribution of\nthe 3D Gaussians at time \u03c4 by a Gaussian distribution N(\u03bd\u03c4, \u0393\u03c4). We calculate the means \u03bd\u03c4 and diagonal covariances\n17\n\u0393\u03c4 of the entire set of 3D Gaussians (using their means \u00b5i) at times \u03c4 along the 4D sequence (see Fig. 4). As explained\nabove, we would like to ensure that this distribution stays approximately the same during the optimization of the deformation\nfield. To this end, we choose the Jensen\u2013Shannon Divergence (JSD) as similarity metric measuring the distance between the\ndistributions at different times \u03c4. The JSD between N(\u03bd0, \u03930) at time 0 and N(\u03bd\u03c4, \u0393\u03c4) at time \u03c4 is\nJSD (N(\u03bd0, \u03930)||N(\u03bd\u03c4, \u0393\u03c4)) = 1\n2KL\n\u0012\nN(\u03bd0, \u03930)\n\f\f\f\f\n\f\f\f\f\n1\n2 (N(\u03bd0, \u03930) + N(\u03bd\u03c4, \u0393\u03c4))\n\u0013\n+ 1\n2KL\n\u0012\nN(\u03bd\u03c4, \u0393\u03c4)\n\f\f\f\f\n\f\f\f\f\n1\n2 (N(\u03bd0, \u03930) + N(\u03bd\u03c4, \u0393\u03c4))\n\u0013\n.\n(6)\nUnfortunately, the mixture distribution on the right hand side of the KL terms is generally not Gaussian. Therefore, it is gener-\nally not possible to derive a closed-form expression for the JSD between two Gaussians. However, we can make a simplifying\nmodification here. Instead of calculating the mixture distribution we average the means and covariances from the two dis-\ntributions N(\u03bd0, \u03930) and N(\u03bd\u03c4, \u0393\u03c4) and construct a corresponding Gaussian distribution N\n\u0000 1\n2 (\u03bd0 + \u03bd\u03c4) , 1\n2 (\u03930 + \u0393\u03c4)\n\u0001\n,\nwhich we use instead of the mixture distribution. Hence, we have\nJSD (N(\u03bd0, \u03930)||N(\u03bd\u03c4, \u0393\u03c4)) \u2192 LJSD-Reg. := 1\n2KL\n\u0012\nN(\u03bd0, \u03930)\n\f\f\f\f\n\f\f\f\fN\n\u00121\n2 (\u03bd0 + \u03bd\u03c4) , 1\n2 (\u03930 + \u0393\u03c4)\n\u0013\u0013\n+ 1\n2KL\n\u0012\nN(\u03bd\u03c4, \u0393\u03c4)\n\f\f\f\f\n\f\f\f\fN\n\u00121\n2 (\u03bd0 + \u03bd\u03c4) , 1\n2 (\u03930 + \u0393\u03c4)\n\u0013\u0013\n,\n(7)\nwhich serves as our novel JSD-based regularization term LJSD-Reg. to regularize the evolving distribution of the dynamic 3D\nGaussians. Specifically, the deformation field is regularized with LJSD-Reg. such that the mean and the variances in xyz-\ndirection of the distribution of the 3D Gaussians at time \u03c4 do not deviate significantly from the corresponding mean and\nvariances at time 0 (recall that the initial Gaussians at \u03c4 = 0 are not subject to any deformation and hence fixed).\nWe can write out the above as\nLJSD-Reg. := 1\n2KL\n\u0012\nN(\u03bd0, \u03930)\n\f\f\f\f\n\f\f\f\fN\n\u00121\n2 (\u03bd0 + \u03bd\u03c4) , 1\n2 (\u03930 + \u0393\u03c4)\n\u0013\u0013\n+ 1\n2KL\n\u0012\nN(\u03bd\u03c4, \u0393\u03c4)\n\f\f\f\f\n\f\f\f\fN\n\u00121\n2 (\u03bd0 + \u03bd\u03c4) , 1\n2 (\u03930 + \u0393\u03c4)\n\u0013\u0013\n=\nX\ni\u2208{x,y,z}\n\u0014\n\u22121\n2 log [2] + 1\n2 log\n\u0002\n\u0393i\n0 + \u0393i\n\u03c4\n\u0003\n\u2212 1\n4 log\n\u0002\n\u0393i\n0\n\u0003\n\u2212 1\n4 log\n\u0002\n\u0393i\n\u03c4\n\u0003\n+ 1\n4\n(\u03bdi\n\u03c4 \u2212 \u03bdi\n0)2\n\u0393i\n0 + \u0393i\u03c4\n\u0015\n,\n(8)\nfor the three dimensions i \u2208 {x, y, z} and where \u0393i denotes the i-th diagonal entry in the diagonal covariance matrix \u0393 (since\nwe use diagonal covariance matrices, we obtain this simple factorized form). The gradients with respect to \u03bd\u03c4 and \u0393\u03c4, which\nare calculated from the learnt distribution of the evolving 3D Gaussians, are\n\u2207\u03bdi\u03c4 LJSD-Reg. = 1\n2\n\u03bdi\n\u03c4 \u2212 \u03bdi\n0\n\u0393i\n0 + \u0393i\u03c4\n,\n(9)\n\u2207\u0393i\u03c4 LJSD-Reg. = 1\n2\n1\n\u0393i\n0 + \u0393i\u03c4\n\u2212 1\n4\n1\n\u0393i\u03c4\n\u2212 1\n4\n(\u03bdi\n\u03c4 \u2212 \u03bdi\n0)2\n(\u0393i\n0 + \u0393i\u03c4)2 .\n(10)\nBy analyzing the first and second derivatives it is easy to see that LJSD-Reg. has a unique minimum when\n\u03bdi\n\u03c4 = \u03bdi\n0\n(11)\nand\n\u0393i\n\u03c4 = 1\n2\n\u0014\n(\u03bdi\n\u03c4 \u2212 \u03bdi\n0)2 +\nq\n(\u03bdi\u03c4 \u2212 \u03bdi\n0)4 + 4\u0393i\n0\n2\n\u0015\n\u03bdi\n\u03c4 =\u03bdi\n= 0\n\u0393i\n0\n(12)\nfor all i \u2208 {x, y, z}. This implies that LJSD-Reg. is a meaningful regularization term for the means and variances in xyz-\ndirection of the distribution of the evolving dynamic 3D Gaussians. It ensures that the distribution\u2019s mean \u03bd\u03c4 and diagonal\ncovariance \u0393\u03c4 remain close to the initial \u03bd0 and \u03930 during the optimization of the deformation field. That said, note that\n18\nLJSD-Reg. is not necessarily an accurate approximation of the true JSD. However, we found that this approach worked very\nwell in practice during score distillation.\nWe opted for the JSD-based approach, because we wanted to use a symmetric distribution similarity metric. Potentially,\nwe could have formulated our regularization also based on a Wasserstein distance, symmetrized KL divergence or regular\nnon-symmetric KL divergence. However, we implemented the JSD-based approach above first and it worked right away and\nsignificantly improved AYG\u2019s optimization behavior and the 4D results. Therefore, we decided to leave the exploration of\ndifferently defined regularizations for the evolving distribution of the dynamic 3D Gaussians to future research.\nC.6. Camera Distribution\nIn the initial 3D stage, we follow MVDream [76] to sample random cameras. At each optimization step, we sample a field-\nof-view fov \u223c U(15, 60), an elevation angle elv \u223c U(10, 45) and an azimuth angle azm \u223c U(0, 360). The camera distance\nis calculated as cam d = s/ tan(fov\n2\n\u2217\n\u03c0\n180) where s \u223c U(0.8, 1.0) is a random scaling factor. The sampled camera looks\ntowards the origin (0,0,0) and the images are rendered at 256 \u00d7 256 resolution.\nAt each 4D optimization step, we sample fov \u223c U(40, 70), elv \u223c U(\u221210, 45), azm \u223c U(0, 360), and cam d \u223c\nU(1.5, 3.0). We use dynamic cameras where the camera location changes across the temporal time domain. For this purpose,\nwe additionally sample offset values elv offset \u223c U(\u221213.5, 30) and azm offset \u223c U(\u221245, 45) and construct 16 cameras\n(to be used for rendering, and consequently, to be used as input to the video diffusion model that expects a 16-frame input)\nby setting elvi = elv + elv offset \u2217 i/15 and azmi = azm + azm offset \u2217 i/15 for i \u2208 {0, ..., 15}, where elvi and\nazmi corresponds to the elevation and azimuth angles of the i-th frame\u2019s camera. We use the same field-of-view and camera\ndistance across the 16 frames. We render frames at 160 \u00d7 256 resolution, which conforms to the aspect ratio that the video\ndiffusion model is trained on.\nC.7. Diffusion Models\nAs discussed, AYG leverages a latent text-to-image diffusion model [70], a latent text-to-video diffusion model [7], and a\nlatent text-to-multiview-image diffusion model [76] in its compositional 4D score distillation framework.\nFor the latent text-to-multiview-image diffusion model we use MVDream [76].3 For the latent text-to-video diffusion\nmodel, we train our own model, largely following VideoLDM [7] but with more training data and additional fps conditioning,\nsee Appendix E.1. For the latent text-to-image diffusion model we use the spatial backbone of our newly trained video\ndiffusion model, removing its temporal layers. This spatial backbone corresponds to a version of Stable Diffusion 2.1 [70]4\nthat was fine-tuned for generation at resolution 320 \u00d7 512 (see details in Blattmann et al. [7]).\nC.8. Rendering Resolution\nIn the initial 3D synthesis stage, we render the scenes at resolution 256 \u00d7 256. This is the resolution at which MVDream\noperates, i.e., its encoder takes images of this resolution. We additionally perform bilinear rescaling to 320\u00d7512 to calculate\nour text-to-image model\u2019s score distillation gradient. The text-to-image model\u2019s encoder accepts images of this resolution\nafter fine-tuning (see previous subsection). In the main 4D synthesis stage, we render at resolution 256 \u00d7 160 and perform\nbilinear rescaling to 320 \u00d7 512 to calculate both the text-to-image model\u2019s and the text-to-video model\u2019s score distillation\ngradients. They share the same encoder, which was fine-tuned to operate at this resolution. For evaluation and visualization,\nwe render at 512 \u00d7 320 resolution and pad to 512 \u00d7 512 if necessary.\nC.9. Additional Fine-tuning and Optimization\nWe explored adding more Gaussians to further improve the quality of the generated 4D scenes. As mentioned in Ap-\npendix C.1, we only perform densification steps if the total number of Gaussians is less than 50,000 for the initial 3D\nsynthesis stage, and then optimize the deformation field in our 4D synthesis stage, as we found that this strategy gave us the\nbest motion. Once optimized, we can further fine-tune the dynamic 4D scenes by doing a similar two-stage optimization. In\nthe first additional stage, we continue optimizing the 3D Gaussians (not the deformation field) and do further densification\nsteps even when the number of Gaussians is more than 50,000. On average, we end up with 150,000 Gaussians in this stage.\nAfter that, in the second fine-tuning stage, we continue optimizing the deformation field, which has already been optimized\nwell previously, with all the Gaussians including the newly added ones. We use the same hyperparameters as in the initial\n3D/4D stages except that we reduce the learning rates by 1/5 and increase the rendering image resolution to 512\u00d7512 for\n3https://huggingface.co/MVDream/MVDream\n4https://huggingface.co/stabilityai/stable-diffusion-2-1\n19\nthe additional 3D optimization stage. For the two additional stages, we optimize for 7,000 steps and 3,000 steps on average,\nrespectively. Also see the hyperparameters in Appendix E.2.\nAll results shown on the project page,\nhttps : / / research . nvidia . com / labs / toronto - ai /\nAlignYourGaussians/, correspond to 4D assets subject to this additional fine-tuning. All results shown in the pa-\nper itself as well as in the supplementary video ayg text to 4d.mp4 only use a single 3D plus 4D optimization run\nwithout this additional fine-tuning. Also the quantitative and qualitative comparisons to MAV3D [79] in Sec. 4 and Ap-\npendix F.1 only use a single 3D and 4D optimization. This implies that with the additional fine-tuning described here and\nshown on the project page, we would possibly outperform MAV3D by an even larger margin in the user study. Also the\nablation studies were carried out with only a single 3D plus 4D optimization without fine-tuning, and in that case we addi-\ntionally only optimized for a total of 4,000 steps in the second 4D optimization stage in the interest of efficiency (see Sec. 4\nand Appendix F.2).\nD. Align Your Gaussian\u2019s Synthesis Framework\nHere, we discuss AYG\u2019s compositional generation framework in more detail and derive AYG\u2019s score distillation scheme.\nD.1. AYG\u2019s Compositional Generation Framework\nOur goal during AYG\u2019s main 4D distillation is to simultaneously synthesize smooth and realistic temporal dynamics while\nalso maintaining a high individual frame quality. Formally, this means that we would like to fulfill two objectives at once:\n1. A set of 2D frames {xci\n\u03c4i}F rendered from the 4D sequence at consecutive time steps \u03c4i and smoothly changing camera\npositions ci (dynamic cameras) should form a realistic dynamic video and should be high probability under the text-to-\nvideo model pvid({zci\n\u03c4i}F ).\n2. Individual frames \u02dcx\u02dccj\n\u02dc\u03c4j at possibly different time steps \u02dc\u03c4j and camera positions \u02dccj should, each individually, be high\nprobability under the text-to-image model pim(\u02dcx\u02dccj\n\u02dc\u03c4j).\nIf we want to ensure that both criteria are fulfilled simultaneously, it means that we are interested in generating 4D sequences\nwhose renderings obey the product distribution of pvid and pim, i.e., pvid pim. This exactly corresponds to a form of composi-\ntional generation [19, 43]. In compositional generation, one often combines different diffusion models, or the same diffusion\nmodel with different text-conditionings, such that multiple conditions are fulfilled during generation, usually with the goal\nof controllable image generation or image editing. In that case, one leverages the product distribution of the corresponding\ndiffusion model distributions with the different conditionings. Conceptually, this is analogous to what we are doing in AYG,\njust that we are doing a version of compositional generation by composing an image and a video diffusion model to generate\ndynamic 4D assets whose renderings obey both the text-to-image and text-to-video model distributions simultaneously. In-\nstead of aiming for controllability our goal is to simultaneously enforce smooth temporal dynamics (video model) and high\nindividual frame quality (image model).\nOne may ask, why do we even need the additional text-to-image model? Should the video model itself not also enforce\nhigh individual frame quality? The number of text-video pairs used to train text-to-video models is typically significantly\nsmaller than the huge text-image datasets utilized when training text-to-image models. This makes text-to-video models,\nincluding the one we trained, often slightly less robust and more unstable than text-to-image models and they yield noisier\ngradients in the score distillation setting. Generally, video diffusion models are not as mature as image diffusion models yet.\nThis motivates us to compose both, the image and the video model, where the video model can synthesize temporal dynamics\nand the image model ensures that high individual frame quality is maintained despite potentially somewhat noisy gradients\nof the video model. Looking at it from another perspective, the image model essentially stabilizes the optimization process.\nWe have explained this for the 4D generation stage here, but something very similar occurs also in AYG\u2019s initial 3D\ngeneration stage, where a text-conditioned multiview-image diffusion model [76] is composed with a regular large-scale\ntext-to-image model [70] in the form of their product distribution for score distillation. The motivation is almost the same.\nThe multiview image model is able to generate a set of multiview-consistent images, similar to the set of consecutive frames\nof the video model, but it was trained only on a limited 3D dataset and therefore does encode the biases of this small 3D\ndataset. Including an additional general text-to-image prior can stabilize optimization and boost performance. Moreover, by\naugmenting the image model\u2019s text conditioning with view prompts, we can easily break the symmetry inherent in the multi-\nview diffusion model, which does not generate objects in a canonical orientation. Note, however, that our main contribution\nis the compositional generation approach for AYG\u2019s 4D stage and this has been our main focus in this paper. AYG\u2019s 4D stage\ncould also be combined with other methods to generate the initial static 3D assets.\n20\nD.2. AYG\u2019s Score Distillation Scheme\nLet us now formalize this, while focusing on the 4D stage (derivations for the 3D stage proceed analogously). Following the\nabove motivation, when optmizing the deformation field \u03a6 through score distillation we seek to minimize a reverse Kullback-\nLeibler divergence (KLD) from q\u03a6, the distribution over the 4D scene\u2019s renderings defined through the deformation field \u03a6, to\nthe product distribution pvid pim of the diffusion models used as teachers, i.e., KL\n\u0000q\u03a6\n\f\f\f\fpvidpim\n\u0001\n. Since pvid and pim are defined\nthrough diffusion models, in practice we minimize the KL not just for the clean renderings, but for diffused renderings at all\ndiffusion time steps t, i.e.,\nKL\n\u0012\nq\u03a6\n\u0010\n{zci\n\u03c4i}F , {\u02dcz\u02dccj\n\u02dc\u03c4j}M\n\u0011 \f\f\f\f\n\f\f\f\fp\u03b3\nvid\n\u0000{zci\n\u03c4i}F\n\u0001 M\nY\nj=1\np\u03ba\nim\n\u0010\n\u02dcz\u02dccj\n\u02dc\u03c4j\n\u0011\u0013\n,\n(13)\nwhich is exactly the equation from the main paper in Sec. 3.2. Here, we now added some details: In general we can always\nconsider a set of F + M rendered frames, where {zci\n\u03c4i}F are the F consecutive frames given to the video model and {\u02dcz\u02dccj\n\u02dc\u03c4j}M\nare another M independent frames given to the image model (since these are independent, the image model operates on them\nindependently; hence the product in the equation). Following the notation in the main paper, z denotes diffused renderings\nand we omit diffusion time subscripts t. Note that, as discussed in the main paper, in practice the M frames given to the\nimage model are just a subset of the F frames given to the video model. However, the framework is general. Moreover, \u03b3 and\n\u03ba are temperatures which scale the width of the distributions pvid and pim. They can act as hyperparameters controlling the\nrelative strength of the gradients coming from the video and image model, respectively. Note that for now we omit explicitly\nindicating the text conditioning in the equations in the interest of brevity.\nThe derivation of our score distillation framework through minimizing the reverse KLD between the rendering distribution\nand the teacher distribution follows Poole et al. [62] (Appendix A.4). In practice, one typically gives different weights to the\nKL minimization for different diffusion times t, and the full SDS gradient can be written as\n\u2207\u03a6LAYG\nSDS ({x} = g(\u03a6)) = Et,\u03f5\n\u0014\nw(t) \u03c3t\n\u03b1t\n\u2207\u03a6KL\n\u0012\nq\u03a6\n\u0010\n{zci\n\u03c4i}F , {\u02dcz\u02dccj\n\u02dc\u03c4j}M\n\u0011 \f\f\f\f\n\f\f\f\fp\u03b3\nvid\n\u0000{zci\n\u03c4i}F\n\u0001 M\nY\nj=1\np\u03ba\nim\n\u0010\n\u02dcz\u02dccj\n\u02dc\u03c4j\n\u0011\u0013\u0015\n,\n(14)\nwhere w(t) is a weighting function and \u03b1t and \u03c3t are the diffusion models\u2019 noise schedule (note that all our diffusion models\nuse the same noise schedules). Specifically, the diffused renderings z are given as z = \u03b1tx + \u03c3t\u03f5, where x is a clean\nrendering, \u03f5 \u223c N(0, I) is noise from a standard normal distribution, and the parameters \u03b1t and \u03c3t are defined such the\nsignal-to-noise ratio \u03b1t/\u03c3t is strictly decreasing as a function of the diffusion time t. Moreover, usually \u03b1t \u2208 [0, 1] and\n\u03c3t \u2208 [0, 1] and in our case we use a \u201cvariance-preserving\u201d noise schdule [27, 81] such that \u03b12\nt + \u03c32\nt = 1. Furthermore, in\nEq. (14) above, g denotes the differentiable rendering function of the 4D scene, which produces all clean renderings {x}F +M\n(omitting camera and 4D sequence time sub- and superscripts to keep notation concise), which are then diffused into {zci\n\u03c4i}F\nand {\u02dcz\u02dccj\n\u02dc\u03c4j}M.\nWe can now expand the KL divergence, similar to Poole et al. [62]:\n\u2207\u03a6LAYG\nSDS ({x}F +M = g(\u03a6)) = Et,\u03f5\n\u0014\nw(t) \u03c3t\n\u03b1t\n\u0012\n\u2207\u03a6 log q\u03a6\n\u0010\n{zci\n\u03c4i}F , {\u02dcz\u02dccj\n\u02dc\u03c4j}M\n\u0011\n\u2212 \u2207\u03a6 log\n\u0014\np\u03b3\nvid\n\u0000{zci\n\u03c4i}F\n\u0001 M\nY\nj=1\np\u03ba\nim\n\u0010\n\u02dcz\u02dccj\n\u02dc\u03c4j\n\u0011\u0015\u0013\u0015\n.\n(15)\nLet us now introduce some abbreviations for brevity, as in the main paper, and define Z := {zci\n\u03c4i}F , \u02dcZ := {\u02dcz\u02dccj\n\u02dc\u03c4j}M and\nX := {x}F +M. Moreover, for a concise notation we write the product over the image distributions as one distribution over\nall the different diffused renderings with a slight abuse of notation, i.e.,\nM\nY\nj=1\np\u03ba\nim\n\u0010\n\u02dcz\u02dccj\n\u02dc\u03c4j\n\u0011\n= p\u03ba\nim\n\u0010\n{\u02dcz\u02dccj\n\u02dc\u03c4j}M\n\u0011\n= p\u03ba\nim\n\u0010\n\u02dcZ\n\u0011\n.\n(16)\nWe can keep in mind that p\u03ba\nim\n\u0010\n\u02dcZ\n\u0011\ndecomposes into a product, which will lead to different independent gradients. Now we\n21\nhave\n\u2207\u03a6LAYG\nSDS (X = g(\u03a6)) = Et,\u03f5\n\u0014\nw(t) \u03c3t\n\u03b1t\n\u0012\n\u2207\u03a6 log q\u03a6\n\u0010\nZ, \u02dcZ\n\u0011\n\u2212 \u2207\u03a6 log\n\u0014\np\u03b3\nvid (Z) p\u03ba\nim\n\u0010\n\u02dcZ\n\u0011\u0015\u0013\u0015\n= Et,\u03f5\n\u0014\nw(t) \u03c3t\n\u03b1t\n\u0012\n\u2207\u03a6 log q\u03a6 (Z) \u2212 \u03b3 \u2207\u03a6 log pvid (Z)\n+ \u2207\u03a6 log q\u03a6\n\u0010\n\u02dcZ\n\u0011\n\u2212 \u03ba \u2207\u03a6 log pim\n\u0010\n\u02dcZ\n\u0011\u0013\u0015\n,\n(17)\nwhere we decomposed the log-terms. We can now analyze the individual terms, analogous to Poole et al. [62]. We have\n\u2207\u03a6 log pvid (Z) = \u2207Z log pvid (Z) \u2202Z\n\u2202\u03a6 = \u2212 1\n\u03c3t\n\u02c6\u03f5vid(Z, v, t) \u2202Z\n\u2202\u03a6 = \u2212\u03b1t\n\u03c3t\n\u02c6\u03f5vid(Z, v, t)\u2202X\n\u2202\u03a6,\n(18)\nwhere we inserted the text-to-video model\u2019s denoiser neural network \u02c6\u03f5vid(Z, v, t), which represents the diffusion model\u2019s\nscore \u2207Z log pvid (Z) as [27, 81, 86]\n\u2207Z log pvid (Z) \u223c \u2212 1\n\u03c3t\n\u02c6\u03f5vid(Z, v, t),\n(19)\nand we now also explicitly indicate the model\u2019s conditioning on the diffusion time t and a text prompt v. The image model\nterm \u2207\u03a6 log pim\n\u0010\n\u02dcZ\n\u0011\ncan be written similarly. Moreover, we have\n\u2207\u03a6 log q\u03a6 (Z) =\n\u0012\u2202 log q\u03a6 (Z)\n\u2202X\n+ \u2202 log q\u03a6 (Z)\n\u2202Z\n\u2202Z\n\u2202X\n\u0013 \u2202X\n\u2202\u03a6\n=\n\u0012\n\u2212 \u2202\n\u2202X(\u03b1tX \u2212 Z)2 \u2212 \u03b1t\n\u2202\n\u2202Z(\u03b1tX \u2212 Z)2\n\u0013\n1\n2\u03c32\nt\n\u2202X\n\u2202\u03a6\n= (\u2212(\u03b1tX \u2212 Z) + (\u03b1tX \u2212 Z)) \u03b1t\n\u03c32\nt\n\u2202X\n\u2202\u03a6\n= (\u2212(\u03b1tX \u2212 \u03b1tX \u2212 \u03c3t\u03f5) + (\u03b1tX \u2212 \u03b1tX \u2212 \u03c3t\u03f5)) \u03b1t\n\u03c32\nt\n\u2202X\n\u2202\u03a6\n=\n\u0012\u03b1t\n\u03c3t\n\u03f5 \u2212 \u03b1t\n\u03c3t\n\u03f5\n\u0013 \u2202X\n\u2202\u03a6\n= 0\n(20)\nwhich follows exactly Poole et al. [62], Appendix A.4, and analogously for \u2207\u03a6 log q\u03a6\n\u0010\n\u02dcZ\n\u0011\n. The first term in Eq. (20)\ncorresponds to the gradient directly with respect to the variational parameters \u03a6 through the renderings X, while the second\nterm is the path derivative through the diffused samples Z [69]. Note that in score distillation, we take an expectation over \u03f5\nand after such expectation both terms are individually zero as the noise \u03f5 has 0 mean.\nWe can now write our SDS gradient as\n\u2207\u03a6LAYG\nSDS (X = g(\u03a6)) = Et,\u03f5\n\u0014\nw(t)\n\u0012\n\u03b3 \u02c6\u03f5vid(Z, v, t) + \u03ba \u02c6\u03f5im(\u02dcZ, v, t)\n\u0013\u0015\u2202X\n\u2202\u03a6.\n(21)\nHowever, in SDS one typically includes the path derivative gradient (the second term in Eq. (20)) as a zero mean control\nvariate to reduce the variance of the SDS gradient when using a sample-based approximation of the expectation, following\nRoeder et al. [69]. In other words, since\nEt,\u03f5\n\u0014\n\u2212w(t)\u03b1t\n\u03c3t\n\u03f5\n\u0015\u2202X\n\u2202\u03a6 = 0\n(22)\nafter the expectation of the diffusion noise \u03f5, we can freely include such terms in the SDS gradient as control variates and\nscale with the temperatures \u03b3 and \u03ba as needed. Now explicitly defining the different noise values \u03f5vid and \u03f5im as the diffusion\nnoises for the perturbed renderings given to the video and the image model, respectively, we can write\n\u2207\u03a6LAYG\nSDS (X = g(\u03a6)) = Et,\u03f5\n\u0014\nw(t)\n\u0012\n\u03b3 \u02c6\u03f5vid(Z, v, t) + \u03ba \u02c6\u03f5im(\u02dcZ, v, t)\n\u0013\u0015\u2202X\n\u2202\u03a6\n= Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u0012\n\u03b3 (\u02c6\u03f5vid(Z, v, t) \u2212 \u03f5vid) + \u03ba (\u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u03f5im)\n\u0013\u0015\u2202X\n\u2202\u03a6.\n(23)\n22\nNote that we have been using different noise values to perturb the different renderings from the very beginning of the deriva-\ntion, but we have not been explicit about it in the notation so far in the interest of conciseness.\nIn practice, one typically employs classifier-free guidance (CFG) [26] with guidance weights \u03c9vid/im for the video and\nimage model, respectively. Hence, we have that\n\u02c6\u03f5vid(Z, v, t) \u2192 \u02c6\u03f5vid(Z, v, t) + \u03c9vid\n\u0002\u02c6\u03f5vid(Z, v, t) \u2212 \u02c6\u03f5vid(Z, t)\n\u0003\n(24)\nfor the video model, and for the image model analogously (\u02c6\u03f5vid(Z, t) indicates the denoiser prediction without text condi-\ntioning). The no-CFG setting is recovered for \u03c9vid/im = 0. Inserting above, we obtain\n\u2207\u03a6LAYG\nSDS (X = g(\u03a6)) = Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03b3\n\u0012\n\u03c9vid\n\u0002\u02c6\u03f5vid(Z, v, t) \u2212 \u02c6\u03f5vid(Z, t)\n\u0003\n+ \u02c6\u03f5vid(Z, v, t) \u2212 \u03f5vid\n|\n{z\n}\n\u03b4vid\ngen\n\u0013\n+ \u03ba\n\u0012\n\u03c9im\n\u0002\u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u02c6\u03f5im(\u02dcZ, t)\n\u0003\n+ \u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u03f5im\n|\n{z\n}\n\u03b4im\ngen\n\u0013\u001b\u2202X\n\u2202\u03a6\n\u0015\n,\n(25)\nwhich is exactly Eq. (3) from the main paper.\nAs discussed in the main paper, recently, ProlificDreamer [92] proposed a scheme where the noise-based control variate\n\u03f5 (\u03f5vid and \u03f5im here) is replaced by a separate diffusion model that models the rendering distribution. More specifically,\nProlificDreamer initializes this separate diffusion model from the diffusion model guiding the synthesis (\u02c6\u03f5vid(Z, v, t) and\n\u02c6\u03f5im(\u02dcZ, v, t) here), and then slowly fine-tunes on the diffused renderings (Z or \u02dcZ here). In our setting, one would initialize\nfrom the video and the image model, respectively, in the two terms. This means that, at the beginning of optimization, the\nterms \u03b4vid/im\ngen\nin Eq. (25) would be zero. Inspired by this observation and aiming to avoid ProlificDreamer\u2019s cumbersome\nfine-tuning, we instead propose to simply set \u03b4vid/im\ngen\n= 0 entirely, and simply optimize with\n\u2207\u03a6LAYG\nCSD(X = g(\u03a6)) = Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03c9vid\n\u0002\u02c6\u03f5vid(Z, v, t) \u2212 \u02c6\u03f5vid(Z, t)\n|\n{z\n}\n\u03b4vid\ncls\n\u0003\n+ \u03c9im\n\u0002\u02c6\u03f5im(\u02dcZ, v, t) \u2212 \u02c6\u03f5im(\u02dcZ, t)\n|\n{z\n}\n\u03b4im\ncls\n\u0003\u001b\u2202X\n\u2202\u03a6\n\u0015\n= Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03c9vid\u03b4vid\ncls + \u03c9im\u03b4im\ncls\n\u001b\u2202X\n\u2202\u03a6\n\u0015\n,\n(26)\nwhich is exactly Eq. (4) from the main paper. Moreover, we have absorbed \u03b3 and \u03ba into \u03c9vid/im. Interestingly, this exactly\ncorresponds to the concurrently proposed classifier score distillation (CSD) [102], which points out that the above two terms\n\u03b4vid/im\ncls\ncorrespond to implicit classifiers predicting the text v from the video or images, respectively. CSD then proposes to\nuse only \u03b4vid/im\ncls\nfor score distillation, resulting in improved performance over SDS. We discovered that scheme independently,\nwhile aiming to inherit ProlificDreamer\u2019s strong performance.\nThe score for the first 3D stage looks exactly analogous, just that instead of the composition of the video and image\ndiffusion model, we have the composition of the MVDream multiview image diffusion model and the regular text-to-image\nmodel.\nD.3. AYG\u2019s Parameter Gradients\u2014Putting it All Together\nIn practice, AYG additionally uses negative prompt guidance, motion amplification, JSD regularization, and rigidity regular-\nization in the 4D stage, as well as view guidance in the initial 3D stage, see Sec. 3.4. Let us now put everything together.\nStage 1: 3D Synthesis. In the 3D stage, the entire gradient backpropagated into the 3D Gaussian splatting representation\n\u03b8 is\n\u2207\u03b8LAYG\n3D-stage = Et,\u03f53D,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03c93D\u03b43D\ncls + \u03c9im\u03b4im\ncls\n+ \u03c9vg\n\u0010\n\u02c6\u03f5im(\u02dcZ, vaug, t) \u2212 \u02c6\u03f5im(\u02dcZ, v, t)\n\u0011\n+ \u03c9neg\n\u0000\u02c6\u03f53D(Z, t) \u2212 \u02c6\u03f53D(Z, vneg, t)\n\u0001\u001b\u2202X\n\u2202\u03b8\n\u0015\n,\n(27)\nwhere \u03b43D\ncls corresponds to the implicit classifier score for the text-conditioned 3D-aware MVDream multiview diffusion\nmodel, analogous to the implicit classifier score for the video model above. Moreover, we added the view guidance and\n23\nnegative prompting terms [102]. Our negative prompt vneg in the 3D stage is \u201cugly, bad anatomy, blurry, pixelated obscure,\nunnatural colors, poor lighting, dull, and unclear, cropped, lowres, low quality, artifacts, duplicate, morbid, mutilated,\npoorly drawn face, deformed, dehydrated, bad proportions\u201d, following Shi et al. [76]. We apply negative prompting only to\nthe MVDream 3D multiview diffusion model (the negative prompt is taken from MVDream, after all). For view guidance,\nwe attach \u201c, front view\u201d (azm \u2264 30\u25e6 or azm \u2265 330\u25e6), \u201c, back view\u201d (150\u25e6 \u2264 azm \u2264 210\u25e6), \u201c, overhead view\u201d (elv \u2265 60\u25e6)\nand \u201c, side view\u201d (otherwise) at the end of a text prompt to construct the augmented prompt vaug. Furthermore, \u03f53D and\n\u03f5im denote the diffusion noise values used to perturb the renderings given to the 3D-aware diffusion model and the regular\ntext-to-image model, respectively.\nStage 2: Adding Dynamics for 4D Synthesis. In the 4D stage, we apply our novel motion amplification mechanism\n(Sec. 3.4) after we combined the regular implicit classifier score of the video model \u03b4vid\ncls with the additional negative prompt-\ning term. Therefore, let us define\n\u02c6\u03b4vid\ncls+neg := \u03b4vid\ncls + \u03c9neg\n\u0000\u02c6\u03f5vid(Z, t) \u2212 \u02c6\u03f5vid(Z, vneg, t)\n\u0001\n.\n(28)\nWe can now write out the entire gradient used to update the deformation field \u03a6 in AYG\u2019s main 4D stage in a concise manner\nas\n\u2207\u03a6LAYG\n4D-stage = Et,\u03f5vid,\u03f5im\n\u0014\nw(t)\n\u001a\n\u03c9vid\n\u0012D\n\u02c6\u03b4vid\ncls+neg\nE\nframe-avg. + \u03c9ma\n\u0014\n\u02c6\u03b4vid\ncls+neg \u2212\nD\n\u02c6\u03b4vid\ncls+neg\nE\nframe-avg.\n\u0015\u0013\n+ \u03c9im\u03b4im\ncls\n\u001b\u2202X\n\u2202\u03a6\n\u0015\n+ \u03bbJSD \u2207\u03a6LJSD-Reg. + \u03bbRigidity\u2207\u03a6LRigidity-Reg.,\n(29)\nwhere we inserted the motion amplification mechanism with scale \u03c9ma, negative prompting with weight \u03c9neg, as well as the\nadditional regularizers. We only do negative prompting for the video model, using \u201clow motion, static statue, not moving, no\nmotion\u201d as the negative prompt vneg.\nUse of Latent Diffusion Models. We would like to remind the reader that in the above derivations we have not explicitly\nincluded our diffusion models\u2019 encoders. All employed diffusion models are latent diffusion models [70, 86], see Ap-\npendix C.7. In practice, all score distillation gradients are calculated in latent space on encodings of the 4D scene renderings,\nand the gradients are backpropagated through the models\u2019 encoders before further backpropagated through the differentiable\nrendering process into the 4D scene representation. However, AYG\u2019s synthesis framework is general.\nE. Experiment Details\nE.1. Video Diffusion Model Training\nWe train a text-to-video diffusion model, following VideoLDM [7]. VideoLDM is a latent diffusion model that builds\non Stable Diffusion [70] as text-to-image backbone model and fine-tunes it on text-video datasets by inserting additional\ntemporal layers into the architecture. For details, see Blattmann et al. [7]. Here, we add conditioning on the frames-per-\nsecond (fps) frame rate, following [78, 79], scale the number of frames from 8 to 16 frames, and train the model on a larger\ndataset. Note that our video model was trained only for the purpose of 4D score distillation and is not meant to be a standalone\nvideo generation system, which would require additional upsampling and interpolation modules.\nDatasets. Our training initially utilizes the WebVid-10M dataset [4], which comprises 10 million text-video pairs. In our\nanalysis of the WebVid dataset, we noted the presence of many slow-motion videos. To address this, we implement a simple\nfiltering algorithm to exclude such data. Initially, we calculate optical flow at 2 fps utilizing the iterative Lucas-Kanade\nmethod [49]. Following this, we filter out any videos where the average optical flow magnitude falls below a specified\nthreshold. Specifically, we set this threshold to be 10. We further filter data with aesthetic score [73] lower than 4.0 and train\nour model for 100K iterations. To enhance the model\u2019s generalization capabilities, we incorporate the HDVG-130M dataset\n[90] in a subsequent fine-tuning phase for an additional 100K iterations. During fine-tuning, we specifically exclude data\ncontaining keywords such as \u2018talk\u2019, \u2018chat\u2019, \u2018game\u2019, \u2018screenshot\u2019, \u2018newspaper\u2019, \u2018screen\u2019, and \u2018microphone\u2019 in text captions.\nThese keywords often indicate video recordings of interviews showing little scene dynamics or computer games with very\nunusual, non-photorealistic visual appearance. Since there are many such videos in the dataset and we are not interested\nin generating such videos or 4D scenes in this paper, we remove these videos from the training data. We also increase the\noptical flow slow motion filtering [49] threshold to 20. Moreover, since WebVid-10M is much smaller than HDVG-130M, but\nprovides valuable training data with high-quality text annotations by human annotators, we slightly oversample the filtered\nWebVid-10M data. Specifically, we do not sample training data as if we just merged WebVid and HDVG and then randomly\ndrew samples from the resulting dataset to form training batches. Instead, we sample in such a way as if we tripled the filtered\n24\nTable 3. Hyperparameters for the first stage (3D synthesis).\nbs per GPU\n# GPUs\n# renders\nlrposition\nlrrgb\nlrsh\nlropacity\nlrscaling\n\u03c9vg\n\u03c9neg\n\u03c93D\n\u03c9im\n4\n1\n16\n0.0002\n0.01\n0.0005\n0.05\n0.005\n3.0\n0.8\n1.6\n0.4\nTable 4. Hyperparameters for the second stage (dynamic 4D synthesis).\nbs per GPU\n# GPUs\n# renders\nlr\u03a6\nnum. hidden \u03a6\nnum. layers \u03a6\n\u03bbRigidity\n\u03bbJSD\n\u03c9ma\n\u03c9neg\n\u03c9vid\n\u03c9im\n1\n4\n64\n0.001\n128\n5\n100.0\n30.0\n24.0\n0.8\n1.0\n1.0\nWebVid data and only then merged with the filtered HDVG and sampled from the full training dataset randomly to construct\ntraining batches. We train the model on 128 NVIDIA-80G-A100 GPUs, utilizing fp16 precision and a gradient accumulation\nstrategy with a batch size of 4 per-GPU. The total batch size, distributed across all GPUs, is 2048, achieved after complete\ngradient accumulation.\nFrames-per-second (fps) Conditioning. We condition the model on the frames-per-second (fps) frame rate (by using\ncross-attention to the corresponding sinusoidal embedding). During training, the fps values are randomly sampled within a\nrange of 2 to 16. Given that lower fps often indicates more motion and presents a greater learning challenge, we sample low\nfps more often. Specifically, we sample the training video fps based on the probability distribution p(fps) \u223c 1/fpsc, where c\nis a hyperparameter. We choose c = 1.6 in our experiments.\nOtherwise, our training exactly follows VideoLDM [7]. Samples from AYG\u2019s latent video diffusion model are shown in\nFigs. 15 to 17 in Appendix G.4 as well as in the supplementary video ayg new video model.mp4, demonstrating the\neffect of the fps conditioning.\nIt is worth noting that we also explored training a larger latent video diffusion model with more parameters, improved\ntemporal attention layers and additional cross-frame attention [38, 94] for higher performance. While this model indeed\nwas able to generate noticeably higher quality videos, this did not translate into improved 4D distillation performance when\nused during score distillation. It would be valuable to study the effect of different video diffusion models during text-to-4D\nsynthesis via score distillation in more detail. Based on these observations, we kept using the more memory efficient model\ndescribed in this section above, which more directly follows the architecture from Blattmann et al. [7], apart from the fps\nconditioning.\nE.2. Text-to-4D Hyperparameters\nStage 1. Table 3 summarizes the hyperparameters for the 3D optimization stage. bs denotes the batch size, and lrposition,\nlrrgb, lrsh, lropacity and lrscaling denote the learning rates for the 3D Gaussians\u2019 position, color, spherical harmonics, opacity\nand scaling parameters, respectively. We use a single GPU for the optimization in the first 3D synthesis stage and a batch size\nof 4. This means that we use 4 independent sets of 4 images each, given to MVDream, which takes sets of 4 images as input.\nEach set of 4 images consists of renders with the correct relative camera angles for the multiview diffusion model MVDream.\nThe 4 sets of images are then fed to MVDream, and all images are additionally fed to the regular text-to-image diffusion\nmodel (Stable Diffusion). Hence, in each optimization step, 16 different images are rendered in total from the 3D scene. For\nlrposition, we start from 0.001 and decay to the specified value by the 500-th iteration. We note that we followed the learning\nrate schedules used by DreamGaussian [83]. \u03c9vg and \u03c9neg denote the view guidance scale and negative prompt guidance scale,\nrespectively. \u03c93D is the weighting factor for the classifier score distillation term from MVDream [76] (p3D) and \u03c9im is the\nweighting factor for the classifier score distillation term from the image diffusion model (pim) (see Section 3.2). We sample\nthe diffusion time t in the range [0.02, 0.98] at the start of optimization and decay the range to [0.02, 0.5] by the 6,000-th\niteration for the image diffusion model (pim). For MVDream [76] (p3D), we directly follow their schedule which samples t\nfrom [0.98, 0.98] at the start of optimization and decay the range to [0.02, 0.5] by the 8,000-th iteration. We randomly choose\nblack or white background during training. We run 10,000 optimization steps on average for this stage.\nStage 2. Table 4 summarizes the hyperparameters for the dynamic 4D optimization stage. bs again denotes the batch size,\nand lr\u03a6, \u201cnum. hidden \u03a6\u201d and \u201cnum. layers \u03a6\u201d denote the learning rate, number of hidden units and number of layers for the\ndeformation MLP, respectively. \u03bbRigidity and \u03bbJSD denote the weighting terms for the rigidity regularization (Appendix C.4)\nand the JSD-based dynamic 3D Gaussian distribution regularization (Appendix C.5). \u03c9ma specifies the motion amplification\nscale, while \u03c9neg is the negative prompt guidance scale for the video diffusion model. \u03c9vid denotes the weighting term for the\nvideo DM\u2019s (pvid) classifier score distillation term and \u03c9im denotes the weighting term for the image model\u2019s (pim) classifier\n25\nFigure 9. Screenshot of instructions provided to participants of the user studies for comparing AYG and MAV3D [79] as well as for the\nablation studies.\nscore distillation term (see Section 3.2). Here, we used 4 GPUs per optimization with a batch size of 1 on each GPU. This\nmeans that on each GPU a single batch of 16 consecutive 2D images is rendered, consistent with the video diffusion model,\nwhich models 16-frame videos. Recall that, as discussed in Sec. 3.2, only four of those frames are also given to the regular\ntext-to-image diffusion model. Hence, in each optimization step, 64 different images are rendered in total from the dynamic\n4D scene. We sample the diffusion time t in the range [0.02, 0.98] throughout the optimization process for the second stage.\nWe also run 10,000 optimization steps on average for this stage.\nE.3. Evaluation Prompts\nFor the baseline comparison to MAV3D [79], we used all the 28 prompts from MAV3D\u2019s project page:\n\u201cAn alien playing the piano.\u201d; \u201cShark swimming in the desert.\u201d; \u201cA dog wearing a Superhero outfit with red cape flying\nthrough the sky.\u201d; \u201cA monkey eating a candy bar.\u201d; \u201cA squirrel DJing.\u201d; \u201cA cat singing.\u201d; \u201cA bear driving a car.\u201d; \u201cChi-\nhuahua running on the grass.\u201d; \u201cA human skeleton drinking wine.\u201d; \u201cA yorkie dog eating a donut\u201d; \u201cA baby panda eating\nice cream\u201d; \u201cA kangaroo cooking a meal.\u201d; \u201cA humanoid robot playing the violin.\u201d; \u201cA squirrel playing the saxophone.\u201d;\n26\nTable 5. R-Precision comparison to MAV3D [79] with the 300 text prompts also used by Singer et al. [78] and Singer et al. [79].\nMethod\nAYG (ours)\nAYG (ours)\nMAV3D [79]\nMAV3D [79]\n3D-stage\n4D-stage\n3D-stage\n4D-stage\nR-Precision \u2191\n82.2\n81.7\n82.4\n83.7\n\u201cAn octopus is underwater.\u201d; \u201cA silver humanoid robot flipping a coin.\u201d; \u201cA goat drinking beer.\u201d; \u201cA squirrel playing on a\nswing set.\u201d; \u201cA panda playing on a swing set.\u201d; \u201cA crocodile playing a drum set.\u201d; \u201cA squirrel riding a motorcycle.\u201d; \u201c3D\nrendering of a fox playing videogame.\u201d; \u201cA dog riding a skateboard.\u201d; \u201cAn emoji of a baby panda reading a book.\u201d; \u201cClown\nfish swimming through the coral reef.\u201d; \u201cA space shuttle launching.\u201d; \u201cA corgi playing with a ball.\u201d; \u201cA panda dancing.\u201d\nFor the ablation study, we selected the following 30 text prompts:\n\u201cA cat singing.\u201d; \u201cA corgi playing with a ball.\u201d; \u201cA cow running fast.\u201d; \u201cA dog wearing a Superhero outfit with red cape\nflying through the sky.\u201d; \u201cA fox dressed in a suit dancing.\u201d; \u201cA monkey eating a candy bar.\u201d; \u201cA monkey is playing bass\nguitar.\u201d; \u201cA panda dancing.\u201d; \u201cA panda surfing a wave.\u201d; \u201cA pig running fast.\u201d; \u201cA purple unicorn flying.\u201d; \u201cA space shuttle\nlaunching.\u201d; \u201cA squirrel DJing.\u201d; \u201cA squirrel playing on a swing set.\u201d; \u201cA squirrel playing the saxophone.\u201d; \u201cA squirrel\nriding a motorcycle.\u201d; \u201cA storm trooper walking forward and vacuuming.\u201d; \u201cAn alien playing the piano.\u201d; \u201can astronaut is\nplaying the electric guitar.\u201d; \u201cAn astronaut riding a horse.\u201d; \u201cAn astronaut riding motorcycle.\u201d; \u201cA panda reading a book.\u201d;\n\u201cBeer pouring into a glass.\u201d; \u201cChihuahua running.\u201d; \u201cClown fish swimming.\u201d; \u201cA dog riding a skateboard.\u201d; \u201cFlying\ndragon on fire.\u201d; \u201cIntricate butterfly flutters its wings.\u201d; \u201cWaves crashing against a lighthous.\u201d; \u201cWood on fire.\u201d\nE.4. User Study Details\nWe conducted human evaluations (user studies) through Amazon Mechanical Turk to assess the quality of our generated 4D\nscenes, comparing them with MAV3D [79] and performing ablation studies.\nFor the MAV3D comparison, we used the 28 rendered videos from MAV3D\u2019s project page (https://make-a-\nvideo3d.github.io/) and compared them against our model (AYG) using identical text prompts (see Appendix E.3\nabove). We rendered our dynamic 4D scenes from similar camera perspectives and created similar videos. We then asked\nthe participants to compare the two videos with respect to 6 different categories and indicate preference for one of the meth-\nods with an option to vote for \u2018equally good\u2019 in a non-forced-choice format. The 6 categories measure overall quality, 3D\nappearance and 3D text alignment, as well as motion amount, motion text alignment and motion realism (see questions in\nFig. 9).\nIn the ablation studies, we proceeded similarly. We showed participants 4D animations for 30 text prompts (see Ap-\npendix E.3 above) generated by the full AYG model and by the modified, ablated models. Again, participants were asked to\nchoose the more favorable 4D character from each pair, with an option to vote for \u2018equally good\u2019.\nFor a visual reference, see Fig. 9 for a screenshot of the evaluation interface. In all user studies, the order of video\npairs (A-B) was randomized for each question. Note that since MAV3D uses an extra background model, while AYG does\nnot, we asked participants to focus only on the moving foreground characters and to not consider the background in their\nresponses. In all user studies, each video pair was evaluated by seven participants, totaling 196 responses for the MAV3D\ncomparison and 210 for each setup in the ablation study. We selected participants based on specific criteria: they had to be\nfrom English-speaking countries, have an acceptance rate above 95%, and have completed over 1000 approved tasks on the\nplatform.\nF. Additional Quantitative Results\nF.1. Comparisons to MAV3D and R-Precision Evaluation\nThe important MAV3D baseline [79] did not release any code or models and its 4D score distillation leverages the large-scale\nMake-A-Video [78] text-to-video diffusion model, which is also not available publicly. This makes comparisons to MAV3D\nsomewhat difficult and this is why we compared to MAV3D by using the available results on their project page. As reported\nin the main text, we performed a user study comparing all their generated 4D scenes with AYG\u2019s generated scenes with the\nsame text prompts. AYG outperforms MAV3D in our user study in all categories (see Table 1 in main text). Moreover, our\ncomparisons to MAV3D do not leverage any fine-tuning as discussed in Appendix C.9. With this fine-tuning, our quality\nimprovements over MAV3D are even larger, which would likely be reflected in an even higher preference for our 4D scenes\nin the user study.\n27\nTable 6. Ablation study by user study on synthesized 4D scenes with 30 text prompts. For each pair of numbers, the left number is the\npercentage that the full AYG model is preferred and the right number indicates preference percentage for ablated model as described in left\ncolumn. The numbers do not add up to 100 and the difference is due to users voting \u201cno preference\u201d (table copied here from main paper\nfor extended discussion in Appendix F.2).\nAlign Your Gaussians\nOverall\n3D\n3D Text\nMotion\nMotion Text\nMotion\n(full model)\nQuality\nAppearance\nAlignment\nAmount\nAlignment\nRealism\nv.s. w/o rigidity regularization\n45.8/13.3\n43.3/19.2\n38.3/15.0\n40.8/15.0\n42.5/18.3\n30.8/26.7\nv.s. w/o motion amplifier\n43.3/23.3\n37.5/28.3\n30.8/26.7\n45.8/10.8\n37.5/26.7\n33.3/31.7\nv.s. w/o initial 3D stage\n67.5/15.0\n57.5/21.7\n64.2/15.0\n60.8/21.7\n60.8/20.8\n59.2/24.2\nv.s. w/o JSD-based regularization\n40.0/25.0\n40.0/27.5\n36.7/27.5\n41.7/24.2\n39.2/29.2\n45.0/24.2\nv.s. w/o image DM score in 4D stage\n42.5/22.5\n39.2/27.5\n36.7/25.8\n33.3/25.9\n37.5/30.0\n27.5/40.0\nv.s. SDS instead of CSD\n44.2/35.8\n40.0/27.5\n35.8/35.0\n35.0/27.5\n35.0/34.2\n32.5/35.8\nv.s. 3D stage w/o MVDream\n66.7/21.7\n48.3/34.2\n38.3/34.2\n41.7/22.5\n40.0/27.5\n40.8/27.5\nv.s. 4D stage with MVDream\n50.8/27.5\n38.3/34.2\n41.6/29.2\n39.2/35.0\n44.2/30.0\n39.2/31.7\nv.s. video model with only fps 4\n46.7/15.8\n27.5/36.7\n30.0/23.3\n36.7/30.0\n31.7/26.7\n32.5/28.3\nv.s. video model with only fps 12\n48.3/29.2\n30.8/29.2\n29.2/28.3\n35.0/28.3\n35.0/30.0\n39.2/26.7\nv.s. w/o dynamic cameras\n32.5/25.0\n32.5/31.7\n35.0/33.3\n35.0/32.5\n35.8/33.3\n32.5/25.0\nv.s. w/o negative prompting\n44.2/28.3\n38.3/32.5\n31.7/29.2\n29.2/31.6\n33.3/30.0\n37.5/28.3\nMAV3D also reports R-Precision [32, 58] in their paper. R-Precision is commonly used in the text-to-3D literature as an\nevaluation metric. In the R-Precision calculation, 2D renderings of the scene are given to a CLIP [65] image encoder and\nthe CLIP encoding is then used to retrieve the closest text prompt among a set of text prompts used in the evaluation. The\nR-Precision value is then the top-1 retrieval accuracy, i.e., the percentage of correctly retrieved text prompts (this is, the text\nprompt which was used to generate the 3D scene is correctly retrieved). However, R-Precision measures only 3D quality\n(and more specifically the alignment of the 3D appearance with the text prompt) and does not in any way capture dynamics\nat all. It is therefore a meaningless metric to evaluate dynamic scenes, which is the focus of our and also MAV3D\u2019s work.\nNevertheless, for future reference and simply to follow MAV3D we also provide R-Precision evaluation results in Table 5.\nWe obtained the list of 300 prompts used in MAV3D\u2019s R-Precision evaluation by the authors of MAV3D. Note that this\nlist of prompts was originally collected not for the evaluation of synthesized 4D dynamic scenes but for the evaluation of\nvideo diffusion models in Make-A-Video [78]. To calculate our R-Precision scores with the 300 text prompts, we used the\nevaluation protocol from https://github.com/Seth-Park/comp-t2i-dataset. Similarly to MAV3D, we\nevaluated R-Precision both after the initial 3D stage and at random times \u03c4 of the dynamic 4D scene after the full 4D stage\noptimization (MAV3D similarly first optimizes a static 3D scene and only then adds an additional temporal dimension to\ngenerate a full dynamic 4D scene). Specifically, for 3D objects, we render with 20 different azimuth angles with a fixed\nelevation of 15 degree, camera distance of 3 and field-of-view of 40. For dynamic 4D scenes, we sample 20 times \u03c4 together\nwith 20 different azimuth angles. We use majority voting over the 20 views as top-1 retrieval results for the R-Precision\ncalculation. The results are shown in Table 5.\nWe see that the two methods perform on par. Importantly, we do not know the exact evaluation protocol MAV3D used\n(e.g. camera poses used for rendering), and in our experience these details matter and can influence the metric non-negligibly.\nHence, considering that the results of the two methods are extremely close, we conclude that the two methods perform\napproximately similarly with respect to R-Precision. We also see that, for both methods, performance does not meaningfully\ndiffer between the 3D and 4D stage. This means that both methods preserve the overall 3D object well when learning\ndynamics in their main 4D stage.\nWe would like to stress again that R-Precision is in the end not very useful for the evaluation of dynamic 4D scenes, as\nit completely misses the important temporal aspect and does not judge scene dynamics and object motion. We believe that\nuser studies are a more meaningful way to evaluate dynamic 4D scenes (also MAV3D performs various user studies in their\npaper). Recall that in our user studies, we outperform MAV3D on all categories.\nF.2. Extended Discussion of Ablation Studies\nHere, we provide an extended discussion of our main ablation studies, originally presented in Table 2 in Sec. 4. We have\ncarried out an extensive number of ablations and there is not enough space in the paper\u2019s main text to discuss all of them in\ndetail. For convenience, we copied the table here, see Table 6, and we will now discuss the different settings one by one.\n28\nAlso see the supplementary video ayg ablation study.mp4, which shows dynamic 4D scenes for all ablations. Note,\nhowever, that these 4D scenes were optimized with only 4,000 optimization steps in the second dynamic 4D optimization\nstage in the interest of efficiency, considering that we had to optimize many 4D scenes for all ablations. The quality of the\nshown 4D scenes is therefore somewhat lower than that of our main results shown elsewhere in the paper and on the project\npage.\nFull AYG v.s. w/o rigidity regularization. We can see a clear preference for the full AYG model compared to a variant\nwithout rigidity regularization. Users strongly prefer the full model in all categories in Table 6. In the supplementary video\nwe see unrealistic distortions of the object for the baseline without rigidity regularization. Such distortions are prevented by\nthe regularization, as expected.\nFull AYG v.s. w/o motion amplifier. Users prefer the full AYG variant that leverages the motion amplifier for all\ncategories. The difference in preference is most pronounced in the \u201cMotion Amount\u201d category, which validates that the\nmotion amplifier indeed amplifies motion. In the supplementary video, we can clearly observe enhanced motion compared\nto the baseline without the motion amplifier.\nFull AYG v.s. w/o initial 3D stage. Without the initial 3D stage, simultaneously optimizing the 3D object itself and\nalso distilling motion into the 4D scene results in unstable optimization behavior and no coherent dynamic 4D scenes can\nbe produced. This is visible in the supplementary video and also validated in the user study. The full AYG model with the\ninitial 3D stage is strongly preferred.\nFull AYG v.s. w/o JSD-based regularization. We can also observe a clear preference for the full AYG model including\nthe JSD-based regularization of the distribution of the dynamic 3D Gaussians over an ablated model that does not use it. This\nis visible in all categories in Table 6. In the supplementary video, we can see that without JSD-based regularization the 4D\nsequences show little motion and only some slow floating of the entire objects can be observed. We hypothesize that this\nslow global motion represents a local minimum of the video diffusion model, whose gradients are used to learn the dynamics.\nFalling into this local minimum is prevented through the JSD-based regularization. With JSD-based regularization, more\ncomplex, local motion is learnt instead of global translations or object size changes.\nFull AYG v.s. w/o image DM score in 4D stage. A central design decision of AYG is the simultaneous use of both a\ntext-to-image diffusion model and a text-to-video diffusion model for score distillation in the main 4D optimization stage.\nHence, we compared to a variant that only uses the video model, but we find that the full AYG approach including the image\ndiffusion model score in the 4D stage is preferred, except for \u201cMotion Realism\u201d. However, this is a somewhat ambiguous\ncategory, as people might have subjective opinions on what constitutes a better motion, such as preferring slow but consistent\nmotion versus large but artifact-prone motion. The full model wins on all other categories. The margin between the full and\nablated model is especially large on the 3D appearance and 3D text alignment categories. This is exactly what we expected\nand why the text-to-image model is used, i.e., to ensure that high visual and 3D quality is maintained while the video model\nis responsible for the optimization of the temporal dynamics. In line with that, in the supplementary video we can observe\nsome degradation in the 3D quality without the text-to-image diffusion model in the 4D stage. This justifies AYG\u2019s design\ncomposing a text-to-image and a text-to-video diffusion model for score distillation in the 4D stage.\nFull AYG v.s. SDS instead of CSD. We have a somewhat similar observation when replacing classifier score distillation\n(CSD) with regular score distillation sampling (SDS). The full model is preferred for all categories, except for the more\nambiguous \u201cMotion Realism\u201d. In the supplementary video we see slightly more distorted 3D objects and less motion when\nusing SDS instead of CSD.\nFull AYG v.s. 3D stage w/o MVDream. Another central design decision of AYG is the use of the 3D-aware multiview-\nimage diffusion model MVDream [76] in its first 3D stage. In this ablation, we removed MVDream from the 3D stage\nand optimized the 3D assets only with the text-to-image Stable Diffusion model [70], and then performed the dynamic 4D\ndistillation. We find that users prefer the full model by a large margin on all categories. In short, AYG is not able to produce\nhigh-quality static 3D assets without MVDream, and then also the dynamic 4D optimization struggles because of the poor\n3D initialization. It is worth pointing out, however, that there is a rich literature on score distillation of 3D assets using only\ntext-to-image models and arguably some of these techniques could help. As pointed out previously, the initial 3D assets used\nby AYG in its main 4D stage could potentially also be produced by other methods.\nFull AYG v.s. 4D stage with MVDream. The previous ablation makes one wonder whether it would help to also\ninclude MVDream in the 4D optimization stage to ensure that geometric and multiview consistency is maintained during 4D\noptimization at all times \u03c4 of the 4D sequences. However, we find that this is not the case. Users prefer the dynamic 4D\nscenes generated by the regular AYG model over the one that also includes MVDream in the 4D stage for all categories in\nthe table. In the supplementary video, we see that including MVDream in the 4D stage can lead to odd motion patterns or\noverall reduced motion. We hypothesize that the video diffusion model and the MVDream multiview diffusion model produce\n29\nTable 7. Ablation study on view guidance by user study on synthesized static 3D scenes from AYG\u2019s initial 3D stage. We used 30 text\nprompts, the same as in the other ablation studies. Numbers are percentages.\nMethod\nAYG w/ view guidance\nAYG w/o view guidance\nEqual\npreference\npreferred\npreferred\npreference\nOverall Quality\n33.3\n33.8\n32.9\n3D Appearance\n28.6\n29.0\n42.4\n3D Text Alignment\n30.0\n34.8\n35.2\nsomewhat conflicting gradients, harming the 4D optimization process and suppressing the learning of smooth 4D dynamics.\nHyperparameter tuning and a carefully chosen weighting between the MVDream and video models in the composed score\ndistillation scheme could potentially address this, but we were not able to make this setting work better than the regular AYG\napproach without MVDream in the 4D stage. Together with the previous ablation, we conclude that MVDream is a crucial\ncomponent of AYG, but only in its initial 3D stage.\nFull AYG v.s. video model with only fps 4. Our newly trained latent video diffusion model is conditioned on the\nframes-per-second (fps) frame rate and when optimizing dynamic scenes during AYG\u2019s 4D stage we sample different fps\nvalues.\nIn this ablation, we ask what would happen if we only sampled the low fps = 4 value, which corresponds\nto videos that show a lot of motion, but therefore are less smooth temporally (see the attached supplementary video\nayg new video model.mp4). We find that the full AYG model that samples different fps conditionings during score\ndistillation is overall preferred, although there is an outlier in the 3D appearance category. However, the varying fps condi-\ntionings in AYG primarily aim at producing better motion and not at improving 3D quality, and for the motion categories\nthe main AYG model is generally preferred over the ablated version with only one fps = 4 value. Visually, in the attached\nablations video we observe slightly lower quality motion, in line with the results from the user study.\nFull AYG v.s. video model with only fps 12. Here, we instead only use fps = 12, corresponding to videos with less\nbut smoother motion. We again see that the full AYG model is preferred over this ablated variant, this time in all categories\nincluding the 3D ones. In the supplementary video we can see significantly reduced motion in the dynamic 4D scenes when\nusing this ablated AYG version.\nNote that these observations are in line with Singer et al. [79], who also used an fps-conditioned video diffusion model\nand different fps during score distillation. Sampling different fps during 4D optimization helps both them and our AYG to\nproduce better dynamic 4D scenes with higher-quality motion.\nFull AYG v.s. w/o dynamic cameras. We can also observe the benefit of dynamic cameras. The full AYG model that\nincludes dynamic cameras is preferred in all categories over the ablated model that uses static cameras when rendering the\nvideo frames given to the video diffusion model during score distillation in the 4D stage. In the supplementary video, we\nsee that the dynamic 4D sequences generated without dynamic cameras have less motion. This is also consistent with Singer\net al. [79], who also observed a similar benefit of dynamic cameras.\nFull AYG v.s. w/o negative prompting. Finally, we also studied the effect of negative prompt guidance during the 4D\nstage. Overall, the main model that includes negative prompting is preferred, although on \u201cMotion Amount\u201d the baseline\nwithout the negative prompt guidance is preferred. In the attached video, we observe lower quality dynamics without negative\nprompting.\nConclusions. Overall, our ablation studies show that all of AYG\u2019s components are important for generating high quality\ndynamic 4D scenes. On \u201cOverall Quality\u201d, the main AYG model wins in all ablations with large margins. This justifies\nAYG\u2019s design choices.\nF.3. View-guidance Ablation Study\nOur main ablation studies focused primarily on the 4D stage and we wanted to study the effects of the different components\nwhen learning dynamic 4D scenes, which is the main novelty of our paper. Here, we show an additional ablation study\nthat analyzes the effect of view-guidance (Sec. 3.4), which we used only in AYG\u2019s initial 3D stage. We performed a user\nstudy using the same prompts and following the exact same protocol as in our other user studies for the other ablations (see\nAppendix E.4). However, since view-guidance has only been used in the 3D stage, we only asked the users about overall\nquality, 3D appearance and 3D text alignment of the static 3D scenes synthesized after the initial 3D stage. We showed the\nusers 3D objects generated with and without view guidance and asked them to compare and rate them. The results are shown\nin Table 7. We see that AYG with or without view-guidance in its 3D stage performs approximately similar according to the\nuser ratings and there is no clear winner in any of the categories.\n30\nWe nevertheless used view guidance in AYG, as we subjectively found in early experiments that it sometimes helped with\noverall 3D appearance and led to a small reduction of the Janus-face problem. However, as the results of the user study here\ndemonstrate, view guidance is certainly not one of the crucial components of AYG that make or break synthesis. However,\nto the best of our knowledge view guidance is a new idea and maybe it can find applications in future work.\nG. Additional Qualitative Results\u2014More AYG Samples\nHere, we show additional generated dynamic 4D scene samples from AYG. We also refer the reader to our supplementary\nvideo ayg text to 4d.mp4, which shows almost all of our dynamic 4D scene samples. We also share videos generated\nby AYG\u2019s newly trained latent video diffusion model for this work.\nG.1. Text-to-4D Samples\nIn Figs. 10, 11 and 13, we show additional text-to-4D samples from AYG, similar to Fig. 6 in the main paper.\nG.2. Autoregressively Extended and Looping Text-to-4D Synthesis\nIn Fig. 12, we show additional samples from AYG that are autoregressively extended to form longer sequences while changing\nthe text prompt and that return to the initial pose to enable looping animations (similar to Fig. 7 in the main paper). For the\nfirst two rows (the assassin) in Fig. 12, we use the following sequence of text prompts: \u201cAssassin with sword running fast,\nportrait, game, unreal, 4K, HD.\u201d, \u201cAssassin walking, portrait, game, unreal, 4K, HD.\u201d and \u201cAssassin dancing, portrait, game,\nunreal, 4K, HD.\u201d. For the next two rows (the lion), we use the following sequence of text prompts: \u201cA lion running fast.\u201d, \u201cA\nlion is jumping.\u201d and \u201cA lion is eating.\u201d.\nFor reference, we also provide the prompts used for Fig. 7 in the main paper. For the first two rows (the bulldog), we use\nthe following sequence of text prompts: \u201cA bulldog is running fast.\u201d and \u201cA bulldog barking loudly\u201d. For the next two rows\n(the panda), we use the following sequence of text prompts: \u201cA panda running.\u201d and \u201cA panda is boxing and punching.\u201d.\nG.3. More Comparisons to Make-A-Video3D\nIn Fig. 14, we show additional visual comparisons to MAV3D [79], similar to Fig. 8 in the main paper.\nG.4. Videos Generated by AYG\u2019s fps-conditioned Video Diffusion Model\nIn Figs. 15 to 17, we present videos generated by AYG\u2019s latent video diffusion model, showing the effect of the fps con-\nditioning. We also refer to the attached supplementary video ayg new video model.mp4, which shows more samples.\n31\n\u201cA cat singing.\u201d\n\u201cTesla trooper shooting lightning, scifi, game character.\u201d\n\u201cVolcano eruption.\u201d\nView 1\nView 2\nView 1\nView 2\nView 1\nView 2\ntime\n<latexit sha1_base64=\"hnLXyj98SyWJxboWlxO2Ayd1ueI=\">AB63icbVBNS8NAE\nJ3Ur1q/qh69BIvgqSTi17HoxWMFawtKJvtpl26uwm7E6GE/gUvHhTx6h/y5r9x0+agrQ8GHu/NMDMvTAQ36HnfTmldW19o7xZ2dre2d2r7h8mjVlLVoLGLdCYlhgivWQo6Cd\nRLNiAwFa4fj29xvPzFteKwecJKwQJKh4hGnBHOphyTtV2te3ZvBXSZ+QWpQoNmvfvUGMU0lU0gFMabrewkGdHIqWDTSi81LCF0TIasa6kikpkgm906dU+sMnCjWNtS6M7U3xMZk\ncZMZGg7JcGRWfRy8T+vm2J0HWRcJSkyReLolS4GLv54+6Aa0ZRTCwhVHN7q0tHRBOKNp6KDcFfHmZPJ7V/cv6xf15rXFTxFGIziGU/DhChpwB01oAYURPMrvDnSeXHenY95a\n8kpZg7hD5zPHyRtjlI=</latexit>\u2327\nFigure 10. Text-to-4D synthesis with AYG. Various samples shown in two views each. Dotted lines denote deformation field dynamics\n(also see supplementary video ayg text to 4d.mp4, where the dynamics are much better visible).\n32\n\u201cPoseidon holding his trident emerging from the sea.\u201d\n\u201cA knight in shining armor holding a sword and shield fighting.\u201d\n\u201cA panda surfing a wave, best quality, 4K, HD.\u201d\ntime\n<latexit sha1_base64=\"hnLXyj98SyWJxboWlxO2Ayd1ueI=\">AB63icbVBNS8N\nAEJ3Ur1q/qh69BIvgqSTi17HoxWMFawtKJvtpl26uwm7E6GE/gUvHhTx6h/y5r9x0+agrQ8GHu/NMDMvTAQ36HnfTmldW19o7xZ2dre2d2r7h8mjVlLVoLGLdCYlhgiv\nWQo6CdRLNiAwFa4fj29xvPzFteKwecJKwQJKh4hGnBHOphyTtV2te3ZvBXSZ+QWpQoNmvfvUGMU0lU0gFMabrewkGdHIqWDTSi81LCF0TIasa6kikpkgm906dU+sMnCjWNt\nS6M7U3xMZkcZMZGg7JcGRWfRy8T+vm2J0HWRcJSkyReLolS4GLv54+6Aa0ZRTCwhVHN7q0tHRBOKNp6KDcFfHmZPJ7V/cv6xf15rXFTxFGIziGU/DhChpwB01oAYURPM\nrvDnSeXHenY95a8kpZg7hD5zPHyRtjlI=</latexit>\u2327\nView 1\nView 2\nView 1\nView 2\nView 1\nView 2\nFigure 11. Text-to-4D synthesis with AYG. Various samples shown in two views each. Dotted lines denote deformation field dynamics\n(also see supplementary video ayg text to 4d.mp4, where the dynamics are much better visible).\n33\nView 2\nInitial pose\nWalking\nRunning\nInitial pose\nView 1\nView 2\nView 1\nDancing\nInitial pose\nJumping\nRunning\nInitial pose\nEating\nFigure 12. Autoregressively extended text-to-4D synthesis. AYG is able to autoregressively extend dynamic 4D sequences, com-\nbine sequences with different text-guidance, and create looping animations, returning to the initial pose (also see supplementary video\nayg text to 4d.mp4, where the different actions are much better visible).\n34\ntime\n<latexit sha1_base64=\"hnLXyj98SyWJxboWlxO2Ayd1ueI=\">AB63icbVBNS8N\nAEJ3Ur1q/qh69BIvgqSTi17HoxWMFawtKJvtpl26uwm7E6GE/gUvHhTx6h/y5r9x0+agrQ8GHu/NMDMvTAQ36HnfTmldW19o7xZ2dre2d2r7h8mjVlLVoLGLdCYlhgivW\nQo6CdRLNiAwFa4fj29xvPzFteKwecJKwQJKh4hGnBHOphyTtV2te3ZvBXSZ+QWpQoNmvfvUGMU0lU0gFMabrewkGdHIqWDTSi81LCF0TIasa6kikpkgm906dU+sMnCjWNtS6\nM7U3xMZkcZMZGg7JcGRWfRy8T+vm2J0HWRcJSkyReLolS4GLv54+6Aa0ZRTCwhVHN7q0tHRBOKNp6KDcFfHmZPJ7V/cv6xf15rXFTxFGIziGU/DhChpwB01oAYURPMrvDn\nSeXHenY95a8kpZg7hD5zPHyRtjlI=</latexit>\u2327\n\u201cA panda reading a book.\u201d\n\u201cA bee fluttering its wings fast.\u201d\nView 1\nView 2\n\u201cA purple unicorn flying.\u201d\n\u201cAn astronaut riding a horse.\u201d\nView 1\nView 2\nView 1\nView 2\nView 1\nView 2\nFigure 13. Text-to-4D synthesis with AYG. Various samples shown in two views each. Dotted lines denote deformation field dynamics\n(also see supplementary video ayg text to 4d.mp4, where the dynamics are much better visible).\n35\nFigure 14. AYG (ours) vs. MAV3D [79]. We show four 4D frames for different times and camera angles (also see supplementary video\nayg text to 4d.mp4, where we also show comparisons to MAV3D and where the dynamics are much better visible).\n36\nfps = 4\nfps = 12\ntime\nFigure 15. Two video samples from AYG\u2019s newly trained latent text-to-video diffusion model for the same text prompt \u201cA corgi\nrunning.\u201d but with different fps conditionings fps = 4 and fps = 12. We see that, as expected, conditioning on the lower fps value\ngenerates a video with more motion for the same 4 frames (the model synthesizes 16 frames and we show the 1st, the 6th, the 11th, and the\n16th frame). Conditioning on the higher fps value results in a video with less motion but good temporal consistency.\n37\nfps = 4\nfps = 12\ntime\nFigure 16. Two video samples from AYG\u2019s newly trained latent text-to-video diffusion model for the same text prompt \u201cAssassin with\nsword running fast, portrait, game, unreal, 4K, HD.\u201d but with different fps conditionings fps = 4 and fps = 12. We see that, as expected,\nconditioning on the lower fps value generates a video with more motion for the same 4 frames (the model synthesizes 16 frames and we\nshow the 1st, the 6th, the 11th, and the 16th frame). Conditioning on the higher fps value results in a video with less motion but good\ntemporal consistency.\n38\nfps = 4\nfps = 12\ntime\nFigure 17. Two video samples from AYG\u2019s newly trained latent text-to-video diffusion model for the same text prompt \u201cA turtle\nswimming.\u201d but with different fps conditionings fps = 4 and fps = 12. We see that, as expected, conditioning on the lower fps value\ngenerates a video with more motion for the same 4 frames (the model synthesizes 16 frames and we show the 1st, the 6th, the 11th, and the\n16th frame). Conditioning on the higher fps value results in a video with less motion but good temporal consistency.\n39\n"
  },
  {
    "title": "ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors",
    "link": "https://arxiv.org/pdf/2312.13324.pdf",
    "upvote": "8",
    "text": "ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors\nWeijia Mao1\nYan-Pei Cao2* Jia-Wei Liu1\nZhongcong Xu1\nMike Zheng Shou1\u2217\n1Show Lab, National University of Singapore\n2ARC Lab, Tencent PCG\nhttps://showroom3d.github.io\nA contemporary space complete with a lamp, computer, TV, work desk, comfy sofa, stylish chairs, captivating art, decorative paintings, a cozy bed, \npowerful fans, an efficient air conditioner, and a practical bookshelf.\nIn traditional, old, wooden style, a versatile room with a sofa as the centerpiece, accompanied by various elements such as plants, a bookshelf, a \nlamp, a TV, games, a desk, a dining table, chairs, art and a meditation corner, providing a space for relaxation, reading, entertainment, work, \ngatherings, creativity, and tranquility.\nPanoramic Images\nNear\nFar\nIn post modern style house, with its clean lines, avant-garde shapes, and a vibrant mix of materials, it challenges traditional design norms. The \nroom features sleek, minimalist furniture pieces with unique, asymmetrical forms, creating an atmosphere that is both functional and visually \ncaptivating.\nFigure 1. ShowRoom3D: A novel method for generating high-quality room-scale scenes that can be rendered at any position.\nAbstract\nWe introduce ShowRoom3D, a three-stage approach for\ngenerating high-quality 3D room-scale scenes from texts.\nPrevious methods using 2D diffusion priors to optimize neu-\nral radiance fields for generating room-scale scenes have\nshown unsatisfactory quality. This is primarily attributed to\n*Corresponding Author.\nthe limitations of 2D priors lacking 3D awareness and con-\nstraints in the training methodology. In this paper, we utilize\na 3D diffusion prior, MVDiffusion, to optimize the 3D room-\nscale scene. Our contributions are in two aspects. Firstly,\nwe propose a progressive view selection process to optimize\nNeRF. This involves dividing the training process into three\nstages, gradually expanding the camera sampling scope.\nSecondly, we propose the pose transformation method in\nthe second stage. It will ensure MVDiffusion provide the\n1\narXiv:2312.13324v1  [cs.CV]  20 Dec 2023\nFirst stage\nSecond stage\nThird stage\n(a)\n(b)\n(c)\nFigure 2.\nThe illustration of every stage\u2019s camera sampling\nmethod. (a) In the first stage, the camera is positioned at the origin\nand can rotate freely. (b) In the second stage, the camera is sam-\npled at various positions, but its direction always faces outward\nfrom the origin. (c) In the third stage, at different iterations, the\ncamera position and perspective are randomly sampled. Within a\nsingle iteration, the cameras remain in the same position.\naccurate view guidance. As a result, ShowRoom3D enables\nthe generation of rooms with improved structural integrity,\nenhanced clarity from any view, reduced content repetition,\nand higher consistency across different perspectives. Exten-\nsive experiments demonstrate that our method, significantly\noutperforms state-of-the-art approaches by a large margin\nin terms of user study.\n1. Introduction\nThe generation of 3D room-scale scenes is crucial for vari-\nous industries, including VR/AR and Metaverse. In the 2D\ndomain, there has been significant progress in image gener-\nation conditioned on user input, thanks to models like Stable\nDiffusion [32] and Imagen [33]. 2D image generation mod-\nels allow users to control content using prompts and other\nmodalities, such as layouts or poses. However, in the 3D\ndomain, the lack of large-scale 3D datasets has led to meth-\nods [14, 26, 42, 43, 49] that combine the text-to-image dif-\nfusion model with 3D representations such as NeRF [21] or\nDMTet [37], often focusing on individual 3D objects gener-\nation or their combinations.\nHowever, few of these methods tackle the challenges of\n3D room-scale scene generation, as the output needs to be\ndense, coherent, and encompass all required structures in\nthe views. When applied to 3D room generation, they en-\ncounter serious issues such as the Janus problem, unreason-\nable room structure, style inconsistencies, and more. At the\nsame time, another line of research [8, 10] involves employ-\ning the Stable Diffusion inpainting model to generate 3D\nscenes. However, the consistency within indoor environ-\nments is notably subpar, characterized by visible distortions\nand blurring.\nRecently, MVDiffusion [40] is proposed to generate the\npanoramic images of a 3D scene, offering several advan-\ntages. (1) It is the first model finetuned on the Stable Diffu-\nsion model using the Matterport3D indoor scene dataset [3].\nIt will provide the model with enhanced prior knowledge\nabout the structures and layouts of 3D rooms. (2) MVD-\niffusion introduces the Correspondence-Aware Attention\n(CAA) module to ensure consistency between views. This\nendows the model with 3D awareness, considering the gen-\nerated images as integral parts of the entire room. How-\never, MVDiffusion is specifically designed to generate only\npanoramic images of a 3D scene and cannot be used to cre-\nate a fully realized 3D space. The geometry and structure\nof the generated scene are not guaranteed.\nIn our approach, we leverage MVDiffusion in conjunc-\ntion with NeRF to create a 3D room-scale scene. How-\never, optimizing a NeRF model, which accurately repre-\nsents the room with high-quality geometry and appearance,\nusing MVDiffusion is not a trivial task. There are two chal-\nlenges: (1) Confirming the room\u2019s geometry and structure\nwhile also aiming for rendering from any view during the\nsame training stage tends to yield suboptimal results. (2)\nThe pretrained MVDiffusion model can not effectively han-\ndle scenarios where the camera is placed at any position\nwithin the room, except for the origin. In such cases, the\nMVDiffusion panoramic model assumes the camera is at\nthe center of the room, providing inaccurate view guidance\nfor NeRF training.\nTo address the first challenge, we adopt a progressive\nview selection approach and divide our training process into\nthree distinct stages. Progressive view selection involves\ngradually expanding the camera sampling scope in differ-\nent stages. We first ensure that the geometry of the room\nis well generated within a limited set of training views, and\nthen we consider rendering the room at any position. As\nillustrated in Figure 2(a), during the first stage, we position\nthe camera at the center of the room to generate a panoramic\nview. This initial step is crucial for determining the struc-\nture and geometry of the room. In the second stage, we\ncontinue to distill the NeRF model, ensuring that the cam-\nera consistently faces outward from the origin at any posi-\ntion, as depicted in Figure 2(b). This stage further improves\nthe room\u2019s geometry and enables rendering from multiple\nviewpoints. In the third stage, we sample the cameras at\nany position and apply rotations to refine the NeRF model\nat different iterations. During each iteration, we randomly\nsample cameras from the same position, as shown in Fig-\nure 2(c). Ultimately, this process will yield a NeRF model\ncapable of rendering the generated rooms from any position\nand at any rotation.\nTo address the second challenge, we introduce a pose\ntransformation method to address situations where the sam-\npled cameras are not at the origin.\nThis transformation\nwill provide an equivalent camera pose to the MVDiffusion\nmodel, as opposed to using the real pose. The equivalent\ncamera will share the similar view with the real one. This\n2\nmethod ensures that MVDiffusion provides accurate view\nguidance, even when the sampled cameras\u2019 positions are not\nat the origin.\nIn summary, our key contributions are as follows: (1)\nWe are the first to explore the utilization of 3D diffusion\nprior for generating high-quality 3D room-scale scenes us-\ning the SDS method. (2) We present a three-stage training\npipeline, incorporating distinct camera sampling methods\nin each stage and pose transformation in the second stage\nto improve the clarity and aesthetics of the generated room.\n(3) Our method enables the generation of state-of-the-art 3D\nroom-scale scenes, showcasing not only more compelling\ngeometry and appearance but also a more reasonable room\nstructure and reduced content repetition. Furthermore, it\nexhibits the capability to render views across a larger space,\nsurpassing the capabilities of previous methods.\n2. Related Work\n3D Content Generation. The emergence of NeRF [21] has\nsignificantly improved the quality of novel view synthesis in\nthe 3D domain. NeRF-based models [16, 18, 21, 24, 27, 39,\n44] integrate volume rendering algorithms with MLPs or\nvoxels to predict color and opacity. In the field of 3D gener-\nation, many works [2, 7, 12, 23, 35, 46] have combined 2D\nunconditional generative models with NeRF to create 3D\ncontents.\nDreamFusion [26] proposes score distillation sampling\n(SDS) to utilize a 2D text-to-image model to optimize the\nNeRF [21] model for generating 3D objects. However, the\ngenerated content is often oversaturated and plagued by\nthe Janus problem (multi-head problem). Subsequent re-\nsearch [1, 43, 49]endeavors aimed to improve the quality\nof 3D content and alleviate oversaturation and the Janus\nproblem.\nProlificdreamer [43] introduced an improved\nmethod, VSD, which utilizes LoRA [11] to train with the\nNeRF model, effectively alleviating oversaturation.\nSet-\nthe-scene [4] and CompoNeRF [15] utilize the SDS distil-\nlation method to create scenes with straightforward object\ncompositions.\nHowever, these approaches are limited to\ngenerating very basic scenes comprising only a few objects.\nSeveral studies [17, 20, 36, 50] also employ the SDS-based\napproaches to edit the NeRF conditioned on users\u2019 prompts.\nSeveral work has developed 3D generation models based\non 2D generation models using 3D datasets. Zero123 [19]\nuses the Objvarse [6] dataset to finetune the Stable Diffu-\nsion model [32], enabling it to generate view-consistent im-\nages. Magic123 [28] leverages both the Stable Diffusion\nmodel and the Zero123 model as 2D and 3D priors to opti-\nmize NeRF, resulting in a consistent structure for 3D ob-\njects. MVDream [38] redesigns the Stable Diffusion ar-\nchitecture, incorporating a 3D-aware attention module. It\nuses the Objvarse dataset [6] for finetuning to generate high-\nquality content that mitigates the Janus problem. However,\nthese efforts predominantly center around 3D object gener-\nation, with limited focus on generating high-quality indoor\nscenes.\nIndoor Scene Generation. Historically, several works [13,\n30, 31, 41, 45] have utilized real indoor scene datasets\nlike Matterport3D [3], ScanNet [5], and RealEstate [48]\ndatasets to train generative models, such as GANs or autore-\ngressive transformers, for synthesizing novel views. How-\never, these approaches have primarily focused on generating\nnovel views within indoor scenes, without capturing the en-\ntire 3D indoor environment. As a result, the generated con-\ntent quality has been suboptimal in consistency. At the same\ntime, Scenescape [8], Text2room [10] and Text2NeRF [47]\nutilize the inpainting function of Stable Diffusion models to\ngenerate 3D scenes. Nevertheless, these methods still face\nchallenges in maintaining style and content consistency, as\nthe generation of each image is conditioned solely on the\nprevious image.\nMVDiffusion [40] aims to address this issue by intro-\nducing a correspondence attention module to capture rela-\ntionships between views. The camera poses in the gener-\nated views will be fed into MVDiffusion model. Besides,\nit finetunes the Stable Diffusion model using the Matter-\nport3D [3] dataset. However, it only can generate view-\nconsistent panoramic images. There are limitations in as-\nsuring the geometry of the room and rendering the room\nfrom any view.\n3. Method\nIn this section, we first briefly introduce the score distilla-\ntion sampling (SDS) method and the MVDiffusion model\nwhich our method is based on (Section. 3.1). Then we will\nintroduce our task setting and our approach ShowRoom3D\nthat adopts three-stage training procedure (Section. 3.2).\nOur pipeline is shown in Figure 3.\n3.1. Preliminary\nText-to-3D Generation by Score Distillation Sampling\n(SDS) [26]. SDS is a promising method that combines 2D\ngenerative models, such as Stable Diffusion [32], with 3D\nrepresentations, like Neural Radiance Fields (NeRF) [21],\nto generate 3D objects and scenes solely based on textual\nprompts.\nWhen using a pretrained text-to-image model,\nnoise is introduced into the NeRF\u2019s output. Then the noisy\nimage xt, a time step t, a text embedding y are then fed into\na U-Net architecture to predict the noise, denoted as \u02c6\u03f5. The\noptimization of NeRF\u2019s parameters, denoted as \u03d5, is based\non minimizing the loss between the noise and the predicted\nnoise. In recent times, distillation methods have been ef-\nfectively applied in the field of 3D generation, producing\n3\np1\np2\no\n!1\n#2\no\nv\ndcam\nd\np\n(a)\nFirst Stage\nNeRF \u03d5s\nCamera at\nthe origin\nSecond Stage\nThird Stage\nTransformation\nModule\n!\nMVDiffusion\n!\nRandomly sample\ncamera\n[\", #\u2019], K\u2019\nat the origin\n!: train from scratch\n\": tuning\n#: frozen\n\"\n[\", #], K\nat any position\nSDS loss\nSDS loss\n\"\np1\np2\no\n(b)\n(c)\n(d)\nPose Transformation\nNeRF \u03d5s\nNeRF \u03d5s\no\no\no\nSDS loss\nFigure 3. Method overview: Left: Our three-stage training pipeline. First Stage: the camera will be at the center of the room and rotate\nany degree. Second stage: the camera will be at any position and face outward from the center o. Third stage: the camera will be at any\nposition and rotate any degree. Right: It introduces the pose transformation module in the second stage. (a) shows the camera sampling\nmethod in the second stage. p1 and p2 are sampled points at one iteration and o is the center of the room in 3D space. (b) shows the\nperspective of (a) observed from a 2D plane. (c) shows the two new cameras after the pose transformation. (d) shows the specific scenes\nobserved by the camera at each sampling point. v, dcam, d represent the observed view, the distance from the camera position to the origin\nand the depth of the view. \u03b81 and \u03b82 represent the FOV of two cameras.\nsignificant improvements. Its gradient is approximated by\n\u2207\u03d5LSDS(\u03d5) \u2248 Et,\u03f5,c\n\u0014\n\u03c9(t)(\u02c6\u03f5(xt, t, y) \u2212 \u03f5)\u2202g(\u03d5, c)\n\u2202\u03d5\n\u0015\n,\n(1)\nwhere \u03c9(t) is a weighting function. Despite SDS has made\nsignificant strides in 3D object generation, how to use the\nSDS method specifically for room-scale scene generation\nremains underexplored.\nMVDiffusion [40]. The MVDiffusion model is a multi-\nview consistency generation model built upon Stable Diffu-\nsion. One of the crucial components that ensures view con-\nsistency is the correspondence-aware attention mechanism\n(CAA module), which is employed to capture the relation-\nship between adjacent views. This mechanism takes into\naccount both the source feature maps, denoted as F, and\nthe target feature maps, denoted as F l.\nFor each source feature, MVDiffusion leverages the rel-\native camera pose to determine the location of the corre-\nsponding target feature. s and t represent the source and\ntarget pixel. \u00afF(s) is the source feature with the positional\nencoding. l is the number of target feature maps and \u00afF l(tl\n\u2217)\nis the target feature with the positional encoding. N(tl) is\nthe neighborhood of the target pixel. Subsequently, the out-\nput of the attention mechanism is calculated as follows,\nQ = WQ \u00afF(s),\nK = WK \u00afF l(tl\n\u2217),\nm =\nX\nl\nX\ntl\u2217\u2208N (tl)\nSoftMax(Q \u00b7 K) \u00b7 WV \u00afF l(tl\n\u2217),\n(2)\nwhere WQ, WV, and WK represent the query, value,\nand key components of the attention mechanism. For clar-\nity, we do not introduce some specific details, such as posi-\ntion encoding and adding integer displacements. When we\nuse the MVDiffusion model to generate the panoramic im-\nages, the prompt and the camera poses of images need to be\nfed into it. The camera poses will be used in the CAA mod-\nule to calculate the relationship between the source feature\nand the target features.\n3.2. ShowRoom3D\nTask Setting. In this paper, we propose a new method to\ngenerate the high-quality room-scale scene that can be ren-\ndered at any position. Given by text prompts y, our objec-\ntive is to generate a 3D room represented by a NeRF model\n\u03a6 using the pretrained MVDiffusion model M. We choose\nInstant-NGP [22] as our NeRF representation due to its nu-\nmerous advantages, including rapid coverage speed and the\nability to reconstruct complex geometries.\nFirst Stage. As the MVDiffusion model initially generates\na panoramic scene, we employ the SDS method to optimize\n4\na NeRF model, specifically designed to represent a room-\nscale scene panorama. Initially, we position the camera at\nthe world coordinate center and randomly sample the rota-\ntion degree to obtain the camera pose like in Figure 2(a).\nThis camera pose is then fed into both the NeRF model and\nthe correspondence-aware attention module of the MVDif-\nfusion model. So the gradient can be approximated by\n\u2207\u03d5L(\u03d5) \u2248 Et,\u03f5,c\n\u0014\n\u03c9(t)(\u02c6\u03f5(xt, t, y, c) \u2212 \u03f5)\u2202g(\u03d5, c)\n\u2202\u03d5\n\u0015\n.\n(3)\nIn this equation, y represents the prompt, and c repre-\nsents the camera pose, including the camera\u2019s extrinsics and\nintrinsics. We will input c into the NeRF model to obtain the\nrendered image. Subsequently, we introduce noise to the\nrendered image, resulting in the noisy latent xt. This noisy\nlatent xt, the prompt y, the camera pose c and the timestep t\nare then fed into the pretrained MVDiffusion model\u2019s U-Net\nmodule and CAA module for noise prediction and gradient\ncalculation.\nSecond Stage.\nAfter the first stage, we obtain the\npanoramic NeRF, which initially determines the geometry\nand layout of the room. We can rotate any degree to render\nthis room, however, it restricts the camera\u2019s position, pre-\nventing us from rendering the room from any position. At\nthe same time, the geometry of the room is also subpar due\nto the limited training views.\nIn the second stage, our goal has two aspects. Firstly,\nthis stage serves to improve the geometry and room layout\nby adding training views. Secondly, we can render the room\nin a larger space compared to the first stage. We modify the\ncamera sampling method, as shown in the Figure 2(b). Now,\nthe camera can be sampled at any position, and it always\nfaces outward from the origin.\nThe challenge lies in ensuring that MVDiffusion offers\nprecise view guidance even when cameras are not posi-\ntioned identically. To solve this problem, we propose pose\ntransformation to obtain a new camera pose to be fed into\nthe MVDiffusion model, rather than using the real cam-\nera pose directly. For any sampled camera not positioned\nat the center, we will employ pose transformation to ob-\ntain an equivalent new pose at the center. The core idea is\nthat the new camera pose at the center will have a smaller\nfield of view (FOV). Even if the new camera is farther from\nthe scene, the smaller FOV will help it see the similar view\ncompared with the old camera. Next, we will demonstrate\nthe procedure for calculating the new camera pose.\nIn Figure 3(a)(b), p1 and p2 are two points we sampled\nat one iteration, which represent the camera positions. In\nFigure 3(d), let point o be the center of the room and the\nworld coordinate, and point p represent the camera posi-\ntion. \u03b81 denotes the FOV of the camera at position p. v\nrepresents the region visible to the camera and d is the av-\neraged depth of this region. The real camera is located at p.\nPose transformation is used to calculate the new camera at\nthe position o. \u03b82 is the FOV of the new camera at position\no. To ensure that two cameras have a similar view v, we\ncalculate \u03b82 using the following formulas.\nv\n2 = tan \u03b82 \u00b7 d,\nv\n2 = tan \u03b81 \u00b7 (d \u2212 dcam),\n\u03b82 = arctan\n\u0012tan \u03b81 \u00b7 (d \u2212 dcam)\nd\n\u0013\n,\n(4)\nwhere dcam, \u03b81 are known parameters and the depth d can\nbe obtained by the first stage\u2019s prior. So we can calculate\nthe new \u03b82. Then we can use the rendered view from the\ncamera at position o approximate the view from the camera\nat position p by changing the FOV \u03b81 to \u03b82.\nAfter the pose transformation, we obtain two differ-\nent yet similar camera poses c(R, t, K), which repre-\nsents the real camera pose fed into the NeRF model, and\nc\u2032(R, t\u2032, K\u2032), an approximated new camera pose fed into\nMVDiffusion.\nThe rotation matrix R remains constant,\nwhile the camera position t and the intrinsics K undergo\nchanges. Then we can confirm that even if the sampled\npoints are at different positions p1 and p2, the new cam-\neras\u2019 positions fed into MVDiffusion are the same position\no. This ensures that MVDiffusion provides the most accu-\nrate guidance about the relationships between views.\nThird Stage. After the two-stage training process, we have\nestablished the geometry and structure of the room, en-\nabling rendering from a wide range of perspectives. How-\never, some issues remain after the two-stage training. The\nfirst concern is the approximated nature of the pose trans-\nformation method, which may not yield highly accurate re-\nsults.\nThe depth prior is not accurate and the rendering\nviews from the real camera pose and the approximated new\ncamera pose are not perfectly identical. Furthermore, dur-\ning the first two stages of training, the camera consistently\nfaces outward from the origin, resulting in some missing\ntraining views.\nIn the third stage, we position the camera freely and\napplied various degrees of rotation to further finetune the\nNeRF model. Following this stage, the NeRF becomes a\nversatile renderer capable of handling scenes from any po-\nsition and at any rotation. To be specific, as shown in the\nFigure 2(c), we sample two points at the same position at\none iteration fed into NeRF and MVDiffusion. It will en-\nsure MVDiffusion provide the relatively accurate guidance\nwhen the sampled cameras are the same position, even if\nthey are not at the center. At different iterations, the camera\nposition and perspective are randomly sampled.\n5\nDreamFusion\nProlificDreamer\nOurs\nA room with various elements such as a bookshelf, a lamp, a computer, a desk, a sofa, chairs, art, paintings, a yoga mat, a dining table, large windows\nAzimuth = 0\nElevation = 0\nAzimuth = 90\nElevation = 0\nAzimuth = 180\nElevation = 0\nAzimuth = 270\nElevation = 0\nAzimuth = 90\nElevation = 20\nAzimuth = 90\nElevation = -20\nz = -1\nz = 1\nLOTR\nText2Room\nFigure 4. Qualitative comparisons of ShowRoom3D and state-of-the-art approaches.\n4. Experiments\n4.1. Implemention details\nIn our approach, we utilize the panoramic model from\nMVDiffusion [40] as the foundational model for optimizing\nthe NeRF [21]. We maintain the same world coordinate sys-\ntem and camera coordinate system as MVDiffusion. We im-\nplement the SDS method in the threestudio framework [9].\nThe first stage of NeRF training requires 10,000 iterations,\nthe second stage needs 15,000 iterations and the third stage\nneeds 5000 iterations. We utilize a single NVIDIA RTX\n3090 GPU to train the three-stage NeRF model. To mitigate\nthe issue of oversaturation introduced by the SDS method,\nwe draw inspiration from MVDream [38] and employ sim-\nilar techniques. Specifically, we anneal the timestep to con-\ntrol the noise added to the NeRF\u2019s output. Another strategy\ninvolves using negative prompts to guide the training pro-\ncess. More details will be provided in the supplementary\nmaterials.\n4.2. Baselines\nWe select four state-of-the-art works to compare with our\nmethod. (1) DreamFusion [26] is the first work to use the\ntext-to-image diffusion model to optimize a 3D object. (2)\nProlificDreamer [43] introduces a new method, VSD, for\noptimizing a NeRF. VSD combines the vanilla Stable Diffu-\nsion model with the LORA [11] model to jointly optimize a\nNeRF, resulting in the alleviation of oversaturation phenom-\nena. (3) Text2Room [10] is another method that leverages a\n2D diffusion model to generate 3D scenes. It utilizes the\nStable Diffusion inpainting model to generate 2D images\nsequentially. (4) Look Outside the Room [30] is a method\nthat generates novel view images based on previously gen-\nerated images and camera poses. It trains an autoregressive\ntransformer model from scratch using the Matterport3D [3]\ndataset and RealEstate [48] dataset.\n4.3. Qualitative Results\nIn Figure 11, we present RGB rendering images of scenes\nwith the crucial coordinate information for our method and\n6\nz = -1.5\nz = -1\nz = 1\nz = 1.5\nRadius =1.5\nElevation = 30\nRadius = 0\nElevation = 30\nRadius = 1.5\nRadius = 1.5\nElevation = -20\nHeight = 1\nElevation = -20\nHeight = 1\nElevation = 60\nRadius = 1.5\nElevation = -30\nRadius =1.5\nAzimuth = 0\nAzimuth = 90\nAzimuth = 180\nAzimuth = 270\nIn traditional, old, wooden style, a versatile room with a sofa as the centerpiece, accompanied by various elements such as plants, a bookshelf, a lamp, a TV, games, a desk, a \ndining table, chairs, art and a meditation corner, providing a space for relaxation, reading, entertainment, work, gatherings, creativity, and tranquility.\nFigure 5. More views of our result.\nMetrics\nHuman Preference\nCLIP Score (\u2191) Aesthetic Score (\u2191)\nTextual Alignment (\u2191) Consistency (\u2191) Overall Quality (\u2191)\nDreamFusion [26]\n23.56\n4.65\n2.75\n2.80\n2.55\nProlificDreamer [43]\n22.45\n4.98\n3.10\n3.12\n2.87\nText2Room [10]\n20.41\n5.21\n3.34\n2.97\n3.20\nLOTR [30]\n13.12\n4.23\n1.25\n1.21\n1.78\nShowRoom3D (Ours)\n25.62\n5.56\n4.55\n4.89\n4.59\nTable 1. Quantitative comparisons of ShowRoom3D against state-of-the-art approaches.\nbaselines. The initial four images depict a panoramic view\nof the entire room, followed by four additional perspec-\ntives in the subsequent set of images. DreamFusion and\nProlificDreamer struggle to generate the correct room struc-\nture, exhibiting issues such as the Janus problem and style\ninconsistency. This results in blurriness in certain views.\nText2Room results in view inconsistencies, stretching, and\nblurring due to its inpainting method. Meanwhile, LOTR\nfails to create panoramic scenes and exhibits poor content\nconsistency between frames. Additionally, we present ad-\nditional views of the rooms generated by our results in Fig-\nure 5. Displaying 16 views of a room, it demonstrates that\nour method produces not only high-quality rooms but can\nalso render in a larger space. We have briefly annotated the\ncrucial coordinate information, and further results compar-\ning baselines with our method can be found in the supple-\nmentary material.\n4.4. Quantitative Results\nWe compute the CLIP Score [29] and aesthetic score [34]\nfor 120 RGB renderings of each scene and conduct a user\nstudy for scene evaluation. The CLIP score is employed\nto assess the alignment between rendered views and pro-\nvided prompts. Additionally, the aesthetic score introduced\nby LAION measures the aesthetic quality of the generated\nimages. Recent methods [25] have demonstrated its authen-\nticity, surpassing the reliability of FID. We present quantita-\ntive results, averaged across multiple scenes, in Table 1. In\nthis table, we observe that our method achieves the highest\naveraged CLIP score which means our method can generate\nscenes that closely align with user prompts and the highest\naveraged aesthetic score which means our results maintain\nthe highest quality.\nAdditionally, in our user study, we leverage Amazon\nMTurk 1to recruit 24 participants to rank the results ob-\ntained from our methods and other baselines. The rendered\nviews from the generated 3D scene are evaluated across\nthree aspects: overall quality, text alignment, and style con-\nsistency. Scores are calculated with a rating of 5 for the\nbest-ordered view and 1 for the last. In the end, we gather\na total of 259 data points to calculate the final scores. As\nillustrated in Table 1, we outperform other methods by a\nsubstantial margin, indicating our superior performance in\nterms of overall quality, text alignment, and consistency.\n1https://requester.mturk.com/\n7\n(c) w/o the second stage\n(d) w/o pose\ntransformation\n(e) w/o the third stage\n(f) full\n(b) only one stage\nIn realistic style, a versatile room with a sofa as the centerpiece, accompanied by various elements such as plants, a bookshelf, a lamp, a TV, games, a desk, a \ndining table, chairs, art and a meditation corner, providing a space for relaxation, reading, entertainment, work, gatherings, creativity, and tranquility.\n(a) w/o CAA module\nFigure 6. Ablation study on each proposed component. We choose two different views of a room. (a) We use the SD(finetuned on\nMatterport3D) model without CAA module to follow our three-stage pipeline. The quality of the room is also high but there are many\nrepetitive contents in different views. Results in (b), (c), (d), and (e) demonstrate the impact of removing each corresponding component,\nshowcasing varying degrees of quality degradation. Finally, (f) presents the result of our full method.\nMetrics\nCLIP Score (\u2191) Aesthetic Score (\u2191)\nOurs w/o CAA module\n28.23\n5.62\nOurs only one stage\n14.48\n4.91\nOurs w/o the second stage\n27.94\n5.21\nOurs w/o pose transformation\n27.63\n5.31\nOurs w/o the third stage\n26.43\n5.44\nShowRoom3D (Ours full)\n28.82\n5.59\nTable 2. Quantitative ablations of ShowRoom3D.\n4.5. Ablations\nOur method\u2019s key components include three-stage training\nand pose transformation in the second stage. We also calcu-\nlate CLIP Score and aesthetic score of every ablation part,\nas shown in Table 2. Our method get the highest CLIP Score\nand the second highest aesthetic score. Next we will illus-\ntrate each component.\nThe Effect of CAA Module. We just use the Stable Diffu-\nsion(finetuned on the Matterport3D dataset) without CAA\nmodule as a prior to follow our three-stage pipeline. In Fig-\nure 6(a), we investigate the impact of the correspondence\nattention module. It is evident that the use of the correspon-\ndence attention module mitigates the Janus problem and\nimproves content diversity of the room in our method, as\nshown in Figure 6(f).\nThe scores of our method are similar to those without the\nCAA module, proving the utility of our three-stage training\npipeline for both MVDiffusion and Stable Diffusion. While\nour method\u2019s aesthetic score is slightly lower than without\nthe CAA module, this discrepancy may be attributed to the\nlimitations of the two metrics used, as they cannot effec-\ntively assess the Janus problem.\nThe Effect of Training with Only One Stage. In Fig-\nure 6(b), we investigate the impact of three-stage training.\nIt is evident that if we employ the SDS method to optimize\nthe NeRF with only one stage, it struggles to generate mean-\ningful content. This is primarily due to the CAA module\u2019s\ninability to correctly handle the camera pose, resulting in\nfailed training.\nThe Effect of the Second Stage.\nAs depicted in Fig-\nure 6(c), if we train the NeRF model without the second\nstage and proceed directly to the third stage after the first\nstage\u2019s training, the geometric quality deteriorates. Some\nfurniture pieces may not be generated effectively, resulting\nin suboptimal wall and shelf shapes, as illustrated in Fig-\nure 6(c).\nThe Effect of Pose Tranformation. In the Figure 6(d),\nwe explore the consequences of further NeRF optimiza-\ntion without pose transformation. The results indicate that\ncontinued NeRF optimization can lead to training instabil-\nity and sometimes the training will fail because the noise\nfrom MVDiffusion model will not provide the accurate\nview guidance.\nThe Effect of the Third Stage. As depicted in Figure 6(e),\nwe investigate the significance of the third stage. Without\nthe third training stage, the quality of rendered views in this\nroom deteriorates because in the first two stages, the camera\nposes are not randomly sampled, resulting in some missing\n8\nviews that are left uncovered.\n5. Conclusion\nIn this work, we introduce ShowRoom3D, a three-stage\npipeline using a 3D diffusion prior, MVDiffusion, to op-\ntimize NeRF for the generation of high-quality 3D room-\nscale scenes. We employ progressive view selection ap-\nproach in three stages. During the second stage, we uti-\nlize pose transformation to ensure accurate guidance from\nMVDiffusion.\nAs a result, we can produce high-quality\nroom-scale scenes which can be rendered at any position.\nLimitations. Our approach enables the generation of high-\nquality 3D room-scale scenes from texts. However, there\nare some limitations.\nFirstly, similar to previous meth-\nods, our approach produces oversaturated results due to the\nSDS loss, despite employing certain training techniques to\nalleviate this occurrence. Secondly, our method is time-\nconsuming due to the three-stage training process.\nAcknowledgement. We thank Ziteng Gao, Hai Ci and Yi-\nquan Chen for their helpful discussions.\nReferences\n[1] Mohammadreza Armandpour, Ali Sadeghian, Huangjie\nZheng, Amir Sadeghian, and Mingyuan Zhou. Re-imagine\nthe negative prompt algorithm:\nTransform 2d diffusion\ninto 3d, alleviate janus problem and beyond.\nCoRR,\nabs/2304.04968, 2023. 3\n[2] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J. Guibas, Jonathan Tremblay, Sameh Khamis,\nTero Karras, and Gordon Wetzstein.\nEfficient geometry-\naware 3d generative adversarial networks. In CVPR, pages\n16102\u201316112. IEEE, 2022. 3\n[3] Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Ma-\nciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang. Matterport3d: Learning from\nRGB-D data in indoor environments. In 3DV, pages 667\u2013\n676. IEEE Computer Society, 2017. 2, 3, 6, 14\n[4] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja\nGiryes, and Daniel Cohen-Or. Set-the-scene: Global-local\ntraining for generating controllable nerf scenes.\nCoRR,\nabs/2303.13450, 2023. 3\n[5] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas A. Funkhouser, and Matthias Nie\u00dfner. Scan-\nnet: Richly-annotated 3d reconstructions of indoor scenes.\nIn CVPR, pages 2432\u20132443. IEEE Computer Society, 2017.\n3\n[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, pages 13142\u2013\n13153. IEEE, 2023. 3\n[7] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGRAM: generative radiance manifolds for 3d-aware image\ngeneration. In CVPR, pages 10663\u201310673. IEEE, 2022. 3\n[8] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation. CoRR,\nabs/2302.01133, 2023. 2, 3\n[9] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 6\n[10] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin John-\nson, and Matthias Nie\u00dfner.\nText2room: Extracting tex-\ntured 3d meshes from 2d text-to-image models.\nCoRR,\nabs/2303.11989, 2023. 2, 3, 6, 7, 12\n[11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nICLR. OpenReview.net, 2022. 3, 6, 12\n[12] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 3\n[13] Jing Yu Koh, Harsh Agrawal, Dhruv Batra, Richard Tucker,\nAustin Waters, Honglak Lee, Yinfei Yang, Jason Baldridge,\nand Peter Anderson. Simple and effective synthesis of indoor\n3d scenes. In AAAI, pages 1169\u20131178. AAAI Press, 2023. 3\n[14] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, pages 300\u2013309. IEEE,\n2023. 2\n[15] Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin,\nHui Xiong, and Lin Wang. Componerf: Text-guided multi-\nobject compositional nerf with editable 3d scene layout.\nCoRR, abs/2303.13843, 2023. 3\n[16] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. Devrf: Fast deformable voxel radi-\nance fields for dynamic scenes. Advances in Neural Infor-\nmation Processing Systems, 35:36762\u201336775, 2022. 3\n[17] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao,\nYuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and\nMike Zheng Shou. Dynvideo-e: Harnessing dynamic nerf\nfor large-scale motion-and view-change human-centric video\nediting. arXiv preprint arXiv:2310.10624, 2023. 3\n[18] Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Zhongcong Xu,\nJussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng\nShou.\nHosnerf: Dynamic human-object-scene neural ra-\ndiance fields from a single video.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 18483\u201318494, 2023. 3\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. CoRR, abs/2303.11328,\n2023. 3\n[20] Aryan Mikaeili, Or Perel, Daniel Cohen-Or, and Ali\nMahdavi-Amiri. SKED: sketch-guided text-based 3d edit-\ning. CoRR, abs/2303.10735, 2023. 3\n9\n[21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV (1), pages 405\u2013421. Springer, 2020. 2, 3, 6,\n12\n[22] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 4\n[23] Michael Niemeyer and Andreas Geiger. GIRAFFE: repre-\nsenting scenes as compositional generative neural feature\nfields.\nIn CVPR, pages 11453\u201311464. Computer Vision\nFoundation / IEEE, 2021. 3\n[24] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, pages 5845\u20135854. IEEE, 2021. 3\n[25] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models\nfor high-resolution image synthesis. CoRR, abs/2307.01952,\n2023. 7\n[26] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR.\nOpenReview.net, 2023. 2, 3, 6, 7, 12\n[27] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. In CVPR, pages 10318\u201310327. Computer\nVision Foundation / IEEE, 2021. 3\n[28] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\nCoRR,\nabs/2306.17843, 2023. 3\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, pages\n8748\u20138763. PMLR, 2021. 7\n[30] Xuanchi Ren and Xiaolong Wang. Look outside the room:\nSynthesizing A consistent long-term 3d scene video from A\nsingle image. In CVPR, pages 3553\u20133563. IEEE, 2022. 3, 6,\n7, 12\n[31] Robin\nRombach,\nPatrick\nEsser,\nand\nBj\u00a8orn\nOmmer.\nGeometry-free view synthesis: Transformers and no 3d pri-\nors. In ICCV, pages 14336\u201314346. IEEE, 2021. 3\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10674\u2013\n10685. IEEE, 2022. 2, 3, 12\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In NeurIPS, 2022. 2\n[34] Christoph Schuhmann. Clip+mlp aesthetic score predictor.\nhttps://github.com/christophschuhmann/\nimproved-aesthetic-predictor, 2023. 7\n[35] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. GRAF: generative radiance fields for 3d-aware im-\nage synthesis. In NeurIPS, 2020. 3\n[36] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects. CoRR, abs/2303.12048, 2023. 3\n[37] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In NeurIPS,\npages 6087\u20136101, 2021. 2\n[38] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. CoRR, abs/2308.16512, 2023. 3, 6\n[39] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, pages 5449\u20135459. IEEE, 2022. 3\n[40] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\nCoRR, abs/2307.01097, 2023. 2, 3, 4, 6, 12\n[41] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-\nBin Huang, and Johannes Kopf. Consistent view synthesis\nwith pose-guided diffusion models. In CVPR, pages 16773\u2013\n16783. IEEE, 2023. 3\n[42] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\npages 12619\u201312629. IEEE, 2023. 2\n[43] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. CoRR, abs/2305.16213, 2023. 2, 3, 6, 7, 12\n[44] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern Recognition, pages\n16210\u201316220, 2022. 3\n[45] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. Synsin: End-to-end view synthesis from a single\nimage. In CVPR, pages 7465\u20137475. Computer Vision Foun-\ndation / IEEE, 2020. 3\n[46] Eric Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Wen-\nqing Zhang, Song Bai, Jiashi Feng, and Mike Zheng Shou.\nPV3D: A 3d generative model for portrait video generation.\nIn ICLR. OpenReview.net, 2023. 3\n[47] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing\nLiao. Text2nerf: Text-driven 3d scene generation with neural\nradiance fields. CoRR, abs/2305.11588, 2023. 3\n[48] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: learning view syn-\nthesis using multiplane images. ACM Trans. Graph., 37(4):\n65, 2018. 3, 6\n10\n[49] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d\nwith advanced diffusion guidance. CoRR, abs/2305.18766,\n2023. 2, 3\n[50] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. CoRR, abs/2306.13455, 2023. 3\n11\nShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors\nSupplementary Material\nThis supplementary mainly includes the implementation\ndetails of ShowRoom3D and other baselines, more compar-\nisions of ShowRoom3D against other baselines and more\nablation results.\nFurthermore, we also provide a supplementary video\nto show 360\u00b0 free-viewpoint renderings of the rooms\nfrom ShowRoom3D and the comparisons of ShowRoom3D\nagainst baselines.\nA. More Implementation Details of Show-\nRoom3D\nWe align our coordinates with MVDiffusion [40] by adopt-\ning the same right-handed world coordinate system and\ncamera coordinate system. Specifically, the x-axis faces for-\nward, the z-axis faces right, and the y-axis faces up. We also\ntake the annealing time tragedy to alleviate the oversatura-\ntion phenomenon. In the first stage, the maximum time step\nis reduced from 0.6 to 0.02, while the minimum timestep\nis adjusted from 0.98 to 0.7. In the second stage, for the\nfirst 10,000 iterations, the maximum and minimum timestep\nvalues are set to 0.7 and 0.02, respectively. For the subse-\nquent 5,000 iterations and during the third stage, the max-\nimum and minimum timestep values become 0.4 and 0.02.\nWe also utilize negative prompts to guide the optimization\nof NeRF. The negative prompt includes descriptors such\nas \u2018ugly, bad anatomy, blurry, pixelated, obscure, unnat-\nural colors, poor lighting, dull, unclear, cropped, lowres,\nlow quality, artifacts, duplicate, morbid, mutilated, poorly\ndrawn face, deformed, dehydrated, bad proportions.\u2019\nB. More Implementation Details of Baselines\nDreamFusion [26] and ProlificDreamer [43]. To ensure a\nfair comparison with our method, we adopt the same right-\nhanded world coordinate system and align the camera coor-\ndinate system accordingly. In this setup, the x-axis points\nforward, the z-axis points right, and the y-axis points up.\nWhile DreamFusion initially operates in an object-centered\ncamera system for 3D object generation, we modify it to a\ncamera-centered system for consistency.\nText2Room [10]. We adhere to the Text2Room pipeline to\ngenerate the room mesh. Subsequently, we employ Poisson\nsurface reconstruction in Meshlab, instead of Python, for\nincreased efficiency in rendering the mesh and comparing\nthe rendered images with our results.\nLook Outside The Room(LOTR) [30]. Because the LOTR\ndo not generate the novel view images according to the\nuser\u2019s input, we use Stable Diffusion to generate the first\nIn realistic style, a versatile room with a sofa as the centerpiece, \naccompanied by various elements such as plants, a bookshelf, a lamp, \na TV, games, a desk, a dining table, chairs, art and a meditation \ncorner, providing a space for relaxation, reading, entertainment, \nwork, gatherings, creativity, and tranquility.\n(a) SDS only 1stage\n(b) full\nFigure 7. Ablation study on one more component.\nimage and use the first image and the corresponding camera\nposes to generate the following images.\nC. More Comparisons with Baselines\nWe can see the results in Figure 11. We also compare the\ntraining time and inference time of our method with other\nbaselines, as depicted in the table.\nDreamFusion.\nDreamFusion utilizes the vanilla Stable\nDiffusion [32] to distill NeRF [21] in a single stage, lead-\ning to several disadvantages. Firstly, the model lacks prior\nknowledge about 3D indoor scenes, resulting in a severe\nJanus problem during room generation. As illustrated in\nFigure 11, there are numerous repetitive contents between\nviews, creating a disjointed appearance that does not resem-\nble a cohesive indoor scene but rather a combination of 2D\nimages. Secondly, due to training in a single stage, the room\nstructure is improper, with furniture and the ceiling not con-\nsistently placed at the same level.\nProlificDreamer. ProlificDreamer shares the same disad-\nvantages with DreamFusion. ProlificDreamer incorporates\nthe LORA [11] model for joint training with NeRF to ad-\n12\nz=-1.5\nz=-1\nz=1\nz=1.5\nRadius = 1.5\nElevation=30\nRadius = 0\nElevation=30\nRadius = 1.5\nRadius = 1.5\nElevation=-20\nheight=1\nElevation=-20\nheight=1\nElevation=60\nRadius = 1.5\nElevation=-30\nRadius =1.5\nAzimuth = 0\nAzimuth = 90\nAzimuth = 180\nAzimuth = 270\nIn lavender style, a versatile room with a sofa as the centerpiece, accompanied by various elements such as plants, a bookshelf, a lamp, a TV, games, a desk, a dining table, \nchairs, art and a meditation corner, providing a space for relaxation, reading, entertainment, work, gatherings, creativity, and tranquility.\nFigure 8. Our more results with 16 views\nDreamFusion ProlificDreamer Text2Room LOTR ShowRoom3D\nTraining Time\n1h30min\n7h20min\n2h\n\u2212\u2212\n9h30min\nInference Time\n26s\n31s\n3min17s\n10min\n27s\nTable 3. Time comparisons of ShowRoom3D and other state-of-the-art approaches.\nFigure 9. User study interface.\ndress oversaturation and enhance content diversity within\na single image. However, this approach results in gener-\nating more crowded content in one view, exacerbating the\nJanus problem. The attempt to maintain consistency with\nthe prompt in each image intensifies the overcrowded ap-\npearance of the room. Additionally, the increased diversity\nin every image can lead to more inconsistencies in style be-\ntween views.\nText2Room.\nText2Room shares the same disadvantages\nwith the aforementioned baselines. Text2Room employs\nvarious strategies to fill the generated mesh and ensure its\n\u2018waterproof\u2019 quality, including random camera sampling\nand Poisson Reconstruction. However, this approach intro-\nduces more stretching and blurring artifacts in the rendered\nimages.\nLook Outside The Room(LOTR). LOTR is a novel view\nsynthesis work to generate the next image conditioned on\nthe previous images. However, it has certain disadvantages.\nFirstly, it struggles to generate images when the rotation de-\ngree varies too much, making it ineffective for panoramic\nimage generation. Secondly, LOTR is constrained to gen-\nerating images in specific directions, and it can not produce\ncorresponding images if the camera is moved backward, up,\nor down.\nD. More Results of ShowRoom3D\nIn this section, we present an additional set of 16 views for\na scene to illustrate that our room can be rendered at any\nposition as shown in Figure 8. Furthermore, we provide\nmore results with 8 views to demonstrate that our method\nis capable of generating diverse types of rooms as shown in\nFigure 10.\nE. User Study\nWe employ our method and other baselines to generate 13\nroom-scale scenes based on 13 prompts. To ensure fairness,\nwe utilize Amazon MTurk to recruit 24 participants who\n13\nAn animated room brimming with whimsical details, including a quirky bookshelf, a magical lamp, a playful computer, a desk having a mind of its own, a sofa with a welcoming \nsmile, chairs dancing, art and paintings that tell whimsical tales a dining table with a feast for the eyes, and windows that frame a colorful cartoon world outside.\nA living room filled with, furniture and some large mirrors, tables and chandelier.\nAn old traditional room with a cozy fireplace, an attic filled with treasures, a warm lamp, a well-stocked bookshelf, a vintage desk, a comfortable sofa, inviting chairs, \ncaptivating art and paintings, a serene yoga mat\nAzimuth: 0\nElevation: 0\nAzimuth : +90\nElevation : 0\nAzimuth : +180\nElevation : 0\nAzimuth : +270\nElevation : 0\nAzimuth : +90\nElevation : +20\nAzimuth : +90\nElevation : -20\nz = -1\nz = 1\nFigure 10. Our more results with 8 views\nrank the results on a scale from 5 (highest score) to 1 (lowest\nscore), as depicted in Figure 9. Users are presented with\nmultiple images from each scene.\nF. Additional Ablation Study\nWe show another ablation study about SDS with our train-\ning tricks.\nStable Diffusion In One Stage. Now we show the results\nof Stable Diffusion (finetuned on Matterport3D dataset [3])\nin one stage. The results will be shown in Figure 7. In\nFigure 7(a), we optimize the NeRF using the Stable Dif-\nfusion model (finetuned on the MatterPort3D dataset) in a\nsingle stage, incorporating all training tricks. Despite the\ninclusion of additional training tricks, proper room geom-\netry generation remains elusive. This will demonstrate the\neffectiveness of our three-stage training pipeline in enhanc-\ning the geometry of the room, as shown in Figure 7(b).\n14\nA living room filled with furniture, grand pianos, fire places, paintings, large windows. A living room with couches and ceiling fans.\nDreamFusion\nProlificDreamer\nOurs\nText2Room\nLOTR\nAzimuth: 0\nElevation: 0\nAzimuth : +90\nElevation : 0\nAzimuth : +180\nElevation : 0\nAzimuth : +270\nElevation : 0\nAzimuth : +90\nElevation : +20\nAzimuth : +90\nElevation : -20\nz = -1\nz = 1\nDreamFusion\nProlificDreamer\nText2Room\nOurs\nIn lavender style, a versatile room with a sofa as the centerpiece, accompanied by various elements such as plants, a bookshelf, a lamp, a TV, \ngames, a desk, a dining table, chairs, art and a meditation corner, providing a space for relaxation, reading, entertainment, work, gatherings, \ncreativity, and tranquility.\nLOTR\nAzimuth: 0\nElevation: 0\nAzimuth : +90\nElevation : 0\nAzimuth : +180\nElevation : 0\nAzimuth : +270\nElevation : 0\nAzimuth : +90\nElevation : +20\nAzimuth : +90\nElevation : -20\nz = -1\nz = 1\nFigure 11. Qualitative comparisons of ShowRoom3D and state-of-the-art approaches.\n15\n"
  },
  {
    "title": "DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular Video",
    "link": "https://arxiv.org/pdf/2312.13528.pdf",
    "upvote": "6",
    "text": "DyBluRF: Dynamic Deblurring Neural Radiance Fields\nfor Blurry Monocular Video\nMinh-Quan Viet Bui1*\nJongmin Park1*\nJihyong Oh2*\nMunchurl Kim1\u2020\n1KAIST\n2Chung-Ang University\n{bvmquan, jm.park, mkimee}@kaist.ac.kr\njihyongoh@cau.ac.kr\nhttps://kaist-viclab.github.io/dyblurf-site/\nFigure 1. Dynamic deblurring novel view synthesis results. We first propose a novel dynamic deblurring NeRF for blurry monocular\nvideos with inaccurate camera poses, called DyBluRF, which significantly outperforms previous SOTA NeRF methods for deblurring static\nscenes (BAD-NeRF [59]) and monocular videos (HexPlane [7], HyperNeRF [42]), trained on the newly synthesized Blurry iPhone Dataset.\nAbstract\nVideo view synthesis, allowing for the creation of vi-\nsually appealing frames from arbitrary viewpoints and\ntimes, offers immersive viewing experiences. Neural radi-\nance fields, particularly NeRF, initially developed for static\nscenes, have spurred the creation of various methods for\nvideo view synthesis. However, the challenge for video view\nsynthesis arises from motion blur, a consequence of ob-\nject or camera movement during exposure, which hinders\nthe precise synthesis of sharp spatio-temporal views.\nIn\nresponse, we propose a novel dynamic deblurring NeRF\nframework for blurry monocular video, called DyBluRF,\nconsisting of an Interleave Ray Refinement (IRR) stage and\na Motion Decomposition-based Deblurring (MDD) stage.\nOur DyBluRF is the first that addresses and handles the\nnovel view synthesis for blurry monocular video. The IRR\nstage jointly reconstructs dynamic 3D scenes and refines the\ninaccurate camera pose information to combat imprecise\npose information extracted from the given blurry frames.\nThe MDD stage is a novel incremental latent sharp-rays\nprediction (ILSP) approach for the blurry monocular video\nframes by decomposing the latent sharp rays into global\ncamera motion and local object motion components. Ex-\ntensive experimental results demonstrate that our DyBluRF\noutperforms qualitatively and quantitatively the very re-\ncent state-of-the-art methods. Our project page including\nsource codes and pretrained model are publicly available at\nhttps://kaist-viclab.github.io/dyblurf-\nsite/.\n1. Introduction\nFree viewpoint rendering for spatio-temporal novel view\nsynthesis has increased lots of interests due to its diverse ap-\nplications. Especially, video view synthesis can render visu-\n*Co-first authors (equal contribution).\n\u2020Corresponding author.\narXiv:2312.13528v1  [cs.CV]  21 Dec 2023\nally pleasing frames at arbitrary camera viewpoints and time\ninstances. After the first advent of neural radiance fields\nfor a static scene, called NeRF [35], diverse types of neural\nrendering methods [26, 47, 58, 62] for video view synthe-\nsis have actively been developed. Multi-view-based meth-\nods [6, 11, 26, 37, 69, 72] have been traditionally adopted\nfor free viewpoint rendering of dynamic videos.\nHow-\never, they need a synchronized capturing process for multi-\ncamera systems which are impractical for general users. To\novercome this, several methods of dynamic view synthesis\n[2, 17, 18, 27, 28, 44, 51, 56] for a casually captured monoc-\nular video have been proposed for easier applications.\nOn the other hand, motion blur arises as a consequence\nof either object motion [38, 68] or camera shake [3, 67]\ncaused by the accumulation of light during video acquisi-\ntion [20, 21, 36, 55]. Therefore, synthesizing sharp novel\nspatio-temporal views from monocular video is faced with\nseveral challenges when blurriness presents in the given\nframes due to the camera capture process: (i) One straight-\nforward solution is to apply 2D video deblurring [40, 71] as\na preprocessing step to the given blurry frames before op-\ntimizing video NeRFs. However, this naive approach has\na significant drawback as independently deblurring frames\nin the pixel domain can introduce inconsistent geometry\nin 3D space [24, 25], which cannot be corrected through\nvideo NeRF optimization; (ii) Although several deblurring\nNeRFs [24, 25, 33, 59] have been developed to address\nblurriness in static multi-view images, they encounter dif-\nficulties in capturing temporal information when extended\nto blurry monocular videos. This is due to the absence of\na motion-aware deblurring module along the temporal di-\nmension. Additionally, the existing state-of-the-art (SOTA)\nmonocular video NeRF methods [7, 15, 42] cannot be di-\nrectly applied to deblur NeRFs from given blurry frames\nbecause they lack an effective deblurring component; (iii)\nThe accuracy of camera poses extracted by Structure-from-\nMotion (SfM) algorithms from blurry monocular videos,\nincluding deformable objects, is particularly low, leading\nto challenges in detecting and matching salient keypoints\n[25]. However, the previous bundle-adjustment for NeRFs\n[30, 43, 61] have primarily focused on rigid scenes.\nTo address these challenges, we first propose a novel dy-\nnamic deblurring NeRF for blurry monocular video, called\nDyBluRF, and our contributions are as follows:\n\u2022 We firstly point out the issue of handling blurry video\nof dynamic scenes for 3D reconstruction, and propose a\nnovel framework, called DyBluRF, which can effectively\nrender the sharp novel spatio-temporal views from blurry\nmonocular videos with imprecise camera poses;\n\u2022 We propose a novel Interleave Ray Refinement (IRR)\nstage which simultaneously performs the reconstruction\nof dynamic 3D scenes and the refinement of camera pose\nto overcome inaccurate camera pose information;\n\u2022 We propose a novel Motion Decomposition-based De-\nblurring (MDD) stage which includes a novel incremen-\ntal latent sharp-rays prediction (ILSP) approach to effec-\ntively handle the blurriness due to global camera and local\nobject motions in the monocular videos;\n\u2022 For experiments, we synthesize a new blurry version of\niPhone dataset [19] which is used to train the DyBluRF\nand other methods under fair comparison.\nThe exper-\nimental results demonstrate that our DyBluRF achieves\nsuperior performance compared to the previous SOTA\nmethods qualitatively (Fig.1) and quantitatively. Notably,\nthe DyBluRF trained using the blurry dataset even shows\ncomparable results with the SOTA methods trained on\nsharp dataset with accurate camera pose information.\n2. Related Work\n2.1. Conventional Video Deblurring\nMotion blur can be attributed to either objects in mo-\ntion [38, 68] or camera shake [3, 67], both of which stem\nfrom the gathering of light during video capture over the\nexposure time [20, 21, 36, 55]. Various deep learning meth-\nods [12, 29, 39, 60, 66, 70] have been developed for video\ndeblurring. However, naively applying conventional video\ndeblurring as a preprocessing step to the given blurry frames\nbefore optimizing video NeRFs induces inconsistent ge-\nometry in 3D space [24, 25], which cannot be corrected\nthrough video NeRF optimization.\n2.2. Deblurring NeRFs for Static Scenes\nTo produce visually appealing frames with consistent\n3D geometry from blurry multi-view static images, sev-\neral NeRF [35]-based methods have emerged. DeblurNeRF\n[33] employs an end-to-end volume rendering framework\n[13] to estimate spatial blur kernels at the pixel level and\nthe latent sharp radiance fields. BAD-NeRF [59] jointly\npredicts the virtual camera trajectories during the image\nexposure capture time. DP-NeRF [24] introduces a rigid\nblurring kernel to maintain 3D consistency by leveraging\nphysical constraints. ExBluRF [25] introduces an MLP-\nbased framework for reducing the dimensionality of 6-DOF\ncamera poses and employing a voxel-based radiance field\n[8, 16]. Nonetheless, none of the above methods can be ap-\nplicable for non-rigid video view synthesis due to the lack\nof motion-aware deblurring for the temporal dimension.\n2.3. NeRFs for Dynamic Scenes\nRecent methods for video view synthesis have expanded\nupon the static NeRF framework [35]. They represent dy-\nnamic NeRFs by incorporating scene flow-based frame-\nworks [18, 27, 28] or canonical fields [1, 15, 22, 32, 41,\n42, 47, 54, 56, 63, 65] to model non-rigid deformable\ntransformations or 4D spatio-temporal radiance fields [2,\n7, 14, 17, 18, 26, 27, 51, 57, 64]. The methods such as\nNSFF [27], DynamicNeRF [18], and DynIBaR [28] typi-\ncally combine two types of NeRFs: time-invariant and time-\nvariant, to generate novel spatio-temporal views for monoc-\nular videos. However, they rely heavily on pretrained mo-\ntion mask extraction for moving objects and various reg-\nularization losses for 3D scene flows, which makes them\nless effective in deblurring video view synthesis. The meth-\nods of D-NeRF [47], HyperNeRF [42], and TiNeuVox [15]\ninitially learn deformation or offset fields that transform\na ray in an observation space to a bent ray in a canoni-\ncal space. However, none of the existing SOTA monocu-\nlar video NeRF methods mentioned above can be readily\napplied for deblurred neural radiance fields from the given\nblurry frames due to the lack of an effective deblurring com-\nponent and robust pose optimization in the presence of cor-\nrupted pose information due to blurriness.\n2.4. NeRFs with Pose Optimization\nIn order to accurately capture fine details, NeRF requires\nprecise camera poses and stationary scenes during the cap-\nture process. However, in real-world scenarios, the cam-\nera poses obtained through SfM algorithms inherently con-\ntain pixel-level inaccuracies [23, 31, 49]. These inaccura-\ncies can be further exacerbated when dealing with blurry\nframes due to challenges in detecting and matching distinc-\ntive keypoints [25], especially in the presence of both global\ncamera and local object movements. Consequently, the ren-\ndered output of NeRF often exhibits significant misalign-\nment with the ground truth images. Several NeRF methods\n[9, 30, 34, 43, 61] have been developed to jointly optimize\nNeRF parameters and pose information. However, these\nmethods primarily concentrate on scenarios with rigid char-\nacteristics, which considerably differ from the non-rigid\nproperties frequently observed in blurry monocular videos.\n3. Proposed Method: DyBluRF\n3.1. Design Considerations\nWe firstly propose a novel dynamic deblurring NeRF,\ncalled DyBluRF, which aims to represent sharp dynamic\nneural radiance fields from blurry monocular videos. Dy-\nBluRF consists of two main procedures which are Inter-\nleave Ray Refinement (IRR) stage (Sec. 3.3) and Motion\nDecomposition-based Deblurring (MDD) stage (Sec. 3.4),\nas shown in Fig. 2 and Algo. 2.\nThe accuracy of pose information extracted by SfM algo-\nrithms [50] from blurry monocular videos with deformable\nobjects is especially compromised due to challenges in de-\ntecting and matching salient keypoints [25]. However, the\nprior bundle-adjustment approaches [30, 43, 61] only focus\non static environments. To solve this issue, in the IRR stage\n(Sec. 3.3), we coarsely reconstruct the dynamic radiance\nfields and optimize the imprecise camera poses. Specifi-\ncally, we propose an interleave optimization strategy (Algo.\n1) based on factorizing radiance fields into static and dy-\nnamic components. The interleave optimization enhances\nour DyBluRF\u2019s training stability while jointly learning the\n3D dynamic reconstruction and refining the camera poses.\nOn the other hand, the existing SOTA methods for de-\nblurring NeRFs [24, 25, 33, 59] only consider for static im-\nages so they encounter difficulties in capturing temporal in-\nformation when extended for blurry monocular videos due\nto the absence of a motion-aware deblurring module. Also,\nthe existing SOTA methods for monocular video view syn-\nthesis [7, 15, 42] are not capable of handling the input blurry\nframes due to the lack of deblurring component. To over-\ncome these limitations, we introduce the MDD stage (Sec.\n3.4), with a novel incremental latent sharp-rays prediction\n(ILSP) method which effectively synthesizes the physical\nblur process considering global camera motion and local\nobject motion in a progressive manner along temporal axis.\n3.2. Preliminaries\nDynamic Neural Radiance Fields. We extend the static\nNeRF model [35] to our DyBluRF for the monocular video\nwhich consists of one frame per time t.\nOur DyBluRF\nlearns to represent the continuous radiance of a video scene\nusing neural networks, taking into account a set of Nf\nframes from the monocular video, denoted as {It}Nf\nt=1,\nand the corresponding camera poses {Pt}Nf\nt=1. Following\n[18, 27, 28, 32], we decompose our radiance representation\ninto Static Net F\u03b8s and Dynamic Net F\u03b8d. Given a 3D po-\nsition x = (x, y, z) of each sampling point and a viewing\ndirection d of each ray, F\u03b8s : \u03b3(x, d) \u2192 (cs, \u03c3s, b) esti-\nmates a color cs, a volume density \u03c3s of a static scene com-\nponent and a volume density blending factor b from the spa-\ntial positional encoded inputs \u03b3(x, d). On the other hand,\nF\u03b8d : (\u03b3(x, d), l(t)) \u2192 (cd, \u03c3d) maps a time-varying em-\nbedding to a color cd and a volume density \u03c3d of a dynamic\nscene component where \u03b3 is the positional encoding [35]\nand l is Generative Latent Optimization (GLO) [5] to en-\ncode t. Let rp;t(s) = ot + sdp;t be the casted ray from a\ncamera origin ot through a given pixel p of the image plane\nat the time t where s and dp;t denote a sampling ray dis-\ntance and a viewing direction through the pixel p at time\nt, respectively. We separately estimate the rendered colors\n\u02c6Cs(rp;t) of the static scene component and \u02c6Cd(rp;t) of the\ndynamic scene component via continuous volume render-\ning [13] by computing the integral on N piecewise constant\nsegments {[sn, sn+1]}N\nn=1 along the ray rp;t as:\n\u02c6Cs(rp;t) =\nN\nX\nn=1\nT s\nn \u03b1s\nncs\nn, \u02c6Cd(rp;t) =\nN\nX\nn=1\nT d\nn \u03b1d\nncd\nn, (1)\nwhere Tn is the accumulated transmittance and \u03b1n is the\nalpha-compositing weight, which are defined as:\n\u03b1n = 1 \u2212 exp(\u2212\u03c3n\u03b4n),\nTn =\nn\u22121\nY\nk=1\n1 \u2212 \u03b1k,\n(2)\nwhere \u03b4n = sn+1 \u2212sn is the segment length. To predict the\nfull rendered color \u02c6Cfull(rp;t) of pixel p with camera pose\n(a) Interleave Ray Refinement (IRR) Stage (Sec. 3.3)\nPE\nInterleave Optim. (Sec 3.3.2 & Algo. 1)  \n(b) Motion Decomposition-based Deblurring (MDD) Stage (Sec. 3.4)\nRay Refinement\nPE\nScrew Axis\nLearn. Embed.\nRay Warp.       (Eq. 8)\nRay Warp.       (Eq. 8)\nLearn. Embed.\nc\nGCRP (Eq. 14) for all region\nLORR (Eq. 15) for dynamic region only\nDynamic NeRF Rendering\nDynamic NeRF Rendering\nDynamic NeRF Rendering\nPE\nPE\nStatic Net\nDynamic Net\n(Eq. 3)\n(Eq. 11)\nDynamic NeRF Rendering\n(Eq. 10)\n(Eq. 10)\n(Eq. 10)\n(Eq. 10)\n(Eq. 10)\n(Eq. 10)\n(Eq. 11)\n(Eq. 7)\n: Positional Encoding\n: Generative Latent Optimization\n: Vol. Rendering\n: Binary Motion Mask (Eq. 6)\n: Blurry GT color\n: Discretized Ray Embedding\nc : Channel-wise Concatenation\nAvg.\n: Vol. Density Blending Factor\nIncremental Latent Sharp-rays Pred. (ILSP) (for q-th latent sharp ray)\nFigure 2. Overview of our DyBluRF framework. To effectively optimize the sharp radiance field with the imprecise camera poses\nextracted from blurry video frames, we design our DyBluRF consisting of two main procedures (Algo. 2) of (a) Interleave Ray Refinement\nStage (Sec. 3.3 and Algo. 1) and (b) Motion Decomposition-based Deblurring Stage (Sec. 3.4).\nPt, our DyBluRF combines the outputs of F\u03b8s and F\u03b8d via\nthe volume density blending factor b as:\n\u02c6Cfull(rp;t) =\nN\nX\nn=1\nT full\nn\n(\u03b1s\nnbncs\nn + \u03b1d\nn(1 \u2212 bn)cd\nn), (3)\nwhere the full accumulated transmittance T full\nn\nis:\nT full\nn\n=\nn\u22121\nY\nk=1\n(1 \u2212 \u03b1s\nkbk)(1 \u2212 \u03b1d\nk(1 \u2212 bk)).\n(4)\nBinary Motion Mask Prediction.\nLearning the mo-\ntion decomposition has been widely adopted in previ-\nous works [18, 26, 28] to stabilize the reconstruction\nof static scene components in the dynamic NeRFs.\nIn\nour DyBluRF, motion decomposition is vital for both the\nIRR stage and the MDD stage.\nWe compute the mo-\ntion uncertainty Muncert(rp;t) by accumulating the alpha-\ncompositing weight \u03b1d\nn of the dynamic scene component by\n(1 \u2212 bn) along rp;t as:\nMuncert(rp;t) =\nN\nX\nn=1\nT full\nn\n\u03b1d\nn(1 \u2212 bn).\n(5)\nThe final binary motion mask M(rp;t) is obtained by\nthresholding Muncert(rp;t) as:\nM(rp;t) =\n(\n1,\nif\nMuncert(rp;t) > 0.5\n0,\notherwise\n,\n(6)\nDeblurring Neural Radiance Fields. To solve the blur\nproblem for reconstructing sharp radiance fields, we predict\nthe pixel-wise blur kernels and sharp pixel colors to simu-\nlate the physical blur process similar to the existing deblur-\nring static NeRF methods [24, 33, 59]. The physical blur\nprocess which generates a blurry color Bp;t of pixel p at\ntime t by applying an unknown motion blur kernel kp;t to\nthe sharp pixel color Ip;t is formulated as Bp;t = kp;t\u2217Ip;t\nwhere \u2217 indicates the convolution operation. We train our\nDyBluRF with the given blurry monocular video frames\n{Bt}Nf\nt=1 with inaccurate camera poses { \u02dcPt}Nf\nt=1. To op-\ntimize our DyBluRF by using the blurry frames, we model\nthe blur process for monocular dynamic radiance fields by\npredicting the set of latent sharp rays { \u02d9rp;t;q}Nb\nq=1 casting\nbased on the target ray rp;t. Then, we average the corre-\nsponding volume rendered pixel colors to generate a blurry\npixel color where q is the index and Nb is the number of\nlatent sharp rays, respectively. We denote this blur process\nas:\n\u02c6CB(rp;t) = A( \u02c6C(rp;t), { \u02c6C( \u02d9rp;t;q)}Nb\nq=1)\n=\n1\nNb + 1\n\u0010\n\u02c6C(rp;t) +\nNb\nX\nq=1\n\u02c6C( \u02d9rp;t;q)\n\u0011\n,\n(7)\nwhere \u02c6CB(rp;t) is a blurry rendered color of the ray rp;t\nand A(\u00b7, \u00b7) is an average function of the rendered color\n\u02c6C(rp;t) of the ray rp;t and the set of rendered colors\n{ \u02c6C( \u02d9rp;t;q)}Nb\nq=1 of the latent sharp rays { \u02d9rp;t;q}Nb\nq=1.\n3.3. Interleave Ray Refinement Stage\nIn the IRR stage, we coarsely train the Static and Dy-\nnamic Nets and optimize the imprecise camera poses via a\nlearnable embedding for a screw axis St in Eq. 8 from the\nblurry frames, as shown in Fig. 2-(a) and Algo. 1. We de-\nscribe our ray refinement and interleave optimization strat-\negy to jointly learn the 3D dynamic reconstruction and ad-\njust the camera pose estimates in the following subsections.\n3.3.1\nRay Refinement\nLet \u02dcrp;t be an inaccurate ray shooted from pixel p with\nan inaccurate camera pose \u02dcPt estimate at time t, we refine\nthe resulting inaccurate ray \u02dcrp;t to more accurate ray \u02c6rp;t\nby a ray warping W as:\n\u02c6rp;t = W(\u02dcrp;t, St) = e[\u03c9t]\u00d7 \u02dcrp;t + Gtvt,\n(8)\nwhere St = (\u03c9t; vt) \u2208 R6 is a learnable screw axis with\nthe corresponding rotation encoding \u03c9t \u2208 so(3) and trans-\nlation encoding vt at time t. Similar to the existing methods\n[24, 41], e[\u03c9t]\u00d7 and Gtvt are the residual rotation and trans-\nlation matrix, respectively, which are derived as:\ne[\u03c9t]\u00d7 = I + sin \u03b8\n\u03b8\n[\u03c9t]\u00d7 + 1 \u2212 cos \u03b8\n\u03b82\n[\u03c9t]2\n\u00d7,\nGt = I + 1 \u2212 cos \u03b8\n\u03b82\n[\u03c9t]\u00d7 + \u03b8 \u2212 sin \u03b8\n\u03b83\n[\u03c9t]2\n\u00d7,\n(9)\nwhere [w]\u00d7 is the cross-product matrix of a vector w and\n\u03b8 = \u2225\u03c9t\u2225 is the angle of rotation at time t. We model St as\na learnable embedding of time t, as shown in Fig. 2-(a).\n3.3.2\nInterleave Optimization\nJointly optimizing dynamic radiance fields and camera\nposes is highly ill-posed and can lead to bad local-minima.\nTo handle this, we propose a novel interleave optimization\nstrategy that alternatively optimizes the ray refinement and\nStatic and Dynamic Nets as described in Algo. 1 with di-\nverse losses in the followings:\nPhotometric Loss. To optimize the DyBluRF architecture\nstably, we minimize the photometric loss of our model for\nthe rendered colors. Given an input ray rp;t that can be\nwhether \u02c6rp;t or \u02d9rp;t;q in Sec.\n3.4, we render the color\n\u02c6Cd(rp;t) of dynamic scene component and the full ren-\ndered color \u02c6Cfull(rp;t). Then, we calculate the L2 loss\nbetween each rendered color \u02c6C(rp;t) with the blurry GT\ncolor Bp;t as:\nLphoto( \u02c6C(rp;t)) =\nX\nrp;t\n\u2225 \u02c6C(rp;t) \u2212 Bp;t\u22252\n2,\n(10)\nwhere \u02c6C(rp;t) can be \u02c6Cd(rp;t) or \u02c6Cfull(rp;t). For the ren-\ndered color \u02c6Cs(rp;t) of static scene component, we adopt\na masked photometric loss to prevent learning the dynamic\nscene component by using predicted M(rp;t) (Eq. 6) as:\nLmphoto( \u02c6Cs(rp;t)) =\nX\nrp;t\n\u2225( \u02c6Cs(rp;t)) \u2212 Bp;t) \u00b7 (1 \u2212 M(rp;t))\u22252\n2.\n(11)\nSurface Normal Loss. DynIBaR [28] applied a pre-trained\nmonocular depth estimation network to regularize NeRF. To\nmitigate the difficulty of scale-ambiguous depth estimation,\nFrom \nLiDAR\nFrom \nDPT\nFrom \nLiDAR\nFrom \nDPT\n: Depth\n: Surface Normal\nFigure 3. Surface Normal Loss. We compute the surface normal\nby first-order finite differences. As surface normal is invariant to\ndepth scale, we can use the surface normal loss to regularize NeRF\nwithout the need of depth alignment.\nDynIBaR [28] suggested aligning the pre-trained monocu-\nlar disparity with the SfM point cloud. However, the SfM\nreconstruction from the blurry video frames without motion\nsegmentation priors may even produce unreliable results.\nDifferent from DynIBaR [28], as shown in Fig. 3, we adopt\na surface normal supervision to regularize the density pre-\ndiction of Dynamic Net F\u03b8d. We compute the ground truth\nsurface normal np;t of pixel p = (pu, pv) at time t from the\npre-trained depth estimation model [48] using the first-order\nfinite differences. Then, we calculate the predicted surface\nnormal \u02c6np;t as:\n\u02c6np;t =\n\u0010\u2202rp;t(s\u2217)\n\u2202pu\n\u00d7 \u2202rp;t(s\u2217)\n\u2202pv\n\u0011\n,\n(12)\nwhere (w) is the normalization operation of a vector w and\ns\u2217 = PN\nn=1 T d\nn \u03b1d\nnsd\nn is the rendered ray distance for each\nray. Our surface normal loss Lsn is the mean square error\nbetween \u02c6np;t and np;t weighted by a constant \u03bbsn as:\nLsn = \u03bbsn\nX\nrp;t\n\u2225\u02c6np;t \u2212 np;t\u22252\n2\n(13)\nStatic Blending Loss. Unlike the existing dynamic NeRF\nmethods [18, 27, 28, 32] which rely on the pre-trained seg-\nmentation models to supervise the moving object region,\nour DyBluRF optimizes the binary motion mask prediction\nin an unsupervised manner. Since the Static and Dynamic\nNets exhibit disparate convergence speeds, the scene de-\ncomposition often tends to favor the dynamic components.\nTo tackle this issue, we introduce a static blending loss\nweighted by \u03bbsb as Lsb = \u2212\u03bbsb\nP\nrp;t log(b) to encourage\nthe blending factor b to maintain static characteristics.\nDistortion Loss. We adopt the distortion loss Ldist in Mip-\nNeRF 360 [4]. The distortion loss is applied to the den-\nsity estimations of static, dynamic and blending renderings,\nwhich are denoted as Ls\ndist, Ld\ndist and Lfull\ndist, respectively.\n3.4. Motion Decomposition-based Deblurring Stage\nIn the MDD stage, we first propose a novel incremental\nlatent sharp-rays prediction (ILSP) approach for the blurry\nmonocular video frames as shown in Fig. 2-(b) combined\nAlgorithm 1 Interleave Ray Refinement Stage (Fig. 2-(a))\n1: procedure IRR(F\u03b8s, F\u03b8d, St)\n2:\nfor it = 0 to 2 \u00d7 105 do\n3:\nSample random rays \u02dcrp;t\n4:\n\u02c6rp;t \u2190 W(\u02dcrp;t, St) (Eq. 8)\n5:\nif it|2 then\n\u25b7 Static Net and Ray Refine.\n6:\nCompute \u02c6\nCs(\u02c6rp;t) (Eq. 1)\n7:\nloss \u2190 Lmphoto( \u02c6\nCs(\u02c6rp;t)) + Ls\ndist (Eq. 11)\n8:\nUpdate Static Net F\u03b8s, St\n9:\nelse\n\u25b7 Dynamic Net and Static Net\n10:\nCompute \u02c6\nCs(\u02c6rp;t), \u02c6\nCd(\u02c6rp;t) (Eq. 1)\n11:\nCompute \u02c6\nCfull(\u02c6rp;t) (Eq. 3)\n12:\nloss \u2190Lmphoto( \u02c6\nCs(\u02c6rp;t))\n+Lphoto( \u02c6\nCd,full(\u02c6rp;t))\n+Ls,d,full\ndist\n+ Lsn\n(Eq. 10, 11, 13)\n13:\nUpdate Static Net F\u03b8s, Dynamic Net F\u03b8d\nwith Algo. 1, which is fully described in Algo. 2. This\napproach comprises a global camera-motion-aware ray pre-\ndiction and a local object-motion-aware ray refinement.\nGlobal Camera-motion-aware Ray Prediction (GCRP).\nTo model the camera motion blur process which occurs in\nboth static and dynamic scene components, we estimate\nmultiple latent sharp rays { \u02d9rg\np;t;q}Nb\nq=1 (Fig. 2-(b)) based on\nthe refined ray \u02c6rp;t (Eq. 8 and Fig. 2-(a)) as:\n{ \u02d9rg\np;t;q}Nb\nq=1 = {W(\u02c6rp;t, Sg\nt;q)}Nb\nq=1,\n(14)\nwhere W is defined in Eq. 8 and Sg\nt;q is a global camera-\nmotion-aware screw axis which is a learnable embedding of\nt for the q-th latent sharp ray \u02d9rg\np;t;q. The global camera-\nmotion-aware ray prediction (GCRP) maps the refined ray\n\u02c6rp;t that is the output of the IRR stage to Nb predicted la-\ntent sharp rays { \u02d9rg\np;t;q}Nb\nq=1 considering the global camera\nmotion (one-to-Nb mapping).\nLocal Object-motion-aware Ray Refinement (LORR).\nIf only a single motion-aware ray prediction, i.e., GCRP,\nis adopted for estimating latent sharp rays, the model tends\nto learn the outer mixture of diverse motions, combining\nglobal camera motion and local object motions. This may\nresult in unnatural artifacts such as afterimage spread over\nthe blurry training images. To delicately handle detailed\nmotions, we further decompose the blurry rays into local\nobject motions along the global camera motion by refining\nthe q-th predicted latent sharp ray \u02d9rg\np;t;q considering pixel-\nwise local object motion as:\n\u02d9rl\np;t;q = W( \u02d9rg\np;t;q, Sl\np;t;q),\n(15)\nwhere Sl\np;t;q = F\u03b8l(\u2308\u03d5( \u02d9rg\np;t;q), l(t)\u230b) is a local object-\nmotion-aware screw axis learned by a local object-\nmotion MLP F\u03b8l which takes a discretized ray embedding\n\u03d5( \u02d9rg\np;t;q) [46] and the encoded time l(t) as inputs. \u2308\u00b7\u230b refers\nto channel-wise concatenation. The local object-motion-\naware ray refinement (LORR) maps each predicted latent\nsharp ray \u02d9rg\np;t;q to a single corresponding refined latent\nsharp ray \u02d9rl\np;t;q considering the local object motion (one-\nto-one mapping). Specifically, the LORR is only applied to\nthe dynamic scene components which are indicated by the\nbinary motion mask M(\u02c6rp;t) (=1) as in Line 12 of Algo. 2.\nTo obtain the blurry color \u02c6CB, we apply Eq. 7 to pre-\ndicted latent sharp rays from Eq. 14 for the static scene\ncomponents or from Eq. 15 for the dynamic scene compo-\nnents as described in Algo. 2.\nAlgorithm 2 Overall DyBluRF Framework (Fig. 2)\n1: Init F\u03b8s, F\u03b8d, F\u03b8l, St, Sg\nt;q\n2: Do IRR(F\u03b8s, F\u03b8d, St) (Fig. 2-(a) and Algo. 1)\n3: for it = 0 to 105 do\n\u25b7 MDD (Sec. 3.4 and Fig. 2-(b))\n4:\nSample random rays \u02dcrp;t\n5:\n\u02c6rp;t \u2190 W(\u02dcrp;t, St) (Eq. 8)\n\u25b7 Freeze St\n6:\nCompute \u02c6\nCs,d,full(\u02c6rp;t), M(\u02c6rp;t) (Eq. 1, 3, 6)\n7:\n{ \u02d9rg\np;t;q}Nb\nq=1 \u2190 {W(\u02c6rp;t, Sg\nt;q)}Nb\nq=1 (Eq. 14)\n\u25b7 GCRP\n8:\nif M(\u02c6rp;t) = 0 then\n\u25b7 Only global camera motion\n9:\nfor q = 1 to Nb do\n10:\n\u02d9rp;t;q \u2190 \u02d9rg\np;t;q\n11:\nCompute \u02c6\nCs,d,full( \u02d9rp;t;q)\n12:\nelse if M(\u02c6rp;t) = 1 then\n\u25b7 Combine local object motion\n13:\nfor q = 1 to Nb do\n14:\nSl\np;t;q \u2190 F\u03b8l(\u2308\u03d5( \u02d9rg\np;t;q), l(t)\u230b)\n15:\n\u02d9rl\np;t;q \u2190 W( \u02d9rg\np;t;q, Sl\np;t;q) (Eq. 15)\n\u25b7 LORR\n16:\n\u02d9rp;t;q \u2190 \u02d9rl\np;t;q\n17:\nCompute \u02c6\nCs,d,full( \u02d9rp;t;q)\n18:\n\u02c6\nCs,d,full\nB\n(\u02c6rp;t)\n\u2190 A( \u02c6\nCs,d,full(\u02c6rp;t), { \u02c6\nCs,d,full( \u02d9rp;t;q)}Nb\nq=1)\n(Eq. 7)\n19:\nloss \u2190Lmphoto( \u02c6\nCs\nB(\u02c6rp;t)) + Lphoto( \u02c6\nCd,full\nB\n(\u02c6rp;t))\n+Ls,d,full\ndist\n+ Lsn\n20:\nUpdate Static Net F\u03b8s, Dynamic Net F\u03b8d, F\u03b8l, Sg\nt;q\n4. Experiments\nImplementation Details. We refer readers to the Supple-\nmental for the details of our implementation.\nBlurry iPhone Dataset.\nTo evaluate our DyBluRF for\ndeblurring novel view synthesis when only given blurry\nframes with imprecise camera poses, we synthesize a new\nblurry dataset with inaccurate camera information, called\nBlurry iPhone Dataset.\nThis dataset includes synthetic\nblurry RGB video frames {Bt}Nf\nt=1 with inaccurate cam-\nera pose information { \u02dcPt}Nf\nt=1 for the training dataset, and\nthe original sharp RGB video frames with accurate camera\npose information for the evaluation dataset, derived from\nthe iPhone dataset [19].\nTo generate these blurry RGB\ntraining video frames, we firstly employ a VFI method\n[53] to interpolate seven intermediate frames (\u00d78) between\neach pair of adjacent sharp video frames from the origi-\nMethod\nApple\nBlock\nPaper-windmill\nSpace-out\nTiNeuVox [15]\n13.53 / 0.680 / 0.723 / 1.704\n10.79 / 0.558 / 0.676 / 1.705\n14.15 / 0.273 / 0.781 / 4.108\n14.18 / 0.557 / 0.587 / 1.385\nHexPlane [7]\n16.80 / 0.715 / 0.523 / 1.239\n15.58 / 0.604 / 0.459 / 0.820\n17.11 / 0.352 / 0.422 / 0.318\n14.73 / 0.558 / 0.511 / 1.270\nT-NeRF [35]\n17.34 / 0.720 / 0.547 / 0.620\n16.48 / 0.644 / 0.411 / 0.795\n16.83 / 0.338 / 0.424 / 0.569\n16.23 / 0.561 / 0.436 / 1.329\nHyperNeRF [42]\n14.31 / 0.681 / 0.663 / 1.411\n16.12 / 0.642 / 0.416 / 0.958\n16.59 / 0.335 / 0.365 / 0.666\n17.79 / 0.631 / 0.332 / 0.402\nDP-NeRF [24]\n11.97 / 0.665 / 0.717 / 2.072\n9.96 / 0.514 / 0.729 / 1.602\n12.66 / 0.241 / 0.713 / 1.482\n13.15 / 0.532 / 0.628 / 0.639\nBAD-NeRF [59]\n12.29 / 0.668 / 0.744 / 1.743\n9.61 / 0.517 / 0.736 / 1.290\n5.98 / 0.033 / 0.961 / 0.978\n12.57 / 0.508 / 0.643 / 0.437\nDyBluRF (Ours)\n18.03 / 0.737 / 0.505 / 0.479\n17.35 / 0.660 / 0.361 / 0.735\n18.24 / 0.410 / 0.300 / 0.242\n18.99 / 0.646 / 0.328 / 0.276\nMethod\nSpin\nTeddy\nWheel\nAverage\nTiNeuVox [15]\n11.13 / 0.411 / 0.726 / 2.239\n10.28 / 0.496 / 0.834 / 1.304\n9.48 / 0.312 / 0.717 / 3.556\n11.93 / 0.470 / 0.721 / 2.286\nHexPlane [7]\n16.02 / 0.482 / 0.563 / 1.253\n12.84 / 0.497 / 0.587 / 1.220\n12.87 / 0.409 / 0.521 / 1.336\n15.14 / 0.517 / 0.512 / 1.065\nT-NeRF [35]\n17.16 / 0.503 / 0.534 / 1.162\n14.07 / 0.562 / 0.464 / 1.094\n14.93 / 0.499 / 0.379 / 1.360\n16.15 / 0.547 / 0.456 / 0.990\nHyperNeRF [42]\n16.39 / 0.498 / 0.499 / 1.277\n13.77 / 0.567 / 0.420 / 1.143\n12.11 / 0.393 / 0.435 / 1.739\n15.30 / 0.535 / 0.447 / 1.085\nDP-NeRF [24]\n10.65 / 0.404 / 0.730 / 1.956\n10.40 / 0.480 / 0.760 / 1.482\n9.26 / 0.299 / 0.736 / 2.561\n11.15 / 0.448 / 0.716 / 1.685\nBAD-NeRF [59]\n10.59 / 0.404 / 0.741 / 1.722\n9.77 / 0.457 / 0.758 / 1.537\n9.23 / 0.303 / 0.748 / 2.544\n10.00 / 0.413 / 0.762 / 1.464\nDyBluRF (Ours)\n18.20 / 0.534 / 0.415 / 1.011\n14.61 / 0.569 / 0.451 / 0.855\n15.09 / 0.501 / 0.316 / 1.095\n17.22 / 0.580 / 0.382 / 0.670\nTable 1. Dynamic deblurring novel view synthesis evaluation on Blurry iPhone Dataset. Red and blue denote the best and second best\nperformances, respectively. Each block element of 4-performance denotes (mPSNR\u2191 / mSSIM\u2191 / mLPIPS\u2193 / tOF\u2193).\nFigure 4. Qualitative comparisons on Blurry iPhone Dataset.\nnal iPhone dataset [19]. Following that, we average seven\nconsecutive frames with an eight-frame stride, incorporat-\ning the original frame and three interpolated frames from\nboth preceding and succeeding moments. This process re-\nsults in the creation of each blurry RGB frame, assuming\nthe camera motion is in constant velocity within exposure\ntime [36, 52, 59]. To calculate the corresponding camera\nrotation for each blurry frame, we utilize the spherical lin-\near interpolation [45] to generate interpolated rotation ma-\ntrices for each interpolated frame. These matrices are then\nconverted into quaternions for averaging the corresponding\nquaternions of the averaged RGB frames. The resulting av-\neraged quaternions are then reconverted into rotation matri-\nces. To calculate the corresponding camera translation, we\nlinearly interpolate the original translation vector and aver-\nage the interpolated translation vectors.\nMetrics. To evaluate our DyBluRF and compare it with\nother SOTA methods in the monocular video settings, we\nutilize the co-visibility masked image metrics, including\nmPSNR, mSSIM, and mLPIPs, following the approach in-\ntroduced by Dycheck [19]. These metrics mask out the re-\ngions of the test video frames which are not observed by the\ntraining camera. We further utilize tOF [10] to measure the\ntemporal consistency of reconstructed video frames.\n4.1. Comparisons with State-of-the-Art Methods\nTo validate the quality of deblurring monocular video\nnovel view synthesis of our DyBluRF, we compare it with\nthe existing dynamic novel view synthesis methods includ-\ning TiNeuVox [15], HexPlane [7], T-NeRF [35] and Hy-\nperNeRF [42] as well as the existing deblurring novel view\nsynthesis methods DP-NeRF [24] and BAD-NeRF [59]. All\nmethods are optimized using the newly synthesized Blurry\niPhone Dataset.\nFor the existing deblurring novel view\nsynthesis methods [24, 59] which are originally designed\nsolely for static novel view synthesis, we incorporate time\ninstances as additional inputs to make them synthesize dy-\nnamic components for a fair comparison. Table 1 shows\nquantitative comparison for various methods. As shown,\nour DyBluRF significantly outperforms the existing SOTA\nmethods for all metrics. Specifically, our model shows sig-\nnificantly better structural quality via deep metrics, e.g. per-\nceptual quality (mLPIPS) and temporal consistency (tOF),\nthan other methods. Figs. 1 and 4 show the superior visual-\nization of our DyBluRF compared to the SOTA methods.\nTo further demonstrate the effectiveness of the video\ndeblurring ability of DyBluRF, we compare the dynamic\nnovel view synthesis results of our DyBluRF with the ex-\nisting monocular video NeRF methods [27, 32, 35, 41, 42]\nwhich are optimized using the sharp video frames and ac-\ncurate camera poses, in Table 2. It should be noted that\nMethod\nTrained w/\nmPSNR\u2191 mSSIM\u2191\nmLPIPS\u2193\ntOF\u2193\nNSFF [27]\nSharp\n15.46\n0.569\n0.396\n-\nNerfies [41]\nSharp\n16.45\n0.569\n0.339\n0.961\nT-NeRF [35]\nSharp\n16.96\n0.577\n0.379\n0.843\nHyperNeRF [42]\nSharp\n16.81\n0.550\n0.332\n0.869\nRoDynRF [32]\nSharp\n17.09\n0.534\n0.517\n0.979\nDyBluRF (Ours)\nBlurry\n17.22\n0.580\n0.382\n0.670\nTable 2.\nDynamic novel view synthesis evaluation.\nOur\nDyBluRF trained with the Blurry iPhone Dataset (\u2018Trained w/\nBlurry\u2019) with inaccurate camera poses even shows comparable\nresults with the SOTA methods trained with the original iPhone\nDataset [19] (\u2018Trained w/ Sharp\u2019) with accurate camera poses.\nwe train our DyBluRF with the new Blurry iPhone dataset,\nwhereas other dynamic novel view synthesis methods are\ntrained with the original iPhone dataset [19]. As shown\nin Table 2, our DyBluRF trained on the blurry dataset of\ninaccurate poses achieves comparable results, even rival-\ning SOTA methods trained on sharp datasets with accurate\ncamera pose information.\n4.2. Ablation Study\nWe conduct an ablation study for three components in-\ncluding the IRR stage, the surface normal loss Lsn and the\nMDD stage. Table 3 presents detailed quantitative results\nfor the average performance across all seven scenes.\nIRR Stage. The effectiveness of our IRR stage (Sec. 3.3\nand Fig. 2-(a)) can be observed by comparing variant (c)\nto variants (a) and (b) in Table 3. Pose optimization with\nthe IRR stage is crucial for effectively handling novel view\nsynthesis with deblurring, especially when trained with the\ngiven blurry frames that come with inaccurate pose infor-\nmation. It is worth noting that naively adopting pose op-\ntimization [61] (b) for the given blurry frames, i.e., with-\nout interleave optimization, results in a more noisy radiance\nfield compared to having no optimization at all (a).\nSurface Normal Loss Lsn. Table 3 also emphasizes the\nimprovement of surface normal loss (e) (Sec. 3.3.2) com-\npared to the common depth supervision (d) for geometry\nregularizing NeRF. Since the pre-trained monocular depth\nestimator is scale-ambiguous, it is difficult to match the\nscale of the pre-trained depth maps and the radiance fields.\nAlthough we apply median scaling to the depth ground\ntruth before supervision for stability, variant (d) trained with\ndepth supervision still produces unreasonable rendering re-\nsults. Finally, the variant trained with Lsn (e) exhibits better\nresults in terms of mPSNR and mSSIM compared to vari-\nant (c) without any geometry regularization. Visual analysis\nfor our Lsn compared to the depth supervision is provided\nin Supplemental.\nMDD Stage. By comparing the final DyBluRF (f) to vari-\nant (e), the MDD stage (Sec. 3.4 and Fig. 2-(b)) especially\nincreases perceptual quality (mLPIPS). In addition, Fig. 5\nVariant\nIRR\nLsn\nMDD\nmPSNR\u2191\nmSSIM\u2191\nmLPIPS\u2193\ntOF\u2193\n(a)\n-\n-\n-\n16.15\n0.547\n0.456\n0.990\n(b)\nN\n-\n-\n16.06\n0.545\n0.449\n0.806\n(c)\n\u2713\n-\n-\n17.00\n0.580\n0.414\n0.662\n(d)\n\u2713\nD\n-\n8.92\n0.376\n0.768\n1.244\n(e)\n\u2713\n\u2713\n-\n17.19\n0.584\n0.414\n0.653\n(f)\n\u2713\n\u2713\n\u2713\n17.22\n0.580\n0.382\n0.670\nTable 3. Ablation Study. The \u2018N\u2019 indicates the naive pose op-\ntimization [61] and the \u2018D\u2019 indicates the depth supervision [48].\nVariant (f) denotes our final DyBluRF.\nOurs (full region)\nw/o MDD\nOurs\nGT\n(a)\n(b)\nw/o LORR (Eq.15)\nOurs\nGT\nFigure 5. Ablation Study on the MDD stage. (a) The effective-\nness of MDD stage (Fig. 2-(b)). (b) The effectiveness of decom-\nposition of both global (GCRP) and local (LORR) motions-aware\nray predictions.\nshows the visual comparison results of the ablation study\non our deblurring approach for the \u2018Block\u2019 scene. Specif-\nically, Fig. 5-(a) demonstrates the effectiveness of our fi-\nnal DyBluRF in terms of novel view synthesis for video\ndeblurring, attributed to the MDD stage. The MDD stage\neffectively handles the blurriness at the edges of objects or\nthe regions of complex textures. In addition, as shown in\nFig. 5-(b), our model better delicately decomposes the mix-\nture of global camera motion and local object (i.e. white\nbrick) motions across the training time indices. This results\nin robust novel view synthesis for a region where the object\npasses during training, compared to our variant not trained\nwith LORR (Eq. 15).\n5. Conclusion\nWe first propose a novel dynamic deblurring NeRF for\nblurry monocular video, called DyBluRF, which can effec-\ntively render the sharp novel spatio-temporal views from\nblurry monocular frames with inaccurate camera poses. Dy-\nBluRF includes an Interleave Ray Refinement (IRR) stage\nand a Motion Decomposition-based Deblurring (MDD)\nstage. The IRR stage simultaneously reconstructs dynamic\n3D scenes and optimizes inaccurate camera pose informa-\ntion. The MDD stage introduces a novel Incremental La-\ntent Sharp Rays Prediction (ILSP) approach to decompose\nlatent sharp rays into global camera motion and local ob-\nject motion components.\nExtensive experimental results\ndemonstrate that DyBluRF outperforms recent state-of-the-\nart methods both qualitatively and quantitatively.\nReferences\n[1] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli\nShechtman, and Zhixin Shu. Rignerf: Fully controllable neu-\nral 3d portraits. In CVPR, 2022. 2\n[2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling. In CVPR, pages 16610\u201316620, 2023.\n2\n[3] Yuval Bahat, Netalee Efrat, and Michal Irani. Non-uniform\nblind deblurring by reblurring. In ICCV, pages 3286\u20133294,\n2017. 2\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. In CVPR, pages 5470\u2013\n5479, 2022. 5\n[5] Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and\nArthur Szlam.\nOptimizing the latent space of generative\nnetworks. In Proceedings of the 35th International Confer-\nence on Machine Learning, ICML 2018, Stockholmsm\u00a8assan,\nStockholm, Sweden, July 10-15, 2018, 2018. 3\n[6] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-\nson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay\nBusch, Matt Whalen, and Paul Debevec. Immersive light\nfield video with a layered mesh representation. ACM Trans-\nactions on Graphics (TOG), 39(4):86\u20131, 2020. 2\n[7] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In CVPR, pages 130\u2013141, 2023. 1,\n2, 3, 7, 8, 9, 10\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In ECCV, pages\n333\u2013350. Springer, 2022. 2\n[9] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and\nSimon Lucey. Garf: gaussian activated radiance fields for\nhigh fidelity reconstruction and pose estimation. arXiv e-\nprints, pages arXiv\u20132204, 2022. 3\n[10] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix\u00b4e,\nand Nils Thuerey.\nLearning temporal coherence via self-\nsupervision for gan-based video generation. ACM Transac-\ntions on Graphics (TOG), 39(4):75\u20131, 2020. 7\n[11] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo. ACM Transactions on Graphics (ToG), 34(4):1\u201313,\n2015. 2\n[12] Senyou Deng, Wenqi Ren, Yanyang Yan, Tao Wang, Feng-\nlong Song, and Xiaochun Cao. Multi-scale separable net-\nwork for ultra-high-definition video deblurring.\nIn ICCV,\npages 14030\u201314039, 2021. 2\n[13] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Vol-\nume rendering. ACM Siggraph Computer Graphics, 22(4):\n65\u201374, 1988. 2, 3\n[14] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-\nbaum, and Jiajun Wu. Neural radiance flow for 4d view syn-\nthesis and video processing. In ICCV, pages 14304\u201314314.\nIEEE Computer Society, 2021. 2\n[15] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural vox-\nels. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139,\n2022. 2, 3, 7, 1\n[16] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR, pages\n5501\u20135510, 2022. 2\n[17] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR, pages 12479\u201312488, 2023. 2\n[18] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nICCV, pages 5712\u20135721, 2021. 2, 3, 4, 5\n[19] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell,\nand Angjoo Kanazawa. Dynamic novel-view synthesis: A\nreality check. In NeurIPS, 2022. 2, 6, 7, 8, 1, 5\n[20] Ankit Gupta, Neel Joshi, C Lawrence Zitnick, Michael Co-\nhen, and Brian Curless. Single image deblurring using mo-\ntion density functions. In ECCV, pages 171\u2013184. Springer,\n2010. 2\n[21] Stefan Harmeling, Hirsch Michael, and Bernhard Sch\u00a8olkopf.\nSpace-variant single-image blind deconvolution for remov-\ning camera shake. NeurIPS, 23:829\u2013837, 2010. 2\n[22] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,\nand Anurag Ranjan. Neuman: Neural human radiance field\nfrom a single video. In ECCV, 2022. 2\n[23] Yifan Jiang, Peter Hedman, Ben Mildenhall, Dejia Xu,\nJonathan T Barron, Zhangyang Wang, and Tianfan Xue.\nAlignerf: High-fidelity neural radiance fields via alignment-\naware training. In CVPR, pages 46\u201355, 2023. 3\n[24] Dogyoon Lee, Minhyeok Lee, Chajin Shin, and Sangyoun\nLee. Dp-nerf: Deblurred neural radiance field with physical\nscene priors. In CVPR, pages 12386\u201312396, 2023. 2, 3, 4,\n5, 7, 1, 8, 9, 10\n[25] Dongwoo Lee, Jeongtaek Oh, Jaesung Rim, Sunghyun Cho,\nand Kyoung Mu Lee. Exblurf: Efficient radiance fields for\nextreme motion blurred images.\nIn ICCV, pages 17639\u2013\n17648, 2023. 2, 3\n[26] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\nand Zhaoyang Lv. Neural 3d video synthesis from multi-\nview video. In CVPR, pages 5521\u20135531, 2022. 2, 4\n[27] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR, pages 6498\u20136508, 2021. 2, 3, 5, 7,\n8\n[28] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In CVPR, 2023. 2, 3, 4, 5\n[29] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan,\nEddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu\nTimofte, and Luc Van Gool.\nRecurrent video restoration\ntransformer with guided deformable attention. In NeurIPS,\n2022. 2\n[30] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. Barf: Bundle-adjusting neural radiance fields.\nIn ICCV, pages 5741\u20135751, 2021. 2, 3\n[31] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson,\nand Marc Pollefeys.\nPixel-perfect structure-from-motion\nwith featuremetric refinement. In ICCV, pages 5987\u20135997,\n2021. 3\n[32] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\nhannes Kopf, and Jia-Bin Huang. Robust dynamic radiance\nfields. In CVPR, pages 13\u201323, 2023. 2, 3, 5, 7, 8\n[33] Li Ma, Xiaoyu Li, Jing Liao, Qi Zhang, Xuan Wang, Jue\nWang, and Pedro V Sander. Deblur-nerf: Neural radiance\nfields from blurry images. In CVPR, pages 12861\u201312870,\n2022. 2, 3, 4\n[34] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\nLan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neu-\nral radiance field without posed camera.\nIn ICCV, pages\n6351\u20136361, 2021. 3\n[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 3, 7, 8, 1, 5\n[36] Jihyong Oh and Munchurl Kim. Demfi: deep joint deblurring\nand multi-frame interpolation with flow-guided attentive cor-\nrelation and recursive boosting. In ECCV, pages 198\u2013215.\nSpringer, 2022. 2, 7\n[37] Martin Ralf Oswald, Jan St\u00a8uhmer, and Daniel Cremers. Gen-\neralized connectivity constraints for spatio-temporal 3d re-\nconstruction. In ECCV, pages 32\u201346. Springer, 2014. 2\n[38] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-\nHsuan Yang.\nBlind image deblurring using dark channel\nprior. In CVPR, pages 1628\u20131636, 2016. 2\n[39] Jinshan Pan, Haoran Bai, and Jinhui Tang. Cascaded deep\nvideo deblurring using temporal sharpness prior. In CVPR,\npages 3043\u20133051, 2020. 2\n[40] Jinshan Pan, Boming Xu, Jiangxin Dong, Jianjun Ge, and\nJinhui Tang. Deep discriminative spatial and temporal net-\nwork for efficient video deblurring. In CVPR, pages 22191\u2013\n22200, 2023. 2\n[41] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, pages 5865\u20135874, 2021. 2, 5, 7, 8\n[42] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), 2021. 1, 2, 3, 7,\n8, 5, 9, 10\n[43] Keunhong\nPark,\nPhilipp\nHenzler,\nBen\nMildenhall,\nJonathan T. Barron, and Ricardo Martin-Brualla.\nCamp:\nCamera preconditioning for neural radiance fields. CoRR,\n2023. 2, 3\n[44] Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun\nAhn, Ji-Yeon Kim, and Nahyup Kang. Temporal interpola-\ntion is all you need for dynamic neural radiance fields. In\nCVPR, pages 4212\u20134221, 2023. 2\n[45] Xavier Pennec. Computing the mean of geometric features\napplication to the mean rotation. PhD thesis, INRIA, 1998.\n7, 1\n[46] Martin Piala and Ronald Clark.\nTerminerf: Ray termina-\ntion prediction for efficient neural rendering. In 3DV, pages\n1106\u20131114, 2021. 6\n[47] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. In CVPR, pages 10318\u201310327, 2021. 2, 3\n[48] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction.\nIn ICCV, pages\n12159\u201312168, 2021. 5, 8\n[49] Vincent Raoult, Sarah Reid-Anderson, Andreas Ferri, and\nJane E Williamson. How reliable is structure from motion\n(sfm) over time and between observers? a case study using\ncoral reef bommies. Remote Sensing, 9(7):740, 2017. 3\n[50] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, pages 4104\u20134113, 2016.\n3\n[51] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In CVPR, pages 16632\u201316642, 2023. 2\n[52] Wang Shen, Wenbo Bao, Guangtao Zhai, Li Chen, Xiongkuo\nMin, and Zhiyong Gao. Blurry video frame interpolation. In\nCVPR, pages 5114\u20135123, 2020. 7\n[53] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: ex-\ntreme video frame interpolation.\nIn ICCV, pages 14489\u2013\n14498, 2021. 6, 1\n[54] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n2\n[55] Jacob Telleen, Anne Sullivan, Jerry Yee, Oliver Wang, Pra-\nbath Gunawardane, Ian Collins, and James Davis. Synthetic\nshutter speed imaging. In Computer Graphics Forum, pages\n591\u2013598. Wiley Online Library, 2007. 2\n[56] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video.\nIn\nICCV, pages 12959\u201312970, 2021. 2\n[57] Basile Van Hoorick, Purva Tendulkar, Didac Suris, Dennis\nPark, Simon Stent, and Carl Vondrick. Revealing occlusions\nwith 4d neural fields. In CVPR, pages 3011\u20133021, 2022. 2\n[58] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In CVPR, pages 13524\u201313534, 2022. 2\n[59] Peng Wang, Lingzhe Zhao, Ruijie Ma, and Peidong Liu.\nBad-nerf: Bundle adjusted deblur neural radiance fields. In\nCVPR, pages 4170\u20134179, 2023. 1, 2, 3, 4, 7\n[60] Yusheng Wang, Yunfan Lu, Ye Gao, Lin Wang, Zhihang\nZhong, Yinqiang Zheng, and Atsushi Yamashita. Efficient\nvideo deblurring guided by motion magnitude. In ECCV,\n2022. 2\n[61] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\ntor Adrian Prisacariu. Nerf-: Neural radiance fields without\nknown camera parameters. CoRR, 2021. 2, 3, 8\n[62] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, pages 16210\u201316220, 2022. 2\n[63] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, pages 16210\u201316220, 2022. 2\n[64] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In CVPR, pages 9421\u20139431, 2021. 2\n[65] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ra-\nmanan, Vedaldi Andrea, and Joo Hanbyul. Banmo: Building\nanimatable 3d neural models from many casual videos. In\nCVPR, 2022. 2\n[66] Huicong Zhang, Haozhe Xie, and Hongxun Yao.\nSpatio-\ntemporal deformable attention network for video deblurring.\nIn ECCV, 2022. 2\n[67] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu,\nand Hongdong Li. Adversarial spatio-temporal learning for\nvideo deblurring. IEEE Transactions on Image Processing,\n28(1):291\u2013301, 2018. 2\n[68] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by realistic\nblurring. In CVPR, pages 2737\u20132746, 2020. 2\n[69] Li Zhang, Brian Curless, and Steven M Seitz. Spacetime\nstereo: Shape recovery for dynamic scenes. In CVPR, pages\nII\u2013367. IEEE, 2003. 2\n[70] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie,\nWangmeng Zuo, and Jimmy Ren.\nSpatio-temporal filter\nadaptive network for video deblurring. In ICCV, pages 2482\u2013\n2491, 2019. 2\n[71] Qi Zhu, Man Zhou, Naishan Zheng, Chongyi Li, Jie Huang,\nand Feng Zhao. Exploring temporal frequency spectrum in\ndeep video deblurring. In ICCV, pages 12428\u201312437, 2023.\n2\n[72] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\nSimon Winder, and Richard Szeliski.\nHigh-quality video\nview interpolation using a layered representation.\nACM\ntransactions on graphics (TOG), 23(3):600\u2013608, 2004. 2\nDyBluRF: Dynamic Deblurring Neural Radiance Fields\nfor Blurry Monocular Video\nSupplementary Material\nA. List of Notations\nTable 4 provides an organized list of notations used in the\nmain paper, designed to improve readers\u2019 comprehension of\nour text and equations.\nB. Implementation Details\nOur DyBluRF is implemented using JAX and built upon\nDycheck [19] codebase. Similar to Dycheck [19], we adopt\na coarse-to-fine sampling strategy for NeRF, employing\nN = 128 samples at each sampling step. Our model un-\ndergoes training for 2 \u00d7 105 iterations in the IRR stage and\n1 \u00d7 105 in the MDD stage. The hyperparameters for the\nloss weighting coefficients, denoted as \u03bbsn, \u03bbsb and \u03bbdist,\nare empirically set to 0.075, 0.002 and 0.01, respectively.\nFor all scenes, our best model is configured with a fixed\nnumber of latent sharp rays Nb = 6. We utilize the Adam\noptimizer with exponential scheduling for training our net-\nworks. Notably, the learning rates for the screw axis em-\nbeddings St and Sg\nt;q are in the range [10\u22124, 10\u22126], while\nthe learning rates for the remaining MLPs are in the range\n[10\u22123, 10\u22124]. For the implementation of the Static Net F\u03b8s\nand Dynamic Net F\u03b8d of the main paper, we adopt the NeRF\narchitecture from the Dycheck [19] codebase to ensure a\nfair comparison. This NeRF architecture comprises a trunk\nMLP, a sigma MLP and an rgb MLP. The sigma and rgb\nMLPs take the output feature of the trunk MLP as their re-\nspective inputs. We configure the number of layers for the\ntrunk, sigma, and rgb MLPs to be 9, 1, and 2, respectively.\nThe dimensions of the hidden layer for the trunk and rgb\nMLPs are set to 256 and 128, respectively. For the Static\nNet, we introduce a blending MLP that consists of a sin-\ngle layer. This MLP takes the output of the trunk MLP\nas input to produce the volume density blending factor b.\nFor the local object-motion MLP F\u03b8l, we set the number\nof layers and dimension of hidden layers to 8 and 128, re-\nspectively. We set the number of frequencies for each po-\nsitional encoding \u03b3 of 3D sample positions x and their cor-\nresponding viewing directions d to 8 and 4, respectively.\nWe also set the output dimension of the time encoding l(t)\nto 8 and the number of samples for the discretized ray em-\nbedding \u03d5 to 32. On the other hand, we also recommend\nreaders to refer to our project page at https://kaist-\nviclab.github.io/dyblurf-site/, which pro-\nvides source codes and pretrained models for the sake of\nreproducibility.\nC. Details for Blurry iPhone Dataset Synthesis\nWe present the visual process of synthesizing our pro-\nposed Blurry iPhone Dataset, which includes blurry RGB\nframes and inaccurate camera poses, in Fig. 6. The synthe-\nsis of the blurry video frame Bt from the original iPhone\nDataset [19] is illustrated in Fig. 6-(a). We employ the\nVFI [53] method to first generate seven interpolated frames\nbetween each pair of original video frames (It\u22121 and It,\nIt and It+1). Subsequently, we select seven frames (It\u2212 5\n8\nto It+ 3\n8 ) and average them. To simulate the blurry cam-\nera pose information \u02dcPt = [ \u02dcRt| \u02dcT t] for our Blurry iPhone\nDataset, we maintain the same interpolation concept for the\ncamera pose matrices to reflect the camera motion trajec-\ntory, as shown in Fig. 6-(b). For the rotation matrix \u02dcRt, we\nuse the Spherical Linear Interpolation (Quaternion Interpo-\nlation) [45] method to generate interpolated rotation matri-\nces between each pair of existing rotation matrices. We then\nchoose the seven rotation matrices (Rt\u2212 5\n8 to Rt+ 3\n8 ) and\naverage them. Since directly calculating the average rota-\ntion matrix from multiple rotation matrices is challenging,\nwe convert the matrices into quaternions and average these\nquaternions. For the translation matrix \u02dcT t, we adopt linear\ninterpolation to generate interpolated matrices and average\nthe seven matrices (T t\u2212 5\n8 to T t+ 3\n8 ). The resulting interpo-\nlated camera pose \u02dcPt has different rotation and translation\nmatrices with the accurate camera pose Pt, making it more\nchallenging for existing methods [7, 15, 24, 35, 42, 59] to\nreconstruct the correct geometry of a scene, except for our\nDyBluRF.\nD. Details for Blurry Color Rendering Process\nwith Latent Sharp Rays\nWe describe the blurry color rendering process of the\nMDD stage (Sec. 3.4) in Fig. 7. In Fig. 7-(a), the pre-\ndicted ray \u02c6rp;t and the latent sharp rays { \u02d9rp;t;q}Nb\nq=1 render\nthe corresponding sharp RGB colors separately. We aver-\nage the individual sharp RGB colors to predict the blurry\nRGB color for the photometric loss function. As shown in\nFig. 7-(b), our predicted latent sharp frames are surprisingly\nlearned as latent sharp rays by naturally following a blurry\nprocess by Eq. 7, where zooming results with red boxes and\nsmaller green and blue boxes in the different latent sharp\nframes show detailed local object motions to synthesize the\ngiven blurry frame. These visualization results support our\nDyBluRF\u2019s superior performance and reliability in dynamic\ndeblurring.\nNotation\nDescription\nIt\nA sharp frame at time t of a monocular video\nIp;t\nA sharp pixel color of pixel p at time t of a monocular video\nBt\nA blurry frame at time t of a monocular video\nBp;t\nA blurry pixel color of pixel p at time t of a monocular video\nPt\nA camera pose information at time t\n\u02dcPt\nAn inaccurate camera pose information at time t\nNf\nThe number of frames in a video\not\nAn origin of a ray at time t\ndp;t\nA viewing direction of a ray of pixel p at time t\ns\nA sampling ray distance\nrp;t\nA casted ray from ot with dp;t\nF\u03b8s\nStatic Net\nF\u03b8d\nDynamic Net\ncs / cd\nAn estimated color by Static/Dynamic Net\n\u03c3s / \u03c3d\nAn estimated volume density by Static/Dynamic Net\nb\nA volume density blending factor\n\u02c6Cs,d,full\nA volume rendered color for static, dynamic or full scene component, respectively\nT s,d,full\nAn accumulated transmittance for static, dynamic or full scene component, respectively\n\u03b1s / \u03b1d\nAn alpha-compositing weight for static/dynamic scene component\nM\nA predicted binary motion mask\n\u02c6CB\nA blurry rendered color\nA\nAn average function\n\u02dcrp;t\nAn inaccurate ray from p with \u02dcPt\n\u02c6rp;t\nAn refined ray from \u02dcrp;t\n\u02d9rp;t;q\nThe q-th predicted latent sharp ray to generate the blurry pixel color \u02c6CB which can be \u02d9rg\np;t;q or \u02d9rl\np;t;q\n\u02d9rg\np;t;q\nThe q-th predicted latent sharp ray to generate the blurry pixel color of static scene component by GCRP (Eq. 14\n\u02d9rl\np;t;q\nThe q-th predicted latent sharp ray to generate the blurry pixel color of dynamic scene component by LORR (Eq. 15)\nNb\nThe number of latent sharp rays\nSt\nA learnable embedding of screw axis for ray refinement of \u02dcrp;t\nSg\nt;q\nA learnable embedding of global camera-motion-aware screw axis for prediction of ray \u02d9rg\np;t;q\nSl\np;t;q\nA local object-motion-aware screw axis for refinement of ray \u02d9rl\np;t;q\nF\u03b8l\nA local object-motion-aware MLP\n\u03b3(\u00b7)\nA positional encoding\nl(\u00b7)\nGenerative Latent Optimization\n\u03d5(\u00b7)\nA discretized ray embedding\nnp;t\nA ground-truth surface normal computed from DPT\u2019s depth estimation\n\u02c6np;t\nA predicted surface normal computed from volume density\nLphoto / Lmphoto\nA photometric/masked photometric loss function\nLdist\nA distortion loss function\nLsn\nA surface normal loss function\nTable 4. List of Notations.\nFigure 6. Simulation of Blurry iPhone Dataset. (a) Simulation of blurry video frames. (b) Simulation of inaccurate camera poses. Please\nnote that we just visualize arrows for the rotation matrices and dots for the translation matrices in (b) for simplicity.\nE. Additional Experimental Results\nE.1. Robustness of our DyBluRF on Two Different\nQualities of Datasets.\nTable 5 shows the robustness of DyBluRF on different\ndataset capturing quality.\nIn particular, the performance\nof the existing methods significantly decreases while train-\ning low-quality data, i.e., Blurry iPhone Dataset.\nEspe-\ncially, there are large quality drops for HyperNeRF of -\n9% (mPSNR), -3% (mSSIM), -35% (mLPIPS) and -25%\n(tOF) and for T-NeRF of -5% (mPSNR), -5% (mSSIM), -\n20% (mLPIPS) and -17% (tOF). Whereas DyBluRF shows\nmuch smaller performance decreases in mLPIPS (-3.8%)\nand tOF (-0.9%) and even shows performance gains in mP-\nSNR (+0.3%) and mSSIM (+0.17%). Note that \u2018-\u2019 denotes\nthe performance drop, while \u2018+\u2019 is the performance gain.\nE.2. Local vs. Global Motion-aware Ray Prediction\nWe provide additional visualizations to illustrate the\nvarying rendering quality of the model without local ob-\nject motion learning and the final DyBluRF in Fig. 8. Our\nfinal model excels at finely breaking down the blend of\nglobal camera motion and local object motions throughout\nthe training times. This leads to a strong and reliable syn-\nthesis of novel views for our final DybluRF, in contrast to\nour variant that wasn\u2019t trained with LORR (Eq. 15).\nE.3. Pose Error\nWe present both quantitative and qualitative results of\ncamera pose error to demonstrate the superiority of our IRR\nstage compared to naive pose optimization. As shown in\nTable 6 and Fig. 9, optimizing the dynamic radiance field\nand the camera poses in the same training iteration can lead\nto bad local minima.\nIn contrast, our DyBluRF adopts\nthe interleave optimization by utilizing the scene compo-\nnents decomposition. It is worth noting that the naive pose\noptimization and our interleave optimization leverage sim-\nple se(3) camera parameterization for pose optimization to\nbe equipped into our effective dynamic deblurring NeRF\nframework. Please note that even though it is not our main\nscope to handle the pose optimization algorithm itself, our\nIRR stage can further incorporate advanced pose optimiza-\ntion approaches such as CamP [43] or BARF [30] in future\nworks.\nFigure 7. Visualization of our blurry color rendering process with latent sharp rays. (a) Blurry color rendering process in the MDD\nstage (Sec. 3.4). (b) Our predicted latent sharp frames are remarkably acquired as latent sharp rays by inherently learning a blurring process\naccording to Eq. 7. The zoomed-in outcomes with red and smaller green and blue boxes across distinct latent sharp frames reveal intricate\nlocal object motion.\nMethod\nTrained w/\nApple\nBlock\nPaper-windmill\nSpace-out\nT-NeRF [35]\nSharp\n17.43 / 0.728 / 0.508 / 0.591\n17.52 / 0.669 / 0.346 / 0.741\n17.55 / 0.367 / 0.258 / 0.418\n17.71 / 0.591 / 0.377 / 0.491\nT-NeRF [35]\nBlurry\n17.34 / 0.720 / 0.547 / 0.620\n16.48 / 0.644 / 0.411 / 0.795\n16.83 / 0.338 / 0.424 / 0.569\n16.23 / 0.561 / 0.436 / 1.329\nHyperNeRF [42]\nSharp\n16.47 / 0.754 / 0.414 / 0.693\n14.71 / 0.606 / 0.438 / 0.722\n14.94 / 0.272 / 0.348 / 0.453\n17.65 / 0.636 / 0.341 / 0.440\nHyperNeRF [42]\nBlurry\n14.31 / 0.681 / 0.663 / 1.411\n16.12 / 0.642 / 0.416 / 0.958\n16.59 / 0.335 / 0.365 / 0.666\n17.79 / 0.631 / 0.332 / 0.402\nDyBluRF (Ours)\nSharp\n18.00 / 0.737 / 0.488 / 0.495\n17.47 / 0.665 / 0.349 / 0.713\n18.19 / 0.405 / 0.301 / 0.224\n18.72 / 0.640 / 0.323 / 0.273\nDyBluRF (Ours)\nBlurry\n18.03 / 0.737 / 0.505 / 0.479\n17.35 / 0.660 / 0.361 / 0.735\n18.24 / 0.410 / 0.300 / 0.242\n18.99 / 0.646 / 0.328 / 0.276\nMethod\nTrained w/\nSpin\nTeddy\nWheel\nAverage\nT-NeRF [35]\nSharp\n19.16 / 0.567 / 0.443 / 1.064\n13.71 / 0.570 / 0.429 / 1.142\n15.65 / 0.548 / 0.292 / 1.453\n16.96 / 0.577 / 0.379 / 0.843\nT-NeRF [35]\nBlurry\n17.16 / 0.503 / 0.534 / 1.162\n14.07 / 0.562 / 0.464 / 1.094\n14.93 / 0.499 / 0.379 / 1.360\n16.15 / 0.547 / 0.456 / 0.990\n-0.81 / -0.030 / +0.077 / +0.147\nHyperNeRF [42]\nSharp\n17.26 / 0.540 / 0.371 / 1.017\n12.59 / 0.537 / 0.527 / 0.940\n14.59 / 0.511 / 0.331 / 1.818\n16.81 / 0.550 / 0.332 / 0.869\nHyperNeRF [42]\nBlurry\n16.39 / 0.498 / 0.499 / 1.277\n13.77 / 0.567 / 0.420 / 1.143\n12.11 / 0.393 / 0.435 / 1.739\n15.30 / 0.535 / 0.447 / 1.085\n-1.51 / -0.015 / +0.115 / +0.216\nDyBluRF (Ours)\nSharp\n18.11 / 0.535 / 0.386 / 0.992\n14.61 / 0.572 / 0.425 / 0.867\n15.03 / 0.497 / 0.303 / 1.082\n17.16 / 0.579 / 0.368 / 0.664\nDyBluRF (Ours)\nBlurry\n18.20 / 0.534 / 0.415 / 1.011\n14.61 / 0.569 / 0.451 / 0.855\n15.09 / 0.501 / 0.316 / 1.095\n17.22 / 0.580 / 0.382 / 0.670\n+0.06 / +0.001 / +0.014 / +0.006\nTable 5. Dynamic novel view synthesis evaluation. Red and blue denote the best and second best performances, respectively. Each block\nelement of 4-performance denotes (mPSNR\u2191 / mSSIM\u2191 / mLPIPS\u2193 / tOF\u2193). The \u2018Trained w/ Sharp\u2019 indicates the result of each method\ntrained with the original iPhone Dataset [19], whereas the \u2018Trained w/ Blurry\u2019 indicates the results of each method trained with the newly\nsynthesized Blurry iPhone Dataset. We indicate the average performance differences of each method across all metrics when trained with\nthe original iPhone Dataset [19] and the Blurry iPhone Dataset below the performance achieved with the Blurry iPhone Dataset. A light\ngreen color denotes a performance gain, while an orange color indicates a performance loss. Different from the existing video view\nsynthesis methods [35, 42] which are highly exacerbated from our Blurry iPhone Dataset, our DyBluRF shows significant robustness.\nFigure 8. Visualization of rendering quality improvement us-\ning motion decomposition.\nVariant\nErrorrot (\u25e6)\u2193\nErrortrans\u2193\nNaive pose optimization\n0.00914\n0.00899\nIRR\n0.00871\n0.00697\nTable 6. Average camera pose error. We compare the average\nrotation error (Errorrot) and translation error (Errortrans) of the\nnaive pose optimization and our IRR stage.\nE.4. Visual Analysis of Surface Normal Loss\nAs depicted in Fig. 10, the depth map estimated by the\npre-trained monocular depth estimation model [48] may ex-\nhibit a significant scale difference compared to the absolute\nscale of the target scene (1.3 vs. 55.7 and 1.8 vs. 68.8 for\ntwo specific pixel locations) which can be obtained from\nLiDAR sensors. Adopting the pre-trained monocular depth\nestimation model for NeRF regularization presents chal-\nlenges related to the scale ambiguity. Common approaches\nto address this issue involve aligning depth maps with SfM\nsparse reconstruction or employing median scaling for su-\npervised loss. For the DyBluRF, we opt for a straightfor-\nward yet effective surface normal supervision, denoted as\nLsn, to enhance the geometric fidelity of our radiance field.\nAs shown in Table 3, the surface normal loss Lsn improves\nour model performance, while the median scaling depth loss\nfailed to reconstruct the target scene. In addition, we pro-\nvide a qualitative comparison to show the superiority of Lsn\nin Fig. 11. As shown in Fig. 11, the depth loss enhances\nthe representation of the surface of the gray block, but it\nstruggles to accurately resolve the scale-ambiguity; thus the\nmodel is not capable of reconstructing the target scene. In\ncontrast, our model without geometry regularization results\nin imprecise geometry of the moving block. In contrast, our\nDyBluRF incorporating Lsn significantly improves the ren-\ndering quality, especially in the moving objects regions.\nNaive pose optimization\nOurs\nPrediction\nGT Comparison\nGT\nFigure 9. Visualization of camera pose error. \u2018GT Comparisons\u2019 shows overlapped GT camera poses extracted from sharp frames and\nthe optimized poses by each method: Naive pose optimization and DyBluRF (Ours).\nLiDAR\nPretrained DPT [48]\n         \nSurface normal\nFigure 10. Visualization of depth maps and the corresponding\nsurface normals. We compare the depth and surface normal val-\nues (yellow dots) obtained from LiDAR sensor and predicted by\nthe pretrained model.\nE.5. Per-Scene Ablation Study Results\nTable 7 presents the per-scene quantitative results for the\nsix variants of our DyBluRF, corresponding to Table 3 in\nthe main paper. By comparing variant (f) to (e) across all\nscenes in terms of mLPIPS, our proposed MDD stage stably\nand significantly increases the perceptual quality of recon-\nstructed frames.\nOurs\nw/o surface \nnormal loss\nDepth loss\nGT\nFigure 11. Qualitative ablation study on surface normal loss.\nWe provide the visualization to show the effectiveness of Lsn for\ngeometry regularization.\nE.6. Additional Qualitative Comparisons\nFigs. 13, 14 and 15 show the abundant visual compar-\nisons for dynamic deblurring novel view synthesis.\nOur\nDyBluRF generally better synthesizes temporally consistent\ndeblurring results, which tend to be failed by all the previ-\nous methods.\nVariant\nIRR\nLsn\nMDD\nApple\nBlock\nPaper-windmill\nSpace-out\n(a)\n-\n-\n-\n17.34 / 0.720 / 0.547 / 0.620\n16.48 / 0.644 / 0.411 / 0.795\n16.83 / 0.338 / 0.424 / 0.569\n16.23 / 0.561 / 0.436 / 1.329\n(b)\nN\n-\n-\n17.07 / 0.721 / 0.542 / 0.636\n16.38 / 0.641 / 0.415 / 0.829\n16.38 / 0.329 / 0.436 / 0.493\n17.99 / 0.629 / 0.368 / 0.309\n(c)\n\u2713\n-\n-\n18.24 / 0.739 / 0.508 / 0.470\n17.31 / 0.664 / 0.388 / 0.687\n18.15 / 0.405 / 0.346 / 0.285\n17.40 / 0.647 / 0.364 / 0.255\n(d)\n\u2713\nD\n-\n8.17 / 0.582 / 0.834 / 1.169\n7.94 / 0.402 / 0.767 / 1.690\n16.76 / 0.382 / 0.432 / 0.429\n6.92 / 0.340 / 0.724 / 0.808\n(e)\n\u2713\n\u2713\n-\n18.02 / 0.739 / 0.509 / 0.473\n17.23 / 0.660 / 0.391 / 0.699\n18.36 / 0.427 / 0.346 / 0.271\n18.56 / 0.649 / 0.352 / 0.251\n(f)\n\u2713\n\u2713\n\u2713\n18.03 / 0.737 / 0.505 / 0.479\n17.35 / 0.660 / 0.361 / 0.735\n18.24 / 0.410 / 0.300 / 0.242\n18.99 / 0.646 / 0.328 / 0.276\nVariant\nIRR\nLsn\nMDD\nSpin\nTeddy\nWheel\nAverage\n(a)\n-\n-\n-\n17.16 / 0.503 / 0.534 / 1.162\n14.07 / 0.562 / 0.464 / 1.094\n14.93 / 0.499 / 0.379 / 1.360\n16.15 / 0.547 / 0.456 / 0.990\n(b)\nN\n-\n-\n16.07 / 0.478 / 0.557 / 1.156\n13.87 / 0.553 / 0.461 / 1.067\n14.66 / 0.466 / 0.365 / 1.153\n16.06 / 0.545 / 0.449 / 0.806\n(c)\n\u2713\n-\n-\n18.14 / 0.522 / 0.481 / 1.026\n14.40 / 0.565 / 0.465 / 0.908\n15.40 / 0.518 / 0.348 / 1.004\n17.00 / 0.580 / 0.414 / 0.662\n(d)\n\u2713\nD\n-\n10.78 / 0.394 / 0.679 / 1.435\n6.84 / 0.360 / 0.866 / 1.698\n5.04 / 0.173 / 1.073 / 1.479\n8.92 / 0.376 / 0.768 / 1.244\n(e)\n\u2713\n\u2713\n-\n18.14 / 0.531 / 0.484 / 0.986\n14.57 / 0.571 / 0.466 / 0.844\n15.43 / 0.512 / 0.349 / 1.049\n17.19 / 0.584 / 0.414 / 0.653\n(f)\n\u2713\n\u2713\n\u2713\n18.20 / 0.534 / 0.415 / 1.011\n14.61 / 0.569 / 0.451 / 0.855\n15.09 / 0.501 / 0.316 / 1.095\n17.22 / 0.580 / 0.382 / 0.670\nTable 7. Per-Scene breakdowns of ablation study. Red and blue denote the best and second best performances, respectively. Each block\nelement of 4-performance denotes (mPSNR\u2191 / mSSIM\u2191 / mLPIPS\u2193 / tOF\u2193).\nE.7. Visual Comparisons with Demo Video\nWe provide a video at https : / / youtu . be /\nUF5QTWF3_Dg for comparing our DyBluRF to the exist-\ning methods [7, 24, 32, 35, 42]. The demo video consists of\nthree comparisons (Dynamic Deblurring Novel View Syn-\nthesis Evaluation on Blurry iPhone Dataset, Table 1) on\nscenes of paper-windmill, block, wheel and one comparison\n(Dynamic Novel View Synthesis Evaluation, Table 2) on a\nscene of apple and two ablation studies on the effectiveness\nof MDD stage and decompositions of GCRP and LORR.\nE.8. The Number of Latent Sharp Rays\nWe provide a quantitative comparison of our DyBluRF\nmodel with varying numbers of latent sharp rays for the\nMDD stage in Table 8. In particular, increasing the number\nNb of latent sharp rays can improve the reconstruction per-\nformance but it requires more computational cost. For the\nbest quality and computational efficiency, we adopt Nb = 6\nfor all experiments of the final DyBluRF. To measure the\ntraining time per iteration, we utilize a single RTX 3090Ti\nGPU and set the batch size to 128. It is notable that the\nnumber of Nb only affects the training time. In the infer-\nence stage, where we render a single predicted sharp ray per\npixel, the inference time remains constant across all vari-\nants, irrespective of the different values of Nb.\nNb\nmPSNR\u2191\nmSSIM\u2191\nmLPIPS\u2193\ntOF\u2193\nTraining time per iter. (s)\n2\n16.90\n0.571\n0.409\n0.750\n0.095\n6\n17.22\n0.580\n0.382\n0.670\n0.161\n10\n17.19\n0.579\n0.383\n0.675\n0.226\nTable 8. Ablation Study on the number of latent sharp rays.\nE.9. Limitations\nIn the Blurry iPhone Dataset, there is one continuous\ntraining monocular video for each scene. Additionally, two\nfixed cameras with novel views are provided for evaluation,\nfollowing the evaluation protocol of the original iPhone\nDataset [19]. This configuration makes it challenging to\ntrain deep-learning-based methods on unseen information\nthat may be requested for evaluation. For instance, in some\nspecific scenes, as shown in Fig. 12, due to different light-\ning environments, the tone of validation views can only be\nobserved in a significantly small number of training samples\n(View 2). Hence, all kinds of radiance fields are generally\noverfitted to the major lighting condition of View 1. This\nchallenge arises from the dataset\u2019s characteristics, which\nexhibit extremely diverse training and validation views. In-\ntroduction of relighting technique, modeling of reflectance\ncomponent or balancing training data can be one of an op-\ntion to handle this issue and we leave it for one of our future\nwork.\nFigure 12. Limitations of our DyBluRF. Since the majority of\ntraining video frames in the Blurry iPhone Dataset exhibit a similar\ntone in View 1 (about 92% of the training frames), reconstructing\nthe accurate tone of validation views with a similar tone in View\n2 (about 8% of the training frames) becomes challenging. This\ndifficulty arises due to the varying lighting effects present in the\nBlurry iPhone Dataset.\nFigure 13. Dynamic deblurring novel view synthesis results. We compare the novel view synthesis quality of our DyBluRF for spin\nscene with the existing methods [7, 24, 42]. Each row corresponds to the 310th, 320th and 330th frame, respectively.\nFigure 14. Dynamic deblurring novel view synthesis results. We compare the novel view synthesis quality of our DyBluRF for paper-\nwindmill scene with the existing methods [7, 24, 42]. Each row corresponds to the 198th, 203th and 298th frame, respectively.\nFigure 15. Dynamic deblurring novel view synthesis results. We compare the novel view synthesis quality of our DyBluRF for space-out\nscene with the existing methods [7, 24, 42]. Each row corresponds to the 336th, 346th and 356th frame, respectively.\n"
  },
  {
    "title": "Unlocking Pre-trained Image Backbones for Semantic Image Synthesis",
    "link": "https://arxiv.org/pdf/2312.13314.pdf",
    "upvote": "6",
    "text": "Unlocking Pre-trained Image Backbones for\nSemantic Image Synthesis\nTariq Berrada1,2, Jakob Verbeek1, Camille Couprie1, Karteek Alahari2\n1FAIR, Meta, 2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK\nSemantic image synthesis, i.e., generating images from user-provided semantic label maps, is an\nimportant conditional image generation task as it allows to control both the content as well as the\nspatial layout of generated images. Although diffusion models have pushed the state of the art in\ngenerative image modeling, the iterative nature of their inference process makes them computationally\ndemanding. Other approaches such as GANs are more efficient as they only need a single feed-forward\npass for generation, but the image quality tends to suffer on large and diverse datasets. In this\nwork, we propose a new class of GAN discriminators for semantic image synthesis that generates\nhighly realistic images by exploiting feature backbone networks pre-trained for tasks such as image\nclassification. We also introduce a new generator architecture with better context modeling and\nusing cross-attention to inject noise into latent variables, leading to more diverse generated images.\nOur model, which we dub DP-SIMS, achieves state-of-the-art results in terms of image quality and\nconsistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes, surpassing recent\ndiffusion models while requiring two orders of magnitude less compute for inference.\nDate: January 9, 2024\nCorrespondence: Tariq Berrada (tariqberrada@meta.com) and Jakob Verbeek (jjverbeek@meta.com)\n1\nIntroduction\nConditional image synthesis aims to generate im-\nages based on information such as text, categories,\nsketches, label maps, etc. While text-based genera-\ntion has seen impressive advances in recent years with\ndiffusion models (Rombach et al., 2022; Nichol et al.,\n2022), it lacks precise control over the location and\nthe boundaries of objects, which are important prop-\nerties for creative content generation tasks like photo\nediting, inpainting, and for data augmentation in dis-\ncriminative learning (Zhao et al., 2023; Akrout et al.,\n2023; Azizi et al., 2023; Hemmat et al., 2023). Conse-\nquently, in this work we focus on semantic image syn-\nthesis (Isola et al., 2017; Wang et al., 2018; Park et al.,\n2019; Sch\u00f6nfeld et al., 2021; Wang et al., 2022b,a),\nwhere the goal is to produce an image, given a seg-\nmentation map, with every pixel assigned to a cate-\ngory, as input. Because of the one-to-many nature of\nthe mapping, prior works have tackled this problem\nunder a conditional GAN (Goodfellow et al., 2014)\nframework by exploring different conditioning mech-\nanisms in GANs to do stochastic generations that\ncorrespond to the input label map (Isola et al., 2017;\nWang et al., 2018; Park et al., 2019). Others devel-\noped conditional discriminator models, which avoid\nimage-to-image reconstruction losses that compro-\nmise diversity in generated images (Sch\u00f6nfeld et al.,\n2021). Diffusion models (Wang et al., 2022b,a) have\nalso been investigated for this problem. SDM (Wang\net al., 2022b) learns to incorporate spatially adaptive\nnormalization layers into the diffusion process of a la-\ntent diffusion model, while PITI (Wang et al., 2022a)\nfinetunes pre-trained text-to-image diffusion models\nsuch as (Nichol et al., 2022). In comparison to GANs,\ndiffusion models often result in improved image qual-\nity, but suffer from lower consistency with the input\nsegmentation maps, and are slower during inference\ndue to the iterative denoising process (Careil et al.,\n2023).\nTo improve the image quality and consistency of\nGAN-based approaches, we explore the use of pre-\ntrained image backbones in discriminators for seman-\ntic image synthesis. Although leveraging pre-trained\nimage models is common in many other vision tasks,\nsuch as classification, segmentation, or detection, and\nmore recently for class-conditional GANs (Sauer et al.,\n2021), to our knowledge this has not been explored\nfor semantic image synthesis. To this end, we develop\na UNet-like encoder-decoder architecture where the\nencoder is a fixed pre-trained image backbone, which\nleverages the multi-scale feature representations em-\nbedded therein, and the decoder is a convolutional\n1\narXiv:2312.13314v2  [cs.CV]  8 Jan 2024\nOASIS \u2013 0.03s\nSDM \u2013 260s\nPITI \u2013 17s\nOurs \u2013 0.04s\nLabel map\nGround truth\nFigure 1 Images generated with models trained on COCO-Stuff, comparing our approach to state-of-the-art methods\nOASIS, SDM, and PITI, along with inference times to generate a single image. Our approach combines high-quality\nsamples with low-latency sampling.\nresidual network. We also propose a novel generator\narchitecture, building on the dual-pyramid modu-\nlation approach (Li et al., 2022) with an improved\nlabel map encoding through attention mechanisms\nfor better diversity and global coherence among the\nimages generated. Finally, we introduce novel loss\nand regularization terms, including a contrastive loss\nand a diversity constraint, which improve generation\nquality and diversity.\nWe validate our contributions with experiments on\nthe ADE-20K, COCO-Stuff, and Cityscapes datasets.\nOur model, termed DP-SIMS for \u201cDiscriminator Pre-\ntraining for Semantic IMage Synthesis\u201d, achieves\nstate-of-the-art performance in terms of image quality\n(measured by FID) and consistency with the input\nsegmentation masks (measured by mIoU) across all\nthree datasets. Our results not only surpass recent\ndiffusion models on both metrics, but also come with\ntwo orders of magnitude faster inference.\nIn summary, our main contributions are the following:\n\u2022 We develop an encoder-decoder discriminator\nthat leverages feature representations from pre-\ntrained networks.\n\u2022 We propose a generator architecture using atten-\ntion mechanisms for noise injection and context\nmodeling.\n\u2022 We introduce additional loss and regularization\nterms that improve sample quality and diversity.\n\u2022 We\noutperform\nstate-of-the-art\nGAN\nand\ndiffusion-based methods in image quality, input\nconsistency, and speed.\n2\nRelated work\nGenerative image modeling. Several frameworks\nhave been explored in deep generative modeling, in-\ncluding GANs (Goodfellow et al., 2014; Karras et al.,\n2021; Isola et al., 2017; Park et al., 2019; Sch\u00f6n-\nfeld et al., 2021; Kang et al., 2023; Huang et al.,\n2022), VAEs (Kingma and Welling, 2014; Vahdat and\nKautz, 2020; Razavi et al., 2019), flow-based mod-\nels (Dinh et al., 2017; Kingma and Dhariwal, 2018;\nFan et al., 2022) and diffusion-based models (Dhari-\nwal and Nichol, 2021; Ho et al., 2020; Rombach et al.,\n2022; Wang et al., 2022b,a). GANs consist of gener-\nator and discriminator networks which partake in a\nmini-max game that results in the generator learn-\ning to model the target data distribution. GANs\nrealized a leap in sample quality, due to the mode-\nseeking rather than mode-covering nature of their\nobjective function (Nowozin et al., 2016; Lucas et al.,\n2019). More recently, breakthrough results in image\nquality have been obtained using text-conditioned\ndiffusion models trained on large-scale text-image\ndatasets (Gafni et al., 2022; Ramesh et al., 2022;\nRombach et al., 2022; amd William Chan et al., 2022;\nNichol et al., 2022).\nThe relatively low sampling\nspeed of diffusion models has triggered research on\nscaling GANs to training on large-scale datasets to\nachieve competitive image quality while being orders\nof magnitude faster to sample (Kang et al., 2023).\nSemantic image synthesis.\nEarly approaches\nfor\nsemantic\nimage\nsynthesis\nleveraged\ncycle-\nconsistency between generated images and condi-\ntioning masks (Isola et al., 2017; Wang et al., 2018)\nand spatially adaptive normalization (SPADE) lay-\n2\nDecoder\nEncoder\nConvolution\nBatch Norm\nLeaky RELU\nSigmoid\nConcatenate\nUpsample \u00d72\nF\"!\nF\"\"\nF\"#\nF!\nF\"\nF#\n\u2131\ud835\udf3d\nFigure 2 Architecture of our discriminator model. The encoder consists of a pre-trained feature backbone F\u03b8\n(left), residual blocks at full image resolution (top), and trained feature decoder that aggregates the multi-scale features\nfrom the frozen backbone (right).\ners (Park et al., 2019). These approaches combined\nadversarial losses with image-to-image feature-space\nreconstruction losses to enforce image quality as well\nas consistency with the input mask (Zhang et al.,\n2018). OASIS (Sch\u00f6nfeld et al., 2021) uses a UNet\ndiscriminator model which labels pixels in real and\ngenerated images with semantic classes and an ad-\nditional \u201cfake\u201d class, which overcomes the need for\nfeature-space losses that inherently limit sample di-\nversity, while also improving consistency with the in-\nput segmentation maps. Further improvements have\nbeen made by adopting losses to learn image details\nat varying scales (Li et al., 2022), or through multi-\nmodal approaches such as PoE-GAN (Huang et al.,\n2022), which leverage data from different modalities\nlike text, sketches and segmentations.\nSeveral works have explored diffusion models for se-\nmantic image synthesis. SPADE layers were incorpo-\nrated in the denoising network of a diffusion model\nin SDM (Wang et al., 2022b) to align the generated\nimages with semantic input maps. PITI (Wang et al.,\n2022a) replaced the text-encoder of pre-trained text-\nto-image diffusion models, with a label map encoder,\nand fine-tuned the resulting model.\nIn our work,\nrather than relying on generative pre-training as in\nPITI, we leverage discriminative pre-training. An-\nother line of work considers generating images from\nsegmentation maps with free-text annotations (Coua-\niron et al., 2023; Avrahami et al., 2023). These dif-\nfusion approaches, however, suffer from poor consis-\ntency with the input label maps while also being much\nmore computationally demanding during inference\nthan their GAN-based counterparts.\nPre-trained backbones in GANs. Pre-trained\nfeature representations have been explored in various\nways in GAN training. When the model is condi-\ntioned on detailed inputs, such as sketches or segmen-\ntation maps, pre-trained backbones are used to define\na reconstruction loss between the generated and train-\ning images (Zhang et al., 2018). Another line of work\nleverages these backbones as fixed encoders in adver-\nsarial discriminators (Sauer et al., 2021; Richter et al.,\n2022). Naively using a pre-trained encoder with a\nfixed decoder yields suboptimal results, thus the Pro-\njected GANs model (Sauer et al., 2021) uses a feature\nconditioning strategy based on random projections\nto make the adversarial game more balanced. While\nthis approach is successful with some backbones, the\nmethod worked best with small pre-trained models\nsuch as EfficientNets (Tan and Le, 2019), while larger\nmodels resulted in lower performance. A related line\nof work (Kumari et al., 2022) uses an ensemble of\nmultiple pre-trained backbones to obtain a set of dis-\ncriminators from which a subset is selected at every\nstep for computing the most informative gradients.\nThis produced impressive results but has the follow-\ning significant overheads which make it inefficient: (i)\nall the discriminators and their associated optimizers\nare stored in memory, (ii) there is a pre-inference\nstep to quantify the suitability of each discriminator\nfor any given batch, and (iii) the main discriminator\nis trained from scratch. Our work is closely related\nto Projected GANs, but to our knowledge the first\none to leverage pre-trained discriminative feature\nnetworks for semantic image synthesis.\nAttention in GANs. While most of the popular\nGAN frameworks such as the StyleGAN family relied\nexclusively on convolutions (Karras et al., 2019, 2020,\n2021), some other works explored the usage of atten-\ntion in GANs to introduce a non-local parametriza-\ntion that operates beyond the receptive field of the\nconvolutions in the form of self attention (Brock et al.,\n2019; Lee et al., 2022; Zhang et al., 2019; Hudson\nand Zitnick, 2021; Kang et al., 2023), as well as cross-\nattention to incroporate information from different\n3\nmodalities (text-to-image). To the best of our knowl-\nedge, this is the first work to explore the usage of\ncross-attention for semantic image synthesis.\n3\nMethod\nSemantic image synthesis aims to produce realistic\nRGB images g \u2208 RW \u00d7H\u00d73 that are diverse and con-\nsistent with an input label map t \u2208 RW \u00d7H\u00d7C, where\nC is the number of semantic classes and W \u00d7 H is\nthe spatial resolution. A one-to-many mapping is\nensured by conditioning on a random noise vector\nz \u223c N(0, I) of dimension dz.\nIn this section, we present our GAN-based approach,\nstarting with our method to leverage pre-trained fea-\nture backbones in the discriminator (Section 3.1). We\nthen describe our noise injection and label map mod-\nulation mechanisms for the generator (Section 3.2),\nand detail the losses we use to train our models (Sec-\ntion 3.3).\n3.1\nPre-trained discriminator backbones\nWe use a UNet architecture (Ronneberger et al., 2015)\nfor the discriminator, similar to OASIS (Sch\u00f6nfeld\net al., 2021). The discriminator is trained to classify\npixels in real training images with the corresponding\nground-truth labels, and the pixels in generated im-\nages as \u201cfake\u201d. Note that the discriminator in (Sch\u00f6n-\nfeld et al., 2021) is trained from scratch and does not\nbenefit from any pre-training.\nLet F\u03b8 be a pre-trained feature backbone with pa-\nrameters \u03b8. We use this backbone, frozen, as part\nof the \u201cencoder\u201d in the UNet discriminator.\nLet\nFl \u2208 RCl\u00d7Wl\u00d7Hl denote the features extracted by\nthe backbone at levels l = 1, . . . , L, which generally\nhave different spatial resolutions Wl \u00d7Hl and number\nof channels Cl. These features are then processed\nby the UNet \u201cdecoder\u201d, which is used to predict per-\npixel labels spanning the semantic categories present\nin the input label map, as well as the \u201cfake\u201d label.\nAdditionally, to exploit high-frequency details in the\nimage, we add a fully trainable path at the full image\nresolution with two relatively shallow residual blocks.\nThe full discriminator architecture is illustrated in\nFigure 2.\nFeature conditioning.\nAn important problem\nwith using pre-trained backbones is feature condition-\ning. Typical backbones are ill-conditioned, meaning\nthat some features are much more prominent than\nothers. This makes it difficult to fully exploit the\nlearned feature representation of the backbone as\nstrong features overwhelm the discriminator\u2019s de-\ncoder and result in exploring only certain regions in\nthe feature representation of the encoder. Previously,\n(Sauer et al., 2021) tried to alleviate this problem by\napplying cross-channel mixing (CCM) and cross-scale\nmixing (CSM) to the features, while (Kumari et al.,\n2022) average the signals from multiple discrimina-\ntors to obtain a more diluted signal. Empirically, the\nfirst approach underperforms in many of our experi-\nments, as the strong features still tend to mask out\ntheir weaker, yet potentially relevant, counterparts.\nOn the other hand, the second introduces a large\noverhead from the multiple models being incorpo-\nrated in training. In our work, we develop a method\nthat better exploits the feature representation from\nthe encoder. We achieve this by aiming to make\nall features have a comparable contribution to the\ndownstream task.\nConsider a feature map Fl \u2208 RCl\u00d7Wl\u00d7Hl at scale l\nfrom the pre-trained backbone. First, we apply a\ncontractive non-linearity (CNL) such as sigmoid to\nobtain F\u2032\nl = \u03c3(Fl). Next, we normalize the features\nto ensure they have a similar contribution in the\nfollowing layers.\nWe choose batch normalization,\nyielding \u02dcFl = (F\u2032\nl \u2212 \u00b5l)/\u03c3l, where \u00b5l and \u03c3l are the\nbatch statistics. In this manner, all features are in\na similar range and therefore the decoder does not\nprioritize features with a high variance or amplitude.\n3.2\nGenerator architecture\nOur generator architecture is based on DP-GAN (Li\net al., 2022), but offers two main novelties: a revisited\nnoise injection mechanism and improved modeling\nof long-range dependencies through self-attention.\nFollowing DP-GAN, we use a mask encoding net-\nwork to condition the SPADE blocks, rather than\nconditioning the SPADE blocks on the label maps\nvia a single convolution layer, which cannot take\ninto account longer-range dependencies encoded in\nthe label map. Each block of the label map encod-\ning pyramid is made of a single convolution layer\nwith downsampling followed by batch norm, GELU\nactivation (Hendrycks and Gimpel, 2016), attention\nmodules, and a pointwise convolution layer. For every\nscale, we obtain a modulation map hi, i \u2208 {1, . . . , L}\nwhich, concatenated with a resized version of the\nultimate map hL, will serve as conditioning for the\nSPADE block at the same resolution.\nWhile (Sch\u00f6nfeld et al., 2021) argued that concate-\nnating a spatial noise map to the label map was\nenough to induce variety in the generated images,\nsince the noise is present in all SPADE blocks, and\ntherefore hard to ignore, the same cannot be said for\n4\nLabel map \nencoding\nConditional Image \ngeneration\n\ud835\udc33 \u2208 \ud835\udc45!!\n\ud835\udc30 \u2208 \ud835\udc45\"\"\u00d7!#\n\ud835\udc30\n\ud835\udc30\n\ud835\udc30\nConvolution\nConv + SPADE + RELU\nCross-Attention\nSelf-Attention\nConv + BN + RELU\nLinear + Leaky RELU\nNoise Mapping\n\ud835\udc2d\n\ud835\udc20\n\ud835\udc21\ud835\udfcf\n\ud835\udc21\ud835\udfd0\n\ud835\udc21\ud835\udfd1\nFigure 3 Our generator architecture consist of two\ncomponents. (i) A conditional image generation net-\nwork (top) that takes a low-resolution label map as input\nand produces the full-resolution output image. (ii) A\nsemantic map encoding network (bottom) that takes the\nfull resolution label map as input and produces multi-\nscale features that are used to modulate the intermediate\nfeatures of the image generation network.\nthe architecture of DP-GAN (Li et al., 2022). The\nnoise is injected only at the first layer of the label\nmap encoding network, hence it is much easier to\nignore. Consequently, we propose a different mecha-\nnism for noise injection, making use of cross-attention\nbetween the learned representations at the different\nscales and the mapping noise obtained by feeding\nz to a three-layer MLP, w = MLP(z) \u2208 Rnk\u00d7dw.\nLet hi \u2208 RCi\u00d7Hi\u00d7Wi be the downsampled feature\nrepresentation from the previous scale, hi first goes\nthrough a convolution to provide an embedding of the\nlabel map, then the spatial dimensions are flattened\nand projected via a linear layer to obtain the queries\nQ \u2208 RHiWi\u00d7dq. The transformed noise vector w is\nprojected via two linear layers to obtain the keys and\nthe values K, V \u2208 Rnk\u00d7dq, then the cross-attention\nis computed as:\nA = SoftMax\n \nQK\u22a4\np\ndq\n!\nV.\n(1)\nEach noise injection block at lower spatial resolutions\n(64 \u00d7 64 and lower) uses a noise injection module\nmade of a residual cross-attention block\na(hi, w) = hi + \u03b7i \u00b7 A(hi, w),\n(2)\nwhere \u03b7i \u2208 R is a trainable gating parameter ini-\ntialized at 0. The noise injection is followed by a\nresidual self-attention block, before having a convo-\nlution output the conditioning at scale i. For higher\nresolutions where attention modules are too costly,\nwe use convolutional blocks only. A diagram of the\ngenerator architecture is provided in Figure 3.\n3.3\nTraining\nWe train our models by minimizing a weighted aver-\nage of three loss functions which we detail below.\nPixel-wise focal loss.\nOur main loss is based\non a pixel-wise GAN loss (Sch\u00f6nfeld et al., 2021),\nwhere the discriminator aims to assign pixels in real\nimages to the corresponding class in the condition-\ning label map, and those in generated images to\nan additional \u201cfake\u201d class. To improve the perfor-\nmance on rare classes, we replace the weighted cross-\nentropy of (Sch\u00f6nfeld et al., 2021) with a weighted\nfocal loss (Lin et al., 2017), while keeping the same\nweighting scheme as in (Sch\u00f6nfeld et al., 2021). Let\np(x) \u2208 [0, 1]H\u00d7W \u00d7(C+1) denote the output class prob-\nability map of the discriminator for a real RGB image\nx, and p(g) \u2208 [0, 1]H\u00d7W \u00d7(C+1) be the probability\nmap for a generated image g = G(z, t), where the\nlabel index C + 1 is used for the \u201cfake\u201d class. Then,\nthe discriminator loss is:\nLD = \u2212E(x,t)\nC\nX\nc=1\n\u03b1c\nH\u00d7W\nX\ni=1\nti,c (1 \u2212 p(x)i,c)\u03b3 log p(x)i,c\n\u2212E(g,t)\nH\u00d7W\nX\ni=1\n(1 \u2212 p(g)i,C+1)\u03b3 log p(g)i,C+1,\n(3)\nwhere the \u03b1c are the class weighting terms and \u03b3 is\na hyper-parameter of the focal loss. The standard\ncross-entropy is recovered for \u03b3 = 0, and for \u03b3 > 0\nthe loss puts more weight on poorly predicted labels.\nThe pixel-wise loss for the generator is then takes the\nform:\nLG = \u2212E(g,t)\nC\nX\nc=1\n\u03b1c\nH\u00d7W\nX\ni=1\nti,c (1 \u2212 p(g)i,c)\u03b3 log p(g)i,c.\n(4)\nUsing the focal loss, both the generator and discrim-\ninator put more emphasis on pixels that are incor-\nrectly classified, these often belong to rare classes\nwhich helps to improve performance for these under-\nrepresented classes. To prevent the discriminator\noutput probabilities from saturating and thus lead-\ning to vanishing gradients, we apply one-sided label\nsmoothing (Salimans et al., 2016) by setting the cross-\nentropy targets to 1 \u2212 \u03f5 for the discriminator loss,\nwhere \u03f5 is a hyper-parameter.\nContrastive loss.\nWe define a patch-wise con-\ntrastive loss that encourages the generated images to\n5\nbe globally coherent. Our contrastive framework is\nbased on InfoNCE (Oord et al., 2019) which aims to\nbring matching patch features closer together, and\npush them further from non-matching features. Given\na pair (x, t) of image and label map, we generate a\ncorresponding image g = G(z, t), and use Hx and\nHg the corresponding multi-scale features obtained\nfrom a pre-trained VGG network (Simonyan and Zis-\nserman, 2015). For every scale, we sample matching\nfeatures z, z+ from the same spatial coordinates in\nHg and Hx respectively. Additionally, we sample\nN non-matching features z\u2212\nn at randomly selected\ncoordinates from Hx.\nThe features are then projected to an embedding\nspace using a convolution followed by a two-layer\nMLP to obtain v, v+, v\u2212\nn \u2208 Rdv before computing\nthe InfoNCE loss as\nLNCE(v, v+,v\u2212) = \u2212 log\n \nev\u22a4v+/\u03c4\nev\u22a4v+/\u03c4 + PN\nn=1 ev\u22a4v\u2212\nn /\u03c4\n!\n,\n(5)\nwhere \u03c4 is a temperature parameter controlling the\nsharpness in the response of the loss. We apply the\nloss at feature scales 1/4, 1/8, 1/16, and take their\nsum. This is similar to the contrastive losses used\nfor image-to-image translation (Park et al., 2020),\nand the main difference comes from the feature rep-\nresentation from which the loss is calculated. While\nother methods reuse the encoder features from their\ntranslation network, we obtain the feature pyramid\nfrom a VGG network and process these features by a\nsimple module made of a convolution block followed\nby a projection MLP.\nDiversity loss.\nTo promote diversity among the\ngenerated images we introduce a loss that encourages\ntwo images generated with the same mask, but dif-\nferent latents z, to be sufficiently distinct from each\nother. In particular, we define\nLDiv =\n\"\n\u03c4div \u2212\n\r\r\u03c3 \u25e6 Gf(z1, t) \u2212 \u03c3 \u25e6 Gf(z2, t)\n\r\r\n1\n\u2225z1 \u2212 z2\u22251\n#+\n,\n(6)\nwhere [\u00b7]+ = max(0, \u00b7) retains the non-negative part\nof its argument, Gf is the output of the generator be-\nfore the final convolution and \u03c3 the sigmoid function.\nWe adopt a cutoff threshold \u03c4div on the loss in order\nto not overly constrain the generator, and apply this\nloss only for similar samples given the same label\nmap.\n4\nExperiments\nWe present our experimental setup in Section 4.1,\nfollowed by our main results in Section 4.2, and abla-\ntions in Section 4.3.\n4.1\nExperimental setup\nDatasets.\nWe consider three popular datasets to\nbenchmark semantic image synthesis: COCO-Stuff,\nCityscapes, ADE-20K. COCO-Stuff provides 118k\ntraining images and 5k validation images, labeled\nwith 183 classes. Cityscapes contains 2,975 training\nimages along with a validation set of 500 images,\nand uses 35 labels. ADE-20K holds 26k images with\nobject segmentations across 151 classes. Similar to\n(Park et al., 2019; Wang et al., 2022b, 2018), we\nuse instance-level annotations when available. For\nCOCO-Stuff and Cityscapes, we use instance segmen-\ntations as in (Cheng et al., 2020), by creating vertical\nand horizontal offset maps of every foreground pixel\nw.r.t. it\u2019s object center of mass, and concatenate these\nto the semantic label maps as input for the model. For\nADE-20K there are no instance segmentations avail-\nable. We generate images at a resolution of 256\u00d7256\nfor ADE-20K and COCO-Stuff, and 256 \u00d7 512 for\nCityscapes. In order to avoid training models on iden-\ntifiable personal data, we use anonymized versions\nof the datasets; see the supplementary material for\nmore details.\nMetrics.\nWe compute FID (Heusel et al., 2017)\nto assess image quality. We also report the mean\nintersection-over-union score (mIoU) to measure the\nconsistency of the generated image with the input\nsegmentation maps. For a fair comparison with pre-\nvious work (Sch\u00f6nfeld et al., 2021; Li et al., 2022;\nPark et al., 2019), we used the segmentation mod-\nels from these works for inferring label maps of the\ngenerated images: UperNet101 (Xiao et al., 2018) for\nADE-20K, multi-scale DRN-D-105 (Yu et al., 2017)\nfor Cityscapes, and DeepLabV2 (Chen et al., 2018)\nfor COCO-Stuff. We refer to the scores obtained\nwith these models as mIoU. In addition, we infer\nlabel masks using Mask2Former (Cheng et al., 2022),\nwhich is more accurate than other segmentation mod-\nels, thus yielding a more meaningful comparison to\nthe ground-truth masks. We denote the resulting\nscores as mIoUMF. Additional details are provided\nin the supplementary material.\nImplementation details. We counter the strong\nclass imbalance in the datasets used in our experi-\nments with a sampling scheme favoring rare classes.\nLet fc be the fraction of training images where class\n6\nCOCO\nADE20k\nCityscapes\nFID (\u2193)\nmIoUMF (\u2191)\nmIoU (\u2191)\nFID (\u2193)\nmIoUMF (\u2191)\nmIoU (\u2191)\nFID (\u2193)\nmIoUMF (\u2191)\nmIoU (\u2191)\nPix2pixHD (Wang et al., 2018)\n111.5\n\u2014\n14.6\n73.3\n\u2014\n22.4\n104.7\n\u2014\n52.4\nSPADE (Park et al., 2019)\n22.6\n\u2014\n37.4\n33.9\n\u2014\n38.5\n71.8\n\u2014\n62.3\nOASIS (Sch\u00f6nfeld et al., 2021)\n17.0\n52.1\n44.1\n28.3\n53.5\n48.8\n47.7\n72.0\n69.3\nDP-GAN (Li et al., 2022)\n\u2014\n\u2014\n\u2014\n26.1\n\u2014\n52.7\n44.1\n\u2014\n73.6\nPoE-GAN (Huang et al., 2022)\n15.8\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nSDM (Wang et al., 2022b)\n15.9\n40.3\n36.8\n27.5\n51.9\n44.0\n42.1\n72.8\n69.1\nPITI (Wang et al., 2022a)\n15.5\n31.2\n29.5\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nDP-SIMS (ours)\n13.6\n65.2\n57.7\n22.7\n67.8\n54.3\n38.2\n78.5\n76.3\nTable 1 Comparison of our results to the state-of-the-art GAN-based (first block) and diffusion-based (second block)\nmethods. Scores for these methods are taken from their corresponding papers. We also report mIOU scores with\nMask2Former (mIoUMF) for methods where pre-trained checkpoints or generated images are available.\nBackbone\nPrms.\nFLOPS\nAcc@1\nFID (\u2193)\nmIoUMF (\u2191)\nSwin-B\n107\n15.4G\n86.4\n29.5\n55.4\nResNet-50\n44\n4.1G\n76.2\n24.6\n60.5\nEfficientNet-Lite1\n3\n631M\n83.4\n24.5\n63.1\nConvNeXt-B\n89\n15.4G\n85.1\n23.5\n63.5\nConvNeXt-L\n198\n34.4G\n85.5\n22.7\n67.8\nTable 2 Comparison of different backbone architectures on\nADE-20K (ordered by FID). The number of parameters\n(Prms.) is reported in millions. For each backbone, we\nalso report the ImageNet-1k top-1 accuracy (Acc@1) for\nreference.\nc appears, then each image is sampled with a proba-\nbility proportional to f \u22121/2\nk\nwith k the sparsest class\npresent in the image.\nEach of our models is trained on one or two machines\nwith eight V100 GPUs. We set the total batch size\nat 64 and use ADAM optimizer in all our experi-\nments with a learning rate of 10\u22123 and momentums\n\u03b21 = 0, \u03b22 = 0.99. For pre-trained Swin backbones,\nwe found it necessary to use gradient clipping to sta-\nbilize training. Following prior work, we track an\nexponential moving average of the generator weight\nand set the decay rate to \u03b1 = 0.9999. For the con-\ntrastive loss, we set the weighting factor \u03bbC = 100,\nthe temperature \u03c4 = 0.3 and select N = 128 negative\nsamples. We set \u03bbGAN = 1 for the GAN loss and\n\u03bbD = 10 for the diversity loss. For the focal loss, we\nset the focusing parameter \u03b3 = 2.\n4.2\nMain results\nComparison to the state of the art. In Table 1,\nwe report the results obtained with our model in com-\nparison to the state of the art. Our DP-SIMS method\n(with ConvNext-L backbone) achieves the best perfor-\nmance across metrics and datasets. On COCO-Stuff,\nwe improve the FID of 15.5 from PITI to 13.6, while\nimproving the mIoUMF of 52.1 from OASIS to 65.2.\nFor ADE-20K, we observe a similar trend with an im-\nprovement of 3.4 FID points w.r.t. DP-GAN, and an\nimprovement of 14.5 points in mIoUMF w.r.t. OASIS.\nFor Cityscapes we obtain smaller improvements of 3.9\nPre-training\nAcc@1\nFID (\u2193)\nmIoUMF (\u2191)\nIN-1k@224\n84.3\n22.7\n62.8\nIN-21k@224\n86.6\n23.6\n64.1\nIN-21k@384\n87.5\n22.7\n67.8\nTable 3 Influence of discriminator pre-training on the\noverall performance for ADE-20K using a ConvNext-L\nbackbone.\nFID points w.r.t. Semantic Diffusion and 5.5 points\nin mIoUMF. See Figure 1 and Figure 4 for qualita-\ntive comparisons of model trained on COCO-Stuff\nand Cityscapes. Please refer to the supplementary\nmaterial for additional examples, including ones for\nADE-20K.\nEncoder architecture. We experiment with dif-\nferent pre-trained backbone architectures for the dis-\ncriminator in Table 2. All the encoders were trained\nfor ImageNet-1k classification.\nWe find that the\nattention-based Swin architecture (Liu et al., 2021)\nhas the best ImageNet accuracy, but compared to\nconvolutional models performs worse as a discrim-\ninator backbone for semantic image synthesis, and\ntends to be more unstable, often requiring gradient\nclipping to converge. For the convolutional models,\nbetter classification accuracy translates to better FID\nand mIoUMF.\nPre-training dataset. In Table 3, we analyze the\nimpact of pre-training the ConvNext-L architecture in\ndifferent ways and training our models on top of these,\nwith everything else being equal. We consider pre-\ntraining on ImageNet-1k (IN-1k@224) and ImageNet-\n21k (IN-21k@224) at 224\u00d7224 resolution, and also on\nImageNet-21k at 384 \u00d7 384 resolution (IN-21k@384).\nIn terms of mIoUMF, the results are in line with those\nobserved for different architectures: discriminators\ntrained with larger datasets (IN-21k) and on higher\nresolutions perform the best. On the other hand, we\nfind that for FID, using the standard ImageNet (IN-\n1k@224) results in better performance than its bigger\nIN-21k@224 counterpart, and performs as well as IN-\n21k@384 pre-training. This is likely due to the use\n7\nLabel map\nOASIS\nSDM\nOurs\nFigure 4 Qualitative comparison with prior work on the Cityscapes dataset. We show the results of OASIS (Sch\u00f6nfeld\net al., 2021), SDM (Wang et al., 2022b), and our approach along with the corresponding label map used for generating\neach image. Note that our method generates more coherent objects with realistic textures in comparison.\nModel\nGen. steps\nUps. steps\n\u2206tgen\n\u2206tups\n\u2206ttot\nPITI\n250\n27\n14.3\n3.1\n17.4\nPITI\n27\n27\n1.5\n3.1\n4.6\nSDM\n1000\n\u2014\n260.0\n\u2014\n260.0\nDP-SIMS\n\u2014\n\u2014\n\u2014\n\u2014\n0.04\nTable 4 Comparison of inference time (in seconds) of\nPITI, SDM and our GAN-based model. We show the\ntime taken by the generative (\u2206tgen) and the upsampling\n(\u2206tups) models in addition to the total time (\u2206ttot) for\nthese steps.\nof the same dataset in the Inception model (Szegedy\net al., 2016), which is the base for calculating FID,\nintroducing a bias in the metric.\nInference speed. An important advantage of GAN\nmodels over their diffusion counterparts is their fast\ninference.\nWhile a GAN only needs one forward\npass to generate an image, a diffusion model requires\nseveral iterative denoising steps, resulting in slower\ninference, which can hamper the practical usability of\nthe model. In Table 4 we report the inference speed\nfor generating a single 256\u00d7256 image, averaged over\n50 different runs. PITI uses 250 denoising steps for\nthe generative model at 64 \u00d7 64 resolution and 27\nsteps for the upsampling model by default, while SDM\nuses 1000 steps at full resolution. We also benchmark\nusing 27 steps for the PITI generative model. Our\ngenerator is two to three orders of magnitude faster\nthan its diffusion counterparts.\nFID (\u2193)\nmIoUMF (\u2191)\nBaseline - no normalization\n27.8\n58.6\nProjected GAN\n28.9\n59.1\nDP-SIMS w/o sigmoid\n24.9\n62.7\nDP-SIMS w/o BatchNorm\n26.0\n61.6\nDP-SIMS\n24.5\n63.1\nTable 5 Ablation on feature conditioning shown on ADE-\n20K with EfficientNet-Lite1 backbone.\n4.3\nAblations\nFeature Conditioning. We perform an ablation\nto validate our feature conditioning mechanisms on\nADE-20K in Table 5. We compare the normaliza-\ntion in DP-SIMS to: (i) a baseline approach where\nthe backbone features are unchanged before entering\nthe discriminator network (\u201cBaseline - no normaliza-\ntion\u201d), (ii) the Projected GAN (Sauer et al., 2021)\napproach with cross-channel and scale mixing, and\nusing the normalization layer without (iii) the con-\ntractive non-linearity (\u201c/ours w/o sigmoid\u201d) or (iv)\nnormalisation (\u201cDP-SIMS w/o BatchNorm\u201d). For a\nfair comparison with (Sauer et al., 2021), these experi-\nments are conducted on their best reported backbone,\nEfficientNet-Lite1 (Tan and Le, 2019). Compared to\nthe baseline method, the Projected GAN approach\nimproves mIoUMF by 0.5 points, but degrades FID\nby 1.1 points. Using our feature conditioning based\non BatchNorm and sigmoid activation, we improve\nthe mIoUMF\nby 4.5 points and FID by 3.3 points\n8\nFID (\u2193)\nmIoUMF (\u2191)\nDP-SIMS\n22.7\n67.8\nGenerator architecture\nOASIS disc + our gen\n29.3\n49.0\nOASIS gen + our disc\n25.6\n63.6\nOurs w/o self-attention\n23.7\n65.4\nOurs w/o cross-attention\n23.6\n64.5\nTraining\nOurs w/o label smoothing\n23.0\n66.3\nOurs w/o contrastive loss\n25.1\n66.0\nTable 6 Ablations on the architectural design and training\nlosses, shown on ADE-20K with ConvNext-L backbone.\n\u03c4\n0.07\n0.3\n0.7\n2.0\nFID\n25.7\n22.7\n24.1\n26.4\nmIoUMF\n62.6\n67.8\n66.3\n61.4\nTable 7 Influence of the contrastive loss evaluated on\nADE-20K.\nw.r.t. the baseline. When we leave out the sigmoid\nactivation both mIoUMF and FID drop by 0.4 points.\nWe observe a similar behavior without normaliza-\ntion: FID and mIoUMF both decrease by 1.5 points.\nThese results validate the fittingness of our feature\nconditioning approach.\nArchitectural modifications. In Table 6, we per-\nform an ablation on our proposed architectural modifi-\ncations. Swapping out our generator or discriminator\nwith the ones from OASIS, suggests that most of\nthe gains are due to our discriminator design. Using\nthe OASIS discriminator instead of ours deteriorates\nmIoUMFby 18.8 points and FID by 6.6 points. We\nalso experiment with removing the cross-attention\nnoise injection mechanism and replacing it with the\nusual concatenation instead, as well as leaving out\nthe self-attention layers. Both of these contribute to\nthe final performance in a notable manner. Finally,\nwe include an ablation on label smoothing, which\ndeteriorates FID by 0.3 and mIoUMFby 1.4 points\nwhen left out.\nContrastive loss. To assess the importance of the\ncontrastive loss, we perform an ablation in the last\nrow of Table 6 where we remove it during training.\nThis substantially impacts the results: worsening FID\nby 2.4 and mIoUMF by 1.8 points. In Table 7 we\nevaluate different values for the temperature param-\neter \u03c4. We find an optimal temperature parameter\n\u03c4C = 0.3, using \u03bbc = 100.\nFocal loss. In Table 8, we consider the impact of\nthe focal loss by comparing it to the weighted cross-\nentropy loss, as used in OASIS, and the effect of\nLoss\nFID (\u2193)\nmIoUMF(\u2191)\nWeighted CE\n39.8\n75.9\nFocal\n39.3\n75.0\nWeighted Focal\n38.2\n78.5\nTable 8 Comparison of pixel-wise losses on Cityscapes\nwith ConvNext-L backbone.\nModel\n3D noise\nLPIPS (\u2191)\nSPADE+\n\u2713\n0.16\nSPADE+\n\u2717\n0.50\nOASIS\n\u2713\n0.35\nDP-SIMS\n\u2717\n0.47\nTable 9 Evaluation of the diversity of images generated.\nResults for SPADE+ and OASIS are taken from (Sch\u00f6n-\nfeld et al., 2021).\nclass weighting in the focal loss. This comparison is\nconducted on Cityscapes because it is more imbal-\nanced than ADE-20K. We find that switching from\nweighted cross-entropy to the focal loss improves FID\nby 0.5 points but worsens mIoUMF by 0.9 points.\nWhen comparing weighted focal loss to weighted CE,\nwe observe that FID improves by 1.6 points, and\nmIoUMF by 2.6 points.\nDiversity. We investigate the effect of the diversity\nloss on the variability of generated images. Follow-\ning (Sch\u00f6nfeld et al., 2021), we report the mean\nLPIPS distance across 20 synthetic images from the\nsame label map, averaged across the validation set,\nin Table 9.\nA qualitative example is provided in\nFigure 5 showing a clear variety in the images gener-\nated. In comparison with OASIS, we generate more\ndiverse images, with an LPIPS score similar to that of\nSPADE, but with a much higher quality, as reported\nin Table 1, in terms of FID and mIoUMF.\nFigure5 Images generated by varying the noise vector with\nDP-SIMS trained on COCO-Stuff and using a ConvNext-\nL backbone.\n9\n5\nConclusion\nWe introduced DP-SIMS that harnesses pre-trained\nbackbones in GAN-based semantic image synthesis\nmodels. We achieve this by using them as an en-\ncoder in UNet-type discriminators, and introduce a\nfeature conditioning approach to maximize the effec-\ntiveness of the pre-trained features. Moreover, we\npropose a novel generator architecture which uses\ncross-attention to inject noise in the image genera-\ntion process, and introduce new loss terms to boost\nsample diversity and input consistency. We exper-\nimentally validate our approach and compare it to\nstate-of-the-art prior work based on GANs as well\nas diffusion models on three standard benchmark\ndatasets. Compared to these, we find improved per-\nformance in terms of image quality, sample diver-\nsity, and consistency with the input segmentation\nmaps. Importantly, with our approach inference is\ntwo orders of magnitude faster than diffusion-based\napproaches.\nIn our experiments we found that transformer-based\nmodels, such as Swin, can lead to instability when\nused as discriminator backbones. Given their strong\nperformance for dense prediction tasks, it would be\nworthwhile to further study and mitigate this issue in\nfuture work, hopefully bringing additional improve-\nments.\nReferences\nMohamed Akrout, B\u00e1lint Gyepesi, P\u00e9ter Holl\u00f3, Adrienn\nPo\u00f3r, Bl\u00e1ga Kincs\u0151, Stephen Solis, Katrina Cirone,\nJeremy Kawahara, Dekker Slade, Latif Abid, M\u00e1t\u00e9\nKov\u00e1cs, and Istv\u00e1n Fazekas. Diffusion-based data aug-\nmentation for skin disease classification: Impact across\noriginal medical datasets to fully synthetic images.\narXiv 2301.04802, 2023.\nChitwan Saharia amd William Chan, Saurabh Sax-\nena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan,\nS. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans,\nJonathan Ho, David J Fleet, and Mohammad Norouzi.\nPhotorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS, 2022.\nOmri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad\nFried, and Xi Yin. Spatext: Spatio-textual represen-\ntation for controllable image generation. In CVPR,\n2023.\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-\nhammad Norouzi, and David J. Fleet. Synthetic data\nfrom diffusion models improves imagenet classification.\nTMLR, 2023.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high fidelity natural image\nsynthesis. In ICLR, 2019.\nMarl\u00e8ne Careil, Jakob Verbeek, and St\u00e9phane Lathuili\u00e8re.\nFew-shot semantic image synthesis with class affinity\ntransfer. In CVPR, 2023.\nLiang-Chieh Chen, George Papandreou, Iasonas Kokki-\nnos, Kevin Murphy, and Alan L. Yuille.\nDeepLab:\nSemantic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected CRFs.\nPAMI, 40(4):834\u2013848, 2018.\nBowen Cheng, Maxwell D Collins, Yukun Zhu, Ting\nLiu, Thomas S Huang, Hartwig Adam, and Liang-\nChieh Chen. Panoptic-deeplab: A simple, strong, and\nfast baseline for bottom-up panoptic segmentation. In\nCVPR, 2020.\nBowen Cheng, Ishan Misra, Alexander G. Schwing,\nAlexander Kirillov, and Rohit Girdhar.\nMasked-\nattention mask transformer for universal image seg-\nmentation. In CVPR, 2022.\nGuillaume Couairon, Marl\u00e8ne Careil, Matthieu Cord,\nSt\u00e9phane Lathuili\u00e8re, and Jakob Verbeek. Zero-shot\nspatial layout conditioning for text-to-image diffusion\nmodels. In ICCV, 2023.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat\nGANs on image synthesis. In NeurIPS, 2021.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.\nDensity estimation using Real NVP. In ICLR, 2017.\nWeichen Fan, Jinghuan Chen, Jiabin Ma, Jun Hou, and\nShuai Yi. Styleflow for content-fixed image to image\ntranslation. arXiv, 2207.01909, 2022.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman. Make-a-scene: Scene-\nbased text-to-image generation with human priors. In\nECCV, 2022.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. In NeurIPS, 2014.\nReyhane Askari Hemmat, Mohammad Pezeshki, Florian\nBordes, Michal Drozdzal, and Adriana Romero-Soriano.\nFeedback-guided data synthesis for imbalanced classifi-\ncation. arXiv, 2310.00158, 2023.\nDan Hendrycks and Kevin Gimpel. Bridging nonlinear-\nities and stochastic regularizers with Gaussian error\nlinear units. arXiv, 1606.08415, 2016.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained\nby a two time-scale update rule converge to a local\nNash equilibrium. In NeurIPS, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. In NeurIPS, 2020.\n10\nXun Huang, Arun Mallya, Ting-Chun Wang, and Ming-\nYu Liu. Multimodal conditional image synthesis with\nproduct-of-experts GANs. In ECCV, 2022.\nDrew A. Hudson and C. Lawrence Zitnick. Generative\nadversarial transformers. In ICML, 2021.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros.\nImage-to-image translation with conditional\nadversarial networks. CVPR, 2017.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scal-\ning up GANs for text-to-image synthesis. In CVPR,\n2023.\nTero Karras, Samuli Laine, and Timo Aila.\nA style-\nbased generator architecture for generative adversarial\nnetworks. In CVPR, 2019.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of StyleGAN. In CVPR,\n2020.\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-\nfree generative adversarial networks. In NeurIPS, 2021.\nDiederik Kingma and Prafulla Dhariwal. Glow: Genera-\ntive flow with invertible 1\u00d71 convolutions. In NeurIPS,\n2018.\nDiederik Kingma and Max Welling. Auto-encoding varia-\ntional Bayes. In ICLR, 2014.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDoll\u00e1r, and Ross Girshick. Segment anything. arXiv\npreprint, 2023.\nNupur Kumari, Richard Zhang, Eli Shechtman, and Jun-\nYan Zhu. Ensembling off-the-shelf models for GAN\ntraining. In CVPR, 2022.\nKwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang,\nZhuowen Tu, and Ce Liu. ViTGAN: Training GANs\nwith vision transformers. In ICLR, 2022.\nShijie Li, Ming-Ming Cheng, and Juergen Gall. Dual\npyramid generative adversarial networks for semantic\nimage synthesis. In BMVC, 2022.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Doll\u00e1r. Focal loss for dense object detection.\nIn ICCV, 2017.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted\nwindows. In ICCV, 2021.\nThomas Lucas, Konstantin Shmelkov, Karteek Alahari,\nCordelia Schmid, and Jakob Verbeek. Adaptive density\nestimation for generative models. In NeurIPS, 2019.\nStanislav Morozov, Andrey Voynov, and Artem Babenko.\nOn self-supervised image representations for gan evalu-\nation. In ICLR, 2021.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. GLIDE: Towards photorealistic im-\nage generation and editing with text-guided diffusion\nmodels. In ICML, 2022.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka.\nf-GAN: Training generative neural samplers using vari-\national divergence minimization. In NeurIPS, 2016.\nA\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Rep-\nresentation learning with contrastive predictive coding.\narXiv, 1807.03748, 2019.\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-\nYan Zhu.\nSemantic image synthesis with spatially-\nadaptive normalization. In CVPR, 2019.\nTaesung Park, Alexei A. Efros, Richard Zhang, and Jun-\nYan Zhu. Contrastive learning for unpaired image-to-\nimage translation. In ECCV, 2020.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. Hierarchical text-conditional\nimage generation with CLIP latents. arXiv preprint,\n2204.06125, 2022.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with VQ-VAE-2.\nIn NeurIPS, 2019.\nStephan R. Richter, Hassan Abu AlHaija, and Vladlen\nKoltun. Enhancing photorealism enhancement. IEEE\nTPAMI, 45(2):1700\u20131715, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution\nimage synthesis with latent diffusion models. In CVPR,\n2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-Net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved tech-\nniques for training GANs. In NeurIPS, 2016.\nAxel Sauer, Kashyap Chitta, Jens M\u00fcller, and Andreas\nGeiger. Projected GANs converge faster. In NeurIPS,\n2021.\nEdgar Sch\u00f6nfeld, Vadim Sushko, Dan Zhang, Juergen\nGall, Bernt Schiele, and Anna Khoreva. You only need\nadversarial supervision for semantic image synthesis.\nIn ICLR, 2021.\nKaren Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition.\nIn ICLR, 2015.\n11\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathan Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR,\n2016.\nMingxing Tan and Quoc V. Le. EfficientNet: Rethinking\nmodel scaling for convolutional neural networks. In\nICML, 2019.\nArash Vahdat and Jan Kautz. NVAE: A deep hierarchical\nvariational autoencoder. In NeurIPS, 2020.\nTengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang,\nDong Chen, Qifeng Chen, and Fang Wen. Pretraining\nis all you need for image-to-image translation. arXiv,\n2205.12952, 2022a.\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew\nTao, Jan Kautz, and Bryan Catanzaro. High-resolution\nimage synthesis and semantic manipulation with con-\nditional GANs. In CVPR, 2018.\nWeilun Wang, Jianmin Bao, Wengang Zhou, Dongdong\nChen, Dong Chen, Lu Yuan, and Houqiang Li. Seman-\ntic image synthesis via diffusion models. arXiv preprint,\n2207.00050, 2022b.\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang,\nand Jian Sun. Unified perceptual parsing for scene\nunderstanding. In ECCV, 2018.\nFisher Yu, Vladlen Koltun, and Thomas Funkhouser.\nDilated residual networks. In CVPR, 2017.\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Au-\ngustus Odena. Self-attention generative adversarial\nnetworks. In ICML, 2019.\nR. Zhang, P. Isola, A. Efros, E. Shechtman, and O. Wang.\nThe unreasonable effectiveness of deep features as a\nperceptual metric. In CVPR, 2018.\nHanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong\nChen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo\nZhou, Qi Chu, Weiming Zhang, and Nenghai Yu. X-\nPaste: Revisiting scalable copy-paste for instance seg-\nmentation using CLIP and StableDiffusion. In ICML,\n2023.\n12\nA\nMore details on the experimental\nsetup\nA.1\nArchitecture details\nGenerator. As can be seen in Figure 3 of the main\npaper, our generator consists of a UNet-like architec-\nture with two pyramidal paths. The label map encod-\ning takes the input segmentation map, and progres-\nsively downsamples it to produce label-conditioned\nmulti-scale features. These features are then used in\nthe image generation path, which progresively upsam-\nples the signal to eventually produce an RGB image.\nThe stochasticity of the images generated is based\non conditioning on the noise vector z. We provide a\nschematic overview of the noise injection operation\nin Figure S1. In Table S1, we provide additional in-\nformation on the label map encoding and the image\ngeneration paths.\nIn the label map encoding branch, each block is made\nof the elements listed in Table S1. Cross-attention\nand self-attention are only applied at lower resolutions\n(64 \u00d7 64, and lower) where we use an embedding di-\nmension that is half of the original feature dimension.\nWe downscale the feature maps by using convolution\nlayers with a stride of 2.\nIn the image synthesis branch, we follow the same\narchitecture as OASIS Sch\u00f6nfeld et al. (2021) with\nthe only difference being the SPADE conditioning\nmaps which are given by the label map encoding path\ninstead of a resized version of the label maps. We\nalso remove the hyperbolic tangent at the output of\nthe network as we found it leads to a more stable\ngenerator.\nFor the contrastive learning branch, features obtained\nfrom VGG19 go through three convolutional blocks\nand two linear layers for projection. We sample 128\ndifferent patches to obtain negative samples from the\nimage.\nDiscriminator.\nWe provide additional details of\nour discriminator architecture in Table S2. The resid-\nual blocks are made of one convolution with kernel\nsize 3 followed by leaky ReLU, then a pointwise con-\nvolution with leaky ReLU. For the full resolution\nchannel, we set the dimensionality to 128. For the\nlower resolution channels, we stick to the same di-\nmensionality as the corresponding encoder feature.\nThe dimensionality of the final convolution before\npredicting the segmentations is set to 256.\nWe use spectral norm on all convolutional and linear\nlayers in both the generator and the discriminator.\nFeature conditioning.\nIn ProjectedGAN\u2019s pa-\nper Sauer et al. (2021), they observe that when using\na fixed feature encoder in the GAN discriminator,\nonly a subset of features is covered by the projec-\ntor. They therefore propose propose to dilute promi-\nnent features, encouraging the discriminator to utilize\nall available information equally across the different\nscales. However, they do not delve deeper into the\npractical reasons behind this. Namely, feature en-\ncoders trained for a discriminative task will have\ndifferent structures to those trained on generative\ntasks.\nFor the former, models tend to capture a\nsubset of key features while disregarding other less\nrelevant features. On the latter however, the model\nneeds an extensive representation of the different ob-\njects it should generate. In practice, this translates to\nfeature encoders having bad conditioning. The range\nof activations differs greatly from one feature to the\nother, which leads to bias towards a minority features\nthat have a high amplitude of activations. A simple\nway to resolve this issue is by applying normalization\nthese features to have a distribution with zero mean\nand a unit standard deviation across the batch.\nIn some situations, linear scaling of the features might\nnot be enough to have proper conditioning of the\nfeatures. Accordingly, we reduce the dynamic range\nof the feature maps before the normalization by using\na sigmoid activation at the feature outputs of the\npretrained encoder.\nFixed vs finetuned backbone. We experimented\nwith finetuning the whole discriminator, encoder in-\ncluded. In all of our attempts, such models diverged\nwithout producing meaningful results. We therefore\nchose to keep the encoder fixed for this work.\nA.2\nComputationofthemIoUevaluationmet-\nrics\nTo compute the mIoU metric, we infer segmentation\nmaps for generated images. As noted in the main\npaper, we infer segmentation maps for the generated\nimages using the same networks as in OASIS Sch\u00f6n-\nfeld et al. (2021), that is: UperNet101 Xiao et al.\n(2018) for ADE-20K, multi-scale DRN-D-105 Yu et al.\n(2017) for Cityscapes, and DeepLabV2 Chen et al.\n(2018) for COCO-Stuff. We also measure mIoU using\nMask2Former Cheng et al. (2022) with Swin-L back-\nbone Liu et al. (2021) (mIoUMF), which infers more\naccurate segmentation mask, thus producing a more\naccurate comparison to the ground-truth masks.\nIn Table S3 we compare the segmentation accuracy on\nthe three datasets we used in our experiments. The\nresults confirm that Mask2Former is more accurate\n13\n\ud835\udc33 \u2208 \u211d!!\n\ud835\udc30 \u2208 \u211d\"\"\u00d7!#\n\ud835\udc21$\n\ud835\udc00(\ud835\udc21$, \ud835\udc30)\n\ud835\udc10 \u2208 \u211d%$&$\u00d7!% \n\ud835\udc0a \u2208 \u211d\"\"\u00d7!% \n\ud835\udc15 \u2208 \u211d\"\"\u00d7!% \n\ud835\udc10\ud835\udc0a'\n\u221a\ud835\udc51(\n\ud835\udc00 =SoftMax\n\ud835\udc10\ud835\udc0a&\n!% \ud835\udc15\nConvolution\nLinear\nLinear + Leaky RELU\n\ud835\udc4e \ud835\udc21!, \ud835\udc30 = \ud835\udc21! + \ud835\udf02!. \ud835\udc00(\ud835\udc21!, \ud835\udc30)\nFigure S1 Schematic overview of our noise injection mechanism using cross-attention.\nParameter\nDescription\nHyperparameters\nz dimension\n64\nw dimension\n256\nBatch size\n64\nLearning rate\n10\u22123\n\u03b21 for Adam\n0\n\u03b22 for Adam\n0.99\nEMA beta\n0.9999\nLabel map encoding\nPyramid block\nConv2d(kernel_size=3), BN, GELU, CrossAttention, BN,\nGELU, SelfAttention, GELU, BN, Conv2d(kernel_size=1)\nSelf Attention channel divider\n2\nCross Attention channel divider\n2\nConv block\nConv2d(kernel_size=3), BN, GELU, Conv2d(kernel_size=1)\nBlock type\n[Conv, Conv, Conv, Linear, Linear]\nImage synthesis branch\nChannel base\n64\nNumber of residual blocks\n6\nChannel depths\n[1024, 1024, 1024, 512, 256, 128, 64]\nResidual block\nSPADE, Leaky RELU, Conv2d(3)\nPyramid dimensionality\n64\nHyperbolic tangent on output\nNo\nContrastive learning branch\nPerceptual network\nVGG19\nContrastive encoding channels\n[64, 128, 256, 512, 512]\nContrastive embedding dimension\n256\nNumber of patches\n128\nTable S1 Architecture details of the generator.\nParameter\nDescription\nHyperparameters\nNumber of multiscale backbone features\n4\nFull resolution embedding dimension\n128\nNumber of residual blocks\n5\nDecoder\nResidual block\nConv2d(kernel_size=3), Leaky RELU, Conv2d(kernel_size=1), Leaky RELU\nLeaky RELU slope\n0.2\nPenultimate channel dimension\n256\nFeature conditioning\nConditioning normalization\nBatch Norm w.o learned affine\nConditioning non-linearity\nHyperbolic tangent\nTable S2 Architecture details of the discriminator.\n14\nADE-20K\nCityscapes\nCOCO-Stuff\nUperNet101\n42.7\n\u2014\n\u2014\nMS DRN-D-105\n\u2014\n61.3\n\u2014\nDeepLab-v2\n\u2014\n\u2014\n35.3\nMask2Former\n45.3\n69.9\n54.5\nTable S3 Segmentation performance in terms of mIoU\non real images using different segmentation models. To\nmatch the setting used in our semantic image synthe-\nsis experiments, evaluation images are downsampled to\n256 \u00d7 256 for ADE-20K and COCO, and to 256 \u00d7 512 for\nCityscapes.\nDataset\nModel\nBlurring\nFID (\u2193)\nmIoUMF(\u2191)\nOASIS\n\u2717\n17.0\n52.1\nOASIS\n\u2713\n18.8\n47.1\nCOCO-Stuff\nDP-SIMS (ours)\n\u2713\n13.6\n65.2\nOASIS\n\u2717\n28.3\n53.5\nOASIS\n\u2713\n29.1\n49.6\nADE-20K\nDP-SIMS (ours)\n\u2713\n22.7\n67.8\nOASIS\n\u2717\n47.7\n72.0\nOASIS\n\u2713\n48.0\n71.6\nCityscapes\nDP-SIMS (ours)\n\u2713\n38.2\n78.5\nTable S4 Influence of face blurring on the performance of\nOASIS.\nfor all three datasets, in particular on COCO-Stuff,\nwhere it boosts mIoU by more than 19 points w.r.t.\nDeepLab-v2.\nA.3\nInfluence of face blurring\nTo avoid training models on identifiable personal\ndata, we did not include such information in the\ntraining dataset. For Cityscapes we use the release\nof the dataset with blurred faces and licence plates,\nwhich is available publicly on the website listed in\nTable S11a. For ADE-20K and COCO-Stuff we exe-\ncuted our pipeline to detect, segment and blur faces.\nTo this end, we use Retina-Net Lin et al. (2017) and\nsegment anything (SAM) Kirillov et al. (2023) to\ndetect and blur human faces. First we detect the\nbounding boxes of the faces for each image in our\ndataset, then we obtain its corresponding segmenta-\ntion through SAM. Finally, this face region detected\nby SAM is blurred with a Gaussian filter.\nTo assess the impact of blurring, we train OASIS on\nblurred images using the original source code from the\nauthors and compare to their reported results on the\nnon-blurred data. We report our results in Table S4.\nHere, and elsewhere in the paper, we also use the\nblurred data to compute FID w.r.t. the generated\nimages. We see that blurring has a negative impact\non FID, most notably for COCO-Stuff (+1.8), and to\na lesser extent for ADE-20K (+0.8) and Cityscapes\n(+0.3). The mIoUMF scores also degrade on all the\ndatasets when using blurred data: on COCO-Stuff,\nADE-20K and Cityscapes by 5.0, 3.9, and 0.4 points\nrespectively. Note that in all comparisons to the state\nof the art, we report metrics obtained using models\ntrained on blurred data for our approach, and models\ntrained on non-blurred data for other approaches.\nTherefore, the real gains of our method over OASIS\n(and probably other methods as well) are even larger\nthan what is shown in our comparisons in Table 1 in\nthe main paper.\nA.4\nCarbon footprint estimation\nOn COCO-Stuff, it takes approximately 10 days =\n240 hours to train our model using 8 GPUs. On ADE-\n20K and Cityscapes the training times are about 6\nand 4 days respectively.\nGiven a thermal design\npower (TDP) of the V100-32G GPU equal to 250W,\na power usage effectiveness (PUE) of 1.1, a carbon\nintensity factor of 0.385 kg CO2 per KWh, a time\nof 240 hours \u00d7 8 GPUs = 1920 GPU hours. The\n250 \u00d7 1.1 \u00d7 1920 = 528 kWh used to train the model\nis approximately equivalent to a CO2 footprint of\n528 \u00d7 0.385 = 208 kg of CO2 for COCO-Stuff. For\nADE-20K this amounts to 124 kg of CO2, and 83 kg\nof CO2 for Cityscapes.\nB\nAdditional experimental results\nB.1\nQuantifying the ImageNet bias\nSince our backbones are pre-trained on ImageNet\nwhich is the same dataset that Inceptionv3 Szegedy\net al. (2016) used for the standard FID model was\ntrained on, a natural question arises whether our\nresults are influenced by this bias towards the Ima-\ngeNet dataset. To analyze this, we report in Table S5\na quantitative comparison following the approach\noutlined in Morozov et al. (2021), where we compute\nthe Fr\u00e9chet distance between two Gaussians fitted to\nfeature representations of the SwAV Resnet50 net-\nwork that was pretrained in a self-supervised manner.\nOur models retain state-of-the-art performance with\nrespect to this metric on all the three datasets stud-\nied. Additionally, we further experiment with the\ninfluence of the backbone pre-training in Table S6.\nDifferently from the main paper where FID is studied,\nwe find than the IN-21k checkpoint brings about bet-\nter performance than its IN-1k counterpart. While\nthe fine-tuning at high resolution (384 vs 224) also\nimproves swav-FID.\nB.2\nInfluence of diversity loss\nWe obtain the diversity cutoff threshold \u03c4div by com-\nputing the mean distance between different generated\n15\n\ud835\udc67!\n\ud835\udc67\"\n\ud835\udc61. \ud835\udc67! + 1 \u2212 \ud835\udc61 . \ud835\udc67\",\n\ud835\udc61 \u2208 [0,1]\nFigure S2 Noise vector interpolation. By interpolating the noise vector between two different values, we can identify\nthe factors of variation in the image which correspond to differences in colors, textures and also object structures.\nOASIS\nSDM\nPITI\nDP-SIMS\nCOCO-Stuff\n3.09\n2.68\n2.52\n2.14\nADE-20K\n4.35\n3.85\n\u2014\n2.84\nCityscapes\n4.75\n3.94\n\u2014\n3.71\nTable S5 Evaluation of SwAV Resnet50 FID on different\nmethods. We use ConvNext-L for our method.\nPre-training\nAcc@1\nFIDSwAV (\u2193)\nIN-1k@224\n84.3\n3.03\nIN-21k@224\n86.6\n2.97\nIN21k@384\n87.5\n2.84\nTable S6 Evaluation of SwAV Resnet50 FID with different\npre-trainings.\nimages in a batch and averaging across 5k different\nbatches in the training set:\n\u03c4div =\nX\ni,j\u2208B\n\r\r\u03c3 \u25e6 Gf(xi, zi) \u2212 \u03c3 \u25e6 Gf(xj, zj)\n\r\r\n1\n\u2225zi \u2212 zj\u22251\n. (7)\nThe distance is computed in the feature space of the\ngenerator before the final convolution that produces\nthe image. It is then normalized by the distance\nbetween the noise vectors. We found it necessary\nto add a sigmoid activation on the features before\ncomputing the loss, similar to backbone condition-\ning. Not using this activation results in unwanted\neffects where a tiny patch in the image has extremely\nhigh values leading to a large distance between two\notherwise similar images.\nWe conduct a more in-depth analysis on the impact of\nthe diversity loss on the image quality and diversity.\nWe train our model with a ConvNext-L backbone\nwith different values for the diversity loss \u03bbdiv. These\nresults are reported in Table S7. Without the di-\nversity loss, the generator ignores the noise input,\nwhich translates to a low LPIPS score. Improving\ndiversity with a weight of \u03bbdiv = 10 results in better\nimage quality, measured by FID as well as mIoUMF,\nwhile adding a variability factor induced from the\nnoise input.\nIf the weight of the diversity loss is\ntoo high, image quality deteriorates (FID reduces by\n0.6 points and mIoUMF\ndiminishes by 0.1 points)\nwithout any noticeable improvements in diversity (no\nimprovements in LPIPS by setting \u03bbdiv = 100 instead\nof 10). We provide more examples of diverse genera-\ntions in Figure S3, and in Figure S2 we perform noise\nvector interpolation in order to better illustrate the\nvariation factors induced by the injected noise.\n\u03bbdiv\n0\n10\n100\nFID (\u2193)\n22.9\n22.7\n23.3\nmIoUMF(\u2191)\n67.7\n67.8\n67.7\nLPIPS (\u2191)\n1.5e-5\n0.47\n0.36\nTable S7 Influence of diversity loss weight on model per-\nformance.\nWe evaluate image quality using FID and\nmIoUMF metrics while diversity is evaluated using LPIPS.\nB.3\nSampling strategy\nWe quantify the influence of the balanced sampling\nstrategy with respect to standard uniform sampling\non COCO-Stuff and Cityscapes datasets. We report\nthese results in Table S8, and find that balanced\nsampling yields performance gains in both FID and\nmIoU for both the datasets. In Figure S4, we present\nqualitative examples of images generated with the\nmodel trained on Cityscapes.\nBalanced sampling\nclearly leads to improvements in the visual quality of\nobjects such as scooters, buses and trams.\n16\nFigure S3 Additional examples of diversity in our gener-\nated images.\nDataset\nSampling strategy\nFID (\u2193)\nmIoUMF(\u2191)\nUniform\n14.1\n62.9\nCOCO-Stuff\nBalanced\n13.6\n65.2\nUniform\n38.7\n75.6\nCityscapes\nBalanced\n38.3\n78.3\nTable S8 Influence of sampling strategy for models trained\non the COCO-Stuff and Cityscapes datasets.\nB.4\nInfluence of pixel-wise loss function\nIn Figure S5, we compare the per-class mIoU values\nwhen training using different loss functions: weighted\ncross-entropy (as in OASIS), focal loss, and weighted\nfocal loss. This extends the class-aggregated results\nreported in Table 8 in the main paper. These ex-\nperiments were conducted on the Cityscapes dataset\nusing a pre-trained ConvNext-L backbone for the\ndiscriminator. Our use of the weighted focal loss to\ntrain the discriminator results in improved IoU for\nmost classes. The improvements tend to be larger\nfor rare classes. Class weighting is still important, as\ncan be seen from the deteriorated IoU for a number\nof classes when using the un-weighted focal loss.\nB.5\nInfluence of instance-level annotations\nSince some works do not use the instance masks\nSch\u00f6nfeld et al. (2021); Li et al. (2022); Wang et al.\n(2022a), we provide an additional ablation where\nwe train our models on COCO-Stuff and Cityscapes\nwithout the instance masks to isolate the gains in\nperformance they may bring. For both these datasets,\nwe observe deterioration in the model\u2019s performance\nwhen not using instance masks. The difference is\nless noticeable on COCO-Stuff where the labels are\nalready partially separated, FID only increases by\n0.3 points. On the other hand, this difference is more\nacute in Cityscapes where FID increases by 1.9 points\nwhile mIoUMFreduces by 2.2 points. In Cityscapes,\ninstances are not separated in the semantic label\nmaps, this adds more ambiguity to the labels pre-\nsented to the model which makes it more difficult to\ninterpret them in a plausible manner.\nDataset\nInstance masks\nFID (\u2193)\nmIoUMF(\u2191)\n\u2717\n13.9\n65.0\nCOCO-Stuff\n\u2713\n13.6\n65.2\n\u2717\n40.1\n76.3\nCityscapes\n\u2713\n38.2\n78.5\nTable S9 Influence of instance masks on model perofr-\nmance.\nB.6\nLarger discriminators\nFor larger datasets, having huge encoder architec-\ntures could prove beneficial in capturing the com-\nplexty of the dataset. Accordingly, we train a model\non COCO-Stuff using a ConvNext-XL model. It is\napproximately 1.76 times bigger than ConvNext-L we\nused previously, with 350M parameters. In Table S10,\nwe report its performance as a pre-trained feature\nencoder in our discriminator.\nFID\nmIOU\nDP-SIMS (ConvNext-L)\n13.6\n65.2\nDP-SIMS (ConvNext-XL)\n13.3\n68.0\nTable S10 Performance of model trained with ConvNext-\nXL on COCO-Stuff.\nAs expected, the larger ConvNext-XL encoder pro-\nduces even better performance, reaching state of the\nart in terms of both FID and mIOU. We observe\na difference of 0.3 points in FID and 2.8 points in\nmIoUMF.\nB.7\nQualitative samples\nWe provide qualitative samples of the images gener-\nated with our DP-SIMS model using different pre-\ntrained backbones for the discriminator in Figure S6.\nIn Figure S7, Figure S8, and Figure S9 we provide\nexamples of images generated with our DP-SIMS\n17\nLabel map\nNo class balancing\nClass balancing\nFigure S4 Qualitative examples of images generated with and without balanced sampling to train models on Cityscapes.\nroad \n0.377\nbuilding \n0.219\nvegetation \n0.173\ncar \n0.065\nsidewalk \n0.054\nsky \n0.033 pole \n0.015\nperson \n0.013terrain \n0.008 fence \n0.008 wall \n0.007\nbicycle \n0.007\ntraffic sign \n0.007\nbus \n0.004 truck \n0.003 rider \n0.002\ntraffic light \n0.002\ntrain \n0.001\nmotorcycle \n0.001\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWeighted Focal\nFocal\nWeighted CE\nroad \n0.377\nbuilding \n0.219\nvegetation \n0.173\ncar \n0.065\nsidewalk \n0.054\nsky \n0.033 pole \n0.015\nperson \n0.013terrain \n0.008 fence \n0.008 wall \n0.007\nbicycle \n0.007\ntraffic sign \n0.007\nbus \n0.004 truck \n0.003 rider \n0.002\ntraffic light \n0.002\ntrain \n0.001\nmotorcycle \n0.001\n8\n6\n4\n2\n0\n2\n4\n IOU with respect to\n weighted cross-entropy\nWeighted Focal\nFocal\nFigure S5 Top: Per-class IOUMF on Cityscapes with models trained with different loss functions using ConvNext-L\nbackbone. Labels are sorted according to their frequency in the validation images, which is written below the class\nname. Bottom: Per-class difference in IOUMF of models trained with weighted and non-weighted focal loss w.r.t. the\nmodel trained with weighted cross-entropy (CE) loss. (Best viewed in color.)\n18\nmodel and compare to other state-of-the-art mod-\nels on the ADE-20K, COCO-Stuff, and Cityscapes\ndatasets, respectively.\nC\nAssets and licensing information\nIn Table S11a and Table S11b, we provide the links\nto the datasets and models used in or work and their\nlicensing.\n19\nGT\nLabel map\nR50\nSwin-B\nEfficientNet-34\nConvNext-L\nFigure S6 Qualitative comparison of DP-SIMS on ADE-20K using different backbones: Swin-B, Resnet50 (R50),\nEfficientNet-34, and ConvNext-L.\n20\nGT\nSemantic map\nOasis\nSDM\nOurs\nFigure S7 Qualitative comparison with prior work on ADE-20K, using a ConvNext-L backbone for DP-SIMS (Ours).\n21\nOASIS\nSDM\nPITI\nOurs\nGT\nLabel map\nFigure S8 Qualitative comparison with prior work on COCO-Stuff, using a ConvNext-L backbone for DP-SIMS (Ours).\n22\nLabel map\nOASIS\nSDM\nOurs\nFigure S9 Qualitative comparison with prior work on Cityscapes, using a ConvNext-L backbone for DP-SIMS (Ours).\n23\nName\nLink\nImageNet\nhttps://www.image-net.org\nCOCO-Stuff\nhttps://cocodataset.org\nCityscapes\nhttps://www.cityscapes-dataset.com\nADE-20K\nhttps://groups.csail.mit.edu/vision/datasets/ADE20K/\nDetectron2\nhttps://github.com/facebookresearch/detectron2\nConvNext\nhttps://github.com/facebookresearch/ConvNeXt\nSwin\nhttps://github.com/microsoft/Swin-Transformer\nEfficientNet\nhttps://github.com/lukemelas/EfficientNet-PyTorch\nVGG19\nhttps://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py\nDeeplab-v2\nhttps://github.com/kazuto1011/deeplab-pytorch/\nUperNet101\nhttps://github.com/CSAILVision/sceneparsing\nMS DRN-D-105\nhttps://github.com/fyu/drn\nMask2Former\nhttps://github.com/facebookresearch/Mask2Former\nSelf-supervised FID\nhttps://github.com/stanis-morozov/self-supervised-gan-eval\n(a) Links to the assets used in the paper.\nName\nLicense\nImageNet\nTerms of access: https://www.image-net.org/download.php\nCOCO-Stuff\nhttps://www.flickr.com/creativecommons\nCityscapes\nhttps://www.cityscapes-dataset.com/license\nADE-20K\nhttps://groups.csail.mit.edu/vision/datasets/ADE20K/terms/\nDetectron2\nApache-2.0 license\nR50\nBSD\nConvNext\nMIT License\nSwin\nMIT License\nEfficientNet\nApache-2.0 license\nVGG19\nBSD-3-Clause license\nUperNet101\nBSD-3-Clause license\nMS DRN-D-105\nBSD-3-Clause license\nDeeplab-v2\nMIT License\nMask2Former\nMIT License\n(b) Assets licensing information.\nTable S11 We provide information about the assets used in our work. Top : links to the assets used. Bottom : Licensing\ninformation of the relevant assets.\n24\n"
  },
  {
    "title": "HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs",
    "link": "https://arxiv.org/pdf/2312.14140.pdf",
    "upvote": "5",
    "text": "HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs\nArtem Sevastopolsky1\nPhilip-William Grassal2\nSimon Giebenhain1\nShahRukh Athar3\nLuisa Verdoliva4,1\nMatthias Nie\u00dfner1\n1 Technical University of Munich (TUM), Germany\n2 Copresence AG, Germany\n3 Stony Brook University, US\n4 University of Naples Federico II, Italy\nGenerative head model\nFitting\n3D Scans Dataset\nParametric\ntemplate\nRandom generations\nDepth \nobservation\nComplete geometry\nFigure 1. We present HeadCraft1, a generative model for highly-detailed human heads, ready for animation. Our method is trained on 2D\ndisplacement maps collected by registering a parametric template head with free surface displacements to a large set of 3D head scans. The\nresulting model is highly versatile, which can be demonstrated by fitting the latent code of the model to an arbitrary depth observation.\nAbstract\nCurrent advances in human head modeling allow to gen-\nerate plausible-looking 3D head models via neural rep-\nresentations.\nNevertheless, constructing complete high-\nfidelity head models with explicitly controlled animation re-\nmains an issue. Furthermore, completing the head geom-\netry based on a partial observation, e.g. coming from a\ndepth sensor, while preserving details is often problematic\nfor the existing methods. We introduce a generative model\nfor detailed 3D head meshes on top of an articulated 3DMM\nwhich allows explicit animation and high-detail preserva-\ntion at the same time. Our method is trained in two stages.\nFirst, we register a parametric head model with vertex dis-\nplacements to each mesh of the recently introduced NPHM\ndataset of accurate 3D head scans. The estimated displace-\nments are baked into a hand-crafted UV layout. Second, we\ntrain a StyleGAN model in order to generalize over the UV\nmaps of displacements. The decomposition of the paramet-\nric model and high-quality vertex displacements allows us\nto animate the model and modify it semantically. We demon-\nstrate the results of unconditional generation and fitting to\nthe full or partial observation.\n1. Introduction\nThe advent of neural representations, such as NeRFs (Neu-\nral Radiance Fields) [38] and SDFs (Signed Distance Func-\ntions) [40], has revolutionized the field of 3D modeling\nby enabling the generation of remarkably realistic 3D head\nmodels. These models have found their applications in var-\nious domains, including computer graphics, virtual reality,\nand digital entertainment. The ability to create lifelike 3D\nhead models is crucial for many applications, ranging from\nvideo game character design to virtual try-on experiences\nand medical simulations. While contemporary techniques\nhave made significant strides in achieving high-quality 3D\nhead models, a series of challenges persist, impeding the\nseamless integration of these models into real-world appli-\ncations. In particular, one of the primary challenges in the\nrealm of 3D head modeling lies in constructing a neural rep-\nresentation amenable to animation and tracking, while pre-\nserving enough detail.\nIn particular, recently introduced implicit generative\nmodels for 3D, such as pi-GAN [14], EG3D [15], or\n1Project page is available at https://seva100.github.io/\nheadcraft.\narXiv:2312.14140v1  [cs.CV]  21 Dec 2023\nStyleSDF [39], fit the distribution of human faces with ex-\ntraordinary detail, and e.g. in [7, 16], the modeled region is\nextended to the whole head. To introduce the animation ca-\npabilities, these methods require ad-hoc modifications such\nas introducing deformations to canonical space [8] or driv-\ning the rendering by a semantic face mask [49, 56]. The\nmethods mostly focused on both modeling and controlling\nthe geometry are typically SDF-based and require learning\na separate latent space for expressions [60], oftentimes with\nthe aid of deformations [26, 61]. At the same time, ap-\nproaches that fit a neural representation to a \u201dtalking head\u201d-\nstyle video, enable explicit articulation by introducing the\nparametric model guidance [27, 64, 65].\nInspired by the combination of these ideas, in this re-\nsearch, we introduce a generative model that allows for an-\nimation and tracking and preserves high level of detail. At\nthe heart of our approach lies the idea of combining an ex-\nplicit parametric head model (FLAME [36]) with surface\ndisplacements complementing the low geometry detail of\nthe head model. FLAME is an example of a 3D Morphable\nModel [10, 20] with a fixed set of vertices and fixed topol-\nogy, constructed as a linear statistical model over the heads\nwith point-to-point correspondence and further controlled\nby shape and expression latent codes. To obtain the neces-\nsary training data, we register a highly subdivided FLAME\nmesh template with free vertex displacements to all 3D head\nscans in the NPHM [26] dataset. To facilitate as high level\nof detail in the displacements as possible, they are fitted in\ntwo steps. First, the optimization problem is solved for vec-\ntor displacements with strong regularization that penalizes\nvery hard for self-intersections of the deformed mesh re-\ngions. Afterwards, a separate optimization step refines the\ndisplacements only along the normals of the deformed ver-\ntices, while keeping the regularization weight low. These\ndisplacements are baked into a predefined UV layout. Fi-\nnally, we train a StyleGAN2 [31] model to generalize over\nthis set of baked 2D displacement maps. This novel archi-\ntecture allows us to operate at a resolution higher than the\nconventional FLAME template, enabling the generation of\nhighly detailed and animatable 3D head models.\nTo validate the efficacy and practical utility of our ap-\nproach, we evaluate it in several settings. The diversity and\nfidelity of the generated 3D head meshes is quantitatively\nand visually compared to other methods w.r.t. the real head\nscans from the FaceVerse dataset [55], both in UV space and\nrendered image space. We also explore the applicability of\nour approach in fitting the latent representation of the gener-\native model to complete or incomplete point cloud data and\ndemonstrate its animation and manipulation capabilities.\nOur contributions are as follows:\n\u2022 We introduce a two-stage registration procedure to craft\nhigh-detail displacement maps on top of 3DMMs from\n3D scanning data.\n\u2022 We propose a generative model over displacement maps\nto not only enhance the low-frequency geometry of\nFLAME with details but also extend its shape space to\nall kinds of hairstyle variations.\n\u2022 We demonstrate the versatility of our method through un-\nconditional sampling, interpolation, semantic hair trans-\nfer, and conditioning by depth map or a complete scan.\n2. Related Work\nMany recent solutions to computer vision problems in-\nvolving human bodies are built on statistical body models.\nThey are the foundation for building personalized avatars\n[4, 5, 25, 27, 66], motion tracking [22, 51], scan registra-\ntion [26], controlling image synthesis [50], and many more.\nTheir line of research divides into two major branches.\nMesh-based Models. Pioneering work in the field [10] pro-\nposed 3D morphable models (3DMMs) for identity, expres-\nsion, and appearance representation of human faces. Their\nmodel is built around a 3D template mesh and linear para-\nmetric blendshapes derived from PCAs over scan data. With\nnew datasets and registration procedures, their work has\nbeen extended from faces to heads [35, 36, 42], hands [46],\nfull-bodies [37], or combinations of these [41, 57]. The\ntemplate mesh has a fixed topology which provides con-\nsistent UV unwrapping and enables fitting to know surface\ncorrespondences, e.g. semantic regions and landmarks. Yet,\nit limits the representative power w.r.t. the overall shape\nand the level of detail beyond what is provided by the tem-\nplate. Downstream approaches compensate this by optimiz-\ning displacements [4, 5, 12, 27, 32, 35, 59] or additional im-\nplicit geometry [13, 23] on top of the mesh. Displacements\nare applied either per-vertex individually [4, 5, 27, 32] or as\na displacement map over the whole surface using the consis-\ntent UV unwrapping of the template [35, 59]. Our method\nfollows this idea by learning a generative model over dis-\nplacements maps while exploiting the animation model\nand surface correspondences of the underlying 3DMM. We\ndemonstrate that learning displacements maps on top of a\nhighly-subdivided template allows to model fine details and,\ncompared to previous works, to introduce significant shape\nvariations.\nImplicit Models. The recent success of implicit SDFs [40]\nand neural radiance fields [38] in 3D modeling has also\nmotivated applying them for statistical body models. Most\nimplicit models learn shape and appearance in a canonical\nreference space [6, 26, 29, 43, 60] or as displacements on\ntop of an existing model [61]. For better generalization and\ndetail preservation, some approaches use a composition of\nimplicit SDFs to model the canonical space [6, 26]. Artic-\nulation and animation is modelled either directly in canoni-\ncal coordinates [29, 61], through implicit deformation fields\n[26, 60], explicit joints [6] or blendshape deformations bor-\n2\nRegistration\n3D Scan\nFLAME\n(N vertices)\nvector \ndisplacements\nnormal \ndisplacements\nUV offset map\nGeneralization\nz  code\nGenerator\nnetwork\nFLAME\n(N vertices)\nFLAME\n+ displacement\nUV offset map\n(a)\n(b)\n(c)\n(d)\n(e)\nmapping\nnetwork\nw+ code\nFigure 2. An overview of the method. In the registration stage, we (a) fit the FLAME template by the face landmarks to the scan from the\nNPHM dataset and highly subdivide it, (b) optimize for the vertex displacements in R3 to fit the rough geometry with strong regularizations,\n(c) optimize for the scalar refinements of the displacements along the normal directions, and (d) bake the displacements into a UV offset\nmap.To generalize over the UV offset maps, we train a StyleGAN2 [31] model. After training, the offsets can be applied to an arbitrary\nFLAME template by subdividing it and (e) querying the generated UV offset map with the (u, v) locations of the FLAME vertices.\nrowed from explicit 3DMMs [56, 64].\nWhile the afore-\nmentioned methods rely on multi-view data and aligned 3D\nscans, a separate line of research demonstrates that statis-\ntical shape and appearance priors can also be learned from\nunstructured image collections [14, 15, 39, 45, 67].\nImplicit approaches do not rely on topology and shape\ntemplates. This allows them to fit more detail and com-\nplex shapes such as hair [26, 60, 61] and even glasses\n[15, 39].\nYet, it prevents consistent surface correspon-\ndences between different samples which need to be explic-\nitly learned [6, 61]. As our approach uses a mesh-based\ntemplate, it does not suffer from these limitations and has\nan explicit model for animation. Still, we are able to show\nthat we can provide a comparable level of detail as implicit\nmethods by learning high-resolution displacement maps on\ntop of a highly subdivided template topology.\n3. Method\nWe describe the stages of our method in several parts. Sub-\nsec. 3.1 describes the registration procedure for FLAME\nwith surface displacements. Subsec. 3.2 describes learn-\ning the generative model. The overview of the approach is\npresented in Fig. 2.\n3.1. Displacements registration procedure\nThe purpose of this step is to align the FLAME [36] head\ntemplate with displacements to each scan of the 3D human\nhead dataset. Let us consider a single scanned mesh P =\n(V gt, Fgt) with vertices V gt \u2208 R|V gt|\u00d73, and faces Fgt \u2208\nR|Fgt|\u00d73.\nIn order to find the appropriate FLAME parameters for\nthe scan, we follow the rigid alignment optimization proce-\ndure outlined in the NPHM work [26]. This procedure re-\nquires face landmarks to be known, which can be annotated\nmanually or, as provided with the dataset in our case, cal-\nculated via 2D face landmark detectors on the projections\nof the colored scans and lifted to 3D. This way, we obtain\na FLAME template, corresponding to the given scan, and\nsubdivide it via Butterfly algorithm [19]. We will refer to\nthe template after subdivision as to F = (V, F, UF), where\nV \u2208 R|V |\u00d73 are the vertices coordinates, F \u2208 R|F|\u00d73 are\nthe corresponding faces, and UF \u2208 R|F|\u00d73\u00d72 are the tex-\nture coordinates of each vertex in a triangle. Note that using\ntriangle coordinates instead of vertex coordinates is impor-\ntant due to the presence of a seam in the FLAME model,\nthus making UVs for the seam vertices ambiguous.\nAs FLAME basis does not represent hair or face details,\nwe define these in a form of vertex displacements and learn\nthem in two stages. During the first stage, we optimize the\nloss function LStage 1(D) for additive vector displacements\nDStage 1 \u2208 R|V |\u00d73 of the vertices:\nL(D, V, F, V gt|\u03bb) = \u03bbChamfer LChamfer(V + D, V gt)\n+ \u03bbedge Ledge(V + D, F)\n+ \u03bblapl Llapl(V + D, F)\n(1)\nLStage 1(D) = L(D, V, F, V gt | \u03bbStage 1)\n(2)\nHyperparameters \u03bbStage 1 = (\u03bbChamfer\nStage 1 , \u03bbedge\nStage 1, \u03bblapl\nStage 1)\ndefine the Chamfer matching term weight, the weight of\nedge length regularization and standard Laplacian regular-\nization. In this stage, the weight of regularizations is high\nin order to prevent self-intersections that can occur when re-\ngressing vector displacements. Also, we only optimize the\nvector displacements for the hair region.\n3\nOurs\nNPHM [26]\nROME (linear) [32]\nPCA baseline\nFigure 3. Visual comparison of fidelity and diversity of the meshes generated by various methods. For Ours, random FLAMEs are sampled\nfrom Gaussian distribution with statistics calculated over the NPHM dataset; same for the PCA baseline pre-fitted to our UV registrations.\nMeshes from NPHM are obtained by sampling the latent codes and running marching cubes over the generated SDF representations. We\ndemonstrate higher variability of produced head geometry and better details than the other methods. Electronic zoom-in recommended.\nIn the second stage, we optimize the loss function\nLStage 2(\u03b1) for displacements DStage 2 \u2208 R|V |\u00d73 that are\nonly allowed to move over the normals of the previously\ndisplaced vertices:\nDStage 2 = DStage 1 + N\u2299 \u03b1,\n(3)\nwhere N \u2208 R|V |\u00d73 corresponds to the normals, calcu-\nlated by numerical difference for vertices deformed after the\nStage 1, and \u2299 defines the element-wise product of rows of\nN and elements of \u03b1 (each normal ni is multiplied by the\nrespective amplitude \u03b1i). LStage 2(\u03b1) is expressed through\nthe same basic loss expression:\nLStage 2(\u03b1) = L(DStage 1+N\u2299 \u03b1, V, F, V gt | \u03bbStage 2), (4)\nwhile hyperparameters \u03bbStage 2 are selected with relatively\nlower regularization weights. This allows for fitting high-\nfrequency details while maintaining the same rough shape\nof the regressed shape. At this stage, we allow both hair and\nface regions to deform, while subtle parts such as ears and\neyeballs are fixed from moving.\nFinally, we bake the displacements DStage 2 into a UV\nmap U \u2208 RH\u00d7W\u00d73 by rendering it onto the UV space with\nknown texture coordinates UF and triangles F.\nThe registration procedure is repeated for the dataset\nconsisting of multiple 3D scans, resulting in a set of UV\ndisplacement maps (U1, . . . , US).\n3.2. Generative model\nThe described registration procedure allows us to relax the\nproblem of 3D head geometry generation into a problem of\ngeneration of 2D UV displacement maps, which allows us\nto apply a 2D generative model. We have selected Style-\nGAN2 for that purpose due to its capability of general-\nizing over relatively small datasets of images [1, 30, 63]\nwhile maintaining close-to-SoTA image generation capabil-\nities [31]. The model consists of a mapping network and a\ngenerator network, which we will refer to as f(z) together,\nwhere z \u2208 Z \u2282 RD is a latent code sampled from a stan-\ndard normal distribution during training. The generator pro-\nduces a UV offset map U = f(z), which we can apply to\nan arbitrary (anyhow densely subdivided) FLAME template\n4\nF = (V, F, UF) by querying the map U with its texture co-\nordinates UF to obtain the respective vertex displacements.\nWe later demonstrate visually that the generated displace-\nments could be applied to an arbitrary FLAME template.\nFigure 4. Comparison of the FLAME layouts. The standard, com-\nmonly used unwrapping for FLAME (left) features a seam corre-\nsponding to the vertical line in the back of the head and pays more\nattention to the facial region than to the scalp. In the hand-crafted\ncustom layout that we employ (right), a different seam around the\nface border is selected, thus making the regions of face and scalp\nseparated and of similar size, which simplifies modeling complex\ngeometry such as long hair without breaks.\nPost-processing. Since the UV map U is generated in the\nUV layout that contains a seam, we expect StyleGAN to\nresolve it in general, i.e. produce similar displacements in\nthe face and scalp region near the same seam vertex. Still,\nthere is no dedicated supervision during StyleGAN training\nthat ensures that it always happens and that the border is\npreserved pixel-perfect. Because of that, before querying\nthe UV map U, we ensure the smoothness of the seam by\nequalizing the displacements at its borders and blending the\ndisplacements in the small vicinity inside and outside of it.\nThe sampling errors near the seam are fixed by filling in all\nthe empty space in the map with the displacements copied\nfrom the nearest neighbor pixels in the valid regions.\n4. Experiments\n4.1. Training procedure\nOur method is trained on the private version2 of the\nNPHM [26] dataset, consisting of 6975 high-resolution\nscans of 327 diverse identities captured by two 3D scanners.\nFor the registration procedure, we use Adam optimizer with\nlearning rate of 3 \u00b7 10\u22122 for the first stage and 3 \u00b7 10\u22124\nfor the second stage.\nThe hyperparameters \u03bbStage 1\n=\n(\u03bbChamfer\nStage 1 , \u03bbedge\nStage 1, \u03bblapl\nStage 1) equal to (2\u00b7103, 2\u00b7105, 104). For\nthe second stage, \u03bbStage 2 = (2 \u00b7 104, 2 \u00b7 104, 104). In the\nChamfer loss, we additionally apply correspondences prun-\ning by distance of 1.0, which defines that all the correspon-\ndences between source and target with the distance more\nthan 1.0 in the NPHM coordinate system are automatically\ndiscarded. This has been introduced for more consistent\ngradual learning of displacements, such that at each opti-\nmization step, only the nearest points affect the deformation\nFID \u2193\nKID \u2193\nIS \u2191\nRel. IS \u2191\nMMD \u2193\nJSD \u2193\nCOV \u2191\nOurs\n72.37\n0.071\n1.67\n88.25%\n6.45\n21.41\n47.12%\nPCA\n102.96\n0.125\n1.46\n77.13%\n9.96\n20.68\n22.12%\nNPHM [26]\n139.82\n0.170\n1.57\n82.95%\n7.80\n19.06\n46.15%\nROME [32]\n169.65\n0.204\n1.63\n86.81%\n10.02\n23.19\n32.69%\nFLAME [36]\n198.85\n0.262\n1.13\n59.86%\n12.95\n23.89\n5.77%\nTable 1. The comparison of quality and diversity of random sam-\nples generated by each of the methods. FID and KID measure the\nsimilarity of the generated mesh renderings vs. the renderings of\nthe ground truth meshes in FaceVerse dataset, while IS measures\nthe realism of the generated renderings. 3D metrics MMD, JSD,\nCOV assess the similarity of point clouds sampled from generated\nand ground truth meshes. For this and all other tables, MMD has\nbeen multiplied by 103 and JSD is multiplied by 102.\nlearning.\nFor the generative model training, we use StyleGAN2\nimplementation with all augmentations turned off (since\nthey wouldn\u2019t yield valid UV maps in our case), 8 map-\nping network layers and a high gradient penalty of 4.0. The\nlearning rates are 2 \u00b7 10\u22123 for the generator and 1 \u00b7 10\u22123 for\nthe discriminator. We train it for 72K steps with the batch\nsize of 8 and the resolution of the UV maps of 256 \u00d7 256.\n4.2. Results\nUnconditional sampling. In Fig. 3, we compare the dif-\nference in details and diversity of the unconditional sam-\nples produced by our method to the ones produced by\nNPHM [26] and ROME [32] methods. Additionally, we\ncompare it to the PCA baseline, whereas PCA linear basis\nis fitted to our UV displacement maps, and provide the num-\nbers for random FLAME samples without added displace-\nments as a reference. For Ours, PCA baseline and ROME, a\nFLAME with random shape, expression and jaw parameters\nare sampled from a Gaussian for every head mesh, in accor-\ndance with the statistics precalculated over the NeRSem-\nble dataset [34]. While NPHM, PCA baseline, and Ours\nhave been fitted to exactly the same training dataset, for\nROME, the authors\u2019 provided checkpoint has been used.\nFor ROME, we sample the FLAME displacements from a\nlinear model provided by the authors of ROME as the sam-\npling strategy proposed by the ROME authors. Visually, we\nobserve both higher diversity and better representation of\nhair details than for all baselines. The details of the facial\nregion are generally the sharpest for ROME, PCA baseline,\nand Ours, due to the use of the FLAME template.\nIn Table 1, we also quantify the level of detail and vari-\nety of the generated meshes w.r.t. the full head scans from\nthe FaceVerse dataset [55] that has not been used for train-\ning. The comparison is performed in two ways.\nFirstly,\n2The private version was provided to us by the authors of NPHM [26]\nand is going to be released to the public. The public version, available at\nthe time of the publication, is roughly a 30% subset of the private version.\n5\nto evaluate the visual plausibility of the generated geome-\ntry, we render 2195 ground truth meshes from FaceVerse\nand the same number of meshes generated by each method\nwith highly metallic material from eight distinct viewpoints,\nuniformly sampled along the circular trajectory in the hori-\nzontal plane. The FID [28] and KID [9] perceptual metrics\nare calculated for all generated and ground truth renderings\nfrom a given viewpoint and then macro-averaged over eight\nviewpoints. Inception Score (IS) [47] evaluated the realism\nof the generated renderings. Because of that, we also report\nthe relative IS as a percentage of the reference IS calculated\nfor the ground truth renderings.\nSecondly, we compare the distributions of point clouds\nsampled from the generated and ground truth meshes. To do\nthat, we sample 10K points from each of the 2195 generated\nand the same number of ground truth meshes and calculate\nseveral 3D similarity metrics. Jensen-Shannon Divergence\n(JSD) is evaluated by comparing the distributions of gen-\nerated and ground truth points, splat into a voxel grid (in\nour case, of 5123 voxels). Minimum Matching Distance\n(MMD) is a measure of 3D object realism that, for each\nground truth sample, involves evaluating the distance to the\nmost similar sample in the generated set. Similarly, Cov-\nerage (COV) indicates the percentage of the ground truth\nsamples, for which the nearest neighbor among all ground\ntruth and generated samples falls into the generated set. The\ndetailed mathematical description of each of the 3D met-\nrics we report can be found in [21, 58]. In addition, we\ndemonstrate how much the generated samples deviate from\nthe NPHM training set in Fig. 5. The nearest neighbor scan\nis found by comparing the generated UV map to the UV\nmaps comprised of registered ground truth displacements\nfor all training scans by L2 distance over the scalp region.\nThe results in Table 1 indicate that the renderings from\nour method appear more realistic than of the other meth-\nods, with either PCA baseline or NPHM performing similar\naccording to different subsets of the metrics. Close promix-\nity to NPHM by MMD, COV, JSD could be explained by\ntraining on exactly the same dataset.\nAblating over the choice of the UV layout. We assess the\neffect of a manually hand-crafted UV space for FLAME\non the quality of generations in Fig. 6. As observed, mov-\ning the seam from the vertical middle line, as in the stan-\ndard UV layout for FLAME, to the face border, allows us to\nmodel more consistent and complex geometry without large\ndistinction between a left and a right part.\nAblating over the choice of the generative model archi-\ntecture. We compare StyleGAN to other state-of-the-art\ngenerative model architectures, namely of VAE [33] and\nVQ-VAE [53] family, with ResNet-18 encoder and decoder.\nFor VQ-VAE, the sampling from the latent space is imple-\nmented via training PixelCNN autoregressive model [52].\nThe results are presented in Table 2 and in Fig. 7.\nGenerated\nNearest\nGenerated\nNearest\nFigure 5. Randomly generated samples from HeadCraft and the\ncorresponding nearest neighbors in the NPHM dataset among the\nscans used for training.\nStandard UV\nCustom UV\nFigure 6. Ablation over the choice of the UV layout. Our method\nutilizes custom UV layout that allows us to model more consistent\ngeometry by mitigating seam artifacts, as seen from the back view\nhere. Layouts are shown in Fig. 4.\nFID \u2193\nKID \u2193\nIS \u2191\nRel. IS \u2191\nMMD \u2193\nJSD \u2193\nCOV \u2191\nOurs\n72.37\n0.07\n1.67\n88.25%\n6.45\n21.41\n47.12%\nVAE [33]\n108.91\n0.13\n1.66\n87.79%\n6.79\n21.65\n42.31%\nVQ-VAE [53]\n125.65\n0.15\n1.62\n85.94%\n7.96\n21.88\n38.46%\nPCA\n102.96\n0.13\n1.46\n77.13%\n9.96\n20.68\n22.12%\nTable 2. Ablation over the generative model design. VAE and\nVQ-VAE follow the ResNet-18 encoder and decoder architecture,\nwhile Ours is based on StyleGAN2. We also include PCA baseline\nscores here as a reference.\nBehavior of the registration procedure. In Fig. 10, we\ndemonstrate the advantage of the two-stage registration pro-\ncedure, described in Subsec. 3.1, over omitting one of the\nstages. As can be seen, keeping only the vector displace-\n6\nment optimization results in too rough shape, and relax-\ning the regularization constraints yields significant artifacts\nsuch as self-intersections and spikes.\nRunning the nor-\nmal displacement stage without any preliminary vector dis-\nplacement stage performs similarly to our two-stage proce-\ndure but produces artifacts for long hair that does not triv-\nially project onto the surface. In turn, it can produce the\nmappings between template vertices and scan vertices, in-\nconsistent across various samples for the long hair parts.\n4.3. Applications\nFitting the latent code to a depth map. Our model can act\nas a prior for completing the partial observations, e.g. when\nthey come from a depth sensor. To evaluate the performance\nof the model in that scenario, we demonstrate the comple-\ntion capabilities of the model over a number of scans from\nNPHM corresponding to the subjects unseen during train-\ning. For each of these scans, we project their depth onto\nrandom viewpoints in the frontal hemisphere and project it\nback to 3D to construct partial point clouds. To obtain a\npartial UV map to be completed, we run our registration\nprocedure with a few modifications to fit a part of the scan.\nNamely, we only fit the points within the convex hull of the\npartial point cloud, apply stronger edge length regulariza-\ntion weight, and constrain the points at the border of the\nallowed region from moving. The final mask of observed\nUV texels is refined by only selecting those points that turn\nout to be close to the partial point cloud. Finally, a latent\ncode of HeadCraft explaining the partial UV map is found\nvia StyleGAN inversion techniques. More technical details\nof the partial registration and inversion are provided in the\nSupplementary [3]. The fitting quality can be evaluated by\nthe visual comparison in Fig. 8.\nVAE\nVQ-VAE\nFigure 7. Ablation over the generative model design. VAE and\nVQ-VAE both follow the ResNet-18 encoder and decoder archi-\ntecture, while our method is based on StyleGAN2. The results\nfrom VAE and VQ-VAE match the diversity of the training data\nbut not the level of detail and handle the UV seam worse.\nThe capabilities of fitting the model to the full scan,\ne.g. created from Structure-from-Motion (SfM), are demon-\nstrated as a part of the semantic editing experiments in\nFig. 11 (top rows, the result of the latent fitting to each of\nthe scans, \u03bb = {0, 1}).\nAnimation. The decomposition of the parametric model\nFigure 9.\nDemonstration of the animation capabilities of the\nmodel. Each of the sequences is created by taking FLAME shape\nparameters, expression, jaw, and head pose parameters from a se-\nquence from NeRSemble dataset [34], subdividing the template,\nand applying randomly generated displacements from HeadCraft.\nand the displacements allows us to animate the complete\nhead model. In our experiments, we take real multi-view\nvideo sequences with talking people from the NeRSemble\ndataset [34] and obtain shape, expression, jaw, and head\npose parameters for each time frame of the speech by run-\nning a FLAME tracker for each sequence. For each of the\nsample shapes, estimated from the sequences, we reenact\nthe corresponding FLAME with estimated expression pa-\nrameters, subdivide the template and query a randomly pre-\nsampled UV displacement map from HeadCraft. Since the\ntemplate is also deforming over time, we rotate the displace-\nments according to the changing surface normals of the tem-\nplate. More results are demonstrated in Fig. 9 and in the\nSupplementary Video.\n4.4. Analysis\nInterpolation between the displacements. In Fig. 11, we\nshow how interpolating the latent code of our generative\nmodel influences the change of the geometry. Further inter-\npolations are presented in the Supplementary Video.\nHair transfer from one scan to another. Access to the\nshared UV space allows us to modify the geometry seman-\ntically. In Fig. 11, the transfer of the scalp region from\none ground truth NPHM scan, unseen during training, to\nthe other is shown. The transfer is performed via fitting the\nlatent representation of HeadCraft to the driver scan (the\nsource of displacements) and feeding it to the model. The\nextracted displacements are later applied to the source scan.\n7\nDepth\nCompletion\nGround truth\nDepth\nCompletion\nGround truth\nDepth\nOurs\nNPHM\nGr. truth\nDepth\nOurs\nNPHM\nGr. truth\nFigure 8. Demonstration of geometry completion aided by the HeadCraft model. Here, we extract depth maps from scans from the NPHM\ndataset, unseen during training, and try to complete them by finding the appropriate latent representation of StyleGAN. As a necessary\nintermediate step, we first apply our registration procedure to the partial point cloud to locate the points in the UV space of the template.\nThe optimal latent is found by minimizing the discrepancy of the complete UV map and registered partial UV map in the observed regions.\nHeadCraft is also capable of estimating plausible details for a very sparse point cloud (1% of # points) \u2013 see the last row.\n(a) Stage 1\n(b) Stage 2\n(c) Stage 1,\n(d) Ours\nonly\nonly\n\u03bb = \u03bbStage 2\nFigure 10. Ablation over the one-stage vs. two-stage registration.\nLeaving only the vector displacements stage (a) yields too smooth\ngeometry, and learning them only along the normals (b) introduces\nunnecessary spikes \u2013 just like running the first stage with weaker\nregularization (c).\n5. Discussion\nIn this work, a generative model for 3D human heads is pre-\nsented. We demonstrate the efficacy of the hybrid approach\ninvolving an underlying animatable parametric model and\na neural vertex displacement modeller. Most importantly,\nour method allows to model high-quality shape variations\nwhile maintaining the realistic animation capability, and the\ninversion framework allows us to find a suitable latent rep-\nresentation to either represent a full head scan or a part of it\nthat could come from e.g. the depth sensor. A direction of\nthe possible future work could be focused on incorporating\nInterpolation\nparametric\ntemplate\ndriving \nperson\nFigure 11. Semantic editing. Interpolating the latent represen-\ntations of HeadCraft allows us to smoothly change the person\u2019s\nappearance from one to another. To do that, we fit the latent codes\nfor two real scans in the NPHM dataset (brown, top rows), unseen\nduring training, and blend them together with a \u03bb weight. Like-\nwise, we can transfer hair geometry from one person to another.\nan appearance model for color and material-based relight-\ning and a physical model of hair movement, based on, for\ninstance, hair strands, to support more realistic animation.\n8\nAcknowledgments.\nWe gratefully acknowledge the sup-\nport of this research by a TUM-IAS Hans Fischer Senior\nFellowship, the ERC Starting Grant Scan2CAD (804724)\nand the Horizon Europe vera.ai project (101070093). We\nalso thank Yawar Siddiqui and Alexey Artemov for help-\nful advice, Tobias Kirschstein for his assistance with the\nNeRSembles dataset and visualization, Taras Khakhulin for\nhis help with the ROME baseline, Peter Kocsis, Antonio Al-\nlegro, Jiapeng Tang for early peer review, Silvia Sell\u00b4an for\nthe Blender visualization course materials that we used, and\nAngela Dai for the video voiceover.\nReferences\n[1] GitHub: Awesome Pretrained Stylegan by justinpinkney.\nA collection of pre-trained StyleGAN models trained\non different datasets at different resolution.\nhttps:\n/ / github . com / justinpinkney / awesome -\npretrained-stylegan. 4\n[2] Learn\nOpenGL\n\u2013\nNormal\nMapping.\nhttps :\n/ / learnopengl . com / Advanced - Lighting /\nNormal-Mapping. 16\n[3] HeadCraft: Modeling High-Detail Shape Variations for Ani-\nmated 3DMMs. Supplementary Material. 7\n[4] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Detailed human avatars\nfrom monocular video. In 2018 International Conference on\n3D Vision (3DV), pages 98\u2013109. IEEE, 2018. 2\n[5] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,\nChristian Theobalt, and Gerard Pons-Moll. Learning to re-\nconstruct people in clothing from a single rgb camera. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1175\u20131186, 2019. 2\n[6] Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu.\nimGHUM: Implicit generative models of 3D human shape\nand articulated pose. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5461\u20135470,\n2021. 2, 3\n[7] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360.\narXiv preprint arXiv:2303.13071,\n2023. 2\n[8] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric\nChan, David Lindell, and Gordon Wetzstein. Generative neu-\nral articulated radiance fields. Advances in Neural Informa-\ntion Processing Systems, 35:19900\u201319916, 2022. 2\n[9] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and\nArthur Gretton. Demystifying mmd gans. arXiv preprint\narXiv:1801.01401, 2018. 6\n[10] Volker Blanz and Thomas Vetter. A Morphable Model for the\nSynthesis of 3D faces. In Proceedings of the 26th Annual\nConference on Computer Graphics and Interactive Tech-\nniques, page 187\u2013194. ACM Press/Addison-Wesley Publish-\ning Co., 1999. 2\n[11] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 13\n[12] Andrei Burov, Matthias Nie\u00dfner, and Justus Thies. Dynamic\nsurface function networks for clothed human bodies.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10754\u201310764, 2021. 2\n[13] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authen-\ntic volumetric avatars from a phone scan. ACM Transactions\non Graphics (TOG), 41(4):1\u201319, 2022. 2\n[14] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-GAN: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 5799\u20135809, 2021. 1, 3\n[15] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3D generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 1, 3\n[16] Yuhao Cheng, Yichao Yan, Wenhan Zhu, Ye Pan, Bowen\nPan, and Xiaokang Yang.\nHead3d:\nComplete 3d head\ngeneration via tri-plane feature distillation. arXiv preprint\narXiv:2303.15892, 2023. 2\n[17] Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Mat-\nteo Dellepiane, Fabio Ganovelli, and Guido Ranzuglia.\nMeshLab: an Open-Source Mesh Processing Tool. In Euro-\ngraphics Italian Chapter Conference. The Eurographics As-\nsociation, 2008. 12\n[18] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 12\n[19] Nira Dyn, David Levine, and John A Gregory.\nA butter-\nfly subdivision scheme for surface interpolation with tension\ncontrol. ACM transactions on Graphics (TOG), 9(2):160\u2013\n169, 1990. 3\n[20] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie\nWuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,\nTimo Bolkart, Adam Kortylewski, Sami Romdhani, et al.\n3d morphable face models\u2014past, present, and future. ACM\nTransactions on Graphics (ToG), 39(5):1\u201338, 2020. 2\n[21] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperDiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 6\n[22] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.\nLearning an animatable detailed 3d face model from in-the-\nwild images. ACM Transactions on Graphics (ToG), 40(4):\n1\u201313, 2021. 2\n[23] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc\nPollefeys, and Michael J. Black.\nLearning disentangled\navatars with hybrid 3d representations. arXiv, 2023. 2\n[24] Steven Fortune. Voronoi diagrams and delaunay triangula-\ntions. In Handbook of discrete and computational geometry,\npages 705\u2013721. Chapman and Hall/CRC, 2017. 13\n[25] Guy Gafni, Justus Thies, Michael Zollh\u00a8ofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\n9\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 8649\u20138658, 2021. 2\n[26] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-\nlos, Martin R\u00a8unz, Lourdes Agapito, and Matthias Nie\u00dfner.\nLearning neural parametric head models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21003\u201321012, 2023. 2, 3, 4, 5\n[27] Philip-William Grassal,\nMalte Prinzler,\nTitus Leistner,\nCarsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neural\nhead avatars from monocular rgb videos. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18653\u201318664, 2022. 2\n[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. Advances in neural information processing systems,\n30, 2017. 6\n[29] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy-\nong Zhang.\nHeadnerf: A real-time nerf-based parametric\nhead model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 20374\u2013\n20384, 2022. 2\n[30] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila. Training generative adver-\nsarial networks with limited data. Advances in neural infor-\nmation processing systems, 33:12104\u201312114, 2020. 4\n[31] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 2, 3, 4\n[32] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and\nEgor Zakharov. Realistic one-shot mesh-based head avatars.\nIn European Conference on Computer Vision, pages 345\u2013\n362. Springer, 2022. 2, 4, 5\n[33] Diederik P Kingma, Max Welling, et al. An introduction to\nvariational autoencoders. Foundations and Trends\u00ae in Ma-\nchine Learning, 12(4):307\u2013392, 2019. 6\n[34] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim\nWalter, and Matthias Nie\u00dfner. NeRSemble: Multi-view Ra-\ndiance Field Reconstruction of Human Heads. arXiv preprint\narXiv:2305.03027, 2023. 5, 7\n[35] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara,\nOwen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha\nPrasad, Bipin Kishore, Jun Xing, et al.\nLearning forma-\ntion of physically-based face attributes. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 3410\u20133419, 2020. 2\n[36] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4D scans. ACM Trans. Graph., 36(6):194\u20131, 2017. 2,\n3, 5, 12\n[37] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black.\nSMPL: A skinned\nmulti-person linear model.\nACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 2\n[38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View\nSynthesis. CoRR, abs/2003.08934, 2020. 1, 2\n[39] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geome-\ntry generation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13503\u2013\n13513, 2022. 2, 3\n[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove.\nDeepSDF: Learning\ncontinuous signed distance functions for shape representa-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 165\u2013174, 2019.\n1, 2\n[41] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3D hands, face,\nand body from a single image. In Proceedings IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR), pages\n10975\u201310985, 2019. 2\n[42] Stylianos Ploumpis, Evangelos Ververas, Eimear O\u2019Sullivan,\nStylianos\nMoschoglou,\nHaoyang\nWang,\nNick\nPears,\nWilliam AP Smith, Baris Gecer, and Stefanos Zafeiriou. To-\nwards a complete 3d morphable model of the human head.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 43(11):4142\u20134160, 2020. 2\n[43] Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola,\nJaime Garcia, Xavier Giro-i Nieto, and Francesc Moreno-\nNoguer. H3d-net: Few-shot high-fidelity 3d head reconstruc-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 5620\u20135629, 2021. 2\n[44] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari.\nAccelerating 3d deep learning with pytorch3d.\narXiv:2007.08501, 2020. 12\n[45] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1558\u20131567, 2022. 3\n[46] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEmbodied hands: Modeling and capturing hands and bod-\nies together. ACM Transactions on Graphics, (Proc. SIG-\nGRAPH Asia), 36(6), 2017. 2\n[47] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 6\n[48] Tilo Strutz.\nThe distance transform and its computation.\narXiv preprint arXiv:2106.03503, 2021. 13\n[49] Junshu Tang, Bo Zhang, Binxin Yang, Ting Zhang, Dong\nChen, Lizhuang Ma, and Fang Wen.\nExplicitly con-\ntrollable 3D-aware portrait generation.\narXiv preprint\narXiv:2209.05434, 2022. 2\n[50] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian\nBernard, Hans-Peter Seidel, Patrick P\u00b4erez, Michael Zoll-\n10\nhofer, and Christian Theobalt.\nStylerig:\nRigging style-\ngan for 3d control over portrait images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6142\u20136151, 2020. 2\n[51] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-\ntian Theobalt, and Matthias Nie\u00dfner. Face2face: Real-time\nface capture and reenactment of rgb videos.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2387\u20132395, 2016. 2\n[52] A\u00a8aron Van Den Oord,\nNal Kalchbrenner,\nand Koray\nKavukcuoglu. Pixel recurrent neural networks. In Interna-\ntional conference on machine learning, pages 1747\u20131756.\nPMLR, 2016. 6\n[53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 6\n[54] J\u00a8org Vollmer, Robert Mencl, and Heinrich Mueller.\nIm-\nproved laplacian smoothing of noisy surface meshes.\nIn\nComputer graphics forum, pages 131\u2013138. Wiley Online Li-\nbrary, 1999. 16\n[55] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang\nLi, and Yebin Liu.\nFaceverse: a fine-grained and detail-\ncontrollable 3D face morphable model from a hybrid dataset.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 20333\u201320342, 2022. 2,\n5\n[56] Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng\nChen, and Xin Tong. Anifacegan: Animatable 3d-aware face\nimage generation for video avatars. Advances in Neural In-\nformation Processing Systems, 35:36188\u201336201, 2022. 2,\n3\n[57] Hongyi Xu,\nEduard Gabriel Bazavan,\nAndrei Zanfir,\nWilliam T. Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. Ghum & ghuml: Generative 3d human shape and\narticulated pose models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020. 2\n[58] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3D point cloud\ngeneration with continuous normalizing flows. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 4541\u20134550, 2019. 6, 13\n[59] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu\nShen, Ruigang Yang, and Xun Cao. Facescape: a large-scale\nhigh quality 3d face dataset and detailed riggable 3d face pre-\ndiction. In Proceedings of the ieee/cvf conference on com-\nputer vision and pattern recognition, pages 601\u2013610, 2020.\n2\n[60] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-\nPeter Seidel, Mohamed Elgharib, Daniel Cremers, and\nChristian Theobalt. i3DMM: Deep implicit 3D morphable\nmodel of human heads. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12803\u201312813, 2021. 2, 3\n[61] Mihai Zanfir, Thiemo Alldieck, and Cristian Sminchis-\nescu. PhoMoH: Implicit Photorealistic 3D Models of Human\nHeads. arXiv preprint arXiv:2212.07275, 2022. 2, 3\n[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 14\n[63] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song\nHan.\nDifferentiable augmentation for data-efficient gan\ntraining. Advances in neural information processing systems,\n33:7559\u20137570, 2020. 4\n[64] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C\nB\u00a8uhler, Xu Chen, Michael J Black, and Otmar Hilliges. I\nM Avatar: Implicit morphable head avatars from videos. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13545\u201313555, 2022. 2,\n3\n[65] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J\nBlack, and Otmar Hilliges. PointAvatar: Deformable point-\nbased head avatars from videos.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21057\u201321067, 2023. 2\n[66] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant\nvolumetric head avatars. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4574\u20134584, 2023. 2\n[67] Jinlong\nYang\nMichael\nJ.Black\nOtmar\nHilliges\nAn-\ndreas Geiger Zijian Dong, Xu Chen.\nAG3D: Learning\nto generate 3D avatars from 2D image collections.\nIn\nInternational Conference on Computer Vision (ICCV), 2023.\n3\n11\nA. Method: Technical Details\nA.1. Displacement registration procedure\nHere we explain the procedure in more detail. The vertices\nand displacements are modeled in the NPHM coordinate\nsystem, aligned with the scans, and the scaling of 30\u00d7 ap-\nplied. The implementation of Butterfly subdivision of the\nFLAME template from MeshLab [17] was used. The pa-\nrameters of the subdivision are constant across the scans\nand equal to 3 subdivision iterations with a threshold of\n42.5. The subdivision produces around 100K vertices and\n200K triangles for the original template consisting of 5023\nvertices and 9976 triangles and smooths the surface. The\ndescription of the optimization problem features individ-\nual loss terms. The expanded expression for the terms are\nas follows. The Chamfer term LChamfer(P1, P2) quantifies\nthe distance between the point clouds P1 \u2208 R|P1|\u00d73 and\nP2 \u2208 R|P2|\u00d73 is supposed to be differentiable w.r.t. the\npoints of P1. In our work, we apply the version pruned\nby the distance of the correspondences, i.e. when the Eu-\nclidean distance between point and its matched version ex-\nceeds the predefined threshold d, this correspondence is not\naccounted in the loss term.\nLChamfer(P1, P2) =\n1\nP\np\u2208P1[d(p, nn(p, P2)) \u2264 d]\u00b7\n\u00b7\nX\np\u2208P1\n(d(p, nn(p, P2)) \u00b7 [d(p, nn(p, P2)) \u2264 d])\n+\n1\nP\np\u2208P2[d(p, nn(p, P1)) \u2264 d]\u00b7\n\u00b7\nX\np\u2208P2\n(d(p, nn(p, P1)) \u00b7 [d(p, nn(p, P1)) \u2264 d]) ,\nwhere d(\u00b7, \u00b7) stands for the Euclidean distance between two\npoints in space and nn(p, P) = arg min\np\u2032\u2208P\nd(p, p\u2032) is the near-\nest neighbor of p in a point cloud P.\nEdge length regularization is defined as follows.\nLedge(V, F) =\n1\n|E|\nX\n(ea,eb)\u2208E\nd2(Vea, Veb),\n(5)\nwhere E = E(F) is a set of graph edges derived from the\nfaces F. To construct it, we consider each face bringing\nthree new edges and later leave only the unique edges in E.\nLaplacian term is defined as the Euclidean distance be-\ntween the vertex and its neighbors, which can be efficiently\ncalculated via computing sparse Laplacian L = L(V, F) of\nthe graph:\nLlapl(V, F) =\nsX\nv\u2208V\n\u2225Lv\u22252\n2\n(6)\nThe outer norm is used instead of e.g.\nL1 averaging\nto enforce the uniform smoothness of the mesh and avoid\nspikes that tend to appear otherwise (see, e.g., the docu-\nmented example in PyTorch3D [44] repository).\nDuring the vector displacement stage, only the scalp\nregion (defined by the semantic mask shipped with\nFLAME [36]) is optimized. During the normal displace-\nment stage, we also unfreeze the facial region but keep the\nneck, eyeballs, ears and inner mouth region frozen (the lat-\nter is annotated manually in Blender [18] package and is\nfrozen because of its absence in the ground truth scans, as it\nis placed fully interior). Each stage takes around 3 min for\n1K steps on a single NVIDIA RTX 2080 Ti GPU. We used\nthe PyTorch3D [44] functions for implementation of all the\nloss terms.\nIn Fig. 12, we show how the seam was annotated for the\ncustom UV layout. The Blender [18] 4.0 package was used\nfor annotation.\nStandard UV\nCustom UV\nFigure 12. Demonstration of the custom UV seam annotation.\nStandard UV corresponds to the UV space shipped with the\nFLAME model. Custom UV stands for the handcrafted layout em-\nployed in our pipeline. This layout simplifies learning sophisti-\ncated shape variations such as long hair in a consistent way.\nThe displacement vectors entries typically belong to the\n[-2, +2] range, while some large shape variations (e.g. a\npony tail) can introduce the offsets into a large range up to\n[-20, +20]. We clip any displacements, obtained after full\nregistration, to the [-20, +20] range. As the last step, the dis-\nplacements are rendered in UV space, and each UV map is\nsaved as uint16 image files linearly renormalized from [-20,\n+20] to [0, 216 \u2212 1]. Saving in uint16 (double-byte inten-\nsity value) instead of the widely used uint8 (single-byte in-\ntensity value) is important, since most of the displacements\nvector entries are concentrated around the small neighbor-\nhood of zero and the precision can be lost when renormal-\n12\nizing from [-20, +20] to [0, 28 \u2212 1] instead and discretiz-\ning. Saving UV maps as raw files would otherwise facili-\ntate much slower training of the generative model due to the\ntime-consuming loading and memory usage overhead.\nA.2. Generative model\nFor training StyleGAN, we used the stylegan2-ada-\nlightning implementation with ADA and augmentations\nturned off and the following hyperparameters:\nlatent dim\n# layers (z \u2192 w)\nG lr\nD lr\n512\n8\n0.002\n0.001\n\u03bbgp\n\u03bbplp\nimg size\nbatch size\n4.0\n2.0\n256 \u00d7 256\n8\nThe model was trained on four NVIDIA RTX 2080 Ti\nGPUs. For training of the StyleGAN, we replaced the facial\npart of the registered UV maps of subjects in the NPHM\ndataset with the corresponding facial part from the neutral\nexpression scan of the same person. This has been done to\nbetter support various expressions We also found it bene-\nficial to disable the StyleGAN noise, typically injected into\nthe generator, for the face part of the UV map, to smooth out\nthe generated facial displacements relative to the scalp dis-\nplacements that normally require higher level of detail. For\nthe scalp region, the StyleGAN noise is constant, initialized\nseparately for each generator layer before training.\nPost-processing. The non-smoothness of the seam is miti-\ngated in two ways, which are described in more detail here.\nThe first step is blending the displacements. To do that, we\nanalyze the UV coordinates of the triangles in the custom\nlayout UF and find the triangles, for which at least one of\nits vertices lies on the seam. These are the triangles, for\nwhich some of the UV coordinates belong to the left part\nof the UV map (scalp) and some belong to the right part\n(face). This also provides us with the texel (i.e. UV map\npixel) correspondence between the seam vertices. To softly\nblend the vertices inside the scalp region (left part of the\nUV map) near the seam, we first copy the displacements\nfrom the seam texels on the right to the respective texels\non the left. Then, we compute the coordinates of the re-\nspective nearest seam texel from all texels inside the scalp\nregion by applying distance transform algorithm [48]. In\nthe vicinity of 10 texels from the seam (in any direction in-\nside the region), we softly blend the displacements between\nthe original and the seam ones:\nU[i, j] := 10 \u2212 dist to seam[i, j]\n10\nU[nni[i, j], nnj[i, j]]\n+ dist to seam[i, j]\n10\nU[i, j],\nfor all (i, j) in the 10-texel vicinity of the seam,\nwhere (nni[i, j], nnj[i, j]) stands for the closest found seam\ntexel to (i, j) and dist to seam[i, j] stands for the distance\nto it. The same procedure to applied vice versa: the seam\ndisplacements from the left are copied to the face region\nborder and blended softly inside the face region.\nThe second step is aimed to fix the sampling errors that\ncan occur when sampling a generated UV map. To do that,\nwe again calculate the distance transform, this time for the\nempty space in the UV map (regions corresponding to no\nuseful information). This way, the nearest to the seam texel\nis found for each texel of the empty space, and we copy\nthe displacement from the nearest seam texel into this texel.\nEffectively, this replaces the empty space with the Voronoi\ndiagram [24] of proximity to the seam, with displacements\nfrom the seam occupying each region of the diagram. As a\nresult, when the displacements are being sampled from the\nUV map with the empty space filling procedure applied in\nadvance, no UV coordinates fall into the space where no\ndisplacements are set. In case of hitting the space that was\npreviously empty, the displacement attains a value close to\nthe nearest displacement at the seam.\nB. Results: Technical Details\nUnconditional sampling. In Fig. 13, we provide more un-\nconditional samples from our model from different view-\npoints. All the samples have been produced by sampling\nz \u2208 N(0, I) with a truncation trick [11] with the power\n\u03c8 = 0.7. For the evaluation in the Table 1 in the main text,\nthe implementation of the metrics MMD, JSD, COV from\nPointFlow [58] was used. Since FaceVerse contains sam-\nples grouped by subjects, the nearest neighbor of a ground\ntruth scan is typically a scan of the same subject with a\ndifferent expression. Because of that, we only select one\nground truth sample per subject (with the same neutral ex-\npression for all subjects) to calculate COV. All FaceVerse\nscans are used to calculate MMD and JSD. As a distance\nmeasure between individual point clouds, aggregated over\nmultiple observations in MMD and COV, we use Chamfer\nDistance (CD).\nAblating over the choice of the generative model archi-\ntecture. The VAE used in our experiments is based on the\nLightning Bolts library. The encoder follows the ResNet-\n18 architecture consisting of blocks of 2 convolutions each,\nwith every second convolution with a stride of two (starting\nfrom the third) to downsample the activations spatially the\nincreasing number of channels (64 in the first two blocks,\nthen 128, 256, 256, 256, 512, 512 in the next blocks, re-\nspectively). The Lightning Bolts implementation adds two\nfully-connected layers on top of the encoder (one for the\n\u00b5 and one for the \u03c3 prediction). The dimension of the la-\ntent space equals 512. The decoder follows the architecture\nsymmetric to the encoder, where the stride two for some\n13\nconvolutions is replaced with a nearest-neighbor 2x upscal-\ning.\nFor VQ-VAE implementation, we used the imple-\nmentation of the VQ layer from vector-quantize-pytorch.\nPixelcnn-pytorch served as a basis for the PixelCNN sam-\npler implementation. Similarly to VAE, ResNet-18 encoder\nand decoder were used, with the exception that fewer down-\nsampling operations have been used: they were introduced\nat each second layer (starting from the third), not each first\nlayer. This is introduced to maintain a trade-off between the\nautoencoder quality and sampling ability, i.e. not to make\nPixelCNN operate in a too small latent space. The spatial\nresolution of the bottleneck is 32 \u00d7 32, which we found to\nbe optimal, as the sampling performance of PixelCNN de-\ngrades from the top-left corner to the bottom-right corner\nand it is very noticeable already at the 64 \u00d7 64 bottleneck\nspatial resolution. The number of channels is 64, 128, 128,\n32 for each two consecutive blocks, respectively. VQ-VAE\nis trained for 10K steps with batch size of 8, which we found\nto be enough to reach the sufficient visual quality of autoen-\ncoding. To facilitate the sampling, we obtain a dataset of\nVQ indices and learn PixelCNN to autoregressively sample\nfrom those for 200 epochs with a batch size of 32.\nBehavior of the registration procedure. In Fig. 14, 15, 16,\n17, we show how the mesh deforms as a result of the vector\ndisplacements optimization and normal displacements opti-\nmization.\nConsistency of registrations. In Fig. 19, we demonstrate\nthe analysis as to which template vertices are selected by the\nregistration procedure to cover various regions of different\nmeshes. Since we know the UV coordinates of all template\nvertices, this can be done by rendering the meshes with a\nUV checker texture image. Note that the long hair parts,\nsuch as pony tails, are mostly explained by the same regions\nof the layout as the vertices they originate from.\nB.1. Applications\nFitting the latent code to a full scan. To fit the latent\nto the complete head scan, we have to apply preliminary\nsteps, similar to the ones used to construct the training set.\nFirstly fit the FLAME to the scan, then apply our registra-\ntion procedure to get a UV map Ugt. After that, we fit a\nw \u2208 W+ \u2282 R16\u00d7512 latent code for the StyleGAN gener-\nator g(w) : W+ \u2192 RH\u00d7W \u00d73 to satisfy the following loss\nterms:\nLfull\nopt (w|Ugt, \u03bb)\n= \u03bbLPIPS \u00b7 LPIPS(g(w), Ugt)\n+ \u03bbL1 \u00b7 L1(g(w), Ugt),\nwhere L1(\u00b7, \u00b7) is an average pixel-wise L1 distance between\ntwo images and LPIPS(\u00b7, \u00b7) corresponds to the LPIPS\nscore [62]. To calculate LPIPS, we cut the 256 \u00d7 256 UV\nmaps (both predicted U = g(w) and ground truth Ugt into\nsixteen 64\u00d764 patches, evaluate LPIPS between the respec-\ntive patches of U and Ugt, and average the obtained sixteen\nscores. The parameters of the loss equal to \u03bbLPIPS = 0.1 and\n\u03bbL1 = 3. The loss is being optimized via Adam algorithm\nwith the learning rate of 10\u22122 for 1K steps. The w is initial-\nized as the average latent predicted by the trained StyleGAN\nmapping network, evaluated over 105 codes z \u2208 N(0, I).\nFinally, we optimize for the StyleGAN noise (only for\nthe scalp region of the UV space) to better fit the tiny de-\ntails of the map Ugt. This step can be omitted in practice\nif fitting very high-frequency details is not required. Ex-\nactly the same loss terms are being optimized, this time not\nwith respect to w but with respect to the StyleGAN noise\ntensors of all generator layers, while w remains fixed. The\noptimization is again carried out by Adam with the same\nlearning rate and number of steps.\nFitting the latent code to a depth map. Fitting the la-\ntent representation to represent a partial observation poses\na more challenging problem than trying to represent a full\nscan, since the resulting displacements must both resem-\nble the original point cloud and complete it in a realistic\nway. This requires several changes to the fitting pipeline,\ndescribed next.\nFirstly, prior to applying the registration procedure to\nregister part of the cloud P in the UV space, we identify\nthe mask of points m \u2208 {0, 1}|V | that are allowed to be\noffset by selecting only the points within the convex hull\nof the point cloud, expanded by 1.5x from its center to\naccount for the possible important regions missing in the\npoint cloud. The points below a certain horizontal plane\nare not accounted for when estimating the convex hull to\ndisregard the shoulders and clothing, usually featured in\nNPHM raw scans. The level of the horizontal plane is se-\nlected as a 30% quantile of the coordinates of the points\nalong the vertical axis. Masking out the points too far from\nthe convex hull of the point cloud is especially important\nwhen the point cloud covers the minority of the geome-\ntry (e.g. if it is coming from a single depth map), since\nin this case, these points tend to pull in to cover the parts\nthat the points inside the hull cannot explain (e.g. due to the\nregularizations), and this results in a non-plausible shape.\nFor the registration procedure itself, stronger regularization\nparameters for the first stage have been selected, namely\n\u03bbStage 1 = (\u03bbChamfer\nStage 1 , \u03bbedge\nStage 1, \u03bblapl\nStage 1) = (2\u00b7103, 8\u00b7105, 105).\nThe correspondence pruning threshold, on the contrary, is\nraised to 10.0 for the first stage to allow the points to move\nfarther while maintaining higher smoothness of the overall\ngeometry due to stronger regularizations. For the second\nstage, the threshold is on the contrary reduced to 0.1 to pe-\nnalize for large false movement of points along the template\nnormals to explain the individual points of the cloud.\n14\nAt the end of the registration, we refine the mask of\nthe points by only selecting those of them that are suffi-\nciently close to the fitted point cloud: mfinal\ni\n= mi \u00b7 [d(vi +\nDStage 2,i, nn(vi + DStage 2,i, P)) \u2264 t], where t defines the\nproximity threshold, and its optimal value depends on the\nsparsity of the cloud. For the point cloud formed from a\ndense depth map, we set t = 0.1, and for a sparse cloud\nwith only 1% points of the original depth map left, we set\nt = 0.3. The regressed displacements and the mask are sep-\narately baked in the UV map as two independent images,\n3-channel real-valued U and 1-channel binary M, respec-\ntively. In Fig. 18, we demonstrate the typical result of the\npartial registration stages.\nAnother important change lies in the latent fitting pro-\ncedure. In our observations, the optimization of w \u2208 W+\nlatent code works great for the visible part but tends to pro-\nduce displacements closer to the average shape for the non-\nvisible part. We explain it by not strong enough supervision\nfrom the prior during fitting in W+ space. To mitigate that\neffect, we first fit the z \u2208 Z \u2282 RD latent code of the Style-\nGAN mapping network map(z) : Z \u2192 W+, obtain the\nrespective w = map(z) \u2208 W+ and regress the delta to\nthe w code: \u2206w. We found that optimizing z code yields\nmuch better, yet rougher result of completion, and refining\nthe map by regressing the \u2206w greatly improves fitting of\nthe details.\nIn more detail, during the first z optimization step, we\noptimize the following loss:\nLz\nopt(z|Ugt, \u03bb)\n= \u03bbLPIPS \u00b7 LPIPS(g(map(z)) \u00b7 M, Ugt \u00b7 M)\n+ \u03bbL1 \u00b7 L1(g(map(z)) \u00b7 M, Ugt \u00b7 M),\nSimilarly to the Lfull\nopt , we use \u03bbLPIPS = 0.1 and \u03bbL1 = 3.\nThe z is initialized from N(0, I) and further optimized by\nAdam with the learning rate of 10\u22122 for 500 steps. Here and\nfurther, LPIPS(\u00b7, \u00b7) and L1(\u00b7, \u00b7) follow the same expressions\nas for the full scan fitting.\nDuring the second \u2206w optimization step, we optimize a\nsimilar expression with a few additional terms:\nL\u2206w\nopt (\u2206w|z, Ugt, \u03bb)\n= \u03bbLPIPS \u00b7 LPIPS(g(map(z) + \u2206w) \u00b7 M, Ugt \u00b7 M)\n+ \u03bbL1 \u00b7 L1(g(map(z) + \u2206w) \u00b7 M, Ugt \u00b7 M)\n+ \u03bbLPIPS\npreserve \u00b7 LPIPS(g(map(z) + \u2206w) \u00b7 (1 \u2212 M),\ng(map(z) \u00b7 (1 \u2212 M))\n+ \u03bbL1\npreserve \u00b7 L1(g(map(z) + \u2206w) \u00b7 (1 \u2212 M),\ng(map(z)) \u00b7 (1 \u2212 M))\n+ \u03bbface\u2225g(map(z) + \u2206w) \u00b7 M face\u22251,\nwhere M face is a predefined mask of the face region in the\nUV space, reduced to the circle including the eyes, nose and\nmouth.\nThe third and second \u201cpreserve\u201d terms are introduced\nto not let the map guided by the \u2206w optimization deviate\nmuch from the output corresponding to the regressed z in\nnon-visible regions, which is essential due to the tendency\nof convergence to the average shape there when optimizing\nin the W+ space. \u03bbLPIPS = 0.1 and \u03bbL1 = 3 remain the\nsame as before, and \u03bbLPIPS\npreserve = 0.01 and \u03bbL1\npreserve = 0.3 are\nselected 10\u00d7 less. The last regularization term is introduced\nto avoid hallucinations in the facial region that otherwise\nbecome visually apparent in the non-visible region of the\nmap even in case of relatively small high-frequency incon-\nsistency. \u03bbface is set to\n10\n256. The optimization is carried out\nby Adam with the same learning rate of 10\u22122 for 500 steps.\nThe \u2206w is initialized with zeros.\nFinally, we optimize the StyleGAN noise to improve the\ndetails in the visible part. Despite that we consider this\nstep optional, we found that it helps reconstruct more detail\neven for a sparse cloud. We optimize the same expression\nas Lz\nopt, with the difference that it is only being optimized\nw.r.t. the StyleGAN noise tensors (only in the scalp region).\nThe only modification is the introduced regularization that\nequals to the sum of the noise tensors L2 norms. The opti-\nmization is carried out by Adam with the same learning rate\nof 10\u22122 for 500 steps. The coefficient of this regularization\nis equal to 10\u22125.\nIn the Supplementary Video, we demonstrate more re-\nsults of fitting the latent to the point clouds with different\nsparsity.\nAnimation. Here we expand on more details regarding ap-\nplying displacements to a template, deforming over time.\nCompared to the simple unconditional scenario, where the\ndisplacements are also applied to a certain FLAME tem-\nplate, we have to introduce two key differences.\nFirst, as mentioned in Subsec. A.1, to apply the displace-\nments to the template, we apply Butterfly subdivision, the\nMeshLab implementation of which also smooths the sur-\nface. However, the result of Butterfly is not consistent over\nvarious FLAME templates and yields a bit different number\nof vertices every time. To solve that, we come up with con-\nsistent subdivision, i.e. the way to construct the same topol-\nogy for every FLAME. To do that, we first apply Butterfly\nsubdivision to an arbitrary scan, and for each vertex after\nthe subdivision, we find which triangle of the original tem-\nplate it belongs to and the barycentric coordinates w.r.t. that\ntriangle. Later, for every new template, the locations of the\nsubdivided vertices are evaluated based on these triangles\nand barycentric coordinates. To handle the seam accurately,\nwe consider each vertex of every triangle after subdivision\nindividually, thus accounting for the duplicate vertices.\nAn artifact of such procedure is that the smoothness of\n15\nthe surface, introduced in the MeshLab implementation of\nButterfly subdivision, cannot be trivially transferred onto\na new mesh this way.\nBecause of this, the surface nor-\nmals remain the same within the large triangles of the orig-\ninal template even after the subdivision, creating a non-\nappealing \u201ctiling\u201d effect. To mitigate that, we apply Lapla-\ncian smoothing [54] in its classical version to smooth the\nsurface. In order to account for important subtle parts, we\napply a different number of Laplacian smoothing iterations\nto different regions, namely, 3 times to the lips region, 5\ntimes to the face skin (face except mouth, eyeballs and eye\nsurroundings), and 10 times to the scalp and the neck. Since\nthe realism of mouth, ears, and eyeballs is important for an-\nimation, they remain intact.\nSecond, as mentioned in the main text, we rotate\nthe displacements according to the rotation of the sur-\nface normals of the template.\nTo do that, we first es-\ntimate the local basis of the TBN space [2] for each\nFLAME in a sequence.\nThis basis defines the nor-\nmalized tangent tk\ni , bitangent bk\ni , and normal nk\ni , pre-\nestimated for the i-th vertex of the FLAME template\nF k = FLAME(shape, expk, jawk, headposek). In addi-\ntion, we estimate the TBN basis (tneutral\ni\n, bneutral\ni\n, nneutral\ni\n) for\na FLAME, corresponding to the same person and a neutral\nexpression and pose F neutral = FLAME(shape, 0, 0, 0).\nThe displacements D, queried from the generated UV map\nU, are first transferred from the object space into the neutral\nTBN space:\nDTBN =\n\u0000(tneutral\ni\n\u00b7 di), (bneutral\ni\n\u00b7 di), (nneutral\ni\n\u00b7 di)\n\u0001|D|\ni=1\nThen, for each of the sequence frames, we transfer them\ninto object space, this time w.r.t. the TBN basis of the given\nframe:\nDobject\nk\n=\n\u0000\u0002\ntk\ni\nbk\ni\nnk\ni\n\u0003\n\u00b7 dTBN\ni\n\u0001|D|\ni=1\n(the tk\ni , bk\ni , nk\ni , dTBN\ni\nvectors above treated as columns).\nMore examples of animations can be found in the Sup-\nplementary Video.\n16\nFrontal\nLeft\nRight\nBack\nFrontal\nLeft\nRight\nBack\nFigure 13. Additional results of the diversity and level of detail of the unconditionally sampled generations from HeadCraft. The genera-\ntions are obtained by randomly sampling z \u223c N(0, I) latent code of the generative model. The displacements, returned by the model, are\napplied to the random FLAMEs sampled from Gaussian distribution with statistics calculated over the NPHM dataset.\n17\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFigure 14. Additional demonstration of the two-stage registration. Stage 1 corresponds to the vector displacements regression; Stage 2 \u2013 to\nthe refinement of the displacements along the normals. The second stage significantly improves the level of detail and allows us to match\nthe high-frequency component of the scans, such as strands and subtle face features.\n18\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFigure 15. Additional demonstration of the two-stage registration. Stage 1 corresponds to the vector displacements regression; Stage 2 \u2013 to\nthe refinement of the displacements along the normals. The second stage significantly improves the level of detail and allows us to match\nthe high-frequency component of the scans, such as strands and subtle face features.\n19\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFigure 16. Additional demonstration of the two-stage registration. Stage 1 corresponds to the vector displacements regression; Stage 2 \u2013 to\nthe refinement of the displacements along the normals. The second stage significantly improves the level of detail and allows us to match\nthe high-frequency component of the scans, such as strands and subtle face features.\n20\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFLAME\nStage 1\nStage 2\nGround truth\nFrontal\nLeft\nTop\nFigure 17. Additional demonstration of the two-stage registration. Stage 1 corresponds to the vector displacements regression; Stage 2 \u2013 to\nthe refinement of the displacements along the normals. The second stage significantly improves the level of detail and allows us to match\nthe high-frequency component of the scans, such as strands and subtle face features.\n21\nInput p.c.\nFLAME\nStage 1\nStage 2\nMasking mfinal\nGround truth\nFrontal\nLeft\nTop\nInput p.c.\nFLAME\nStage 1\nStage 2\nMasking mfinal\nGround truth\nFrontal\nLeft\nTop\nFigure 18. Demonstration of the stages of the partial registration procedure required to fit a part of the scan. The key difference between\nthis procedure and the standard registration used to generate training data for HeadCraft is in the presence of only a part of the scan, e.g. a\npoint cloud coming from the depth map. To overcome that obstacle, the displacements are being estimated only in the convex hull of the\npoint cloud, and are subsequently filtered out by a separate mask mfinal, leaving only the displacements close enough the ground truth scan\n(others are nullified in this visualization).\n22\nFigure 19. Consistency analysis of the registration. We demonstrate which template vertices are offset with the registration procedure to\ncover various regions of different meshes. Since we know the UV coordinates of all template vertices, this can be done by rendering the\nmeshes with a UV checker texture image. For clarity of the visualization, the texture is applied to the standard FLAME layout. Note that\nthe long hair parts, such as pony tails, are mostly explained by the same regions of the layout as the vertices they originate from.\n23\n"
  }
]