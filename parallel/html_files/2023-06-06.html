<!DOCTYPE html><html class=""><head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<meta name="description" content="Your daily dose of AI research from AK">
		<meta property="fb:app_id" content="1321688464574422">
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@huggingface">
		<meta property="og:title" content="Daily Papers - Hugging Face">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://huggingface.co/papers">
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/papers.png">

		<link rel="stylesheet" href="/front/build/kube-351e67d/style.css">

		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&amp;display=swap" rel="stylesheet">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		  

		<title>Daily Papers - Hugging Face</title>

		<script defer="" data-domain="huggingface.co" src="/js/script.js"></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer=""></script>
	<script src="https://js.stripe.com/v3/" async=""></script><script src="https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapersBannerSubscribe-98f5dbb5.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconCheckmarkFilled-0fb1cbef.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapers-ff82993e.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/Contributors-aac8a263.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/autoplay-4f99a53d.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconMessage-6ab20750.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconArrowLeft-a638f296.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/ModalBody-205aeb00.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/urlWatcher-b1dcfbe0.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-79e4fb58.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconSpinner-f6a85825.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UpvoteControl-821d7cbb.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconUpvoteFilled-f11951bc.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UsersListModal-9fa7b704.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/FollowButton-18e671ce.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-997dbc18.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconBellWatching-37394c1a.js"><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"></head>
	<body class="flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black DailyPapersPage">
		<div class="flex min-h-screen flex-col">
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false}" data-target="MainHeader"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg"> <span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a> <div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..." spellcheck="false" type="text"> <svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> </div> <div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg> </button> </div></div> <nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 xl:space-x-2"><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Models</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> Datasets</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg> Spaces</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg> Posts</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg> Docs</a></li> <li class="max-2xl:hidden"><div class="relative "><button class="px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center " type="button"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z" fill="currentColor"></path></svg> Solutions </button> </div></li> <li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/pricing">Pricing</a></li> <li><div class="relative group"><button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button"><svg class="mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>  </button> </div></li> <li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li> <li><a class="block cursor-pointer px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/login">Log In</a></li> <li><a class="rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up</a></li></ul></nav></div></header></div>
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="GoogleAnalyticsTracker"></div>
	
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="SSOBanner"></div>
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-props="{&quot;isLoggedIn&quot;:false}" data-target="DailyPapersBannerSubscribe"><div class="-mt-px flex h-9 w-full justify-center text-gray-600"><svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z"></path></svg> <div class="flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1"><div class="rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600">new</div> <p class="hidden sm:inline">Get trending papers in your email inbox once a day!</p> <p class="inline sm:hidden">Get trending papers in your email inbox!</p> <a href="/login?next=%2Fpapers" class="btn !px-2 text-sm leading-none">Subscribe</a></div> <svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z"></path></svg></div></div>
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;date&quot;:&quot;2023-06-06T00:00:00.000Z&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02707&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647e843218274bce0307a091&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/eefec358f0d36b9c18d548f4cd2d8806.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Subhabrata Mukherjee&quot;,&quot;user&quot;:&quot;subho&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Subhabrata Mukherjee&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T01:20:13.175Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e843218274bce0307a092&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b5429d85c030a9f08e1d2bb37ce9974d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Arindam Mitra&quot;,&quot;user&quot;:&quot;ari9dam&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Arindam Mitra&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:01.086Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e843218274bce0307a093&quot;,&quot;name&quot;:&quot;Ganesh Jawahar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e843218274bce0307a094&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e0ed6aa70e706865a6755c5fd10d5766.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sahaj Agarwal&quot;,&quot;user&quot;:&quot;sahajgg&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sahaj Agarwal&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-17T06:49:24.105Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e843218274bce0307a095&quot;,&quot;name&quot;:&quot;Hamid Palangi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e843218274bce0307a096&quot;,&quot;name&quot;:&quot;Ahmed Awadallah&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T08:58:39.000Z&quot;,&quot;title&quot;:&quot;Orca: Progressive Learning from Complex Explanation Traces of GPT-4&quot;,&quot;summary&quot;:&quot;Recent research has focused on enhancing the capability of smaller models\nthrough imitation learning, drawing on the outputs generated by large\nfoundation models (LFMs). A number of issues impact the quality of these\nmodels, ranging from limited imitation signals from shallow LFM outputs; small\nscale homogeneous training data; and most notably a lack of rigorous evaluation\nresulting in overestimating the small model's capability as they tend to learn\nto imitate the style, but not the reasoning process of LFMs. To address these\nchallenges, we develop Orca (We are working with our legal team to publicly\nrelease a diff of the model weights in accordance with LLaMA's release policy\nto be published at https://aka.ms/orca-lm), a 13-billion parameter model that\nlearns to imitate the reasoning process of LFMs. Orca learns from rich signals\nfrom GPT-4 including explanation traces; step-by-step thought processes; and\nother complex instructions, guided by teacher assistance from ChatGPT. To\npromote this progressive learning, we tap into large-scale and diverse\nimitation data with judicious sampling and selection. Orca surpasses\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B by\nmore than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard\n(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH\nbenchmark and shows competitive performance (4 pts gap with optimized system\nmessage) in professional and academic examinations like the SAT, LSAT, GRE, and\nGMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our\nresearch indicates that learning from step-by-step explanations, whether these\nare generated by humans or more advanced AI models, is a promising direction to\nimprove model capabilities and skills.&quot;,&quot;upvotes&quot;:43},&quot;publishedAt&quot;:&quot;2023-06-06T00:56:19.207Z&quot;,&quot;title&quot;:&quot;Orca: Progressive Learning from Complex Explanation Traces of GPT-4&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YbzxRvprz0WcAwFSqvUiy.png&quot;,&quot;numComments&quot;:18,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02858&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647e9efc0ed7d0c8760edaf7&quot;,&quot;name&quot;:&quot;Hang Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e9efc0ed7d0c8760edaf8&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/aec44edd5470dd6e767e0a25efd6fb5d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xin Li&quot;,&quot;user&quot;:&quot;lixin4ever&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xin Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:33.808Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e9efc0ed7d0c8760edaf9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lidong Bing&quot;,&quot;user&quot;:&quot;LidongBing&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Lidong Bing&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:37.008Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T13:17:27.000Z&quot;,&quot;title&quot;:&quot;Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding&quot;,&quot;summary&quot;:&quot;We present Video-LLaMA, a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual \\&amp; audio encoders and the frozen LLMs. Unlike\nprevious vision- LLMs that focus on static image comprehensions such as\nMiniGPT-4~zhu2023minigpt and LLaVA~liu2023visualit, Video-LLaMA\ntackles two challenges in video understanding: (1) capturing the temporal\nchanges in visual scenes, (2) integrating audio-visual signals. For the first\nchallenge, we propose Video Q-former to extend the pre-trained image encoder to\na video encoder and introduce a video-to-text generation task to learn\nvideo-language correspondence. For the second challenge, we leverage\nImageBind~girdhar2023imagebind as the pre-trained audio encoder which\nperforms exceptionally well in aligning different modalities to a common\nembedding space. And then introduce an Audio Q-former to learn auditory query\ntokens. To align the output of both visual \\&amp; audio encoder with LLM's\nembedding space, we train Video-LLaMA on a large-scale vision caption dataset\nand a hign-quantity vision-instruction-tuning dataset. We found Video-LLaMA\nshowcases the ability to perceive and comprehend video content, generating\nmeaningful responses that are grounded in the visual and auditory information\npresent in the videos. This highlights the potential of Video-LLaMA as a\npromising prototype for audio-visual AI assistants. Our code, pre-trained\nmodel, and demo are available at\nhttps://github.com/DAMO-NLP-SG/Video-LLaMA.&quot;,&quot;upvotes&quot;:12},&quot;publishedAt&quot;:&quot;2023-06-06T02:50:37.297Z&quot;,&quot;title&quot;:&quot;Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KCCf3luovZZdHTgICVAvk.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02254&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eaa82becb41a2729772cd&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634604273263-5fd888cf61e46993190ce543.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyunwoong Ko&quot;,&quot;user&quot;:&quot;hyunwoongko&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hyunwoong Ko&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T19:41:00.982Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772ce&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fa11101a13e063b8b2b5d34/p2Sm4b64hn_xXg1GtrkIA.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kichang Yang&quot;,&quot;user&quot;:&quot;jason9693&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kichang Yang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:49:30.533Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772cf&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623809612769-60436d159e905013ae8715d7.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minho Ryu&quot;,&quot;user&quot;:&quot;bzantium&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Minho Ryu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:50:04.739Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772d0&quot;,&quot;name&quot;:&quot;Taekyoon Choi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772d1&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/ecb4c6db9092c0ba84695cf0a33e5a91.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Seungmoo Yang&quot;,&quot;user&quot;:&quot;Moo&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Seungmu Yang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T10:09:40.241Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772d2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662805242568-61e78aa282b19b93e1a53cbe.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Jiwung Hyun&quot;,&quot;user&quot;:&quot;kabbi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;jiwung Hyun&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:52:26.750Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa82becb41a2729772d3&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660573263107-625fc71affe8827cb1d3b1f8.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sungho Park&quot;,&quot;user&quot;:&quot;naem1023&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sungho Park&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:53:18.455Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-04T04:04:04.000Z&quot;,&quot;title&quot;:&quot;A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean\n  Language Models&quot;,&quot;summary&quot;:&quot;Polyglot is a pioneering project aimed at enhancing the non-English language\nperformance of multilingual language models. Despite the availability of\nvarious multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et\nal., 2022), and BLOOM (Scao et al., 2022), researchers and developers often\nresort to building monolingual models in their respective languages due to the\ndissatisfaction with the current multilingual models non-English language\ncapabilities. Addressing this gap, we seek to develop advanced multilingual\nlanguage models that offer improved performance in non-English languages. In\nthis paper, we introduce the Polyglot Korean models, which represent a specific\nfocus rather than being multilingual in nature. In collaboration with TUNiB,\nour team collected 1.2TB of Korean data meticulously curated for our research\njourney. We made a deliberate decision to prioritize the development of Korean\nmodels before venturing into multilingual models. This choice was motivated by\nmultiple factors: firstly, the Korean models facilitated performance\ncomparisons with existing multilingual models; and finally, they catered to the\nspecific needs of Korean companies and researchers. This paper presents our\nwork in developing the Polyglot Korean models, which propose some steps towards\naddressing the non-English language performance gap in multilingual language\nmodels.&quot;,&quot;upvotes&quot;:8},&quot;publishedAt&quot;:&quot;2023-06-06T03:39:47.702Z&quot;,&quot;title&quot;:&quot;A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nGxYptQ7gym1rK_mhNgi8.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02561&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647e8610e4d52fe0e02108bf&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/FOdpXWhe4vZbLHM3TQnQU.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dongfu Jiang&quot;,&quot;user&quot;:&quot;DongfuTingle&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Dongfu Jiang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T10:03:21.541Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e8610e4d52fe0e02108c0&quot;,&quot;name&quot;:&quot;Xiang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e8610e4d52fe0e02108c1&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bill Yuchen Lin&quot;,&quot;user&quot;:&quot;yuchenlin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Bill Yuchen Lin&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:40:05.051Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T03:32:26.000Z&quot;,&quot;title&quot;:&quot;LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and\n  Generative Fusion&quot;,&quot;summary&quot;:&quot;We present LLM-Blender, an ensembling framework designed to attain\nconsistently superior performance by leveraging the diverse strengths of\nmultiple open-source large language models (LLMs). Our framework consists of\ntwo modules: PairRanker and GenFuser, addressing the observation that optimal\nLLMs for different examples can significantly vary. PairRanker employs a\nspecialized pairwise comparison method to distinguish subtle differences\nbetween candidate outputs. It jointly encodes the input text and a pair of\ncandidates, using cross-attention encoders to determine the superior one. Our\nresults demonstrate that PairRanker exhibits the highest correlation with\nChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,\ngenerating an improved output by capitalizing on their strengths and mitigating\ntheir weaknesses. To facilitate large-scale evaluation, we introduce a\nbenchmark dataset, MixInstruct, which is a mixture of multiple instruction\ndatasets featuring oracle pairwise comparisons. Our LLM-Blender significantly\noutperform individual LLMs and baseline methods across various metrics,\nestablishing a substantial performance gap.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-06T01:04:16.956Z&quot;,&quot;title&quot;:&quot;LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0xiBm_RWrnJMJ-ifsIQOX.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.03038&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eae12a49bffab5d7503c7&quot;,&quot;name&quot;:&quot;Xiao Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503c8&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yukang Cao&quot;,&quot;user&quot;:&quot;yukangcao&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yukang Cao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-08-21T18:04:39.516Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503c9&quot;,&quot;name&quot;:&quot;Kai Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503ca&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/dc23e15acf7ee7f3b9f1a68c2716a66d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiatian Zhu&quot;,&quot;user&quot;:&quot;Xiatian-Zhu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xiatian Zhu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T10:15:46.790Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503cb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657566065867-noauth.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JiankangDeng&quot;,&quot;user&quot;:&quot;JiankangDeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jiankang Deng&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T10:17:24.390Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503cc&quot;,&quot;name&quot;:&quot;Yi-Zhe Song&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503cd&quot;,&quot;name&quot;:&quot;Tao Xiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eae12a49bffab5d7503ce&quot;,&quot;name&quot;:&quot;Kwan-Yee K. Wong&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T16:53:58.000Z&quot;,&quot;title&quot;:&quot;HeadSculpt: Crafting 3D Head Avatars with Text&quot;,&quot;summary&quot;:&quot;Recently, text-guided 3D generative methods have made remarkable advancements\nin producing high-quality textures and geometry, capitalizing on the\nproliferation of large vision-language and image diffusion models. However,\nexisting methods still struggle to create high-fidelity 3D head avatars in two\naspects: (1) They rely mostly on a pre-trained text-to-image diffusion model\nwhilst missing the necessary 3D awareness and head priors. This makes them\nprone to inconsistency and geometric distortions in the generated avatars. (2)\nThey fall short in fine-grained editing. This is primarily due to the inherited\nlimitations from the pre-trained 2D image diffusion models, which become more\npronounced when it comes to 3D head avatars. In this work, we address these\nchallenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt\nfor crafting (i.e., generating and editing) 3D head avatars from textual\nprompts. Specifically, we first equip the diffusion model with 3D awareness by\nleveraging landmark-based control and a learned textual embedding representing\nthe back view appearance of heads, enabling 3D-consistent head avatar\ngenerations. We further propose a novel identity-aware editing score\ndistillation strategy to optimize a textured mesh with a high-resolution\ndifferentiable rendering technique. This enables identity preservation while\nfollowing the editing instruction. We showcase HeadSculpt's superior fidelity\nand editing capabilities through comprehensive experiments and comparisons with\nexisting methods.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-06T03:55:02.859Z&quot;,&quot;title&quot;:&quot;HeadSculpt: Crafting 3D Head Avatars with Text&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y0LZuefVWUGQpb3F2CRN8.qt&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.03082&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647e877fe4d52fe0e0210f88&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62323bb408bcea92917e42ee/iD3PJfKNp_n-PRMvL4MUO.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lichang Chen&quot;,&quot;user&quot;:&quot;Lichang-Chen&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Lichang Chen&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:06.023Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e877fe4d52fe0e0210f89&quot;,&quot;name&quot;:&quot;Jiuhai Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e877fe4d52fe0e0210f8a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/84dfdca8e1cd6fbf50d6fb2a6f1b488d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tom Goldstein&quot;,&quot;user&quot;:&quot;tomgoldstein&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tom Goldstein&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T09:59:12.898Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e877fe4d52fe0e0210f8b&quot;,&quot;name&quot;:&quot;Heng Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e877fe4d52fe0e0210f8c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;user&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tianyi Zhou&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:10.958Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T17:55:22.000Z&quot;,&quot;title&quot;:&quot;InstructZero: Efficient Instruction Optimization for Black-Box Large\n  Language Models&quot;,&quot;summary&quot;:&quot;Large language models~(LLMs) are instruction followers, but it can be\nchallenging to find the best instruction for different situations, especially\nfor black-box LLMs on which backpropagation is forbidden. Instead of directly\noptimizing the discrete instruction, we optimize a low-dimensional soft prompt\napplied to an open-source LLM to generate the instruction for the black-box\nLLM. On each iteration of the proposed method, which we call InstructZero, a\nsoft prompt is converted into an instruction using the open-source LLM, which\nis then submitted to the black-box LLM for zero-shot evaluation, and the\nperformance is sent to Bayesian optimization to produce new soft prompts\nimproving the zero-shot performance. We evaluate InstructZero on different\ncombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our\nresults show that InstructZero outperforms SOTA auto-instruction methods across\na variety of downstream tasks. Our code and data are publicly available at\nhttps://github.com/Lichang-Chen/InstructZero.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-06T01:10:24.626Z&quot;,&quot;title&quot;:&quot;InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/F3oBas2kcucW5R55In6iE.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02982&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eab8f18274bce03088d01&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/60d92bef43a7d1342f12ef5ac984f025.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qianqian Dong&quot;,&quot;user&quot;:&quot;QQD&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Qianqian Dong&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T10:09:51.551Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d02&quot;,&quot;name&quot;:&quot;Zhiying Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d03&quot;,&quot;name&quot;:&quot;Chen Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d04&quot;,&quot;name&quot;:&quot;Yunlong Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d05&quot;,&quot;name&quot;:&quot;Kexin Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d06&quot;,&quot;name&quot;:&quot;Xuxin Cheng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d07&quot;,&quot;name&quot;:&quot;Tom Ko&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d08&quot;,&quot;name&quot;:&quot;Qiao Tian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d09&quot;,&quot;name&quot;:&quot;Tang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0a&quot;,&quot;name&quot;:&quot;Fengpeng Yue&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0b&quot;,&quot;name&quot;:&quot;Ye Bai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0c&quot;,&quot;name&quot;:&quot;Xi Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0d&quot;,&quot;name&quot;:&quot;Lu Lu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0e&quot;,&quot;name&quot;:&quot;Zejun Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d0f&quot;,&quot;name&quot;:&quot;Yuping Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d10&quot;,&quot;name&quot;:&quot;Mingxuan Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eab8f18274bce03088d11&quot;,&quot;name&quot;:&quot;Yuxuan Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T15:53:15.000Z&quot;,&quot;title&quot;:&quot;PolyVoice: Language Models for Speech to Speech Translation&quot;,&quot;summary&quot;:&quot;We propose PolyVoice, a language model-based framework for speech-to-speech\ntranslation (S2ST) system. Our framework consists of two language models: a\ntranslation language model and a speech synthesis language model. We use\ndiscretized speech units, which are generated in a fully unsupervised way, and\nthus our framework can be used for unwritten languages. For the speech\nsynthesis part, we adopt the existing VALL-E X approach and build a unit-based\naudio language model. This grants our framework the ability to preserve the\nvoice characteristics and the speaking style of the original speech. We examine\nour system on Chinese rightarrow English and English rightarrow Spanish\npairs. Experimental results show that our system can generate speech with high\ntranslation quality and audio quality. Speech samples are available at\nhttps://speechtranslation.github.io/polyvoice.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-06T03:44:16.180Z&quot;,&quot;title&quot;:&quot;PolyVoice: Language Models for Speech to Speech Translation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UIMTZmYEAm6zLO8ozXQm2.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01741&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ec078a49bffab5d75689d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1ff8a1c9d4c6bde1893e4f80c5fd9fdb.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wake&quot;,&quot;user&quot;:&quot;NaokiWake&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Naoki Wake&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-09-21T05:17:39.810Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec078a49bffab5d75689e&quot;,&quot;name&quot;:&quot;Atsushi Kanehira&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec078a49bffab5d75689f&quot;,&quot;name&quot;:&quot;Kazuhiro Sasabuchi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec078a49bffab5d7568a0&quot;,&quot;name&quot;:&quot;Jun Takamatsu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec078a49bffab5d7568a1&quot;,&quot;name&quot;:&quot;Katsushi Ikeuchi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-05-10T10:14:16.000Z&quot;,&quot;title&quot;:&quot;GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System&quot;,&quot;summary&quot;:&quot;This technical paper introduces a chatting robot system that utilizes recent\nadvancements in large-scale language models (LLMs) such as GPT-3 and ChatGPT.\nThe system is integrated with a co-speech gesture generation system, which\nselects appropriate gestures based on the conceptual meaning of speech. Our\nmotivation is to explore ways of utilizing the recent progress in LLMs for\npractical robotic applications, which benefits the development of both chatbots\nand LLMs. Specifically, it enables the development of highly responsive chatbot\nsystems by leveraging LLMs and adds visual effects to the user interface of\nLLMs as an additional value. The source code for the system is available on\nGitHub for our in-house robot\n(https://github.com/microsoft/LabanotationSuite/tree/master/MSRAbotChatSimulation)\nand GitHub for Toyota HSR\n(https://github.com/microsoft/GPT-Enabled-HSR-CoSpeechGestures).&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-06T05:13:28.893Z&quot;,&quot;title&quot;:&quot;GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/irHmtQiRTIOrO_7DjQi3K.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01841&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ebce9becb41a27297d7f6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/aaa2d0eadbcf96c2eb9059e3d73c2760.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Liu&quot;,&quot;user&quot;:&quot;Zechun&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zechun Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:48.383Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ebce9becb41a27297d7f7&quot;,&quot;name&quot;:&quot;Barlas Oguz&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ebce9becb41a27297d7f8&quot;,&quot;name&quot;:&quot;Aasish Pappu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ebce9becb41a27297d7f9&quot;,&quot;name&quot;:&quot;Yangyang Shi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ebce9becb41a27297d7fa&quot;,&quot;name&quot;:&quot;Raghuraman Krishnamoorthi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-02T18:01:02.000Z&quot;,&quot;title&quot;:&quot;Binary and Ternary Natural Language Generation&quot;,&quot;summary&quot;:&quot;Ternary and binary neural networks enable multiplication-free computation and\npromise multiple orders of magnitude efficiency gains over full-precision\nnetworks if implemented on specialized hardware. However, since both the\nparameter and the output space are highly discretized, such networks have\nproven very difficult to optimize. The difficulties are compounded for the\nclass of transformer text generation models due to the sensitivity of the\nattention operation to quantization and the noise-compounding effects of\nautoregressive decoding in the high-cardinality output space. We approach the\nproblem with a mix of statistics-based quantization for the weights and elastic\nquantization of the activations and demonstrate the first ternary and binary\ntransformer models on the downstream tasks of summarization and machine\ntranslation. Our ternary BART base achieves an R1 score of 41 on the\nCNN/DailyMail benchmark, which is merely 3.9 points behind the full model while\nbeing 16x more efficient. Our binary model, while less accurate, achieves a\nhighly non-trivial score of 35.6. For machine translation, we achieved BLEU\nscores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full\nprecision mBART model score of 26.8. We also compare our approach in the 8-bit\nactivation setting, where our ternary and even binary weight models can match\nor outperform the best existing 8-bit weight models in the literature. Our code\nand models are available at:\nhttps://github.com/facebookresearch/Ternary_Binary_Transformer&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-06T04:58:18.342Z&quot;,&quot;title&quot;:&quot;Binary and Ternary Natural Language Generation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/n64Q0PlXmOD7AHgH44mYg.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.03083&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017a&quot;,&quot;name&quot;:&quot;Chiyu Max Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017b&quot;,&quot;name&quot;:&quot;Andre Cornman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017c&quot;,&quot;name&quot;:&quot;Cheolho Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017d&quot;,&quot;name&quot;:&quot;Ben Sapp&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017e&quot;,&quot;name&quot;:&quot;Yin Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb52fcfca67bc5000017f&quot;,&quot;name&quot;:&quot;Dragomir Anguelov&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T17:55:52.000Z&quot;,&quot;title&quot;:&quot;MotionDiffuser: Controllable Multi-Agent Motion Prediction using\n  Diffusion&quot;,&quot;summary&quot;:&quot;We present MotionDiffuser, a diffusion based representation for the joint\ndistribution of future trajectories over multiple agents. Such representation\nhas several key advantages: first, our model learns a highly multimodal\ndistribution that captures diverse future outcomes. Second, the simple\npredictor design requires only a single L2 loss training objective, and does\nnot depend on trajectory anchors. Third, our model is capable of learning the\njoint distribution for the motion of multiple agents in a permutation-invariant\nmanner. Furthermore, we utilize a compressed trajectory representation via PCA,\nwhich improves model performance and allows for efficient computation of the\nexact sample log probability. Subsequently, we propose a general constrained\nsampling framework that enables controlled trajectory sampling based on\ndifferentiable cost functions. This strategy enables a host of applications\nsuch as enforcing rules and physical priors, or creating tailored simulation\nscenarios. MotionDiffuser can be combined with existing backbone architectures\nto achieve top motion forecasting results. We obtain state-of-the-art results\nfor multi-agent motion prediction on the Waymo Open Motion Dataset.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-06T04:25:20.063Z&quot;,&quot;title&quot;:&quot;MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/14fFUDqr_XlwClJrNdHdJ.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.03024&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647e86aa0ed7d0c8760e482d&quot;,&quot;name&quot;:&quot;Laura Cabello&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e86aa0ed7d0c8760e482e&quot;,&quot;name&quot;:&quot;Jiaang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647e86aa0ed7d0c8760e482f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646037955111-noauth.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ilias Chalkidis&quot;,&quot;user&quot;:&quot;kiddothe2b&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ilias Chalkidis&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:08.642Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T16:44:27.000Z&quot;,&quot;title&quot;:&quot;PokemonChat: Auditing ChatGPT for Pokémon Universe Knowledge&quot;,&quot;summary&quot;:&quot;The recently released ChatGPT model demonstrates unprecedented capabilities\nin zero-shot question-answering. In this work, we probe ChatGPT for its\nconversational understanding and introduce a conversational framework\n(protocol) that can be adopted in future studies. The Pok\\'emon universe serves\nas an ideal testing ground for auditing ChatGPT's reasoning capabilities due to\nits closed world assumption. After bringing ChatGPT's background knowledge (on\nthe Pok\\'emon universe) to light, we test its reasoning process when using\nthese concepts in battle scenarios. We then evaluate its ability to acquire new\nknowledge and include it in its reasoning process. Our ultimate goal is to\nassess ChatGPT's ability to generalize, combine features, and to acquire and\nreason over newly introduced knowledge from human feedback. We find that\nChatGPT has prior knowledge of the Pokemon universe, which can reason upon in\nbattle scenarios to a great extent, even when new information is introduced.\nThe model performs better with collaborative feedback and if there is an\ninitial phase of information retrieval, but also hallucinates occasionally and\nis susceptible to adversarial attacks.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-06T01:06:52.476Z&quot;,&quot;title&quot;:&quot;PokemonChat: Auditing ChatGPT for Pokémon Universe Knowledge&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yrLLbGoIcWSr0JhB1FViK.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.03092&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c957&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/37ececdbede0be5770719884be4095d8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaoshuo Li&quot;,&quot;user&quot;:&quot;mli0603&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhaoshuo Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:39.807Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c958&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qXrAoX_wCrtIUfSCfwgSq.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Thomas Müller&quot;,&quot;user&quot;:&quot;Tom94&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Thomas Müller&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:19.174Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c959&quot;,&quot;name&quot;:&quot;Alex Evans&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c95a&quot;,&quot;name&quot;:&quot;Russell H. Taylor&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c95b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/9a028ef6cac40956f357b4a0481fc1d7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mathias Unberath&quot;,&quot;user&quot;:&quot;mathiasunberath&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Mathias Unberath&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T12:32:24.563Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c95c&quot;,&quot;name&quot;:&quot;Ming-Yu Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ec1f19bb822b5cd43c95d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/4bb215bd711452ee6d28894706773009.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen-Hsuan Lin&quot;,&quot;user&quot;:&quot;chenhsuanlin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chen-Hsuan Lin&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T19:40:56.149Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T17:59:57.000Z&quot;,&quot;title&quot;:&quot;Neuralangelo: High-Fidelity Neural Surface Reconstruction&quot;,&quot;summary&quot;:&quot;Neural surface reconstruction has been shown to be powerful for recovering\ndense 3D surfaces via image-based neural rendering. However, current methods\nstruggle to recover detailed structures of real-world scenes. To address the\nissue, we present Neuralangelo, which combines the representation power of\nmulti-resolution 3D hash grids with neural surface rendering. Two key\ningredients enable our approach: (1) numerical gradients for computing\nhigher-order derivatives as a smoothing operation and (2) coarse-to-fine\noptimization on the hash grids controlling different levels of details. Even\nwithout auxiliary inputs such as depth, Neuralangelo can effectively recover\ndense 3D surface structures from multi-view images with fidelity significantly\nsurpassing previous methods, enabling detailed large-scale scene reconstruction\nfrom RGB video captures.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T05:19:46.994Z&quot;,&quot;title&quot;:&quot;Neuralangelo: High-Fidelity Neural Surface Reconstruction&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gXehQPJtd247aiKdLBvW.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02531&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc6006c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/c320b53a31ffba41bd9ad71da99ebc83.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yizhe Zhang&quot;,&quot;user&quot;:&quot;YizheZ&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yizhe Zhang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:37.181Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc6006d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634002684894-noauth.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiatao Gu&quot;,&quot;user&quot;:&quot;thomagram&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jiatao Gu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T21:03:22.734Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc6006e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6fbe669287405b2b47cafcf4961cfa75.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhuofeng Wu&quot;,&quot;user&quot;:&quot;cserxy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhuofeng Wu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-02-26T21:10:06.449Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc6006f&quot;,&quot;name&quot;:&quot;Shuangfei Zhai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc60070&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/52c5eca12499a1aa9bd49c43d4f20685.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Joshua M. Susskind&quot;,&quot;user&quot;:&quot;jsusskind&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Josh Susskind&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:41.773Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eb35d1a1fcad2fdc60071&quot;,&quot;name&quot;:&quot;Navdeep Jaitly&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-05T01:36:39.000Z&quot;,&quot;title&quot;:&quot;PLANNER: Generating Diversified Paragraph via Latent Language Diffusion\n  Model&quot;,&quot;summary&quot;:&quot;Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n\&quot;decoding\&quot; module with a \&quot;planning\&quot; module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T04:17:34.565Z&quot;,&quot;title&quot;:&quot;PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ARsmeoQXcaozotiF2NmjC.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01923&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f36ff&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/69f1e19921ba3ecdcd40bf55a6196aca.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Saurabh Saxena&quot;,&quot;user&quot;:&quot;saurabhsaxena&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Saurabh Saxena&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-07T19:39:18.577Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3700&quot;,&quot;name&quot;:&quot;Charles Herrmann&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3701&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1074ee5ec1b2d5d297406f9d455213c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junhwa Hur&quot;,&quot;user&quot;:&quot;hurjunhwa&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Junhwa Hur&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T17:23:17.244Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3702&quot;,&quot;name&quot;:&quot;Abhishek Kar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3703&quot;,&quot;name&quot;:&quot;Mohammad Norouzi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3704&quot;,&quot;name&quot;:&quot;Deqing Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eacb20ed7d0c8760f3705&quot;,&quot;name&quot;:&quot;David J. Fleet&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-02T21:26:20.000Z&quot;,&quot;title&quot;:&quot;The Surprising Effectiveness of Diffusion Models for Optical Flow and\n  Monocular Depth Estimation&quot;,&quot;summary&quot;:&quot;Denoising diffusion probabilistic models have transformed image generation\nwith their impressive fidelity and diversity. We show that they also excel in\nestimating optical flow and monocular depth, surprisingly, without\ntask-specific architectures and loss functions that are predominant for these\ntasks. Compared to the point estimates of conventional regression-based\nmethods, diffusion models also enable Monte Carlo inference, e.g., capturing\nuncertainty and ambiguity in flow and depth. With self-supervised pre-training,\nthe combined use of synthetic and real data for supervised training, and\ntechnical innovations (infilling and step-unrolled denoising diffusion\ntraining) to handle noisy-incomplete training data, and a simple form of\ncoarse-to-fine refinement, one can train state-of-the-art diffusion models for\ndepth and optical flow estimation. Extensive experiments focus on quantitative\nperformance against benchmarks, ablations, and the model's ability to capture\nuncertainty and multimodality, and impute missing values. Our model, DDVM\n(Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth\nerror of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26\\%\non the KITTI optical flow benchmark, about 25\\% better than the best published\nmethod. For an overview see https://diffusion-vision.github.io.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T03:49:10.961Z&quot;,&quot;title&quot;:&quot;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1qATxed_oaYaBxIuMqiBw.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.02245&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df92&quot;,&quot;name&quot;:&quot;Dingyuan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df93&quot;,&quot;name&quot;:&quot;Dingkang Liang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df94&quot;,&quot;name&quot;:&quot;Hongcheng Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df95&quot;,&quot;name&quot;:&quot;Zhikang Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df96&quot;,&quot;name&quot;:&quot;Xiaoqing Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df97&quot;,&quot;name&quot;:&quot;Zhe Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647eaa0ae4d52fe0e021df98&quot;,&quot;name&quot;:&quot;Xiang Bai&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-04T03:09:21.000Z&quot;,&quot;title&quot;:&quot;SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model&quot;,&quot;summary&quot;:&quot;With the development of large language models, many remarkable linguistic\nsystems like ChatGPT have thrived and achieved astonishing success on many\ntasks, showing the incredible power of foundation models. In the spirit of\nunleashing the capability of foundation models on vision tasks, the Segment\nAnything Model (SAM), a vision foundation model for image segmentation, has\nbeen proposed recently and presents strong zero-shot ability on many downstream\n2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be\nexplored, especially 3D object detection. With this inspiration, we explore\nadapting the zero-shot ability of SAM to 3D object detection in this paper. We\npropose a SAM-powered BEV processing pipeline to detect objects and get\npromising results on the large-scale Waymo open dataset. As an early attempt,\nour method takes a step toward 3D object detection with vision foundation\nmodels and presents the opportunity to unleash their power on 3D vision tasks.\nThe code is released at https://github.com/DYZhang09/SAM3D.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T03:37:46.882Z&quot;,&quot;title&quot;:&quot;SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/haRnjW3WRqiqp8Vg7tHpi.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01879&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ea243770c299e56fcebf7&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/34e8c3b7d8fa6cdfd18717ec4513315e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhiqiu Lin&quot;,&quot;user&quot;:&quot;linzhiqiu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhiqiu Lin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T08:59:19.446Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea243770c299e56fcebf8&quot;,&quot;name&quot;:&quot;Xinyue Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea243770c299e56fcebf9&quot;,&quot;name&quot;:&quot;Deepak Pathak&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea243770c299e56fcebfa&quot;,&quot;name&quot;:&quot;Pengchuan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea243770c299e56fcebfb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/14a723cafc5587043bdfb19304fc202d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Deva Ramanan&quot;,&quot;user&quot;:&quot;devakramanan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Deva Ramanan&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T09:00:23.979Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-02T19:19:43.000Z&quot;,&quot;title&quot;:&quot;VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative\n  Pre-Training Scores&quot;,&quot;summary&quot;:&quot;Vision-language models (VLMs) discriminatively pre-trained with contrastive\nimage-text matching losses such as P(match|text, image)\nhave been criticized for lacking compositional understanding. This means they\nmight output similar scores even if the original caption is rearranged into a\ndifferent semantic statement. To address this, we propose to use the {bf\nV}isual {bf G}enerative {bf P}re-{bf T}raining Score ({bf\nVisualGPTScore}) of P(text|image), a multimodal\ngenerative score that captures the likelihood of a text caption conditioned\non an image using an image-conditioned language model. Contrary to the belief\nthat VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore\ndemonstrates top-tier performance on recently proposed image-text retrieval\nbenchmarks like ARO and Crepe that assess compositional reasoning. Furthermore,\nwe factorize VisualGPTScore into a product of the marginal P(text)\nand the Pointwise Mutual Information (PMI). This helps to (a)\ndiagnose datasets with strong language bias, and (b) debias results on other\nbenchmarks like Winoground using an information-theoretic framework.\nVisualGPTScore provides valuable insights and serves as a strong baseline for\nfuture evaluation of visio-linguistic compositionality.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T03:04:36.674Z&quot;,&quot;title&quot;:&quot;VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QnewpfePUXHwCwm9FP_8g.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01872&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d4&quot;,&quot;name&quot;:&quot;Mengjiao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d5&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/def472d1ab3fbf751225357c0932ae7e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yilun Du&quot;,&quot;user&quot;:&quot;yilundu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yilun Du&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T03:21:53.895Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d6&quot;,&quot;name&quot;:&quot;Bo Dai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d7&quot;,&quot;name&quot;:&quot;Dale Schuurmans&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d8&quot;,&quot;name&quot;:&quot;Joshua B. Tenenbaum&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea166cfca67bc50ff87d9&quot;,&quot;name&quot;:&quot;Pieter Abbeel&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-02T19:00:17.000Z&quot;,&quot;title&quot;:&quot;Probabilistic Adaptation of Text-to-Video Models&quot;,&quot;summary&quot;:&quot;Large text-to-video models trained on internet-scale data have demonstrated\nexceptional capabilities in generating high-fidelity videos from arbitrary\ntextual descriptions. However, adapting these models to tasks with limited\ndomain-specific data, such as animation or robotics videos, poses a significant\ncomputational challenge, since finetuning a pretrained large model can be\nprohibitively expensive. Inspired by how a small modifiable component (e.g.,\nprompts, prefix-tuning) can adapt a large language model to perform new tasks\nwithout requiring access to the model weights, we investigate how to adapt a\nlarge pretrained text-to-video model to a variety of downstream domains and\ntasks without finetuning. In answering this question, we propose Video Adapter,\nwhich leverages the score function of a large pretrained video diffusion model\nas a probabilistic prior to guide the generation of a task-specific small video\nmodel. Our experiments show that Video Adapter is capable of incorporating the\nbroad knowledge and preserving the high fidelity of a large pretrained video\nmodel in a task-specific small video model that is able to generate\nhigh-quality yet specialized videos on a variety of tasks such as animation,\negocentric modeling, and modeling of simulated and real-world robotics data.\nMore videos can be found on the website https://video-adapter.github.io/.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T03:00:55.455Z&quot;,&quot;title&quot;:&quot;Probabilistic Adaptation of Text-to-Video Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ucJ_eLQKVBwfQryyMthlr.qt&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.01754&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;647ea05c11084fb5831e6091&quot;,&quot;name&quot;:&quot;Aaron Chan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6092&quot;,&quot;name&quot;:&quot;Anant Kharkar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6093&quot;,&quot;name&quot;:&quot;Roshanak Zilouchian Moghaddam&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6094&quot;,&quot;name&quot;:&quot;Yevhen Mohylevskyy&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6095&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/bd793f0f456cbb1fca47ce9aca9c2acc.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alec Helyar&quot;,&quot;user&quot;:&quot;alech97&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alec Helyar&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-06T09:53:17.489Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6096&quot;,&quot;name&quot;:&quot;Eslam Kamal&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6097&quot;,&quot;name&quot;:&quot;Mohamed Elkamhawy&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;647ea05c11084fb5831e6098&quot;,&quot;name&quot;:&quot;Neel Sundaresan&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-05-23T01:21:55.000Z&quot;,&quot;title&quot;:&quot;Transformer-based Vulnerability Detection in Code at EditTime:\n  Zero-shot, Few-shot, or Fine-tuning?&quot;,&quot;summary&quot;:&quot;Software vulnerabilities bear enterprises significant costs. Despite\nextensive efforts in research and development of software vulnerability\ndetection methods, uncaught vulnerabilities continue to put software owners and\nusers at risk. Many current vulnerability detection methods require that code\nsnippets can compile and build before attempting detection. This,\nunfortunately, introduces a long latency between the time a vulnerability is\ninjected to the time it is removed, which can substantially increases the cost\nof fixing a vulnerability. We recognize that the current advances in machine\nlearning can be used to detect vulnerable code patterns on syntactically\nincomplete code snippets as the developer is writing the code at EditTime. In\nthis paper we present a practical system that leverages deep learning on a\nlarge-scale data set of vulnerable code patterns to learn complex\nmanifestations of more than 250 vulnerability types and detect vulnerable code\npatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuning\napproaches on state of the art pre-trained Large Language Models (LLMs). We\nshow that in comparison with state of the art vulnerability detection models\nour approach improves the state of the art by 10%. We also evaluate our\napproach to detect vulnerability in auto-generated code by code LLMs.\nEvaluation on a benchmark of high-risk code scenarios shows a reduction of up\nto 90% vulnerability reduction.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-06T02:56:29.244Z&quot;,&quot;title&quot;:&quot;Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/n4rg3EvCt4waxDbvXzY_1.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false}],&quot;lastDate&quot;:&quot;2024-03-19T05:05:26.926Z&quot;,&quot;nextDate&quot;:&quot;2023-06-07&quot;,&quot;prevDate&quot;:&quot;2023-06-05&quot;,&quot;publisher&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;}}" data-target="DailyPapers"><section class="container relative mb-20 mt-8 md:mt-14"><div class="mb-12 grid grid-cols-2 items-start md:mb-16 md:grid-cols-3"><div class="md:pl-4"><h1 class="text-2xl font-bold md:text-3xl"><a href="/papers" class="hover:text-gray-600 dark:hover:text-gray-300">Daily Papers</a></h1> <div class="flex flex-wrap items-center gap-2 text-gray-500"><h2 class="flex items-center gap-2.5 text-xl">by <a href="/akhaliq" class="flex items-center gap-2"><img alt="" class="h-4 w-4 rounded-full ring-2 ring-white dark:ring-gray-900" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face"><span class="underline">AK</span></a></h2></div></div> <div class="order-last col-span-2 mt-6 md:order-none md:col-span-1 md:mt-0"><button class="mx-auto flex w-full translate-y-1 items-center justify-center rounded-full border py-1 text-gray-400 shadow-sm hover:shadow-inner md:w-80" title="Search papers"><svg class="mr-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				Search by arxiv id or title</button></div> <div class="flex items-stretch justify-end"><a href="/papers?date=2023-06-05" class="group my-0.5 -mr-2 flex w-8 items-center justify-center rounded-l-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 -translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a> <time class="relative flex flex-col items-stretch" datetime="2023-06-06T00:00:00.000Z"><span class="rounded-t-lg bg-gray-500 px-3.5 py-0.5 text-center text-xs font-bold uppercase leading-none text-white dark:bg-gray-700">Jun</span> <span class="rounded-b-lg bg-gray-100 px-3.5 py-1 text-center text-xl font-semibold text-gray-800 dark:bg-gradient-to-br dark:from-gray-800 dark:to-gray-950">5</span></time> <a href="/papers?date=2023-06-07" class="group my-0.5 -ml-2 flex w-8 items-center justify-center rounded-r-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 rotate-180 translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a></div></div> <div class="relative grid grid-cols-1 gap-14 lg:grid-cols-2"><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02707" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YbzxRvprz0WcAwFSqvUiy.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02707" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">43</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02707" class="cursor-pointer">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02707" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hamid Palangi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ganesh Jawahar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sahajgg" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e0ed6aa70e706865a6755c5fd10d5766.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ari9dam" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b5429d85c030a9f08e1d2bb37ce9974d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="subho" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/eefec358f0d36b9c18d548f4cd2d8806.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> <a href="/papers/2306.02707#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 18</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02858" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KCCf3luovZZdHTgICVAvk.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02858" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">12</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02858" class="cursor-pointer">Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02858" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hang Zhang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="LidongBing" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lixin4ever" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/aec44edd5470dd6e767e0a25efd6fb5d.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> <a href="/papers/2306.02858#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 3</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02254" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nGxYptQ7gym1rK_mhNgi8.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02254" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">8</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02254" class="cursor-pointer">A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02254" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="kabbi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662805242568-61e78aa282b19b93e1a53cbe.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Moo" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ecb4c6db9092c0ba84695cf0a33e5a91.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="bzantium" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623809612769-60436d159e905013ae8715d7.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jason9693" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fa11101a13e063b8b2b5d34/p2Sm4b64hn_xXg1GtrkIA.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hyunwoongko" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634604273263-5fd888cf61e46993190ce543.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> <a href="/papers/2306.02254#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02561" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0xiBm_RWrnJMJ-ifsIQOX.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02561" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02561" class="cursor-pointer">LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02561" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xiang Ren" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yuchenlin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="DongfuTingle" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/FOdpXWhe4vZbLHM3TQnQU.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> <a href="/papers/2306.02561#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y0LZuefVWUGQpb3F2CRN8.qt" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.03038" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.03038" class="cursor-pointer">HeadSculpt: Crafting 3D Head Avatars with Text</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.03038" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kai Han" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xiao Han" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="JiankangDeng" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657566065867-noauth.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xiatian-Zhu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/dc23e15acf7ee7f3b9f1a68c2716a66d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yukangcao" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.03082" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/F3oBas2kcucW5R55In6iE.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.03082" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.03082" class="cursor-pointer">InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.03082" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Heng Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jiuhai Chen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="zhoutianyi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tomgoldstein" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/84dfdca8e1cd6fbf50d6fb2a6f1b488d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Lichang-Chen" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62323bb408bcea92917e42ee/iD3PJfKNp_n-PRMvL4MUO.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02982" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UIMTZmYEAm6zLO8ozXQm2.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02982" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02982" class="cursor-pointer">PolyVoice: Language Models for Speech to Speech Translation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02982" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kexin Wang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yunlong Zhao" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chen Xu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zhiying Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="QQD" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/60d92bef43a7d1342f12ef5ac984f025.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 17 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.01741" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/irHmtQiRTIOrO_7DjQi3K.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01741" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01741" class="cursor-pointer">GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01741" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Katsushi Ikeuchi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jun Takamatsu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kazuhiro Sasabuchi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Atsushi Kanehira" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="NaokiWake" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1ff8a1c9d4c6bde1893e4f80c5fd9fdb.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.01841" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/n64Q0PlXmOD7AHgH44mYg.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01841" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01841" class="cursor-pointer">Binary and Ternary Natural Language Generation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01841" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Raghuraman Krishnamoorthi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yangyang Shi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Aasish Pappu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Barlas Oguz" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zechun" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/aaa2d0eadbcf96c2eb9059e3d73c2760.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.03083" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/14fFUDqr_XlwClJrNdHdJ.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.03083" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.03083" class="cursor-pointer">MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.03083" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yin Zhou" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ben Sapp" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Cheolho Park" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andre Cornman" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chiyu Max Jiang" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.03024" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yrLLbGoIcWSr0JhB1FViK.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.03024" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.03024" class="cursor-pointer">PokemonChat: Auditing ChatGPT for Pokémon Universe Knowledge</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.03024" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jiaang Li" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Laura Cabello" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="kiddothe2b" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646037955111-noauth.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> <a href="/papers/2306.03024#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.03092" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6gXehQPJtd247aiKdLBvW.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.03092" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.03092" class="cursor-pointer">Neuralangelo: High-Fidelity Neural Surface Reconstruction</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.03092" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Alex Evans" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="chenhsuanlin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/4bb215bd711452ee6d28894706773009.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mathiasunberath" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/9a028ef6cac40956f357b4a0481fc1d7.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Tom94" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qXrAoX_wCrtIUfSCfwgSq.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mli0603" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/37ececdbede0be5770719884be4095d8.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> <a href="/papers/2306.03092#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02531" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ARsmeoQXcaozotiF2NmjC.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02531" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02531" class="cursor-pointer">PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02531" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shuangfei Zhai" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jsusskind" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/52c5eca12499a1aa9bd49c43d4f20685.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="cserxy" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6fbe669287405b2b47cafcf4961cfa75.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="thomagram" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634002684894-noauth.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="YizheZ" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/c320b53a31ffba41bd9ad71da99ebc83.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.01923" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1qATxed_oaYaBxIuMqiBw.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01923" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01923" class="cursor-pointer">The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01923" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Mohammad Norouzi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Abhishek Kar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Charles Herrmann" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hurjunhwa" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1074ee5ec1b2d5d297406f9d455213c4.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="saurabhsaxena" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/69f1e19921ba3ecdcd40bf55a6196aca.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.02245" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/haRnjW3WRqiqp8Vg7tHpi.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.02245" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.02245" class="cursor-pointer">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.02245" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xiaoqing Ye" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zhikang Zou" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hongcheng Yang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Dingkang Liang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Dingyuan Zhang" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.01879" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QnewpfePUXHwCwm9FP_8g.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01879" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01879" class="cursor-pointer">VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01879" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Pengchuan Zhang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Deepak Pathak" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xinyue Chen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="devakramanan" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/14a723cafc5587043bdfb19304fc202d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="linzhiqiu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/34e8c3b7d8fa6cdfd18717ec4513315e.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ucJ_eLQKVBwfQryyMthlr.qt" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01872" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01872" class="cursor-pointer">Probabilistic Adaptation of Text-to-Video Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01872" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joshua B. Tenenbaum" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Dale Schuurmans" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Bo Dai" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Mengjiao Yang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yilundu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/def472d1ab3fbf751225357c0932ae7e.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.01754" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/n4rg3EvCt4waxDbvXzY_1.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.01754" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.01754" class="cursor-pointer">Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.01754" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yevhen Mohylevskyy" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Roshanak Zilouchian Moghaddam" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Anant Kharkar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Aaron Chan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="alech97" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/bd793f0f456cbb1fca47ce9aca9c2acc.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> <a href="/papers/2306.01754#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div> <div class="col-span-1 flex lg:col-span-2"><a class="btn gap-2" href="/papers?date=2023-06-05"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z" fill="currentColor"></path></svg>Previous</a> <a class="btn ml-auto gap-2" href="/papers?date=2023-06-07">Next<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z" fill="currentColor"></path></svg></a></div></div></section> </div></main>
	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="font-semibold text-black md:hidden">Company</div>
		<div class="order-last pt-6 text-gray-400 md:order-none md:pt-0" href="Terms">© Hugging Face</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="group order-first flex-none pb-6 md:order-none md:pb-0"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="pt-6 font-semibold text-black md:hidden md:pt-0">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("/front/build/kube-351e67d/index.js");
			window.moonSha = "kube-351e67d/";
			window.hubConfig = JSON.parse(`{"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https://huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetsServerPublicUrl":"https://datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)"}`);
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>

		<!-- Google analytics v4 -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL";
				script.async = true;
				document.head.appendChild(script);

				window.dataLayer = window.dataLayer || [];
				function gtag() {
					if (window.dataLayer !== undefined) {
						window.dataLayer.push(arguments);
					}
				}
				gtag("js", new Date());
				gtag("config", "G-8Q63TH4CSL", { page_path: "/papers" });
				/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages
				gtag("consent", "default", { ad_storage: "denied", analytics_storage: "denied" });
				/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent
				/// TODO: ask the user for their consent and update this with gtag('consent', 'update')
			}
		</script>
	

<iframe name="__privateStripeMetricsController1010" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="https://js.stripe.com/v3/m-outer-3437aaddcdf6922d623e172c2d6f9278.html#url=https%3A%2F%2Fhuggingface.co%2Fpapers%3Fdate%3D2023-06-06&amp;title=Daily%20Papers%20-%20Hugging%20Face&amp;referrer=&amp;muid=NA&amp;sid=NA&amp;version=6&amp;preview=false" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe></body></html>