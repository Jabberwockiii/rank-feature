<!DOCTYPE html><html class=""><head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<meta name="description" content="Your daily dose of AI research from AK">
		<meta property="fb:app_id" content="1321688464574422">
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@huggingface">
		<meta property="og:title" content="Daily Papers - Hugging Face">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://huggingface.co/papers">
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/papers.png">

		<link rel="stylesheet" href="/front/build/kube-351e67d/style.css">

		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&amp;display=swap" rel="stylesheet">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		  

		<title>Daily Papers - Hugging Face</title>

		<script defer="" data-domain="huggingface.co" src="/js/script.js"></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer=""></script>
	<script src="https://js.stripe.com/v3/" async=""></script><script src="https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapersBannerSubscribe-98f5dbb5.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconCheckmarkFilled-0fb1cbef.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapers-ff82993e.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/Contributors-aac8a263.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/autoplay-4f99a53d.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconMessage-6ab20750.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconArrowLeft-a638f296.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/ModalBody-205aeb00.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/urlWatcher-b1dcfbe0.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-79e4fb58.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconSpinner-f6a85825.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UpvoteControl-821d7cbb.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconUpvoteFilled-f11951bc.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UsersListModal-9fa7b704.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/FollowButton-18e671ce.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-997dbc18.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconBellWatching-37394c1a.js"><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"></head>
	<body class="flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black DailyPapersPage">
		<div class="flex min-h-screen flex-col">
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false}" data-target="MainHeader"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg"> <span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a> <div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..." spellcheck="false" type="text"> <svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> </div> <div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg> </button> </div></div> <nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 xl:space-x-2"><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Models</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> Datasets</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg> Spaces</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg> Posts</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg> Docs</a></li> <li class="max-2xl:hidden"><div class="relative "><button class="px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center " type="button"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z" fill="currentColor"></path></svg> Solutions </button> </div></li> <li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/pricing">Pricing</a></li> <li><div class="relative group"><button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button"><svg class="mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>  </button> </div></li> <li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li> <li><a class="block cursor-pointer px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/login">Log In</a></li> <li><a class="rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up</a></li></ul></nav></div></header></div>
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="GoogleAnalyticsTracker"></div>
	
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="SSOBanner"></div>
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-props="{&quot;isLoggedIn&quot;:false}" data-target="DailyPapersBannerSubscribe"><div class="-mt-px flex h-9 w-full justify-center text-gray-600"><svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z"></path></svg> <div class="flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1"><div class="rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600">new</div> <p class="hidden sm:inline">Get trending papers in your email inbox once a day!</p> <p class="inline sm:hidden">Get trending papers in your email inbox!</p> <a href="/login?next=%2Fpapers" class="btn !px-2 text-sm leading-none">Subscribe</a></div> <svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z"></path></svg></div></div>
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;date&quot;:&quot;2023-06-14T00:00:00.000Z&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07954&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6489262f50aa32474db581de&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664764278226-62a54d0410334c1d024e2f59.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuai Yang&quot;,&quot;user&quot;:&quot;PKUWilliamYang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shuai Yang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:12:52.839Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489262f50aa32474db581df&quot;,&quot;name&quot;:&quot;Yifan Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489262f50aa32474db581e0&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ziwei Liu&quot;,&quot;user&quot;:&quot;liuziwei7&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ziwei Liu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:14:22.224Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489262f50aa32474db581e1&quot;,&quot;name&quot;:&quot;Chen Change Loy&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:52:23.000Z&quot;,&quot;title&quot;:&quot;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&quot;,&quot;summary&quot;:&quot;Large text-to-image diffusion models have exhibited impressive proficiency in\ngenerating high-quality images. However, when applying these models to video\ndomain, ensuring temporal consistency across video frames remains a formidable\nchallenge. This paper proposes a novel zero-shot text-guided video-to-video\ntranslation framework to adapt image models to videos. The framework includes\ntwo parts: key frame translation and full video translation. The first part\nuses an adapted diffusion model to generate key frames, with hierarchical\ncross-frame constraints applied to enforce coherence in shapes, textures and\ncolors. The second part propagates the key frames to other frames with\ntemporal-aware patch matching and frame blending. Our framework achieves global\nstyle and local texture temporal consistency at a low cost (without re-training\nor optimization). The adaptation is compatible with existing image diffusion\ntechniques, allowing our framework to take advantage of them, such as\ncustomizing a specific subject with LoRA, and introducing extra spatial\nguidance with ControlNet. Extensive experimental results demonstrate the\neffectiveness of our proposed framework over existing methods in rendering\nhigh-quality and temporally-coherent videos.&quot;,&quot;upvotes&quot;:111},&quot;publishedAt&quot;:&quot;2023-06-14T02:30:10.571Z&quot;,&quot;title&quot;:&quot;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3Lwy5zqpR76RhFY4Zt1ig.mp4&quot;,&quot;numComments&quot;:11,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07967&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648938c3de81ed071f971cf2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6b7bde1d453f0328fcd5a86cfd9eb3b2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Arnav Chavan&quot;,&quot;user&quot;:&quot;Arnav0400&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Arnav Chavan&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:29:11.295Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648938c3de81ed071f971cf3&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/56efca912a66ef9d4f2f3679aac31bae.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhuang Liu&quot;,&quot;user&quot;:&quot;liuzhuang13&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhuang Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-02-21T19:38:50.005Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648938c3de81ed071f971cf4&quot;,&quot;name&quot;:&quot;Deepak Gupta&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648938c3de81ed071f971cf5&quot;,&quot;name&quot;:&quot;Eric Xing&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648938c3de81ed071f971cf6&quot;,&quot;name&quot;:&quot;Zhiqiang Shen&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:59:32.000Z&quot;,&quot;title&quot;:&quot;One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning&quot;,&quot;summary&quot;:&quot;We present Generalized LoRA (GLoRA), an advanced approach for universal\nparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),\nGLoRA employs a generalized prompt module to optimize pre-trained model weights\nand adjust intermediate activations, providing more flexibility and capability\nacross diverse tasks and datasets. Moreover, GLoRA facilitates efficient\nparameter adaptation by employing a scalable, modular, layer-wise structure\nsearch that learns individual adapter of each layer. Originating from a unified\nmathematical formulation, GLoRA exhibits strong transfer learning, few-shot\nlearning and domain generalization abilities, as it adjusts to new tasks\nthrough additional dimensions on weights and activations. Comprehensive\nexperiments demonstrate that GLoRA outperforms all previous methods in natural,\nspecialized, and structured benchmarks, achieving superior accuracy with fewer\nparameters and computations on various datasets. Furthermore, our structural\nre-parameterization design ensures that GLoRA incurs no extra inference cost,\nrendering it a practical solution for resource-limited applications. Code is\navailable at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.&quot;,&quot;upvotes&quot;:23},&quot;publishedAt&quot;:&quot;2023-06-14T03:49:23.559Z&quot;,&quot;title&quot;:&quot;One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gniDuj07YAzfHiQYrEBej.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07476&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648922da50aa32474db4c063&quot;,&quot;name&quot;:&quot;Zhengyu Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648922da50aa32474db4c064&quot;,&quot;name&quot;:&quot;Haoran Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648922da50aa32474db4c065&quot;,&quot;name&quot;:&quot;Tsukasa Fukusato&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648922da50aa32474db4c066&quot;,&quot;name&quot;:&quot;Kazunori Miyata&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T00:43:47.000Z&quot;,&quot;title&quot;:&quot;AniFaceDrawing: Anime Portrait Exploration during Your Sketching&quot;,&quot;summary&quot;:&quot;In this paper, we focus on how artificial intelligence (AI) can be used to\nassist users in the creation of anime portraits, that is, converting rough\nsketches into anime portraits during their sketching process. The input is a\nsequence of incomplete freehand sketches that are gradually refined stroke by\nstroke, while the output is a sequence of high-quality anime portraits that\ncorrespond to the input sketches as guidance. Although recent GANs can generate\nhigh quality images, it is a challenging problem to maintain the high quality\nof generated images from sketches with a low degree of completion due to\nill-posed problems in conditional image generation. Even with the latest\nsketch-to-image (S2I) technology, it is still difficult to create high-quality\nimages from incomplete rough sketches for anime portraits since anime style\ntend to be more abstract than in realistic style. To address this issue, we\nadopt a latent space exploration of StyleGAN with a two-stage training\nstrategy. We consider the input strokes of a freehand sketch to correspond to\nedge information-related attributes in the latent structural code of StyleGAN,\nand term the matching between strokes and these attributes stroke-level\ndisentanglement. In the first stage, we trained an image encoder with the\npre-trained StyleGAN model as a teacher encoder. In the second stage, we\nsimulated the drawing process of the generated images without any additional\ndata (labels) and trained the sketch encoder for incomplete progressive\nsketches to generate high-quality portrait images with feature alignment to the\ndisentangled representations in the teacher encoder. We verified the proposed\nprogressive S2I system with both qualitative and quantitative evaluations and\nachieved high-quality anime portraits from incomplete progressive sketches. Our\nuser study proved its effectiveness in art creation assistance for the anime\nstyle.&quot;,&quot;upvotes&quot;:16},&quot;publishedAt&quot;:&quot;2023-06-14T02:15:57.839Z&quot;,&quot;title&quot;:&quot;AniFaceDrawing: Anime Portrait Exploration during Your Sketching&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NmTLwrfoEO_pc4zJ0QyLH.mp4&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07906&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648927699ab28735f19c358d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiao Liu&quot;,&quot;user&quot;:&quot;ShawLiu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xiao Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T15:18:39.336Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c358e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/89b215dafa503b51ab212a9b63c82aca.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hanyu Lai&quot;,&quot;user&quot;:&quot;hanyullai&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hanyu Lai&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:29:17.707Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c358f&quot;,&quot;name&quot;:&quot;Hao Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3590&quot;,&quot;name&quot;:&quot;Yifan Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3591&quot;,&quot;name&quot;:&quot;Aohan Zeng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3592&quot;,&quot;name&quot;:&quot;Zhengxiao Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3593&quot;,&quot;name&quot;:&quot;Peng Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3594&quot;,&quot;name&quot;:&quot;Yuxiao Dong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648927699ab28735f19c3595&quot;,&quot;name&quot;:&quot;Jie Tang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T16:57:53.000Z&quot;,&quot;title&quot;:&quot;WebGLM: Towards An Efficient Web-Enhanced Question Answering System with\n  Human Preferences&quot;,&quot;summary&quot;:&quot;We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\nhttps://github.com/THUDM/WebGLM.&quot;,&quot;upvotes&quot;:12},&quot;publishedAt&quot;:&quot;2023-06-14T03:42:47.307Z&quot;,&quot;title&quot;:&quot;WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Z49a41V6vQ-6D30SCTekf.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07536&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64891e04ef2989c5dfe6a276&quot;,&quot;name&quot;:&quot;Kush Bhatia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891e04ef2989c5dfe6a277&quot;,&quot;name&quot;:&quot;Avanika Narayan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891e04ef2989c5dfe6a278&quot;,&quot;name&quot;:&quot;Christopher De Sa&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891e04ef2989c5dfe6a279&quot;,&quot;name&quot;:&quot;Christopher Ré&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T04:37:00.000Z&quot;,&quot;title&quot;:&quot;TART: A plug-and-play Transformer module for task-agnostic reasoning&quot;,&quot;summary&quot;:&quot;Large language models (LLMs) exhibit in-context learning abilities which\nenable the same model to perform several tasks without any task-specific\ntraining. In contrast, traditional adaptation approaches, such as fine-tuning,\nmodify the underlying models for each specific task. In-context learning,\nhowever, consistently underperforms task-specific tuning approaches even when\npresented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM's learned representations to patch this\nperformance gap, our analysis actually reveal that LLM representations contain\nsufficient information to make good predictions. As such, we focus on the LLM's\nreasoning abilities and demonstrate that this performance gap exists due to\ntheir inability to perform simple probabilistic reasoning tasks. This raises an\nintriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose TART which\ngenerically improves an LLM's reasoning abilities using a synthetically trained\nTransformer-based reasoning module. TART trains this reasoning module in a\ntask-agnostic manner using only synthetic logistic regression tasks and\ncomposes it with an arbitrary real-world pre-trained model without any\nadditional training. With a single inference module, TART improves performance\nacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -\n6B), tasks (14 NLP binary classification tasks), and even across different\nmodalities (audio and vision). Additionally, on the RAFT Benchmark, TART\nimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),\nand is within 4% of GPT-3 (175B). Our code and models are available at\nhttps://github.com/HazyResearch/TART .&quot;,&quot;upvotes&quot;:10},&quot;publishedAt&quot;:&quot;2023-06-14T01:55:19.126Z&quot;,&quot;title&quot;:&quot;TART: A plug-and-play Transformer module for task-agnostic reasoning&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Nb5RyRF5-toD8IPURlC0l.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07915&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6489312e9ab28735f19eff7c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/54354c1e5774cadd1d83d42054e9d96b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Michael Tschannen&quot;,&quot;user&quot;:&quot;mitsch&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Michael Tschannen&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:15:11.324Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489312e9ab28735f19eff7d&quot;,&quot;name&quot;:&quot;Manoj Kumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489312e9ab28735f19eff7e&quot;,&quot;name&quot;:&quot;Andreas Steiner&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489312e9ab28735f19eff7f&quot;,&quot;name&quot;:&quot;Xiaohua Zhai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489312e9ab28735f19eff80&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/a362a236c0654b7605dcb7673e309335.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Neil Houlsby&quot;,&quot;user&quot;:&quot;neilhoulsby&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Neil Houlsby&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-03T15:23:05.878Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489312e9ab28735f19eff81&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d334ff65714b4585f2de4/gxBynq5KyoUP0VlAQD3-w.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lucas Beyer&quot;,&quot;user&quot;:&quot;giffmana&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Lucas Beyer&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-01-09T14:26:23.485Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:18:01.000Z&quot;,&quot;title&quot;:&quot;Image Captioners Are Scalable Vision Learners Too&quot;,&quot;summary&quot;:&quot;Contrastive pretraining on image-text pairs from the web is one of the most\npopular large-scale pretraining strategies for vision backbones, especially in\nthe context of large multimodal models. At the same time, image captioning on\nthis type of data is commonly considered an inferior pretraining strategy. In\nthis paper, we perform a fair comparison of these two pretraining strategies,\ncarefully matching training data, compute, and model capacity. Using a standard\nencoder-decoder transformer, we find that captioning alone is surprisingly\neffective: on classification tasks, captioning produces vision encoders\ncompetitive with contrastively pretrained encoders, while surpassing them on\nvision &amp; language tasks. We further analyze the effect of the model\narchitecture and scale, as well as the pretraining data on the representation\nquality, and find that captioning exhibits the same or better scaling behavior\nalong these axes. Overall our results show that plain image captioning is a\nmore powerful pretraining strategy than was previously believed.&quot;,&quot;upvotes&quot;:9},&quot;publishedAt&quot;:&quot;2023-06-14T03:17:03.316Z&quot;,&quot;title&quot;:&quot;Image Captioners Are Scalable Vision Learners Too&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wxIpCvQZfSQMLlTyNJWNa.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07349&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f15&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/12faffe4a2e4bf411220221375763d76.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jonathan Lorraine&quot;,&quot;user&quot;:&quot;lorraine2&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jonathan Lorraine&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:41:48.557Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f16&quot;,&quot;name&quot;:&quot;Kevin Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f17&quot;,&quot;name&quot;:&quot;Xiaohui Zeng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f18&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/4bb215bd711452ee6d28894706773009.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen-Hsuan Lin&quot;,&quot;user&quot;:&quot;chenhsuanlin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chen-Hsuan Lin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:43:23.447Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f19&quot;,&quot;name&quot;:&quot;Towaki Takikawa&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f1a&quot;,&quot;name&quot;:&quot;Nicholas Sharp&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f1b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/23300548e5c50f44e95d63568221f47b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tsung-Yi Lin&quot;,&quot;user&quot;:&quot;tsungyi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tsung-Yi Lin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:54:06.020Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f1c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e272e58ad996733d7098e50248e5b57e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ming-Yu Liu&quot;,&quot;user&quot;:&quot;mingyuliutw&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ming-Yu Liu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T08:44:08.862Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f1d&quot;,&quot;name&quot;:&quot;Sanja Fidler&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648940b751b69a8c82ec1f1e&quot;,&quot;name&quot;:&quot;James Lucas&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-06T17:59:10.000Z&quot;,&quot;title&quot;:&quot;ATT3D: Amortized Text-to-3D Object Synthesis&quot;,&quot;summary&quot;:&quot;Text-to-3D modelling has seen exciting progress by combining generative\ntext-to-image models with image-to-3D methods like Neural Radiance Fields.\nDreamFusion recently achieved high-quality results but requires a lengthy,\nper-prompt optimization to create 3D objects. To address this, we amortize\noptimization over text prompts by training on many prompts simultaneously with\na unified model, instead of separately. With this, we share computation across\na prompt set, training in less time than per-prompt optimization. Our framework\n- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to\ngeneralize to unseen setups and smooth interpolations between text for novel\nassets and simple animations.&quot;,&quot;upvotes&quot;:8},&quot;publishedAt&quot;:&quot;2023-06-14T04:23:25.442Z&quot;,&quot;title&quot;:&quot;ATT3D: Amortized Text-to-3D Object Synthesis&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O9ZWUnu0C-8by-FbvjuVC.mp4&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07580&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64891faa7987267e4fc3f608&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6c936818a3814c1c32a8eaae8b8cb6d6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yujin tang&quot;,&quot;user&quot;:&quot;tyj2022&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yujin Tang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:14:58.914Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891faa7987267e4fc3f609&quot;,&quot;name&quot;:&quot;Wenhao Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891faa7987267e4fc3f60a&quot;,&quot;name&quot;:&quot;Jie Tan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891faa7987267e4fc3f60b&quot;,&quot;name&quot;:&quot;Heiga Zen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891faa7987267e4fc3f60c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a3b828225b2121f6b3beb3/KArimb-AY5REzu67VRIm6.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aleksandra Faust&quot;,&quot;user&quot;:&quot;sandraorion&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Aleksandra Faust&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-04T07:24:07.283Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891faa7987267e4fc3f60d&quot;,&quot;name&quot;:&quot;Tatsuya Harada&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T07:09:11.000Z&quot;,&quot;title&quot;:&quot;SayTap: Language to Quadrupedal Locomotion&quot;,&quot;summary&quot;:&quot;Large language models (LLMs) have demonstrated the potential to perform\nhigh-level planning. Yet, it remains a challenge for LLMs to comprehend\nlow-level commands, such as joint angle targets or motor torques. This paper\nproposes an approach to use foot contact patterns as an interface that bridges\nhuman commands in natural language and a locomotion controller that outputs\nthese low-level commands. This results in an interactive system for quadrupedal\nrobots that allows the users to craft diverse locomotion behaviors flexibly. We\ncontribute an LLM prompt design, a reward function, and a method to expose the\ncontroller to the feasible distribution of contact patterns. The results are a\ncontroller capable of achieving diverse locomotion patterns that can be\ntransferred to real robot hardware. Compared with other design choices, the\nproposed approach enjoys more than 50% success rate in predicting the correct\ncontact patterns and can solve 10 more tasks out of a total of 30 tasks. Our\nproject site is: https://saytap.github.io.&quot;,&quot;upvotes&quot;:7},&quot;publishedAt&quot;:&quot;2023-06-14T02:02:27.820Z&quot;,&quot;title&quot;:&quot;SayTap: Language to Quadrupedal Locomotion&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8dJP3w7fje_vNhP_XBOh8.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07970&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cac&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/f7a0fc6816535945e11bac1212dd7b57.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haotong Lin&quot;,&quot;user&quot;:&quot;haotongl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Haotong Lin&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:15:16.943Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cad&quot;,&quot;name&quot;:&quot;Qianqian Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cae&quot;,&quot;name&quot;:&quot;Ruojin Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41caf&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sida Peng&quot;,&quot;user&quot;:&quot;pengsida&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sida Peng&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:20:00.215Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cb0&quot;,&quot;name&quot;:&quot;Hadar Averbuch-Elor&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cb1&quot;,&quot;name&quot;:&quot;Xiaowei Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893e904b47b34bd3d41cb2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e2ca585398a3212b2956a14804d0ba67.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Noah Snavely&quot;,&quot;user&quot;:&quot;jimantha&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Noah Snavely&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:19:47.227Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:59:58.000Z&quot;,&quot;title&quot;:&quot;Neural Scene Chronology&quot;,&quot;summary&quot;:&quot;In this work, we aim to reconstruct a time-varying 3D model, capable of\nrendering photo-realistic renderings with independent control of viewpoint,\nillumination, and time, from Internet photos of large-scale landmarks. The core\nchallenges are twofold. First, different types of temporal changes, such as\nillumination and changes to the underlying scene itself (such as replacing one\ngraffiti artwork with another) are entangled together in the imagery. Second,\nscene-level temporal changes are often discrete and sporadic over time, rather\nthan continuous. To tackle these problems, we propose a new scene\nrepresentation equipped with a novel temporal step function encoding method\nthat can model discrete scene-level content changes as piece-wise constant\nfunctions over time. Specifically, we represent the scene as a space-time\nradiance field with a per-image illumination embedding, where\ntemporally-varying scene changes are encoded using a set of learned step\nfunctions. To facilitate our task of chronology reconstruction from Internet\nimagery, we also collect a new dataset of four scenes that exhibit various\nchanges over time. We demonstrate that our method exhibits state-of-the-art\nview synthesis results on this dataset, while achieving independent control of\nviewpoint, time, and illumination.&quot;,&quot;upvotes&quot;:6},&quot;publishedAt&quot;:&quot;2023-06-14T04:14:13.259Z&quot;,&quot;title&quot;:&quot;Neural Scene Chronology&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Z_ZIDg1PC9Bm1nQlG1VQY.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07968&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64891cbe563d6345d8ea4b66&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/a70517208abecc1d6f245ebc63cb4ed6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gyungin Shin&quot;,&quot;user&quot;:&quot;noelshin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Gyungin Shin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T11:47:17.339Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891cbe563d6345d8ea4b67&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5398658c7d0cf556531a625a4ca5d18a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xie&quot;,&quot;user&quot;:&quot;Weidi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Weidi Xie&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T11:47:52.962Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64891cbe563d6345d8ea4b68&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660153837083-noauth.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Samuel Albanie&quot;,&quot;user&quot;:&quot;albanie&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Samuel Albanie&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T11:51:18.735Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:59:57.000Z&quot;,&quot;title&quot;:&quot;arXiVeri: Automatic table verification with GPT&quot;,&quot;summary&quot;:&quot;Without accurate transcription of numerical data in scientific documents, a\nscientist cannot draw accurate conclusions. Unfortunately, the process of\ncopying numerical data from one paper to another is prone to human error. In\nthis paper, we propose to meet this challenge through the novel task of\nautomatic table verification (AutoTV), in which the objective is to verify the\naccuracy of numerical data in tables by cross-referencing cited sources. To\nsupport this task, we propose a new benchmark, arXiVeri, which comprises\ntabular data drawn from open-access academic papers on arXiv. We introduce\nmetrics to evaluate the performance of a table verifier in two key areas: (i)\ntable matching, which aims to identify the source table in a cited document\nthat corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row\nand column indices accurately. By leveraging the flexible capabilities of\nmodern large language models (LLMs), we propose simple baselines for table\nverification. Our findings highlight the complexity of this task, even for\nstate-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made\npublicly available.&quot;,&quot;upvotes&quot;:6},&quot;publishedAt&quot;:&quot;2023-06-14T01:49:52.581Z&quot;,&quot;title&quot;:&quot;arXiVeri: Automatic table verification with GPT&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/48UOputM6b4eFmF2d_ehs.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07473&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648924d39ab28735f19b9bee&quot;,&quot;name&quot;:&quot;Pedro O. Pinheiro&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bef&quot;,&quot;name&quot;:&quot;Joshua Rackers&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf0&quot;,&quot;name&quot;:&quot;Joseph Kleinhenz&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf1&quot;,&quot;name&quot;:&quot;Michael Maser&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf2&quot;,&quot;name&quot;:&quot;Omar Mahmood&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf3&quot;,&quot;name&quot;:&quot;Andrew Martin Watkins&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf4&quot;,&quot;name&quot;:&quot;Stephen Ra&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf5&quot;,&quot;name&quot;:&quot;Vishnu Sresht&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648924d39ab28735f19b9bf6&quot;,&quot;name&quot;:&quot;Saeed Saremi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T00:38:51.000Z&quot;,&quot;title&quot;:&quot;3D molecule generation by denoising voxel grids&quot;,&quot;summary&quot;:&quot;We propose a new score-based approach to generate 3D molecules represented as\natomic densities on regular grids. First, we train a denoising neural network\nthat learns to map from a smooth distribution of noisy molecules to the\ndistribution of real molecules. Then, we follow the neural empirical Bayes\nframework [Saremi and Hyvarinen, 2019] and generate molecules in two steps: (i)\nsample noisy density grids from a smooth distribution via underdamped Langevin\nMarkov chain Monte Carlo, and (ii) recover the ``clean'' molecule by denoising\nthe noisy grid with a single step. Our method, VoxMol, generates molecules in a\nfundamentally different way than the current state of the art (i.e., diffusion\nmodels applied to atom point clouds). It differs in terms of the data\nrepresentation, the noise model, the network architecture and the generative\nmodeling algorithm. VoxMol achieves comparable results to state of the art on\nunconditional 3D molecule generation while being simpler to train and faster to\ngenerate molecules.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-14T02:24:21.671Z&quot;,&quot;title&quot;:&quot;3D molecule generation by denoising voxel grids&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yagEXIp3waPt5fg9vtDBN.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07944&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca1f&quot;,&quot;name&quot;:&quot;Mingqiu Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca20&quot;,&quot;name&quot;:&quot;Izhak Shafran&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca21&quot;,&quot;name&quot;:&quot;Hagen Soltau&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca22&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e637e6caab6df56da6005f73c613a3d0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei Han&quot;,&quot;user&quot;:&quot;weihan3&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wei Han&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T20:37:16.101Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca23&quot;,&quot;name&quot;:&quot;Yuan Cao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca24&quot;,&quot;name&quot;:&quot;Dian Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648921ae47b1bd57a66bca25&quot;,&quot;name&quot;:&quot;Laurent El Shafey&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T22:33:22.000Z&quot;,&quot;title&quot;:&quot;Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for\n  Speech Understanding&quot;,&quot;summary&quot;:&quot;Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-14T02:10:54.889Z&quot;,&quot;title&quot;:&quot;Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/axuEKIPeXCUREl9REGjwe.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07969&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648939664cf3474dd1041d51&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/abebd42399decafbccc8579faa34e7d3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sagar Vaze&quot;,&quot;user&quot;:&quot;sgvaze&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sagar Vaze&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:12:04.852Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648939664cf3474dd1041d52&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/9d7cb38db7831a05e35c157a71f5be4a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nicolas Carion&quot;,&quot;user&quot;:&quot;alcinos&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Nicolas Carion&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:12:21.063Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648939664cf3474dd1041d53&quot;,&quot;name&quot;:&quot;Ishan Misra&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T17:59:58.000Z&quot;,&quot;title&quot;:&quot;GeneCIS: A Benchmark for General Conditional Image Similarity&quot;,&quot;summary&quot;:&quot;We argue that there are many notions of 'similarity' and that models, like\nhumans, should be able to adapt to these dynamically. This contrasts with most\nrepresentation learning methods, supervised or self-supervised, which learn a\nfixed embedding function and hence implicitly assume a single notion of\nsimilarity. For instance, models trained on ImageNet are biased towards object\ncategories, while a user might prefer the model to focus on colors, textures or\nspecific elements in the scene. In this paper, we propose the GeneCIS\n('genesis') benchmark, which measures models' ability to adapt to a range of\nsimilarity conditions. Extending prior work, our benchmark is designed for\nzero-shot evaluation only, and hence considers an open-set of similarity\nconditions. We find that baselines from powerful CLIP models struggle on\nGeneCIS and that performance on the benchmark is only weakly correlated with\nImageNet accuracy, suggesting that simply scaling existing methods is not\nfruitful. We further propose a simple, scalable solution based on automatically\nmining information from existing image-caption datasets. We find our method\noffers a substantial boost over the baselines on GeneCIS, and further improves\nzero-shot performance on related image retrieval benchmarks. In fact, though\nevaluated zero-shot, our model surpasses state-of-the-art supervised models on\nMIT-States. Project page at https://sgvaze.github.io/genecis/.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-14T03:52:07.709Z&quot;,&quot;title&quot;:&quot;GeneCIS: A Benchmark for General Conditional Image Similarity&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qNfHruBJrQd0MgW4nNOoW.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07552&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648941f837666e47d1dfc688&quot;,&quot;name&quot;:&quot;Vincent-Pierre Berges&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc689&quot;,&quot;name&quot;:&quot;Andrew Szot&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc68a&quot;,&quot;name&quot;:&quot;Devendra Singh Chaplot&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc68b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/9353c064ef8ccac84d0397411d38fa90.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aaron Gokaslan&quot;,&quot;user&quot;:&quot;Skylion007&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Aaron Gokaslan&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:13:48.931Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc68c&quot;,&quot;name&quot;:&quot;Roozbeh Mottaghi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc68d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e5a7428bb0acb43a240f1c4a8c1a5639.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dhruv Batra&quot;,&quot;user&quot;:&quot;dbatra&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Dhruv Batra&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:14:20.962Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648941f837666e47d1dfc68e&quot;,&quot;name&quot;:&quot;Eric Undersander&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T05:53:23.000Z&quot;,&quot;title&quot;:&quot;Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at\n  100k Steps-Per-Second&quot;,&quot;summary&quot;:&quot;We present Galactic, a large-scale simulation and reinforcement-learning (RL)\nframework for robotic mobile manipulation in indoor environments. Specifically,\na Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion,\nand onboard sensing) is spawned in a home environment and asked to rearrange\nobjects - by navigating to an object, picking it up, navigating to a target\nlocation, and then placing the object at the target location.\n  Galactic is fast. In terms of simulation speed (rendering + physics),\nGalactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which\nis 54x faster than Habitat 2.0 (7699 SPS). More importantly, Galactic was\ndesigned to optimize the entire rendering + physics + RL interplay since any\nbottleneck in the interplay slows down training. In terms of simulation+RL\nspeed (rendering + physics + inference + learning), Galactic achieves over\n108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS).\n  These massive speed-ups not only drastically cut the wall-clock training time\nof existing experiments, but also unlock an unprecedented scale of new\nexperiments. First, Galactic can train a mobile pick skill to >80% accuracy in\nunder 16 minutes, a 100x speedup compared to the over 24 hours it takes to\ntrain the same skill in Habitat 2.0. Second, we use Galactic to perform the\nlargest-scale experiment to date for rearrangement using 5B steps of experience\nin 46 hours, which is equivalent to 20 years of robot experience. This scaling\nresults in a single neural network composed of task-agnostic components\nachieving 85% success in GeometricGoal rearrangement, compared to 0% success\nreported in Habitat 2.0 for the same approach. The code is available at\ngithub.com/facebookresearch/galactic.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-14T04:28:41.605Z&quot;,&quot;title&quot;:&quot;Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GNUnR-WrGvmc7jodcltoQ.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07437&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6489301d50396a20780960d4&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/ac7b858f504648c1f5d1c3e3034c7996.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Timo Bolkart&quot;,&quot;user&quot;:&quot;tbolkart&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Timo Bolkart&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:04:58.495Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489301d50396a20780960d5&quot;,&quot;name&quot;:&quot;Tianye Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6489301d50396a20780960d6&quot;,&quot;name&quot;:&quot;Michael J. Black&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-12T21:45:18.000Z&quot;,&quot;title&quot;:&quot;Instant Multi-View Head Capture through Learnable Registration&quot;,&quot;summary&quot;:&quot;Existing methods for capturing datasets of 3D heads in dense semantic\ncorrespondence are slow, and commonly address the problem in two separate\nsteps; multi-view stereo (MVS) reconstruction followed by non-rigid\nregistration. To simplify this process, we introduce TEMPEH (Towards Estimation\nof 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads\nin dense correspondence from calibrated multi-view images. Registering datasets\nof 3D scans typically requires manual parameter tuning to find the right\nbalance between accurately fitting the scans surfaces and being robust to\nscanning noise and outliers. Instead, we propose to jointly register a 3D head\ndataset while training TEMPEH. Specifically, during training we minimize a\ngeometric loss commonly used for surface registration, effectively leveraging\nTEMPEH as a regularizer. Our multi-view head inference builds on a volumetric\nfeature representation that samples and fuses features from each view using\ncamera calibration information. To account for partial occlusions and a large\ncapture volume that enables head movements, we use view- and surface-aware\nfeature fusion, and a spatial transformer-based head localization module,\nrespectively. We use raw MVS scans as supervision during training, but, once\ntrained, TEMPEH directly predicts 3D heads in dense correspondence without\nrequiring scans. Predicting one head takes about 0.3 seconds with a median\nreconstruction error of 0.26 mm, 64% lower than the current state-of-the-art.\nThis enables the efficient capture of large datasets containing multiple people\nand diverse facial motions. Code, model, and data are publicly available at\nhttps://tempeh.is.tue.mpg.de.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-14T03:12:32.604Z&quot;,&quot;title&quot;:&quot;Instant Multi-View Head Capture through Learnable Registration&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x3hg1ZqfrPEROMq-oagUz.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07941&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648920dd9ab28735f19aafae&quot;,&quot;name&quot;:&quot;Itzik Malkiel&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafaf&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657635637504-62cd8153248f9e6bc20ab250.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Uri Alon&quot;,&quot;user&quot;:&quot;urialon&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Uri Alon&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T11:55:32.702Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafb0&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/232f51fe49c7d8d491655ad2979009b9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yakir yehuda&quot;,&quot;user&quot;:&quot;yakiryehuda&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yakir Yehuda&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T11:55:59.273Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafb1&quot;,&quot;name&quot;:&quot;Shahar Keren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafb2&quot;,&quot;name&quot;:&quot;Oren Barkan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafb3&quot;,&quot;name&quot;:&quot;Royi Ronen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648920dd9ab28735f19aafb4&quot;,&quot;name&quot;:&quot;Noam Koenigstein&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-09T15:47:22.000Z&quot;,&quot;title&quot;:&quot;GPT-Calls: Enhancing Call Segmentation and Tagging by Generating\n  Synthetic Conversations via Large Language Models&quot;,&quot;summary&quot;:&quot;Transcriptions of phone calls are of significant value across diverse fields,\nsuch as sales, customer service, healthcare, and law enforcement. Nevertheless,\nthe analysis of these recorded conversations can be an arduous and\ntime-intensive process, especially when dealing with extended or multifaceted\ndialogues. In this work, we propose a novel method, GPT-distilled Calls\nSegmentation and Tagging (GPT-Calls), for efficient and accurate call\nsegmentation and topic extraction. GPT-Calls is composed of offline and online\nphases. The offline phase is applied once to a given list of topics and\ninvolves generating a distribution of synthetic sentences for each topic using\na GPT model and extracting anchor vectors. The online phase is applied to every\ncall separately and scores the similarity between the transcripted conversation\nand the topic anchors found in the offline phase. Then, time domain analysis is\napplied to the similarity scores to group utterances into segments and tag them\nwith topics. The proposed paradigm provides an accurate and efficient method\nfor call segmentation and topic extraction that does not require labeled data,\nthus making it a versatile approach applicable to various domains. Our\nalgorithm operates in production under Dynamics 365 Sales Conversation\nIntelligence, and our research is based on real sales conversations gathered\nfrom various Dynamics 365 Sales tenants.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-14T02:07:27.691Z&quot;,&quot;title&quot;:&quot;GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VHz88eNikjfauhmB3GDBz.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.07946&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64893195b168777dcf752428&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/dc50dc74ec345a86b8f9a1352bc3f16c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Eltayeb Ahmed&quot;,&quot;user&quot;:&quot;clockwork7&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Eltayeb Ahmed&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T12:10:36.771Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893195b168777dcf752429&quot;,&quot;name&quot;:&quot;Diana Mincu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893195b168777dcf75242a&quot;,&quot;name&quot;:&quot;Lauren Harrell&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893195b168777dcf75242b&quot;,&quot;name&quot;:&quot;Katherine Heller&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64893195b168777dcf75242c&quot;,&quot;name&quot;:&quot;Subhrajit Roy&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-02T14:47:56.000Z&quot;,&quot;title&quot;:&quot;STUDY: Socially Aware Temporally Casual Decoder Recommender Systems&quot;,&quot;summary&quot;:&quot;With the overwhelming amount of data available both on and offline today,\nrecommender systems have become much needed to help users find items tailored\nto their interests. When social network information exists there are methods\nthat utilize this information to make better recommendations, however the\nmethods are often clunky with complex architectures and training procedures.\nFurthermore many of the existing methods utilize graph neural networks which\nare notoriously difficult to train. To address this, we propose Socially-aware\nTemporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint\ninference over groups of users who are adjacent in the social network graph\nusing a single forward pass of a modified transformer decoder network. We test\nour method in a school-based educational content setting, using classroom\nstructure to define social networks. Our method outperforms both social and\nsequential methods while maintaining the design simplicity of a single\nhomogeneous network that models all interactions in the data. We also carry out\nablation studies to understand the drivers of our performance gains and find\nthat our model depends on leveraging a social network structure that\neffectively models the similarities in user behavior.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-14T03:18:46.079Z&quot;,&quot;title&quot;:&quot;STUDY: Socially Aware Temporally Casual Decoder Recommender Systems&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aTqG7NfC1svUnyk3fPdh3.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false}],&quot;lastDate&quot;:&quot;2024-03-19T05:05:26.926Z&quot;,&quot;nextDate&quot;:&quot;2023-06-16&quot;,&quot;prevDate&quot;:&quot;2023-06-13&quot;,&quot;publisher&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;}}" data-target="DailyPapers"><section class="container relative mb-20 mt-8 md:mt-14"><div class="mb-12 grid grid-cols-2 items-start md:mb-16 md:grid-cols-3"><div class="md:pl-4"><h1 class="text-2xl font-bold md:text-3xl"><a href="/papers" class="hover:text-gray-600 dark:hover:text-gray-300">Daily Papers</a></h1> <div class="flex flex-wrap items-center gap-2 text-gray-500"><h2 class="flex items-center gap-2.5 text-xl">by <a href="/akhaliq" class="flex items-center gap-2"><img alt="" class="h-4 w-4 rounded-full ring-2 ring-white dark:ring-gray-900" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face"><span class="underline">AK</span></a></h2></div></div> <div class="order-last col-span-2 mt-6 md:order-none md:col-span-1 md:mt-0"><button class="mx-auto flex w-full translate-y-1 items-center justify-center rounded-full border py-1 text-gray-400 shadow-sm hover:shadow-inner md:w-80" title="Search papers"><svg class="mr-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				Search by arxiv id or title</button></div> <div class="flex items-stretch justify-end"><a href="/papers?date=2023-06-13" class="group my-0.5 -mr-2 flex w-8 items-center justify-center rounded-l-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 -translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a> <time class="relative flex flex-col items-stretch" datetime="2023-06-14T00:00:00.000Z"><span class="rounded-t-lg bg-gray-500 px-3.5 py-0.5 text-center text-xs font-bold uppercase leading-none text-white dark:bg-gray-700">Jun</span> <span class="rounded-b-lg bg-gray-100 px-3.5 py-1 text-center text-xl font-semibold text-gray-800 dark:bg-gradient-to-br dark:from-gray-800 dark:to-gray-950">13</span></time> <a href="/papers?date=2023-06-16" class="group my-0.5 -ml-2 flex w-8 items-center justify-center rounded-r-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 rotate-180 translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a></div></div> <div class="relative grid grid-cols-1 gap-14 lg:grid-cols-2"><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3Lwy5zqpR76RhFY4Zt1ig.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07954" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">111</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07954" class="cursor-pointer">Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07954" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chen Change Loy" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yifan Zhou" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="liuziwei7" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="PKUWilliamYang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664764278226-62a54d0410334c1d024e2f59.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> <a href="/papers/2306.07954#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 11</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07967" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gniDuj07YAzfHiQYrEBej.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07967" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">23</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07967" class="cursor-pointer">One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07967" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zhiqiang Shen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Eric Xing" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Deepak Gupta" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="liuzhuang13" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/56efca912a66ef9d4f2f3679aac31bae.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Arnav0400" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6b7bde1d453f0328fcd5a86cfd9eb3b2.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NmTLwrfoEO_pc4zJ0QyLH.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07476" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">16</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07476" class="cursor-pointer">AniFaceDrawing: Anime Portrait Exploration during Your Sketching</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07476" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kazunori Miyata" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Tsukasa Fukusato" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Haoran Xie" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zhengyu Huang" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> <a href="/papers/2306.07476#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07906" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Z49a41V6vQ-6D30SCTekf.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07906" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">12</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07906" class="cursor-pointer">WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07906" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Aohan Zeng" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yifan Xu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hao Yu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hanyullai" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/89b215dafa503b51ab212a9b63c82aca.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ShawLiu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 9 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07536" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Nb5RyRF5-toD8IPURlC0l.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07536" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">10</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07536" class="cursor-pointer">TART: A plug-and-play Transformer module for task-agnostic reasoning</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07536" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Christopher Ré" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Christopher De Sa" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Avanika Narayan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kush Bhatia" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07915" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wxIpCvQZfSQMLlTyNJWNa.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07915" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">9</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07915" class="cursor-pointer">Image Captioners Are Scalable Vision Learners Too</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07915" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andreas Steiner" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Manoj Kumar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="giffmana" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642d334ff65714b4585f2de4/gxBynq5KyoUP0VlAQD3-w.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="neilhoulsby" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/a362a236c0654b7605dcb7673e309335.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mitsch" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/54354c1e5774cadd1d83d42054e9d96b.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O9ZWUnu0C-8by-FbvjuVC.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07349" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">8</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07349" class="cursor-pointer">ATT3D: Amortized Text-to-3D Object Synthesis</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07349" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kevin Xie" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mingyuliutw" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e272e58ad996733d7098e50248e5b57e.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tsungyi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/23300548e5c50f44e95d63568221f47b.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="chenhsuanlin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/4bb215bd711452ee6d28894706773009.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lorraine2" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/12faffe4a2e4bf411220221375763d76.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 10 authors</div></li></ul></a> <a href="/papers/2306.07349#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8dJP3w7fje_vNhP_XBOh8.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07580" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">7</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07580" class="cursor-pointer">SayTap: Language to Quadrupedal Locomotion</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07580" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Heiga Zen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jie Tan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Wenhao Yu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sandraorion" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a3b828225b2121f6b3beb3/KArimb-AY5REzu67VRIm6.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tyj2022" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6c936818a3814c1c32a8eaae8b8cb6d6.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Z_ZIDg1PC9Bm1nQlG1VQY.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07970" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">6</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07970" class="cursor-pointer">Neural Scene Chronology</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07970" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ruojin Cai" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Qianqian Wang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jimantha" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e2ca585398a3212b2956a14804d0ba67.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="pengsida" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="haotongl" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/f7a0fc6816535945e11bac1212dd7b57.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07968" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/48UOputM6b4eFmF2d_ehs.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07968" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">6</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07968" class="cursor-pointer">arXiVeri: Automatic table verification with GPT</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07968" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="albanie" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660153837083-noauth.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Weidi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5398658c7d0cf556531a625a4ca5d18a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="noelshin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/a70517208abecc1d6f245ebc63cb4ed6.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07473" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yagEXIp3waPt5fg9vtDBN.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07473" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07473" class="cursor-pointer">3D molecule generation by denoising voxel grids</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07473" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Omar Mahmood" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Michael Maser" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joseph Kleinhenz" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joshua Rackers" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Pedro O. Pinheiro" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 9 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07944" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/axuEKIPeXCUREl9REGjwe.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07944" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07944" class="cursor-pointer">Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07944" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yuan Cao" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hagen Soltau" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Izhak Shafran" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Mingqiu Wang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="weihan3" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e637e6caab6df56da6005f73c613a3d0.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07969" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qNfHruBJrQd0MgW4nNOoW.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07969" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07969" class="cursor-pointer">GeneCIS: A Benchmark for General Conditional Image Similarity</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07969" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ishan Misra" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="alcinos" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/9d7cb38db7831a05e35c157a71f5be4a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sgvaze" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/abebd42399decafbccc8579faa34e7d3.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07552" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GNUnR-WrGvmc7jodcltoQ.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07552" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07552" class="cursor-pointer">Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07552" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Devendra Singh Chaplot" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andrew Szot" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Vincent-Pierre Berges" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="dbatra" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e5a7428bb0acb43a240f1c4a8c1a5639.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Skylion007" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/9353c064ef8ccac84d0397411d38fa90.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x3hg1ZqfrPEROMq-oagUz.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07437" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07437" class="cursor-pointer">Instant Multi-View Head Capture through Learnable Registration</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07437" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Michael J. Black" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Tianye Li" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tbolkart" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ac7b858f504648c1f5d1c3e3034c7996.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07941" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VHz88eNikjfauhmB3GDBz.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07941" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07941" class="cursor-pointer">GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07941" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Oren Barkan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shahar Keren" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Itzik Malkiel" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yakiryehuda" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/232f51fe49c7d8d491655ad2979009b9.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="urialon" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657635637504-62cd8153248f9e6bc20ab250.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.07946" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aTqG7NfC1svUnyk3fPdh3.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.07946" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.07946" class="cursor-pointer">STUDY: Socially Aware Temporally Casual Decoder Recommender Systems</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.07946" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Subhrajit Roy" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Katherine Heller" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Lauren Harrell" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Diana Mincu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="clockwork7" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/dc50dc74ec345a86b8f9a1352bc3f16c.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div> <div class="col-span-1 flex lg:col-span-2"><a class="btn gap-2" href="/papers?date=2023-06-13"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z" fill="currentColor"></path></svg>Previous</a> <a class="btn ml-auto gap-2" href="/papers?date=2023-06-16">Next<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z" fill="currentColor"></path></svg></a></div></div></section> </div></main>
	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="font-semibold text-black md:hidden">Company</div>
		<div class="order-last pt-6 text-gray-400 md:order-none md:pt-0" href="Terms">© Hugging Face</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="group order-first flex-none pb-6 md:order-none md:pb-0"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="pt-6 font-semibold text-black md:hidden md:pt-0">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("/front/build/kube-351e67d/index.js");
			window.moonSha = "kube-351e67d/";
			window.hubConfig = JSON.parse(`{"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https://huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetsServerPublicUrl":"https://datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)"}`);
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>

		<!-- Google analytics v4 -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL";
				script.async = true;
				document.head.appendChild(script);

				window.dataLayer = window.dataLayer || [];
				function gtag() {
					if (window.dataLayer !== undefined) {
						window.dataLayer.push(arguments);
					}
				}
				gtag("js", new Date());
				gtag("config", "G-8Q63TH4CSL", { page_path: "/papers" });
				/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages
				gtag("consent", "default", { ad_storage: "denied", analytics_storage: "denied" });
				/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent
				/// TODO: ask the user for their consent and update this with gtag('consent', 'update')
			}
		</script>
	

<iframe name="__privateStripeMetricsController7590" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="https://js.stripe.com/v3/m-outer-3437aaddcdf6922d623e172c2d6f9278.html#url=https%3A%2F%2Fhuggingface.co%2Fpapers%3Fdate%3D2023-06-14&amp;title=Daily%20Papers%20-%20Hugging%20Face&amp;referrer=&amp;muid=NA&amp;sid=NA&amp;version=6&amp;preview=false" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe></body></html>