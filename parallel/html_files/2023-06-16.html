<!DOCTYPE html><html class=""><head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<meta name="description" content="Your daily dose of AI research from AK">
		<meta property="fb:app_id" content="1321688464574422">
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@huggingface">
		<meta property="og:title" content="Daily Papers - Hugging Face">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://huggingface.co/papers">
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/papers.png">

		<link rel="stylesheet" href="/front/build/kube-351e67d/style.css">

		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&amp;display=swap" rel="stylesheet">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		  

		<title>Daily Papers - Hugging Face</title>

		<script defer="" data-domain="huggingface.co" src="/js/script.js"></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer=""></script>
	<script src="https://js.stripe.com/v3/" async=""></script><script src="https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapersBannerSubscribe-98f5dbb5.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconCheckmarkFilled-0fb1cbef.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapers-ff82993e.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/Contributors-aac8a263.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/autoplay-4f99a53d.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconMessage-6ab20750.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconArrowLeft-a638f296.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/ModalBody-205aeb00.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/urlWatcher-b1dcfbe0.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-79e4fb58.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconSpinner-f6a85825.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UpvoteControl-821d7cbb.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconUpvoteFilled-f11951bc.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UsersListModal-9fa7b704.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/FollowButton-18e671ce.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-997dbc18.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconBellWatching-37394c1a.js"><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"></head>
	<body class="flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black DailyPapersPage">
		<div class="flex min-h-screen flex-col">
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false}" data-target="MainHeader"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg"> <span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a> <div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..." spellcheck="false" type="text"> <svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> </div> <div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg> </button> </div></div> <nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 xl:space-x-2"><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Models</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> Datasets</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg> Spaces</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg> Posts</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg> Docs</a></li> <li class="max-2xl:hidden"><div class="relative "><button class="px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center " type="button"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z" fill="currentColor"></path></svg> Solutions </button> </div></li> <li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/pricing">Pricing</a></li> <li><div class="relative group"><button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button"><svg class="mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>  </button> </div></li> <li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li> <li><a class="block cursor-pointer px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/login">Log In</a></li> <li><a class="rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up</a></li></ul></nav></div></header></div>
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="GoogleAnalyticsTracker"></div>
	
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="SSOBanner"></div>
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-props="{&quot;isLoggedIn&quot;:false}" data-target="DailyPapersBannerSubscribe"><div class="-mt-px flex h-9 w-full justify-center text-gray-600"><svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z"></path></svg> <div class="flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1"><div class="rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600">new</div> <p class="hidden sm:inline">Get trending papers in your email inbox once a day!</p> <p class="inline sm:hidden">Get trending papers in your email inbox!</p> <a href="/login?next=%2Fpapers" class="btn !px-2 text-sm leading-none">Subscribe</a></div> <svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z"></path></svg></div></div>
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;date&quot;:&quot;2023-06-16T00:00:00.000Z&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08276&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bf365f11318e0a448a91c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6b2925901e36fddb95760e3cbd20d8d7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Luyang Zhu&quot;,&quot;user&quot;:&quot;lyzhu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Luyang Zhu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:06:44.785Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a91d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/aab6be8626d9a2a87f58c8a5803c72f0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;DaweiYang&quot;,&quot;user&quot;:&quot;DaweiYang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Dawei Yang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:07:01.097Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a91e&quot;,&quot;name&quot;:&quot;Tyler Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a91f&quot;,&quot;name&quot;:&quot;Fitsum Reda&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a920&quot;,&quot;name&quot;:&quot;William Chan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a921&quot;,&quot;name&quot;:&quot;Chitwan Saharia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a922&quot;,&quot;name&quot;:&quot;Mohammad Norouzi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf365f11318e0a448a923&quot;,&quot;name&quot;:&quot;Ira Kemelmacher-Shlizerman&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T06:25:58.000Z&quot;,&quot;title&quot;:&quot;TryOnDiffusion: A Tale of Two UNets&quot;,&quot;summary&quot;:&quot;Given two images depicting a person and a garment worn by another person, our\ngoal is to generate a visualization of how the garment might look on the input\nperson. A key challenge is to synthesize a photorealistic detail-preserving\nvisualization of the garment, while warping the garment to accommodate a\nsignificant body pose and shape change across the subjects. Previous methods\neither focus on garment detail preservation without effective pose and shape\nvariation, or allow try-on with the desired shape and pose but lack garment\ndetails. In this paper, we propose a diffusion-based architecture that unifies\ntwo UNets (referred to as Parallel-UNet), which allows us to preserve garment\ndetails and warp the garment for significant pose and body change in a single\nnetwork. The key ideas behind Parallel-UNet include: 1) garment is warped\nimplicitly via a cross attention mechanism, 2) garment warp and person blend\nhappen as part of a unified process as opposed to a sequence of two separate\ntasks. Experimental results indicate that TryOnDiffusion achieves\nstate-of-the-art performance both qualitatively and quantitatively.&quot;,&quot;upvotes&quot;:69},&quot;publishedAt&quot;:&quot;2023-06-16T05:30:18.263Z&quot;,&quot;title&quot;:&quot;TryOnDiffusion: A Tale of Two UNets&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xyJguN1HCd_gEblq7byX-.mp4&quot;,&quot;numComments&quot;:6,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09348&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bcc25a566447524183c7d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/93072cde421c9d7d25614f967a9aa45a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hadi Alzayer&quot;,&quot;user&quot;:&quot;HadiZayer&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hadi Alzayer&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T07:16:03.563Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcc25a566447524183c7e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5c90681d63cb3297176b1a68d024cc39.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kevin Zhang&quot;,&quot;user&quot;:&quot;kzhang0&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kevin Zhang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:39.331Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcc25a566447524183c7f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/3e05289d1876d7b7ef2c6a595a7f39d7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Brandon Feng&quot;,&quot;user&quot;:&quot;brandonyfeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Brandon Feng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T18:50:47.635Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcc25a566447524183c80&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0659d41aab160b3997f52c9c8fd5f2c1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;chris metzler&quot;,&quot;user&quot;:&quot;strikemetz&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Christopher Metzler&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:57:23.898Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcc25a566447524183c81&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5a2550d95e686640242840ad3bd0e680.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiabin Huang&quot;,&quot;user&quot;:&quot;YellowAddice&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jia-Bin Huang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:20:18.098Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:59:59.000Z&quot;,&quot;title&quot;:&quot;Seeing the World through Your Eyes&quot;,&quot;summary&quot;:&quot;The reflective nature of the human eye is an underappreciated source of\ninformation about what the world around us looks like. By imaging the eyes of a\nmoving person, we can collect multiple views of a scene outside the camera's\ndirect line of sight through the reflections in the eyes. In this paper, we\nreconstruct a 3D scene beyond the camera's line of sight using portrait images\ncontaining eye reflections. This task is challenging due to 1) the difficulty\nof accurately estimating eye poses and 2) the entangled appearance of the eye\niris and the scene reflections. Our method jointly refines the cornea poses,\nthe radiance field depicting the scene, and the observer's eye iris texture. We\nfurther propose a simple regularization prior on the iris texture pattern to\nimprove reconstruction quality. Through various experiments on synthetic and\nreal-world captures featuring people with varied eye colors, we demonstrate the\nfeasibility of our approach to recover 3D scenes using eye reflections.&quot;,&quot;upvotes&quot;:30},&quot;publishedAt&quot;:&quot;2023-06-16T02:42:52.769Z&quot;,&quot;title&quot;:&quot;Seeing the World through Your Eyes&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pbKOx4b3d30XsvRN9AVVo.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08568&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb882625&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090ff099a8bcaa437b234a4/nxN64gFkr5o5UZM-i03re.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ziyang Luo&quot;,&quot;user&quot;:&quot;Ziyang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ziyang Luo&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:00:11.858Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb882626&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/98d6610a3cd17a27a4201ec926c0e7ff.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Can Xu&quot;,&quot;user&quot;:&quot;nlpxucan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Can Xu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:01:17.861Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb882627&quot;,&quot;name&quot;:&quot;Pu Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb882628&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;WizardLM&quot;,&quot;user&quot;:&quot;WizardLM&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Qingfeng Sun&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:04:04.309Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb882629&quot;,&quot;name&quot;:&quot;Xiubo Geng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb88262a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/4daea27b432ff428c41d590669f7330c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenxiang Hu&quot;,&quot;user&quot;:&quot;wenxcs&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wenxiang Hu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:43.282Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb88262b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/7a640980ed225e31117f750e09bcc18c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chongyang Tao&quot;,&quot;user&quot;:&quot;chongyang09&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chongyang Tao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-08-29T06:59:36.222Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb88262c&quot;,&quot;name&quot;:&quot;Jing Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb88262d&quot;,&quot;name&quot;:&quot;Qingwei Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc6c9afaac0d4cb88262e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b1e0aae690f0ca0e2dbe9e0412c786ef.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Daxin Jiang&quot;,&quot;user&quot;:&quot;djiang3&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Daxin Jiang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:57:28.286Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T15:18:48.000Z&quot;,&quot;title&quot;:&quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct&quot;,&quot;summary&quot;:&quot;Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM&quot;,&quot;upvotes&quot;:26},&quot;publishedAt&quot;:&quot;2023-06-16T02:19:54.441Z&quot;,&quot;title&quot;:&quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pOqOag3cCyJRL1LnC15B0.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08640&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add4&quot;,&quot;name&quot;:&quot;Difei Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add5&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/3369af1f4e16dfaf727f78e63b21a7ad.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JiLei&quot;,&quot;user&quot;:&quot;AlexJi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Lei Ji&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T13:04:39.130Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add6&quot;,&quot;name&quot;:&quot;Luowei Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add7&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qinghong Lin&quot;,&quot;user&quot;:&quot;KevinQHLin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kevin Qinghong Lin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T13:03:55.677Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add8&quot;,&quot;name&quot;:&quot;Joya Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1add9&quot;,&quot;name&quot;:&quot;Zihan Fan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc7de6e8ac6a9cef1adda&quot;,&quot;name&quot;:&quot;Mike Zheng Shou&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T17:12:56.000Z&quot;,&quot;title&quot;:&quot;AssistGPT: A General Multi-modal Assistant that can Plan, Execute,\n  Inspect, and Learn&quot;,&quot;summary&quot;:&quot;Recent research on Large Language Models (LLMs) has led to remarkable\nadvancements in general NLP AI assistants. Some studies have further explored\nthe use of LLMs for planning and invoking models or APIs to address more\ngeneral multi-modal user queries. Despite this progress, complex visual-based\ntasks still remain challenging due to the diverse nature of visual tasks. This\ndiversity is reflected in two aspects: 1) Reasoning paths. For many real-life\napplications, it is hard to accurately decompose a query simply by examining\nthe query itself. Planning based on the specific visual content and the results\nof each step is usually required. 2) Flexible inputs and intermediate results.\nInput forms could be flexible for in-the-wild cases, and involves not only a\nsingle image or video but a mixture of videos and images, e.g., a user-view\nimage with some reference videos. Besides, a complex reasoning process will\nalso generate diverse multimodal intermediate results, e.g., video narrations,\nsegmented video clips, etc. To address such general cases, we propose a\nmulti-modal AI assistant, AssistGPT, with an interleaved code and language\nreasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate\nLLMs with various tools. Specifically, the Planner is capable of using natural\nlanguage to plan which tool in Executor should do next based on the current\nreasoning progress. Inspector is an efficient memory manager to assist the\nPlanner to feed proper visual information into a specific tool. Finally, since\nthe entire reasoning process is complex and flexible, a Learner is designed to\nenable the model to autonomously explore and discover the optimal solution. We\nconducted experiments on A-OKVQA and NExT-QA benchmarks, achieving\nstate-of-the-art results. Moreover, showcases demonstrate the ability of our\nsystem to handle questions far more complex than those found in the benchmarks.&quot;,&quot;upvotes&quot;:25},&quot;publishedAt&quot;:&quot;2023-06-16T02:24:30.930Z&quot;,&quot;title&quot;:&quot;AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/z1JT5nfgc-6JhJrtDRfXV.mp4&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09296&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c88&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/fba1fb87bcac340a8eee3b9f4fc35bc5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jifan Yu&quot;,&quot;user&quot;:&quot;JovanYu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jifan Yu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T12:04:33.769Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c89&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/47d71d80f9901313feb0199c37296389.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiaozhi Wang&quot;,&quot;user&quot;:&quot;wangxz098&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xiaozhi Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T07:15:58.863Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8a&quot;,&quot;name&quot;:&quot;Shangqing Tu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8b&quot;,&quot;name&quot;:&quot;Shulin Cao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8c&quot;,&quot;name&quot;:&quot;Daniel Zhang-Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8d&quot;,&quot;name&quot;:&quot;Xin Lv&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8e&quot;,&quot;name&quot;:&quot;Hao Peng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c8f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zijun&quot;,&quot;user&quot;:&quot;TranSirius&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zijun Yao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-10-16T07:25:21.350Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c90&quot;,&quot;name&quot;:&quot;Xiaohan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c91&quot;,&quot;name&quot;:&quot;Hanming Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c92&quot;,&quot;name&quot;:&quot;Chunyang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c93&quot;,&quot;name&quot;:&quot;Zheyuan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c94&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yushi Bai&quot;,&quot;user&quot;:&quot;bys0318&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yushi Bai&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-02-01T08:45:17.984Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c95&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yantao&quot;,&quot;user&quot;:&quot;RicardoL1u&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yantao Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:29.470Z&quot;,&quot;hidden&quot;:true},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c96&quot;,&quot;name&quot;:&quot;Amy Xin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c97&quot;,&quot;name&quot;:&quot;Nianyi Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c98&quot;,&quot;name&quot;:&quot;Kaifeng Yun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c99&quot;,&quot;name&quot;:&quot;Linlu Gong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9a&quot;,&quot;name&quot;:&quot;Jianhui Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9b&quot;,&quot;name&quot;:&quot;Zhili Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9c&quot;,&quot;name&quot;:&quot;Yunjia Qi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9d&quot;,&quot;name&quot;:&quot;Weikai Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9e&quot;,&quot;name&quot;:&quot;Yong Guan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7c9f&quot;,&quot;name&quot;:&quot;Kaisheng Zeng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca0&quot;,&quot;name&quot;:&quot;Ji Qi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca1&quot;,&quot;name&quot;:&quot;Hailong Jin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca2&quot;,&quot;name&quot;:&quot;Jinxin Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca3&quot;,&quot;name&quot;:&quot;Yu Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca4&quot;,&quot;name&quot;:&quot;Yuan Yao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca5&quot;,&quot;name&quot;:&quot;Ning Ding&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca6&quot;,&quot;name&quot;:&quot;Lei Hou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca7&quot;,&quot;name&quot;:&quot;Zhiyuan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca8&quot;,&quot;name&quot;:&quot;Bin Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7ca9&quot;,&quot;name&quot;:&quot;Jie Tang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be739152fc5a80d2c7caa&quot;,&quot;name&quot;:&quot;Juanzi Li&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:20:46.000Z&quot;,&quot;title&quot;:&quot;KoLA: Carefully Benchmarking World Knowledge of Large Language Models&quot;,&quot;summary&quot;:&quot;The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate 21 open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems.&quot;,&quot;upvotes&quot;:18},&quot;publishedAt&quot;:&quot;2023-06-16T04:38:18.138Z&quot;,&quot;title&quot;:&quot;KoLA: Carefully Benchmarking World Knowledge of Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ctllsIWgk2GiA2BPQFS4N.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08161&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b4f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Arno Candel&quot;,&quot;user&quot;:&quot;arnocandel&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Arno Candel&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:56:07.593Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b50&quot;,&quot;name&quot;:&quot;Jon McKinney&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b51&quot;,&quot;name&quot;:&quot;Philipp Singer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b52&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Pascal Pfeiffer&quot;,&quot;user&quot;:&quot;ilu000&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Pascal Pfeiffer&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:58:13.063Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b53&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674725099569-614c9145b44c102499783617.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Maximilian Jeblick&quot;,&quot;user&quot;:&quot;MaxJeblick&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Maximilian Jeblick&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:58:32.992Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b54&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f1d71e9d06393691191a0/TwhM8xiUUfK6EV9rGrh5x.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Prithvi Prabhu&quot;,&quot;user&quot;:&quot;lo5&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Prithvi Prabhu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:58:50.465Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b55&quot;,&quot;name&quot;:&quot;Jeff Gambera&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b56&quot;,&quot;name&quot;:&quot;Mark Landry&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b57&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/c2ab0d46514b9e81926990ea0a032531.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;bansal&quot;,&quot;user&quot;:&quot;shivam-h2o&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shivam Bansal&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T10:52:00.374Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b58&quot;,&quot;name&quot;:&quot;Ryan Chesler&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b59&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/137d5698d8bfac2d8a4175e892d7c206.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chun Ming Lee&quot;,&quot;user&quot;:&quot;leecming&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chun Ming Lee&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:59:44.081Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b5a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665231535844-634169d1a7582111c3f46bae.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Marcos V. Conde&quot;,&quot;user&quot;:&quot;marcosv&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Marcos V. Conde&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:04:14.346Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b5b&quot;,&quot;name&quot;:&quot;Pasha Stetsenko&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b5c&quot;,&quot;name&quot;:&quot;Olivier Grellier&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc2ee9f4df9495b861b5d&quot;,&quot;name&quot;:&quot;SriSatish Ambati&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T22:19:53.000Z&quot;,&quot;title&quot;:&quot;h2oGPT: Democratizing Large Language Models&quot;,&quot;summary&quot;:&quot;Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution\nin AI due to their real-world applications though natural language processing.\nHowever, they also pose many significant risks such as the presence of biased,\nprivate, or harmful text, and the unauthorized inclusion of copyrighted\nmaterial.\n  We introduce h2oGPT, a suite of open-source code repositories for the\ncreation and use of Large Language Models (LLMs) based on Generative Pretrained\nTransformers (GPTs). The goal of this project is to create the world's best\ntruly open-source alternative to closed-source GPTs. In collaboration with and\nas part of the incredible and unstoppable open-source community, we open-source\nseveral fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for\ncommercial use under fully permissive Apache 2.0 licenses. Included in our\nrelease is 100% private document search using natural language.\n  Open-source language models help boost AI development and make it more\naccessible and trustworthy. They lower entry hurdles, allowing people and\ngroups to tailor these models to their needs. This openness increases\ninnovation, transparency, and fairness. An open-source strategy is needed to\nshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.&quot;,&quot;upvotes&quot;:18},&quot;publishedAt&quot;:&quot;2023-06-16T02:03:27.692Z&quot;,&quot;title&quot;:&quot;h2oGPT: Democratizing Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Po1LsyKAB-X3XZCUPXnvB.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08543&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc4a8d60140c4bbca5278&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/8de6e319246500c460cf41163462c214.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuxian Gu&quot;,&quot;user&quot;:&quot;t1101675&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuxian Gu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:47.105Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc4a8d60140c4bbca5279&quot;,&quot;name&quot;:&quot;Li Dong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc4a8d60140c4bbca527a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Furu Wei&quot;,&quot;user&quot;:&quot;thegenerality&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Furu Wei&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T12:03:25.107Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc4a8d60140c4bbca527b&quot;,&quot;name&quot;:&quot;Minlie Huang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T14:44:03.000Z&quot;,&quot;title&quot;:&quot;Knowledge Distillation of Large Language Models&quot;,&quot;summary&quot;:&quot;Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge from white-box generative LLMs is still under-explored,\nwhich becomes more and more important with the prosperity of LLMs. In this\nwork, we propose MiniLLM that distills smaller language models from generative\nlarger language models. We first replace the forward Kullback-Leibler\ndivergence (KLD) objective in the standard KD approaches with reverse KLD,\nwhich is more suitable for KD on generative language models, to prevent the\nstudent model from overestimating the low-probability regions of the teacher\ndistribution. Then, we derive an effective optimization approach to learn this\nobjective. Extensive experiments in the instruction-following setting show that\nthe MiniLLM models generate more precise responses with the higher overall\nquality, lower exposure bias, better calibration, and higher long-text\ngeneration performance. Our method is also scalable for different model\nfamilies with 120M to 13B parameters. We will release our code and model\ncheckpoints at https://aka.ms/MiniLLM.&quot;,&quot;upvotes&quot;:16},&quot;publishedAt&quot;:&quot;2023-06-16T02:10:48.809Z&quot;,&quot;title&quot;:&quot;Knowledge Distillation of Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/m0dbqjbl790Fi6SKI-aa2.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09329&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc79&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/53163398f1a629b9838548b808af53f7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nikos Kolotouros&quot;,&quot;user&quot;:&quot;kolotouros&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Nikos Kolotouros&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T07:15:43.820Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc7a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648c964d39d2584ee47af19c/5UEkzDTMY8I3svKjR7kxN.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Thiemo Alldieck&quot;,&quot;user&quot;:&quot;thiemoall&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Thiemo Alldieck&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:24.141Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc7b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/c8bd4b51155cd37fb7e76c97b843d461.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andrei Zanfir&quot;,&quot;user&quot;:&quot;Andreiz&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Andrei Zanfir&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T07:15:48.199Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc7c&quot;,&quot;name&quot;:&quot;Eduard Gabriel Bazavan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc7d&quot;,&quot;name&quot;:&quot;Mihai Fieraru&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c00c8f11318e0a44bcc7e&quot;,&quot;name&quot;:&quot;Cristian Sminchisescu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:58:21.000Z&quot;,&quot;title&quot;:&quot;DreamHuman: Animatable 3D Avatars from Text&quot;,&quot;summary&quot;:&quot;We present DreamHuman, a method to generate realistic animatable 3D human\navatar models solely from textual descriptions. Recent text-to-3D methods have\nmade considerable strides in generation, but are still lacking in important\naspects. Control and often spatial resolution remain limited, existing methods\nproduce fixed rather than animated 3D human models, and anthropometric\nconsistency for complex structures like people remains a challenge. DreamHuman\nconnects large text-to-image synthesis models, neural radiance fields, and\nstatistical human body models in a novel modeling and optimization framework.\nThis makes it possible to generate dynamic 3D human avatars with high-quality\ntextures and learned, instance-specific, surface deformations. We demonstrate\nthat our method is capable to generate a wide variety of animatable, realistic\n3D human models from text. Our 3D models have diverse appearance, clothing,\nskin tones and body shapes, and significantly outperform both generic\ntext-to-3D approaches and previous text-based 3D avatar generators in visual\nfidelity. For more results and animations please check our website at\nhttps://dream-human.github.io.&quot;,&quot;upvotes&quot;:14},&quot;publishedAt&quot;:&quot;2023-06-16T06:27:21.819Z&quot;,&quot;title&quot;:&quot;DreamHuman: Animatable 3D Avatars from Text&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gIWnWezBrscS3RrhZeAdb.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09093&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcb7&quot;,&quot;name&quot;:&quot;Chenyang Lyu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcb8&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/db64e2d2ed905e4f1c187c046fa2948d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minghao Wu&quot;,&quot;user&quot;:&quot;minghaowu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Minghao Wu&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T03:44:40.608Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcb9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/15d5d5403fef2f1368bb4185b199061d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Longyue Wang&quot;,&quot;user&quot;:&quot;longyuewang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Longyue Wang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T11:59:44.116Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcba&quot;,&quot;name&quot;:&quot;Xinting Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcbb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/3941abd3cfdb68212db6106e2ce521ed.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bingshuai Liu&quot;,&quot;user&quot;:&quot;bsliu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Bingshuai Liu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T11:59:35.636Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcbc&quot;,&quot;name&quot;:&quot;Zefeng Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcbd&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/971763f3b724d63bc64b7d3599cfc753.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuming Shi&quot;,&quot;user&quot;:&quot;Shuming&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shuming Shi&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T12:00:32.800Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd92a9f4df9495b8adcbe&quot;,&quot;name&quot;:&quot;Zhaopeng Tu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T12:45:25.000Z&quot;,&quot;title&quot;:&quot;Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and\n  Text Integration&quot;,&quot;summary&quot;:&quot;Although instruction-tuned large language models (LLMs) have exhibited\nremarkable capabilities across various NLP tasks, their effectiveness on other\ndata modalities beyond text has not been fully studied. In this work, we\npropose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,\naudio, and textual information. Macaw-LLM consists of three main components: a\nmodality module for encoding multi-modal data, a cognitive module for\nharnessing pretrained LLMs, and an alignment module for harmonizing diverse\nrepresentations. Our novel alignment module seamlessly bridges multi-modal\nfeatures to textual features, simplifying the adaptation process from the\nmodality modules to the cognitive module. In addition, we construct a\nlarge-scale multi-modal instruction dataset in terms of multi-turn dialogue,\nincluding 69K image instances and 50K video instances. We have made our data,\ncode and model publicly available, which we hope can pave the way for future\nresearch in multi-modal LLMs and expand the capabilities of LLMs to handle\ndiverse data modalities and address complex real-world scenarios.&quot;,&quot;upvotes&quot;:12},&quot;publishedAt&quot;:&quot;2023-06-16T03:38:20.424Z&quot;,&quot;title&quot;:&quot;Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sHXt38101YyXqmWqRFNsO.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08647&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faab6&quot;,&quot;name&quot;:&quot;Wenhao Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faab7&quot;,&quot;name&quot;:&quot;Nimrod Gileadi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faab8&quot;,&quot;name&quot;:&quot;Chuyuan Fu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faab9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/bdde73b95a36e32cb2975656cca46022.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sean Kirmani&quot;,&quot;user&quot;:&quot;skirmani&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sean Kirmani&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:34.340Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faaba&quot;,&quot;name&quot;:&quot;Kuang-Huei Lee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faabb&quot;,&quot;name&quot;:&quot;Montse Gonzalez Arenas&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faabc&quot;,&quot;name&quot;:&quot;Hao-Tien Lewis Chiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faabd&quot;,&quot;name&quot;:&quot;Tom Erez&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faabe&quot;,&quot;name&quot;:&quot;Leonard Hasenclever&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faabf&quot;,&quot;name&quot;:&quot;Jan Humplik&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac0&quot;,&quot;name&quot;:&quot;Brian Ichter&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac1&quot;,&quot;name&quot;:&quot;Ted Xiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac2&quot;,&quot;name&quot;:&quot;Peng Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac3&quot;,&quot;name&quot;:&quot;Andy Zeng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac4&quot;,&quot;name&quot;:&quot;Tingnan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac5&quot;,&quot;name&quot;:&quot;Nicolas Heess&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac6&quot;,&quot;name&quot;:&quot;Dorsa Sadigh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac7&quot;,&quot;name&quot;:&quot;Jie Tan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac8&quot;,&quot;name&quot;:&quot;Yuval Tassa&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcdb4a7bfa231b27faac9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/8bfe3bf63f0609b3b6b7d7c5d7f6b3e9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fei Xia&quot;,&quot;user&quot;:&quot;fxia22&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Fei Xia&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:57:17.803Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T17:27:10.000Z&quot;,&quot;title&quot;:&quot;Language to Rewards for Robotic Skill Synthesis&quot;,&quot;summary&quot;:&quot;Large language models (LLMs) have demonstrated exciting progress in acquiring\ndiverse new capabilities through in-context learning, ranging from logical\nreasoning to code-writing. Robotics researchers have also explored using LLMs\nto advance the capabilities of robotic control. However, since low-level robot\nactions are hardware-dependent and underrepresented in LLM training corpora,\nexisting efforts in applying LLMs to robotics have largely treated LLMs as\nsemantic planners or relied on human-engineered control primitives to interface\nwith the robot. On the other hand, reward functions are shown to be flexible\nrepresentations that can be optimized for control policies to achieve diverse\ntasks, while their semantic richness makes them suitable to be specified by\nLLMs. In this work, we introduce a new paradigm that harnesses this realization\nby utilizing LLMs to define reward parameters that can be optimized and\naccomplish variety of robotic tasks. Using reward as the intermediate interface\ngenerated by LLMs, we can effectively bridge the gap between high-level\nlanguage instructions or corrections to low-level robot actions. Meanwhile,\ncombining this with a real-time optimizer, MuJoCo MPC, empowers an interactive\nbehavior creation experience where users can immediately observe the results\nand provide feedback to the system. To systematically evaluate the performance\nof our proposed method, we designed a total of 17 tasks for a simulated\nquadruped robot and a dexterous manipulator robot. We demonstrate that our\nproposed method reliably tackles 90% of the designed tasks, while a baseline\nusing primitive skills as the interface with Code-as-policies achieves 50% of\nthe tasks. We further validated our method on a real robot arm where complex\nmanipulation skills such as non-prehensile pushing emerge through our\ninteractive system.&quot;,&quot;upvotes&quot;:10},&quot;publishedAt&quot;:&quot;2023-06-16T02:49:29.342Z&quot;,&quot;title&quot;:&quot;Language to Rewards for Robotic Skill Synthesis&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9Iy5lzF81D8qn7DT-suOk.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08997&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc37095d85bfac020d484&quot;,&quot;name&quot;:&quot;Sarah J. Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d485&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/971c17600395b1037f676710b32b3909.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sam Florin&quot;,&quot;user&quot;:&quot;sflorin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Samuel Florin&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T18:06:50.672Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d486&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638bcfa91987d67b340e6c1c/3tHCB_J6c4-lsEZ_zJSlp.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ariel N. Lee&quot;,&quot;user&quot;:&quot;arielnlee&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ariel N. Lee&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T02:36:08.413Z&quot;,&quot;hidden&quot;:true},{&quot;_id&quot;:&quot;648bc37095d85bfac020d487&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/cdad9af0749a52945116b385e2c2591b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Eamon Niknafs&quot;,&quot;user&quot;:&quot;eamonniknafs&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Eamon Niknafs&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-20T08:50:15.096Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d488&quot;,&quot;name&quot;:&quot;Andrei Marginean&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d489&quot;,&quot;name&quot;:&quot;Annie Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48a&quot;,&quot;name&quot;:&quot;Keith Tyser&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48b&quot;,&quot;name&quot;:&quot;Zad Chin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48c&quot;,&quot;name&quot;:&quot;Yann Hicke&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e436988005ad4f9d99fcafb8bdfade9c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nikhil Singh&quot;,&quot;user&quot;:&quot;nsingh1&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Nikhil Singh&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T07:56:07.095Z&quot;,&quot;hidden&quot;:true},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48e&quot;,&quot;name&quot;:&quot;Madeleine Udell&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d48f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b282162d6d9ae8512873eba0275f603e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yoon Kim&quot;,&quot;user&quot;:&quot;yoon-kim&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yoon Kim&quot;,&quot;status&quot;:&quot;extracted_pending&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T02:05:38.166Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d490&quot;,&quot;name&quot;:&quot;Tonio Buonassisi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d491&quot;,&quot;name&quot;:&quot;Armando Solar-Lezama&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc37095d85bfac020d492&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657402192472-62c9f32a334a6ee11c35d601.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Iddo Drori&quot;,&quot;user&quot;:&quot;idrori&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Iddo Drori&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T13:57:32.533Z&quot;,&quot;hidden&quot;:true}],&quot;publishedAt&quot;:&quot;2023-06-15T09:48:14.000Z&quot;,&quot;title&quot;:&quot;Exploring the MIT Mathematics and EECS Curriculum Using Large Language\n  Models&quot;,&quot;summary&quot;:&quot;We curate a comprehensive dataset of 4,550 questions and solutions from\nproblem sets, midterm exams, and final exams across all MIT Mathematics and\nElectrical Engineering and Computer Science (EECS) courses required for\nobtaining a degree. We evaluate the ability of large language models to fulfill\nthe graduation requirements for any MIT major in Mathematics and EECS. Our\nresults demonstrate that GPT-3.5 successfully solves a third of the entire MIT\ncurriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate\non a test set excluding questions based on images. We fine-tune an open-source\nlarge language model on this dataset. We employ GPT-4 to automatically grade\nmodel responses, providing a detailed performance breakdown by course,\nquestion, and answer type. By embedding questions in a low-dimensional space,\nwe explore the relationships between questions, topics, and classes and\ndiscover which questions and classes are required for solving other questions\nand classes through few-shot learning. Our analysis offers valuable insights\ninto course prerequisites and curriculum design, highlighting language models'\npotential for learning and improving Mathematics and EECS education.&quot;,&quot;upvotes&quot;:10},&quot;publishedAt&quot;:&quot;2023-06-16T02:05:38.215Z&quot;,&quot;title&quot;:&quot;Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3LgRK-8RX5uJzoPXdJGod.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08205&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a1&quot;,&quot;name&quot;:&quot;Saminda Abeyruwan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e1d8be8bb68200230cdf05497479e6f4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alex Bewley&quot;,&quot;user&quot;:&quot;bewley&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alex Bewley&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-21T20:46:05.285Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a3&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/11aa33b32ef4b6aa27ab8e6c068afa14.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nicholas Boffi&quot;,&quot;user&quot;:&quot;nmboffi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Nicholas M. Boffi&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-01-23T17:09:35.679Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a4&quot;,&quot;name&quot;:&quot;Krzysztof Choromanski&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a5&quot;,&quot;name&quot;:&quot;David D'Ambrosio&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/ad3e078c6df8b319ef5fa85e556f8a3c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Deepali Jain&quot;,&quot;user&quot;:&quot;jaindeepali&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Deepali Jain&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:38:19.893Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a7&quot;,&quot;name&quot;:&quot;Pannag Sanketi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a8&quot;,&quot;name&quot;:&quot;Anish Shankar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40a9&quot;,&quot;name&quot;:&quot;Vikas Sindhwani&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40aa&quot;,&quot;name&quot;:&quot;Sumeet Singh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40ab&quot;,&quot;name&quot;:&quot;Jean-Jacques Slotine&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c028817c7ceb9b41a40ac&quot;,&quot;name&quot;:&quot;Stephen Tu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T02:13:25.000Z&quot;,&quot;title&quot;:&quot;Agile Catching with Whole-Body MPC and Blackbox Policy Learning&quot;,&quot;summary&quot;:&quot;We address a benchmark task in agile robotics: catching objects thrown at\nhigh-speed. This is a challenging task that involves tracking, intercepting,\nand cradling a thrown object with access only to visual observations of the\nobject and the proprioceptive state of the robot, all within a fraction of a\nsecond. We present the relative merits of two fundamentally different solution\nstrategies: (i) Model Predictive Control using accelerated constrained\ntrajectory optimization, and (ii) Reinforcement Learning using zeroth-order\noptimization. We provide insights into various performance trade-offs including\nsample efficiency, sim-to-real transfer, robustness to distribution shifts, and\nwhole-body multimodality via extensive on-hardware experiments. We conclude\nwith proposals on fusing \&quot;classical\&quot; and \&quot;learning-based\&quot; techniques for agile\nrobot control. Videos of our experiments may be found at\nhttps://sites.google.com/view/agile-catching&quot;,&quot;upvotes&quot;:9},&quot;publishedAt&quot;:&quot;2023-06-16T06:34:49.760Z&quot;,&quot;title&quot;:&quot;Agile Catching with Whole-Body MPC and Blackbox Policy Learning&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qg1jrH5qKWi4Jgso5QPVE.mp4&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09200&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f4&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/3309f1c8f1340a93e8b7a22fb38069fa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xidong Feng&quot;,&quot;user&quot;:&quot;Waterhorse&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xidong Feng&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:21:58.913Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f5&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/8936667d7c6200ad3058f22329df5542.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yicheng Luo&quot;,&quot;user&quot;:&quot;ethanluoyc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yicheng Luo&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:22:36.525Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/f5ef14e60f06dd99e4c6ff4df0f225c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ziyan wang&quot;,&quot;user&quot;:&quot;ziyan98&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ziyan Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-10T09:25:15.878Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f7&quot;,&quot;name&quot;:&quot;Hongrui Tang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f8&quot;,&quot;name&quot;:&quot;Mengyue Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24f9&quot;,&quot;name&quot;:&quot;Kun Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24fa&quot;,&quot;name&quot;:&quot;David Mguni&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24fb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/f96a4d375ea44d10891f4b47b7d7986e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yali du&quot;,&quot;user&quot;:&quot;YaliDU&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yali Du&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:25:58.126Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be5a02d180f88bd6f24fc&quot;,&quot;name&quot;:&quot;Jun Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T15:35:31.000Z&quot;,&quot;title&quot;:&quot;ChessGPT: Bridging Policy Learning and Language Modeling&quot;,&quot;summary&quot;:&quot;When solving decision-making tasks, humans typically depend on information\nfrom two key sources: (1) Historical policy data, which provides interaction\nreplay from the environment, and (2) Analytical insights in natural language\nform, exposing the invaluable thought process or strategic considerations.\nDespite this, the majority of preceding research focuses on only one source:\nthey either use historical replay exclusively to directly learn policy or value\nfunctions, or engaged in language model training utilizing mere language\ncorpus. In this paper, we argue that a powerful autonomous agent should cover\nboth sources. Thus, we propose ChessGPT, a GPT model bridging policy learning\nand language modeling by integrating data from these two sources in Chess\ngames. Specifically, we build a large-scale game and language dataset related\nto chess. Leveraging the dataset, we showcase two model examples ChessCLIP and\nChessGPT, integrating policy learning and language modeling. Finally, we\npropose a full evaluation framework for evaluating language model's chess\nability. Experimental results validate our model and dataset's effectiveness.\nWe open source our code, model, and dataset at\nhttps://github.com/waterhorse1/ChessGPT.&quot;,&quot;upvotes&quot;:9},&quot;publishedAt&quot;:&quot;2023-06-16T04:31:29.174Z&quot;,&quot;title&quot;:&quot;ChessGPT: Bridging Policy Learning and Language Modeling&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7rWRuAMvKjGDaaGjOL8O-.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09316&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bfc021c113a0526221dfc&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/85840faf1055ec2fcae495db6f042a27.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Laurynas&quot;,&quot;user&quot;:&quot;lkarazija&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Laurynas Karazija&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T07:15:52.907Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bfc021c113a0526221dfd&quot;,&quot;name&quot;:&quot;Iro Laina&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bfc021c113a0526221dfe&quot;,&quot;name&quot;:&quot;Andrea Vedaldi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bfc021c113a0526221dff&quot;,&quot;name&quot;:&quot;Christian Rupprecht&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:51:28.000Z&quot;,&quot;title&quot;:&quot;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&quot;,&quot;summary&quot;:&quot;The variety of objects in the real world is nearly unlimited and is thus\nimpossible to capture using models trained on a fixed set of categories. As a\nresult, in recent years, open-vocabulary methods have attracted the interest of\nthe community. This paper proposes a new method for zero-shot open-vocabulary\nsegmentation. Prior work largely relies on contrastive training using\nimage-text pairs, leveraging grouping mechanisms to learn image features that\nare both aligned with language and well-localised. This however can introduce\nambiguity as the visual appearance of images with similar captions often\nvaries. Instead, we leverage the generative properties of large-scale\ntext-to-image diffusion models to sample a set of support images for a given\ntextual category. This provides a distribution of appearances for a given text\ncircumventing the ambiguity problem. We further propose a mechanism that\nconsiders the contextual background of the sampled images to better localise\nobjects and segment the background directly. We show that our method can be\nused to ground several existing pre-trained self-supervised feature extractors\nin natural language and provide explainable predictions by mapping back to\nregions in the support set. Our proposal is training-free, relying on\npre-trained components only, yet, shows strong performance on a range of\nopen-vocabulary segmentation benchmarks, obtaining a lead of more than 10% on\nthe Pascal VOC benchmark.&quot;,&quot;upvotes&quot;:8},&quot;publishedAt&quot;:&quot;2023-06-16T06:07:00.256Z&quot;,&quot;title&quot;:&quot;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QNSzM2VCdNxOU3H3QF0aL.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08620&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bf0a8ded4c3eb970c8647&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/619828ff465b7973a532dee6/A55HdKmHcZ9w6BxN-cGoE.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;John Thickstun&quot;,&quot;user&quot;:&quot;jthickstun&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;John Thickstun&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:26:42.846Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf0a8ded4c3eb970c8648&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/308ec832279f118f5e981cbf392d798b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;David Hall&quot;,&quot;user&quot;:&quot;dlwh&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;David Hall&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:27:35.275Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf0a8ded4c3eb970c8649&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/155351aa15967b5346144c03fc87d1ca.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chris Donahue&quot;,&quot;user&quot;:&quot;chrisdonahue&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chris Donahue&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:27:59.520Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf0a8ded4c3eb970c864a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1fb8c80b60f21f65a0a027319101f236.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Percy Liang&quot;,&quot;user&quot;:&quot;percyliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Percy Liang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:28:15.548Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T16:27:53.000Z&quot;,&quot;title&quot;:&quot;Anticipatory Music Transformer&quot;,&quot;summary&quot;:&quot;We introduce anticipation: a method for constructing a controllable\ngenerative model of a temporal point process (the event process) conditioned\nasynchronously on realizations of a second, correlated process (the control\nprocess). We achieve this by interleaving sequences of events and controls,\nsuch that controls appear following stopping times in the event sequence. This\nwork is motivated by problems arising in the control of symbolic music\ngeneration. We focus on infilling control tasks, whereby the controls are a\nsubset of the events themselves, and conditional generation completes a\nsequence of events given the fixed control events. We train anticipatory\ninfilling models using the large and diverse Lakh MIDI music dataset. These\nmodels match the performance of autoregressive models for prompted music\ngeneration, with the additional capability to perform infilling control tasks,\nincluding accompaniment. Human evaluators report that an anticipatory model\nproduces accompaniments with similar musicality to even music composed by\nhumans over a 20-second clip.&quot;,&quot;upvotes&quot;:8},&quot;publishedAt&quot;:&quot;2023-06-16T05:18:33.974Z&quot;,&quot;title&quot;:&quot;Anticipatory Music Transformer&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FxI7nZ2uRna4A1c5JJo4b.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09327&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648be987335645aa03ede32d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/29cf7ec18f500349ca6c3654e195ab32.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Daniel McKee&quot;,&quot;user&quot;:&quot;dmckee5&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Daniel McKee&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:34:12.968Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be987335645aa03ede32e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/a762a03b753a7ad1839b4074221cfab6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Justin Salamon&quot;,&quot;user&quot;:&quot;salamon&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Justin Salamon&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-21T22:07:37.581Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be987335645aa03ede32f&quot;,&quot;name&quot;:&quot;Josef Sivic&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648be987335645aa03ede330&quot;,&quot;name&quot;:&quot;Bryan Russell&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:58:01.000Z&quot;,&quot;title&quot;:&quot;Language-Guided Music Recommendation for Video via Prompt Analogies&quot;,&quot;summary&quot;:&quot;We propose a method to recommend music for an input video while allowing a\nuser to guide music selection with free-form natural language. A key challenge\nof this problem setting is that existing music video datasets provide the\nneeded (video, music) training pairs, but lack text descriptions of the music.\nThis work addresses this challenge with the following three contributions.\nFirst, we propose a text-synthesis approach that relies on an analogy-based\nprompting procedure to generate natural language music descriptions from a\nlarge-scale language model (BLOOM-176B) given pre-trained music tagger outputs\nand a small number of human text descriptions. Second, we use these synthesized\nmusic descriptions to train a new trimodal model, which fuses text and video\ninput representations to query music samples. For training, we introduce a text\ndropout regularization mechanism which we show is critical to model\nperformance. Our model design allows for the retrieved music audio to agree\nwith the two input modalities by matching visual style depicted in the video\nand musical genre, mood, or instrumentation described in the natural language\nquery. Third, to evaluate our approach, we collect a testing dataset for our\nproblem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset\nwith natural language music descriptions which we make publicly available. We\nshow that our approach can match or exceed the performance of prior methods on\nvideo-to-music retrieval while significantly improving retrieval accuracy when\nusing text guidance.&quot;,&quot;upvotes&quot;:7},&quot;publishedAt&quot;:&quot;2023-06-16T04:48:09.220Z&quot;,&quot;title&quot;:&quot;Language-Guided Music Recommendation for Video via Prompt Analogies&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MGIEsAoxwAbUtb-RETJyf.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08893&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bd85c33603b7ec89a4cab&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e7c009e54424c5334a6ae2f20d1576fc.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Orr Zohar&quot;,&quot;user&quot;:&quot;orrzohar&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Orr Zohar&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:05:33.437Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd85c33603b7ec89a4cac&quot;,&quot;name&quot;:&quot;Shih-Cheng Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd85c33603b7ec89a4cad&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/45bcc3ee5d3231644fe58e4352afe735.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kuan-Chieh Wang&quot;,&quot;user&quot;:&quot;wangkua1&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kuan-Chieh Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T18:50:42.605Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bd85c33603b7ec89a4cae&quot;,&quot;name&quot;:&quot;Serena Yeung&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T06:53:05.000Z&quot;,&quot;title&quot;:&quot;LOVM: Language-Only Vision Model Selection&quot;,&quot;summary&quot;:&quot;Pre-trained multi-modal vision-language models (VLMs) are becoming\nincreasingly popular due to their exceptional performance on downstream vision\napplications, particularly in the few- and zero-shot settings. However,\nselecting the best-performing VLM for some downstream applications is\nnon-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive\nevaluation of all available VLMs on a novel application is not only time and\ncomputationally demanding but also necessitates the collection of a labeled\ndataset for evaluation. As the number of open-source VLM variants increases,\nthere is a need for an efficient model selection strategy that does not require\naccess to a curated evaluation dataset. This paper proposes a novel task and\nbenchmark for efficiently evaluating VLMs' zero-shot performance on downstream\napplications without access to the downstream task dataset. Specifically, we\nintroduce a new task LOVM: Language-Only Vision Model Selection, where methods\nare expected to perform both model selection and performance prediction based\nsolely on a text description of the desired downstream application. We then\nintroduced an extensive LOVM benchmark consisting of ground-truth evaluations\nof 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the\npre-trained VLMs and predict their zero-shot performance.&quot;,&quot;upvotes&quot;:7},&quot;publishedAt&quot;:&quot;2023-06-16T03:34:54.465Z&quot;,&quot;title&quot;:&quot;LOVM: Language-Only Vision Model Selection&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xq0HQ3p0J0yWnqodPKZ1i.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08068&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bf1bc335645aa03f03965&quot;,&quot;name&quot;:&quot;Allan Jabri&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf1bc335645aa03f03966&quot;,&quot;name&quot;:&quot;Sjoerd van Steenkiste&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf1bc335645aa03f03967&quot;,&quot;name&quot;:&quot;Emiel Hoogeboom&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf1bc335645aa03f03968&quot;,&quot;name&quot;:&quot;Mehdi S. M. Sajjadi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bf1bc335645aa03f03969&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6413572b6cd62eb3ba2024e9/UCtMdeV8_N6w-SBnTDXBy.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Thomas Kipf&quot;,&quot;user&quot;:&quot;tkipf&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Thomas Kipf&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T10:52:05.911Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T18:32:35.000Z&quot;,&quot;title&quot;:&quot;DORSal: Diffusion for Object-centric Representations of Scenes\n  et al.&quot;,&quot;summary&quot;:&quot;Recent progress in 3D scene understanding enables scalable learning of\nrepresentations across large datasets of diverse scenes. As a consequence,\ngeneralization to unseen scenes and objects, rendering novel views from just a\nsingle or a handful of input images, and controllable scene generation that\nsupports editing, is now possible. However, training jointly on a large number\nof scenes typically compromises rendering quality when compared to single-scene\noptimized models such as NeRFs. In this paper, we leverage recent progress in\ndiffusion models to equip 3D scene representation learning models with the\nability to render high-fidelity novel views, while retaining benefits such as\nobject-level scene editing to a large degree. In particular, we propose DORSal,\nwhich adapts a video diffusion architecture for 3D scene generation conditioned\non object-centric slot-based representations of scenes. On both complex\nsynthetic multi-object scenes and on the real-world large-scale Street View\ndataset, we show that DORSal enables scalable neural rendering of 3D scenes\nwith object-level editing and improves upon existing approaches.&quot;,&quot;upvotes&quot;:6},&quot;publishedAt&quot;:&quot;2023-06-16T05:23:10.676Z&quot;,&quot;title&quot;:&quot;DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DCmPeYfCS_gPjKLs0qk7Z.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08707&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648beeec10bced401c50e780&quot;,&quot;name&quot;:&quot;Paul Couairon&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648beeec10bced401c50e781&quot;,&quot;name&quot;:&quot;Clément Rambour&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648beeec10bced401c50e782&quot;,&quot;name&quot;:&quot;Jean-Emmanuel Haugeard&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648beeec10bced401c50e783&quot;,&quot;name&quot;:&quot;Nicolas Thome&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T19:15:49.000Z&quot;,&quot;title&quot;:&quot;VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing&quot;,&quot;summary&quot;:&quot;Recently, diffusion-based generative models have achieved remarkable success\nfor image generation and edition. However, their use for video editing still\nfaces important limitations. This paper introduces VidEdit, a novel method for\nzero-shot text-based video editing ensuring strong temporal and spatial\nconsistency. Firstly, we propose to combine atlas-based and pre-trained\ntext-to-image diffusion models to provide a training-free and efficient editing\nmethod, which by design fulfills temporal smoothness. Secondly, we leverage\noff-the-shelf panoptic segmenters along with edge detectors and adapt their use\nfor conditioned diffusion-based atlas editing. This ensures a fine spatial\ncontrol on targeted regions while strictly preserving the structure of the\noriginal video. Quantitative and qualitative experiments show that VidEdit\noutperforms state-of-the-art methods on DAVIS dataset, regarding semantic\nfaithfulness, image preservation, and temporal consistency metrics. With this\nframework, processing a single video only takes approximately one minute, and\nit can generate multiple compatible edits based on a unique text prompt.\nProject web-page at https://videdit.github.io&quot;,&quot;upvotes&quot;:6},&quot;publishedAt&quot;:&quot;2023-06-16T05:11:27.621Z&quot;,&quot;title&quot;:&quot;VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/RPYbrld2IZ0RAmVZff2nd.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09349&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc91aded16248631ede5c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/d51ccb2e2b3e9756dea3d06b6e62f438.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhi-Hao Lin&quot;,&quot;user&quot;:&quot;j1a0m0e4s&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhi-Hao Lin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:02:03.385Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede5d&quot;,&quot;name&quot;:&quot;Bohan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede5e&quot;,&quot;name&quot;:&quot;Yi-Ting Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede5f&quot;,&quot;name&quot;:&quot;David Forsyth&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede60&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5a2550d95e686640242840ad3bd0e680.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiabin Huang&quot;,&quot;user&quot;:&quot;YellowAddice&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jia-Bin Huang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:49:05.020Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede61&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1e433b7fb9d30b9f42dec487d658b1e3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Anand Bhattad&quot;,&quot;user&quot;:&quot;anandbhattad&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Anand Bhattad&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:04:59.622Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc91aded16248631ede62&quot;,&quot;name&quot;:&quot;Shenlong Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:59:59.000Z&quot;,&quot;title&quot;:&quot;UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video&quot;,&quot;summary&quot;:&quot;We show how to build a model that allows realistic, free-viewpoint renderings\nof a scene under novel lighting conditions from video. Our method -- UrbanIR:\nUrban Scene Inverse Rendering -- computes an inverse graphics representation\nfrom the video. UrbanIR jointly infers shape, albedo, visibility, and sun and\nsky illumination from a single video of unbounded outdoor scenes with unknown\nlighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many\nviews of the same points in typical NeRF-style estimation). As a result,\nstandard methods produce poor geometry estimates (for example, roofs), and\nthere are numerous ''floaters''. Errors in inverse graphics inference can\nresult in strong rendering artifacts. UrbanIR uses novel losses to control\nthese and other sources of error. UrbanIR uses a novel loss to make very good\nestimates of shadow volumes in the original scene. The resulting\nrepresentations facilitate controllable editing, delivering photorealistic\nfree-viewpoint renderings of relit scenes and inserted objects. Qualitative\nevaluation demonstrates strong improvements over the state-of-the-art.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-16T02:29:52.434Z&quot;,&quot;title&quot;:&quot;UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/knIRvXpaUBLGUU2iBUoW1.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09109&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648c044f335645aa03f51f55&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0b742ff094a09f9374fafcd97ab9e002.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Varun Jampani&quot;,&quot;user&quot;:&quot;varunjampani&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Varun Jampani&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:15:21.983Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f56&quot;,&quot;name&quot;:&quot;Kevis-Kokitsi Maninis&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f57&quot;,&quot;name&quot;:&quot;Andreas Engelhardt&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f58&quot;,&quot;name&quot;:&quot;Arjun Karpur&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f59&quot;,&quot;name&quot;:&quot;Karen Truong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/56a00b2e6211f678ec040ea379e7387a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kyle Sargent&quot;,&quot;user&quot;:&quot;ksahug&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kyle Sargent&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:17:08.825Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/3b1d8959207e2f518d80dbcaba7c0d96.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Stefan Popov&quot;,&quot;user&quot;:&quot;stefangp&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Stefan Popov&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T17:17:29.443Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5c&quot;,&quot;name&quot;:&quot;André Araujo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5d&quot;,&quot;name&quot;:&quot;Ricardo Martin-Brualla&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5e&quot;,&quot;name&quot;:&quot;Kaushal Patel&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f5f&quot;,&quot;name&quot;:&quot;Daniel Vlasic&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f60&quot;,&quot;name&quot;:&quot;Vittorio Ferrari&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f61&quot;,&quot;name&quot;:&quot;Ameesh Makadia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f62&quot;,&quot;name&quot;:&quot;Ce Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f63&quot;,&quot;name&quot;:&quot;Yuanzhen Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c044f335645aa03f51f64&quot;,&quot;name&quot;:&quot;Howard Zhou&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T13:11:30.000Z&quot;,&quot;title&quot;:&quot;NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and\n  Pose Annotations&quot;,&quot;summary&quot;:&quot;Recent advances in neural reconstruction enable high-quality 3D object\nreconstruction from casually captured image collections. Current techniques\nmostly analyze their progress on relatively simple image collections where\nStructure-from-Motion (SfM) techniques can provide ground-truth (GT) camera\nposes. We note that SfM techniques tend to fail on in-the-wild image\ncollections such as image search results with varying backgrounds and\nilluminations. To enable systematic research progress on 3D reconstruction from\ncasual image captures, we propose NAVI: a new dataset of category-agnostic\nimage collections of objects with high-quality 3D scans along with per-image\n2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D\nalignments allow us to extract accurate derivative annotations such as dense\npixel correspondences, depth and segmentation maps. We demonstrate the use of\nNAVI image collections on different problem settings and show that NAVI enables\nmore thorough evaluations that were not possible with existing datasets. We\nbelieve NAVI is beneficial for systematic research progress on 3D\nreconstruction and correspondence estimation. Project page:\nhttps://navidataset.github.io&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-16T06:42:27.848Z&quot;,&quot;title&quot;:&quot;NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/M4SWP9uE4UaF6Ygx2xrFh.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08133&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bed8a4b8596d017f16759&quot;,&quot;name&quot;:&quot;Tongzhou Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675a&quot;,&quot;name&quot;:&quot;Cyril Allauzen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675b&quot;,&quot;name&quot;:&quot;Yinghui Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675c&quot;,&quot;name&quot;:&quot;Daniel Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675d&quot;,&quot;name&quot;:&quot;David Rybach&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675e&quot;,&quot;name&quot;:&quot;W. Ronny Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f1675f&quot;,&quot;name&quot;:&quot;Rodrigo Cabrera&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f16760&quot;,&quot;name&quot;:&quot;Kartik Audhkhasi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f16761&quot;,&quot;name&quot;:&quot;Bhuvana Ramabhadran&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f16762&quot;,&quot;name&quot;:&quot;Pedro J. Moreno&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bed8a4b8596d017f16763&quot;,&quot;name&quot;:&quot;Michael Riley&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T20:54:12.000Z&quot;,&quot;title&quot;:&quot;Large-scale Language Model Rescoring on Long-form Data&quot;,&quot;summary&quot;:&quot;In this work, we study the impact of Large-scale Language Models (LLM) on\nAutomated Speech Recognition (ASR) of YouTube videos, which we use as a source\nfor long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error\nEate (WER) on US English (en-us) and code-switched Indian English (en-in)\nlong-form ASR test sets and a reduction of up to 30\\% relative on Salient Term\nError Rate (STER) over a strong first-pass baseline that uses a maximum-entropy\nbased language model. Improved lattice processing that results in a lattice\nwith a proper (non-tree) digraph topology and carrying context from the 1-best\nhypothesis of the previous segment(s) results in significant wins in rescoring\nwith LLMs. We also find that the gains in performance from the combination of\nLLMs trained on vast quantities of available data (such as C4) and conventional\nneural LMs is additive and significantly outperforms a strong first-pass\nbaseline with a maximum entropy LM.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-16T05:05:14.619Z&quot;,&quot;title&quot;:&quot;Large-scale Language Model Rescoring on Long-form Data&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vAUF7w7vKrq7FCj2tpv58.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08129&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d95&quot;,&quot;name&quot;:&quot;Ziniu Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d96&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/c3e2bbe793cdd146aea27c0011aace71.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ahmet Iscen&quot;,&quot;user&quot;:&quot;ahmetius&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ahmet Iscen&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:14:37.813Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d97&quot;,&quot;name&quot;:&quot;Chen Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d98&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1622653364258-noauth.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kai-Wei Chang&quot;,&quot;user&quot;:&quot;kaiweichang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kai-Wei Chang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:15:59.421Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d99&quot;,&quot;name&quot;:&quot;Yizhou Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d9a&quot;,&quot;name&quot;:&quot;David A Ross&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d9b&quot;,&quot;name&quot;:&quot;Cordelia Schmid&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc615d0f0f5b02eaf5d9c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b83e043784b6db225ae4d0422aabbb3a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;alireza fathi&quot;,&quot;user&quot;:&quot;alireza007&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alireza Fathi&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-16T16:16:51.620Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T20:50:22.000Z&quot;,&quot;title&quot;:&quot;AVIS: Autonomous Visual Information Seeking with Large Language Models&quot;,&quot;summary&quot;:&quot;In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \&quot;What event is commemorated by the building\ndepicted in this image?\&quot;, is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-16T02:16:58.065Z&quot;,&quot;title&quot;:&quot;AVIS: Autonomous Visual Information Seeking with Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d5qfrEefTEMl-JaKN18vk.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.09322&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b771f&quot;,&quot;name&quot;:&quot;Shizhan Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b7720&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1681e465d7649f67f94e1b69d236cb1e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shunsuke Saito&quot;,&quot;user&quot;:&quot;psyth&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shunsuke Saito&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T11:50:06.742Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b7721&quot;,&quot;name&quot;:&quot;Aljaz Bozic&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b7722&quot;,&quot;name&quot;:&quot;Carlos Aliaga&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b7723&quot;,&quot;name&quot;:&quot;Trevor Darrell&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648c000c5bfc81b14f4b7724&quot;,&quot;name&quot;:&quot;Christop Lassner&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-15T17:56:04.000Z&quot;,&quot;title&quot;:&quot;Neural Relighting with Subsurface Scattering by Learning the Radiance\n  Transfer Gradient&quot;,&quot;summary&quot;:&quot;Reconstructing and relighting objects and scenes under varying lighting\nconditions is challenging: existing neural rendering methods often cannot\nhandle the complex interactions between materials and light. Incorporating\npre-computed radiance transfer techniques enables global illumination, but\nstill struggles with materials with subsurface scattering effects. We propose a\nnovel framework for learning the radiance transfer field via volume rendering\nand utilizing various appearance cues to refine geometry end-to-end. This\nframework extends relighting and reconstruction capabilities to handle a wider\nrange of materials in a data-driven fashion. The resulting models produce\nplausible rendering results in existing and novel conditions. We will release\nour code and a novel light stage dataset of objects with subsurface scattering\neffects publicly available.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-16T06:24:15.550Z&quot;,&quot;title&quot;:&quot;Neural Relighting with Subsurface Scattering by Learning the Radiance Transfer Gradient&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/69iBueEHKQUyDYQSnLBy0.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08651&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdcc9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/9a3ec029036403c3c57428c98fa1501c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minae Kwon&quot;,&quot;user&quot;:&quot;minaek&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Minae Kwon&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T11:48:18.565Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdcca&quot;,&quot;name&quot;:&quot;Hengyuan Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdccb&quot;,&quot;name&quot;:&quot;Vivek Myers&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdccc&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634666324094-6150b090d84cf0532aa1764b.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Siddharth Karamcheti&quot;,&quot;user&quot;:&quot;skaramcheti&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Siddharth Karamcheti&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-19T11:48:50.569Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdccd&quot;,&quot;name&quot;:&quot;Anca Dragan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bcf5ad60140c4bbccdcce&quot;,&quot;name&quot;:&quot;Dorsa Sadigh&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-14T17:30:57.000Z&quot;,&quot;title&quot;:&quot;Toward Grounded Social Reasoning&quot;,&quot;summary&quot;:&quot;Consider a robot tasked with tidying a desk with a meticulously constructed\nLego sports car. A human may recognize that it is not socially appropriate to\ndisassemble the sports car and put it away as part of the \&quot;tidying\&quot;. How can a\nrobot reach that conclusion? Although large language models (LLMs) have\nrecently been used to enable social reasoning, grounding this reasoning in the\nreal world has been challenging. To reason in the real world, robots must go\nbeyond passively querying LLMs and *actively gather information from the\nenvironment* that is required to make the right decision. For instance, after\ndetecting that there is an occluded car, the robot may need to actively\nperceive the car to know whether it is an advanced model car made out of Legos\nor a toy car built by a toddler. We propose an approach that leverages an LLM\nand vision language model (VLM) to help a robot actively perceive its\nenvironment to perform grounded social reasoning. To evaluate our framework at\nscale, we release the MessySurfaces dataset which contains images of 70\nreal-world surfaces that need to be cleaned. We additionally illustrate our\napproach with a robot on 2 carefully designed surfaces. We find an average\n12.9% improvement on the MessySurfaces benchmark and an average 15% improvement\non the robot experiments over baselines that do not use active perception. The\ndataset, code, and videos of our approach can be found at\nhttps://minaek.github.io/groundedsocialreasoning.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-16T02:56:30.392Z&quot;,&quot;title&quot;:&quot;Toward Grounded Social Reasoning&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3W0eGW1OC3O2vJKslGe74.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.08055&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648bc57995d85bfac02180d9&quot;,&quot;name&quot;:&quot;Abraham J. Fetterman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180da&quot;,&quot;name&quot;:&quot;Ellie Kitanidis&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180db&quot;,&quot;name&quot;:&quot;Joshua Albrecht&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180dc&quot;,&quot;name&quot;:&quot;Zachary Polizzi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180dd&quot;,&quot;name&quot;:&quot;Bryden Fogelman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180de&quot;,&quot;name&quot;:&quot;Maksis Knutins&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180df&quot;,&quot;name&quot;:&quot;Bartosz Wróblewski&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180e0&quot;,&quot;name&quot;:&quot;James B. Simon&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648bc57995d85bfac02180e1&quot;,&quot;name&quot;:&quot;Kanjun Qiu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-13T18:22:24.000Z&quot;,&quot;title&quot;:&quot;Tune As You Scale: Hyperparameter Optimization For Compute Efficient\n  Training&quot;,&quot;summary&quot;:&quot;Hyperparameter tuning of deep learning models can lead to order-of-magnitude\nperformance gains for the same amount of compute. Despite this, systematic\ntuning is uncommon, particularly for large models, which are expensive to\nevaluate and tend to have many hyperparameters, necessitating difficult\njudgment calls about tradeoffs, budgets, and search bounds. To address these\nissues and propose a practical method for robustly tuning large models, we\npresent Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian\noptimization algorithm that performs local search around the performance-cost\nPareto frontier. CARBS does well even in unbounded search spaces with many\nhyperparameters, learns scaling relationships so that it can tune models even\nas they are scaled up, and automates much of the \&quot;black magic\&quot; of tuning. Among\nour results, we effectively solve the entire ProcGen benchmark just by tuning a\nsimple baseline (PPO, as provided in the original ProcGen paper). We also\nreproduce the model size vs. training tokens scaling result from the Chinchilla\nproject (Hoffmann et al. 2022), while simultaneously discovering scaling laws\nfor every other hyperparameter, via an easy automated process that uses\nsignificantly less compute and is applicable to any deep learning problem (not\njust language models).&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-16T02:14:20.707Z&quot;,&quot;title&quot;:&quot;Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kThLvw5FZJoF2dyZuooMe.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false}],&quot;lastDate&quot;:&quot;2024-03-19T05:05:26.926Z&quot;,&quot;nextDate&quot;:&quot;2023-06-19&quot;,&quot;prevDate&quot;:&quot;2023-06-14&quot;,&quot;publisher&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;}}" data-target="DailyPapers"><section class="container relative mb-20 mt-8 md:mt-14"><div class="mb-12 grid grid-cols-2 items-start md:mb-16 md:grid-cols-3"><div class="md:pl-4"><h1 class="text-2xl font-bold md:text-3xl"><a href="/papers" class="hover:text-gray-600 dark:hover:text-gray-300">Daily Papers</a></h1> <div class="flex flex-wrap items-center gap-2 text-gray-500"><h2 class="flex items-center gap-2.5 text-xl">by <a href="/akhaliq" class="flex items-center gap-2"><img alt="" class="h-4 w-4 rounded-full ring-2 ring-white dark:ring-gray-900" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face"><span class="underline">AK</span></a></h2></div></div> <div class="order-last col-span-2 mt-6 md:order-none md:col-span-1 md:mt-0"><button class="mx-auto flex w-full translate-y-1 items-center justify-center rounded-full border py-1 text-gray-400 shadow-sm hover:shadow-inner md:w-80" title="Search papers"><svg class="mr-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				Search by arxiv id or title</button></div> <div class="flex items-stretch justify-end"><a href="/papers?date=2023-06-14" class="group my-0.5 -mr-2 flex w-8 items-center justify-center rounded-l-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 -translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a> <time class="relative flex flex-col items-stretch" datetime="2023-06-16T00:00:00.000Z"><span class="rounded-t-lg bg-gray-500 px-3.5 py-0.5 text-center text-xs font-bold uppercase leading-none text-white dark:bg-gray-700">Jun</span> <span class="rounded-b-lg bg-gray-100 px-3.5 py-1 text-center text-xl font-semibold text-gray-800 dark:bg-gradient-to-br dark:from-gray-800 dark:to-gray-950">15</span></time> <a href="/papers?date=2023-06-19" class="group my-0.5 -ml-2 flex w-8 items-center justify-center rounded-r-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 rotate-180 translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a></div></div> <div class="relative grid grid-cols-1 gap-14 lg:grid-cols-2"><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xyJguN1HCd_gEblq7byX-.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08276" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">69</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08276" class="cursor-pointer">TryOnDiffusion: A Tale of Two UNets</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08276" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="William Chan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Fitsum Reda" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Tyler Zhu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="DaweiYang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/aab6be8626d9a2a87f58c8a5803c72f0.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lyzhu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6b2925901e36fddb95760e3cbd20d8d7.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> <a href="/papers/2306.08276#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 6</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09348" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pbKOx4b3d30XsvRN9AVVo.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09348" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">30</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09348" class="cursor-pointer">Seeing the World through Your Eyes</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09348" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="YellowAddice" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5a2550d95e686640242840ad3bd0e680.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="strikemetz" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0659d41aab160b3997f52c9c8fd5f2c1.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="brandonyfeng" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/3e05289d1876d7b7ef2c6a595a7f39d7.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="kzhang0" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5c90681d63cb3297176b1a68d024cc39.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="HadiZayer" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/93072cde421c9d7d25614f967a9aa45a.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> <a href="/papers/2306.09348#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08568" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pOqOag3cCyJRL1LnC15B0.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08568" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">26</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08568" class="cursor-pointer">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08568" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="chongyang09" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/7a640980ed225e31117f750e09bcc18c.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="wenxcs" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/4daea27b432ff428c41d590669f7330c.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="WizardLM" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627b9f3f4d0858f0034efbb9/2Qnattrzv6qvqiZVVfV5x.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="nlpxucan" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/98d6610a3cd17a27a4201ec926c0e7ff.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ziyang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090ff099a8bcaa437b234a4/nxN64gFkr5o5UZM-i03re.png?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 10 authors</div></li></ul></a> <a href="/papers/2306.08568#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/z1JT5nfgc-6JhJrtDRfXV.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08640" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">25</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08640" class="cursor-pointer">AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08640" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joya Chen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Luowei Zhou" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Difei Gao" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="KevinQHLin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="AlexJi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/3369af1f4e16dfaf727f78e63b21a7ad.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> <a href="/papers/2306.08640#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09296" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ctllsIWgk2GiA2BPQFS4N.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09296" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">18</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09296" class="cursor-pointer">KoLA: Carefully Benchmarking World Knowledge of Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09296" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="RicardoL1u" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="bys0318" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="TranSirius" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="wangxz098" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/47d71d80f9901313feb0199c37296389.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="JovanYu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/fba1fb87bcac340a8eee3b9f4fc35bc5.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 35 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08161" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Po1LsyKAB-X3XZCUPXnvB.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08161" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">18</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08161" class="cursor-pointer">h2oGPT: Democratizing Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08161" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="shivam-h2o" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/c2ab0d46514b9e81926990ea0a032531.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lo5" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f1d71e9d06393691191a0/TwhM8xiUUfK6EV9rGrh5x.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="MaxJeblick" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674725099569-614c9145b44c102499783617.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ilu000" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="arnocandel" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 15 authors</div></li></ul></a> <a href="/papers/2306.08161#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 3</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08543" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/m0dbqjbl790Fi6SKI-aa2.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08543" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">16</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08543" class="cursor-pointer">Knowledge Distillation of Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08543" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Minlie Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Li Dong" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="thegenerality" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="t1101675" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/8de6e319246500c460cf41163462c214.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09329" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gIWnWezBrscS3RrhZeAdb.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09329" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">14</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09329" class="cursor-pointer">DreamHuman: Animatable 3D Avatars from Text</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09329" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Mihai Fieraru" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Eduard Gabriel Bazavan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andreiz" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/c8bd4b51155cd37fb7e76c97b843d461.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="thiemoall" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648c964d39d2584ee47af19c/5UEkzDTMY8I3svKjR7kxN.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="kolotouros" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/53163398f1a629b9838548b808af53f7.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> <a href="/papers/2306.09329#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09093" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sHXt38101YyXqmWqRFNsO.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09093" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">12</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09093" class="cursor-pointer">Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09093" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chenyang Lyu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shuming" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/971763f3b724d63bc64b7d3599cfc753.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="bsliu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/3941abd3cfdb68212db6106e2ce521ed.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="longyuewang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/15d5d5403fef2f1368bb4185b199061d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="minghaowu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/db64e2d2ed905e4f1c187c046fa2948d.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> <a href="/papers/2306.09093#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 4</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9Iy5lzF81D8qn7DT-suOk.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08647" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">10</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08647" class="cursor-pointer">Language to Rewards for Robotic Skill Synthesis</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08647" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chuyuan Fu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Nimrod Gileadi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Wenhao Yu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="fxia22" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/8bfe3bf63f0609b3b6b7d7c5d7f6b3e9.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="skirmani" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/bdde73b95a36e32cb2975656cca46022.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 20 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08997" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3LgRK-8RX5uJzoPXdJGod.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08997" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">10</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08997" class="cursor-pointer">Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08997" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yoon-kim" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b282162d6d9ae8512873eba0275f603e.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="nsingh1" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e436988005ad4f9d99fcafb8bdfade9c.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="eamonniknafs" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/cdad9af0749a52945116b385e2c2591b.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="arielnlee" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638bcfa91987d67b340e6c1c/3tHCB_J6c4-lsEZ_zJSlp.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sflorin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/971c17600395b1037f676710b32b3909.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 15 authors</div></li></ul></a> <a href="/papers/2306.08997#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qg1jrH5qKWi4Jgso5QPVE.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08205" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">9</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08205" class="cursor-pointer">Agile Catching with Whole-Body MPC and Blackbox Policy Learning</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08205" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Krzysztof Choromanski" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Saminda Abeyruwan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jaindeepali" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ad3e078c6df8b319ef5fa85e556f8a3c.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="nmboffi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/11aa33b32ef4b6aa27ab8e6c068afa14.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="bewley" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e1d8be8bb68200230cdf05497479e6f4.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 12 authors</div></li></ul></a> <a href="/papers/2306.08205#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09200" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7rWRuAMvKjGDaaGjOL8O-.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09200" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">9</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09200" class="cursor-pointer">ChessGPT: Bridging Policy Learning and Language Modeling</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09200" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hongrui Tang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="YaliDU" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/f96a4d375ea44d10891f4b47b7d7986e.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ziyan98" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/f5ef14e60f06dd99e4c6ff4df0f225c4.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ethanluoyc" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/8936667d7c6200ad3058f22329df5542.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Waterhorse" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/3309f1c8f1340a93e8b7a22fb38069fa.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 9 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09316" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QNSzM2VCdNxOU3H3QF0aL.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09316" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">8</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09316" class="cursor-pointer">Diffusion Models for Zero-Shot Open-Vocabulary Segmentation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09316" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Christian Rupprecht" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andrea Vedaldi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Iro Laina" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lkarazija" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/85840faf1055ec2fcae495db6f042a27.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08620" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FxI7nZ2uRna4A1c5JJo4b.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08620" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">8</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08620" class="cursor-pointer">Anticipatory Music Transformer</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08620" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="percyliang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1fb8c80b60f21f65a0a027319101f236.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="chrisdonahue" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/155351aa15967b5346144c03fc87d1ca.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="dlwh" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/308ec832279f118f5e981cbf392d798b.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jthickstun" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/619828ff465b7973a532dee6/A55HdKmHcZ9w6BxN-cGoE.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MGIEsAoxwAbUtb-RETJyf.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09327" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">7</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09327" class="cursor-pointer">Language-Guided Music Recommendation for Video via Prompt Analogies</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09327" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Bryan Russell" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Josef Sivic" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="salamon" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/a762a03b753a7ad1839b4074221cfab6.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="dmckee5" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/29cf7ec18f500349ca6c3654e195ab32.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08893" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xq0HQ3p0J0yWnqodPKZ1i.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08893" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">7</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08893" class="cursor-pointer">LOVM: Language-Only Vision Model Selection</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08893" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Serena Yeung" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shih-Cheng Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="wangkua1" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/45bcc3ee5d3231644fe58e4352afe735.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="orrzohar" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e7c009e54424c5334a6ae2f20d1576fc.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08068" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DCmPeYfCS_gPjKLs0qk7Z.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08068" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">6</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08068" class="cursor-pointer">DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08068" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Mehdi S. M. Sajjadi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Emiel Hoogeboom" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Sjoerd van Steenkiste" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Allan Jabri" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tkipf" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6413572b6cd62eb3ba2024e9/UCtMdeV8_N6w-SBnTDXBy.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08707" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/RPYbrld2IZ0RAmVZff2nd.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08707" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">6</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08707" class="cursor-pointer">VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08707" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Nicolas Thome" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jean-Emmanuel Haugeard" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Clément Rambour" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Paul Couairon" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09349" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/knIRvXpaUBLGUU2iBUoW1.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09349" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09349" class="cursor-pointer">UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09349" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yi-Ting Chen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Bohan Liu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="anandbhattad" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1e433b7fb9d30b9f42dec487d658b1e3.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="YellowAddice" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5a2550d95e686640242840ad3bd0e680.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="j1a0m0e4s" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/d51ccb2e2b3e9756dea3d06b6e62f438.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09109" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/M4SWP9uE4UaF6Ygx2xrFh.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09109" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09109" class="cursor-pointer">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09109" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Andreas Engelhardt" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kevis-Kokitsi Maninis" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="stefangp" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/3b1d8959207e2f518d80dbcaba7c0d96.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ksahug" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/56a00b2e6211f678ec040ea379e7387a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="varunjampani" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0b742ff094a09f9374fafcd97ab9e002.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 16 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08133" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vAUF7w7vKrq7FCj2tpv58.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08133" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08133" class="cursor-pointer">Large-scale Language Model Rescoring on Long-form Data</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08133" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="David Rybach" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Daniel Park" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yinghui Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Cyril Allauzen" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Tongzhou Chen" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 11 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08129" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d5qfrEefTEMl-JaKN18vk.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08129" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08129" class="cursor-pointer">AVIS: Autonomous Visual Information Seeking with Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08129" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chen Sun" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ziniu Hu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="alireza007" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b83e043784b6db225ae4d0422aabbb3a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="kaiweichang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1622653364258-noauth.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ahmetius" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/c3e2bbe793cdd146aea27c0011aace71.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.09322" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/69iBueEHKQUyDYQSnLBy0.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.09322" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.09322" class="cursor-pointer">Neural Relighting with Subsurface Scattering by Learning the Radiance Transfer Gradient</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.09322" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Trevor Darrell" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Carlos Aliaga" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Aljaz Bozic" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shizhan Zhu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="psyth" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1681e465d7649f67f94e1b69d236cb1e.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08651" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3W0eGW1OC3O2vJKslGe74.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08651" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08651" class="cursor-pointer">Toward Grounded Social Reasoning</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08651" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Anca Dragan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Vivek Myers" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hengyuan Hu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="skaramcheti" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634666324094-6150b090d84cf0532aa1764b.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="minaek" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/9a3ec029036403c3c57428c98fa1501c.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.08055" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kThLvw5FZJoF2dyZuooMe.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.08055" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.08055" class="cursor-pointer">Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.08055" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Bryden Fogelman" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zachary Polizzi" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joshua Albrecht" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ellie Kitanidis" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Abraham J. Fetterman" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 9 authors</div></li></ul></a> </div></div></div></div></article> </div> <div class="col-span-1 flex lg:col-span-2"><a class="btn gap-2" href="/papers?date=2023-06-14"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z" fill="currentColor"></path></svg>Previous</a> <a class="btn ml-auto gap-2" href="/papers?date=2023-06-19">Next<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z" fill="currentColor"></path></svg></a></div></div></section> </div></main>
	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="font-semibold text-black md:hidden">Company</div>
		<div class="order-last pt-6 text-gray-400 md:order-none md:pt-0" href="Terms">© Hugging Face</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="group order-first flex-none pb-6 md:order-none md:pb-0"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="pt-6 font-semibold text-black md:hidden md:pt-0">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("/front/build/kube-351e67d/index.js");
			window.moonSha = "kube-351e67d/";
			window.hubConfig = JSON.parse(`{"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https://huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetsServerPublicUrl":"https://datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)"}`);
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>

		<!-- Google analytics v4 -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL";
				script.async = true;
				document.head.appendChild(script);

				window.dataLayer = window.dataLayer || [];
				function gtag() {
					if (window.dataLayer !== undefined) {
						window.dataLayer.push(arguments);
					}
				}
				gtag("js", new Date());
				gtag("config", "G-8Q63TH4CSL", { page_path: "/papers" });
				/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages
				gtag("consent", "default", { ad_storage: "denied", analytics_storage: "denied" });
				/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent
				/// TODO: ask the user for their consent and update this with gtag('consent', 'update')
			}
		</script>
	

<iframe name="__privateStripeMetricsController9720" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="https://js.stripe.com/v3/m-outer-3437aaddcdf6922d623e172c2d6f9278.html#url=https%3A%2F%2Fhuggingface.co%2Fpapers%3Fdate%3D2023-06-16&amp;title=Daily%20Papers%20-%20Hugging%20Face&amp;referrer=&amp;muid=NA&amp;sid=NA&amp;version=6&amp;preview=false" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe></body></html>