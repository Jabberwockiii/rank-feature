<!DOCTYPE html><html class=""><head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<meta name="description" content="Your daily dose of AI research from AK">
		<meta property="fb:app_id" content="1321688464574422">
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@huggingface">
		<meta property="og:title" content="Daily Papers - Hugging Face">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://huggingface.co/papers">
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/papers.png">

		<link rel="stylesheet" href="/front/build/kube-351e67d/style.css">

		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&amp;display=swap" rel="stylesheet">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		  

		<title>Daily Papers - Hugging Face</title>

		<script defer="" data-domain="huggingface.co" src="/js/script.js"></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer=""></script>
	<script src="https://js.stripe.com/v3/" async=""></script><script src="https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapersBannerSubscribe-98f5dbb5.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconCheckmarkFilled-0fb1cbef.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/DailyPapers-ff82993e.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/Contributors-aac8a263.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/autoplay-4f99a53d.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconMessage-6ab20750.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconArrowLeft-a638f296.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/ModalBody-205aeb00.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/urlWatcher-b1dcfbe0.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-79e4fb58.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconSpinner-f6a85825.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UpvoteControl-821d7cbb.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconUpvoteFilled-f11951bc.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/UsersListModal-9fa7b704.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/FollowButton-18e671ce.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/index-997dbc18.js"><link rel="modulepreload" as="script" crossorigin="" href="/front/build/kube-351e67d/IconBellWatching-37394c1a.js"><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"></head>
	<body class="flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black DailyPapersPage">
		<div class="flex min-h-screen flex-col">
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false}" data-target="MainHeader"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg"> <span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a> <div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..." spellcheck="false" type="text"> <svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> </div> <div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg> </button> </div></div> <nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 xl:space-x-2"><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Models</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> Datasets</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg> Spaces</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg> Posts</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg> Docs</a></li> <li class="max-2xl:hidden"><div class="relative "><button class="px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center " type="button"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z" fill="currentColor"></path></svg> Solutions </button> </div></li> <li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/pricing">Pricing</a></li> <li><div class="relative group"><button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button"><svg class="mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>  </button> </div></li> <li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li> <li><a class="block cursor-pointer px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/login">Log In</a></li> <li><a class="rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up</a></li></ul></nav></div></header></div>
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="GoogleAnalyticsTracker"></div>
	
	
	<div class="SVELTE_HYDRATER contents" data-props="{}" data-target="SSOBanner"></div>
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-props="{&quot;isLoggedIn&quot;:false}" data-target="DailyPapersBannerSubscribe"><div class="-mt-px flex h-9 w-full justify-center text-gray-600"><svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z"></path></svg> <div class="flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1"><div class="rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600">new</div> <p class="hidden sm:inline">Get trending papers in your email inbox once a day!</p> <p class="inline sm:hidden">Get trending papers in your email inbox!</p> <a href="/login?next=%2Fpapers" class="btn !px-2 text-sm leading-none">Subscribe</a></div> <svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z"></path></svg></div></div>
	<div class="SVELTE_HYDRATER contents" data-props="{&quot;date&quot;:&quot;2023-06-09T00:00:00.000Z&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05284&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8592&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/49f08d989ca505ae01bce5578a94f6fe.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jade Copet&quot;,&quot;user&quot;:&quot;JadeCopet&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jade Copet&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:29:28.726Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8593&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IOWMd17Iwls0dXsY1OWjK.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Felix Kreuk&quot;,&quot;user&quot;:&quot;felixkreuk&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Felix Kreuk&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T09:31:22.026Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8594&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/73519deba3176be9c23d49f749aee5da.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Itai Gat&quot;,&quot;user&quot;:&quot;itaigat&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Itai Gat&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T09:31:18.811Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8595&quot;,&quot;name&quot;:&quot;Tal Remez&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8596&quot;,&quot;name&quot;:&quot;David Kant&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8597&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b7ccbddfa745db854dc342be1327cd53.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gabriel Synnaeve&quot;,&quot;user&quot;:&quot;gsynnaeve&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Gabriel Synnaeve&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T09:31:28.583Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8598&quot;,&quot;name&quot;:&quot;Yossi Adi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829a5f709d2cdcac4a8599&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666708948380-noauth.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexandre Défossez&quot;,&quot;user&quot;:&quot;adefossez&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alexandre Défossez&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T15:31:53.230Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T15:31:05.000Z&quot;,&quot;title&quot;:&quot;Simple and Controllable Music Generation&quot;,&quot;summary&quot;:&quot;We tackle the task of conditional music generation. We introduce MusicGen, a\nsingle Language Model (LM) that operates over several streams of compressed\ndiscrete music representation, i.e., tokens. Unlike prior work, MusicGen is\ncomprised of a single-stage transformer LM together with efficient token\ninterleaving patterns, which eliminates the need for cascading several models,\ne.g., hierarchically or upsampling. Following this approach, we demonstrate how\nMusicGen can generate high-quality samples, while being conditioned on textual\ndescription or melodic features, allowing better controls over the generated\noutput. We conduct extensive empirical evaluation, considering both automatic\nand human studies, showing the proposed approach is superior to the evaluated\nbaselines on a standard text-to-music benchmark. Through ablation studies, we\nshed light over the importance of each of the components comprising MusicGen.\nMusic samples, code, and models are available at\nhttps://github.com/facebookresearch/audiocraft.&quot;,&quot;upvotes&quot;:123},&quot;publishedAt&quot;:&quot;2023-06-09T03:19:59.863Z&quot;,&quot;title&quot;:&quot;Simple and Controllable Music Generation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/EZZk6ml6GhwlzSahUlnge.png&quot;,&quot;numComments&quot;:22,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05425&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f274&quot;,&quot;name&quot;:&quot;Bo Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f275&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhang Yuanhan&quot;,&quot;user&quot;:&quot;ZhangYuanhan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuanhan Zhang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:32:49.378Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f276&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Liangyu Chen&quot;,&quot;user&quot;:&quot;liangyuch&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Liangyu Chen&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:12:26.078Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f277&quot;,&quot;name&quot;:&quot;Jinghao Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f278&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/_vJC0zeVOIvaNV2R6toqg.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Pu Fanyi&quot;,&quot;user&quot;:&quot;pufanyi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Fanyi Pu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:33:45.435Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f279&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingkang Yang&quot;,&quot;user&quot;:&quot;Jingkang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jingkang Yang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:34:01.256Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f27a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/430560ec2c2547f819225769ab432f30.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chunyuan Li&quot;,&quot;user&quot;:&quot;Chunyuan24&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chunyuan Li&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-11T05:13:22.469Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a1fa93362a0d1216f27b&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ziwei Liu&quot;,&quot;user&quot;:&quot;liuziwei7&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ziwei Liu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:34:25.660Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:56.000Z&quot;,&quot;title&quot;:&quot;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&quot;,&quot;summary&quot;:&quot;High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.&quot;,&quot;upvotes&quot;:9},&quot;publishedAt&quot;:&quot;2023-06-09T03:52:28.724Z&quot;,&quot;title&quot;:&quot;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/niAEeYP_jD85khS61oIu2.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05422&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482c29c99110097e5490fe1&quot;,&quot;name&quot;:&quot;Qianqian Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1cb7f8d4062838710365b79b2f1d595a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yen-Yu Chang&quot;,&quot;user&quot;:&quot;KAXY&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yen-Yu Chang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:04:56.683Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe3&quot;,&quot;name&quot;:&quot;Ruojin Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe4&quot;,&quot;name&quot;:&quot;Zhengqi Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe5&quot;,&quot;name&quot;:&quot;Bharath Hariharan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/85d94b4022577747b8d2d10a82c2f3c7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aleksander Holynski&quot;,&quot;user&quot;:&quot;holynski&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Aleksander Holynski&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:05:46.990Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c29c99110097e5490fe7&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e2ca585398a3212b2956a14804d0ba67.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Noah Snavely&quot;,&quot;user&quot;:&quot;jimantha&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Noah Snavely&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:06:03.310Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:29.000Z&quot;,&quot;title&quot;:&quot;Tracking Everything Everywhere All at Once&quot;,&quot;summary&quot;:&quot;We present a new test-time optimization method for estimating dense and\nlong-range motion from a video sequence. Prior optical flow or particle video\ntracking algorithms typically operate within limited temporal windows,\nstruggling to track through occlusions and maintain global consistency of\nestimated motion trajectories. We propose a complete and globally consistent\nmotion representation, dubbed OmniMotion, that allows for accurate, full-length\nmotion estimation of every pixel in a video. OmniMotion represents a video\nusing a quasi-3D canonical volume and performs pixel-wise tracking via\nbijections between local and canonical space. This representation allows us to\nensure global consistency, track through occlusions, and model any combination\nof camera and object motion. Extensive evaluations on the TAP-Vid benchmark and\nreal-world footage show that our approach outperforms prior state-of-the-art\nmethods by a large margin both quantitatively and qualitatively. See our\nproject page for more results: http://omnimotion.github.io/&quot;,&quot;upvotes&quot;:8},&quot;publishedAt&quot;:&quot;2023-06-09T06:11:41.875Z&quot;,&quot;title&quot;:&quot;Tracking Everything Everywhere All at Once&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WJW-d7rlY52prwX6VjViL.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05424&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64828c62e4bbb1c2dd322d1e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kmCo6brXhQ7SyxVx3lpsx.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Muhammad Maaz&quot;,&quot;user&quot;:&quot;mmaaz60&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Muhammad Maaz&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:12:33.798Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828c62e4bbb1c2dd322d1f&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hanoona Rasheed&quot;,&quot;user&quot;:&quot;Hanoona&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hanoona Rasheed&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:39:27.933Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828c62e4bbb1c2dd322d20&quot;,&quot;name&quot;:&quot;Salman Khan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828c62e4bbb1c2dd322d21&quot;,&quot;name&quot;:&quot;Fahad Shahbaz Khan&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:56.000Z&quot;,&quot;title&quot;:&quot;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and\n  Language Models&quot;,&quot;summary&quot;:&quot;Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the underexplored\nfield of video-based conversation by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with a LLM. The\nmodel is capable of understanding and generating human-like conversations about\nvideos. We introduce a new dataset of 100,000 video-instruction pairs used to\ntrain Video-ChatGPT acquired via manual and semi-automated pipeline that is\neasily scalable and robust to label noise. We also develop a quantiative\nevaluation framework for video-based dialogue models to objectively analyse the\nstrengths and weaknesses of proposed models. Our code, models, instruction-sets\nand demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.&quot;,&quot;upvotes&quot;:6},&quot;publishedAt&quot;:&quot;2023-06-09T02:20:20.315Z&quot;,&quot;title&quot;:&quot;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nrf864ImEaaP4pD8phJqs.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05399&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482aabe85ce4d2973d47af0&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6305b47efca1d8d92b82e0b4/6G1QlXHSeiSx9rPnUvjhz.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiachen Li&quot;,&quot;user&quot;:&quot;jiachenl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jiachen Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:55:38.134Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482aabe85ce4d2973d47af1&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jitesh Jain&quot;,&quot;user&quot;:&quot;praeclarumjj3&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jitesh Jain&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:55:41.979Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482aabe85ce4d2973d47af2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642793395811-61e1188afc27c0f5e3641eb3.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Humphrey Shi&quot;,&quot;user&quot;:&quot;Humphrey&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Humphrey Shi&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:29:38.536Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:51:58.000Z&quot;,&quot;title&quot;:&quot;Matting Anything&quot;,&quot;summary&quot;:&quot;In this paper, we propose the Matting Anything Model (MAM), an efficient and\nversatile framework for estimating the alpha matte of any instance in an image\nwith flexible and interactive visual or linguistic user prompt guidance. MAM\noffers several significant advantages over previous specialized image matting\nnetworks: (i) MAM is capable of dealing with various types of image matting,\nincluding semantic, instance, and referring image matting with only a single\nmodel; (ii) MAM leverages the feature maps from the Segment Anything Model\n(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha\nmatte through iterative refinement, which has only 2.7 million trainable\nparameters. (iii) By incorporating SAM, MAM simplifies the user intervention\nrequired for the interactive use of image matting from the trimap to the box,\npoint, or text prompt. We evaluate the performance of MAM on various image\nmatting benchmarks, and the experimental results demonstrate that MAM achieves\ncomparable performance to the state-of-the-art specialized image matting models\nunder different metrics on each benchmark. Overall, MAM shows superior\ngeneralization ability and can effectively handle various image matting tasks\nwith fewer parameters, making it a practical solution for unified image\nmatting. Our code and models are open-sourced at\nhttps://github.com/SHI-Labs/Matting-Anything.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-09T04:29:53.052Z&quot;,&quot;title&quot;:&quot;Matting Anything&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dKVT3TXJACso0fxoD1Zfc.mp4&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05087&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad3&quot;,&quot;name&quot;:&quot;Yidong Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad4&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/65fbea940fad211462ecc5ad725e0c28.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhuohao Yu&quot;,&quot;user&quot;:&quot;narcissus&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhuohao Yu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:12:37.731Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad5&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e5eb4d5580d1109de7e6a2769514a30d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhengran Zeng&quot;,&quot;user&quot;:&quot;ZHENGRAN&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhengran Zeng&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:24:19.832Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/73cc9e6db6db86793787750776b57c63.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Linyi Yang&quot;,&quot;user&quot;:&quot;linyiyang2023&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Linyi Yang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-14T09:29:34.202Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad7&quot;,&quot;name&quot;:&quot;Cunxiang Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad8&quot;,&quot;name&quot;:&quot;Hao Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ad9&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5cdaa04e970e1e3dcfbb38ba89c0660a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;jiangchaoya&quot;,&quot;user&quot;:&quot;jcy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Chaoya Jiang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:38:34.729Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ada&quot;,&quot;name&quot;:&quot;Rui Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396adb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/18daf2de5671e711dc745388dd60569d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jindong Wang&quot;,&quot;user&quot;:&quot;jindongwang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jindong Wang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:32:31.918Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396adc&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b0c4f7fb4d4c556118fa6b52d3615429.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xing Xie&quot;,&quot;user&quot;:&quot;xingustc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xing Xie&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:38:52.626Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396add&quot;,&quot;name&quot;:&quot;Wei Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396ade&quot;,&quot;name&quot;:&quot;Shikun Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827ce72e73ce18ad396adf&quot;,&quot;name&quot;:&quot;Yue Zhang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T10:41:56.000Z&quot;,&quot;title&quot;:&quot;PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning\n  Optimization&quot;,&quot;summary&quot;:&quot;Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.&quot;,&quot;upvotes&quot;:5},&quot;publishedAt&quot;:&quot;2023-06-09T01:14:15.786Z&quot;,&quot;title&quot;:&quot;PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uxXL2AhheMyFx5TlFSl5W.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05178&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482b06a474d4d463d4c5342&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \&quot;Phillip\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuseung Lee&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:12:20.366Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482b06a474d4d463d4c5343&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/be2983528677d88647e031538f3f6c40.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kunho Kim&quot;,&quot;user&quot;:&quot;Kunho&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kunho Kim&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:27:21.011Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482b06a474d4d463d4c5344&quot;,&quot;name&quot;:&quot;Hyunjin Kim&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482b06a474d4d463d4c5345&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minhyuk Sung&quot;,&quot;user&quot;:&quot;Minhyuk&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Minhyuk Sung&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:27:45.261Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T13:18:23.000Z&quot;,&quot;title&quot;:&quot;SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions&quot;,&quot;summary&quot;:&quot;The remarkable capabilities of pretrained image diffusion models have been\nutilized not only for generating fixed-size images but also for creating\npanoramas. However, naive stitching of multiple images often results in visible\nseams. Recent techniques have attempted to address this issue by performing\njoint diffusions in multiple windows and averaging latent features in\noverlapping regions. However, these approaches, which focus on seamless montage\ngeneration, often yield incoherent outputs by blending different scenes within\na single image. To overcome this limitation, we propose SyncDiffusion, a\nplug-and-play module that synchronizes multiple diffusions through gradient\ndescent from a perceptual similarity loss. Specifically, we compute the\ngradient of the perceptual loss using the predicted denoised images at each\ndenoising step, providing meaningful guidance for achieving coherent montages.\nOur experimental results demonstrate that our method produces significantly\nmore coherent outputs compared to previous methods (66.35% vs. 33.65% in our\nuser study) while still maintaining fidelity (as assessed by GIQA) and\ncompatibility with the input prompt (as measured by CLIP score).&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-09T04:54:07.763Z&quot;,&quot;title&quot;:&quot;SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GqQucPe09dGzVJtpdRScP.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.04757&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64827bfa120600dcbe532f44&quot;,&quot;name&quot;:&quot;Yew Ken Chia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827bfa120600dcbe532f45&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657009213654-noauth.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hong Pengfei&quot;,&quot;user&quot;:&quot;emrys-hong&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Pengfei Hong&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:22:39.632Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827bfa120600dcbe532f46&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lidong Bing&quot;,&quot;user&quot;:&quot;LidongBing&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Lidong Bing&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:23:01.390Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64827bfa120600dcbe532f47&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Soujanya Poria&quot;,&quot;user&quot;:&quot;soujanyaporia&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Soujanya Poria&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T15:32:05.893Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-07T20:12:29.000Z&quot;,&quot;title&quot;:&quot;INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large\n  Language Models&quot;,&quot;summary&quot;:&quot;Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-09T01:10:19.116Z&quot;,&quot;title&quot;:&quot;INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4yEyg1jrJec0DKtb61Zkj.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.04751&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d4&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/858ce56df314107cb63920d1a511b146.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yizhong Wang&quot;,&quot;user&quot;:&quot;yizhongw&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yizhong Wang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:18:21.440Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d5&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hamish Ivison&quot;,&quot;user&quot;:&quot;hamishivi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hamish Ivison&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T15:32:10.401Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d6&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/619f3653911d111f046a5a6c30fc8319.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Pradeep Dasigi&quot;,&quot;user&quot;:&quot;pradeepd&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Pradeep Dasigi&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:18:41.250Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d7&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664497251391-625498644f4edf771516b2cb.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jack Hessel&quot;,&quot;user&quot;:&quot;jmhessel&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jack Hessel&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:19:08.476Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d8&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cdf2dbaac2c91c95581830/N3NuO7j53qkauJOf3kkRS.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tushar Khot&quot;,&quot;user&quot;:&quot;tusharkhot&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tushar Khot&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:19:27.010Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0d9&quot;,&quot;name&quot;:&quot;Khyathi Raghavi Chandu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0da&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1609884433338-5ff4e2a1463be69ae4bd42bd.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;David Wadden&quot;,&quot;user&quot;:&quot;dwadden&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;David Wadden&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:19:55.961Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0db&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/685e3f4bf768a372a30615ec37b0f25b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kelsey MacMillan&quot;,&quot;user&quot;:&quot;kmacmillan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Kelsey MacMillan&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:20:21.497Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0dc&quot;,&quot;name&quot;:&quot;Noah A. Smith&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0dd&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1589816744885-5ec07694ed25d76864d553f4.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Iz Beltagy&quot;,&quot;user&quot;:&quot;beltagy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Iz Beltagy&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:20:45.755Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;648270672bc908e2d9e2a0de&quot;,&quot;name&quot;:&quot;Hannaneh Hajishirzi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-07T19:59:23.000Z&quot;,&quot;title&quot;:&quot;How Far Can Camels Go? Exploring the State of Instruction Tuning on Open\n  Resources&quot;,&quot;summary&quot;:&quot;In this work we explore recent advances in instruction-tuning language models\non a range of open instruction-following datasets. Despite recent claims that\nopen models can be on par with state-of-the-art proprietary models, these\nclaims are often accompanied by limited evaluation, making it difficult to\ncompare models across the board and determine the utility of various resources.\nWe provide a large set of instruction-tuned models from 6.7B to 65B parameters\nin size, trained on 12 instruction datasets ranging from manually curated\n(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and\nsystematically evaluate them on their factual knowledge, reasoning,\nmultilinguality, coding, and open-ended instruction following abilities through\na collection of automatic, model-based, and human-based metrics. We further\nintroduce T\\\&quot;ulu, our best performing instruction-tuned model suite finetuned\non a combination of high-quality open resources.\n  Our experiments show that different instruction-tuning datasets can uncover\nor enhance specific skills, while no single dataset (or combination) provides\nthe best performance across all evaluations. Interestingly, we find that model\nand human preference-based evaluations fail to reflect differences in model\ncapabilities exposed by benchmark-based evaluations, suggesting the need for\nthe type of systemic evaluation performed in this work. Our evaluations show\nthat the best model in any given evaluation reaches on average 83% of ChatGPT\nperformance, and 68% of GPT-4 performance, suggesting that further investment\nin building better base models and instruction-tuning data is required to close\nthe gap. We release our instruction-tuned models, including a fully finetuned\n65B T\\\&quot;ulu, along with our code, data, and evaluation framework at\nhttps://github.com/allenai/open-instruct to facilitate future research.&quot;,&quot;upvotes&quot;:4},&quot;publishedAt&quot;:&quot;2023-06-09T01:05:46.385Z&quot;,&quot;title&quot;:&quot;How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ia_Lth3vu95J-ejQJUb_h.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05428&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482c405ba6c556892041f84&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e2b33b1fe6f43919dcb4df3410deda71.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manel Baradad&quot;,&quot;user&quot;:&quot;manebaradad&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Manel Baradad&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T15:31:48.005Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f85&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/9821af26a585f54b595ec39f061f2250.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuanzhen Li&quot;,&quot;user&quot;:&quot;yzli&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuanzhen Li&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:02:44.252Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f86&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/32f46e6778d1c52ebdd6f2511ba48257.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Forrester Cole&quot;,&quot;user&quot;:&quot;fcole&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Forrester Cole&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:03:04.407Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f87&quot;,&quot;name&quot;:&quot;Michael Rubinstein&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f88&quot;,&quot;name&quot;:&quot;Antonio Torralba&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f89&quot;,&quot;name&quot;:&quot;William T. Freeman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c405ba6c556892041f8a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0b742ff094a09f9374fafcd97ab9e002.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Varun Jampani&quot;,&quot;user&quot;:&quot;varunjampani&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Varun Jampani&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:04:10.813Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:59.000Z&quot;,&quot;title&quot;:&quot;Background Prompting for Improved Object Depth&quot;,&quot;summary&quot;:&quot;Estimating the depth of objects from a single image is a valuable task for\nmany vision, robotics, and graphics applications. However, current methods\noften fail to produce accurate depth for objects in diverse scenes. In this\nwork, we propose a simple yet effective Background Prompting strategy that\nadapts the input object image with a learned background. We learn the\nbackground prompts only using small-scale synthetic object datasets. To infer\nobject depth on a real image, we place the segmented object into the learned\nbackground prompt and run off-the-shelf depth networks. Background Prompting\nhelps the depth networks focus on the foreground object, as they are made\ninvariant to background variations. Moreover, Background Prompting minimizes\nthe domain gap between synthetic and real object images, leading to better\nsim2real generalization than simple finetuning. Results on multiple synthetic\nand real datasets demonstrate consistent improvements in real object depths for\na variety of existing depth networks. Code and optimized background prompts can\nbe found at: https://mbaradad.github.io/depth_prompt.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-09T06:17:46.261Z&quot;,&quot;title&quot;:&quot;Background Prompting for Improved Object Depth&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S78WAnvam_Ri2o6VpPKap.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.04845&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdc8&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/20ca0523fa980da7063a0b83e20ef7b2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ganesh Jawahar&quot;,&quot;user&quot;:&quot;ganeshjwhr&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ganesh Jawahar&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T17:06:17.883Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdc9&quot;,&quot;name&quot;:&quot;Haichuan Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdca&quot;,&quot;name&quot;:&quot;Yunyang Xiong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdcb&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/aaa2d0eadbcf96c2eb9059e3d73c2760.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Liu&quot;,&quot;user&quot;:&quot;Zechun&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zechun Liu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:42:14.734Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdcc&quot;,&quot;name&quot;:&quot;Dilin Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdcd&quot;,&quot;name&quot;:&quot;Fei Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdce&quot;,&quot;name&quot;:&quot;Meng Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdcf&quot;,&quot;name&quot;:&quot;Aasish Pappu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdd0&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/c2b4eefa7c17cdf2299c07466c361ab0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Barlas Oguz&quot;,&quot;user&quot;:&quot;barlaso&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Barlas Oguz&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:44:23.294Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdd1&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/1d66840db4ed6dc12bdee7ba8168ea9f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Muhammad Abdul-Mageed&quot;,&quot;user&quot;:&quot;mageed&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Muhammad Abdul-Mageed&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:44:43.727Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdd2&quot;,&quot;name&quot;:&quot;Laks V. S. Lakshmanan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdd3&quot;,&quot;name&quot;:&quot;Raghuraman Krishnamoorthi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828d90709d2cdcac47bdd4&quot;,&quot;name&quot;:&quot;Vikas Chandra&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T00:35:36.000Z&quot;,&quot;title&quot;:&quot;Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with\n  Architecture-Routed Mixture-of-Experts&quot;,&quot;summary&quot;:&quot;Weight-sharing supernet has become a vital component for performance\nestimation in the state-of-the-art (SOTA) neural architecture search (NAS)\nframeworks. Although supernet can directly generate different subnetworks\nwithout retraining, there is no guarantee for the quality of these subnetworks\nbecause of weight sharing. In NLP tasks such as machine translation and\npre-trained language modeling, we observe that given the same model\narchitecture, there is a large performance gap between supernet and training\nfrom scratch. Hence, supernet cannot be directly used and retraining is\nnecessary after finding the optimal architectures.\n  In this work, we propose mixture-of-supernets, a generalized supernet\nformulation where mixture-of-experts (MoE) is adopted to enhance the expressive\npower of the supernet model, with negligible training overhead. In this way,\ndifferent subnetworks do not share the model weights directly, but through an\narchitecture-based routing mechanism. As a result, model weights of different\nsubnetworks are customized towards their specific architectures and the weight\ngeneration is learned by gradient descent. Compared to existing weight-sharing\nsupernet for NLP, our method can minimize the retraining time, greatly\nimproving training efficiency. In addition, the proposed method achieves the\nSOTA performance in NAS for building fast machine translation models, yielding\nbetter latency-BLEU tradeoff compared to HAT, state-of-the-art NAS for MT. We\nalso achieve the SOTA performance in NAS for building memory-efficient\ntask-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various\nmodel sizes.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-09T02:25:21.428Z&quot;,&quot;title&quot;:&quot;Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r1JgnM4b-1rqmZOWk7asZ.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.04707&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64828cc35f581f451849684c&quot;,&quot;name&quot;:&quot;Jing Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f451849684d&quot;,&quot;name&quot;:&quot;Da Ju&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f451849684e&quot;,&quot;name&quot;:&quot;Joshua Lane&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f451849684f&quot;,&quot;name&quot;:&quot;Mojtaba Komeili&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496850&quot;,&quot;name&quot;:&quot;Eric Michael Smith&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496851&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/e75bb87af183aa5c3c5a7d09f2de1d60.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Megan Ung&quot;,&quot;user&quot;:&quot;meganung&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Megan Ung&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:53:55.559Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496852&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0209948739811faefd8bb387a33a7a0c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Morteza Behrooz&quot;,&quot;user&quot;:&quot;lessteza&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Morteza Behrooz&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:46:40.806Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496853&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/33cd9046d9833f0a9bc945ae776778e6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;William Ngan&quot;,&quot;user&quot;:&quot;metaphorical&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;William Ngan&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:47:00.053Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496854&quot;,&quot;name&quot;:&quot;Rashel Moritz&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496855&quot;,&quot;name&quot;:&quot;Sainbayar Sukhbaatar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496856&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/d7d7c3ad870541017551594bd30b6fde.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Y-Lan Boureau&quot;,&quot;user&quot;:&quot;notebleue&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Y-Lan Boureau&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-07-10T09:25:19.383Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496857&quot;,&quot;name&quot;:&quot;Jason Weston&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64828cc35f581f4518496858&quot;,&quot;name&quot;:&quot;Kurt Shuster&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-07T18:19:46.000Z&quot;,&quot;title&quot;:&quot;Improving Open Language Models by Learning from Organic Interactions&quot;,&quot;summary&quot;:&quot;We present BlenderBot 3x, an update on the conversational model BlenderBot 3,\nwhich is now trained using organic conversation and feedback data from\nparticipating users of the system in order to improve both its skills and\nsafety. We are publicly releasing the participating de-identified interaction\ndata for use by the research community, in order to spur further progress.\nTraining models with organic data is challenging because interactions with\npeople \&quot;in the wild\&quot; include both high quality conversations and feedback, as\nwell as adversarial and toxic behavior. We study techniques that enable\nlearning from helpful teachers while avoiding learning from people who are\ntrying to trick the model into unhelpful or toxic responses. BlenderBot 3x is\nboth preferred in conversation to BlenderBot 3, and is shown to produce safer\nresponses in challenging situations. While our current models are still far\nfrom perfect, we believe further improvement can be achieved by continued use\nof the techniques explored in this work.&quot;,&quot;upvotes&quot;:3},&quot;publishedAt&quot;:&quot;2023-06-09T02:21:56.903Z&quot;,&quot;title&quot;:&quot;Improving Open Language Models by Learning from Organic Interactions&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NLtTx__apfiEDSJIcyM2d.png&quot;,&quot;numComments&quot;:1,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05410&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aea&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/9C84YbzVwXrWkbf6tPAcH.png?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Cheng&quot;,&quot;user&quot;:&quot;Zezhou&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zezhou Cheng&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:11:47.623Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aeb&quot;,&quot;name&quot;:&quot;Carlos Esteves&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aec&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0b742ff094a09f9374fafcd97ab9e002.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Varun Jampani&quot;,&quot;user&quot;:&quot;varunjampani&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Varun Jampani&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:12:33.370Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aed&quot;,&quot;name&quot;:&quot;Abhishek Kar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aee&quot;,&quot;name&quot;:&quot;Subhransu Maji&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c04c2bc908e2d9f25aef&quot;,&quot;name&quot;:&quot;Ameesh Makadia&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:56:22.000Z&quot;,&quot;title&quot;:&quot;LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs&quot;,&quot;summary&quot;:&quot;A critical obstacle preventing NeRF models from being deployed broadly in the\nwild is their reliance on accurate camera poses. Consequently, there is growing\ninterest in extending NeRF models to jointly optimize camera poses and scene\nrepresentation, which offers an alternative to off-the-shelf SfM pipelines\nwhich have well-understood failure modes. Existing approaches for unposed NeRF\noperate under limited assumptions, such as a prior pose distribution or coarse\npose initialization, making them less effective in a general setting. In this\nwork, we propose a novel approach, LU-NeRF, that jointly estimates camera poses\nand neural radiance fields with relaxed assumptions on pose configuration. Our\napproach operates in a local-to-global manner, where we first optimize over\nlocal subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and\ngeometry for this challenging few-shot task. The mini-scene poses are brought\ninto a global reference frame through a robust pose synchronization step, where\na final global optimization of pose and scene can be performed. We show our\nLU-NeRF pipeline outperforms prior attempts at unposed NeRF without making\nrestrictive assumptions on the pose prior. This allows us to operate in the\ngeneral SE(3) pose setting, unlike the baselines. Our results also indicate our\nmodel can be complementary to feature-based SfM pipelines as it compares\nfavorably to COLMAP on low-texture and low-resolution images.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-09T06:01:52.724Z&quot;,&quot;title&quot;:&quot;LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bmYaKumOTgnOUWlcUIdyp.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05357&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482be9b474d4d463d4e941c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/251e209b60178a2ffc555b2ecdd29cf1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nan Liu&quot;,&quot;user&quot;:&quot;nanliu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Nan Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:55:34.264Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482be9b474d4d463d4e941d&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/def472d1ab3fbf751225357c0932ae7e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yilun Du&quot;,&quot;user&quot;:&quot;yilundu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yilun Du&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:40:30.390Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482be9b474d4d463d4e941e&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/b988df8d5acae79e144df20c41fa7a0a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuang Li&quot;,&quot;user&quot;:&quot;Shuang59&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shuang Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-09T16:55:29.788Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482be9b474d4d463d4e941f&quot;,&quot;name&quot;:&quot;Joshua B. Tenenbaum&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482be9b474d4d463d4e9420&quot;,&quot;name&quot;:&quot;Antonio Torralba&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:02:15.000Z&quot;,&quot;title&quot;:&quot;Unsupervised Compositional Concepts Discovery with Text-to-Image\n  Generative Models&quot;,&quot;summary&quot;:&quot;Text-to-image generative models have enabled high-resolution image synthesis\nacross different domains, but require users to specify the content they wish to\ngenerate. In this paper, we consider the inverse problem -- given a collection\nof different images, can we discover the generative concepts that represent\neach image? We present an unsupervised approach to discover generative concepts\nfrom a collection of images, disentangling different art styles in paintings,\nobjects, and lighting from kitchen scenes, and discovering image classes given\nImageNet images. We show how such generative concepts can accurately represent\nthe content of images, be recombined and composed to generate new artistic and\nhybrid images, and be further used as a representation for downstream\nclassification tasks.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-09T05:54:36.595Z&quot;,&quot;title&quot;:&quot;Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/V1z6DBuC-oUkTsFueQn_C.mp4&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.04822&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482b89556e161f11f28014b&quot;,&quot;name&quot;:&quot;Shreyank N Gowda&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482b89556e161f11f28014c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/2421bfb7747664d1acefbac1c00c3475.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Anurag Arnab&quot;,&quot;user&quot;:&quot;anurag-arnab&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Anurag Arnab&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:16:36.095Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482b89556e161f11f28014d&quot;,&quot;name&quot;:&quot;Jonathan Huang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-07T23:06:53.000Z&quot;,&quot;title&quot;:&quot;Optimizing ViViT Training: Time and Memory Reduction for Action\n  Recognition&quot;,&quot;summary&quot;:&quot;In this paper, we address the challenges posed by the substantial training\ntime and memory consumption associated with video transformers, focusing on the\nViViT (Video Vision Transformer) model, in particular the Factorised Encoder\nversion, as our baseline for action recognition tasks. The factorised encoder\nvariant follows the late-fusion approach that is adopted by many state of the\nart approaches. Despite standing out for its favorable speed/accuracy tradeoffs\namong the different variants of ViViT, its considerable training time and\nmemory requirements still pose a significant barrier to entry. Our method is\ndesigned to lower this barrier and is based on the idea of freezing the spatial\ntransformer during training. This leads to a low accuracy model if naively\ndone. But we show that by (1) appropriately initializing the temporal\ntransformer (a module responsible for processing temporal information) (2)\nintroducing a compact adapter model connecting frozen spatial representations\n((a module that selectively focuses on regions of the input image) to the\ntemporal transformer, we can enjoy the benefits of freezing the spatial\ntransformer without sacrificing accuracy. Through extensive experimentation\nover 6 benchmarks, we demonstrate that our proposed training strategy\nsignificantly reduces training costs (by sim 50%) and memory consumption\nwhile maintaining or slightly improving performance by up to 1.79\\% compared to\nthe baseline model. Our approach additionally unlocks the capability to utilize\nlarger image transformer models as our spatial transformer and access more\nframes with the same memory consumption.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-09T05:28:53.608Z&quot;,&quot;title&quot;:&quot;Optimizing ViViT Training: Time and Memory Reduction for Action Recognition&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ttlUdqXeM9Mued9f248B4.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05427&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482a54fd3973485034deac0&quot;,&quot;name&quot;:&quot;Quynh Phung&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a54fd3973485034deac1&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/f610873f6080453b655fb230d6f3b765.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Songwei Ge&quot;,&quot;user&quot;:&quot;songweig&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Songwei Ge&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:30:53.093Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482a54fd3973485034deac2&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/5a2550d95e686640242840ad3bd0e680.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiabin Huang&quot;,&quot;user&quot;:&quot;YellowAddice&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jia-Bin Huang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:31:15.328Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:59.000Z&quot;,&quot;title&quot;:&quot;Grounded Text-to-Image Synthesis with Attention Refocusing&quot;,&quot;summary&quot;:&quot;Driven by scalable diffusion models trained on large-scale paired text-image\ndatasets, text-to-image synthesis methods have shown compelling results.\nHowever, these models still fail to precisely follow the text prompt when\nmultiple objects, attributes, and spatial compositions are involved in the\nprompt. In this paper, we identify the potential reasons in both the\ncross-attention and self-attention layers of the diffusion model. We propose\ntwo novel losses to refocus the attention maps according to a given layout\nduring the sampling process. We perform comprehensive experiments on the\nDrawBench and HRS benchmarks using layouts synthesized by Large Language\nModels, showing that our proposed losses can be integrated easily and\neffectively into existing text-to-image methods and consistently improve their\nalignment between the generated images and the text prompts.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-09T04:06:43.973Z&quot;,&quot;title&quot;:&quot;Grounded Text-to-Image Synthesis with Attention Refocusing&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ymLefEnI_dmgdOrwthGfx.mp4&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05392&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da74&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/84393f357c0370edbb90a4188df795d6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sanjay Subramanian&quot;,&quot;user&quot;:&quot;sanjayss34&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sanjay Subramanian&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:34:58.858Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da75&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/873ea85dbd5d79509faa3fedfcaa33d8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Medhini Narasimhan&quot;,&quot;user&quot;:&quot;medhini&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Medhini Narasimhan&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:35:16.781Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da76&quot;,&quot;name&quot;:&quot;Kushal Khangaonkar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da77&quot;,&quot;name&quot;:&quot;Kevin Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da78&quot;,&quot;name&quot;:&quot;Arsha Nagrani&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da79&quot;,&quot;name&quot;:&quot;Cordelia Schmid&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da7a&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/120917da2d8587674505163fb960e27d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andy Zeng&quot;,&quot;user&quot;:&quot;andyzeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Andy Zeng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:12:29.862Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da7b&quot;,&quot;name&quot;:&quot;Trevor Darrell&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;64829bf6e4bbb1c2dd35da7c&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/74ff8f30b3662db2602495bdf493d397.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dan Klein&quot;,&quot;user&quot;:&quot;danjklein&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Dan Klein&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:36:14.695Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:45:14.000Z&quot;,&quot;title&quot;:&quot;Modular Visual Question Answering via Code Generation&quot;,&quot;summary&quot;:&quot;We present a framework that formulates visual question answering as modular\ncode generation. In contrast to prior work on modular approaches to VQA, our\napproach requires no additional training and relies on pre-trained language\nmodels (LMs), visual models pre-trained on image-caption pairs, and fifty VQA\nexamples used for in-context learning. The generated Python programs invoke and\ncompose the outputs of the visual models using arithmetic and conditional\nlogic. Our approach improves accuracy on the COVR dataset by at least 3% and on\nthe GQA dataset by roughly 2% compared to the few-shot baseline that does not\nemploy code generation.&quot;,&quot;upvotes&quot;:2},&quot;publishedAt&quot;:&quot;2023-06-09T03:26:47.903Z&quot;,&quot;title&quot;:&quot;Modular Visual Question Answering via Code Generation&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/k7zjOWNX7XPzYbovUn2V2.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05420&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482c190120600dcbe602419&quot;,&quot;name&quot;:&quot;Carlos Esteves&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c190120600dcbe60241a&quot;,&quot;name&quot;:&quot;Jean-Jacques Slotine&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c190120600dcbe60241b&quot;,&quot;name&quot;:&quot;Ameesh Makadia&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:59:08.000Z&quot;,&quot;title&quot;:&quot;Scaling Spherical CNNs&quot;,&quot;summary&quot;:&quot;Spherical CNNs generalize CNNs to functions on the sphere, by using spherical\nconvolutions as the main linear operation. The most accurate and efficient way\nto compute spherical convolutions is in the spectral domain (via the\nconvolution theorem), which is still costlier than the usual planar\nconvolutions. For this reason, applications of spherical CNNs have so far been\nlimited to small problems that can be approached with low model capacity. In\nthis work, we show how spherical CNNs can be scaled for much larger problems.\nTo achieve this, we make critical improvements including novel variants of\ncommon model components, an implementation of core operations to exploit\nhardware accelerator characteristics, and application-specific input\nrepresentations that exploit the properties of our model. Experiments show our\nlarger spherical CNNs reach state-of-the-art on several targets of the QM9\nmolecular benchmark, which was previously dominated by equivariant graph neural\nnetworks, and achieve competitive performance on multiple weather forecasting\ntasks. Our code is available at\nhttps://github.com/google-research/spherical-cnn.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-09T06:07:13.688Z&quot;,&quot;title&quot;:&quot;Scaling Spherical CNNs&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KPcc38C7fC2c0KlEoteSj.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2306.05411&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6482c107474d4d463d4eee53&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/4a6b5748b8d95a15be560af5fd2de998.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kien Nguyen&quot;,&quot;user&quot;:&quot;duykien&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Duy-Kien Nguyen&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:09:02.154Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee54&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/6903732275dc71213cfbf82a98c39084.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Vaibhav Aggarwal&quot;,&quot;user&quot;:&quot;vaibhavagg303&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Vaibhav Aggarwal&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:10:14.815Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee55&quot;,&quot;name&quot;:&quot;Yanghao Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee56&quot;,&quot;name&quot;:&quot;Martin R. Oswald&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee57&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/0e4e3839a4e6f182c2618eb35acef29a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexander Kirillov&quot;,&quot;user&quot;:&quot;akirillov&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alexander Kirillov&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T13:11:21.278Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee58&quot;,&quot;name&quot;:&quot;Cees G. M. Snoek&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6482c107474d4d463d4eee59&quot;,&quot;user&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/75262a35b27a2ae1939df9118120d99e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinlei Chen&quot;,&quot;user&quot;:&quot;endernewton&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xinlei Chen&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2023-06-12T09:11:53.653Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2023-06-08T17:56:46.000Z&quot;,&quot;title&quot;:&quot;R-MAE: Regions Meet Masked Autoencoders&quot;,&quot;summary&quot;:&quot;Vision-specific concepts such as \&quot;region\&quot; have played a key role in extending\ngeneral machine learning frameworks to tasks like object detection. Given the\nsuccess of region-based detectors for supervised learning and the progress of\nintra-image methods for contrastive learning, we explore the use of regions for\nreconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a\nbaseline and an inspiration, we propose a parallel pre-text task tailored to\naddress the one-to-many mapping between images and regions. Since such regions\ncan be generated in an unsupervised way, our approach (R-MAE) inherits the wide\napplicability from MAE, while being more \&quot;region-aware\&quot;. We conduct thorough\nanalyses during the development of R-MAE, and converge on a variant that is\nboth effective and efficient (1.3% overhead over MAE). Moreover, it shows\nconsistent quantitative improvements when generalized to various pre-training\ndata and downstream detection and segmentation benchmarks. Finally, we provide\nextensive qualitative visualizations to enhance the understanding of R-MAE's\nbehaviour and potential. Code will be made available at\nhttps://github.com/facebookresearch/r-mae.&quot;,&quot;upvotes&quot;:1},&quot;publishedAt&quot;:&quot;2023-06-09T06:04:57.879Z&quot;,&quot;title&quot;:&quot;R-MAE: Regions Meet Masked Autoencoders&quot;,&quot;mediaUrl&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zVE87q3f-sRtnZCsOYsEV.png&quot;,&quot;numComments&quot;:0,&quot;upvoted&quot;:false}],&quot;lastDate&quot;:&quot;2024-03-19T05:05:26.926Z&quot;,&quot;nextDate&quot;:&quot;2023-06-12&quot;,&quot;prevDate&quot;:&quot;2023-06-08&quot;,&quot;publisher&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;}}" data-target="DailyPapers"><section class="container relative mb-20 mt-8 md:mt-14"><div class="mb-12 grid grid-cols-2 items-start md:mb-16 md:grid-cols-3"><div class="md:pl-4"><h1 class="text-2xl font-bold md:text-3xl"><a href="/papers" class="hover:text-gray-600 dark:hover:text-gray-300">Daily Papers</a></h1> <div class="flex flex-wrap items-center gap-2 text-gray-500"><h2 class="flex items-center gap-2.5 text-xl">by <a href="/akhaliq" class="flex items-center gap-2"><img alt="" class="h-4 w-4 rounded-full ring-2 ring-white dark:ring-gray-900" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg?w=200&amp;h=200&amp;f=face"><span class="underline">AK</span></a></h2></div></div> <div class="order-last col-span-2 mt-6 md:order-none md:col-span-1 md:mt-0"><button class="mx-auto flex w-full translate-y-1 items-center justify-center rounded-full border py-1 text-gray-400 shadow-sm hover:shadow-inner md:w-80" title="Search papers"><svg class="mr-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
				Search by arxiv id or title</button></div> <div class="flex items-stretch justify-end"><a href="/papers?date=2023-06-08" class="group my-0.5 -mr-2 flex w-8 items-center justify-center rounded-l-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 -translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a> <time class="relative flex flex-col items-stretch" datetime="2023-06-09T00:00:00.000Z"><span class="rounded-t-lg bg-gray-500 px-3.5 py-0.5 text-center text-xs font-bold uppercase leading-none text-white dark:bg-gray-700">Jun</span> <span class="rounded-b-lg bg-gray-100 px-3.5 py-1 text-center text-xl font-semibold text-gray-800 dark:bg-gradient-to-br dark:from-gray-800 dark:to-gray-950">8</span></time> <a href="/papers?date=2023-06-12" class="group my-0.5 -ml-2 flex w-8 items-center justify-center rounded-r-lg border border-gray-100 hover:bg-gray-50 dark:hover:bg-gray-900"><svg class="text-gray-600 dark:text-gray-400 rotate-180 translate-x-0.5 h-2.5 group-hover:dark:text-gray-200 group-hover:text-gray-800" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a></div></div> <div class="relative grid grid-cols-1 gap-14 lg:grid-cols-2"><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05284" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/EZZk6ml6GhwlzSahUlnge.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05284" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">123</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05284" class="cursor-pointer">Simple and Controllable Music Generation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05284" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="adefossez" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666708948380-noauth.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="gsynnaeve" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b7ccbddfa745db854dc342be1327cd53.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="itaigat" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/73519deba3176be9c23d49f749aee5da.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="felixkreuk" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IOWMd17Iwls0dXsY1OWjK.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="JadeCopet" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/49f08d989ca505ae01bce5578a94f6fe.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> <a href="/papers/2306.05284#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 22</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05425" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/niAEeYP_jD85khS61oIu2.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05425" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">9</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05425" class="cursor-pointer">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05425" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Chunyuan24" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/430560ec2c2547f819225769ab432f30.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jingkang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="pufanyi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/_vJC0zeVOIvaNV2R6toqg.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="liangyuch" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ZhangYuanhan" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 8 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05422" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WJW-d7rlY52prwX6VjViL.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05422" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">8</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05422" class="cursor-pointer">Tracking Everything Everywhere All at Once</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05422" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ruojin Cai" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Qianqian Wang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jimantha" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e2ca585398a3212b2956a14804d0ba67.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="holynski" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/85d94b4022577747b8d2d10a82c2f3c7.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="KAXY" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1cb7f8d4062838710365b79b2f1d595a.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> <a href="/papers/2306.05422#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05424" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nrf864ImEaaP4pD8phJqs.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05424" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">6</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05424" class="cursor-pointer">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05424" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Fahad Shahbaz Khan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Salman Khan" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hanoona" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mmaaz60" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kmCo6brXhQ7SyxVx3lpsx.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> <a href="/papers/2306.05424#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dKVT3TXJACso0fxoD1Zfc.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05399" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05399" class="cursor-pointer">Matting Anything</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05399" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Humphrey" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642793395811-61e1188afc27c0f5e3641eb3.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="praeclarumjj3" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jiachenl" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6305b47efca1d8d92b82e0b4/6G1QlXHSeiSx9rPnUvjhz.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> <a href="/papers/2306.05399#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 3</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05087" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uxXL2AhheMyFx5TlFSl5W.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05087" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">5</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05087" class="cursor-pointer">PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05087" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jindongwang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/18daf2de5671e711dc745388dd60569d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jcy" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5cdaa04e970e1e3dcfbb38ba89c0660a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="linyiyang2023" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/73cc9e6db6db86793787750776b57c63.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ZHENGRAN" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e5eb4d5580d1109de7e6a2769514a30d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="narcissus" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/65fbea940fad211462ecc5ad725e0c28.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 13 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05178" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GqQucPe09dGzVJtpdRScP.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05178" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05178" class="cursor-pointer">SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05178" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Hyunjin Kim" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Minhyuk" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kunho" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/be2983528677d88647e031538f3f6c40.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="phillipinseoul" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/98575092404c4197b20c929a6499a015.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.04757" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4yEyg1jrJec0DKtb61Zkj.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.04757" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.04757" class="cursor-pointer">INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.04757" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yew Ken Chia" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="soujanyaporia" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="LidongBing" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="emrys-hong" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657009213654-noauth.jpeg?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 4 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.04751" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ia_Lth3vu95J-ejQJUb_h.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.04751" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">4</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.04751" class="cursor-pointer">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.04751" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="tusharkhot" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cdf2dbaac2c91c95581830/N3NuO7j53qkauJOf3kkRS.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jmhessel" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664497251391-625498644f4edf771516b2cb.jpeg?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="pradeepd" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/619f3653911d111f046a5a6c30fc8319.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hamishivi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png?w=200&amp;h=200&amp;f=face"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yizhongw" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/858ce56df314107cb63920d1a511b146.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 11 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S78WAnvam_Ri2o6VpPKap.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05428" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05428" class="cursor-pointer">Background Prompting for Improved Object Depth</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05428" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Michael Rubinstein" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="varunjampani" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0b742ff094a09f9374fafcd97ab9e002.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="fcole" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/32f46e6778d1c52ebdd6f2511ba48257.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yzli" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/9821af26a585f54b595ec39f061f2250.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="manebaradad" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e2b33b1fe6f43919dcb4df3410deda71.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.04845" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r1JgnM4b-1rqmZOWk7asZ.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.04845" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.04845" class="cursor-pointer">Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.04845" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Haichuan Yang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="mageed" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/1d66840db4ed6dc12bdee7ba8168ea9f.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="barlaso" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/c2b4eefa7c17cdf2299c07466c361ab0.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zechun" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/aaa2d0eadbcf96c2eb9059e3d73c2760.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ganeshjwhr" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/20ca0523fa980da7063a0b83e20ef7b2.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 13 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.04707" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NLtTx__apfiEDSJIcyM2d.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.04707" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">3</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.04707" class="cursor-pointer">Improving Open Language Models by Learning from Organic Interactions</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.04707" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jing Xu" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="notebleue" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/d7d7c3ad870541017551594bd30b6fde.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="metaphorical" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/33cd9046d9833f0a9bc945ae776778e6.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lessteza" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0209948739811faefd8bb387a33a7a0c.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="meganung" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/e75bb87af183aa5c3c5a7d09f2de1d60.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 13 authors</div></li></ul></a> <a href="/papers/2306.04707#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 1</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05410" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bmYaKumOTgnOUWlcUIdyp.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05410" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05410" class="cursor-pointer">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05410" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Subhransu Maji" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Abhishek Kar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Carlos Esteves" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="varunjampani" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0b742ff094a09f9374fafcd97ab9e002.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Zezhou" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/9C84YbzVwXrWkbf6tPAcH.png?w=200&amp;h=200&amp;f=face"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 6 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/V1z6DBuC-oUkTsFueQn_C.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05357" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05357" class="cursor-pointer">Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05357" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Antonio Torralba" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Joshua B. Tenenbaum" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shuang59" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b988df8d5acae79e144df20c41fa7a0a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="yilundu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/def472d1ab3fbf751225357c0932ae7e.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="nanliu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/251e209b60178a2ffc555b2ecdd29cf1.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 5 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.04822" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ttlUdqXeM9Mued9f248B4.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.04822" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.04822" class="cursor-pointer">Optimizing ViViT Training: Time and Memory Reduction for Action Recognition</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.04822" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jonathan Huang" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shreyank N Gowda" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="anurag-arnab" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/2421bfb7747664d1acefbac1c00c3475.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><video src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ymLefEnI_dmgdOrwthGfx.mp4" class="shadow-alternate-sm h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white object-top sm:h-64 md:h-72 lg:h-80" controls="" playsinline=""></video> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05427" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05427" class="cursor-pointer">Grounded Text-to-Image Synthesis with Attention Refocusing</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05427" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Quynh Phung" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="YellowAddice" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/5a2550d95e686640242840ad3bd0e680.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="songweig" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/f610873f6080453b655fb230d6f3b765.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> <a href="/papers/2306.05427#community" class="text-md flex items-center gap-2 text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg> 2</a></div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05392" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/k7zjOWNX7XPzYbovUn2V2.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05392" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">2</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05392" class="cursor-pointer">Modular Visual Question Answering via Code Generation</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05392" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kushal Khangaonkar" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="danjklein" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/74ff8f30b3662db2602495bdf493d397.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="andyzeng" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/120917da2d8587674505163fb960e27d.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="medhini" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/873ea85dbd5d79509faa3fedfcaa33d8.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sanjayss34" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/84393f357c0370edbb90a4188df795d6.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 9 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05420" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KPcc38C7fC2c0KlEoteSj.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05420" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05420" class="cursor-pointer">Scaling Spherical CNNs</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05420" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ameesh Makadia" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jean-Jacques Slotine" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Carlos Esteves" style="content-visibility:auto;"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 3 authors</div></li></ul></a> </div></div></div></div></article> </div><div><article class="flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2306.05411" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl border bg-white dark:border-gray-700 sm:h-64 md:h-72 lg:h-80"><img src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zVE87q3f-sRtnZCsOYsEV.png" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a> <div class="from-gray-50-to-white -mt-2 flex bg-gradient-to-b px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1"><a href="/login?next=%2Fpapers%2F2306.05411" class="self-start"> <div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input type="checkbox" class="peer hidden" disabled=""> <svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> <div class="leading-none">1</div></div></a> </div>  <div class="w-full"><h3 class="mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl"><a href="/papers/2306.05411" class="cursor-pointer">R-MAE: Regions Meet Masked Autoencoders</a></h3> <div class="flex items-center justify-between"><a href="/papers/2306.05411" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yanghao Li" style="content-visibility:auto;"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="endernewton" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/75262a35b27a2ae1939df9118120d99e.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="akirillov" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/0e4e3839a4e6f182c2618eb35acef29a.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="vaibhavagg303" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/6903732275dc71213cfbf82a98c39084.svg"></li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  block flex-none rounded-full border-2 border-white bg-gradient-to-br from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="duykien" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/4a6b5748b8d95a15be560af5fd2de998.svg"></li> <li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div> 7 authors</div></li></ul></a> </div></div></div></div></article> </div> <div class="col-span-1 flex lg:col-span-2"><a class="btn gap-2" href="/papers?date=2023-06-08"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z" fill="currentColor"></path></svg>Previous</a> <a class="btn ml-auto gap-2" href="/papers?date=2023-06-12">Next<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z" fill="currentColor"></path></svg></a></div></div></section> </div></main>
	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="font-semibold text-black md:hidden">Company</div>
		<div class="order-last pt-6 text-gray-400 md:order-none md:pt-0" href="Terms">© Hugging Face</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="group order-first flex-none pb-6 md:order-none md:pb-0"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="pt-6 font-semibold text-black md:hidden md:pt-0">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("/front/build/kube-351e67d/index.js");
			window.moonSha = "kube-351e67d/";
			window.hubConfig = JSON.parse(`{"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https://huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetsServerPublicUrl":"https://datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)"}`);
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>

		<!-- Google analytics v4 -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL";
				script.async = true;
				document.head.appendChild(script);

				window.dataLayer = window.dataLayer || [];
				function gtag() {
					if (window.dataLayer !== undefined) {
						window.dataLayer.push(arguments);
					}
				}
				gtag("js", new Date());
				gtag("config", "G-8Q63TH4CSL", { page_path: "/papers" });
				/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages
				gtag("consent", "default", { ad_storage: "denied", analytics_storage: "denied" });
				/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent
				/// TODO: ask the user for their consent and update this with gtag('consent', 'update')
			}
		</script>
	

<iframe name="__privateStripeMetricsController4600" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="https://js.stripe.com/v3/m-outer-3437aaddcdf6922d623e172c2d6f9278.html#url=https%3A%2F%2Fhuggingface.co%2Fpapers%3Fdate%3D2023-06-09&amp;title=Daily%20Papers%20-%20Hugging%20Face&amp;referrer=&amp;muid=NA&amp;sid=NA&amp;version=6&amp;preview=false" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe></body></html>