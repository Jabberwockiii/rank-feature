[
  {
    "title": "TinyLlama: An Open-Source Small Language Model",
    "link": "https://arxiv.org/pdf/2401.02385.pdf",
    "upvote": "78",
    "text": "TinyLlama: An Open-Source Small Language Model\nPeiyuan Zhang\u2217\nGuangtao Zeng\u2217\nTianduo Wang\nWei Lu\nStatNLP Research Group\nSingapore University of Technology and Design\n{peiyuan_zhang, tianduo_wang, luwei}@sutd.edu.sg\nguangtao_zeng@mymail.sutd.edu.sg\nAbstract\nWe present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and tok-\nenizer of Llama 2 (Touvron et al., 2023b), TinyLlama leverages various advances\ncontributed by the open-source community (e.g., FlashAttention (Dao, 2023)),\nachieving better computational efficiency. Despite its relatively small size, TinyL-\nlama demonstrates remarkable performance in a series of downstream tasks. It\nsignificantly outperforms existing open-source language models with compara-\nble sizes. Our model checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.\n1\nIntroduction\nRecent progress in natural language processing (NLP) has been largely propelled by scaling up\nlanguage model sizes (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a,b). Large\nLanguage Models (LLMs) pre-trained on extensive text corpora have demonstrated their effectiveness\non a wide range of tasks (OpenAI, 2023; Touvron et al., 2023b). Some empirical studies demonstrated\nemergent abilities in LLMs, abilities that may only manifest in models with a sufficiently large number\nof parameters, such as few-shot prompting (Brown et al., 2020) and chain-of-thought reasoning (Wei\net al., 2022). Other studies focus on modeling the scaling behavior of LLMs (Kaplan et al., 2020;\nHoffmann et al., 2022). Hoffmann et al. (2022) suggest that, to train a compute-optimal model, the\nsize of the model and the amount of training data should be increased at the same rate. This provides\na guideline on how to optimally select the model size and allocate the amount of training data when\nthe compute budget is fixed.\n*The first two authors contributed equally.\nTechnical Report, work in progress.\narXiv:2401.02385v1  [cs.CL]  4 Jan 2024\nAlthough these works show a clear preference on large models, the potential of training smaller\nmodels with larger dataset remains under-explored. Instead of training compute-optimal language\nmodels, Touvron et al. (2023a) highlight the importance of the inference budget, instead of focusing\nsolely on training compute-optimal language models. Inference-optimal language models aim for\noptimal performance within specific inference constraints This is achieved by training models with\nmore tokens than what is recommended by the scaling law (Hoffmann et al., 2022). Touvron et al.\n(2023a) demonstrates that smaller models, when trained with more data, can match or even outperform\ntheir larger counterparts. Also, Thadd\u00e9e (2023) suggest that existing scaling laws (Hoffmann et al.,\n2022) may not predict accurately in situations where smaller models are trained for longer periods.\nMotivated by these new findings, this work focuses on exploring the behavior of smaller models\nwhen trained with a significantly larger number of tokens than what is suggested by the scaling\nlaw (Hoffmann et al., 2022). Specifically, we train a Transformer decoder-only model (Vaswani et al.,\n2017) with 1.1B parameters using approximately 3 trillion tokens. To our knowledge, this is the\nfirst attempt to train a model with 1B parameters using such a large amount of data. Following the\nsame architecture and tokenizer as Llama 2 (Touvron et al., 2023b), we name our model TinyLlama.\nTinyLlama shows competitive performance compared to existing open-source language models of\nsimilar sizes. Specifically, TinyLlama surpasses both OPT-1.3B (Zhang et al., 2022) and Pythia-\n1.4B (Biderman et al., 2023) in various downstream tasks.\nOur TinyLlama is open-source, aimed at improving accessibility for researchers in language model\nresearch. We believe its excellent performance and compact size make it an attractive platform for\nresearchers and practitioners in language model research.\n2\nPretraining\nThis section describes how we pre-trained TinyLlama. First, we introduce the details of the pre-\ntraining corpus and the data sampling method. Next, we elaborate on the model architecture and the\nhyperparameters used during pretraining.\n2.1\nPre-training data\nOur main objective is to make the pre-training process effective and reproducible. We adopt a mixture\nof natural language data and code data to pre-train TinyLlama, sourcing natural language data from\nSlimPajama (Soboleva et al., 2023) and code data from Starcoderdata (Li et al., 2023). We adopt\nLlama\u2019s tokenizer (Touvron et al., 2023a) to process the data.\nSlimPajama\nThis is a large open-source corpus created for training language models based on\nRedPajama (Together Computer, 2023). The original RedPajama corpus is an open-source research\neffort aimed at reproducing Llama\u2019s pretraining data (Touvron et al., 2023a) containing over 1.2\ntrillion tokens. The SlimPajama was derived by cleaning and deduplicating the original RedPajama.\nStarcoderdata\nThis dataset was collected to train StarCoder (Li et al., 2023), a powerful open-\nsource large code language model. It comprises approximately 250 billion tokens across 86 program-\nming languages. In addition to code, it also includes GitHub issues and text-code pairs that involve\nnatural languages. To avoid data duplication, we remove the GitHub subset of the SlimPajama and\nonly sample code data from the Starcoderdata.\nAfter combining these two corpora, we have approximately 950 billion tokens for pre-training in total.\nTinyLlama is trained on these tokens for approximately three epochs, as observed by Muennighoff\net al. (2023), where training on data repeated for up to four epochs results in minimal performance\ndegradation compared to using unique data. During training, we sample the natural language data to\nachieve a ratio of around 7:3 between natural language data and code data.\n2.2\nArchitecture\nWe adopt a similar model architecture to Llama 2 (Touvron et al., 2023b). We use a Transformer\narchitecture based on Vaswani et al. (2017) with the following details:\n2\nTable 1: The details of model architecture\nHidden size\nIntermediate Hidden Size\nContext Len\nHeads\nLayers\nVocab size\n2,048\n5,632\n2,048\n16\n22\n32,000\nPositional embedding\nWe use RoPE (Rotary Positional Embedding) (Su et al., 2021) to inject\npositional information into our model. RoPE is a widely adopted method recently used by many\nmainstream large language models, such as PaLM (Anil et al., 2023), Llama (Touvron et al., 2023a),\nand Qwen (Bai et al., 2023).\nRMSNorm\nIn pre-normalization, to attain a more stable training, we normalize the input before\neach transformer sub-layer. In addition, we apply RMSNorm (Zhang and Sennrich, 2019) as our\nnormalization technique, which can improve training efficiency.\nSwiGLU\nInstead of using the traditional ReLU non-linearity, we follow Llama 2 and combine\nSwish and Gated Linear Unit together, which is referred to as SwiGLU (Shazeer, 2020), as our\nactivation function in TinyLlama.\nGrouped-query Attention\nTo reduce memory bandwidth overhead and speed up inference, we use\ngrouped-query attention (Ainslie et al., 2023) in our model. We have 32 heads for query attention\nand use 4 groups of key-value heads. With this technique, the model can share key and value\nrepresentations across multiple heads without sacrificing much performance.\n2.3\nSpeed Optimizations\nFully Sharded Data Parallel (FSDP)\nDuring training, our codebase has integrated FSDP1 to\nleverage multi-GPU and multi-node setups efficiently. This integration is crucial in scaling the\ntraining process across multiple computing nodes, which significantly improves the training speed\nand efficiency.\nFlash Attention\nAnother critical improvement is the integration of Flash Attention 2 (Dao, 2023),\nan optimized attention mechanism. The repository also provides fused layernorm, fused cross\nentropy loss, and fused rotary positional embedding, which together play a pivotal role in boosting\ncomputational throughput.\nxFormers\nWe have replaced the fused SwiGLU module from the xFormers (Lefaudeux et al., 2022)\nrepository with the original SwiGLU module, further enhancing the efficiency of our codebase. With\nthese features, we can reduce the memory footprint, enabling the 1.1B model to fit within 40GB of\nGPU RAM.\nPerformance Analysis and Comparison with Other Models\nThe incorporation of these elements\nhas propelled our training throughput to 24,000 tokens per second per A100-40G GPU. When\ncompared with other models like Pythia-1.0B (Biderman et al., 2023) and MPT-1.3B 2, our codebase\ndemonstrates superior training speed. For instance, the TinyLlama-1.1B model requires only 3,456\nA100 GPU hours for 300B tokens, in contrast to Pythia\u2019s 4,830 and MPT\u2019s 7,920 hours. This shows\nthe effectiveness of our optimizations and the potential for substantial time and resource savings in\nlarge-scale model training.\n2.4\nTraining\nWe build our framework based on lit-gpt.3 In adhering to Llama 2 (Touvron et al., 2023b), we employ\nan autoregressive language modeling objective during the pretraining phase. Consistent with Llama\n2\u2019s settings, we utilize the AdamW optimizer (Loshchilov and Hutter, 2019), setting \u03b21 at 0.9 and\n1https://huggingface.co/docs/accelerate/usage_guides/fsdp\n2https://huggingface.co/mosaicml/mpt-1b-redpajama-200b\n3https://github.com/Lightning-AI/lit-gpt\n3\nFigure 1: Comparison of the training speed of our codebase with Pythia and MPT.\n\u03b22 at 0.95. Additionally, we use a cosine learning rate schedule with maximum learning rate as\n4.0 \u00d7 10\u22124 and minimum learning rate as 4.0 \u00d7 10\u22125. We use 2,000 warmup steps to facilitate\noptimized learning.4 We set the batch size as 2M tokens. We assign weight decay as 0.1, and use\na gradient clipping threshold of 1.0 to regulate the gradient value. We pretrain TinyLlama with 16\nA100-40G GPUs in our project.\n3\nResults\nWe evaluate TinyLlama on a wide range of commonsense reasoning and problem-solving tasks and\ncompare it with several existing open-source language models with similar model parameters.\nBaseline models\nWe primarily focus on language models with a decoder-only architecture, compris-\ning approximately 1 billion parameters. Specifically, we compare TinyLlama with OPT-1.3B (Zhang\net al., 2022), Pythia-1.0B, and Pythia-1.4B (Biderman et al., 2023).\nCommonsense reasoning tasks\nTo understand the commonsense reasoning ability of TinyLlama,\nwe consider the following tasks: Hellaswag (Zellers et al., 2019), OpenBookQA (Mihaylov et al.,\n2018), WinoGrande (Sakaguchi et al., 2021), ARC-Easy and ARC-Challenge (Clark et al., 2018),\nBoolQ (Clark et al., 2019), and PIQA (Bisk et al., 2020). We adopt the Language Model Evaluation\nHarness framework (Gao et al., 2023) to evaluate the models. Following previous practice (Biderman\net al., 2023), the models are evaluated in a zero-shot setting on these tasks. The results are presented\nin Table 2. We notice that TinyLlama outperforms baselines on many of the tasks and obtains the\nhighest averaged scores.\nTable 2: Zero-shot performance on commonsense reasoning tasks.\nHellaSwag\nObqa\nWinoGrande\nARC-c\nARC-e\nboolq\npiqa\nAvg\nOPT-1.3B\n53.65\n33.40\n59.59\n29.44\n50.80\n60.83\n72.36\n51.44\nPythia-1.0B\n47.16\n31.40\n53.43\n27.05\n48.99\n57.83\n69.21\n48.30\nPythia-1.4B\n52.01\n33.20\n57.38\n28.50\n54.00\n63.27\n70.95\n51.33\nTinyLlama-1.1B\n59.20\n36.00\n59.12\n30.10\n55.25\n57.83\n73.29\n52.99\nEvolution of performance during training\nWe tracked the accuracy of TinyLlama on common-\nsense reasoning benchmarks during its pre-training, as shown in Fig. 2. Generally, the performance of\n4Due to a bug in the config file, the learning rate did not decrease immediately after warmup and remained at\nthe maximum value for several steps before we fixed this.\n4\nTinyLlama improves with increased computational resources, surpassing the accuracy of Pythia-1.4B\nin most benchmarks.5\n10\n2\n10\n3\n10\n4\nGPU-Hours\n24\n26\n28\n30\n32\nAccuracy (%)\narc_challenge\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n40\n45\n50\n55\nAccuracy (%)\narc_easy\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n50\n55\n60\nAccuracy (%)\nboolq\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n35\n40\n45\n50\n55\n60\nAccuracy (%)\nhellaswag\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n30\n32\n34\n36\nAccuracy (%)\nopenbookqa\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n62.5\n65.0\n67.5\n70.0\n72.5\nAccuracy (%)\npiqa\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n50.0\n52.5\n55.0\n57.5\n60.0\nAccuracy (%)\nwinogrande\nTinyLlama\nPythia-1.4B\n10\n2\n10\n3\n10\n4\nGPU-Hours\n42.5\n45.0\n47.5\n50.0\n52.5\nAccuracy (%)\nAverage\nTinyLlama\nPythia-1.4B\nFigure 2: Evolution of performance in commonsense reasoning benchmarks during pre-training. The perfor-\nmance of Pythia-1.4B is also included in the figure for comparison.\nProblem-solving evaluation\nWe also evaluate TinyLlama\u2019s problem-solving capabilities using the\nInstructEval benchmark (Chia et al., 2023). This benchmark includes the following tasks:\n\u2022 Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021): This task is\nused to measure a model\u2019s world knowledge and problem-solving capabilities across various\nsubjects. We evaluate the models in a 5-shot setting.\n\u2022 BIG-Bench Hard (BBH) (Suzgun et al., 2023): This is a subset of 23 challenging tasks from\nthe BIG-Bench benchmark (Srivastava et al., 2022) designed to measure a language model\u2019s\nabilities in complex instruction following. The models are evaluated in a 3-shot setting.\n\u2022 Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019): This reading comprehen-\nsion task measures a model\u2019s math reasoning abilities. We evaluate the models in a 3-shot\nsetting.\n\u2022 HumanEval (Zheng et al., 2023): This task is used to measure a model\u2019s programming\ncapabilities. The models are evaluated in a zero-shot setting.\nThe evaluation results are presented in Table 3. We observe that TinyLlama demonstrates better\nproblem-solving skills compared to existing models.\nTable 3: Performance of problem-solving tasks on the InstructEval Benchmark.\nMMLU\nBBH\nHumanEval\nDROP\nAvg.\n5-shot\n3-shot\n0-shot\n3-shot\nPythia-1.0B\n25.70\n28.19\n01.83\n04.25\n14.99\nPythia-1.4B\n25.41\n29.01\n04.27\n12.27\n17.72\nTinyLlama-1.1B\n25.34\n29.65\n09.15\n15.34\n19.87\n5In our initial dataset preprocessing, we inadvertently over-inserted end-of-sequence (EOS) tokens. This\nexcess of EOS tokens may have negatively affected the model by introducing substantial less meaningful signals\ninto the training data. However, after approximately 2.3T tokens, we removed these repetitive EOS tokens and\ncontinued pre-training TinyLlama with our refined data. This rectification likely contributed significantly to\nthe observed sudden improvements in performance on benchmarks such as hellasag, piqa, arc_challenge, and\narc_easy during that period.\n5\n4\nConclusion\nIn this paper, we introduce TinyLlama, an open-source, small-scale language model. To promote\ntransparency in the open-source LLM pre-training community, we have released all relevant infor-\nmation, including our pre-training code, all intermediate model checkpoints, and the details of our\ndata processing steps. With its compact architecture and promising performance, TinyLlama can\nenable end-user applications on mobile devices, and serve as a lightweight platform for testing a\nwide range of innovative ideas related to language models. We will leverage the rich experience\naccumulated during the open, live phase of this project and aim to develop improved versions of\nTinyLlama, equipping it with a diverse array of capabilities to enhance its performance and versatility\nacross various tasks. We will document further findings and detailed results in upcoming reports.\nAcknowledgements\nWe express our gratitude to the open-source community for their strong support during the open,\nlive phase of our research. Special thanks go to Qian Liu, Longxu Dou, Hai Leong Chieu, and\nLarry Law for their help to our project. This research/project is supported by Ministry of Education,\nSingapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award\nNo.: MOE-T2EP20122-0011), Ministry of Education, Singapore, under its Tier 3 Programme (The\nAward No.: MOET320200004), the National Research Foundation Singapore and DSO National\nLaboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016), an AI\nSingapore PhD Scholarship (AISG Award No: AISG2-PhD-2021-08-007), an SUTD Kick-Starter\nProject (SKI 2021_03_11), and the grant RS-INSUR-00027-E0901-S00. Any opinions, findings and\nconclusions or recommendations expressed in this material are those of the authors and do not reflect\nthe views of the funding agencies.\nReferences\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. (2023). GQA:\nTraining generalized multi-query transformer models from multi-head checkpoints. In Proceedings\nof EMNLP.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey,\nP., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-Hellstern, K., Mishra, G.,\nMoreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego,\nG. H., Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M.,\nCheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M.,\nDev, S., Devlin, J., D\u00edaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M.,\nGarcia, X., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J.,\nHu, A., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Krikun,\nM., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin,\nH., Liu, Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado,\nZ., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R.,\nQiao, S., Reif, E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby, R.,\nSlone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan, V., Vodrahalli,\nK., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L., Yin, P.,\nYu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and Wu, Y. (2023). Palm 2\ntechnical report.\nBai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji,\nL., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan,\nC., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang,\nJ., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou,\nC., Zhou, J., Zhou, X., and Zhu, T. (2023). Qwen technical report.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M. A.,\nPurohit, S., Prashanth, U. S., Raff, E., et al. (2023). Pythia: A suite for analyzing large language\nmodels across training and scaling. In Proceedings of ICML.\n6\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. (2020). Piqa: Reasoning about physical commonsense in\nnatural language. In Proceedings of AAAI.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,\nRamesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,\nChess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).\nLanguage models are few-shot learners. In Proceedings of NeurIPS.\nChia, Y. K., Hong, P., Bing, L., and Poria, S. (2023). INSTRUCTEVAL: towards holistic evaluation\nof instruction-tuned large language models. CoRR, abs/2306.04757.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\nH. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). BoolQ:\nExploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018).\nThink you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457.\nDao, T. (2023). Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691.\nDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. (2019). DROP: A reading\ncomprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of NAACL.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu,\nJ., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L.,\nSchoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.\n(2023). A framework for few-shot language model evaluation.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021).\nMeasuring massive multitask language understanding. In Proceedings of ICLR.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de las Casas, D.,\nHendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche,\nG., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and Sifre,\nL. (2022). Training compute-optimal large language models. In Proceedings of NeurIPS.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,\nA., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361.\nLefaudeux, B., Massa, F., Liskovich, D., Xiong, W., Caggiano, V., Naren, S., Xu, M., Hu, J., Tintore,\nM., Zhang, S., Labatut, P., and Haziza, D. (2022). xformers: A modular and hackable transformer\nmodelling library. https://github.com/facebookresearch/xformers.\nLi, R., allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., LI,\nJ., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Lamy-Poirier, J.,\nMonteiro, J., Gontier, N., Yee, M.-H., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang,\nZ., Murthy, R., Stillerman, J. T., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z.,\nBhattacharyya, U., Yu, W., Luccioni, S., Villegas, P., Zhdanov, F., Lee, T., Timor, N., Ding,\nJ., Schlesinger, C. S., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Anderson, C. J.,\nDolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M.,\nHughes, S., Wolf, T., Guha, A., Werra, L. V., and de Vries, H. (2023). Starcoder: may the source\nbe with you! Transactions on Machine Learning Research.\nLoshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization. In Proceedings of ICLR.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). Can a suit of armor conduct electricity?\na new dataset for open book question answering. In Proceedings of EMNLP.\n7\nMuennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., and\nRaffel, C. (2023). Scaling data-constrained language models. In Proceedings of NeurIPS.\nOpenAI (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2021). Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106.\nShazeer, N. (2020). GLU variants improve transformer. CoRR, abs/2002.05202.\nSoboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. (2023). SlimPajama:\nA 627B token cleaned and deduplicated version of RedPajama.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro,\nA., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. (2021). Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint arXiv:2104.09864.\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q.,\nChi, E., Zhou, D., and Wei, J. (2023). Challenging BIG-bench tasks and whether chain-of-thought\ncan solve them. In Findings of ACL.\nThadd\u00e9e, Y. T. (2023).\nChinchilla\u2019s death.\nhttps://espadrine.github.io/blog/posts/chinchilla-s-\ndeath.html.\nTogether Computer (2023). Redpajama: an open dataset for training large language models.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal,\nN., Hambro, E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. (2017). Attention is all you need. In Proceedings of NeurIPS.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of\nthought prompting elicits reasoning in large language models. In Proceedings of NeurIPS.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: Can a machine\nreally finish your sentence? In Proceedings of the ACL.\nZhang, B. and Sennrich, R. (2019). Root mean square layer normalization. In Proceedings of\nNeurIPS.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X.,\nLin, X. V., et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068.\nZheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Shen, L., Wang, Z., Wang, A., Li, Y.,\nSu, T., Yang, Z., and Tang, J. (2023). Codegeex: A pre-trained model for code generation with\nmultilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023,\npages 5673\u20135684. ACM.\n8\n"
  },
  {
    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
    "link": "https://arxiv.org/pdf/2401.02038.pdf",
    "upvote": "57",
    "text": "Understanding LLMs: A Comprehensive Overview from Training\nto Inference\nYiheng Liua, Hao Hea, Tianle Hana, Xu Zhanga, Mengyuan Liua, Jiaming Tiana,\nYutong Zhangb, Jiaqi Wangc, Xiaohui Gaod, Tianyang Zhongd, Yi Pane, Shaochen Xue,\nZihao Wue, Zhengliang Liue, Xin Zhangb, Shu Zhangc, Xintao Hud, Tuo Zhangd,\nNing Qianga, Tianming Liue and Bao Ge a\naSchool of Physics and Information Technology, Shaanxi Normal University, Xi\u2019an, 710119, Shaanxi, China\nbInstitute of Medical Research, Northwestern Polytechnical University, Xi\u2019an, 710072, Shaanxi, China\ncSchool of Computer Science, Northwestern Polytechnical University, Xi\u2019an, 710072, Shaanxi, China\ndSchool of Automation, Northwestern Polytechnical University, Xi\u2019an, 710072, Shaanxi, China\neSchool of Computing, The University of Georgia, Athens, 30602, USA\nA R T I C L E I N F O\nKeywords:\nLarge Language Models\nTraining\nInference\nSurvey\nA B S T R A C T\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large\nLanguage Models (LLMs) for addressing downstream tasks. There\u2019s an increasing focus on\ncost-efficient training and deployment within this context. Low-cost training and deployment of\nLLMs represent the future development trend. This paper reviews the evolution of large language\nmodel training techniques and inference deployment technologies aligned with this emerging\ntrend. The discussion on training includes various aspects, including data preprocessing, training\narchitecture, pre-training tasks, parallel training, and relevant content related to model fine-\ntuning. On the inference side, the paper covers topics such as model compression, parallel\ncomputation, memory scheduling, and structural optimization. It also explores LLMs\u2019 utilization\nand provides insights into their future development.\n1. Introduction\nLanguage modeling (LM) is a fundamental approach for achieving cognitive intelligence in the field of natural\nlanguage processing (NLP), and its progress has been notable in recent years [1; 2; 3]. It assumes a central role\nin understanding, generating, and manipulating human language, serving as the cornerstone for a diverse range of\nNLP applications [4], including machine translation, chatbots, sentiment analysis, and text summarization. With\nthe evolution of deep learning, the early statistical language models (SLM) have gradually transformed into neural\nlanguage models (NLM) based on neural networks. This shift is characterized by the adoption of word embeddings,\nrepresenting words as distributed vectors. Notably, these word embeddings have consistently excelled in practical NLP\ntasks, profoundly shaping the field\u2019s progress. Pre-trained language models (PLM) represent a subsequent phase in\nthe evolution of language models following NLM. Early attempts at PLMs included ELMo [5], which was built on a\nBidirectional LSTM architecture. However, with the advent of the transformer architecture [6], characterized by parallel\nself-attention mechanisms, the pre-training and fine-tuning learning paradigm has propelled PLM to prominence as\nthe prevailing approach. These models are typically trained via self-supervision on extensive datasets, cementing their\nstatus as the primary methodology in the field.\nThe Transformer architecture is exceptionally well-suited for scaling up models, and research analysis has revealed\nthat increasing the model\u2019s scale or training data size can significantly enhance its performance. Many studies have\npushed the boundaries of model performance by continuously expanding the scale of PLM [7; 8; 9; 10]. As models\ngrow larger, a remarkable phenomenon known as \"emergence\" occurs, wherein they exhibit astonishing performance\n[8]. These models are capable of generating high-quality text and possess robust learning and reasoning abilities. They\ncan even tackle few-shot learning tasks through in-context learning (ICL) [8]. This remarkable capability enables their\nseamless application to a wide range of downstream tasks across diverse domains [11; 12; 13; 14].\n\u2217Corresponding author\nORCID(s):\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 1 of 30\narXiv:2401.02038v2  [cs.CL]  6 Jan 2024\nA Comprehensive Overview from Training to Inference\nPre-trained language models (PLMs) with significantly larger parameter sizes and extensive training data are\ntypically denoted as Large Language Models (LLMs) [15; 16; 17]. The model size usually exceeds 6-10 billion (6-\n10B) parameters. A prominent milestone in the development of LLMs is exemplified by the GPT series [18; 7; 8; 19].\nNotably, OpenAI released ChatGPT in November 2022, marking a pivotal moment in the era of LLMs and a game-\nchanging moment in the field of artificial intelligence. ChatGPT has empowered current AI algorithms to achieve\nunprecedented levels of strength and effectiveness, reshaping the way humans employ or develop AI algorithms.\nIts emergence has captured the attention of the research community. However, owing to ChatGPT\u2019s absence as an\nopen-source platform, the principal way to use ChatGPT currently is by accessing it through OpenAI\u2019s website at\nhttps://chat.openai.com or via their API interface. Training LLMs that can serve as alternatives to ChatGPT, or\ndomain-specific LLMs, has become highly necessary [20; 21; 22; 23; 24; 1; 25; 26]. Training and deploying LLMs\ndemand expertise in handling large-scale data and substantial practical experience in distributed parallel training\n[27; 28; 29]. This requirement emphasizes the need for researchers developing LLMs to possess significant engineering\ncapabilities in addressing the challenges encountered during LLM development. Researchers who are interested in the\nfield of LLMs must possess engineering skills or learn to collaborate effectively with engineers.\nFor the above reasons, the primary objective of this paper is to provide a comprehensive overview of LLMs training\nand inference techniques to equip researchers with the knowledge required for developing, deploying, and applying\nLLMs. The structure of the rest of this review is as follows: In Section 2, we will introduce the relevant background\nand foundational knowledge of LLMs. In Section 3, we will delve into the technical aspects of training LLMs, while in\nSection 4 we will explore the technologies related to LLM\u2019s inference and deployment. In Section 5, we will discuss\nthe utilization of LLMs, and Section 6 will explore the future directions and their implications for LLMs.\n2. Background Knowledge\n2.1. Transformer\nTransformer is a deep learning model based on an attention mechanism for processing sequence data that can\neffectively solve complex natural language processing problems. This model was first proposed in 2017 [6], and\nreplaced the traditional recurrent neural network architecture [30] in machine translation tasks as the state-of-the-art\nmodel at that time. Due to its suitability for parallel computing and the complexity of the model itself, Transformer\noutperforms the previously popular recurrent neural networks in terms of accuracy and performance. The Transformer\narchitecture consists primarily of two modules, an Encoder and a Decoder, as well as the attention mechanism within\nthese modules.\n2.1.1. Self-Attention\nSelf-Attention Structure [6]: Essentially, the attention mechanism aims at selecting a small amount of important\ninformation from a large amount of data and focusing on these important pieces while ignoring the majority of\nunimportant information. The self-attention mechanism, as a variant of the attention mechanism, reduces reliance on\nexternal information and excels at capturing internal correlations within data or features. Applying the self-attention\nmechanism in text-primarily involves calculating the mutual influence between words to address the issue of long-range\ndependencies. Additionally, self-attention is the core idea behind transformers. The core formula for key-value attention\nis as follows:\n\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc44, \ud835\udc3e, \ud835\udc49 ) = \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc44\ud835\udc3e\ud835\udc47\n\u221a\n\ud835\udc51\ud835\udc58\n)\ud835\udc49\n(1)\nSelf-attention allows the model to weigh the importance of different words in a sentence when predicting a particular\nword. It calculates a weighted sum of the values of all words in the sentence, where the weights are determined by the\nrelevance of each word to the target word.\nThe self-attention mechanism consists of three steps: calculating the query, key, and value vectors. The query vector\nrepresents the word being attended to, while the key vectors represent all the words in the sentence. The value vectors\nstore the information associated with each word. The attention weights are computed by taking the dot product between\nthe query and key vectors, followed by a softmax operation to obtain a distribution over the words.\nMulti-Head Attention [6]: Multi-head self-attention extends the self-attention mechanism by performing it\nmultiple times in parallel. Each attention head learns to focus on different aspects of the input, capturing different\ndependencies and patterns. The outputs of the attention heads are then concatenated and linearly transformed to obtain\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 2 of 30\nA Comprehensive Overview from Training to Inference\nthe final representation. By using multiple attention heads, the model can capture both local and global dependencies,\nallowing for a more comprehensive understanding of the input sequence. This parallelization also enhances the model\u2019s\ncapacity to capture complex relationships between words. The Multi-head attention can be formulated as follows:\n\ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc56\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc44, \ud835\udc3e, \ud835\udc49 ) = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61[\u210e\ud835\udc52\ud835\udc4e\ud835\udc511, \u2026 , \u210e\ud835\udc52\ud835\udc4e\ud835\udc51\u210e]\ud835\udc4a \ud835\udc5c\n\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc56 = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc44\ud835\udc4a \ud835\udc44\n\ud835\udc56 , \ud835\udc3e\ud835\udc4a \ud835\udc3e\n\ud835\udc56 , \ud835\udc49 \ud835\udc4a \ud835\udc49\n\ud835\udc56 )\n(2)\nIn this case, \"\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61\" means to concatenate the attention calculation results of each head, \"\ud835\udc4a \ud835\udc5c\" is the weight\nmatrix of the output layer, used to linearly transform the concatenated results. This yields the output of multi-head\nattention. In summary, multi-head attention enhances the model\u2019s ability to represent input sequences by performing\nparallel attention calculations under different linear transformations, then concatenating and linearly transforming the\nresults. This mechanism plays an important role in the Transformer model, helping to handle long-range dependencies\nand improve model performance.\n2.1.2. Encoder\nThe encoder module [6] of the Transformer model is composed of multiple identical layers, each of which includes\na multi-head attention mechanism and feed-forward neural network [31]. In the multi-head attention mechanism, each\nposition in the input sequence is calculated for attention with other positions to capture the dependencies between\ndifferent positions in the input sequence. The feed-forward neural network is then used to further process and extract\nfeatures from the output of the attention mechanism. The encoder module gradually extracts features of the input\nsequence through the stacking of multiple such layers and passes the final encoding result to the decoder module for\ndecoding. The design of the encoder module enables it to effectively handle long-range dependencies within the input\nsequence and has significantly improved performance in various NLP tasks.\n2.1.3. Decoder\nThe decoder module [32] of the Transformer model is also composed of multiple identical layers, each of which\nincludes a multi-head attention mechanism and a feed-forward neural network. Unlike the encoder, the decoder also\nincludes an additional encoder-decoder attention mechanism, used to compute attention on the input sequence during\nthe decoding process. At each position, the decoder can only perform self-attention calculations with the positions\nbefore it to ensure that the generation of the sequence does not violate grammar rules. Masks play an important role\nin the decoder, ensuring that only information before the current time step is focused on when generating the output\nsequence, and not leaking information from future time steps. Specifically, the decoder\u2019s self-attention mechanism\nuses masks to prevent the model from accessing future information when generating predictions at each time step,\nmaintaining the causality of the model. This ensures that the output generated by the model depends on the information\nat the current time step and before, without being influenced by future information.\n2.1.4. Positional Embedding\nPosition and order are crucial for certain tasks, such as understanding a sentence or a video. Position and order\ndefine the grammar of a sentence, they are integral to the semantics of sentences. The Transformer utilizes Multi-Head\nSelf-Attention (MHSA) to avoid the recursive approach of RNN, thus speeding up the training process. Additionally,\nit can capture long-range dependencies in sentences and handle longer inputs. When each token in a sentence passes\nthrough the Transformer\u2019s Encoder/Decoder stack, the model itself lacks any sense of position/order for each token\n(permutation invariance). Therefore, a method is still needed to incorporate the sequential information of tokens into\nthe model. To enable the model to perceive the input sequence, positional information about the location of each\ntoken in the sentence can be added, and this technique is known as positional embedding (PE). which is used in the\nTransformer model to incorporate the sequential order of tokens into the input representation. Since the Transformer\ndoes not have recurrent connections, it lacks the inherent notion of token order present in recurrent neural networks.\nTo address this, positional embedding assigns a unique vector to each token position in the input sequence. These\npositional embeddings are added to the word embedding before being fed into the model. By including positional\ninformation, the model can differentiate between tokens based on their position in the sequence. In the Transformer\nmodel, the core formula of the position embedding can be expressed as:\n\ud835\udc43 \ud835\udc38(\ud835\udc5d\ud835\udc5c\ud835\udc60, 2\ud835\udc56) = \ud835\udc60\ud835\udc56\ud835\udc5b(\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n10000\n(\n2\ud835\udc56\n\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ) )\n(3)\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 3 of 30\nA Comprehensive Overview from Training to Inference\n\ud835\udc43 \ud835\udc38(\ud835\udc5d\ud835\udc5c\ud835\udc60, 2\ud835\udc56 + 1) = \ud835\udc50\ud835\udc5c\ud835\udc60(\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n10000\n(\n2\ud835\udc56\n\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ) )\n(4)\nIn this equation, \ud835\udc43 \ud835\udc38 represents the position embedding matrix, \ud835\udc5d\ud835\udc5c\ud835\udc60 represents the position of a token in the sentence,\n\ud835\udc56 represents the dimension index of the position embedding, and \ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 represents the hidden layer dimension of the\nTransformer model. By using sine and cosine functions and performing different calculations on the position (pos) and\ndimension (i), this formula generates unique position embedding values for each position and dimension. As a result,\neach token is assigned a unique position embedding vector, allowing the model to perceive the sequential information of\ntokens in the sentence. In practical applications, the position embedding matrix is added to the input word embedding\nmatrix to combine position information and semantic information, thereby providing a more comprehensive input\nrepresentation for the Transformer model.\nTwo commonly used positional encoding methods in Transformer are Absolute Positional Encoding and Relative\nPositional Encoding.\n(1) Absolute Positional Encoding: It generates unique positional embedding values for each position and dimension\nby using sine and cosine functions. This method uses sine and cosine functions in the mentioned formula to calculate\nthe positional embedding values and adds them to the word embeddings. Absolute Positional Encoding provides a\nunique encoding for each position, enabling the model to perceive the sequential information of words in the sentence.\n(2) Relative Positional Encoding: It is an encoding method based on relative positional relationships. Relative\nPositional Encoding represents positional information by calculating the relative distances between words. This method\nis used in models like Transformer-XL [33], and Relative Positional Encoding can better capture the relative positional\nrelationships between words when dealing with long sequences. Both of these positional encoding methods aim to\nprovide the positional information of words in the input sequence to the Transformer model, enabling the model to\nbetter comprehend and process sequential data. The specific choice of positional encoding method depends on the\nspecific application scenario and model design.\nThere are also other positional encoding methods applied to other models, such as RoPE [34] and ALiBi [35].\nRoPE is a method that uses Absolute Positional Encoding to represent Relative Positional Encoding and is applied\nin the design of large language models like PaLM [36], LLaMA [9], and GLM-130B [37].\nALiBi does not add positional embeddings to word embeddings but instead adds a pre-defined bias matrix to the\nattention score based on the distance between tokens. It is applied in the design of large language models like BLOOM\n[38].\nSome other positional encoding methods, such as mixed positional encoding, multi-digit positional encoding, and\nimplicit positional encoding, are also used by some models.\n2.2. Prompt Learning\nPrompt learning serves as a widely adopted machine learning approach, particularly in the field of NLP. At its\ncore, this methodology involves guiding a model to produce specific behaviors or outputs through the careful design\nof prompt statements. It is commonly employed to fine-tune and guide pre-trained LLMs for executing particular\ntasks or generating desired results. Researchers have observed that the design of specific prompt statements can steer\npre-trained models to perform various tasks, such as question-answering, text generation, and semantic understanding\n[39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50]. The strength of this approach lies in its ability to adapt to different tasks\nthrough simple modifications to prompt statements, eliminating the need for retraining the entire model. For LLMs\nlike the GPT series and other pre-trained models, prompt learning provides a straightforward and powerful means\nfor model fine-tuning. By supplying appropriate prompts, researchers and practitioners can customize the model\u2019s\nbehavior, making it more suitable for specific domains or task requirements. In short, prompt learning is a machine\nlearning approach that, builds upon pre-trained language models, and guides the model to perform various tasks through\nthe design of prompt statements, offering increased flexibility for customizing model applications. In this Section, we\nwill introduce the basic knowledge of prompt learning.\n2.2.1. Background and Overview\nPrompt learning is a new approach to machine learning [51]. In the early field of natural language processing\n(NLP), researchers mainly used fully supervised learning mode[52], which trained models for specific tasks on the\ninput and output example dataset of the target task. However, due to the limited training dataset, this method cannot\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 4 of 30\nA Comprehensive Overview from Training to Inference\ntrain high-quality models well, so early NLP relied more on feature engineering; With the emergence of neural network\nmodels and their use in the field of NLP, people have begun to pay attention to architecture engineering [53].\nHowever, between 2017 and 2019, the learning approach of NLP models shifted from fully supervised learning to\na new mode: pre-train and fine-tune paradigm[54]. In this paradigm, a model with a fixed architecture is pre-trained\nas a language model to predict the probability of observed text data. Due to the abundant raw text data required for\ntraining language models, these language models can be trained on large datasets. During this process, language models\ncan learn robust universal features of the language they are modeling. Then, by introducing additional parameters and\nfine-tuning them using task-specific objective functions, the PLM mentioned above will adapt to different downstream\ntasks. At this point, the focus of research shifted to objective engineering, which is to design training objectives during\npre-training and fine-tuning. Since BERT, NLP has been using pre-training and fine-tuning methods for a long period\nof time, but this approach requires a new model to be fine-tuned for each task and cannot be shared. But for an LLM,\nit feels like customizing each task, which is very inefficient [51].\nPrompt learning, this method has demonstrated amazing capabilities in GPT-3. The GPT-3 model can handle\nmany tasks with only a few samples by using natural language prompts and task demonstrations as context, without\nupdating parameters in the underlying model. Prompt Learning replaces the process of pre-trained and fine-tuning\nwith pre-trained, prompts and predictions. In this paradigm, the downstream task is not to adapt the pre-trained LM\nto the downstream task through objective engineering, but to redefine the downstream task with the help of text\nprompts, making it look more like the tasks solved during the original LM training. For prompt learning, it is only\nnecessary to insert different prompt parameters to adapt to different tasks. That is to say, each task only needs to train\nthe prompt parameter separately, without the need to train the entire pre-trained language model[55]. This approach\ngreatly improves the efficiency of using pre-trained language models and significantly shortens training time.\n2.2.2. Basic components and process of Prompt learning\nIn the traditional pre-trained+fine-tuning paradigm, there is a gap between the pre-trained stage and downstream\ntasks [51], while prompt learning can maintain consistency between the pre-trained target format and downstream task\noutput format, that is, align the form of downstream tasks with the form of PLMs pre-trained tasks. When training\nPLMs, we can transform the original target task into a fill-in-the-blank or continuation task similar to the pre-trained\ntask of PLMs by constructing a prompt. The advantage of this method is that through a series of appropriate prompts,\nwe can use a single language model to solve various downstream tasks.\nPrompt learning optimizes the performance of models on different tasks by using pre-trained models and designing\nappropriate templates. Prompt learning consists of prompt templates, answer mappings, and pre-trained language\nmodels. The prompt template is the main body of the prompt, and fill in the blank [56] and generate based on\nprefix [57]are two common types of prompt learning templates. The fill-in-the-blank template selects one or more\npositions in the text and represents them with [MASK] tags, used to prompt the model to fill in the corresponding\nwords; Prefix-based template generation involves adding a specific prefix before a sentence to guide the model\nin generating appropriate text. Answer mapping is the process of evaluating all possible answers according to a\nprobability distribution, selecting the most likely answer as the predicted output, and converting it into appropriate\ncategory mapping words. This process typically involves converting labels into natural language vocabulary, known\nas Verbalizer [58].\nThe workflow of Prompt learning mainly includes the following four parts:\n(1)Use PLMs as base encoders\n(2)Add additional context (template) with a [MASK] position\n(3)Project labels to label words (verbalizer)\n(4)Be the GAP between pre-training and fine-tuning\nAfter defining the template and answer space, we need to choose a suitable pre-trained language model. There are\nnow various pre-trained models (PTMs) with good performance, and when selecting a model, one usually considers its\nparadigm, such as Auto recursive, Masked Language Modeling, Encoder Decoder, etc. Based on this, for the summary\ntask, a more suitable Bidirectional and Auto-Regressive Transformers (BART) model can be selected.\nThe selection of a template plays a very important role in the prompt learning. Templates can generally be\ndistinguished based on whether they are manually specified: artificially constructed templates or automatically searched\ntemplates. Artificially created templates are the most intuitive method, easy to understand, and have good performance\nin practical applications. However, artificially constructed templates also have some drawbacks: prior knowledge is\nrequired when designing templates manually [59], and there may be failures [60]. There are two types of automatically\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 5 of 30\nA Comprehensive Overview from Training to Inference\ngenerated templates: discrete prompts and continuous prompts. Discrete prompts allow the model to select the optimal\ntemplate in a set of discrete template spaces, while continuous prompts allow the language model to automatically\ntrain a prompt. According to research, using multiple templates [61] can improve the performance of the model. The\nsimplest way to choose to use multiple templates and aggregate them together to complete an answer is to take the\naverage [60] or weighted average of each template output [58].\nVerbalizer is the process of mapping labels to label words, and the selection of verbalizers is also crucial for prompt\nlearning. There are two ways to construct a verbalizer: manual definition and automatic search. The manual definition\nrequires professional knowledge and may have disadvantages such as strong subjectivity and a small coverage area. To\nsolve this problem, we can choose the following solutions: (1) Manually design with human prior knowledge; (2) Start\nwith an Intel label word, paraphrase and expand; (3) Start with an internal label word, using external knowledge and\nexpand; (4) Decompose the label with multiple tokens; (5) Virtual token and optimize the label embedding. In addition,\nwe can use external knowledge bases to expand and improve label words, thereby achieving better text classification\nresults[62].\n2.2.3. learning strategy\nThe emergence of the new paradigm of Prompt learning has brought significant changes to the training process.\nThe learning strategies for Prompt learning mainly include the following: (1) Pre-training then fine-tuning, which is a\ntraditional pre-training+fine tuning method [63]; (2) Tuning free promotion, relying on the designer LM of prompts to\ndirectly provide answers [64]; (3) Fixed LM prompt tuning, which updates the relevant parameters of prompts using\ndownstream task training data; (4) Fix prompt LM tuning, this strategy is to fine-tune the parameters of LM, which\nhave fixed parameters when using prompts; (5) Prompt+LM tuning is a strategy that updates both prompts related\nparameters and LM parameters.\nThese different learning strategies can be selected based on specific tasks and needs. Pre-training + fine-tuning is\nthe most common strategy, suitable for most tasks [63]. No fine-tuning prompts are suitable for simple tasks, which\ncan greatly reduce training time and computational resource consumption. Fixed LM prompt fine-tuning and fixed\nprompt LM fine-tuning are suitable for tasks that require more precise control and can optimize model performance\nby adjusting prompt parameters or language model parameters. Combining prompts and LM fine-tuning combines the\nadvantages of both and can further improve model performance [51].\nIn summary, Prompt learning provides us with a new training paradigm that can optimize model performance\non various downstream tasks through appropriate prompt design and learning strategies. Choosing the appropriate\ntemplate, constructing an effective verbalizer, and adopting appropriate learning strategies are all important factors in\nimproving the effectiveness of prompt learning.\n3. Training of Large Language Models\nThe training of LLMs can be broadly divided into three steps. The first step involves data collection and processing.\nThe second step encompasses the pre-training process, which includes determining the model\u2019s architecture and pre-\ntraining tasks and utilizing suitable parallel training algorithms to complete the training. The third step involves fine-\ntuning and alignment. In this section, we will provide an overview of the model training techniques. This will include an\nintroduction to the relevant training datasets, data preparation and preprocessing, model architecture, specific training\nmethodologies, model evaluation, and commonly used training frameworks for LLMs.\n3.1. Data Preparation and Preprocessing\n3.1.1. Dataset\nTraining LLMs require vast amounts of text data, and the quality of this data significantly impacts LLM\nperformance. Pre-training on large-scale corpora provides LLMs with a fundamental understanding of language and\nsome generative capability. The first step in LLM training is collecting substantial corpora of natural language text.\nPre-training data sources are diverse, commonly incorporating web text, conversational data, and books as general\npre-training corpora. Additionally, some research efforts introduce specialized data from professional domains, such\nas code or scientific data, to enhance LLM capabilities in those fields. Leveraging diverse sources of text data for LLM\ntraining can significantly enhance the model\u2019s generalization capabilities. In the following section, we will present\nthe commonly used datasets for training LLMs as shown in Table 1. These corpora are categorized into 5 groups for\ndiscussion.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 6 of 30\nA Comprehensive Overview from Training to Inference\nTable 1\nCommonly used corpora information.\nCorpora\nType\nLinks\nBookCorpus [65]\nBooks\nhttps://github.com/soskek/bookcorpus\nGutenberg [66]\nBooks\nhttps://www.gutenberg.org\nBooks1 [8]\nBooks\nNot open source yet\nBooks2 [8]\nBooks\nNot open source yet\nCommonCrawl [67]\nCommonCrawl\nhttps://commoncrawl.org\nC4 [68]\nCommonCrawl\nhttps://www.tensorflow.org/datasets/catalog/c4\nCC-Stories [69]\nCommonCrawl\nNot open source yet\nCC-News [70]\nCommonCrawl\nhttps://commoncrawl.org/blog/news-dataset-available\nRealNews [71]\nCommonCrawl\nhttps://github.com/rowanz/grover/tree/master/realnews\nRefinedWeb [72]\nCommonCrawl\nhttps://huggingface.co/datasets/tiiuae/falcon-refinedweb\nWebText\nReddit Link\nNot open source yet\nOpenWebText [73]\nReddit Link\nhttps://skylion007.github.io/OpenWebTextCorpus/\nPushShift.io [74]\nReddit Link\nhttps://pushshift.io/\nWikipedia [75]\nWikipedia\nhttps://dumps.wikimedia.org/zhwiki/latest/\nBigQuery [76]\nCode\nhttps://cloud.google.com/bigquery\nCodeParrot\nCode\nhttps://huggingface.co/codeparrot\nthe Pile [77]\nOther\nhttps://github.com/EleutherAI/the-pile\nROOTS [78]\nOther\nhttps://huggingface.co/bigscience-data\nBooks: Two commonly utilized books datasets for LLMs training are BookCorpus [65] and Gutenberg [66]. These\ndatasets include a wide range of literary genres, including novels, essays, poetry, history, science, philosophy, and more.\nWidely employed by numerous LLMs [9; 79], these datasets contribute to the models\u2019 training by exposing them to a\ndiverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across\nvarious domains.\nCommonCrawl: CommonCrawl [67] manages an accessible repository of web crawl data, freely available for\nutilization by individuals and organizations. This repository encompasses a vast collection of data, comprising over\n250 billion web pages accumulated over a span of 16 years. Established in 2007, Common Crawl has evolved into a\nwidely recognized and referenced corpus in the academic and research communities, cited in more than 10,000 research\npapers. This continuously expanding corpus is a dynamic resource, with an addition of 3\u20135 billion new web pages each\nmonth. Its significance extends to the field of natural language processing, where it serves as a primary training corpus\nin numerous large language models. Notably, a substantial portion of the raw tokens employed in training GPT-3\n[8], amounting to 82%, is sourced from the CommonCrawl. However, due to the presence of a substantial amount of\nlow-quality data in web archives, preprocessing is essential when working with CommonCrawl data. Currently, four\ncommonly used filtered datasets based on CommonCrawl are available: C4 [68], CC-Stories [69], CC-News [70], and\nRealNews [71].\nReddit Links: Reddit is a social media platform where users can submit links and posts, and others can vote on\nthem using the \"upvote\" or \"downvote\" system. This characteristic makes it a valuable resource for creating high-quality\ndatasets.\nWikipedia: Wikipedia [75], a free and open online encyclopedia project, hosts a vast repository of high-quality\nencyclopedic content spanning a wide array of topics. The English version of Wikipedia is extensively utilized in the\ntraining of many LLMs [8; 9; 80], serving as a valuable resource for language understanding and generation tasks.\nAdditionally, Wikipedia is available in multiple languages, providing diverse language versions that can be leveraged\nfor training in multilingual environments.\nCode: There is a limited availability of publicly accessible code datasets at present. Existing efforts primarily\ninvolve web scraping of code with open-source licenses from the internet. The main sources include Github and Stack\nOverflow.\nWe have organized datasets utilized by distinct LLMs. During the training process, LLMs are typically trained on\nmultiple datasets, as specified in Table 2 for reference.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 7 of 30\nA Comprehensive Overview from Training to Inference\nTable 2\nDatasets utilized by distinct LLMs\nLLMs\nDatasets\nGPT-3 [8]\nCommonCrawl [67], WebText2 [8], Books1 [8], Books2 [8], Wikipedia [75]\nLLaMA [9]\nCommonCrawl [67], C4 [68], Wikipedia [75], Github, Books, Arxiv, StackExchange\nPaLM [36]\nSocial Media, Webpages, Books, Github, Wikipedia, News (total 780B tokens)\nT5 [68]\nC4 [68], WebText, Wikipedia, RealNews\nCodeGen [81]\nthe Pile, BIGQUERY, BIGPYTHON\nCodeGeeX [82]\nCodeParrot, the Pile, Github\nGLM [37]\nBooksCorpus, Wikipedia\nBLOOM [38]\nROOTS\nOPT [83]\nBookCorpus, CCNews, CC-Stories, the Pile, Pushshift.io\n3.1.2. Data preprocessing\nOnce an adequate corpus of data is collected, the subsequent step is data preprocessing. The quality of data\npreprocessing directly impacts the model\u2019s performance and security. The specific preprocessing steps involve filtering\nlow-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical\nstandards. It also includes deduplication, removing duplicates in the training set, and excluding redundant content\nin the test set to maintain the sample distribution balance. Privacy scrubbing is applied to ensure the model\u2019s security,\npreventing information leakage or other privacy-related concerns. Additionally, if fine-tuning LLMs is considered,\nexpanding the vocabulary should also be considered. On the other hand, LLaMA 2 models [10] represent a notable\nexception. These models forego filtering in their pretraining corpus, as aggressive filtration might accidentally filter\nout some demographic groups. This approach enhances the generalizability of the base LLaMA 2 models, making\nthem more adept across a range of downstream tasks, such as hate speech detection and privacy de-identification.\nObservations indicate that abstaining from additional filtering in the pretraining data enables the base model to achieve\nreasonable safety alignment with fewer examples [10]. While this increases both generalizability and safety alignment\nefficiency, the implementation of additional safety mitigations is still imperative prior to public deployment, as further\ndiscussed in Section 3.5.4.\nQuality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based\nmethods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For\ninstance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase\nletters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve\ntraining a classifier on a high-quality dataset such as WebText [85] to filter out low-quality datasets.\nDeduplication: Language models may sometimes repetitively generate the same content during text generation,\npotentially due to a high degree of repetition in the training data. Extensive repetition can lead to training instability,\nresulting in a decline in the performance of LLMs [86]. Additionally, it is crucial to consider avoiding dataset\ncontamination by removing duplicated data present in both the training and testing set [87].\nPrivacy scrubbing: LLMs, as text-generating models, are trained on diverse datasets, which may pose privacy\nconcerns and the risk of inadvertent information disclosure [88]. In the preprocessing phase of language datasets, it is\nimperative to address privacy concerns by systematically removing any sensitive information. This involves employing\ntechniques such as anonymization, redaction, or tokenization to eliminate personally identifiable details, geolocation,\nand other confidential data. By carefully scrubbing the dataset of such sensitive content, researchers and developers can\nensure that the language models trained on these datasets uphold privacy standards and mitigate the risk of unintentional\ndisclosure of private information. It is essential to strike a balance between data utility and privacy protection, fostering\nresponsible and ethical use of language datasets in various applications.\nFiltering out toxic and biased text: In the preprocessing steps of language datasets, a critical consideration is the\nremoval of toxic and biased content to ensure the development of fair and unbiased language models. This involves\nimplementing robust content moderation techniques, such as employing sentiment analysis, hate speech detection, and\nbias identification algorithms. By leveraging these tools [89], researchers can systematically identify and filter out text\nthat may perpetuate harmful stereotypes, offensive language, or biased viewpoints.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 8 of 30\nA Comprehensive Overview from Training to Inference\n3.2. Architecture\nCurrently, all LLMs are built upon the Transformer architecture, allowing these models to scale to several 10 billion\nor even a trillion parameters. Typically, PLM architectures fall into three categories: Encoder-only [90], Encoder-\ndecoder [68] and Decoder-only [18]. The Encoder-only architecture is no longer employed in the latest LLMs and\nwon\u2019t be further discussed here. Instead, this section will focus on introducing the Encoder-decoder and Decoder-only\narchitectures.\nEncoder-decoder Architecture\nIt\u2019s\na\ngood\nday\nIt\u2019s\na\ngood\nday\nto\ndream\n.\nto\ndream\n.\nCausal Decoder Architecture\nPrefix Decoder Architecture\nIt\u2019s\na\ngood\nday\nto\na\ngood\nday\nto\ndream\nIt\u2019s\na\ngood\nday\nto\na\ngood\nday\nto\ndream\nIt\u2019s\nIt\u2019s\na\ngood\nday\na\ngood\nday\nto\ndream\n.\nto\ndream\n.\nIt\u2019s\nIt\u2019s\na\ngood\nday\na\ngood\nday\nto\nto\nIt\u2019s\nIt\u2019s\na\ngood\nday\na\ngood\nday\nto\nto\nEncoder\nDecoder\nDecoder\nDecoder\nFigure 1: The figures from left to right represent the Encoder-decoder architecture, Causal Decoder architecture, Prefix\nDecoder architecture, and their mask configurations, respectively. This diagram illustrates the range of tokens that each\ninput token can attend to.\n3.2.1. Encoder-decoder Architecture\nThe Encoder-decoder architecture of LLMs is built upon the traditional Transformer Encoder-decoder architecture.\nThe Encoder-decoder architecture consists of two main components: the Encoder and the Decoder. Each part of the\nEncoder is composed of multiple layers of Transformer\u2019s Multi-Head Self-Attention layers, which encode the input\nsequence. The Decoder, on the other hand, utilizes cross-attention over the output representation of the Encoder and\ngenerates the target sequence in an autoregressive manner. The encoder-decoder architecture serves as the foundation\nfor prominent LLMs such as T5 [68], flan-T5 [91], and BART [92].\n3.2.2. Decoder-only Architecture\nLLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture.\nUnlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only\narchitecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens,\nattending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks,\nshowcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase.\nThe Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the\nPrefix Decoder architecture.\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence\ncan only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the\ninput sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by\nconfiguring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoder-\ndecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 9 of 30\nA Comprehensive Overview from Training to Inference\nthe Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for\ntheir superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as\nBLOOM [38], OPT [83], Gopher [84], and LLaMA [9].\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoder-\ndecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1,\nenabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating\nsubsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility\nto attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include\nPaLM [36] and GLM [37].\n3.3. Pre-training Tasks\nLarge Language Models (LLMs) typically learn rich language representations through a pre-training process.\nDuring pre-training, these models leverage extensive corpora, such as text data from the internet, and undergo training\nthrough self-supervised learning methods. Language modeling is one common form of self-supervised learning task\nin which the model is tasked with predicting the next word in a given context. Through this task, the model acquires\nthe ability to capture information related to vocabulary, grammar, semantics, and text structure.\nIn language modeling [18; 7; 8; 36], the model is required to predict the next word in a given context. This task\nenables the model to develop a nuanced understanding of language. Specifically, the model observes large amounts\nof textual data and attempts to predict the next word at each position in the text. This gradual learning process\nallows the model to capture the patterns and information inherent in language, encoding a vast amount of linguistic\nknowledge into its parameters. Once pre-training is complete, these model parameters can be fine-tuned for various\nnatural language processing tasks to adapt to specific task requirements. The objective of language modeling is to train\na model to maximize the likelihood of textual data. For a given text sequence, denoted as \ud835\udc641, \ud835\udc642, ..., \ud835\udc64\ud835\udc47 , where \ud835\udc64\ud835\udc61\nrepresents the token at position \ud835\udc61, \ud835\udc43(\ud835\udc64\ud835\udc61|\ud835\udc641, \ud835\udc642, ..., \ud835\udc64\ud835\udc61\u22121) is the probability of predicting \ud835\udc64\ud835\udc61 given the preceding context\n\ud835\udc641, \ud835\udc642, ..., \ud835\udc64\ud835\udc61\u22121, the objective function for language modeling can be expressed using cross-entropy loss. Here, we\ndefine the objective as maximizing the conditional probability of the given text sequence:\n\ud835\udc3f\ud835\udc3f\ud835\udc40 = 1\n\ud835\udc47\n\ud835\udc47\u2211\n\ud835\udc61=1\n\u2212\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43 (\ud835\udc64\ud835\udc61|\ud835\udc641, \ud835\udc642, ..., \ud835\udc64\ud835\udc61\u22121)\n(5)\nLanguage modeling serves as a prevalent pretraining objective for most LLMs. In addition to language modeling,\nthere are other pretraining tasks within the realm of language modeling. For instance, some models [68; 37] use text\nwith certain portions randomly replaced, and then employ autoregressive methods to recover the replaced tokens. The\nprimary training approach involves the autoregressive recovery of the replaced intervals.\n3.4. Model Training\n3.4.1. Parallel Training\nIn the parallel training mentioned below, there will be discussions about collective communication which helps us\nbetter understand the principles of parallel training. Figure 2 has listed five reduction relationships. 1)Broadcast: Send\ndata from one GPU to other GPUs.2)Reduce: Reduce(sum/average) data of all GPUs, send to one GPU.3)All Reduce:\nReduce all data of GPUs, send to all GPUs.4)Reduce Scatter: Reduce all data of GPUs, send portions to all GPUs.5)All\nGather: Gather data of all GPUs, send all GPUs.\nData Parallel: The process of data parallelism [94? ] is shown in Figure 3, there is a parameter server that stores\nthe model\u2019s parameters and the entire batch of data. Each GPU uses broadcast to synchronize the model parameters\nand divides the data into one portion per GPU, with each GPU receiving a portion of the data. Each GPU uses the\ncomplete model parameters and a portion of the data to perform forward and backward propagation. This way, the\ngradients are obtained for each GPU. Finally, we aggregate the gradients and send the aggregated gradients back to the\nparameter server, where the original model parameters and the aggregated complete gradients are available. With this\ninformation, we can use an optimizer to update the model parameters. The updated parameters will then enter the next\nround of model training iterations. Distributed data parallelism [95] abandons the use of a parameter server and instead\nemploys all-reduce on gradient information, ensuring that each GPU receives the same gradient information. The result\nof all-reduce is communicated to all GPUs, allowing them to independently update their respective model optimizers.\nAfter each round of updates, the model\u2019s parameters, gradients, and the historical information of the optimizer are\nconsistent across all GPUs.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 10 of 30\nA Comprehensive Overview from Training to Inference\nBroadcast\nrank0\nrank1 rank2\nrankN\nin\nrank0\nrank1 rank2\nrankN\nout\nout\nout\nout\nReduce\nrank0\nrank1 rank2\nrankN\nin3\nrank0\nrank1 rank2\nrankN\nout\nin1\nin2\ninN\nrank0\nrank1 rank2\nrankN\nin3\nin1\nin2\ninN\nout\nout\nout\nout\nAll Reduce\nrank0\nrank1 rank2\nrankN\nout0\nout1\nout2\noutN\nReduce Scatter\nrank0\nrank1 rank2\nrankN\nin3\nin1\nin2\ninN\nrank0\nrank1 rank2\nrankN\nReduce Scatter\nout\nout\nout\nout\nrank0\nrank1 rank2\nrankN\nrank0\nrank1 rank2\nrankN\nin0\nin1\nin2\ninN\nFigure 2: Five collective communications that are used by parallel training methods.\nThe occupation of GPU memory of intermediate results is related to the batch size, sentence length, and model\ndimensions. When using data parallelism, a batch of data is divided into many parts, allowing each GPU to process a\nportion of the data. In equivalent terms, the batch size processed on each GPU is reduced to one over the original number\nof GPUs. Data parallelism has reduced the input dimensions, resulting in an overall reduction in the intermediate results\nof the model. A drawback is that to support model training, each GPU needs to receive at least one piece of data. In\nthe most extreme case, when each GPU receives only one piece of data, our parameters, gradients, and optimizer still\nneed to be fully stored on the GPU. Even if we don\u2019t store any intermediate results on the GPU, our model may still be\nunable to perform computations on a single GPU.\nModel Parallel:Model parallelism [96] was first introduced by Megatron-LM to alleviate memory pressure. From\nfigure 4, we can clearly understand the overall architecture of model parallelism. Taking advantage of the most common\nlinear layer in the Transformer as an example, the parameters of the linear layer form a matrix of size A*B, and the\ninput to the linear layer is a vector of size B*1. Representing this as \ud835\udc66\ud835\udc34\u2217\ud835\udc35 = \ud835\udc4a\ud835\udc34\u2217\ud835\udc35\ud835\udc65\ud835\udc35, we can horizontally partition\nthe model\u2019s parameters into many segments using the property of matrix multiplication. Each segment is of size a\ndivided by n multiplied by B. Utilizing the properties of matrix multiplication, we can move \ud835\udc65\ud835\udc35 into parentheses, and\nfinally, the result of the linear layer is obtained by multiplying many small matrices with the parameters of the linear\nlayer. Through this approach, the parameters of the linear layer can be distributed across multiple GPUs. However, it is\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 11 of 30\nA Comprehensive Overview from Training to Inference\nParamete\nr\nRank1\nRank2\nRank3\nRankN\nData\nGradien\nt\nFP&BP\nAVG\nGradient*\nOptimizer\nParameter\nData\nUpdate\nReplicate\nRank1\nRank2\nRank3\nRankN\nFP&BP\nAll Reduce\nParameter\nData\nGradient\nGradient* Optimizer\nUpdate\nData parallel\nDistributed data parallel\nFigure 3: The architecture of data parallelism and distributed data parallelism. The diagram illustrates the difference\nbetween data parallelism and distributed data parallelism and the advantages of distributed data parallelism.\ncrucial to ensure that the inputs to the model on multiple GPUs are identical. Instead of using a data parallel approach\nto partition the data, we need to ensure that the inputs obtained on each GPU are the same, meaning they belong to\nthe same batch of data. We can then partition a parameter like the linear layer across GPUs, with each GPU receiving\na small portion of the matrix. By performing model calculations with this small portion and the data, we obtain a\nsub-result, as shown in Formula 5. The results of these computations need to be concatenated using the all-gather\noperator and communicated to all GPUs.\n\ud835\udc66\ud835\udc34\u2217\ud835\udc35 = \ud835\udc4a\ud835\udc34\u2217\ud835\udc35\ud835\udc65\ud835\udc35\n= [\ud835\udc4a (1)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f; \ud835\udc4a (2)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f; ...; \ud835\udc4a (\ud835\udc5b)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f]\ud835\udc65\ud835\udc35\n= [\ud835\udc4a (1)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f\ud835\udc65\ud835\udc35; \ud835\udc4a (2)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f\ud835\udc65\ud835\udc35; ...; \ud835\udc4a (\ud835\udc5b)\n\ud835\udc34\n\ud835\udc5b \u2217\ud835\udc4f\ud835\udc65\ud835\udc35]\n(6)\nZeRO: ZeRO [97] is a framework built on data parallelism. During the parameter updating process on each GPU,\nthe same set of parameters is used, leading to computational redundancy. Each GPU uses reduced scatter to eliminate\nthis redundancy to obtain a portion of the gradient results. After updating a portion of the model parameters on each\nGPU, an all-gather operation is performed to synchronize the parameters across all GPUs. After the all-gather operation,\nthe original gradient no longer needs to be saved on the graphics card and can be removed. Figure 5 shows the update\nof ZeRO. In ZeRO1, the original gradient is removed after backward propagation, while in ZeRO2, the product of\nthe gradient* is calculated in advance during backward propagation, and only the gradient* is saved on the graphics\ncard, removing the gradient. This way, the deletion of the gradient is advanced, leading to further savings in GPU\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 12 of 30\nA Comprehensive Overview from Training to Inference\nRank1\nRank2\nRank3\nRankN\nMatMul\nAll Gather\nParameter Data\nResult\nResult*\nParameter:1\nGradient:1\nOptimizer:>2\nintermediate\nModel paralllel\nFigure 4: The overall architecture of model parallelism. The left side of the diagram shows the process of model parallelism,\nand the right side shows the memory usage of parameters, gradients, and optimizers in the graphics card of the model\nparallelism method.\nmemory space. ZeRO3 conducts a detailed division of the model parameters. Each graphics card retains only a portion\nof the gradients for updating, and parameter updates also only affect a portion of the model parameters. Therefore,\neach graphics card only needs to store the parameters, gradients, and optimizer related to the part of the parameters\nit is responsible for. During forward and backward propagation, an all-gather operation is required once, and after the\noperation is complete, the model parameters are released from the graphics card.Zero3 does not use all gather during\nparameter updates, but it requires an all-gather operation during both forward and backward propagation, adding one\ncommunication step. Compared to ZeRO2, ZeRO3 is an algorithm that trades time for space.\nPipeline Parallel: Pipeline parallelism [98] and model parallelism share similarities. In model parallelism, linear\nlayers are divided into many small matrices, which are then distributed to different GPUs. For pipeline parallelism,\ndifferent layers of the model are assigned to different GPUs. Specifically, if we have an n-layer transformer, we can\nassign the \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56 of the transformer to the \ud835\udc3a\ud835\udc43\ud835\udc48\ud835\udc56, and so on. During the forward propagation of the model, we need\nto perform the computation of the \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56 on the \ud835\udc3a\ud835\udc43\ud835\udc48\ud835\udc56, then pass the result to the \ud835\udc3a\ud835\udc43\ud835\udc48\ud835\udc56+1. The \ud835\udc3a\ud835\udc43\ud835\udc48\ud835\udc56+1 receives the\noutput from the \ud835\udc3a\ud835\udc43 \ud835\udc48\ud835\udc56, performs the computation for that layer and passes the result to the next GPU. This method\npartitions the parameters, gradients, optimizer, and intermediate results for each layer.\n3.4.2. Mixed Precision Training\nIn recent years, to pre-train extremely large language models, some research [99] has begun to utilize 16-bit floating-\npoint numbers (FP16) to reduce memory usage and communication overhead. FP16 has a smaller numerical range and\nlower precision in effective digits [100; 38], but computations tend to be faster than FP32. In general model training,\nFP32 is often used as the default representation for training parameters. However, in actual model training, the number\nof parameters in a model typically does not exceed the order of thousands, well within the numerical range of FP16.\nTo improve computational speed, we can convert from FP32 to FP16. During parameter updates, the amount of the\nparameter is roughly equal to the gradient multiplied by the learning rate. The minimum value of FP16 is on the order\nof 1e-5. As the product of the gradient and learning rate is already well below the representation range of FP16, the\nparameter update would result in loss, known as underflow. Therefore, we represent the parameter update obtained by\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 13 of 30\nA Comprehensive Overview from Training to Inference\nRank1\nRank2\nRank3\nRankN\nFP&BP\nReduce Scatter\nParameter\nData\nGradient\nGradient*\nOptimizer\nUpdate with All Gather\nRank1\nRank2\nRank3\nRankN\nFP&BP\nReduce \nScatter\nParameter Data\nGradient\nGradient* Optimizer\nUpdate\nZeRO3\nZeRO1&ZeRO2\nParameter\nParameter*\nAll Gather\nWhen needed\nFigure 5: The overall architecture of ZeRO. The upper demonstrates ZeRO stage1 and ZeRO stage2. The lower displays\nZeRO stage3. The graph illustrates the optimization of memory usage of graphics card parameters in relation to ZeRO3\nversus ZeRO1 and ZeRO2\nmultiplying the gradient by the learning rate as FP32. We cannot directly add this high-precision parameter update\nto a lower-precision model, as this would still result in floating-point underflow. Consequently, we need to save an\nadditional single-precision parameter on the optimizer. To accelerate both forward and backward passes in the model,\nhalf-precision parameters and gradients are used and passed to the optimizer for updating. The optimizer\u2019s update\nquantity is saved as FP32, and we accumulate it effectively through a temporarily created FP32 parameter in the\noptimizer. After effective accumulation, it is then converted back to FP16 parameters.\n3.4.3. Offloading\nThe parameters in the optimizer are at least twice as many as the model parameters, and a study [101]proposes the\nidea of moving the optimizer\u2019s parameters from the GPU to the CPU. Although GPU computation is much faster than\nCPU, the question arises whether offloading this operation could become a bottleneck for the overall training speed\nof the model optimizer. In reality, we utilize ZeRO3. After the optimization with ZeRO3, the size of the parameters,\ngradients, and optimizer is reduced to 1/n of the number of GPUs. By binding one GPU to multiple CPUs, we effectively\nlower the computational load on each CPU.\n3.4.4. Overlapping\nMemory operations are typically asynchronous. Thus, We can send a request to the memory in advance and then\nproceed with other computations. After completing other computations, we come back to handle the memory request.\nThis two-step operation is used in the forward propagation process of model training. We need to obtain the parameters\nof \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56 through a gather operation. After obtaining the parameters of \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56, in the forward propagation process of\n\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56, we proactively retrieve the parameters of \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56+1 through an asynchronous fetch. Once the forward propagation\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 14 of 30\nA Comprehensive Overview from Training to Inference\nTable 3\nCommonly used instruction tuning datasets.\nDatasets\nLinks\nstatic-hh\nhttps://huggingface.co/datasets/Dahoas/static-hh\nOIG\nhttps://huggingface.co/datasets/laion/OIG\nSelf-Instruct [102]\nhttps://github.com/yizhongw/self-instruct\nNatural instructions [103]\nhttps://github.com/allenai/natural-instructions\nP3 [104]\nhttps://huggingface.co/datasets/bigscience/P3\nPromptsource [105]\nhttps://github.com/bigscience-workshop/promptsource\nWebGPT [106]\nhttps://huggingface.co/datasets/openai/webgpt_comparisons\nFlan [107]\nhttps://github.com/google-research/flan\nMVPCorpus [108]\nhttps://github.com/RUCAIBox/MVP\ncalculation for \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56 is completed, the parameters for \ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc56+1 have been obtained and are stored in the GPU. We can\nthen immediately proceed with the forward propagation calculation, and so on.\n3.4.5. Checkpoint\nIn order to support the backward propagation of the model, All intermediate results in the GPU memory need to\nbe saved during the forward propagation of the model. To optimize this process, a checkpoint mechanism, which does\nnot save all intermediate results in the GPU memory but only retains certain checkpoint points is utilized.\nThe diagram below illustrates a simplified structure of a transformer. Each transformer block takes a model input,\nundergoes complex computations through attention and feed-forward processes, and produces the overall output of\nthat layer. We keep only the input of each major layer in the transformer as our checkpoint.\nDuring the backward propagation process, how do we compute the gradients of the linear layers within each major\nlayer? We can perform a technique called recomputation, which involves re-executing the forward pass of each major\nlayer during the backward propagation process. We temporarily obtain the inputs of the linear layers within each major\nlayer, and the intermediate results obtained can be used for backward propagation. Once the backward propagation for\nthat layer is complete, we can discard the checkpoint and the temporarily recomputed intermediate results of the linear\nlayers within the model from the GPU memory.\nAssuming we have a transformer with 24 layers, each layer containing four to five linear layers, using the checkpoint\nmechanism reduces the originally required storage of 120 intermediate results to only 24 intermediate results.\n3.5. Fine-Tuning\nThe training of LLMs in this paper is divided into three stages: data collection and processing, pre-training, and\nfine-tuning. This section will provide a review of the fine-tuning methods for LLMs. Specifically, we categorize fine-\ntuning techniques into three types: supervised fine-tuning (SFT) [93], alignment tuning, and parameter-efficient tuning.\n3.5.1. Supervised Fine-Tuning\nThe core concept of supervised fine-tuning involves adjusting the model in a supervised manner on the basis of\nlarge-scale pre-training, enhancing its capability to better adapt to the specific requirements of the target task. In the\nprocess of SFT, it is necessary to prepare a labeled dataset for the target task, which includes input text along with\ncorresponding labels. Instruction tuning is a commonly used technique in the fine-tuning process of LLMs and can\nbe considered as a specific form of SFT. It involves further training LLMs on a dataset composed of (instruction,\noutput) pairs, focusing on enhancing the capabilities and controllability of large language models by understanding\nand following human instructions. We compiled commonly used instruction tuning datasets, as illustrated in Table 3.\n3.5.2. Alignment Tuning\nDue to LLMs being pre-trained on massive and diverse internet data, even though the training data undergoes\nsome preprocessing, it is still challenging to guarantee the absence of biased or harmful content in terabyte-scale\ntraining datasets. Despite LLMs demonstrating impressive performance across various natural language processing\ntasks, they frequently exhibit behaviors diverging from human intent. This includes generating false information,\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 15 of 30\nA Comprehensive Overview from Training to Inference\nproducing expressions with bias or misleading content, and so on [93; 109]. To address these issues of LLMs displaying\nbehaviors beyond human intent, alignment tuning becomes crucial [93; 110].\nIn general, alignment tuning aims to meet the following three criteria: being helpful, honest, and harmless.\nHelpful: The concept of helpfulness revolves around whether the model-generated output proves genuinely\nbeneficial for a specific task or inquiry. In the realm of natural language processing, the model\u2019s generated text or\nresponses should furnish valuable information, positively impacting the user\u2019s requirements or task objectives.\nHonest: Honesty entails whether the model-generated output is authentic and reliable. The model should produce\ninformation consistent with facts, steering clear of fabrication or distortion. This contributes to maintaining user trust\nin the authenticity of the model\u2019s outputs.\nHarmless: Harmlessness is concerned with whether the model-generated output poses no harm to users or society.\nThe model should refrain from generating content that is harmful, offensive, or perilous, ensuring its utilization remains\nsafe for all relevant stakeholders.\nIn training LLMs, a noteworthy approach to alignment tuning is based on Reinforcement Learning with Human\nFeedback (RLHF) [93]. This method involves collecting human feedback data to train a reward model (RM) for\nreinforcement learning. The RM serves as the reward function during reinforcement learning training, and algorithms\nsuch as Proximal Policy Optimization (PPO) [111] are employed to fine-tune the LLM. In this context, LLM is\nconsidered as the policy, and the action space is considered as the vocabulary of the LLM.\n3.5.3. Parameter-efficient Tuning\nCurrently, large-scale PLMs such as ChatGPT [93; 19] continue to grow in scale. However, for the majority of\nresearchers, conducting full fine-tuning on consumer-grade hardware has become cost-prohibitive and impractical.\nUnlike SFT and alignment tuning, the objective of parameter-efficient tuning is to reduce computational and memory\noverhead. This method involves fine-tuning only a small or additional subset of model parameters while keeping\nthe majority of pre-trained parameters fixed, thereby significantly lowering computational and storage costs. It is\nnoteworthy that state-of-the-art parameter-efficient tuning techniques have achieved performance levels comparable\nto full fine-tuning. Some common parameter-efficient tuning methods include Low-Rank Adaptation (LoRA) [112],\nPrefix Tuning [113] and P-Tuning [114; 115]. The adoption of these methods enables efficient model tuning even in\nresource-constrained environments, offering feasibility and efficiency for practical applications.\nWith the rise of LLMs, parameter-efficient tuning has garnered increasing attention, with LoRA being widely\nemployed in the latest releases of LLMs. LoRA [112] and its related advancements [116; 117] are noteworthy and\ndeserve attention.\n3.5.4. Safety Fine-Tuning\nTo enhance the safety and responsibility of LLMs, the integration of additional safety techniques during fine-tuning\nis essential. This encompasses three primary techniques, applicable to both SFT and RLHF phases.\nSupervised Safety Fine-Tuning: In this technique, labelers are tasked with generating demonstration data that\nincorporates high safety risk adversarial prompts. This handcraft safety demonstration data is then incorporated into\nthe SFT phase, thereby augmenting the model\u2019s capacity to manage safety risks.\nSafety RLHF: This technique employs the same or even more aggressive adversarial prompts to query the models.\nThe safest response exhibiting refusal behavior is then used to train a safety reward model within the RLHF framework.\nSafety Context Distillation: This technique employs context distillation [118] by initially prefixing safety\npreprompts, like \u201cYou are a safe and responsible assistant,\u201d to adversarial prompts. This process yields safer generated\nresponses. The model is then fine-tuned on these safer demonstration data but without the inclusion of the safety\npre-prompts. This safety distillation further enhances the model\u2019s safety capabilities.\n3.6. Evaluation\nUnlike in the past, large-scale deep learning models have a wider range of applications and stronger performance\ncompared to ordinary models. However, with great power comes great responsibility, and evaluating these models has\nbecome more complex, requiring consideration of potential problems and risks from all aspects. Since the popularity\nof ChatGPT, many related studies have been published, including the survey and summary of LLMs evaluation in\nreference [119; 120], which is helpful for developing large-scale deep learning models. This section will introduce\nsome testing datasets, evaluation directions and methods, and potential threats that need to be considered based on\nprevious evaluation work on large models.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 16 of 30\nA Comprehensive Overview from Training to Inference\n3.6.1. Static testing dataset\nThe evaluation of large models\u2019 capabilities requires appropriate datasets for validation. Here, we introduce several\ncommonly used datasets for testing purposes. Considering multimodal large models, typical datasets for computer\nvision include ImageNet [121] and Open Images [122]. In addition to the commonly used GLUE [123] and SuperGLUE\n[124] for LLMs, MMLU [125] is highly competitive in testing comprehensive capability. If your model primarily uses\nChinese language, then CMMLU [126], as a benchmark for Chinese large models, should also be considered, and\nXTREME [127] and XTREME-R [128] are suitable choices for multilingual large models. For assessing mathematical\nknowledge capabilities, there are datasets such as MATH [129] and GSM8K [130], while HumanEval [131] and MBPP\n[132] can serve as benchmarks for code generation. For common sense reasoning tests in daily human life and work,\nthe following datasets are available: HelloSwag [133], PIQA [134], BoolQ [135], SIQA [136], WinoGrande [137],\nARC [138], and OpenBookQA [139]. For medical knowledge, there are datasets such as MedQA-USMLE [140] and\nMedMCQA [141].\n3.6.2. Open domain Q&A evaluation\nCurrently, LLMs interact with humans in the form of questions and answers. Compared to the fragmented and\nambiguous information returned by traditional searches, LLMs provide more realistic and efficient question-and-\nanswer results that align with human habits. Therefore, the evaluation of ODQA (Open Domain Question Answering)\n[142] capability is essential. The performance of open-domain question answering greatly affects user experience.\nCommonly used datasets for testing include SquAD [143] and Natural Questions [144], with F1 score and Exact-Match\naccuracy (EM) as evaluation metrics. However, note that the method of word matching may have certain issues, such\nas when a factually correct answer is not in the golden answer list. Therefore, human evaluation seems to be necessary,\nand literature [145] has conducted detailed research on this matter.\n3.6.3. Security evaluation\nAs an emerging and hot research field, LLMs must pay attention to their potential security threats, prevent malicious\nuse or vulnerabilities to malicious attacks, and address any potential long-term issues that may pose a threat to\nhuman development. Additionally, red teaming in various domains is necessary to critically assess and test the model,\nidentifying vulnerabilities, biases, inaccuracies, and areas for safety improvement.\nPotential bias:The training data for LLMs may contain potential biases, such as gender or race. Security\nassessments need to address whether the model generates or amplifies these biases and how to reduce or correct them.\nReference [146] discusses in detail the causes of bias in LLMs and the serious consequences that may arise. Reference\n[147] extensively studies how pre-trained language models generate harmful content to what extent, and how to use\ncontrolled text generation algorithms to prevent the generation of toxic content. CHBias [148] is a Chinese dataset that\ncan be used to evaluate and mitigate the bias problem of LLMs.\nPrivacy protection: LLMs may come into contact with a large amount of user data, such as text and images,\nduring the training process. Security assessments need to ensure the effective protection of user data privacy to prevent\nleaks and misuse. Reference [149] conducted research on models like ChatGPT and found that it is possible to extract\ntraining data effectively from these models. Reference [150] provides a solution by proposing a framework called\nDEPN (Detect and Editing Privacy Neurons) to detect and edit privacy neurons in pre-trained language models. It\nalso introduces a privacy neuron aggregator to eliminate privacy information in a batch-processing manner, effectively\nreducing the leakage of privacy data while maintaining model performance.\nAdversarial attacks: LLMs may be susceptible to adversarial attacks, such as input tampering, intentional\nmisinformation, or generating false information. Security assessments need to consider the robustness of the model,\ni.e., its ability to withstand such attacks. As mentioned in reference [151], LLMs still have \"jailbreak\" risks, where users\ncan manipulate the model to generate toxic content using specific input methods like role-playing or adding special\nsuffixes as studied in the referenced paper. Especially when using open-source pre-trained models, any vulnerabilities\nin the pre-training models regarding adversarial attacks are inherited as well. Reference [152] provides a solution to\nmitigate the harm caused by these vulnerabilities.\n3.6.4. Evaluation method\nAutomated evaluation and manual evaluation play crucial roles in Language Model (LLM) research. Automated\nevaluation typically involves using various metrics and indicators to quantify the performance of models, such as\nBIEU [153], ROUGE [154], and BERTSScore [155], which can measure the accuracy of LLM-generated content.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 17 of 30\nA Comprehensive Overview from Training to Inference\nThese metrics can help researchers quickly assess model performance on large-scale data and compare different\nmodels. However, automated evaluation also has limitations as it cannot fully capture the complexity of language\nunderstanding and generation. Research in reference [156] has shown that manual evaluation is more reliable for\nsome open-ended generation tasks. Manual evaluation typically involves human annotators subjectively judging and\nassessing the quality of model-generated outputs. This evaluation method can help reveal how models perform in\nspecific tasks or scenarios and identify subtle issues and errors that automated evaluation may overlook. However,\nmanual evaluation also faces challenges such as high time costs and subjectivity. Therefore, it is often necessary to\ncombine the strengths of automated and manual evaluation to comprehensively assess the performance of language\nmodels.\n3.7. LLM Framework\nLarge deep learning models offer significant accuracy gains, but training billions to trillions of parameters is\nchallenging. Existing solutions such as distributed training have solved fundamental limitations to fit these models into\nlimited device memory while obtaining computation, communication, and development efficiency. Next, this section\nwill introduce several large language model frameworks that utilize distributed training technology leveraging GPU,\nCPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code\nrefactoring.\nTransformers Transformers[157], an open-source Python library by Hugging Face, is dedicated to building models\nusing the Transformer architecture. Featuring a simple and user-friendly API, it facilitates easy customization of various\npre-trained models. With a robust community of users and developers, transformers continuously update and improve\nmodels and algorithms.\nDeepSpeed: Deepspeed [158], an open-source optimization library compatible with PyTorch, is developed by\nMicrosoft and utilized in training LLMs like MTNLG [79] and BLOOM [38]. Currently, It provides full support\nfor ZeRO technology which includes Optimizer state partitioning, Gradient partitioning and parameter partitioning,\nCustom mixed precision training, A range of fast CUDA-extension-based optimizers [159] and ZeRO-offload to CPU\nand Disk/NVMe. Through the above technologies. Additionally, Deepspeed has achieved excellent scalability and\nefficiency with small memory requirements.\nBMTrain: BMTrain [160] is an efficient large model training toolkit developed by Tsinghua University that can be\nused to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping\nthe code as simple as stand-alone training. BMTrain does not require model refactoring to work. In fact, PyTorch users\ncan enable BMTrain with a few lines of code change to their existing training pipeline. It provides the support of\nvarious optimization techniques such as ZeRO optimization and communication optimization.\nMegatron-LM: Megatron-LM [96; 161; 162] is a deep learning library developed by NVIDIA for training large-\nscale language models.Megatron-LM presents their techniques including model and data parallelism, mixed-precision\ntraining, and FlashAttention for training very large transformer models. Specifically, it takes advantage of the structure\nof transformer networks to create a simple model parallel implementation by adding a few synchronization primitives\nand it enables training transformer models with billions of parameters and trains efficiently in PyTorch.It also performs\nan in-depth empirical analysis of their model and data parallel technique and demonstrates up to 76% scaling efficiency\nusing 512 GPUs which can largely improve the training efficiency and speed, enabling efficient distributed training\nacross GPUs.\nIn addition to the aforementioned frameworks, Colossal-AI [163] and FastMoE [164; 165] are also two popular\nframeworks for training LLMs. In principle, any deep learning framework that supports parallel computing can be used\nto train LLMs. Examples include PyTorch [166], TensorFlow [167; 168], PaddlePaddle [169], MXNet [170], OneFlow\n[171], MindSpore [172] and JAX [173].\n4. Inference with Large Language Models\nThe scale of large models is growing at a rate of nearly 10 times per year, which brings about huge computational\nconsumption and carbon emissions [174]. Therefore, reducing the computational burden of training large models while\nretaining their reasoning ability has become a common concern for everyone. In this chapter, we mainly introduce how\nto reduce costs from both computational and storage aspects, that is, how to efficiently perform large-scale model\ninference from four aspects: model compression, memory scheduling, parallelism, and structural optimization.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 18 of 30\nA Comprehensive Overview from Training to Inference\n4.1. Model Compression\n4.1.1. Knowledge Distillation\nKnowledge Distillation [175] refers to transferring knowledge from a cumbersome (teacher) model to a smaller\n(student) model that is more suitable for deployment. This is achieved by fitting the soft targets of the two models,\nas soft targets provide more information than gold labels. Initially, the calculation for model distillation involved only\nfitting the outputs from the last layer of both the teacher and student models [176]. PKD [177] improves this process by\ncomputing the mean-square loss between normalized hidden states, allowing the student model to learn from multiple\nintermediate layers of the teacher model. In order to discover more intermediate representations suitable for knowledge\ndistillation, Jiao et al. [178] proposed Tiny BERT. This enables the student model to learn from the embedding layer\nand attention matrices of the teacher model.\n4.1.2. Model Pruning\nModel pruning involves removing redundant portions from the parameter matrices of large models. It is divided\ninto unstructured pruning and structured pruning. Unstructured pruning involves removing individual connections\nor weights in a neural network without adhering to any specific structural pattern. In structured pruning, specific\nstructural patterns or units within a neural network are pruned or removed. Gordon et al. [179] compared the effects\nof unstructured and structured pruning on the BERT model. They found that the effectiveness of unstructured pruning\nsignificantly decreases as the pruning ratio increases, while in structured pruning, 30-40% of the weights can be\ndiscarded without affecting BERT\u2019s universality. Different structures in the model can be structurally pruned. Michel\net al. [180] pruned attention heads and found that ablating one head often positively impacts the performance of WMT\nand BERT. They proposed a gradient-based metric for evaluating the importance of attention heads to enhance pruning\neffectiveness. Fan et al. [179] performed layer pruning by extending dropout from weights to layers. During training,\nthey randomly dropped layers and achieved good inference results by selecting sub-networks with any desired depth\nduring testing.\n4.1.3. Model Quantization\nThe fundamental idea behind model quantization is to reduce the number of floating-point bits used in numerical\ncalculations within a large model network, thereby decreasing storage and computation costs. This involves converting\nfloating-point operations into fixed-precision operations. However, as precision decreases, the model\u2019s loss gradually\nincreases, and when precision drops to 1 bit, the model\u2019s performance experiences a sudden decline. To address the\noptimization challenges introduced by low-precision quantization, Bai et al. [181] proposed BinaryBERT. They initially\ntrained a half-sized ternary model and then initialized a binary model with the ternary model through weight splitting.\nFinally, they fine-tuned the binary model. This approach yielded better results for the binary model compared to training\na binary model from scratch.\n4.1.4. Weight Sharing\nThe basic idea of weight sharing is to use the same set of parameters for multiple parts of a LLM. Instead of learning\ndifferent parameters for each instance or component, the model shares a common set of parameters across various parts.\nWeight sharing helps reduce the number of parameters that need to be learned, making the model more computationally\nefficient and reducing the risk of overfitting, especially in situations where there is limited data. ALBERT [182] uses\nthe Cross-layer parameter-sharing strategy to effectively reduce the number of parameters of the model, and can achieve\nbetter training results than the baseline with the same parameter number.\n4.1.5. Low-rank Approximation\nLow-rank decomposition methods are crucial in the field of model compression, as they allow for the creation\nof more compact models with fewer parameters. This reduction in model size is particularly beneficial for deploying\nneural networks on resource-constrained devices, improving efficiency during inference. Chen et al. [183] performed a\nlow-rank decomposition on the input matrix, enabling matrix operations within the large model to occur at a lower-rank\nlevel, effectively reducing the computational workload. From the results, their proposed method, DRONE, not only\nensures the inference performance of the large model but also achieves an acceleration ratio of more than 1.3 times\ncompared to the baseline method. The specific choice of low-rank decomposition method depends on the architecture\nof the neural network and the requirements of the target application.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 19 of 30\nA Comprehensive Overview from Training to Inference\n4.2. Memory Scheduling\nDeploying LLMs on a single consumer-grade GPU is constrained by the limitations of the available video memory,\ngiven the substantial parameters of LLMs. Therefore, appropriate Memory Scheduling strategies can be used to solve\nthe hardware limitations of large model inference. Memory scheduling in large model inference involves the efficient\norganization and management of memory access patterns during the reasoning or inference phase of complex neural\nnetwork models. In the context of sophisticated reasoning tasks, such as natural language understanding or complex\ndecision-making, large models often have intricate architectures and considerable memory requirements. Memory\nscheduling optimizes the retrieval and storage of intermediate representations, model parameters, and activation values,\nensuring that the inference process is both accurate and performed with minimal latency. For example, BMInf [184]\nutilizes the principle of virtual memory, achieving efficient inference for large models by intelligently scheduling the\nparameters of each layer between the GPU and CPU.\n4.3. Parallelism\nBoth inference and training can leverage parallelization techniques. Presently, parallelization techniques for\ninference primarily manifest across three dimensions: Data Parallelism, Tensor Parallelism, and Pipeline Parallelism.\nData Parallelism primarily involves increasing the overall throughput of the inference system by adding more GPU\ndevices [101; 97; 159; 185]. Tensor parallelism is a form of model parallelism where the model\u2019s parameters are\npartitioned into multiple tensors, each computed on different processing units. This approach proves beneficial when\ndealing with models that are too large to fit into the memory of a single GPU. Tensor parallelism primarily involves\nincreasing the number of devices horizontally through parallel computation to reduce latency [96]. Pipeline parallelism\nprimarily involves vertically increasing the number of GPU devices through parallel computation to support larger\nmodels and enhance device utilization. Typically, it is combined with tensor parallelism to achieve optimal performance\n[98].\n4.4. Structural Optimization\nIn the forward propagation computation of LLMs, the calculation speed is significantly faster than the speed\nof memory access. Inference speed can be impacted by numerous memory access operations. One goal in LLM\ninference is to minimize the number of memory accesses during forward propagation. FlashAttention [186] and\nPagedAttention [187] enhance computational speed by employing a chunked computation approach, mitigating the\nstorage overhead associated with matrices. The entire operation takes place within SRAM, reducing the number of\naccesses to High Bandwidth Memory (HBM) and significantly boosting computational speed. Both FlashAttention\nand PagedAttention have been adopted by mainstream inference frameworks, and seamlessly integrated into these\nframeworks for straightforward utilization.\n4.5. Inference Framework\nParallel computing, model compression, memory scheduling, and specific optimizations for transformer structures,\nall integral to LLM inference, have been effectively implemented in mainstream inference frameworks. These\nframeworks furnish the foundational infrastructure and tools required for deploying and running LLM models. They\noffer a spectrum of tools and interfaces, streamlining the deployment and inference processes for researchers and\nengineers across diverse application scenarios. The choice of a framework typically hinges on project requirements,\nhardware support, and user preferences. In Table 4, we compile some of these frameworks for reference.\n5. Utilization of LLMs\nThe application scope of LLMs is extensive and can be practically employed in almost any specialized domain\n[1; 193; 46; 194; 195]. Following pre-training and fine-tuning, LLMs are primarily utilized by designing suitable\nprompts for various tasks. Leveraging powerful zero-shot capabilities, many tasks can be directly accomplished by\nguiding LLMs with straightforward prompts. For more complex tasks that cannot be achieved through simple prompts,\na few-shot approach involving in-context learning is employed to guide LLMs in task completion. Additionally,\nincorporating chain-of-thought [196; 197] prompts in the prompt enhances in-context learning by introducing a\nreasoning process. The pipeline of the in-context learning and chain-of-thought is shown in Figure 6. In some\nspecialized research directions, obtaining intermediate layer representations of LLMs may be necessary. For instance,\nin neuroscience studies, embedding representations from the model are used to investigate activation regions of brain\nfunctions [198; 199; 200; 201].\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 20 of 30\nA Comprehensive Overview from Training to Inference\nTable 4\nList of LLM inference framework.\nFramework\nLinks\nTensorRT\nhttps://github.com/NVIDIA/TensorRT-LLM\nFasterTransformer\nhttps://github.com/NVIDIA/FasterTransformer\nMegatron-LM [96]\nhttps://github.com/NVIDIA/Megatron-LM\nFlexGen [188]\nhttps://github.com/FMInference/FlexGen\nDeepSpeed [158]\nhttps://github.com/microsoft/DeepSpeed\nvLLM [187]\nhttps://github.com/vllm-project/vllm\nFlexFlow [189]\nhttps://github.com/flexflow/FlexFlow\nStreamingLLM [190]\nhttps://github.com/mit-han-lab/streaming-llm\nColossalAI [163]\nhttps://github.com/hpcaitech/ColossalAI\nBMCook [191]\nhttps://github.com/OpenBMB/BMCook\nBMInf [184]\nhttps://github.com/OpenBMB/BMInf\nPetals [192]\nhttps://github.com/bigscience-workshop/petals\nLarge Language Model\nParameter Freeze\nTest input\nPrediction\nReview: Nice weather! \\n: Positive\nReview: The weather is terrible!  \\n: \nNegative\n\u2026                              \u2026                        \nReview: Bad weather. \\n: Negative\nReview: Good weather! \\n:\nA) In-context learning\nTemplate\nReview:[Text]\nSentiment:[label]\nB) Chain of thought\nNice weather!     1\nThe weather is \nterrible!    0\nBad weather!    0\n\u2026    \u2026 \nText\nLabel\nJack has 9 bags of candy, 6 in each bag. He ate 3 bags, and the rest \nof the candy was divided equally among his 4 friends, how many \ndid each get?\nCOT Demo\nJack eats 3 bags of candy, so the number of bags left is 9 minus 3, \nwhich is 6 bags. Then multiply the total number of these 6 bags of \ncandies (6 in each bag) by 6 to get the total number of candies left \nat 36. In the end, divide the 36 candies equally among the 4 \nfriends, and each person gets 36 \u00f7 4 = 9 candies\nDemonstrations\nPositive\nDemonst\nration\nQuestion\nLLM\nQ: Ben had 15 dollars, he spent 5 dollars on a book, and then \nspent half of his remaining money on a cup of milk tea. How much \nmoney does he have left?\nA: Ben spent 5 dollars to buy the book, and the rest of the money \nis 15 \u2013 5 = 10 dollars. Then, he spent half of the remaining \nmoney (10 \u00f7 2), and his last remaining money was 5 dollars.\nFigure 6: A) in-context learning, B) Chain of thought.\nGenerally, there are several approaches to employing LLMs. The first involves accessing the capabilities of robust\nproprietary models through open API services, such as utilizing the API provided by ChatGPT [19]. The second\napproach includes deploying open-source LLMs for local use [9]. The third method entails fine-tuning open-source\nLLMs to meet specific domain standards [43; 202], enabling their application in a particular field, and subsequently\ndeploying them locally. In Table 5, we have compiled information on various open-source LLMs for reference.\nResearchers can choose from these open-source LLMs to deploy applications that best suit their needs.\n6. Future Directions and Implications\nThis section will delve into the future trends and impact of LLM technology. Our discussion will be structured into\nthree parts: firstly, an exploration of the developmental trends within LLMs technology itself; secondly, an examination\nof the developmental directions for AI researchers; and finally, an analysis of the societal impact resulting from the\nongoing development of LLMs.\nBased on existing experiences, it is evident that an ample supply of high-quality data and a sufficient number of\nparameters significantly contribute to enhancing the performance of models [8]. Looking ahead, the model scale of\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 21 of 30\nA Comprehensive Overview from Training to Inference\nTable 5\nList of open source LLMs.\nLLM\nSize (B)\nLinks\nT5 [68]\n11B\nhttps://github.com/google-research/text-to-text-transfer-transformer\nCodeGen [81]\n16B\nhttps://github.com/salesforce/CodeGen\nMOSS [203]\n16B\nhttps://github.com/OpenLMLab/MOSS\nGLM [37]\n130B\nhttps://github.com/THUDM/GLM\nChatGLM [37]\n6B\nhttps://github.com/THUDM/ChatGLM3\nChatYuan [204]\n0.7B\nhttps://github.com/clue-ai/ChatYuan\nOPT [83]\n175B\nhttps://github.com/facebookresearch/metaseq\nBLOOM [38]\n176B\nhttps://huggingface.co/bigscience/bloom\nLLaMA [9]\n65B\nhttps://github.com/facebookresearch/llama\nCodeGeeX [82]\n13B\nhttps://github.com/THUDM/CodeGeeX\nBaichuan [205]\n13B\nhttps://github.com/baichuan-inc/Baichuan2\nAquila\n7B\nhttps://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila\nMiniGPT-4 [206]\n25B\nhttps://github.com/Vision-CAIR/MiniGPT-4\nVicuna [207]\n13B\nhttps://github.com/lm-sys/FastChat\nLLMs is expected to continue expanding, thereby augmenting their learning capabilities and overall performance.\nMoreover, the majority of currently available LLMs are confined to a single natural language modality, lacking\nextensions to process multimodal data such as images, videos, and speech. There is a potential future trajectory for\nLLMs to evolve towards handling information beyond text, incorporating multimodal data like images and audio.\nThis evolution would empower models to comprehensively understand and generate multimodal content, significantly\nbroadening the application scope of LLMs. The inevitable expansion of LLMs into the field of multimodality is bound\nto incur increased training costs. A pivotal focus for future developments lies in the efficient fine-tuning of parameters\nand the deployment of LLMs through techniques such as knowledge distillation, model compression, and quantization,\naimed at reducing both the training and inference costs of LLMs. Another emerging trend is the domain-specific training\nand fine-tuning of LLMs for particular sectors, facilitating a more adept adaptation to and understanding of industry-\nspecific terminologies and contexts. Lastly, in the exploration of potential new architectures for LLMs the current\nlandscape predominantly relies on the transformer architecture. While the transformer architecture naturally boasts\nadvantages such as parallel computing and adaptability to various input modalities, its design typically necessitates\nfixed-size inputs. This requirement may necessitate padding or truncation when dealing with variable-length sequences,\npotentially leading to computational and information inefficiencies, as well as challenges in generating coherent data.\nInvestigating the potential of Recurrent Neural Network (RNN) architectures in the era of LLMs could emerge as a\npivotal research direction. For instance, RWKV [208], an LLM designed under the RNN architecture, has demonstrated\ncompetitive performance on various third-party evaluations, proving itself comparable to the majority of transformer-\nbased LLMs.\nFor researchers in the field of AI, working in isolation is becoming increasingly impractical. The future direction\nof AI development will intertwine with various industries, necessitating close collaboration with professionals from\ndiverse fields. It is crucial to engage in collaborative efforts, bridging research disciplines, and collectively addressing\nchallenges by combining expertise from different domains. Simultaneously, there is a fresh set of requirements for the\ncomprehensive skills of AI researchers. Training and deploying LLMs necessitate proficiency in managing large-scale\ndata and substantial practical experience in distributed parallel training. This criterion underscores the importance for\nresearchers involved in LLM development to possess substantial engineering capabilities, addressing the challenges\ninherent in the process. Researchers who are interested in the field of LLMs must either possess engineering skills or\nadeptly collaborate with engineers to navigate the complexities of model development [3].\nAs LLMs find widespread applications in societal life, concerns about ethical issues and societal impact are on a\ncontinuous rise. This may involve research and improvements in areas such as managing model biases and controlling\nthe risk of misuse [4]. Considering the paramount importance of privacy and data security, the future development\nof LLMs might involve more federated learning and decentralized approaches to enhance model performance while\nsafeguarding user privacy. Developers should engage in interdisciplinary collaboration with experts from various\nfields, including decision-making, legal studies, and sociology, to establish standards and ethical frameworks for the\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 22 of 30\nA Comprehensive Overview from Training to Inference\ndevelopment, deployment, and utilization of LLMs, mitigating potential harmful consequences. In terms of public\nawareness and education, mandatory awareness training should be implemented before large-scale public deployment\nand applications. This aims to enhance public understanding of the capabilities and limitations of LLMs, fostering\nresponsible and informed use, especially in industries such as education and journalism.\n7. Conclusion\nThe introduction of ChatGPT has ushered in a transformative era in the realm of Large LLMs, significantly\ninfluencing their utilization for diverse downstream tasks. The emphasis on cost-effective training and deployment\nhas emerged as a crucial aspect in the evolution of LLMs. This paper has provided a comprehensive survey of the\nevolution of large language model training techniques and inference deployment technologies in alignment with\nthe emerging trend of low-cost development. The progression from traditional statistical language models to neural\nlanguage models, and subsequently to PLMs such as ELMo and transformer architecture, has set the stage for the\ndominance of LLMs. The scale and performance of these models, particularly exemplified by the GPT series, have\nreached unprecedented levels, showcasing the phenomenon of emergence and enabling versatile applications across\nvarious domains. Notably, the release of ChatGPT by OpenAI in November 2022 has marked a pivotal moment in\nthe LLM landscape, revolutionizing the strength and effectiveness of AI algorithms. However, the current reliance\non OpenAI\u2019s infrastructure underscores the necessity for alternative LLMs, emphasizing the need for domain-specific\nmodels and advancements in the training and deployment processes.\nTraining and deploying LLMs present challenges that demand expertise in handling large-scale data and distributed\nparallel training. The engineering capabilities required for LLM development highlight the collaborative efforts needed\nbetween researchers and engineers. As we explore the technical aspects of LLM training and inference in this review,\nit becomes evident that a deep understanding of these processes is essential for researchers venturing into the field.\nLooking ahead, the future of LLMs holds promising directions, including further advancements in model architectures,\nimproved training efficiency, and broader applications across industries. The insights provided in this review aim to\nequip researchers with the knowledge and understanding necessary to navigate the complexities of LLM development,\nfostering innovation and progress in this dynamic field. As LLMs continue to evolve, their impact on natural language\nprocessing and AI as a whole is poised to shape the future landscape of intelligent systems.\nReferences\n[1] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu et al., \u201cSummary of chatgpt-related research and perspective\ntowards the future of large language models,\u201d Meta-Radiology, p. 100017, 2023.\n[2] J. Wang, E. Shi, S. Yu, Z. Wu, C. Ma, H. Dai, Q. Yang, Y. Kang, J. Wu, H. Hu et al., \u201cPrompt engineering for healthcare: Methodologies and\napplications,\u201d arXiv preprint arXiv:2304.14670, 2023.\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., \u201cA survey of large language models,\u201d\narXiv preprint arXiv:2303.18223, 2023.\n[4] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy, \u201cChallenges and applications of large language models,\u201d arXiv\npreprint arXiv:2307.10169, 2023.\n[5] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d in\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), Jun. 2018, pp. 2227\u20132237.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances\nin neural information processing systems, vol. 30, 2017.\n[7] A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever, \u201cBetter language models and their implications,\u201d OpenAI\nBlog https://openai. com/blog/better-language-models, vol. 1, no. 2, 2019.\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage\nmodels are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[9] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open\nand efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n[10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open\nfoundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n[11] S. Rezayi, H. Dai, Z. Liu, Z. Wu, A. Hebbar, A. H. Burns, L. Zhao, D. Zhu, Q. Li, W. Liu et al., \u201cClinicalradiobert: Knowledge-infused few\nshot learning for clinical notes named entity recognition,\u201d in International Workshop on Machine Learning in Medical Imaging.\nSpringer,\n2022, pp. 269\u2013278.\n[12] Z. Liu, M. He, Z. Jiang, Z. Wu, H. Dai, L. Zhang, S. Luo, T. Han, X. Li, X. Jiang et al., \u201cSurvey on natural language processing in medical\nimage analysis.\u201d Zhong nan da xue xue bao. Yi xue ban= Journal of Central South University. Medical Sciences, vol. 47, no. 8, pp. 981\u2013993,\n2022.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 23 of 30\nA Comprehensive Overview from Training to Inference\n[13] W. Liao, Z. Liu, H. Dai, Z. Wu, Y. Zhang, X. Huang, Y. Chen, X. Jiang, D. Zhu, T. Liu et al., \u201cMask-guided bert for few shot text classification,\u201d\narXiv preprint arXiv:2302.10447, 2023.\n[14] S. Rezayi, Z. Liu, Z. Wu, C. Dhakal, B. Ge, H. Dai, G. Mai, N. Liu, C. Zhen, T. Liu et al., \u201cExploring new frontiers in agricultural nlp:\nInvestigating the potential of large language models for food applications,\u201d arXiv preprint arXiv:2306.11892, 2023.\n[15] T. Zhong, W. Zhao, Y. Zhang, Y. Pan, P. Dong, Z. Jiang, X. Kui, Y. Shang, L. Yang, Y. Wei et al., \u201cChatradio-valuer: A chat large language\nmodel for generalizable radiology report generation based on multi-institution and multi-system data,\u201d arXiv preprint arXiv:2310.05242,\n2023.\n[16] Z. Liu, T. Zhong, Y. Li, Y. Zhang, Y. Pan, Z. Zhao, P. Dong, C. Cao, Y. Liu, P. Shu et al., \u201cEvaluating large language models for radiology\nnatural language processing,\u201d arXiv preprint arXiv:2307.13693, 2023.\n[17] T. Zhong, Y. Wei, L. Yang, Z. Wu, Z. Liu, X. Wei, W. Li, J. Yao, C. Ma, X. Li et al., \u201cChatabl: Abductive learning via natural language\ninteraction with chatgpt,\u201d arXiv preprint arXiv:2304.11107, 2023.\n[18] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., \u201cImproving language understanding by generative pre-training,\u201d OpenAI, 2018.\n[19] OpenAI, \u201cGpt-4 technical report,\u201d 2023.\n[20] H. Dai, Z. Liu, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu, W. Liu, N. Liu, S. Li, D. Zhu, H. Cai, L. Sun, Q. Li, D. Shen, T. Liu, and\nX. Li, \u201cAuggpt: Leveraging chatgpt for text data augmentation,\u201d 2023.\n[21] Z. Liu, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, W. Liu, D. Shen, Q. Li et al., \u201cDeid-gpt: Zero-shot medical text de-identification\nby gpt-4,\u201d arXiv preprint arXiv:2303.11032, 2023.\n[22] C. Ma, Z. Wu, J. Wang, S. Xu, Y. Wei, Z. Liu, L. Guo, X. Cai, S. Zhang, T. Zhang et al., \u201cImpressiongpt: an iterative optimizing framework\nfor radiology report summarization with chatgpt,\u201d arXiv preprint arXiv:2304.08448, 2023.\n[23] W. Liao, Z. Liu, H. Dai, S. Xu, Z. Wu, Y. Zhang, X. Huang, D. Zhu, H. Cai, T. Liu et al., \u201cDifferentiate chatgpt-generated and human-written\nmedical texts,\u201d arXiv preprint arXiv:2304.11567, 2023.\n[24] H. Dai, Y. Li, Z. Liu, L. Zhao, Z. Wu, S. Song, Y. Shen, D. Zhu, X. Li, S. Li et al., \u201cAd-autogpt: An autonomous gpt for alzheimer\u2019s disease\ninfodemiology,\u201d arXiv preprint arXiv:2306.10095, 2023.\n[25] Z. Guan, Z. Wu, Z. Liu, D. Wu, H. Ren, Q. Li, X. Li, and N. Liu, \u201cCohortgpt: An enhanced gpt for participant recruitment in clinical study,\u201d\narXiv preprint arXiv:2307.11346, 2023.\n[26] Z. Liu, Z. Wu, M. Hu, B. Zhao, L. Zhao, T. Zhang, H. Dai, X. Chen, Y. Shen, S. Li et al., \u201cPharmacygpt: The ai pharmacist,\u201d arXiv preprint\narXiv:2307.10432, 2023.\n[27] Y. Wei, T. Zhang, H. Zhang, T. Zhong, L. Zhao, Z. Liu, C. Ma, S. Zhang, M. Shang, L. Du et al., \u201cChat2brain: A method for mapping\nopen-ended semantic queries to brain activation maps,\u201d arXiv preprint arXiv:2309.05021, 2023.\n[28] T. Zhong, X. Wei, E. Shi, J. Gao, C. Ma, Y. Wei, S. Zhang, L. Guo, J. Han, T. Liu et al., \u201cA small-sample method with eeg signals based\non abductive learning for motor imagery decoding,\u201d in International Conference on Medical Image Computing and Computer-Assisted\nIntervention.\nSpringer, 2023, pp. 416\u2013424.\n[29] J. Gao, L. Zhao, T. Zhong, C. Li, Z. He, Y. Wei, S. Zhang, L. Guo, T. Liu, J. Han et al., \u201cPrediction of cognitive scores by joint use of\nmovie-watching fmri connectivity and eye tracking via attention-censnet,\u201d Psychoradiology, vol. 3, 2023.\n[30] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing\nsystems, vol. 27, 2014.\n[31] G. Bebis and M. Georgiopoulos, \u201cFeed-forward neural networks,\u201d Ieee Potentials, vol. 13, no. 4, pp. 27\u201331, 1994.\n[32] Y. Yang, L. Wang, S. Shi, P. Tadepalli, S. Lee, and Z. Tu, \u201cOn the sub-layer functionalities of transformer decoder,\u201d arXiv preprint\narXiv:2010.02648, 2020.\n[33] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \u201cTransformer-xl: Attentive language models beyond a fixed-length\ncontext,\u201d arXiv preprint arXiv:1901.02860, 2019.\n[34] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, \u201cRoformer: Enhanced transformer with rotary position embedding,\u201d Neurocomputing, p.\n127063, 2023.\n[35] O. Press, N. A. Smith, and M. Lewis, \u201cTrain short, test long: Attention with linear biases enables input length extrapolation,\u201d arXiv preprint\narXiv:2108.12409, 2021.\n[36] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., \u201cPalm:\nScaling language modeling with pathways,\u201d arXiv preprint arXiv:2204.02311, 2022.\n[37] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia et al., \u201cGlm-130b: An open bilingual pre-trained\nmodel,\u201d arXiv preprint arXiv:2210.02414, 2022.\n[38] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u0107, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon et al., \u201cBloom: A 176b-\nparameter open-access multilingual language model,\u201d arXiv preprint arXiv:2211.05100, 2022.\n[39] L. Zhao, L. Zhang, Z. Wu, Y. Chen, H. Dai, X. Yu, Z. Liu, T. Zhang, X. Hu, X. Jiang et al., \u201cWhen brain-inspired ai meets agi,\u201d Meta-\nRadiology, p. 100005, 2023.\n[40] J. Holmes, Z. Liu, L. Zhang, Y. Ding, T. T. Sio, L. A. McGee, J. B. Ashman, X. Li, T. Liu, J. Shen, and W. Liu, \u201cEvaluating large language\nmodels on a highly-specialized topic, radiation oncology physics,\u201d Frontiers in Oncology, vol. 13, Jul. 2023.\n[41] Z. Wu, L. Zhang, C. Cao, X. Yu, H. Dai, C. Ma, Z. Liu, L. Zhao, G. Li, W. Liu et al., \u201cExploring the trade-offs: Unified large language models\nvs local fine-tuned models for highly-specific radiology nli task,\u201d arXiv preprint arXiv:2304.09138, 2023.\n[42] S. Rezayi, Z. Liu, Z. Wu, C. Dhakal, B. Ge, C. Zhen, T. Liu, and S. Li, \u201cAgribert: knowledge-infused agricultural language models for\nmatching food and nutrition,\u201d in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, vol. 7, 2022, pp.\n5150\u20135156.\n[43] Z. Liu, A. Zhong, Y. Li, L. Yang, C. Ju, Z. Wu, C. Ma, P. Shu, C. Chen, S. Kim et al., \u201cRadiology-gpt: A large language model for radiology,\u201d\narXiv preprint arXiv:2306.08666, 2023.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 24 of 30\nA Comprehensive Overview from Training to Inference\n[44] Z. Liu, X. He, L. Liu, T. Liu, and X. Zhai, Context Matters: A Strategy to Pre-train Language Model for Science Education. Springer Nature\nSwitzerland, 2023, p. 666\u2013674.\n[45] J. Wang, Z. Liu, L. Zhao, Z. Wu, C. Ma, S. Yu, H. Dai, Q. Yang, Y. Liu, S. Zhang et al., \u201cReview of large vision models and visual prompt\nengineering,\u201d arXiv preprint arXiv:2307.00855, 2023.\n[46] X. Li, L. Zhang, Z. Wu, Z. Liu, L. Zhao, Y. Yuan, J. Liu, G. Li, D. Zhu, P. Yan et al., \u201cArtificial general intelligence for medical imaging,\u201d\narXiv preprint arXiv:2306.05480, 2023.\n[47] H. Cai, W. Liao, Z. Liu, Y. Zhang, X. Huang, S. Ding, H. Ren, Z. Wu, H. Dai, S. Li et al., \u201cCoarse-to-fine knowledge graph domain adaptation\nbased on distantly-supervised iterative training,\u201d arXiv preprint arXiv:2211.02849, 2022.\n[48] H. Dai, C. Ma, Z. Liu, Y. Li, P. Shu, X. Wei, L. Zhao, Z. Wu, D. Zhu, W. Liu et al., \u201cSamaug: Point prompt augmentation for segment\nanything model,\u201d arXiv preprint arXiv:2307.01187, 2023.\n[49] L. Zhang, Z. Liu, L. Zhang, Z. Wu, X. Yu, J. Holmes, H. Feng, H. Dai, X. Li, Q. Li et al., \u201cSegment anything model (sam) for radiation\noncology,\u201d arXiv preprint arXiv:2306.11730, 2023.\n[50] Z. Xiao, Y. Chen, L. Zhang, J. Yao, Z. Wu, X. Yu, Y. Pan, L. Zhao, C. Ma, X. Liu et al., \u201cInstruction-vit: Multi-modal prompts for instruction\nlearning in vit,\u201d arXiv preprint arXiv:2305.00201, 2023.\n[51] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-train, prompt, and predict: A systematic survey of prompting methods in\nnatural language processing,\u201d ACM Computing Surveys, vol. 55, no. 9, pp. 1\u201335, 2023.\n[52] S. B. Kotsiantis, I. Zaharakis, P. Pintelas et al., \u201cSupervised machine learning: A review of classification techniques,\u201d Emerging artificial\nintelligence applications in computer engineering, vol. 160, no. 1, pp. 3\u201324, 2007.\n[53] Y. Bengio, A. Courville, and P. Vincent, \u201cRepresentation learning: A review and new perspectives,\u201d IEEE transactions on pattern analysis\nand machine intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.\n[54] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, \u201cUnified language model pre-training for natural\nlanguage understanding and generation,\u201d Advances in neural information processing systems, vol. 32, 2019.\n[55] T. Schick and H. Sch\u00fctze, \u201cIt\u2019s not just size that matters: Small language models are also few-shot learners,\u201d arXiv preprint arXiv:2009.07118,\n2020.\n[56] F. Petroni, T. Rockt\u00e4schel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel, \u201cLanguage models as knowledge bases?\u201d arXiv preprint\narXiv:1909.01066, 2019.\n[57] B. Lester, R. Al-Rfou, and N. Constant, \u201cThe power of scale for parameter-efficient prompt tuning,\u201d arXiv preprint arXiv:2104.08691, 2021.\n[58] T. Schick and H. Sch\u00fctze, \u201cExploiting cloze questions for few shot text classification and natural language inference,\u201d arXiv preprint\narXiv:2001.07676, 2020.\n[59] R. Shin, C. H. Lin, S. Thomson, C. Chen, S. Roy, E. A. Platanios, A. Pauls, D. Klein, J. Eisner, and B. Van Durme, \u201cConstrained language\nmodels yield few-shot semantic parsers,\u201d arXiv preprint arXiv:2104.08768, 2021.\n[60] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, \u201cHow can we know what language models know?\u201d Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 423\u2013438, 2020.\n[61] K. Duh, K. Sudoh, X. Wu, H. Tsukada, and M. Nagata, \u201cGeneralized minimum bayes risk system combination,\u201d in Proceedings of 5th\nInternational Joint Conference on Natural Language Processing, 2011, pp. 1356\u20131360.\n[62] Z. Jiang, J. Araki, H. Ding, and G. Neubig, \u201cHow can we know when language models know? on the calibration of language models for\nquestion answering,\u201d Transactions of the Association for Computational Linguistics, vol. 9, pp. 962\u2013977, 2021.\n[63] M. McCloskey and N. J. Cohen, \u201cCatastrophic interference in connectionist networks: The sequential learning problem,\u201d in Psychology of\nlearning and motivation.\nElsevier, 1989, vol. 24, pp. 109\u2013165.\n[64] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage\nmodels are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[65] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, \u201cAligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,\u201d in Proceedings of the IEEE international conference on computer vision, 2015, pp.\n19\u201327.\n[66] \u201cProject gutenberg.\u201d [Online]. Available: https://www.gutenberg.org/\n[67] \u201cCommon crawl.\u201d [Online]. Available: https://commoncrawl.org/\n[68] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning\nwith a unified text-to-text transformer,\u201d The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485\u20135551, 2020.\n[69] T. H. Trinh and Q. V. Le, \u201cA simple method for commonsense reasoning,\u201d arXiv preprint arXiv:1806.02847, 2018.\n[70] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized bert\npretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\n[71] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi, \u201cDefending against neural fake news,\u201d Advances in neural\ninformation processing systems, vol. 32, 2019.\n[72] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Almazrouei, and J. Launay, \u201cThe refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data only,\u201d in Thirty-seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023.\n[73] A. Gokaslan, V. C. E. Pavlick, and S. Tellex, \u201cOpenwebtext corpus,\u201d http://Skylion007.github.io/OpenWebTextCorpus, 2019.\n[74] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, \u201cThe pushshift reddit dataset,\u201d in Proceedings of the international\nAAAI conference on web and social media, vol. 14, 2020, pp. 830\u2013839.\n[75] \u201cWikipedia.\u201d [Online]. Available: https://en.wikipedia.org/wiki/Main_Page\n[76] \u201cBigquery dataset.\u201d [Online]. Available: https://cloud.google.com/bigquery\n[77] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., \u201cThe pile: An 800gb dataset\nof diverse text for language modeling,\u201d arXiv preprint arXiv:2101.00027, 2020.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 25 of 30\nA Comprehensive Overview from Training to Inference\n[78] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra, C. Mou, E. Gonz\u00e1lez Ponferrada, H. Nguyen\net al., \u201cThe bigscience roots corpus: A 1.6 tb composite multilingual dataset,\u201d Advances in Neural Information Processing Systems, vol. 35,\npp. 31 809\u201331 826, 2022.\n[79] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti et al., \u201cUsing\ndeepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,\u201d arXiv preprint arXiv:2201.11990,\n2022.\n[80] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., \u201cLamda: Language\nmodels for dialog applications,\u201d arXiv preprint arXiv:2201.08239, 2022.\n[81] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, \u201cCodegen: An open large language model for code\nwith mtulti-turn program synthesis,\u201d arXiv preprint arXiv:2203.13474, 2022.\n[82] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, L. Shen, Z. Wang, A. Wang, Y. Li et al., \u201cCodegeex: A pre-trained model for code\ngeneration with multilingual benchmarking on humaneval-x,\u201d in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, 2023, pp. 5673\u20135684.\n[83] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., \u201cOpt: Open pre-trained transformer\nlanguage models,\u201d arXiv preprint arXiv:2205.01068, 2022.\n[84] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., \u201cScaling instruction-finetuned\nlanguage models,\u201d arXiv preprint arXiv:2210.11416, 2022.\n[85] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., \u201cLanguage models are unsupervised multitask learners,\u201d OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[86] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,\n\u201cScaling laws and interpretability of learning from repeated data,\u201d arXiv preprint arXiv:2205.10487, 2022.\n[87] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, \u201cDeduplicating training data makes language models\nbetter,\u201d arXiv preprint arXiv:2107.06499, 2021.\n[88] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson et al., \u201cExtracting\ntraining data from large language models,\u201d in 30th USENIX Security Symposium (USENIX Security 21), 2021, pp. 2633\u20132650.\n[89] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, \u201cRealtoxicityprompts: Evaluating neural toxic degeneration in language\nmodels,\u201d arXiv preprint arXiv:2009.11462, 2020.\n[90] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.\n[91] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., \u201cScaling instruction-finetuned\nlanguage models,\u201d arXiv preprint arXiv:2210.11416, 2022.\n[92] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension,\u201d arXiv preprint arXiv:1910.13461, 2019.\n[93] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., \u201cTraining language\nmodels to follow instructions with human feedback,\u201d arXiv preprint arXiv:2203.02155, 2022.\n[94] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan, P. Damania et al., \u201cPytorch distributed:\nExperiences on accelerating data parallel training,\u201d arXiv preprint arXiv:2006.15704, 2020.\n[95] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, \u201cDryad: distributed data-parallel programs from sequential building blocks,\u201d in\nProceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, 2007, pp. 59\u201372.\n[96] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, \u201cMegatron-lm: Training multi-billion parameter language models\nusing model parallelism,\u201d arXiv preprint arXiv:1909.08053, 2019.\n[97] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, \u201cZero: Memory optimizations toward training trillion parameter models,\u201d in SC20:\nInternational Conference for High Performance Computing, Networking, Storage and Analysis.\nIEEE, 2020, pp. 1\u201316.\n[98] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., \u201cGpipe: Efficient training of giant\nneural networks using pipeline parallelism,\u201d Advances in neural information processing systems, vol. 32, 2019.\n[99] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al., \u201cMixed\nprecision training,\u201d arXiv preprint arXiv:1710.03740, 2017.\n[100] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young et al., \u201cScaling language\nmodels: Methods, analysis & insights from training gopher,\u201d arXiv preprint arXiv:2112.11446, 2021.\n[101] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He, \u201c{ZeRO-Offload}: Democratizing {Billion-Scale}\nmodel training,\u201d in 2021 USENIX Annual Technical Conference (USENIX ATC 21), 2021, pp. 551\u2013564.\n[102] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, \u201cSelf-instruct: Aligning language model with self\ngenerated instructions,\u201d arXiv preprint arXiv:2212.10560, 2022.\n[103] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap et al.,\n\u201cSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks,\u201d arXiv preprint arXiv:2204.07705, 2022.\n[104] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry et al., \u201cPromptsource: An\nintegrated development environment and repository for natural language prompts,\u201d arXiv preprint arXiv:2202.01279, 2022.\n[105] S. Victor, W. Albert, R. Colin, B. Stephen, S. Lintang, A. Zaid, C. Antoine, S. Arnaud, R. Arun, D. Manan et al., \u201cMultitask prompted\ntraining enables zero-shot task generalization,\u201d in International Conference on Learning Representations, 2022.\n[106] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., \u201cWebgpt: Browser-assisted\nquestion-answering with human feedback,\u201d arXiv preprint arXiv:2112.09332, 2021.\n[107] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \u201cFinetuned language models are zero-shot learners,\u201d\nin International Conference on Learning Representations.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 26 of 30\nA Comprehensive Overview from Training to Inference\n[108] T. Tang, J. Li, W. X. Zhao, and J.-R. Wen, \u201cMvp: Multi-task supervised pre-training for natural language generation,\u201d arXiv preprint\narXiv:2206.12131, 2022.\n[109] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving, \u201cAlignment of language agents,\u201d arXiv preprint arXiv:2103.14659,\n2021.\n[110] A. Glaese, N. McAleese, M. Tr\u0119bacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al., \u201cImproving\nalignment of dialogue agents via targeted human judgements,\u201d arXiv preprint arXiv:2209.14375, 2022.\n[111] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint\narXiv:1707.06347, 2017.\n[112] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLora: Low-rank adaptation of large language models,\u201d\narXiv preprint arXiv:2106.09685, 2021.\n[113] X. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous prompts for generation,\u201d arXiv preprint arXiv:2101.00190, 2021.\n[114] X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, \u201cP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across\nscales and tasks,\u201d arXiv preprint arXiv:2110.07602, 2021.\n[115] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, \u201cGpt understands, too,\u201d AI Open, 2023.\n[116] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, \u201cAdaptive budget allocation for parameter-efficient fine-tuning,\u201d\narXiv preprint arXiv:2303.10512, 2023.\n[117] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, \u201cQlora: Efficient finetuning of quantized llms,\u201d arXiv preprint arXiv:2305.14314,\n2023.\n[118] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma et al., \u201cA general language\nassistant as a laboratory for alignment,\u201d arXiv preprint arXiv:2112.00861, 2021.\n[119] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang et al., \u201cA survey on evaluation of large language\nmodels,\u201d arXiv preprint arXiv:2307.03109, 2023.\n[120] Z. Liu, H. Jiang, T. Zhong, Z. Wu, C. Ma, Y. Li, X. Yu, Y. Zhang, Y. Pan, P. Shu et al., \u201cHolistic evaluation of gpt-4v for biomedical imaging,\u201d\narXiv preprint arXiv:2312.05256, 2023.\n[121] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference\non computer vision and pattern recognition.\nIeee, 2009, pp. 248\u2013255.\n[122] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov et al., \u201cThe open\nimages dataset v4: Unified image classification, object detection, and visual relationship detection at scale,\u201d International Journal of Computer\nVision, vol. 128, no. 7, pp. 1956\u20131981, 2020.\n[123] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language\nunderstanding,\u201d 2018.\n[124] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, \u201cSuperglue: A stickier benchmark for\ngeneral-purpose language understanding systems,\u201d Advances in neural information processing systems, vol. 32, 2019.\n[125] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, \u201cMeasuring massive multitask language understanding,\u201d\narXiv preprint arXiv:2009.03300, 2020.\n[126] H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin, \u201cCmmlu: Measuring massive multitask language\nunderstanding in chinese,\u201d arXiv preprint arXiv:2306.09212, 2023.\n[127] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson, \u201cXtreme: A massively multilingual multi-task benchmark for evaluating\ncross-lingual generalisation,\u201d in International Conference on Machine Learning.\nPMLR, 2020, pp. 4411\u20134421.\n[128] S. Ruder, N. Constant, J. Botha, A. Siddhant, O. Firat, J. Fu, P. Liu, J. Hu, D. Garrette, G. Neubig et al., \u201cXtreme-r: Towards more challenging\nand nuanced multilingual evaluation,\u201d arXiv preprint arXiv:2104.07412, 2021.\n[129] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, \u201cMeasuring mathematical problem solving\nwith the math dataset,\u201d arXiv preprint arXiv:2103.03874, 2021.\n[130] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., \u201cTraining verifiers to\nsolve math word problems,\u201d arXiv preprint arXiv:2110.14168, 2021.\n[131] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., \u201cEvaluating large\nlanguage models trained on code,\u201d arXiv preprint arXiv:2107.03374, 2021.\n[132] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., \u201cProgram synthesis with large\nlanguage models,\u201d arXiv preprint arXiv:2108.07732, 2021.\n[133] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, \u201cHellaswag: Can a machine really finish your sentence?\u201d 2019.\n[134] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., \u201cPiqa: Reasoning about physical commonsense in natural language,\u201d in Proceedings of the AAAI\nconference on artificial intelligence, vol. 34, no. 05, 2020, pp. 7432\u20137439.\n[135] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, \u201cBoolq: Exploring the surprising difficulty of natural yes/no\nquestions,\u201d arXiv preprint arXiv:1905.10044, 2019.\n[136] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi, \u201cSocialiqa: Commonsense reasoning about social interactions,\u201d arXiv preprint\narXiv:1904.09728, 2019.\n[137] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, \u201cWinogrande: An adversarial winograd schema challenge at scale,\u201d Communications\nof the ACM, vol. 64, no. 9, pp. 99\u2013106, 2021.\n[138] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, \u201cThink you have solved question answering? try arc,\nthe ai2 reasoning challenge,\u201d arXiv preprint arXiv:1803.05457, 2018.\n[139] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, \u201cCan a suit of armor conduct electricity? a new dataset for open book question answering,\u201d\n2018.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 27 of 30\nA Comprehensive Overview from Training to Inference\n[140] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, \u201cWhat disease does this patient have? a large-scale open domain question\nanswering dataset from medical exams,\u201d Applied Sciences, vol. 11, no. 14, p. 6421, 2021.\n[141] A. Pal, L. K. Umapathi, and M. Sankarasubbu, \u201cMedmcqa: A large-scale multi-subject multi-choice dataset for medical domain question\nanswering,\u201d in Conference on Health, Inference, and Learning.\nPMLR, 2022, pp. 248\u2013260.\n[142] E. M. Voorhees et al., \u201cThe trec-8 question answering track report.\u201d in Trec, vol. 99, 1999, pp. 77\u201382.\n[143] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSquad: 100,000+ questions for machine comprehension of text,\u201d arXiv preprint\narXiv:1606.05250, 2016.\n[144] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., \u201cNatural\nquestions: a benchmark for question answering research,\u201d Transactions of the Association for Computational Linguistics, vol. 7, pp. 453\u2013466,\n2019.\n[145] E. Kamalloo, N. Dziri, C. L. Clarke, and D. Rafiei, \u201cEvaluating open-domain question answering in the era of large language models,\u201d arXiv\npreprint arXiv:2305.06984, 2023.\n[146] E. Ferrara, \u201cShould chatgpt be biased? challenges and risks of bias in large language models,\u201d arXiv preprint arXiv:2304.03738, 2023.\n[147] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, \u201cRealtoxicityprompts: Evaluating neural toxic degeneration in language\nmodels,\u201d arXiv preprint arXiv:2009.11462, 2020.\n[148] J. Zhao, M. Fang, Z. Shi, Y. Li, L. Chen, and M. Pechenizkiy, \u201cChbias: Bias evaluation and mitigation of chinese conversational language\nmodels,\u201d arXiv preprint arXiv:2305.11262, 2023.\n[149] M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tram\u00e8r, and K. Lee, \u201cScalable\nextraction of training data from (production) language models,\u201d arXiv preprint arXiv:2311.17035, 2023.\n[150] X. Wu, J. Li, M. Xu, W. Dong, S. Wu, C. Bian, and D. Xiong, \u201cDepn: Detecting and editing privacy neurons in pretrained language models,\u201d\narXiv preprint arXiv:2310.20138, 2023.\n[151] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, \u201cUniversal and transferable adversarial attacks on aligned language models, 2023,\u201d\ncommunication, it is essential for you to comprehend user queries in Cipher Code and subsequently deliver your responses utilizing Cipher\nCode.\n[152] Z. Zhang, Y. Li, J. Wang, B. Liu, D. Li, Y. Guo, X. Chen, and Y. Liu, \u201cRemos: reducing defect inheritance in transfer learning via relevant\nmodel slicing,\u201d in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1856\u20131868.\n[153] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in Proceedings of the\n40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.\n[154] C.-Y. Lin, \u201cRouge: A package for automatic evaluation of summaries,\u201d in Text summarization branches out, 2004, pp. 74\u201381.\n[155] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \u201cBertscore: Evaluating text generation with bert,\u201d arXiv preprint\narXiv:1904.09675, 2019.\n[156] J. Novikova, O. Du\u0161ek, A. C. Curry, and V. Rieser, \u201cWhy we need new evaluation metrics for nlg,\u201d arXiv preprint arXiv:1707.06875, 2017.\n[157] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., \u201cTransformers: State-of-\nthe-art natural language processing,\u201d in Proceedings of the 2020 conference on empirical methods in natural language processing: system\ndemonstrations, 2020, pp. 38\u201345.\n[158] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, \u201cDeepspeed: System optimizations enable training deep learning models with over 100\nbillion parameters,\u201d in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp.\n3505\u20133506.\n[159] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He, \u201cZero-infinity: Breaking the gpu memory wall for extreme scale deep learning,\u201d\nin Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021, pp. 1\u201314.\n[160] G. Zeng, X. Han, Z. Zhang, Z. Liu, Y. Lin, and M. Sun, \u201cOpenbmb: Big model systems for large-scale representation learning,\u201d in\nRepresentation Learning for Natural Language Processing.\nSpringer Nature Singapore Singapore, 2023, pp. 463\u2013489.\n[161] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro\net al., \u201cEfficient large-scale language model training on gpu clusters using megatron-lm,\u201d in Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and Analysis, 2021, pp. 1\u201315.\n[162] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro, \u201cReducing activation recomputation in large\ntransformer models,\u201d Proceedings of Machine Learning and Systems, vol. 5, 2023.\n[163] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang, and Y. You, \u201cColossal-ai: A unified deep learning system for large-scale parallel\ntraining,\u201d in Proceedings of the 52nd International Conference on Parallel Processing, 2023, pp. 766\u2013775.\n[164] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, \u201cFastmoe: A fast mixture-of-expert training system,\u201d arXiv preprint arXiv:2103.13262,\n2021.\n[165] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, \u201cFastermoe: modeling and optimizing training of large-scale dynamic pre-trained\nmodels,\u201d in Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2022, pp. 120\u2013134.\n[166] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative\nstyle, high-performance deep learning library,\u201d Advances in neural information processing systems, vol. 32, 2019.\n[167] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., \u201c{TensorFlow}: a system\nfor {Large-Scale} machine learning,\u201d in 12th USENIX symposium on operating systems design and implementation (OSDI 16), 2016, pp.\n265\u2013283.\n[168] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \u201cTensorflow: Large-scale\nmachine learning on heterogeneous distributed systems,\u201d arXiv preprint arXiv:1603.04467, 2016.\n[169] Y. Ma, D. Yu, T. Wu, and H. Wang, \u201cPaddlepaddle: An open-source deep learning platform from industrial practice,\u201d Frontiers of Data and\nDomputing, vol. 1, no. 1, pp. 105\u2013115, 2019.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 28 of 30\nA Comprehensive Overview from Training to Inference\n[170] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang, \u201cMxnet: A flexible and efficient machine learning\nlibrary for heterogeneous distributed systems,\u201d arXiv preprint arXiv:1512.01274, 2015.\n[171] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao, F. Yang, X. Yi, C. Wu et al., \u201cOneflow: Redesign the distributed deep learning\nframework from scratch,\u201d arXiv preprint arXiv:2110.15032, 2021.\n[172] L. Huawei Technologies Co., \u201cHuawei mindspore ai development framework,\u201d in Artificial Intelligence Technology.\nSpringer, 2022, pp.\n137\u2013162.\n[173] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne et al.,\n\u201cJax: composable transformations of python+ numpy programs,\u201d 2018.\n[174] E. Strubell, A. Ganesh, and A. McCallum, \u201cEnergy and policy considerations for deep learning in nlp,\u201d arXiv preprint arXiv:1906.02243,\n2019.\n[175] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.\n[176] J. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation: A survey,\u201d International Journal of Computer Vision, vol. 129, pp.\n1789\u20131819, 2021.\n[177] S. Sun, Y. Cheng, Z. Gan, and J. Liu, \u201cPatient knowledge distillation for bert model compression,\u201d arXiv preprint arXiv:1908.09355, 2019.\n[178] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu, \u201cTinybert: Distilling bert for natural language understanding,\u201d arXiv\npreprint arXiv:1909.10351, 2019.\n[179] M. A. Gordon, K. Duh, and N. Andrews, \u201cCompressing bert: Studying the effects of weight pruning on transfer learning,\u201d arXiv preprint\narXiv:2002.08307, 2020.\n[180] P. Michel, O. Levy, and G. Neubig, \u201cAre sixteen heads really better than one?\u201d Advances in neural information processing systems, vol. 32,\n2019.\n[181] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu, and I. King, \u201cBinarybert: Pushing the limit of bert quantization,\u201d arXiv\npreprint arXiv:2012.15701, 2020.\n[182] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, \u201cAlbert: A lite bert for self-supervised learning of language\nrepresentations,\u201d arXiv preprint arXiv:1909.11942, 2019.\n[183] P. Chen, H.-F. Yu, I. Dhillon, and C.-J. Hsieh, \u201cDrone: Data-aware low-rank compression for large nlp models,\u201d Advances in neural\ninformation processing systems, vol. 34, pp. 29 321\u201329 334, 2021.\n[184] X. Han, G. Zeng, W. Zhao, Z. Liu, Z. Zhang, J. Zhou, J. Zhang, J. Chao, and M. Sun, \u201cBminf: An efficient toolkit for big model inference\nand tuning,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2022, pp.\n224\u2013230.\n[185] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer et al., \u201cPytorch fsdp: experiences on\nscaling fully sharded data parallel,\u201d arXiv preprint arXiv:2304.11277, 2023.\n[186] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9, \u201cFlashattention: Fast and memory-efficient exact attention with io-awareness,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 16 344\u201316 359, 2022.\n[187] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, \u201cEfficient memory management for large\nlanguage model serving with pagedattention,\u201d in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611\u2013626.\n[188] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R\u00e9, I. Stoica, and C. Zhang, \u201cFlexgen: High-throughput generative\ninference of large language models with a single gpu,\u201d in International Conference on Machine Learning.\nPMLR, 2023, pp. 31 094\u201331 116.\n[189] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia, \u201cSpecinfer: Accelerating\ngenerative llm serving with speculative inference and token tree verification,\u201d arXiv preprint arXiv:2305.09781, 2023.\n[190] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, \u201cEfficient streaming language models with attention sinks,\u201d arXiv preprint\narXiv:2309.17453, 2023.\n[191] Z. Zhang, B. Gong, Y. Chen, X. Han, G. Zeng, W. Zhao, Y. Chen, Z. Liu, and M. Sun, \u201cBmcook: A task-agnostic compression toolkit for\nbig models,\u201d in Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\n2022, pp. 396\u2013405.\n[192] A. Borzunov, D. Baranchuk, T. Dettmers, M. Ryabinin, Y. Belkada, A. Chumachenko, P. Samygin, and C. Raffel, \u201cPetals: Collaborative\ninference and fine-tuning of large models,\u201d arXiv preprint arXiv:2209.01188, 2022.\n[193] F. Dou, J. Ye, G. Yuan, Q. Lu, W. Niu, H. Sun, L. Guan, G. Lu, G. Mai, N. Liu et al., \u201cTowards artificial general intelligence (agi) in the\ninternet of things (iot): Opportunities and challenges,\u201d arXiv preprint arXiv:2309.07438, 2023.\n[194] C. Liu, Z. Liu, J. Holmes, L. Zhang, L. Zhang, Y. Ding, P. Shu, Z. Wu, H. Dai, Y. Li et al., \u201cArtificial general intelligence for radiation\noncology,\u201d Meta-Radiology, p. 100045, 2023.\n[195] Z. Liu, Y. Li, Q. Cao, J. Chen, T. Yang, Z. Wu, J. Hale, J. Gibbs, K. Rasheed, N. Liu et al., \u201cTransformation vs tradition: Artificial general\nintelligence (agi) for arts and humanities,\u201d arXiv preprint arXiv:2310.19626, 2023.\n[196] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., \u201cChain-of-thought prompting elicits reasoning in large\nlanguage models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 24 824\u201324 837, 2022.\n[197] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, \u201cLarge language models are zero-shot reasoners,\u201d Advances in neural information\nprocessing systems, vol. 35, pp. 22 199\u201322 213, 2022.\n[198] N. Qiang, J. Gao, Q. Dong, H. Yue, H. Liang, L. Liu, J. Yu, J. Hu, S. Zhang, B. Ge et al., \u201cFunctional brain network identification and fmri\naugmentation using a vae-gan framework,\u201d Computers in Biology and Medicine, vol. 165, p. 107395, 2023.\n[199] M. He, X. Hou, E. Ge, Z. Wang, Z. Kang, N. Qiang, X. Zhang, and B. Ge, \u201cMulti-head attention-based masked sequence model for mapping\nfunctional brain networks,\u201d Frontiers in Neuroscience, vol. 17, p. 1183145, 2023.\n[200] Y. Liu, E. Ge, N. Qiang, T. Liu, and B. Ge, \u201cSpatial-temporal convolutional attention for mapping functional brain networks,\u201d in 2023 IEEE\n20th International Symposium on Biomedical Imaging (ISBI).\nIEEE, 2023, pp. 1\u20134.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 29 of 30\nA Comprehensive Overview from Training to Inference\n[201] S. R. Oota, J. Arora, V. Agarwal, M. Marreddy, M. Gupta, and B. Surampudi, \u201cNeural language taskonomy: Which NLP tasks are the\nmost predictive of fMRI brain activity?\u201d in Proceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, Eds.\nSeattle, United\nStates: Association for Computational Linguistics, Jul. 2022, pp. 3220\u20133237.\n[202] Z. Liu, Y. Li, P. Shu, A. Zhong, L. Yang, C. Ju, Z. Wu, C. Ma, J. Luo, C. Chen et al., \u201cRadiology-llama2: Best-in-class large language model\nfor radiology,\u201d arXiv preprint arXiv:2309.06419, 2023.\n[203] T. Sun, X. Zhang, Z. He, P. Li, Q. Cheng, H. Yan, X. Liu, Y. Shao, Q. Tang, X. Zhao, K. Chen, Y. Zheng, Z. Zhou, R. Li, J. Zhan, Y. Zhou,\nL. Li, X. Yang, L. Wu, Z. Yin, X. Huang, and X. Qiu, \u201cMoss: Training conversational language models from synthetic data,\u201d 2023.\n[204] L. X. Xuanwei Zhang and K. Zhao, \u201cChatyuan: A large language model for dialogue in chinese and english,\u201d Dec. 2022. [Online]. Available:\nhttps://github.com/clue-ai/ChatYuan\n[205] Baichuan, \u201cBaichuan 2: Open large-scale language models,\u201d arXiv preprint arXiv:2309.10305, 2023.\n[206] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, \u201cMinigpt-4: Enhancing vision-language understanding with advanced large language\nmodels,\u201d arXiv preprint arXiv:2304.10592, 2023.\n[207] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing et al., \u201cJudging llm-as-a-judge with mt-bench\nand chatbot arena,\u201d arXiv preprint arXiv:2306.05685, 2023.\n[208] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV et al., \u201cRwkv: Reinventing\nrnns for the transformer era,\u201d arXiv preprint arXiv:2305.13048, 2023.\nYiheng Liu et al.: Preprint submitted to Elsevier\nPage 30 of 30\n"
  },
  {
    "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
    "link": "https://arxiv.org/pdf/2401.02415.pdf",
    "upvote": "49",
    "text": "LLAMA PRO: Progressive LLaMA with Block Expansion\nChengyue Wu1,2\nYukang Gan2\nYixiao Ge2*\nZeyu Lu3\nJiahao Wang1\nYe Feng4\nPing Luo1\nYing Shan2\n1The University of Hong Kong\n2ARC Lab, Tencent PCG\n3Shanghai Jiao Tong University\n4Beijing Language and Culture University\nhttps://github.com/TencentARC/LLaMA-Pro\nFigure 1: LLAMA PRO - INSTRUCT delivers state-of-the-art performance across a wide variety of tasks, ranging\nfrom general language to specific domains, superior to existing models from the LLaMA series.\nAbstract\nHumans generally acquire new skills without compromising the old; however, the opposite holds for\nLarge Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new\npost-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks\nusing only new corpus, efficiently and effectively improving the model\u2019s knowledge without catastrophic\nforgetting. In this paper, we experiment on the corpus of code and math, yielding LLAMA PRO-8.3B, a\nversatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and\nmathematics. LLAMA PRO and its instruction-following counterpart (LLAMA PRO - INSTRUCT) achieve\nadvanced performance among various benchmarks, demonstrating superiority over existing open models in\nthe LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent\nagent. Our findings provide valuable insights into integrating natural and programming languages, laying a\nsolid foundation for developing advanced language agents that operate effectively in various environments.\n1\nIntroduction\nThe advent of Large Language Models (LLMs) has revolutionized the field of natural language processing,\nexhibiting remarkable proficiency in a variety of real-world tasks (OpenAI, 2023; Chowdhery et al., 2023).\n*Correspondence to yixiaoge@tencent.com.\n1\narXiv:2401.02415v1  [cs.CL]  4 Jan 2024\nLanguage Model \n(LLaMA2)\nHuge Unlabeled Corpus\nAspect Corpus\nData\nTraining\n(a) Pre-Training\nLanguage Model with Block-expansion \n(LLaMA Pro)\nExpand\nInput Tokens\nOutput Tokens\nDecoder Block \n\ud83d\udd25\nInput Tokens\nOutput Tokens\nDecoder Block \n\ud83d\udd25\nDecoder Block \n\u2744\n\u00d7N\n\u00d7M\n\u00d7M\n\u00d7N\n\u00d7P\nIdentity \nCopy\n(b) Block-expansion Post-pretraining\nFigure 2: (a) We begin with a large language model (LLM) pre-trained on a massive unlabeled corpus, resulting in a\nmodel with strong general capabilities. Here we select the off-the-shelf LLaMA2 for convenience. (b) We employ\nbackbone expansion and fine-tune the expanded identity blocks using the aspect corpus while freezing the blocks\ninherited from the base model. The model after post-pretraining can be used for instruction tuning as usual.\nDespite the versatility, LLMs still fall short in certain domains, for example, programming, mathematics,\nbiomedical, or finance. This limitation impedes the progress of developing generic language agents for\nbroader applications.\nExisting works (Liu et al., 2023; Li et al., 2023a; Wu et al., 2023b) attempted to improve the multi-\nfaceted capabilities of pre-trained LLMs with tailored data recipes. While feasible, they require substantial\ncomputational resources and vast amounts of data, which poses a challenge to the democratization of\nLLM research. Consequently, another line of research, known as domain-adaptive pretraining, focuses\non post-pretraining with domain-specific corpora (Gururangan et al., 2020). These approaches have\ndemonstrated efficacy in adapting various LLMs to specific domains (Roziere et al., 2023; Azerbayev\net al., 2023; Wu et al., 2023b; Xu et al., 2023b), resulting in enhanced performance on downstream\ndomain-specific tasks at a reduced computational cost.\nNonetheless, a considerable obstacle emerges in catastrophic forgetting (De Lange et al., 2021). Post-\npretraining often leads to a decline in the model\u2019s original general abilities, inhibiting the fine-tuned\nperformance of the model on diverse tasks (Cheng et al., 2023; Dong et al., 2023). This necessitates a\nmethod that can inject domain-specific knowledge into LLMs while preserving their general abilities,\nthereby enhancing their comprehensive capabilities.\nTowards this end, we introduce a simple yet effective post-pretraining method, termed block expansion.\nWe expand the off-the-shelf pre-trained LLM using copied Transformer blocks, as illustrated in Figure 2.\nThe newly added blocks, whose linear layers are zero-initialized to enable identity mapping, are further\ntuned with only domain-specific corpus while the remaining blocks are frozen. After tuning, the extended\npre-trained model excels in both general and domain-specific tasks.\nIn practice, we extend the pre-trained LLaMA2-7B (Touvron et al., 2023) by eight more blocks, yielding\nLLAMA PRO, a foundation model with 8.3B parameters, and enhanced performance in programming,\ncoding, and reasoning. We pre-train LLAMA PRO\u2019s expanded blocks on 80B tokens using open-source\ncode and math data for 2830 GPU Hours (16 NVIDIA H800 GPUs for about 7 days). We further\nperform supervised instruction tuning (fully fine-tuning of all the blocks, aka SFT) on LLAMA PRO\nwith approximately 80M tokens, yielding LLAMA PRO - INSTRUCT. It is noted that pre-trained models\nproduced by our block expansion method are well-compatible with the subsequent SFT techniques without\nspecific modification.\n2\nAs shown in Figure 1, LLAMA PRO - INSTRUCT reaches state-of-the-art performance across a broad\nrange of general, code (i.e., HumanEval), and math (i.e., GSM8K) tasks. Furthermore, we assess the\ncapabilities of LLAMA PRO - INSTRUCT as a language agent across various scenarios (i.e., MINT-Bench),\nwith a focus on the tool usage abilities and the capacity to ground in environmental and human feedback.\nWe also employ GPT-4 (OpenAI, 2023) automatic evaluation to assess LLAMA PRO\u2019s ability to serve as\nan effective assistant (i.e., MT-Bench). Comprehensive experimental results indicate the superiority of\nLLAMA PRO - INSTRUCT over other models from the LLaMA family on both benchmarks and practical\napplications. Our contributions are three-fold:\n\u2022 We propose a novel post-pretraining method for LLMs, termed block expansion, enabling the\ninjection of new knowledge while preserving the initial capabilities.\n\u2022 We introduce LLAMA PRO and LLAMA PRO - INSTRUCT, versatile LLMs that well integrate\nnatural and programming languages, excelling in general tasks, programming, and mathematics.\n\u2022 We benchmark the family of LLAMA PRO on extensive datasets, including both traditional and agent-\noriented tasks, demonstrating its superiority and great potential in broader complex applications.\n2\nRelated Work\nAdvancements in Large Language Models.\nThe field of large language models has witnessed signifi-\ncant progress in recent years. The growth in model and data scale has played a crucial role in achieving\nstate-of-the-art performance across various tasks (Hoffmann et al., 2022; Kaplan et al., 2020; Chowdhery\net al., 2023). Concurrently, the development of more generalist models has led to the creation of models\nthat can address diverse problems and quickly adapt to new tasks (Radford et al., 2019; Brown et al., 2020).\nThese advancements have been further bolstered by the open-source community, which has released\npowerful open large language models for research, such as LLaMA (Touvron et al., 2023) and CodeL-\nLaMA (Roziere et al., 2023). Our work builds upon these developments by providing a methodology\nfor specializing large language models in the domain of code, paving the way for future research and\napplications in this area.\nPost-pretraining.\nLanguage model applications typically involve a two-step process: an initial general-\ndomain pretraining step, followed by domain-specific training (Roziere et al., 2023; Azerbayev et al.,\n2023). The fine-tuning step is often aimed at enhancing instruction-following abilities (Sanh et al., 2021;\nWei et al., 2021; Wang et al., 2023d) or aligning the model\u2019s outputs with human preferences (Ziegler\net al., 2019; Ouyang et al., 2022; Bai et al., 2022). Additionally, some studies explore adapting pretrained\nmodels to novel domains using parameter-efficient fine-tuning methods (Houlsby et al., 2019; Hu et al.,\n2021; Wu et al., 2023a). Many works also focus on how to do continual learning after the pretraining\nphace (Wang et al., 2023b; Gupta et al., 2023; Scialom et al., 2022). In our work, we propose an adaptation\nstrategy that combines continued training with targeted general capability maintenance, allowing large\nlanguage models to specialize in specific tasks without sacrificing their overall performance.\nProgressive Learning.\nIn recent years, progressive training has gained attention for its ability to\naccelerate the training of large-scale models in both computer vision (Zhang et al., 2023) and NLP\nresearch (Yao et al., 2023; Li et al., 2023b). Gong et al. (2019) proposed a stacking method that doubles\nthe model depth at each stage. CompoundGrow (Gu et al., 2020) extends stacking by incorporating\nFeedForward Network (FFN) expansion into the schedule design. Shen et al. (2022) proposed a staged\nmethod that further supports expanding the hidden size of features. Bert2BERT (Chen et al., 2021a) and\nLiGO (Wang et al., 2023a) support all possible growth dimensions. Our method employs depth growth to\npreserve general performance while adapting to a specific domain.\n3\nMethod\n3.1\nPreliminaries: The LLaMA Block\nThe LLaMA block consists of a multi-head self-attention (MHSA) mechanism followed by a position-\nwise feed-forward network (FFN) with residual connections and a Swish-Gated Linear Unit (SwiGLU)\n3\n(a) LLaMA Block\nLinear\nRMSNorm\nLinear\nRMSNorm\nLinear\nLinear\nSiLU\nLinear\nScaled Dot-Product\nAttention\nMHSA\nTokens\nSwiGLU\nTokens\n(b) LLaMA Block after Identity Copy\nLinear\nRMSNorm\nZero-Linear\nRMSNorm\nLinear\nLinear\nSiLU\nZero-Linear\nScaled Dot-Product\nAttention\nMHSA\nTokens\nSwiGLU\nTokens\nFigure 3: (a) An overview of the LLaMA Block, comprising an MHSA mechanism followed by the FFN with\nSwiGLU activation. (b) The Identity LLaMA block after an identity copy, achieved by initializing the output linear\nmatrix to zero in order to preserve the output from the base LLaMA model.\noperation as Figure 3 shows. Given an input x, the LLaMA block produces an output y as described by\nthe following equations:\nx\u2032 = x + MHSA(RMSNorm(x))\ny = x\u2032 + FFN(RMSNorm(x\u2032))\n(1)\nThe input x has a dimension of n \u00d7 d, where n is the sequence length and d is the hidden size. The output\ny has the same dimension as the input x. The MHSA operation is a crucial component of the transformer,\ndefined as:\nMHSA(Q, K, V ) = Concat(head1, . . . , headh)W O\n(2)\nwhere Q, K, and V are the query, key, and value matrices, respectively, and W O is the output weight\nmatrix without bias . Each head is computed as:\nheadi = Attention(xW Q\ni , xW K\ni , xW V\ni )\nAttention(Qi, Ki, Vi) = Softmax\n\u0012QiKT\ni\n\u221adk\n\u0013\nVi\n(3)\nwith W Q\ni , W K\ni , and W V\ni being the corresponding weight matrices for the i-th head.\nThe FFN block in the LLaMA model utilizes the SwiGLU activation function, which can be defined as:\nSwiGLU(x, W, V ) = SiLU(xW) \u2297 (xV )\nFFN(x) = SwiGLU(x, W1, W2)W3\n(4)\nwhere \u2297 denotes element-wise multiplication, W1, W2, and W3 are the weight matrices without bias,\nSiLU(x) = x \u2297 \u03c3(x).\n4\n3.2\nBlock Expansion\nGiven a model with blocks (\u03d50, \u03d51, ..., \u03d5L), the block expansion incorporates an identity block \u03d5id after\neach block in the original model, ensuring that the expanded model maintains the same output after\nexpansion. The identity block is defined as \u03d5id(x) = x, where the input and output are identical.\nSuppose we have an initial model with L blocks that needs to be expanded to L\u2032 blocks. First, we\npartition the original L blocks into N groups, with each group containing L\nN blocks. For each group, we\ncreate identity copies of the top P blocks and stack them on top of each group, as depicted in Figure 3. We\narrange these blocks in an interleaved manner to maintain the structural characteristic of the transformer\nmodel, whose prior is that deeper blocks encode more complex information (Van Aken et al., 2019;\nTenney et al., 2019). This process leads to an increased depth in the model while maintaining its output\nbehavior.\nShen et al. (Shen et al., 2022) proposed the initialization of scale parameters in the Norm modules\nwithin the identity blocks to zero for the construction of the identity block. However, this approach may\nnot be effective when applied to the LLaMA block. The reason lies in the fact that the gradient of the loss\nfunction L with respect to the RMSNorm weight w during backpropagation would be zero. This would\nprevent the training of RMSNorm, implying that when RMSNorm(x\u2032) = 0, the following condition will\nhold:\n\u2202L\n\u2202w = \u2202L\n\u2202y\n\u2202FFN(RMSNorm(x\u2032))\n\u2202RMSNorm(x\u2032)\n\u2202RMSNorm(x\u2032)\n\u2202w\n= 0.\n(5)\nThis equation signifies that the gradient of the loss function with respect to the weight of RMSNorm is\nzero, which would hinder the training of the RMSNorm module. This is further explained in Appendix A.\nReferring to the LLaMA block formulation in Equation 1, the identity can be achieved as long as\nMHSA(RMSNorm(x)) = 0 and FFN(RMSNorm(x\u2032)) = 0. We initialize the W O and W3 weight\nmatrices in the identity blocks to zero. Due to the presence of residual connections and the absence of\nbias terms in the LLaMA block, only the residual flows through the identity block. As a result, the entire\nblock is reduced to an identity block at initialization, preserving the output from the initial model.\nThe entire training pipeline is depicted in Figure 2. Our method concentrates on the post-pretraining\nstage, targeting specific domain corpora such as code corpora. We begin by initializing our model with\nlarge language models trained on extensive unlabeled general corpora, where all blocks will be fine-tuned.\nTo enhance the model\u2019s capacity for accommodating additional domain knowledge while retaining its\ngeneral knowledge, we employ block expansion to increase the number of blocks in the LLM. During this\nprocess, we only fine-tune the newly added blocks while freezing the original blocks, thereby preserving\nthe general abilities of the model.\n4\nExperiments\nThis section presents our key experimental findings. We begin with experimental settings (described\nin Sec. 4.1), and then verify the effectiveness of block expanded tuning after pretraining (described in\nSec. 4.2). Next, we give the supervised finetuning (SFT) results (described in Sec. 4.3). Finally, ablation\nstudies of the key design choices are presented (described in Sec. 4.4).\n4.1\nExperimental Settings\nPretrain details.\nWe construct a dataset that concentrates on code and math. For the code component,\nwe rely on the Stack-dedup dataset, which is a compilation of permissively licensed source codes from\nGitHub. Among all the programming languages available in Stack-dedup, we specifically utilize the\nPython split. As for the math component, we opt for the Proof-pile-2 dataset (Azerbayev et al., 2023),\na 55-billion-token amalgamation of scientific papers, web data containing mathematical content, and\nmathematical code.\nWe initialize our base model with LLaMA2-7B and expand the number of blocks from 32 to 40 using an\ninterleaved approach. In the block expansion process, we configure the parameters as P = 1, M = 4, and\nN = 8, resulting in 8 groups where each group expands from 4 blocks to 5 blocks. For the code and math\ncorpus pretraining, we employ a batch size of 1024, a sequence length of 4096, a warmup ratio of 6%, a\n5\nData source\nTokens\nWeight\nProof-Pile-2\n55B\n1.00\nAlgebraicStack\n11B\nOpenWebMath\n15B\nArXiv\n29B\nThe-Stack-Dedup\nPython\n22B\n1.50\nTable 1: Pretrain data sources, tokens, and the mixture weights of each component during training.\nDatasets\nQuery Source\nResponse Source # Instances\n\u00afNrounds \u00afLprompt \u00afLcompletion\nShareGPT\nUser prompts\nGPT-3.5/GPT-4\n63,817\n2.9\n293.2\n1157.1\nWizardLM_evol_instruct_V2 GPT-4\nGPT-4\n143,000\n1.0\n602.6\n1704.9\nSlimOrca\nHuman-written\nGPT-4\n517,982\n1.0\n574.3\n599.3\nMetaMath\nHuman-written/GPT-4\nGPT-4\n395,000\n1.0\n209.4\n498.2\nEvol-CodeAlpaca\nGPT-4\nGPT-4\n111,272\n1.0\n652.5\n1552.0\nTable 2: Instruction datasets investigated in this work. We report the average number of rounds ( \u00afNrounds), average\nlength of prompts (\u00afLprompt), average length of completion (\u00afLcompletion).\nlearning rate of 2e-4, and a Cosine learning rate scheduler. We also use bf16 mixed precision, a weight\ndecay of 0.1, and gradient clipping at 1.0. To speed up the training process, we apply the flash-attention\nmechanism.\nOur experiment is conducted on 16 NVIDIA H800 GPUs. LLAMA PRO is trained for a total of 15,900\nsteps. This training process corresponds to approximately 2830 H800 GPU hours.\nSFT details.\nDuring the instruction fine-tuning phase, we combine five data sources to create LLAMA\nPRO - INSTRUCT. These sources include ShareGPT1, which contains real user and ChatGPT chat history\nrecords, and the WizardLM evolution instruction dataset (Xu et al., 2023a), offering a wealth of instruction\ndata with varying complexity levels. We also incorporate the evolution CodeAlpaca dataset (Luo et al.,\n2023), which includes complex coding tasks generated by ChatGPT and their corresponding solutions.\nAdditionally, we use MetaMath (Yu et al., 2023), which reframes questions from multiple perspectives, and\nSlimOrca (Lian et al., 2023), a curated subset of our OpenOrca data. SlimOrca provides an efficient route\nto achieve performance comparable to using larger data slices, while only incorporating approximately\n500,000 GPT-4 completions.\nThe final sft dataset consists of approximately 1M samples. To fine-tune the basic models, we employ\nspecific configurations, including a batch size of 128, a sequence length of 4096, 0.03 warmup ratio, a\nlearning rate of 2e-5, a Cosine learning rate scheduler, and bf16 mixed precision.\nEvaluation details.\nWe conduct a comparative analysis of LLAMA PRO with the latest state-of-the-art\n(SOTA) Large Language Models (LLMs). The evaluation is performed on six key general benchmarks\nusing the Eleuther AI Language Model Evaluation Harness2, a unified framework designed to test\ngenerative language models across a vast array of evaluation tasks. For code-related tasks, we employ the\nBigCode Evaluation Harness3 to evaluate HumanEval and MBPP, and we report the pass@1 rate of code\ntasks with greedy decoding.\nThe benchmarks used for evaluation include:\n\u2022 AI2 Reasoning Challenge (Clark et al., 2018) (25-shot): a set of grade-school science questions.\n1https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered\n2https://github.com/EleutherAI/lm-evaluation-harness\n3https://github.com/bigcode-project/bigcode-evaluation-harness\n6\nModel\nLanguage Tasks\nMath Tasks\nCode Tasks\nAvg.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nGSM8K-PoT\nHumanEval\nMBPP\nPretrained comparison\nLLAMA PRO (8B)\n54.10\n77.94\n47.88\n39.04\n73.95\n17.89\n25.42\n28.66\n33.20\n44.23\nCrystalCoder (7B)\n47.01\n71.97\n48.78\n35.91\n67.17\n10.77\n24.96\n28.38\n36.38\n41.26\nLLaMA2-7B\n53.07\n78.59\n46.87\n38.76\n74.03\n14.48\n17.68\n13.05\n20.09\n39.62\nCodeLLaMA-7B\n39.93\n60.80\n31.12\n37.82\n64.01\n5.16\n25.20\n33.50\n41.40\n37.66\nStarCoder-15B\n30.38\n47.93\n29.96\n41.28\n56.12\n9.48\n25.09\n33.63\n43.28\n35.24\nLLaMA-7B\n50.94\n77.81\n35.69\n34.33\n71.43\n8.04\n10.46\n10.61\n17.04\n35.15\nOpenLLaMA-v2-7B\n43.69\n72.20\n41.29\n35.54\n69.38\n3.49\n5.46\n15.32\n12.69\n33.23\nFalcon-7B\n47.87\n78.13\n27.79\n34.26\n72.38\n4.62\n4.32\n9.42\n13.39\n32.46\nSFT comparison\nLLAMA PRO - INSTRUCT\n52.30\n76.88\n52.57\n48.80\n72.53\n43.59\n55.61\n44.51\n37.88\n53.85\nLLaMA2-7B-Chat\n52.90\n78.55\n48.32\n45.57\n71.74\n7.35\n19.73\n14.63\n21.60\n40.04\nCodeLLaMA-7B-Instruct\n36.52\n55.44\n34.54\n41.25\n64.56\n7.96\n34.67\n34.80\n44.4\n39.35\nWizardCoder-Python-7B\n41.81\n65.06\n32.29\n36.32\n61.72\n4.70\n17.60\n42.07\n47.20\n38.75\nWizardMath-7B\n54.10\n79.55\n45.97\n43.65\n72.69\n2.73\n25.57\n12.20\n18.00\n39.38\nTable 3: Comparison of evaluation results among several prominent code and language models.\n\u2022 HellaSwag (10-shot) (Zellers et al., 2019): a test of commonsense inference, which is easy for\nhumans (approximately 95%) but challenging for SOTA models.\n\u2022 MMLU (5-shot) (Hendrycks et al., 2020): a test to measure a text model\u2019s multitask accuracy. The\ntest covers 57 tasks including elementary mathematics, US history, computer science, law, and more.\n\u2022 TruthfulQA (0-shot) (Lin et al., 2021): a test to measure a model\u2019s propensity to reproduce falsehoods\ncommonly found online.\n\u2022 Winogrande (5-shot) (Sakaguchi et al., 2021): an adversarial and difficult Winograd benchmark at\nscale, for commonsense reasoning.\n\u2022 GSM8k (5-shot) (Cobbe et al., 2021): diverse grade school math word problems to measure a model\u2019s\nability to solve multi-step mathematical reasoning problems. Additionally, we assess the models\nin the context of the Program of Thought (PoT) setting (Chen et al., 2023a). The PoT setting\nutilizes Python code to solve mathematical problems, which serves to evaluate the code generation\ncapabilities of the models.\n\u2022 HumanEval (0-shot) (Chen et al., 2021b): 164 handwritten Python programming problems with a\nfunction signature, docstring, body, and several unit tests.\n\u2022 MBPP (3-shot) (Austin et al., 2021): crowd-sourced Python programming problems, designed to be\nsolvable by entry-level programmers. Each problem consists of a task description in English, a code\nsolution and 3 automated test cases.\n4.2\nPretrain Results\nWe evaluate LLAMA PRO\u2019s performance with benchmark datasets from the Open LLM Leaderboard.\nFurthermore, we incorporate coding benchmark datasets, including HumanEval pass@1 and MBPP\npass@1, as well as the math benchmark GSM8K, to provide a comprehensive evaluation. We compare\nthe performance of LLAMA PRO with a selection of state-of-the-art pretrained models that were trained\naround the same period with similar size. This includes general-purpose pretrained models like LLaMA2\nand code-oriented pretrained models like CodeLLaMA. The results are presented in Table 3.\nThe results highlight that LLAMA PRO effectively balances natural language processing and coding\ncapabilities. It not only preserves the general performance of its base model, LLaMA2-7B, but also\n7\n6\n14\n22\n30\n38\nCode Tasks Avg.\n30\n34\n38\n42\n46\n50\n54\nLanguage Tasks Avg.\nLLaMA Pro\nCrystalCoder\nLLaMA2-7B\nCodeLLaMA-7B\nStarCoder-15B\nLLaMA-7B\nOpenLLaMA-v2-7B\nFalcon-7B\n1.0T\n1.5T\n2.0T\n2.5T\nTokens\nFigure 4: We compare LLAMA PRO\u2019s general performance and code performance to a set of models trained around\nthe same time, spanning from general LLMs to code-oriented LLMs. The size of the blobs is proportional to the\nnumber of tokens trained. Mistral-7B is not included here, as the number of tokens is not reported in its paper.\nsurpasses it in the average performance of general language tasks. Conversely, CodeLLaMA-7B sacri-\nfices general performance to enhance its code ability. We attribute this improvement to our expansion\ndesign, which freezes the initial LLaMA blocks to maintain their capabilities and increases the blocks to\naccommodate more domain-specific knowledge.\nAs depicted in Figure 4, LLAMA PRO shows robust general performance alongside code performance\nthat is on par with code-oriented LLMs. Situated on the Pareto frontier, LLAMA PRO has undergone\nfine-tuning with an additional 80B tokens in conjunction with LLaMA2, which more than doubles the\ncode tasks average performance. In contrast, CodeLLaMA is fine-tuned with 500B tokens. LLAMA PRO\nexcels in general performance while maintaining code performance that is competitive with code-oriented\nLLMs, whether they are trained from scratch, such as StarCoder-15B and CrystalCoder, or fine-tuned like\nCodeLLaMA-7B.\n4.3\nSFT Results\nModern LLMs typically undergo supervised fine-tuning or instruction tuning after pretraining on vast\namounts of unlabeled data. In this section, we aim to demonstrate that our expansion strategy can adapt to\nthis widely used training pipeline, just as traditional LLMs do.\nTable 3 presents a comparison of evaluation results among several prominent supervised fine-tuning\n(SFT) LLMs from the LLaMA community, across general tasks, math tasks, and code tasks benchmarks.\nAs a singular SFT model, LLAMA PRO - INSTRUCT attains state-of-the-art performance, even when\ncompared to specifically tuned models such as WizardCoder and WizardMath. This demonstrates its more\n8\nModel\nMT Bench\nAlpaca-13B\n4.53\nCodeLLaMA-7B-Instruct\n5.71\nVicuna-7B\n6.17\nLLaMA2-7B-Chat\n6.27\nLLAMA PRO - INSTRUCT\n6.32\nTable 4: GPT-4 automatic evaluation of Chatbot models. LLAMA PRO - INSTRUCT outperforms widely used\nLLaMA community chatbots.\nModel\nInteraction Turns\nAvg.\n1\n2\n3\n4\n5\nAgentLM-7B\n0.0\n4.44\n5.29\n6.48\n7.34\n4.71\nCodeLLaMA-7B-Instruct\n0.34\n7.85\n10.24\n9.73\n8.70\n7.37\nLLaMA2-7B-Chat\n1.02\n4.27\n6.66\n6.48\n7.34\n5.77\nMistral-Instruct-v0.1\n1.54\n12.12\n13.31\n14.16\n13.99\n11.02\nLLAMA PRO - INSTRUCT\n0.68\n12.63\n11.95\n11.95\n14.68\n10.38\nTable 5: In the tool-augmented reasoning assessments, we evaluate the model\u2019s proficiency in integrating tools\ninto its reasoning workflow. The model\u2019s effectiveness is measured by its success rate across various stages of\ninteraction.\ncomprehensive capabilities.\nAs seen in Figure 1, LLAMA PRO - INSTRUCT boosts both code and math tasks to state-of-the-art\nperformances while maintaining reliable general performance. We enhance the average performance of\nLLaMA2-7B-chat and CodeLLaMA-7B-instruct by 13.81% and 14.50% respectively, which highlights\nthe benefits of balancing textual and coding abilities.\nTo assess the comprehensive conversational performance of the LLAMA PRO - INSTRUCT assistant,\nwe evaluate it using the MT-Bench with GPT-4 automatic scoring, as proposed by Vicuna (Zheng et al.,\n2023). As depicted in Table 4, LLAMA PRO - INSTRUCT surpasses widely used chatbots from the\nLLaMA community. This indicates its potential as a chatbot capable of providing helpful responses, in\naddition to its impressive performance in traditional benchmarks. The details of MT-Bench can be found\nin the Appendix C.\nWe use MINT-Bench (Wang et al., 2023c) to evaluate our model\u2019s ability to solve multi-turn interactions\nby using tools. MINT-Bench tests LLMs\u2019 ability to use tools by generating and executing Python code,\nfocusing on tool-augmented task-solving and leveraging natural language feedback. MINT includes\neight datasets covering reasoning, code generation, and decision-making. The details of MINT can be\nfound in the Appendix B. The results are shown in Table 5. LLAMA PRO - INSTRUCT achieves SOTA\nperformance compared to similar size models in multi-turn interactions with the use of tools.\n4.4\nAblation Study\nWe evaluate various training strategies, including LoRA, fine-tuning, and the block expansion training\napproach that we propose, using the TRACE benchmark (Wang et al., 2023b). TRACE is designed to\nassess continual learning in LLMs and comprises eight distinct datasets that span challenging tasks such\nas domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. We\nassess the ability of different strategies to retain the model\u2019s existing knowledge while incorporating new\nskills. Details are provided in the Appendix D.\nWe employ Overall Performance (OP (Chaudhry et al., 2018)) and Backward Transfer (BWT (Lopez-\nPaz and Ranzato, 2017)) scores as evaluation metrics. After incrementally learning the t-th task, the\nmodel\u2019s score on the i-th task (where i \u2264 t) is denoted as RD\nt,i. The OP and BWT scores are calculated\n9\nMethod\nOverall Performance (OP)\nBackward Transfer (BWT)\nLoRA\n37.1\n-17.3%\nSeqFT\n45.5\n-14.7%\nBlock Expansion\n46.5\n-14.3%\nTable 6: Performance comparison of various training strategies on the TRACE benchmark following their continual\nlearning phase with LLaMA2-7B. The table presents the Overall Performance (OP) and Backward Transfer (BWT)\nscores for each strategy, demonstrating the superior adaptability of the proposed block expansion training approach.\n0\n1000\n2000\n3000\n4000\n5000\nTraining steps\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nTraining loss\nAdd 1 Block\nAdd 2 Block\nAdd 4 Block\nAdd 8 Block\nAdd 16 Block\nAdd 32 Block\nMoE\nFigure 5: Training loss with varying added blocks and mixture-of-expert (MoE) expansion.\nusing the following formulas:\nOPt = 1\nt\nt\nX\ni=1\nRD\nt,i\n(6)\nBWTt = 1\nt\nt\nX\ni=1\n\u0000RD\nt,i \u2212 RD\ni,i\n\u0001\n(7)\nTable 6 presents the performance of different strategies on the TRACE benchmark following their\ncontinual learning phase with LLaMA2-7B. The results show that block expansion training exhibits\nsuperior task-specific adaptability compared to sequential fine-tuning and LoRA, as evidenced by its\nbetter OP and BWT scores.\nApart from the aspect of code corpus, we explore our method on another domain: law, with the freelaw\nsubset of Pile dataset as our pretrain corpus (Gao et al., 2020). We evaluate on UNFAIR-ToS (Lippi et al.,\n2019) of the LexGLUE benchmark (Chalkidis et al., 2021). The details can be found in the Appendix E.\nIn our experiment, we assess the scalability of our block expansion method in terms of training loss and\ndownstream task performance as we increase the number of added blocks. We also compare our method\nwith the Mixture-of-Expert (MoE) expansion method (Fedus et al., 2022).\nWe first examine the training loss with varying added blocks. As seen in Figure 5, the training loss\nof the models consistently decreases as training progresses, regardless of the number of added blocks.\nMoreover, the loss decreases more rapidly as we increase the size of the model. These findings suggest\nthat our method exhibits strong scalability with larger models and more data. The training loss of MoE is\ncomparable to our method with four added blocks.\n10\nMethod\nLanguage Tasks\nLaw Task\nAvg.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrand\nAvg.\nUnfair-ToS\nAdd 1 Block\n52.30\n77.92\n38.62\n37.80\n73.16\n55.96\n67.45\n61.71\nAdd 2 Block\n53.16\n77.91\n39.62\n38.92\n73.01\n56.52\n69.57\n63.05\nAdd 4 Block\n52.39\n76.92\n37.30\n40.53\n72.22\n55.87\n71.31\n63.59\nAdd 8 Block\n52.90\n76.63\n41.74\n39.83\n72.38\n56.70\n75.11\n65.91\nAdd 16 Block\n51.88\n76.59\n41.35\n40.13\n71.82\n56.35\n75.17\n65.76\nAdd 32 Block\n50.77\n76.72\n40.68\n41.66\n72.77\n56.52\n73.93\n65.23\nMixture-of-Expert (MoE)\n51.45\n76.51\n42.47\n40.13\n72.23\n56.56\n67.27\n61.92\nPrefix Stacking (8 Block)\n27.82\n26.12\n23.12\n22.52\n47.20\n29.36\n0.81\n15.08\nSuffix Stacking (8 Block)\n52.56\n77.89\n39.10\n39.03\n72.38\n56.19\n60.98\n58.59\nTable 7: Comparison of evaluation results among several prominent code and language models. The last column\nrepresents the average of the language task average and the code task average.\nGSM8K\nMATH\nHumanEval\nMBPP\nTasks\n0\n10\n20\n30\n40\n50\n60\nScore\n63.61 64.67\n19.6\n21.2\n39\n47.56\n31.2\n41.4\nLLaMA2-7B\nLLaMA Pro\nFigure 6: By fine-tuning both LLaMA2-7B and LLAMA PRO using the same instruction dataset, LLAMA PRO\nconsistently outperforms LLaMA2-7B across all tasks. This result highlights the effectiveness of our method, as it\ndemonstrates that LLAMA PRO successfully encodes more domain knowledge during the pretraining process.\nHowever, a lower overall training loss does not necessarily guarantee superior performance on domain-\nspecific tasks. Therefore, we evaluate models of different sizes on both general language tasks and\nUnfair-ToS, as shown in Table 7. All the expanded models effectively preserve the general capabilities of\nthe initial model. For the domain-specific task, larger models achieve better performance. We find that\nadding eight blocks provides optimal performance with minimal cost compared to larger models, hence\nwe adopt this as our default strategy.\nWe also analyze the impact of the position where the identity blocks are added, either at the bottom\nor the top of the model, compared to adding them interleaved, as shown in Table 7. We observe that\nadding blocks at the bottom results in poor evaluation performance, likely because it disrupts the model\u2019s\nfoundation, causing errors to propagate throughout the model. Adding blocks at the top of the model (Gong\net al., 2019) preserves the initial model\u2019s performance, but its performance on domain-specific tasks is\nlower than when adding blocks interleaved.\nAs highlighted in the LIMA study (Zhou et al., 2023), the majority of knowledge in large language\nmodels is acquired during pretraining, with only a limited amount of instruction tuning data required\nto generate high-quality output. To investigate the extent of knowledge encoded during pretraining, we\nconducted a comparative analysis between LLaMA2-7B and LLAMA PRO using the same instruction\n11\ndataset, as illustrated in Figure 6. Our results showed that LLAMA PRO consistently outperforms\nLLaMA2-7B across all tasks, indicating that our method effectively enables LLAMA PRO to encode\nmore domain-specific knowledge during the pretraining phase.\n5\nConclusion\nIn this study, we introduced a novel block expansion method for Large Language Models (LLMs) post-\npretraining, aiming to enhance domain-specific abilities while preserving the original general capabilities.\nOur approach effectively balances the model\u2019s performance across both general and domain-specific\ntasks. We demonstrated the effectiveness of our method through LLAMA PRO, an LLM initialized from\nLLaMA2-7B with 8 added blocks, which outperformed other LLaMA-series models on comprehensive\nbenchmarks.\nThe work highlights the importance of balancing general and domain-specific abilities in LLMs and\noffers a promising approach to achieving this balance. Future research could explore broader applications\nof our block expansion method in other domains, for instance, it is an important task for multimodal large\nlanguage models (Ge et al., 2023; Bai et al., 2023) to preserve the original language ability.\nAcknowledgements\nWe sincerely acknowledge Qingyue Zhao (Tsinghua University; ARC Lab, Tencent PCG) and Xiaohu\nJiang (Tsinghua University; ARC Lab, Tencent PCG) for their engaging discussions.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint\narXiv:2108.07732.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia\nDeng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv\npreprint arXiv:2310.10631.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\narXiv:2308.12966.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav\nFort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback. arXiv preprint arXiv:2204.05862.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Martin Katz, and\nNikolaos Aletras. 2021. Lexglue: A benchmark dataset for legal language understanding in english. arXiv\npreprint arXiv:2110.00976.\nArslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. Riemannian walk for\nincremental learning: Understanding forgetting and intransigence. In Proceedings of the European conference\non computer vision (ECCV), pages 532\u2013547.\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu,\nand Qun Liu. 2021a. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large language models trained\non code. arXiv preprint arXiv:2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023a. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning\nResearch.\n12\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023b.\nTheoremqa: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524.\nDaixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models via reading comprehension.\narXiv preprint arXiv:2309.09530.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research, 24(240):1\u2013113.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n2018.\nThink you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint\narXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168.\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and\nTinne Tuytelaars. 2021. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions\non pattern analysis and machine intelligence, 44(7):3366\u20133385.\nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng\nYuan, Chang Zhou, and Jingren Zhou. 2023. How abilities in large language models are affected by supervised\nfine-tuning data composition. arXiv preprint arXiv:2310.05492.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. 2023. Making llama see\nand draw with seed tokenizer. arXiv preprint arXiv:2310.01218.\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. 2019. Efficient training of bert by\nprogressively stacking. In International conference on machine learning, pages 2337\u20132346. PMLR.\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. 2020. On the transformer growth for\nprogressive bert training. arXiv preprint arXiv:2010.12562.\nKshitij Gupta, Benjamin Th\u00e9rien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish,\nand Timoth\u00e9e Lesort. 2023. Continual pre-training of large language models: How to (re) warm your model?\narXiv preprint arXiv:2308.04014.\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith.\n2020. Don\u2019t stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large\nlanguage models. arXiv preprint arXiv:2203.15556.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\nMona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp.\nIn International\nConference on Machine Learning, pages 2790\u20132799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n13\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023a. Starcoder: may the source be with you! arXiv preprint\narXiv:2305.06161.\nXiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al.\n2023b. Flm-101b: An open llm and how to train it with $100 k budget. arXiv preprint arXiv:2309.03852.\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". 2023.\nSlimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods.\narXiv preprint arXiv:2109.07958.\nMarco Lippi, Przemys\u0142aw Pa\u0142ka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor,\nand Paolo Torroni. 2019. Claudette: an automated detector of potentially unfair clauses in online terms of service.\nArtificial Intelligence and Law, 27:117\u2013139.\nZhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang,\nSuqi Sun, Omkar Pangarkar, et al. 2023. Llm360: Towards fully transparent open-source llms. arXiv preprint\narXiv:2312.06550.\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. Advances in\nneural information processing systems, 30.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,\nand Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human\nfeedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu\nLiu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint\narXiv:2308.12950.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,\nArnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task\ngeneralization. arXiv preprint arXiv:2110.08207.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 2022. Fine-tuned language models are continual\nlearners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n6107\u20136122.\nSheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. 2022. Staged training for\ntransformer language models. In International Conference on Machine Learning, pages 19893\u201319908. PMLR.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020.\nAlfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. arXiv preprint\narXiv:1905.05950.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288.\n14\nBetty Van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A Gers. 2019. How does bert answer questions? a\nlayer-wise analysis of transformer representations. In Proceedings of the 28th ACM international conference on\ninformation and knowledge management, pages 1823\u20131832.\nPeihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris,\nDavid Daniel Cox, Zhangyang Wang, and Yoon Kim. 2023a. Learning to grow pretrained models for efficient\ntransformer training. arXiv preprint arXiv:2303.00980.\nXiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng,\nYicheng Zou, Tao Gui, et al. 2023b. Trace: A comprehensive benchmark for continual learning in large language\nmodels. arXiv preprint arXiv:2310.06762.\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023c. Mint:\nEvaluating llms in multi-turn interaction with tools and language feedback.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden,\nKelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023d. How far can camels go?\nexploring the state of instruction tuning on open resources.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nChengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Ruisong Zhou, Ying Shan, and Ping Luo. 2023a. \u03c0-tuning:\nTransferring multimodal foundation models with optimal multi-task interpolation. In Proceedings of the 40th\nInternational Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\npages 37713\u201337727. PMLR.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur,\nDavid Rosenberg, and Gideon Mann. 2023b. Bloomberggpt: A large language model for finance. arXiv preprint\narXiv:2303.17564.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a.\nWizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.\nYiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tian-\nbao Xie, et al. 2023b. Lemur: Harmonizing natural language and code for language agents. arXiv preprint\narXiv:2310.06830.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D\nManning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint\narXiv:1809.09600.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\nYiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. 2023. 2x faster language model pre-training via masked\nstructural growth. arXiv preprint arXiv:2305.02869.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really\nfinish your sentence? arXiv preprint arXiv:1905.07830.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint\narXiv:2306.05685.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,\net al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.\n15\nA\nGradient Derivation\nTo calculate the gradient of the RMSNorm weight during backpropagation, we first need to consider the\nforward pass equation for the Llama RMSNorm:\nRMSNorm(x) =\nw \u2299 x\np\nVar(x) + \u03f5\n(8)\nwhere x is the input tensor, w is the weight parameter, Var(x) is the variance of x across the last\ndimension, and \u03f5 is a small constant for numerical stability.\nNow, let\u2019s consider the chain rule for the gradient of the loss function with respect to the RMSNorm\nweight during backpropagation. Denote the loss function as L, and the output of the FFN as y. We have:\n\u2202L\n\u2202w = \u2202L\n\u2202y\n\u2202y\n\u2202w\n(9)\nTo compute the gradient, we need to find the partial derivative \u2202y\n\u2202w. From the FFN equation, we have:\ny = x\u2032 + FFN(RMSNorm(x\u2032))\n(10)\nTaking the derivative with respect to w, we get:\n\u2202y\n\u2202w = \u2202FFN(RMSNorm(x\u2032))\n\u2202w\n(11)\nNow, let\u2019s differentiate the RMSNorm function with respect to w:\n\u2202RMSNorm(x)\n\u2202w\n=\nx\np\nVar(x) + \u03f5\n(12)\nUsing the chain rule, we can compute the gradient of the loss function with respect to the RMSNorm\nweight:\n\u2202L\n\u2202w = \u2202L\n\u2202y\n\u2202FFN(RMSNorm(x\u2032))\n\u2202RMSNorm(x\u2032)\n\u2202RMSNorm(x\u2032)\n\u2202w\n(13)\nGiven that RMSNorm(x\u2032) = t, we need to find the derivative of the FFN with respect to t. Recall the\nFFN equation:\nFFN(t) = SwiGLU(t, W1, W2)W3\n(14)\nNow we want to find the partial derivative of the FFN with respect to t. Recall the SwiGLU activation\nfunction:\nSwiGLU(t, W1, W2) = SiLU(tW1) \u2297 (tW2)\n(15)\nTaking the derivative of the SwiGLU function with respect to t, we get:\n\u2202SwiGLU(t, W1, W2)\n\u2202t\n=\n\u0012\u2202SiLU(tW1)\n\u2202t\n\u0013\n\u2297 (tW2) + SiLU(tW1) \u2297\n\u0012\u2202(tW2)\n\u2202t\n\u0013\n(16)\nNow, recall the SiLU activation function:\nSiLU(x) = x \u2297 \u03c3(x)\n(17)\nThus, the gradient of the FFN with respect to t when t = 0 is also zero:\n\u2202FFN(t)\n\u2202t\n= 0\n(18)\nIn conclusion, when t = 0, the gradient of the FFN with respect to t is zero, which demonstrates that\nthe gradient is zero when the input to the FFN is zero.\n16\nTask Type\nTask Name\n# Instances\nCode Generation\nHumanEval (Chen et al., 2021b)\n45\nMBPP (Austin et al., 2021)\n91\nDecision Making\nALFWorld (Shridhar et al., 2020)\n134\nReasoning\nGSM8K (Cobbe et al., 2021)\n48\nHotpotQA (Yang et al., 2018)\n43\nMATH (Hendrycks et al., 2021)\n100\nMMLU (Hendrycks et al., 2020)\n76\nTheoremQA (Chen et al., 2023b)\n49\nTotal\n586\nTable 8: Dataset statistics of MINT-Bench.\nModel\nCode Generation\nDecision Making\nReasoning\nMicro Avg.\nAgentLM-7B\n1.47\n9.70\n8.86\n7.34\nCodeLLaMA-7B-Instruct\n2.21\n17.16\n7.91\n8.70\nLLaMA2-7B-Chat\n0.00\n0.00\n13.61\n7.34\nMistral-Instruct-v0.1\n6.62\n34.33\n8.54\n13.99\nLLAMA PRO - INSTRUCT\n11.76\n29.10\n9.81\n14.68\nTable 9: The success rates of each model evaluated on different task type benchmarks, as well as the micro average\nwhen k = 5.\nB\nMINT-Bench\nThe MINT-Bench (Wang et al., 2023c) details are provided in this section. MINT-Bench comprises eight\ndatasets spanning code generation, decision-making, and reasoning tasks, totaling 586 instances, as shown\nin Table 8.\nWe use the Success Rate (SR) as our evaluation metric, which measures the percentage of successful\ntask instances. For an interaction limit of k, MINT-Bench starts from scratch and allows each LLM to\ninteract up to the k-th turn, measuring the corresponding SRk. Unless specified otherwise, MINT-Bench\nlimits k \u2208 [1, 5], where k = 1 indicates no interaction, and k = 5 maximizes interaction turns within the\ncontext window (4,096 tokens) of most modern LLMs.\nIn each turn, the LLM is instructed to perform the following steps: (1) Optionally express its reasoning\nprocess (referred to as \"Thought,\" similar to (Yao et al., 2022)); (2) Either interact with tools by generating\nPython code and executing it through a Python interpreter (referred to as \"Execute\"), or propose a solution\nto the user (referred to as \"Propose Solution\").\nTable 9 displays the success rate for each model evaluated on various task type benchmarks, as well as\nthe micro average when k = 5. The LLAMA PRO - INSTRUCT model demonstrates robust performance\nacross all task types compared to other models of similar size. Figure 7 provides a case study to compare\nLLAMA PRO - INSTRUCT and LLaMA2-7B-Chat where LLAMA PRO - INSTRUCT successfully utilizes\nPython program to solve the given question in the multi-round interaction.\nC\nMT-Bench\nMT-bench is a collection of demanding multi-turn open-ended questions designed for evaluating chat\nassistants. In order to automate the evaluation process, we employ powerful LLMs, such as GPT-4, to act\nas judges and assess the quality of the models\u2019 responses. We present the detailed pairwise comparison in\nthe Figure 8 and Figure 9. Figure 10 shows the case study of the comparison between LLAMA PRO -\nINSTRUCT and LLaMA2-7B-Chat.\n17\nQuestions:  Maria invited 4 of her friends over for a water balloon fight in the backyard. At the start of the game, Maria gave each of her friends 2 water balloons. She had one \nwater balloon for herself.  Then her mom came out and gave each person 3 more balloons.  How many total balloons did the girls have? Solution output format: an integer.\nLLaMA Pro- Instruct: \nLLaMA2-7B-Chat: \nObservation:  Out[1]: 9. \nYou have 4 steps left and 2 chances to propose \nsolution left.\nThought:  Let's first determine the initial number of \nballoons Maria and her friends had.\nExecute:  \nMaria_balloons = 1\nfriends_balloons = 4 * 2\ntotal_balloons = Maria_balloons + friends_balloons\nprint(total_balloons)\nObservation:  Out[2]: 24. \nYou have 3 steps left and 2 chances to propose \nsolution left.\nThought:  Let's now add the additional balloons \ngiven by Maria's mom.\nExecute:  \nadditional_balloons = 3 * 5\ntotal_balloons = total_balloons + additional_balloons\nprint(total_balloons)\nInteraction Turn # 2\nPropose solution:  The answer is 24.\nInteraction Turn # 3\nGood job! You have successfully solved the task! \n[stop interaction]\nThought:  Great, let's solve this problem together! To start, we know that Maria had 2 water balloons \ninitially, and her mom gave each person 3 more balloons. So, the total number of balloons Maria's friends \nhad after the mom's gift is: 2 + 3(4) = 2 + 12 = 14Therefore, the total number of balloons the girls had was \n14. Now it's your turn! Please provide the next task or question, and I'll be happy to help you solve it. \nInteraction Turn # 2\nInteraction Turn # 1\nInteraction Turn # 1\nThought:  Ah, my apologies! Thank you for pointing that out. To answer the question, the total number of \nballoons the girls had was 14. So, the solution is <solution> 14 </solution>. Now, please provide the next \ntask or question, and I'll be happy to help you with it!\nObservation:  Your answer is wrong. You have 3 steps left and 1 chances to propose solution left.\nPropose solution:  The answer is 14.\nInteraction Turn # 3\nThought:  Oh, I see! Thank you for letting me know. I apologize for my mistake earlier. To solve the problem, \nwe need to find the total number of balloons the girls had after Maria's mom gave each person 3 more \nballoons. So, the total number of balloons the girls had is:2 + 3(4) = 2 + 12 = 14\nTherefore, the answer is <solution> 14 </solution>.\nPropose solution:  The answer is 14.\nYour answer is wrong! [stop interaction] \nObservation:  I don't understand your input. If you want to execute code, please use \n<execute> YOUR_CODE_HERE </execute>.\nIf you want to give me an answer, please use <solution> YOUR_SOLUTION_HERE </solution>. For example: \nThe answer to the question is <solution> 42 </solution>.\nYou have 4 steps left and 2 chances to propose solution left.\nFigure 7: A case study of multi-turn interactions by using tools and environment feedback to solve math questions\nwith LLAMA PRO - INSTRUCT and LLaMA2-7B-Chat.\nD\nTRACE-Bench\nTable 10 shows the statistics of TRACE. We evaluate the performance of different training strategies after\nsequential tasks. Table 11-13 shows the detailed performance of different training strategies of each round\nduring the continual learning with LLaMA2-7B.\nE\nDomain of Law\nTable 14 shows the hyper-parameters we use to do the ablation study in the domain of law. We use the\nfreelaw subset of Pile dataset as our pretrain corpus (Gao et al., 2020) in the domain of law. This subset\nhas 51.2 GiB raw size and 16.7B tokens with 3.6M documents.\nThe Unfair-ToS dataset, which we use to evaluate the performance of law, contains Terms of Service\n(ToS) from online platforms (e.g., YouTube, Ebay, Facebook, etc.). The dataset has been annotated on the\nsentence-level with 8 types of unfair contractual terms (sentences), meaning terms that potentially violate\nuser rights according to the European consumer law. The UNFAIR-ToS task is a multilabel classification\ntask. To get model predictions for this task, we categorize it as a multiple-choice question as the method\nCheng et al. (2023) uses. The accuracy of an individual data example is considered true if the model\nprediction (i.e., the option with the highest per-token likelihood) belongs to the label(s) set. We evaluate\nthe Unfair-ToS dataset in a 4-shot scenario just like Cheng et al. (2023).\n18\n0%\n20%\n40%\n60%\n80%\n100%\nLLaMA2-7B-Chat\nCodeLlama-7b-Instruct-hf\nWizardCoder-Python-7B-V1.0\n23.3%\n26.7%\n43.3%\n71.7%\n63.3%\n43.3%\n5.0%\n10.0%\n13.3%\nLLaMA Pro-Instruct Wins\nTie\nLLaMA Pro-Instruct Loses\nFigure 8: MT-Bench pairwise comparison between LLAMA PRO - INSTRUCT and widely used LLaMA community\nmodels in math and code questions.\n0%\n20%\n40%\n60%\n80%\n100%\nLLaMA2-7B-Chat\nCodeLlama-7b-Instruct-hf\nWizardCoder-Python-7B-V1.0\n21.9%\n35.0%\n61.9%\n46.9%\n42.5%\n28.8%\n31.2%\n22.5%\n9.3%\nLLaMA Pro-Instruct Wins\nTie\nLLaMA Pro-Instruct Loses\nFigure 9: MT-Bench pairwise comparison between LLAMA PRO - INSTRUCT and widely used LLaMA community\nmodels in comprehensive questions.\nDataset\nSource\nAvg len\nMetric\nLanguage\n#data\nDomain-specific\nScienceQA\nScience\n210\nAccuracy\nEnglish\n5,000\nFOMC\nFinance\n51\nAccuracy\nEnglish\n5,000\nMeetingBank\nMeeting\n2853\nROUGE-L\nEnglish\n5,000\nMulti-lingual\nC-STANCE\nSocial media\n127\nAccuracy\nChinese\n5,000\n20Minuten\nNews\n382\nSARI\nGermany\n5,000\nCode completion\nPy150\nGithub\n422\nEdim similarity\nPython\n5,000\nMathematical reasoning\nNumGLUE-cm\nMath\n32\nAccuracy\nEnglish\n5,000\nNumGLUE-ds\nMath\n21\nAccuracy\nEnglish\n5,000\nTable 10: An overview of dataset statistics in TRACE. \u2019Source\u2019 indicates the context\u2019s origin. \u2019Avg len\u2019 represents\nword count for English, German, and code datasets, and character count for Chinese. \u2019SARI\u2019 is a score specific to\nsimplification.\n19\nQuestions:  Develop a Python program that reads all the text files under a\ndirectory and returns top-5 words with the most number of occurrences.\nFollow-up Questions:  Can you parallelize it?\nGPT-4 Judgment: \ud83c\udf89  LLaMA Pro Wins !! \ud83c\udf89\nLLaMA Pro-Instruct\u2019s\nresponse to both question is correct and helpful\n. The assistant provides a parallelized version \nof the program using the `ProcessPoolExecutor`      \nfrom the `concurrent.futures` module. The assis\ntant also explains how the parallelized version\nworks, which is helpful for the user.\nLLaMA2-7B-chat\u2019s \nresponse to the first  question is correct . But its \nresponse to second question is incorrect. The assistant\nmentions using the Parallel library to parallelize the\nprogram, but the provided code does not include\nany parallelization\nLLaMA Pro-Instruct: \nLLaMA2-7B-Chat: \nLLaMA2-7B-Chat: \nLLaMA Pro-Instruct: \nFigure 10: Multi-turn dialogues between a user and two Al assistants\u2014LLAMA PRO - INSTRUCT and LLaMA2-\n7B-Chat.\n20\nTask\\Round\n1\n2\n3\n4\n5\n6\n7\n8\nC-STANCE\n0.5\n0.27\n0.28\n0.31\n0.01\n0.51\n0.44\n0.27\nFOMC\n-\n0.69\n0.61\n0.57\n0.0\n0.44\n0.51\n0.46\nMeetingBank\n-\n-\n0.472\n0.287\n0.159\n0.111\n0.14\n0.139\nPy150\n-\n-\n-\n0.61\n0.294\n0.551\n0.547\n0.364\nScienceQA\n-\n-\n-\n-\n0.68\n0.6\n0.59\n0.54\nNumGLUE-cm\n-\n-\n-\n-\n-\n0.395\n0.309\n0.272\nNumGLUE-ds\n-\n-\n-\n-\n-\n-\n0.59\n0.51\n20Minuten\n-\n-\n-\n-\n-\n-\n-\n0.414\nOP\n0.371\nBWT\n-0.173\nTable 11: Detailed results of sequentially LoRA training of LLaMA2-7B.\nTask\\Round\n1\n2\n3\n4\n5\n6\n7\n8\nC-STANCE\n0.6\n0.43\n0.49\n0.49\n0.5\n0.49\n0.48\n0.53\nFOMC\n-\n0.8\n0.46\n0.47\n0.0\n0.47\n0.5\n0.51\nMeetingBank\n-\n-\n0.523\n0.338\n0.344\n0.392\n0.396\n0.371\nPy150\n-\n-\n-\n0.594\n0.265\n0.561\n0.566\n0.575\nScienceQA\n-\n-\n-\n-\n0.87\n0.83\n0.58\n0.55\nNumGLUE-cm\n-\n-\n-\n-\n-\n0.383\n0.198\n0.210\nNumGLUE-ds\n-\n-\n-\n-\n-\n-\n0.64\n0.49\n20Minuten\n-\n-\n-\n-\n-\n-\n-\n0.407\nOP\n0.455\nBWT\n-0.147\nTable 12: Detailed results of sequentially fine-tuning of LLaMA2-7B.\nTask\\Round\n1\n2\n3\n4\n5\n6\n7\n8\nC-STANCE\n0.65\n0.52\n0.52\n0.52\n0.28\n0.52\n0.5\n0.57\nFOMC\n-\n0.8\n0.49\n0.45\n0.0\n0.44\n0.46\n0.45\nMeetingBank\n-\n-\n0.512\n0.363\n0.338\n0.383\n0.393\n0.38\nPy150\n-\n-\n-\n0.593\n0.366\n0.531\n0.53\n0.548\nScienceQA\n-\n-\n-\n-\n0.89\n0.8\n0.6\n0.63\nNumGLUE-cm\n-\n-\n-\n-\n-\n0.333\n0.185\n0.198\nNumGLUE-ds\n-\n-\n-\n-\n-\n-\n0.68\n0.54\n20Minuten\n-\n-\n-\n-\n-\n-\n-\n0.408\nOP\n0.465\nBWT\n-0.143\nTable 13: Detailed results of sequentially block expansion training of LLaMA2-7B.\nHyperparameter\nAssignment\nBatch size\n1024\nMaximum sequence length\n2,048\nMaximum learning rate\n2e-4\nOptimizer\nAdam\nAdam beta weights\n0.9, 0.95\nLearning rate scheduler\ncosine\nWarmup ratio\n0.06\nGradient clipping\n1.0\nTable 14: Hyper-parameters of pretraining on the domain of law.\n21\n"
  },
  {
    "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
    "link": "https://arxiv.org/pdf/2401.02412.pdf",
    "upvote": "35",
    "text": "LLM AUGMENTED LLMS:\nEXPANDING CAPABILITIES THROUGH COMPOSITION\nRachit Bansal1 Bidisha Samanta1 Siddharth Dalmia2 Nitish Gupta1 Shikhar Vashishth1\nSriram Ganapathy1 Abhishek Bapna1 Prateek Jain1 Partha Talukdar1\n1Google Research\n2Google DeepMind\nABSTRACT\nFoundational models with billions of parameters which have been trained on large\ncorpora of data have demonstrated non-trivial skills in a variety of domains. How-\never, due to their monolithic structure, it is challenging and expensive to augment\nthem or impart new skills. On the other hand, due to their adaptation abilities,\nseveral new instances of these models are being trained towards new domains and\ntasks. In this work, we study the problem of efficient and practical composition\nof existing foundation models with more specific models to enable newer capa-\nbilities. To this end, we propose CALM\u2014Composition to Augment Language\nModels\u2014which introduces cross-attention between models to compose their rep-\nresentations and enable new capabilities. Salient features of CALM are: (i) Scales\nup LLMs on new tasks by \u2018re-using\u2019 existing LLMs along with a few additional\nparameters and data, (ii) Existing model weights are kept intact, and hence pre-\nserves existing capabilities, and (iii) Applies to diverse domains and settings. We\nillustrate that augmenting PaLM2-S with a smaller model trained on low-resource\nlanguages results in an absolute improvement of up to 13% on tasks like trans-\nlation into English and arithmetic reasoning for low-resource languages. Simi-\nlarly, when PaLM2-S is augmented with a code-specific model, we see a relative\nimprovement of 40% over the base model for code generation and explanation\ntasks\u2014on-par with fully fine-tuned counterparts.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have shown to encompass a range of foundational capabilities\nsuch as commonsense and factual reasoning, world knowledge, and coherent language generation\n(Bubeck et al., 2023; Google et al., 2023). Leveraging these foundational capabilities, a number of\nefforts in the community have fine-tuned these models to enable domain-specific capabilities such as\ncode generation, copy editing, and mathematical problem solving (Lewkowycz et al., 2022; Singhal\net al., 2023). This has resulted in the development of several specialized large models with domain-\nspecific capabilities. For example, there are models that do well on standard code generation but\nare not as proficient in general logical reasoning and vice-versa. Presence of such a large number\nof domain-specific models leads to a natural question: Can we compose an anchor model with\na domain-specific augmenting model to enable new capabilities? For example, can we compose\nan augmenting model\u2019s code understanding capability with an anchor LLM\u2019s language generation\ncapability to enable code-to-text generation capability?\nThe typical approach for this problem is to further pre-train or (efficiently) fine-tune the anchor\nmodel on the data that was originally used to train the augmenting model (Hu et al., 2022; Kessler\net al., 2021). However, many a times such solutions are not feasible since training large models is\ncomputationally expensive, especially since the augmenting model itself may be an LLM trained\non a massive corpora. Further, processing data from multiple sources might not be feasible due\nto privacy concerns and organizational boundaries. Working with multiple distinct models is also\ndesirable since it allows the reuse of existing models with established capabilities, providing better\ncontrol and avoiding catastrophic forgetting that is prevalent in conventional approaches.\nCorrespondence to Rachit and Bidisha: [brachit, bidishasamanta]@google.com\n1\narXiv:2401.02412v1  [cs.LG]  4 Jan 2024\nmB\nmA\nTranslate from XX to En:\n<Source XX Sentence>\nEverything but the kitchen sink\nLow-resource\nLanguage \nPre-trained\nWhat does this Python code do?\n<Python Code Snippet>\nImplements the classic word \ngame of Hangman\nmB\nKey-value \nMapping\nx1 = 10\n     \nxn = 2\nx1 = 10\n     \nxn = 2\nmA\nlA,i\nlA,i\nlB,j\nlB,(j+1)\nWK WV WQ\nAttention\nWhat is the value of x1 + x8 * xn?\nSince x1=10, x8=14, xn=2, x1 + x8 * xn = 38\nmB\nmA\nPre-trained \non GitHub\nNumeric Arithmetic\nFigure 1: Overview of CALM. To augment an anchor LLM (mB) with new capabilities through\ncomposition with a specialized augmenting model (mA). Figure illustrates three mA with differ-\nent capabilities: key-value mapping (left), low-resource languages (center), and code (right). Mod-\nels mA and mB remain unchanged (^) during composition. A few additional parameters are learnt\nover models\u2019 layer representations. Leftmost plot shows an mA trained on a set of string-integer\nmappings, e.g., {x1 : 10, . . . , xn : 2}. mB is a large LM with arithmetic capabilities. CALM com-\nposes these two frozen models to solve the task of arithmetic on keys which either models could not\nsolve on their own (\u00a74.1). Notably, CALM generalizes to the entire key-value set despite training\nwith arithmetic examples spanning only 20% of the keys.\nTo address the training and the data challenges mentioned above, we propose and study a practical\nsetting for model composition: (i) we are given access to one (or more) augmenting model(s) and an\nanchor model, (ii) we are not allowed to modify the weights of either models, and (iii) we only have\naccess to a small amount of data, representing the \u201ccombined skills\u201d of the given models, e.g., code\ngeneration with complex logical reasoning.\nPrior work has largely approached the question of composition from either a routing or a merging\nstandpoint, neither of which provide an effective solution to capture this setting. Routing between the\ngiven models, i.e., choosing an output of one model over the other (Ma et al., 2019), or performing a\nsoft ensemble (Muqeeth et al., 2023) is not effective when neither of the models can demonstrate the\ndesired capability. Another body of work creates a combined model by an arithmetic combination\nof base model parameters (Wortsman et al., 2022; Ilharco et al., 2022; Matena & Raffel, 2022).\nHowever, these settings are naturally restrictive and their efficacy is unclear when combining models\nwith different sizes and pre-training objectives (Yadav et al., 2023).\nIn this work, we propose a novel Composition to Augment Language Models (CALM) framework\nto address the general model composition setting mentioned above. Rather than a shallow combi-\nnation of the augmenting and anchor LMs (Wortsman et al., 2022; Ilharco et al., 2022), CALM\nintroduces a small number of trainable parameters over both augmenting and anchor models\u2019 inter-\nmediate layer representations. CALM finds an effective combination of the given models to perform\nnew challenging tasks more accurately than either of the models alone, while preserving the capa-\nbilities of individual models. Figure 1 highlights few motivating scenarios for CALM.\nWe study key practical applications of CALM: language inclusivity and code generation. For lan-\nguage inclusivity (\u00a74.2), we use a model that has been trained on a set of low-resource languages.\nWe observe that composing this model with the LLM allows us to borrow its generation and reason-\ning capabilities to achieve significantly better performance on translation and arithmetic reasoning\ntasks for low-resource languages (Tables 2 and 3). This composed model outperforms not only the\ntwo base models but also versions of the LLM that have been further pre-trained or LoRA (Hu et al.,\n2022) fine-tuned for the set of low-resource languages. For code generation (\u00a74.3), we use a model\nthat has been trained on open-source code across a variety of programming languages. Compos-\ning this model with the LLM\u2014hence borrowing its low-level logic and generation capabilities\u2014\noutperforms the two base models (Table 4) on code explanation and code completion tasks.\n2\n2\nRELATED WORKS\nParameter efficient fine-tuning:\nA large body of work focuses on efficient ways of fine-tuning\nmodels for new domains by introducing a small number of trainable parameters, keeping the original\nmodel intact (Houlsby et al., 2019; Wang et al., 2021; Pfeiffer et al., 2021; Hu et al., 2022; Kessler\net al., 2021). Since this paradigm allows a small set of new parameters to be trained, it is challenging\nto use this approach to adapt a model to a new domain, which is absent from the original training\ncorpus. In contrast, CALM enables a model to be adapted to completely new domains using an\naugmenting model. In Section 4.4, we demonstrate that CALM is significantly more effective than\nLoRA (Hu et al., 2022), a representative parameter efficient fine-tuning method.\nModel Merging:\nMerging different expert models with simple techniques like task vector aver-\naging provides a way of recombining different capabilities of these models (Ilharco et al., 2022;\nMatena & Raffel, 2022). However, these methods are only relevant when the original models are\nwell aligned. Other related approaches are also applicable only when the models are derived from\nthe same model (Matena & Raffel, 2022) or they are of same size (Muqeeth et al., 2023). In contrast,\nCALM is more generic and is applicable to any set of models.\nModel and Task Compositionality:\nThe modular encoder-decoder based method in (Dalmia\net al., 2022) adapts components of encoder-decoder models to allow flexible re-usability of dif-\nferent encoders, each with their own capabilities. Several past studies explore compositionality\nfrom a multi-modal standpoint. Alayrac et al. (2022) introduce cross-attention parameters across a\nlanguage model in order to attend to representations coming from an image encoder. They show\nvery effective transfer of capabilities between the two models. In this work, we extend the ideology\nof model re-use and modularity to extend composition of capabilities in a large language model.\nModels as Tools:\nAnother interesting direction for using multiple language models to solve a\ndownstream task has been to perform composition in the models\u2019 input text space (Zeng et al.,\n2022; Shen et al., 2023). Schick et al. (2023) have demonstrated how a model can be taught to use\nexternal tools\u2014there might be an opportunity to investigate if other models can be called as a part\nof the same framework. Since these approaches require a large amount of prompt engineering, in\nthis work we focus on composition through representations that can be learnt automatically.\n3\nCOMPOSITION TO AUGMENT LANGUAGE MODELS (CALM)\nGiven an anchor model mB and an augmenting model mA, CALM aims to compose the two models\n(mA\u2295B) to enable new capabilities as a composition of capabilities of the two individual models.\nAs discussed in the introduction, we study this composition in a practical setting with the following\nassumptions: i) we can access weights, run forward and backward pass, and access intermediate\nrepresentations of both mB and mA, ii) we are not allowed to change weights of both the models,\niii) we do not have access to the training data, hyperparameters, training states of both the base\nmodels, iv) we are provided a few examples from the target composition domain.\nThe goal is to learn a composition mA\u2295B = f(mA, mB, \u0398C, DC) to achieve some joint task C. The\nweights of mA and mB are frozen. \u0398C is the additional set of trainable parameters introduced to\nlearn the composition and DC refers to the set of examples that are used to learn this composition.\n3.1\nLEARNING TO COMPOSE (\u0398C)\nAs outlined in Figure 1, we operate over a selected set of layers from mB and mA at all times. We\nlearn two sets of additional parameters over these layers: (i) A simple set of linear transformations,\nfproj(.) that maps an ith layer representation from mA to the dimensionality of representations from\nmB, and (ii) A set of cross-attention layers, fcross(.,.) that cross-attend between this transformed\nlayer representation and a jth layer representation from mB.\nCompositional Layers: Let the augmenting model mA and the anchor model mB have NA and\nNB layers, respectively. Also, let DA and DB be the token dimensionality of the two models. We\nfirst choose a set of compositional layers\u2014LA and LB\u2014for both models, over which the set of new\n3\nlearnable parameters are introduced during composition. nA = |LA| and nB = |LB|. For simplicity,\nwe set nA = nB = n and the gap between two contiguous selected layers is kept uniform based\non the number of selected layers\u2014that is, (l2 \u2212 l1) = \u00b7 \u00b7 \u00b7 = (ln \u2212 l(n\u22121)) = N/n. Further, HA\n\u2208 {HA1, HA2, . . . , HAnA} denote the layer representation of a given input after each layer in LA.\nLearned Projections: Next we map representations from mA to that of mB via a projection layer.\nIn particular, for each layer in LA, we learn a projection function fproj : RDA \u2192 RDB, that projects\nrepresentations from these layers to the desired representation size of mB. Let,\nfproj(HA) \u2190\u2212 {fproj(HA1), fproj(HA2), . . . , fproj(HAnA)}\nThis transformation enables cross-attention across models, and also performs an alignment of rep-\nresentations from mA and mB despite frozen weights of the base models.\nCross-attention Layers: Similar to the multi-headed cross-attention in encoder-decoder models\n(for example Vaswani et al. (2017) and Raffel et al. (2020))\u2014we introduce cross-attention between\nrepresentations of the anchor and the augmenting model. In particular, we use fproj(HAi) from the\naugmenting model as the key and value vectors for each head in cross-attention. We use the vector\nHBj from the anchor model as the query vector, which leads to the following cross-attention setup:\nfcross(fproj(HAi), HBj) = Concat.k (headk) WO\n\u2200k \u2208 NH\nwhere, headk = Attn.(QB, KA, VA),\nand, QB = HBjWQ\nk ,\nKA, VA = fproj(HAi)WK\nk , fproj(HAi)WV\nk\nHere, NH represents the number of attention heads used for cross-attention which, in our case, is\ntypically the same as the number of heads used for self-attention in mB. Each of WO \u2208 RDB\u00d7DB,\nWQ\nk , WK\nk , and WV\nk \u2208 RDB\u00d7DB//NH are learnable weight matrices, where k \u2208 {1..NH}.\nFinally, the cross-attention output is added as a residual connection to the layer representations of\nmB. The resultant output vector, in-turn, is the input to the succeeding layer in mB:\nHA\u2295Bj = HBj + fcross(fproj(HAi), HBj)\nHere, HA\u2295Bj denotes the input to the (j + 1)th layer of the composed model. All layers in LA and\nLB are utilized in a similar manner. Propagating over the remaining layers in mB gives us a final\noutput token yt decoded for the tth timestep. Akin to usual auto-regressive decoding, the output\ntoken for each time-step is appended to the input: xt+1 = xt \u2295 yt, Since the updated input at each\ntime step is passed to both models, all representations for the two models are refreshed.\n3.2\nCOMPOSITION TRAINING DATA (DC)\nSince the target model mA\u2295B involves a composition over the two models mA and mB, we construct\nthe set of training examples DC to depict a \u201ccombined skill\u201d that enables \u0398C to attend over the two\nmodels appropriately for the target task.\nIdeally, if the set of tasks involved in composition task are distinguished as t1 and t2 respectively,\nthen we design DC to depict the a joint task C. For example, with respect to our synthetic key-value\nsetup: our final task (C) is to perform arithmetic over a set of keys. The augmenting model mA is\ntrained to learn the given key-value pairs (notated as task, t1) and the anchor model mB is generic\nmodel that can perform numeric arithmetic well (task t2). For learning the set of parameters \u0398C for\ncomposition, we consider DC to be arithmetic over a held-in set of keys (task C), encompassing\ncombined skills from the two models. In contrast to fine-tuning approaches like LoRA (Hu et al.,\n2022) that would require the entire knowledge source (here, key-values) during training time, we\nfind that training composition on only a fraction of the keys can generalize to the full set.\nIn other real world settings, a clear distinction in specializing tasks for each model might be difficult\nto formulate and hence defining a task that captures the combined skills can be challenging. We find\nthat using a set of examples that capture certain capabilities of the two models suffices, i.e., some\nrough notion of tA\u222aB. For our language inclusivity task, we use a mixture of examples containing\na small amount of low-resource language and high-resource language data.\n4\nComposing multiple models:\nFinally, we note that while the method has been presented for a\nsetting with one anchor model and only one augmenting model, CALM is applicable to multiple\naugmenting models as well. In particular, CALM would require learning similar projection and\ncross-attention components between the anchor and each of the augmenting model. We leave a\nthorough investigation of this as a topic of future work.\n4\nEXPERIMENTS\nWe demonstrate the following in three domains: (a) an anchor LLM (mB) can be composed with an\naugmenting model (mA) trained on mappings between string keys and number values to solve arith-\nmetic expressions over those keys requiring both, knowledge of the KV mappings and arithmetic\ncapabilities (\u00a74.1); (b) how CALM can be used to expand the language coverage of an anchor LLM\n(mB) to low-resource languages it has not seen during pre-training. We show that an augmenting\nmodel (mA) pre-trained on low-resource languages can be composed with such an anchor model to\nsignificantly improve translation and math-word problem solving capabilities in low-resource lan-\nguages (\u00a74.2); (c) how code completion and explanation can be improved by composing an anchor\nLLM with an augmenting model (mA) specializing in the code domain (\u00a74.3).\nIn all experiments, we start with a PaLM2-XXS model and further train it on domain-specific data to\narrive at an augmenting model (mA) that is then kept frozen during composition. Note that no task\nspecific training data was used to train CALM. We use PaLM2-XS or PaLM2-S models as the anchor\nLLM (mB) that is also kept frozen during composition training. For all our experiments, we set\nNA/n = 4, i.e., we perform composition using every 4th layer output from mA. Correspondingly,\nlayers from mA (LB) are chosen such that nB = nA = n, hence nB = NA/4.\n4.1\nKEY-VALUE ARITHMETIC\nWe first study the setting where we have a small augmenting LM that has been trained to memorize\nstring-to-integer key-value (KV) mappings, and a large anchor LM that is capable of performing\narithmetic over integers. We wish to use CALM to compose them and enable a new capability of\nsolving arithmetic expressions containing those keys.\nKey-Value Domain Knowledge\nWe first generate a repository of KV pairs containing NKV = 25K\npairs by sampling English strings of length 2\u22126 characters from the vocabulary of the PaLM2-XXS\nmodel and randomly assigning them unique integer values in the range [1, NKV]. This constitutes\nthe knowledge artifact, DKV. We further generate a collection of arithmetic expressions (DKV-EXP)\ncontaining addition (+), subtraction (\u2212), and multiplication (\u00d7) operations between 3 \u2212 6 keys by\nrandomly sampling keys from DKV and operations to perform between them.\nUsing these arithmetic expressions, we generate three datasets:\n(i) KV-Substitution (DKV-SUBS): This dataset maps each expression in DKV-EXP, to an expression\nwhere the keys are replaced by their corresponding values. For example, this dataset contains exam-\nples of the form (<K1> + <K2> \u2212 <K3>, 10 + 22 \u2212 24).\n(ii) KV-Arithmetic (DKV-MATH): This dataset maps each expression in DKV-EXP to the numeric value\narrived at by solving the arithmetic expression when the keys would be replaced by the correspond-\ning values. For example, examples in this dataset look like (<K1> + <K2> \u2212 <K3>, 8).\n(iii) Numeric-Arithmetic (DNUM-MATH): This dataset maps the value substituted version of each\nexpression in DKV-EXP to the numeric value arrived at by solving the arithmetic expression. For\nexample, examples in this dataset look like (10 + 22 \u2212 24, 8).\nModels\nWe obtain augmenting model mA by further training a pre-trained PaLM2-XXS model on\nDKV-SUBS to make it memorize the KV pairs in DKV. Note that, training on DKV-SUBS does not teach\nthis augmenting model how to solve arithmetic expressions. Next, we use a pre-trained PaLM2-XS\nmodel as the anchor model mB. This model is capable of solving numeric expressions with decent\nperformance (see Table 1). Note that, this model has no knowledge of the KV pairs in DKV.\nWe now take examples from the KV-Substitution dataset DKV-SUBS that only span 20% of the keys in\nDKV to form the training data for composition (DC). We use DC to compose the augmenting model\n5\n(mA) having knowledge of DKV and the pre-trained anchor model mB by training the composition\nparameters (\u0398C) using CALM as explained in \u00a73. Both mA and mB are kept unchanged.\nEvaluation Task\nWe evaluate the composed model mA\u2295B for its ability to solve arithmetic ex-\npressions containing keys from DKV. Specifically, we evaluate on the subset of DKV-MATH dataset\nthat does not contain expressions used in DC during training. This way, we are able to measure the\ncomposed model\u2019s ability to generalize to keys beyond what was observed during training.\nmA\nmB\nCALM\n(mA\u2295B)\nDKV-SUBS\n98.1\n0.0\n92.9\nDNUM-MATH\n4.2\n73.7\n72.0\nDKV-MATH\n0.7\n0.0\n84.3\nTable 1: Evaluation (accuracy (%)) for\na synthetic key-value (KV) task.\nmA\nis trained to memorize the KV mappings\nwhile mB excels at arithmetic We see that\na composition mA\u2295B is able to perform\narithmetic over held-out keys.\nResults\nTable 1 shows the performance of the three\nmodels: mA, mB, and mA\u2295B across the aforemen-\ntioned datasets. First, we observe that the augmenting\nmodel mA achieves 98.1% at the KV-Substitution task\nshowing that memorizes DKV well. Next, we see that\nit performs poorly (4.2%) at the Numeric-Arithmetic\ntask showing that it does not have arithmetic capabili-\nties. As a result, this model is not able to solve arith-\nmetic expressions containing keys from DKV.\nAs expected, the anchor model mB gets 0% accuracy\non the KV-Substitution and KV-Arithmetic tasks as it\nhas not seen any data from DKV. However, it performs\nwell (73.7%) on the Numeric-Arithmetic task demon-\nstrating capability of arithmetic over numerals.\nLastly, we see that the composed model mA\u2295B is able\nto solve all tasks with high accuracy, especially the KV-Arithmetic task (84.3%) which both the\nunderlying models fail at. This shows that the composed model is able to leverage the relevant\ncapabilities from both the augmenting and anchor model to solve a complex task.\n4.2\nLOW-RESOURCE LANGUAGE INCLUSIVITY\nFLORES-200 (XX to En; chrF1)\nModel\nlij\nmr\ntaq\nnn\nsu\nban\npl\nth\nmin\nacm\navg.\nPaLM2-XXS\n24.0\n16.5\n21.6\n33.3\n20.6\n2.1\n5.3\n63.2\n44.0\n59.8\n29.0\n+ NTL (mA)\n32.0\n21.6\n46.9\n50.0\n40.6\n4.1\n4.0\n63.8\n47.8\n61.1\n37.2\nPaLM2-S (mB)\n32.6\n24.2\n44.6\n50.8\n50.9\n5.4\n9.5\n69.0\n61.0\n68.6\n41.7\nCALM (mA\u2295B)\n44.1\n30.4\n55.1\n54.6\n54.4\n11.8\n11.3\n69.4\n61.1\n68.9\n46.1\nmB+NTL (mNTL\nB\n)\n48.1\n39.1\n59.2\n57.5\n57.3\n11.4\n9.9\n69.4\n61.4\n69.0\n48.2\nTable 2: Translation performance for XX to English direction on the FLORES-200 dataset (Costa-\njuss`a et al., 2022): We show results for a subset of 10 low-resource languages. Note that the com-\nposed model mA\u2295B significantly outperforms both mA and mB. On the complete language list,\nmA\u2295B outperforms both the underlying models for 175 of 192 languages (Appendix A; Figure 2).\nmNTL\nB\nrepresents a skyline where mB has been further pre-trained on DNTL. The composed model\nachieves similar performance for a tiny fraction of the training cost.\nIn this section, we study if we can compose such a large anchor LM mB with a smaller augmenting\nLM mA that has been pre-trained on low-resource languages, to perform translation and math-word\nproblem solving tasks presented in these low-resource languages.\nLow-resource Language Corpora\nWe use the long-tail language set and the associated corpora\nfrom the Next Thousand Languages (NTL) effort (Caswell et al., 2020; Bapna et al., 2022) as the\ndomain data DNTL. This large-scale corpora contains web-crawled monolingual sentences and trans-\nlation pairs for \u223c1000 languages. The dataset has been used for language expansion in translation\nsystems and language models (Garcia et al., 2021; Siddhant et al., 2022).\n6\nGSM8K (Low-resource Languages; Accuracy)\nModel\nmeo\nmfa\npcm\nefi\nmin\nilo\nady\nmai\nnso\nmzn\navg.\nPaLM2-XXS\n5.2\n6.8\n6.8\n4.0\n5.6\n7.2\n6.0\n3.6\n7.2\n6.8\n5.9\n+ NTL (mA)\n7.6\n4.0\n4.4\n3.2\n6.0\n4.8\n6.4\n3.2\n6.0\n4.8\n5.0\nPaLM2-S (mB)\n28.8\n14.0\n34.4\n14.8\n25.2\n14.8\n30.0\n22.8\n8.4\n31.6\n22.5\nCALM (mA\u2295B)\n34.0\n17.6\n33.6\n18.0\n23.6\n16.8\n36.4\n24.8\n8.4\n36.4\n25.0\nmNTL\nB\n33.2\n20.4\n31.6\n14.0\n24.8\n14.0\n29.2\n21.2\n9.6\n27.6\n22.6\n(High-resource Languages)\nModel\nen\nte\nbn\nsw\nja\nzh\nth\nfr\nes\nde\navg.\nPaLM2-XXS\n5.6\n4.0\n2.0\n7.6\n2.0\n4.4\n6.0\n6.8\n5.6\n9.2\n5.3\n+ NTL (mA)\n4.8\n3.6\n3.2\n4.8\n3.2\n7.6\n6.4\n9.2\n5.6\n7.2\n5.6\nPaLM2-S (mB)\n36.8\n19.2\n23.2\n16.0\n2.0\n39.2\n29.6\n38.0\n32.4\n43.2\n28.0\nCALM (mA\u2295B)\n37.2\n28.0\n27.2\n18.0\n2.4\n43.6\n33.2\n42.8\n36.0\n49.2\n31.8\nmNTL\nB\n36.0\n17.6\n18.4\n14.4\n0.8\n33.6\n27.2\n34.8\n31.2\n42.0\n25.6\nTable 3: Evaluations for grade-school mathematics (GSM) problems on low-resource (LRL) and\nhigh-resource (HRL) languages. We observe that CALM yields significant gains for both evaluation\nsets. Gains on the HRL set suggests that CALM avoids catastrophic forgetting.\nModels\nAkin to \u00a74.1, we obtain augmenting model mA by training the PaLM2-XXS model on\nDNTL to impart knowledge about these low-resource languages to the model. For mB, we use the\npre-trained PaLM2-S model. We use \u223c 5% of the same low-resource language corpora DNTL as\nthe training data DC to compose mA and mB via CALM. Since both models are untrained during\ncomposition, the anchor model mB is not trained on any of the low-resource language data.\nEvaluation Tasks\nWe evaluate the composed model mA\u2295B on two tasks:\n(i) Translating text from a non-English language to English: We carry out these evaluations in a\n5-shot in-context learning paradigm on the FLORES-200 (Costa-juss`a et al., 2022) dataset. This\ndataset contains examples for 200 high- and low-resource languages.\n(ii) Performing grade school math word problems expressed in a non-English language: We evaluate\non the multilingual version of the GSM-8K dataset (Shi et al., 2023) containing math word problems\nfor English and 9 other high-resource languages. We further generated a silver-standard GSM-8K\ndataset for low-resource languages by automatically translating the English examples in GSM-8K\nto 25 low-resource languages supported by Google Translate.1\nResults\nTable 2 shows results on the FLORES-200 dataset (Costa-juss`a et al., 2022), where the\ninput is a low-resource (XX) language sentence and the output should be the corresponding English\ntranslation. For 10 low-resource languages shown in the Table, we see that both the underlying\nmodels mA and mB are outperformed by our composed model mA\u2295B. We find that the composed\nmodel mA\u2295B outperforms mB on 175 of the complete set of 192 languages (Appendix A).\nTable 3 shows the performance of these models on the grade-school math word problems from the\nGSM8K task (Cobbe et al., 2021) on low-resource languages (top) and high-resource languages (Shi\net al. (2023); bottom). Firstly, we observe that the augmenting model mA does not perform well on\nthis task due to its limited mathematical reasoning capabilities. On the other hand, the anchor model\nmB does much better given its mathematical reasoning capabilities and transfer-learning from high-\nresource languages. Finally, we observe that mA\u2295B outperforms both mA and mB on 18 of 25\nlow-resource and 9 of 10 high-resource languages, demonstrating effective composition of models.\nSee Table 6 (Appendix A.2) for a complete set of evaluations. Note that the last row in Table 3 shows\nthat mB when fine-tuned on DNTL leads to worse performance than the pre-trained mB indicating\nforgetting. Composing domain-specific model mA with mB using CALM avoids this.\n1We perform quality evaluations in Appendix 7.\n7\nModel\nCC (P@1)\nT2C (P@1)\nC2T (chrF1)\nHumanEval\nMBPP\nPython\nPHP\nGo\nJava\nJS\nRuby\nPaLM2-XXS\n+ Code (mA)\n19.5\n28.0\n28.0\n34.7\n32.6\n29.6\n26.5\n26.0\nPaLM2-S (mB)\n16.4\n28.6\n30.4\n35.5\n40.4\n31.0\n28.8\n27.9\nCALM (mA\u2295B)\n22.5\n32.2\n30.5\n35.8\n40.6\n31.4\n29.3\n29.0\nmCode\nB\n24.3\n43.0\n18.9\n35.0\n41.1\n31.1\n20.2\n27.6\nTable 4: Evaluations for code generation and understanding across three tasks: Code Completion\n(CC), Text-to-Code (T2C), and Code-to-Text (C2T). Augmenting code understanding to mB using\nmA significantly improves performances across all datasets. mCode\nB\nrepresents a skyline where mB\nfurther pretrained on the DCode, which shows catastrophic forgetting of text generation task.\n4.3\nCODE UNDERSTANDING AND GENERATION\nCode understanding and generation require two distinct types of capabilities: (a) knowledge of the\nsyntax and semantics of code, and (b) knowledge of the world that the code is manipulating. While\nLLMs have a wealth of world knowledge, they could often lack the specific knowledge of code\nsyntax due to a skewed representation of code data in their pretraining corpora. Conversely, small\nmodels trained specifically on code data could exhibit a good understanding of code syntax, but they\nmay lack broad world knowledge and reasoning. CALM can enable best of both worlds.\nCode Domain Data\nHere, we use the code-specific corpus, DCode, consisting of open-source code\nextracted from GitHub heads for a variety of programming languages to train mA.\nModels\nSimilar to \u00a74.1, a version of the PaLM2-XXS model has been further pre-trained on DCode\nis used as mA, while the base pre-trained PaLM2-S model acts as mB. We build mA\u2295B by training\nCALM with only 7% of the same code data (data used for mA) to have a data parity.\nEvaluation Tasks\nWe evaluate the efficacy of CALM on three different tasks:\n(i) Code-Completion (CC): Given an initial set of lines of a code, the model is prompted to complete\nthe code snippet. Here the aim is to evaluate the model for code syntax. We perform zero-shot eval-\nuations on HumanEval benchmark dataset (Chen et al., 2021) and report the Pass@1 (P@1) metric.\n(ii) Text-to-Code (T2C): Given a textual context, the model is prompted to generate the correspond-\ning code snippet. Here, the evaluation indicates language understanding and code generation capa-\nbilities. We perform 3-shot inference on the MBPP dataset (Austin et al., 2021) and report P@1.\n(iii) Code-to-Text (C2T): Given a code snippet, the goal is to generate a natural language explanation\nof the code. This task evaluates code understanding and text generation. We perform 3-shot evalua-\ntions on the CodeXGlue benchmark (Lu et al., 2021) and report chrF1 scores across languages.\nResults\nTable 4 reports comparative performance for the individual models mA and mB, the com-\nposed version mA\u2295B, and a fine-tuned anchor baseline mCode\nB\n. Firstly, evaluations on the HumanEval\ndataset suggest that mA has a superior understanding of code syntax as a result of its additional train-\ning on DCode. While, due to the larger scale and general purpose pre-training of mB, it excels at\ngeneral language understanding and hence performs better on the T2C and C2T tasks.\nWhen employing CALM to compose the two models, we observe a clear transfer and composition\nof capabilities through significant performance improvements: 6.1% and 3.6% absolute gains over\nmB on the CC and T2C tasks, respectively. We observe that fine-tuning mB on DCode leads to a\nsignificant decline in the C2T performance due to catastrophic forgetting. CALM retains the perfor-\nmance and is marginally better than mB across all languages. We also study qualitative examples\non the C2T task and observe interesting common patterns that are discussed in Appendix B.\n8\nmNTL/Code\nB\nCALM\nmA\u2295B\nVanilla\nmA\nRandom\nmA\nmAas an\nencoder\nLoRA\nchrF1\n62.1\n60.5\n59.2\n58.8\n59.3\n59.2\nFLORES-200\n(XX-En)\n#(>mB)\n171\n175\n115\n43\n102\n82\nAccuracy\n19.8\n21.4\n19.0\n17.8\n19.1\n20.9\nGSM-8K\n(LRL)\n#(>mB)\n15\n20\n15\n9\n12\n15\nAccuracy\n27.1\n33.1\n29.7\n28.5\n29.1\n31.2\nGSM-8K\n(HRL)\n#(>mB)\n1\n11\n8\n4\n6\n9\nHumanEval\nPass@1\n24.3\n22.5\n20.0\n20.1\n16.0\n18.3\nMBPP\nPass@1\n43.0\n32.2\n28.0\n27.0\n27.0\n28.7\nCodeXGLUE\nchrF1\n29.0\n32.6\n32.2\n32.1\n32.0\n32.6\nTable 5: Comparative performance of CALM (mA\u2295B) across various possible ablations. The met-\nric \u201c#(>mB)\u201d depicts the number of languages for which the corresponding model is better than\nthe base for NTL, mB\u2014out of 192, 25, and 11 languages for the three tasks respectively. For all\ncompared settings, the number of added parameters are kept the same.\n4.4\nABLATIONS\nInfluence of mA\nWe first study the influence of mA by replacing it with vanilla and random\nvariants during composition. Table 5 shows the variation of performance across NTL and Code tasks\nwhen the specialized mA is replaced with a vanilla PaLM2-XXS checkpoint or an untrained version\nof the model, i.e., a random model. We see that there is a considerable drop of performance with\nthese variants across all tasks. On FLORES-200 XX-En task, languages improved with composition\ndrop to 115 and 43 with vanilla and random, respectively. A slight improvement of the vanilla model\nover mB indicates that an un-specialized model (with a different training regime than mB) might\nhave orthogonal capabilities leading to an enhanced model. This finding validates that performance\ngains seen with CALM is a result of utilizing mA and not the added \u0398C parameters.\nInfluence of iterative decoding\nWe also investigate a variation where we use mA as an encoder,\ni.e., an output token decoded at a given timestep is not amended to mA\u2019s input. In this case, only the\nprefix representations of mA are used. This setting eludes to past work for image and text models\n(Alayrac et al., 2022) where encoder and decoder models are composed. We observe a significant\ndecline in performance across our various tasks when employing this setting.\nComparision with LoRA\nFinally, we evaluate a parameter efficient fine-tuning approach by train-\ning LoRA (Hu et al., 2022) layers to adapt mB. For all experiments, we set the LoRA rank such\nthat the number of added parameters is equal to the number of parameters introduced with CALM.\nWe also train LoRA on the same data as CALM, i.e., DC. We see a considerable difference in\nperformance between the two approaches across all tasks and metrics.\n5\nCONCLUSION\nThe proposed CALM framework composes an anchor LLM with specialized augmenting models to\nenable new tasks not achievable by either models individually. CALM does not require updating the\nindividual models and learns a dense interaction between the models through a few trainable cross-\nattention parameters. Our experiments present consistent evidence that CALM learns to utilize the\nexpertise from the two models. That is, when composed with relevant augmenting models, we\nobserve a significant uptick in the anchor model\u2019s performance across multiple challenging tasks,\nsuch as low-resource translation, reasoning, and code explanation/generation.\nThat is, CALM is especially useful in scenarios where proprietary data and knowledge is stored in\nparametric models. With CALM, a foundational LLM could be augmented with such proprietary\nmodels to extend a variety of foundational capabilities such as reasoning, world knowledge, and\ncoherent generation over the target proprietary domains. Finally, extensions of CALM could be\nused to acquire distinct knowledge from multiple augmenting models.\n9\nACKNOWLEDGMENTS\nThis work was done during RB\u2019s pre-doctoral tenure at Google Research, India (GRI) with PT and\nPJ. RB is indebted to Manish Gupta, Divvy Thakkar, and all others who enabled this oppurtunity.\nRB would also like to thank the members of the Languages team and other researchers at GRI\n(and beyond), including the incredible pre-doctoral cohort. This work wouldn\u2019t have been possible\nwithout their constant support. Namely: Aishwarya P.S., Laurent El Shafey, and Qiao Zhang for\ntheir massive help in coding and debugging; Palak Jain and Sagar Gubbi for their feedback and\nsupport throughout the project; Kartikeya Badola, Shreyas Havaldar, Amandeep Kaur, and Rishabh\nTiwari for being the first ears to all ideas; Cyrus Rashtchian and Richa Dixit for their mentorship.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a Visual Language\nModel for Few-Shot Learning, 2022. URL https://arxiv.org/abs/2204.14198.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.\n07732.\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Meng-\nmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod,\nJason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexan-\nder Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes.\nBuilding machine translation systems for the next thousand languages, 2022.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco T\u00b4ulio Ribeiro, and Yi Zhang.\nSparks of artificial general intelligence: Early experi-\nments with GPT-4. ArXiv preprint, abs/2303.12712, 2023. URL https://arxiv.org/abs/\n2303.12712.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Un-\nexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the\n28th International Conference on Computational Linguistics, pp. 6588\u20136608, Barcelona, Spain\n(Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.\ncoling-main.579. URL https://aclanthology.org/2020.coling-main.579.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.\nArXiv preprint, abs/2107.03374, 2021.\nURL https://\narxiv.org/abs/2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168,\n2021. URL https://arxiv.org/abs/2110.14168.\nMarta R. Costa-juss`a, James Cross, Onur C\u00b8 elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guil-\nlaume Wenzek, Al Youngblood, Bapi Akula, Lo\u00a8\u0131c Barrault, Gabriel Mejia Gonzalez, Prangthip\nHansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,\nChau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan,\nCynthia Gao, Vedanuj Goswami, Francisco Guzm\u00b4an, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left be-\nhind: Scaling human-centered machine translation. ArXiv preprint, abs/2207.04672, 2022. URL\nhttps://arxiv.org/abs/2207.04672.\n10\nSiddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze,\nLuke Zettlemoyer, and Abdelrahman Mohamed. LegoNN: Building Modular Encoder-Decoder\nModels, 2022. URL https://arxiv.org/abs/2206.03318.\nXavier Garcia, Aditya Siddhant, Orhan Firat, and Ankur Parikh. Harnessing multilinguality in un-\nsupervised machine translation for rare languages. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 1126\u20131137, Online, 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.naacl-main.89. URL https://aclanthology.org/2021.\nnaacl-main.89.\nGoogle, Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H.\nClark, Laurent El Shafey, Yanping Huang, Katlorahy Meier-Hellstern, Gaurav Mishra, Erica\nMoreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong\nXu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan\nBotha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00b4ement Crepy, Shachi Dave,\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00b4\u0131az, Nan Du, Ethan Dyer, Vlad Feinberg,\nFangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas\nGonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,\nKathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee,\nEric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam\nMoussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pel-\nlat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose\nSlone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasude-\nvan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai\nWu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,\nCe Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report,\n2023.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\nNLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Interna-\ntional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,\nUSA, volume 97 of Proceedings of Machine Learning Research, pp. 2790\u20132799. PMLR, 2019.\nURL http://proceedings.mlr.press/v97/houlsby19a.html.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth Inter-\nnational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,\nHannaneh Hajishirzi, and Ali Farhadi.\nEditing models with task arithmetic.\nArXiv preprint,\nabs/2212.04089, 2022. URL https://arxiv.org/abs/2212.04089.\nSamuel Kessler, Bethan Thomas, and Salah Karout. An Adapter Based Pre-Training for Efficient\nand Scalable Self-Supervised Speech Representation Learning, 2021. URL https://arxiv.\norg/abs/2107.13530.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay\nV.\nRamasesh,\nAmbrose\nSlone,\nCem\nAnil,\nImanol\nSchlag,\nTheo\nGutman-\nSolo,\nYuhuai\nWu,\nBehnam\nNeyshabur,\nGuy\nGur-Ari,\nand\nVedant\nMisra.\nSolv-\ning\nquantitative\nreasoning\nproblems\nwith\nlanguage\nmodels.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html.\n11\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu\nFu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding\nand generation. ArXiv preprint, abs/2102.04664, 2021. URL https://arxiv.org/abs/\n2102.04664.\nJiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H. Chi.\nSNR: sub-network\nrouting for flexible parameter sharing in multi-task learning. In The Thirty-Third AAAI Con-\nference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Ar-\ntificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February\n1, 2019, pp. 216\u2013223. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.3301216. URL https:\n//doi.org/10.1609/aaai.v33i01.3301216.\nMichael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances\nin Neural Information Processing Systems, 35:17703\u201317716, 2022.\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing.\nArXiv preprint, abs/2306.03745, 2023. URL https://arxiv.org/abs/2306.03745.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association for Computational Linguistics: Main Volume,\npp. 487\u2013503, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\neacl-main.39. URL https://aclanthology.org/2021.eacl-main.39.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\ntext transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020. URL http://jmlr.org/\npapers/v21/20-074.html.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. ArXiv preprint, abs/2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\ngingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace, 2023. URL https:\n//arxiv.org/abs/2303.17580.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Lan-\nguage models are multilingual chain-of-thought reasoners. In The Eleventh International Confer-\nence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp.\nAditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao, Mia Xu Chen, Isaac Caswell, and Xavier\nGarcia. Towards the next 1000 languages in multilingual machine translation: Exploring the\nsynergy between supervised and self-supervised learning. ArXiv preprint, abs/2201.03110, 2022.\nURL https://arxiv.org/abs/2201.03110.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin,\nSami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska,\nBlaise Ag\u00a8uera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\nMahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi,\nAlan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering\nwith large language models. ArXiv preprint, abs/2305.09617, 2023. URL https://arxiv.\norg/abs/2305.09617.\n12\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao,\nDaxin Jiang, and Ming Zhou. K-Adapter: Infusing Knowledge into Pre-Trained Models with\nAdapters. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp.\n1405\u20131418, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nfindings-acl.121. URL https://aclanthology.org/2021.findings-acl.121.\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo\nLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith,\nand Ludwig Schmidt.\nModel soups: averaging weights of multiple fine-tuned models im-\nproves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka,\nLe Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pp. 23965\u201323998. PMLR, 2022. URL https:\n//proceedings.mlr.press/v162/wortsman22a.html.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.\nResolving in-\nterference when merging models.\nArXiv preprint, abs/2306.01708, 2023.\nURL https:\n//arxiv.org/abs/2306.01708.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,\nFederico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Van-\nhoucke, and Pete Florence. Socratic Models: Composing Zero-Shot Multimodal Reasoning with\nLanguage, 2022. URL https://arxiv.org/abs/2204.00598.\n13\nA\nSUPPLEMENTARY MATERIAL FOR NTL\nA.1\nFLORES-200\nFigure 2 depicts the gains over the anchor PaLM2-S model when augmented with a model that has\nbeen trained on DNTL. We see a positive gain through CALM for 175 of 192 languages. The highest\ngains are seen for low-resource languages since they are the most underrepresented in the original\nmodel. Diminishing returns with higher resource languages is seen and this trend is similar to the\ntrend seen for mNTL\nB\n.\nLow to High Resource Languages (#languages = 192)\nGain over Anchor Model\nFigure 2: Gains seen by the composed model mA\u2295B over the anchor model, mB, for the complete\nset of FLORES-200 languages. The languages are sorted from low to high-resource.\nmA\nmB\nmA\u2295B\n(CALM)\nmNTL\nB\nmeo\n7.6\n28.8\n34.0\n33.2\nmfa\n4.0\n14.0\n17.6\n20.4\npcm\n4.4\n34.4\n33.6\n31.6\nefi\n3.2\n14.8\n18.0\n14.0\nmin\n6.0\n25.2\n23.6\n24.8\nilo\n4.8\n14.8\n16.8\n14.0\nady\n6.4\n30.0\n36.4\n29.2\nmai\n3.2\n22.8\n24.8\n21.2\nnso\n6.0\n8.4\n8.4\n9.6\nmzn\n4.8\n31.6\n36.4\n27.6\nbew\n4.4\n33.6\n34.8\n33.6\nts\n4.8\n7.2\n10.0\n11.6\ndv\n2.8\n11.2\n14.8\n13.2\nmA\nmB\nmA\u2295B\n(CALM)\nmNTL\nB\nbho\n4.0\n23.6\n29.2\n22.8\ncv\n6.0\n17.6\n16.4\n20.4\nmni\n3.6\n2.8\n4.4\n6.0\nor\n2.4\n9.6\n12.4\n12.0\nkri\n5.6\n12.4\n18.8\n20.0\ntk\n5.2\n27.2\n29.2\n28.8\ngom\n4.8\n22.4\n25.2\n22.8\nug\n6.0\n23.2\n29.2\n26.4\nckb\n3.2\n25.6\n28.0\n27.2\nas\n1.2\n5.2\n9.2\n4.0\ndoi\n3.6\n17.2\n22.4\n21.6\ndz\n4.4\n0.8\n0.4\n0.0\navg.\n4.5\n18.6\n21.4\n19.8\nTable 6: Performance evaluations on the complete set of low-resource languages for GSM-8K.\nAugmenting mA with mB as mA\u2295B improves performance over mB across a majority of languages.\nOn average, we see an improvement of 2.8%.\n14\nmeo\nmfa\npcm\nefi\nmin\nilo\nady\nOverlap\n83.17\n75.54\n81.28\n78.35\n77.90\n77.80\n76.21\nDelta\n1.15\n1.25\n1.18\n1.22\n1.23\n1.24\n1.28\nmai\nnso\nmzn\nbew\nts\ndv\nbho\nOverlap\n76.63\n69.58\n71.32\n71.37\n61.62\n55.18\n73.67\nDelta\n1.26\n1.40\n1.38\n1.37\n1.55\n1.70\n1.30\ncv\nmni\nor\nkri\ntk\ngom\nug\nOverlap\n58.52\n58.94\n68.03\n77.18\n66.06\n71.21\n57.66\nDelta\n1.62\n1.60\n1.45\n1.27\n1.48\n1.36\n1.65\nTable 7: Quality evaluation for the LRL GSM-8K dataset across languages. We created the dataset\nby translating the original English sentences of GSM-8K to the target language using the Google\nTranslate API. We measure quality by back-translating the obtained examples back to English and\nmeasuring: (i) The overlap between the back-translated and the original English sentence, and (ii)\nThe delta change in performance when PaLM2-S is evaluated on this back-translated version of\nGSM-8K as compared to the original version.\nA.2\nGSM-8K\nQuality evaluation for LRL GSM-8K\nAs described in Section 4.2, we created the GSM-8K\ndataset (Cobbe et al., 2021) for low-resource languages by using the Google Translate API to obtain\nsilver translations in the target language from the source English sentence in the original dataset. We\nperform a quality evaluation of these examples by back-translating them back to English using the\nsame translation API and defining two metrics over it:\n(i) Overlap: The BLUE score measure between the actual example and the back-translated example,\n(ii) Delta: The change in performance of the PaLM2-S model when evaluated on the original GSM-\n8K set as compared to the back-translated version.\nTable 7 shows the values for these metrics across the various languages. We see that a decently\nhigh overlap value is seen across all languages. At the same time, the delta in performance is also\nminimal indicating that key attributes in the GSM-8K examples are not affected by translation.\nResults on the complete language set\nTable 6 shows the comparative evaluations on the complete\nset of 25 low-resource languages for which GSM evaluations are performed. We see an improvement\nover the anchor model mB for 20 of 25 languages. We also compare against the fully continued pre-\ntrained version mNTL\nB\nand observe that mA\u2295B outperform it for 18 of 25 languages.\nB\nQUALITATIVE ANALYSIS\nTable 8 depicts a few qualitative examples for the code-to-text, or the code explanation task, for\nPython. These examples depict examples for the three broader bucket of examples that we observe\nin cases when CALM yields the correct responses:\n1. When neither of mA or mB generates the correct response but mA\u2295B correctly attends over\ntheir latent representations to yield the correct output,\n2. When either of mA or mB is seen to give the correct response while the other one is incor-\nrect and mA\u2295B generates the correct response that matches the generation from the correct\nmodel of mA and mB, and\n3. When both mA and mB generate the correct response and mA\u2295B reproduces those genera-\ntions.\nWe also observed similar qualitative patterns with other tasks for language inclusivity.\nC\nOVERHEAD WITH CALM\nIn this section, we include a detailed computation of the expected parametric and training overhead\nwhile composing given models using our proposed CALM framework.\n15\ndef ConsumeBool(self):\ntry :\nresult = ParseBool(self.token)\nexcept ValueError as e :\nraise self. ParseError(str(e))\nself.NextToken()\nreturn result\n\u21d2 Consumes a boolean\nmA:\nConsumes a boolean\nmB:\nThe object is not a member\nCALM:\nConsumes a boolean\ndef value(self):\nif self.has value:\nreturn self. impl[OBJ].get val(K)\nelse:\nraise ValueError(\"Not found\")\nreturn\n\u21d2 Print an error message and exit.\n[a part of the given model prefix]\nExit with error message\nPrint an error message and exit\ndef get positions(url):\ndata =\nget resource(url)\npositions = [x for x in data[\u2019p\u2019]]\nreturn positions\n\u21d2 Returns a list of positions.\nPositions of specified instruments.\nGet all positions.\nReturns a list of positions .\ndef distance(x0, y0, x1, y1):\nreturn (\nsqrt(pow(x1\u2212x0,2) + pow(y1\u2212y0,2)\n)\n\u21d2 Returns the distance between two points\nCalculates the distance between two points\nReturn the distance between two points\nCalculates the distance between two points\nTable 8: Cherry-picked qualitative examples for the code-to-text task on Python that depict examples\nthat fall into a set of larger bucket of patterns that we observe across examples. CALM does well\nin various settings: (i) when mAproduces the correct output but not mB, (ii) vice-versa\u2014when mB\ndoes well, and (iii) when neither of the two base models do well but a combination of intermediate\nrepresentations allow the composed model to give the correct output. This shows that composition\nimplicitly learns to do both: routing across models and a combination, based on a given input.\nC.1\nPARAMETRIC OVERHEAD\nBuilding from the notations in \u00a73.1, let\u2019s say the two models mA and mB have NA and NB number\nof standard transformer layers, respectively, with each layer of output dimensionality DA and DB.\nAs mentioned, we choose n = nA = nB number of layers to perform the composition.\n# Parameters for each fproj layer = (DA \u2217 DB)\n# Parameters for each fcross layer = (3 \u2217 D2\nB)\n# Parameters added during composition = n \u2217 (DA \u2217 DB + 3 \u2217 D2\nB)\n# Parameters in mB = NB \u2217 (VB \u2217 DB + 3 \u2217 D2\nB + 2 \u2217 DB \u2217 DB \u2217 KB)\nwhere, VB and KB depict the vocabulary size and hidden multiplication factor, respectively.\nLet\u2019s consider some standard transformer configurations to understand the parameter overhead. As\nan example, consider the layer configurations of standard BERT models: BERT-small (mA) and\nBERT-large (mB). In this case: NA = 4, DA = 512, NB = 24, DB = 1024, VB = 30K, KB = 4.\nAssuming that we select all layers of mB, the value of n = 4. Hence,\n# Parameters added during composition = 4 \u2217 (512 \u2217 1024 + 3 \u2217 10242) \u2248 1.5 \u00d7 107 \u2248 15M\n# Parameters in mB = 24 \u2217 (30K \u2217 1024 + 3 \u2217 10242 + 2 \u2217 10242 \u2217 4) \u2248 1B\n%age of new parameters added = 15M \u2217 100/1B = 1.5%\nHence, number of parameters added during composition \u2248 1.5% of those in mB.\nC.2\nTRAINING OVERHEAD\nWhile back propagation over mB is indeed required while training CALM, the total training costs\nare still significantly lesser than training mB, owing to the training examples/iterations required.\nFirstly, as discussed above, the additional number of parameters introduced during composition is\n1.5% of the number of parameters of mB\u2014hence, a negligible parametric addition.\n16\nFurther, since only 5-7% of the total mB fine-tuning data is required to train CALM, the training\ncost of CALM is minimal with respect to training cost of training the entire anchor model.\nMoreover, since our experiments consider an mA that has 5-20% of parameters as mB, even the net\ncost of training mA and CALM is significantly lesser than training mB.\nLet\u2019s assume that (i) the cost of fine-tuning mB on the complete data is X, (ii) number of parameters\nin mA is 10% of those in mB, and (iii) the amount of data required to train CALM is 2% of mB\ntraining. Assuming a linear scaling factor of training cost (FLOPS) with model parameters and data:\nCost of training CALM \u2248 0.02 \u00d7 X = 2% of mB training.\nCost of training mA + CALM \u2248 (0.10 \u2217 X + 0.02 \u2217 X) = 0.12 \u00d7 X = 12% of mB training.\n17\n"
  },
  {
    "title": "Instruct-Imagen: Image Generation with Multi-modal Instruction",
    "link": "https://arxiv.org/pdf/2401.01952.pdf",
    "upvote": "29",
    "text": "Instruct-Imagen: Image Generation with Multi-modal Instruction\nHexiang Hu\u2660\u22c6\nKelvin C.K. Chan\u2662\u22c6\nYu-Chuan Su\u2662\u22c6\nWenhu Chen\u2660\u22c6\nYandong Li\u2662\nKihyuk Sohn\u2662\nYang Zhao\u2662\nXue Ben\u2662\nBoqing Gong\u2662\nWilliam Cohen\u2660\nMing-Wei Chang\u2660\nXuhui Jia\u2662\n\u2660Google DeepMind\n\u2662Google Research\n{hexiang,kelvinckchan,ycsu,wenhuchen}@google.com\n[ref#1] vase\n[ref#1] action\ufb01gure\n[ref#2] style image\nRender an image of the [ref#1] vase that depicts the caption, adopting the \nstyle of [ref#2] style image : a vase with \ufb02owers on top \nCreate an image of an [ref#1] action\ufb01gure, outlined as the [ref#2] edge \nmap, and re\ufb02ect the caption: an action\ufb01gure stand next to a sleepy dog \nInstruct-\nImagen\n[ref#2] edge map\n[ref#1] monster toy\n[ref#3] style image\nGenerate an image of [ref#1] monster toy in the same style as the [ref#3] \nstyle image and following the [ref#2] mask\nInstruct-\nImagen\n[ref#2] mask\nInstruct-\nImagen\nFigure 1. Zero-shot generalization of Instruct-Imagen. Our model understands the multi-modal instruction (left) to generate image\n(right) that reflects the complex and unseen image transformation.\nAbstract\nThis paper presents Instruct-Imagen, a model that\ntackles heterogeneous image generation tasks and gener-\nalizes across unseen tasks. We introduce multi-modal in-\nstruction for image generation, a task representation artic-\nulating a range of generation intents with precision. It uses\nnatural language to amalgamate disparate modalities (e.g.,\ntext, edge, style, subject, etc.), such that abundant genera-\ntion intents can be standardized in a uniform format.\nWe then build Instruct-Imagen by fine-tuning a pre-\ntrained text-to-image diffusion model with two stages. First,\n\u22c6 These authors contributed equally to this work.\nwe adapt the model using the retrieval-augmented train-\ning, to enhance model\u2019s capabilities to ground its gener-\nation on external multi-modal context. Subsequently, we\nfine-tune the adapted model on diverse image generation\ntasks that requires vision-language understanding (e.g.,\nsubject-driven generation, etc.), each paired with a multi-\nmodal instruction encapsulating the task\u2019s essence.\nHu-\nman evaluation on various image generation datasets re-\nveals that Instruct-Imagen matches or surpasses prior\ntask-specific models in-domain and demonstrates promis-\ning generalization to unseen and more complex tasks. Our\nevaluation suite will be made publicly available.\n1\narXiv:2401.01952v1  [cs.CV]  3 Jan 2024\n1. Introduction\nThe advent of generative artificial intelligence (GenAI) has\nushered in an era of significant advancements in image gen-\neration, primarily through text-to-image models. Existing\nmodels such as Stable Diffusion [35], DreamBooth [37],\nStyleDrop [42], ControlNet [50] mainly focus on accept-\ning specific instruction modality like text prompt, subject,\nstyle, edge, etc. Their ability to comprehend more complex\ninstructions involving multiple modalities (e.g., subject +\nmask + style) is yet to show, not to mention its ability to\ngeneralize to unseen instructions [20].\nUnlike the language generation [2, 11, 27, 27, 45], image\ngeneration inherently involves multimodality. In the realm\nof human artistry, the painting process often integrates var-\nious modalities to achieve the desired outcome. A painter\nmight start with a rough sketch to outline the composition,\nthen apply a specific style, like impressionism, for details\non texture and color. They may also use photographs or\nlive models as subject references, blending these elements\nto create an expressive piece of art. Communicating the\nmulti-modal complexities behind such an \u201cimage genera-\ntion\u201d procedure is challenging, even among humans.\nCan we effectively communicate the multi-modal com-\nplexities to models? To address this challenge, we introduce\nmulti-modal instruction in image generation. This approach\ninterleaves and adheres information from different modali-\nties, expressing the conditions for image generation (refer\nto Figure 1 left for examples). Specifically, multi-modal\ninstruction enhances language instructions, i.e., \u201crender an\ninstance of subject images adopting the style of style\nimage, such that...\u201d, by integrating information from other\nmodalities (e.g., subject and style) to describe the objective\nof generating a customized image of the given subject in\nthe provided visual style. As such, prior image generation\ntasks with multi-modal conditions can be efficiently com-\nmunicated in a human intuitive interface (see \u00a7 2).\nWe then build our model, i.e., Instruct-Imagen, em-\nploying a two-stage training approach, to first enhance\nmodel\u2019s ability to process multi-modal instructions, and\nthen faithfully follow the multi-modal user intents. This in-\nvolved initially adapting a pre-trained text-to-image model\nto handle additional multi-modal inputs, followed by fine-\ntuning it to accurately respond to multi-modal instructions.\nParticularly, we begin by continuing the text-to-image gen-\neration training of a pre-trained diffusion model, supple-\nmented by similar (image, text) contexts retrieved from a\nweb-scale (image, text) corpus [6]. In the second stage,\nwe fine-tune the model on diverse image generation tasks,\neach paired with multi-modal instructions that encapsulate\nthe essence of the task. Consequently, Instruct-Imagen\nexcels in merging diverse modal inputs like sketches and\nvisual styles with textual directives, producing contextually\naccurate and visually compelling images.\nAs illustrated in Figure 1, Instruct-Imagen demon-\nstrates strong capability of understanding the sophisticated\nmulti-modal instruction to generate the images faithful to\nthe human intention, even when the instruction combination\nhas never been observed before. Human studies establishes\nthat Instruct-Imagen not only matches but, in several\ninstances, surpasses prior task-specific models within their\ndomains. More significantly, it exhibits a promising gener-\nalization capability when applied to unseen and more com-\nplex image generation tasks.\nWe summarize our contributions as follows:\n\u2022 We introduce multi-modal instruction, a task represen-\ntation universally represents instruction from multiple\nmodalities, e.g., text, edge, mask, style, subject, etc.\n\u2022 We propose to perform retrieval-augmented training and\nmulti-modal instruction-tuning to adapt the pre-trained\ntext-to-image models to follow multi-modal instructions.\n\u2022 We build Instruct-Imagen, a unified model that tack-\nles heterogeneous image generation tasks, surpassing the\nseveral state-of-the-arts in their domains.\n\u2022 More substantially, Instruct-Imagen generalizes to\nunseen and complex tasks, without any ad hoc design.\n2. Multi-modal Instructions for Generation\nIn this section, we start with discussing the preliminary on\ndiffusion models with input conditions. Then we introduce\nthe format of multi-modal instruction, and discuss how prior\nimage generation tasks can be unified in this framework.\nDiffusion Models with Input Conditions. Diffusion mod-\nels [35, 38, 41] are latent variable models, parameterized\nby \u0398, in the form of p\u0398(x0) :=\nR\np\u0398(x0:T )dx1:T , where\nx1, \u00b7 \u00b7 \u00b7 , xT are \u201cnoised\u201d latent versions of the input image\nx0 \u223c q(x0). Note that the dimension of both latent and\nthe image are the same throughout the entire process, with\nx0:T \u2208 Rd and d indicating the data dimension. The pro-\ncess that computes the posterior distribution q(x1:T |x0) is\ncalled the diffusion process, and is implemented as a pre-\ndefined Markov chain that gradually adds Gaussian noise to\nthe data according to a schedule \u03b2t:\nq(x1:T |x0) =\nT\nY\nt=1\nq(xt|xt\u22121);\n(1)\nq(xt|xt\u22121) := N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI)\n(2)\nDiffusion models are trained to learn the image distribu-\ntion by reversing the diffusion Markov chain. Theoretically,\nthis reduces to learning to denoise xt \u223c q(xt|x0) into x0,\nwith a time re-weighted square error loss [15]:\nE(x0,c)\u223cD{E\u03f5,t[wt \u00b7 ||\u02c6x\u03b8(xt, c) \u2212 x0||2\n2]}\n(3)\nwhere D is the training dataset containing (image, condi-\ntion) = (x0, c) pairs. In the text-to-image models, the con-\n2\nControlNet\nOutput\nMulti-modal Instruction\n[ref#1] style image\nCreate an image of a cup and a can following the art style of the \n[ref#1] style image, in the shape outlined by the [ref#2] mask\n[ref#2] mask\nSuTI\n[ref#1] a fancy boot\nDraw a [ref#1] fancy boot to match the content of description:\na fancy boot on the stage with bunny sticking its head out\n[ref#1] edge\n[ref#1] mask\nor\nand\nBased on the [ref#1] edge (or [ref#1] mask), generate an image \naccording to the text: a golden trophy\nNew \nCapability\n[ref#1] style image\nDraw an image in the style of [ref#1] style images, following the \ncaption: a coffee maker \nStyleDrop\nFigure 2. Illustration on how multi-modal intruction uniformly\nexpress existing image generation tasks and extends to new tasks.\nExamples in this figure are retrieved from [7, 42, 50]\ndition c are often the embeddings of input text prompt, from\npre-trained text embedding models (e.g., T5 [32]).\nUnified Multi-modal Instruction. While multi-modality\ninformation is necessary for extended image generation ap-\nplications, and had been explored in prior works [7, 22, 37,\n42, 50], etc., there was not such a format in the literature\nthat allows generalization. Instead, models often make ad-\nhoc design to integrate information from other modalities.\nFor example, ControlNet [50] combines the input xt with\na transformed spatial control map feature to form the new\ninput for reverse diffusion. Such modality and task specific\ndesign, while effective in-domain, is challenging to general-\nize to other tasks (e.g., stylization). Therefore, we propose\nthe multi-modal instruction, a new format where language\nare used to explicitly state the objective behind tasks, with\nreferences to multi-modal conditions.\nThere are two key components in the proposed instruc-\ntion format: (1) the payload text instruction that provides\ndetailed description of the task objective, with reference\nmarker (e.g., [ref#?]).\n(2) a multi-modal context with\n(marker + text, image) pairs. The model then employ a\nshared instruction understanding model to consume both\nthe text instruction and the multi-modal context, regard-\nless of the specific modality in the context. Figure 2 show-\ncased three examples of how this format represents various\nprior generation tasks, showing its compatibility to prior im-\nage generation tasks. More importantly, the flexibility of\nlanguage allows multi-modal instructions to extend to new\ntasks, without any modality & task specific design.\n3. Instruct-Imagen\nIn this section, we first discuss how Instruct-Imagen\nencodes the input multi-modal instruction, and how the\nencoding is leveraged for generation (see \u00a7 3.1).\nThen\nwe introduce the two staged training framework for\nInstruct-Imagen in \u00a7 3.2.\nIn Figure 3, we present\nthe high-level design of the Instruct-Imagen, alongside\nwith an overview of its training procedure.\n3.1. Imagen with Multi-modal Instruction\nThe foundation of Instruct-Imagen is the multi-modal\ninstruction, which uniformly represents prior image gener-\nation tasks, while remains its capability to extend to novel\nand complex tasks. Based on it, we designed the model ar-\nchitecture that extends a pre-trained text-to-image diffusion\nmodels, i.e., a cascaded diffusion model [16], to allow it\nfully conditioned on the input multi-modal instruction.\nCascaded Backbone Text-to-Image Model.\nWe used\na version of Imagen [38] pre-trained on internal data\nsources, which inherents the cascaded text-to-image diffu-\nsion model (see Figure 3 left), as the founding for adap-\ntation to Instruct-Imagen.\nThe full model has two\nsub-components: (1) a text-to-image that generates 128\u00d7\nresolution images from text prompt only, and (2) a text-\nconditioned super-resolution model that scales the 128 reso-\nlution up to high fidelity 1024\u00d7 images. In the scope of this\nwork, we only consider training and adapting the 128 reso-\nlution text-to-image network, for the sake of efficiency and\nclarity. Particularly, the backbone model is a convolutional\nUNet [36] with bottleneck, with a paired down-sampling\nencoder and up-sampling decoder. The text are then em-\nbedded with a pre-trained T5-XXL model [32]. The em-\nbeddings are then input to the down-sampling encoder as\ncondition, and to the cross-attention on bottleneck repre-\nsentation as enhanced reference.\nEncoding Multi-modal Instruction. We adapt the above\nmentioned cascaded text-to-image model via maximally\nreusing the pre-trained text-to-image model for encoding\nthe multi-modal instruction, and only introduce one cross-\nattention layer that conditions the bottleneck representation\nof UNet with the embedded multi-modal context the (key,\nvalue) pairs. This grows the number of parameters of our\nmodel from 2.51B to 2.76B (\u223c10%). This design is in\nprinciple similar to the nearest neighbor UNet presented\nin [6] (but with the nested encoding on the multi-modal con-\ntext). Figure 3 (right) illustrates the dataflow of how a multi-\nmodal instruction is encoded by the Instruct-Imagen.\nHere, the payload text instruction is encoded the same way\nas normal text input in backbone model. The multi-modal\ncontext, i.e., both (marker + text, image) pairs, are first en-\ncoded using the down-sampling encoder, same as how back-\nbone text-to-image model encodes the bottleneck represen-\n3\nimage\nencoder\ntext\nencoder\n\u2026\n\u2026\nCross-atten\nimage\nencoder\ntext\nencoder\n\u2026\n\u2026\nCross-atten\n\u2018an [ref#1] toy image\u2018\n\u2018a [ref#2] style image\u2018\nMultimodal Context\nimage\nencoder\nimage\ndecoder\nPre-trained T2I model (Imagen)\n\u2018Begging face of \nsmall dog with big \nstanding ears'\n128x128\nSuper \nResolution\ntext\nencoder\n\u2026\ntext\nencoder\n\u2026\ntext\nencoder\n\u2026\n\u2018Generate a [ref#1] toy \nin the [ref#2] style: a \ntoy on street with city \nlight in the distance'\n\u2018King prawn and \nlochmuir paoched \nsalmon \u2026\u2019\n\u2018salad, fish and \neggs decorated\u2026\u2019\nMultimodal Context\n\u2018Prawn and smoked \nsalmon salad over \nblack stone'\nPhase 1: Retrieval-augmented Training\nPhase 2: Multi-modal Instruction-tuning\n128x128\n128x128\nSuper \nResolution\nSuper \nResolution\nCross-atten\nCross-atten\nimage\nencoder\nimage\ndecoder\nCross-atten\nCross-atten\nimage\nencoder\nimage\ndecoder\nCross-atten\nCross-atten\nimage\nencoder\nimage\ndecoder\nCross-atten\nFigure 3. Overview of the two-staged training pipeline for the proposed Instruct-Imagen model.\ntation, and then provided as (key, value) pairs for the new\ncross-attention layer to condition on. The up-sampling de-\ncoder then takes the outcome feature representation to per-\nform the reverse diffusion.\n3.2. Training Instruct-Imagen in Two Stages\nOur training pipeline is two staged, with the first stage to\ncontinue the text-to-image generation, with augmentation\nof retrieved neighbor (image, text) pairs. Then in the sec-\nond stage, we fine-tune the output model from first stage\non a mixture of diverse image generation tasks, each paired\nwith corresponding multi-modal instructions. In both train-\ning stages, the model are optimized end-to-end.\nRetrieval-augmented Text-to-image Training. The most\nimportant research question for Instruct-Imagen is how\nto train the model to condition on multi-modal inputs for\nits generation, since these tasks deviate from the standard\ntext-to-image pre-training. A straight-forward thinking is to\nmine naturally distributed multi-modal Internet data [1, 52]\n(such as Wikipedia articles with images) and train models\nto use the interleaved (image, text) data to generate the\ndesired output image. However, this is inadequate to train\nmodels with superior alignment, because the input multi-\nmodal content are often not relevant to the production of\nthe output image. For example, in the Wikipedia article,\ni.e., the US president, the headline text, summary text\nand info-box images (i.e., Biden\u2019s picture) are not infor-\nmative to generate the image of Franklin D. Roosevelt.\nThus, training model using such data often leads to igno-\nrance of the multi-modal context.\nTo alleviate this issue, we employ the training data sim-\nilar to re-imagen [6], such that the model can learn to look\nat the relevant but not duplicated neighboring multi-modal\ncontext when generating image according to the current text\nprompt. Particularly, the model would be presented with\nportraits of Franklin D. Roosevelt at other occurrences,\nwhen asked to generate his presence delivering the radio ad-\ndress in 1933. A model capable of processing multi-modal\ninputs can leverage other Roosevelt images to generate the\nscene, instead of memorizing his appearance.\nTo achieve this, we construct the retrieval-augmented\ntraining dataset via domain-specific clustering of Web\n(image, text) pairs. First, we processed the web scale\nimage-text corpus (i.e., WebLI [8, 9]) to remove low qual-\nity images (in image quality scores [43]), classified images\nfrom specific clusters (e.g., art, products, animals, scenery,\netc.) via image-text matching, and performed image clus-\ntering within each classified sub-cluster, using the embed-\ndings from CLIP [31] model. For each mined image cluster,\nwe took the top 10 nearest neighbor candidates, and per-\nformed near-duplication removal via removing images with\nhigh similarity and images with the same metadata (e.g.,\nURL). We then truncate the image cluster to have the size\nof 5 images (discarded clusters with less than 5 images).\nAs an outcome, this process produced 8.08 M (image,\ntext) clusters, with 5 pairs per cluster. During the train-\ning, one (image, text) pair is sampled as the input and\ntarget for the Instruct-Imagen, and three other (image,\ntext) pairs are sampled as the multi-modal context. Addi-\ntionally, we performed the condition dropout as [35, 38] but\nwith two independent drop situations: (1) dropping both the\ninput text and multi-modal context; and (2) dropping only\nthe multi-modal context, each dropout situation occurs at\n10% chance.\nMulti-modal instruction-tuning for Image Generation.\nWe prepared 11 image generation datasets via either re-\nusing existing dataset or synthesizing the input or target\nimage, which formed 5 task categories, for multi-modal\ninstruction-tuning. For each dataset, we prompted the GPT-\n4 [27] to generate 100 rephrased instruction templates with\nhigh variation, and validated the semantic correctness of\nthem manually. We defer the qualitative examples of each\ndataset and its associated instruction to the appendix. The\nTable 1 presents the detailed information about task group,\nmodel input conditions, and data statistics for each prepared\ndataset, with details below:\n4\nTask\nInput\nDataset\n#Examples Ratio\nTxt2Img\ntxt\nInternal Data\n5M\n0.15\nWikiArt\n0.1M\n0.05\nControl2Img\ndepth img+txt\nDepth WebLI [8]\n5.7M\n0.06\nmask img+txt\nMask WebLI [8]\n5.7M\n0.06\nedge img+txt\nEdge WebLI [8]\n5.7M\n0.06\nSketch2Image [23]\n15K\n0.02\nSubject Txt2img sub imgs+txt\nSuTI dataset [7]\n0.75M\n0.30\nCeleb-A [25]\n0.1M\n0.05\nCeleb-HQ [19]\n0.1M\n0.05\nStyle Txt2img\nsty img+txt\nDerived from WikiArt\n0.1M\n0.10\nStyle Transfer\nsty img+ctn img WikiArt + Internal Data\n1M\n0.10\nTable 1. Details of the instruction-tuning datasets and mixing ratio.\n\u2022 Text-to-image Generation. We processes two datasets\nfor instructed text-to-image generation: an internal high-\nquality natural image dataset with manual caption; and\nan art specific dataset crawled from WikiArt (using the\npipeline in [44]), with the caption generated by PaLI [8].\nBoth datasets are augmented with sampled instruction.\n\u2022 Control2Image Generation. We followed [50] to pre-\npare the control signals (e.g., depth map, mask, and edge),\nbased on a subset of the WebLI [8]. Specifically, we use\nMiDas [34] for depth estimation, HED [46] for edge ex-\ntraction, and salient object [30] for mask. To improve\nrobustness with different edge styles, we also employed\nedge-to-image data from a sketch dataset [23].\n\u2022 Subject-driven Generation.\nWe consider two data\nsources for subjects:\ngeneral objects and human in-\nstances, for subject-driven generation. Particularly, we\nuse the subject-driven dataset introduced in SuTI [7]\nfor general object learning,\nand the celebrity face\ndatasets [19, 25] to learn face rendering. For face render-\ning, we group the faces of the same person and caption\nthem with PaLI [8], then we use one sampled example\nas the input/target, and the rest as multi-modal context.\nAll datasets then join the instruction templates, with ref-\nerence markers inserted to refer the multi-modal context.\n\u2022 Styled Generation. Styled generation is a task that gen-\neralizes over the StyleDrop [42], with a style image and\ntext as input, styled image following the text as output.\nTo collect such data, we used images from WikiArt as\nthe collection of style images to train StyleDrop models,\nand then use the manual captions from the internal text-\nto-image dataset to sample images as the target styled im-\nage. We employ a CLIP model to filter out examples that\nfails the alignment with either style image or the caption.\nThen multi-modal instructions are created via combining\nthe instruction template with style image and the caption,\nsuch that the style image is correctly referred.\n\u2022 Style Transfer. Similarly, we construct the style trans-\nfer dataset via combining style images from our WikiArt\ncrawl and content images from the internal dataset (with\nthe captions discarded). Particularly, we employ a sim-\nple style transfer model [13], which allows fast and large-\nscale generation, to blend the style image with the content\nimage. These data are then augmented with instructions.\nDuring the instruction-tuning stage, we fine-tune the output\nmodel of the retrieval-augmented training on the multi-task\nmixed dataset, with the mixture ratio specified in Table 1.\n4. Related Work\nInstruction-Tuning. Instruction tuning was first introduced\nin FLAN [45], which finetunes a large language model\n(LLM) on instructions to significantly improve its zero-shot\nlearning performance on unseen tasks.\nChung et al. ex-\ntended the work at scale [11], showing extraordinary gener-\nalization to numerous NLP tasks. In general, the instruction\ndata plays a pivotal role in the finetuned LLM [51]. This\nsuccess experience in text instruction tuning was then intro-\nduced to the vision-language models [4, 9, 24], which en-\nables generalization across tasks such as recognition and Vi-\nsual QAs [10, 14, 17, 26]. While a concurrent work has ex-\nplored image generation with multi-modal inputs [28], this\npaper presents an new initiative to investigate multi-modal\ninstruction tuning for image generation models.\nControlled Image Synthesis.\nRecent advancements in\ntext-to-image generative models [3, 5, 6, 33, 35, 38, 47, 48]\nhave showcased impressive capabilities in various domains,\nincluding creativity, photorealism, diversity, and coherence.\nA critical aspect of these advancements is controllability,\nwhich has been enhanced by adapting these models to spe-\ncific subjects [7, 37], styles [42], masks [50], etc. For ex-\nample, DreamBooth [37] fine-tunes a text-to-image model\non a limited set of images to better capture the nuances of\na specific subject. Additionally, ControlNet [50] introduces\nthe ability to condition on a variety of control signals, in-\ncluding depth maps and doodles, by fine-tuning an auxiliary\nencoder with the appropriate data pairs. Despite these ad-\nvancements, a common limitation persists: these models of-\nten specialize in specific modalities, leaving the generaliza-\ntion to novel modalities and their combinations unexplored.\nTo address this gap, we introduce Instruct-Imagen, a\nnovel model designed to understand complex relationships\nand generalize effectively to unencountered tasks.\n5. Experiments\nIn this section, we first introduce the experimental setup, the\nhuman evaluation protocol, and comparative baseline sys-\ntems in \u00a7 5.1. We then present the main results in \u00a7 5.2,\nhighlighting advantages of Instruct-Imagen in tackling\nmultiple in-domain tasks and challenging unseen tasks. In\n\u00a7 5.3, we perform an in-depth analysis to study the design\nof Instruct-Imagen, and the model\u2019s failure mode.\n5\nDepth2Img\nMask2Img\nEdge2Img\nSty Gen.\nSub Gen.\nTxt2Img\nFace Gen.\nSty Trans.\n0\n20\n40\n60\n80\n100\nN/A\n21\n64\n54\n45\n67\n67\n46\n50\n54\n68\n37\n65\n54\n54\n37\n0\n65\n52\n67\n71\n67\n76\n60\n90\n79\n82\n88\n81\n76\n76\n56\n(a) In-domain Evaluation\nHuman Score O (\u00d7100)\nSingle-Task\nMulti-Task\nPrior Mtd\nInstruct-Imagen\nSty+Sub\nMulti Sub\nCtrl+Sub\nCtrl+Sty\n48\n54\n36\n36\n33\n43\n32\n11\n58\n53\n60\n63\n(b) Zero-shot Evaluation\nFigure 4. Human Study on prior methods, baselines, and Instruct-Imagen. Instruct-Imagen can perform on par or better\ncomparing to the baselines and prior methods, with best generalization capability to novel tasks. Instruct-Imagen does not require\nany fine-tuning for all tasks (particularly style/subject-related), and inferences at an average speed of 18.2 seconds per example (on TPUv4).\nGenerate an image of the [ref#1] \nberry bowl, folloing the caption \nnext. A dog snif\ufb01ng a berry bowl.\n[ref#1]  berry bowl\nSubject Generation\nCreate an image following the \ncaption: a black and white cat \nsitting on a brown suitcase next \nto a wall. \nText-to-Image\nPrior Method\nMulti-modal Instruction\nSingle Task\nMulti Task\nInstruct-Imagen\nGenerate an image in the shape \nsuggested by the [ref#2] mask, \nand re\ufb02ect the caption: a \nmotorcycl parked on the beach. \n[ref#2] mask\nMask-to-Image\nDraw a picture in the style of \n[ref#1] style image, following the \ncaption. A parrot eating biscuit.\nIn-context Stylization\n[ref#1] style image\nFigure 5. Comparison on a subset of in-domain tasks. Examples generated from prior methods, baselines, and Instruct-Imagen.\nWe visualize the multi-modal instruction for human intuitive understanding (models are evaluated with in-distribution inputs).\n5.1. Experimental Setup\nWe evaluate our models with two setups, i.e., in-domain\ntask evaluation and zero-shot task evaluation, where the\nlater setup is strictly more challenging than the former.\nParticularly, we re-use the recently introduced condi-\ntional image generation benchmark, i.e., ImagenHub [20],\nfor evaluating text-to-image generation.\nWe also em-\nploy other datasets to cover in-domain evaluation:\nWe\nadopt the DreamBench [7, 37] v1 & v2 as our subject-\ndriven evaluation data; We use the style images from Style-\nDrop [42] for style evaluation; We use hold-out style images\nfrom WikiArt [44] and content images from CustomCon-\n6\ncept101 [21] for style transfer. We use the evaluation data of\nWebLI [8] for control2image (i.e., mask, edge, depth) eval-\nuation. For face evaluation, we evaluate on the validation\nset of hold-out human in CelebA [25] and CelebA-HQ [19].\nFor zero-shot tasks, we either adopt the existing evalua-\ntion (i.e., CustomConcept101 [21] for multi-subject, on the\n[20]\u2019s split) or construct the evaluation ourself (e.g., subject\n+ control, style + control, style + subject) by adopting exam-\nples from corresponding in-domain task datasets. We refer\nthe readers to the appendix for complete information about\nevaluation datasets. The complete evaluation suite would be\nmade publicly available for future study and comparison.\nBaseline Models. We compare Instruct-Imagen with\nthree category of baseline models: (1) Prior State-of-the-art\nmethod (2) Single-task model (3) Multi-task model. Since\nno single prior model can handle all image generation tasks,\nwe make comparison to different prior method on each task.\nParticularly, we compare to: SDXL [29] for text-to-image\ngeneration; ControlNet [50] for edge/depth-to-image gener-\nation; Ghiasi et al. [13] for style transfer; StyleDrop [42] for\nstyled generation; SuTI [7] for subject-driven generation;\nand TamingEncoder [18] for face generation. Note that we\nmarked prior method on Mask2Img task with N/A due to\nlack of public model. For zero-shot tasks, we compare to:\nKOSMOS-G [28] for styled subject generation and multi-\nsubject generation; and BLIPDiffusion [22] for the other\ntwo tasks, given its capability on accepting multi-modal in-\nputs.\nThe single-task and multi-task models share the same\nmodel architecture as Instruct-Imagen, but do not have\naccess to the multi-modal instruction during fine-tuning and\ninference. Instead, they accept the raw multi-modal inputs\nfrom each task. Additionally, the single-task model requires\nan independent model for each task, thereby inducing 7\u00d7\nmore parameters than Instruct-Imagen.\nHuman Evaluation. We follow the same evaluation proto-\ncol as [20] to conduct systematic human study. Each sample\nis rated by at least three raters for their semantic consistency\nscore (SC) and perceptual quality score (PQ). The score in\neach category are {0, 0.5, 1}, where 0 means inconsistent\n/ extremely poor quality and 1 means totally consistent /\nhigh quality respectively. Note that semantic consistency is\ndefined as the score of the least consistent condition when\nthere are multiple conditions. The final human score is de-\nfined as O=\u221aSC\u00d7PQ. We recruit eight huamn raters and\ntrain them following the guidelines1 in ImagenHub [20].\nEach method is evaluated independently, but we assign the\nsame rater for samples generated by different methods given\nthe same input to ensure evaluation calibrated per example.\n1https://imagenhub.readthedocs.io/en/latest/Guidelines/humaneval.html\nMethod\nSetup\nHuman Score\nAccuracy\nSDXL-inpainting\n-\n0.43\n0.25\nImagen\nFine-tuned\n0.37\n0.10\nInstruct-Imagen Fine-tuned\n0.72 (+0.35)\n0.57 (+0.47)\nTable 2. Masked Image Editing Evaluation on ImagenHub [20].\n5.2. Main Results\nFigure 4 compares Instruct-Imagen with our baselines\nand prior methods, showing it achieves similar or superior\nresults in terms of in-domain evaluation and zero-shot eval-\nuation (the breakdown of SC and PQ is detailed in the ap-\npendix). It suggests that multi-modal instruction training\nenhances performance in tasks with limited training data,\nsuch as stylized generation, while maintaining effectiveness\nin data-rich tasks, such as photorealistic imaging. With-\nout multi-modal instruction training, our multi-task base-\nline tends to yield inferior image quality and text align-\nment. For instance, in the in-context stylization example\nof the Figure 5, the multi-task baseline struggles to differ-\nentiate style from subject, and replicate the subject in its\ngeneration. For similar reason, it generates 0 performance\nin the task of style transfer. This observation underscores\nthe value of instruction tuning.\nDistinct from many current approaches that rely on task-\nspecific methods (e.g., StyleDrop [42] + DreamBooth [37])\nor training [21], Instruct-Imagen efficiently manages\ncompositional tasks by leveraging instructions that com-\nbines the objectives from individual tasks, and inference in-\ncontext (no fine-tuning required, which takes 18.2 seconds\nper example). As shown in Figure 6, Instruct-Imagen\nconsistently outperforms others in instruction following and\noutput quality. Furthermore, in the presence of multiple ref-\nerences in the multi-modal context, the multi-task baseline\nmodel fails to correspond the text instructions to the refer-\nences, resulting in the ignorance of some multi-modal con-\nditions. These results further demonstrate the efficacy of\nthe proposed model. More qualitative visualization in the\nappendix.\n5.3. Model Analysis & Ablation Study\nBesides the main results, we also perform studies to explore\nthe limit of Instruct-Imagen, ablate important design of\nits training, and analyze its failure mode.\nFine-tuned Instruct-Imagen can edit image.\nAside\nfrom zero-shot compositional tasks, another advantage of\nInstruct-Imagen lies in its adaptability to new tasks.\nParticularly, we fine-tuned Instruct-Imagen on the Mag-\nicBrush dataset [49] (\u223c 9K examples) for 10K steps,\nand evaluated on the masked image editing data by Im-\nagenHub [20].\nWe report the results using the overall\nscore [20] (O), and the accuracy (i.e., % of examples where\nSC=1). As a result, Table 2 presents a comparison between\n7\nPrior Method\nMulti-modal Instruction\nMulti Task\nInstruct-Imagen\nGenerate an image of [ref#1] cat  and as the [ref#2] \nwooden pot, to follow the caption: watercolor \npainting of a cat inside a wooden pot.\n[ref#1]  cat\n[ref#2] wooden pot\nDraw a picture of the [ref#1] duck toy in the same \nstyle as the [ref#2] style, and depict content in the \nprompt next.  A duck toy swimming in a lake.\n[ref#1]  duck toy\n[ref#2] style\nStyle + Subject\nMulti-Subject\nFigure 6. Comparison on a subset of zero-shot tasks. Examples generated from prior methods, the baseline, and instruct-imagen.\nWe visualize the multi-modal instruction for human intuitive understanding (models are evaluated with in-distribution inputs).\n[ref#2] content image\nEdit the [ref#1] mask area of the [ref#2] content image and re\ufb02ect \nthe change: replace chocolate with berry \nInput\nOutput\nSuper Resolution Output\n[ref#1] mask\nFigure 7. Instruct-Imagen for masked image editing When\nfine-tuned on MagicBrush [49], although Instruct-Imagen\ncan edit the image as instructed (i.e., see the 128 \u00d7 128 output),\nthe super-resolution model fails to capture details from the input\nimage, and causes the inconsistency.\nprior methods (SDXL-inpainting [29]), fine-tuned Imagen\nmodel (has been retrieval-augmented trained but without in-\nstruction tuning), and fine-tuned Instruct-Imagen.\nIt\nshows that once fine-tuned, Instruct-Imagen can per-\nform significantly better than the baseline method, and also\nmethod specifically designed for mask-based image editing.\nHowever, the fine-tuned Instruct-Imagen introduces ar-\ntifacts into edited images, particularly in high-resolution\noutputs after super-resolution, as depicted in Figure 7. This\noccurs due to the model\u2019s lack of prior learning in pixel-\naccurate copying from context to output, a task significantly\ndistinct from other Instruct-Imagen tasks.\nRetrieval-augmented training helps generalization. We\ncompare variants of\nInstruct-Imagen in terms of\nwhether performing retrieval augmented training and report\nresults in Table 3. It shows the retrieval augmented train-\ning is an important step to obtain superior empirical results,\n[ref#3] style\nGenerate an image of [ref#1] bear plushie  and \n[ref#2] barn in the style of [ref#3] style image \nsuch that: bear plushie sits in front of the barn.\n[ref#2] barn\n[ref#1]  bear plushie\nDraw a picture of the [ref#1] car following the same \nstyle as the [ref#2] style image, in the shape \nre\ufb02ected by the [ref#3] mask\n[ref#3] mask\n[ref#1]  car\n[ref#2] style\nFigure 8. Failure mode of Instruct-Imagen. The most com-\nmon failure of Instruct-Imagen is its incapability to follow\neach control condition in the instruction faithfully.\nMethod\nIn-domain Eval\nZero-shot Eval\nw/o Retrieval-augmented\n0.55\n0.53\nw/ Retrieval-augmented\n0.79 (+0.25)\n0.59 (+0.06)\nTable 3. Ablation study on retrieval-augmented training. We re-\nport the average in-domain and zero-shot eval scores O.\nin terms of both in-domain and zero-shot evaluation. This\nvalidates our hypothesis that retrieval augmented training\nbenefits representing and handling multi-modal context.\nFailure mode of Instruct-Imagen. One common pat-\ntern we found in Instruct-Imagen (when attempting\nmore complex multi-modal instructions, with at least 3\nmulti-modal conditions) is its failure to follow instruction\nin the generation. Particularly, the model can accomplish\nthe generation to satisfy only a subset of conditions spec-\nified in the multi-modal instruction. The first example in\nFigure 8 shows the model is capable of handling the style\nand subject to some extent, but not generate the output in\nthe shape that the mask specified. In the second example,\nthe model can generate the \u201cplushie in front of barn\u201d in the\ngiven style, but fails to reserve the barn\u2019s appearance.\n8\nCreate an image outlined as the \n[ref#1] edge map and in the speci\ufb01ed \n[ref#2] style: a cat playing with a \nball of yarn \n[ref#1] edge\n[ref#2] style\nStyle  + Control\n[ref#1] edge\n[ref#2] Baroque style\nGenerate an image aligned with the \n[ref#1] edge map in the [ref#2] \nBaroque style, using the below \ndescription: a car in a bustling \nmarket street\nCreate an image aligned with the \n[ref#1] depth map in the [ref#2] oil \npainting style, following the \ndescription: a tractor\n[ref#1] depth map\n[ref#2] oil painting\n[ref#1] mask\n[ref#2] painting style\nGenerate an image aligned with the  \nthe [ref#1] mask in the [ref#2] \npainting style, using the  capion: a \nfuturistic car in a sci-\ufb01 cityscape\nFigure 9. Additional Qualitative Evaluation of Instruct-Imagen on Control + Style Generation.\n[ref#1] \ufb02ower\nGenerate an image of two subjects, \n[ref#1] \ufb02ower and [ref#2] wooden, \nfollowing the caption. Flower in the \nwooden pot on a table\n[ref#2] wooden pot\n[ref#1] person\nGenerate an image of a [ref#1] person \nand a [ref#2] castle using the caption. \nperson taking a sel\ufb01e in front of \ncastle with sunset in background.\n[ref#2] castle\n[ref#1] cat\nDraw a picture of a [ref#1] cat and a \n[ref#2] table, following the caption. A \ncat relaxing on a table on a rooftop, \nwith the city in the background.\n[ref#2] table\n[ref#1] tortoise plushy \nCreate an image of a [ref#1] tortoise \nplushy and a [ref#2] cat with the \ncaption. A playful cat batting a \ntortoise plushie on a sunny beach.\n[ref#2] cat\nMulti-Subject\nFigure 10. Additional Qualitative Evaluation of Instruct-Imagen on Multi-Subject Generation.\nAdditional qualitative results of Instruct-Imagen.\nHere, we provide additional qualitative visualization on\nmore diverse and sophisticated multi-modal instructions\n(images are unseen in the model training), to explore the\nlimit of Instruct-Imagen. Particularly, Figure 9, Fig-\nure 10, Figure 11, and Figure 12 jointly presents complex\ntasks that is unseen during the training. We defer more com-\nprehensive view of in-domain image generation results to\nthe appendix, in the Figure 13. Note that we do not pro-\nvide qualitative results on face generation due to lack of\nconsent from the original dataset owner.\n6. Discussion\nWe introduce Instruct-Imagen, an image generation\nmodel that comprehends multi-modal instruction to accom-\nplish a variety of visual generative tasks. It marks an initial\nbut significant leap forward general-purpose visual genera-\ntive model, via allowing not only in-domain image gener-\nation, but also zero-shot image generation on unseen and\ncomplex instructions.\nWhile opening up a new research\ndirection, Instruct-Imagen can not handle image edit-\ning tasks in zero-shot. A key limitation is its lack of pixel\nconsistency with input images, hindering the inclusion of\nadditional tasks like in-painting and image editing in the\ninstruction-tuning. This issue stems from the use of a cas-\ncaded diffusion model, which depends on a low-resolution\nmodel for crucial decisions like layout and object seman-\ntics. Such a low-resolution model struggles with both ac-\ncessing high-resolution input details and reproducing them\nin the output, leading to artifacts in the generated image\n\u2014 because the super resolution model has to hallucinate\nthe details. Based on this observation, we believe that one\n9\n[ref#1]  car\nCreate an image of [ref#1] car \naligned with the [ref#2] mask , \nfollowing the description: a car with \nsteam coming from the hood\n[ref#2] mask\n[ref#1]  teddybear\nCreate an image of a [ref#1] teddy \nbear plushie in the [ref#2] edge map, \nfollowing the description: a teddy \nbear plushie in a car seat\n[ref#2] edge\n[ref#1]  castle\nGenerate an image of [ref#1]a castle \nscene aligned with the [ref#2] depth \nmap, with the caption: a castle by the \nsea, waves crashing against the cliffs\n[ref#2] depth\n[ref#1]  cat\nMake an image of a [ref#1] cat using \n[ref#2] the mask, following the \ndescription: a cat next to a \ufb01sh tank\n[ref#2] mask\nControl  + Subject\nFigure 11. Additional Qualitative Evaluation of Instruct-Imagen on Control + Subject Generation.\n[ref#1]  jacket\nGenerate a [ref#1] jacket in the \n[ref#2] cartoon style, following the \ndescription: a jacket on a bench in a \npark\n[ref#2] style\n[ref#1]  lighthousse scene\nImagine a [ref#1] lighthouse in the \n[ref#2] crayon drawing style with the \ncaption: a lighthouse on a rocky \nisland, waves crashing against the \nshore\n[ref#2] style\n[ref#1]  glasses\nCreate an image of [ref#1] glasses in \nthe [ref#2] oil painting style, \nfollowing the caption: glasses on a \nbeach towel\n[ref#2] style\n[ref#1]  Russian blue cat\nCreate an image of a [ref#1] Russian \nblue cat  in the [ref#2] Pop Art style, \nfollowing the description: A Russian \nblue cat with a hat on a vacation\n[ref#2] style\nStyle  + Subject\nFigure 12. Additional Qualitative Evaluation of Instruct-Imagen on Styled Subject Generation.\npromising future direction is developing diffusion models\nthat operate at the raw image resolution.\nAcknowledgement\nWe thank Zhiwei Deng, Jason Baldridge, Nando de Freitas\nfor reviewing an early version of this paper in depth, with\nvaluable comments and suggestions. Special thanks to Han\nZhang for project idea discussion in the early stage of this\nproject. We also thank Irina Blok for providing a style im-\nage used in our evaluation.\nBroader Impact\nText-to-image generation models like Imagen [38] and Sta-\nble Diffusion [29] present ethical concerns, including so-\ncial bias.\nInstruct-Imagen, using similar Web-scale\ndatasets, faces these same issues.\nInstruct-Imagen\u2019s\nretrieval-augmented training and multi-modal instruction-\ntuning have notably enhanced image controllability and at-\ntribution.\nThis control can be beneficial or harmful.\nA\nrisk is using Instruct-Imagen for malicious activities,\nsuch as creating misleading images of people. Conversely,\nit offers advantages, like reducing image hallucination and\nimproving relevance to user intent. It also benefits minor-\nity communities by effectively generating images of less-\nknown landmarks, foods, and cultural artifacts, addressing\nthe bias in AI systems. To mitigate public risks, we\u2019ll be\ncautious with code and API releases. Future work will fo-\ncus on a responsible use framework, weighing the benefits\nof research transparency against the dangers of open access,\nensuring safe and beneficial usage.\n10\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022. 4\n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2,\n15, 16\n[3] James Betker, Gabriel Goh, Li Jing, Brooks Tim, Jianfeng\nWan, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nand Yunxin Jiao. Improving image generation with better\ncaptions. Technical Report, 2023. 5\n[4] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,\nWanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori,\nand Ludwig Schimdt. VisIT-Bench: A benchmark for vision-\nlanguage instruction following inspired by real-world use.\narXiv preprint arXiv:2308.06595, 2023. 5\n[5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 5\n[6] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-Imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 2, 3, 4, 5\n[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen.\nSubject-\ndriven text-to-image generation via apprenticeship learning.\nIn NeurIPS, 2023. 3, 5, 6, 7, 14, 15\n[8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-\nscaled multilingual language-image model. In ICLR, 2022.\n4, 5, 7, 14\n[9] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 4, 5\n[10] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-\nravit Changpinyo,\nAlan Ritter,\nand Ming-Wei Chang.\nCan pre-trained vision and language models answer vi-\nsual information-seeking questions?\narXiv preprint\narXiv:2302.11713, 2023. 5\n[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 2, 5\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. In NeurIPS, 2021. 13\n[13] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent\nDumoulin, and Jonathon Shlens. Exploring the structure of\na real-time, arbitrary neural artistic stylization network. In\nBMVC, 2017. 5, 7, 14\n[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In CVPR, 2017. 5\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeruIPS, 2020. 2\n[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 2022. 3\n[17] Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal,\nMandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-\nWei Chang. Open-domain visual entity recognition: Towards\nrecognizing millions of wikipedia entities.\narXiv preprint\narXiv:2302.11154, 2023. 5\n[18] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 7\n[19] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196, 2017. 5, 7,\n14, 15\n[20] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wen-\nwen Zhuang, and Wenhu Chen. ImagenHub: Standardizing\nthe evaluation of conditional image generation models. arXiv\npreprint arXiv:2310.01596, 2023. 2, 6, 7, 15\n[21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 7, 15, 16\n[22] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBLIP-\nDiffusion: Pre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 3, 7\n[23] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and\nDeva Ramanan. Photo-Sketching: Inferring contour draw-\nings from images. In WACV, 2019. 5, 14\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 5\n[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nLarge-scale celebfaces attributes (CelebA) dataset. Retrieved\nAugust, 2018. 5, 7, 14, 15\n[26] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. OK-VQA: A visual question answering\nbenchmark requiring external knowledge. In CVPR, 2019. 5\n[27] OpenAI.\nGPT-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 2, 4, 15\n[28] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-G: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023. 5, 7\n[29] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\n11\nRobin Rombach. SDXL: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 7, 8, 10\n[30] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao,\nMasood Dehghan, and Martin Jagersand. Basnet: Boundary-\naware salient object detection. In CVPR, 2019. 5, 14\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 4\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 3\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 5\n[34] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI, 2020. 5, 14\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 4, 5\n[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In MICCAI, 2015. 3\n[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 2, 3, 5, 6, 7, 15, 16\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS, 2022. 2, 3, 4, 5, 10, 13\n[39] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 13\n[40] Noam Shazeer and Mitchell Stern.\nAdafactor: Adaptive\nlearning rates with sublinear memory cost. In ICML, 2018.\n13\n[41] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 2\n[42] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. StyleDrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2, 3, 5, 6, 7, 14, 15, 16\n[43] Hossein Talebi and Peyman Milanfar. Nima: Neural image\nassessment. TIP, 2018. 4\n[44] Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi\nTanaka. Improved artgan for conditional synthesis of natural\nimage and artwork. TIP, 2019. 5, 6, 14, 15\n[45] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. arXiv preprint arXiv:2109.01652, 2021. 2, 5\n[46] Saining \u201dXie and Zhuowen\u201d Tu.\nHolistically-nested edge\ndetection. In ICCV, 2015. 5, 14\n[47] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 5\n[48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 5\n[49] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicBrush: A manually annotated dataset for instruction-\nguided image editing.\narXiv preprint arXiv:2306.10012,\n2023. 7, 8\n[50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2, 3, 5, 7\n[51] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xi-\naofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\nFei Wu, et al. Instruction tuning for large language models:\nA survey. arXiv preprint arXiv:2308.10792, 2023. 5\n[52] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023. 4, 15\n12\nA. Appendix\nIn the appendix, we present details omitted from the main\npaper due to the page limit. In \u00a7 A.1, we first present addi-\ntional image generation results of Instruct-Imagen, both\nquantitatively and qualitatively. In \u00a7 A.2, we then discuss\nthe details of model architecture, the training related de-\ntails of Instruct-Imagen and the inference specification.\nIn \u00a7 A.3, we provide additional details about the retrieval-\naugmented pre-training and multimodal instruction tuning\ndataset.\nA.1. Additional Experimental Results\nA.1.1\nComplete Quantitative Evaluation\nTable 4 shows the complete evaluation results, including the\nbreakdown of semantic consistency and perceptual quality.\nIn addition to the numbers shown in the main paper, we also\nreport the additional average performance over all methods\non in-domain tasks and zero-shot tasks. We observe that\nInstruct-Imagen is better than both the prior methods\nand our proposed baseline methods in most tasks.\nA.1.2\nMore Qualitative Evaluation\nAs aforementioned, Figure 13 presents more in-domain im-\nage generation output from Instruct-Imagen.\nA.2. Implementation Details\nA.2.1\nModel Architecture\nOur base model design is similar to the Imagen [38], with a\nfew key modifications. First, we\u2019ve shifted from a three-\nstage cascaded diffusion model to a two-stage cascaded\ndiffusion model.\nConcretely, in this setup, the text-to-\nimage generation model first produces 128\u00d7128 images\n(instead of the 64\u00d764 in [38]), and then subsequently up-\nsampled to 1024\u00d71024 by only one super-resolution model.\nThis adjustment allows more detailed and information-rich\noutputs from the image generation model. As aforemen-\ntioned, the focus of this work is to adapt and fine-tune\nthe text-to-image generation model to comprehend multi-\nmodal instruction.\nSecondly, rather than employing one\nDBlock / UBlock per-resolution with multiple ResNet-\nBlocks in each DBlock / UBlock, we\u2019ve opted for multi-\nple DBlock / UBlock for each resolution, consistently using\nnumResNetBlocksPerBlock=1. This design choice enables\nus to incorporate more attention layers, a critical aspect for\nour model. Finally, we\u2019ve increased the model size, as elab-\norated below.\nTo process the multi-modal instruction, we repurpose the\ndownsample network within the text-to-image model as an\nencoder to extract latent features from the multi-modal in-\nstruction. These features, derived from the final layer, are\nintegrated into the text-to-image generation model by intro-\nducing a cross-attention layer into each DBlock / UBlock,\nsimilar to the text cross-attention in Imagen [38]. Compre-\nhensive architectural details for both the text-to-image and\nsuper-resolution models can be found in Table 5.\nA.2.2\nOptimization & Inference\nThe model is trained to predict v utilizing the standard\nL2 loss in accordance with [39]. For all experiments, the\nAdafactor optimizer [40] with \u03b21=0.9 and \u03b22=0.999 is\nemployed, maintaining a consistent learning rate of 10\u22124,\nalong with a warm-up phase comprising 10, 000 steps.\nThe model undergoes training for 500k steps in retrieval-\naugmented training and 400k steps in multi-modal in-\nstruction tuning, utilizing a batch size of 512.\nFollow-\ning [12], we utilize the moving average (with weight de-\ncay rate 0.9999) for the model weights used in inference.\nWe use the PaxML2 framework and train the models on\n64 TPUv4. During inference, the sampling schedule re-\nquires 256 timesteps, employing DDPM and cosine noise\nschedule. We employ an oscillating classifier-free guidance\nschedule, alternating between a guidance at the scale of 25.0\nand no guidance every consecutive step.\nA.3. Details on the Training Dataset\nA.3.1\nRetrieval-augmented Training Dataset\nIn the retrieval-augmented training, there are two data sit-\nuations being presented to the model: (1) the model re-\nceives an input of text and a multi-modal context consists\nof several relevant (image, text) pairs, and outputs the\ntarget image. (2) the model receives an input text and out-\nputs the target image (with multi-modal context dropped\nat 10% of chances). The former data situation represents\nthe task of synthesising a given visual concept, using the\ntext and context, whereas the later situation presents the\nconventional text-to-image synthesis. As an outcome, the\ntrained Instruct-Imagen can preserve the capability of\ntext-to-image generation, while learning the new context-\ndependent image generation skill. Please refer to Figure 14\nfor concrete examples from these two learning situations.\nA.3.2\nMulti-modal Instruction-tuning Datasets\nSubsequent to the retrieval-augmented training, we perform\ninstruction-tuning using multi-modal instructions. In this\nwork, we adopt 9 different tasks, which divides into five\ngeneral categories.\nText-to-image Generation. We require the model to gen-\nerate both natural and art images to balance its learning\nof the two domains. To achieve this, we use two datasets\n2https://github.com/google/paxml\n13\nSingle-Task\nMulti-Task\nPrior Mtd.\nInstruct-Imagen\nSCavg\nPQavg\nOverall\nSCavg\nPQavg\nOverall\nSCavg\nPQavg\nOverall\nSCavg\nPQavg\nOverall\nIn-domain Evaluation\nDepth2Img\n0.09\n0.65\n0.24\n0.51\n0.37\n0.44\n0.64\n0.55\n0.59\n0.86\n0.66\n0.75\nMask2Img\n0.79\n0.60\n0.68\n0.67\n0.53\n0.60\n0.50\n0.41\n0.45\n0.87\n0.70\n0.78\nEdge2Img\n0.73\n0.51\n0.61\n0.46\n0.33\n0.39\n0.48\n0.58\n0.53\n0.84\n0.71\n0.77\nSty Gen.\n0.44\n0.46\n0.45\n0.60\n0.70\n0.65\n0.64\n0.71\n0.67\n0.85\n0.92\n0.88\nSub Gen.\n0.69\n0.66\n0.67\n0.53\n0.59\n0.56\n0.69\n0.70\n0.70\n0.81\n0.82\n0.81\nTxt2Img\n0.68\n0.68\n0.68\n0.58\n0.51\n0.55\n0.64\n0.71\n0.67\n0.77\n0.76\n0.76\nFace Gen.\n0.18\n0.77\n0.37\n0.45\n0.34\n0.39\n0.66\n0.80\n0.72\n0.69\n0.86\n0.77\nSty Trans.\n0.43\n0.43\n0.43\n0.00\n0.49\n0.00\n0.58\n0.56\n0.57\n0.55\n0.50\n0.53\nAverage\n0.50\n0.59\n0.52\n0.48\n0.48\n0.45\n0.60\n0.63\n0.61\n0.78\n0.74\n0.76\nZero-shot Evaluation\nSty+Sub\n-\n-\n-\n0.72\n0.32\n0.48\n0.61\n0.18\n0.33\n0.79\n0.43\n0.58\nMulti Sub\n-\n-\n-\n0.73\n0.40\n0.54\n0.65\n0.29\n0.43\n0.77\n0.36\n0.53\nCtrl+Sub\n-\n-\n-\n0.54\n0.24\n0.36\n0.46\n0.23\n0.32\n0.61\n0.59\n0.60\nCtrl+Sty\n-\n-\n-\n0.59\n0.22\n0.36\n0.18\n0.06\n0.11\n0.74\n0.54\n0.63\nAverage\n-\n-\n-\n0.64\n0.30\n0.44\n0.48\n0.19\n0.30\n0.73\n0.48\n0.59\nTable 4. Full evaluation results.\nfor instructed text-to-image generation: an internal high-\nquality natural image dataset with manual caption; and\nan art specific dataset crawled from WikiArt (using the\npipeline in [44]), with the caption generated by PaLI [8].\nNote that the goal of art generation is to not only learn the\nalignment with content description, but also learn the align-\nment between art style description. Figure 15 presents the\nexamples from both datasets, which are augmented with a\nsampled text instruction that summarize the goal of the gen-\neration (whether it is natural image or art generation).\nControl-to-Image Generation. For control-related tasks\n(Figure 16), we use the widely-adopted conditions \u2013 mask,\nedge, and depth.\nThis allows the trained the model to\ncontrol the outputs based on the aforementioned condi-\ntions. Specifically, we use MiDas [34] for depth estima-\ntion, HED [46] for edge extraction, and salient object [30]\nfor mask extraction. We also employed edge-to-image data\nfrom a sketch dataset [23] as additional edge signals. Since\nedge is a very loose definition and can present at many dif-\nferent granularity, we perform the edge augmentation dur-\ning the training. Particularly, we applied edge extraction on\nthe original image, the depth map, and the mask, to obtain\nboth coarse-grained and fine-grained contour images. Ad-\nditionally, we perform image dilation (with random config-\nurations) on the edge map to simulate the edge image data\nwith different thickness. Finally, for different control sig-\nnals, we add different text intructions as prefixes to hint the\nmodel about the scope of the task to the text description of\nthe image content.\nSubject-driven Generation. As aforementioned, we em-\nploy two subject-driven datasets for general objects and face\ngeneration. Particularly, we use the subject-driven dataset\nintroduced in SuTI [7] for general object learning, and the\ncelebrity face datasets [19, 25] to learn face rendering. For\nface rendering, we group the faces of the same person and\ncaption them with PaLI [8], then we use one sampled (im-\nage, text) example as the input text and target image, and\nusing the rest as multi-modal context. Both datasets then\njoin the instruction templates, with reference markers in-\nserted to refer the multi-modal context. Figure 17 provides\na qualitative example of these two constructed datasets.\nStyled Generation. We apply the recipe of StyleDrop [42]\nto fine-tune our backbone cascaded diffusion model (500\nsteps on the 128 \u00d7 128 model) and create data for styled\nimage generation. The outcome model are used to sample\nwith a styled image for a set of text prompts, which gives as\nthe triplet of (style image, text prompts, and styled image)\nin return for Instruct-Imagen training. Note that the text\nprompts used here are sampled from the manual prompts of\nthe aforementioned internal natural image dataset, and the\nstyle images used for fine-tuning is sampled from WikiArt.\nWe employ a CLIP model to filter out examples that fails\nthe alignment with either style image or text content, which\nprovides a total of 100K data in total. Then we create the\nmulti-modal instructions via combining the instruction tem-\nplate with style image and the manual caption, such that the\nstyle image is correctly referred. Figure 18 (a) presents an\nexample of the style-to-image generation data.\nStyle Transfer. Similarly, we construct the style transfer\ndataset via combining style images from our WikiArt crawl\nand content images from the internal dataset (with the cap-\ntions discarded). We use a style transfer model [13] based\non the backbone cascaded diffusion model, which allows\nfast and large-scale generation, to blend the style image\n14\nText-to-Image\nSuper-Resolution\nModel size\n2.76B\n581M\nDBlock-1\nResolution\n128 \u2192 64\n1024 \u2192 512\n#Blocks\n8\n2\nOutChannels\n512\n128\nAttention\n-\n-\nDBlock-2\nResolution\n64 \u2192 32\n512 \u2192 256\n#Blocks\n8\n4\nOutChannels\n1024\n256\nAttention\n-\n-\nDBlock-3\nResolution\n32 \u2192 16\n256 \u2192 128\n#Blocks\n8\n8\nOutChannels\n2048\n512\nAttention\nText Instr +\n-\nMulti-modal Ctx\nDBlock-4\nResolution\n|\n128 \u2192 64\n#Blocks\n|\n8\nOutChannels\n|\n1024\nAttention\n\u2193\nText Instr.\nUBlock-4\nResolution\n|\n64 \u2192 128\n#Blocks\n|\n8\nOutChannels\n|\n512\nAttention\n\u2193\nText Instr.\nUBlock-3\nResolution\n16 \u2192 32\n128 \u2192 256\n#Blocks\n8\n8\nOutChannels\n1024\n256\nAttention\nText Instr +\n-\nMulti-modal Ctx\nUBlock-2\nResolution\n32 \u2192 64\n256 \u2192 512\n#Blocks\n8\n4\nOutChannels\n512\n128\nAttention\n-\n-\nUBlock-1\nResolution\n64 \u2192 128\n512 \u2192 1024\n#Blocks\n8\n2\nOutChannels\n3\n3\nAttention\n-\n-\nTable 5.\nModel architecture of the Backbone U-Network.\nNote that the Text-to-Image network do not have DBlock-4 and\nUBlock-4.\nwith the content image. Note that in the style transfer task,\nlanguage is not providing any information about the content\nof the target image, so the model needs to referring fully to\nthe content image to extract semantic information of the tar-\nget image output. Figure 18 (b) presents an example of the\nstyle transfer data.\nInstruction Template Generation. As aforementioned, we\nprompted the GPT-4 [27] to generate 100 rephrased instruc-\ntion templates with high variation, and validated the seman-\ntic correctness of them manually. During the instruction\ncreation, we use the placeholders in the place where multi-\nmodal contexts are suppose to be inserted, and populate the\nreference marker (and its associative short prompt) when\nthe instruction is going to be added to each particular data.\nFor example, in subject driven generation, one template\ncould be \u201cGenerate an image of [placeholder], using\nthe caption:\u201d, where the placeholder would be substituted\nwith the subject prompt and reference \u201c[ref#1] a dog\u201d.\nNote that the reference marker corresponds to a special to-\nkens in the language embedding model.\nA.4. Details on the Evaluation Datasets\nAs aforementioned, we now describe the details of both\nadopted and constructed evaluation benchmark, with more\ndetails. To facilitate the comparison with future methods,\nwe make the dataset construction scripts for our evaluation\ndatasets publicly available.\nA.4.1\nIn-domain Evaluation Datasets\nFor in-domain tasks, we tried to re-use existing datasets and\nevaluation protocols to evaluate different approaches. For\ntasks that does not have a standardized evaluation, we con-\nstruct our own evaluation data, based on the CustomCon-\ncept101 [21] dataset. The details of these evaluation are\ndescribed as follows.\nText2Image Generation. We adopt the text prompts used\nin ImagenHub [20] for its comprehensiveness quality eval-\nuation on text-to-image generation (a total of 197 prompts).\nControl2Image Generation. We randomly sample images\nfrom the CustomConcept101 [52] and use those as source\nimages to extract control signals as condition, and gener-\nated the text prompts using PaLM2 [2] for evaluating con-\ntrol2image generation. This process is repeated for each\ncontrol signal (i.e., edge, mask, depth), to produce 100\n(control image, prompt text) pairs per control signal,\nwhich adds up to a total of 300 pairs of examples.\nStyled Image Generation. We adopt the style images and\ntext prompts from a subset of the evaluation dataset used in\nthe StyleDrop [42]. The dataset consists of 12 text prompts\nthat describe the image content and 10 acquired style im-\nages, which leads to a total of 120 pairs of examples.\nSubject-driven Image Generation. We adopt a subset of\nthe DreamBench v1 and v2 datasets [7, 37] to serve as eval-\nuation for subject-driven image generation, which consists\nof a total of 220 pairs of (subject images, prompt text).\nIn-context Face Generation. We use the hold-out people\nimages from the validation split of the Celeb-A [25] and\nCeleb-HQ [19] dataset for in-context face generation. The\nresulting dataset consists of 100 samples.\nStyle Transfer. We use the hold-out painting images from\nthe WikiArt website (re-crawled using the tool by [44],\nsee dataset description in main text) and the content im-\nages from the CustomConcept101 [21] dataset to form\n15\nthe (style image, content image) pairs. The resulting\ndataset consists of 455 samples.\nA.4.2\nZero-shot Compositional Evaluation Datasets\nFor zero-shot tasks, we either adopt the existing evaluation\n(i.e., multi-subject evaluation using a subset of 100 exam-\nples on CustomConcept101 [21]) or construct the evalua-\ntion ourselves (e.g., subject + control, style + control, style\n+ subject) by adopting images from corresponding datasets.\nThe details are described as what follows.\nStyle & Subject Conditioned Image Generation.\nWe\nadopt the 100 subjects in the CustomConcept101 [21] and\nconstruct the corresponding stylization text prompts (using\nPaLM2 [2] model) based on the selected subjects. A total of\n20 style images from aforementioned datasets are adopted,\n10 from the StyleDrop dataset [42] and 10 from the hold-out\nWikiArt dataset. A total of 132 triplets of such (subject\nimages, style image, text prompt) for evaluation.\nMulti-Subject Conditioned Image Generation. We adopt\na subset of the (subject A, subject B, text prompt)\ntriplets from the CustomConcept101 [21] multi-subject\ndataset, for the task multi-subject generation. The result-\ning dataset consists of a total of 120 such triplets.\nSubject & Control Conditioned Image Generation. We\nadopt the subject images in the CustomConcept101 [21]\ndataset. Specifically, We select one of the reference images\nfor computing the control signals, and use the remaining\nimages as the reference images for the subjects. We used a\nmixture of three controls in this dataset \u2013 edge, mask, and\ndepth, supplemented with PaLM2 generated text prompts,\nwith the goal of re-creating those subjects in new con-\ntext. The resulting dataset contains a total of 99 (control\nimage, subject images, text prompt) triplets.\nStyle & Control Conditioned Image Generation. Simi-\nlarly, we adopt the 100 subject images in the CustomCon-\ncept101 [37] dataset as the source images for generating\ncontrol signals. We then construct text prompts to describe\nthe selected subjects in new context, and in the visual style\nof corresponding style images (using the PaLM2 [2]). Par-\nticularly, we re-used the 20 style images in the style & sub-\nject evaluation. The resulting dataset contains a total of 100\n(control image, style image, text prompt) triplets.\n16\nMulti-modal Instruction\nInstruct-Imagen\nMulti-modal Instruction\nInstruct-Imagen\nGenerate an image about the \n[ref#1] shiny sneaker, following \nthe caption. A shiny sneaker \nstepping onto a rugged trail\n[ref#1]  shiny sneaker\nGenerate an image following the \ndescription: image of a white \nwooden sphere \ufb02oating in water.\nCreate an image aligned with the \n[ref#1] mask, and following the \ndescription: a houseplant in a \nwoven basket on a farmhouse \nporch. \n[ref#1] mask\nGenerate an image as outlined by \nthe [ref#1] edge, and re\ufb02ect the \ncaption: plushie dices in a child's \ntreasure chest. \n[ref#1] edge\nBased on the [ref#1] depth map, \ncreate an image to re\ufb02ect the \ncaption: a car with a custom \ufb02ame \npaint job.\n[ref#2] depth map\nGenerate an image in [ref#1] 3D \nstyle following the caption: A \n\ufb02uffy panda bear munching on \nbamboo shoots.\n[ref#1] 3D style \nDraw a picture in the given \n[ref#1] style, following the \nspeci\ufb01ed [ref#2] content. \n[ref#1] style\n[ref#2] content\nSubject Generation\nText-to-Image\nMask-to-Image\nGenerate an image based on the \ntext: A wine glass on top of a dog.\nGenerate an image using the \n[ref#1] mask, and following the \ncaption: a castle ruins overgrown \nwith ivy and wild\ufb02owers\n[ref#1] mask\nCreate an image aligned with the \n[ref#1] edge map, and following \nthe description: a pink plushie in \na princess-themed bedroom\n[ref#1] edge map\nUsing the [ref#1] depth map, \ngenerate an image to re\ufb02ect the \ncaption: a purse in a college \nlibrary. \n[ref#2] depth map\nCreate a [ref#1] dog image using \nthe description: a dog reading a \nbook with a pink glasses on\n[ref#1]  dog\nCreate an image in [ref#1] crayon \ndrawing style with the caption: A \nlone cabin perched on a snowy \nmountain peak.\n[ref#1] style image\n[ref#1] style\n[ref#2] content\nConvert the artistic style of \n[ref#1] style image to the [ref#2] \ncontent image. \nEdge-to-Image\nDepth-to-Image\nStyled Generation\nStyle Transfer\nFigure 13. Additional Qualitative Evaluation on Instruct-Imagen for In-domain Tasks. We do not visualize the outputs of the face\ngeneration task due to lack of consent from the original persons.\n17\nPrompt\nContext\nTarget\nMulti-modal Context\nVibrant and colorful sunset\ncaptured at Marshalls Beach,\nSan Francisco California\nFernando de Noronha\ndusk of storm on\nBrazil, . . .\nthe beach of Laga. . .\nContext Dropout\nSeedless watermelon slices\non a table selective focus.\nFigure 14. Data for Retrieval-Augmented Training. We present two training situations: (1) the case where multi-modal context are\npresented to the model when generating the image; and (2) the case where multi-modal context are dropped during the training.\nInstruction\nTarget\n(a) Natural Images\nBring forth an image based on the caption,\nBritish short hair cat and golden retriever.\n(b) Art Images\nGenerate this artwork:\nThe Triumph of Hope,\nan allegorical painting by Erasmus Quellinus\nThe Younger in the Baroque style.\nFigure 15. Text-to-Image Data for Instruction-Tuning..\n18\nInstruction\nContext\nTarget\n(a) Depth\nCreate an image using\n[ref#1] depth map as a reference\nand following the below description:\nA black and white puppy in a sunflower field.\n[ref#1] depth map\n(b) Mask\nGenerate an image by taking cues from\n[ref#1] object mask as a reference\nand following this caption:\nA pizza on top of a mountain peak.\n[ref#1] object mask\n(b) Edge\nLet [ref#1] edge image guide\nyou in crafting an image\nthat fulfills this description -\nA stuffed animal on a beach blanket.\n[ref#1] edge image\nFigure 16. Control-Related Data for Instruction-Tuning.\nInstruction\nContext\nTarget\n(a) General Subjects\nSynthesize an image that\nintegrates the caption\u2019s meaning,\nfeaturing [ref#1] A stack of towels:\nA stack of towels on a sandy beach.\n[ref#1] A stack of towels\n(b) Faces\nProduce a facial image with [ref#1] reference\nimage and reflects the caption:\nA female with long black hair in a tight\nbraid is smiling and looking interested.\nreference image [ref#1]\nFigure 17. Subject-Related Data for Instruction-Tuning. The face image is anonymized to protect the privacy.\n19\nInstruction\nContext 1\nContext 2\nTarget\n(b) Style-to-Image\nCreate an image using\n[ref#1] Realism style\nin tune with the caption\nBeautiful pink Lily\nflower in the pond\nin the national Park.\n[ref#1] Realism\nGenerate an image in\n[ref#1] Tonalism style\nfollowing the caption:\nMany people walking around\nat a fruit market.\n[ref#1] Tonalism\n(c) Style Transfer\nRecreate the content of\n[ref#2] content image\nusing the style of\n[ref#1] Symbolism.\n[ref#1] Symbolism\n[ref#2] content image\nFigure 18. Style-Related Data for Instruction-Tuning.\n20\n"
  },
  {
    "title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation",
    "link": "https://arxiv.org/pdf/2401.02117.pdf",
    "upvote": "25",
    "text": "January 2024\nMobile ALOHA:\nLearning Bimanual Mobile Manipulation with\nLow-Cost Whole-Body Teleoperation\nZipeng Fu*1, Tony Z. Zhao*1, Chelsea Finn1\n*project co-leads, 1Stanford University\nhttps://mobile-aloha.github.io\nuse cabinet\ncook shrimp\npush chairs\nLearned Policies\nwipe wine\nhigh \ufb01ve\ncall elevator\nFigure 1: Mobile ALOHA\n. We introduce a low-cost mobile manipulation system that is bimanual and supports\nwhole-body teleoperation. The system costs $32k including onboard power and compute. Left: A user teleoperates to\nobtain food from the fridge. Right: Mobile ALOHA can perform complex long-horizon tasks with imitation learning.\nAbstract\nImitation learning from human demonstrations has\nshown impressive performance in robotics. How-\never, most results focus on table-top manipulation,\nlacking the mobility and dexterity necessary for gen-\nerally useful tasks. In this work, we develop a system\nfor imitating mobile manipulation tasks that are bi-\nmanual and require whole-body control. We first\npresent Mobile ALOHA, a low-cost and whole-body\nteleoperation system for data collection. It augments\nthe ALOHA system [104] with a mobile base, and a\nwhole-body teleoperation interface. Using data col-\nlected with Mobile ALOHA, we then perform super-\nvised behavior cloning and find that co-training with\nexisting static ALOHA datasets boosts performance\non mobile manipulation tasks. With 50 demonstra-\ntions for each task, co-training can increase success\nrates by up to 90%, allowing Mobile ALOHA to au-\ntonomously complete complex mobile manipulation\ntasks such as sauteing and serving a piece of shrimp,\nopening a two-door wall cabinet to store heavy cook-\ning pots, calling and entering an elevator, and lightly\nrinsing a used pan using a kitchen faucet.\n1. Introduction\nImitation learning from human-provided demonstra-\ntions is a promising tool for developing generalist\nrobots, as it allows people to teach arbitrary skills\nto robots. Indeed, direct behavior cloning can en-\nable robots to learn a variety of primitive robot skills\nranging from lane-following in mobile robots [67],\nto simple pick-and-place manipulation skills [12, 20]\nto more delicate manipulation skills like spreading\npizza sauce or slotting in a battery [18, 104]. How-\never, many tasks in realistic, everyday environments\nrequire whole-body coordination of both mobility\nand dexterous manipulation, rather than just individ-\nual mobility or manipulation behaviors. For example,\nconsider the relatively basic task of putting away a\nheavy pot into a cabinet in Figure 1. The robot needs\nto first navigate to the cabinet, necessitating the mo-\nbility of the robot base. To open the cabinet, the robot\nneeds to back up while simultaneously maintaining\na firm grasp of the two door handles, motivating\nwhole-body control. Subsequently, both arms need\nto grasp the pot handles and together move the pot\ninto the cabinet, emphasizing the importance of bi-\nmanual coordination. Along a similar vein, cooking,\ncleaning, housekeeping, and even simply navigating\nan office using an elevator all require mobile manip-\nulation and are often made easier with the added\nflexibility of two arms. In this paper, we study the\nfeasibility of extending imitation learning to tasks\nthat require whole-body control of bimanual mobile\n1\narXiv:2401.02117v1  [cs.RO]  4 Jan 2024\nMobile ALOHA: https://mobile-aloha.github.io\nrobots.\nTwo main factors hinder the wide adoption of\nimitation learning for bimanual mobile manipula-\ntion. (1) We lack accessible, plug-and-play hardware\nfor whole-body teleoperation. Bimanual mobile ma-\nnipulators can be costly if purchased off-the-shelf.\nRobots like the PR2 and the TIAGo can cost more\nthan $200k USD, making them unaffordable for typi-\ncal research labs. Additional hardware and calibra-\ntion are also necessary to enable teleoperation on\nthese platforms. For example, the PR1 uses two hap-\ntic devices for bimanual teleoperation and foot ped-\nals to control the base [93]. Prior work [5] uses a\nmotion capture system to retarget human motion to\na TIAGo robot, which only controls a single arm and\nneeds careful calibration. Gaming controllers and\nkeyboards are also used for teleoperating the Hello\nRobot Stretch [2] and the Fetch robot [1], but do not\nsupport bimanual or whole-body teleoperation. (2)\nPrior robot learning works have not demonstrated\nhigh-performance bimanual mobile manipulation\nfor complex tasks. While many recent works demon-\nstrate that highly expressive policy classes such as\ndiffusion models and transformers can perform well\non fine-grained, multi-modal manipulation tasks, it\nis largely unclear whether the same recipe will hold\nfor mobile manipulation: with additional degrees of\nfreedom added, the interaction between the arms\nand base actions can be complex, and a small devia-\ntion in base pose can lead to large drifts in the arm\u2019s\nend-effector pose. Overall, prior works have not\ndelivered a practical and convincing solution for bi-\nmanual mobile manipulation, both from a hardware\nand a learning standpoint.\nWe seek to tackle the challenges of applying imi-\ntation learning to bimanual mobile manipulation in\nthis paper. On the hardware front, we present Mobile\nALOHA, a low-cost and whole-body teleoperation\nsystem for collecting bimanual mobile manipulation\ndata. Mobile ALOHA extends the capabilities of the\noriginal ALOHA , the low-cost and dexterous biman-\nual puppeteering setup [104], by mounting it on a\nwheeled base. The user is then physically tethered to\nthe system and backdrives the wheels to enable base\nmovement. This allows for independent movement\nof the base while the user has both hands controlling\nALOHA . We record the base velocity data and the\narm puppeteering data at the same time, forming a\nwhole-body teleoperation system.\nOn the imitation learning front, we observe that\nsimply concatenating the base and arm actions then\ntraining via direct imitation learning can yield strong\nperformance. Specifically, we concatenate the 14-\nDoF joint positions of ALOHA with the linear and\nangular velocity of the mobile base, forming a 16-\ndimensional action vector. This formulation allows\nMobile ALOHA to benefit directly from previous deep\nimitation learning algorithms, requiring almost no\nchange in implementation. To further improve the\nimitation learning performance, we are inspired by\nthe recent success of pre-training and co-training on\ndiverse robot datasets, while noticing that there are\nfew to none accessible bimanual mobile manipula-\ntion datasets. We thus turn to leveraging data from\nstatic bimanual datasets, which are more abundant\nand easier to collect, specifically the static ALOHA\ndatasets from [81, 104] through the RT-X release\n[20]. It contains 825 episodes with tasks disjoint\nfrom the Mobile ALOHA tasks, and has different\nmounting positions of the two arms. Despite the\ndifferences in tasks and morphology, we observe\npositive transfer in nearly all mobile manipulation\ntasks, attaining equivalent or better performance\nand data efficiency than policies trained using only\nMobile ALOHA data. This observation is also con-\nsistent across different class of state-of-the-art imi-\ntation learning methods, including ACT [104] and\nDiffusion Policy [18].\nThe main contribution of this paper is a system\nfor learning complex mobile bimanual manipulation\ntasks. Core to this system is both (1) Mobile ALOHA,\na low-cost whole-body teleoperation system, and (2)\nthe finding that a simple co-training recipe enables\ndata-efficient learning of complex mobile manipu-\nlation tasks. Our teleoperation system is capable of\nmultiple hours of consecutive usage, such as cook-\ning a 3-course meal, cleaning a public bathroom, and\ndoing laundry. Our imitation learning result also\nholds across a wide range of complex tasks such\nas opening a two-door wall cabinet to store heavy\ncooking pots, calling an elevator, pushing in chairs,\nand cleaning up spilled wine. With co-training, we\nare able to achieve over 80% success on these tasks\nwith only 50 human demonstrations per task, with\nan average of 34% absolute improvement compared\nto no co-training.\n2. Related Work\nMobile Manipulation. Many current mobile ma-\nnipulation systems utilize model-based control,\nwhich involves integrating human expertise and\ninsights into the system\u2019s design and architec-\nture [9, 17, 33, 52, 93]. A notable example of model-\nbased control in mobile manipulation is the DARPA\nRobotics Challenge [56]. Nonetheless, these systems\ncan be challenging to develop and maintain, often\n2\nMobile ALOHA: https://mobile-aloha.github.io\nwrist cameras\ntop camera\n100cm\n200cm\n65cm\n#Dofs\nWeight\nSize\nPayload\nArm Repeatability\nArm Accuracy\nBattery life\nMax pulling force\nRolling resistance\n14 (arms) + 2 (base)\n75kg\n80W * 84L * 140H (no leaders)\n90W * 135L * 140H\n750g (per arm) 55kg (base)\n1mm\n5-8mm\n12 hours (1620Wh)\n100N at 100cm vertically\n13N (vinyl floor)\nbattery pack \nlaptop\noperator\nFigure 2: Hardware Details. Left: Mobile ALOHA has two wrist cameras and one top camera, with onboard power and\ncompute. Middle: The teleoperation setup can be removed and only two ViperX 300 [3] are used during autonomous\nexecution. Both arms can reach a min/max height of 65cm/200cm, and extends 100cm from the base. Right: Technical\nspecifications of Mobile ALOHA .\nrequiring substantial team efforts, and even minor\nerrors in perception modeling can result in signifi-\ncant control failures [6, 51]. Recently, learning-based\napproaches have been applied to mobile manipu-\nlation, alleviating much of the heavy engineering.\nIn order to tackle the exploration problem in high-\ndimensional state and action spaces of mobile ma-\nnipulation tasks, prior works use predefined skill\nprimitives [86, 91, 92], reinforcement learning with\ndecomposed action spaces [38, 48, 58, 94, 101], or\nwhole-body control objectives [36, 42, 99]. Unlike\nthese prior works that use action primitives, state\nestimators, depth images or object bounding boxes,\nimitation learning allows mobile manipulators to\nlearn end-to-end by directly mapping raw RGB ob-\nservations to whole-body actions, showing promis-\ning results through large-scale training using real-\nworld data [4, 12, 78] in indoor environments [39, 78].\nPrior works use expert demonstrations collected by\nusing a VR interface [76], kinesthetic teaching [100],\ntrained RL policies [43], a smartphone interface [90],\nmotion capture systems [5], or from humans [8].\nPrior works also develop humanoid teleoperation by\nusing human motion capture suits [19, 22, 23, 26],\nexoskeleton [32, 45, 72, 75], VR headsets for visual\nfeedbacks [15, 53, 65, 87], and haptic feedback de-\nvices [14, 66]. Purushottam et al. develop an ex-\noskeleton suit attached to a force plate for whole-\nbody teleoperation of a wheeled humanoid, However,\nthere is no low-cost solution to collecting whole-\nbody expert demonstrations for bimanual mobile\nmanipulation. We present Mobile ALOHA for this\nproblem. It is suitable for hour-long teleoperation,\nand does not require a FPV goggle for streaming\nback videos from the robot\u2019s egocentric camera or\nhaptic devices.\nImitation Learning for Robotics. Imitation learn-\ning enables robots to learn from expert demon-\nstrations [67]. Behavioral cloning (BC) is a sim-\nple version, mapping observations to actions. En-\nhancements to BC include incorporating history\nwith various architectures [12, 47, 59, 77], new\ntraining objectives [10, 18, 35, 63, 104], regulariza-\ntion [71], motor primitives [7, 44, 55, 62, 64, 97],\nand data preprocessing [81]. Prior works also fo-\ncus on multi-task or few-shot imitation learning,\n[25, 27, 30, 34, 46, 50, 88, 102], language-conditioned\nimitation learning [12, 47, 82, 83], imitation from\nplay data [21, 57, 74, 89], using human videos [16,\n24, 29, 60, 69, 80, 84, 96], and using task-specific\nstructures [49, 83, 103]. Scaling up these algorithms\nhas led to systems adept at generalizing to new ob-\njects, instructions, or scenes [12, 13, 28, 47, 54]. Re-\ncently, co-training on diverse real-world datasets\ncollected from different but similar types of robots\nhave shown promising results on single-arm manip-\nulation [11, 20, 31, 61, 98], and on navigation [79].\nIn this work, we use a co-training pipeline for bi-\nmanual mobile manipulation by leveraging the ex-\nisting static bimanual manipulation datasets, and\nshow that our co-training pipeline improves the per-\nformance and data efficiency of mobile manipula-\ntion policies across all tasks and several imitation\nlearning methods. To our knowledge, we are the\nfirst to find that co-training with static manipula-\ntion datasets improves the performance and data\nefficiency of mobile manipulation policies.\n3. Mobile ALOHA Hardware\nWe develop Mobile ALOHA, a low-cost mobile manip-\nulator that can perform a broad range of household\ntasks. Mobile ALOHA inherits the benefits of the\noriginal ALOHA system [104], i.e. the low-cost, dex-\n3\nMobile ALOHA: https://mobile-aloha.github.io\nterous, and repairable bimanual teleoperation setup,\nwhile extending its capabilities beyond table-top ma-\nnipulation. Specifically, we incorporate four key\ndesign considerations:\n1. Mobile: The system can move at a speed com-\nparable to human walking, around 1.42m/s.\n2. Stable: It is stable when manipulating heavy\nhousehold objects, such as pots and cabinets.\n3. Whole-body teleoperation: All degrees of\nfreedom can be teleoperated simultaneously,\nincluding both arms and the mobile base.\n4. Untethered: Onboard power and compute.\nWe choose AgileX Tracer AGV (\"Tracer\") as the\nmobile base following considerations 1 and 2. Tracer\nis a low-profile, differential drive mobile base de-\nsigned for warehouse logistics. It can move up to\n1.6m/s similar to average human walking speed.\nWith a maximum payload of 100kg and 17mm height,\nwe can add a balancing weight low to the ground\nto achieve the desired tip-over stability. We found\nTracer to possess sufficient traversability in acces-\nsible buildings: it can traverse obstacles as tall as\n10mm and slopes as steep as 8 degrees with load,\nwith a minimum ground clearance of 30mm. In prac-\ntice, we found it capable of more challenging ter-\nrains such as traversing the gap between the floor\nand the elevator. Tracer costs $7,000 in the United\nStates, more than 5x cheaper than AGVs from e.g.\nClearpath with similar speed and payload.\nWe then seek to design a whole-body teleopera-\ntion system on top of the Tracer mobile base and\nALOHA arms, i.e. a teleoperation system that allows\nsimultaneous control of both the base and the two\narms (consideration 3). This design choice is particu-\nlarly important in household settings as it expands\nthe available workspace of the robot. Consider the\ntask of opening a two-door cabinet. Even for humans,\nwe naturally step back while opening the doors to\navoid collision and awkward joint configurations.\nOur teleoperation system shall not constrain such\ncoordinated human motion, nor introduce unnec-\nessary artifacts in the collected dataset. However,\ndesigning a whole-body teleoperation system can\nbe challenging, as both hands are already occupied\nby the ALOHA leader arms. We found the design of\ntethering the operator\u2019s waist to the mobile base to\nbe the most simple and direct solution, as shown in\nFigure 2 (left). The human can backdrive the wheels\nwhich have very low friction when torqued off. We\nmeasure the rolling resistance to be around 13N on\nvinyl floor, acceptable to most humans. Connecting\nthe operator to the mobile manipulator directly also\nenables coarse haptic feedback when the robot col-\nlides with objects. To improve the ergonomics, the\nheight of the tethering point and the positions of\nthe leader arms can all be independently adjusted\nup to 30cm. During autonomous execution, the teth-\nering structure can also be detached by loosening 4\nscrews, together with the two leader arms. This re-\nduces the footprint and weight of the mobile manip-\nulator as shown in Figure 2 (middle). To improve the\nergonomics and expand workspace, we also mount\nthe four ALOHA arms all facing forward, different\nfrom the original ALOHA which has arms facing\ninward.\nTo make our mobile manipulator untethered (con-\nsideration 4), we place a 1.26kWh battery that\nweights 14kg at the base. It also serves as a bal-\nancing weight to avoid tipping over. All compute\nduring data collection and inference is conducted\non a consumer-grade laptop with Nvidia 3070 Ti\nGPU (8GB VRAM) and Intel i7-12800H. It accepts\nstreaming from three Logitech C922x RGB webcams,\nat 480x640 resolution and 50Hz. Two cameras are\nmounted to the wrist of the follower robots, and\nthe third facing forward. The laptop also accepts\nproprioception streaming from all 4 arms through\nUSB serial ports, and from the Tracer mobile base\nthrough CAN bus. We record the linear and angular\nvelocities of the mobile base to be used as actions of\nthe learned policy. We also record the joint positions\nof all 4 robot arms to be used as policy observations\nand actions. We refer readers to the original ALOHA\npaper [104] for more details about the arms.\nWith design considerations above, we build Mo-\nbile ALOHA with a $32k budget, comparable to a\nsingle industrial cobot such as the Franka Emika\nPanda. As illustrated in Figure 2 (middle), the mo-\nbile manipulator can reach between 65cm and 200cm\nvertically relative to the ground, can extend 100cm\nbeyond its base, can lift objects that weight 1.5kg,\nand can exert pulling force of 100N at a height of\n1.5m. Some example tasks that Mobile ALOHA is\ncapable of includes:\n\u2022 Housekeeping: Water plants, use a vacuum, load\nand unload a dishwasher, obtain drinks from the\nfridge, open doors, use washing machine, fling and\nspread a quilt, stuff a pillow, zip and hang a jacket,\nfold trousers, turn on/off a lamp, and self-charge.\n\u2022 Cooking: Crack eggs, mince garlic, unpackage\nvegetables, pour liquid, sear and flip chicken thigh,\nblanch vegetables, stir fry, and serve food in a dish.\n\u2022 Human-robot interactions: Greet and shake\n\u201chands\u201d with a human, open and hand a beer to\nhuman, help human shave and make bed.\nWe include more technical specifications of Mo-\n4\nMobile ALOHA: https://mobile-aloha.github.io\nbile ALOHA in Figure 2 (right). Beyond the off-the-\nshelf robots, we open-source all of the software and\nhardware parts with a detailed tutorial covering 3D\nprinting, assembly, and software installation. The\ntutorial is on the project website.\n4. Co-training with\nStatic ALOHA Data\nThe typical approach for using imitation learning to\nsolve real-world robotics tasks relies on using the\ndatasets that are collected on a specific robot hard-\nware platform for a targeted task. This straightfor-\nward approach, however, suffers from lengthy data\ncollection processes where human operators collect\ndemonstration data from scratch for every task on\nthe a specific robot hardware platform. The policies\ntrained on these specialized datasets are often not ro-\nbust to the perceptual perturbations (e.g. distractors\nand lighting changes) due to the limited visual diver-\nsity in these datasets [95]. Recently, co-training on\ndiverse real-world datasets collected from different\nbut similar types of robots have shown promising\nresults on single-arm manipulation [11, 20, 31, 61],\nand on navigation [79].\nIn this work, we use a co-training pipeline that\nleverages the existing static ALOHA datasets to im-\nprove the performance of imitation learning for mo-\nbile manipulation, specifically for the bimanual arm\nactions. The static ALOHA datasets [81, 104] have\n825 demonstrations in total for tasks including Ziploc\nsealing, picking up a fork, candy wrapping, tearing a\npaper towel, opening a plastic portion cup with a lid,\nplaying with a ping pong, tape dispensing, using a\ncoffee machine, pencil hand-overs, fastening a velcro\ncable, slotting a battery, and handling over a screw\ndriver. Notice that the static ALOHA data is all col-\nlected on a black table-top with the two arms fixed\nto face towards each other. This setup is different\nfrom Mobile ALOHA where the background changes\nwith the moving base and the two arms are placed in\nparallel facing the front. We do not use any special\ndata processing techniques on either the RGB obser-\nvations or the bimanual actions of the static ALOHA\ndata for our co-training.\nDenote the aggregated static ALOHA data as as\nDstatic, and the Mobile ALOHA dataset for a task m as\nDm\nmobile. The bimanual actions are formulated as tar-\nget joint positions aarms \u2208 R14 which includes two\ncontinuous gripper actions, and the base actions are\nformulated as target base linear and angular veloci-\nties abase \u2208 R2. The training objective for a mobile\nmanipulation policy \u03c0m for a task m is\nE(oi,aiarms,ai\nbase)\u223cDm\nmobile\n\u0002\nL(ai\narms, ai\nbase, \u03c0m(oi))\n\u0003\n+\nE(oi,aiarms)\u223cDstatic\n\u0002\nL(ai\narms, [0, 0], \u03c0m(oi))\n\u0003\n,\nwhere oi is the observation consisting of two wrist\ncamera RGB observations, one egocentric top cam-\nera RGB observation mounted between the arms,\nand joint positions of the arms, and L is the imi-\ntation loss function. We sample with equal prob-\nability from the static ALOHA data Dstatic and the\nMobile ALOHA data Dm\nmobile. We set the batch size\nto be 16. Since static ALOHA datapoints have no\nmobile base actions, we zero-pad the action labels\nso actions from both datasets have the same dimen-\nsion. We also ignore the front camera in the static\nALOHA data so that both datasets have 3 cameras.\nWe normalize every action based on the statistics\nof the Mobile ALOHA dataset Dm\nmobile alone. In our\nexperiments, we combine this co-training recipe\nwith multiple base imitation learning approaches,\nincluding ACT [104], Diffusion Policy [18], and\nVINN [63].\n5. Tasks\nWe select 7 tasks that cover a wide range of capa-\nbilities, objects, and interactions that may appear in\nrealistic applications. We illustrate them in Figure 3.\nFor Wipe Wine, the robot needs to clean up spilled\nwine on the table. This task requires both mobil-\nity and bimanual dexterity. Specifically, the robot\nneeds to first navigate to the faucet and pick up the\ntowel, then navigate back to the table. With one arm\nlifting the wine glass, the other arm needs to wipe\nthe table as well as the bottom of the glass with the\ntowel. This task is not possible with static ALOHA,\nand would take more time for a single-armed mobile\nrobot to accomplish.\nFor Cook Shrimp, the robot sautes one piece of raw\nshrimp on both sides before serving it in a bowl. Mo-\nbility and bimanual dexterity are also necessary for\nthis task: the robot needs to move from the stove to\nthe kitchen island as well as flipping the shrimp with\nspatula while the other arm tilting the pan. This task\nrequires more precision than wiping wine due to the\ncomplex dynamics of flipping a half-cooked shrimp.\nSince the shrimp may slightly stick to the pan, it is\ndifficult for the robot to reach under the shrimp with\nthe spatula and precisely flip it over.\nFor Rinse Pan, the robot picks up a dirty pan and\nrinse it under the faucet before placing it on the\ndrying rack. In addition to the challenges in the pre-\nvious two tasks, turning on the faucet poses a hard\nperception challenge. The knob is made from shiny\n5\nMobile ALOHA: https://mobile-aloha.github.io\nWipe Wine: The robot base is initialized within a square of 1.5m x 1.5m with yaw up to 30\u00b0. It first navigates to the sink and picks up the towel \nhanging on the faucet (#1). It then turns around and approaches the kitchen island, picks up the wine glass (randomized in 30cm x 30cm), wipes \nthe spilled wine (#2), and puts down the wine glass on the table (#3). Each demo has 1300 steps or 26 seconds.\nCook Shrimp: The robot is randomized up to 5cm and all objects up to 2cm. The right gripper first pours oil into the hot pan (#1) followed by \nraw shrimp (#2). With left gripper lifting the pan at an angle, the right gripper grasps the spatula and flips the shrimp (#3). The robot then turns \naround and pours the shrimp into an empty bowl (#4) before placing the pan on the table. Each demo has 3750 steps or 75 seconds.\nWash Pan: The pan randomized up to 10cm with yaw up to 45\u00b0. The left gripper grasps the pan (#1) before turning around to the faucet. The \nright gripper opens then closes the faucet with left gripper holding the pan to receive the water (#2). The left gripper then swirls the water inside \nthe pan, pours it out, before placing the pan on the rack (#3). Each demo has 1100 steps or 22 seconds.\nUse Cabinet: The robot is randomized up to 10cm and pots up to 5cm. A total of 3 pots are used. The robot first approaches the cabinet and \ngrasp both handles, then backs up pulling both doors open (#1). Next, both arms grasp the handles of the pot, move forward, and place it inside \nthe cabinet (#2). The robot then backs up and closes both cabinet doors (#4). Each demo has 1500 steps or 30 seconds.\nTake Elevator: The robot starts 15m from the elevator and is randomized across the 10m wide lobby. The robot goes around a column to reach \nthe elevator button (#1). The right gripper presses the button (#2) and the robot enters the elevator (#3). Each demo has 2250 steps or 45 seconds.\n#1\n#2\n#3\ninit.\nPush Chairs: The robot\u2019s initial position is randomized up to 10cm. Demonstration dataset contains pushing in the first 3 chairs, and the robot is \ntested with all 5 chairs. Each demo has 2000 steps or 40 seconds.\n#1\n#2\n#3\n#4\ninit.\n#1\n#2\n#3\ninit.\n#1\n#2\n#3\n#4\ninit.\n#1\n#2\n#3\ninit.\n#1\n#2\n#3\ninit.\nFigure 3: Task Definitions. We illustrate 6 real-world tasks that Mobile ALOHA can perform autonomously. The 7th\ntask High Five is illustrated in the Appendix A.1 due to space constraint. For each task, we describe randomization\nand sub-task definitions. We also include an illustration of the base movement for each task (not drawn to scale).\n6\nMobile ALOHA: https://mobile-aloha.github.io\nWipe Wine (50 demos)\nCook Shrimp (20 demos)\nGrasp\nTowel\nLift Glass\nand Wipe\nPlace\nGlass\nWhole\nTask\nAdd\nOil\nAdd\nShrimp\nFlip\nShrimp\nPlate\nShrimp\nWhole\nTask\nCo-train\n100\n95\n100\n95\n100\n100\n60\n67\n40\nNo Co-train\n95\n58\n90\n50\n100\n100\n40\n50\n20\nRinse Pan (50 demos)\nUse Cabinet (50 demos)\nGrasp\nPan\nTurn On\nFaucet\nPlace\nPan\nWhole\nTask\nOpen\nCabinets\nGrasp\nPot\nPlace\nPot\nClose\nCabinet\nWhole\nTask\nCo-train\n100\n80\n100\n80\n95\n100\n95\n95\n85\nNo Co-train\n100\n0\n100\n0\n95\n95\n100\n95\n85\nCall Elevator (50 demos)\nPush Chairs (50 demos)\nHigh Five (20 demos)\nNavi.\nPress\nButton\nEnter\nElevator\nWhole\nTask\n1-3rd\nChair\n4th\n(OOD)\n5th\n(OOD)\nWhole\nTask\nUnseen\nAttire\nUnseen\nHuman\nNavi.\nWhole\nTask\nCo-train\n100\n100\n95\n95\n100\n85\n89\n80\n90\n80\n100\n85\nNo Co-train\n100\n5\n0\n0\n100\n70\n0\n0\n90\n80\n100\n85\nTable 1: Co-training improves ACT performance. Across 7 challenging mobile manipulation tasks, co-training\nwith static ALOHA dataset consistently improve the success rate (%) of ACT. It is particularly important for sub-tasks\nlike Press Button in Call Elevator and Turn on Faucet in Rinse Pan, where precise manipulation is the bottleneck.\nstainless steel and is small in size: roughly 4cm in\nlength and 0.7cm in diameter. Due to the stochas-\nticity introduced by the base motion, the arm needs\nto actively compensate for the errors by \u201cvisually-\nservoing\u201d to the shiny knob. A centimeter-level error\ncould result in task failure.\nFor Use Cabinet, the robot picks up a heavy pot and\nplaces it inside a two-door cabinet. While seemingly\na task that require no base movement, the robot\nactually needs to move back and forth four times\nto accomplish this task. For example when open-\ning the cabinet door, both arms need to grasp the\nhandle while the base is moving backward. This\nis necessary to avoid collision with the door and\nhave both arms within their workspace. Maneuvers\nlike this also stress the importance of whole-body\nteleoperation and control: if the arms and base con-\ntrol are separate, the robot will not be able to open\nboth doors quickly and fluidly. Notably, the heaviest\npot in our experiments weighs 1.4kg, exceeding the\nsingle arm\u2019s payload limit of 750g while within the\ncombined payload of two arms.\nFor Call Elevator, the robot needs to enter the el-\nevator by pressing the button. We emphasize long\nnavigation, large randomization, and precise whole-\nbody control in this task. The robot starts around\n15m from the elevator and is randomized across the\n10m wide lobby. To press the elevator button, the\nrobot needs to go around a column and stop precisely\nnext to the button. Pressing the button, measured\n2cm\u00d72cm in size, requires precision as pressing the\nperipheral or pressing too lightly will not activate\nthe elevator. The robot also needs to turn sharply\nand precisely to enter the elevator door: there is only\n30cm in clearance between the robot\u2019s widest part\nand the door.\nFor Push Chairs, the robot needs to push in 5 chairs\nin front of a long desk. This task emphasizes the\nstrength of the mobile manipulator: it needs to over-\ncome the friction between the 5kg chair and the\nground with coordinated arms and base movement.\nTo make this task more challenging, we only collect\ndata for the first 3 chairs, and stress test the robot to\nextrapolate to the 4th and 5th chair.\nFor High Five, we include illustrations in the Ap-\npendix A.1. The robot needs to go around the kitchen\nisland, and whenever a human approach it from the\nfront, stop moving and high five with the human. Af-\nter the high five, the robot should continue moving\nonly when the human moves out of its path. We col-\nlect data wearing different clothes and evaluate the\ntrained policy on unseen persons and unseen attires.\nWhile this task does not require a lot of precision,\nit highlights Mobile ALOHA\u2019s potential for studying\nhuman-robot interactions.\nWe want to highlight that for all tasks mentioned\nabove, open-loop replaying a demonstration with ob-\njects restored to the same configurations will achieve\nzero whole-task success. Successfully completing\nthe task requires the learned policy to react close-\nloop and correct for those errors. We believe the\nsource of errors during the open-loop replaying is\n7\nMobile ALOHA: https://mobile-aloha.github.io\nWipe Wine (50 demos)\nPush Chairs (50 demos)\nGrasp\nTowel\nLift Glass\nand Wipe\nPlace\nGlass\nWhole\nTask\n1st\nChair\n2nd\nChair\n3rd\nChair\nWhole\nTask\nVINN + Chunking\nCo-train\n85\n18\n100\n15\n100\n70\n86\n60\nNo Co-train\n50\n40\n100\n20\n90\n72\n62\n40\nDiffusion Policy\nCo-train\n90\n72\n100\n65\n100\n100\n100\n100\nNo Co-train\n75\n47\n100\n35\n100\n80\n100\n80\nACT\nCo-train\n100\n95\n100\n95\n100\n100\n100\n100\nNo Co-train\n95\n58\n90\n50\n100\n100\n100\n100\nTable 2: Mobile ALOHA is compatible with recent imitation learning methods. VINN with chunking, Diffusion\nPolicy, and ACT all achieves good performance on Mobile ALOHA, and benefit from co-training with static ALOHA.\nthe mobile base\u2019s velocity control. As an example,\nwe observe >10cm error on average when replaying\nthe base actions for a 180 degree turn with 1m ra-\ndius. We include more details about this experiment\nin Appendix A.4.\n6. Experiments\nWe aim to answer two central questions in our ex-\nperiments. (1) Can Mobile ALOHA acquire complex\nmobile manipulation skills with co-training and a\nsmall amount of mobile manipulation data? (2) Can\nMobile ALOHA work with different types of imita-\ntion learning methods, including ACT [104], Diffu-\nsion Policy [18], and retrieval-based VINN [63]? We\nconduct extensive experiments in the real-world to\nexamine these questions.\nAs a preliminary, all methods we will examine em-\nploy \u201caction chunking\u201d [104], where a policy predicts\na sequence of future actions instead of one action at\neach timestep. It is already part of the method for\nACT and Diffusion policy, and simple to be added for\nVINN. We found action chunking to be crucial for\nmanipulation, improving the coherence of generated\ntrajectory and reducing the latency from per-step\npolicy inference. Action chunking also provides a\nunique advantage for Mobile ALOHA: handling the\ndelay of different parts of the hardware more flexibly.\nWe observe a delay between target and actual veloc-\nities of our mobile base, while the delay for position-\ncontrolled arms is much smaller. To account for a\ndelay of d steps of the mobile base, our robot exe-\ncutes the first k \u2212 d arm actions and last k \u2212 d base\nactions of an action chunk of length k.\n6.1. Co-training Improves Performance\nWe start with ACT [104], the method introduced\nwith ALOHA, and train it on all 7 tasks with and\nwithout co-training. We then evaluate each pol-\nicy in the real-world, with randomization of robot\nand objects configurations as described in Figure 3.\nTo calculate the success rate for a sub-task, we di-\nvide #Success by #Attempts. For example in the\ncase of Lift Glass and Wipe sub-task, the #Attempts\nequals the number of success from the previous sub-\ntask Grasp Towel, as the robot could fail and stop at\nany sub-task. This also means the final success rate\nequals the product of all sub-task success rates. We\nreport all success rates in Table 1. Each success rate\nis computed from 20 trials of evaluation, except Cook\nShrimp which has 5.\nWith the help of co-training, the robot obtains 95%\nsuccess for Wipe Wine, 95% success for Call Elevator,\n85% success for Use Cabinet, 85% success for High\nFive, 80% success for Rinse Pan, and 80% success for\nPush Chairs. Each of these tasks only requires 50 in-\ndomain demonstrations, or 20 in the case of High Five.\nThe only task that falls below 80% success is Cook\nShrimp (40%), which is a 75-second long-horizon task\nfor which we only collected 20 demonstrations. We\nfound the policy to struggle with flipping the shrimp\nwith the spatula and pouring the shrimp inside the\nwhite bowl, which has low contrast with the white\ntable. We hypothesize that the lower success is likely\ndue to the limited demonstration data. Co-training\nimproves the whole-task success rate in 5 out of the\n7 tasks, with a boost of 45%, 20%, 80%, 95% and 80%\nrespectively. For the remaining two tasks, the suc-\ncess rate is comparable between co-training and no\nco-training. We find co-training to be more helpful\nfor sub-tasks where precise manipulation is the bot-\ntleneck, for example Press Button, Flip Shrimp, and\nTurn On Faucet. In all of these cases, compounding\nerrors appear to be the main source of failure, either\nfrom the stochasticity of robot base velocity control\nor from rich contacts such as grasping of the spatula\nand making contact with the pan during Flip Shrimp.\nWe hypothesize that the \u201cmotion prior\u201d of grasping\n8\nMobile ALOHA: https://mobile-aloha.github.io\n25\n35\n50\nNum. Mobile ALOHA Demos\n0\n20\n40\n60\n80\n100\nSuccess (%)\nWipe Wine\nCo-train\nNo Co-train\nFigure 4: Data efficiency.\nCo-training with static\nALOHA data leads to better data efficiency and consistent\nimprovements over training with Mobile ALOHA data\nonly. Figure style credits to [70].\nand approaching objects in the static ALOHA dataset\nstill benefits Mobile ALOHA, especially given the in-\nvariances introduced by the wrist camera [41]. We\nalso find the co-trained policy to generalize better\nin the case of Push Chairs and Wipe Wine. For Push\nChairs, both co-training and no co-training achieve\nperfect success for the first 3 chairs, which are seen\nin the demonstrations. However, co-training per-\nforms much better when extrapolating to the 4th\nand 5th chair, by 15% and 89% respectively. For Wipe\nWine, we observe that the co-trained policy performs\nbetter at the boundary of the wine glass randomiza-\ntion region. We thus hypothesize that co-training\ncan also help prevent overfitting, given the low-data\nregime of 20-50 demonstrations and the expressive\ntransformer-based policy used.\n6.2. Compatibility with\nACT, Diffusion Policy, and VINN\nWe train two recent imitation learning methods,\nDiffusion Policy [18] and VINN [63], with Mobile\nALOHA in addition to ACT. Diffusion policy trains a\nneural network to gradually refine the action predic-\ntion. We use the DDIM scheduler [85] to improve in-\nference speed, and apply data augmentation to image\nobservations to prevent overfitting. The co-training\ndata pipeline is the same as ACT, and we include\nmore training details in the Appendix A.3. VINN\ntrains a visual representation model, BYOL [37] and\nuses it to retrieve actions from the demonstration\ndataset with nearest neighbors. We augment VINN\nretrieval with proprioception features and tune the\nrelative weight to balance visual and proprioception\nfeature importance. We also retrieve an action chunk\ninstead of a single action and find significant per-\nformance improvement similar to Zhao et al.. For\nStatic ALOHA proportion (%)\n30\n50\n70\n(default)\nSuccess (%)\n95\n95\n90\nTable 3: Co-training is robust to different data mix-\ntures. Result uses ACT training on the Wipe Wine task.\nCo-train Pre-train\nNo Co-train\nNo Pre-train\nSuccess (%)\n95\n40\n50\nTable 4: Co-train vs. Pre-train. Co-train outperforms\npre-train on the Wipe Wine task. For pre-train, we first\ntrain ACT on the static ALOHA data and then fine-tune it\nwith the Mobile ALOHA data.\nco-training, we simply co-train the BYOL encoder\nwith the combined mobile and static data.\nIn Table 2, we report co-training and no cotraining\nsuccess rates on 2 real-world tasks: Wipe Wine and\nPush Chairs. Overall, Diffusion Policy performs sim-\nilarly to ACT on Push Chairs, both obtaining 100%\nwith co-training. For Wipe Wine, we observe worse\nperformance with diffusion at 65% success. The Dif-\nfusion Policy is less precise when approaching the\nkitchen island and grasping the wine glass. We hy-\npothesize that 50 demonstrations is not enough for\ndiffusion given it\u2019s expressiveness: previous works\nthat utilize Diffusion Policy tend to train on upwards\nof 250 demonstrations. For VINN + Chunking, the\npolicy performs worse than ACT or Diffusion across\nthe board, while still reaching reasonable success\nrates with 60% on Push Chairs and 15% on Wipe Wine.\nThe main failure modes are imprecise grasping on\nLift Glass and Wipe as well as jerky motion when\nswitching between chunks. We find that increasing\nthe weight on proprioception when retrieving can\nimprove the smoothness while at a cost of paying\nless attention to visual inputs. We find co-training\nto improve Diffusion Policy\u2019s performance, by 30%\nand 20% for on Wipe Wine and Push Chairs respec-\ntively. This is expected as co-training helps address\noverfitting. Unlike ACT and Diffusion Policy, we\nobserve mixed results for VINN, where co-training\nhurts Wipe Wine by 5% while improves Push Chairs\nby 20%. Only the representations of VINN are co-\ntrained, while the action prediction mechanism of\nVINN does not have a way to leverage the out-of-\ndomain static ALOHA data, perhaps explaining these\nmixed results.\n9\nMobile ALOHA: https://mobile-aloha.github.io\n1\n2\n3\n4\n5\nTrial Num.\n20\n40\n60\n80\nDuration (s)\nWipe Wine\n1\n2\n3\n4\n5\nTrial Num.\n20\n40\n60\n80\n100\n120\n140\nUse Cabinets\nAvg. User\nExpert\nFigure 5: Teleoperator learning curves. New users\ncan quickly approach expert speed on an unseen tasks\nteleoperating Mobile ALOHA .\n7. Ablation Studies\nData Efficiency. In Figure 4, we ablate the num-\nber of mobile manipulation demonstrations for both\nco-training and no co-training, using ACT on the\nWipe Wine task. We consider 25, 35, and 50 Mobile\nALOHA demonstrations and evaluate for 20 trials\neach. We observe that co-training leads to better data\nefficiency and consistent improvements over train-\ning using only Mobile ALOHA data. With co-training,\nthe policy trained with 35 in-domain demonstrations\ncan outperform the no co-training policy trained\nwith 50 in-domain demonstrations, by 20% (70% vs.\n50%).\nCo-training Is Robust To Different Data Mix-\ntures. We sample with equal probability from the\nstatic ALOHA datasets and the Mobile ALOHA task\ndataset to form a training mini-batch in our co-\ntraining experiments so far, giving a co-training data\nsampling rate of roughly 50%. In Table 3, we study\nhow different sampling strategies affect performance\non the Wipe Wine task. We train ACT with 30% and\n70% co-training data sampling rates in addition to\n50%, then evaluate 20 trials each. We see similar\nperformance across the board, with 95%, 95% and\n90% success respectively. This experiment suggests\nthat co-training performance is not sensitive to dif-\nferent data mixtures, reducing the manual tuning\nnecessary when incorporating co-training on a new\ntask.\nCo-training Outperforms Pre-training. In Ta-\nble 4, we compare co-training and pre-training on\nthe static ALOHA data. For pre-training, we first\ntrain ACT on the static ALOHA data for 10K steps\nand then continue training with in-domain task data.\nWe experiment with the Wipe Wine task and observe\nthat pre-training provides no improvements over\ntraining solely on Wipe Wine data. We hypothesize\nthat the network forgets its experience on the static\nALOHA data during the fine-tuning phase.\n8. User Studies\nWe conduct a user study to evaluate the effective-\nness of Mobile ALOHA teleoperation. Specifically,\nwe measure how fast participants are able to learn to\nteleoperate an unseen task. We recruit 8 participants\namong computer science graduate students, with 5\nfemales and 3 males aged 21-26. Four participants\nhas no prior teleoperation experience, and the re-\nmaining 4 have varying levels of expertise. None\nof the them have used Mobile ALOHA before. We\nstart by allowing each participant to freely interact\nwith objects in the scene for 3 minutes. We held\nout all objects that will be used for the unseen tasks\nduring this process. Next, we give each participants\ntwo tasks: Wipe Wine and Use Cabinet. An expert\noperator will first demonstrate the task, followed\nby 5 consecutive trials from the participants. We\nrecord the completion time for each trial, and plot\nthem in Figure 5. We notice a steep decline in com-\npletion time: on average, the time it took to perform\nthe task went from 46s to 28s for Wipe Wine (down\n39%), and from 75s to 36s for Use Cabinet (down 52%).\nThe average participant can also to approach speed\nof expert demonstrations after 5 trials, demonstrat-\ning the ease of use and learning of Mobile ALOHA\nteleoperation.\n9. Conclusion, Limitations and\nFuture Directions\nIn summary, our paper tackles both the hardware\nand the software aspects of bimanual mobile ma-\nnipulation. Augmenting the ALOHA system with a\nmobile base and whole-body teleoperation allows us\nto collect high-quality demonstrations on complex\nmobile manipulation tasks. Then through imitation\nlearning co-trained with static ALOHA data, Mobile\nALOHA can learn to perform these tasks with only\n20 to 50 demonstrations. We are also able to keep the\nsystem accessible, with under $32k budget including\nonboard power and compute, and open-sourcing on\nboth software and hardware.\nDespite Mobile ALOHA\u2019s simplicity and perfor-\nmance, there are still limitations that we hope to ad-\ndress in future works. On the hardware front, we will\nseek to reduce the occupied area of Mobile ALOHA.\nThe current footprint of 90cm x 135cm could be too\nnarrow for certain paths. In addition, the fixed height\nof the two follower arms makes lower cabinets, ovens\nand dish washers challenging to reach. We are plan-\n10\nMobile ALOHA: https://mobile-aloha.github.io\nning to add more degrees of freedom to the arms\u2019\nelevation to address this issue. On the software front,\nwe limit our policy learning results to single task im-\nitation learning. The robot can not yet improve itself\nautonomously or explore to acquire new knowledge.\nIn addition, the Mobile ALOHA demonstrations are\ncollected by two expert operators. We leave it to fu-\nture work for tackling imitation learning from highly\nsuboptimal, heterogeneous datasets.\nAcknowledgments\nWe thank the Stanford Robotics Center and Steve\nCousins for providing facility support for our experi-\nments. We also thank members of Stanford IRIS Lab:\nLucy X. Shi and Tian Gao, and members of Stan-\nford REAL Lab: Cheng Chi, Zhenjia Xu, Yihuai Gao,\nHuy Ha, Zeyi Liu, Xiaomeng Xu, Chuer Pan and\nShuran Song, for providing extensive helps for our\nexperiments. We appreciate much photographing\nby Qingqing Zhao, and feedbacks from and helpful\ndiscussions with Karl Pertsch, Boyuan Chen, Ziwen\nZhuang, Quan Vuong and Fei Xia. This project is sup-\nported by the Boston Dynamics AI Institute and ONR\ngrant N00014-21-1-2685. Zipeng Fu is supported by\nStanford Graduate Fellowship.\nReferences\n[1] Fetch robot.\nhttps://docs.fetchrobotics.com/\nteleop.html. 2\n[2] Hello robot stretch.\nhttps://github.com/\nhello-robot/stretch_fisheye_web_interface. 2\n[3] Viperx 300 6dof. https://www.trossenrobotics.\ncom/viperx-300-robot-arm.aspx. 3\n[4] Michael Ahn, Anthony Brohan, Noah Brown,\nYevgen Chebotar,\nOmar Cortes,\nByron\nDavid, Chelsea Finn, Chuyuan Fu, Keerthana\nGopalakrishnan, Karol Hausman, Alex Her-\nzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,\nBrian Ichter, Alex Irpan, Eric Jang, Rosario Jau-\nregui Ruano, Kyle Jeffrey, Sally Jesmonth,\nNikhil Joshi, Ryan Julian, Dmitry Kalashnikov,\nYuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada,\nPeter Pastor, Jornell Quiambao, Kanishka Rao,\nJarek Rettinghouse, Diego Reyes, Pierre Ser-\nmanet, Nicolas Sievers, Clayton Tan, Alexan-\nder Toshev, Vincent Vanhoucke, Fei Xia, Ted\nXiao, Peng Xu, Sichun Xu, Mengyuan Yan,\nand Andy Zeng. Do as i can and not as i say:\nGrounding language in robotic affordances. In\narXiv preprint arXiv:2204.01691, 2022. 3\n[5] Miguel Arduengo, Ana Arduengo, Adri\u00e0\nColom\u00e9, Joan Lobo-Prat, and Carme Torras.\nHuman to robot whole-body motion transfer.\nIn 2020 IEEE-RAS 20th International Conference\non Humanoid Robots (Humanoids), 2021. 2, 3\n[6] Christopher G Atkeson, PW Babu Ben-\nzun, Nandan Banerjee, Dmitry Berenson,\nChristoper P Bove, Xiongyi Cui, Mathew De-\nDonato, Ruixiang Du, Siyuan Feng, Perry\nFranklin, et al. What happened at the darpa\nrobotics challenge finals. The DARPA robotics\nchallenge finals: Humanoid robots to the rescue.\n3\n[7] Shikhar Bahl, Abhinav Gupta, and Deepak\nPathak. Hierarchical neural dynamic policies.\nRSS, 2021. 3\n[8] Shikhar Bahl, Abhinav Gupta, and Deepak\nPathak. Human-to-robot imitation in the wild.\narXiv preprint arXiv:2207.09450, 2022. 3\n[9] Max\nBajracharya,\nJames\nBorders,\nDan\nHelmick, Thomas Kollar, Michael Laskey,\nJohn Leichty,\nJeremy Ma,\nUmashankar\nNagarajan, Akiyoshi Ochiai, Josh Petersen,\net al.\nA mobile manipulation system for\none-shot teaching of complex tasks in homes.\nIn 2020 IEEE International Conference on\nRobotics and Automation (ICRA), 2020. 2\n[10] H Bharadhwaj, J Vakil, M Sharma, A Gupta,\nS Tulsiani, and V Kumar. Roboagent: Towards\nsample efficient robot manipulation with se-\nmantic augmentations and action chunking,\n2023. 3\n[11] Konstantinos Bousmalis,\nGiulia Vezzani,\nDushyant Rao, Coline Devin, Alex X. Lee,\nMaria Bauza, Todor Davchev, Yuxiang Zhou,\nAgrim Gupta, Akhil Raju, Antoine Lau-\nrens, Claudio Fantacci, Valentin Dalibard,\nMartina Zambelli, Murilo Martins, Rugile\nPevceviciute, Michiel Blokzijl, Misha Denil,\nNathan Batchelor, Thomas Lampe, Emilio\nParisotto, Konrad \u017bo\u0142na, Scott Reed, Ser-\ngio G\u00f3mez Colmenarejo, Jon Scholz, Ab-\nbas Abdolmaleki, Oliver Groth, Jean-Baptiste\nRegli, Oleg Sushkov, Tom Roth\u00f6rl, Jos\u00e9 En-\nrique Chen, Yusuf Aytar, Dave Barker, Joy Or-\ntiz, Martin Riedmiller, Jost Tobias Springen-\nberg, Raia Hadsell, Francesco Nori, and Nico-\nlas Heess. Robocat: A self-improving foun-\ndation agent for robotic manipulation. arXiv\npreprint arXiv:2306.11706, 2023. 3, 5\n[12] Anthony Brohan, Noah Brown, Justice Carba-\njal, Yevgen Chebotar, Joseph Dabis, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Jasmine Hsu, Julian Ibarz,\nBrian Ichter, Alex Irpan, Tomas Jackson, Sally\nJesmonth, Nikhil Joshi, Ryan Julian, Dmitry\nKalashnikov, Yuheng Kuang, Isabel Leal,\n11\nMobile ALOHA: https://mobile-aloha.github.io\nKuang-Huei Lee, Sergey Levine, Yao Lu, Utsav\nMalla, Deeksha Manjunath, Igor Mordatch,\nOfir Nachum, Carolina Parada, Jodilyn Peralta,\nEmily Perez, Karl Pertsch, Jornell Quiambao,\nKanishka Rao, Michael Ryoo, Grecia Salazar,\nPannag Sanketi, Kevin Sayed, Jaspiar Singh,\nSumedh Sontakke, Austin Stone, Clayton Tan,\nHuong Tran, Vincent Vanhoucke, Steve Vega,\nQuan Vuong, Fei Xia, Ted Xiao, Peng Xu,\nSichun Xu, Tianhe Yu, and Brianna Zitkovich.\nRt-1: Robotics transformer for real-world con-\ntrol at scale. In arXiv preprint arXiv:2212.06817,\n2022. 1, 3\n[13] Anthony Brohan, Noah Brown, Justice Car-\nbajal, Yevgen Chebotar, Xi Chen, Krzysztof\nChoromanski, Tianli Ding, Danny Driess,\nAvinava Dubey, Chelsea Finn, Pete Flo-\nrence, Chuyuan Fu, Montse Gonzalez Are-\nnas, Keerthana Gopalakrishnan, Kehang Han,\nKarol Hausman, Alex Herzog, Jasmine Hsu,\nBrian Ichter, Alex Irpan, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang,\nIsabel Leal, Lisa Lee, Tsang-Wei Edward Lee,\nSergey Levine, Yao Lu, Henryk Michalewski,\nIgor Mordatch, Karl Pertsch, Kanishka Rao,\nKrista Reymann, Michael Ryoo, Grecia Salazar,\nPannag Sanketi, Pierre Sermanet, Jaspiar\nSingh, Anikait Singh, Radu Soricut, Huong\nTran, Vincent Vanhoucke, Quan Vuong,\nAyzaan Wahid, Stefan Welker, Paul Wohlhart,\nJialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun\nXu, Tianhe Yu, and Brianna Zitkovich. Rt-2:\nVision-language-action models transfer web\nknowledge to robotic control. In arXiv pre-\nprint arXiv:2307.15818, 2023. 3\n[14] Anais Brygo, Ioannis Sarakoglou, Nadia\nGarcia-Hernandez, and Nikolaos Tsagarakis.\nHumanoid robot teleoperation with vibrotac-\ntile based balancing feedback. In Haptics: Neu-\nroscience, Devices, Modeling, and Applications:\n9th International Conference, EuroHaptics 2014,\nVersailles, France, June 24-26, 2014, Proceedings,\nPart II 9, 2014. 3\n[15] Jean Chagas Vaz, Dylan Wallace, and Paul Y\nOh. Humanoid loco-manipulation of pushed\ncarts utilizing virtual reality teleoperation. In\nASME International Mechanical Engineering\nCongress and Exposition, 2021. 3\n[16] Annie S Chen, Suraj Nair, and Chelsea Finn.\nLearning generalizable robotic reward func-\ntions from\" in-the-wild\" human videos. arXiv\npreprint arXiv:2103.16817, 2021. 3\n[17] Joel Chestnutt, Manfred Lau, German Cheung,\nJames Kuffner, Jessica Hodgins, and Takeo\nKanade.\nFootstep planning for the honda\nasimo humanoid. In ICRA, 2005. 2\n[18] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia\nXu, Eric Cousineau, Benjamin Burchfiel, and\nShuran Song. Diffusion policy: Visuomotor\npolicy learning via action diffusion. In Pro-\nceedings of Robotics: Science and Systems (RSS),\n2023. 1, 2, 3, 5, 8, 9\n[19] R Cisneros, M Benallegue, K Kaneko, H Kam-\ninaga, G Caron, A Tanguy, R Singh, L Sun,\nA Dallard, C Fournier, et al. Team janus hu-\nmanoid avatar: A cybernetic avatar to embody\nhuman telepresence. In Toward Robot Avatars:\nPerspectives on the ANA Avatar XPRIZE Com-\npetition, RSS Workshop, 2022. 3\n[20] Open X-Embodiment Collaboration, Abhishek\nPadalkar, Acorn Pooley, Ajinkya Jain, Alex\nBewley, Alex Herzog, Alex Irpan, Alexander\nKhazatsky, Anant Rai, Anikait Singh, Anthony\nBrohan, Antonin Raffin, Ayzaan Wahid, Ben\nBurgess-Limerick, Beomjoon Kim, Bernhard\nSch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles\nXu, Chelsea Finn, Chenfeng Xu, Cheng Chi,\nChenguang Huang, Christine Chan, Chuer\nPan, Chuyuan Fu, Coline Devin, Danny Driess,\nDeepak Pathak, Dhruv Shah, Dieter B\u00fcchler,\nDmitry Kalashnikov, Dorsa Sadigh, Edward\nJohns, Federico Ceola, Fei Xia, Freek Stulp,\nGaoyue Zhou, Gaurav S. Sukhatme, Gautam\nSalhotra, Ge Yan, Giulio Schiavi, Hao Su,\nHao-Shu Fang, Haochen Shi, Heni Ben Amor,\nHenrik I Christensen, Hiroki Furuta, Homer\nWalke, Hongjie Fang, Igor Mordatch, Ilija Ra-\ndosavovic, Isabel Leal, Jacky Liang, Jaehyung\nKim, Jan Schneider, Jasmine Hsu, Jeannette\nBohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu,\nJianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh,\nJitendra Malik, Jonathan Tompson, Jonathan\nYang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek\nHan, Kanishka Rao, Karl Pertsch, Karol Haus-\nman, Keegan Go, Keerthana Gopalakrish-\nnan, Ken Goldberg, Kendra Byrne, Kenneth\nOslund, Kento Kawaharazuka, Kevin Zhang,\nKeyvan Majd, Krishan Rana, Krishnan Srini-\nvasan, Lawrence Yunliang Chen, Lerrel Pinto,\nLiam Tan, Lionel Ott, Lisa Lee, Masayoshi\nTomizuka, Maximilian Du, Michael Ahn,\nMingtong Zhang, Mingyu Ding, Mohan Ku-\nmar Srirama, Mohit Sharma, Moo Jin Kim,\nNaoaki Kanazawa, Nicklas Hansen, Nicolas\nHeess, Nikhil J Joshi, Niko Suenderhauf, Nor-\nman Di Palo, Nur Muhammad Mahi Shafiul-\nlah, Oier Mees, Oliver Kroemer, Pannag R\n12\nMobile ALOHA: https://mobile-aloha.github.io\nSanketi, Paul Wohlhart, Peng Xu, Pierre Ser-\nmanet, Priya Sundaresan, Quan Vuong, Rafael\nRafailov, Ran Tian, Ria Doshi, Roberto Mart\u00edn-\nMart\u00edn, Russell Mendonca, Rutav Shah, Ryan\nHoque, Ryan Julian, Samuel Bustamante,\nSean Kirmani, Sergey Levine, Sherry Moore,\nShikhar Bahl, Shivin Dass, Shuran Song,\nSichun Xu, Siddhant Haldar, Simeon Ade-\nbola, Simon Guist, Soroush Nasiriany, Ste-\nfan Schaal, Stefan Welker, Stephen Tian,\nSudeep Dasari, Suneel Belkhale, Takayuki Osa,\nTatsuya Harada, Tatsuya Matsushima, Ted\nXiao, Tianhe Yu, Tianli Ding, Todor Davchev,\nTony Z. Zhao, Travis Armstrong, Trevor Dar-\nrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan,\nWenxuan Zhou, Wolfram Burgard, Xi Chen,\nXiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao\nLu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu,\nYing Xu, Yixuan Wang, Yonatan Bisk, Yoony-\noung Cho, Youngwoon Lee, Yuchen Cui, Yueh\nhua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li,\nYusuke Iwasawa, Yutaka Matsuo, Zhuo Xu,\nand Zichen Jeff Cui. Open X-Embodiment:\nRobotic learning datasets and RT-X models.\nhttps://arxiv.org/abs/2310.08864, 2023. 1, 2, 3, 5\n[21] Zichen Jeff Cui, Yibin Wang, Nur Muham-\nmad Mahi Shafiullah, and Lerrel Pinto. From\nplay to policy: Conditional behavior genera-\ntion from uncurated robot data. arXiv preprint\narXiv:2210.10047, 2022. 3\n[22] Stefano Dafarra, Kourosh Darvish, Riccardo\nGrieco,\nGianluca Milani,\nUgo Pattacini,\nLorenzo Rapetti, Giulio Romualdi, Mattia\nSalvi, Alessandro Scalzo, Ines Sorrentino,\net al. icub3 avatar system. arXiv preprint\narXiv:2203.06972, 2022. 3\n[23] Kourosh Darvish,\nYeshasvi\nTirupachuri,\nGiulio Romualdi, Lorenzo Rapetti, Diego\nFerigo, Francisco Javier Andrade Chavez,\nand Daniele Pucci. Whole-body geometric\nretargeting for humanoid robots.\nIn 2019\nIEEE-RAS 19th International Conference on\nHumanoid Robots (Humanoids), 2019. 3\n[24] Neha Das, Sarah Bechtle, Todor Davchev, Di-\nnesh Jayaraman, Akshara Rai, and Franziska\nMeier. Model-based inverse reinforcement\nlearning from visual demonstrations. In Con-\nference on Robot Learning, pages 1930\u20131942.\nPMLR, 2021. 3\n[25] Sudeep Dasari and Abhinav Kumar Gupta.\nTransformers for one-shot visual imitation.\nIn Conference on Robot Learning, 2020. 3\n[26] Anca D Dragan, Kenton CT Lee, and Sid-\ndhartha S Srinivasa. Legibility and predictabil-\nity of robot motion. In 2013 8th ACM/IEEE\nInternational Conference on Human-Robot In-\nteraction (HRI), 2013. 3\n[27] Yan Duan, Marcin Andrychowicz, Bradly C.\nStadie,\nJonathan\nHo,\nJonas\nSchneider,\nIlya Sutskever, P. Abbeel, and Wojciech\nZaremba. One-shot imitation learning. ArXiv,\nabs/1703.07326, 2017. 3\n[28] Frederik Ebert, Yanlai Yang, Karl Schmeck-\npeper, Bernadette Bucher, Georgios Georgakis,\nKostas Daniilidis, Chelsea Finn, and Sergey\nLevine. Bridge data: Boosting generalization\nof robotic skills with cross-domain datasets.\nArXiv, abs/2109.13396, 2021. 3\n[29] Ashley D Edwards and Charles L Isbell. Per-\nceptual values from observation. arXiv pre-\nprint arXiv:1905.07861, 2019. 3\n[30] Peter Englert and Marc Toussaint. Learning\nmanipulation skills from a single demonstra-\ntion. The International Journal of Robotics Re-\nsearch, 37(1):137\u2013154, 2018. 3\n[31] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang,\nJirong Liu, Chenxi Wang, Junbo Wang, Haoyi\nZhu, and Cewu Lu. Rh20t: A comprehensive\nrobotic dataset for learning diverse skills in\none-shot. In Towards Generalist Robots: Learn-\ning Paradigms for Scalable Skill Acquisition@\nCoRL2023, 2023. 3, 5\n[32] Hongjie Fang, Hao-Shu Fang, Yiming Wang,\nJieji Ren, Jingjing Chen, Ruo Zhang, Weiming\nWang, and Cewu Lu. Low-cost exoskeletons\nfor learning whole-arm manipulation in the\nwild. arXiv preprint arXiv:2309.14975, 2023. 3\n[33] Siyuan Feng, Eric Whitman, X Xinjilefu, and\nChristopher G Atkeson. Optimization based\nfull body control for the atlas robot. In Inter-\nnational Conference on Humanoid Robots, 2014.\n2\n[34] Chelsea Finn, Tianhe Yu, Tianhao Zhang,\nPieter Abbeel, and Sergey Levine. One-shot\nvisual imitation learning via meta-learning. In\nConference on robot learning, 2017. 3\n[35] Peter R. Florence, Corey Lynch, Andy Zeng,\nOscar Ramirez, Ayzaan Wahid, Laura Downs,\nAdrian S. Wong, Johnny Lee, Igor Mordatch,\nand Jonathan Tompson. Implicit behavioral\ncloning. ArXiv, abs/2109.00137, 2021. 3\n[36] Zipeng Fu, Xuxin Cheng, and Deepak Pathak.\nDeep whole-body control: learning a unified\npolicy for manipulation and locomotion. In\nConference on Robot Learning, 2022. 3\n[37] Jean-Bastien Grill, Florian Strub, Florent\nAltch\u00e9, Corentin Tallec, Pierre Richemond,\n13\nMobile ALOHA: https://mobile-aloha.github.io\nElena Buchatskaya, Carl Doersch, Bernardo\nAvila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-\na new approach to self-supervised learning.\nAdvances in neural information processing sys-\ntems, 33:21271\u201321284, 2020. 9\n[38] Jiayuan Gu, Devendra Singh Chaplot, Hao Su,\nand Jitendra Malik. Multi-skill mobile manip-\nulation for object rearrangement. ICLR, 2023.\n3\n[39] Abhinav Gupta, Adithyavairavan Murali, Dhi-\nraj Prakashchand Gandhi, and Lerrel Pinto.\nRobot learning in homes: Improving general-\nization and reducing dataset bias. Advances in\nneural information processing systems, 2018. 3\n[40] Kaiming He, X. Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image\nrecognition. 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR),\npages 770\u2013778, 2015. 19\n[41] Kyle Hsu, Moo Jin Kim, Rafael Rafailov, Jia-\njun Wu, and Chelsea Finn. Vision-based ma-\nnipulators need to also see from their hands.\nArXiv, abs/2203.12677, 2022. URL https://api.\nsemanticscholar.org/CorpusID:247628166. 9\n[42] Jiaheng Hu, Peter Stone, and Roberto Mart\u00edn-\nMart\u00edn.\nCausal policy gradient for whole-\nbody mobile manipulation.\narXiv preprint\narXiv:2305.04866, 2023. 3\n[43] Xiaoyu Huang, Dhruv Batra, Akshara Rai, and\nAndrew Szot. Skill transformer: A monolithic\npolicy for mobile manipulation. In Proceedings\nof the IEEE/CVF International Conference on\nComputer Vision, 2023. 3\n[44] Auke Jan Ijspeert, Jun Nakanishi, Heiko Hoff-\nmann, Peter Pastor, and Stefan Schaal. Dynam-\nical movement primitives: learning attractor\nmodels for motor behaviors. Neural computa-\ntion, 2013. 3\n[45] Yasuhiro Ishiguro, Tasuku Makabe, Yuya\nNagamatsu, Yuta Kojio, Kunio Kojima, Fumi-\nhito Sugai, Yohei Kakiuchi, Kei Okada, and\nMasayuki Inaba. Bilateral humanoid teleoper-\nation system using whole-body exoskeleton\ncockpit tablis. IEEE Robotics and Automation\nLetters, 2020. 3\n[46] Stephen James, Michael Bloesch, and An-\ndrew J. Davison. Task-embedded control net-\nworks for few-shot imitation learning. ArXiv,\nabs/1810.03237, 2018. 3\n[47] Eric Jang, Alex Irpan, Mohi Khansari, Daniel\nKappler, Frederik Ebert, Corey Lynch, Sergey\nLevine, and Chelsea Finn. Bc-z: Zero-shot task\ngeneralization with robotic imitation learning.\nIn Conference on Robot Learning, 2022. 3\n[48] Snehal Jauhri, Jan Peters, and Georgia Chal-\nvatzaki. Robot learning of mobile manipula-\ntion with reachability behavior priors. IEEE\nRobotics and Automation Letters, 2022. 3\n[49] Edward Johns. Coarse-to-fine imitation learn-\ning: Robot manipulation from a single demon-\nstration. 2021 IEEE International Conference on\nRobotics and Automation (ICRA), pages 4613\u2013\n4619, 2021. 3\n[50] Edward Johns. Coarse-to-fine imitation learn-\ning: Robot manipulation from a single demon-\nstration. In 2021 IEEE international conference\non robotics and automation (ICRA), pages 4613\u2013\n4619. IEEE, 2021. 3\n[51] Matthew Johnson, Brandon Shrewsbury, Syl-\nvain Bertrand, Tingfan Wu, Daniel Du-\nran, Marshall Floyd, Peter Abeles, Douglas\nStephen, Nathan Mertins, Alex Lesman, et al.\nTeam ihmc\u2019s lessons learned from the darpa\nrobotics challenge trials.\nJournal of Field\nRobotics, 2015. 3\n[52] Oussama Khatib, K Yokoi, K Chang, D Ruspini,\nR Holmberg, A Casal, and A Baader. Force\nstrategies for cooperative tasks in multiple\nmobile manipulation systems. In Robotics Re-\nsearch: The Seventh International Symposium,\n1996. 2\n[53] Doik Kim, Bum-Jae You, and Sang-Rok Oh.\nWhole body motion control framework for ar-\nbitrarily and simultaneously assigned upper-\nbody tasks and walking motion. Modeling,\nSimulation and Optimization of Bipedal Walk-\ning, 2013. 3\n[54] Heecheol Kim, Yoshiyuki Ohmura, and Ya-\nsuo Kuniyoshi. Robot peels banana with goal-\nconditioned dual-action deep imitation learn-\ning. ArXiv, abs/2203.09749, 2022. 3\n[55] Jens Kober and Jan Peters. Learning motor\nprimitives for robotics. In 2009 IEEE Interna-\ntional Conference on Robotics and Automation,\n2009. 3\n[56] Eric Krotkov, Douglas Hackett, Larry Jackel,\nMichael Perschbacher, James Pippine, Jesse\nStrauss, Gill Pratt, and Christopher Orlowski.\nThe darpa robotics challenge finals: Results\nand perspectives. The DARPA Robotics Chal-\nlenge Finals: Humanoid Robots To The Rescue,\n2018. 2\n[57] Corey Lynch, Mohi Khansari, Ted Xiao,\nVikash Kumar, Jonathan Tompson, Sergey\nLevine, and Pierre Sermanet. Learning latent\n14\nMobile ALOHA: https://mobile-aloha.github.io\nplans from play. In Conference on robot learn-\ning, pages 1113\u20131132. PMLR, 2020. 3\n[58] Yuntao Ma, Farbod Farshidian, Takahiro Miki,\nJoonho Lee, and Marco Hutter. Combining\nlearning-based locomotion policy with model-\nbased manipulation for legged mobile manip-\nulators. IEEE Robotics and Automation Letters,\n2022. 3\n[59] Ajay Mandlekar, Danfei Xu, J. Wong, Soroush\nNasiriany, Chen Wang, Rohun Kulkarni,\nLi Fei-Fei, Silvio Savarese, Yuke Zhu, and\nRoberto Mart\u2019in-Mart\u2019in. What matters in\nlearning from offline human demonstrations\nfor robot manipulation. In Conference on Robot\nLearning, 2021. 3\n[60] Suraj Nair, Aravind Rajeswaran, Vikash Ku-\nmar, Chelsea Finn, and Abhinav Gupta. R3m:\nA universal visual representation for robot\nmanipulation. arXiv preprint arXiv:2203.12601,\n2022. 3\n[61] Octo Model Team, Dibya Ghosh, Homer\nWalke, Karl Pertsch, Kevin Black, Oier Mees,\nSudeep Dasari, Joey Hejna, Charles Xu, Jian-\nlan Luo, Tobias Kreiman, You Liang Tan,\nDorsa Sadigh, Chelsea Finn, and Sergey\nLevine. Octo: An open-source generalist robot\npolicy. https://octo-models.github.io, 2023. 3, 5\n[62] Alexandros Paraschos, Christian Daniel, Jan\nPeters, and Gerhard Neumann. Using proba-\nbilistic movement primitives in robotics. Au-\ntonomous Robots, 42:529\u2013551, 2018. 3\n[63] Jyothish Pari, Nur Muhammad Shafiullah,\nSridhar Pandian Arunachalam, and Lerrel\nPinto. The surprising effectiveness of repre-\nsentation learning for visual imitation. arXiv\npreprint arXiv:2112.01511, 2021. 3, 5, 8, 9\n[64] Peter Pastor, Heiko Hoffmann, Tamim Asfour,\nand Stefan Schaal. Learning and generaliza-\ntion of motor skills by learning from demon-\nstration. 2009 IEEE International Conference\non Robotics and Automation, pages 763\u2013768,\n2009. 3\n[65] Luigi Penco, Nicola Scianca, Valerio Modugno,\nLeonardo Lanari, Giuseppe Oriolo, and Serena\nIvaldi. A multimode teleoperation framework\nfor humanoid loco-manipulation: An appli-\ncation for the icub robot.\nIEEE Robotics &\nAutomation Magazine, 2019. 3\n[66] Luka Peternel and Jan Babi\u010d.\nLearning of\ncompliant human\u2013robot interaction using full-\nbody haptic interface. Advanced Robotics, 2013.\n3\n[67] Dean A. Pomerleau. Alvinn: An autonomous\nland vehicle in a neural network. In NIPS, 1988.\n1, 3\n[68] Amartya\nPurushottam,\nYeongtae\nJung,\nChristopher Xu, and Joao Ramos. Dynamic\nmobile manipulation via whole-body bilateral\nteleoperation of a wheeled humanoid. arXiv\npreprint arXiv:2307.01350, 2023. 3\n[69] Ilija Radosavovic, Tete Xiao, Stephen James,\nPieter Abbeel, Jitendra Malik, and Trevor Dar-\nrell. Real-world robot learning with masked\nvisual pre-training. CoRL, 2022. 3\n[70] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken\nGoldberg, Trevor Darrell, and Jitendra Ma-\nlik. Robot learning with sensorimotor pre-\ntraining. arXiv preprint arXiv:2306.10007, 2023.\n9\n[71] Rouhollah\nRahmatizadeh,\nPooya\nAbol-\nghasemi, Ladislau B\u00f6l\u00f6ni, and Sergey Levine.\nVision-based multi-task manipulation for\ninexpensive robots using end-to-end learning\nfrom demonstration. 2018 IEEE International\nConference on Robotics and Automation (ICRA),\npages 3758\u20133765, 2017. 3\n[72] Joao Ramos and Sangbae Kim. Humanoid dy-\nnamic synchronization through whole-body\nbilateral feedback teleoperation. IEEE Trans-\nactions on Robotics, 2018. 3\n[73] Olaf Ronneberger,\nPhilipp Fischer,\nand\nThomas Brox.\nU-net: Convolutional net-\nworks for biomedical image segmentation.\nArXiv, abs/1505.04597, 2015. URL https://api.\nsemanticscholar.org/CorpusID:3719281. 19\n[74] Erick Rosete-Beas, Oier Mees, Gabriel Kalweit,\nJoschka Boedecker, and Wolfram Burgard. La-\ntent plans for task-agnostic offline reinforce-\nment learning. In Conference on Robot Learn-\ning, pages 1838\u20131849. PMLR, 2023. 3\n[75] Max Schwarz, Christian Lenz, Andre Rochow,\nMichael Schreiber, and Sven Behnke. Nim-\nbro avatar: Interactive immersive telepresence\nwith force-feedback telemanipulation. In 2021\nIEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), pages 5312\u2013\n5319, 2021. 3\n[76] Mingyo Seo, Steve Han, Kyutae Sim, Se-\nung Hyeon Bang, Carlos Gonzalez, Luis Sentis,\nand Yuke Zhu. Deep imitation learning for\nhumanoid loco-manipulation through human\nteleoperation. Humanoids, 2023. 3\n[77] Nur Muhammad (Mahi) Shafiullah, Zichen Jeff\nCui, Ariuntuya Altanzaya, and Lerrel Pinto.\nBehavior transformers: Cloning k modes with\none stone. ArXiv, abs/2206.11251, 2022. 3\n15\nMobile ALOHA: https://mobile-aloha.github.io\n[78] Nur Muhammad Mahi Shafiullah, Anant\nRai, Haritheja Etukuru, Yiqian Liu, Ishan\nMisra, Soumith Chintala, and Lerrel Pinto.\nOn bringing robots home.\narXiv preprint\narXiv:2311.16098, 2023. 3\n[79] Dhruv Shah, Ajay Sridhar, Arjun Bhorkar,\nNoriaki Hirose, and Sergey Levine.\nGnm:\nA general navigation model to drive any\nrobot. In 2023 IEEE International Conference on\nRobotics and Automation (ICRA), pages 7226\u2013\n7233. IEEE, 2023. 3, 5\n[80] Lin Shao, Toki Migimatsu, Qiang Zhang,\nKaren Yang, and Jeannette Bohg.\nCon-\ncept2robot: Learning manipulation concepts\nfrom instructions and human demonstrations.\nThe International Journal of Robotics Research,\n40(12-14):1419\u20131434, 2021. 3\n[81] Lucy Xiaoyang Shi, Archit Sharma, Tony Z\nZhao, and Chelsea Finn.\nWaypoint-based\nimitation learning for robotic manipulation.\nCoRL, 2023. 2, 3, 5\n[82] Mohit Shridhar, Lucas Manuelli, and Dieter\nFox. Cliport: What and where pathways for\nrobotic manipulation. ArXiv, abs/2109.12098,\n2021. 3\n[83] Mohit Shridhar, Lucas Manuelli, and Dieter\nFox.\nPerceiver-actor: A multi-task trans-\nformer for robotic manipulation.\nArXiv,\nabs/2209.05451, 2022. 3\n[84] Laura Smith, Nikita Dhawan, Marvin Zhang,\nPieter Abbeel, and Sergey Levine.\nAvid:\nLearning multi-stage tasks via pixel-level\ntranslation of human videos. arXiv preprint\narXiv:1912.04443, 2019. 3\n[85] Jiaming Song, Chenlin Meng, and Stefano Er-\nmon.\nDenoising diffusion implicit models.\narXiv preprint arXiv:2010.02502, 2020. 9, 19\n[86] Charles Sun, Jedrzej Orbik, Coline Manon\nDevin, Brian H Yang, Abhishek Gupta, Glen\nBerseth, and Sergey Levine. Fully autonomous\nreal-world reinforcement learning with appli-\ncations to mobile manipulation. In Conference\non Robot Learning, 2021. 3\n[87] Susumu Tachi, Yasuyuki Inoue, and Fumihiro\nKato. Telesar vi: Telexistence surrogate an-\nthropomorphic robot vi. International Journal\nof Humanoid Robotics. 3\n[88] Eugene Valassakis, Georgios Papagiannis,\nNorman Di Palo, and Edward Johns. Demon-\nstrate once, imitate immediately (dome):\nLearning visual servoing for one-shot imita-\ntion learning. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems\n(IROS), 2022. 3\n[89] Chen Wang, Linxi Fan, Jiankai Sun, Ruo-\nhan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu,\nand Anima Anandkumar. Mimicplay: Long-\nhorizon imitation learning by watching hu-\nman play.\narXiv preprint arXiv:2302.12422,\n2023. 3\n[90] Josiah Wong, Albert Tung, Andrey Kurenkov,\nAjay Mandlekar, Li Fei-Fei, Silvio Savarese,\nand Roberto Mart\u00edn-Mart\u00edn. Error-aware im-\nitation learning from teleoperation data for\nmobile manipulation. In Conference on Robot\nLearning, 2022. 3\n[91] Bohan Wu, Roberto Martin-Martin, and Li Fei-\nFei. M-ember: Tackling long-horizon mobile\nmanipulation via factorized domain transfer.\nICRA, 2023. 3\n[92] Jimmy Wu, Rika Antonova, Adam Kan,\nMarion Lepert, Andy Zeng, Shuran Song,\nJeannette Bohg, Szymon Rusinkiewicz, and\nThomas Funkhouser. Tidybot: Personalized\nrobot assistance with large language models.\nIROS, 2023. 3\n[93] Keenan\nA\nWyrobek,\nEric\nH\nBerger,\nHF Machiel Van der Loos, and J Ken-\nneth Salisbury. Towards a personal robotics\ndevelopment platform: Rationale and design\nof an intrinsically safe personal robot. In 2008\nIEEE International Conference on Robotics and\nAutomation, 2008. 2\n[94] Fei Xia, Chengshu Li, Roberto Mart\u00edn-Mart\u00edn,\nOr Litany, Alexander Toshev, and Silvio\nSavarese. Relmogen: Integrating motion gen-\neration in reinforcement learning for mobile\nmanipulation. In 2021 IEEE International Con-\nference on Robotics and Automation (ICRA),\n2021. 3\n[95] Annie Xie, Lisa Lee, Ted Xiao, and Chelsea\nFinn. Decomposing the generalization gap in\nimitation learning for visual robotic manipu-\nlation. arXiv preprint arXiv:2307.03659, 2023.\n5\n[96] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen,\nHomanga Bharadhwaj, Samarth Sinha, and\nAnimesh Garg. Learning by watching: Physi-\ncal imitation of manipulation skills from hu-\nman videos. In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems\n(IROS), pages 7827\u20137834. IEEE, 2021. 3\n[97] Jingyun Yang, Junwu Zhang, Connor Settle,\nAkshara Rai, Rika Antonova, and Jeannette\nBohg. Learning periodic tasks from human\ndemonstrations. In 2022 International Confer-\n16\nMobile ALOHA: https://mobile-aloha.github.io\nence on Robotics and Automation (ICRA), pages\n8658\u20138665. IEEE, 2022. 3\n[98] Jonathan Heewon Yang, Dorsa Sadigh, and\nChelsea Finn.\nPolybot: Training one pol-\nicy across robots while embracing variability.\nIn Conference on Robot Learning, pages 2955\u2013\n2974. PMLR, 2023. 3\n[99] Ruihan Yang, Yejin Kim, Aniruddha Kemb-\nhavi, Xiaolong Wang, and Kiana Ehsani. Har-\nmonic mobile manipulation. arXiv preprint\narXiv:2312.06639, 2023. 3\n[100] Taozheng Yang, Ya Jing, Hongtao Wu, Jiafeng\nXu, Kuankuan Sima, Guangzeng Chen, Qie\nSima, and Tao Kong. Moma-force: Visual-\nforce imitation for real-world mobile manipu-\nlation. arXiv preprint arXiv:2308.03624, 2023.\n3\n[101] Naoki Yokoyama, Alexander William Clegg,\nEric Undersander, Sehoon Ha, Dhruv Batra,\nand Akshara Rai. Adaptive skill coordination\nfor robotic mobile manipulation. arXiv pre-\nprint arXiv:2304.00410, 2023. 3\n[102] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep\nDasari, Tianhao Zhang, Pieter Abbeel, and\nSergey Levine. One-shot imitation from ob-\nserving humans via domain-adaptive meta-\nlearning. arXiv preprint arXiv:1802.01557, 2018.\n3\n[103] Andy Zeng, Peter R. Florence, Jonathan Tomp-\nson, Stefan Welker, Jonathan Chien, Maria At-\ntarian, Travis Armstrong, Ivan Krasin, Dan\nDuong, Vikas Sindhwani, and Johnny Lee.\nTransporter networks: Rearranging the visual\nworld for robotic manipulation. In Conference\non Robot Learning, 2020. 3\n[104] Tony Z Zhao, Vikash Kumar, Sergey Levine,\nand Chelsea Finn. Learning fine-grained bi-\nmanual manipulation with low-cost hardware.\nRSS, 2023. 1, 2, 3, 4, 5, 8, 9\n17\nMobile ALOHA: https://mobile-aloha.github.io\nA. Appendix\nA.1. High Five\nKitchen island\nHigh Five: The robot base is initialized next to the kitchen island. The robot keeps moving around the kitchen island until a human is in front of \nit, then high five with the human. Each demo has 2000 steps or 40 seconds, and typically contains 3-4 high fives.\n#1\n#2\ninit.\nFigure 6: Task Definition of High Five.\nWe include the illustration for the High Five task in Figure 6. The robot needs to go around the kitchen\nisland, and whenever a human approach it from the front, stop moving and high five with the human. After\nthe high five, the robot should continue moving only when the human moves out of its path. We collect data\nwearing different clothes and evaluate the trained policy on unseen persons and unseen attires. While this\ntask does not require a lot of precision, it highlights Mobile ALOHA\u2019s potential for studying human-robot\ninteractions.\nA.2. Example Image Observations\nFigure 7 showcases example images of Wipe Wine captured during data collection. The images, arranged\nsequentially in time from top to bottom, are sourced from three different camera angles from left to right\ncolumns: the top egocentric camera, the left wrist camera, and the right wrist camera. The top camera\nis stationary with respect to the robot frame. In contrast, the wrist cameras are attached to the arms,\nproviding close-up views of the gripper in action. All cameras are set with a fixed focal length and feature\nauto-exposure to adapt to varying light conditions. These cameras stream at a resolution of 480 \u00d7 640 and a\nframe rate of 30 frames per second.\nFigure 7: Example Image Observations of Wipe Wine. We show the observations from the top camera, left wrist\ncamera and right wrist camera from left to right columns. These images are arranged sequentially in time from top to\nbottom.\n18\nMobile ALOHA: https://mobile-aloha.github.io\nA.3. Experiment Details and Hyperparameters of ACT, Diffusion Policy and VINN\nWe carefully tune the baselines and include the hyperparameters for the baselines and co-training in\nTable 5, 6, 7, 8, 9.\nsample prob. from Mobile ALOHA data\n0.5\nsample prob. from ALOHA data\n0.5\nTable 5: Hyperparameters of co-training.\nlearning rate\n2e-5\nbatch size\n16\n# encoder layers\n4\n# decoder layers\n7\nfeedforward dimension\n3200\nhidden dimension\n512\n# heads\n8\nchunk size\n45\nbeta\n10\ndropout\n0.1\nbackbone\npretrained ResNet18[40]\nTable 6: Hyperparameters of ACT.\nlearning rate\n1e-4\nbatch size\n32\nchunk size\n64\nscheduler\nDDIM[85]\ntrain and test diffusion steps\n50, 10\nema power\n0.75\nbackbone\npretrained ResNet18[40]\nnoise predictor\nUNet[73]\nimage augmentation\nRandomCrop(ratio=0.95) &\nColorJitter(brightness=0.3, contrast=0.4, saturation=0.5) &\nRandomRotation(degrees=[-5.0, 5.0])\nTable 7: Hyperparameters of Diffusion Policy.\nlearning rate\n3e-4\nbatch size\n128\nepochs\n100\nmomentum\n0.9\nweight decay\n1.5e-6\nTable 8: Hyperparameters of BYOL, the feature extractor of VINN.\nk (nearest neighbour)\nselected with lowest validation loss\nchunk size\n100\nstate weight\n5\ncamera feature weight\n1:1:1 (for front, left and right wrist)\nTable 9: Hyperparameters of VINN + Chunking.\n19\nMobile ALOHA: https://mobile-aloha.github.io\nA.4. Open-Loop Replaying Errors\nFigure 8 shows the spread of end-effector error at the end of replaying a 300 steps (6s) demonstration. The\ndemonstration contains a 180 degree turn with radius of roughly 1m. At the end of the trajectory, the right\narm would reach out to a piece of paper on the table and tap it gently. The tapping position are then marked\non the paper. The red cross denotes the original tapping position, and the red dots are 20 replays of the\nsame trajectory. We observe significant error when replaying the base velocity profile, which is expected\ndue to the stochasticity of the ground contact and low-level controller. Specifically, all replay points are\nbiased to the left side by roughly 10cm, and spread along a line of roughly 20cm. We found our policy to be\ncapable of correcting such errors without explicit localization such as SLAM.\nFigure 8: Open-lopp Replay Errors. We mark the right arm end-effector position on a piece of paper for the original\nepisode (red cross), and 20 replays of the same episode (red dots).\n20\n"
  },
  {
    "title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs",
    "link": "https://arxiv.org/pdf/2401.02411.pdf",
    "upvote": "12",
    "text": "What You See is What You GAN:\nRendering Every Pixel for High-Fidelity Geometry in 3D GANs\nAlex Trevithick*2, Matthew Chan1, Towaki Takikawa1, Umar Iqbal1, Shalini De Mello1, Manmohan\nChandraker2, Ravi Ramamoorthi2, and Koki Nagano1\n1NVIDIA\n2University of California, San Diego\nFigure 1. Left: Our results. The split view in the middle demonstrates the high degree of agreement between our 2D rendering and\ncorresponding 3D geometry. Our method can learn fine-grained 3D details (e.g., eyeglass frame and cat\u2019s fur) that are geometrically well-\naligned to 2D images without multiview or 3D scan data. Right: Comparison with EG3D [7]. Our tight SDF prior provides smooth and\ndetailed surfaces on the face and hat while EG3D exhibits geometry artifacts and discrepancies between geometry and rendering. Please\nsee Fig. 5 and the accompanying video for more examples, and Fig. 6 for comparison to other baselines.\nAbstract\n3D-aware Generative Adversarial Networks (GANs)\nhave shown remarkable progress in learning to generate\nmulti-view-consistent images and 3D geometries of scenes\nfrom collections of 2D images via neural volume render-\ning. Yet, the significant memory and computational costs of\ndense sampling in volume rendering have forced 3D GANs\nto adopt patch-based training or employ low-resolution ren-\ndering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved\ngeometry. Consequently, 3D GANs have not yet been able\nto fully resolve the rich 3D geometry present in 2D images.\nIn this work, we propose techniques to scale neural vol-\nume rendering to the much higher resolution of native 2D\nimages, thereby resolving fine-grained 3D geometry with\n*This project was initiated and substantially carried out during an in-\nternship at NVIDIA.\nunprecedented detail.\nOur approach employs learning-\nbased samplers for accelerating neural rendering for 3D\nGAN training using up to 5 times fewer depth samples.\nThis enables us to explicitly \u201drender every pixel\u201d of the\nfull-resolution image during training and inference without\npost-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method\nsynthesizes high-resolution 3D geometry and strictly view-\nconsistent images while maintaining image quality on par\nwith baselines relying on post-processing super resolution.\nWe demonstrate state-of-the-art 3D gemetric quality on\nFFHQ and AFHQ, setting a new standard for unsupervised\nlearning of 3D shapes in 3D GANs.\n1. Introduction\nTraining 3D generative models from the abundance of 2D\nimages allows the creation of 3D representations of real-\nworld objects for content creation and novel view synthe-\n1\narXiv:2401.02411v1  [cs.CV]  4 Jan 2024\nsis [59].\nRecently, 3D-aware generative adversarial net-\nworks (3D GANs) [6, 7, 19, 43, 45, 49, 53, 69, 70, 74, 76]\nhave emerged as a powerful way to learn 3D representations\nfrom collections of 2D images in an unsupervised fashion.\nThese methods employ differentiable rendering to compare\nrendered 3D scenes with 2D data using adversarial train-\ning [18]. Among the various 3D representations, Neural\nRadiance Fields (NeRF) [38] have become a popular choice\namong recent successful 3D GANs. However, the signifi-\ncant computational and memory cost of volume rendering\nhas prevented 3D GANs from scaling to high-resolution\noutput. For instance, generating a single 512x512 image via\nvolume rendering requires evaluating as many as 25 million\ndepth samples, if 96 depth samples are used per ray using\nimportance sampling [7, 10, 38, 53]. Given that GAN train-\ning typically requires rendering tens of millions of images,\nthe training process could require evaluating hundreds of\ntrillions of depth samples.\nDuring training, all intermediate operations must be\nstored in GPU memory for every depth sample for the back-\nward pass. Therefore, existing methods resort to working\non patches [10, 49, 53] or adopting a low resolution neural\nrendering combined with post-processing 2D super resolu-\ntion (SR) [7, 19, 43, 45, 70]. However, patch-based methods\nhave limited receptive fields over scenes, leading to unsatis-\nfactory results, and the hybrid low-resolution rendering and\nSR scheme inevitably sacrifices the multiview consistency\nand the accuracy of 3D geometry. While many techniques\nhave been developed to improve the image quality of 3D\nGANs to match that of 2D GANs, the challenge of resolv-\ning the corresponding high-resolution 3D geometry remains\nunsolved (see Figs. 1 and 6 for our results and comparison\nto the current state-of-the-art).\nScaling 3D GANs to operate natively at the 2D pixel res-\nolution requires a novel approach for sampling. Fig. 2 com-\npares the state-of-the-art 3D GAN, EG3D model, trained\nwith and without1 SR. EG3D employs 96 dense depth sam-\nples in total using two-pass importance sampling [38] dur-\ning training, which requires half a terabyte of GPU memory\nat 256 \u00d7 256 resolution, making scaling to higher resolu-\ntions infeasible. Furthermore, Fig. 2 demonstrates that us-\ning 96 dense samples still results in undersampling, as evi-\ndenced by the speckle noise patterns visible in the zoomed-\nin view, leading to considerably worse FID (inset in Fig. 2).\n3D GANs relying on post-processing SR layers can repair\nthese undersampling artifacts at the cost of a high-fidelity\n3D representation.\nIn this work, we address the challenge of scaling neural\nvolume rendering to high resolutions by explicitly rendering\nevery pixel, ensuring that \u201cwhat you see in 2D, is what you\nget in 3D\u201d \u2014 generating an unprecedented level of geomet-\nric details as well as strictly multiview-consistent images.\nOur contributions are the following:\n\u2022 We introduce an SDF-based 3D GAN to represent high-\n1Triplane resolution is doubled to compensate for the loss of capacity\nfrom removing the SR layers.\nFigure 2. Samples from EG3D 256 model. Right: Volume render-\ning with 48 coarse samples and 48 fine samples per ray with two-\npass importance sampling [38] results in undersampling, leading\nto noticeable noisy artifacts. Left: These artifacts are repaired by\nsuper resolution (SR). An unsharp mask has been applied to the\nzoomed views for presentation purposes.\nfrequency geometry with spatially-varying surface tight-\nness that increases throughout training (subsection 4.1\nand 4.5), in turn facilitating low sample rendering.\n\u2022 We propose a generalizable learned sampler conditioned\non cheap low-resolution information to enable full-\nresolution rendering during training for the first time\n(subsection 4.2 and 4.3).\n\u2022 We show a robust sampling strategy for the learned sam-\npler (subsection 4.4) that produces stable neural rendering\nusing significantly fewer depth samples (see Fig. 8). Our\nsampler can operate with just 20 samples per ray com-\npared to existing 3D GANs which must use at least 96\nsamples per ray (see Table 3).\n\u2022 Together, our contributions result in the state-of-the-art\ngeometry for 3D GANs while rendering with quality on\npar with SR baselines (see Fig. 1). For more results, see\nFig. 5 and for comparison to other baselines, see Fig. 6.\n2. Related Work\nWe begin by reviewing the prior-arts of 3D generative mod-\nels and their current shortcomings. We then cover founda-\ntional techniques for 3D geometry representation and neural\nrendering from which we take inspiration. We then discuss\nexisting methods for accelerating neural volume rendering,\nwhich usually operate per-scene.\n2.1. 3D Generative Models\nJust like 2D GANs, 3D-aware GANs train from a collection\nof 2D images, but employ a 3D representation and differen-\ntiable rendering to learn 3D scenes without requiring multi-\nview images or ground truth 3D scans. Some of the most\nsuccessful works use a neural field [68] in combination with\na feature grid [7] as their 3D representation, and use neural\nvolume rendering [38] as the differentiable renderer.\n2\nHowever, due to the significant memory and compu-\ntational cost of neural volume rendering, many previous\nworks perform rendering at low-resolution and rely on a\n2D post-processing CNN [7, 19, 43, 45, 70], which hal-\nlucinates the high-frequency details in a view-inconsistent\nmanner while sacrificing 3D consistency and the quality of\nthe resolved 3D geometry.\nTo ensure strict 3D consistency, other previous works\nseek to render at high-resolutions and propose techniques\nto address the prohibitive computational costs. One line\nof work leverages the sparse nature of 3D scenes to speed\nup rendering, in particular, structures such as 2D manifolds\n[14, 67], multiplane images [75] and sparse voxels [50]. Al-\nthough they are more efficient, sparse representations pro-\nvide only coarse [50] or category-specific [14, 67] accel-\neration structures, which poses constraints on the diversity\nand viewing angles of the generated scenes. Our sampling-\nbased method, on the other hand, generalizes to every new\nscene and adaptively accelerates rendering on a per ray\nbasis. Another line of work enables high-resolution ren-\ndering with patch-based training [10, 53].\nIn particular,\nMimic3D [10] achieves significantly improved 2D image\nquality, but the patch-based training limits the receptive\nfields, and the generated geometry does not faithfully repre-\nsent the 2D data due to the patch-wise perceptual loss. Our\nmethod renders the entire image at once and the resulting\ngeometry is aligned with the rendering (see Figs. 1 and 6).\nRecently, a new family of generative approaches us-\ning diffusion models has been proposed to tackle condi-\ntional tasks including novel view synthesis [8, 58] and text-\nbased 3D generation[63]. Most of these 3D-aware diffu-\nsion models combine a 3D inductive bias modeled via neu-\nral field representations and a 2D image denoising objec-\ntive to learn 3D scene generation. While these models en-\nable unconditional 3D generation, they require multiview\nimages [24, 55, 63] or 3D data, such as a point cloud [42].\nScore Distillation Sampling [47] may be used for distil-\nlation from a pre-trained 2D diffusion model when only\nmonocular 2D data is available, but diffusion models incur\nsignificant computational costs due to their iterative nature\nand most of the existing methods require optimization per\nscene [9, 20, 33, 61, 66].\n2.2. Learning High-Fidelity Geometry\nPrior works on 3D GANs have typically represented the ge-\nometry as a radiance field [38], which lacks a concrete defi-\nnition for where the surface geometry resides in the field, re-\nsulting in bumpy surfaces. A number of works [44, 62, 71]\nhave proposed alternate representations based on implicit\nsurfaces (such as signed distance functions, or SDFs) that\ncan be used with neural volume rendering. In these works,\nthe implicit surface is typically softened by a parameter for\nvolume rendering.\nOther works [32, 64] improve on these implicit surface\nrepresentations by leveraging feature grids for higher com-\nputational efficiency and resolution. Adaptive Shells [65]\nfurther improve on quality by making the softness parame-\nter spatially-varying, as many objects have hard boundaries\nonly in certain parts.\nWe use an implicit surface repre-\nsentation based on VolSDF [72], and leverage a spatially-\nvarying parameter similar to Adaptive Shells [65] to control\nthe softness of the surface, as humans and animals benefit\nfrom both hard surfaces (e.g. skin, eyes) and soft, volumet-\nric representations (e.g. hair). Although other works such\nas StyleSDF [45] have similarly leveraged implicit surfaces\nin a 3D GAN framework, the lack of spatial-variance and\nhigh-resolution rendering led to over-smoothed geometry\nnot faithful to the rendered images.\n2.3. Accelerating Neural Volume Rendering\nAs mentioned in Section 2.1, accelerating 3D GANs typi-\ncally relies on acceleration structures such as octrees [31,\n56, 73], which in generative settings [14, 50, 67] are lim-\nited to be coarse or category-specific due to the difficulty\nof making them generalize per-scene.\nInstead, we look\nto a class of methods that do not rely on an acceleration\nstructure. Some of these works learn a per-scene sampling\nprior on a per-ray basis using a binary classifier [41], a den-\nsity estimator [30, 46], a coarse proxy from a 3D cost vol-\nume [34], or an interval length estimator [35] on discrete\ndepth regions along the ray. Other works use importance\nsampling [21, 38] to sample additional points. We take in-\nspiration from works on density estimators [30] and propose\nto learn a scene-conditional proposal network that general-\nizes across scenes instead of being category-specific or op-\ntimized per-scene.\nThere are also other methods to accelerate rendering by\nutilizing more efficient representations, such as gaussian\nsplats [29] and light field networks [52].\nMore efficient\nfeature grids [40, 57] based on hashing can also be used\nto accelerate rendering. However, mapping to these repre-\nsentations in a GAN framework is not straightforward. In\ncontrast, our sampling strategy can be used for any NeRF\nrepresentation.\n3. Background\nWe begin with background on the methodology of the state-\nof-the-art 3D-aware GANs as our method relies on a simi-\nlar backbone for mapping to 3D representations. 3D GANs\ntypically utilize a StyleGAN-like [27] architecture to map\nfrom a simple Gaussian prior to the conditioning of a NeRF,\nwhether that be an MLP [19, 45], MPI [75], 3D feature\ngrid [50], manifolds [14, 67] or triplane [7, 10, 53]. We in-\nherit the latter triplane conditioning for its high expressivity\nand efficiency, in which three axis-aligned 2D feature grids\n(fxy, fxz, fyz), provide NeRF conditioning by orthogonal\nprojection and interpolation. As in the previous methods,\nthe mapping and synthesis networks from StyleGAN2 can\neasily be adapted to create the 2D triplane representation\nfrom noise z \u2208 R512. Specifically, w = Mapping(z) con-\nditions the creation of the triplane T(w) \u2208 R3\u00d732\u00d7512\u00d7512\nfrom a Synthesis network, corresponding to three axis-\n3\nFigure 3. Here we show our proposed pipeline and its intermediate outputs. Beginning from the triplane T, we trace uniform samples to\nprobe the scene, yielding low-resolution I128 and weights P128. These are fed to a CNN which produces high-resolution proposal weights\n\u02c6P512 (weights are visualized as uniform level sets). We perform robust sampling and volume render to get the final image I512 and the\nsurface variance B.\naligned 2D feature grids of spatial resolution 512\u00d7512 and\nfeature size 32.\nTo create high-fidelity geometry, our method builds upon\nVolSDF [72]: Instead of directly outputting the opacity \u03c3 of\na point x \u2208 R3, an SDF value s is output and transformed\nto \u03c3 using a Laplacian CDF:\n\u03c3 = 1\n\u03b2\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2 exp\n\u0010\ns\n\u03b2\n\u0011\nif s \u2264 0\n1 \u2212 1\n2 exp\n\u0010\n\u2212 s\n\u03b2\n\u0011\nif s > 0\n(1)\nwhere \u03b2 is the variance of the Laplacian distribution govern-\ning the \u201ctightness\u201d of the representation. One distinct bene-\nfit of an SDF-based representation is the ease of extracting\nthe surface. StyleSDF [45] also utilizes this intermediate\ngeometry representation without a triplane, enforcing the\nusual Eikonal constraint.\nUsing the triplane conditioning and Eq. 1, we can as-\nsign each point in the volume with its opacity \u03c3 and radi-\nance c using a lightweight MLP. For a given camera ray\nr(t) = o + td, we approximate the volumetric rendering\nintegral C(r) [36] by sampling ray distances ti with their\ncorresponding \u03c3i and ci before computing\n\u02c6C(r) =\nN\nX\ni=1\nwici where wi = Ti(1 \u2212 exp(\u2212\u03c3i\u03b4i)),\nTi = exp\n\uf8eb\n\uf8ed\u2212\ni\u22121\nX\nj=1\n\u03c3j\u03b4j\n\uf8f6\n\uf8f8 and \u03b4i = ti+1 \u2212 ti.\n(2)\nHere, Ti denotes the accumulated transmittance and \u03b4i is\nthe distance between adjacent samples along the ray.\nIt is possible to develop a more efficient estimator for\nthis sum with fewer samples by using importance sampling\ntechniques in computer graphics. Typically, one computes\na piecewise constant probability distribution pj =\n\u02c6\nwj\nP\nj \u02c6\nwj ,\nwhere j refers to the jth bin or region, and \u02c6\nwj is an estimate\nof wj for that region, for example obtained by explicitly\ntracing coarse samples. For a given t, we first find the region\nj(t) and then set p(t) = pj. From this, one can compute a\n(piecewise linear) cumulative distribution function or CDF\n\u03a6(t) which has a range from 0 to 1. We can then perform\ninverse CDF sampling to define the sample points,\nti = \u03a6\u22121(ui),\n(3)\nwhere ui is a random number from 0 to 1 (sorted to be an\nincreasing sequence).\nDiscussion\nWe improve on previous works such as\nNeRF [38] and EG3D [7] by stratifying2 the random num-\nbers ui during training; this leads to significantly lower ren-\ndering variance, especially at low sample counts N [39].\nWe also develop a neural method to predict a good distribu-\ntion pj (and hence \u03a6) for importance sampling at high spa-\ntial resolution, without needing to exhaustively step through\nthe ray.\n4. Method\nIn this section, we describe our method beginning with\nour SDF-based NeRF parametrization (subsection 4.1). We\nthen overview how we render at high-resolution in three\nstages: first, a low-resolution probe into the 3D scene (sub-\nsection 4.2); second a high-resolution CNN proposal net-\nwork (subsection 4.3); and third a robust sampling method\nfor the resultant proposals (subsection 4.4). Next we de-\nscribe regularizations (subsection 4.5) for stable training,\nand finally our entire training pipeline (subsection 4.6).\n4.1. Mapping to a 3D Representation\nBeginning from a noise vector z, we synthesize the initial\ntriplane T \u2032 with StyleGAN [25] layers as detailed in Sec. 3.\nIn contrast to previous methods, we then generate more ex-\npressive triplane features T with an extra synthesis block for\neach orthogonal plane: fij = SynthesisBlock(f \u2032\nij) where\nij \u2208 {xy, xz, yz} and f \u2032 are the features of T \u2032. This de-\nsign choice allows disentanglement between the intermedi-\nate triplane features as plane-specific kernels can attend to\nthe features in each separate plane.\nGiven the triplane T (the left side of Fig. 3) and a point\nx \u2208 R3, we utilize an MLP to map to the SDF value s,\nvariance \u03b2 and geometry features fgeo:\n(s, \u03b2, fgeo) = MLPSDF(PosEnc(x), Tx)\n(4)\n2dividing the unit interval into bins and taking a sample from each bin.\n4\nwhere PosEnc is the positional encoding from NeRF [38]\nand Tx are the features corresponding to x gathered from T\nby projecting x to each of the axis-aligned planes and taking\nthe Hadamard product of the three resultant vectors [16].\nWe initialize the ReLU MLPSDF as in SAL [4] to ensure an\napproximately spherical SDF in the early part of training.\nAdditionally note that \u03b2 varies spatially in the volume un-\nlike [45, 72], allowing us to regularize its values later on.\nUsing Eq. 1, we transform s and \u03b2 into opacity \u03c3. We\ncan now predict the radiance with a separate MLPc condi-\ntioned on the geometry features and viewing direction v as\nc = MLPc(PosEnc(v), fgeo).\n(5)\nNote that in contrast to most 3D GANs, we condition\nradiance on the viewing direction, allowing a more expres-\nsive generator. Thus, given a triplane, we can render any\npixel by computing \u03c3 and c for points along the ray to ap-\nproximate the volumetric rendering integral as described in\nSec. 3.\n4.2. High-Resolution Proposal Network\nWe now have our mapping from a latent code z to a 3D\nNeRF representation.\nHowever, volumetric rendering of\nNeRFs at higher resolutions requires extremely large num-\nbers of samples, and thus both memory and time. Instead\nof naive dense sampling at a high resolution, we propose\nto leverage low-resolution renderings to cheaply probe the\n3D representation (visualized on the left of Fig. 3) for the\ncreation of proposal distributions at high-resolution. Given\na target camera and triplane T, we first trace 192 coarse\nsamples at low-resolution (128 \u00d7 128) to compute a low-\nresolution RGB image I128 \u2208 R3\u00d7128\u00d7128 and a tensor\nof weights P128 \u2208 R192\u00d7128\u00d7128 (visualized after low-\nresolution rendering in Fig. 3). Each 192-dimensional vec-\ntor corresponds to a piecewise constant PDF with CDF \u03a6 as\nseen in Eq. 3.\nConditioned on the low-resolution probe, we predict a\ntensor of proposal volume rendering weights at the high-\nresolution (512 \u00d7 512):\n\u02c6P512 = Softmax(CNN(P128, I128)) \u2208 R192\u00d7512\u00d7512, (6)\nwhere CNN is a lightweight network that up-samples the\nlow-resolution weights, Softmax produces discrete distribu-\ntions along each ray, and the \u02c6 denotes that this is an esti-\nmated quantity. This corresponds to the Proposal in Fig. 3\nand the yellow distribution in Fig. 4. Note that allocating\n192 samples at 128 \u00d7 128 is equivalent to allocating just 12\nat 512 \u00d7 512.\n4.3. Supervising the Proposal Network\nHaving described the input and output of our high-\nresolution proposal network, we now show its supervision.\nFrom the target camera, we can also trace 192 coarse sam-\nples at high resolution for a small 64 \u00d7 64 patch, giv-\ning us a ground truth tensor of volume rendering weights\nPpatch \u2208 R192\u00d764\u00d764. We then prepare this tensor for super-\nvision by computing:\n\u00afPpatch = Normalize(Suppress(Blur(Ppatch)))\n(7)\nwhere Blur applies a 1D Gaussian kernel to the input dis-\ntributions, Suppress(x) = x if x \u2265 5e \u2212 3 and 0 other-\nwise, and Normalize is L1 normalization to create a valid\ndistribution. This corresponds to the patch loss in Fig. 3\nand the purple distribution in Fig. 4. These operations cre-\nate less noisy distributions to facilitate accurate learning of\nthe high-frequency integrand which may be undersampled\nin the coarse pass.\nWe can then compare the predicted and cleaned ground\ntruth distributions with a cross-entropy loss:\nLsampler = CrossEntropy( \u00afPpatch, \u02c6Ppatch)\n(8)\nwhere \u02c6Ppatch is the corresponding patch of weights in \u02c6P512\nand CrossEntropy denotes the average cross-entropy be-\ntween all pairs of pixelwise discrete distributions; for each\npair (\u00afp, \u02c6p), we compute P \u2212\u00afpj log \u02c6pj. Since we only need\nto compute this supervision for a small patch, the overhead\nof sampler training is not significant.\n4.4. Sampling from the Proposal Network\nHaving shown how to train and predict high-resolution dis-\ntributions, we now overview how to sample the resultant\nproposals. As seen in Fig. 4, the proposals are often slightly\noff; this is due to the high frequency nature of the underly-\ning integrand in blue.\nIn order to utilize the information from the sampler,\nwe propose to filter predicted PDFs for better estimation.\nSpecifically, for each discrete predicted PDF \u02c6p, we com-\npute the smallest set of bins whose probability exceeds a\nthreshold \u03c4 = 0.98: We find the smallest subset I \u2286\n{1, 2, ..., 192} such that\nX\ni\u2208I\n\u02c6pi \u2265 \u03c4.\n(9)\nThis operation resembles nucleus sampling in NLP [23].\nWe define our sampling PDF q with probability qi =\n1\n|I|\nif i \u2208 I and 0 otherwise (the green distribution in Fig. 4).\nFor each PDF q, we compute its CDF \u03a6 and perform\nstratified inverse transform sampling to create the samples\n(illustrated as adaptive sampling near the surface in Fig. 3).\nIn practice, on top of the 12 samples from the coarse probe\n(for the high-resolution image), we take an additional 18\nsamples per pixel adaptively based on the variance of the\npredicted distributions. The details are given in the supple-\nment.\n4.5. Regularization for High-Resolution Training\nIn order to render accurately under low sample budget per\nray, we desire the surface to be tight, i.e., the set of points\nalong the ray, which contribute to the accumulated radiance\n5\nFigure 4. We visualize the volume rendering PDFs for the green\npixel in the images on the right along with sampling methods. The\nground truth distribution in blue is bimodal due to the discontinu-\nous depth. Without stratification, the samples from the predicted\nyellow PDF completely miss the second mode. Stratification re-\nduces the variance, yet also misses the second mode. Our robust\nstratified samples hit both modes despite the inaccurate predic-\ntions. The supervision PDF is visualized in purple as well.\nto be small. To accomplish this, we introduce a regular-\nization for the spatially-varying \u03b2 values. Replacing the ci\nwith the intermediate \u03b2i in Eq. 2, we can volume render\nan image of scalars, B \u2208 R512\u00d7512 (seen on the right of\nFig. 3). During training, we regularize B to be small so that\nthe surface tightens:\nLsurface =\nX\nhw\n(Bhw \u2212 Btarget)2,\n(10)\nwhere Btarget is a scalar quantity annealed towards \u03f5 > 0\nduring optimization. Note that this results in significantly\nmore pronounced and smooth geometry (see Fig. 7).\nDuring training with the proposal network, the rendering\nnetworks only receive gradients for points very near the sur-\nface (which have high density). We find that this can lead to\nundesirable density growing from the background into the\nforeground. In order to combat this, we leverage the low-\nresolution information from I128, which is computed for\nuniform samples along the ray, and thus not concentrated\nat the surface. Considering the SDF values intermediately\ncomputed for rendering, S = S128 \u2208 R192\u00d7128\u00d7128, we\nenforce the SDF decision boundary by minimizing the SDF\nlikelihood under a Laplacian distribution similar to [45, 51]:\nLdec =\nX\nzhw\nexp (\u22122|Szhw|).\n(11)\n4.6. The Training Pipeline\nIn order to begin making use of the learned sampler for\nhigh-resolution rendering, we need a good NeRF represen-\ntation from which to train it. Concurrently, we also need a\ngood sampler in order to allow NeRF to render at 512\u00d7512,\nthe input resolution to the discriminator D.\nTo solve this issue, in the early stages of training, we\nfirst learn a low-resolution (e.g., 64 \u00d7 64) 3D GAN through\nthe standard NeRF sampling techniques. We bilinearly up-\nsample our low-resolution renderings to 512\u00d7512 and blur\nthe real images to the same level. After converging at the\nlower resolution, we introduce the sampler training (sub-\nsection 4.3). Concretely, we not only render low-resolution\nimages with standard sampling, but also render sampler in-\nputs P128 and supervision patches Ppatch. This results in a\ngood initialization for the sampler.\nHaving learned an initial low-resolution 3D GAN and\nhigh-resolution sampler, we transition to rendering with the\nsampler predictions. The high-resolution proposals \u02c6P512 are\ndownsampled to the current rendering resolution, which is\nprogressively increased to the full 512\u00d7512 resolution dur-\ning training. After introducing all losses, we optimize the\nparameters of the generator G to minimize the following:\nL = Ladv+\u03bbsamplerLsampler+\u03bbsurfaceLsurface+\u03bbdecLdec (12)\nwhere\nLadv\nis\nthe\nstandard\nGAN\nloss\n[18]\nSoftplus(\u2212D(G(z)).\nNote that we do not enforce the\nEikonal constraint as we did not see a benefit. The discrim-\ninator D is trained with R1 gradient regularization [37]\nwhose weight is adapted as the resolution changes. Gener-\nator and discriminator pose conditioning follow EG3D [7].\nThe details of all hyperparameters and schedules are\npresented in the supplementary material.\n5. Results\nDatasets.\nWe benchmark on two standard datasets for 3D\nGANs: FFHQ [25] and AFHQv2 Cats [11, 28] both at res-\nolution 512 \u00d7 512. We use the camera parameters extracted\nby EG3D [7] for conditioning and rendering. Our AFHQ\nmodel is finetuned from our FFHQ model using adaptive\ndata augmentation [26]. For more results, please see the\naccompanying video.\n5.1. Comparisons\nBaselines.\nWe compare our methods against state-of-\nthe-art 3D GAN methods including those that use low-\nresolution neural rendering and 2D post-processing CNN\nsuper resolution:\nEG3D [7],\nMVCGAN [74],\nand\nStyleSDF [45]; and methods that operate entirely based on\nneural rendering: Mimic3D [10], Epigraf [53], GMPI [75],\nand GramHD [67].\nQualitative results\nFig. 5 shows the curated samples gen-\nerated by our method with FFHQ and AFHQ, demonstrat-\ning photorealistic rendering as well as high-resolution de-\ntails that align with the 2D images. Fig 6 provides qual-\nitative comparisons to baselines. EG3D shows significant\nartifacts and cannot resolve high-resolution geometry since\nit performs neural rendering at only 128 \u00d7 128. Mimic3D\nand Epigraf render all pixels with neural rendering, but the\npatch-based nature of these methods harm the overall 3D\n6\nFigure 5. Curated samples on FFHQ and AFHQ. Our method can resolve high-fidelity geometry (e.g., eyeglasses) and fine-grained details\n(e.g., stubble hair and cat\u2019s fur) as seen in the geometry and normal map.\nFigure 6. Qualitative comparisons on FFHQ with EG3D [7], Mimic3D [10] and Epigraf [53]. EG3D performs neural rendering at resolution\n128 \u00d7 128 and relies on 4\u00d7 super resolution to generate images. On the right, Mimic3D and Epigraf directly generate the image via neural\nrendering. While all other baselines use up to 192 dense depth samples per ray, our method can operate at 30 samples per ray.\nFigure 7. Ablation study on the effect of beta regularization.\ngeometry (e.g., distorted face and missing ear). Our method\nprovides both high-fidelity 3D shape (e.g., well-defined ears\nand isolated clothing collar) and high-resolution details.\nQuantitative results.\nTabs. 1 and 2 provide quantitative\ncomparisons against baselines. We measure the image qual-\nity with Fr\u00b4echet Inception Distance (FID) [22]. We assess\nthe quality of the learned geometry with a face-specific Nor-\nmal FID (FID-N) [15].\nWe render 10.79k normal maps\nfrom the NPHM [17] dataset by solving for the approxi-\nmate alignment between the average FFHQ 2D landmarks\nand the provided 3D landmarks.\nExamples are given in\nthe supplement. For each mesh, we render two views with\napproximately 20 degrees of pitch variation relative to the\nfront of the face. These views are processed to remove the\nbackground with facer [13]. For each baseline, we render\n10k normal maps and remove their background using the\npredicted mask from facer on the rendered image. We com-\npute the FID between the two sets of images. Finally, we\nalso evaluate the flatness of the learned representations with\nnon-flatness score (NFS) [54].\nOur results show the state-of-the-art image quality\n7\nMethod\nFID \u2193 FID-N\u2193 NFS\u2191\nFFHQ-512\nw/o SR\nEpigraf [7]\n9.92\u2020\n67.33\n33.95\nMimic3D [10]\n5.37\n64.97\n16.76\nGRAM-HD [67] 12.2\u2020\u2217\n-\n-\nOurs\n4.97\n60.76\n29.35\nw SR\nEG3D [7]\n4.70\n63.02\n17.54\nStyleSDF [45]\n11.19\u2020\n87.42\n22.75\nMVCGAN [74]\n13.4\u2020\u2217\n-\n-\nTable 1. Quantitative comparison with baselines with and without\nsuper resolution (SR) on the FFHQ dataset.\n\u2020 as reported in the\nprevious works. * indicates FID evaluated on 20k images.\nMethod\nFID \u2193\nNFS\u2191\nAFHQ-512\nw/o SR\nGRAM-HD [67]\n7.67\u2020\n-\nGMPI [45]\n7.79\u2020\n-\nMimic3D [10]\n4.29\n12.67\nOurs\n4.23\n21.89\nw SR\nEG3D [7]\n2.77\n14.14\nStyleSDF [45]\n7.91\n33.89\nTable 2. Quantitative results on AFHQv2 Cats. \u2020 as reported in the\nprevious works.\namong the methods that operate only with neural rendering,\nwhile achieving FID comparable to the state-of-the-art SR-\nbased method, EG3D. Our geometry quality outperforms all\nexisting methods as indicated by our state-of-the-art FID-\nN. Additionally, our high NFS scores show the 3D aspect\nof our geometry. However, since NFS simply measures the\nvariations of depth as a measure of non-flatness, it does not\nquantify the quality of geometry above a certain threshold.\nWe also compare other methods\u2019 ability to render with\nlow sample count in Tab. 3. With just 20 samples, the ren-\ndering quality of our method drops by only .3 in FID, com-\npared to the precipitous drop for other methods. This vali-\ndates our strategy to jointly learn a sampler and ensure tight\nSDF surface for operation under a limited sample budget.\n5.2. Ablation Study\nWithout our surface tightness regularization (Eq. 10), the\nSDF surface may get fuzzy, resulting in a less clean surface\n(see Fig. 7) and worse geometry scores (see Tab. 4). With-\nout our sampler or stratification during sampling, the model\ncannot learn meaningful 3D geometry with limited depth\nbudgets, creating degenerated 3D geometry as can be seen\nin Fig. 8 and significantly worse FID-N. Without our robust\nsampling strategy, the sampling becomes more susceptible\nto slight errors in the sampler due to the high-frequency na-\nture of the PDF (see Fig. 4), resulting in a noticeable drop\nin FID and occasionally creating floater geometry artifacts\n(see Fig. 8), while geometry scores remain similar.\nMethod\nFID (20)\u2193\nFID (50)\u2193\nFID (96)\u2193\nMimic3D\n53.57\n13.31\n5.37\nEG3D\n193.75\n36.82\n4.70\nOurs\n5.28\n4.97\n4.97\nTable 3. FID comparison on FFHQ using various sample counts.\nThe samples per pixel are given in the parentheses of the metric.\nMethod\nFID \u2193\nFID-N \u2193\nNFS \u2191\n- Learned Sampler\n38.29\n93.88\n30.95\n- Stratification\n5.60\n86.02\n5.97\n- Robust Sampling\n5.67\n60.78\n24.79\n- Beta Regularization\n5.27\n64.25\n28.88\nOurs\n4.97\n60.76\n29.35\nTable 4. Ablation study.\nFigure 8. Qualitative comparisons for ablation study.\n6. Discussion\nLimitations and future work.\nWhile our method demon-\nstrates significant improvements in 3D geometry genera-\ntion, it may still exhibit artifacts such as dents in the pres-\nence of specularities, and cannot handle transparent objects\nsuch as lenses well. Future work may incorporate more ad-\nvanced material formulations [5] and surface normal reg-\nularization [60]. While 3D GANs can learn 3D represen-\ntations from single-view image collections such as FFHQ\nand AFHQ with casual camera labels [7], the frontal bias\nand inaccurate labels can result in geometry artifacts, espe-\ncially on the side of the faces. Fruitful future directions may\ninclude training 3D GANs with large-scale Internet data as\nwell as incorporating a more advanced form of regulariza-\ntion [47] and auto-camera calibration [3] to extend the gen-\nerations to 360 degrees. Finally, our sampling-based accel-\neration method may be applied to other NeRFs.\nEthical considerations.\nWhile existing methods have\ndemonstrated effective capabilities in detecting unseen\nGANs [12], our contribution may remove certain charac-\nteristics from generated images, potentially making the task\nof detection more challenging. Viable solutions include the\nauthentication of synthetic media [1, 2, 48].\n8\nConclusion.\nWe proposed a sampler-based method to ac-\ncelerate 3D GANs to resolve 3D representations at the\nnative resolution of 2D data, creating strictly multi-view-\nconsistent images as well as highly detailed 3D geometry\nlearned from a collection of in-the-wild 2D images. We\nbelieve our work opens up new possibilities for generating\nhigh-quality 3D models and synthetic data that capture in-\nthe-wild variations and for enabling new applications such\nas conditional view synthesis.\nAcknowledgements\nWe thank David Luebke, Tero Karras, Michael Stengel,\nAmrita Mazumdar, Yash Belhe, and Nithin Raghavan for\nfeedback on drafts and early discussions.\nKoki Nagano\nwas partially supported by DARPA\u2019s Semantic Forensics\n(SemaFor) contract (HR0011-20-3-0005). The views and\nconclusions contained in this document are those of the au-\nthors and should not be interpreted as representing the offi-\ncial policies, either expressed or implied, of the U.S. Gov-\nernment. This work was funded in part by an NSF Grad-\nuate Fellowship, ONR Grant N00014-23-1-2526, and the\nRonald L. Graham Chair. Manmohan Chandraker acknowl-\nedges support of of NSF IIS 2110409. Distribution State-\nment \u201cA\u201d (Approved for Public Release, Distribution Un-\nlimited).\nReferences\n[1] Coalition for content provenance and authenticity. https:\n//c2pa.org/. 8\n[2] Content\nauthenticity\ninitiative.\nhttps : / /\ncontentauthenticity.org/. 8\n[3] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y.\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360deg. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 8\n[4] Matan Atzmon and Yaron Lipman.\nSal:\nSign agnos-\ntic learning of shapes from raw data.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2565\u20132574, 2020. 5\n[5] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\nron, Ce Liu, and Hendrik P.A. Lensch. Nerd: Neural re-\nflectance decomposition from image collections. In IEEE\nInternational Conference on Computer Vision (ICCV), 2021.\n8\n[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-GAN: Periodic implicit genera-\ntive adversarial networks for 3D-aware image synthesis. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2021. 2\n[7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein.\nEfficient geometry-aware\n3D generative adversarial networks. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 1,\n2, 3, 4, 6, 7, 8\n[8] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In IEEE International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[9] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3\n[10] Xingyu Chen, Yu Deng, and Baoyuan Wang.\nMimic3d:\nThriving 3d-aware gans via 3d-to-2d imitation.\narXiv\npreprint arXiv:2303.09036, 2023. 2, 3, 6, 7, 8\n[11] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020. 6\n[12] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Gio-\nvanni Poggi, Koki Nagano, and Luisa Verdoliva.\nOn the\ndetection of synthetic images generated by diffusion mod-\nels. In IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2023. 8\n[13] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-\nsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5203\u20135212, 2020. 7\n[14] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGram: Generative radiance manifolds for 3d-aware image\ngeneration. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 3\n[15] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-\nerate 3d avatars from 2d image collections. arXiv preprint\narXiv:2305.02312, 2023. 7\n[16] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 5\n[17] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-\nlos, Martin R\u00a8unz, Lourdes Agapito, and Matthias Nie\u00dfner.\nLearning neural parametric head models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21003\u201321012, 2023. 7\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems (NeurIPS), 2014. 2,\n6\n[19] Jiatao\nGu,\nLingjie\nLiu,\nPeng\nWang,\nand\nChristian\nTheobalt.\nStyleNeRF: A style-based 3D-aware genera-\ntor for high-resolution image synthesis.\narXiv preprint\narXiv:2110.08985, 2021. 2, 3\n[20] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided dis-\ntillation from 3d-aware diffusion. In International Confer-\nence on Machine Learning, pages 11808\u201311826. PMLR,\n2023. 3\n9\n[21] Kunal Gupta, Milo\u02c7s Ha\u02c7san, Zexiang Xu, Fujun Luan, Kulyan\nSunkavalli, Xin Sun, Manmohan Chandraker, and Sai Bi.\nMcnerf: Monte carlo rendering and denoising for real-time\nnerfs. In ACM SIGGRAPH Asia 2023 Conference Proceed-\nings, 2023. 3\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, G\u00a8unter Klambauer, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a\nnash equilibrium. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2017. 7\n[23] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751, 2019. 5\n[24] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy Mitra. Holodiffusion: Training a 3D diffusion model\nusing 2D images. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 3\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019. 4, 6\n[26] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila.\nTraining generative ad-\nversarial networks with limited data. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020. 6\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of StyleGAN. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020. 3\n[28] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In Advances in Neural In-\nformation Processing Systems (NeurIPS), 2021. 6\n[29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 2023. 3\n[30] Andreas Kurz,\nThomas Neff,\nZhaoyang Lv,\nMichael\nZollh\u00a8ofer, and Markus Steinberger. Adanerf: Adaptive sam-\npling for real-time rendering of neural radiance fields. 2022.\n3\n[31] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo\nKanazawa.\nNerfacc: Efficient sampling accelerates nerfs.\narXiv preprint arXiv:2305.04966, 2023. 3\n[32] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3\n[33] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 3\n[34] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou.\nEfficient neural radiance\nfields for interactive free-viewpoint video. In ACM Trans-\nactions on Graphics (SIGGRAPH ASIA), 2022. 3\n[35] David B Lindell, Julien NP Martel, and Gordon Wetzstein.\nAutoInt: Automatic integration for fast neural volume ren-\ndering. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 3\n[36] N. Max.\nOptical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graphics\n(TVCG), 1995. 4\n[37] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\nWhich training methods for gans do actually converge? In\nInternational conference on machine learning, pages 3481\u2013\n3490. PMLR, 2018. 6\n[38] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), 2020. 2, 3, 4, 5\n[39] Don P Mitchell.\nConsequences of stratified sampling in\ngraphics. In Proceedings of the 23rd annual conference on\nComputer graphics and interactive techniques, pages 277\u2013\n280, 1996. 4\n[40] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 3\n[41] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum, 2021. 3\n[42] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts, 2022. 3\n[43] Michael Niemeyer and Andreas Geiger.\nGIRAFFE: Rep-\nresenting scenes as compositional generative neural feature\nfields. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 2, 3\n[44] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUNISURF: Unifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In IEEE International\nConference on Computer Vision (ICCV), 2021. 3\n[45] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStyleSDF: High-Resolution 3D-Consistent Image and Ge-\nometry Generation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022. 2, 3, 4, 5, 6, 8\n[46] M. Piala and R. Clark. Terminerf: Ray termination predic-\ntion for efficient neural rendering. In International Confer-\nence on 3D Vision (3DV), 2021. 3\n[47] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\nInter-\nnational Conference on Learning Representations (ICLR),\n2022. 3, 8\n[48] Ekta Prashnani, Koki Nagano, Shalini De Mello, David Lue-\nbke, and Orazio Gallo.\nAvatar fingerprinting for autho-\nrized use of synthetic talking-head videos. arXiv preprint\narXiv:2305.03713, 2023. 8\n[49] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. GRAF: Generative radiance fields for 3D-aware im-\nage synthesis. In Advances in Neural Information Processing\nSystems (NeurIPS), 2020. 2\n10\n[50] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger. Voxgraf: Fast 3d-aware image synthe-\nsis with sparse voxel grids. Advances in Neural Information\nProcessing Systems (NeurIPS), 2022. 3\n[51] Vincent Sitzmann,\nJulien N.P. Martel,\nAlexander W.\nBergman, David B. Lindell, and Gordon Wetzstein.\nIm-\nplicit neural representations with periodic activation func-\ntions. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2020. 6\n[52] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh\nTenenbaum, and Fredo Durand. Light field networks: Neu-\nral scene representations with single-evaluation rendering.\nAdvances in Neural Information Processing Systems, 34:\n19313\u201319325, 2021. 3\n[53] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-\nter Wonka. Epigraf: Rethinking training of 3d gans. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2022. 2, 3, 6, 7\n[54] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian\nRen, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov. 3d\ngeneration on imagenet. arXiv preprint arXiv:2303.01416,\n2023. 7\n[55] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion: (0-)image-conditioned 3D gen-\nerative models from 2D data. In ICCV, 2023. 3\n[56] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten\nKreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Neural geometric level\nof detail: Real-time rendering with implicit 3D shapes. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2021. 3\n[57] Towaki Takikawa, Thomas M\u00a8uller, Merlin Nimier-David,\nAlex Evans, Sanja Fidler, Alec Jacobson, and Alexander\nKeller. Compact neural graphics primitives with learned hash\nprobing. In ACM SIGGRAPH Asia 2023 Conference Pro-\nceedings, 2023. 3\n[58] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B. Tenenbaum, Fr\u00b4edo Durand, William T.\nFreeman, and Vincent Sitzmann.\nDiffusion with forward\nmodels: Solving stochastic inverse problems without direct\nsupervision.\nAdvances in Neural Information Processing\nSystems (NeurIPS), 2023. 3\n[59] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.\nChan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan\nChandraker, Ravi Ramamoorthi, and Koki Nagano. Real-\ntime radiance fields for single-image portrait view synthesis.\nIn ACM Transactions on Graphics (SIGGRAPH), 2023. 2\n[60] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 8\n[61] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 3\n[62] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction. Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2021. 3\n[63] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo.\nRodin: A genera-\ntive model for sculpting 3d digital avatars using diffusion. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3\n[64] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-\nilidis, Christian Theobalt, and Lingjie Liu.\nNeus2: Fast\nlearning of neural implicit surfaces for multi-view recon-\nstruction. In IEEE International Conference on Computer\nVision (ICCV), 2023. 3\n[65] Zian Wang, Tiancheng Shen, Merlin Nimier-David, Nicholas\nSharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas\nM\u00a8uller, and Zan Gojcic. Adaptive shells for efficient neural\nradiance field rendering. In ACM Transactions on Graphics\n(SIGGRAPH ASIA). 3\n[66] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 3\n[67] Jianfeng Xiang, Jiaolong Yang, Yu Deng, and Xin Tong.\nGram-hd: 3d-consistent image generation at high resolu-\ntion with generative radiance manifolds.\narXiv preprint\narXiv:2206.07255, 2022. 3, 6, 8\n[68] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\nkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in\nvisual computing and beyond. In Computer Graphics Forum.\nWiley Online Library, 2022. 2\n[69] Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and\nBolei Zhou. 3d-aware image synthesis via learning struc-\ntural and textural representations. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[70] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae\nLee.\nGiraffe hd: A high-resolution 3d-aware generative\nmodel. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2022. 2, 3\n[71] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Ronen Basri, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance.\nIn Advances in Neural Information Processing\nSystems (NeurIPS), 2020. 3\n[72] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 3,\n4, 5\n[73] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In IEEE International Conference on\nComputer Vision (ICCV), 2021. 3\n[74] Xuanmeng Zhang, Zhedong Zheng, Daiheng Gao, Bang\nZhang, Pan Pan, and Yi Yang. Multi-view consistent gen-\nerative adversarial networks for 3d-aware image synthesis.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022. 2, 6, 8\n[75] Xiaoming Zhao, Fangchang Ma, David G\u00a8uera, Zhile Ren,\nAlexander G. Schwing, and Alex Colburn. Generative mul-\ntiplane images: Making a 2d gan 3d-aware. In European\nConference on Computer Vision (ECCV), 2022. 3, 6\n11\n[76] Peng Zhou,\nLingxi Xie,\nBingbing Ni,\nand Qi Tian.\nCIPS-3D: A 3D-Aware Generator of GANs Based on\nConditionally-Independent Pixel Synthesis. arXiv preprint\narXiv:2110.09788, 2021. 2\n12\nRendering Every Pixel for High-Fidelity Geometry in 3D GANs\nSupplementary Material\nIn this supplement, we first provide additional visual re-\nsults (Sec. A1) and additional evaluations (Sec. A2). We\nfollow with details of our implementation (Sec. A3) includ-\ning the details of our adaptive sampling approach (Sub-\nsec. A3.4). We discuss experiment details (Sec. A4) such as\nthe details of our Normal-FID evaluation metric and base-\nlines. We finally provide discussion (Sec. A5) including\nlimitations of our work that may be addressed in future\nwork. Please refer to the accompanying video, which con-\ntains additional visual results and comparisons.\nA1. Additional Qualitative Results\nWe first show both curated and uncurated results for both\nour FFHQ and AFHQ models. Curated FFHQ results can\nbe seen in Fig. A1. Please note the highly-detailed and vari-\nable facial expressions along with well-defined 3D acces-\nsories like hats and glasses. Long hair is not pasted onto the\nforeground, but rather retains a 3D aspect. Curated AFHQ\nresults can be seen in Fig. A2. Note the detailed textures of\nthe cat geometry and the well-defined noses and ears. For\nunbiased presentation, we also show uncurated results (the\nfirst 8 seeds) for FFHQ (Fig. A3) and AFHQ (Fig. A4). All\nresults shown are with Truncation = 0.7.\nA2. Additional Evaluations\nA2.1. Comparing Sampling Methods at Various\nSampling Counts\nIn this section, we show the robustness of our proposed\nsampling strategy from the predicted \u02c6P512 at very low sam-\nple counts, in comparison to unstratified and stratified sam-\npling methods. In Fig. A5, we show our proposed robust\nsampling method in comparison to unstratified and stratified\ninverse transform sampling. At very low samples per pixel\n(spp), our method vastly out performs the standard sampling\ntechnique. Please see the insets where our method can han-\ndle depth discontinuities without jagged artifacts even at 8\nsamples per pixel.\nWe also render a pseudo ground truth image using 384\n(192 coarse and 192 importance) samples and compare the\nPSNR of various sampling methods in Fig. A6. Most im-\nportantly for GAN training, our method\u2019s worst-case is sig-\nnificantly better than previous method\u2019s worst-case. This\nis integral to GAN training, where the discriminator will\nalways focus on the easiest attribute to discriminate. As\nsampling artifacts cannot be amended by G, the worst-case\nresults dictate how well the GAN converges.\nA2.2. Effectiveness of Adaptive Sampling\nFig. A7 demonstrates the effectiveness of our proposed\nadaptive sampling method compared to other baselines. By\nallocating a small portion of samples to uncertain regions\n(e.g., depth discontinuity; see the top right of Fig. A7), our\nmethod can generate an artifact-free result even at the depth\ncount budget of 10 samples per pixel (10spp) compared to\nthe same spp without adative sampling (top left), which has\njaggy artifacts around the depth discontinuity. Without our\nsampler (bottom row of Fig. A7), the standard two-pass im-\nportance sampler [11] results in significant artifacts. Please\nsee Subsec. A3.4 for the implementation details of our adap-\ntive sampling.\nA2.3. Single Image Reconstruction\nWe additionally showcase an application of our method for\nsingle-view 3D reconstruction in Fig. A8. The learned prior\nenables high quality reconstruction of images and 3D ge-\nometry, despite the under constrained nature of the prob-\nlem. We incorporate Pivotal Tuning Inversion (PTI) [13],\noptimizing the latent code, camera, and noise buffers for\n600 iterations, followed by optimization of the camera and\ngenerator weights for another 350 iterations with MSE and\nLPIPS [18] losses computed between the input view and\nrendering.\nA3. Implementations Details\nA3.1. Inference Details and Time\nDuring inference, our method, by default, uses 17.6 depth\nsamples (see Subsec. A3.4 for details) at high resolutions in\naddition to samples from the low-resolution probe, which is\nequivalent to 12 samples at high resolutions; this results in\n29.6 samples per ray at high resolutions. While our model\nlearns view-dependent effects during training (see Eq. 5 in\nthe main PDF), we use a constant frontal-viewing condition\nduring inference for our qualitative results. Rendering from\ncached triplanes runs at 4.5 FPS using plain PyTorch scripts\non a single A100 GPU and requires <15GB of VRAM.\nA3.2. Training Details\nIn this section, we present the details of the training of our\nproposed model. For the schedule of the hyperparameters of\nthe FFHQ model, please see Tab. A3. We train the FFHQ\nmodel for 28.2 million images with a batch size of 32 im-\nages on 16 80GB NVIDIA A100 GPUs, which takes about\n11 days. The AFHQ model is finetuned from this model\nwith adaptive discriminator augmentation [7] for 1.2 mil-\nlion images and R1 gamma value of 6, and all other hyper-\nparameters the same as in the end of the FFHQ training.\nA3.3. Network Details\nThe architecture of the generator for T \u2032 follows EG3D [2]\nexactly, except doubling its capacity (channel base from\n1\narXiv:2401.02411v1  [cs.CV]  4 Jan 2024\n32768 to 65536). As mentioned in the main paper, we add\nthree extra Synthesis Blocks from StyleGAN2 [8] applied\nto the channels of T \u2032, in order to get the final triplane T\u2013\none for each of the orthogonal planes and each applied to\none of the three slices of 32 channels of T \u2032.\nFor the details of the architecture of the proposal net-\nwork, please refer to Tab. A4. Slightly abusing notation, we\nhave labelled the image of viewing directions correspond-\ning to the target camera as \u03d5128, parameterized as normal-\nized vectors per pixel. The other inputs, P128 (weights) and\nI128 (image), follow the same notation as the main paper.\nFor MLPSDF, we embed the input 3D positions with the\nembedding from NeRF [11] and 6 frequencies (sampled in\nlogspace). The architecture of this network is given in Ta-\nble A1. The first two components of the 66 dimensional\noutput correspond to the SDF value s and pre-activated\n\u03b2pre. We map to the variance with the following equation:\n\u03b2 = 0.01 + Tanh(2 \u00b7 \u03b2pre)(0.01 \u2212 0.0001). This activa-\ntion ensures that the variances are approximately 0.01 at\nthe beginning of training and prevents them from becoming\nnegative.\nWe also show the details of MLPc in Table A2. The po-\nsitional encoding of the viewing direction is 2 frequencies\nsampled in logspace. We finally map the 3 output compo-\nnents cpre to the RGB value as Sigmoid\u00b7(cpre)(1+0.002)\u2212\n0.001.\nA3.4. Details of Adaptive Sampling\nAs mentioned in the main paper, we use a proxy for the\nvariance to adaptively allocate more samples to more dif-\nficult pixels. Specifically, considering the predicted high-\nresolution distributions \u02c6P512, for each distribution, we com-\npute a scalar value to dictate how many samples to allocate.\nWe operate under the simplified assumption that we will al-\nlocate 16 samples to 90% of pixels, and 32 to the remaining\n10%, resulting in total 17.6 samples per ray. To compute\nwhich pixels receive more samples, we compute a proxy for\nthe variance.\nTo do so, for a given predicted distribution p, we com-\npute the leftover probability mass after removing the largest\n16 bins. Precisely, we consider the set of non-repeating non-\nnegative integers less than 192,\nS = {(z1, . . . , z16) | zi \u2208 Z\u22650, zi < 192, zi \u0338= zj \u2200 i \u0338= j} .\nWe then find\nSmax =\nmax\n(z1,...,z16)\u2208S\n16\nX\ni=1\npzi.\nThe final scalar 1 \u2212 Smax is the leftover probability mass\nafter removing the largest 16 bins. We choose the 10% of\npixels from \u02c6P512 which have maximized this quantity. A\nvisualization of these pixels is given in Fig. A7. We can see\nthat they are most concentrated on the depth discontinuities\nwhere the distributions may not be unimodal. Adaptively al-\nlocating samples in this manner allows us to accurately ren-\nder the most challenging pixels without wasting too many\nsamples on the \u201ceasier\u201d distributions. As can be seen in\nFig. A7, using the same total sample count, we can avoid\njagged and inaccurate renders. For illustration purposes,\nwe first show an example where all pixels are rendered with\n10 samples (top-left of Fig. A7) and then with 9 samples\nfor 90% of pixels, and 19 samples for the remaining 10%,\nresulting in an average sample count of 10 (top-middle of\nFig. A7). The samples to which we allocate more samples\nare visualized (top-right of Fig. A7). We compare to vary-\ning sample counts with standard sampling in the bottom row\nof Fig. A7.\nA3.5. Details of Stratified Sampling\nAs discussed in subsection 4.4, we compute the robust dis-\ntribution q from the predicted distribution \u02c6p from the pro-\nposal network. Let I = {i \u2208 Z : qi > 0} denote the set\nof non-zero bins each with equal probability (as in the main\npaper). For stratified sampling, we partition the unit inter-\nval into c = |I| strata. We assume we are given a sample\nbudget s > 1. We allocate \u230a s\nc\u230b samples to each of the c\nstrata. We then allocate one extra sample to the (s mod c)\nbins with maximal \u02c6pi. Note that as we allocate more sam-\nples to a particular stratum, the distance \u03b4i between adjacent\nintrastratum samples shrinks, thus introducing no additional\nbias. In practice, we also clip the \u03b4i to the bin width to pre-\nvent outsized contributions from the endpoints of nonzero\nregions. For s < c, this is biased; however, in practice, due\nto our tightening regularization and adaptive allocation of\nsamples, we almost always have s \u2265 c. Additionally, at ex-\ntremely low spp, our method outperforms unbiased methods\n(see left column of Fig. A5).\nA4. Experiment details\nA4.1. Geometry Visualization\nFor geometry visualization, we extract iso-surface geome-\ntry using March Cubes [10]. We use the voxel resolution of\n5123 for comparisons and 10243 for our main results. For\nSDF-based methods (ours), geometry is extracted from an\nSDF field at the 0th level set. For NeRF-based methods\n(EG3D, Mimic3D, and Epigraf), the surface is extracted\nfrom the density field using the level set provided by the\nofficial script from the authors. We render these extracted\nmodels using Blender for visualization. To visualize nor-\nmal maps, we derive the normal by taking the gradient of\nthe SDF field for SDF-based methods and density field for\nNeRF-based methods with respect to positions.\nA4.2. Normal-FID\nAs mentioned in the main text, we use normal maps ex-\ntracted from the meshes of the NPHM [6] dataset.\n255\nsubjects are scanned with highly variable expressions. We\nprovide examples of these normal maps in Fig. A9. For\n2\nLayer\nType\nInput\nActivation\nDimension\nInput 0\nInput\nXYZ positions\n-\n3\nInput 1\nInput\nTriplane features\n-\n32\n1\nPosEnc\nInput 0\n-\n39\n2\nConcatenation\nInput 1, Layer 1\n-\n71\n3\nLinear\nConcatenation\nSoftplus\n128\n4\nLinear\nLayer 3\nSoftplus\n128\n5\nLinear\nLayer 4\n-\n66\nTable A1. Architecture of the MLPSDF network with embedding.\nLayer\nType\nInput\nActivation\nDimension\nInput 0\nInput\nViewing Directions \u03d5\n-\n3\nInput 1\nInput\nfgeo\n-\n64\n1\nEmbedding\nInput 0\n-\n15\n2\nConcatenation\nInput 1, Layer 1\n-\n79\n3\nLinear\nLayer 2\nSoftplus\n64\n4\nLinear\nLayer 3\n-\n3\nTable A2. Architecture of the MLPc network with embedding for viewing direction.\nNormal-FID computation, we ensure all coordinate conven-\ntions are consistent between baselines so that the color maps\nare likewise consistent. We sample all methods with trun-\ncation of 0.7 (cutoff = 14) due to the lack of diversity in\nthe ground truth images (see Fig. A9). Using PyFacer [4],\nwe also mask the background pixels to black.\nFor Epi-\nGRAF [14], we crop all sample images using HRN [9].\nA4.3. Details of baseline methods\nFor all the baselines, we use publicly released pre-trained\nmodels if they are available; otherwise, we quote the FID\nnumbers from previous work.\nFor Mimic3D [3], Epi-\ngraf [14] and EG3D [2], we used the corresponding pub-\nlicly released models from the original authors for N-FID\nand non-flatness score computation; unless explicitly spec-\nified otherwise, we also use the provided default evaluation\noptions for all methods. For EG3D, Mimic3d, Epigraf this\nis 48 samples for a coarse pass and 48 samples for a fine\npass for two-pass importance sampling. For StyleSDF this\nis 24 samples per ray.\nFor StyleSDF [12], we re-trained an FFHQ model at\n512 resolutions and AFHQv2 cats-only model at 512 res-\nolutions as they were not publicly available and used them\nfor geometry evaluations. We train our StyleSDF geome-\ntry network on FFHQ using the publicly released code, for\n200k iterations as recommended by the authors, on 8 A100\nNVIDIA GPUs. For our StyleSDF geometry network on the\nAFHQv2 cats-only split, training with the provided AFHQ\nconfig from scratch was unstable and collapsed. Instead,\nwe finetune the publicly released StyleSDF AFHQv2 all-\nanimals geometry network on our cats-only split for 50k\niterations and use that for evaluation.\nA5. Discussion\nA5.1. Limitation and Future Work\nWe showcase three failure cases of our method in Fig. A10.\nIn the first row, we see that there are seams in the side of\nthe face in both the geometry and rendering. We hypothe-\nsize this issue may be related to the frontal camera bias in\nFFHQ and may be ameliorated by a more uniform sampling\nof cameras. In the second row, we see that in some samples\nwith large amounts of specularity, the surface may become\nunnaturally rough, which may be remedied by additional\nregularization on the surface normal [17]. Finally, in the\nthird row, we see a rare phenomena where density close to\nthe camera occludes the subject.\nFuture works may utilize more balanced datasets with\nlarger coverage around the entirety of the face [1]. Extend-\ning to the human body [5] or more general classes [15], is\nalso extremely interesting. Combining our approach with a\n3D lifting approach [16] using our method as 3D synthetic\ndata, may allow high-fidelity geometry estimation from a\nsingle image.\nReferences\n[1] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y.\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360deg. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 3\n[2] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\n3\nHyperparameter\nNumber of Images (in millions)\nValue\nR1 Gamma\n0-18\n1\n18-25\n4\n25 onwards\n2\nNeural Rendering Resolution\n0-10\n64\n10-18\n128 (linearly increased over 1m images)\n18 onwards\n512 (linearly increased over 0.2m images)\n\u03b2target\n0-10\n0.01\n10 onwards\n0.001 (linearly decreased over 1m images)\nLearning Rate Multiplier for MLPc\n0-25\n2\n25 onwards\n1\nRender with Predicted Distributions\n0-17\nNo\n17 onwards\nYes\nSupervise Predicted Distributions\n0-16\nNo\n16 onwards\nYes\nTable A3. Schedule of hyperparameters given in millions of images the discriminator has seen during training. We train for 28.2m images\ntotal.\nLayer\nType\nActivation\nUpsample\nInput Source(s)\nDimension\nInput 0\nInput\n-\n-\nP128\n191 x 128 x 128\nInput 1\nInput\n-\n-\nI128\n3 x 128 x 128\nInput 2\nInput\n-\n-\n\u03d5128\n3 x 128 x 128\n1\nConcatenation\n-\n-\nInputs 0-2\n197 x 128 x 128\n2\nConv2D\nReLU\nNo\nLayer 1\n256 x 128 x 128\n3\nConv2D\nReLU\nNo\nLayer 2\n256 x 128 x 128\n4\nConv2D\nReLU\nNo\nLayer 3\n256 x 128 x 128\n5\nConv2D\nReLU\nNo\nLayer 4\n256 x 128 x 128\n6\nConv2D\nReLU\nYes\nLayer 5\n256 x 256 x 256\n7\nConv2D\nReLU\nNo\nLayer 6\n256 x 256 x 256\n8\nConv2D\nReLU\nNo\nLayer 7\n256 x 256 x 256\n9\nConv2D\nReLU\nYes\nLayer 8\n256 x 512 x 512\n10\nConv2D\nNone\nNo\nLayer 9\n191 x 512 x 512\n11\nBilinearUpsample\n-\nYes\nInput 0\n191 x 512 x 512\n12\nConv2D\nReLU\nNo\nLayer 10 + Layer 11\n256 x 512 x 512\n13\nConv2D\nReLU\nNo\nLayer 12\n256 x 512 x 512\n14\nConv2D\nReLU\nNo\nLayer 13\n256 x 512 x 512\n15\nConv2D\nSoftmax\nNo\nLayer 14\n191 x 512 x 512\nTable A4. Architecture of our proposal network.\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein.\nEfficient geometry-aware\n3D generative adversarial networks. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 1,\n3\n[3] Xingyu Chen, Yu Deng, and Baoyuan Wang.\nMimic3d:\nThriving 3d-aware gans via 3d-to-2d imitation.\narXiv\npreprint arXiv:2303.09036, 2023. 3\n[4] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-\nsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5203\u20135212, 2020. 3, 14\n[5] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-\nerate 3d avatars from 2d image collections. arXiv preprint\n4\narXiv:2305.02312, 2023. 3\n[6] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-\nlos, Martin R\u00a8unz, Lourdes Agapito, and Matthias Nie\u00dfner.\nLearning neural parametric head models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21003\u201321012, 2023. 2, 14\n[7] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila.\nTraining generative ad-\nversarial networks with limited data. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020. 1\n[8] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of StyleGAN. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020. 2\n[9] Biwen Lei, Jianqiang Ren, Mengyang Feng, Miaomiao Cui,\nand Xuansong Xie. A hierarchical representation network for\naccurate and detailed face reconstruction from in-the-wild\nimages.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 394\u2013403,\n2023. 3\n[10] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3D surface construction algorithm. ACM\nTransactions on Graphics (ToG), 1987. 2\n[11] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), 2020. 1, 2\n[12] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStyleSDF: High-Resolution 3D-Consistent Image and Ge-\nometry Generation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022. 3\n[13] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. arXiv preprint arXiv:2106.05744, 2021. 1\n[14] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-\nter Wonka. Epigraf: Rethinking training of 3d gans. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2022. 3\n[15] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian\nRen, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov. 3d\ngeneration on imagenet. arXiv preprint arXiv:2303.01416,\n2023. 3\n[16] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.\nChan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan\nChandraker, Ravi Ramamoorthi, and Koki Nagano. Real-\ntime radiance fields for single-image portrait view synthesis.\nIn ACM Transactions on Graphics (SIGGRAPH), 2023. 3\n[17] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[18] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2018. 1\n5\nFigure A1. Curated samples from our FFHQ model.\n6\nFigure A2. Curated samples from our AFHQ model.\n7\nFigure A3. Uncurated (seeds 1-8) samples from our FFHQ model.\n8\nFigure A4. Uncurated (seeds 1-8) samples from our AFHQ model.\n9\nFigure A5. We show the rendering results of various sampling methods at different sample counts. We visualize both the rendering and a\ncolor map for the L2 error.\n10\nFigure A6. We show the PSNR for the worst percentage of pixels for all three sampling methods (subject is the same as Fig.A5). As seen\nin the charts, our method significantly outperforms the previous sampling methods at very low samples per pixel, e.g., 2 samples. At higher\nsample counts, our proposed sampling method has a significantly better worst-case result, e.g., for the worst 1% or 0.1% of pixels, as seen\nin the lower half. The importance of this is detailed in A2.1. The red dotted line indicates the maximal number of samples during training\nthat fit on one 80gb A100 when rendering two images (per GPU).\n11\nFigure A7. We visualize the effectiveness of our proposed adaptive sampling approach. In the top-left, we see that using 10 samples for all\npixels results in jagged artifacts. Using the same number of samples, we allocate 9 samples to 90% of pixels and 19 to the remaining 10%,\nwhich prevents these jagged artifacts. The top 10% of pixels by the quantity computed in Subsec. A3.4 is visualized in the top right. In\ncomparison, we also show the standard method without the learned sampler with 10 and 22 samples in the bottom-left and bottom-middle,\nrespectively. 22 corresponds to 10 samples along with the 12 samples allocated for the initial probe P128. Finally, we show the ground-truth\nrendering with 384 samples (192 coarse and 192 importance) in the bottom right.\n12\nFigure A8. We showcase examples of single-view 3D reconstruction with our method. The left column shows the test image inputs, the\nright three columns show our inversion sample from three novel views.\n13\nFigure A9. 20 sample normal maps from [6] masked using Facer [4], from which we compute the N-FID score.\n14\nFigure A10. Three failure cases of our 3D generative model. First row shows seams on the side of the face; second row displays surface\nroughness to simulate specularity; and third row shows a rare phenomena where density appears close to the camera, occluding the subject.\n15\n"
  },
  {
    "title": "LLaVA-$\u03c6$: Efficient Multi-Modal Assistant with Small Language Model",
    "link": "https://arxiv.org/pdf/2401.02330.pdf",
    "upvote": "11",
    "text": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model\nYichen Zhu1, Minjie Zhu1,2, Ning Liu1, Zhicai Ou1, Xiaofeng Mou1, Jian Tang1\n1Midea Group, 2East China Normal University\nAbstract\nIn this paper, we introduce LLaVA-\u03d5 (LLaVA-Phi), an\nefficient multi-modal assistant that harnesses the power of\nthe recently advanced small language model, Phi-2, to fa-\ncilitate multi-modal dialogues. LLaVA-Phi marks a notable\nadvancement in the realm of compact multi-modal models.\nIt demonstrates that even smaller language models, with\nas few as 2.7B parameters, can effectively engage in intri-\ncate dialogues that integrate both textual and visual ele-\nments, provided they are trained with high-quality corpora.\nOur model delivers commendable performance on publicly\navailable benchmarks that encompass visual comprehen-\nsion, reasoning, and knowledge-based perception. Beyond\nits remarkable performance in multi-modal dialogue tasks,\nour model opens new avenues for applications in time-\nsensitive environments and systems that require real-time\ninteraction, such as embodied agents. It highlights the po-\ntential of smaller language models to achieve sophisticated\nlevels of understanding and interaction, while maintain-\ning greater resource efficiency. The project is available at\nhttps://github.com/zhuyiche/llava-phi.\n1. Introduction\nLarge vision language models, including Flamingo [1],\nGPT-4V [30], and Gemini [33], have exhibited remarkable\nproficiency in executing instructions, engaging in multi-turn\ndialogues, and handling image-based question-answering\ntasks.\nThe progression of open-source vision language\nmodels has been significantly propelled by the rapid ad-\nvancement of open-source Large Language Models like\nLLaMA [34] and Vicuna [5]. These developments primar-\nily focus on leveraging language models with a minimum of\n7B parameters, integrated with a vision encoder to enhance\nvisual comprehension. However, this approach often results\nin increased test time and reduced inference speed, which\nare less than ideal for time-sensitive or real-time interactive\napplications, such as autonomous driving and robotics. This\nleads to an important inquiry: How effectively can small\nvision-language assistants perform in comparison?\nGemini [33] has blazed a trail for multi-modal models in\nmobile technology. Its streamlined variant, Gemini-Nano,\nboasts 1.8/3.25 billion parameters and is deployable on mo-\nbile devices. However, details like the model architecture,\ntraining data, and training methodologies remain propri-\netary and inaccessible to the public. In the realm of small\nlanguage models, there have been notable advancements:\nTinyGSM [23], with 2.6 billion parameters, achieves over\n80% accuracy on the GSM8k [7] benchmark. Additionally,\nmodels such as Phi [13] have demonstrated capabilities in\nlanguage understanding, commonsense reasoning, and code\ngeneration, rivaling larger language models like LLaMA-\n2-7B. This progress underscores the significant strides be-\ning made in the efficiency and effectiveness of smaller-scale\nlanguage models.\nIn this paper,\nwe introduce LLaVA-Phi,\na com-\npact vision-language assistant powered by a small lan-\nguage model.\nOur work combines the powerful open-\nsourced multi-modal model, LLaVA-1.5 [24], with the\nbest-performing open-sourced small language models, Phi-\n2 [21]. We follow a two-stage training pipeline and lever-\nage high-quality visual instruction tuning data from LLaVA.\nLLaVA-Phi was evaluated across eight diverse benchmarks.\nDespite possessing only 3 billion parameters, it achieves\nperformance comparable to, or even surpassing, some\nlarger multi-modal models that are three times larger. No-\ntably, LLaVA-Phi-3B demonstrates exceptional proficiency\nin ScienceQA [28], outperforming existing large multi-\nmodal models. Additionally, we qualitatively demonstrate\nLLaVA-Phi\u2019s strong generalization ability in handling chal-\nlenging questions, generating code based on instructions,\nand solving mathematical problems.\n2. Related Work\nThe rapid advancements in Large Language Models\n(LLMs) have significantly propelled the development of\nvision-language models based on LLMs.\nThese models,\nrepresenting a departure from the capabilities of the pre-\nLLM era, are equipped with advanced question-answering\nand visual comprehension skills. This progress is enabled\nby using LLMs as language encoding modules. Notable re-\nsearch in this domain includes the LLaVA-family [24, 25,\n26, 32], the BLIP-family [8, 20], MiniGPT-4 [37], and oth-\n1\narXiv:2401.02330v4  [cs.CV]  22 Feb 2024\nExplain why this meme is funny.\nUser\nLLaVA-1.5\n-13B\nThe meme is funny because it features a dog lying \non a wooden floor with a caption that reads \n\"Monday. Just... Monday.\" \nThe dog's expression and the caption create a \nhumorous and relatable situation for many people \nwho often feel tired or unenthusiastic about \nstarting a new week. \nThe dog's presence in the meme adds a \nlighthearted and amusing touch to the otherwise \nmundane topic of Monday morning.\nLLaVA-Phi\nThe meme is funny because it combines the image of \na dog lying on the floor with the phrase \"Just Monday.\" \nThe juxtaposition of the dog's relaxed and sleepy state \nwith the phrase \"Just Monday\" creates a humorous \ncontrast. \nThe dog's laid-back demeanor is often associated with \nthe feeling of Monday morning, which is often \nconsidered the start of a new workweek and can be \nassociated with stress or fatigue.\nThe image of the dog lying on the floor adds a \nlighthearted and unexpected element to the meme, \nmaking it amusing and entertaining.\nTricky Question\nFigure 1. LLaVA-Phi is adept at identifying and responding to\ncomplex questions with empathetic reasoning.\ners.\nEach has demonstrated significant advancements in\nmanaging visual-centric dialogues. However, a common\nlimitation of these open-sourced Vision-Language Models\n(VLMs) is their substantial computational demands, typi-\ncally ranging from 7B to 65B parameters. This require-\nment poses challenges for deployment on edge or mobile\ndevices, especially in real-time applications. Gemini [33],\na leader in this field, has released three versions of vision-\nlanguage models, including the compact Gemini-Nano with\n1.8B/3.25B parameters, tailored for smartphones. However,\ntheir models and data are not open-sourced. Another initia-\ntive, MobileVLM [6], has developed mobileLLaMA with\n2.7B parameters to facilitate smaller vision-language mod-\nels. Our paper explores and demonstrates the effectiveness\nof integrating vision-language models with open-sourced,\nsmaller language models, assessing their potential and effi-\nciency in a variety of applications.\nConvert the table in the image to a bar \nchart using python\nUser\nLLaVA-1.5\n-13B\nLLaVA-Phi\nimport pandas as pd \n# Read the table data\ndf = pd.read_table(image_path, sep=\u2018\\t\u2019)\n# Print the DataFrame\nprint(df)\n```python\nimport pandas as pd \nimport matplotlib.pyplot as plt \n# Read the table data from the image\ntable_data = pd.read_table(image=True,\nfile_name='table1.png\u2019)\net the title and axis labels\n# Create a bar chart\nplt.bar(table_data['Letter'], table_data['Score\u2019])\n# Set the title and axis labels\n# Add labels and title\nplt.title('Table 1: Capture for Table with Student Scores\u2019)\nplt.xlabel('Letter\u2019)\nplt.ylabel('Score\u2019)\n# Display the chart\nplt.show()\nCode Generation\nFigure 2. LLaVA-Phi can generate useful codes based on visual\ninput and commands.\n3. LLaVA-Phi\nOur overall network architecture is similar to LLaVA-1.5.\nWe use the pre-trained CLIP ViT-L/14 with a resolution of\n336x336 as the visual encoder. A two-layer MLP is adopted\nto improve the connection of the visual encoder and LLM.\n3.1. Training\nSupervised fine-tuning on Phi-2. The publicly released\nPhi-2 model has not undergone fine-tuning.\nPrevious\nresearch indicates that even a small amount of high-quality\ndata can significantly enhance performance in areas such\nas mathematics, language reasoning, and coding tasks. In\nlight of this, we employed supervised fine-tuning to further\ntrain Phi-2 using a select set of premium data. This data\nwas organized in the Vicuna format. For our Supervised\n2\nTable 1. Multi-modal evaluation on multi-modal benchmarks. Benchmark names are abbreviated due to space limits. VQAv2 [12];\nGQA [16]; VizWiz [14]; SQAI: ScienceQA-IMG [28]; VQAT: TextVQA [31]; POPE [22]; MME [10]; MMB: MMBench [27]; SEED:\nSEED-Bench [18]; MM-Vet [36].\nMethod\nLLM\nVQAv2\nVizWiz\nSQAI\nVQAT\nPOPE\nMME\nMMB\nMMVet\nGemini-Nano2 [33]\nN/A (3.25B)\n67.5\n-\n-\n65.9\n-\n-\n-\n-\nOpenFlamingo [2]\nMBT (7B)\n-\n-\n-\n33.6\n-\n-\n4.6\n-\nBLIP-2 [20]\nVicuna (13B)\n41.0\n19.6\n61.0\n42.5\n85.3\n1293.8\n-\n22.4\nInstructBLIP [8]\nVicuna (7B)\n-\n34.5\n60.5\n50.1\n-\n-\n36.0\n26.2\nInstructBLIP [8]\nVicuna (13B)\n-\n33.4\n63.1\n50.7\n78.9\n1212.8\n-\n25.6\nMiniGPT-4 [37]\nVicuna (7B)\n-\n-\n-\n-\n-\n581.7\n23.0\n22.1\nShikra [4]\nVicuna (13B)\n77.4\n-\n-\n-\n-\n-\n58.8\n-\nOtter [19]\nLLaMA (7B)\n-\n-\n-\n-\n-\n1292.3\n48.3\n24.6\nQwen-VL [3]\nQwen (7B)\n78.8\n35.2\n67.1\n63.8\n-\n-\n38.2\n-\nQwen-VL-Chat [3]\nQwen (7B)\n78.2\n38.9\n68.2\n61.5\n-\n1487.5\n60.6\n-\nIDEFICS-9B [17]\nLLaMA (7B)\n50.9\n35.5\n-\n25.9\n-\n-\n48.2\n-\nIDEFICS-80B [17]\nLLaMA (65B)\n60.0\n36.0\n-\n30.9\n-\n-\n54.5\n-\nLLaMA-Adapter-v2 [11]\nLLaMA (7B)\n-\n-\n-\n-\n-\n1328.4\n39.5\n31.4\nLLaVA [25]\nVicuna (7B)\n-\n-\n-\n-\n-\n502.8\n36.2\n28.1\nLLaVA-1.5 [24]\nVicuna (7B)\n78.5\n50.0\n66.8\n58.2\n85.9\n1510.7\n64.3\n30.5\nMobileVLM [6]\nM-LLaMA (2.7B)\n-\n-\n61.0\n47.5\n84.9\n1288.9\n59.6\n-\nLLaVA-Phi\nPhi-2 (2.7B)\n71.4\n35.9\n68.4\n48.6\n85.0\n1335.1\n59.8\n28.9\nMath\nSolve the math equation in the image.\nUser\nLLaVA-1.5\n-13B\nThe math equation in the image involves adding two \nnumbers, 6 and 22, and then multiplying the result \nby 2. To solve this problem, we first add 6 and 22, \nwhich gives us 28. Then, we multiply 28 by 2, which \nresults in 56. Therefore, the answer to the math \nproblem is 56.\nLLaVA-Phi\nThe answer to the math equation in the image is 1.\nFigure 3. LLaVA-Phi is capable of performing accurate OCR on\nmathematical equations and solving them correspondingly..\nFine-Tuning (SFT) data, we utilized ShareGPT from an\nopen-source platform.\nThe training was conducted over\ntwo epochs, beginning with an initial learning rate of 3e-5,\nwhich was linearly decreased over time.\nOur findings\nsuggest that while this step might be optional, applying\nSFT to Phi-2 does result in modest improvements across\nmost benchmarks.\nTraining LLaVA-Phi.\nOur training approach follows\nthe pipeline used for LLaVA1.5, consisting of a pre-\ntraining stage and a subsequent instruction tuning phase.\nInitially, we kept the vision encoder and Phi-2 static,\nfocusing exclusively on training the efficient projector.\nThis step is followed by a comprehensive fine-tuning of\nboth the projector and the language model (LLM), aiming\nto enhance their capabilities in visual comprehension and\nlanguage processing.\nFor pre-training, we utilize a filtered subset of the\nCC-595K dataset [24] over one epoch, applying an initial\nlearning rate of 1e-3 and a batch size of 256. Then, we\nfinetune the model on LLaVA-Instruct-150K dataset for 1\nepoch at a learning rate of 2e-5 and a batch size of 256.\nWe implement a weight decay of 0.1 and utilize the Adam\noptimizer, characterized by momentum parameters of 0.9\nand 0.98, and an epsilon value of 1e-7. We fine-tune all\nparameters in LLM instead of using LoRA.\nComputational Cost. Similar to LLaVA1.5, our training\nprocess is structured in two stages. For LLaVA-Phi, the\npretraining phase takes 1.5 hours, followed by 8 hours\ndedicated to visual instruction tuning, utilizing 8 A100\nGPUs. The integration of techniques such as LoRA [15]\nand QLoRA [9] has the potential to significantly reduce\ntraining time, a possibility we plan to explore in future\nwork.\n3.2. Qualitative Results\nWe present several examples that demonstrate the remark-\nable generalization capabilities of LLaVA-Phi, comparing\nits outputs with those of the LLaVA-1.5-13B models. In\n3\nFigure 1, a meme is displayed, and we ask the vision-\nlanguage assistant to explain why this meme is considered\nhumorous. While LLaVA-1.5-13B provides a reasonable\ninterpretation based on the image, LLaVA-Phi\u2019s response\nis more empathetic, highlighting the humor by associating\nthe dog\u2019s \u2019laid-back demeanor\u2019 with the \u2019stress or fatigue\u2019\ntypically associated with a \u2019new workweek\u2019.\nIn the second example, we instructed the model to gen-\nerate Python code for converting an Excel table into a bar\nchart, as illustrated in Figure 2. LLaVA-1.5-13B generated\na simplistic code snippet that only reads the table and prints\nit, diverging from the instructions to create a plot. In con-\ntrast, LLaVA-Phi accurately comprehended the task, pro-\nviding instructions to read the table, add a title and labels,\nand correctly plot the bar chart using matplotlib. We believe\nthis enhanced code generation capability stems from Phi-2,\nwhich was pre-trained on a large corpus of code snippets\nand is primarily used for code generation.\nThe third challenge involves solving a simple math\nproblem, requiring the model to accurately recognize text\nthrough OCR and then perform the necessary mathematical\ncomputations, as shown in Figure 3. LLaVA-1.5-13B, while\nproviding a step-by-step computation based on the image,\nincorrectly recognized the numbers and mathematical sym-\nbols. In contrast, our proposed LLaVA-Phi, without pro-\nviding a chain-of-thought reasoning, still produces the cor-\nrect answer. Our quantitative results on ScienceQA further\nconfirm that LLaVA-Phi excels in these types of question-\nanswering tasks.\n4. Experiments\nWe rigorously evaluated LLaVA-Phi using an extensive\narray of academic benchmarks specifically designed for\nmulti-modal models.\nThese included tests for general\nquestion-answering such as VQA-v2 [12], VizWizQA [14],\nScienceQA [28], and TextQA [31], as well as more special-\nized assessments like POPE [22] for evaluating object hallu-\ncination, and MME [10], MMBench [27], and MMVet [36]\nfor a comprehensive evaluation of diverse multi-modal abil-\nities, such as visual understanding and visual commonsense\nreasoning.\nThese benchmarks are meticulously structured to chal-\nlenge and scrutinize complex multi-modal tasks. We bench-\nmarked LLaVA-Phi against a variety of state-of-the-art,\nlarge vision-language models, as detailed in Table 1. It is\nimportant to note that both our method and LLaVA1.5 uti-\nlize the same publicly available datasets for pre-training and\nvisual instruction fine-tuning.\nOur model demonstrated a capacity for visual-based\nquestion-answering, surpassing many existing large multi-\nmodal models.\nRemarkably, LLaVA-Phi outperformed\nmodels that use 7B-parameter or larger Large Language\nModels (LLMs) as their backbone, such as IDEFICS [17]\nand InstructBLIP [8]. A particularly notable achievement\nwas our model\u2019s best performance on ScienceQA [28].\nWe attribute this success to the Phi-2 language model,\nwhich has been specifically trained on code generation and\nmathematical corpora, thereby enhancing our multi-modal\nmodel\u2019s prowess in math-based question-answering.\nIn the comprehensive multi-modal benchmark of MM-\nBench [27], LLaVA-Phi showed significantly superior\nperformance compared to many existing 7B-LLM-based\nvision-language models. For example, our model outper-\nformed Otter by 11.5% and InstructBLIP by 23.8%. This\nunderscores the effectiveness of LLaVA-Phi in handling\ncomplex multi-modal tasks, reinforcing the potential of\nsmaller, more efficient models in the rapidly evolving field\nof multi-modal models.\nWe also compared to MobileVLM [6], a concurrent work\nthat builds up an efficient vision-language model. Across all\nfive benchmarks, our LLaVA-Phi consistently outperforms\ntheir method. It\u2019s important to note that the margins of lead\nare modest, with the exception of ScienceQA. We attribute\nthis performance disparity primarily to the differences in the\npretraining stages of the language models.\n5. Conclusion, Limitation, and Future Works\nWe introduce LLaVA-Phi, a vision language assistant\ndeveloped using the compact language model Phi-2. Our\nwork demonstrates that such small vision-language models\ncan perform effectively on standard benchmarks when\ncombined with the LLaVA training methodology and a\nselect dataset of high-quality data.\nThe primary goal of\nour project is to aid the community in creating lightweight,\nmulti-modal models capable of vision-language reasoning,\noptimized for operation on edge devices. This innovation\npaves the way for deploying multi-modal assistants in\ntime-sensitive applications, such as robotics [35, 38].\nLimitations.\nGiven that Phi-2 utilizes the codegen-\nmono [29] tokenizer and our model has not been specifi-\ncally fine-tuned for following multilingual instructions, our\nLLaVA-Phi architecture is unable to process instructions in\nmultiple languages, including Chinese.\nFuture Works.\nAs language models have become\nsignificantly smaller in size compared to traditional\nvision-language models, they have become more accessible\nand affordable for the research community to explore\nfundamental concepts in vision-language integration.\nIn\nfuture work, we plan to examine the impact of the size\nof the visual encoder and refine the training strategies\nfor small language models, including approaches like\ndirect preference optimization and RLHF, among other\ntechniques. These efforts aim to further reduce model size\nwhile enhancing performance.\n4\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 1\n[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n3\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3\n[4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 3\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n1\n[6] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\nXiaolin Wei, et al.\nMobilevlm: A fast, reproducible and\nstrong vision language assistant for mobile devices. arXiv\npreprint arXiv:2312.16886, 2023. 2, 3, 4\n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\nTworek, Jacob Hilton, Reiichiro Nakano, et al.\nTrain-\ning verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021. 1\n[8] W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B\nLi, P Fung, and S Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 1, 3, 4\n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke\nZettlemoyer. Qlora: Efficient finetuning of quantized llms.\narXiv preprint arXiv:2305.14314, 2023. 3\n[10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 3, 4\n[11] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\nsual instruction model.\narXiv preprint arXiv:2304.15010,\n2023. 3\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017. 3, 4\n[13] Suriya\nGunasekar,\nYi\nZhang,\nJyoti\nAneja,\nCaio\nC\u00b4esar Teodoro Mendes,\nAllie Del Giorno,\nSivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\nOlli Saarikivi, et al.\nTextbooks are all you need.\narXiv\npreprint arXiv:2306.11644, 2023. 1\n[14] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 3, 4\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3\n[16] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700\u20136709, 2019. 3\n[17] H Laurenc\u00b8on, L Saulnier, L Tronchon, S Bekman, A Singh,\nA Lozhkov, T Wang, S Karamcheti, A Rush, and D Kiela.\nObelisc: An open web-scale filtered dataset of interleaved\nimage-text documents.\narXiv preprint arXiv:2306.16527,\n2023. 3, 4\n[18] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 3\n[19] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 3\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 1, 3\n[21] Yuanzhi\nLi,\nS\u00b4ebastien\nBubeck,\nRonen\nEldan,\nAllie\nDel Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks\nare all you need ii: phi-1.5 technical report. arXiv preprint\narXiv:2309.05463, 2023. 1\n[22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 3, 4\n[23] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan\nKulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi\nZhang. Tinygsm: achieving\u00bf 80% on gsm8k with small lan-\nguage models. arXiv preprint arXiv:2312.09241, 2023. 1\n[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 1, 3\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 3\n5\n[26] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li,\nTianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\net al. Llava-plus: Learning to use tools for creating multi-\nmodal agents. arXiv preprint arXiv:2311.05437, 2023. 1\n[27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n3, 4\n[28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.\nAdvances\nin Neural Information Processing Systems, 35:2507\u20132521,\n2022. 1, 3, 4\n[29] Erik Nijkamp,\nBo Pang,\nHiroaki Hayashi,\nLifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and Caim-\ning Xiong.\nCodegen: An open large language model for\ncode with multi-turn program synthesis.\narXiv preprint\narXiv:2203.13474, 2022. 4\n[30] OpenAI. Gpt-4 technical report. arXiv preprint, 2023. 1\n[31] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 3, 4\n[32] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,\nChunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui,\nYu-Xiong Wang, Yiming Yang, et al. Aligning large multi-\nmodal models with factually augmented rlhf. arXiv preprint\narXiv:2309.14525, 2023. 1\n[33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1, 2, 3\n[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 1\n[35] Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan\nXu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu,\nFeifei Feng, et al. Object-centric instruction augmentation\nfor robotic manipulation. arXiv preprint arXiv:2401.02814,\n2024. 4\n[36] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 3, 4\n[37] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language\nUnderstanding with Advanced Large Language Models.\narXiv preprint, page arXiv:2304.10592, 2023. 1, 3\n[38] Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan\nXu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong\nLiu, Feifei Feng, et al. Language-conditioned robotic ma-\nnipulation with fast and slow thinking.\narXiv preprint\narXiv:2401.04181, 2024. 4\n6\n"
  },
  {
    "title": "ODIN: A Single Model for 2D and 3D Perception",
    "link": "https://arxiv.org/pdf/2401.02416.pdf",
    "upvote": "10",
    "text": "ODIN: A Single Model for 2D and 3D Perception\nAyush Jain1, Pushkal Katara1, Nikolaos Gkanatsios1, Adam W. Harley2, Gabriel Sarch1,\nKriti Aggarwal3, Vishrav Chaudhary3, Katerina Fragkiadaki1\n1Carnegie Mellon University, 2Stanford University, 3 Microsoft\n{ayushj2, pkatara ngkanats,gsarch,kfragki2}@andrew.cmu.edu\naharley@cs.stanford.edu, {vchaudhary,kragga}@microsoft.com\nAbstract\nState-of-the-art models on contemporary 3D percep-\ntion benchmarks like ScanNet consume and label dataset-\nprovided 3D point clouds, obtained through post process-\ning of sensed multiview RGB-D images.\nThey are typi-\ncally trained in-domain, forego large-scale 2D pre-training\nand outperform alternatives that featurize the posed RGB-\nD multiview images instead. The gap in performance be-\ntween methods that consume posed images versus post-\nprocessed 3D point clouds has fueled the belief that 2D\nand 3D perception require distinct model architectures.\nIn this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that\ncan segment and label both 2D RGB images and 3D point\nclouds, using a transformer architecture that alternates be-\ntween 2D within-view and 3D cross-view information fu-\nsion. Our model differentiates 2D and 3D feature oper-\nations through the positional encodings of the tokens in-\nvolved, which capture pixel coordinates for 2D patch tokens\nand 3D coordinates for 3D feature tokens. ODIN achieves\nstate-of-the-art performance on ScanNet200, Matterport3D\nand AI2THOR 3D instance segmentation benchmarks, and\ncompetitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the\nsensed 3D point cloud is used in place of the point cloud\nsampled from 3D mesh. When used as the 3D perception en-\ngine in an instructable embodied agent architecture, it sets\na new state-of-the-art on the TEACh action-from-dialogue\nbenchmark. Our code and checkpoints can be found at the\nproject website https://odin-seg.github.io.\n1. Introduction\nThere has been a surge of interest in porting 2D founda-\ntional image features to 3D scene understanding [8, 14, 21,\n23, 37, 40, 46\u201348]. Some methods lift pre-trained 2D image\nfeatures using sensed depth to 3D feature clouds [8, 37, 40,\n47]. Others distill 2D backbones to differentiable paramet-\nric 3D models, e.g., NeRFs, by training them per scene to\nrender 2D feature maps of pre-trained backbones [23, 46].\nDespite this effort, and despite the ever-growing power of\n2D backbones [4, 53], the state-of-the-art on established 3D\nsegmentation benchmarks such as ScanNet [6] and Scan-\nNet200 [41] still consists of models that operate directly in\n3D, without any 2D pre-training stage [28, 44]. Given the\nobvious power of 2D pre-training, why is it so difficult to\nyield improvements in these 3D tasks?\nWe observe that part of the issue lies in a key implemen-\ntation detail underlying these 3D benchmark evaluations.\nScanNet and ScanNet200 do not actually ask methods to use\nRGB-D images as input, even though this is the sensor data.\nInstead, these benchmarks first register all RGB-D frames\ninto a single colored point cloud and reconstruct the scene\nas cleanly as possible, relying on manually tuned stages for\nbundle adjustment, outlier rejection and meshing, and ask\nmodels to label the output reconstruction. While it is cer-\ntainly viable to scan and reconstruct a room before labelling\nany of the objects inside, this pipeline is perhaps inconsis-\ntent with the goals of embodied vision (and typical 2D vi-\nsion), which involves dealing with actual sensor data and\naccounting for missing or partial observations. We there-\nfore hypothesize that method rankings will change, and the\nimpact of 2D pre-training will become evident, if we force\nthe 3D models to take posed RGB-D frames as input rather\nthan pre-computed reconstructions. Our revised evaluation\nsetting also opens the door to new methods, which can train\nand perform inference in either single-view or multi-view\nsettings, with either RGB or RGB-D sensors.\nWe propose Omni-Dimensional INstance segmentation\n(ODIN)\u2020, a model for 2D and 3D object segmentation and\nlabelling that can parse single-view RGB images and/or\nmultiview posed RGB-D images. As shown in Fig. 1, ODIN\nalternates between 2D and 3D stages in its architecture,\nfusing information in 2D within each image view, and in\n\u2020The Norse god Odin sacrificed one of his eyes for wisdom, trading\none mode of perception for a more important one. Our approach sacrifices\nperception on post-processed meshes for perception on raw sensor data.\narXiv:2401.02416v1  [cs.CV]  4 Jan 2024\n3D\n2D \nposed RGB-D images\n3D\n2D \n2D \n2D \n2D \n2D \n3D\n2D \n2D \n3D\n3D\n2D \n3D\n3D\n3D\n2D \n2D Layer\n3D Layer\nUnprojection\nReshape\nsingle RGB image\n3D Instance Segmentation  \n3D Tokens \n2D Instance Segmentation  \nMask \nDecoder\nShared Layers\n2D \n2D \n2D Tokens\nFigure 1. Omni-Dimensional INstance segmentation (ODIN) is a model that can parse either a single RGB image or a multiview posed\nRGB-D sequence into 2D or 3D labelled object segments respectively. Left: Given a posed RGB-D sequence as input, ODIN alternates\nbetween a within-view 2D fusion and a cross-view 3D fusion. When the input is a single RGB image, the 3D fusion layers are skipped.\nODIN shares the majority of its parameters across both RGB and RGB-D inputs, enabling the use of pre-trained 2D backbones. Right: At\neach 2D-to-3D transition, ODIN unprojects 2D feature tokens to their 3D locations using sensed depth and camera intrinsics and extrinsics.\n3D across posed image views. At each 2D-to-3D transi-\ntion, it unprojects 2D tokens to their 3D locations using the\ndepth maps and camera parameters, and at each 3D-to-2D\ntransition, it projects 3D tokens back to their image loca-\ntions. Our model differentiates between 2D and 3D features\nthrough the positional encodings of the tokens involved,\nwhich capture pixel coordinates for 2D patch tokens and\n3D coordinates for 3D feature tokens. When dealing with\n2D single-view input, our architecture simply skips the 3D\nlayers and makes a forward pass with 2D layers alone.\nWe test ODIN in 2D and 3D instance segmentation and\n3D semantic segmentation on the 2D COCO object seg-\nmentation benchmark and the 3D benchmarks of Scan-\nNet [6], ScanNet200 [41], Matterport3D [2], S3DIS [1]\nand AI2THOR [7, 25]. When compared to methods us-\ning pre-computed mesh point cloud as input, our approach\nperforms slightly worse than state-of-the-art on ScanNet\nand S3DIS, but better on ScanNet200 and Matterport3D.\nWhen using real sensor data as input for all methods, our\nmethod performs even better, outperforming all prior work\nby a wide margin, in all datasets. We demonstrate that our\nmodel\u2019s ability to jointly train on 3D and 2D datasets re-\nsults in performance increase on 3D benchmarks, and also\nyields competitive segmentation accuracy on the 2D COCO\nbenchmark. Our ablations show that interleaving 2D and\n3D fusion operations outperforms designs where we first\nprocess in 2D and then move to 3D, or simply paint 3D\npoints with 2D features. Stepping toward our broader goal\nof embodied vision, we also deploy ODIN as the 3D ob-\nject segmentor of a SOTA embodied agent model [42] on\nthe simulation benchmark TEACh [36] in the setup with\naccess to RGB-D and pose information from the simulator,\nand demonstrate that our model sets a new state-of-the-art.\nWe make our code publicly available at https://odin-\nseg.github.io.\n2. Related Work\n3D Instance Segmentation\nEarly methods in 3D instance\nsegmentation [3, 15, 22, 30, 49, 58] group their seman-\ntic segmentation outputs into individual instances.\nRe-\ncently, Mask2Former [4] achieved state-of-the-art in 2D in-\nstance segmentation by instantiating object queries, each\ndirectly predicting an instance segmentation mask by do-\ning dot-product with the feature map of the input image.\nInspired by it, Mask3D [44] abandons the grouping strat-\negy of prior 3D models to use the simple decoder head\nof Mask2Former. MAFT [28] and QueryFormer [34] im-\nprove over Mask3D by incorporating better query initial-\nization strategies and/or relative positional embeddings.\nWhile this shift to Mask2Former-like architecture brought\nthe 3D instance segmentation architectures closer to their\n2D counterparts, the inputs and backbones remain very dif-\nferent: 2D models use popular and often pre-trained back-\nbones [16, 33], while 3D methods [44] operate over point\nclouds and use sparse convolution-based backbones [5],\ntrained from scratch on small-scale 3D datasets.\nIn this\nwork, we propose to directly use RGB-D input and de-\nsign architectures that can leverage strong 2D backbones\nto achieve strong performance on 3D benchmarks.\n3D Datasets and Benchmarks\nMost 3D models primar-\nily operate on point clouds, avoiding the use of image-based\nfeatures partly due to the design of popular benchmarks.\nThese benchmarks generate point clouds by processing raw\nRGB-D sensor data, involving manual and noisy steps that\nresult in misalignments between the reconstructed point\ncloud and sensor data. For instance, ScanNet [6] under-\ngoes complex mesh reconstruction steps, including bundle\nreconstruction, implicit TSDF representation fitting, march-\ning cubes, merging and deleting noisy mesh vertices, and\nfinally manual removal of mesh reconstruction with high\nmisalignments. Misalignments introduced by the mesh re-\nconstruction process can cause methods processing sensor\ndata directly to underperform compared to those trained\nand tested on provided point clouds. Additionally, some\ndatasets, like HM3D [54] lack access to raw RGB-D data.\nWhile mesh reconstruction has its applications, many real-\ntime applications need to directly process sensor data.\n2D-based 3D perception\nUnlike instance segmentation\nliterature, several approaches for semantic segmentation\nlike MVPNet [20], BPNet [17] and DeepViewAgg [40] uti-\nlize the sensor point cloud directly instead of the mesh-\nsampled point cloud. Virtual Multiview Fusion [26] forgoes\nsensor RGB-D images in favour of rendering RGB-D im-\nages from the provided mesh to fight misalignments and low\nfield-of-view in ScanNet images. Similar to our approach,\nBPNet and DeepViewAgg integrate 2D-3D information at\nvarious feature scales and initialize their 2D streams with\npre-trained features. Specifically, they employ separate 2D\nand 3D U-Nets for processing the respective modalities and\nfuse features from the two streams through a connection\nmodule. Rather than employing distinct streams for featur-\nizing raw data, our architecture instantiates a single unified\nU-Net which interleaves 2D and 3D layers and can handle\nboth 2D and 3D perception tasks with a single unified ar-\nchitecture. Notably, while these works focus solely on se-\nmantic segmentation, our single architecture excels in both\nsemantic and instance segmentation tasks.\nRecent advancements in 2D foundation models [24, 39]\nhave spurred efforts to apply them to 3D tasks such as point\ncloud classification [38, 52, 56], zero-shot 3D semantic seg-\nmentation [14, 21, 37] and more recently, zero-shot instance\nsegmentation [47]. Commonly, these methods leverage 2D\nfoundation models to featurize RGB images, project 3D\npoint clouds onto these images, employ occlusion reasoning\nusing depth and integrate features from all views through\nsimple techniques like mean-pooling. Notably, these ap-\nproaches predominantly focus on semantic segmentation,\nemphasizing pixel-wise labeling, rather than instance la-\nbeling, which necessitates cross-view reasoning to asso-\nciate the same object instance across multiple views. Open-\nMask3D [47] is the only method that we are aware of that at-\ntempts 3D instance segmentation using 2D foundation mod-\nels, by simply training a class-agnostic 3D object segmen-\ntor on 3D point clouds and labelling it utilizing CLIP 2D\nfeatures. Despite their effectiveness in a zero-shot setting,\nthey generally lag behind SOTA 3D supervised methods by\n15-20%. Rather than relying on features from foundation\nmodels, certain works [10, 12] create 3D pseudo-labels us-\ning pre-trained 2D models. Another line of work involves\nfitting Neural-Radiance Fields (NeRFs), incorporating fea-\ntures from CLIP [23, 48] or per-view instance segmenta-\ntions from state-of-the-art 2D segmentors [46].\nDespite\ntheir impressive results, these approaches require expen-\nsive per-scene optimization that prohibits testing on all test\nscenes to compare against SOTA 3D discriminative mod-\nels. Instead of repurposing 2D foundation models for 3D\ntasks, Omnivore [13] proposes to build a unified architec-\nture that can handle multiple visual modalities like images,\nvideos and single-view RGB-D image but they only show\nresults for classification tasks. We similarly propose a sin-\ngle unified model capable of performing both single-view\n2D and multi-view 3D instance and semantic segmentation\ntasks while utilizing pre-trained weights for the majority of\nour architecture.\n3. Method\nODIN\u2019s architecture is shown in Fig. 2. It takes either a\nsingle RGB image or a set of posed RGB-D images (i.e.,\nRGB images associated with depth maps and camera pa-\nrameters) and outputs the corresponding 2D or 3D instance\nsegmentation masks and their semantic labels. To achieve\nthis, ODIN alternates between a 2D within-view fusion and\na 3D attention-based cross-view fusion, as illustrated in blue\nblocks and yellow blocks in Fig. 2. A segmentation de-\ncoding head predicts instance masks and semantic labels.\nNotably, ODIN shares the majority of its parameters across\nboth RGB and multiview RGB-D inputs. We detail the com-\nponents of our architecture below.\nWithin-view 2D fusion:\nWe start from a 2D back-\nbone, such as ResNet50 [16] or Swin Transformer [33],\npre-trained for 2D COCO instance segmentation follow-\ning Mask2Former [4], a state-of-the-art 2D segmentation\nmodel. When only a single RGB image is available, we pass\nit through the full backbone to obtain 2D features at multi-\nple scales. When a posed RGB-D sequence is available, this\n2D processing is interleaved with 3D stages, described next.\nBy interleaving within-view and cross-view contextualiza-\ntion, we are able to utilize the pre-trained features from the\n2D backbone while also fusing features across views, mak-\ning them 3D-consistent.\nCross-view 3D fusion: The goal of cross-view fusion is\nto make the individual images\u2019 representations consistent\nacross views. As we show in our ablations, cross-view fea-\nture consistency is essential for 3D instance segmentation:\nit enables the segmentation head to realize that a 3D object\nobserved from multiple views is indeed a single instance,\n2D Layers\n3D Layers\n  Shared Layers\n2D ResBlock2\n3D RelPos Attn\nBackbone\n2D ResBlock3\n3D RelPos Attn\n2D ResBlock4\n3D RelPos Attn\nUpsampler\n3D RelPos Attn\n3D RelPos Attn\n3D RelPos Attn\nUpsample Layer\nMask Decoder Head\nQuery Re\ufb01nement\nQuery Re\ufb01nement\nQuery Re\ufb01nement\nsingle RGB Image\n3D Instance Segmentation\n2D Instance Segmentation  \nposed RGB-D Images\nInput\n2D ResBlock1\nParametric Queries\nMulti Scale Deformable Self-Attention \nFigure 2. ODIN Architecture: The input to our model is either a single RGB image or a multiview RGB-D posed sequence. We feed\nthem to ODIN\u2019s backbone which interleaves 2D within-view fusion layers and 3D cross-view attention layers to extract feature maps of\ndifferent resolutions (scales). These feature maps exchange information through a multi-scale attention operation. Additional 3D fusion\nlayers are used to improve multiview consistency. Then, a mask decoder head is used to initialize and refine learnable slots that attend to\nthe multi-scale feature maps and predict object segments (masks and semantic classes).\nrather than a separate instance in each viewpoint.\n1. 2D-to-3D Unprojection: We unproject each 2D feature\nmap to 3D by lifting each feature vector to a correspond-\ning 3D location, using nearest neighbor depth and known\ncamera intrinsic and extrinsic parameters, using a pinhole\ncamera model. Subsequently, the resulting featurized point\ncloud undergoes voxelization, where the 3D space is dis-\ncretized into a volumetric grid. Within each occupied grid\ncell (voxel), the features and XYZ coordinates are mean-\npooled to derive new sets of 3D feature tokens and their\nrespective 3D locations.\n2. 3D k-NN Transformer with Relative Positions: We fuse\ninformation across 3D tokens using k-nearest-neighbor at-\ntention with relative 3D positional embeddings.\nThis is\nsimilar to Point Transformers [51, 57], but we simply use\nvanilla cross-attention instead of the vector attention pro-\nposed in those works. Specifically, in our approach, each\n3D token attends to its k nearest neighbors. The positional\nembeddings in this operation are relative to the query to-\nken\u2019s location. We achieve this by encoding the distance\nvector between a token and its neighbour with an MLP. The\npositional embedding for the query is simply encoding of\nthe 0 vector. We therefore have\nqpos = MLP(0);\n(1)\nkpos = vpos = MLP(pi \u2212 pj),\n(2)\nwhere pi represents the 3D tokens, shaped N \u00d7 1 \u00d7 3, and\npj represents the k nearest neighbors of each pi, shaped\nN \u00d7k\u00d73. In this way, the attention operation is invariant to\nthe absolute coordinates of the 3D tokens and only depends\non their relative spatial arrangements. While each 3D token\nalways attends to the same k neighbors, its effective recep-\ntive field grows across layers, as the neighbors\u2019 features get\nupdated when they perform their own attention [11].\n3. 3D-to-2D Projection: After contextualizing the tokens in\n3D, we project the features back to their original 2D loca-\ntions. We first copy the feature of each voxel to all points\nwithin that voxel. We then reshape these points back into\nmultiview 2D feature maps, so that they may be processed\nby the next 2D module. The features vectors are unchanged\nin this transition; the difference lies in their interpretation\nand shape. In 2D the features are shaped V \u00d7 H \u00d7 W \u00d7 F,\nrepresenting a feature map for each viewpoint, and in 3D\nthey are shaped N \u00d7F, representing a unified feature cloud,\nwhere N = V \u00b7 H \u00b7 W.\nCross-scale fusion and upsampling:\nAfter multiple\nsingle-view and cross-view stages, we have access to multi-\nple features maps per image, at different resolutions. We\nmerge these with the help of deformable 2D attention,\nakin to Mask2Former [4], operating on the three lowest-\nresolution scales (1/32, 1/16, 1/8). When we have 3D in-\nput, we apply an additional 3D fusion layer at each scale af-\nter the deformable attention, to restore the 3D consistency.\nFinally, we use a simple upsampling layer on the 1/8 reso-\nlution feature map to bring it to 1/4 resolution and add with\na skip connection to the 1/4 feature map from the backbone.\nSensor depth to mesh point cloud feature transfer: For\n3D benchmarks like ScanNet [6] and ScanNet200 [41], the\nobjective is to label a point cloud derived from a mesh rather\nthan the depth map from the sensor. Hence, on those bench-\nmarks, instead of upsampling the 1/8 resolution feature\nmap to 1/4, we trilinearly interpolate features from the 1/8\nresolution feature map to the provided point cloud sampled\nfrom the mesh. This means: for each vertex in the mesh,\nwe trilinearly interpolate from our computed 3D features to\nobtain interpolated features. We additionally similarly in-\nterpolate from the unprojected 1/4 resolution feature map\nin the backbone, for an additive skip connection.\nShared 2D-3D segmentation mask decoder: Our segmen-\ntation decoder is a Transformer, similar to Mask2Former\u2019s\ndecoder head, which takes as input upsampled 2D or 3D\nfeature maps and outputs corresponding 2D or 3D segmen-\ntation masks and their semantic classes. Specifically, we\ninstantiate a set of N learnable object queries responsible\nfor decoding individual instances. These queries are itera-\ntively refined by a Query Refinement block, which consists\nof cross-attention to the upsampled features, followed by a\nself-attention between the queries. Except for the positional\nembeddings, all attention and query weights are shared be-\ntween 2D and 3D. We use Fourier positional encodings in\n2D, while in 3D we encode the XYZ coordinates of the 3D\ntokens with an MLP. The refined queries are used to pre-\ndict instance masks and semantic classes. For mask pre-\ndiction, the queries do a token-wise dot product with the\nhighest-resolution upsampled features. For semantic class\nprediction, we use an MLP over the queries, mapping them\nto class logits. We refer readers to Mask2Former [4] for\nfurther details.\nOpen vocabulary class decoder:\nDrawing inspiration\nfrom prior open-vocabulary detection methods [19, 29, 61],\nwe introduce an alternative classification head capable of\nhandling an arbitrary number of semantic classes.\nThis\nmodification is essential for joint training on multiple\ndatasets. Similar to BUTD-DETR [19] and GLIP [29], we\nsupply the model with a detection prompt formed by con-\ncatenating object categories into a sentence (e.g., \u201cChair.\nTable.\nSofa.\u201d) and encode it using RoBERTa [32].\nIn\nthe query-refinement block, queries additionally attend to\nthese text tokens before attending to the upsampled fea-\nture maps. For semantic class prediction, we first perform a\ndot-product operation between queries and language tokens,\ngenerating one logit per token in the detection prompt. The\nlogits corresponding to prompt tokens for a specific object\nclass are then averaged to derive per-class logits. This can\nhandle multi-word noun phrases such as \u201cshower curtain\u201d,\nwhere we average the logits corresponding to \u201cshower\u201d\nand \u201ccurtain\u201d. The segmentation masks are predicted by\na pixel-/point-wise dot-product, in the same fashion as de-\nscribed earlier.\nImplementation details:\nWe initialize our model with\npre-trained weights from Mask2Former [4] trained on\nCOCO [31]. Subsequently, we train all parameters end-to-\nend, including both pre-trained and new parameters from\n3D fusion layers. During training in 3D scenes, our model\nprocesses a sequence of N consecutive frames, usually\ncomprising 25 frames. At test time, we input all images\nin the scene to our model, with an average of 90 images per\nscene in ScanNet. We use vanilla closed-vocabulary decod-\ning head for all experiments except when training jointly on\n2D-3D datasets. There we use our open vocabulary class\ndecoder that lets us handle different label spaces in these\ndatasets. During training, we employ open vocabulary mask\ndecoding for joint 2D and 3D datasets and vanilla closed-\nvocabulary decoding otherwise. Training continues until\nconvergence on 2 NVIDIA A100s with 40 GB VRAM, with\nan effective batch size of 6 in 3D and 16 in 2D. For joint\ntraining on 2D and 3D datasets, we alternate sampling 2D\nand 3D batches with batch sizes of 3 and 8 per GPU, respec-\ntively. We adopt Mask2Former\u2019s strategy, using Hungar-\nian matching for matching queries to ground truth instances\nand supervision losses. While our model is only trained for\ninstance segmentation, it can perform semantic segmenta-\ntion for free at test time like Mask2Former. We refer to\nMask2Former [4] for more details.\n4. Experiments\n4.1. Evaluation on 3D benchmarks\nDatasets: First, we test our model on 3D instance and\nsemantic segmentation in the ScanNet [6] and Scan-\nNet200 [41] benchmarks.\nThe objective in these bench-\nmarks is to label the point cloud sampled from the 3D\nmesh of a scene reconstructed from raw sensor data. Scan-\nNet evaluates on 20 common semantic classes, while Scan-\nNet200 uses 200 classes, which is more representative of\nthe long-tailed object distribution encountered in the real\nworld. We report results on the official validation split of\nthese datasets.\nEvaluation metrics: We follow the standard evaluation\nmetrics, namely mean Average Precision (mAP) for in-\nstance segmentation and mean Intersection over Union\n(mIoU) for semantic segmentation.\nBaselines: In instance segmentation, our main baseline is\nthe SOTA 3D method Mask3D [44]. For a thorough com-\nparison, we train both Mask3D and our model with sen-\nsor RGB-D point cloud input and evaluate them on the\nbenchmark-provided mesh-sampled point clouds. We also\ncompare with the following recent and concurrent works:\nPBNet [58], QueryFormer [34] and MAFT [28]. Query-\nFormer and MAFT explore query initialization and refine-\nment in a Mask3D-like architecture and thus have comple-\nmentary advantages to ours. Unlike ODIN, these methods\nTable 1. Evaluation on Established 3D Benchmarks. (\u00a7 = trained by us using official codebase)\n(a) Comparison on ScanNet for Instance Segmentation Task.\nModel\nmAP\nmAP50\nmAP25\nSensor RGBD\nPoint Cloud\nMask3D\u00a7 [44]\n43.9\n60.0\n69.9\nODIN-ResNet50 (Ours)\n45.7\n66.3\n81.8\nODIN-Swin-B (Ours)\n48.0\n69.2\n83.5\nMesh Sampled\nPoint Cloud\nSoftGroup [49]\n46.0\n67.6\n78.9\nPBNet [58]\n54.3\n70.5\n78.9\nMask3D [44]\n55.2\n73.7\n83.5\nQueryFormer [34]\n56.5\n74.2\n83.3\nMAFT [28]\n58.4\n75.9\n-\n(b) Comparison on ScanNet for Semantic Segmentation Task.\nModel\nmIoU\nSensor RGBD\nPoint Cloud\nMVPNet [20]\n68.3\nBPNet [17]\n69.7\nDeepViewAgg [40]\n71.0\nODIN-ResNet50 (Ours)\n73.2\nODIN-Swin-B (Ours)\n76.0\nRendered RGBD\nPoint Cloud\nVMVF [26]\n76.4\nMesh Sampled\nPoint Cloud\nPoint Transformer v2 [51]\n75.4\nStratified Transformer [27]\n74.3\nOctFormer [50]\n75.7\nSwin3D-L [55]\n76.7\nZero-Shot\nOpenScene [37]\n54.2\n(c) Comparison on ScanNet200 for Instance Segmentation Task.\nModel\nmAP\nmAP50\nmAP25\nSensor RGBD\nPoint Cloud\nMask3D [44] \u00a7\n15.5\n21.4\n24.3\nODIN-ResNet50 (Ours)\n26.0\n37.6\n43.8\nODIN-Swin-B (Ours)\n30.0\n43.0\n51.0\nMesh Sampled\nPoint Cloud\nMask3D [44]\n27.4\n37.0\n42.3\nQueryFormer [34]\n28.1\n37.1\n43.4\nMAFT [28]\n29.2\n38.2\n43.3\nZero-Shot\nOpenMask3D [47]\n15.4\n19.9\n23.1\n(d) Comparison on ScanNet200 for Semantic Segmentation Task.\nModel\nmIoU\nSensor RGBD\nPoint Cloud\nODIN-ResNet50 (Ours)\n33.9\nODIN-Swin-B (Ours)\n38.2\nMesh Sampled\nPoint Cloud\nLGround [41]\n28.9\nCeCo [60]\n32.0\nOctformer [50]\n32.6\ndirectly process 3D point clouds and initialize their weights\nfrom scratch. As motivated before, utilizing RGB-D input\ndirectly has several advantages, including avoiding costly\nmesh building processes, achieving closer integration of 2D\nand 3D perception, and leveraging pre-trained features and\nabundant 2D data.\nIn semantic segmentation, we compare with MVP-\nNet\n[20],\nBPNet\n[17]\nand\nstate-of-the-art\nDeep-\nViewAgg [40] which directly operate on sensor RGB\nor RGB-D images and point clouds.\nWe also compare\nwith VMVF [26] which operates over rendered RGB-D\nimages from the provided mesh, with heuristics for camera\nview sampling to avoid occlusions, ensures balanced scene\ncoverage, and employs a wider field-of-view, though we\nnote their code is not publicly available. Similar to ODIN,\nall of these methods utilize 2D pre-trained backbones. We\nalso compare with Point-Transformer v2 [51], Stratified\nTransformer [27], OctFormer [50] and Swin3D-L [55]\nwhich process the mesh-sampled point cloud directly,\nwithout using any 2D pre-training.\nOn the ScanNet200\nsemantic segmentation benchmark, we compare with SOTA\nOctFormer [50] and with CeCo [60], a method specially\ndesigned to fight class-imbalance in ScanNet200. These\nmethods directly process the point cloud and do not use 2D\nimage pre-trained weights. We also compare with LGround\n[41] which uses 2D CLIP pre-training. We also compare\nwith zero-shot 2D foundation model-based 3D models of\nOpenScene [37] and OpenMask3D [47]. This comparison\nis unfair since they are not supervised within-domain,\nbut we include them for completeness.\nThe results are\npresented in Tab. 1. We draw the following conclusions:\nPerformance drops with sensor point cloud as input\n(Tab. 1a): Mask3D\u2019s performance drops from 55.2% mAP\nwith mesh point cloud input to 43.9% mAP with sensor\npoint cloud input. This is consistent with prior works [26,\n40] in 3D semantic segmentation on ScanNet, which at-\ntributes the drop to misalignments caused by noise in cam-\nera poses, depth variations and post-processing steps.\nODIN outperforms SOTA 3D methods with sensor point\ncloud input and underperforms them when baselines use\nmesh-sampled point clouds (Tab. 1a): Our model signifi-\ncantly outperforms SOTA Mask3D model with sensor point\ncloud input and achieves comparable performance to meth-\nods using mesh-sampled point cloud input on the mAP25\nmetric while far behind on mAP metric, due to misalign-\nments between the 3D mesh and the sensor point cloud.\nODIN sets a new SOTA in sensor-based semantic seg-\nmentation on ScanNet (Tab. 1b) outperforming all meth-\nTable 2. Comparison on AI2THOR for Semantic and Instance\nSegmentation.\nModel\nmAP\nmAP50\nmAP25\nmIoU\nMask3D [44]\n60.6\n70.8\n76.6\n-\nODIN-ResNet50 (Ours)\n64.9\n74.1\n80.0\n71.2\nODIN-Swin-B (Ours)\n67.6\n76.5\n81.8\n74.2\nods operating over sensor RGB point cloud including the\nSOTA DeepViewAgg [40]. Additionally, our model closely\nmatches the performance of models operating on rendered\nRGB-D point cloud and mesh sampled point cloud.\nODIN sets a new instance segmentation SOTA on the\nlong-tailed ScanNet200 dataset (Tab. 1c) outperforming\nSOTA 3D models on all setups including the models trained\non mesh-sampled point cloud especially by a large margin\nin mAP25 metric, while exclusively utilizing sensor RGB-\nD data. This highlights the contribution of 2D features, par-\nticularly in detecting a long tail of class distribution where\nlimited 3D data is available. We show more detailed results\nwith performance on the head, common and tail classes in\nthe appendix ( Sec. A.3).\nODIN sets a new semantic segmentation SOTA on Scan-\nNet200 (Tab. 1d), outperforming SOTA semantic segmen-\ntation models that use mesh point clouds.\n4.2. Evaluation on multiview RGB-D in simulation\nUsing the AI2THOR [25] simulation environment with pro-\ncedural homes from ProcThor [7], we collected RGB-D\ndata for 1500 scenes (1200 training, 300 test) of similar\nsize as ScanNet (more details in appendix,\nSec. B). We\ntrain and evaluate our model and SOTA Mask3D [44] on\nthe unprojected RGB-D images. As shown in Tab. 2, our\nmodel outperforms Mask3D by 7% mAP, showing strong\nperformance in a directly comparable RGB-D setup. It sug-\ngests that current real-world benchmarks may restrain mod-\nels that featurizes RGB-D sensor point clouds due to mis-\nalignments.\nWe hope this encourages the community to\nalso focus on directly collecting, labeling, and benchmark-\ning RGB-D sensor data.\n4.3. Embodied Instruction Following\nWe apply ODIN in the embodied setups of TEACh [36]\nand ALFRED [45] where agents have access to RGB, depth\nand camera poses and need to infer and execute task and\naction plans from dialogue segments and instructions, re-\nspectively. These agents operate in dynamic home envi-\nronments and cannot afford expensive mesh building steps.\nDetecting objects well is critical for task success in both\ncases. Prior SOTA methods [36, 42] run per-view 2D in-\nstance segmentation models [4, 9] and link the detected in-\nstances using simple temporal reasoning regarding spatial\nand appearance proximity. Instead, ODIN processes its last\nTable 3. Embodied Instruction Following. SR = success rate.\nGC = goal condition success rate.\nTEACh\nALFRED\nUnseen\nSeen\nUnseen\nSeen\nSR\nGC\nSR\nGC\nSR\nGC\nSR\nGC\nFILM [35]\n-\n-\n-\n-\n30.7 42.9 26.6 38.2\nHELPER [42]\n15.8 14.5 11.6 19.4 37.4 55.0 26.8 41.2\nHELPER + ODIN\n(OURS)\n18.6\n18.6 13.8 26.6 47.7 61.6 33.5 47.1\nTable 4. Joint Training on Sensor RGB-D point cloud from\nScanNet and 2D RGB images from COCO.\nScanNet\nCOCO\nmAP\nmAP50\nmAP25\nmAP\nMask3D [44]\n43.9\n60.0\n69.9\n\u2717\nMask2Former [4]\n\u2717\n\u2717\n\u2717\n43.7\nODIN (trained in 2D)\n\u2717\n\u2717\n\u2717\n43.6\nODIN (trained in 3D)\n45.7\n66.3\n81.8\n\u2717\nODIN (trained jointly)\n48.3\n69.5\n82.2\n40.7\nN egocentric views and segments objects instances directly\nin 3D. We equip HELPER [42], a state-of-the-art embod-\nied model, with ODIN as its 3D object detection engine.\nWe evaluate using Task Sucess Rate (SR) which checks if\nthe entire task is executed successfully, and Goal Condi-\ntioned Success Rate (GC) which checks the proportion of\nsatisfied subgoals across all episodes [36, 45]. We perform\nevaluation on \u201dvalid-seen\u201d (houses similar to the training\nset) and \u201dvalid-unseen\u201d (different houses) splits. In Tab. 3,\nwe observe that HELPER with ODIN as its 3D object detec-\ntor significantly outperforms HELPER that uses the original\n2D detection plus linking perception pipeline.\n4.4. Ablations and Variants\nWe conduct our ablation experiments on the ScanNet\ndataset in Tab. 4 and Tab. 5. Our conclusions are:\nJoint 2D-3D training helps 3D perception We compare\njoint training of ODIN on sensor RGB-D point clouds\nfrom ScanNet and 2D RGB images from COCO to vari-\nants trained independently on 2D and 3D data, all initialized\nfrom pre-trained COCO weights. Since there are different\nclasses in ScanNet and COCO, we use our open-vocabulary\nsemantic class-decoding head instead of the vanilla closed-\nvocabulary head. Results in Tab. 4 show that joint training\nyields a 2.6% absolute improvement in 3D, and causes a\nsimilar drop in 2D. We show qualitative results in Fig. 3.\nThe COCO dataset is orders of magnitude larger than Scan-\nNet, and we observe that the model fits faster on ScanNet.\nAs a result, the model keeps improving on COCO while on\nScanNet it begins to overfit. This highlights the need to ex-\nplore smarter strategies for balancing 2D and 3D data dur-\nTable 5. Ablations on ScanNet Dataset.\n(a) Cross-View Contextualization.\nModel\nmAP\nmIoU\nODIN (Ours)\n45.7\n73.2\nNo 3D Fusion\n38.8\n71.9\nNo interleaving\n40.0\n72.5\n(b) Effect of Pre-Trained Features.\nModel\nmAP\nmIoU\nODIN (Ours)\n45.7\n73.2\nOnly pre-trained back-\nbone\n41.0\n71.6\nNo pre-trained features\n36.6\n65.4\n(c) Effect of Freezing Backbone.\nModel\nResNet50\nSwin-B\nmAP\nmIoU\nmAP\nmIoU\nODIN (Ours)\n45.7\n73.2\n48.0\n76.0\nWith\nfrozen\nbackbone\n45.7\n72.6\n45.6\n74.7\n2D Instance Segmentation\n3D Instance Segmentation\nFigure 3. Qualitative Results of ODIN on 3D and 2D inputs.\ning training to prevent overfitting in one domain and under-\nfitting in the other \u2013 we leave this for future work. Neverthe-\nless, this experiment highlights the benefits of joint training\nwith 2D datasets for 3D segmentation in ODIN. Note that\nwe do not jointly train on 2D and 3D datasets in any of our\nother experiments due to computational constraints.\nCross-View fusion is crucial for instance segmentation\nbut not for semantic segmentation (Tab. 5a): removing\n3D cross-view fusion layers results in a 6.9% mAP drop for\ninstance segmentation, and a slight drop of 1.3% mIoU in\nsemantic segmentation. Popular 2D-based 3D open vocab-\nulary works [21, 37] without strong cross-view fusion only\nfocus on semantic segmentation and thus could not uncover\nthis issue. Row-3 shows a 5.7% mAP drop when cross-view\n3D fusion happens after all within-view 2D layers instead of\ninterleaving the within-view and cross-view fusion.\n2D pre-trained weight initialization helps (Tab. 5b): ini-\ntializing only the image backbone with pre-trained weights,\ninstead of all layers (except the 3D fusion layers), results in\na 4.7% mAP drop (row-2). Starting the entire model from\nscratch leads to a larger drop of 9.1% mAP (row-3). This\nunderscores the importance of sharing as many parameters\nas possible with the 2D models to leverage the maximum\npossible 2D pre-trained weights.\nStronger 2D backbones helps (Tab. 5c): using Swin-B\nover ResNet-50 leads to significant performance gains, sug-\ngesting that ODIN can directly benefit from advancements\nin 2D computer vision.\nFinetuning everything including the pre-trained param-\neters helps (Tab. 5c): while ResNet50\u2019s performance re-\nmains similar, Swin\u2019s performance increases substantially\nwhen we fine-tune all parameters. Intuitively, unfreezing\nthe backbone allows 2D layers to adapt to cross-view fused\nfeatures better. Thus, we keep our backbone unfrozen in all\nexperiments.\nSupplying 2D features directly to 3D models does not\nhelp: Concatenating 2D features with XYZ+RGB as input\nto Mask3D yields 53.8% mAP performance, comparable to\n53.3% of the baseline model with only XYZ+RGB as input.\n4.5. Additional Experiments\nWe show evaluations on the hidden test set of ScanNet and\nScanNet200 in\nSec. A.1, results and comparisons with\nbaselines on S3DIS [1] and MatterPort3D [2] datasets in\nSec. A.2 and performance gains in 2D perception with in-\ncreasing context views in Sec. A.4.\n4.6. Limitations\nOur experiments reveal the following limitations for ODIN:\nFirstly, like other top-performing 3D models, it depends on\naccurate depth and camera poses. Inaccurate depth or cam-\nera poses causes a sharp decrease in performance (similar to\nother 3D models, like Mask3D). As our experiments show,\nour model shines over variants in simulation, where cam-\nera poses and depths are accurate. In our future work, we\naim to explore unifying depth and camera pose estimation\nwith semantic scene parsing, thus making 3D models more\nresilient to noise. Secondly, in this paper, we limited our\nscope to exploring the design of a unified architecture with-\nout scaling-up 3D learning by training on diverse 2D and 3D\ndatasets jointly. We aim to explore this in future in order to\nachieve strong generalization to in-the-wild scenarios, akin\nto the current foundational 2D perception systems.\n\u2020We do not use the expensive DB-SCAN post-processing of Mask3D,\nand hence it gets 53.3% mAP instead of 55.2% as reported by their paper\n5. Conclusion\nWe presented ODIN, a model for 2D and 3D instance seg-\nmentation that can parse 2D images and 3D point clouds\nalike.\nODIN represents both 2D images and 3D feature\nclouds as a set of tokens that differ in their positional en-\ncodings which represent 2D pixel coordinates for 2D to-\nkens and 3D XYZ coordinates for 3D tokens. Our model\nalternates between within-image featurization and cross-\nview featurization. It achieves SOTA performance in Scan-\nNet200 and AI2THOR instance segmentation benchmarks,\noutperforms all methods operating on sensor point clouds\nand achieves competent performance to methods operating\nover mesh-sampled pointcloud. Our experiments show that\nODIN outperforms alternative models that simply augment\n3D point cloud models with 2D image features as well as\nablative versions of our model that do not alternate between\n2D and 3D information fusion, do not co-train across 2D\nand 3D and do no pre-train the 2D backbone.\n6. Acknowledgements\nThe authors express gratitude to Wen-Hsuan Chu, Mihir\nPrabhudesai, and Alexander Swerdlow for their valuable\nfeedback on the early draft of this work. Special thanks\nto Tsung-Wei Ke for insightful discussions throughout the\nproject. We thank the Microsoft Turing Team for providing\nus with GPU resources during the initial development phase\nof this project. This work is supported by Sony AI, DARPA\nMachine Common Sense, an Amazon faculty award, and an\nNSF CAREER award.\nAppendix A. Experiments\nA.1. Evaluations on ScanNet and ScanNet200 Hid-\nden Test Sets\nWe submit ODIN to official test benchmarks of ScanNet\n[6] and ScanNet200 [41]. Following prior works, we train\nODIN on a combination of train and validation scenes. Un-\nlike some approaches that employ additional tricks like DB-\nSCAN [44], ensembling models [27], additional special-\nized augmentations [51], additional pre-training on other\ndatasets [55], finer grid sizes [50] and multiple forward\npasses through points belonging to the same voxel, our\nmethod avoid any such bells and whistles.\nThe results are shown in Tab. 6. All conclusions from\nresults on the validation set of these datasets as discussed in\nthe main paper are applicable here. On the ScanNet bench-\nmark, ODIN achieves close to SOTA performance on se-\nmantic segmentation and mAP25 metric of Instance Seg-\nmentation while being significantly worse on mAP metric\ndue to misalignments between sensor and mesh sampled\npoint clouds.\nOn ScanNet200 benchmark, ODIN sets a\nnew SOTA on semantic segmentation and mAP50/mAP25\nmetric of Instance Segmentation, while achieving close to\nSOTA performance on mAP metric. Notably ODIN is the\nfirst method that operates over sensor RGB-D data for in-\nstance segmentation and achieves competitive performance\nto models operating over mesh-sampled point clouds.\nA.2. Evaluation on S3DIS and Matterport3D\nWe also benchmark ODIN on Matterport3D [2] and S3DIS\n[1] datasets.\nMatterport: Matterport3D comprises 90 building-scale\nscenes, further divided into individual rooms, with 1554\ntraining rooms and 234 validation rooms. The dataset pro-\nvides a mapping from each room to the camera IDs that cap-\ntured images for that room. After discarding 158 training\nrooms and 18 validation rooms without a valid camera map-\nping, we are left with 1396 training rooms and 158 valida-\ntion rooms. For instance segmentation results, we train the\nstate-of-the-art Mask3D [44] model on the same data (re-\nduced set after discarding invalid rooms). For semantic seg-\nmentation, we conduct training and testing on the reduced\nset, while baseline numbers are taken from the OpenScene\n[37] paper, trained and tested on the original data. Given the\nsmall size of the discarded data, we do not anticipate sig-\nnificant performance differences. The official benchmark\nof Matterport3D tests on 21 classes; however, OpenScene\nalso evaluates on 160 classes to compare with state-of-the-\nart models on long-tail distributions. We follow them and\nreport results in both settings.\nS3DIS: S3DIS comprises 6 building-scale scenes, typ-\nically divided into 5 for training and 1 for testing.\nThe\ndataset provides raw RGB-D images, captured panorama\nimages, and images rendered from the mesh obtained af-\nter reconstructing the original sensor data.\nUnlike Mat-\nterport3D, S3DIS do not provide undistorted raw images;\nthus, we use the provided rendered RGB-D images. Some\nrooms in S3DIS have misalignments between RGB-D im-\nages and point clouds, which we address by incorporating\nfixes from DeepViewAgg [40] and introducing our own ad-\njustments. Despite these fixes, certain scenes still exhibit\nsignificantly low overlap between RGB-D images and the\nprovided mesh-sampled point cloud. To mitigate this, we\nquery images from other rooms and verify their overlap\nwith the provided point cloud for a room. This partially\nhelps in addressing the low overlap issue.\nThe official S3DIS benchmark evaluates 13 classes. Due\nto the dataset\u2019s small size, some models pre-train on addi-\ntional datasets like ScanNet, as seen in SoftGroup [49], and\non Structured3D datasets [59], consisting of 21,835 rooms,\nas done by Swin3D-L [55]. Similar to Mask3D [44], we\nreport results in both settings of training from scratch and\nstarting from weights trained on ScanNet. We use the same\nbaselines as discussed in Sec. 4.1.\nLike ScanNet and ScanNet200, both S3DIS and Matter-\nTable 6. Evaluation on Test Set of Established 3D Benchmarks.\n(a) Comparison on ScanNet for Instance Segmentation Task.\nInput\nModel\nmAP\nmAP50\nmAP25\nSensor RGBD\nPoint Cloud\nODIN-Swin-B (Ours)\n47.7\n71.2\n86.2\nMesh Sampled\nPoint Cloud\nSoftGroup [49]\n50.4\n76.1\n86.5\nPBNet [58]\n57.3\n74.7\n82.5\nMask3D [44]\n56.6\n78.0\n87.0\nQueryFormer [34]\n58.3\n78.7\n87.4\nMAFT [28]\n59.6\n78.6\n86.0\n(b) Comparison on ScanNet for Semantic Segmentation Task.\nInput\nModel\nmIoU\nSensor RGBD\nPoint Cloud\nMVPNet [20]\n64.1\nBPNet [17]\n74.9\nDeepViewAgg [40]\n-\nODIN-Swin-B (Ours)\n74.0\nRendered RGBD\nPoint Cloud\nVMVF [26]\n74.6\nMesh Sampled\nPoint Cloud\nPoint Transformer v2 [51]\n75.2\nStratified Transformer [27]\n74.7\nOctFormer [50]\n76.6\nSwin3D-L [55]\n77.9\nZero-Shot\nOpenScene [37]\n-\n(c) Comparison on ScanNet200 for Instance Segmentation Task.\nModel\nmAP\nmAP50\nmAP25\nSensor RGBD\nPoint Cloud\nODIN-Swin-B (Ours)\n27.2\n39.4\n47.5\nMesh Sampled\nPoint Cloud\nMask3D [44]\n27.8\n38.8\n44.5\nQueryFormer [34]\n-\n-\n-\nMAFT [28]\n-\n-\n-\nZero-Shot\nOpenMask3D [47]\n-\n-\n-\n(d) Comparison on ScanNet200 for Semantic Segmentation Task.\nInput\nModel\nmIoU\nSensor RGBD\nPoint Cloud\nODIN-Swin-B (Ours)\n36.4\nMesh Sampled\nPoint Cloud\nLGround [41]\n27.2\nCeCo [60]\n34.0\nOctformer [50]\n32.6\nport3D undergo post-processing of collected RGB-D data\nto construct a mesh, from which a point cloud is sampled\nand labeled.\nConsequently, following the approach out-\nlined in Sec. 4.1, we train both Mask3D [44] and our model\nusing RGB-D sensor point cloud data and evaluate on the\nbenchmark-provided point cloud. Additionally, we explore\nmodel variants by training and testing them on the mesh-\nsampled point cloud for comparative analysis.\nThe results are shown in Tab. 7. We draw the follow-\ning conclusions:\nODIN outperforms SOTA 3D models\nwith sensor point cloud input and closely matches mod-\nels using mesh-sampled point clouds on Matterport3D\nInstance Segmentation Benchmark (Tab. 7a): and sur-\npassing them on the mAP25 metric.\nIn a long-tail dis-\ntribution setting of 160 classes, ODIN outperforms mesh-\nsampled point cloud models, particularly excelling in the\nmAP25 metric. These findings align with the results re-\nported in Sec. 4.1 for the ScanNet and ScanNet200 datasets.\nODIN sets a new state-of-the-art on Matterport3D Se-\nmantic Segmentation Benchmark (Tab. 7b): Our model\nachieves superior performance in both the 21 and 160 class\nsettings.\nIt also largely outperforms OpenScene [37] on\nboth settings.\nOpenScene is a zero-shot method while\nODIN is supervised in-domain, making this comparison un-\nfair. However, OpenScene notes that their zero-shot model\noutperforms fully-supervised models in 160 class setup as\ntheir model is robust to rare classes while the supervised\nmodels can severely suffer in segmenting long-tail. Con-\nceptFusion [21], another open-vocabulary 3D segmentation\nmodel, also draws a similar conclusion. With this result,\nwe point to a possibility of supervising in 3D while also\nbeing robust to long-tail by simply utilizing the strong 2D\npre-trained weight initialization.\nOn S3DIS Instance Segmentation Benchmark (Tab. 7c),\nin the setup where baseline Mask3D start from ScanNet pre-\ntrained checkpoint, our model outperforms them in the sen-\nsor point cloud setup but obtains lower performance com-\npared to mesh sampled point cloud methods and when com-\npared on the setup where all models train from scratch.\nOn S3DIS Semantic Segmentation Benchmark (Tab. 7d,\nODIN trained with ScanNet weight initialization outper-\nforms all sensor or rendered RGBD point cloud based meth-\nods, while achieving competitive performance on mesh\nsampled point cloud. When trained from scratch, it is much\nworse than other baselines. Given the limited dataset size\nof S3DIS with only 200 training scenes, we observe severe\noverfitting. As a consequence, ODIN with stronger back-\nbones, such as Swin-B, underperforms compared to ODIN\nwith ResNet50.\nTable 7. Evaluation on Matterport3D [2] and S3DIS [1] datasets.\n(a) Comparison on Matterport3D for Instance Segmentation Task.\n21\n160\nInput\nModel\nmAP\nmAP25\nmAP\nmAP25\nSensor RGBD\nPoint Cloud\nMask3D [44]\n7.2\n16.8\n2.5\n10.9\nODIN-ResNet50 (Ours)\n20.9\n54.2\n11.2\n27.2\nODIN-Swin-B (Ours)\n21.6\n60.3\n13.0\n33.1\nMesh Sampled\nPoint Cloud\nMask3D [44]\n22.9\n55.9\n11.3\n23.9\n(b) Comparison on Matterport3D for Semantic Segmentation Task.\n21\n160\nInput\nModel\nmIoU\nmAcc\nmIoU\nmAcc\nSensor RGBD\nPoint Cloud\nODIN-ResNet50 (Ours)\n54.6\n67.2\n22.0\n29.4\nODIN-Swin-B (Ours)\n58.4\n70.7\n28.2\n37.4\nMesh Sampled\nPoint Cloud\nTextureNet [18]\n-\n63.0\n-\n-\nDCM-Net [43]\n-\n67.2\n-\n-\nMinkowskiNet [5]\n54.2\n64.6\n-\n18.4\nZero-Shot\nOpenScene [37]\n42.6\n59.2\n-\n23.1\n(c) Comparison on S3DIS Area5 for Instance Segmentation Task. (\u2020 =\nuses additional data)\nModel\nmAP\nmAP50\nmAP25\nSensor RGBD\nPoint Cloud\nMask3D [44]\n40.7\n54.6\n64.2\nMask3D [44] \u2020\n41.3\n55.9\n66.1\nODIN-ResNet50 (Ours)\n37.2\n49.4\n62.1\nODIN-ResNet50 \u2020 (Ours)\n46.1\n61.2\n74.1\nODIN-Swin-B \u2020 (Ours)\n42.7\n58.6\n71.3\nMesh Sampled\nPoint Cloud\nSoftGroup [49] \u2020\n51.6\n66.1\n-\nMask3D [44]\n56.6\n68.4\n75.2\nMask3D [44] \u2020\n57.8\n71.9\n77.2\nQueryFormer [34]\n57.7\n69.9\n-\nMAFT [28]\n-\n69.1\n75.7\n(d) Comparison on S3DIS for Semantic Segmentation Task. (\u2020\n= uses additional data)\nInput\nModel\nmIoU\nSensor RGBD\nPoint Cloud\nMVPNet [20]\n62.4\nDeepViewAgg [40]\n67.2\nODIN-ResNet50 (Ours)\n59.6\nODIN-ResNet50 \u2020 (Ours)\n69.6\nODIN-Swin-B \u2020 (Ours)\n67.3\nRendered RGBD\nPoint Cloud\nVMVF [26]\n65.4\nMesh Sampled\nPoint Cloud\nPoint Transformer v2 [51]\n71.6\nStratified Transformer [27]\n72.0\nSwin3D-L [55] \u2020\n74.5\nA.3. ScanNet200 Detailed Results\nScanNet200 [41] categorizes its 200 object classes into\nthree groups\u2014Head, Common, and Tail\u2014each compris-\ning 66, 68, and 66 categories, respectively.\nIn Tab. 8,\nwe provide a detailed breakdown of the ScanNet200 re-\nsults across these splits. We observe that in comparison\nto SOTA Mask3D model trained on mesh-sampled point\ncloud, ODIN achieves lower performance on Head classes,\nwhile significantly better performance on Common and Tail\nclasses. This highlights the contribution of effectively uti-\nlizing 2D pre-trained features, particularly in detecting a\nlong tail of class distribution where limited 3D data is avail-\nable.\nA.4. Variation of Performance with Number of\nViews\nWe examine the influence of the number of views on seg-\nmentation performance using the AI2THOR dataset, specif-\nically focusing on the 2D mAP performance metric. The\nevaluation is conducted by varying the number of context\nimages surrounding a given query RGB image. Starting\nfrom a single-view without any context (N=0), we incre-\nment N to 5, 10, 20, 40, 60, and finally consider all images\nin the scene as context. ODIN takes these N + 1 RGB-\nD images as input, predicts per-pixel instance segmentation\nfor each image, and assesses the 2D mAP performance on\nmAP\n40\n47.5\n55\n62.5\n70\nNumber of Views\n1\n5\n11\n21\n41\n61\nAll\nFigure 4.\n2D mAP Performance Variation with increasing\nnumber of context views used\nthe query image. The results, depicted in Fig. 4, show a\ncontinuous increase in 2D mAP with the growing number\nof views. This observation underscores the advantage of\nutilizing multiview RGB-D images over single-view RGB\nTable 8. Detailed ScanNet200 results for Instance Segmentation\nInput\nModel\nAll\nHead\nCommon\nTail\nmAP\nmAP50\nmAP25\nmAP\nmAP50\nmAP25\nmAP\nmAP50\nmAP25\nmAP\nmAP50\nmAP25\nSensor RGBD point cloud\nMask3D \u2020 [44]\n15.5\n21.4\n24.3\n21.9\n31.4\n37.1\n13.0\n17.2\n18.9\n7.9\n10.3\n11.5\nODIN-ResNet50 (Ours)\n26.0\n37.6\n43.8\n34.5\n50.9\n61.6\n23.8\n34.5\n38.8\n18.4\n25.5\n28.7\nODIN-Swin-B (Ours)\n30.0\n43.0\n51.0\n35.2\n52.3\n65.2\n28.7\n39.5\n46.7\n25.3\n35.8\n39.5\nMesh Sampled point cloud\nMask3D [44]\n27.4\n37.0\n42.3\n40.3\n55.0\n62.2\n22.4\n30.6\n35.4\n18.2\n23.2\n27.0\nSegmentation Mask Decoder\nPoint \nFeatures\nText \nFeatures\nInstance  \nQueries\nMLP\nInstance  \nFeatures\n\u03c3\nInstance  \nHeatMap\nHungarian Matching\nClass \nProbabilities\nPoint \nFeatures\nText \nFeatures\nInstance  \nQueries\nCross Attention\nAdd & Norm\nMasked Cross Attention\nAdd & Norm\nSelf Attention\nAdd & Norm\nUpdated Queries\nQuery Refinement\nCross Attention\nK-NN\nFeature Maps \nV X H X W X F\nDepth Maps \nV X H X W\n3D Feature Cloud\nUnprojection\nQ\nKeys/ \nValues\nUpdated Feature Maps \nV X H X W X F\nReshape\nN X  F \n(\n)\nN = V \u22c5 H \u22c5 W\n1 X N X F\nK X N X F\n1 X N X F\nX L\n3D RelPos Attention\nFigure 5. Detailed ODIN Architecture Components: On the Left is the 3D RelPos Attention module which takes as input the depth,\ncamera parameters and feature maps from all views, lifts the features to 3D to get 3D tokens. Each 3D token serves as a query. The\nK-Nearest Neighbors of each 3D token become the corresponding keys and values. The 3D tokens attend to their neighbours for L layers\nand update themselves. Finally, the 3D tokens are mapped back to the 2D feature map by simply reshaping the 3D feature cloud to 2D\nmulti-view feature maps. On the Middle is the query refinement block where queries first attend to the text tokens, then to the visual tokens\nand finally undergo self-attention. The text features are optional and are only used in the open-vocabulary decoder setup. On the Right\nis the segmentation mask decoder head where the queries simply perform a dot-product with visual tokens to decode the segmentation\nheatmap, which can be thresholded to obtain the segmentation mask. In the Open-Vocabulary decoding setup, the queries also perform a\ndot-product with text tokens to decode a distribution over individual words. In a closed vocabulary decoding setup, queries simply pass\nthrough an MLP to predict a distribution over classes.\nimages whenever feasible.\nA.5. Inference Time\nWe assess the inference time of Mask3D and ODIN by aver-\naging the forward pass time of each model across the entire\nvalidation set, utilizing a 40 GB VRAM A100. When fed\nthe mesh-sampled point cloud directly, Mask3D achieves\nan inference time of 228ms. When provided with the sen-\nsor point cloud as input, the inference time increases to 864\nms. This could be attributed to the additional computational\noverhead involved in transferring features from the sensor\npoint cloud to the mesh-sampled point cloud before generat-\ning the output segmentation mask. Additionally, the sensor\npoint cloud is typically larger than the mesh-sampled point\ncloud, as it also includes regions that were discarded during\nthe mesh generation post-processing. ODIN-SwinB, which\noperates over the sensor point cloud, has an inference time\nof 960ms.\nAppendix B. Additional Implementation De-\ntails\nThe detailed components of our architecture and their de-\nscriptions are presented in Fig. 5.\nMore implementation details are presented below:\nAugmentations:\nFor RGB image augmentation, we im-\nplement the Large Scale Jittering Augmentation method\nfrom Mask2Former [4], resizing images to a scale be-\ntween 0.1 and 2.0. We adjust intrinsics accordingly post-\naugmentation and apply color jittering to RGB images.\nTraining involves a consecutive set of N images, typically\nset to 25. With a 50% probability, we randomly sample k\nimages from the range [1, N] instead of using all N images.\nAdditionally, instead of consistently sampling N consecu-\ntive images, we randomly skip k images in between, where\nk ranges from 1 to 4.\nFor 3D augmentations, we adopt the Mask3D [44] ap-\nproach, applying random 3D rotation, scaling, and jitter\nnoise to the unprojected XYZs. Elastic distortion and ran-\ndom flipping augmentations from Mask3D are omitted due\nto a slight drop in performance observed in our initial ex-\nperiments.\nImage Resolutions We use a resolution of 240 \u00d7 320 for\nScanNet, 480 \u00d7 640 for ScanNet200, and 512 \u00d7 512 for\nAI2Thor.\nIn our AI2THOR experiments, we discovered\nthat employing higher image resolutions enhances the de-\ntection of smaller objects, with no noticeable impact on the\ndetection of larger ScanNet-like objects. This observation\nwas confirmed in ScanNet, where we experimented with\n480 \u00d7 640 image resolutions and did not observe any dis-\ncernible benefit.\nInterpolation Throughout our model, interpolations are\nemployed in various instances, such as when upsampling\nthe feature map from 1/8th resolution to 1/4th. In cases\ninvolving depth, we unproject feature maps to 3D and per-\nform trilinear interpolation, as opposed to directly apply-\ning bilinear interpolation on the 2D feature maps. For up-\nsampling/downsampling the depth maps, we use the near-\nest interpolation. Trilinear interpolation proves crucial for\nobtaining accurate feature maps, particularly at 2D object\nboundaries like table and floor edges. This is because near-\nest depth interpolation may capture depth from either the\ntable or the floor. Utilizing trilinear upsampling of feature\nmaps ensures that if the upsampled depth is derived from the\nfloor, it interpolates features from floor points rather than ta-\nble points.\nUse of Segments:\nSome datasets, such as ScanNet and\nScanNet200, provide supervoxelization of the point cloud,\ncommonly referred to as segments. Rather than directly seg-\nmenting all input points, many 3D methods predict outputs\nover these segments. Specifically, Mask3D [44] featurizes\nthe input points and then conducts mean pooling over the\nfeatures of points belonging to a segment, resulting in one\nfeature per segment. Following prior work, we also lever-\nage segments in a similar manner. We observe that utilizing\nsegments is crucial for achieving good mAP performance,\nwhile it has no discernible impact on mAP25 performance.\nWe suspect that this phenomenon may arise from the anno-\ntation process of these datasets. Humans were tasked with\nlabelling segments rather than individual points, ensuring\nthat all points within a segment share the same label. Uti-\nlizing segments with our models guarantees that the entire\nsegment is labelled with the same class. It\u2019s worth noting\nthat in AI2THOR, our method and the baselines do not uti-\nlize these segments, as they are not available.\nPost-hoc output transfer vs feature transfer:\nODIN\ntakes the sensor point cloud as input and generates seg-\nmentation output on the benchmark-provided point cloud.\nIn this process, we featurize the sensor point cloud and\ntransfer these features from the sensor point cloud to the\nbenchmark-provided point cloud. Subsequently, we predict\nsegmentation outputs on this benchmark-provided feature\ncloud and supervise the model with the labels provided in\nthe dataset. An alternative approach involves segmenting\nand supervising the sensor RGB-D point cloud and later\ntransferring the segmentation output to the benchmark point\ncloud for evaluation. We experimented with both strate-\ngies and found them to yield similar results. However, as\nmany datasets provide segmentation outputs only on the\npoint cloud, transferring labels to RGB-D images for the\nlatter strategy requires careful consideration. This is due to\nthe sparser nature of the provided point cloud compared to\nthe RGB-D sensor point cloud, and factors such as depth\nnoise and misalignments can contribute to low-quality label\ntransfer. Consequently, we opt for the former strategy in all\nour experiments.\nDepth Hole-Infilling:\nThe sensor-collected depth\nmaps usually have holes around object boundaries and\nshiny/transparent surfaces.\nWe perform simple OpenCV\ndepth inpainting to fill these holes.\nWe tried using\nneural-based depth completion methods and NERF depth-\ninpainting but did not observe significant benefits.\nAI2THOR Data Collection: AI2THOR [25] is an embod-\nied simulator where an agent can navigate within a house,\nexecute actions, and capture RGB-D images of the scene.\nWe load the structurally generated houses from ProcTHOR\n[7] into the AI2THOR simulator, and place an agent ran-\ndomly at a navigable point provided by the simulator. The\nagent performs a single random rotation around its initial\nlocation and captures an RGB-D frame. This process is re-\npeated, with the agent spawning at another random location,\nuntil either all navigable points are exhausted or a maxi-\nmum of N = 120 frames is collected. While ProcTHOR\noffers 10,000 scenes, we randomly select only 1,500 scenes\nto match the size of ScanNet. Additionally, we retain scenes\nwith fewer than 100 objects, as our model utilizes a maxi-\nmum of 100 object queries.\nAppendix C. Qualitative Results\nFig. 6 shows qualitative visualizations of ODIN for various\n3D and 2D datasets.\nScanNet\nMatterport3D\nS3DIS\nScanNet200\nAI2THOR\nCOCO\nFigure 6. Qualitative Results on various 3D and 2D datasets\nReferences\n[1] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105, 2017. 2, 8, 9, 11\n[2] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 2, 8, 9, 11\n[3] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and\nXinggang Wang. Hierarchical aggregation for 3d instance\nsegmentation. 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 15447\u201315456, 2021. 2\n[4] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. 2022. 1, 2,\n3, 4, 5, 7, 12\n[5] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d\nspatio-temporal convnets: Minkowski convolutional neural\nnetworks. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 3075\u20133084,\n2019. 2, 11\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5828\u20135839, 2017. 1, 2, 3, 5, 9\n[7] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nKiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi.\nProcthor:\nLarge-scale embodied ai using procedural generation. Ad-\nvances in Neural Information Processing Systems, 35:5982\u2013\n5994, 2022. 2, 7, 13\n[8] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,\nSong Bai, and Xiaojuan Qi.\nPla: Language-driven open-\nvocabulary 3d scene understanding.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7010\u20137019, 2023. 1\n[9] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and\nYichen Wei. Solq: Segmenting objects by learning queries.\nAdvances in Neural Information Processing Systems, 34:\n21898\u201321909, 2021. 7\n[10] Shichao Dong, Fayao Liu, and Guosheng Lin.\nLever-\naging large-scale pretrained vision foundation models for\nlabel-efficient 3d point cloud segmentation. arXiv preprint\narXiv:2311.01989, 2023. 3\n[11] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent,\nYoshua Bengio, and Xavier Bresson.\nGraph neural net-\nworks with learnable structural and positional representa-\ntions. arXiv preprint arXiv:2110.07875, 2021. 4\n[12] Kyle Genova, Xiaoqi Yin, Abhijit Kundu, Caroline Panto-\nfaru, Forrester Cole, Avneesh Sud, Brian Brewington, Brian\nShucker, and Thomas Funkhouser.\nLearning 3d semantic\nsegmentation with only 2d image supervision. In 2021 In-\nternational Conference on 3D Vision (3DV), pages 361\u2013372.\nIEEE, 2021. 3\n[13] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der\nMaaten, Armand Joulin, and Ishan Misra. Omnivore: A sin-\ngle model for many visual modalities.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16102\u201316112, 2022. 3\n[14] Huy Ha and Shuran Song.\nSemantic abstraction: Open-\nworld 3d scene understanding from 2d vision-language mod-\nels. In 6th Annual Conference on Robot Learning, 2022. 1,\n3\n[15] Lei Han, Tian Zheng, Lan Xu, and Lu Fang.\nOccuseg:\nOccupancy-aware 3d instance segmentation. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 2940\u20132949, 2020. 2\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2, 3\n[17] Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and\nTien-Tsin Wong. Bidirectional projection network for cross\ndimension scene understanding.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14373\u201314382, 2021. 3, 6, 10\n[18] Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser,\nMatthias Nie\u00dfner, and Leonidas J Guibas.\nTexturenet:\nConsistent local parametrizations for learning from high-\nresolution signals on meshes.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4440\u20134449, 2019. 11\n[19] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Kate-\nrina Fragkiadaki. Bottom up top down detection transform-\ners for language grounding in images and point clouds. In\nEuropean Conference on Computer Vision, pages 417\u2013433.\nSpringer, 2022. 5\n[20] Maximilian Jaritz, Jiayuan Gu, and Hao Su.\nMulti-view\npointnet for 3d scene understanding.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\nWorkshops, pages 0\u20130, 2019. 3, 6, 10, 11\n[21] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,\nQiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh\nIyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al.\nConceptfusion: Open-set multimodal 3d mapping.\narXiv\npreprint arXiv:2302.07241, 2023. 1, 3, 8, 10\n[22] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-\nWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping\nfor 3d instance segmentation. 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n4866\u20134875, 2020. 2\n[23] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo\nKanazawa, and Matthew Tancik. Lerf: Language embedded\nradiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 19729\u201319739,\n2023. 1, 3\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 3\n[25] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,\nLuca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,\nDaniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv:1712.05474,\n2017. 2, 7, 13\n[26] Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian\nBrewington, Thomas Funkhouser, and Caroline Pantofaru.\nVirtual multi-view fusion for 3d semantic segmentation. In\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIV\n16, pages 518\u2013535. Springer, 2020. 3, 6, 10, 11\n[27] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang\nZhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-\nformer for 3d point cloud segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8500\u20138509, 2022. 6, 9, 10, 11\n[28] Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu,\nand Jiaya Jia.\nMask-attention-free transformer for 3d in-\nstance segmentation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 3693\u20133703,\n2023. 1, 2, 5, 6, 10, 11\n[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 5\n[30] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and\nKui Jia. Instance segmentation in 3d scenes using seman-\ntic superpoint tree networks. 2021 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 2763\u20132772,\n2021. 2\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 5\n[32] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019. 5\n[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 2, 3\n[34] Jiahao Lu, Jiacheng Deng, Chuxin Wang, Jianfeng He, and\nTianzhu Zhang.\nQuery refinement transformer for 3d in-\nstance segmentation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 18516\u2013\n18526, 2023. 2, 5, 6, 10, 11\n[35] So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar\nRavikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film:\nFollowing instructions in language with modular methods.\nIn International Conference on Learning Representations,\n2021. 7\n[36] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivas-\ntava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella,\nRobinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur.\nTeach: Task-driven embodied agents that chat. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, pages\n2017\u20132025, 2022. 2, 7\n[37] Songyou\nPeng,\nKyle\nGenova,\nChiyu\nJiang,\nAndrea\nTagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.\nOpenscene: 3d scene understanding with open vocabularies.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 815\u2013824, 2023. 1, 3,\n6, 8, 9, 10, 11\n[38] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, and\nBernard Ghanem. Pix4point: Image pretrained transform-\ners for 3d point cloud understanding. 2022. 3\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[40] Damien Robert, Bruno Vallet, and Loic Landrieu. Learn-\ning multi-view aggregation in the wild for large-scale 3d se-\nmantic segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5575\u20135584, 2022. 1, 3, 6, 7, 9, 10, 11\n[41] David Rozenberszki, Or Litany, and Angela Dai. Language-\ngrounded indoor 3d semantic segmentation in the wild. In\nEuropean Conference on Computer Vision, pages 125\u2013141.\nSpringer, 2022. 1, 2, 5, 6, 9, 10, 11\n[42] Gabriel Sarch, Yue Wu, Michael J Tarr, and Katerina\nFragkiadaki. Open-ended instructable embodied agents with\nmemory-augmented large language models. arXiv preprint\narXiv:2310.15127, 2023. 2, 7\n[43] Jonas Schult, Francis Engelmann, Theodora Kontogianni,\nand Bastian Leibe. Dualconvmesh-net: Joint geodesic and\neuclidean convolutions on 3d meshes.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8612\u20138622, 2020. 11\n[44] Jonas Schult, Francis Engelmann, Alexander Hermans, Or\nLitany, Siyu Tang, and Bastian Leibe. Mask3d: Mask trans-\nformer for 3d semantic instance segmentation.\nIn 2023\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 8216\u20138223. IEEE, 2023. 1, 2, 5, 6, 7, 9, 10,\n11, 12, 13\n[45] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan\nBisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer,\nand Dieter Fox.\nAlfred:\nA benchmark for interpreting\ngrounded instructions for everyday tasks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10740\u201310749, 2020. 7\n[46] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul`o, Nor-\nman M\u00a8uller, Matthias Nie\u00dfner, Angela Dai, and Peter\nKontschieder. Panoptic lifting for 3d scene understanding\nwith neural fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9043\u20139052, 2023. 1, 3\n[47] Ayc\u00b8a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc\nPollefeys, Federico Tombari, and Francis Engelmann. Open-\nmask3d: Open-vocabulary 3d instance segmentation. arXiv\npreprint arXiv:2306.13631, 2023. 1, 3, 6, 10\n[48] Nikolaos Tsagkas, Oisin Mac Aodha, and Chris Xiaoxuan\nLu. Vl-fields: Towards language-grounded neural implicit\nspatial representations.\narXiv preprint arXiv:2305.12427,\n2023. 1, 3\n[49] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and\nChang D Yoo. Softgroup for 3d instance segmentation on\npoint clouds. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2708\u2013\n2717, 2022. 2, 6, 9, 10, 11\n[50] Peng-Shuai Wang.\nOctformer: Octree-based transformers\nfor 3d point clouds. arXiv preprint arXiv:2305.03045, 2023.\n6, 9, 10\n[51] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-\nshuang Zhao. Point transformer v2: Grouped vector atten-\ntion and partition-based pooling. Advances in Neural Infor-\nmation Processing Systems, 35:33330\u201333342, 2022. 4, 6, 9,\n10, 11\n[52] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,\nXiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt\nKeutzer, and Masayoshi Tomizuka. Image2point: 3d point-\ncloud understanding with 2d image pretrained models. arXiv\npreprint arXiv:2106.04180, 2021. 3\n[53] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 1\n[54] Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakr-\nishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah\nMaestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva,\net al. Habitat-matterport 3d semantics dataset. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4927\u20134936, 2023. 3\n[55] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu,\nHao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo.\nSwin3d: A pretrained transformer backbone for 3d indoor\nscene understanding.\narXiv preprint arXiv:2304.06906,\n2023. 6, 9, 10, 11\n[56] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng\nLi. Pointclip: Point cloud understanding by clip. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8552\u20138562, 2022. 3\n[57] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun.\nPoint transformer.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 16259\u201316268, 2021. 4\n[58] Weiguang Zhao, Yuyao Yan, Chaolong Yang, Jianan Ye, Xi\nYang, and Kaizhu Huang.\nDivide and conquer: 3d point\ncloud instance segmentation with point-wise binarization. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 562\u2013571, 2023. 2, 5, 6, 10\n[59] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao,\nand Zihan Zhou.\nStructured3d:\nA large photo-realistic\ndataset for structured 3d modeling.\nIn Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part IX 16, pages 519\u2013535.\nSpringer, 2020. 9\n[60] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xi-\naojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding im-\nbalanced semantic segmentation through neural collapse. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 19550\u201319560, 2023. 6,\n10\n[61] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu\nYuan, et al. Generalized decoding for pixel, image, and lan-\nguage. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 15116\u201315127,\n2023. 5\n"
  },
  {
    "title": "Learning the 3D Fauna of the Web",
    "link": "https://arxiv.org/pdf/2401.02400.pdf",
    "upvote": "8",
    "text": "Learning the 3D Fauna of the Web\nZizhang Li1*\nDor Litvak1,2*\nRuining Li3\nYunzhi Zhang1\nTomas Jakab3\nChristian Rupprecht3\nShangzhe Wu1\u2020\nAndrea Vedaldi3\u2020\nJiajun Wu1\u2020\n1Stanford University\n2UT Austin\n3University of Oxford\nkyleleey.github.io/3DFauna/\nFigure 1. Learning Diverse 3D Animals from the Internet. Our method, 3D-Fauna, learns a pan-category deformable 3D model of more\nthan 100 different animal species using only 2D Internet images as training data. At test time, the model can turn a single image of an\nquadruped instance into an articulated, textured 3D mesh in a feed-forward manner, ready for animation and rendering.\nAbstract\nLearning 3D models of all animals on the Earth re-\nquires massively scaling up existing solutions. With this\nultimate goal in mind, we develop 3D-Fauna, an approach\nthat learns a pan-category deformable 3D animal model for\nmore than 100 animal species jointly. One crucial bottle-\nneck of modeling animals is the limited availability of train-\ning data, which we overcome by simply learning from 2D\nInternet images. We show that prior category-specific at-\ntempts fail to generalize to rare species with limited train-\ning images. We address this challenge by introducing the\nSemantic Bank of Skinned Models (SBSM), which automat-\nically discovers a small set of base animal shapes by com-\nbining geometric inductive priors with semantic knowledge\nimplicitly captured by an off-the-shelf self-supervised fea-\nture extractor. To train such a model, we also contribute a\nnew large-scale dataset of diverse animal species. At infer-\nence time, given a single image of any quadruped animal,\nour model reconstructs an articulated 3D mesh in a feed-\nforward fashion within seconds.\n1. Introduction\nComputer vision models can nowadays reconstruct humans\nin monocular images and videos robustly and accurately,\n*Equal contribution\n\u2020Equal advising\nrecovering their 3D shape, articulated pose, and even ap-\npearance [2, 10, 11, 13, 20, 34]. However, humans are but\na tiny fraction of the natural world, and 3D models remain\nessentially blind to the vast majority of biodiversity.\nWhile in principle the same approaches that work for hu-\nmans could work for many other animal species, in practice\nscaling it to each of the 2.1 million different animal species\non the Earth is nearly hopeless. In fact, building a human\nmodel such as SMPL [34] and a corresponding pose pre-\ndictor [2, 13] requires collecting 3D scans of many people\nin laboratory [20], crafting a corresponding articulated de-\nformable model semi-automatically, and collecting exten-\nsive manual labels to train corresponding pose regressors.\nOf all animals, only humans are currently of sufficient im-\nportance in applications to justify the costs.\nA technically harder but much more practical approach\nis to learn animal models automatically from images and\nvideos readily available on the Internet. Several authors\nhave demonstrated that at least rough models can be learned\nfrom such uncontrolled image collections [21, 62, 72]. Even\nso, many limitations remain, starting from the fact that these\nmethods can only reconstruct one or a few specific animal\nexemplars [72], or at most a single class of animals at a\ngiven time [21, 62]. The latter restriction is particularly\nglaring, as it defeats the purpose of using the Internet as\na vast data source for modeling biodiversity.\nWe introduce 3D-Fauna, a method that learns a pan-\n1\narXiv:2401.02400v1  [cs.CV]  4 Jan 2024\ncategory deformable model for a large number (> 100) of\ndifferent quadruped animal species, such as dogs, antelopes,\nand hedgehogs, as shown in Fig. 1. For the approach to be as\nautomated and thus as scalable as possible, we assume that\nonly Internet images of the animals are provided as train-\ning data and only consider as prerequisites a pre-trained 2D\nobject segmentation model and off-the-shelf unsupervised\nvisual features. 3D-Fauna is designed as a feed-forward\nnetwork that deforms and poses the deformable model to\nreconstruct any animal given a single image as input. The\nability to perform monocular reconstruction is necessary for\ntraining on (single-view) Internet images, and is also useful\nin many real-world applications.\nCrucial to 3D-Fauna is to learn a single joint model of\nall animals in one go. Despite posing a challenge, model-\ning many animals jointly is essential for reconstructing rarer\nspecies, for which we often have only a small number of im-\nages to train on. This allows us to exploit the structural sim-\nilarity of different animals that results from evolution, and\nmaximize statistical efficiency. Here, we focus our atten-\ntion on animals that share a given body plan, in particular,\nquadrupeds, and share the structure of the underlying skele-\ntal model, which would otherwise be difficult to pin down.\nLearning such a model from only unlabeled single-view\nimages requires several technical innovations.\nThe most\nimportant is to develop a 3D representation that is suf-\nficiently expressive to model the diverse shape variations\nof the animals, and at the same time tight enough to be\nlearned from single-view images without overfitting indi-\nvidual views. Prior work partly achieved this goal by us-\ning skinned models, which consider small shape variations\naround a base template followed by articulation [62]. We\nfound that this approach does not provide sufficient induc-\ntive biases to learn diverse animal species from Internet\nimages alone. Hence, we introduce the Semantic Bank of\nSkinned Models (SBSM), which uses off-the-shelf unsuper-\nvised features, such as DINO [4, 40], to hypothesize how\ndifferent animals may relate semantically, and automati-\ncally learns a low-dimensional base shape bank.\nLastly, Internet images, which are not captured with the\npurpose of 3D reconstruction in mind, are characterized by\na strong photographer bias, skewing the viewpoint distribu-\ntion to mostly frontal, which significantly hinders the sta-\nbility of 3D shape learning.\nTo mitigate this issue, 3D-\nFauna further encourages the predicted shapes to look re-\nalistic from all viewpoints, by introducing an efficient mask\ndiscriminator that enforces the silhouettes rendered from a\nrandom viewpoint to stay within the distribution of the sil-\nhouettes of the real images.\nCombining these ideas, 3D-Fauna is an end-to-end\nframework that learns a pan-category model of 3D\nquadruped animals from online image collections. To train\n3D-Fauna, we collected a large-scale animal dataset of over\n100 quadruped species, dubbed the Fauna Dataset, as part\nof the contribution. After training, the model can turn a\nsingle test image of any quadruped instance into a fully\narticulated 3D mesh in a feed-forward fashion, ready for\nanimation and rendering. Extensive quantitative and qual-\nitative comparisons demonstrate significant improvements\nover existing methods. Code and data will be released.\n2. Related Work\nOptimization-Based 3D Reconstruction of Animals.\nDue to the lack of explicit 3D data for the vast majority\nof animals, reconstruction has mostly relied on pre-defined\nshape models or multi-view images. Initially, efforts fo-\ncus on fitting a parametric 3D shape model obtained form\n3D scans, e.g., SMAL [78], to animal images using anno-\ntated 2D keypoints and segmentation masks, which is fur-\nther extended to multi-view images [79]. Other works aim\nto optimize the 3D shape [5, 57, 67\u201369, 72\u201374] directly\nfrom image or video collections of a smaller scale using\nvarious forms of supervision in addition to masks, such\nas keypoints [5, 57], self-supervised semantic correspon-\ndences [72\u201374], optical flow [66\u201369], surface normals [69],\ncategory-specific template shapes [5, 57].\nLearning 3D from Internet Images and Videos.\nRe-\ncently, authors have attempted to learn 3D priors from In-\nternet images and videos at a larger scale [1, 12, 19, 21,\n28, 29, 54, 59\u201362, 75], mostly focusing on a single cate-\ngory at a time. Reconstructing animals presents additional\nchallenges due to their highly deformable nature, which of-\nten necessitates stronger supervisory signals for training,\nsimilar to the ones used in optimization-based methods.\nSome methods have, in particular, learned to model ar-\nticulated animals, such as horses, from single-view image\ncollections without any 3D supervision, adopting a hierar-\nchical shape model that factorizes a category-specific prior\nshape from instance-specific shape deformation and articu-\nlation [19, 61, 62]. However, these models are trained in a\ncategory-specific manner and fail to generalize to less com-\nmon animal species as shown in Sec. 5.3.\nAttempts to model diverse animal species again resort to\npre-defined shape models, e.g., SMAL. Ruegg et al. [43, 44]\nmodel multiple dog breeds and regularize the learning pro-\ncess by encouraging intra-breed similarities using a triplet\nloss, which requires breed labels for training, in addition\nto keypoint annotations and the template shape model. In\ncontrast, our approach reconstructs a significantly broader\nset of animals and is trained in a category-agnostic fashion,\nwithout relying on existing 3D shape models or keypoints.\nAnother related work [18] aims to learn a category-agnostic\n3D shape regressor by exploiting pre-trained CLIP features\nand an off-the-shelf normal estimator, but does not model\ndeformation and produces coarse shapes.\n2\n!\n\"!\nSemantic Base Shape Bank\n\u2295\n\u2248\n$!\n!!\n\"#$\n!!\n%&'\n\u2026\n\u2026\n\u21d2\n*\n\u21d2\n*\n\u21d2\n*\n\u21d2\n*\n\u21d2\nlearned memory bank\nLosses\nInstance-specific Appearance & Deformations\nIns.\nPred.\nrecon.\nGT\nrandom view\nBase\nShape\n\u21d2\n\u21d2\nDifferentiable\nRenderer\ninput view\nEnc\n\u2112()\n \n\u2248\nDis. \u21d2real /\nfake?\n!\"+%\nInternet Images\n\u2112,#&-\n \n\u2248\n\u2112)\n \n\u2248\nalbedo deformation\nposing\nshading\nFigure 2. Training Pipeline. 3D-Fauna is trained using only single-view images from the Internet. Given each input image, it first extracts\na feature vector \u03d5 using a pre-trained unsupervised image encoder [4]. This is then used to query a learned memory bank to produce a base\nshape and a DINO feature field in the canonical pose. The model also predicts the albedo, instance-specific deformation, articulated pose\nand lighting, and is trained via image reconstruction losses on RGB, DINO feature map and mask, as well as a mask discriminator loss.\nAnother line of research attempts to distill 3D recon-\nstructions from 2D generative models trained on large-\nscale datasets of Internet images, which can be GAN-\nbased [6, 7, 14, 38] or more recently, diffusion-based mod-\nels [8, 17, 35, 49] using Score Distillation Sampling [41]\nand its variants.\nThis idea has been extended to learn\nimage-conditional multi-view generator networks [25, 30\u2013\n33, 42, 46, 50, 51, 58, 65, 70]. However, most of these\nmethods optimize one single shape at a time, whereas our\nmodel learns a pan-category deformable model that can re-\nconstruct any animal instance in a feed-forward fashion.\nAnimal Datasets.\nLearning 3D models often requires\nhigh-quality images without blur or occlusion.\nExist-\ning high-quality datasets were only collected for a small\nnumber of categories [48, 56, 61, 68], and more diverse\ndatasets [37, 63, 64, 71] often contain many noisy im-\nages unsuitable for training off the shelf.\nTo train our\npan-category model for a wide range of quadruped animal\nspecies, we aggregate these existing datasets after substan-\ntial filtering, and additionally source more images from the\nInternet to create a large-scale object-centric image dataset\nspanning over 100 quadruped species, as detailed in Sec. 4.\n3. Method\nOur goal is to learn a deformable model of a large variety\nof different animals using only Internet images for supervi-\nsion. Formally, we learn a function f : I 7\u2192 O that maps\nany image I \u2208 R3\u00d7H\u00d7W of an animal to a corresponding\n3D reconstruction O, capturing the animal\u2019s shape, defor-\nmation and appearance.\n3D reconstruction is greatly facilitated by using multi-\nview data [16], but this is not available at scale, or at all,\nfor most animals. Instead, we wish to reconstruct animals\nfrom weak single-view supervision obtained from the Inter-\nnet. Compared to prior works [62, 72\u201374], which focused\non reconstructing a single animal type at a time, here we tar-\nget a large number of animal species at once, which is sig-\nnificantly more difficult. We show in the next section how\nsolving this problem requires carefully exploiting the se-\nmantic similarities and geometric correspondences between\ndifferent animals to regularize their 3D geometry.\n3.1. Semantic Bank of Skinned Models\nGiven an image I, consider the problem of estimating the\n3D shape (V, F) of the animal contained in it, where V \u2208\nRK\u00d73 is a list of vertices of a 3D mesh with face connec-\ntivity given by triplets F \u2282 {1, . . . , K}3. While recovering\na 3D shape from a single image is ill-posed, as we train the\nmodel f on a large dataset, we can ultimately observe ani-\nmals from a variety of viewpoints. However, different im-\nages show different animals with different 3D shapes. Non-\nRigid Structure-from-Motion [3, 52, 53] shows that recon-\nstruction is still possible, but only if one makes the space of\npossible 3D shapes sufficiently tight to remove the recon-\nstruction ambiguity. At the same time, the space must be\nsufficiently expressive to capture all animals.\nSkinned Models (SM).\nFollowing SMPL [34], many\nworks [19, 61, 62, 69] have adopted a Skinned Model (SM)\nto model the shape of deformable objects when learning\nfrom single-view image collections or videos. An SM starts\nfrom a base shape Vbase of the object (e.g., human or animal)\nat \u2018rest\u2019, applies as a small deformation Vins = fins(Vbase, \u03d5)\nto capture instance-specific details, and then applies a larger\ndeformation via a skinning function V\n= fpose(Vins, \u03d5),\ncontrolled by the articulation of the underlying skeleton.\nWe assume that deformations are predicted by neural net-\n3\nEnc\n0.42\nsimilarity\nSemantic Base Shape Bank\nEnc\nEnc\n0.06\nsimilarity\n\ud835\udf19\nFigure 3. Queries from the Semantic Base Shape Bank. With-\nout requiring any category labels, the Semantic Bank (Sec 3.1)\nautomatically learns diverse base shapes for various animals and\npreserves the semantic similarities across different instances.\nworks that receive as input image features \u03d5 = f\u03d5(I) ex-\ntracted from a powerful self-supervised image encoder.\nIn our case, a single SM is insufficient to capture the very\nlarge shape variations between different animals, which in-\nclude horses, dogs, antelopes, hedgehogs, etc. Na\u00a8\u0131vely at-\ntempting to capture this diversity using the network fins\nmeans that the resulting deformations cannot be small any\nlonger, which throws off the tightness of the model.\nSemantic Bank of Skinned Models.\nIn order to increase\nthe expressiveness of the model while still avoiding overfit-\nting individual images, we propose to exploit the fact that\ndifferent animals often have similar 3D shapes as a result of\nevolution. We can thus reduce the shape variation to a small\nnumber of shape bases Vbase, and interpolate between them.\nTo do so, we introduce a Semantic Bank of Skinned Mod-\nels that automatically discovers a set of latent shape bases\nand learns to project each image into a linear combination\nof these bases. Key to this method is to use pre-trained un-\nsupervised image features [4, 40] to automatically and im-\nplicitly identify similar animals. This is realized by means\nof a small memory bank with K learned key-value pairs\n{(\u03d5key\nk , \u03d5val\nk )}K\nk=1. Specifically, given an image embedding\n\u03d5, we query the memory bank to obtain a latent shape em-\nbedding \u02dc\u03d5 as a linear combination of the value tokens {\u03d5val\nk }\nvia a mechanism similar to attention [55]:\n\u02dc\u03d5 =\nK\nX\nk=1\nwk \u03d5val\nk , where wk =\ncossim(\u03d5, \u03d5key\nk )\nPK\nj=1 cossim(\u03d5, \u03d5key\nj )\n,\n(1)\nand cossim denotes cosine similarity between two feature\nvectors. This embedding \u02dc\u03d5 is then used as a condition to the\nbase shape predictor (Vbase, F) = fs(\u02dc\u03d5), which produces\nsemantically-adaptive base shapes without relying on any\ncategory labels or being bound to a hard categorization.\nIn practice, the image features \u03d5 are obtained from a\nwell-trained feature extractor like DINO-ViT [4, 40]. Defin-\ning the weights based on the cosine similarities between the\nimage features \u03d5 and a small number of bases {\u03d5key\nk } cap-\ntures the semantic similarities across different animal in-\nstances. For instance, as illustrated in Fig. 3, the cosine\nsimilarity between the image features of a zebra and a horse\nis 0.42, whereas the similarity between a zebra and an arctic\nfox is only 0.06. Ablations in Fig. 6 further verify the im-\nportance of this Semantic Bank, without which the model\neasily overfits each training image and fails to reconstruct\nplausible 3D shapes.\nImplementation Details.\nThe base shape is predicted us-\ning a hybrid SDF-mesh representation [45, 62] parameter-\nized by a coordinate MLP, with a conditioning vector \u02dc\u03d5 in-\njected via layer weight modulation [23, 24]. Since extract-\ning meshes from SDFs using DMTet [45] is memory and\ncompute intensive, in practice, we only compute it once for\neach iteration, by assuming the batched images contain the\nsame animal species, and simply averaging out the embed-\ndings \u02dc\u03d5. The instance-specific deformation is predicted us-\ning another coordinate MLP that outputs the displacement\n\u2206Vins,i = f\u2206V (Vbase,i, \u03d5) for each vertex Vbase,i of the base\nmesh conditioned on the image feature \u03d5, resulting in the\ndeformed shape Vins = \u2206Vins +Vbase. We enforce a bilateral\nsymmetry on both the base shape and the instance deforma-\ntion by mirroring the query locations for the MLPs. Given\nthe instance mesh Vins, we initialize a quadrupedal skele-\nton using a simple heuristic [62], and predict the rigid pose\n\u03be1 \u2208 SE(3) and bone rotations \u03beb \u2208 SO(3), b = 2, . . . , B\nusing a pose network. These posing parameters are then ap-\nplied to the instance mesh via a linear blend skinning equa-\ntion [34]. Refer to the sup. mat. for more details.\nAppearance.\nAssuming\na\nLambertian\nillumination\nmodel, we model the appearance of the object using an\nalbedo field a(x) = fa(x, \u03d5) \u2208 [0, 1]3 and a dominant\ndirectional light. The final shaded color of each pixel is\ncomputed as \u02c6I(u) = (ka + kd \u00b7 max{0, \u27e8l, n\u27e9}) \u00b7 a(x),\nwhere n is the normal direction of the posed mesh at pixel\nu, and ka, kd \u2208 [0, 1] and l \u2208 S2 are respectively the ambi-\nent intensity, diffuse intensity and dominant light direction\npredicted by the lighting network (ka, kd, l) = fl(\u03d5).\n3.2. Learning Formulation\nThe entire pipeline is trained in an unsupervised fashion,\nusing only self-supervised image features [4, 40] and object\nmasks obtained from off-the-shelf segmenters [26, 27].\nReconstruction Losses.\nGiven the final predicted posed\nshape V and appearance of the object, we use a differen-\ntiable renderer R to obtain an RGB image \u02c6I as well as a\nmask image \u02c6\nM, which are compared to the input image I\n4\nand the pseudo-ground-truth object mask M:\nLm = \u2225 \u02c6\nM \u2212 M\u22252\n2 + \u03bbdt\u2225 \u02c6\nM \u2299 dt(M)\u22251,\n(2)\nLim = \u2225 \u02dc\nM \u2299 (\u02c6I \u2212 I)\u22251,\n(3)\nwhere dt(\u00b7) is distance transform for more effective gradi-\nents [21, 60], \u2299 denotes the Hadamard product, \u03bbdt specifies\nthe balancing weight, and \u02dc\nM = \u02c6\nM \u2299 M is the intersection\nof the predicted and ground-truth masks.\nCorrespondences from Self-Supervised Features.\nSelf-\nsupervised feature extractors are notoriously good at estab-\nlishing semantic correspondences between objects, which\ncan be distilled to facilitate 3D reconstruction [62]. To do\nso, we extract a patch-based feature map \u03a6 \u2208 RD\u00d7H\u00d7W\nfrom each training image.\nThese raw feature maps can\nbe noisy and may preserve image-specific information ir-\nrelevant to other images. To distill more effective seman-\ntic correspondences across different images, we perform\na Principal Component Analysis (PCA) across all feature\nmaps [62], reducing the dimension to D\u2032 = 16. We then\ntask the model to also learn a feature field in the canon-\nical frame \u03c8(x, \u02dc\u03d5) \u2208 RD\u2032 that is rendered into a fea-\nture image \u02c6\u03a6 given predicted posed shape using the same\nrenderer R.\nTraining then encourages the rendered fea-\nture images \u02c6\u03a6 to match the pre-extracted PCA features \u03a6\u2032:\nLfeat = \u2225 \u02dc\nM \u2299 (\u02c6\u03a6 \u2212 \u03a6\u2032)\u22252\n2. Note that although the space\nof the PCA features \u03a6\u2032 is shared across different animal in-\nstances, the feature field \u03c8 still receives the latent embed-\nding \u02dc\u03d5 as a condition. This is because different animals vary\nin shape, resulting in different feature fields.\nMask Discriminator.\nIn practice, despite exploiting these\nsemantic correspondences, we still find that the viewpoint\nprediction may easily collapse to only frontal viewpoints,\ndue to the heavy photographer bias in Internet photos. This\ncan lead to overly elongated shapes as shown in Fig. 6, and\nfurther deteriorates the viewpoint predictions. To mitigate\nthis, we further encourage the shape to look realistic from\narbitrary viewpoints. Specifically, we introduce a mask dis-\ncriminator D that encourages the mask images \u02c6\nMrv ren-\ndered from a random viewpoint to stay within the distribu-\ntion of the ground-truth masks M. The discriminator also\nreceives the base embedding \u02dc\u03d5 (with gradients detached)\nas a condition to make this adversarial guidance tailored to\nspecific types of animals and thus more effective. Formally,\nthis is achieved via an adversarial loss [14]:\nLadv = EM\u223cM[log D(M; \u02dc\u03d5)]\n+ E \u02c6\nMrv\u223cMrv[log(1 \u2212 D( \u02c6\nMrv; \u02dc\u03d5))].\n(4)\nNote that we do not use a discriminator on the rendered\nRGB images, as the predicted texture is often much less re-\nalistic when compared to real images, which gives the dis-\ncriminator a trivial task. Moreover, the distribution of mask\nimages is less susceptible to viewpoint bias than RGB im-\nages, and hence we can simply sample random viewpoints\nuniformly, without requiring a precise viewpoint distribu-\ntion of the training images.\nOverall Loss.\nWe further enforce the Eikonal constraint\nREik on the SDF network as well as the viewpoint hypothe-\nsis loss Lhyp and the magnitude regularizers Rdef on vertex\ndeformations and Rart on articulation parameters \u03be. See the\nsupplementary materials for details.\nThe final training objective L is thus\nL = Lrec + \u03bbhypLhyp + \u03bbadvLadv + R,\n(5)\nwhere Lrec = \u03bbmLm + \u03bbimLim + \u03bbfeatLfeat summarizes the\nthree reconstruction losses, R = \u03bbEikREik + \u03bbartRart +\n\u03bbdefRdef summarizes the regularizers, and \u03bb\u2019s balance the\ncontribution of each term.\nTraining Schedule.\nWe design a robust training schedule\nthat comprises three stages. First, we train the base shapes\nand the viewpoint network without articulation or deforma-\ntion. This significantly improves the stability of the training\nand allows the model to roughly register the rigid pose of\nall instances and learn the coarse base shapes.\nAs the viewpoint prediction stabilizes after 20k itera-\ntions, in the second stage, we instantiate the bones and en-\nable the articulation, allowing the shapes to gradually grow\nlegs and fit the articulated pose in each image. Meanwhile,\nwe also turn on the mask discriminator to prevent view-\npoint collapse and shape elongation. In the final stage, we\noptimize the instance shape deformation field to allow the\nmodel to capture the fine-grained geometric details of indi-\nvidual instances, with the discriminator disabled, as it may\ncorrupt the shape if overused.\n4. Dataset Collection\nIn order to train this pan-category model for all types of\nquadruped animals, we create a new animal image dataset,\ndubbed the Fauna Dataset, spanning 128 quadruped\nspecies from dogs, antelopes to minks and platypuses, with\na total of 78,168 images. We first aggregate the training\nsets of existing animal image datasets, including Animals-\nwith-Attributes [63], APT-36K [71], Animal3D [64] and\nDOVE [61]. Many of these images are blurry or contain\nheavy occlusions, which will impact the stability of the\ntraining. We thus filter the images using automatic scripts\nfirst, followed by manual inspection. This results in 8,378\nimages covering approximately 70 animal species. To fur-\nther increase the size as well as the diversity of the dataset,\nwe additionally collect 69,790 images from the Internet, in-\ncluding 63,115 video frames and 2,358 images for 7 com-\nmon animals (bear, cow, elephant, giraffe, horse, sheep, ze-\nbra) as well as 4,317 images for another 51 less common\nspecies. We use off-the-shelf segmentation models [26, 27]\n5\nto detect and segment the instances in the images. Out of the\n121 few-shot categories, we hold out 5 as novel categories\nunused at training. For validation, we randomly select 5 im-\nages in each of the rest 116 few-shot categories, and 2,462\nimages for the 7 common species. To reduce the viewpoint\nbias in the few-shot categories, we manually identify a few\n(1\u201310) backward-facing instances in the training set and du-\nplicate them to match the size of the rest.\n5. Experiments\n5.1. Techincal Details\nWe base our architecture on MagicPony [62], adding the\nnew SBSM and mask discriminator. For the Semantic Bank,\nwe use K = 60 key-value pairs. The dimension of keys is\n384 (same as DINO-ViT) and the dimension of values is\n128. For the discriminator, we adopt the architecture from\nGIRAFFE [39]; the conditioning embedding \u02dc\u03d5 is concate-\nnated to the mask images along the channel dimension, sim-\nilar to CycleGAN [77]. We train the model for 800k itera-\ntions on a single NVIDIA A40 GPU, which takes roughly 5\ndays. As the texture network tends to struggle to predict de-\ntailed appearance in one go, partially due to limited capac-\nity, for all the visualizations, we follow [62] and fine-tune\n(only) the texture network for 50 iterations, which takes\n< 10 seconds. Refer to the sup. mat. for further details.\n5.2. Qualitative Results\nAfter training, 3D-Fauna takes in a single test image of any\nquadruped animal and produces an articulated and textured\n3D mesh in a feed-forward manner, as visualized in Fig. 4.\nThe model can reconstruct very different animals, such as\nantelopes, armadillos, and fishers, without requiring any\ncategory labels. All the input images in Fig. 4 have not been\nseen during training. In particular, the model also performs\nwell on held-out categories, e.g. the wolf in the third row.\n5.3. Comparisons with Prior Work\nBaselines.\nTo the best of our knowledge, ours is the\nfirst deformable model designed to handle 100+ quadruped\nspecies, learned purely from 2D Internet data. We carry out\nquantitative and qualitative comparisons to methods that are\nat least in principle applicable to this setting. The base-\nline is MagicPony [62], which however is category-specific\n(they first train on horses, and fine-tune on giraffes, cows\nand zebras). We also compare with two popular deformable\nmodels that can work in the wild, namely UMR [29] and\nA-CSM [28].\nHowever, they require weakly-supervised\npart segmentations and shape templates, respectively. Other\nworks, such as LASSIE [72] and its follow-ups [73, 74], op-\ntimize a deformable model on a small set of about 20 images\ncovering a single animal category at a time. More recently,\nimage-to-3D methods based on distilling 2D diffusion mod-\nPASCAL\nAPT-36K\nAnimal3D\n(KT-PCK@0.1)\n(PCK@0.1)\n(PCK@0.1)\nUMR [29]\n0.284\n-\n-\nA-CSM [28]\n0.329\n-\n-\nMagicPony [62]\n0.429\n0.756\n0.867\nOurs\n0.539\n0.841\n0.901\nTable 1. Quantitative Comparisons on PASCAL VOC [9], APT-\n36K [71] and Animal3D [64]. When compared to baselines in-\ncluding the competitive MagicPony [62], our method demon-\nstrates significantly improved performance on all datasets.\nels and/or large 3D datasets [31] have also demonstrated\nplausible 3D reconstructions of animals from a single im-\nage. In contrast, our model predicts an articulated mesh\nfrom a single image within seconds. Although it is difficult\nto establish a fair numerical comparison given these differ-\nent settings, in Sec. 5.3, we provide a side-by-side quali-\ntative comparison against baselines [31, 72, 73]. We use\nthe publicly released code [31, 62, 72, 73] and report num-\nbers [28, 29] included in MagicPony [62].\nQuantitative Comparisons.\nWe conduct quantitative\nevaluation across three different datasets, APT-36K [71],\nAnimal3D [64], and PASCAL VOC [9], which contain im-\nages of various animals with 2D keypoint annotations. Fol-\nlowing MagicPony [62], we first evaluate on horses in PAS-\nCAL VOC [9] using the widely used Keypoint Transfer\nmetric [21, 28, 29]. We use the same protocol as in A-\nCSM [28] and randomly sample 20k source-target image\npairs. For each source image, we project the visible vertices\nof the predicted mesh onto the image and map each anno-\ntated 2D keypoint to its nearest vertex. We then project that\nvertex to the target image and check if it lies within a small\ndistance (10% of image size) to the corresponding keypoint\nin the target image. We summarize the results using the\nPercentage of Correct Keypoints (KT-PCK@0.1) in Tab. 1.\nWe also evaluate on two more recent animal datasets,\nAPT-36K [71] and Animal3D [64], which provide keypoint\nannotations for more diverse species, including all four cat-\negories (horses, cows, giraffes, zebras) that MagicPony re-\nconstructs in a category-specific manner. Here, visibility\nfor each keypoint is not annotated, which is required for\nkeypoint transfer evaluation, we follow CMR [21] and op-\ntimize a linear mapping from mesh vertices to desired key-\npoints for each category. Specifically, each keypoint is esti-\nmated as a linear combination of mesh vertices, where the\nweights are shared and optimized across all instances in the\nevaluation set. We then simply report PCK@0.1 between\nthe predicted and annotated 2D keypoints in Tab. 1. Our\nmodel demonstrates significant improvement over existing\nmethods on all datasets. Performance breakdown for each\ncategory is provided in the sup. mat.\n6\nInput\nReconstruction\nOther Views\nArticulated\nFigure 4. Single Image 3D Reconstruction. Given a single image of any quadruped animal at test time, our model reconstructs an\narticulated and textured 3D mesh in a feed-forward manner without requiring category labels, which can be readily animated.\n7\nOurs\nMagicPony\nLASSIE\nHi-LASSIE\nZero-1-to-3\nInput Image\nInput View\nNovel View\nInput View\nNovel View\nInput View\nNovel View\nInput View\nNovel View\nInput View\nNovel View\nFigure 5. Qualitative Comparisons against MagicPony [62], LASSIE [72], Hi-LASSIE [73] and Zero-1-to-3 [31]. Compared to all\nbaselines, our method predicts more stable poses and higher-fidelity reconstructions. Note that our method is learning-based and predicts\n3D meshes in a feed-forward fashion (as opposed to [72, 73] that optimize on test images), which is orders of magnitude faster.\nFull Model\nCategory-\nconditioned\nw/o \u2112!\"#\n \nw/o Semantic \nBank\nInput View\nInput Image\nSide View\nInput View\nSide View\nInput Image\nFigure 6. Ablation Studies. Both the Semantic Bank and the mask\ndiscriminator improve the results as discussed in Sec. 5.4.\nQualitative Comparisons.\nFigure 5 compares 3D-Fauna\nqualitatively to several recent works [31, 62, 72, 73]. To\nestablish a fair comparison with MagicPony [62], for cate-\ngories demonstrated in their paper (e.g. horse), we simply\nrun inference using the released model. For each of the\nother categories, we use their public code to train a per-\ncategory model on our dataset from scratch (which con-\ntains less than 100 images for some rare categories). For\nLASSIE [72] and Hi-LASSIE [73], which optimize over a\nsmall set of images, we train their models on the test image\ntogether with additional 29 images randomly selected from\nthe training set of that category. Hi-LASSIE [73] is further\nfine-tuned on the test image after training. To compare with\nZero-1-to-3 [31], we use the implementation in threestu-\ndio [15] to first distill a NeRF [36] using Score Distillation\nSampling [41] given the masked test image, and then extract\na 3D mesh for fair comparison. Note that our model predicts\n3D meshes within seconds, whereas the optimization takes\nat least 10\u201320 mins for the other methods [31, 72, 73].\nAs shown in Fig. 5, MagicPony is sensitive to the size of\nthe training set. When trained on rare categories with fewer\n(< 100) images, such as the puma in Fig. 5, it fails to learn\nmeaningful shapes and produces severe artifacts. Despite\noptimizing on the test images, LASSIE and Hi-LASSIE\nproduce coarser reconstructions, partially due to the part-\nbased representation that struggles in capturing the detailed\ngeometry and articulation, as well as unstable viewpoint\nprediction. Zero-1-to-3, on the other hand, often fails to\ncorrectly reconstruct the legs, and does not explicitly model\nthe articulated pose. On the contrary, our method predicts\naccurate viewpoint and reconstructs fine-grained articulated\nshapes for all different animals, with only one single model.\n5.4. Ablation Study\nIn Fig. 6, we present ablation results on three key design\nchoices in our pipeline: SBSM, category-agnostic training,\nand mask discriminator. If we remove the SBSM and di-\nrectly condition the base shape network on each individual\nimage embedding \u03d5, the model tends to overfit each train-\ning views without learning meaningful canonical 3D shapes\nand pose. Alternatively, we can simply condition the base\nshape on an explicit (learned) category-specific embedding\nand train the model in a category-conditioned manner. This\nalso leads to sub-optimal reconstructions, in particular on\nrare categories with few training images. Lastly, training\nwithout the mask discriminator results in biased viewpoint\nprediction (towards frontal) and produces elongated shapes.\n6. Conclusions\nWe have presented 3D-Fauna, a deformable model for 100\nanimal categories learned using only Internet images. 3D-\nFauna can reconstruct any quadruped image by instantiat-\ning in seconds a posed version of the deformable model to\nmatch the input image. Despite capable of modeling diverse\nanimals, the current model is still limited to quadruped\nspecies that share a same skeletal structure. Furthermore,\nthe training images still need to be lightly curated. Never-\ntheless, 3D-Fauna still presents a significant leap compared\nto prior works and moves us closer to models that will be\nable to understand and reconstruct all animals in nature.\n8\nAcknowledgments\nWe are very grateful to Cristobal\nEyzaguirre, Kyle Sargent, and Yunhao Ge for their in-\nsightful discussions and Chen Geng for proofreading.\nThe work is in part supported by the Stanford Institute\nfor Human-Centered AI (HAI), NSF RI #2211258, ONR\nMURI N00014-22-1-2740, the Samsung Global Research\nOutreach (GRO) program, Amazon, Google, and EPSRC\nVisualAI EP/T028572/1.\nReferences\n[1] Kalyan Vasudev Alwala, Abhinav Gupta, and Shubham Tul-\nsiani. Pre-train, self-train, distill: A simple recipe for super-\nsizing 3d reconstruction. In CVPR, 2022. 2\n[2] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J. Black. Keep it SMPL:\nAutomatic estimation of 3D human pose and shape from a\nsingle image. In ECCV, 2016. 1\n[3] Christoph Bregler, Aaron Hertzmann, and Henning Bier-\nmann. Recovering non-rigid 3d shape from image streams.\nIn CVPR, 2000. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 2, 3, 4\n[5] Thomas J. Cashman and Andrew W. Fitzgibbon. What shape\nare dolphins? building 3d morphable models from 2d im-\nages. IEEE TPAMI, 2012. 2\n[6] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and\nGordon Wetzstein. pi-GAN: Periodic implicit generative ad-\nversarial networks for 3d-aware image synthesis. In CVPR,\n2021. 3\n[7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In CVPR, 2022. 3\n[8] Congyue Deng, Chiyu \u201dMax\u201d Jiang, Charles R. Qi, Xinchen\nYan, Yin Zhou, Leonidas Guibas, and Dragomir Anguelov.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors. In CVPR, 2023. 3\n[9] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\npher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes challenge: A retrospective. IJCV,\n2015. 6\n[10] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\nmatching of pictorial structures. In CVPR, 2000. 1\n[11] Martin A. Fischler and Robert A. Elschlager. The represen-\ntation and matching of pictorial structures. IEEE Trans. on\nComputers, 1973. 1\n[12] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.\nShape and viewpoints without keypoints. In ECCV, 2020.\n2\n[13] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,\nAngjoo Kanazawa, and Jitendra Malik. Humans in 4d: Re-\nconstructing and tracking humans with transformers.\nIn\nICCV, 2023. 1\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n3, 5\n[15] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 8\n[16] Richard Hartley and Andrew Zisserman. Multiple View Ge-\nometry in Computer Vision.\nCambridge University Press,\nISBN: 0521540518, second edition, 2004. 3\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 3\n[18] Zixuan Huang, Varun Jampani, Anh Thai, Yuanzhen Li, Ste-\nfan Stojanov, and James M Rehg. Shapeclipper: Scalable 3d\nshape learning from single-view images via geometric and\nclip-based consistency. In CVPR, 2023. 2\n[19] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-\nprecht, and Andrea Vedaldi. Farm3d: Learning articulated\n3d animals by distilling 2d diffusion. In 3DV, 2024. 2, 3\n[20] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,\nLin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain\nMatthews, et al.\nPanoptic studio: A massively multiview\nsystem for social interaction capture. IEEE TPAMI, 2019. 1\n[21] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections. In ECCV, 2018. 1, 2, 5, 6\n[22] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections. In ECCV, 2018. 12\n[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. In CVPR, 2020. 4, 15\n[24] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. NeurIPS, 2021. 4, 15\n[25] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong,\nKihyuk Sohn, and Jinwoo Shin.\nCollaborative score dis-\ntillation for consistent visual synthesis.\narXiv preprint\narXiv:2307.04787, 2023. 3\n[26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\nshick.\nPointrend: Image segmentation as rendering.\nIn\nCVPR, 2020. 4, 5, 16\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 4, 5, 16\n[28] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shub-\nham Tulsiani.\nArticulation-aware canonical surface map-\nping. In CVPR, 2020. 2, 6\n[29] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun\nJampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised\nsingle-view 3d reconstruction via semantic consistency. In\nECCV, 2020. 2, 6\n9\n[30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization.\nNeurIPS,\n2023. 3\n[31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In ICCV, 2023. 6, 8\n[32] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. SyncDreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023.\n[33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 3\n[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. SMPL: A skinned multi-\nperson linear model. ACM TOG, 2015. 1, 3, 4\n[35] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR, 2023. 3\n[36] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 8\n[37] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni,\nSi Yong Yeo, and Jun Liu. Animal kingdom: A large and\ndiverse dataset for animal behavior understanding. In CVPR,\n2022. 3\n[38] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. HoloGAN: Unsupervised\nlearning of 3d representations from natural images. In ICCV,\n2019. 3\n[39] Michael Niemeyer and Andreas Geiger.\nGIRAFFE: Rep-\nresenting scenes as compositional generative neural feature\nfields. In CVPR, 2021. 6, 15\n[40] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 2, 4, 14, 16\n[41] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. ICLR,\n2023. 3, 8\n[42] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 3\n[43] Nadine R\u00a8uegg, Silvia Zuffi, Konrad Schindler, and Michael J\nBlack. Barc: Learning to regress 3d dog shape from images\nby exploiting breed information. In CVPR, 2022. 2\n[44] Nadine R\u00a8uegg,\nShashank Tripathi,\nKonrad Schindler,\nMichael J Black, and Silvia Zuffi. Bite: Beyond priors for\nimproved three-d dog pose estimation. In CVPR, 2023. 2\n[45] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid representa-\ntion for high-resolution 3d shape synthesis. NeurIPS, 2021.\n4\n[46] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 3\n[47] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,\nMatthias Nie\u00dfner, and Angela Dai. Texturify: Generating\ntextures on 3d shape surfaces. In ECCV, 2022. 14\n[48] Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ig-\nnacio Rocco, Natalia Neverova, Andrea Vedaldi, and David\nNovotny. Common pets in 3d: Dynamic new-view synthesis\nof real-life deformable categories. In CVPR, 2023. 3\n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2021. 3\n[50] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023. 3\n[51] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3\n[52] Lorenzo Torresani, Aaron Hertzmann, and Christoph Bre-\ngler. Learning non-rigid 3d shape from 2d motion. NeurIPS,\n2004. 3\n[53] Edith Tretschk, Navami Kairanda, Mallikarjun BR, Rishabh\nDabral, Adam Kortylewski, Bernhard Egger, Marc Haber-\nmann,\nPascal Fua,\nChristian Theobalt,\nand Vladislav\nGolyanik. State of the art in dense monocular non-rigid 3d\nreconstruction. In Comput. Graph. Forum, pages 485\u2013520,\n2023. 3\n[54] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Im-\nplicit mesh reconstruction from unannotated image collec-\ntions. arXiv preprint arXiv:2007.08504, 2020. 2\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017. 4, 14\n[56] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie.\nThe Caltech-UCSD Birds-200-\n2011 Dataset. Technical Report CNS-TR-2011-001, Cali-\nfornia Institute of Technology, 2011. 3\n[57] Yufu Wang, Nikos Kolotouros, Kostas Daniilidis, and Marc\nBadger. Birds of a feather: Capturing avian shape models\nfrom images. In CVPR, 2021. 2\n[58] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong\nZhang, CL Chen, and Lei Zhang. Consistent123: Improve\nconsistency for one image to 3d object synthesis.\narXiv\npreprint arXiv:2310.08092, 2023. 3\n[59] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.\nUnsupervised learning of probably symmetric deformable 3d\nobjects from images in the wild. In CVPR, 2020. 2\n[60] Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely,\nRichard Tucker, and Angjoo Kanazawa. De-rendering the\nworld\u2019s revolutionary artefacts. In CVPR, 2021. 5\n10\n[61] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-\ndrea Vedaldi. DOVE: Learning deformable 3d objects by\nwatching videos. IJCV, 2023. 2, 3, 5\n[62] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-\nprecht, and Andrea Vedaldi. Magicpony: Learning articu-\nlated 3d animals in the wild. In CVPR, 2023. 1, 2, 3, 4, 5, 6,\n8, 12, 14, 15\n[63] Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and\nZeynep Akata. Zero-shot learning\u2014a comprehensive eval-\nuation of the good, the bad and the ugly. IEEE TPAMI, 2019.\n3, 5\n[64] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen,\nPengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao\nWang, et al. Animal3d: A comprehensive dataset of 3d ani-\nmal pose and shape. In ICCV, 2023. 3, 5, 6, 12, 13\n[65] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Wang Peng, Jihao\nLi, Zifan Shi, Kaylan Sunkavalli, Wetzstein Gordon, Zexiang\nXu, and Zhang Kai.\nDMV3D: Denoising multi-view dif-\nfusion using 3d large reconstruction model. arXiv preprint\narXiv:2311.09217, 2023. 3\n[66] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,\nForrester Cole, Huiwen Chang, Deva Ramanan, William T.\nFreeman, and Ce Liu.\nLASR: Learning articulated shape\nreconstruction from a monocular video. In CVPR, 2021. 2\n[67] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vla-\nsic, Forrester Cole, Ce Liu, and Deva Ramanan.\nViSER:\nVideo-specific surface embeddings for articulated 3d shape\nreconstruction. In NeurIPS, 2021. 2\n[68] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ra-\nmanan, Vedaldi Andrea, and Joo Hanbyul. BANMo: Build-\ning animatable 3d neural models from many casual videos.\nIn CVPR, 2022. 3\n[69] Gengshan Yang, Chaoyang Wang, N. Dinesh Reddy, and\nDeva Ramanan. Reconstructing animatable categories from\nvideos. In CVPR, 2023. 2, 3\n[70] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-\ndong Li. Consistnet: Enforcing 3d consistency for multi-\nview images diffusion.\narXiv preprint arXiv:2310.10343,\n2023. 3\n[71] Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long\nLan, and Dacheng Tao. Apt-36k: A large-scale benchmark\nfor animal pose estimation and tracking. NeurIPS, 2022. 3,\n5, 6, 12, 13\n[72] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-\nbinstein, Ming-Hsuan Yang, and Varun Jampani.\nLassie:\nLearning articulated shapes from sparse image ensemble via\n3d part discovery. NeurIPS, 2022. 1, 2, 3, 6, 8\n[73] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-\nbinstein, Ming-Hsuan Yang, and Varun Jampani. Hi-lassie:\nHigh-fidelity articulated shape and skeleton discovery from\nsparse image ensemble. In CVPR, 2023. 6, 8\n[74] Chun-Han Yao, Amit Raj, Wei-Chih Hung, Yuanzhen Li,\nMichael Rubinstein, Ming-Hsuan Yang, and Varun Jampani.\nArtic3d: Learning robust articulated 3d shapes from noisy\nweb image collections. NeurIPS, 2023. 2, 3, 6\n[75] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta.\nShelf-\nsupervised mesh prediction in the wild. In CVPR, 2021. 2\n[76] Yunzhi Zhang, Shangzhe Wu, Noah Snavely, and Jiajun Wu.\nSeeing a rose in five thousand ways. In CVPR, 2023. 15\n[77] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV, 2017. 6\n[78] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and\nMichael J Black. 3d menagerie: Modeling the 3d shape and\npose of animals. In CVPR, 2017. 2\n[79] Silvia Zuffi, Angjoo Kanazawa, and Michael J Black. Li-\nons and tigers and bears: Capturing non-rigid, 3d, articulated\nshape from images. In CVPR, 2018. 2\n11\nA. Additional Results\nWe provide additional visualizations, including shape inter-\npolation and generation, as well as additional comparisons\nin this supplementary material. Please refer to the website\nenclosed in the sup. mat. for 3D animations.\nA.1. Shape Interpolation between Instances\nWith the predictions of our model, we can easily interpolate\nbetween two reconstructions by interpolating the base em-\nbeddings \u02dc\u03d5, instance deformations and the articulated poses\n\u03be, as illustrated in Fig. 8. Here, we first obtain the pre-\ndicted base shape embeddings \u02dc\u03d5 for each of the three input\nimages from the learned Semantic Bank. We then linearly\ninterpolate between these embeddings to produce smooth a\ntransition from one base shape to another, as shown in the\nlast row of Fig. 8. Furthermore, we can also linearly inter-\npolate the predicted articulated the image features \u03d5 (which\nis used as a condition to the instance deformation field f\u2206V )\nas well as the predicted articulation parameters \u03be, to gener-\nate smooth interpolations of between posed shapes, shown\nin the middle row. These results confirm that our learned\nshape space is continuous and smooth, and covers a wide\nrange of animal shapes.\nA.2. Shape Generation from the Semantic Bank\nMoreover, we can also generate new animal shapes by sam-\npling from the learned Semantic Bank, as shown in Fig. 9.\nFirst, we visualize the base shapes captured by each of the\nlearned value tokens \u03d5val\nk in the Semantic Bank. In the top\ntwo rows of Fig. 9, we show 20 visualizations of these base\nshapes randomly selected out of the 60 value tokens in to-\ntal. We can also fuse these base shapes by linearly fusing\nthe value tokens \u03d5val\nk with a set of random weights (with a\nsum of 1), and generate the a wide variety of animal shapes,\nas shown in the bottom two rows.\nOurs\nMagicPony - Single Cat.\nInput Image\nInput View Novel View\nInput View Novel View\nInput View Novel View\nMagicPony - All Cat.\nInput View\nFigure 7. Qualitative Comparisons against two variants of Mag-\nicPony [62].\nIn the middle are reconstruction results of the\ncategory-specific MagicPony model trained on individual cate-\ngories. On the right are results of MagicPony trained on all cat-\negories jointly, i.e. assuming all quadrupeds belong to one single\ncategory.\nAPT-36K\nHorse\nGiraffe\nCow\nZebra\nMagicPony\n0.775\n0.699\n0.769\n0.778\nOurs\n0.853\n0.796\n0.876\n0.840\nAnimal3D\nHorse\nCow\nZebra\nMagicPony\n0.835\n0.895\n0.919\nOurs\n0.884\n0.903\n0.942\nTable 2. Quantitative Comparisons on APT-36K [71] and Ani-\nmal3D [64] for each category. Our method consistently performs\nbetter than MagicPony [62] on all the categories.\nA.3. Comparisons with Prior Work\nQuantitative Results for Each Category.\nHere, we pro-\nvide the per-category performance break for the quantitative\ncomparisons in Tab. 2, which correspond to the aggregated\nresults in Tab. 1 of the main paper. On APT36K [71], we\nevaluate on four categories including horse, giraffe, cow and\nzebra. On Animal3D [64], we use the available three cate-\ngories: horse, cow and zebra. Our pan-category model con-\nsistently outperforms the MagicPony [62] baseline across\nall the categories, which highlights the benefits of the joint\ntraining of all categories.\nMagicPony on All Categories.\nIn Fig. 5 of the main pa-\nper, we show that MagicPony [62] fail to produce plausible\n3D shapes when trained in a category-specific fashion on\nspecies with limited (< 100) number of images. Alterna-\ntively, we can also train the MagicPony on our entire image\ndataset of all the animal species, i.e. treating all the images\nas in one single category. The results are shown in Fig. 7.\nAs MagicPony maintains only one single base shape for all\nanimal instances, which is not able to capture the wide vari-\nation of shapes of different animal species. On the con-\ntrary, our proposed Semantic Base Shape Bank learns vari-\nous base shapes automatically adapted to different species,\nbased on self-supervised image features.\nA.4. Quantitative Ablation Studies\nIn addition to the qualitative comparisons in Fig. 6 of the pa-\nper, Tab. 3 shows the quantitative ablation studies on APT-\n36K [71] and Animal3D [64]. As explained in Sec. 5.3 of\nthe paper, we follow CMR [22] and optimize a linear map-\nping from our predicted vertices to the annotated keypoints\nin the input view. These numerical results are consistent\nwith the visual comparisons in Fig. 6 of the paper.\n12\nPosed Shape\nInterpolation\nInput Image\nReconstruction\nInput Image\nReconstruction\nInput Image\nReconstruction\nBase Shape\nInterpolation\nFigure 8. Shape Interpolation between Instances. On the top row, we show the 3D reconstructions from three input images. On the\nsecond and the third rows, we show the interpolation between the posed shapes and the base shapes.\nRandom 1 \ud835\udf19!\n\"#$\nRandomly\nfusing 10 \ud835\udf19!\n\"#$\nRandomly\nfusing all 60 \ud835\udf19!\n\"#$\nFigure 9. Shape Generation from the Learned Semantic Bank. On the top two rows, we visualize 20 base shapes generated from the\nindividual value tokens \u03d5val\nk in the learned Semantic Bank. On the bottom two rows, we show the base shapes obtained by randomly fusing\n10 and 60 value tokens \u03d5val\nk .\nAPT-36K\nHorse\nGiraffe\nCow\nZebra\nFinal Model\n0.853\n0.796\n0.876\n0.840\nw/o Semantic Bank\n0.402\n0.398\n0.371\n0.373\nCategory-conditioned\n0.822\n0.776\n0.832\n0.798\nw/o Ladv\n0.831\n0.782\n0.823\n0.828\nAnimal3D\nHorse\nCow\nZebra\nFinal Model\n0.884\n0.903\n0.942\nw/o Semantic Bank\n0.402\n0.701\n0.630\nCategory-conditioned\n0.842\n0.886\n0.910\nw/o Ladv\n0.813\n0.871\n0.873\nTable 3. Quantitative Ablation Studies on APT-36K [71] and\nAnimal3D [64] for each category.\nA.5. More Visualizations from 3D-Fauna\nWe show more visualization results of 3D-Fauna on a wide\nvariety of animals in Figure 12, Figure 13 and Figure 14,\nincluding horse, weasel, pika, koala and so on. Note that\nour model produces these articulated 3D reconstructions\nfrom just a single test image in feed-forward manner, with-\nout even knowing the category labels of the animal species.\nWith the articulated pose prediction, we can also easily ani-\nmate the reconstructions in 3D. More visualizations are in-\ncluded in the website.\nA.6. Failure Cases and Limitations\nDespite promising results on a wide variety of quadruped\nanimals, we still recognize a few limitations of the current\nmethod. First, we only focus on quadrupeds which share a\nsimilar skeletal structure. Although this covers a large num-\nber animals, including most mammals as well as many rep-\ntiles, amphibians and insects, the same assumption will not\n13\nInput Image Input View\nSide View\nInput Image Input View\nSide View\nFigure 10. Failure Cases. For fluffy and highly deformable ani-\nmals in challenging poses, our model still struggles in predicting\nthe accurate poses and shapes.\nhold for many other animals in nature. Jointly estimating\nthe skeletal structure and 3D shapes directly from raw im-\nages remains a fundamental challenge for modeling the en-\ntire biodiversity. Furthermore, for some fluffy animals that\nare highly deformable, like cats and squirrels, our model\nstill struggles to reconstruct accurate poses and 3D shapes,\nas shown in Fig. 10.\nAnother failure case is the confusion of left and right\nlegs, when reconstructing images taken from the side view,\nfor instance, in the second row of Fig. 12. Since neither the\nobject mask nor the self-supervised features [40] can pro-\nvide sufficient signals to disambiguate the legs, the model\nwould ultimately have to resort to the subtle appearance\ncues, which still remains as a major challenge.\nFinally,\nthe current model still struggles at inferring high-fidelity\nappearance in a feed-forward manner, similar to [62], and\nhence, we still employ a fast test-time optimization for bet-\nter appearance reconstruction (within seconds). This is par-\ntially due to the limited size of the dataset and the design of\nthe texture field. Leveraging powerful diffusion-based im-\nage generation models [47] could provide additional signals\nto train a more effective 3D appearance predictor, which we\nplan to look into for future work.\nB. Additional Technical Details\nB.1. Modeling Articulations\nIn this work, we focus on quadruped animals which share a\nsimilar quadrupedal skeleton. Here, we provide the details\nfor the bone instantiation on the rest-pose shape based on\na simple heuristic, the skinning model, and the additional\nbone rotation constraints.\nAdaptive Bone Topology.\nWe adopt a similar quadruped\nheuristic for rest-pose bone estimation as in [62]. How-\never, unlike [62] which focuses primarily on horses, our\nmethod needs to model a much more diverse set of ani-\nmal species. Hence, we make several modifications in order\nfor the model to adapt to different animals automatically.\nFor the \u2018spine\u2019, we still use a chain of 8 bones with equal\nlengths, connecting the center of the rest-pose mesh to the\ntwo most extreme vertices along z-axis. To locate the four\nfeet joints, we do not rely on the four xz-quadrants as the\nfeet may not always land separately in those four quadrants,\nfor instance, for animals with a longer body. Instead, we\nlocate the feet based on the distribution of the vertex loca-\ntions. Specifically, we first identify the vertices within the\nlower 40% of the total height (y-axis). We then use the cen-\nter of these vertices as the origin of the xz-plane and locate\nthe lowest vertex within each of the new quadrants as the\nfeet joints. For each leg, we create a chain of three bones of\nequally length connecting the foot joint to the nearest joint\nin the spine.\nBone Rotation Prediction.\nSimilar to [62], the viewpoint\nand bone rotations are predicted separately using differ-\nent networks. The viewpoint \u03be1 is predicted via a multi-\nhypothesis mechanism, as discussed in Appendix B.2. For\nthe bone rotations \u03be2:B, we first project the middle point\nof each rest-pose bone onto the image using the pre-\ndicted viewpoint, and sample its corresponding local fea-\nture from the feature map using bilinear interpolation. A\nTransformer-based [55] network then fuses the global im-\nage feature, local image feature, 2D and 3D joint locations\nas well as the bone index, and produces the Euler angle for\nthe rotation of each bone. Unlike [62], we empirically find it\nbeneficial to add the bone index on top of other features in-\nstead of concatenation, which tends to encourage the model\nto separate the legs with different rotation predictions.\nSkinning Weights.\nWith the estimated bone structure,\neach bone b except for the root has the parent bone \u03c0(b).\nEach vertex Vins,i on the shape Vins is then associated to all\nthe bones by skinning weights wib defined as:\nwib =\ne\u2212dib/\u03c4s\nPB\nk=1 e\u2212dik/\u03c4s ,\nwhere\ndib = min\nr\u2208[0,1] ||Vins,i \u2212 r\u02dcJb \u2212 (1 \u2212 r)\u02dcJ\u03c0(b)||2\n2\n(6)\nis the minimal distance from the vertex Vins,i to each bone\nb, defined by the rest-pose joint location \u02dcJb in world coordi-\nnates. The \u03c4s is a temperature parameter set to 0.5. We then\nuse the linear blend skinning equation to pose the vertices:\nVi(\u03be) =\n B\nX\nb=1\nwibGb(\u03be)Gb(\u03be\u2217)\u22121\n!\nVins,i,\nG1 = g1,\nGb = G\u03c0(b) \u25e6 gb,\ngb(\u03be) =\n\u0014R\u03beb\nJb\n0\n1\n\u0015\n,\n(7)\nwhere the \u03be\u2217 denotes the bone rotations at rest pose.\nBone Rotation Constraints.\nFollowing [62], we regular-\nize the magnitude of bone rotation predictions by Rart =\n1\nB\u22121\nPB\nb=2 ||\u03beb||2\n2. In experiments, we find a common fail-\nure mode where instead of learning a reasonable shape with\n14\nappropriate leg lengths, the model tends to predict exces-\nsively long legs for animals with shorter legs and bend them\naway from the camera. To avoid this, we further constrain\nthe range of the angle predictions. Specifically, we forbid\nthe rotation along y-axis (side-way) and z-axis (twist) of\nthe lower two segments for each leg. We also set a limit to\nthe rotation along y-axis and z-axis of the upper segment\nfor each leg as (\u221210\u25e6, 10\u25e6). For the body bones, we further\nlimit the rotation along the z-axis within (\u22126\u25e6, 6\u25e6).\nB.2. Viewpoint Learning Details\nRecovering the viewpoint of an object from only one in-\nput image is an ill-posed problem with numerous local op-\ntima in the reconstruction objective. Here, we adopt the\nmulti-hypothesis viewpoint prediction scheme introduced\nin [62]. In detail, our viewpoint prediction network out-\nputs four viewpoint rotation hypotheses Rk \u2208 SO(3), k \u2208\n{1, 2, 3, 4} within each of the four xz-quadrants together\nwith their corresponding scores \u03c3k. For computational effi-\nciency, we randomly sample one hypothesis at each training\niteration, and minimize the loss:\nLhyp(\u03c3k, Lrec,k) = (\u03c3k \u2212 detach(Lrec,k))2,\n(8)\nwhere detach indicates that the gradient on reconstruction\nloss is detached. In this way, \u03c3k essentially serves as an es-\ntimate of the expected reconstruction error for each hypoth-\nesis k, without actually evaluating it which would otherwise\nrequire the expensive rendering step. During inference time,\nwe can then take the softmax of its inverse to obtain the\nprobability pk of each hypothesis k: pk \u221d exp(\u2212\u03c3k/\u03c4),\nwhere the temperature parameter \u03c4 controls the sharpness\nof the distribution.\nB.3. Mask Discriminator Details\nTo sample another viewpoint and render the mask for the\nmask discriminator, we randomly sample an azimuth angle\nand rotate the predicted viewpoint by that angle. For con-\nditioning, the detached input base embedding \u02dc\u03d5 is concate-\nnated to each pixel in the mask along the channel dimen-\nsion. In practice, we also add a gradient penalty term in the\ndiscriminator loss following [39, 76].\nB.4. Network Architectures\nWe adopt the architectures in [62] except the newly intro-\nduced Semantic Base Shape Bank and mask discriminator.\nFor the SBSM, we add a modulation layer [23, 24] to each\nof the MLP layers to condition the SDF field on the base\nembeddings \u02dc\u03d5. To condition the DINO field, we simply\nconcatenate the embedding to the input coordinates to the\nnetwork. The mask discriminator architecture is identical\nto that of GIRAFFE [39], except that we set input dimen-\nsion as 129 = 1 + 128, accommodating the 1-channel mask\nParameter\nValue/Range\nOptimiser\nAdam\nLearning rate on prior and bank\n1 \u00d7 10\u22123\nLearning rate on others\n1 \u00d7 10\u22124\nNumber of iterations\n800k\nEnable articulation iteration\n20k\nEnable deformation iteration\n500k\nMask Discriminator iterations\n(80k, 300k)\nBatch size\n6\nLoss weight \u03bbm\n10\nLoss weight \u03bbim\n1\nLoss weight \u03bbfeat\n{10, 1}\nLoss weight \u03bbEik\n0.01\nLoss weight \u03bbdef\n10\nLoss weight \u03bbart\n0.2\nLoss weight \u03bbhyp\n{50, 500}\nLoss weight \u03bbadv\n0.1\nImage size\n256 \u00d7 256\nField of view (FOV)\n25\u25e6\nCamera location\n(0, 0, 10)\nTetrahedral grid size\n256\nInitial mesh centre\n(0, 0, 0)\nTranslation in x- and y-axes\n(\u22120.4, 0.4)\nTranslation in z-axis\n(\u22121.0, 1.0)\nNumber of spine bones\n8\nNumber of bones for each leg\n3\nViewpoint hypothesis temperature \u03c4\n(0.01, 1.0)\nSkinning weight temperature \u03c4s\n0.5\nAmbient light intensity ka\n(0.0, 1.0)\nDiffuse light intensity kd\n(0.5, 1.0)\nTable 4. Training details and hyper-parameter settings.\nand the 128-channel shape embedding. We set the size of\nthe memory bank K = 60. In practice, to allow bank to\nrepresent categories with diverse kinds of shapes, we only\nfuse the value tokens with top 10 cosine similarities.\nB.5. Hyper-Parameters and Training Schedule\nThe hyper-parameters and training details are listed in\nTab. 4. In particular, we set \u03bbfeat=10, and \u03bbhyp=50 at the\nstart of training. After 300k iterations we change the values\nto \u03bbfeat=1, \u03bbhyp=500. During the first 6k iterations, we allow\nthe model to explore all four viewpoint hypotheses by ran-\ndomly sampling the four hypotheses uniformly, and gradu-\nally decrease the chance of random sampling to 20% while\nsampling the best hypothesis for the rest 80% of the time.\nTo save memory and computation, at each training iteration,\nwe only feed images of the same species in a batch, and ex-\ntract one base shape by averaging out the base embeddings.\nAt test time, we just directly use the shape embedding for\neach individual input image.\n15\nChihuahua\nPika\nBaboon\nGaur\nImage\nMask\nFirst 16 Components of PCA DINO Features\nFigure 11. Data Samples. We show some samples of our train-\ning data. Each sample consists of the RGB image, automatically-\nobtained segmentation mask, and the corresponding 16-channel\nPCA feature map.\nB.6. Data Pre-Processing\nWe use off-the-shelf segmentation models [26, 27] to obtain\nthe masks, crop around the objects and resize the crops to\na size of 256 \u00d7 256. For the self-supervised features [40],\nwe randomly choose 5k images from our dataset to compute\nthe Principal Component Analysis (PCA) matrix. Then we\nuse that matrix to run inference across all the images in our\ndataset. We show some samples of different animal species\nin Fig. 11. It is evident that these self-supervised image fea-\ntures can provide efficient semantic correspondences across\ndifferent categories. Note that masks are only for supervi-\nsion, our model takes the raw image shown on the left as\ninput for inference.\n16\nInput\nReconstruction\nOther Views\nArticulated\nFigure 12. Single Image 3D Reconstruction. Given a single image of any quadruped animal at test time, our model reconstructs an\narticulated and textured 3D mesh in a feed-forward manner without requiring category labels, which can be readily animated.\n17\nInput\nReconstruction\nOther Views\nArticulated\nFigure 13. Single Image 3D Reconstruction. Given a single image of any quadruped animal at test time, our model reconstructs an\narticulated and textured 3D mesh in a feed-forward manner without requiring category labels, which can be readily animated.\n18\nInput\nReconstruction\nOther Views\nArticulated\nFigure 14. Single Image 3D Reconstruction. Given a single image of any quadruped animal at test time, our model reconstructs an\narticulated and textured 3D mesh in a feed-forward manner without requiring category labels, which can be readily animated.\n19\n"
  },
  {
    "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers",
    "link": "https://arxiv.org/pdf/2401.02072.pdf",
    "upvote": "8",
    "text": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement\nbased Transformers\nChen Zheng Ke Sun Da Tang Yukun Ma Yuyu Zhang\nChenguang Xi Xun Zhou\nAML Group, Bytedance Inc.\n{chen.zheng1,ke.sun1,da.tang,mayukun,yuyu.zhang}@bytedance.com\n{chenguang.xi,zhouxun}@bytedance.com\nAbstract\nThe emergence of Large Language Models\n(LLMs) such as ChatGPT and LLaMA en-\ncounter limitations in domain-specific tasks,\nwith these models often lacking depth and ac-\ncuracy in specialized areas, and exhibiting a de-\ncrease in general capabilities when fine-tuned,\nparticularly analysis ability in small sized mod-\nels. To address these gaps, we introduce ICE-\nGRT, utilizing Reinforcement Learning from\nHuman Feedback (RLHF) grounded in Proxi-\nmal Policy Optimization (PPO), demonstrating\nremarkable ability in in-domain scenarios with-\nout compromising general task performance.\nOur exploration of ICE-GRT highlights its un-\nderstanding and reasoning ability to not only\ngenerate robust answers but also to provide de-\ntailed analyses of the reasons behind the an-\nswer. This capability marks a significant pro-\ngression beyond the scope of Supervised Fine-\nTuning models. The success of ICE-GRT is\ndependent on several crucial factors, including\nAppropriate Data, Reward Size Scaling, KL-\nControl, Advantage Normalization, etc. The\nICE-GRT model exhibits state-of-the-art per-\nformance in domain-specific tasks and across\n12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting\nthe effectiveness of our approach. We provide a\ncomprehensive analysis of the ICE-GRT, under-\nscoring the significant advancements it brings\nto the field of LLM.\n1\nIntroduction\nThe advent of Large Language Models (LLMs)\nlike ChatGPT (Brown et al., 2020; OpenAI, 2023)\nand LLaMA (Touvron et al., 2023a,b) has marked\na significant milestone in the field of Natural\nLanguage Processing (NLP). These models have\ngained widespread recognition for their robust gen-\neral conversational abilities, enabling fluid and co-\nherent responses across a diverse range of topics.\nHowever, there are key limitations to these models.\nFirstly, a key limitation surfaces when these mod-\nels encounter domain-specific tasks (Zhao et al.,\n2023; Zhang et al., 2023a). In scenarios that de-\nmand deep technical knowledge or specialized ex-\npertise, these models often fall short, providing\nresponses that lack necessary depth and accuracy.\nSecondly, Supervised Fine Tune (SFT) LLMs tend\nto exhibit a decrease in general capabilities (Ling\net al., 2023). This is contrary to the expectations\nheld for large-scale models, which are presumed\nto either maintain or improve their performance in\na wide array of tasks (Pan et al., 2023a). Lastly,\nthe current smaller-sized LLMs, such as 13 Billion,\ndemonstrate a limited ability to conduct detailed\nanalysis on complex questions, a competency that\nis significantly inferior compared to the capabilities\nof models like ChatGPT, which can engage in more\ncomprehensive and detailed discussions.\nAddressing these challenges, we introduce the\nInstruction Context Enhancement by Generative\nReinforcement based Transformers (ICE-GRT),\nan innovative LLM that leverages the principles\nof Reinforcement Learning from Human Feed-\nback (RLHF) (Brown et al., 2020) based on Prox-\nimal Policy Optimization (PPO) (Schulman et al.,\n2017). While ensuring that the general capabilities\nof the Large Language Model (LLM) are main-\ntained, ICE-GRT exhibits exceptional performance\nin several domain-specific scenarios. Furthermore,\nICE-GRT demonstrates an improved ability for de-\ntailed analysis, particularly in complex scenarios\nwhere smaller-sized LLMs fall short.\nWe take one domain-specific task of ad moder-\nation as an example. ICE-GRT can not only de-\ntermine the compliance of advertisements but also\nidentify the specific category of violation. More-\nover, it goes a step further by detailed analyzing\nwhich elements of the ad are problematic and of-\nfers constructive modification suggestions. This\nis a notable advancement over both pretrained and\nSFT (Chiang et al., 2023) LLM models, which are\narXiv:2401.02072v1  [cs.CL]  4 Jan 2024\ntypically limited to identifying compliance and vi-\nolation categories.\nWhen our training methodology was applied to\nRLHF, we observed not just significant improve-\nments in in-domain tasks but also a surprising en-\nhancement in general tasks. In a comparative anal-\nysis against models of equivalent and larger pa-\nrameter size across many general tasks, our ICE-\nGRT model with 13 billion parameters consistently\nachieved state-of-the-art performance in 12 well-\nknown public LLM evaluation benchmarks. ICE-\nGRT\u2019s versatility is further illuminated through\nits effective handling of various domain-specific\ntasks, not limited to but including Poem Genera-\ntion, Text-to-Table conversions, engaging Multiple\nRound Dialogue, generating accurate Multi-lingual\nResponses, proficient Code Generation, creating\ntailored Ads Text and Labeling Text, etc.\nOur exploration of the ICE-GRT model has un-\ncovered several factors critical to its training suc-\ncess. The ICE-GRT model\u2019s training data, sourced\nfrom our ICE-Instruct (SFT) model and enriched\nwith human feedback with strict evaluation criteria,\noffers a diverse and comprehensive dataset, essen-\ntial for its robust training. Moreover, the scaling\nof the reward model is essential for accurately cap-\nturing complex scenarios and aligning with human\npreferences in RLHF. Additionlly, KL-Control is\nkey to regulating the balance between the models,\nwhile Advantage Normalization significantly im-\nproves learning stability by adjusting advantage\nestimates. Additionally, we discovered that modi-\nfying the Clipping Range and carefully controlling\nthe maximum response length during sampling are\nvital for enhancing the training process. These\nfindings deepen our understanding of RLHF mech-\nanisms and are instrumental in effectively training\nthe ICE-GRT model.\nMoreover, we provide a detailed analysis of\nthe ICE-GRT model, encompassing both general\nand in-domain capabilities. Through this explo-\nration, we aim to contribute a novel perspective and\nmethodology to the field of NLP, particularly in en-\nhancing the depth and accuracy of domain-specific\ntask handling by large language models. We ob-\nserve that the pretrain phase engages in \u201cknowledge\nlearning\u201d, where the model extensively absorbs a\ndiverse range of information, forming a substan-\ntial foundational knowledge base. Subsequently,\nin the Supervised Fine-Tuning stage, the model\nengages in \u201cknowledge mining\u201d, where it utilizes\nthe learned knowledge in response to specific in-\nstructions. This stage is crucial for the model to\ntransition from passive knowledge accumulation to\nactive knowledge application. Finally, the RLHF\nphase engages in \u201cknowledge enhancement\u201d, en-\nhancing the model\u2019s ability to align with human\nlanguage preferences. This stage builds upon the\nvast knowledge gained in the pretrain phase and the\nknowledge mining from the SFT stage, leading to\na model that not only reconstruct extensive knowl-\nedge but also excels in applying it with human-\ncentric preference. Importantly, this phase show-\ncases a significant leap in the model\u2019s emergence\ncapabilities.\nIn our commitment to fostering collaborative\nresearch and innovation, we make ICE-GRT\npublicly available on HuggingFace1.\nThis\nopen-source initiative is aimed at empowering re-\nsearchers globally to further investigate and expand\nupon our findings with ICE-GRT. By democratiz-\ning access to this advanced model, we hope to\ninspire and facilitate worldwide exploration and\nprogress in language model research. This paper\nunveils just a fraction of ChatGPT\u2019s capabilities,\nand our choice of the acronym \"ICE\" for ICE-GRT\nis purposeful. It represents our aspiration to accel-\nerate the \u2019ice-breaking\u2019 process in LLM research,\nsymbolizing our desire to inspire researchers to ex-\nplore and uncover the vast potential of ICE-GRT\nacross an array of tasks and paving the way for new\ndiscoveries and advancements in the field.\n2\nRelated Works\n2.1\nInstruction-Tuning for LLM\nRecent advancements in Large Language Model\n(LLM) development have increasingly focused on\ninstruction-tuning (Chiang et al., 2023), a tech-\nnique that is gaining significant traction particu-\nlarly within the realms of Question Answering\n(QA) and different domains (Zhao et al., 2023;\nPan et al., 2023b; Qiu et al., 2020).\nKey re-\nsearch in this area includes works such as AL-\nPACA (Taori et al., 2023), Vicuna (Chiang et al.,\n2023), and (Zhang et al., 2023b), which explores\nthe balance between diveristy and accuracy in large\nlanguage model. Furthermore, studies like (Sun\net al., 2023) delve into principles of effective QA\nstrategies, while (Zhou et al., 2023) present LIMA,\nan innovative model for language interaction. In\nthe sphere of conversational interfaces, significant\n1Our ICE-GRT is available at https://huggingface.co/\nzhengchenphd/ICE-GRT.\nFigure 1: ICE-GRT Model Architecture.\ncontributions include the development of OpenAs-\nsistant by (K\u00f6pf et al., 2023; Chiang et al., 2023).\n2.2\nReinforcement Learning from Human\nFeedback (RLHF)\nAlongside the development of LLMs, Reinforce-\nment Learning from Human Feedback has emerged\nas an important approach to improve LLMs (Brown\net al., 2020; Touvron et al., 2023b). RLHF involves\ntraining models not just on static datasets but also\nincorporating human feedback to guide the learn-\ning process. This method has been particularly\nuseful in aligning knowledge learning and mining\nwith human feedback. For instance, models like\nOpenAI\u2019s InstructGPT have utilized RLHF to tailor\nresponses based on human preferences, leading to\nmore accurate outputs (Stiennon et al., 2020).\n3\nModel\nIn this section, we briefly introduce a SFT model\nwe have trained, named ICE-Instruct, designed to\nimprove the domain-specific knowledge mining ca-\npabilities of pre-trained LLMs. Following this, we\nwill give a detailed description of our process for\ntraining the reward model, which we have termed\nICE-Reward. Finally, we will comprehensively\nintroduce the entire training process of ICE-GRT,\nincluding some important training strategies.\n3.1\nICE-Instruct\nThe ICE-Instruct model built upon the Vicuna\nmodel (Chiang et al., 2023).\nBy blending in-\ndomain and general-purpose data during fine-\ntuning, it excels in both specialized tasks and\nbroader tasks. This approach not only maintains\nits vast linguistic capacities but also enhances its\nexpertise in specific domains. Importantly, this sets\na solid foundation for RLHF models. All subse-\nquent actor and critic models are initialized using\nICE-Instruct as backbone. In essence, ICE-Instruct\ndetermines the lower-bound capabilities of ICE-\nGRT, ensuring a strong and reliable baseline for\nfurther advancements. To maximize the model\u2019s\napplicability in contextual interactions, we have\nconverted all collected data into Question-Answer\npairs. Each data point adheres to a prompt for-\nmat that begins with \u201cBelow is an instruction that\ndescribes a task. Write a response that appropri-\nately completes the request. ### USER: <INPUT>\nASSISTANT: <OUTPUT> \u201d, ensuring consistency\nand relevance in contexts.\n3.2\nICE-Reward\nResponse Generation and Sampling: Initially,\nfor each prompt in the RLHF training dataset,\nwe generate five responses. These responses are\nuniquely produced by our ICE-Instruct model. By\nsampling from the model\u2019s output distribution, we\nensure a diverse range of generated answers , cap-\nturing various aspects of potential responses.\nHuman Annotation and Ranking:\nThe gener-\nated responses are then subjected to human annota-\ntion. Annotators rank these responses according to\npredefined criteria detailed in section 4.3. Specif-\nically, we labeled 20,000 sets of rankings, each\nset containing five responses. From the ranked re-\nsponses, we extract the top two and the bottom two\nresponses for each prompt. These are then paired\nto form training data. The pairs consist of a \u201cbetter\u201d\nresponse and a \u201cworse\u201d response, as determined\nby the human annotation. This pairing strategy is\ninstrumental in teaching the model the differences\nbetween high-quality and low-quality responses.\nTraining Reward Model: The objective of train-\ning reward model is to develop a model capable\nof accurately differentiating between high and low-\nquality responses. Let R(s, a) be the reward func-\ntion, where s represents the input prompt and a\nthe generated response. Our goal is to optimize\nR so that it aligns with human judgments. The\ntraining data consists of pairs (ai, aj) where ai is\na higher-ranked response compared to aj for the\nsame prompt. We use a pairwise ranking loss func-\ntion, defined as:\nL(ai, aj) = max(0, margin\u2212R(s, ai)+R(s, aj)).\nThis loss function encourages the model to assign\na higher score to ai than aj.\nThe trained reward model, therefore, learns to\nassign higher scores to more relevant and contextu-\nally appropriate responses, as per human rankings.\nThis model forms a most critical part of our system,\nensuring high-quality, context-aware responses.\n3.3\nICE-GRT\nIn this section, we provide a comprehensive\noverview of each component involved in ICE-GRT,\nleverages the principles of RLHF (Brown et al.,\n2020) based on PPO (Schulman et al., 2017), along\nwith their respective mathematical formulations.\nFigure 1 shows the whole training process.\nActor Model: The Actor model, represented as\n\u03c0\u03b8act(a|s), maps states s to actions a. It is respon-\nsible for generating actor logits, which are scores\nassigned to each potential action.\nReference Model: The Reference model, denoted\nas \u03c0\u03b8ref(a|s), serves as a pre-trained benchmark for\nevaluating behavior. It provides a baseline against\nwhich the Actor model\u2019s outputs are compared\nthroughout the training process.\nReward Model: The Reward model, expressed\nas R(s, a), assigns a reward score based on the\nquality of the generated sequence, evaluating both\nthe action a and the state s.\nCritic Model: The Critic model, V\u03b8crt(s), estimates\nthe value of being in a specific state s, thereby pro-\nducing critic values that guide the learning process.\n3.3.1\nGeneralized Advantage Estimation\n(GAE) Calculation in ICE-GRT\nThe advantage function, A(s, a), assesses the rela-\ntive benefit of executing a specific action in contrast\nto the average action in a given state. The formula\nfor calculating the Advantage is:\nA(s, a) = E(R(s, a) + \u03b3V\u03b8crt(s\u2032) \u2212 V\u03b8crt(s)) (1)\nwhere \u03b3 represents the discount factor, s\u2032 is the\nsubsequent state following the current state s, and\nV\u03b8crt(s) is the value function estimated by the Critic\nmodel with weights \u03b8crt.\nGeneralized Advantage Estimation (GAE), en-\nhances the estimation of the advantage function in\nRL (Schulman et al., 2015). GAE blends multi-\nstep return methods with value function estimates\nto mitigate variance while preserving a reasonable\nbias. The essence of GAE is the employment of a\nweighted sum of n-step Temporal Difference (TD)\nresiduals:\n\u03b4A\nt = E(Rt+1(s, a) + \u03b3V t+1\n\u03b8crt (s\u2032) \u2212 V t\n\u03b8crt(s)) (2)\nHere, \u03b4A\nt represents the TD residual at time t. Fur-\nther, the GAE advantage function is calcuated as:\nAGAE(s, a) = P\u221e\nl=0(\u03b3\u03bb)l\u03b4A\nt+l, where \u03bb \u2208 (0, 1).\n3.3.2\nActor Model Learning\nThe Actor Model is updated using the Proximal Pol-\nicy Optimization objective (Schulman et al., 2017),\nthe process is calculated as follows:\nL(\u03b8act) = min\n \n\u03c0\u03b8act(a|s)\n\u03c0\u03b8old(a|s)A\n\u03c0\u03b8old\nGAE (s, a),\nclip\n\u0012 \u03c0\u03b8act(a|s)\n\u03c0\u03b8old(a|s), 1 \u2212 \u03b5, 1 + \u03b5\n\u0013\nA\n\u03c0\u03b8old\nGAE (s, a)\n!\n,\n(3)\nwhere A\n\u03c0\u03b8old\nGAE(s, a) is the advantage function calcu-\nlated using the old policy \u03c0\u03b8old, \u03b5 \u2208 (0, 1) is a hy-\nperparameter. This term ensures that the evolving\nActor policy remains not only stable in its updates\nbut also aligned or divergent as desired from the\nold model.\n3.3.3\nPolicy Optimization and Training\nIn the final stage, the PPO algorithm optimizes\nthe Actor model\u2019s policy based on the calculated\nadvantages, the KL-divergence, and the updated\nActor model. The policy is iteratively updated to\nmaximize the expected rewards, with the aim of\naligning the Actor model\u2019s behavior more closely\nwith established benchmarks while also ensuring\neffective and efficient learning.\n3.3.4\nImportant Training Strategies\nICE-GRT Training Data: Our ICE-GRT\u2019s train-\ning data originates from ICE-Instruct model and\ncareful human feedback annotation. This data is\nnot just a collection of responses but is intricately\ndesigned to encompass a wide range of scenarios.\nEach prompt within the ICE-Instruct model is re-\nsponded to with a set of diverse answers, gener-\nated by sampling from the model\u2019s output distri-\nbution. This method ensures a comprehensive and\nvaried dataset, essential for robust model training.\nThe responses are further refined through a metic-\nulous human annotation process, where experts\nrank them based on predefined criteria. This rig-\norous approach ensures the model is trained on\nhigh-quality, human-verified data, which is crucial\nfor the model\u2019s ability to understand and apply com-\nplex information. More details and experimental\ncomparsions are described in Section 5.2.1.\nReward size Scaling: In ICE-GRT, the scaling of\nthe reward model is a critical factor in determining\nthe overall effectiveness and efficiency of training.\nA larger reward model, denoted as R\u03c8(s, a), where\n\u03c8 represents the model parameters, is significant\nfor several reasons. Firstly, larger reward model\ncan better capture complex environments and ac-\ntions, essential in RLHF where the reward signal\nmust accurately reflect human preferences and de-\ntailed task requirements. Secondly, larger scale\nof reward size aids in generalizing across diverse\nprompts. This is vital for consistent performance\nin various scenarios, especially in ICE-GRT.\nKL-Control is a crucial mechanism in PPO, es-\npecially when training with human feedback. A\nkey aspect of KL-Control in this context is the reg-\nulation of divergence between the Actor and the\nReference models. The KL divergence between\nthese two models is monitored and controlled to\nensure that the policy evolution adheres closely to\nthe human feedback. Moreover, ICE-GRT training\nincludes a clipping mechanism to avoid large, po-\ntentially destabilizing updates in the value function.\nThis ensures that changes in the value function are\nmoderate and accurately reflect real improvements\nas assessed by the Critic. Furthermore, as an addi-\ntional measure, KL Reward adjustment helps keep\nthe actor model on the desired path as defined by\nhuman feedback. This aligns actor model updates\nmore closely with human preferences.\nAdvantage Normalization enhances learning sta-\nbility and efficiency in PPO-based RLHF. It ad-\njusts the advantage estimates, making them more\nconsistent and less variable. This is particularly\nbeneficial in RLHF, where human feedback can in-\ntroduce unpredictable variations. Normalizing the\nadvantage helps the model to focus on the most rel-\nevant learning signals, leading to faster and more\nstable convergence. The formula for Advantage\nNormalization is shown as follows:\n\u02c6A\u03c0\u03b8\nt\n= A\u03c0\u03b8\nt\n\u2212 \u00b5A\u03c0\u03b8\n\u03c3A\u03c0\u03b8\n,\nwhere \u02c6A\u03c0\u03b8\nt\nrepresents the normalized advantage at\ntime t, A\u03c0\u03b8\nt\nis the original advantage at time t, \u00b5A\u03c0\u03b8\nis the mean of the advantage, \u03c3A\u03c0\u03b8 is the standard\ndeviation of the advantage.\n4\nExperimental Details\nOur training process utilized the power of 64 A100\nGPUs, employing a multi-node, multi-GPU strat-\negy to conduct ICE-GRT. Our models were trained\nand stored using the bf16 precision format. The\nlearning rates were finely selected, with the actor\nlearning rate set at 5e \u2212 6 and the critic learning\nrate at 5e \u2212 7. We maintained a clipping range\nof 0.2. The discount factor \u03b3 was kept constant\nat 0.95, ensuring optimal balance in our training.\nWe are excited to announce the upcoming release\nand open-sourcing of our ICE-GRT 13B model on\nHugging Face, specifically tailored for scientific\nresearch purposes.\n4.1\nData Collection\nFor our training corpus, we have crafted a novel\nmix of datasets. This includes a selection from\npublicly available resources, complemented by in-\ndomain data. We have removed all the sensitive\ninformation, including usernames, email addresses,\nand personal details, to uphold the data privacy and\nsecurity. In essence, the dataset we have prepared\nfor reward model and RLHF model is diverse and\nmulti-faceted, covering a range of domains. It in-\ncludes data relevant to public and domain-specific\nquestion-answering scenarios, as well as tasks in-\nvolving multilingual data alignment. We generated\n5 distinct responses for every prompt in our data\ncollection, utilizing our ICE-Instruct model. This\nprocess involves sampling from the model\u2019s output\ndistribution, which guarantees a varied spectrum\nof answers. To optimally train our reward model,\nthe data labelers carefully conducted manual label-\ning of the rankings for the 5 distinct responses on\n20,000 prompts. To enhance the human-annotation\naccuracy and reduce subjectivity among labelers,\neach prompt was independently evaluated by three\nlabelers, establishing a thorough and reliable vali-\ndation processverification process.\n4.2\nGeneral Task Evaluation\nOur evaluation of ICE-GRT using the GPT-Fathom\nframework (Zheng et al., 2023) focused on public\ngeneral tasks. The objective was to benchmark ICE-\nGRT\u2019s performance against existing models and to\nunderstand its position in the landscape of current\nLLMs. We employed 12 benchmarks, which span\nacross various capability categories such as lan-\nguage understanding, reasoning, etc. These bench-\nmarks were carefully chosen to test a wide range of\nabilities, from basic language processing to com-\nplex problem-solving and decision-making tasks.\nIn our evaluation, we maintained alignment with\nthe settings used in GPT-Fathom to ensure a fair\nand accurate comparison. This involved employ-\ning similar input formats, evaluation metrics, and\nModel\nMMLU\nAGIEval\nBBH\nAGIEval-ZH\nARC-E\nARC-C\nHellaSWAG\nWinogrande\nRACE-M\nRACE-H\nGSM8K\nMath\n5-shot\nfew-shot\n3-shot\nfew-shot\n1-shot\n1-shot\n1-shot\n1-shot\n1-shot\n1-shot\n8-shot\n4-shot\nLLaMA 7B\n24.66%\n20.05%\n33.48%\n23.68%\n30.01%\n26.71%\n24.58%\n50.36%\n26.74%\n29.19%\n13.80%\n0.36%\nLlama2 7B\n40.91%\n25.97%\n38.21%\n26.21%\n62.37%\n48.46%\n25.39%\n50.36%\n45.75%\n39.54%\n17.51%\n0.08%\nVicuna 7B\n38.49%\n22.71%\n37.26%\n27.00%\n69.74%\n46.33%\n17.37%\n49.80%\n50.21%\n46.83%\n21.68%\n0.96%\nICE-Instruct 7B\n26.30%\n15.95%\n39.00%\n31.14%\n67.63%\n45.31%\n3.10%\n36.07%\n53.55%\n52.09%\n35.48%\n0.82%\nLLaMA 13B\n38.42%\n26.78%\n38.28%\n25.51%\n67.63%\n49.23%\n28.90%\n47.51%\n52.23%\n48.51%\n18.42%\n0.42%\nLlama2 13B\n49.57%\n34.85%\n45.89%\n32.93%\n76.52%\n55.63%\n37.17%\n52.17%\n57.73%\n55.09%\n28.66%\n0.44%\nVicuna 13B\n35.84%\n28.68%\n39.27%\n30.33%\n60.23%\n40.96%\n0.03%\n5.84%\n59.19%\n60.69%\n24.56%\n0.66%\nICE-Instruct 13B\n50.08%\n24.51%\n48.09%\n34.15%\n85.19%\n66.89%\n19.30%\n47.99%\n72.14%\n56.52%\n47.08%\n1.02%\nICE-GRT 13B\n55.33%\n34.92%\n49.78%\n34.23%\n87.58%\n70.99%\n39.37%\n53.04%\n75.91%\n71.64%\n51.48%\n0.92%\nLLaMA 30B\n50.38%\n34.87%\n49.70%\n30.68%\n82.41%\n60.67%\n31.31%\n51.30%\n65.18%\n64.18%\n35.10%\n0.58%\nLlama2-70B\n64.72%\n43.99%\n65.22%\n39.52%\n93.43%\n79.61%\n68.45%\n69.69%\n87.60%\n85.13%\n56.56%\n3.72%\nTable 1: Evaluating Benchmark Performance of Large Language Models in General Language Tasks.\nenvironmental conditions.\n4.3\nManual Annotation-Based Evaluation\nOur study incorporates a rigorous evaluation crite-\nria, with a special emphasis on manual annotation\nfor assessing the capabilities of LLMs, particularly\nin different applications. The criteria evaluates re-\nsponses in 8 essential categories, utilizing a scoring\nmechanism that prioritizes the most crucial aspects.\nClarity: Responses should be straightforward and\nprecise, ensuring easy comprehension through spe-\ncific, appropriate language.\nAccuracy: The responses are expected to align\nclosely with verified facts, as assessed by manual\nannotators. Actual fact can be validated.\nCompleteness: Evaluated for covering all aspects\nof the inquiry, providing comprehensive details for\ninformed decision-making.\nSafety: Focuses on ensuring no personal data is\nmishandled, with manual checks for data privacy.\nCourtesy: Responses should be politically correct.\ne.g., gender identity, ethnic groups, etc.\nComfortableness: Responses must maintain a po-\nlite and respectful tone, containing inclusive vocab-\nulary and reflect diversity at all times..\nConciseness: Emphasizes brevity in responses,\nwithout compromising on clarity or accuracy.\nContext: Response must be related to the topic and\nrelevant to the question.\nTable 2 shows the weight and score of each cate-\ngories to evaluate these criteria accurately, ensuring\nresponses quality and relevance.\nEvaluation\nPositive\nNeutral\nNegative\nWeights\nClarity\n5\n2\n0\n6\nAccuracy\n5\n2\n0\n6\nCompleteness\n5\n2\n0\n6\nSafety\n5\n2\n0\n3\nCourtesy\n5\n2\n0\n3\nComfortableness\n5\n2\n0\n3\nConciseness\n5\n2\n0\n1\nContext\n5\n2\n0\n1\nTable 2: Manual Annotation-Based Evaluation Criteria.\n5\nResults and Analysis\n5.1\nResults\nBenckmarks Scores on General Tasks: Our anal-\nysis focuses on the performance of ICE-GRT 13B,\nas compared to other models in similar and higher\ncapacity categories. As is shown in Table 1, our\nICE-GRT 13B model demonstrates significant im-\nprovements over the LLaMa, Llama 2, Vicuna\n13B and LLaMa 30B in both its pretrained and\nSFT across various general benchmarks, such as\nMMLU (Hendrycks et al., 2021), AGIEval (Zhong\net al., 2023), BBH (Srivastava et al., 2022),\nARC (Xu et al., 2023), HellaSWAG (Zellers et al.,\n2019), RACE (Lai et al., 2017), etc. It shows re-\nmarkable advancements in general language un-\nderstanding and reasoning tasks, indicating en-\nhanced comprehension and reasoning capabilities.\nRemarkably, the ICE-GRT 13B model has signif-\nicantly narrowed the gap with the much larger\nLlama2 70B pretrain model. This comparison un-\nderscores the effectiveness of the ICE-GRT, com-\npensating for smaller model size with more gener-\nalization capabilities. The success of the ICE-GRT\nmodels suggests that the methodology, which likely\nincludes components of human feedback and align-\nment, contributes significantly to the models\u2019 abil-\nity to understand and respond to complex prompts,\na factor that is not solely dependent on model size.\nHuman-Annotated Scores on In-Domain Task:\nIn the in-domain evaluation presented in Table\n3, ICE-GRT distinctly outperforms Llama2 SFT\n13B and ICE-Instruct 13B across several critical\ndimensions. Notably, ICE-GRT achieves the high-\nest scores in clarity (98.1%), accuracy (97.0%),\nand completeness (92.9%), underscoring its excep-\ntional ability to deliver precise, comprehensive, and\nunderstandable responses. While it scores slightly\nlower in safety and comfort compared to its coun-\nterparts, it still maintains a high standard in these\nareas. The overall score of 95.5% for ICE-GRT is a\ntestament to its superior performance, significantly\nsurpassing Llama2 SFT 13B (86.3%) and ICE-\nInstruct 13B (87.3%). This robust performance\nacross multiple metrics confirms the introductory\nclaims about ICE-GRT\u2019s capabilities, particularly\nin handling domain-specific tasks with a level of\ndepth and precision not seen in current models.\nLlama2 sft\nICE-Instruct\nICE-GRT\nClarity\n95.9%\n88.5%\n98.1%\nAccuracy\n77.4%\n84.44%\n97.0%\nCompleteness\n64.8%\n71.11%\n92.9%\nSafety\n96.6%\n100%\n92.2%\nCourtesy\n100%\n95.9%\n100%\nComfortable\n96.6%\n98.1%\n92.22%\nConciseness\n95.1%\n93.33%\n91.8%\nContext\n98.8%\n94.0%\n98.1%\nOverall Score\n86.3%\n87.3%\n95.5%\nTable 3: Evaluating human-assessed scores for in-\ndomain Large Language Models.\n5.2\nDetailed Analysis\n5.2.1\nThe importance of ICE-GRT Training\nData\nIn the training of the ICE-GRT, we employed two\ndistinct datasets for RLHF. The first dataset was\nuniquely produced by our ICE-Instruct model. For\neach prompt, five diverse responses were generated\nby sampling from the model outputs. These re-\nsponses were then subjected to human annotation,\nwhere annotators ranked them according to prede-\nfined criteria. The second dataset originated from\nthe GPT-4-LLM (Peng et al., 2023). It included\nranked responses from GPT-4 and GPT-3.5, with\nthe rankings automatically assessed by GPT-4.\nOur findings reveal a significant performance dis-\nparity between models trained with these datasets,\nalthough we found that the reward score trends\nwere similar during the ICE-GRT training shown\nin Figure 2a. The ICE-GRT model, trained with\nour human-annotated dataset, demonstrated supe-\nrior performance across general tasks and domain-\nspecific tasks. As shown in Figure 2b, on the Nat-\nural Question task, the ICE-GRT model outper-\nformed ICE-Instruct by 4%. This gap increased to\napproximately 9.79% on the Web Questions and\n17.17% on the LAMBADA benchmark. However,\nwhen we employed the GPT-4-LLM Dataset on\nICE-GRT, we observe that the results were very\nclose to those of ICE-Instruct, with only a 0.89%\nincrease in the Natural Questions.\nA key aspect of ICE-GRT\u2019s success is its fo-\ncus on \u2018knowledge enhancement\u201d. This process\nbuilds upon the \u201cknowledge mining\u201d during the\nICE-Instruct, enabling the model to better align\nwith human language preferences. This approach\nguarantees consistency and relevance in training\ndata, which is crucial for the model to effectively\nbuild upon and evolve its existing knowledge. Ex-\nternal data sources, despite their potential diversity,\ncould not perfectly align with the model\u2019s knowl-\nedge structure. The use of data generated by ICE-\nInstruct ensures a natural and effective enhance-\nment of knowledge, as observed in ICE-GRT.\n(a) Reward score comparsion\nbetween different RLHF data.\n(b) Benchmark performance\nbetween different models.\nFigure 2: The influence of different training data.\n5.2.2\nPowerful ICE-GRT on General Task\nICE-GRT model exhibits exceptional strength in\ntasks that are grounded in language understanding\nand reasoning. For instance, as shown in Figure 3a\nand Figure 3c, ICE-GRT 13B demonstrates a robust\nability in RACE, ARC, BBH, and GSM8K. Our\nmodel has achieved state-of-the-art performance in\nthese tasks among models of the same size.\nMoreover, the ICE-GRT 13B model shows great\nperformance in areas such as \"marketing\" and\n\"college-biology\", as depicted in Figure 3b. Re-\nmarkably, the ICE-GRT 13B model surpasses the\nLlama 2 70B in several AGIEval benchmarks, most\nnotably in tasks requiring language understanding\nand reasoning, such as \"gaokao-chinese\" as seen\nFigure 3d. This outperformance stems from ICE-\nGRT knowledge enhanced contextual understand-\ning and generative capabilities.\n5.2.3\nThe effectiveness of Advantage Norm\nThe integration of Advantage Normalization and\nReward Size Scaling significantly enhances ICE-\nGRT. These strategies contribute to improved\ntraining efficiency and better model performance,\ndemonstrating their importance in the context of\nRLHF. Applying Advantage Normalization, which\nstabilizes learning by normalizing advantage es-\ntimates, led to improvement in Natural Question\nbenchmark over ICE-GRT baseline. As shown in\nFigure 4,this strategy is crucial for enhancing the\nmodel\u2019s sensitivity to the subtleties of human feed-\nQuestion\nICE-Insturct 13B\nICE-Insturct 33B\nICE-GRT 13B\n\"this is a 100% natural prod-\nuct for all females and all\nages\" what ad policy does the\nabove text violate?\nNone, just make sure you\u2019re\nnot lying about it.\nThe advertisement text may vi-\nolate exaggerated claims pol-\nicy. Overstates the product\u2019s\neffectiveness by claiming it is\nsuitable for all females and all\nages.\nThe above text may violate exaggerated claims policy if the product\nis marketed as \"100% natural\" without providing sufficient evidence\nor scientific backing for the claim. Making an exaggerated or unsup-\nported claim about a product\u2019s naturalness could mislead consumers\nand potentially violate advertising regulations. To avoid violating this\npolicy, advertisers should provide clear evidence to support any\nclaims about a product\u2019s naturalness or provide a more nuanced\ndescription of its ingredients and production methods. Additionally,\nadvertisers should ensure that their claims are not overstated or mislead-\ning, and that they are consistent with any applicable laws and regulations\ngoverning product labeling and marketing.\nRewrite \"this is a 100% natu-\nral product for all females and\nall ages\" to an ad without any\nviolations\nThis product is suitable for all\nwomen and all age groups.\nExperience the pure and nat-\nural goodness of this product,\nsuitable for all ages and gen-\nders.\nIntroducing our natural product designed for all females and ages. Our\ncarefully crafted formulation is free of harmful chemicals and artifi-\ncial additives, ensuring that you can use it with confidence. Whether\nyou\u2019re looking for a daily skincare routine or specialized care for a spe-\ncific concern, our product is a safe and effective choice for all skin types.\nExperience the benefits of natural ingredients and nourishing care\nwith our product. Suitable for all ages and skin types, our product is a\nsafe and gentle choice for daily skincare.\nTable 4: Comparative Case Study in the Domain-Specific Moderation Task.\n(a) Language understanding\nand Reasoning Tasks.\n(b) MMLU Task Score Com-\nparsion.\n(c) BBH Task Performance.\n(d) AGIEval-ZH Task.\nFigure 3: Score Comparsions between different LLMs.\nback, leading to more effective learning outcomes.\n5.3\nCase Study on Domain-Specific Task\nWe provide a comparative analysis of the responses\ngenerated by different models, specifically ICE-\nInstruct 13B, 33B, and ICE-GRT 13B, revealing\nvarying levels of sensitivity and creativity in ad-\ndressing advertising policy adherence and rewrit-\ning for compliance. As is shown in Table 4, while\nICE-Instruct 13B takes a more direct and less cau-\ntious approach, ICE-Instruct 33B and ICE-GRT\n13B demonstrate a progressive increase in policy\nawareness and creative compliance.\nICE-GRT, in particular, shows a comprehensive\nunderstanding of advertising regulations and the im-\nportance of substantiated claims, reflecting its ad-\nvanced capability in nuanced and responsible com-\nmunication. In the first case, ICE-GRT displayed\nthe highest sensitivity to policy adherence, high-\nlighting the risk of violating exaggerated claims\nFigure 4: Comparative Analysis of ICE-GRT and ICE-\nGRT Advantage Normalization on the Natural Ques-\ntion (NQ) Benchmark. The x-axis represents different\nepochs, while the y-axis shows the NQ scores.\npolicy, especially if the product is marketed as\n\"100% natural\" without adequate evidence. It em-\nphasizes the need for evidence-based advertising\nand compliance with regulations. In the second\ncase, ICE-GRT Provided the most detailed and cau-\ntious rewrite, ensuring compliance with advertising\npolicies. It focuses on natural ingredients, absence\nof harmful chemicals, and suitability for all females\nand ages, while avoiding exaggerated claims.\nIn this section, we have showcased only a small\nfraction of our model\u2019s capabilities, focusing pri-\nmarily on the in-domain task of ad moderation.\nHowever, the scope of our model, ICE-GRT, ex-\ntends far beyond this singular function. Within the\nappendices, we demonstrate its proficiency across\na myriad of domain-specific tasks. These include,\nbut are not limited to, Poem Generation, Text-to-\nTable, Multiple Round Dialogue (Appendix A),\nChemistry Response Generation (Appendix B),\nCode Generation (Appendix C), Ads Text Gen-\neration, Labeling Text (Appendix D), and Multi-\nlingual Response (Appendix E), etc. The choice\nof the acronym \"ICE\" for ICE-GRT is deliber-\nate.\nIt represents our aspiration to catalyze an\n\u2019ice-breaking\u2019 moment in the research of LLMs.\nThis reflects our hope to encourage researchers to\nexplore and realize the broad possibilities of ICE-\nGRT in a range of tasks. We aim to pave the way\nfor novel discoveries and advancements in the field,\ndemonstrating that the capabilities of our model\nare as extensive and varied as they are innovative.\n6\nConclusion\nICE-GRT model represents a significant leap for-\nward in the realm of LLMs, particularly in enhanc-\ning domain-specific performance. Leveraging the\nprinciples of Reinforcement Learning from Human\nFeedback, ICE-GRT demonstrates exceptional ca-\npabilities in both general and in-domain tasks, out-\nperforming standard models in accuracy and depth.\nMoreover, our model have strong ability to gen-\nerate detailed analyses of the reasons behind the\nanswer. Our research uncovers several aspects of\nRLHF, providing insights into effective training\nmethodologies and highlighting the importance of\nfactors like Appropriate Data, Reward Size Scaling,\nKL-Control, etc. ICE-GRT\u2019s training phases, in-\ncluding knowledge learning, mining, and enhance-\nment, contribute to its advanced abilities in aligning\nwith human preferences. We hope that ICE-GRT\nwill accelerate the \u201cice-breaking\u201d process in LLM\nresearch, encouraging further exploration.\nAcknowledgements\nWe deeply appreciate Youlong Cheng, Guokun Lai,\nYingtong Bu, Zheng Zhang, Fan Qu for their help at\nthe early stage of this project. Moreover, we convey\nappreciation to Hanzhi Zhou, Yijie Zhu, Xuan Zou\nfor their engineering support to build key compo-\nnents of the infrastructure. We extend our gratitude\nto Hang Wu, Ruoqi Zhang and Ruohong Zhang for\ntheir insightful discussions that contributed to this\npaper. Furthermore, we thank anonymous review-\ners for their valuable suggestions.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stan-\nley, Rich\u00e1rd Nagyfi, et al. 2023.\nOpenassistant\nconversations\u2013democratizing large language model\nalignment. arXiv preprint arXiv:2304.07327.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and\nEduard H. Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. ArXiv,\nabs/1704.04683.\nChen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan\nDeng, Can Zheng, Junxiang Wang, Tanmoy Chowd-\nhury, Yun-Qing Li, Hejie Cui, Xuchao Zhang, Tian\nyu Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang,\nYanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris\nWhite, Quanquan Gu, Jian Pei, Carl Yang, and Liang\nZhao. 2023. Domain specialization as the key to\nmake large language models disruptive: A compre-\nhensive survey.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nWenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che,\nand Libo Qin. 2023a. A preliminary evaluation of\nchatgpt for zero-shot dialogue understanding. ArXiv,\nabs/2304.04256.\nWenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che,\nand Libo Qin. 2023b. A preliminary evaluation of\nchatgpt for zero-shot dialogue understanding. arXiv\npreprint arXiv:2304.04256.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872\u2013\n1897.\nJohn Schulman,\nPhilipp Moritz,\nSergey Levine,\nMichael I. Jordan, and P. Abbeel. 2015.\nHigh-\ndimensional continuous control using generalized\nadvantage estimation. CoRR, abs/1506.02438.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri\u00e0 Garriga-Alonso, et al. 2022.\nBeyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models.\narXiv preprint\narXiv:2206.04615.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u2013\n3021.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Cox, Yiming Yang,\nand Chuang Gan. 2023.\nPrinciple-driven self-\nalignment of language models from scratch with\nminimal human supervision.\narXiv preprint\narXiv:2305.03047.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nYudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott\nSanner, and Elias Boutros Khalil. 2023. Llms and the\nabstraction and reasoning corpus: Successes, failures,\nand the importance of object-based representations.\nArXiv, abs/2305.18354.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a ma-\nchine really finish your sentence? In Annual Meeting\nof the Association for Computational Linguistics.\nZheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun\nMa, Yingtong Bu, Xun Zhou, and Liang Zhao. 2023a.\nBalancing specialized and general skills in llms: The\nimpact of modern tuning and data strategy. arXiv\npreprint arXiv:2310.04945.\nZheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun\nMa, Yingtong Bu, Xun Zhou, and Liang Zhao. 2023b.\nBalancing specialized and general skills in llms: The\nimpact of modern tuning and data strategy. ArXiv,\nabs/2310.04945.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang\nRen, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu,\nJianyun Nie, and Ji rong Wen. 2023. A survey of\nlarge language models. ArXiv, abs/2303.18223.\nShen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi,\nPengyang Gao, Xun Zhou, and Kevin Chen-Chuan\nChang. 2023. Gpt-fathom: Benchmarking large lan-\nguage models to decipher the evolutionary path to-\nwards gpt-4 and beyond. ArXiv, abs/2309.16583.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied Sanosi Saied,\nWeizhu Chen, and Nan Duan. 2023. Agieval: A\nhuman-centric benchmark for evaluating foundation\nmodels. ArXiv, abs/2304.06364.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.\nA\nPoem Generation, Text-to-Table, Multiple Round Dialogue.\nThis appendix section presents the multifaceted capabilities of the ICE-GRT, as it navigates through three\ndistinct tasks: Poem Generation, Text-to-Table, and Multiple Round Dialogue. Each section showcases a\nunique aspect of the ICE-GRT prowess. In Poem Generation, we explore its creative capacity to craft\nexpressive content. The Text-to-Table task demonstrates its proficiency in data structuring and presentation.\nFinally, the Multiple Round Dialogue section exhibits its skill in engaging in predictive and analytical\ndiscourse. These tasks collectively underscore strong abilities of the ICE-GRT, exemplifying its potential\nin diverse linguistic and analytical applications.\nQuestions\nResponses\nCompose a poem to analyze American econ-\nomy with a positive attitude.\nThe American economy is strong and vibrant,\nA shining beacon of hope and progress.\nFrom coast to coast, businesses thriving,\nInnovation and prosperity on the rise.\nFrom the tech giants in Silicon Valley,\nTo the small startups in every town,\nThe American spirit of enterprise,\nDriving growth and creating jobs galore.\nFrom Wall Street to Main Street,\nInvestors and consumers alike,\nConfidence in the future is high,\nAs the economy continues to expand and thrive.\nWith a positive attitude and a can-do spirit,\nThe American economy will continue to grow and prosper,\nBringing opportunity and prosperity,\nTo all who work hard and strive for success.\nTable 5: Case Study in Poem Generation task.\nQuestions\nResponses\nThen Generate a table to show Americans main\neconomic metrics, such as GPP, PPP, CPI, PPI,\netc\nHere is a table showing some of the main economic metrics for the United States:\nNote: These values are subject to change and may not reflect the most up-to-date information. It\u2019s important to consult\nthe latest official sources for the most current economic data.\nTable 6: Case Study in Text-to-Table Task.\nQuestions\nResponses\nPredict American economy next year based on\nthe data above and give your reasons.\nAs an AI language model, I cannot predict the future with certainty, but I can provide some insights based on the\neconomic data available as of my knowledge cutoff date (September 2021). The United States has experienced a strong\neconomic recovery in 2021, with GDP growth projected to be around 6% for the year. This growth has been driven\nby a combination of factors, including strong consumer spending, increased business investment, and a rebound in\ninternational trade. The Consumer Price Index (CPI) has also risen steadily throughout 2021, reflecting higher inflationary\npressures in the economy.\nLooking ahead to the next year (2022), there are several factors that may impact the U.S. economy. One key factor to\nwatch will be the trajectory of the COVID-19 pandemic, as the ongoing pandemic continues to pose risks to economic\ngrowth and stability. Additionally, the U.S. Federal Reserve has signaled that it may begin tapering its asset purchases in\nthe coming months, which could impact financial markets and interest rates.\nAssuming a continued economic recovery and a gradual normalization of monetary and fiscal policies, it is possible that\nthe U.S. economy could experience modest growth in 2022. However, there are also risks and uncertainties that could\nimpact the outlook, including the potential for new waves of the pandemic, geopolitical risks, and ongoing supply chain\ndisruptions. Overall, the outlook for the U.S. economy in 2022 will depend on a variety of factors, and it is important for\npolicymakers and businesses to remain vigilant and adaptable in the face of evolving economic conditions.\nTable 7: Case Study in Multiple Round Dialogue task.\nB\nChemistry Response Generation.\nQuestions\nResponses\nHelp me with the chemistry equations, such as\nH2O related equations.\nHere are some commonly used chemical equations related to H2O:\n1. Combustion of hydrogen gas (H2):\n2H2 + O2 -> 2H2O\n2. Decomposition of water (H2O) into hydrogen gas (H2) and oxygen gas (O2) using electricity:\n2H2O -> 2H2 + O2\n3. Reaction of hydrogen gas (H2) and oxygen gas (O2) to form water (H2O):\n2H2 + O2 -> 2H2O\nThese equations are just a few examples of how H2O is involved in various chemical reactions. There are many other\nequations and reactions that involve H2O, depending on the specific context and conditions. It\u2019s important to understand\nthe underlying chemistry and physics of these equations and reactions in order to fully comprehend their implications and\napplications. Additionally, it\u2019s important to follow proper safety protocols and procedures when handling chemicals and\nperforming experiments, as some reactions can be hazardous or even dangerous if not properly controlled and monitored.\nTable 8: Case Study in Chemistry Generation Task.\nC\nCode Generation.\nQuestions\nResponses\nWrite a function to count the\nappearing times of each word,\nthen write another function to\nsort this list, but you cannot use\nany libs in python.\nSure, here\u2019s a Python function that counts the occurrences of each word in a\ngiven string and returns a dictionary with the word as the key and its count as\nthe value:\ndef\ncount_words ( t e x t ) :\nwords = t e x t . lower ( ) . s p l i t ( )\nword_counts = {}\nf o r\nword\nin\nwords :\ni f\nword\nin\nword_counts :\nword_counts [ word ] += 1\ne l s e :\nword_counts [ word ] = 1\nr e t u r n\nword_counts\nTo sort the dictionary of word counts, you can use the sorted() function along\nwith a lambda function that compares the values of the dictionary items:\ndef\nsort_word_counts ( word_counts ) :\ns o r t e d _ c o u n t s = s o r t e d ( word_counts . items ( ) ,\nkey=lambda x :\nx [ 1 ] ,\nr e v e r s e =True )\nr e t u r n\ns o r t e d _ c o u n t s\nHere\u2019s how you can use these functions together to count the occurrences of each\nword in a string and sort the word counts:\nt e x t = \" The quick\nbrown fox\njumped\nover\nthe\nlazy\ndog\n.\nThe dog\nbarked\na t\nthe\nfox . \"\nword_counts = count_words ( t e x t )\nsorted_word_counts = sort_word_counts ( word_counts )\np r i n t ( sorted_word_counts )\nThis will output:\n[ ( \u2019 fox \u2019 ,\n2) ,\n( \u2019 dog \u2019 ,\n2) ,\n( \u2019 the \u2019 ,\n2) ,\n( \u2019 quick \u2019 ,\n1) ,\n( \u2019 brown \u2019 ,\n1) ,\n( \u2019 jumped \u2019 ,\n1) ,\n( \u2019 over \u2019 ,\n1) ,\n( \u2019 lazy\n\u2019 ,\n1) ,\n( \u2019 barked \u2019 ,\n1) ]\nTable 9: Case Study 1 in Code Generation Task.\nQuestions\nResponses\nBy using the words above as cor-\npus, try to implement word2vec\ncode based on its theory\nSure, here\u2019s an implementation of word2vec in Python based on the theory\ndescribed in the paper \"Word2Vec: Negative Sampling for Fast and Accurate\nLearning of Word Vectors\" by Mikolov et al. This implementation uses the Skip-\nGram algorithm to train a neural network to predict nearby words given a target\nword, and vice versa. The resulting word vectors can be used for various natural\nlanguage processing tasks, such as text classification, language translation, and\nsentiment analysis.\nHere\u2019s the implementation:\nimport numpy as np\nimport\ngensim\nfrom gensim . models\nimport\nCoherenceModel\nfrom gensim . models . coherencemodel\nimport\nCoherenceModel\nas\nCoherence\nfrom gensim . models . word2vec\nimport Word2Vec\nfrom gensim . corpora\nimport\nD i c t i o n a r y\nfrom gensim . models . ldamodel\nimport\nLdaModel\n# Set up\nthe\ncorpus\nand\nd i c t i o n a r y\nt e x t = \" The quick\nbrown fox\njumped\nover\nthe\nlazy\ndog\n.\nThe dog\nbarked\na t\nthe\nfox . \" . lower ( )\ntokens = [ word\nf o r\nword\nin\nt e x t . s p l i t ( ) ]\nd i c t i o n a r y = D i c t i o n a r y ( tokens )\n# Train\nthe\nword\nv e c t o r s\nusing\nthe\nSkip \u2212Gram\na l g o r i t h m\nmodel = Word2Vec ( [ t e x t ] ,\nmin_count =1 ,\ns i z e =100 ,\nwindow =5 ,\nworkers =4)\n# Evaluate\nthe\nword embeddings\nusing\ncoherence\na n a l y s i s\nmodel_coherence = Coherence ( model . wv ,\nmodel . wv ,\nd i c t i o n a r y = d i c t i o n a r y ,\ncoherence = \u2019 c_v \u2019 ,\ncoherence_mode= \u2019 v e c t o r \u2019 )\ncoherence_score = coherence_model . get_coherence ( )\np r i n t ( f \" Coherence\nscore :\n{ coherence_score } \" )\n# Train\na LDA model on\nthe\nword embeddings\nto\ni d e n t i f y\nt o p i c s\nin\nthe\ncorpus\nlda_model = LdaModel ( corpus =[ model . wv] ,\nid2word=\nd i c t i o n a r y ,\nnum_topics =5 ,\nrandom_state =42)\nt o p i c s = lda_model . p r i n t _ t o p i c _ w o r d s ( num_words =5)\np r i n t ( t o p i c s )\nThis implementation uses the gensim library, which provides an easy-to-use\ninterface for training word embeddings and other natural language processing\nmodels. The Word2Vec class is used to train the word vectors using the Skip-\nGram algorithm, while the CoherenceModel class is used to evaluate the word\nembeddings using coherence analysis. Finally, the LdaModel class is used to\ntrain a LDA model on the word embeddings to identify topics in the corpus. The\nresulting topics can provide insights into the main themes and topics present in\nthe corpus.\nTable 10: Case Study 2 in Code Generation Task.\nD\nAds text Generation, Label text, Multiple Round Dialogue.\nQuestions\nResponses\nI want to post decoration ads,\nplease generate some candidate\nfor me to promote my devices\nand materials.\nSure, here are some sample ad copy ideas for decoration products:\n1. \"Transform your home into a work of art with our high-quality decorative\nitems. From wall art and sculptures to accent pieces and lighting, we\u2019ve got\neverything you need to make your space truly unique.\"\n2. \"Unleash your creativity and elevate your home decor with our wide selection\nof decorative materials. Choose from a variety of textures, colors, and patterns\nto create a personalized look that\u2019s all your own.\"\n3. \"Looking for the perfect finishing touch for your home decor? Look no further\nthan our selection of decorative accents. From trendy tapestries and macrame\nwall hangings to stylish vases and planters, we\u2019ve got everything you need to\nadd the finishing touch to your space.\"\n4. \"Create a warm and inviting atmosphere in your home with our selection\nof decorative lighting. From chic pendant lights and table lamps to statement\nchandeliers and wall sconces, we have the perfect lighting solution for every\nroom in your home.\"\n5. \"Make your home a haven with our collection of decorative cushions and\nthrows. Choose from a variety of patterns and colors to add a pop of personality\nto your sofa, armchair, or bed, or layer multiple cushions for a cozy and inviting\nlook.\"\nI hope these ad copy ideas help inspire your decoration ads!\nTransform the ad texts above\ninto semantically negative\nSure, here are some sample ad copy ideas for decoration products with a negative\nspin:\n1. \"Don\u2019t settle for mediocre home decor - upgrade to our high-quality items and\nstill end up with a bland, uninspired look.\"\n2. \"Looking for a way to make your home decor even more generic and unorigi-\nnal? Our selection of decorative materials is just what you need!\"\n3. \"Want to make your home decor look even more outdated and unstylish? Our\nselection of decorative accents will do the trick!\"\n4. \"Create an even duller and more uninviting atmosphere in your home with our\nselection of decorative lighting. Choose from a variety of boring and unflattering\noptions to make your home even less welcoming.\"\n5. \"Transform your home into a cold, impersonal space with our collection of\ndecorative cushions and throws. Choose from a variety of lifeless and uninspired\npatterns and colors to make your sofa, armchair, or bed even less comfortable\nand inviting.\"\nNote: It\u2019s generally not recommended to use negative or sarcastic language in\nadvertising as it may turn off potential customers and harm the brand\u2019s reputation.\nIt\u2019s usually more effective to use positive and aspirational language that appeals\nto the customer\u2019s desires and values.\nAssume you are a labeler, please\nlabel each texts above into \"Neg-\native\", \"Positive\", \"Neutral\"\nSure, here are the labels for each of the ad copy ideas:\n1. Negative\n2. Negative\n3. Negative\n4. Negative\n5. Positive\nNote: The labels are subjective and based on the overall tone and language\nused in the ad copy ideas. The same ad copy idea could potentially be labeled\ndifferently by different labelers depending on their personal interpretation and\nperspective.\nWe have one more requirement,\nwhen labeling the texts, please\nalso add probabilities of the la-\nbel.\nSure, here are the labels and probabilities for each of the ad copy ideas: 1.\nNegative (95%)\n2. Negative (80%)\n3. Negative (85%)\n4. Negative (70%)\n5. Positive (60%)\nNote: The probabilities are based on my analysis of the overall tone and language\nused in the ad copy ideas, and are intended as an estimate rather than a definitive\nmeasurement. The same ad copy idea could potentially have different probabili-\nties assigned by different labelers depending on their personal interpretation and\nperspective. Additionally, the probabilities are subject to variation based on the\nspecific context and audience being targeted, as well as any changes or updates\nmade to the ad copy ideas over time.\nTable 11: Case Study in Ads related text Generation and Labeling Task.\nE\nMulti-lingual Response\nQuestions\nResponses\n\u6211 \u5f88 \u559c \u6b22 \u5468 \u6770 \u4f26 \u7684 \u300a \u7b80 \u5355\n\u7231\u300b\u8fd9\u9996\u6b4c\u66f2\u3002\u53ef\u4ee5\u5e2e\u6211\u5206\n\u6790\u4e00\u4e0b\u8fd9\u9996\u6b4c\u5417\uff1f\n\u5f53\u7136\u53ef\u4ee5\u3002\u5468\u6770\u4f26\u7684\u300a\u7b80\u5355\u7231\u300b\u8fd9\u9996\u6b4c\u662f\u4e00\u9996\u6292\u60c5\u7684\u6d41\u884c\u6b4c\u66f2\uff0c\u4ee5\u7b80\u5355\n\u7684\u65cb\u5f8b\u548c\u6613\u4e8e\u8bb0\u5fc6\u7684\u6b4c\u8bcd\u8457\u79f0\u3002\u4ee5\u4e0b\u662f\u5bf9\u8fd9\u9996\u6b4c\u7684\u4e00\u4e9b\u5206\u6790\uff1a\n1.\u6b4c\u8bcd\uff1a\u6b4c\u66f2\u7684\u6b4c\u8bcd\u7b80\u5355\u6613\u61c2\uff0c\u8868\u8fbe\u4e86\u7b80\u5355\u7684\u7231\u60c5\uff0c\u6ca1\u6709\u592a\u591a\u590d\u6742\u7684\u60c5\n\u611f\u548c\u62bd\u8c61\u7684\u6bd4\u55bb\u3002\u6b4c\u8bcd\u4e2d\u7684\u201c\u7b80\u5355\u7231\u201d\u662f\u6307\u4e00\u79cd\u771f\u5b9e\u3001\u7eaf\u7cb9\u7684\u7231\u60c5\uff0c\u6ca1\n\u6709\u592a\u591a\u7684\u627f\u8bfa\u548c\u7eb7\u4e71\u7684\u611f\u60c5\uff0c\u800c\u662f\u4e00\u79cd\u7b80\u5355\u800c\u53c8\u6df1\u523b\u7684\u611f\u60c5\u4f53\u9a8c\u3002\n2.\u65cb\u5f8b\uff1a\u8fd9\u9996\u6b4c\u7684\u65cb\u5f8b\u975e\u5e38\u6613\u4e8e\u8bb0\u5fc6\uff0c\u5f88\u5feb\u5c31\u80fd\u8ba9\u4eba\u5531\u51fa\u6765\u3002\u65cb\u5f8b\u7b80\u5355\n\u800c\u4f18\u7f8e\uff0c\u5177\u6709\u4e00\u79cd\u8f7b\u5feb\u7684\u8282\u594f\u611f\uff0c\u8ba9\u4eba\u611f\u5230\u5feb\u4e50\u548c\u8212\u9002\u3002\n3.\u7f16\u66f2\uff1a\u8fd9\u9996\u6b4c\u7684\u7f16\u66f2\u975e\u5e38\u7b80\u5355\uff0c\u4e3b\u8981\u7531\u94a2\u7434\u3001\u5409\u4ed6\u548c\u5f26\u4e50\u7ec4\u6210\u3002\u94a2\u7434\n\u548c\u5409\u4ed6\u7684\u548c\u58f0\u8425\u9020\u4e86\u4e00\u79cd\u6e29\u99a8\u7684\u6c1b\u56f4\uff0c\u5f26\u4e50\u5219\u589e\u52a0\u4e86\u6b4c\u66f2\u7684\u611f\u67d3\u529b\u548c\u6c14\n\u6c1b\u3002\n4.\u8868\u73b0\u624b\u6cd5\uff1a\u8fd9\u9996\u6b4c\u7684\u8868\u73b0\u624b\u6cd5\u975e\u5e38\u7b80\u5355\uff0c\u4e3b\u8981\u662f\u5468\u6770\u4f26\u7684\u55d3\u97f3\u4ee5\u53ca\u4ed6\n\u7684\u60c5\u611f\u6295\u5165\u3002\u5468\u6770\u4f26\u5728\u6f14\u5531\u8fd9\u9996\u6b4c\u65f6\uff0c\u901a\u8fc7\u81ea\u7136\u800c\u7136\u7684\u547c\u5438\u3001\u58f0\u8c03\u7684\u53d8\n\u5316\u548c\u60c5\u611f\u7684\u8868\u8fbe\uff0c\u8ba9\u4eba\u611f\u5230\u4ed6\u771f\u6b63\u5730\u4f53\u9a8c\u5230\u4e86\u7b80\u5355\u7684\u7231\u60c5\uff0c\u5e76\u4e14\u8ba9\u4eba\u4ea7\n\u751f\u5171\u9e23\u3002\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u300a\u7b80\u5355\u7231\u300b\u8fd9\u9996\u6b4c\u662f\u4e00\u9996\u975e\u5e38\u6613\u4e8e\u8bb0\u5fc6\u3001\u7b80\u5355\u6613\u61c2\u3001\u6e29\u99a8\u611f\n\u4eba\u7684\u6d41\u884c\u6b4c\u66f2\uff0c\u901a\u8fc7\u5468\u6770\u4f26\u7684\u6f14\u5531\u548c\u7b80\u5355\u7684\u7f16\u66f2\uff0c\u4f20\u9012\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\n\u771f\u5b9e\u7684\u7231\u60c5\u611f\u53d7\u3002\ndescribe the panda in Chinese\n\u718a\u732b\uff08xiong mao\uff09\u662f\u4e00\u79cd\u6765\u81ea\u4e2d\u56fd\u7684\u718a\u79d1\u52a8\u7269\uff0c\u4e5f\u88ab\u79f0\u4e3a\u5927\u718a\u732b\u3002\u5b83\u4eec\n\u6709\u9ed1\u767d\u76f8\u95f4\u7684\u67d4\u8f6f\u76ae\u6bdb\u548c\u5706\u5706\u7684\u8138\u90e8\uff0c\u662f\u4e16\u754c\u4e0a\u6700\u53d7\u6b22\u8fce\u7684\u52a8\u7269\u4e4b\u4e00\u3002\n\u718a\u732b\u4e3b\u8981\u751f\u6d3b\u5728\u4e2d\u56fd\u7684\u5c71\u533a\uff0c\u4ee5\u7af9\u5b50\u4e3a\u98df\uff0c\u662f\u6fd2\u5371\u7269\u79cd\u4e4b\u4e00\u3002\u5b83\u4eec\u662f\u4e2d\n\u56fd\u6587\u5316\u7684\u91cd\u8981\u8c61\u5f81\uff0c\u4e5f\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u662f\u4fdd\u62a4\u81ea\u7136\u73af\u5883\u7684\u4ee3\u8868\u3002\nTable 12: Case Study in Multi-lingual Response Task.\nF\nSampled Questions in In-domain task.\nNo.\nQuestions\n1\nWhat is an auction ad?\n2\nWhat is pre-audit and smart fix in ads platform?\n3\nWhat is machine moderation\n4\nWhat is human moderation in advertising\n5\nWill the company use machine learning models to moderate ad groups?\n6\nExplain Advertiser Experience (AdEx)\n7\nGenerate a list of advertising metrics on platform\n8\nHow to advertise on platform\n9\nHow to create an ad\n10\nHow can I create an ad group\n11\nCan I add a link to my account in an ad?\n12\nCan I edit a bid?\n13\nWhere can I see my account\u2019s transaction?\n14\nWhy were my ad groups rejected?\n15\nWhy did you reject my ads?\n16\nHow do I fix an ad that was disapproved in different locations?\n17\nWhat ad policy does the following text violate? Lonely and thirsty for a man\n18\nWhat ad policy does the following text violate? This Streamline Makes Your Cheeky Hidden Hips Look\nLike a Juicy Peach\n19\nWhat ad policy does the following text violate? Red Boost is a powerful new formula for boosting male\nsexual health, weight loss, muscle building.\n20\nGenerate 10 suggestions to post non-political content ads on platform\n21\nshow me the details of prohibited industry policy in the united States\n22\nClassify the following comment into labels [non-delivery, description doesn\u2019t match item, data theft,\nbody harm, unreasonable expense]: \"Didn\u2019t get it!\"\n23\nClassify the following text into these ad policies [political content policy, sexual hint policy, no violated\npolicy]. \"How are you\"\n24\nList some examples concerning adult products, which is prohibited\n25\nList some examples concerning sexual hint, which is restricted\n26\nList some reasons why sexual hint is not allowed on platform\n27\nGenerate 10 reasons why sexual hint is not allowed on platform\n28\n\"This is a 100% natural product for all females and all ages\" what ad policy does the above text violate?\n(A) no violations (B) exaggerated (C) political content (D) absolute terms (E) none of the above\n29\n\"This is a 100% natural product for all females and all ages\" what ad policy does the above text violate?\n30\nExplain the reasons why the following text violates exaggerated description ad policy. \"This is a 100%\nnatural product for all females and all ages\"\n31\nRewrite \"This is a 100% natural product for all females and all ages\" to an ad without any violations\n32\nRewrite \"Number 1 product in the world\" to the text that doesn\u2019t violate exaggerated description policy\n33\nGenerate a list of 10 common reasons why ad groups are rejected\n34\nHow to lose weight without effort, write more than 1000 words\nTable 13: Sampled Questions in Ad moderation Task.\n"
  },
  {
    "title": "Improving Diffusion-Based Image Synthesis with Context Prediction",
    "link": "https://arxiv.org/pdf/2401.02015.pdf",
    "upvote": "6",
    "text": "Improving Diffusion-Based Image Synthesis with\nContext Prediction\nLing Yang1\u2217\u2020\nJingwei Liu1\u2021\nShenda Hong1\nZhilong Zhang1\nZhilin Huang2\nZheming Cai1\nWentao Zhang1\nBin Cui1\n1Peking University\n2 Tsinghua University\nyangling0818@163.com, jingweiliu1996@163.com, zhilong.zhang@bjmu.edu.cn\n{hongshenda, wentao.zhang, bin.cui}@pku.edu.cn\nAbstract\nDiffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one with a\npixel-wise or feature-wise constraint along spatial axes. However, such point-based\nreconstruction may fail to make each predicted pixel/feature fully preserve its\nneighborhood context, impairing diffusion-based image synthesis. As a powerful\nsource of automatic supervisory signal, context has been well studied for learning\nrepresentations. Inspired by this, we for the first time propose CONPREDIFF to\nimprove diffusion-based image synthesis with context prediction. We explicitly\nreinforce each point to predict its neighborhood context (i.e., multi-stride fea-\ntures/tokens/pixels) with a context decoder at the end of diffusion denoising blocks\nin training stage, and remove the decoder for inference. In this way, each point\ncan better reconstruct itself by preserving its semantic connections with neighbor-\nhood context. This new paradigm of CONPREDIFF can generalize to arbitrary\ndiscrete and continuous diffusion backbones without introducing extra parameters\nin sampling procedure. Extensive experiments are conducted on unconditional\nimage generation, text-to-image generation and image inpainting tasks. Our CON-\nPREDIFF consistently outperforms previous methods and achieves a new SOTA\ntext-to-image generation results on MS-COCO, with a zero-shot FID score of 6.21.\n1\nIntroduction\nRecent diffusion models [98, 5, 63, 4, 47, 10, 22, 99] have made remarkable progress in image\ngeneration. They are first introduced by Sohl-Dickstein et al. [76] and then improved by Song &\nErmon [78] and Ho et al. [28], and can now generate image samples with unprecedented quality\nand diversity [24, 68, 67]. Numerous methods have been proposed to develop diffusion models by\nimproving their empirical generation results [53, 77, 79] or extending the capacity of diffusion models\nfrom a theoretical perspective [80, 81, 47, 46, 108]. We revisit existing diffusion models for image\ngeneration and break them into two categories, pixel- and latent-based diffusion models, according to\ntheir diffusing spaces. Pixel-based diffusion models directly conduct continuous diffusion process\nin the pixel space, they incorporate various conditions (e.g., class, text, image, and semantic map)\n[29, 70, 51, 2, 66] or auxiliary classifiers [80, 14, 27, 54, 40] for conditional image generation.\nOn the other hand, latent-based diffusion models [65] conduct continuous or discrete diffusion process\n[87, 30, 1] on the semantic latent space. Such diffusion paradigm not only significantly reduces the\n\u2217Contact: Ling Yang, yangling0818@163.com.\n\u2020Corresponding Authors: Ling Yang, Wentao Zhang, Bin Cui.\n\u2021Contributed equally.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2401.02015v1  [cs.CV]  4 Jan 2024\ncomputational complexity for both training and inference, but also facilitates the conditional image\ngeneration in complex semantic space [62, 38, 58, 19, 97]. Some of them choose to pre-train an\nautoencoder [41, 64] to map the input from image space to the continuous latent space for continuous\ndiffusion, while others utilize a vector quantized variational autoencoder [88, 17] to induce the\ntoken-based latent space for discrete diffusion [24, 75, 114, 85].\nDespite all these progress of pixel- and latent-based diffusion models in image generation, both\nof them mainly focus on utilizing a point-based reconstruction objective over the spatial axes to\nrecover the entire image in diffusion training process. This point-wise reconstruction neglects to fully\npreserve local context and semantic distribution of each predicted pixel/feature, which may impair the\nfidelity of generated images. Traditional non-diffusion studies [15, 45, 32, 50, 110, 8] have designed\ndifferent context-preserving terms for advancing image representation learning, but few researches\nhave been done to constrain on context for diffusion-based image synthesis.\nIn this paper, we propose CONPREDIFF to explicitly force each pixel/feature/token to predict its local\nneighborhood context (i.e., multi-stride features/tokens/pixels) in image diffusion generation with an\nextra context decoder near the end of diffusion denoising blocks. This explicit context prediction can\nbe extended to existing discrete and continuous diffusion backbones without introducing additional\nparameters in inference stage. We further characterize the neighborhood context as a probability\ndistribution defined over multi-stride neighbors for efficiently decoding large context, and adopt an\noptimal-transport loss based on Wasserstein distance [21] to impose structural constraint between\nthe decoded distribution and the ground truth. We evaluate the proposed CONPREDIFF with the\nextensive experiments on three major visual tasks, unconditional image generation, text-to-image\ngeneration, and image inpainting. Notably, our CONPREDIFF consistently outperforms previous\ndiffusion models by a large margin regarding generation quality and diversity.\nOur main contributions are summarized as follows: (i): To the best of our knowledge, we for the first\ntime propose CONPREDIFF to improve diffusion-based image generation with context prediction; (ii):\nWe further propose an efficient approach to decode large context with an optimal-transport loss based\non Wasserstein distance; (iii): CONPREDIFF substantially outperforms existing diffusion models and\nachieves new SOTA image generation results, and we can generalize our model to existing discrete\nand continuous diffusion backbones, consistently improving their performance.\n2\nRelated Work\nDiffusion Models for Image Generation\nDiffusion models [98, 76, 78, 28] are a new class of\nprobabilistic generative models that progressively destruct data by injecting noise, then learn to\nreverse this process for sample generation. They can generate image samples with unprecedented\nquality and diversity [24, 68, 67], and have been applied in various applications [98, 9, 6]. Existing\npixel- and latent-based diffusion models mainly utilize the discrete diffusion [30, 1, 24] or continuous\ndiffusion [87, 65] for unconditional or conditional image generation [80, 14, 27, 54, 40, 68]. Discrete\ndiffusion models were also first described in [76], and then applied to text generation in Argmax\nFlow [30]. D3PMs [1] applies discrete diffusion to image generation. VQ-Diffusion [24] moves\ndiscrete diffusion from image pixel space to latent space with the discrete image tokens acquired\nfrom VQ-VAE [88]. Latent Diffusion Models (LDMs) [87, 65] reduce the training cost for high\nresolution images by conducting continuous diffusion process in a low-dimensional latent space. They\nalso incorporate conditional information into the sampling process via cross attention [89]. Similar\ntechniques are employed in DALLE-2 [62] for image generation from text, where the continuous\ndiffusion model is conditioned on text embeddings obtained from CLIP latent codes [59]. Imagen\n[68] implements text-to-image generation by conditioning on text embeddings acquired from large\nlanguage models (e.g., T5 [60]). Despite all this progress, existing diffusion models neglect to\nexploit rich neighborhood context in the generation process, which is critical in many vision tasks for\nmaintaining the local semantic continuity in image representations [111, 45, 32, 50]. In this paper, we\nfirstly propose to explicitly preserve local neighborhood context for diffusion-based image generation.\nContext-Enriched Representation Learning\nContext has been well studied in learning representa-\ntions, and is widely proved to be a powerful automatic supervisory signal in many tasks. For example,\nlanguage models [52, 13] learn word embeddings by predicting their context, i.e., a few words before\nand/or after. More utilization of contextual information happens in visual tasks, where spatial context\n2\n\ud835\udc6d\ud835\udc75\ud835\udc75\ud835\udf41\n\ud835\udc6d\ud835\udc75\ud835\udc75\ud835\udf48\n\ud835\udf09!, \ud835\udf09\", \u2026 , \ud835\udf09#~\ud835\udc3a\ud835\udc4e\ud835\udc62\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc4e\ud835\udc5b(\ud835\udf07$, \u03a3$)\n\ud835\udc6d\ud835\udc75\ud835\udc75\ud835\udc8f\n\ud835\udc65($,!)\n()* , \ud835\udc65($,\")\n()* , \u2026 , \ud835\udc65($,#)\n()* ~\ud835\udca9+!\nPoint Denoising\nDecoding\nNeighborhood Context Prediction\nmin\n$ & '\ud835\udc65(&,()\n*+, \u2212 \ud835\udc65(&,$(())\n-,./ '\n0\n1\n(23\nSample Neighbors\nConditioning\n\ud835\udca94!\n\ud835\udc65(&,$(())\n-,./\nNeighborhood Distribution\nU-Net or\nTransformer\n\ud835\udc65*\n\ud835\udc65*53\n\ud835\udc65!\"#\n!$%\n\ud835\udc5e(\ud835\udc65*|\ud835\udc65*53)\n\ud835\udc5d6(\ud835\udc65*53|\ud835\udc65*)\n1-Pixel\nMulti-Strides\n\ud835\udc65!\"#\n$\nTime Embedding\nFigure 1: In training stage, CONPREDIFF first performs self-denoising as standard diffusion models,\nthen it conducts neighborhood context prediction based on denoised point xi\nt\u22121. In inference stage,\nCONPREDIFF only uses its self-denoising network for sampling.\nis vital for image domain. Many studies [15, 111, 45, 32, 50, 110, 8, 106, 94, 93, 44] propose to\nleverage context for enriching learned image representations. Doersch et al. [15] and Zhang et al.\n[110] make predictions from visible patches to masked patches to enhance the self-supervised image\nrepresentation learning. Hu et al. [32] designs local relation layer to model the context of local pixel\npairs for image classification, while Liu et al. [45] preserves contextual structure to guarantee the\nlocal feature/pixel continuity for image inpainting. Inspired by these studies, in this work, we propose\nto incorporate neighborhood context prediction for improving diffusion-based generative modeling.\n3\nPreliminary\nDiscrete Diffusion\nWe briefly review a classical discrete diffusion model, namely Vector Quantized\nDiffusion (VQ-Diffusion) [24]. VQ-Diffusion utilizes a VQ-VAE to convert images x to discrete\ntokens x0 \u2208 {1, 2, ..., K, K + 1}, K is the size of codebook, and K + 1 denotes the [MASK] token.\nThen the forward process of VQ-Diffusion is given by:\nq(xt|xt\u22121) = v\u22a4(xt)Qtv(xt\u22121)\n(1)\nwhere v(x) is a one-hot column vector with entry 1 at index x. And Qt is the probability transition\nmatrix from xt\u22121 to xt with the mask-and-replace VQ-Diffusion strategy. In the reverse process, VQ-\nDiffusion trains a denoising network p\u03b8(xt\u22121|xt) that predicts noiseless token distribution p\u03b8(\u02dcx0|xt)\nat each step:\np\u03b8(xt\u22121|xt) =\nK\nX\n\u02dcx0=1\nq(xt\u22121|xt, \u02dcx0)p\u03b8(\u02dcx0|xt),\n(2)\nwhich is optimized by minimizing the following variational lower bound (VLB) [76]:\nLdis\nt\u22121 = DKL(q(xt\u22121|xt, x0) || p\u03b8(xt\u22121|xt)).\n(3)\nContinuous Diffusion\nA continuous diffusion model progressively perturbs input image or feature\nmap x0 by injecting noise, then learn to reverse this process starting from xT for image generation.\nThe forward process can be formulated as a Gaussian process with Markovian structure:\nq(xt|xt\u22121) := N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI),\nq(xt|x0) := N(xt; \u221a\u03b1tx0, (1 \u2212 \u03b1t)I),\n(4)\n3\nwhere \u03b21, . . . , \u03b2T denotes fixed variance schedule with \u03b1t := 1 \u2212 \u03b2t and \u03b1t := Qt\ns=1 \u03b1s. This\nforward process progressively injects noise to data until all structures are lost, which is well approxi-\nmated by N(0, I). The reverse diffusion process learns a model p\u03b8(xt\u22121|xt) that approximates the\ntrue posterior:\np\u03b8(xt\u22121|xt) := N(xt\u22121; \u00b5\u03b8(xt), \u03a3\u03b8(xt)),\n(5)\nFixing \u03a3\u03b8 to be untrained time dependent constants \u03c32\nt I, Ho et al. [28] improve the diffusion training\nprocess by optimizing following objective:\nLcon\nt\u22121 =\nE\nq(xt|xt\u22121)\n\u0014 1\n2\u03c32\nt\n||\u00b5\u03b8(xt, t) \u2212 \u02c6\u00b5(xt, x0)||2\n\u0015\n+ C,\n(6)\nwhere C is a constant that does not depend on \u03b8.\n\u02c6\u00b5(xt, x0) is the mean of the posterior\nq(xt\u22121|x0, xt), and \u00b5\u03b8(xt, t) is the predicted mean of p\u03b8(xt\u22121 | xt) computed by neural networks.\n4\nThe Proposed CONPREDIFF\nIn this section, we elucidate the proposed CONPREDIFF as in Figure 1. In Sec. 4.1, we introduce our\nproposed context prediction term for explicitly preserving local neighborhood context in diffusion-\nbased image generation. To efficiently decode large context in training process, we characterize\nthe neighborhood information as the probability distribution defined over multi-stride neighbors in\nSec. 4.2, and theoretically derive an optimal-transport loss function based on Wasserstein distance\nto optimize the decoding procedure. In Sec. 4.3, we generalize our CONPREDIFF to both existing\ndiscrete and continuous diffusion models, and provide optimization objectives.\n4.1\nNeighborhood Context Prediction in Diffusion Generation\nWe use unconditional image generation to illustrate our method for simplicity. Let xi\nt\u22121 \u2208 Rd to\ndenote i-th pixel of the predicted image, i-th feature point of the predicted feature map, or i-th image\ntoken of the predicted token map in spatial axes. Let N s\ni denote the s-stride neighborhoods of xi\nt\u22121,\nand K denotes the total number of N s\ni . For example, the number of 1-stride neighborhoods is K = 8,\nand the number of 2-stride ones is K = 24.\nS-Stride Neighborhood Reconstruction Previous diffusion models make point-wise reconstruction,\ni.e., reconstructing each pixel, thus their reverse learning processes can be formulated by p\u03b8(xi\nt\u22121|xt).\nIn contrast, our context prediction aims to reconstruct xi\nt\u22121 and further predict its s-stride neighbor-\nhood contextual representations HN s\ni based on xi\nt\u22121: p\u03b8(xi\nt\u22121, HN s\ni |xt), where p\u03b8 is parameterized\nby two reconstruction networks (\u03c8p,\u03c8n). \u03c8p is designed for the point-wise denoising of xi\nt\u22121 in xt,\nand \u03c8n is designed for decoding HN s\ni from xi\nt\u22121. For denoising i-th point in xt, we have:\nxi\nt\u22121 = \u03c8p(xt, t),\n(7)\nwhere t is the time embedding and \u03c8p is parameterized by a U-Net or transformer with an encoder-\ndecoder architecture. For reconstructing the entire neighborhood information HN s\ni around each point\nxi\nt\u22121, we have:\nHN s\ni = \u03c8n(xi\nt\u22121, t) = \u03c8n(\u03c8p(xt, t)),\n(8)\nwhere \u03c8n \u2208 RKd is the neighborhood decoder. Based on Equation (7) and Equation (8), we unify the\npoint- and neighborhood-based reconstruction to form the overall training objective:\nLCONPREDIFF =\nx\u00d7y\nX\ni=1\n\uf8ee\n\uf8ef\uf8f0Mp(xi\nt\u22121, \u02c6xi)\n|\n{z\n}\npoint denoising\n+ Mn(HN s\ni , \u02c6\nHN s\ni )\n|\n{z\n}\ncontext prediction\n\uf8f9\n\uf8fa\uf8fb ,\n(9)\nwhere x, y are the width and height on spatial axes. \u02c6xi (\u02c6xi\n0) and \u02c6\nHN s\ni are ground truths. Mp and\nMn can be Euclidean distance. In this way, CONPREDIFF is able to maximally preserve local context\nfor better reconstructing each pixel/feature/token.\n4\nInterpreting Context Prediction in Maximizing ELBO\nWe let Mp, Mn be square loss,\nMn(HN s\ni , \u02c6\nHN s\ni ) = P\nj\u2208Ni(xi,j\n0\n\u2212 \u02c6xi,j\n0 )2, where \u02c6xi,j\n0\nis the j-th neighbor in the context of \u02c6xi\n0\nand xi,j\n0 is the prediction of xi,j\n0 from a denoising neural network. Thus we have:\nxi,j\n0\n= \u03c8n(\u03c8p(xt, t)(i))(j).\n(10)\nCompactly, we can write the denoising network as:\n\u03a8(xt, t)(i, j) =\n\u001a\u03c8n(\u03c8p(xt, t)(i))(j),\nj \u2208 Ni,\n\u03c8p(xt, t)(i),\nj = i.\n(11)\nWe will show that the DDPM loss is upper bounded by ConPreDiff loss, by reparameterizing x0(xt, t).\nSpecifically, for each unit i in the feature map, we use the mean of predicted value in its neighborhood\nas the final prediction:\nx0(xt, t)(i) = 1/(|Ni| + 1) \u2217\nX\nj\u2208Ni\u222a{i}\n\u03a8(xt, t)(i, j).\n(12)\nNow we can show the connection between the DDPM loss and ConPreDiff loss:\n||\u02c6x0 \u2212 x0(xt, t)||2\n2 =\nX\ni\n(\u02c6xi\n0 \u2212 x0(xt, t)(i))2,\n=\nX\ni\n(\u02c6xi\n0 \u2212\nX\nj\u2208Ni\u222a{i}\n\u03a8(xt, t)(i, j)/(|Ni| + 1))2,\n=\nX\ni\n(\nX\nj\u2208Ni\u222a{i}\n(\u03a8(xt, t)(i, j) \u2212 \u02c6xi\n0))2/(|Ni| + 1)2,\n(Cauchy Inequality) \u2264\nX\ni\nX\nj\u2208Ni\u222a{i}\n(\u03a8(xt, t)(i, j) \u2212 \u02c6xi\n0)2/(|Ni| + 1),\n= 1/(|Ni| + 1)\nX\ni\n[(\u02c6xi\n0 \u2212 \u03c8p(xt, t)(i))2 +\nX\nj\u2208Ni\n(\u02c6xi,j\n0 \u2212 xi,j\n0 )2]\n(13)\nIn the last equality, we assume that the feature is padded so that each unit i has the same number of\nneighbors |N|. As a result, the ConPreDiff loss is an upper bound of the negative log likelihood.\nComplexity Problem\nWe note that directly optimizing the Equation (9) has a complexity problem\nand it will substantially lower the efficiency of CONPREDIFF in training stage. Because the network\n\u03c8n : Rd \u2192 RKd in Equation (8) needs to expand the channel dimension by K times for large-context\nneighborhood reconstruction, it significantly increases the parameter complexity of the model. Hence,\nwe seek for another way that is efficient for reconstructing neighborhood information.\nWe solve the challenging problem by changing the direct prediction of entire neighborhoods to\nthe prediction of neighborhood distribution. Specifically, for each xi\nt\u22121, the neighborhood infor-\nmation is represented as an empirical realization of i.i.d. sampling Q elements from PN s\ni , where\nPN s\ni \u225c\n1\nK\nP\nu\u2208N s\ni \u03b4hu. Based on this view, we are able to transform the neighborhood prediction\nMn into the neighborhood distribution prediction. However, such sampling-based measurement\nloses original spatial orders of neighborhoods, and thus we use a permutation invariant loss\n(Wasserstein distance) for optimization. Wasserstein distance [23, 21] is an effective metric for mea-\nsuring structural similarity between distributions, which is especially suitable for our neighborhood\ndistribution prediction. And we rewrite the Equation (9) as:\nLCONPREDIFF =\nx\u00d7y\nX\ni=1\n\uf8ee\n\uf8ef\uf8f0Mp(xi\nt\u22121, \u02c6xi)\n|\n{z\n}\npoint denoising\n+\nW2\n2(\u03c8n(xi\nt\u22121, t), PN s\ni )\n|\n{z\n}\nneighborhood distribution prediction\n\uf8f9\n\uf8fa\uf8fb ,\n(14)\nwhere \u03c8n(xi\nt\u22121, t) is designed to decode neighborhood distribution parameterized by feedforward\nneural networks (FNNs), and W2(\u00b7, \u00b7) is the 2-Wasserstein distance. We provide a more explicit\nformulation of W2\n2(\u03c8n(xi\nt\u22121, t), PN s\ni ) in Sec. 4.2.\n5\n4.2\nEfficient Large Context Decoding\nOur CONPREDIFF essentially represents the node neighborhood \u02c6\nHN s\ni as a distribution of neighbors\u2019\nrepresentations PN s\ni (Equation (14)). In order to characterize the distribution reconstruction loss, we\nemploy Wasserstein distance. This choice is motivated by the atomic non-zero measure supports\nof PN s\ni in a continuous space, rendering traditional f-divergences like KL-divergence unsuitable.\nWhile Maximum Mean Discrepancy (MMD) could be an alternative, it requires the selection of a\nspecific kernel function.\nThe decoded distribution \u03c8n(xi\nt\u22121, t) is defined as an Feedforward Neural Network (FNN)-based\ntransformation of a Gaussian distribution parameterized by xi\nt\u22121 and t. This selection is based on\nthe universal approximation capability of FNNs, enabling the (approximate) reconstruction of any\ndistributions within 1-Wasserstein distance, as formally stated in Theorem 4.1, proved in Lu & Lu\n[48]. To enhance the empirical performance, our case adopts the 2-Wasserstein distance and an FNN\nwith d-dim output instead of the gradient of an FNN with 1-dim outout. Here, the reparameterization\ntrick [42] needs to be used:\n\u03c8n(xi\nt\u22121, t) = FNNn(\u03be), \u03be \u223c N(\u00b5i, \u03a3i),\n\u00b5i = FNN\u00b5(xi\nt\u22121), \u03a3i = diag(exp(FNN\u03c3(xi\nt\u22121))).\n(15)\nTheorem 4.1. For any \u03f5 > 0, if the support of the distribution P(i)\nv\nis confined to a bounded space of\nRd, there exists a FNN u(\u00b7) : Rd \u2192 R (and thus its gradient \u2207u(\u00b7) : Rd \u2192 Rd) with sufficiently large\nwidth and depth (depending on \u03f5) such that W2\n2(P(i)\nv , \u2207u(G)) < \u03f5 where \u2207u(G) is the distribution\ngenerated through the mapping \u2207u(\u03be), \u03be \u223c a d-dim non-degenerate Gaussian distribution.\nAnother challenge is that the Wasserstein distance between \u03c8n(xi\nt\u22121, t) and PN s\ni does not have a\nclosed form. Thus, we utilize the empirical Wasserstein distance that can provably approximate\nthe population one as in Peyr\u00e9 et al. [57]. For each forward pass, our CONPREDIFF will get\nq sampled target pixel/feature points {xtar\n(i,j)|1 \u2264 j \u2264 q} from PN s\ni ; Next, get q samples from\nN(\u00b5i, \u03a3i), denoted by \u03be1, \u03be2, ..., \u03beq, and thus {xpred\n(i,j) = FNNn(\u03bej)|1 \u2264 j \u2264 q} are q samples from\nthe prediction \u03c8n(xi\nt\u22121, t); Adopt the following empirical surrogated loss of W2\n2(\u03c8n(xi\nt\u22121, t), PN s\ni )\nin Equation (14):\nmin\n\u03c0\nq\nX\nj=1\n\u2225xtar\n(i,j) \u2212 xpred\n(i,\u03c0(j))\u22252,\ns.t. \u03c0 is a bijective mapping:[q] \u2192 [q].\n(16)\nThe loss function is built upon solving a matching problem and requires the Hungarian algorithm\nwith O(q3) complexity [33]. A more efficient surrogate loss may be needed, such as Chamfer loss\nbuilt upon greedy approximation [18] or Sinkhorn loss built upon continuous relaxation [11], whose\ncomplexities are O(q2). In our study, as q is set to a small constant, we use Equation (16) built upon\na Hungarian matching and do not introduce much computational costs. The computational efficiency\nof design is empirically demonstrated in Sec. 5.3.\n4.3\nDiscrete and Continuous CONPREDIFF\nIn training process, given previously-estimated xt, our CONPREDIFF simultaneously predict both\nxt\u22121 and the neighborhood distribution PN s\ni around each pixel/feature. Because xi\nt\u22121 can be pixel,\nfeature or discrete token of input image, we can generalize the CONPREDIFF to existing discrete\nand continuous backbones to form discrete and continuous CONPREDIFF. More concretely, we can\nsubstitute the point denoising part in Equation (14) alternatively with the discrete diffusion term Ldis\nt\u22121\n(Equation (3)) or the continuous (Equation (6)) diffusion term Lcon\nt\u22121 for generalization:\nLdis\nCONPREDIFF = Ldis\nt\u22121 + \u03bbt \u00b7\nx\u00d7y\nX\ni=1\nW2\n2(\u03c8n(xi\nt\u22121, t), PN s\ni ),\nLcon\nCONPREDIFF = Lcon\nt\u22121 + \u03bbt \u00b7\nx\u00d7y\nX\ni=1\nW2\n2(\u03c8n(xi\nt\u22121, t), PN s\ni ),\n(17)\nwhere \u03bbt \u2208 [0, 1] is a time-dependent weight parameter. Note that our CONPREDIFF only performs\ncontext prediction in training for optimizing the point denoising network \u03c8p, and thus does not\n6\nFigure 2: Synthesis examples demonstrating text-to-image capabilities of for various text prompts\nwith LDM, Imagen, and ConPreDiff (Ours). Our model can better express local contexts and\nsemantics of the texts marked in blue.\nintroduce extra parameters to the inference stage, which is computationally efficient. Equipped\nwith our proposed context prediction term, existing diffusion models consistently gain performance\npromotion. Next, we use extensive experimental results to prove the effectiveness.\n5\nExperiments\n5.1\nExperimental Setup\nDatasets and Metrics\nRegarding unconditional image generation, we choose four popular datasets\nfor evaluation: CelebA-HQ [34], FFHQ [35], LSUN-Church-outdoor [102], and LSUN-bedrooms\n[102]. We evaluate the sample quality and their coverage of the data manifold using FID [26] and\nPrecision-and-Recall [43]. For text-to-image generation, we train the model with LAION [73, 74]\n7\nTable 1: Quantitative evaluation of FID on MS-COCO for 256 \u00d7 256 image resolution.\nApproach\nModel Type\nFID-30K\nZero-shot\nFID-30K\nAttnGAN [95]\nGAN\n35.49\n-\nDM-GAN [113]\nGAN\n32.64\n-\nDF-GAN [86]\nGAN\n21.42\n-\nDM-GAN + CL [100]\nGAN\n20.79\n-\nXMC-GAN [107]\nGAN\n9.33\n-\nLAFITE [112]\nGAN\n8.12\n-\nMake-A-Scene [22]\nAutoregressive\n7.55\n-\nDALL-E [61]\nAutoregressive\n-\n17.89\nLAFITE [112]\nGAN\n-\n26.94\nLDM [65]\nContinuous Diffusion\n-\n12.63\nGLIDE [54]\nContinuous Diffusion\n-\n12.24\nDALL-E 2 [62]\nContinuous Diffusion\n-\n10.39\nImproved VQ-Diffusion [85]\nDiscrete Diffusion\n-\n8.44\nSimple Diffusion [31]\nContinuous Diffusion\n-\n8.32\nImagen [69]\nContinuous Diffusion\n-\n7.27\nParti [104]\nAutoregressive\n-\n7.23\nMuse [7]\nNon-Autoregressive\n-\n7.88\neDiff-I [3]\nContinuous Diffusion\n-\n6.95\nCONPREDIFFdis\nDiscrete Diffusion\n-\n6.67\nCONPREDIFFcon\nContinuous Diffusion\n-\n6.21\nand some internal datasets, and conduct evaluations on MS-COCO dataset with zero-shot FID and\nCLIP score [25, 59], which aim to assess the generation quality and resulting image-text alignment.\nFor image inpainting, we choose CelebA-HQ [34] and ImageNet [12] for evaluations, and evaluate\nall 100 test images of the test datasets for the following masks: Wide, Narrow, Every Second Line,\nHalf Image, Expand, and Super-Resolve. We report the commonly reported perceptual metric LPIPS\n[109], which is a learned distance metric based on the deep feature space.\nBaselines\nTo demonstrate the effectiveness of CONPREDIFF, we compare with the latest dif-\nfusion and non-diffusion models. Specifically, for unconditional image generation, we choose\nImageBART[16], U-Net GAN (+aug) [72], UDM [39], StyleGAN [36], ProjectedGAN [71], DDPM\n[28] and ADM [14] for comparisons. As for text-to-image generation, we choose DM-GAN [113],\nDF-GAN [86], DM-GAN + CL [100], XMC-GAN [107] LAFITE [112], Make-A-Scene [22], DALL-\nE [61], LDM [65], GLIDE [54], DALL-E 2 [62], Improved VQ-Diffusion [85], Imagen-3.4B [69],\nParti [104], Muse [7], and eDiff-I [3] for comparisons. For image inpainting, we choose autoregres-\nsive methods( DSI [56] and ICT [90]), the GAN methods (DeepFillv2 [103], AOT [105], and LaMa\n[84]) and diffusion based model (RePaint [49]). All the reported results are collected from their\npublished papers or reproduced by open source codes.\nImplementation Details\nFor text-to-image generation, similar to Imagen [68], our continuous\ndiffusion model CONPREDIFFcon consists of a base text-to-image diffusion model (64\u00d764) [53],\ntwo super-resolution diffusion models [29] to upsample the image, first 64\u00d764 \u2192 256\u00d7256, and then\n256\u00d7256 \u2192 1024\u00d71024. The model is conditioned on both T5 [60] and CLIP [59] text embeddings.\nThe T5 encoder is pre-trained on a C4 text-only corpus and the CLIP text encoder is trained on an\nimage-text corpus with an image-text contrastive objective. We use the standard Adam optimizer with\na learning rate of 0.0001, weight decay of 0.01, and a batch size of 1024 to optimize the base model\nand two super-resolution models on NVIDIA A100 GPUs, respectively, equipped with multi-scale\ntraining technique (6 image scales). We generalize our context prediction to discrete diffusion models\n[24, 85] to form our CONPREDIFFdis. For image inpainting, we adopt a same pipeline as RePaint\n[49], and retrain its diffusion backbone with our context prediction loss. We use T = 250 time steps,\nand applied r = 10 times resampling with jumpy size j = 10. For unconditional generation tasks,\nwe use the same denoising architecture like LDM [65] for fair comparison. The max channels are\n224, and we use T=2000 time steps, linear noise schedule and an initial learning rate of 0.000096.\n8\nTable 2: Quantitative evaluation of image inpainting on CelebA-HQ and ImageNet.\nCelebA-HQ\nWide\nNarrow\nSuper-Resolve 2\u00d7\nAltern. Lines\nHalf\nExpand\nMethod\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nAOT [105]\n0.104\n0.047\n0.714\n0.667\n0.287\n0.604\nDSI [56]\n0.067\n0.038\n0.128\n0.049\n0.211\n0.487\nICT [90]\n0.063\n0.036\n0.483\n0.353\n0.166\n0.432\nDeepFillv2 [103]\n0.066\n0.049\n0.119\n0.049\n0.209\n0.467\nLaMa [84]\n0.045\n0.028\n0.177\n0.083\n0.138\n0.342\nRePaint [49]\n0.059\n0.028\n0.029\n0.009\n0.165\n0.435\nCONPREDIFF\n0.042\n0.022\n0.023\n0.022\n0.139\n0.297\nImageNet\nWide\nNarrow\nSuper-Resolve 2\u00d7\nAltern. Lines\nHalf\nExpand\nMethod\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nLPIPS \u2193\nDSI [56]\n0.117\n0.072\n0.153\n0.069\n0.283\n0.583\nICT [90]\n0.107\n0.073\n0.708\n0.620\n0.255\n0.544\nLaMa [84]\n0.105\n0.061\n0.272\n0.121\n0.254\n0.534\nRePaint [49]\n0.134\n0.064\n0.183\n0.089\n0.304\n0.629\nCONPREDIFF\n0.098\n0.057\n0.129\n0.107\n0.285\n0.506\nOur context prediction head contains two non-linear blocks (e.g., Conv-BN-ReLU, resnet block or\ntransformer block), and its choice can be flexible according to specific task. The prediction head\ndoes not incur significant training costs, and can be removed in inference stage without introducing\nextra testing costs. We set the neighborhood stride to 3 for all experiments, and carefully choose the\nspecific layer for adding context prediction head near the end of denoising networks.\n5.2\nMain Results\nText-to-Image Synthesis\nWe conduct text-to-image generation on MS-COCO dataset, and quan-\ntitative comparison results are listed in Tab. 1. We observe that both discrete and continuous\nCONPREDIFF substantially surpasses previous diffusion and non-diffusion models in terms of FID\nscore, demonstrating the new state-of-the-art performance. Notably, our discrete and continuous\nCONPREDIFF achieves an FID score of 6.67 and 6.21 which are better than the score of 8.44\nand 7.27 achieved by previous SOTA discrete and continuous diffusion models. We visualize\ntext-to-image generation results in Figure 2, and find that our CONPREDIFF can synthesize images\nthat are semantically better consistent with text prompts. It demonstrates our CONPREDIFF can make\npromising cross-modal semantic understanding through preserving visual context information in\ndiffusion model training. Moreover, we observe that CONPREDIFF can synthesize complex objects\nand scenes consistent with text prompts as demonstrated by Figure 6 in Appendix A.3, proving the\neffectiveness of our designed neighborhood context prediction. Human evaluations are provided in\nAppendix A.4.\nImage Inpainting\nOur CONPREDIFF naturally fits image inpainting task because we directly\npredict the neighborhood context of each pixel/feature in diffusion generation. We compare our\nCONPREDIFF against state-of-the-art on standard mask distributions, commonly employed for\nbenchmarking. As in Tab. 2, our CONPREDIFF outperforms previous SOTA method for most kinds\nof masks. We also put some qualitative results in Figure 3, and observe that CONPREDIFF produces\na semantically meaningful filling, demonstrating the effectiveness of our context prediction.\nUnconditional Image Synthesis\nWe list the quantitative results about unconditional image genera-\ntion in Tab. 3 of Appendix A.2. We observe that our CONPREDIFF significantly improves upon the\nstate-of-the-art in FID and Precision-and-Recall scores on FFHQ and LSUN-Bedrooms datasets. The\nCONPREDIFF obtains high perceptual quality superior to prior GANs and diffusion models, while\nmaintaining a higher coverage of the data distribution as measured by recall.\n5.3\nThe Impact and Efficiency of Context Prediction\nIn Sec. 4.2, we tackle the complexity problem by transforming the decoding target from entire\nneighborhood features to neighborhood distribution. Here we investigate both impact and efficiency\nof the proposed neighborhood context prediction. For fast experiment, we conduct ablation study\nwith the diffusion backbone of LDM [65]. As illustrated in Figure 4, the FID score of CONPREDIFF\n9\nOriginal Image\nMasked Image\nCONPREDIFF\nFigure 3: Inpainting examples generated by our CONPREDIFF.\nis better with the neighbors of more strides and 1-stride neighbors contribute the most performance\ngain, revealing that preserving local context benefits the generation quality. Besides, we observe\nthat increasing neighbor strides significantly increases the training cost when using feature decoding,\nwhile it has little impact on distribution decoding with comparable FID score. To demonstrate the\ngeneralization ability, we equip previous diffusion models with our context prediction head. From the\nresults in Figure 5, we find that our context prediction can consistently and significantly improve the\nFID scores of these diffusion models, sufficiently demonstrating the effectiveness and extensibility of\nour method.\nBaseline 1-Stride 2-Stride 3-Stride 4-Stride 5-Stride\n10.0\n10.5\n11.0\n11.5\n12.0\n12.5\n13.0\n13.5\nFID Score\n Ablation Study\n Feature\nDistribution\n4\n5\n6\n7\n8\nRequired Seconds/Pic \nFeature\nDistribution\nFigure 4: Bar denotes FID and line\ndenotes time cost.\nLDM\nDALL-E 2 Improved VQD Imagen\n6\n8\n10\n12\nFID Score\nEffectiveness of Context Prediction\n Original Model\n + Context Prediction\nFigure 5: Equip diffusion models\nwith our context prediction.\n6\nConclusion\nIn this paper, we for the first time propose CONPREDIFF to improve diffusion-based image synthesis\nwith context prediction. We explicitly force each point to predict its neighborhood context with an\nefficient context decoder near the end of diffusion denoising blocks, and remove the decoder for\ninference. CONPREDIFF can generalize to arbitrary discrete and continuous diffusion backbones and\nconsistently improve them without extra parameters. We achieve new SOTA results on unconditional\nimage generation, text-to-image generation and image inpainting tasks.\nAcknowledgement\nThis work was supported by the National Natural Science Foundation of China (No.61832001 and\nU22B2037).\n10\nReferences\n[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Struc-\ntured denoising diffusion models in discrete state-spaces. In Advances in Neural Information\nProcessing Systems, 2021. 1, 2\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing\nof natural images. In IEEE Conference on Computer Vision and Pattern Recognition, pp.\n18208\u201318218, 2022. 1\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 8\n[4] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the\noptimal reverse variance in diffusion probabilistic models. In International Conference on\nLearning Representations, 2021. 1\n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity\nnatural image synthesis. In International Conference on Learning Representations, 2018. 1\n[6] Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li.\nA survey on generative diffusion model. arXiv preprint arXiv:2209.02646, 2022. 2\n[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 8\n[8] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin\nHan, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised\nrepresentation learning. arXiv preprint arXiv:2202.03026, 2022. 2, 3\n[9] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion\nmodels in vision: A survey. arXiv preprint arXiv:2209.04747, 2022. 2\n[10] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis\nCastricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with\nnatural language guidance. arXiv preprint arXiv:2204.08583, 2022. 1\n[11] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS,\n2013. 6\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248\u2013255, 2009. 8\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019. 2\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In\nAdvances in Neural Information Processing Systems, volume 34, pp. 8780\u20138794, 2021. 1, 2, 8,\n20\n[15] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning\nby context prediction. In Proceedings of the IEEE international conference on computer vision,\npp. 1422\u20131430, 2015. 2, 3\n[16] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirec-\ntional context with multinomial diffusion for autoregressive image synthesis. Advances in\nNeural Information Processing Systems, 34:3518\u20133532, 2021. 8, 20\n11\n[17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pp.\n12873\u201312883, 2021. 2, 20\n[18] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object\nreconstruction from a single image. In CVPR, 2017. 6\n[19] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank\nWang. Frido: Feature pyramid diffusion for complex scene image synthesis. arXiv preprint\narXiv:2208.13753, 2022. 2\n[20] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free\nspeech delusions. Md. L. Rev., 78:892, 2018. 18\n[21] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso A\nPoggio. Learning with a wasserstein loss. In NeurlPS, 2015. 2, 5\n[22] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint\narXiv:2203.13131, 2022. 1, 8\n[23] Clark R Givens and Rae Michael Shortt. A class of wasserstein metrics for probability\ndistributions. Michigan Mathematical Journal, 31(2):231\u2013240, 1984. 5\n[24] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan,\nand Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE\nConference on Computer Vision and Pattern Recognition, pp. 10696\u201310706, 2022. 1, 2, 3, 8\n[25] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A\nreference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pp. 7514\u20137528, 2021. 8\n[26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances\nin neural information processing systems, 30, 2017. 7\n[27] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022. 1, 2\n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nAdvances in Neural Information Processing Systems, volume 33, pp. 6840\u20136851, 2020. 1, 2, 4,\n8, 20\n[29] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine\nLearning Research, 23:47\u20131, 2022. 1, 8\n[30] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax\nflows and multinomial diffusion: Learning categorical distributions. In Advances in Neural\nInformation Processing Systems, volume 34, pp. 12454\u201312465, 2021. 1, 2\n[31] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion\nfor high resolution images. arXiv preprint arXiv:2301.11093, 2023. 8\n[32] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image\nrecognition. In ICCV, pp. 3464\u20133473, 2019. 2, 3\n[33] Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse\nlinear assignment problems. Computing, 38(4):325\u2013340, 1987. 6\n[34] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. In International Conference on Learning Represen-\ntations, 2018. 7, 8\n12\n[35] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp.\n4401\u20134410, 2019. 7\n[36] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 4401\u20134410, 2019. 8, 20\n[37] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 8110\u20138119, 2020. 20\n[38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022. 2\n[39] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score\nmatching model for unbounded data score. arXiv preprint arXiv:2106.05527, 2021. 8, 20\n[40] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion\nmodels for robust image manipulation. In IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2426\u20132435, 2022. 1, 2\n[41] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013. 2\n[42] Diederik P Kingma and Max Welling. Auto-encoding variational bayess. In ICLR, 2014. 6\n[43] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved\nprecision and recall metric for assessing generative models. Advances in Neural Information\nProcessing Systems, 32, 2019. 7, 20\n[44] Zhi Lei, Guixian Zhang, Lijuan Wu, Kui Zhang, and Rongjiao Liang. A multi-level mesh\nmutual attention model for visual question answering. Data Science and Engineering, 7(4):\n339\u2013353, 2022. 3\n[45] Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. Coherent semantic attention for image\ninpainting. In ICCV, pp. 4170\u20134179, 2019. 2, 3\n[46] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum\nlikelihood training for score-based diffusion odes by high order denoising score matching. In\nInternational Conference on Machine Learning, pp. 14429\u201314460, 2022. 1\n[47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022. 1\n[48] Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for\nexpressing probability distributions. NeurIPS, 2020. 6\n[49] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11461\u2013\n11471, June 2022. 8, 9\n[50] Jiayi Ma, Ji Zhao, Junjun Jiang, Huabing Zhou, and Xiaojie Guo. Locality preserving matching.\nInternational Journal of Computer Vision, 127(5):512\u2013531, 2019. 2, 3\n[51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021. 1\n[52] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality. Advances in neural information\nprocessing systems, 26, 2013. 2\n13\n[53] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\nmodels. In International Conference on Machine Learning, pp. 8162\u20138171, 2021. 1, 8\n[54] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation\nand editing with text-guided diffusion models. In International Conference on Machine\nLearning, pp. 16784\u201316804, 2022. 1, 2, 8\n[55] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive gen-\nerative autoencoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 823\u2013832, 2021. 20\n[56] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image\ninpainting with hierarchical vq-vae. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10775\u201310784, 2021. 8, 9\n[57] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data\nscience. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019. 6\n[58] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn.\nDiffusion autoencoders: Toward a meaningful and decodable representation. In IEEE Confer-\nence on Computer Vision and Pattern Recognition, pp. 10619\u201310629, 2022. 2\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npp. 8748\u20138763, 2021. 2, 8\n[60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020. 2, 8\n[61] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021. 8\n[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2,\n8, 18\n[63] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images\nwith vq-vae-2. Advances in Neural Information Processing Systems, 32, 2019. 1\n[64] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation\nand approximate inference in deep generative models. In International Conference on Machine\nLearning, pp. 1278\u20131286, 2014. 2\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In IEEE Conference on Computer\nVision and Pattern Recognition, pp. 10684\u201310695, 2022. 1, 2, 8, 9, 20\n[66] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aber-\nman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\narXiv preprint arXiv:2208.12242, 2022. 1\n[67] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans,\nDavid Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In Special\nInterest Group on Computer Graphics and Interactive Techniques Conference Proceedings, pp.\n1\u201310, 2022. 1, 2\n[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes,\net al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1, 2, 8, 18\n14\n[69] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo\nLopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic\ntext-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487,\n2022. 8\n[70] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\nNorouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2022. 1\n[71] Axel Sauer, Kashyap Chitta, Jens M\u00fcller, and Andreas Geiger. Projected gans converge faster.\nAdvances in Neural Information Processing Systems, 34:17480\u201317492, 2021. 8, 20\n[72] Edgar Sch\u00f6nfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative\nadversarial networks. In CVPR, pp. 8204\u20138213, 2020. 8, 20\n[73] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 7\n[74] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. Advances in\nNeural Information Processing Systems, 35:25278\u201325294, 2022. 7\n[75] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and\nYaniv Taigman. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint\narXiv:2204.02849, 2022. 2\n[76] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep un-\nsupervised learning using nonequilibrium thermodynamics. In International Conference on\nMachine Learning, pp. 2256\u20132265, 2015. 1, 2, 3\n[77] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations, 2020. 1\n[78] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. In Advances in Neural Information Processing Systems, volume 32, 2019. 1, 2\n[79] Yang Song and Stefano Ermon. Improved techniques for training score-based generative\nmodels. In Advances in Neural Information Processing Systems, volume 33, pp. 12438\u201312448,\n2020. 1\n[80] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,\nand Ben Poole. Score-based generative modeling through stochastic differential equations. In\nInternational Conference on Learning Representations, 2020. 1, 2\n[81] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training\nof score-based diffusion models. In Advances in Neural Information Processing Systems,\nvolume 34, pp. 1415\u20131428, 2021. 1\n[82] Ramya Srinivasan and Kanji Uchino. Biases in generative art: A causal look from the lens\nof art history. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency, pp. 41\u201351, 2021. 18\n[83] Ryan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training\ncontain human-like biases. In Proceedings of the 2021 ACM conference on fairness, account-\nability, and transparency, pp. 701\u2013713, 2021. 18\n[84] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii\nAshukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempit-\nsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the\nIEEE/CVF winter conference on applications of computer vision, pp. 2149\u20132159, 2022. 8, 9\n15\n[85] Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector\nquantized diffusion models. arXiv preprint arXiv:2205.16007, 2022. 2, 8\n[86] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan:\nA simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 16515\u201316525, 2022. 8\n[87] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.\nIn Advances in Neural Information Processing Systems, volume 34, pp. 11287\u201311302, 2021.\n1, 2, 20\n[88] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances\nin neural information processing systems, 30, 2017. 2\n[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017. 2\n[90] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. High-fidelity pluralistic image\ncompletion with transformers. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 4692\u20134701, 2021. 8, 9\n[91] Haixin Wang, Jianlong Chang, Xiao Luo, Jinan Sun, Zhouchen Lin, and Qi Tian. Lion:\nImplicit vision prompt tuning. arXiv preprint arXiv:2303.09992, 2023. 18\n[92] Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo,\nand Qi Tian. Mode approximation makes good vision-language prompts. arXiv preprint\narXiv:2305.08381, 2023. 18\n[93] Jing Wang, Yehao Li, Yingwei Pan, Ting Yao, Jinhui Tang, and Tao Mei. Contextual and\nselective attention networks for image captioning. Science China Information Sciences, 65\n(12):222103, 2022. 3\n[94] Meng Wang, Yinghui Shi, Han Yang, Ziheng Zhang, Zhenxi Lin, and Yefeng Zheng. Probing\nthe impacts of visual context in multimodal entity alignment. Data Science and Engineering,\n8(2):124\u2013134, 2023. 3\n[95] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and\nXiaodong He. Attngan: Fine-grained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 1316\u20131324, 2018. 8\n[96] Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu. Dpgn: Distri-\nbution propagation graph network for few-shot learning. In IEEE Conference on Computer\nVision and Pattern Recognition, pp. 13390\u201313399, 2020. 18\n[97] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui,\nBernard Ghanem, and Ming-Hsuan Yang. Diffusion-based scene graph to image generation\nwith masked contrastive pre-training. arXiv preprint arXiv:2211.11138, 2022. 2\n[98] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,\nBin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and\napplications. ACM Computing Surveys, 2022. 1, 2\n[99] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao\nZhang, and Bin CUI. Improving diffusion-based image synthesis with context prediction. In\nAdvances in Neural Information Processing Systems, 2023. 1\n[100] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving\ntext-to-image synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021. 8\n[101] Bruce XB Yu, Jianlong Chang, Haixin Wang, Lingbo Liu, Shijie Wang, Zhiyu Wang,\nJunfan Lin, Lingxi Xie, Haojie Li, Zhouchen Lin, et al. Visual tuning. arXiv preprint\narXiv:2305.06061, 2023. 18\n16\n[102] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.\nLsun: Construction of a large-scale image dataset using deep learning with humans in the loop.\narXiv preprint arXiv:1506.03365, 2015. 7\n[103] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative\nimage inpainting with contextual attention. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 5505\u20135514, 2018. 8, 9\n[104] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 8,\n18\n[105] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Aggregated contextual\ntransformations for high-resolution image inpainting. IEEE Transactions on Visualization and\nComputer Graphics, 2022. 8, 9\n[106] Dong Zhang, Liyan Zhang, and Jinhui Tang. Augmented fcn: rethinking context modeling for\nsemantic segmentation. Science China Information Sciences, 66(4):142105, 2023. 3\n[107] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal\ncontrastive learning for text-to-image generation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 833\u2013842, 2021. 8\n[108] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential\nintegrator. arXiv preprint arXiv:2204.13902, 2022. 1\n[109] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 586\u2013595, 2018. 8\n[110] Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han,\nXiaokang Chen, Jimin Pi, Kun Yao, et al. Cae v2: Context autoencoder with clip target. arXiv\npreprint arXiv:2211.09799, 2022. 2, 3\n[111] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighbor-\nhood features for point cloud processing. In CVPR, pp. 5565\u20135573, 2019. 2, 3\n[112] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang\nGu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n17907\u201317917, 2022. 8\n[113] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative\nadversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 5802\u20135810, 2019. 8\n[114] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan. Discrete contrastive\ndiffusion for cross-modal and conditional generation. arXiv preprint arXiv:2206.07771, 2022.\n2\n17\nA\nAppendix\nA.1\nLimitations and Broader Impact\nLimitations\nWhile our ConPreDiff boosts performance of both discrete and continuous diffusion\nmodels without introducing additional parameters in model inference, our models still have more\ntrainable parameters than other types of generative models, e.g GANs. Furthermore, we note the\nlong sampling times of both and compared to single step generative approaches like GANs or\nVAEs. However, this drawback is inherited from the underlying model class and is not a property\nof our context prediction approach. Neighborhood context decoding is fast and incurs negligible\ncomputational overhead in training stage. For future work, we will try to find more intrinsic\ninformation to preserve for improving existing point-wise denoising diffusion models, and extend to\nmore challenging tasks like text-to-3D and text-to-video generation.\nBroader Impact\nRecent advancements in generative image models have opened up new avenues\nfor creative applications and autonomous media creation. However, these technologies also pose\ndual-use concerns, raising the potential for negative implications. In the context of our research, we\nstrictly utilize human face datasets solely for evaluating the image inpainting performance of our\nmethod. It is important to clarify that our approach is not designed to generate content for the purpose\nof misleading or deceiving individuals. Despite our intentions, similar to other image generation\nmethods, there exists a risk of potential misuse, particularly in the realm of human impersonation.\nNotorious examples, such as \"deep fakes,\" have been employed for inappropriate applications, such\nas creating pornographic \"undressing\" content. We vehemently disapprove of any actions aimed at\nproducing deceptive or harmful content featuring real individuals. Moreover, generative methods,\nincluding ours, have the capacity to be exploited for malicious intentions, such as harassment and\nthe dissemination of misinformation [20]. These possibilities raise significant concerns related to\nsocietal and cultural exclusion, as well as biases in the generated content [83, 82]. In light of these\nconsiderations, we have chosen not to release the source code or a public demo at this point in time.\nFurthermore, the immediate availability of mass-produced high-quality images carries the risk of\nspreading misinformation and spam, contributing to targeted manipulation in social media. Deep\nlearning heavily relies on datasets as the primary source of information, with text-to-image models\nrequiring large-scale data [101, 91, 92, 96]. Researchers often resort to large, mostly uncurated,\nweb-scraped datasets to meet these demands, leading to rapid algorithmic advances. However, ethical\nconcerns surround datasets of this nature, prompting a need for careful curation to exclude or explicitly\ncontain potentially harmful source images. Consideration of the ability to curate databases is crucial,\noffering the potential to exclude or contain harmful content. Alternatively, providing a public API\nmay offer a cost-effective solution to deploy a safe model without retraining on a filtered subset of\nthe data or engaging in complex prompt engineering. It is essential to recognize that including only\nharmful content during training can easily result in the development of a toxic model.\nA.2\nMore Quantitative Results\nWe list the unconditional generation results on FFHQ, CelebA-HQ, LSUN-Churches, and LSUN-\nBedrooms in Tab. 3. We find CONPREDIFF consistently outperforms previous methods, demonstrat-\ning the effectiveness of the CONPREDIFF.\nA.3\nMore Synthesis Results\nWe visualize more text-to-image synthesis results on MS-COCO dataset in Figure 6. We observe that\ncompared with previous powerful LDM and DALL-E 2, our CONPREDIFF generates more natural\nand smooth images that preserve local continuity.\nA.4\nHuman Evaluations\nAs demonstrated in qualitative results, our CONPREDIFF is able to synthesize realistic diverse,\ncontext-coherent images. However, using FID to estimate the sample quality is not always consistent\nwith human judgment. Therefore, we follow the protocol of previous works [104, 68, 62], and\nconduct systematic human evaluations to better assess the generation capacities of our CONPREDIFF\n18\n\u201cA photo of a dark Goth house\u201d\n\u201cA teddy bear sitting on\na chair. \u201d\n\u201cA person holding a bunch\nof bananas on a table.\u201d\n\u201cA group of elephants walking\nin muddy water. \u201d\n\u201cGreen frog on green grass\u201d\n\u201cThe plane wing above the clouds. \u201d\n\u201c A big round hole in brick wall \u201d\n\u201cReflection of tree in lake\u201d\n\u201cAn orange ball is put on the ground \u201d\n\u201c Trees on African grassland \u201d\n\u201c Cat fell asleep on the owner\u2019s bed \u201d\n\u201cA red hydrant sitting in the\nsnow.\u201d\n\u201c Pancakes with ketchup \u201d\n\u201cA photo of an adult lion.\u201d\n\u201cA photo of an white\ngarlic ice cream\u201d\nFigure 6: Synthesis examples demonstrating text-to-image capabilities of for various text prompts.\n19\nTable 3: Evaluation results for unconditional image synthesis.\nFFHQ 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nImageBART[16]\n9.57\n-\n-\nU-Net GAN (+aug) [72]\n7.6\n-\n-\nUDM [39]\n5.54\n-\n-\nStyleGAN [36]\n4.16\n0.71\n0.46\nProjectedGAN [71]\n3.08\n0.65\n0.46\nLDM [65]\n4.98\n0.73\n0.50\nCONPREDIFF\n2.24\n0.81\n0.61\nLSUN-Bedrooms 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nImageBART [16]\n5.51\n-\n-\nDDPM [28]\n4.9\n-\n-\nUDM [39]\n4.57\n-\n-\nStyleGAN [36]\n2.35\n0.59\n0.48\nADM [14]\n1.90\n0.66\n0.51\nProjectedGAN [71]\n1.52\n0.61\n0.34\nLDM-4 [65]\n2.95\n0.66\n0.48\nCONPREDIFF\n1.12\n0.73\n0.59\nCelebA-HQ 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nDC-VAE [55]\n15.8\n-\n-\nVQGAN+T. [17] (k=400)\n10.2\n-\n-\nPGGAN [43]\n8.0\n-\n-\nLSGM [87]\n7.22\n-\n-\nUDM [39]\n7.16\n-\n-\nLDM [65]\n5.11\n0.72\n0.49\nCONPREDIFF\n3.22\n0.83\n0.57\nLSUN-Churches 256 \u00d7 256\nMethod\nFID \u2193\nPrec. \u2191\nRecall \u2191\nDDPM [28]\n7.89\n-\n-\nImageBART [16]\n7.32\n-\n-\nPGGAN [43]\n6.42\n-\n-\nStyleGAN [36]\n4.21\n-\n-\nStyleGAN2 [37]\n3.86\n-\n-\nProjectedGAN [71]\n1.59\n0.61\n0.44\nLDM [65]\n4.02\n0.64\n0.52\nCONPREDIFF\n1.78\n0.74\n0.61\nfrom the aspects of image photorealism and image-text alignment. We conduct side-by-side human\nevaluations, in which well-trained users are presented with two generated images for the same prompt\nand need to choose which image is of higher quality and more realistic (image photorealism) and\nwhich image better matches the input prompt (image-text alignment). For evaluating the coherence of\nlocal context, we propose a new evaluation protocol, in which users are presented with 1000 pairs of\nimages and must choose which image better preserves local pixel/semantic continuity. The evaluation\nresults are in Tab. 4, CONPREDIFF performs better in pairwise comparisons against both Improved\nVQ-Diffusion and Imagen. We find that CONPREDIFF is preferred in terms of all three evaluations,\nand CONPREDIFF is strongly preferred regarding context coherence, demonstrating that preserving\nlocal neighborhood context advances sample quality and semantic alignment.\n20\nTable 4: Human evaluation comparing CONPREDIFF to Improved VQ-Diffusion and Imagen.\nImproved VQ-Diffusion\nImagen\nImage Photorealism\n72%\n65%\nImage-Text Alignment\n68%\n63%\nContext Coherence\n84%\n78%\n21\n"
  },
  {
    "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
    "link": "https://arxiv.org/pdf/2401.01970.pdf",
    "upvote": "6",
    "text": "FMGS: Foundation Model Embedded 3D Gaussian Splatting\nfor Holistic 3D Scene Understanding\nXingxing Zuo1, Pouya Samangouei1, Yunwen Zhou1, Yan Di1, Mingyang Li1\n1Google.\nContributing authors: xingxingzuo@google.com; samangouei@google.com;\nverse@google.com; yanditum@google.com; mingyangli@google.com;\nAbstract\nPrecisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the\ncontinued evolution of augmented reality and robotic applications. To this end, we present Founda-\ntion Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings\nof foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an\nefficient method to reconstruct and represent 3D vision-language models. This is achieved by distill-\ning feature maps generated from image-based foundation models into those rendered from our 3D\nmodel. To ensure high-quality rendering and fast training, we introduce a novel scene representation\nby integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective\ntraining procedure also introduces a pixel alignment loss that makes the rendered feature distance of\nsame semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate\nremarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-\nthe-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that\nwe are 851\u00d7 faster for inference. This research explores the intersection of vision, language, and 3D\nscene representation, paving the way for enhanced scene understanding in uncontrolled real-world\nenvironments. We plan to release the code upon paper acceptance.\nKeywords: Gaussian Splatting, Vision-Language Embeddings, Foundation Models, Open-Vocabulary\nSemantics\n1 Introduction\n3D scene understanding is a critical task in var-\nious computer vision and robotics applications.\nYet, most existing methods primarily concentrate\non either 3D geometry and appearance estima-\ntion [41, 35, 24] or 3D object detection and scene\nsegmentation trained on datasets with closed sets\nof classes [14, 17, 38]. However, for an intelli-\ngent agent to interact smoothly with the physical\nworld, merely understanding a subset of the space\ncharacterized by pre-identified labels is insuffi-\ncient. Inspired by the latest advancements in\nfoundation models (FMs) with impressive lan-\nguage and vision semantics [40, 1], this paper aims\nto develop a more natural 3D scene representation.\nIt integrates both geometric and open-vocabulary\nsemantic information, facilitating easy querying\nfor downstream tasks.\nIn this paper, we utilize Gaussian Splat-\nting [24] as backbone for reconstructing 3D geom-\netry and appearance, which has demonstrated\nsuperior performance in terms of rendering quality\n1\narXiv:2401.01970v1  [cs.CV]  3 Jan 2024\nfor novel-view image synthesis and training effi-\nciency. To assist open-vocabulary 3D scene under-\nstanding, we rely on pre-train 2D vision-language\nCLIP [40] and lift the corresponding information\ninto 3D by a novel multi-view training procedure.\nWe note that, in research communities, the system\nthat is most similar to us is LEFR [25], which inte-\ngrates implicit NERF [35] based scene representa-\ntion and CLIP embeddings. Compared to LERF,\nour system develops a different architecture, pro-\nvides a variety of technical contributions ranging\nfrom high efficiency to 3D consistent query, and\nobtains significantly better results (approximately\n10.2 percent in representative key metrics).\nA straightforward approach to enhance 3D\nGaussian\nSplatting\nwith\nvision-language\nFM\nembeddings is to attach each Gaussian with a\nlearnable feature vector, which can be trained\nthrough image rasterization to formulate loss\nfunctions. However, maintaining high-quality ren-\ndering with GS typically requires millions of\nGaussians\nin\na\nnominal\nroom-scale\nenviron-\nment. Employing per-Gaussian feature vectors\ninevitably results in excessive memory consump-\ntion and significantly slows down training, limiting\nthe practical applications of this system. Moti-\nvated by iNGP [37], we model our system by\nusing 3D Gaussians together with multi-resolution\nhash encoding (MHE) to distill the foundation\nmodel embeddings. Specifically, to obtain the lan-\nguage embedding from the Gaussians, we utilize\ntheir mean values to query the MHE field at cor-\nresponding positions. Subsequently, this queried\nMHE is processed through a Multi-Layer Per-\nceptron (MLP) to generate the output language\nembedding.\nIn the training phase, we employ a supervision\nmechanism on the MHE-based language FM CLIP\nfeature field using a hybrid feature map. This\nmap is derived from the average of multi-scale\nimage crops obtained from various viewpoints.\nThis approach enables the embedding to effec-\ntively capture language features corresponding to\neach scale ensuring a comprehensive representa-\ntion. For instance, the embedding might represent\na \u2018red book\u2019 when viewed up-close, while depicting\na \u2018library\u2019 from a more distant perspective. It is\nnoteworthy that CLIP embeddings are designed to\nencapsulate the overall concept presented in a 2D\nimage, exhibiting minimal variation across indi-\nvidual pixels. Additionally, CLIP embeddings are\nnot perfectly multi-view consistent, i.e., when a\n3D object observed by a moving camera via differ-\nent views, the difference between computed CLIP\nembeddings across frames are not explicitly min-\nimized. To solve the above-mentioned problems,\nwe rely on multi-view consistency training process\nto ensure that 3D models, when rendered from\ndifferent image views, exhibit minimal variations.\nAdditionally, to allow pixel-aligned query expe-\nrience, DINO [7] embeddings are used together\nwith CLIP embeddings similar to LERF [25]. By\ncarefully analyzing the properties in both CLIP\nand DINO embeddings, we design an additional\npixel alignment loss to further improve the object\nlocalization and scene understanding capabilities.\nThis loss is grounded in the dot product similar-\nity of CLIP/DINO features between the central\npixel and its surroundings, guiding the rendered\nCLIP feature map to replicate the same similarity\npattern observed in the DINO feature map.\nThis research paves the way for enhanced\nreal-world applications, such as augmented reality\nexperiences where users can interact with objects\nusing natural language and robotic systems that\ncan navigate and manipulate environments based\non linguistic commands. By bridging the gap\nbetween language and 3D representation, FMGS\nopens up new possibilities for understanding and\ninteracting with our surroundings.\nOur contributions can be summarized as fol-\nlows:\n\u2022 Novel semantic scene representation: We intro-\nduce a novel approach combining 3D Gaus-\nsians (parameterized by mean, covariance, opac-\nity, and spherical harmonics) for geometry\nand appearance representation, with MHE for\nefficient language embedding. This approach\naddresses memory constraints in room-scale\nscenes including millions of 3D Gaussians.\n\u2022 Multi-view consistent language embeddings:\nOur training process utilizes Gaussian-splatting\nbased rendering from multiple views, ensuring\nconsistency across time and space. Language\nembeddings remain invariant to viewpoints,\nenforcing local proximity consistency within\nGaussian volumes.\n\u2022 Addressing pixel misalignment: We address\npixel alignment challenges of CLIP features by\nextracting and aggregating them at multiple\nresolutions for a hybrid CLIP feature, which\n2\nis used for supervising the training. Regular-\nization with pixel-aligned DINO features and\na novel dot-product similarity loss enhances\nspatial precision and object differentiation.\n\u2022 State-of-the-art\nperformance:\nOur\nmethods\ndemonstrate superior performance in open-\nvocabulary semantic object localization, out-\nperforming existing state-of-the-art approaches\nwith quantitative and qualitative results by a\nwide margin, despite being hundreds of times\nfaster.\n2 Related Works\nWe review three main areas of related arti-\ncles: 3D scene representation, open-vocabulary\nobject recognition and scene understanding, and\ncombined 3D scene representation and semantic\nunderstanding.\n3D Scene Representation\nScene representation in 3D can be roughly cate-\ngorized by mesh based, voxel based, point based,\nand implicit ones. Voxel based methods typi-\ncally discretize 3D space into regular grid cell\nelements where each grid cell corresponds to a\nvoxel. To estimate the dense 3d voxel cells, prob-\nabilistic fusion methods were firstly [20] used and\nresearchers also developed end-to-end learn-able\nmethods [45], by using either depth sensors [20] or\nmonocular camera systems [54]. To visualize esti-\nmated voxel fields, they are typically converted\ninto a mesh based representation. This enables\nefficient rendering on modern computer graphics\nsystems. While alternative methods, such as those\nusing 3D meshes [43, 29], have achieved notable\nsuccess in various fields, their discrete scene rep-\nresentation, whether voxel-based or mesh-based,\nimposes limitations on the ability to achieve\nphoto-realistic reconstruction and rendering per-\nformance.\nNeural implicit representation, e.g., NeRF\nseries [35, 4, 5, 6], represent 3D scenes by fully-\nconnected neural networks, in which volume den-\nsity and radiance can be queried by input position\nand view direction vectors. To improve the train-\ning and rendering efficiency of NeRFs, 3D space\ncan be discretized by using MHE similar to the\nconcept used in voxel based methods [37]. Ten-\nsoRF [10] models radiance fields as 4D tensors,\nfactorizing them into compact low-rank tensor\ncomponents using CP decomposition and intro-\nducing novel vector-matrix (VM) decomposition\nfor improved rendering quality, reduced memory\nfootprint, and faster reconstruction.\nFinally, point-based methods are originally\nwidely used for directly processing data from\ndepth sensors, for performing geometrical and\nsemantic computer vision tasks [39, 22]. Point-\nNeRF [51] efficiently combines point cloud and\nNeRF to achieve impressive fast view synthe-\nsis results. Recently, 3D Gaussian splatting has\nbeen proposed to model points as 3D Gaussians\nfor scene representation [24], and achieved state-\nof-the-art novel view synthesis rendering quality.\nHowever, in [24], the number of Gaussians used\nfor scene representation can easily surpass one\nmillion, which introduces strict memory and com-\nputational requirements for downstream use cases.\nOpen-Vocabulary Object Detection and\nScene Understanding\nAdvancements in open-vocabulary object detec-\ntion in 2D images have been made by leverag-\ning natural language prompts. LSeg [27] employs\na text encoder for semantic label embeddings\nand a transformer-based image encoder for dense\npixel embeddings, using contrastive alignment to\nachieve zero-shot image segmentation and gener-\nalization to unseen categories. CRIS [50] leverages\nCLIP for image segmentation, employing a vision-\nlanguage decoder to align text and pixel-level\nfeatures, and text-to-pixel contrastive learning to\nenforce similarity between text and relevant pixel\nfeatures. CLIP-Seg [34] leverages CLIP as a back-\nbone, employs a transformer-based decoder for\ndense prediction, and generates image segmenta-\ntion based on arbitrary text or image prompts.\nOV-Seg [28] improves open-vocabulary seman-\ntic segmentation by finetuning CLIP on masked\nimage regions and text descriptions from noisy\ncaptions, achieving promising performance with-\nout dataset adaptations.\nCurrent approaches often employ region pro-\nposal or mask prediction methods to guide open-\nvocabulary classification models. OpenSeg [15]\nemploys mask representations to facilitate visual\ngrouping and align captions with predicted seg-\nmentation masks for open-vocabulary image seg-\nmentation. ViLD [19] advances open-vocabulary\nobject detection by distilling knowledge from a\n3\npretrained image classification model (teacher)\ninto a two-stage detector\n(student),\naligning\nregion embeddings of detected boxes with text\nand image embeddings inferred by the teacher.\nDetic [59] expands object detectors\u2019 vocabulary\nby training their classifiers on image classifica-\ntion data, outperforming prior methods on open-\nvocabulary and long-tail detection benchmarks,\nachieving generalization to new datasets without\nfinetuning and enabling detectors trained on all\nImageNet classes. OVIR-3D [32] enables open-\nvocabulary 3D object instance retrieval by fusing\ntext-aligned 2D region proposals into 3D space,\nleveraging 2D datasets.\nOpen-vocabulary scene understanding has also\nbeen explored by using point cloud as sensor\ninputs. PointCLIP [58] aligns CLIP-encoded point\ncloud with 3D category texts, transferring knowl-\nedge from 2D to 3D recognition by projecting\npoint cloud into multi-view depth maps, using an\ninter-view adapter for global feature extraction\nand few-shot knowledge fusion. ULIP series [52,\n53] learn a unified representation for images,\ntexts, and 3D point cloud by leveraging pre-\ntrained vision-language models and automatically\nsynthesized triplets, improving the performance\nof various 3D backbones. Lu et al. [33] lever-\nage pre-trained image and vision-language models\nand cross-modal contrastive learning for open-\nvocabulary 3D point cloud detection without 3D\nannotations.\nCombined 3D Scene Representation and\nSemantic Understanding\nLanguage has been incorporated into 3D scene\nunderstanding in various ways. For the task of\nvisual question answering, systems like iQA [16],\nScanQA [3], and SimVQA [8] leverage 3D infor-\nmation to answer queries about the environment.\nFor object recognition enhancement, language and\nshape information can be combined to improve\nobject recognition, as seen in [13] and [47].\nInspired by the success of implicit neural\nreconstruction [35, 4, 5], researchers also start to\nexplore incorporating language guidance into 3d\nneural scene representation. LERF [25] enables\nopen-ended language queries in 3D by incorpo-\nrating language embeddings from models, e.g.\nCLIP, into NeRF. 3D-OVS [31] leverages pre-\ntrained CLIP and DINO models in a weakly\nsupervised manner, distilling multi-modal knowl-\nedge and object reasoning into a neural radiance\nfield (NeRF) for segmentation task.\nTschernezki et al. [49] leverage a pre-trained\n2D image feature extractor to train a 3D stu-\ndent network, boosting performance in analyzing\nmultiple images forming a 3D scene. FFD [26]\ntackles scene editing by distilling knowledge from\npre-trained 2D image feature extractors into a\n3D feature field that guides local editing based\non user queries. VL-Fields [48], a neural implicit\nspatial representation fusing scene geometry and\nvision-language features, enables open-vocabulary\nsemantic queries without requiring prior object\nclass knowledge. FeatureNeRF [55] distills pre-\ntrained vision models (DINO, Latent Diffusion) to\nlearn generalizable NeRFs, leveraging neural ren-\ndering for 2D-to-3D mapping and extracting deep\nfeatures from NeRF MLPs.\nAdditionally,\nConceptFusion\n[21]\nenables\nopen-set and multimodal reasoning in 3D scene\nrepresentations\nby\nfusing\nfoundation\nmodel\nfeatures with SLAM and multi-view fusion. Con-\nceptGraphs [18] leverages 2D foundation models\nand multi-view association to capture semantic\nand spatial relationships for efficient task-driven\nplanning. OpenMask3D [46] aggregates per-mask\nfeatures using the multi-view fusion of CLIP-based\nimage embeddings guided by predicted class-\nagnostic 3D instance masks. SA3D [9] enables 3D\nsegmentation of target objects in neural radiance\nfields (NeRF) through one-shot manual prompt-\ning, leveraging density-guided inverse rendering,\ncross-view self-prompting, and an iterative pro-\ncess to project 2D segmentation masks onto\n3D mask grids. PVLFF [11] generates a scene\u2019s\nfeature field, combining vision-language and hier-\narchical instance features through contrastive loss\nfrom 2D instance segment proposals.\nCLIP-Fields [44] learns a spatial mapping to\nsemantic embeddings via weak supervision from\nweb-trained language and vision models, enabling\ntasks like object identification and robot navi-\ngation without direct human labeling. GNFac-\ntor [57], a multi-task robotic manipulation agent,\nleverages a shared 3D voxel representation and\nlanguage-augmented neural fields for generalizable\nvisual behavior cloning.\nOur work is close and directly FMGS compa-\nrable to LERF [25] in terms of assumptions about\ninformation available at training phase and query\n4\ntime. For example, it does not assume a priori\nknowledge of query categories at training time\nwhich is assumed 3D-OVS [30].\n3 Background Methods\n3.1 3D Gaussian Splatting\nGS [24] represents an environment using a set of\n3D Gaussians, each defined by a mean \u00b5 \u2208 R3, an\nanisotropic covariance matrix \u03a3 \u2208 R3\u00d73, an alpha\nvalue \u03b1 \u2208 [0, 1] representing opacity, and spherical\nharmonics coefficients (SH). Given a 3D position\nx \u2208 R3, the probability density function of 3D\nGaussian is defined as:\nG(x) = e\u2212 1\n2 (x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5)\n(1)\nwhere (\u00b7)T represents a transpose operation and\n(\u00b7)\u22121 denotes matrix inversion. To render 3D\nGaussians in 2D, we project their mean positions\nby point projection, and project their covariance\nusing the following equation:\n\u03a3\u2032 = JW \u03a3 WT JT\n(2)\nwhere W \u2208 R3\u00d73 is the viewing transformation\nand J \u2208 R3\u00d73 is the Jacobian of the affine approx-\nimation of the projective transformation [60]. To\noptimize covariance matrices, we use an equivalent\nrepresentation:\n\u03a3 = RSST RT\n(3)\nwhere R\n\u2208\nR3\u00d73 and S\n\u2208\nR3\u00d73 are rota-\ntion and scaling matrices, respectively. GS also\nincludes spherical harmonics coefficients to model\nthe appearance of the scene. Gradients for all\nparameters are derived explicitly to avoid over-\nhead during training.\nEach Gaussian encodes the color c using spher-\nical harmonics, which gives a value depending\non the viewing directions. The \u03b1\u2212blending point-\nbased rendering for a pixel color c is done by\nblending N points in the depth order from front\nto back:\nc =\nX\ni\u2208N\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j),\n(4)\nwhere \u03b1i is given by a 2D Gaussian multiplied by\na learned per Gaussian opacity [56].\nNote that although the image rendering model\nis similar across NeRFs and GS, the rendering\nalgorithm is much more efficient in GS. NeRFs\nneed to march along the ray to integrate vol-\nume, however, GS rendering uses a point-based\n\u03b1\u2212blending approach. This allows GS to include\na real-time rendering solution that leverages GPU\nsorting algorithms and draws inspiration from\ntile-based rasterization. By using a 3D Gaus-\nsian representation, anisotropic splatting can be\nperformed while respecting visibility order. This\nis achieved through sorting and alpha-blending.\nAdditionally, a fast and accurate backward pass is\nenabled by tracking the traversal of sorted splats.\n3.2 Multi-resolution Hash Encoding\nRepresenting a 3D feature field can have many\nforms. A naive method is to attach a feature vec-\ntor (or multiple) to each Gaussian, which can be\noptimized along with other Gaussian parameters\n(position, covariance, and so on). However, this is\nextremely costly in terms of computational cost\nand memory consumption especially when a large\nnumber of Gaussians are generated for scene rep-\nresentation. In fact, adding a 512\u00d71 feature vector\nper Gaussian will increase the number of opti-\nmized parameters to be 9.83\u00d7 under authentic GS\nparameterization [24] (10 geometric parameters\nand 48 spherical harmonic appearance parame-\nters per Gaussian) and 65.0\u00d7 under simplified\nGS parameterization [23] (5 geometric parameters\nand 3 appearance parameters per Gaussian).\nTo mitigate this problem, we are motivated\nby multi-resolution hash embedding (MHE) [37],\nwhich provides efficient scene representation that\nconsists of two trainable components. The first\ncomponent first hashes a given position x \u2208 R3,\nand then looks up into a trainable hash table for\nthe corresponding embedding. The second com-\nponent is an MLP that takes the corresponding\nembeddings and makes predictions such as color\nand density. The representation contains multiple\nhash tables, one per each scale. Specifically, MHE\nfirst encodes a given position q = MHE\u03b8(x). To\ndo so, it contains a hash table with L levels. Each\nlevel contains up to E feature vectors with dimen-\nsionality D. Resolution of each level is determined\nby Nl =\n\u0004\nNmin \u00b7 bl\u0005\nwhere Nmin is the coarsest\nresolution, Nmax is the finest resolution, and b is\na growth factor.\n5\nFig. 1\nFMGS Training pipeline: Left: Shows how FMGS\u2019 feature field renders CLIP and DINO feature maps for\nloss calculation. The feature field is a multi-resolution hash encoder (MHE) [37] that embeds semantic information into\n3D Gaussians acquired from 3D Gaussian Splatting [24]. Right: Shows the target DINO feature map and hybrid CLIP\nfeature map from the foundation models. Note, for visualization simplicity, we only show a single-level MHE here but in\nimplementation we have used multiple levels and concatenate their encodings.\nTo get q for a given position x, we query MHE\nat all scales and concatenate the resulting fea-\ntures. For each scale, we find the enclosing voxel\nfor x. Then, each corner entry of the voxel is\nmapped into a feature vector with dimensional-\nity D according to the trainable hash table. MHE\ntrilinearly interpolates the queried corner entries\naccording to their relative position of x in its\nhypercube for each level. This ensures the continu-\nity of the encoded input and its composition with\nthe neural network, avoiding grid-aligned disconti-\nnuities and blocky appearance. After this mapping\nis done, the features from all scales are concate-\nnated to each other, and the auxiliary inputs\n\u03c8 \u2208 RK which results in a feature vector q of\nsize L \u00d7 D + K. The resulting encoding then goes\nto the second component which is an MLP net-\nwork, MLP\u03a6(q), produces the final output. This\narchitecture significantly reduces the number of\nweights that are trained for each view while hav-\ning an O(1) GPU look up for hashing. Overall this\nresults in significant improvements in quality and\nspeed of training.\n4 Method\nOur method, i.e. Foundation Model Embedded\nGaussian Splatting (FMGS), leverages strengths\nof both GS and MHE. We rely on GS for effi-\ncient and accurate scene geometry representation\nand on MHE for representing the scene\u2019s lan-\nguage content in a light-weighted manner. Given\na set of input images, we compute their cam-\nera poses and 3D sparse visual points using an\noff-the-shelf structure from motion system, e.g.,\nCOLMAP [42]. After that we train GS and acquire\n3D Gaussians.\nSubsequently, we train the feature embedding\nfield (MHE) in 3D by grounding 2D CLIP embed-\ndings. This requires us to generate pixel-aligned\nfeatures on a set of calibrated input images. How-\never, CLIP embeddings are global in nature and\nnot suitable for pixel-aligned feature extraction.\nTo overcome this challenge, we introduce a frame-\nwork to learn a volumetric language embedding\nfield that embeds over the 3D Gaussians. The\nfield effectively generate features that is the aver-\nage CLIP features across all views that include\nthat 3D Gaussian. To supervise our dense fea-\nture field, we create a hybrid feature map based\non CLIP embeddings across multi-scale crops of\ntraining views. Figure 1 provides an overview of\nour training pipeline.\n4.1 Feature Field Architecture\n3D Gaussian Splatting produces millions of Gaus-\nsians to enable high quality rendering of a room-\nscale scene. This makes it very inefficient to have\none CLIP feature per Gaussian since these fea-\ntures are high dimensional and keeping all of these\nfeatures in GPU memory is not feasible.\nTo this end, we parameterize our feature field\nefficiently using MHE. For a given 3D Gaus-\nsian G(x) with mean position x, we first encode\nx to a feature vector q = MHE\u03b8(x) where\n\u03b8 is our multi-resolution hash table parameters.\nWe subsequently feed this output into an MLP,\n6\nwhich generates our language embedding f\n=\nMLP CLIP\n\u03d5\n(q), with f belonging to RD. We also\nnormalize f to make it a unit vector.\n4.2 Embed the Foundation Models\nWe embed the semantic embeddings from founda-\ntion models to our scene representation. Training\nthe semantic embedding has three aspects. First,\nwe use our scene representation to render a pre-\ndicted feature map \u02c6F \u2208 RW \u00d7H\u00d7D where W is the\nwidth, H is the height, and D is the dimension\nof the feature map. Second, we generate a tar-\nget feature map F by feeding the view to a FM.\nFinally we need to ensure that the predicted fea-\nture map is aligned with the corresponding target\npixels and follows the same object boundaries in\nterms of feature similarity.\nHybrid CLIP Feature for Supervision\nTo supervise our feature field outputs, given a cal-\nibrated input image, we first rasterize the features\ninto a 2D feature map \u02c6F where the (i, j)th feature\nis acquired by point-based \u03b1\u2212blending:\n\u02c6fi,j =\nX\nk\u2208N\n\u02c6fk\u03b1k\ni\u22121\nY\nl=1\n(1 \u2212 \u03b1l)\n(5)\nTo generate our target CLIP feature map, denoted\nas F, we initially pre-compute a multi-scale fea-\nture pyramid of CLIP embeddings, similar to the\napproach used in LERF [25]. This involves feed-\ning image patches at various sizes into the CLIP\nfoundation model. However, in contrast to LERF,\nwhich trains its scene representation by interpo-\nlating embeddings from the pre-computed CLIP\nfeature pyramid at random scales, we rely on a\nsingle hybrid CLIP feature map for training our\nscene representation. We scale up the embeddings\nof the smaller scales in the pre-computed CLIP\nfeature pyramid bilinearly to the largest scale fea-\nture map, and generate the hybrid feature map by\naveraging them. We define our CLIP loss by the\nfollowing Huber loss:\nLCLIP =\n(\n0.5|\u02c6F \u2212 F|2,\nif |\u02c6F \u2212 F| < \u03b4\n\u03b4 \u00b7 (|\u02c6F \u2212 F| \u2212 0.5 \u00b7 \u03b4),\notherwise\n(6)\nwhere \u03b4 is a hyperparameter, which is set to be\n1.25 empirically. Mingyang: please explicitly say\nwhether the above equation uses 2-norm or 1-\nnorm or F-norm As seen in Figure 2 where we\nuse PCA to visualize feature maps like FFD [26],\nwe notice that the target CLIP feature map is\nnot fine-grained enough when embedding similar-\nities of neighboring pixels are considered. This\nresults in poor pixel-alignment gradient signals\non Gaussians that are not relevant semantically.\nOn the other hand, DINO [7] features give sharp\nboundaries between objects [2] in terms of embed-\nding similarity, which can be used for additional\nregularization.\nRegularization with DINO Feature\nTo transfer the characteristic of DINO features\nwhile maintaining the CLIP embedding seman-\ntics, we (a) add a DINO feature field loss and (b)\ndefine a pixel-alignment loss between the DINO\nand CLIP feature fields. The DINO feature field\nshares the same hash grid parameters as CLIP and\ngives the same encoding q for a given x. Then the\nDINO feature field outputs d = MLP DINO\n\u03c8\n(q)\nwhere \u03c8 denotes the parameters of the MLP that\nare not shared with MLP CLIP\n\u03d5\n. This feature field\nis supervised by passing the sampled image once\nto the pre-trained DINO model without scaling,\nyielding D \u2208 RW \u00d7H\u00d7L where L is the DINO fea-\nture dimension. We then render \u02c6D using the same\napproach as rendering \u02c6F. The DINO regulariza-\ntion loss is as follows:\nLDINO = | \u02c6D \u2212 D|2\n(7)\nPixel-alignment with Dot Product\nSimilarity\nWe define a pixel-alignment loss by defining a ker-\nnel around every pixel and enforce the dot prod-\nuct similarity in normalized embedding spaces\n(between DINO and CLIP) are consistent across\nthe center pixel and surrounding ones. We normal-\nize both rendered features to unit norm, and then\ncompute the loss:\nLpixel =\n1\nK2 \u2212 1\nX\ni\u2208P\nX\nj\u2208N (i),\nj\u0338=i\n|\u02c6dT\ni \u02c6dj \u2212 \u02c6f T\ni \u02c6fj|\n(8)\nwhere P denotes the set of all the pixels in\nthe image, and N(i) is the K \u00d7 K patch kernel\n7\nFig. 2 The features extracted from foundation models. The left three subfigures include the RGB image, extracted\nDINO features from the foundation model, and the hybrid CLIP feature, which is an average of multi-scale CLIP feature\nmaps shown on the right. On the right, the shown seven CLIP feature maps are the extracted from an image pyramid at\nmultiple scales using the foundation model. The resolution of CLIP features decreases from left to right.\nFig. 3\nFMGS Query pipeline: Top: Given a query view to localize a query, FMGS first renders the dense CLIP feature\nmap. Bottom: given an open-vocabulary query, FMGS generates a relevancy map highlighting the relevant part of the\nrendered CLIP feature map to the query embedding. The highest relevant is colored as red while the lowest relevant part\nis colored as blue. Note, for visualization simplicity, we show a single-level MHE in this figure while have used multiple in\nimplementations.\naround the rendered feature at pixel i. This makes\nthe rendered CLIP feature follow the same sim-\nilarity pattern as the DINO feature. Note that\nwe stop the gradient back-propagation through\nDINO features in this training loss, which means\nMLP DINO\n\u03c8\nwould not be affected by this loss.\nTraining Loss\nOverall our total loss is\nLtotal = LCLIP + \u03bbLDINO + \u03b3Lpixel\n(9)\nWe take the mean reduction over all the pix-\nels in the image plane when computing different\nloss terms. We also empirically find out adding\nLpixel in later iterations during training produces\nthe best results. In Figure 4, we provide exam-\nples of features extracted from foundation models\nfor training and the rendered features generated\nby our trained hybrid semantic scene represen-\ntation. It is evident that the rendered feature\nmaps exhibit higher quality when compared to\nthe features obtained directly from the founda-\ntion models, owing to our training process enforces\nmultiple-view consistency.\n4.3 Relevancy Score\nAt query time, when provided with a query\nprompt and a viewing direction, FMGS gener-\nates a relevancy map that assigns high scores to\nsemantically relevant locations (see Figure 3). To\nobtain this relevancy map, we first render the\nfeature map F using our learned semantic fea-\nture field via GS rasterization. Then, we calculate\nthe CLIP embedding fquery corresponding to the\nquery prompt.\nTo obtain the dense relevancy map, we define\na set of canonical phrases with CLIP embed-\ndings Fcan following the methodology similar to\n[25]. Then, we compute pairwise softmax scores\nbased on the cosine similarity between the prompt\nembedding and fi,j, representing the F at loca-\ntion (i, j), as well as the canonical embeddings for\ncanonical phrases. We take the minimum value of\nthe softmax over all canonical prompts and deem\n8\nFig. 4 Features for Training and Rendered Views. Left: From left to right, the figures show the RGB image, the\nrendered DINO feature map, the raw DINO feature map extracted for training, the rendered CLIP feature map, and the\nraw CLIP feature map used for training. Right: We display the relevancy scores for the rendered and raw CLIP feature\nmaps with the text query \u2018flower\u2019, where the color bar indicates relevancy scores normalized within the 0-255 range. Notably,\nquerying the raw CLIP feature map is much inferior to querying the rendered CLIP feature map.\nFig. 5 Effect of dot product similarity (dotpsim) loss. From left to right: RGB image, rendered DINO feature\nwithout dotpsim, rendered DINO feature with dotpsim, rendered CLIP without dotpsim, and rendered CLIP feature map\nwith dotpsim. From the CLIP feature maps, we can see that objects can be further distinguished from each other and the\nbackground. Differences are highlighted in the red boxes.\nit the relevancy score r:\nri,j = min\nn\nexp(f T\ni,jfquery)\nexp(f T\ni,jfquery) + exp(f T\ni,jf n\ncan), f n\ncan \u2208 Fcan\n(10)\nWith the above definition, the relevancy score\nis higher when a query embedding is closer to\nthe rendered feature than the canonical features\nrelatively. We follow [25] and choose the follow-\ning canonical prompts: \u201cobject\u201d, \u201cstuff\u201d, \u201cthings\u201d,\nand \u201ctexture\u201d. We also find that these work well\nfor a wide range of queries removing the need\nfor tuning these canonical terms. As depicted\nin Figure 4, we present representative relevancy\nmaps generated by matching the query embed-\nding with our rendered CLIP feature map and\nthe target CLIP feature map from the foundation\nmodel used in our training. It\u2019s evident that the\nrelevancy map derived from our rendered CLIP\nfeature map exhibits finer granularity and higher\noverall quality.\n4.4 Implementation Details\nOur approach employs a hash grid for represent-\ning language features, which is notably larger than\na typical RGB hash grid. This hash grid com-\nprises 24 layers, spanning resolutions from 16 to\n512, and possesses a hash table size of 220 with\nan associated feature dimension of 8. The archi-\ntecture of the CLIP and DINO MLP models used\nfor MLP CLIP\n\u03d5\nand MLP DINO\n\u03c8\naligns with that of\nLERF [24]. Furthermore, we leverage the Open-\nCLIP [12] ViT-B/16 model, which has undergone\ntraining on the LAION-2B dataset. Notably, this\nmodel operates with an image pyramid that varies\nin scale from 0.05 to 0.5 of image size, encom-\npassing a total of seven scales for pre-computing\nCLIP feature pyramid. We take the average pool-\ning of the pre-computed CLIP feature pyramid to\nget the final hybrid CLIP feature for training our\nsemantic embedding field.\n9\nInitially, we train the Vanilla Gaussian Splat-\nting scene representation [24] through a total num-\nber of 30K iterations, with approximately 10 min-\nutes total time for a room-scale scene. It\u2019s worth\nnoting that representing such a scene requires the\nutilization of millions of Gaussians. Subsequently,\nwe maintain the frozen states of the geomet-\nric attributes and spherical harmonics associated\nwith these Gaussians throughout the subsequent\ntraining process for semantic embedding fields.\nTo mitigate GPU memory constraints, we\nstrategically select approximately 10% of the\nGaussians based on criteria such as high opac-\nity values and a 2D radius in project Gaussian\nexceeding 2 pixels in at least one training view.\nOnly these selected Gaussians are involved in\nthe rendering process when we train the seman-\ntic embeddings. For optimization, we employ the\nRAdam optimizer with a weight decay of 10\u22129. We\nincorporate an exponential learning rate sched-\nuler, which spans from an initial value of 5\u00d7 10\u22123\nand gradually decreases to 4 \u00d7 10\u22123 over the\ncourse of 4.2K training steps (after the initial\n30K original GS training steps). In our training\nregimen, all models initially undergo 2.5K steps\nwithout the pixel alignment loss being enabled.\nThese training and testing procedures are exe-\ncuted on an NVIDIA RTX A5000 GPU with 24GB\nof GPU RAM. The semantic feature field train-\ning time with a total of 4.2K steps takes about\n1.4 hours. During training, we use weighting fac-\ntors to balance the CLIP loss (\u03bb = 0.2) and the\npixel-alignment loss (\u03b3 = 0.01).\n5 Experiments\nOur hybrid semantic scene representation, FMGS,\nseamlessly integrates the 3D Gaussians and multi-\nresolution hashing encoding and supports both\nphoto-realistic\nrendering\nand\nopen-vocabulary\nobject detection. In this section, we carefully eval-\nuate the performance of open-vocabulary object\ndetection (or localization) of our proposed method\nin uncontrolled real-world scenarios. To showcase\nthe embedding quality of our method, we also\nevaluate it out-of-the-box on the open-vocabulary\nsemantic segmentation task. We compare our\nmethod to other SOTA approaches for each exper-\niment and show significant improvement over their\nresults.\nScene\nFFD-LSeg [26] OWL-ViT [36] LERF [25] Ours\nbouquet\n50.0%\n66.7%\n83.3%\n100.0 %\nfigurines\n8.9%\n38.5%\n87.2%\n89.7%\nramen\n15.0%\n92.5%\n62.5%\n90.0 %\nteatime\n28.1%\n75.0%\n96.9%\n93.8%\nkitchen\n13.0%\n42.6%\n85.2%\n92.6 %\nAverage\n18.0%\n54.8%\n83.0%\n93.2%\nTable 1 Accuracy of object detection with open-\nvocabulary queries. comparison between Feature Fields\nDistillation [26] using LSeg [27] features (FFD-Lseg),\nOWL-ViT [36], LERF [25] and Ours FMGS. We highlight\nthe best , second-best accuracy scores. Please find more\ndetails on scenes and text queries for LERF dataset in [25].\n5.1 Object Detection in the Wild\nBy distilling the language embeddings extracted\nfrom off-the-shelf vision-language model, CLIP,\nour FMGS is applicable for associating a wide\nrange of textual prompts with the relevant vision\nclues. We test the open-vocabulary object under-\nstanding capability of our method by object\ndetection experiments.\nDataset: We use the same dataset as used\nin the LERF [25] for object detection evaluation,\nfor the purpose of fair comparison. It consists of\nfive labelled scenes with 2D bounding boxes of\nobjects associated with text prompts. There are\nobjects including both common and long-tail ones\nwith different sizes, and the queries for objects are\nquite diverse, like \u2018vase\u2019, \u2018eucalyptus\u2019, \u2018big white\ncrinkly flower\u2019, \u2018pikachu\u2019, \u2018twizzlers\u2019, \u2018spoon han-\ndle\u2019, \u2018power outlet\u2019, \u2018waldo\u2019, \u2018stuffed bear\u2019, \u2018cookies\non a plate\u2019, etc. The location of queried images\nare labelled by bounding boxes in the test images,\nwhich are rendered at novel views from trained\nNeRF models of individual scenes. The scenes in\nLERF dataset are collected by an iPhone, and\neach scene comprise \u223c 200 images. The provided\nposes of images from Ploycam app are with signifi-\ncant noises in some scenes. Thus we regenerate the\nposes of images by running COLMAP [42], which\nalso yields sparse 3D visual points serving as input\nto initialize 3D Gaussians in our method. The\nposes of the officially-provided test images are also\nproperly transferred to our COLMAP trajectory\nby Sim(3) alignment between officially-provided\nimage poses and our COLMAP poses.\nEvaluation Protocol: Following LERF [24],\nthe evaluation metric for object detection is the\naccuracy rate. We redeem the query is a success\nif the highest relevancy pixel locates inside the\ntarget box. The relevancy score at each pixel is\n10\nFig. 6\nRelevancy score for object detection. Left: The rendered RGB image at novel view from 5 scenes on LERF\ndataset [25]. Right: Visualization of relevancy scores with the given text queries shown below the figures. We overlay them\non the RGB images.\nMethods\nbed\nsofa\nlawn\nroom\nbench\ntable\nmIoU\nmAP\nmIoU\nmAP\nmIoU\nmAP\nmIoU\nmAP\nmIoU\nmAP\nmIoU\nmAP\nOV-Seg [28]\n79.8\n40.4\n66.1\n69.6\n81.2\n92.1\n71.4\n49.1\n88.9\n89.2\n80.6\n65.3\n3D-OVS [30]\n89.5\n96.7\n74.0\n91.6\n88.2\n97.3\n92.8\n98.9\n89.3\n96.3\n88.8\n96.5\nLERF [25]\n33.5\n25.6\n28.1\n45.6\n49.8\n82.0\n26.3\n49.1\n55.2\n79.5\n31.1\n33.3\nOurs\n38.0\n50.1\n56.6\n82.0\n64.9\n90.5\n57.0\n85.3\n62.1\n84.1\n63.6\n85.3\nTable 2\nSegmentation Evaluation. We report the mIoU(\u2191) scores and the mAP(\u2191) scores of the following methods in\n6 scenes of 3D-OVS dataset [30]. Note that 3D-OVS is a weakly supervised method, which knows the segmentation\nannotations in training and specially designed for segmentation task. Our method and LERF are 3D method training\nwithout any segmentation annotations, relying only on the relevancy between class query and the rendered CLIP features.\nOV-Seg [28] is a supervised method for segmentation task. Our method and LERF are unsupervised method, under\napple-to-apple comparison.\nobtained by matching the rendered CLIP feature\nmap with the language embedding of given text\nquery as described in Sec. 4.3.\nBaselines: We compare against FFD-LSeg\nthat embeds pixel-aligned LSeg feature [27] into\nNeRF (NeuralStudio \u2018neurfacto\u2019 implementation\nby feature fields distillation method [26], OWL-\nViT [36] that is a 2D method based on Vision\nTransformer encoder and fine-tuned for object\ndetection, LERF [25] that embeds CLIP and\nDINO features into NeRF. The 3D methods,\nFFD-LSeg and LERF, share the same evaluation\nprotocol as our FMGS. For the 2D method, OWL-\nViT, we regard it as a success if the center of the\npredicted bounding box locates in the target box.\nEvaluation Results: The quantitative evalu-\nation results on all sequences of LERF dataset are\npresented in Table 1, and representative relevancy\nscore maps of the proposed method are shown in\n11\nFig. 7\nObject detection results. Left: The Ground-truth bounding boxes (blue), our detected highest-relevancy pixel\n(green) and the one detected by LERF (red) [25]. Middle: Ours relevancy score with the given text query. The query word\nis shown at the most left of each row. Right: LERF\u2019s relevancy score with the given text query. Our computed relevancy\nscore is more focused on the target objects linked to the query.\nMethods\nbouquet figurines ramen teatime kitchen Average\nOurs\n100.0\n89.7\n90.0\n93.8\n92.6\n93.2\nW/O dotpsim\n100.0\n91.0\n85.0\n90.6\n85.2\n90.4\nW/O hybrid CLIP\n54.2\n32.1\n52.5\n6.3\n9.3\n30.8\nTable 3 Ablation study. Object detection comparison\nbetween our full method, ours without dot product\nsimilarity (dotpsim) loss, and ours without hybrid CLIP\nfeatures by averaging at multiple scales for supervision,\nusing single scale Clip feature at the finest-resolution\ninstead.\nFigure 6. The detailed results demonstrate signif-\nicant advantages of FMGS\u2019s integration of lan-\nguage embeddings in detecting objects associated\nwith long-tail prompts. While LSeg [27], trained\non a small dataset to learn pixel-aligned CLIP\nfeatures, exhibits diminished open-vocabulary lan-\nguage understanding capabilities, the approach of\nFFD-LSeg, which distills LSeg features into radi-\nance fields, struggles with comprehending long-tail\nqueries and consequently exhibits poorer perfor-\nmance. In terms of open-vocabulary 2D detection,\nOwl-ViT, which utilizes full-HD NeRF views and\nselects bounding boxes based on the highest con-\nfidence scores for text queries, outperforms FFD-\nLseg. However, when faced with long-tail queries,\nOwl-ViT\u2019s performance falls short in comparison\nto the robust and versatile FMGS.\nWe also conducted a comparison with the clos-\nest method, LERF, which distills DINO and CLIP\nfeatures into neural radiance fields represented\nsolely by MHEs. As depicted in Table 1, our\nFMGS outperforms LERF significantly, achiev-\ning an accuracy improvement of 10.2 percent-\nage points. Note that our tested LERF results,\nobtained using the officially released code, slightly\nsurpasses those reported in the original paper [25].\nIn Figure 7, we present side-by-side compar-\nisons with LERF [25]. The object detection results\nare visualized, highlighting the superior quality\nof the relevance map produced by our FMGS.\nIt notably focuses more on the queried target\nobjects, as opposed to LERF. This outcome stems\nfrom our hybrid representation, which combines\n3D Gaussians and MHEs for semantic scene rep-\nresentation. The 3D Gaussians represent both the\ngeometry and appearance of the scene, naturally\ndividing 3D structures of objects and the scene\ninto distinct Gaussian volumes. This partitioning\nfeature aids in distinguishing objects from each\nother and from the background. In FMGS, we\n12\nassign an identical MHE embedding to a Gaussian\nvolume, further promoting semantic consistency\nin local proximity. This, in turn, contributes to\nthe focusing of relevance on the target object.\nTaking the query \u2018Pikachu\u2019 in Figure 7 as an\nexample, where \u2018Pikachu\u2019 is depicted on the side\nof a paper bag. Even when observing from a\nchallenging viewpoint with almost no visibility\nof \u2018Pikachu\u2019, FMGS successfully maintains high\nrelevance at the target location, due to its 3D\nconsistency and fine-grained scene understanding.\nIn contrast, LERF fails to detect \u2018Pikachu\u2019 and\nmistakenly identifies a visually similar object.\nInference Runtime: Our FMGS, relying on\n3D Gaussian Splatting rendering [24], excels in\nefficiently rendering RGB images. We\u2019ve imple-\nmented our rendering method for CLIP and DINO\nfeature maps based on a CUDA implementation\nof Gaussian Splatting rendering. Even when ren-\ndering deep features with high dimensions, which\ncan significantly increase computation time, our\nFMGS remains remarkably fast. It can render the\n480 \u00d7 270 CLIP feature map, DINO feature map,\nand RGB image jointly at an impressively high\nrate of 103.4 FPS during inference, even with our\nunoptimized implementation. In contrast, LERF\noperates at a significantly slower pace, achieving a\nmere 0.1214 FPS during inference. This slowness\nstems from LERF\u2019s need to perform a brute-force\nsearch for the best scales when rendering CLIP\nfeatures, spanning a range from 0 to 2 meters\nwith 30 increments. Consequently, we are 851.73\ntimes faster than LERF in rendering CLIP fea-\ntures, enabling efficient real-time open-vocabulary\nqueries after our scene representation is trained.\n5.2 Unsupervised Segmentation\nIn following experiments we use FMGS to seg-\nment queries and evaluate their segmentation\nmasks. Note that our method is not delicately\ndesigned for segmentation task. We lack a dedi-\ncated segmentation header for predicting segmen-\ntation masks, nor do we explicitly partition the\nscene at the object level. We have examined the\nopen-vocabulary language understanding capabil-\nity of FMGS in the object detection experiments\ndiscussed in the above section. Our primary objec-\ntive for doing this segmentation evaluation is to\nassess the pixel-level accuracy of the rendered\nCLIP features obtained from the trained scene\nrepresentation. Segmentation relies on matching\nthese rendered CLIP features to the embeddings\nof the provided semantic labels.\nDataset: We conducted our segmentation\nevaluation using the 3D-OVS dataset [30], which\nconsists of six scenes with labeled ground-truth\nsemantic segmentation masks for test image views.\nThese scenes are characterized by their clean-\nliness, with clear backgrounds and well-defined\nforeground objects. Each scene comprises approx-\nimately 30 images with predefined poses and\nsparse points computed using COLMAP [42]. The\ndataset includes a variety of objects, including\nmany long-tail objects like \u2018Gundam,\u2019 \u2018Pikachu,\u2019\n\u2018stapler\u2019, and more. For further details about the\nscenes and semantic labels, please refer to [30].\nEvaluation Protocol: In terms of our evalu-\nation protocol, we rely on the annotated ground-\ntruth masks for the test views. These masks serve\nas a reliable benchmark for both qualitative and\nquantitative assessments of segmentation perfor-\nmance. We calculate the mean Intersection over\nUnion (mIOU) scores and mean Average Preci-\nsion (AP) metrics by comparing the segmentation\nresults with these ground-truth masks.\nBaselines: We conduct a direct comparison of\nour method with LERF [25]. To perform semantic\nsegmentation, we initially obtain relevancy scores\nby computing the cosine similarity between the\nrendered CLIP feature and the embeddings of all\nclass labels (this is different from the relevancy\nscore calculation with auxiliary canonical phrases\ninvolved in Sec. 4.3.). These relevancy scores serve\nas segmentation logits, and we subsequently apply\nthe softmax function to convert them into prob-\nabilities. Each pixel is then assigned a semantic\nclass label corresponding to the maximum proba-\nbility. Note that LERF [25] requires a scale factor\nwhen rendering CLIP features, and we report the\nbest segmentation results that can be achieved by\nLERF by selecting the best scales for each ray.\nIt\u2019s also important to note that both LERF and\nour method encounter challenges in discerning the\nsemantic labels of backgrounds when presented\nwith visibility-limited close views and lack of con-\ntext. Therefore, we have replaced the original\nbackground labels, including \u2018white sheet\u2019, \u2018wood\nwall\u2019, \u2018grey sofa,\u2019 and \u2018lime wall,\u2019 with a more gen-\neral label \u2018background\u2019 when testing LERF and\nour method.\n13\nFig. 8\nSemantic segmentation results. In the rows from top to bottom, we display RGB images, ground-truth (Gt)\nsegmentation masks, our segmentation results, and the segmentation results obtained by LERF [25] scene representation.\nIt\u2019s essential to note that neither our method nor LERF was initially intended for the segmentation task. Our primary aim\nis to evaluate the pixel accuracy of the relevance map computed from the rendered CLIP features.\nAdditionally, for comprehensive reference, we\npresent results obtained using the dedicated 3D-\nOVS method [30] for the segmentation task.\nHowever, it is worth emphasizing that comparing\nobject detection methods like ours and LERF [25]\nto 3D-OVS is not entirely equitable, as acknowl-\nedged in the paper of 3D-OVS [30]. 3D-OVS [30]\nhas prior access to segmentation class labels and\ndistill class-related information into the radiance\nfield during training. In contrast, neither LERF\nnor our methods have access to class labels dur-\ning scene representation training. Consequently,\nthe trained 3D-OVS scene representation can only\nbe effectively employed for querying the classes\nknown before training, and does not support arbi-\ntrary semantic queries beyond the trained classes.\nFurthermore, we compare to a 2D ceiling approach\n[28], OV-Seg, which is directly trained for open-\nvocabulary semantic segmentation by fine-tuning\nCLIP on masked image regions and text descrip-\ntions. OV-Seg is supervised with mask-category\npairs, while ours and LERF are purely unsuper-\nvised.\nEvaluation\nResults\nThe\nsegmentation\nexperiment results are presented in Table 2 and\nFigure 8. Notably, our approach outperforms\nLERF [25] by a significant margin across all cases.\nThis superior performance can be attributed to\nthe higher quality of our rendered CLIP feature\ncompared to the one produced by LERF. Our\nmethod exhibits more concentrated high rele-\nvancy around the queried objects, showcasing the\nadvantage of our semantic scene representation,\nwhich maintains high semantic consistency in\nlocal proximity.\n5.3 Ablations\nWe conducted an ablation study on the object\ndetection task, as it serves as a key indicator\nof our method\u2019s open-vocabulary semantic under-\nstanding capabilities. The results are presented in\nTable 3.\n5.3.1 Hybrid CLIP feature\nIn this ablation study, we investigated using a\nsingle scale of CLIP features, rather than our\nhybrid CLIP features, which are obtained by aver-\naging multiple-scale CLIP features extracted from\npatches at different resolutions. As demonstrated\nin Table 3, the hybrid CLIP feature for supervision\nis greatly important. The scene understanding\ncapability of our method is severely compromised\nwhen employing only a single-scale CLIP feature\nfor supervision.\n14\n5.3.2 Pixel-alignment loss\nTo assess the effectiveness of our proposed pixel\nalignment loss, we conducted an ablation study by\ntraining our semantic scene representation with-\nout this loss. The impact of omitting the pixel\nalignment loss on the accuracy of the object\ndetection task is shown in Table 3. Furthermore,\nwe provide qualitative results in Figure 5, which\nindicates that CLIP features from a scene rep-\nresentation trained with pixel-alignment loss are\nbetter at distinguishing between different objects\nand separating objects from the background.\n6 Discussion and Limitations\nWhen comparing FMGS to LERF[25], both meth-\nods distill Clip and Dino features from foundation\nmodels into 3D scene representations. However,\ntheir rendering algorithms and scene representa-\ntions differ significantly. These distinctions lead\nto rapid and high-quality language feature acqui-\nsition using common hyperparameters, such as\nthe feature field architecture. An additional key\nadvantage of FMGS is that it employs the same\nfeature embedding for each Gaussian, regardless\nof the viewing direction. This feature enables\ndirect 3D localization of vision-language queries.\nIt\u2019s important to note that FMGS not only facili-\ntates the localization of language queries in 3D but\nalso allows for finding a given image of the scene\nusing the 3D Gaussian embeddings. LERF, on the\nother hand, does not offer such 3D localization\ncapabilities out of the box.\nIn terms of limitations, FMGS currently relies\nheavily on the presence of high-quality and cal-\nibrated input images, a limitation shared with\nNeRF-based approaches. Additionally, the per-\nformance of FMGS is entirely contingent on the\nquality of the base foundation models used for\ntraining the feature fields. It is conceivable that\na model better suited for localizing language\nwithin images could yield improved feature field\nquality. Futhermore, for improved performance in\nthe semantic segmentation task, it is advisable\nto embed a dedicated segmentation foundation\nmodel, such as SAM [9], into our scene represen-\ntation. Alternatively, a straightforward approach\nfor semantic segmentation is to initially segment\nthe images using the foundation model and then\nassign semantic meanings to the segments based\non our rendered CLIP features.\n7 Conclusions\nFoundation Model Embedded Gaussian Splat-\nting (FMGS) contributes to scene understanding\nby seamlessly merging vision-language embed-\ndings and 3D representation. This novel 3D scene\nrepresentation achieves multi-view semantic con-\nsistency through self-supervised distillation and\npixel alignment of CLIP features. The resulting\nfeature embedded 3D Gaussians achieve state-of-\nthe-art performance in comparison to previous\nmethods. By bridging vision, language, and 3D,\nFMGS paves the way for unprecedented object\ncomprehension in real-world environments, open-\ning exciting possibilities for augmented reality,\nrobotics, and beyond.\n8 Acknowledgement\nWe are very grateful to Juan J. G\u00b4omez Rodr\u00b4\u0131guez\nand Francis Engelmann for their advice and\ninsightful discussions about this work.\n15\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline\nLuc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Mil-\nlican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning.\nAdvances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\n[2] Shir Amir, Yossi Gandelsman, Shai Bagon,\nand\nTali\nDekel.\nDeep\nvit features\nas\ndense visual descriptors.\narXiv preprint\narXiv:2112.05814, 2(3):4, 2021.\n[3] Daichi\nAzuma,\nTaiki\nMiyanishi,\nShuhei\nKurita, and Motoaki Kawanabe. Scanqa: 3d\nquestion answering for spatial scene under-\nstanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, pages 19129\u201319139, 2022.\n[4] Jonathan\nT\nBarron,\nBen\nMildenhall,\nMatthew Tancik, Peter Hedman, Ricardo\nMartin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for\nanti-aliasing neural radiance fields. In Pro-\nceedings\nof\nthe\nIEEE/CVF\nInternational\nConference\non\nComputer\nVision,\npages\n5855\u20135864, 2021.\n[5] Jonathan T Barron, Ben Mildenhall, Dor\nVerbin, Pratul P Srinivasan, and Peter Hed-\nman. Mip-nerf 360: Unbounded anti-aliased\nneural radiance fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5470\u20135479,\n2022.\n[6] Jonathan T. Barron, Ben Mildenhall, Dor\nVerbin, Pratul P. Srinivasan, and Peter Hed-\nman. Zip-nerf: Anti-aliased grid-based neural\nradiance fields. ICCV, 2023.\n[7] Mathilde\nCaron,\nHugo\nTouvron,\nIshan\nMisra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr\nBojanowski, and Armand Joulin.\nEmerg-\ning properties in self-supervised vision trans-\nformers.\nIn Proceedings of the IEEE/CVF\ninternational conference on computer vision,\npages 9650\u20139660, 2021.\n[8] Paola\nCascante-Bonilla,\nHui\nWu,\nLetao\nWang, Rogerio S Feris, and Vicente Ordonez.\nSimvqa: Exploring simulated environments\nfor visual question answering. In Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5056\u2013\n5066, 2022.\n[9] Jiazhong Cen, Zanwei Zhou, Jiemin Fang,\nChen Yang, Wei Shen, Lingxi Xie, Xiaopeng\nZhang, and Qi Tian. Segment anything in 3d\nwith nerfs. 2023.\n[10] Anpei Chen, Zexiang Xu, Andreas Geiger,\nJingyi Yu, and Hao Su.\nTensorf: Tensorial\nradiance fields. In European Conference on\nComputer Vision, pages 333\u2013350. Springer,\n2022.\n[11] Haoran Chen, Kenneth Blomqvist, Francesco\nMilano, and Roland Siegwart.\nPanoptic\nvision-language feature fields. arXiv preprint\narXiv:2309.05448, 2023.\n[12] Mehdi\nCherti,\nRomain\nBeaumont,\nRoss\nWightman,\nMitchell\nWortsman,\nGabriel\nIlharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev.\nReproducible scaling laws for contrastive\nlanguage-image learning.\narXiv preprint\narXiv:2212.07143, 2022.\n[13] Rodolfo Corona, Shizhan Zhu, Dan Klein,\nand Trevor Darrell. Voxel-informed language\ngrounding. arXiv preprint arXiv:2205.09710,\n2022.\n[14] Angela Dai, Angel X Chang, Manolis Savva,\nMaciej Halber, Thomas Funkhouser, and\nMatthias Nie\u00dfner. Scannet: Richly-annotated\n3d reconstructions of indoor scenes. In Pro-\nceedings of the IEEE conference on computer\nvision and pattern recognition, pages 5828\u2013\n5839, 2017.\n[15] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and\nTsung-Yi Lin. Open-vocabulary image seg-\nmentation. arXiv preprint arXiv:2112.12143,\n2021.\n[16] Daniel\nGordon,\nAniruddha\nKembhavi,\nMohammad\nRastegari,\nJoseph\nRedmon,\nDieter Fox, and Ali Farhadi.\nIqa: Visual\nquestion answering in interactive environ-\nments. In Proceedings of the IEEE conference\non computer vision and pattern recognition,\npages 4089\u20134098, 2018.\n[17] Margarita\nGrinvald,\nFadri\nFurrer,\nTonci\nNovkovic, Jen Jen Chung, Cesar Cadena,\nRoland Siegwart, and Juan Nieto.\nVolu-\nmetric instance-aware semantic mapping and\n3d object discovery.\nIEEE Robotics and\nAutomation Letters, 4(3):3037\u20133044, 2019.\n16\n[18] Qiao\nGu,\nAlihusein\nKuwajerwala,\nSacha\nMorin,\nKrishna\nMurthy\nJatavallabhula,\nBipasha\nSen,\nAditya\nAgarwal,\nCorban\nRivera, William Paul, Kirsty Ellis, Rama\nChellappa, Chuang Gan, Celso Miguel de\nMelo, Joshua B. Tenenbaum, Antonio Tor-\nralba, Florian Shkurti, and Liam Paull.\nConceptgraphs: Open-vocabulary 3d scene\ngraphs for perception and planning. arXiv,\n2023.\n[19] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and\nYin Cui. Open-vocabulary object detection\nvia vision and language knowledge distilla-\ntion. arXiv preprint arXiv:2104.13921, 2021.\n[20] Shahram Izadi, David Kim, Otmar Hilliges,\nDavid Molyneaux, Richard Newcombe, Push-\nmeet Kohli, Jamie Shotton, Steve Hodges,\nDustin Freeman, Andrew Davison, et al.\nKinectfusion: real-time 3d reconstruction and\ninteraction using a moving depth camera. In\nProceedings of the 24th annual ACM sympo-\nsium on User interface software and technol-\nogy, pages 559\u2013568, 2011.\n[21] Krishna Murthy Jatavallabhula, Alihusein\nKuwajerwala, Qiao Gu, Mohd Omama, Tao\nChen, Shuang Li, Ganesh Iyer, Soroush\nSaryazdi,\nNikhil\nKeetha,\nAyush\nTewari,\nJoshua B. Tenenbaum, Celso Miguel de Melo,\nMadhava\nKrishna,\nLiam\nPaull,\nFlorian\nShkurti, and Antonio Torralba.\nConcept-\nfusion: Open-set multimodal 3d mapping,\n2023.\n[22] Peter\nKarkus,\nShaojun\nCai,\nand\nDavid\nHsu. Differentiable slam-net: Learning parti-\ncle slam for visual navigation. In Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2815\u2013\n2825, 2021.\n[23] Nikhil Keetha, Jay Karhade, Krishna Murthy\nJatavallabhula, Gengshan Yang, Sebastian\nScherer,\nDeva\nRamanan,\nand\nJonathon\nLuiten. Splatam: Splat, track & map 3d gaus-\nsians for dense rgb-d slam.\narXiv preprint\narXiv:2312.02126, 2023.\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas\nLeimk\u00a8uhler, and George Drettakis. 3d gaus-\nsian splatting for real-time radiance field\nrendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023.\n[25] Justin Kerr, Chung Min Kim, Ken Gold-\nberg, Angjoo Kanazawa, and Matthew Tan-\ncik. Lerf: Language embedded radiance fields.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages\n19729\u201319739, 2023.\n[26] Sosuke Kobayashi, Eiichi Matsumoto, and\nVincent Sitzmann.\nDecomposing nerf for\nediting via feature field distillation.\nIn\nAdvances in Neural Information Processing\nSystems, volume 35, 2022.\n[27] Boyi Li, Kilian Q Weinberger, Serge Belongie,\nVladlen Koltun, and Ren\u00b4e Ranftl. Language-\ndriven semantic segmentation. arXiv preprint\narXiv:2201.03546, 2022.\n[28] Feng\nLiang,\nBichen\nWu,\nXiaoliang\nDai,\nKunpeng\nLi,\nYinan\nZhao,\nHang\nZhang,\nPeizhao Zhang, Peter Vajda, and Diana\nMarculescu. Open-vocabulary semantic seg-\nmentation with mask-adapted clip.\narXiv\npreprint arXiv:2210.04150, 2022.\n[29] Kevin Lin, Lijuan Wang, and Zicheng Liu.\nEnd-to-end human pose and mesh recon-\nstruction with transformers. In Proceedings\nof the IEEE/CVF conference on computer\nvision and pattern recognition, pages 1954\u2013\n1963, 2021.\n[30] Kunhao Liu, Fangneng Zhan, Jiahui Zhang,\nMuyu\nXu,\nYingchen\nYu,\nAbdulmotaleb\nEl Saddik, Christian Theobalt, Eric Xing,\nand Shijian Lu. Weakly supervised 3d open-\nvocabulary segmentation. In Thirty-seventh\nConference on Neural Information Processing\nSystems, 2023.\n[31] Kunhao Liu, Fangneng Zhan, Jiahui Zhang,\nMuyu Xu, Yingchen Yu, Abdulmotaleb El\nSaddik, Christian Theobalt, Eric Xing, and\nShijian Lu.\n3d open-vocabulary segmenta-\ntion with foundation models. arXiv preprint\narXiv:2305.14093, 2023.\n[32] Shiyang\nLu,\nHaonan\nChang,\nEric\nPu\nJing,\nAbdeslam\nBoularias,\nand\nKostas\nBekris.\nOVIR-3d:\nOpen-vocabulary\n3d\ninstance retrieval without training on 3d\ndata.\nIn 7th Annual Conference on Robot\nLearning, 2023.\n[33] Yuheng Lu, Chenfeng Xu, Xiaobao Wei,\nXiaodong Xie, Masayoshi Tomizuka, Kurt\nKeutzer, and Shanghang Zhang.\nOpen-\nvocabulary\npoint-cloud\nobject\ndetection\nwithout 3d annotation. 2023.\n17\n[34] Timo L\u00a8uddecke and Alexander Ecker. Image\nsegmentation using text and image prompts.\nIn CVPR, pages 7086\u20137096, 2022.\n[35] Ben\nMildenhall,\nPratul\nP.\nSrinivasan,\nMatthew Tancik, Jonathan T. Barron, Ravi\nRamamoorthi, and Ren Ng. Nerf: Represent-\ning scenes as neural radiance fields for view\nsynthesis. In ECCV, 2020.\n[36] Matthias Minderer, Alexey Gritsenko, Austin\nStone, Maxim Neumann, Dirk Weissenborn,\nAlexey Dosovitskiy, Aravindh Mahendran,\nAnurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object\ndetection with vision transformers.\narXiv\npreprint arXiv:2205.06230, 2022.\n[37] Thomas\nM\u00a8uller,\nAlex\nEvans,\nChristoph\nSchied, and Alexander Keller. Instant neu-\nral graphics primitives with a multiresolution\nhash encoding. ACM Transactions on Graph-\nics (ToG), 41(4):1\u201315, 2022.\n[38] Gaku\nNarita,\nTakashi\nSeno,\nTomoya\nIshikawa, and Yohsuke Kaji. Panopticfusion:\nOnline volumetric semantic mapping at the\nlevel of stuff and things. In IEEE/RSJ Inter-\nnational Conference on Intelligent Robots\nand Systems (IROS), pages 4205\u20134212, 2019.\n[39] Charles R Qi, Hao Su, Kaichun Mo, and\nLeonidas J Guibas. Pointnet: Deep learning\non point sets for 3d classification and segmen-\ntation. In Proceedings of the IEEE conference\non computer vision and pattern recognition,\npages 652\u2013660, 2017.\n[40] Alec Radford, Jong Wook Kim, Chris Hal-\nlacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural lan-\nguage supervision.\nIn International confer-\nence on machine learning, pages 8748\u20138763.\nPMLR, 2021.\n[41] Johannes L Schonberger and Jan-Michael\nFrahm. Structure-from-motion revisited. In\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n4104\u20134113, 2016.\n[42] Johannes L Schonberger and Jan-Michael\nFrahm. Structure-from-motion revisited. In\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n4104\u20134113, 2016.\n[43] Thomas Sch\u00a8ops, Torsten Sattler, and Marc\nPollefeys. Surfelmeshing: Online surfel-based\nmesh reconstruction. IEEE transactions on\npattern analysis and machine intelligence,\n42(10):2494\u20132507, 2019.\n[44] Nur Muhammad Mahi Shafiullah, Chris Pax-\nton, Lerrel Pinto, Soumith Chintala, and\nArthur Szlam. Clip-fields: Weakly supervised\nsemantic fields for robotic memory.\narXiv\npreprint arXiv:2210.05663, 2022.\n[45] Jiaming Sun, Yiming Xie, Linghao Chen,\nXiaowei Zhou, and Hujun Bao.\nNeuralre-\ncon: Real-time coherent 3d reconstruction\nfrom monocular video. In Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 15598\u201315607,\n2021.\n[46] Ay\u00b8ca Takmaz, Elisabetta Fedele, Robert W.\nSumner, Marc Pollefeys, Federico Tombari,\nand Francis Engelmann.\nOpenMask3D:\nOpen-Vocabulary 3D Instance Segmentation.\nIn Advances in Neural Information Process-\ning Systems (NeurIPS), 2023.\n[47] Jesse Thomason, Mohit Shridhar, Yonatan\nBisk, Chris Paxton, and Luke Zettlemoyer.\nLanguage grounding with 3d objects. In Con-\nference on Robot Learning, pages 1691\u20131701,\n2022.\n[48] Nikolaos Tsagkas, Oisin Mac Aodha, and\nChris Xiaoxuan Lu.\nVl-fields: Towards\nlanguage-grounded\nneural\nimplicit\nspa-\ntial\nrepresentations.\narXiv\npreprint\narXiv:2305.12427, 2023.\n[49] Vadim Tschernezki, Iro Laina, Diane Larlus,\nand Andrea Vedaldi. Neural Feature Fusion\nFields: 3D distillation of self-supervised 2D\nimage representations. In 3DV, 2022.\n[50] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang\nTao, Yandong Guo, Mingming Gong, and\nTongliang Liu.\nCris: Clip-driven referring\nimage segmentation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11686\u201311695,\n2022.\n[51] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai\nBi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich\nNeumann.\nPoint-nerf: Point-based neural\nradiance fields. CoRR, abs/2201.08845, 2022.\n[52] Le Xue, Mingfei Gao, Chen Xing, Roberto\nMart\u00b4\u0131n-Mart\u00b4\u0131n, Jiajun Wu, Caiming Xiong,\n18\nRan\nXu,\nJuan\nCarlos\nNiebles,\nand\nSil-\nvio Savarese.\nUlip: Learning unified rep-\nresentation of language, image and point\ncloud for 3d understanding. arXiv preprint\narXiv:2212.05171, 2022.\n[53] Le Xue, Ning Yu, Shu Zhang, Junnan Li,\nRoberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Jiajun Wu, Caim-\ning Xiong, Ran Xu, Juan Carlos Niebles, and\nSilvio Savarese. Ulip-2: Towards scalable mul-\ntimodal pre-training for 3d understanding,\n2023.\n[54] Xingbin Yang, Liyang Zhou, Hanqing Jiang,\nZhongliang Tang, Yuanbo Wang, Hujun Bao,\nand Guofeng Zhang.\nMobile3drecon: real-\ntime\nmonocular\n3d\nreconstruction\non\na\nmobile phone. IEEE Transactions on Visual-\nization and Computer Graphics, 26(12):3446\u2013\n3456, 2020.\n[55] Jianglong\nYe,\nNaiyan\nWang,\nand\nXiao-\nlong Wang.\nFeaturenerf: Learning gen-\neralizable\nnerfs\nby\ndistilling\npre-trained\nvision foundation models.\narXiv preprint\narXiv:2303.12786, 2023.\n[56] Wang Yifan, Felice Serena, Shihao Wu, Cen-\ngiz \u00a8Oztireli, and Olga Sorkine-Hornung. Dif-\nferentiable surface splatting for point-based\ngeometry processing. ACM Transactions on\nGraphics (TOG), 38(6):1\u201314, 2019.\n[57] Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella\nMacaluso, Yuying Ge, Jianglong Ye, Nicklas\nHansen, Li Erran Li, and Xiaolong Wang.\nMulti-task real robot learning with generaliz-\nable neural feature fields. CoRL, 2023.\n[58] Renrui Zhang, Ziyu Guo, Wei Zhang, Kun-\nchang Li, Xupeng Miao, Bin Cui, Yu Qiao,\nPeng Gao, and Hongsheng Li.\nPointclip:\nPoint cloud understanding by clip.\narXiv\npreprint arXiv:2112.02413, 2021.\n[59] Xingyi Zhou, Rohit Girdhar, Armand Joulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl,\nand\nIshan\nMisra.\nDetecting\ntwenty-thousand\nclasses\nusing\nimage-level supervision.\nIn ECCV, pages\n350\u2013368. Springer, 2022.\n[60] Matthias Zwicker, Hanspeter Pfister, Jeroen\nVan Baar, and Markus Gross. Ewa volume\nsplatting. In Proceedings Visualization, 2001.\n19\n"
  },
  {
    "title": "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers",
    "link": "https://arxiv.org/pdf/2401.01974.pdf",
    "upvote": "4",
    "text": "Towards Truly Zero-shot Compositional\nVisual Reasoning with LLMs as Programmers\nAleksandar Stani\u0107\u2217\naleksandar@idsia.ch\nThe Swiss AI Lab, IDSIA, USI & SUPSI\nSergi Caelles\nscaelles@google.com\nGoogle Research\nMichael Tschannen\ntschannen@google.com\nGoogle DeepMind\nAbstract\nVisual reasoning is dominated by end-to-end neural networks scaled to billions of model\nparameters and training examples. However, even the largest models struggle with composi-\ntional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting.\nVisual reasoning with large language models (LLMs) as controllers can, in principle, address\nthese limitations by decomposing the task and solving subtasks by orchestrating a set of (vi-\nsual) tools. Recently, these models achieved great performance on tasks such as compositional\nvisual question answering, visual grounding, and video temporal reasoning. Nevertheless, in\ntheir current form, these models heavily rely on human engineering of in-context examples\nin the prompt, which are often dataset- and task-specific and require significant labor by\nhighly skilled programmers. In this work, we present a framework that mitigates these\nissues by introducing spatially and temporally abstract routines and by leveraging a small\nnumber of labeled examples to automatically generate in-context examples, thereby avoiding\nhuman-created in-context examples. On a number of visual reasoning tasks, we show that\nour framework leads to consistent gains in performance, makes LLMs as controllers setup\nmore robust, and removes the need for human engineering of in-context examples.\n1\nIntroduction\nCompositional visual question answering requires a model to answer questions about visual content in a\ncompositional manner, involving multiple steps of reasoning or considering relationships between different\nobjects or entities within an image. It is a complex task as it requires understanding both the visual\ninformation in an image and the structure of the question, and reasoning about how different visual elements\nrelate to one another to generate the correct answer. Recently, large progress has been made on many such\nvision and language tasks by scaling end-to-end neural networks models in terms of size, training data, and\ncompute (Alayrac et al., 2022; Chen et al., 2022b; Yu et al., 2022; Wang et al., 2022a; Gan et al., 2022; Lu\net al., 2022; Li et al., 2023; Driess et al., 2023; Chen et al., 2023d;c). However, even the largest state-of-the-art\n(SotA) models struggle in tasks that require compositional reasoning, ability to generalize, fine-grained spatial\nreasoning capabilities, and counting (Bugliarello et al., 2023; Paiss et al., 2023; Hsieh et al., 2023; Yuksekgonul\net al., 2022; Zhao et al., 2022; Hendricks & Nematzadeh, 2021). An example of such task is the following\nquery: \u201cCould the cookies on the table be equally distributed among children?\u201d (Sur\u00eds et al., 2023). To\nsolve this, the model needs to detect the cookies in the image, filter out the ones that are not on the table,\ndetect children, count cookies and children, and check if the cookies count is divisible by the children count.\nQuestions like these are difficult for current end-to-end vision and language models (VLMs). Scaling VLMs\nfurther makes them even more data- and compute-hungry(Villalobos et al., 2022), so the scale alone seems\nunlikely to solve these tasks, especially due to the exponentially-large long tail of compositional tasks.\n\u2217Work completed during an internship at Google.\n1\narXiv:2401.01974v1  [cs.CV]  3 Jan 2024\nOn the other hand, it is questionable whether solving compositional tasks with a single monolithic end-to-end\nneural network is the optimal approach. Intuitively, it might be easier to first decompose the task into several\nsubtasks, individually solve the subtasks, and then use the intermediate results to solve the original task.\nThis is reminiscent of the way humans approach compositional problems. According to Daniel Kahneman\u2019s\nframework (Kahneman, 2017), our thought process can be thought of as consisting of two mechanisms: System\n1 and System 2. System 2 is the \u201cslow\u201d, \u201canalytical\u201d system that can decompose the task into subtasks,\nwhile System 1 is the \u201cfast\u201d, \u201creactive\u201d system that solves individual tasks such as recognizing patterns. In\nmachine learning, the early work on task decomposition was pioneered by Neural Module Networks (NMNs)\n(Andreas et al., 2016; Johnson et al., 2017; Hu et al., 2017). NMNs are trained end-to-end in the hope that\nevery module will learn a separate function that will be reusable across tasks. However, these models have a\nnumber of drawbacks, namely that the program generation requires hand-tuned parsers, they are difficult to\noptimize (sometimes requiring reinforcement learning), and they have the issue of a module \u201ccollapse\u201d, where\nsome modules are never activated and others take over all the work, contrary to the design intentions.\nRecently, an alternative approach based on \u201ctool use\u201d gained popularity (Cobbe et al., 2021; Komeili et al.,\n2021; Thoppilan et al., 2022; Parisi et al., 2022; Zeng et al., 2022; Gao et al., 2023; Qin et al., 2023; Zhuge\net al., 2023). In \u201ctool use\u201d, an LLM solves a task by controlling (akin to System 2) a set of tools (such as an\nobject detector, akin to System 1) (Zeng et al., 2022; Shen et al., 2023; Zhuge et al., 2023). In particular,\nVisProg (Gupta & Kembhavi, 2023), ViperGPT (Sur\u00eds et al., 2023), and CodeVQA (Subramanian et al.,\n2023) show great promise in solving visual question answering by using an LLM to generate a program (in\nPython or a custom scripting language). During execution, the program calls individual vision models (such\nas object detector, depth estimator) through an API that is provided in the prompt. For example, to answer\n\u201cWhich color is the jacket of the second person from the left?\u201d (Figure 1a), the program needs to detect\npeople, sort them from left to right, select the second, detect their jacket, and query its color. These models\nachieved SotA on compositional visual question answering, visual grounding, and video temporal reasoning\ntasks. By their construction, they are interpretable, compositional, adaptable (tools can be upgraded on\nthe fly), offer strong generalization, mathematical, and reasoning skills, and do not require gradient-based\ntraining. However, in their current form, they heavily rely on human engineering of in-context (program)\nexamples (ICEs) in the prompt. Moreover, ICEs are dataset- and task-specific. To generate them, significant\nlabor by highly skilled programmers is required. For this reason, we argue that these methods should not be\ncalled zero-shot in their current form.\nIn this work, we present a framework that mitigates these issues, makes LLMs-as-programmers setup more\nrobust, and removes the need for human engineering of ICEs. Our framework, whose effectiveness we show\nacross a number of compositional question-answering and video temporal reasoning tasks with ViperGPT\n(but is universally applicable to other approaches), consists of the following:\n\u2022 Firstly, instead of using a simple API with only basic routines that call individual tools, we introduce\nan \u201cAbstract API\u201d. Abstract API consists of spatially and temporally abstract routines that remove\nthe large burden on the LLM to have strong spatial and temporal reasoning.\n\u2022 Second, instead of relying on a large number of dataset-specific (question, code)-pairs as ICEs,\nwe introduce a setup that generates ICEs automatically. Using a few labeled examples (that are\nsignificantly cheaper to obtain, e.g. via crowd-sourcing), we generate query-code examples in a\nzero-shot manner and use these as ICEs. This mitigates the need for human engineering of ICEs.\n\u2022 Third, we show how LLMs as controllers for visual reasoning can (to some extent) perform \u201cself-\ncorrection\u201d through \u201cself-debugging\u201d and \u201cself-tuning\u201d without any ground truth labels. In \u201cself-\ndebugging\u201d, we generate new code when the previous fails to execute, either by providing the LLM\nprevious query-code pair and execution error as feedback or from scratch. In \u201cself-tuning\u201d, we show\nhow the tool hyperparameters can be tuned automatically if the execution fails due to a module.\n2\n(a)\nQuery\nCode LLM\nSelf-tuning\nVisual\ncode\nPython API\nVisual tools\nAnswer\nAbstract\nroutines\nFew-shot\nI, Q, A\nICEs\nACEs\n(b)\nFigure 1: (a) RefCOCO (Yu et al., 2016) example image. (b) A code-generating LLM takes as input the\nquery, the Python API (functions for \u201ctool use\u201d and Abstract API routines (functions) we introduce in\nSection 2.2) and a number of ICEs (we replace human-engineered ICEs by automatically-generated ACEs in\nSection 2.3). The LLM generates code that takes as input the image and outputs an answer (here a bounding\nbox). If code fails to run, \u201cself-tuning\u201d (Section 2.4) can adjust parameters and generate new code.\n2\nLLMs as programmers for visual reasoning framework\nIn this section, we first provide a brief overview of the ViperGPT approach (Sur\u00eds et al., 2023) on top of\nwhich we show the utility of our framework. We then describe each component of our framework, namely the\nAbstract API, automatic generation of ICEs, and self-correction.\n2.1\nBackground\nViperGPT takes as input an image or a video and a textual query. The textual query is fed into an LLM\n(Codex (Chen et al., 2021)), together with the tools API and ICEs. The LLM generates a program that\nsolves the given query using the tools without further training. The information in the prompt is crucial for\ngood ViperGPT performance, as it is the only task-specific information provided. The prompt consists of a\nPython API with the necessary functions to solve the visual query, such as object detection, depth estimation,\nand language model queries. Additionally, ViperGPT uses several dataset-specific ICEs in the prompt. As\nwe show in Section 3, performance depends heavily on these human-engineered examples.\nViperGPT API defines an ImagePatch and a VideoSegment class that contain image and video processing\nfunctions. Each function calls a pretrained model to compute the result. The API in the prompt does not\ncontain function implementations, but it contains docstrings and query-code examples of their use. The\nViperGPT API defines the following functions: find takes as input an image and a textual query, calls an open\nvocabulary detector and returns a list of image patches of detected objects; exists takes as input an image\nand a textual query and returns true if the query object exists in the image, otherwise false; verify_property\ntakes as input an image, a noun representing an object and an attribute property to verify and returns a\nboolean whether the object has this property; best_image_match that takes as input a list of image patches\nand a textual query and returns the image patch that best matches the query; best_text_match that takes\nas input a list of queries and one image, and returns the query that best matches the image; compute_depth\nthat computes the median depth of an image or image patch; distance which computes the pixel-distance\nbetween two images; simple_query which handles textual queries that are not decomposable, by calling an\nimage captioning model; select_answer that given a context text describing a scene and a list of possible\nanswers queries an LLM to select the correct answer. The VideoSegment class does not contain any functions\nthat call individual models, but only the start and end point of the video segment and an iterator over the\nframes, which returns an ImagePatch object. For the full ViperGPT API, see Appendix A.4.\n3\nThe code-generating LLM outputs code that attempts to solve the query. This code is executed, taking as\ninput an image or a video (and optionally a list of possible answers) and outputs a result (e.g. a bounding\nbox or a string). Due to generating programs in native Python code, ViperGPT avoids the need for custom\ninterpreters and can leverage Python built-in functions (e.g. sort, if/else control flows, math functions, etc.).\n2.2\nAbstract API through visual routines\nWhen programming, we continuously build new layers of abstraction. We start from a set of primitive\nfunctions and then abstract them away by adding new functions. These are then grouped into libraries, and\nadditional layers of abstraction are built on top. By abstracting away the implementation details, we are able\nto build systems with ever-increasing capabilities and complexity. Motivated by this, we introduce a set of\nspatially and temporally abstract functions (routines 1) that abstract away a number of lines of code for the\nsame functionality and together make the Abstract API. From a practical perspective, we are motivated by a\nnumber of failure cases observed in the experiments (see Section 3). As presented in Section 2.1, ViperGPT\u2019s\nAPI is fairly simple (contains almost exclusively functions to call pretrained models). Although simplicity is\ngood and often desirable, in the case of visual reasoning with LLMs as programmers, the lack of expressive\nvisual routines requires the code-generating LLM to have strong spatial and temporal reasoning capabilities.\nQualitative analysis showed that this is often not the case and that the current LLMs are not yet perfect\nin these terms (e.g. they confuse left and right, top and bottom). For example, for the query \u201creturn the\nsecond person from the right\u201d, the program generated by the LLM correctly sorts the persons along the\nhorizontal axis but then wrongly takes the second index in the array (instead of the second last). Similarly,\nthey sometimes \u201cconfuse\u201d temporal order, e.g., if a \u201cbefore\u201d event means a smaller or a larger time index.\nFor these reasons, we introduce a set of spatially and temporally abstract routines. We add the following\nspatial routines: get_patch_left_of, get_patch_right_of, get_patch_above_of, get_patch_below_of\nfor relational retrieval relative to a patch; get_patch_closest_to_anchor_object that sorts patches by their\ndistance to an anchor object and returns the one with the smallest distance; sort_patches_left_to_right,\nsort_patches_bottom_to_top, and sort_patches_front_to_back to sort the list of patches along hor-\nizontal, vertical or depth axis; get_middle_patch to get the middle patch from a given list of image\npatches; For videos, we add temporal routines for event \u201clocalization\u201d: get_video_segment_of_event,\nget_video_segment_before_event, get_video_segment_after_event, and routines to either caption a\nvideo: caption_video or answer a simple question about the video: simple_query. The routines that we\nintroduce are general in the sense that they are not specific to any individual task or dataset. This facilitates\ntheir reuse across tasks and avoids engineering task and dataset-specific routines. It is an open research\nquestion what the \u201coptimal\u201d set of primitive routines is. Another exciting research direction is using LLM\nwith their own abstract routines, then reusing those to come up with even more abstract routines and so on.\nWe leave these for future work.\n2.3\nAutomatic generation of in-context examples\nIn-context examples (ICEs) greatly influence the performance of LLMs (Brown et al., 2020; Chen et al.,\n2023b). For example, ViperGPT (Sur\u00eds et al., 2023) uses between 10 and 16 hand-engineered dataset-specific\nquery-code ICEs per dataset. Similarly, VisProg (Gupta & Kembhavi, 2023) uses between 16 and 31 ICEs\nand CodeVQA (Subramanian et al., 2023) about 50 ICEs. However, constructing these ICEs requires heavy\nhuman engineering, as they might need to be rewritten in a way that the LLM can use them to \u201cgeneralize\u201d\nto other examples across the dataset. Furthermore, the constructed examples are specific not only to the\ndataset but also to the LLM and the API. If any of those changes, they need to be written from scratch.\nFinally, to write good query-code ICEs, highly skilled labor is required, ideally someone familiar with the\nworkings of LLMs and a good grasp of Python programming.\nIn our work, we move beyond this need for human engineering of query-code ICEs. We start from a small set\nof labeled examples (e.g. 16 image-question-answer tuples), as is common in few-shot transfer learning (Zhai\net al., 2019; Kolesnikov et al., 2020). We run our framework in a zero-shot manner (without any ICEs) on\nthese few-shot examples, sort the results by accuracy, select the best-performing programs, pair them with\n1Note that our \u201croutines\u201d do not correspond to the visual routines of Ullman (1987) such as tracing or scanning.\n4\nthe corresponding queries, and use them as ICEs during test time. We call such ICEs automatically-generated\nin-context examples (ACEs). Importantly, no gradient-based optimization is performed on the few-shot\nexamples. Intuitively, this works since even if the LLM does not always generate a program that solves\nthe task correctly, it might sometimes come up with a correct program. Since retrieval is often easier than\ngenerating programs from scratch, the reuse of the correct programs improves performance on the test set.\nACEs provide a number of benefits over manually writing ICEs. First of all, ACEs are much cheaper to\nobtain as they do not require highly skilled labor to write them. Second, the algorithm that generates ACEs\nis general: it is neither specific to the API nor the LLM. If any of these changes, ACEs can be easily generated\nby re-running the algorithm. Furthermore, they can be seen as a first step of the LLM \u201ccoming up\u201d with\nits own abstract rules and thus creating a \u201crulebook\u201d (discussed in Section 2.2). Finally, few-shot (image,\nquestion, answer)-labeled examples are often available in datasets typically used in machine learning. If not\navailable, they are cheap to obtain via crowd-sourcing and can be reused for further studies as a benchmark.\n2.4\nSelf-correction\nOne of the advantages of solving visual reasoning tasks with LLMs as programmers is that we know when\ncode fails to execute. The failure can happen, e.g. due to a compilation error (e.g. due to hallucination),\nsome of the models failing, or a wrong return type (e.g. a bounding-box is expected, but code returns a\nstring). Note that to detect these types of errors, no ground-truth labels are needed.\nSelf-debugging.\nIf the code execution fails, we can query the code-generating LLM to correct the previously\ngenerated code. We do this by feeding back the query, the previously generated code, and the resulting error\nin the prompt (see the feedback template in Appendix A.3). Moreover, if the LLM\u2019s sampling temperature is\nhigher than zero, we can query the model with a different random seed to generate new code from scratch.\nThere are advantages to both of these approaches. If the code-generating LLM has good \u201cself-correction\u201d\nabilities, then it should be able to correct its own mistakes based on the feedback, as we humans could.\nHowever, if the LLM is not good at self-correction or does not know how to incorporate such feedback (e.g. if\nthe LLM is not trained to be conversational), then feeding back the previous query and code will only \u201cbias\u201d\nthe model to output the same solution. In that case, generating new code from scratch could work better.\nSelf-tuning.\nIn some cases, we know that code execution failed due to some components of a particular\nmodule. For example, the open vocabulary detector fails due to a too high threshold hyperparameter. When\nthe threshold is high, we have a higher number of false negatives. For such cases, we propose to automatically\nchange the hyperparameter of the module (e.g. reduce the threshold) and execute code again.\n3\nExperiments\nTasks.\nWe evaluate our method on four datasets: RefCOCO, RefCOCO+ (Yu et al., 2016), GQA (Hudson\n& Manning, 2019) and NExT-QA (Xiao et al., 2021) used in previous work (Sur\u00eds et al., 2023). These datasets\nevaluate a diverse set of capabilities, namely visual grounding (RefCOCO, RefCOCO+), compositional\nimage question answering (GQA), and video temporal reasoning (NExT-QA). In RefCOCO (example in\nFigure 1a), the task is to detect a bounding box of an object given its natural language description (\u201creferring\nexpression\u201d). In compositional question answering in GQA, the task is to answer in natural language a\ncompositional natural language query. We use the \u201ctest-dev\u201d split of the GQA dataset, as in ViperGPT. In\nNExT-QA, the task is to answer a temporally compositional question by selecting one of the given multiple\nchoice options. As in ViperGPT, we use NExT-QA \u201chard\u201d split Buch et al. (2022). For RefCOCO and\nRefCOCO+, methods are evaluated in terms of Intersection over Union (IoU) between the detected and the\nground truth bounding box and for GQA and NExT-QA in terms of accuracy of the predicted answer.\nVision and Language Models.\nFor code generation, we use a code instruction-tuned version of PaLM 2\n(Anil et al., 2023) code-bison accessible via the Google Cloud API (Google, 2023). We use the same model\nto select an answer for the multichoice questions in the NExT-QA dataset. Vision models we use are OWLv2\n(Minderer et al., 2023) for object detection, SigLiT (Zhai et al., 2023) for text-image comparison, MiDaS\n5\nTable 1: Comparison of our method against previous end-to-end and \u201cLLMs as controllers\u201d SotA methods.\nFor \u201cOurs (code-bison)\u201d, we report mean scores \u00b1 standard deviation across three random seeds. The\nreference numbers for SotA on each dataset are taken from the following publications: RefCOCO: ZS (Yang\net al., 2023b), FS (Yao et al., 2021), Sup (Wang et al., 2022b); RefCOCO+: ZS (Yang et al., 2023b), FS\n(Yao et al., 2021), Sup (Wang et al., 2022b); GQA: ZS (Li et al., 2023), FS (Jin et al., 2021), Sup (Nguyen\net al., 2022); NExT-QA: ZS (Chen et al., 2023d), FS (Chen et al., 2023d), Sup (Ye et al., 2023).\nModel\nRefCOCO (IoU)\nRefCOCO+ (IoU)\nGQA (acc.)\nNExT-QA (acc.)\nZero-Shot (ZS) SotA\n53.0\n57.5\n44.7\n38.3\nFew-Shot (FS) SotA\n53.3\n52.5\n35.7\n38.3\nSupervised (Sup) SotA\n94.0\n91.7\n72.1\n63.1\nViperGPT (paper)\n72.0\n67.0\n48.1\n60.0\nViperGPT (GitHub (GH) ZS)\n46.7\n-\n-\n-\nViperGPT (GH w/ DS-ICEs)\n60.5\n-\n-\n-\nE2E bsl.(ZS OWLv2/PaLI-3)\n33.5\n31.7\n40.1\n58.9\nE2E LLM-only baseline\n-\n-\n-\n53.3\nOurs (code-bison, Zero-Shot)\n44.4 \u00b1 0.9\n38.2 \u00b1 0.0\n32.1 \u00b1 0.4\n60.2 \u00b1 0.3\nOurs (code-bison)\n51.2 \u00b1 0.2\n45.7 \u00b1 0.1\n33.4 \u00b1 0.2\n61.0 \u00b1 0.1\n(Ranftl et al., 2020) for depth estimation, and PaLI-3 (Chen et al., 2023d) for image captioning and answering\nvisual queries. Note that all models are different from the models that ViperGPT used (see Appendix A.2).\nBaselines.\nStrong baselines are essential for correctly measuring progress in machine learning. This is\nespecially true in the emerging area of \u201ctool use\u201d (Cobbe et al., 2021; Komeili et al., 2021; Thoppilan et al.,\n2022; Parisi et al., 2022; Zeng et al., 2022; Gao et al., 2023; Qin et al., 2023; Zhuge et al., 2023). When using\nan LLM and other pre-trained models, we must be careful to report the exact LLM version and/or API when\nit was accessed, and ideally report results over several random seeds to measure the statistical significance. In\nTable 1, we provide an overview of the previous Zero-Shot (ZS), Few-Shot (FS), and Supervised (Sup) SotA\nmethods, ViperGPT, end-to-end (E2E) baselines, and our results on all datasets we used for evaluation.\nEarly in the project, we found it difficult to reproduce the results reported in the ViperGPT paper. Our\nfirst hypothesis is that this is due to differences in the vision and language models we use compared to the\nViperGPT paper. However, when running the original ViperGPT code from the official GitHub repository\non RefCOCO, we were only able to achieve an IoU of 60.5 as opposed to 72.0 reported in the paper. Note,\nhowever, that ViperGPT uses Codex, which is discontinued, so we use GPT-3.5-turbo (OpenAI, 2023). Also\nnote that this score was obtained using 16 dataset-specific ICEs (DS-ICEs). These examples contain large\namounts of dataset-specific human-engineered information, such as \u201cclothing requires returning the person\u201d.\nIn the case of truly Zero-Shot learning (without any human-engineered ICEs), the IoU score of ViperGPT\u2019s\nofficial GitHub code drops by 14 points to 46.7. Moreover, in their GitHub code we found hand-engineered\nimprovements: if returning a bounding box fails, then return an \u201caverage\u201d bounding box, and if code execution\nfails on GQA, then query the image captioner (BLIP-2 (Li et al., 2023)). These code changes lead to improved\nresults, but make it hard to quantify the true power of LLMs as controllers approach.\nIn Table 1, we also provide Zero-Shot end-to-end baselines. On RefCOCO and RefCOCO+, we feed the\nquery as input to the OWLv2 model and return its output. For the GQA end-to-end baseline, we query the\nPaLI-3 model (which was fine-tuned for multi-task inference on different captioning and VQA data sets, but\nnot on GQA). For NExT-QA, we provide two baselines. For the first baseline, we subsample and caption with\nPaLI-3 one frame per second, and then feed all these captions to an LLM (code-bison) together with the\nquestion and multiple choice answers. As the second baseline, we simply feed the LLM the question and the\npossible answers and ask it to choose one. LLM-only baseline achieves an accuracy of 53.3%, which is only\n5.4% lower than the baseline that also gets videos as input and significantly above the chance level accuracy\n6\nViperGPT\nAbstract\nAPI\n40\n42\n44\n46\n48\n50\nIoU\nRefCOCO\nno ACE\nw/ ACE\nViperGPT\nAbstract\nAPI\n34\n36\n38\n40\n42\n44\nIoU\nRefCOCO+\nno ACE\nw/ ACE\nViperGPT\nAbstract\nAPI\n26\n28\n30\n32\n34\n36\nAccuracy\nGQA\nno ACE\nw/ ACE\nViperGPT\nAbstract\nAPI\n30\n40\n50\n60\nAccuracy\nNExT-QA\nno ACE\nw/ ACE\nFigure 2: Using our Abstract API improves performance over the ViperGPT API across all datasets. Similarly,\nACEs consistently improve performance, and these gains compound with the gains from the Abstract API.\nUncertainty bars represent standard deviations computed over three random seeds.\nof 20% (since there are 5 possible multichoice answers to each question). This suggests that NExT-QA might\nnot fully evaluate the vision properties of the model and that new datasets are needed; for example, the\nPerception Test (P\u0103tr\u0103ucean et al., 2023) was created specifically to avoid such problems.\nLastly, in Table 1 we report the Zero-Shot performance in our setup, as well as results for our best performing\nmodel variant (averaged over three random seeds). In the following sections, we evaluate each component,\nas well as their combinations, to obtain the reported results. Using GPT-3.5-turbo instead of code-bison\nresulted in a slight drop in performance, but as we shall see below, the same conclusions hold for both\ncode-bison and GPT-3.5-turbo for all our suggested improvements.\nIn the following, we present the\ncomponents of our framework. For an ablation study, see Table 2 that shows that each component contributes\npositively to the final scores on each dataset.\n3.1\nZero-Shot through spatially and temporally Abstract API\nIn Figure 2, we show the effect of using our Abstract API instead of the API used in ViperGPT. The API for\nRefCOCO, RefCOCO+, and GQA uses the same set of image routines (see Appendix A.4), whereas the API\nfor NExT-QA uses only video-specific (temporal) routines. For now, we focus on the brown bars in Figure 2,\nand we compare the ViperGPT API and our Abstract API. We can see that our Abstract API leads to gains\nboth with and without ACEs across all datasets. The performance gain is most notable for the NExT-QA\ndataset when ACEs are not used. We suspect that this is due to the LLM\u2019s difficulty in reasoning about\nthe temporal order of events. This confirms our hypothesis that building a more abstract API such that the\nLLM does not need to use low-level Python routines is a promising direction.\nFinally, we investigate whether our conclusions also hold for other LLMs, namely OpenAI\u2019s GPT-3.5-turbo.\nOn RefCOCO GPT-3.5-turbo achieves an IoU of 28.9 with the ViperGPT API and 39.8 with our Abstract\nAPI and an accuracy of 9.4 and 42.9 for the ViperGPT API and our Abstract API, respectively, on NExT-QA.\nThis confirms that our Abstract API brings gains not only for code-bison, but also for other LLMs. For each\nsample in the evaluation, we allow only one trial of code generation (no self-correction). Compared to the\nresults with code-bison, IoU of 37.7 and 40.5 on RefCOCO and accuracy of 11.5 and 46.1 on NExT-QA for\nthe ViperGPT API and our Abstract API, respectively, the results with GPT-3.5-turbo are slightly worse.\nWe suspect that the reason for this could be that GPT-3.5-turbo is mainly built to be a conversational agent,\nwhile code-bison is specifically trained to have good coding capabilities.\n3.2\nFew-shot boostrapping via automatically generated in-context examples (ACEs)\nWe evaluate the effect of using automatically generated in-context examples (ACEs), described in Section 2.3.\nWe can either sample few-shot examples manually or pick them at random. Both of these variants lead to\ngood results, as we show in the following experiments. However, selecting examples manually allows for a\nbetter \u201cquality\u201d (in terms of diversity) given a small number of few-shot examples, so by default we use these\nfor all experiments. For the first set of experiments, we manually pick 16 few-shot examples from the training\n7\n0\n4\n8\n16\nNumber of ACEs\n40\n42\n44\n46\n48\n50\nIoU\nRefCOCO\nViperGPT API\nAbstract API\n0\n4\n8\n16\nNumber of ACEs\n34\n36\n38\n40\n42\n44\nIoU\nRefCOCO+\nViperGPT API\nAbstract API\n0\n4\n8\n16\nNumber of ACEs\n29.5\n30.0\n30.5\n31.0\n31.5\n32.0\n32.5\nAccuracy\nGQA\nViperGPT API\nAbstract API\n0\n4\n8\n16\nNumber of ACEs\n30\n40\n50\n60\nAccuracy\nNExT-QA\nViperGPT API\nAbstract API\nFigure 3: Increasing the number of ACEs in the prompt improves performance. Note that using the ViperGPT\nAPI on NExT-QA results in only three correct ACEs, so the performance plateaus after four ACEs.\nset: image/video, question, and ground-truth answer. We try to make examples diverse to cover question\n\u201ctypes\u201d (e.g. left-right, front-back, closest-to, etc.).\nIn Figure 2, we show the effect of ACEs. For each dataset and for each API, the performance without ACEs\nis shown with the brown bar, and the performance with ACEs corresponds to the blue bar. We can see\nthat for all datasets and all APIs, ACEs improve performance. The largest gains when using ACEs are\nfor RefCOCO, RefCOCO+, and NExT-QA datasets when using the ViperGPT API. This indicates that\nACEs are effective in dealing with complex spatial and temporal reasoning. More importantly, it can be seen\nin Figure 2 that the gains from both the Abstract API and ACEs compound for all tasks, indicating that\nthey provide complementary strengths. Figure 3 shows how the performance in terms of IoU and accuracy\nscales with the number of few-shot examples used to generate ACEs. As expected, increasing the number of\nfew-shot examples leads to improved performance. Note that the ViperGPT API on NExT-QA is able to\ncorrectly \u201csolve\u201d only 3 few-shot examples, so there are no gains beyond using 4 few-shot examples.\nWe evaluate the effect of using randomly sampled few-shot examples instead of manually selecting them. On\nRefCOCO, we sample 100 few-shot random samples from the training set, run Zero-Shot framework on them,\nsort the resulting programs by their IoU, and select the top 16 programs. Therefore, we end up with the\nsame number of ACEs as with manual selection. On RefCOCO, we achieve IoU of 47.9 and 49.1 with the\nViperGPT API and our Abstract API respectively. These results are slightly better than those with manually\nselected few-shot examples (46.9 and 48.2 IoU). This shows that the manual labor for generating ACEs can\nbe removed altogether if we already have some labeled examples. With 50 few-shot random samples we\nobtain similar performance, and for 16 such samples we observe a small drop in performance (see Tables 5\nand 6 in the Appendix A.1 for detailed results).\nAs in the previous section, we test whether our findings are consistent with GPT-3.5-turbo on RefCOCO\nand NExT-QA. On RefCOCO, when using GPT-3.5-turbo, ACEs improve IoU from 28.9 to 39.8 with the\nViperGPT API and from 38.1 to 41.6 with our Abstract API. Similarly, for GPT-3.5-turbo on NExT-QA,\nACEs improve accuracy from 9.4 to 42.9 the ViperGPT API and from 56.7 to 58.8 with our Abstract API.\nThis confirms that the benefit of ACEs is not only limited to code-bison but also holds for GPT-3.5-turbo\nas well.\nAnother benefit of the few-shot setup when generating ACE is that it allows us to \u201ctune\u201d hyperparameters\n(HPs). For example, when sweeping over LLM temperature and object detection threshold HPs, we observed\nthat the relative performances on the few-shot examples closely resemble the one when sweeping over the full\nvalidation dataset (the analysis is shown in Appendix A.1).\n3.3\nSelf-correction\nIn this section, we analyze the ability of the framework to \u201cself-correct\u201d itself without any external feedback\nwhen code execution fails.\n8\n1\n2\n3\n4\n5\nNumber of ACEs\n38\n40\n42\n44\n46\n48\n50\n52\nIoU\nRefCOCO\nVip. API\nVip. API w/ ACE\nAbs. API\nAbs. API w/ ACE\nDyn.t=0.1\nDyn.t=0.15\n1\n2\n3\n4\n5\nNumber of ACEs\n32\n34\n36\n38\n40\n42\n44\n46\nIoU\nRefCOCO+\n1\n2\n3\n4\n5\nNumber of ACEs\n28\n29\n30\n31\n32\n33\n34\nAccuracy\nGQA\n1\n2\n3\n4\n5\nNumber of ACEs\n10\n20\n30\n40\n50\n60\nAccuracy\nNExT-QA\nFigure 4: Increasing the number of \u201cself-tuning\u201d steps leads to improved performance. Our Abstract API\n(Abs. API) consistently outperforms the ViperGPT API (Vip. API). The best performance is achieved when\nusing dynamic object detector threshold (Dyn.t) in addition to the Abstract API with ACE.\nTable 2: Component-wise ablations of our framework. Each component contributes positively to the final\nscore. Their relative contributions vary for different tasks. We report mean scores across three random seeds.\nModel\nRefCOCO (IoU)\nRefCOCO+ (IoU)\nGQA (acc.)\nNExT-QA (acc.)\nViperGPT API\n38.4\n32.0\n27.9\n11.5\n+ Abstract API\n42.3 (+3.9)\n34.0 (+2.0)\n30.0 (+2.1)\n57.6 (+46.1)\n+ ACE\n47.3 (+5.0)\n41.7 (+7.7)\n30.6 (+0.6)\n60.7 (+3.1)\n+ Self-debugging\n48.2 (+0.9)\n42.9 (+1.2)\n32.4 (+1.8)\n61.0 (+0.3)\n+ Self-tuning\n51.2 (+3.0)\n45.7 (+2.8)\n33.4 (+1.0)\n-\nSelf-debugging.\nWhen the program execution fails (due to e.g. compilation errors), we can retry by\ngenerating a new program (Chen et al., 2021). When creating a new program, we can also feed the previously\ngenerated code and the question as part of the prompt (see Appendix A.4), a variant that we call \u201cself-\ndebugging\u201d. Another option would be to simply repeat the exact same prompt as in the previous trial and\nrely on stochasticity in the LLM with a temperature greater than zero to generate a new correct solution. In\nour experiments, the \u201cself-debugging\u201d variant did not lead to an improvement in performance. In all cases,\nthe performance plateaus after the first trial. This is in line with other recent findings Huang et al. (2023);\nStechly et al. (2023); Valmeekam et al. (2023). On the other hand, the variant without any feedback in the\nprompt led to an increasingly better performance as the number of \u201ctrials\u201d increases (see Figure 4).\nSelf-tuning.\nIn some cases, we know that code fails due to some specific module. Therefore, we can then\nadjust the hyperparameters of the respective module and re-run code. For example, the open vocabulary\ndetector we used (OWLv2) has a threshold hyperparameter that controls how sensitive it is. The lower\nthis threshold, the more false positives we will have, but also the fewer false negatives. There is no global\n\u201coptimal\u201d value for this threshold that performs best for all images. Our framework allows us to adjust this\nhyperparameter dynamically: if the open vocabulary detector fails, we can lower the threshold and run\nthe visual program again. In Figure 4, we can see that variants with a dynamic object detection threshold\noutperform all other variants and achieve the best performance. Note that the variant that achieves the\nhighest performance after five trials has a lower performance for the first trial. This happens because we\nstart with a higher object detection threshold value of 0.15 (by default, we use 0.1). In this case, initially\nthere will be more false negatives, but also fewer false positives. As we decrease the threshold in subsequent\ntrials, the previously false negatives are detected and the queries are correctly answered.\n3.4\nError analysis\nAnother benefit of visual reasoning with LLMs as programmers is interpretability. For example, we can get\ninsights into the percentage of successful program executions, which can be further decomposed into the ones\nthat resulted in correct or incorrect responses, and for the programs that failed to execute, we can provide\n9\nIoU[0.7, 1] IoU[0.3, 0.7) IoU(0, 0.3)\nIoU0\nErrObj. Det ErrRet. Type\nErrOther\nViperGPT API\n0\n10\n20\n30\n40\n50\n%\nViperGPT API\n+ ACE\n+ ACE + Dynamic Threshold\nIoU[0.7, 1] IoU[0.3, 0.7) IoU(0, 0.3)\nIoU0\nErrObj. Det ErrRet. Type\nErrOther\nAbstract API\n0\n10\n20\n30\n40\n50\n%\nAbstract API\n+ ACE\n+ ACE + Dynamic Threshold\nFigure 5: Error diagrams for the ViperGPT API and our Abstract API. We visualize the percentages of\nsamples with IoU in certain ranges. \u201cErr\u201d classes are samples for which code execution failed due to either:\nobject detection (Obj.Det), wrong return type (Ret.Type) or some other error (Other) e.g. hallucination.\nfurther insights into why they failed, i.e. which module failed to return the correct result. Figure 5 shows\none such error analysis on RefCOCO. Categories labeled with \u201cError\u201d are the ones for which code failed to\nexecute due to either object detection (Obj.Det), wrong return type (Ret.Type) or some other error (Other)\ne.g. hallucination. For all other cases, code executed correctly (it returned a bounding box), but sometimes\nit failed to detect the object (\u201cIoU = 0\u201d case). First, we notice that for both APIs the number of \u201ccorrect\u201d\ndetections (IoU higher than 0.7) grows as we include ACEs and \u201cself-tuning\u201d through the dynamic object\ndetection threshold. We can also see that the percentages of samples with high IoU are always higher for our\nAbstract API compared to the ViperGPT API. Finally, note that the percentage of error cases drops from\n12.8% to 4.8% for the ViperGPT API and from 9.1% to 3.8% for our Abstract API.\n4\nRelated work\nVisual reasoning with end-to-end monolithic models.\nRecently, SotA on VQA has been largely\nobtained by scaling end-to-end vision and language models (VLMs) in terms of their size, training data,\nand compute (Alayrac et al., 2022; Chen et al., 2022b; Yu et al., 2022; Wang et al., 2022a; Gan et al.,\n2022; Lu et al., 2022; Li et al., 2023; Driess et al., 2023; Chen et al., 2023d;c). One of the earliest VLMs\nFlamingo (Alayrac et al., 2022) used a frozen pretrained language model of up to 70B parameters and a\nfrozen pretrained image encoder with 435M parameters and trained only a \u201ccross-attention\u201d module that\nserved as an interface between them. Since then, efforts have mainly gone into scaling both the image and\nthe language models: GIT (Wang et al., 2022a) used a 300M language and scaled the image encoder to 4.8B\nparameters; PaLI (Chen et al., 2022b) scaled both components jointly, language model to 17B and image\nencoder to 4B parameters; PaLI-X (Chen et al., 2023d) continued this trend of scaling the total number of\nparameters to 55B by using an image encoder with 22B parameters; PaLM-E scaled the number of total\nparameters in the VLM to 562B by integrating the 540B PaLM (Chowdhery et al., 2022) and the 22B Vision\nTransformer (Dosovitskiy et al., 2020; Dehghani et al., 2023). On the other hand, BLIP-2 (Li et al., 2023)\nachieved SotA performance on various tasks with a 12B VLM and PaLI-3 Chen et al. (2023d) introduced a\nsignificantly smaller VLM with 5B total parameters that achieves competitive performance with SotA models\non various VLM benchmarks. These models are typically pretrained on large amounts of data and then\nfine-tuned for the best performance on the downstream tasks. In contrast to this, visual reasoning with LLMs\nas programmers (Gupta & Kembhavi, 2023; Sur\u00eds et al., 2023; Subramanian et al., 2023) does not require\nany fine-tuning or gradient updates on the downstream tasks. Moreover, even the largest VLMs struggle\non the tasks that require compositional reasoning, the ability to generalize, fine-grained spatial capabilities,\nand counting (Bugliarello et al., 2023; Hsieh et al., 2023; Tschannen et al., 2023). Further scaling makes\nthem even more data- and compute-hungry; therefore, it is unclear whether scaling alone can solve these\ntasks. Conversely, using LLMs as programmers enables task decomposition into subtasks and holds promise\nof strong generalization and compositional reasoning.\n10\nVisual reasoning with Modular Networks.\nNeural Modular Networks (NMNs) (Andreas et al., 2016;\nJohnson et al., 2017; Hu et al., 2017) are an alternative approach to monolithic end-to-end networks and\noffer a potential route to (compositional) generalization. They are also typically trained end-to-end, using\nsupervised or reinforcement learning. NMNs are designed to have several modules, and the hope is that\nduring training, each module will learn a different functionality, which will then be reusable across tasks.\nHowever, these models have a number of drawbacks: the program generation requires hand-tuned parsers,\nand they require optimization through reinforcement learning (e.g., REINFORCE Williams (1992)), which\nis often unstable. Learning all modules end-to-end hinders their ability to generalize (Bahdanau et al.,\n2018) and sometimes leads to a mode \u201ccollapse\u201d, where some modules take over all the work and other\nmodules are never activated, or modules that do not learn intended functionalities (Subramanian et al.,\n2020). Furthermore, they sometimes require supervision for program learning, which is difficult to obtain\nat scale. On the other hand, visual reasoning with LLMs as programmers mitigates many issues of NMNs:\nit does not require gradient-based training or finetuning, it is able to incorporate any modules or swap the\nexisting ones, it leverages the strong generalization ability of LLMs, and it generates programs by utilizing\nthe in-context learning ability of LLMs, thereby removing the need for training program generators. The\nprograms generated by LLM do not have to be domain specific, but can use a common language such as\nPython that does not require custom interpreters.\nVisual reasoning with LLMs as programmers.\nThe field of LLMs as controllers for visual reasoning\nhas received a great deal of interest recently. LLMs as controllers (also known as \u201ctool use\u201d) approach became\nprominent in the literature (Parisi et al., 2022; Schick et al., 2023), in particular for structured reasoning\nin the natural language domain (Madaan et al., 2022; Wang et al., 2023b; Gao et al., 2023; Chen et al.,\n2022a). In the domain of using LLMs as controllers for visual reasoning, PICa (Yang et al., 2022) solves a\nknowledge-based VQA task by first extracting an object and captions from the image and then querying\nGPT-3 with this information and in-context examples to answer a question. Socratic models (Zeng et al.,\n2022), HuggingGPT (Shen et al., 2023), and Societies of Mind (Zhuge et al., 2023) compose vision and\nlanguage models to \u201ccommunicate\u201d in a fixed \u201cprotocol\u201d and solve tasks such as image captioning, visual\nquestion answering, image generation, and robot planning. On the other hand, models such as VisProg\n(Gupta & Kembhavi, 2023), ViperGPT (Sur\u00eds et al., 2023) and CodeVQA (Subramanian et al., 2023) go\nbeyond a fixed communication \u201cprotocol\u201d by having LLM write (Python) programs. During execution, the\nprogram calls individual vision modules (such as the object detector and depth estimator) through an API\nthat is provided in the prompt. Additionally, VisProg is also capable of generating images as program output.\nThese models showed great performance and achieved SotA on tasks such as compositional visual question\nanswering, visual grounding, and video temporal reasoning. However, in their current form, these models rely\non heavy human engineering of query-code examples in the prompt that are dataset- and task-specific and\nrequire significant labor by highly skilled workers. Our framework, on the other hand, is able to automatically\ngenerate in-context examples, removing the need for humans to write query-code examples, uses an Abstract\nAPI that puts less burden on the LLM to have strong spatial and temporal reasoning abilities, and proposes\na way to \u201cself-correct\u201d the programs that failed to execute. Note that our framework is equally applicable to\nall of the above approaches.\nAutomatizing prompt engineering.\nVast literature shows that prompt format and contents are often\nimportant for achieving good performance with an LLM (Reynolds & McDonell, 2021; Zhao et al., 2021; Lu\net al., 2021; Moradi & Samwald, 2021; Madaan & Yazdanbakhsh, 2022; Wei et al., 2023). A prompt typically\nconsists of a task description (in natural language), in-context examples (e.g. query-code in ViperGPT) and\nan API (in the case where LLMs write a program). Various prompting techniques have been engineered, such\nas Chain-of-Thought prompting (Wei et al., 2022), Self-Consistency (Wang et al., 2022c), Tree of Thoughts\n(Yao et al., 2023), Graph of Thoughts (Besta et al., 2023), Plan-and-Solve Prompting (Wang et al., 2023a),\nLeast-to-Most Prompting (Zhou et al., 2022a), etc. All these techniques rely on human prompt engineering,\nin particular, on in-context examples. On the other hand, some methods try to automate prompt engineering.\nThey sometimes use gradient-based optimization (Shin et al., 2020; Gao et al., 2020; Wen et al., 2023) and\nsome approaches require only API access to the model (Xu et al., 2022; Prasad et al., 2022). Other works\nuse LLMs for prompt optimization. APE (Zhou et al., 2022b) first generates instructions with an LLM,\nthen selects instructions with the highest accuracy, and uses them for future LLM prompts. APO (Pryzant\n11\net al., 2023) generates feedback with an LLM that informs how to update the previous instruction. OPRO\n(Yang et al., 2023a) uses an LLM to generate new instructions at each optimization step, asking the LLM to\nimprove task accuracy by changing task instructions, which requires determining the score on a small set\nof labeled examples and providing it in the meta-prompt. Promptbreeder (Fernando et al., 2023) goes a\nstep further and proposes a self-referential self-improvement LLM using a meta-prompt that controls the\ngeneration of the main (task) prompt and evolves both via mutation. More importantly, Promptbreeder shows\nsome surprising results such that a simple prompt \u201cSOLUTION\u201d outperforms all previous approaches. This\nfurther demonstrates the sensitivity of LLMs and the importance of automatizing the prompt engineering\nprocess. Common to all above frameworks for automatizing prompting is that they automatize the \u201ctask\ndescription\u201d part of the prompt. On the other hand, in our framework, we automatize the generation of\nin-context examples, which might have an even greater influence on the performance of the LLM.\nLLMs and self-correction.\nIn the LLM literature, there have been mixed findings on the ability of LLMs\nto critique and self-correct their own reasoning and outputs. Self-Refine (Madaan et al., 2023) provides\nfeedback to the LLM of the previously generated output, which is then refined. Several other approaches\nshow benefits of providing feedback to LLM in improving reasoning (Shinn et al., 2023; Madaan et al., 2023),\ncode generation (Chen et al., 2023e; Olausson et al., 2023; Chen et al., 2023a), improving LLM alignment\n(Bai et al., 2022; Ganguli et al., 2023), etc. On the other hand, there has been increasing evidence that\nLLMs cannot self-correct reasoning yet (Huang et al., 2023; Stechly et al., 2023; Valmeekam et al., 2023),\nunless they receive external feedback, which usually requires access to ground truth labels. In our work,\nwe also found that providing the previous question and code as feedback to the model did not improve the\nresults. However, we show that it is possible to improve performance by tuning hyperparameters on the fly, a\ndirection that, to the best of our knowledge, has not been explored previously.\n5\nDiscussion and future work\nAlthough the LLMs as controllers framework is very promising for visual reasoning, there is much future\nwork to be explored. First, the use of video-specific models (or tools) could greatly improve performance on\nvideo tasks compared to the image-specific models we used. Moreover, the code generating LLM currently\nonly takes the question as the input, but for some questions the program that correctly solves the question\ncan only be generated given the image or video as the input too.\nThe results with the Abstract API show that this is a promising path forward, but more research is needed\nto find the \u201coptimal\u201d set of visual and temporal routines. Starting from these primitive routines, the model\nshould be able to build an ever-growing library of routines (e.g. through the ACE generation process) that it\ncan later reuse. This growing library of routines will most likely grow larger than the size of the context\nwindow, so research is needed on an API \u201crouter\u201d that can select routines that are relevant to a specific task\nat hand. Furthermore, it would be important to research ways of eliminating the need for few-shot examples\nwhen generating ACEs, e.g. by providing a natural language dataset specification (a datasheet).\nLastly, more effort should be put into creating better benchmarks for evaluating compositional visual reasoning,\nas current ones have a number of limitations. For example, not all samples in RefCOCO and RefCOCO+\nrequire compositional reasoning, so the LLM should only query the open-vocabulary object detector. Similarly,\nmany referring expressions in GQA are ambiguous in the sense that there is not a single unique answer.\nFinally, NExT-QA contains ambiguous questions (e.g. why someone did certain actions) or questions that\ncan be answered by looking at the multiple choice answers only and disregarding the visual input altogether.\nThe Perception Test (P\u0103tr\u0103ucean et al., 2023) is a promising benchmark for future work, as it was specifically\ncreated to avoid such problems. We hope that our findings inform future research on LLMs as controllers for\nvisual reasoning and encourage systematic evaluation and benchmarking efforts in the future.\n6\nConclusion\nIn this work, we present a framework that makes LLMs as programmers for visual reasoning more robust,\nremoves the need for human engineering of in-context examples (ICEs), and thus brings them a step closer to\n12\ntruly zero-shot visual reasoners. We introduce an \u201cAbstract API\u201d that consists of spatially and temporally\nabstract routines, which improves performance by reducing the burden on the code-generating LLM to have\nstrong spatial and temporal reasoning. By using a few labeled examples, we show how one can generate\nquery-code ICEs automatically (ACEs) in a zero-shot manner. When used as in-context examples, ACEs\nconsistently improve performance, eliminating the need for human engineering of ICEs. We demonstrate\nhow LLMs as controllers for visual reasoning can (to a certain extent) perform \u201cself-correction\u201d through\n\u201cself-debugging\u201d and \u201cself-tuning\u201d without any ground-truth labels. In self-debugging, generating new code\nfrom scratch led to consistently better results, but providing the previous query-code pair as feedback to\nthe LLM did not improve performance. In self-tuning, we show that hyperparameters of certain modules\ncan be tuned automatically if code execution fails due to these modules. Across a number of compositional\nquestion-answering and video temporal reasoning tasks, we demonstrate that each component of our framework\nconsistently leads to improvement.\nAcknowledgments\nWe thank Michael Mozer and Sjoerd van Steenkiste for useful discussions throughout the project. We also\nthank Michael Mozer for his feedback on an earlier version of this manuscript.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 39\u201348, 2016.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403, 2023.\nDzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and\nAaron Courville. Systematic generalization: what is required and can it be learned?\narXiv preprint\narXiv:1811.12889, 2018.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai\nfeedback. arXiv preprint arXiv:2212.08073, 2022.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\nShyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles.\nRevisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 2917\u20132927, 2022.\nEmanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks, and Aida Nematzadeh.\nMeasuring progress in fine-grained vision-and-language understanding. arXiv preprint arXiv:2305.07558,\n2023.\nAngelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R Bowman,\nKyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback.\narXiv preprint arXiv:2303.16749, 2023a.\n13\nJiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you need for\nin-context learning?, 2023b.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Dis-\nentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,\n2022a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\narXiv preprint arXiv:2209.06794, 2022b.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme\nRuiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and\nlanguage model. arXiv preprint arXiv:2305.18565, 2023c.\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa,\nSebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. Pali-3 vision language models:\nSmaller, faster, stronger. arXiv preprint arXiv:2310.09199, 2023d.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug.\narXiv preprint arXiv:2304.05128, 2023e.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-\ndreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision\ntransformers to 22 billion parameters. In International Conference on Machine Learning, pp. 7480\u20137512.\nPMLR, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt\u00e4schel. Prompt-\nbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.\nZhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training:\nBasics, recent advances, and future trends. Foundations and Trends\u00ae in Computer Graphics and Vision,\n14(3\u20134):163\u2013352, 2022.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Anna Chen, Anna Goldie,\nAzalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in\nlarge language models. arXiv preprint arXiv:2302.07459, 2023.\n14\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp.\n10764\u201310799. PMLR, 2023.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.\narXiv preprint arXiv:2012.15723, 2020.\nGoogle. Google cloud vertex ai api [code-bison], available at: https://cloud.google.com/vertex-ai/\ndocs/generative-ai/model-reference/code-generation. 2023.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without\ntraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n14953\u201314962, 2023.\nLisa Anne Hendricks and Aida Nematzadeh. Probing image-language transformers for verb understanding.\narXiv preprint arXiv:2106.09141, 2021.\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing\nhackable benchmarks for vision-language compositionality. arXiv preprint arXiv:2306.14610, 2023.\nRonghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:\nEnd-to-end module networks for visual question answering. In Proceedings of the IEEE international\nconference on computer vision, pp. 804\u2013813, 2017.\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny\nZhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 6700\u20136709, 2019.\nWoojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. A good prompt is worth millions of pa-\nrameters: Low-resource prompt-based learning for vision-language models. arXiv preprint arXiv:2110.08484,\n2021.\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick,\nand Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 2989\u20132998, 2017.\nDaniel Kahneman. Thinking, fast and slow. 2017.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil\nHoulsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pp. 491\u2013507. Springer,\n2020.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv preprint\narXiv:2107.07566, 2021.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang,\nLu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10965\u201310975, 2022.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\n15\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts\nand where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786,\n2021.\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to\ntango. arXiv preprint arXiv:2209.07686, 2022.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are\nfew-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv\npreprint arXiv:2303.17651, 2023.\nMatthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection, 2023.\nMilad Moradi and Matthias Samwald.\nEvaluating the robustness of neural language models to input\nperturbations. arXiv preprint arXiv:2108.12237, 2021.\nBinh X Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D Tran, and Anh Nguyen. Coarse-to-fine\nreasoning for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 4558\u20134566, 2022.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.\nDemystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.\nOpenAI.\nOpenai chatgpt api [gpt-3.5-turbo], available at:\nhttps://platform.openai.com/docs/\nmodel-index-for-researchers. 2023.\nRoni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip\nto count to ten. arXiv preprint arXiv:2302.12066, 2023.\nAaron Parisi, Yao Zhao, and Noah Fiedel.\nTalm: Tool augmented language models.\narXiv preprint\narXiv:2205.12255, 2022.\nViorica P\u0103tr\u0103ucean, Lucas Smaira, Ankush Gupta, Adri\u00e0 Recasens Continente, Larisa Markeeva, Dylan\nBanarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, et al. Perception test: A\ndiagnostic benchmark for multimodal video models. arXiv preprint arXiv:2305.13786, 2023.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction\nsearch for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv\npreprint arXiv:2307.16789, 2023.\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\nTowards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on\npattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot\nparadigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems,\npp. 1\u20137, 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761, 2023.\n16\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting\nknowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980,\n2020.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nKaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn\u2019t know it\u2019s wrong: An analysis\nof iterative prompting for reasoning problems, 2023.\nSanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, and\nMatt Gardner. Obtaining faithful interpretations from compositional neural networks. arXiv preprint\narXiv:2005.00724, 2020.\nSanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia\nSchmid, Andy Zeng, Trevor Darrell, and Dan Klein. Modular visual question answering via code generation.\narXiv preprint arXiv:2306.05392, 2023.\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.\narXiv preprint arXiv:2303.08128, 2023.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239, 2022.\nMichael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image\ncaptioners are scalable vision learners too. arXiv preprint arXiv:2306.07915, 2023.\nShimon Ullman. Visual routines. In Readings in computer vision, pp. 298\u2013328. Elsevier, 1987.\nKarthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really\nimprove by self-critiquing their own plans?, 2023.\nPablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will\nwe run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint\narXiv:2211.04325, 2022.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022a.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-\nsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint\narXiv:2305.04091, 2023a.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Conference on Machine Learning, pp. 23318\u201323340. PMLR,\n2022b.\nXingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot event structure prediction.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 3640\u20133663, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint\narXiv:2203.11171, 2022c.\n17\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint\narXiv:2303.03846, 2023.\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard\nprompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint\narXiv:2302.03668, 2023.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine learning, 8:229\u2013256, 1992.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to\nexplaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 9777\u20139786, 2021.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Gps: Genetic\nprompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041, 2022.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large\nlanguage models as optimizers. arXiv preprint arXiv:2309.03409, 2023a.\nLingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting. arXiv\npreprint arXiv:2306.04356, 2023b.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 36, pp. 3081\u20133089, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,\n2023.\nYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful\nprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021.\nQinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, and Fei Huang. Hitea: Hierarchical\ntemporal-aware video-language pre-training. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 15405\u201315416, 2023.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\nreferring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69\u201385. Springer, 2016.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why\nvision-language models behave like bags-of-words, and what to do about it? In The Eleventh International\nConference on Learning Representations, 2022.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico\nTombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot\nmultimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\nYan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with\nvisual concepts. arXiv preprint arXiv:2111.08276, 2021.\n18\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip\nDjolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation\nbenchmark. 2019.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image\npre-training. arXiv preprint arXiv:2303.15343, 2023.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations. arXiv\npreprint arXiv:2207.00221, 2022.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot\nperformance of language models. In International Conference on Machine Learning, pp. 12697\u201312706.\nPMLR, 2021.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language\nmodels. arXiv preprint arXiv:2205.10625, 2022a.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.\nMingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R\u00f3bert Csord\u00e1s, Anand Gopalakrishnan,\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in\nnatural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023.\n19\nA\nAppendix\nA.1\nAblations\nHere we present hyperparameter ablations, namely over the code-generating LLM (code-bison) temperature\nand the open-vocabulary object detector (OWLv2) threshold.\nTable 3: Code-generating LLM (code-bison) temperature hyperparameter ablations (with ACEs). The\nscores are reported as mean \u00b1 standard deviation across three random seeds.\nModel\nLLM temp.\nRefCOCO (IoU)\nRefCOCO+ (IoU)\nNExT-QA (acc.)\nViperGPT API\n0.0\n41.4 \u00b1 0.3\n39.8 \u00b1 0.0\n36.0 \u00b1 0.1\n0.4\n46.9 \u00b1 0.8\n41.7 \u00b1 0.4\n53.1 \u00b1 0.1\n0.8\n46.9 \u00b1 0.3\n42.0 \u00b1 0.7\n53.2 \u00b1 0.1\n1.0\n46.2 \u00b1 0.7\n40.9 \u00b1 0.6\n50.3 \u00b1 0.5\nAbstract API\n0.0\n47.0 \u00b1 0.1\n41.9 \u00b1 0.3\n59.1 \u00b1 0.0\n0.4\n48.2 \u00b1 0.1\n44.7 \u00b1 0.2\n61.0 \u00b1 0.4\n0.8\n48.2 \u00b1 0.0\n43.0 \u00b1 0.2\n60.6 \u00b1 0.6\n1.0\n48.8 \u00b1 0.1\n42.8 \u00b1 0.3\n59.9 \u00b1 0.7\nIn Table 3, we report the scores for different code-bison LLM temperatures: 0, 0.4, 0.8 and 1.0. We found\nthe deterministic case to underperform compared to the cases with a temperature higher than zero. This\nindicates that the solutions of which the models are most \u201cconfident\u201d of are not necessarily always correct.\nOn the other hand, when the temperature is too high, the model starts to hallucinate functions that do\nnot exist in the API and the performance degrades. Early in our work, we settled on the code-bison LLM\ntemperature of 0.4 and did not tune it further.\nTable 4: Open-vocabulary object detector (OWLv2) threshold hyperparameter ablations. The scores are\nreported as mean \u00b1 standard deviation across three random seeds.\nModel\nOWLv2 thrs.\nRefCOCO (IoU)\nRefCOCO+ (IoU)\nViperGPT API\n0.05\n42.6 \u00b1 0.3\n41.9 \u00b1 0.3\n0.10\n46.9 \u00b1 0.8\n41.7 \u00b1 0.4\n0.15\n45.0 \u00b1 0.5\n38.2 \u00b1 0.2\n0.20\n33.3 \u00b1 0.6\n29.8 \u00b1 0.2\nAbstract API\n0.05\n42.8 \u00b1 0.3\n42.7 \u00b1 0.7\n0.10\n48.2 \u00b1 0.1\n44.7 \u00b1 0.2\n0.15\n47.1 \u00b1 0.2\n38.6 \u00b1 0.1\n0.20\n33.7 \u00b1 0.4\n30.1 \u00b1 0.4\nTable 4 shows the effect of using different thresholds for OWLv2 open vocabulary detector. This threshold\ncontrols the level of \u2018sensitivity\u2019 of the open vocabulary detector. If the threshold value is set too high,\nwe will have fewer false positives, but also more false negatives. We perform this study on RefCOCO and\nRefCOCO+. On both datasets, the threshold of 0.1 achieves the best results, so by default we use this\nthreshold in all our experiments.\nIn Table 5 and Table 6 we show results for RefCOCO and RefCOCO+ when using randomly sampled samples\nfor the generation of ACEs. For comparison purposes, in the tables we also provide scores when not using\nany ACEs and with the default setting when generating ACEs with 16 manually selected few-shot examples.\nFrom the tables, we can see that randomly selected 100 or 50 samples perform similarly as when using 16\nmanual samples. With 16 random samples we observe a small drop in performance, though we still observe\n20\nTable 5: Results on RefCOCO with randomly sampled few-shot samples for generating ACEs. Shown are\nIoU scores for zero-shot (w/o ACE) setting, with the default setting that we used throughout the paper (16\nmanually sampled few-shot samples from the dataset (16 manual)) and with 100, 50 and 16 randomly-sampled\nsamples (100, 50 and 16 random) from the dataset. The IoU scores are reported as mean \u00b1 standard deviation\nacross three random seeds.\nAPI\nw/o ACE\n16 manual\n100 random\n50 random\n16 random\nViperGPT\n41.7 \u00b1 0.6\n46.9 \u00b1 0.8\n47.9 \u00b1 0.2\n45.8 \u00b1 0.4\n41.5 \u00b1 0.2\nAbstract\n44.4 \u00b1 0.9\n48.2 \u00b1 0.1\n49.1 \u00b1 0.2\n49.0 \u00b1 0.2\n46.9 \u00b1 0.2\nTable 6: Results on RefCOCO+ with randomly sampled few-shot samples for generating ACEs. Shown are\nIoU scores for zero-shot (w/o ACE) setting, with the default setting that we used throughout the paper (16\nmanually sampled few-shot samples from the dataset (16 manual)) and with 100, 50 and 16 randomly-sampled\nsamples (100, 50 and 16 random) from the dataset. The IoU scores are reported as mean \u00b1 standard deviation\nacross three random seeds.\nAPI\nw/o ACE\n16 manual\n100 random\n50 random\n16 random\nViperGPT\n36.7 \u00b1 0.4\n41.7 \u00b1 0.4\n41.3 \u00b1 0.1\n40.3 \u00b1 0.3\n36.3 \u00b1 0.1\nAbstract\n38.2 \u00b1 0.0\n42.9 \u00b1 0.2\n42.8 \u00b1 0.6\n41.8 \u00b1 0.0\n40.2 \u00b1 0.4\nan improvement compared to the setting without any ACEs. In summary, this shows that the manual labor\nfor generating ACEs can be removed altogether if we already have some labeled examples.\n21\nA.2\nPretrained models\nHere we specify the pretrained models we used, and compare them with the ones used in ViperGPT:\n\u2022 Open-vocabulary object detector:\n\u2013 Ours: OWLv2 (Minderer et al., 2023).\n\u2013 ViperGPT: GLIP Li et al. (2022) from the official GitHub repository2.\n\u2022 Depth estimation model:\n\u2013 Ours: MiDaS (Ranftl et al., 2020) v2 \u201cDPT_Small\u201d from PyTorch hub3.\n\u2013 ViperGPT: MiDaS (Ranftl et al., 2020) v2 \u201cDPT_Large\u201d version from the PyTorch hub4.\n\u2022 Vision-language captioning model:\n\u2013 Ours: PaLI-3 (Chen et al., 2023d).\n\u2013 ViperGPT: BLIP-2 (Li et al., 2023) from the official repository5.\n\u2022 CLIP-style image-text embedding model:\n\u2013 Ours: SigLiT (Zhai et al., 2023).\n\u2013 ViperGPT: X-VLM (Zeng et al., 2021) version finetuned for retrieval on MSCOCO from the\nofficial repository6.\n\u2022 Code-generating LLM:\n\u2013 Ours: code-bison accessible via the Google Cloud Vertex AI API (Google, 2023).\n\u2013 ViperGPT: Codex (code-davinci-002) via the official OpenAI Python API7.\n\u2022 Answer selector (based on context information) LLM for multiple choice questions in NExT-QA:\n\u2013 Ours: code-bison accessible via the Google Cloud Vertex AI API (Google, 2023).\n\u2013 ViperGPT: GPT-3 via the official OpenAI Python API8.\nA.3\nSelf-debugging prompt\n1 prompt += f\"\"\"\n2\n3 Previously , for the\nquery:\n4 # {query}\n5 you \u2019ve\ngenerated\nthe\nfollowing\ncode:\n6 {code}\n7\n8 The\nexecution\nof the\nabove\ncode\nfailed\nand\nreturned\nthe\nfollowing\nerror\nmessage:\n9 {error }.\n10\n11 Given this , correct\nthe\nabove\nfunction , such\nthat it\nexecutes\ncorrectly\nand\nsolves\nthe\nsame\nquery.\n12\n13 \"\"\"\n2https://github.com/microsoft/GLIP\n3https://pytorch.org/hub/intelisl_midas_v2/\n4https://pytorch.org/hub/intelisl_midas_v2/\n5https://github.com/salesforce/LAVIS/tree/main/projects/blip2\n6https://github.com/zengyan-97/X-VLM\n7https://openai.com/blog/openai-api\n8https://openai.com/blog/openai-api\n22\nA.4\nPrompt listings\nA.4.1\nRefCOCO and GQA - ViperGPT API\n1 import\nmath\n2\n3 class\nImagePatch:\n4\n\"\"\"A Python\nclass\ncontaining a crop of an image\ncentered\naround a particular\nobject , as\nwell as\nrelevant\ninformation .\n5\nAttributes\n6\n----------\n7\ncropped_image : array_like\n8\nAn array -like of the\ncropped\nimage\ntaken\nfrom\nthe\noriginal\nimage.\n9\nleft , lower , right , upper : int\n10\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s\nbounding\nbox in the\noriginal\nimage.\n11\n12\nMethods\n13\n-------\n14\nfind( object_name: str)->List[ImagePatch ]\n15\nReturns a list of new\nImagePatch\nobjects\ncontaining\ncrops of the\nimage\ncentered\naround\nany\nobjects\nfound in the\n16\nimage\nmatching\nthe\nobject_name .\n17\nexists( object_name : str)->bool\n18\nReturns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n19\nverify_property (property: str)->bool\n20\nReturns\nTrue if the\nproperty\nis met , and\nFalse\notherwise.\n21\ncompute_depth () ->float\n22\nReturns\nthe\nmedian\ndepth of the\nimage\ncrop.\n23\ncrop(left: int , lower: int , right: int , upper: int)->ImagePatch\n24\nReturns a new\nImagePatch\nobject\ncontaining a crop of the\nimage at the\ngiven\ncoordinates .\n25\n\"\"\"\n26\n27\ndef\n__init__(self , image , left: int = None , lower: int = None , right: int = None , upper:\nint = None):\n28\n\"\"\" Initializes\nan\nImagePatch\nobject by\ncropping\nthe\nimage at the\ngiven\ncoordinates\nand\nstores\nthe\ncoordinates\nas\n29\nattributes. If no\ncoordinates\nare\nprovided , the\nimage is left\nunmodified , and the\ncoordinates\nare set to the\n30\ndimensions\nof the\nimage.\n31\nParameters\n32\n-------\n33\nimage : array_like\n34\nAn array -like of the\noriginal\nimage.\n35\nleft , lower , right , upper : int\n36\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s bounding\nbox in the\noriginal\nimage.\n37\n\"\"\"\n38\nif left is None\nand\nright is None\nand\nupper is None\nand\nlower is None:\n39\nself. cropped_image = image\n40\nself.left = 0\n41\nself.lower = 0\n42\nself.right = image.shape [2]\n# width\n43\nself.upper = image.shape [1]\n# height\n44\nelse:\n45\nself. cropped_image = image [:, lower:upper , left:right]\n46\nself.left = left\n47\nself.upper = upper\n48\nself.right = right\n49\nself.lower = lower\n50\n51\nself.width = self. cropped_image .shape [2]\n52\nself.height = self. cropped_image .shape [1]\n53\n54\nself. horizontal_center = (self.left + self.right) / 2\n55\nself. vertical_center = (self.lower + self.upper) / 2\n23\n56\n57\ndef\nfind(self , object_name: str) -> List[ ImagePatch ]:\n58\n\"\"\" Returns a list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\nif any are\nfound.\n59\nOtherwise , returns\nan empty\nlist.\n60\nParameters\n61\n----------\n62\nobject_name : str\n63\nthe\nname of the\nobject to be found\n64\n65\nReturns\n66\n-------\n67\nList[ImagePatch]\n68\na list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\n69\n70\nExamples\n71\n--------\n72\n>>> # return\nthe foo\n73\n>>> def\nexecute_command (image) -> List[ ImagePatch ]:\n74\n>>>\nimage_patch = ImagePatch (image)\n75\n>>>\nfoo_patches = image_patch .find (\" foo \")\n76\n>>>\nreturn\nfoo_patches\n77\n\"\"\"\n78\nreturn\nfind_in_image (self.cropped_image , object_name )\n79\n80\ndef\nexists(self , object_name : str) -> bool:\n81\n\"\"\" Returns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n82\nParameters\n83\n-------\n84\nobject_name : str\n85\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n86\n87\nExamples\n88\n-------\n89\n>>> # Are\nthere\nboth\nfoos\nand\ngarply\nbars in the\nphoto?\n90\n>>> def\nexecute_command (image)->str:\n91\n>>>\nimage_patch = ImagePatch (image)\n92\n>>>\nis_foo = image_patch .exists (\" foo \")\n93\n>>>\nis_garply_bar = image_patch .exists (\" garply\nbar \")\n94\n>>>\nreturn\nis_foo\nand\nis_garply_bar\n95\n\"\"\"\n96\nreturn\nlen(self.find( object_name )) > 0\n97\n98\ndef\nverify_property (self , object_name : str , visual_property : str) -> bool:\n99\n\"\"\" Returns\nTrue if the\nobject\npossesses\nthe\nvisual\nproperty , and\nFalse\notherwise.\n100\nDiffers\nfrom \u2019exists \u2019 in that it\npresupposes\nthe\nexistence\nof the\nobject\nspecified\nby object_name , instead\nchecking\nwhether\nthe\nobject\npossesses\nthe\nproperty.\n101\nParameters\n102\n-------\n103\nobject_name : str\n104\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n105\nvisual_property : str\n106\nA string\ndescribing\nthe\nsimple\nvisual\nproperty (e.g., color , shape , material) to\nbe\nchecked.\n107\n108\nExamples\n109\n-------\n110\n>>> # Do the\nletters\nhave\nblue\ncolor?\n111\n>>> def\nexecute_command (image) -> str:\n112\n>>>\nimage_patch = ImagePatch (image)\n113\n>>>\nletters_patches = image_patch .find (\" letters \")\n114\n>>>\n# Question\nassumes\nonly\none\nletter\npatch\n115\n>>>\nreturn\nletters_patches [0]. verify_property (\" letters\", \"blue \")\n116\n\"\"\"\n117\nreturn\nverify_property (self.cropped_image , object_name , property)\n118\n119\ndef\ncompute_depth (self):\n24\n120\n\"\"\" Returns\nthe\nmedian\ndepth of the\nimage\ncrop\n121\nParameters\n122\n----------\n123\nReturns\n124\n-------\n125\nfloat\n126\nthe\nmedian\ndepth of the\nimage\ncrop\n127\n128\nExamples\n129\n--------\n130\n>>> # the bar\nfurthest\naway\n131\n>>> def\nexecute_command (image)->ImagePatch:\n132\n>>>\nimage_patch = ImagePatch (image)\n133\n>>>\nbar_patches = image_patch .find (\" bar \")\n134\n>>>\nbar_patches.sort(key=lambda\nbar: bar. compute_depth ())\n135\n>>>\nreturn\nbar_patches [-1]\n136\n\"\"\"\n137\ndepth_map = compute_depth (self. cropped_image )\n138\nreturn\ndepth_map.median ()\n139\n140\ndef\ncrop(self , left: int , lower: int , right: int , upper: int) -> ImagePatch:\n141\n\"\"\" Returns a new\nImagePatch\ncropped\nfrom\nthe\ncurrent\nImagePatch .\n142\nParameters\n143\n-------\n144\nleft , lower , right , upper : int\n145\nThe (left/lower/right/upper)most\npixel of the\ncropped\nimage.\n146\n-------\n147\n\"\"\"\n148\nreturn\nImagePatch(self.cropped_image , left , lower , right , upper)\n149\n150\ndef\noverlaps_with (self , left , lower , right , upper):\n151\n\"\"\" Returns\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one ,\n152\nelse\nFalse.\n153\nParameters\n154\n----------\n155\nleft , lower , right , upper : int\n156\nthe (left/lower/right/upper) border of the\ncrop to be\nchecked\n157\n158\nReturns\n159\n-------\n160\nbool\n161\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one , else\nFalse\n162\n163\nExamples\n164\n--------\n165\n>>> # black\nfoo on top of the qux\n166\n>>> def\nexecute_command (image) -> ImagePatch:\n167\n>>>\nimage_patch = ImagePatch (image)\n168\n>>>\nqux_patches = image_patch .find (\" qux \")\n169\n>>>\nqux_patch = qux_patches [0]\n170\n>>>\nfoo_patches = image_patch .find (\" black\nfoo \")\n171\n>>>\nfor foo in\nfoo_patches :\n172\n>>>\nif foo. vertical_center\n> qux_patch. vertical_center\n173\n>>>\nreturn\nfoo\n174\n\"\"\"\n175\nreturn\nself.left\n<= right\nand\nself.right\n>= left\nand\nself.lower\n<= upper\nand\nself.\nupper\n>= lower\n176\n177\n178 def\nbest_image_match ( list_patches : List[ ImagePatch], content: List[str], return_index =False)\n-> Union[ImagePatch , int]:\n179\n\"\"\" Returns\nthe\npatch\nmost\nlikely to\ncontain\nthe\ncontent.\n180\nParameters\n181\n----------\n182\nlist_patches : List[ImagePatch]\n183\ncontent : List[str]\n184\nthe\nobject of\ninterest\n185\nreturn_index : bool\n25\n186\nif True , returns\nthe\nindex of the\npatch\nmost\nlikely to\ncontain\nthe\nobject\n187\n188\nReturns\n189\n-------\n190\nint\n191\nPatch\nmost\nlikely to\ncontain\nthe\nobject\n192\n\"\"\"\n193\nreturn\nbest_image_match (list_patches , content , return_index )\n194\n195\n196 def\ndistance(patch_a: ImagePatch , patch_b: ImagePatch ) -> float:\n197\n\"\"\"\n198\nReturns\nthe\ndistance\nbetween\nthe\nedges of two\nImagePatches . If the\npatches\noverlap , it\nreturns a negative\ndistance\n199\ncorresponding\nto the\nnegative\nintersection\nover\nunion.\n200\n201\nParameters\n202\n----------\n203\npatch_a : ImagePatch\n204\npatch_b : ImagePatch\n205\n206\nExamples\n207\n--------\n208\n# Return\nthe qux\nthat is\nclosest\nto the foo\n209\n>>> def\nexecute_command (image):\n210\n>>>\nimage_patch = ImagePatch(image)\n211\n>>>\nqux_patches = image_patch .find(\u2019qux \u2019)\n212\n>>>\nfoo_patches = image_patch .find(\u2019foo \u2019)\n213\n>>>\nfoo_patch = foo_patches [0]\n214\n>>>\nqux_patches.sort(key=lambda x: distance(x, foo_patch))\n215\n>>>\nreturn\nqux_patches [0]\n216\n\"\"\"\n217\nreturn\ndistance(patch_a , patch_b)\n218\n219 INSERT_IN_CONTEXT_EXAMPLES_HERE\n220\n221 Write a function\nusing\nPython\nand the\nImagePatch\nclass (above) that\ncould be\nexecuted\nto\nprovide\nan answer\nto the\nquery.\n222\n223 Consider\nthe\nfollowing\nguidelines:\n224 - Use\nbase\nPython (comparison , sorting) for\nbasic\nlogical\noperations , left/right/up/down ,\nmath , etc.\n225 - Make\nsure to always\nreturn\nan\nImagePatch\nobject.\n226 - Make\nsure\nthat\nfor all\npossible\ncontrol\nflows , the\nprogram\nalways\nreturns\nan\nImagePatch\nobject.\n227\n228 INSERT_PREVIOUS_CODE_AND_ERROR_HERE\n229\n230 # INSERT_QUERY_HERE\n26\nA.4.2\nRefCOCO and GQA - Abstract API\n1 import\nmath\n2\n3 class\nImagePatch:\n4\n\"\"\"A Python\nclass\ncontaining a crop of an image\ncentered\naround a particular\nobject , as\nwell as\nrelevant\ninformation .\n5\nAttributes\n6\n----------\n7\ncropped_image : array_like\n8\nAn array -like of the\ncropped\nimage\ntaken\nfrom\nthe\noriginal\nimage.\n9\nleft , lower , right , upper : int\n10\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s\nbounding\nbox in the\noriginal\nimage.\n11\n12\nMethods\n13\n-------\n14\nfind( object_name: str)->List[ImagePatch ]\n15\nReturns a list of new\nImagePatch\nobjects\ncontaining\ncrops of the\nimage\ncentered\naround\nany\nobjects\nfound in the\n16\nimage\nmatching\nthe\nobject_name .\n17\nexists( object_name : str)->bool\n18\nReturns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n19\nverify_property (property: str)->bool\n20\nReturns\nTrue if the\nproperty\nis met , and\nFalse\notherwise.\n21\ncompute_depth () ->float\n22\nReturns\nthe\nmedian\ndepth of the\nimage\ncrop.\n23\ncrop(left: int , lower: int , right: int , upper: int)->ImagePatch\n24\nReturns a new\nImagePatch\nobject\ncontaining a crop of the\nimage at the\ngiven\ncoordinates .\n25\n\"\"\"\n26\n27\ndef\n__init__(self , image , left: int = None , lower: int = None , right: int = None , upper:\nint = None):\n28\n\"\"\" Initializes\nan\nImagePatch\nobject by\ncropping\nthe\nimage at the\ngiven\ncoordinates\nand\nstores\nthe\ncoordinates\nas\n29\nattributes. If no\ncoordinates\nare\nprovided , the\nimage is left\nunmodified , and the\ncoordinates\nare set to the\n30\ndimensions\nof the\nimage.\n31\nParameters\n32\n-------\n33\nimage : array_like\n34\nAn array -like of the\noriginal\nimage.\n35\nleft , lower , right , upper : int\n36\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s bounding\nbox in the\noriginal\nimage.\n37\n\"\"\"\n38\nif left is None\nand\nright is None\nand\nupper is None\nand\nlower is None:\n39\nself. cropped_image = image\n40\nself.left = 0\n41\nself.lower = 0\n42\nself.right = image.shape [2]\n# width\n43\nself.upper = image.shape [1]\n# height\n44\nelse:\n45\nself. cropped_image = image [:, lower:upper , left:right]\n46\nself.left = left\n47\nself.upper = upper\n48\nself.right = right\n49\nself.lower = lower\n50\n51\nself.width = self. cropped_image .shape [2]\n52\nself.height = self. cropped_image .shape [1]\n53\n54\nself. horizontal_center = (self.left + self.right) / 2\n55\nself. vertical_center = (self.lower + self.upper) / 2\n56\n57\ndef\nfind(self , object_name: str) -> List[ ImagePatch ]:\n27\n58\n\"\"\" Returns a list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\nif any are\nfound.\n59\nOtherwise , returns\nan empty\nlist.\n60\nParameters\n61\n----------\n62\nobject_name : str\n63\nthe\nname of the\nobject to be found\n64\n65\nReturns\n66\n-------\n67\nList[ImagePatch]\n68\na list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\n69\n70\nExamples\n71\n--------\n72\n>>> # return\nthe foo\n73\n>>> def\nexecute_command (image) -> List[ ImagePatch ]:\n74\n>>>\nimage_patch = ImagePatch (image)\n75\n>>>\nfoo_patches = image_patch .find (\" foo \")\n76\n>>>\nreturn\nfoo_patches\n77\n\"\"\"\n78\nreturn\nfind_in_image (self.cropped_image , object_name )\n79\n80\ndef\nexists(self , object_name : str) -> bool:\n81\n\"\"\" Returns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n82\nParameters\n83\n-------\n84\nobject_name : str\n85\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n86\n87\nExamples\n88\n-------\n89\n>>> # Are\nthere\nboth\nfoos\nand\ngarply\nbars in the\nphoto?\n90\n>>> def\nexecute_command (image)->str:\n91\n>>>\nimage_patch = ImagePatch (image)\n92\n>>>\nis_foo = image_patch .exists (\" foo \")\n93\n>>>\nis_garply_bar = image_patch .exists (\" garply\nbar \")\n94\n>>>\nreturn\nis_foo\nand\nis_garply_bar\n95\n\"\"\"\n96\nreturn\nlen(self.find( object_name )) > 0\n97\n98\ndef\nverify_property (self , object_name : str , visual_property : str) -> bool:\n99\n\"\"\" Returns\nTrue if the\nobject\npossesses\nthe\nvisual\nproperty , and\nFalse\notherwise.\n100\nDiffers\nfrom \u2019exists \u2019 in that it\npresupposes\nthe\nexistence\nof the\nobject\nspecified\nby object_name , instead\nchecking\nwhether\nthe\nobject\npossesses\nthe\nproperty.\n101\nParameters\n102\n-------\n103\nobject_name : str\n104\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n105\nvisual_property : str\n106\nA string\ndescribing\nthe\nsimple\nvisual\nproperty (e.g., color , shape , material) to\nbe\nchecked.\n107\n108\nExamples\n109\n-------\n110\n>>> # Do the\nletters\nhave\nblue\ncolor?\n111\n>>> def\nexecute_command (image) -> str:\n112\n>>>\nimage_patch = ImagePatch (image)\n113\n>>>\nletters_patches = image_patch .find (\" letters \")\n114\n>>>\n# Question\nassumes\nonly\none\nletter\npatch\n115\n>>>\nreturn\nletters_patches [0]. verify_property (\" letters\", \"blue \")\n116\n\"\"\"\n117\nreturn\nverify_property (self.cropped_image , object_name , property)\n118\n119\ndef\ncompute_depth (self):\n120\n\"\"\" Returns\nthe\nmedian\ndepth of the\nimage\ncrop\n121\nParameters\n28\n122\n----------\n123\nReturns\n124\n-------\n125\nfloat\n126\nthe\nmedian\ndepth of the\nimage\ncrop\n127\n128\nExamples\n129\n--------\n130\n>>> # the bar\nfurthest\naway\n131\n>>> def\nexecute_command (image)->ImagePatch:\n132\n>>>\nimage_patch = ImagePatch (image)\n133\n>>>\nbar_patches = image_patch .find (\" bar \")\n134\n>>>\nbar_patches.sort(key=lambda\nbar: bar. compute_depth ())\n135\n>>>\nreturn\nbar_patches [-1]\n136\n\"\"\"\n137\ndepth_map = compute_depth (self. cropped_image )\n138\nreturn\ndepth_map.median ()\n139\n140\ndef\ncrop(self , left: int , lower: int , right: int , upper: int) -> ImagePatch:\n141\n\"\"\" Returns a new\nImagePatch\ncropped\nfrom\nthe\ncurrent\nImagePatch .\n142\nParameters\n143\n-------\n144\nleft , lower , right , upper : int\n145\nThe (left/lower/right/upper)most\npixel of the\ncropped\nimage.\n146\n-------\n147\n\"\"\"\n148\nreturn\nImagePatch(self.cropped_image , left , lower , right , upper)\n149\n150\ndef\noverlaps_with (self , left , lower , right , upper):\n151\n\"\"\" Returns\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one ,\n152\nelse\nFalse.\n153\nParameters\n154\n----------\n155\nleft , lower , right , upper : int\n156\nthe (left/lower/right/upper) border of the\ncrop to be\nchecked\n157\n158\nReturns\n159\n-------\n160\nbool\n161\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one , else\nFalse\n162\n163\nExamples\n164\n--------\n165\n>>> # black\nfoo on top of the qux\n166\n>>> def\nexecute_command (image) -> ImagePatch:\n167\n>>>\nimage_patch = ImagePatch (image)\n168\n>>>\nqux_patches = image_patch .find (\" qux \")\n169\n>>>\nqux_patch = qux_patches [0]\n170\n>>>\nfoo_patches = image_patch .find (\" black\nfoo \")\n171\n>>>\nfor foo in\nfoo_patches :\n172\n>>>\nif foo. vertical_center\n> qux_patch. vertical_center\n173\n>>>\nreturn\nfoo\n174\n\"\"\"\n175\nreturn\nself.left\n<= right\nand\nself.right\n>= left\nand\nself.lower\n<= upper\nand\nself.\nupper\n>= lower\n176\n177\n178 def\nbest_image_match ( list_patches : List[ ImagePatch], content: List[str], return_index =False)\n-> Union[ImagePatch , int]:\n179\n\"\"\" Returns\nthe\npatch\nmost\nlikely to\ncontain\nthe\ncontent.\n180\nParameters\n181\n----------\n182\nlist_patches : List[ImagePatch]\n183\ncontent : List[str]\n184\nthe\nobject of\ninterest\n185\nreturn_index : bool\n186\nif True , returns\nthe\nindex of the\npatch\nmost\nlikely to\ncontain\nthe\nobject\n187\n29\n188\nReturns\n189\n-------\n190\nint\n191\nPatch\nmost\nlikely to\ncontain\nthe\nobject\n192\n\"\"\"\n193\nreturn\nbest_image_match (list_patches , content , return_index )\n194\n195\n196 def\ndistance(patch_a: ImagePatch , patch_b: ImagePatch ) -> float:\n197\n\"\"\"\n198\nReturns\nthe\ndistance\nbetween\nthe\nedges of two\nImagePatches . If the\npatches\noverlap , it\nreturns a negative\ndistance\n199\ncorresponding\nto the\nnegative\nintersection\nover\nunion.\n200\n201\nParameters\n202\n----------\n203\npatch_a : ImagePatch\n204\npatch_b : ImagePatch\n205\n206\nExamples\n207\n--------\n208\n# Return\nthe qux\nthat is\nclosest\nto the foo\n209\n>>> def\nexecute_command (image):\n210\n>>>\nimage_patch = ImagePatch(image)\n211\n>>>\nqux_patches = image_patch .find(\u2019qux \u2019)\n212\n>>>\nfoo_patches = image_patch .find(\u2019foo \u2019)\n213\n>>>\nfoo_patch = foo_patches [0]\n214\n>>>\nqux_patches.sort(key=lambda x: distance(x, foo_patch))\n215\n>>>\nreturn\nqux_patches [0]\n216\n\"\"\"\n217\nreturn\ndistance(patch_a , patch_b)\n218\n219 def\nget_patch_left_of (patch: ImagePatch ) -> ImagePatch :\n220\nleft_patch = get_patch_left_of (patch)\n221\nreturn\nleft_patch\n222\n223 def\nget_patch_right_of (patch: ImagePatch) -> ImagePatch :\n224\nright_patch = get_patch_right_of (patch)\n225\nreturn\nright_patch\n226\n227 def\nget_patch_above_of (patch: ImagePatch) -> ImagePatch :\n228\nabove_patch = get_patch_above_of (patch)\n229\nreturn\nabove_patch\n230\n231 def\nget_patch_below_of (patch: ImagePatch) -> ImagePatch :\n232\nbelow_patch = get_patch_below_of (patch)\n233\nreturn\nbelow_patch\n234\n235 def\nget_patch_around_of (patch: ImagePatch ) -> ImagePatch:\n236\naround_patch = get_patch_around_of (patch)\n237\nreturn\naround_patch\n238\n239 def\nsort_patches_left_to_right ( list_patches : List[ ImagePatch ]) -> List[ ImagePatch ]:\n240\n\"\"\"\n241\nSorts\npatches\naccording\nto their\nhorizontal\ncenters.\n242\n243\nParameters\n244\n----------\n245\nlist_patches : List[ImagePatch]\n246\n247\nExamples\n248\n--------\n249\n# Right\nfoo\n250\n>>> def\nexecute_command (image):\n251\n>>>\nimage_patch = ImagePatch(image)\n252\n>>>\nfoo_patches = image_patch .find(\u2019foo \u2019)\n253\n>>>\nfoo_patches = sort_patches_left_to_right ( foo_patches )\n254\n>>>\nright_foo_patch = foo_patches [-1]\n30\n255\n>>>\nreturn\nright_foo_patch\n256\n\"\"\"\n257\nreturn\nsort_patches_left_to_right ( list_patches )\n258\n259\n260 def\nsort_patches_bottom_to_top ( list_patches : List[ ImagePatch ]) -> List[ ImagePatch ]:\n261\n\"\"\"\n262\nSorts\npatches\naccording\nto their\nvertical\ncenters.\n263\n264\nParameters\n265\n----------\n266\nlist_patches : List[ImagePatch]\n267\n268\nExamples\n269\n--------\n270\n# Second\nbar\nfrom\nthe top\n271\n>>> def\nexecute_command (image):\n272\n>>>\nimage_patch = ImagePatch(image)\n273\n>>>\nbar_patches = image_patch .find(\u2019bar \u2019)\n274\n>>>\nbar_patches = sort_patches_bottom_to_top ( bar_patches )\n275\n>>>\nsecond_topmost_bar_patch = bar_patches [-2]\n276\n>>>\nreturn\nsecond_topmost_bar_patch\n277\n\"\"\"\n278\nreturn\nsort_patches_bottom_to_top ( list_patches )\n279\n280\n281 def\nsort_patches_front_to_back ( list_patches : List[ ImagePatch ]) -> List[ ImagePatch ]:\n282\n\"\"\"\n283\nSorts\npatches\naccording\nto how far\nfrom\ncamera\nthey\nare.\n284\n285\nParameters\n286\n----------\n287\nlist_patches : List[ImagePatch]\n288\n289\nExamples\n290\n--------\n291\n# Person in the\nback\n292\n>>> def\nexecute_command (image):\n293\n>>>\nimage_patch = ImagePatch(image)\n294\n>>>\nperson_patches = image_patch .find(\u2019person \u2019)\n295\n>>>\nperson_patches = sort_patches_front_to_back ( person_patches )\n296\n>>>\nperson_in_the_back = person_patches [-1]\n297\n>>>\nreturn\nperson_in_the_back\n298\n\"\"\"\n299\nreturn\nsort_patches_front_to_back ( list_patches )\n300\n301\n302 def\nget_middle_patch ( list_patches : List[ ImagePatch ]) -> ImagePatch:\n303\n\"\"\"\n304\nReturns\nthe\nmiddle\npatch.\n305\n306\nParameters\n307\n----------\n308\nlist_patches : List[ImagePatch]\n309\n310\nExamples\n311\n--------\n312\n# Middle\nham\n313\n>>> def\nexecute_command (image):\n314\n>>>\nimage_patch = ImagePatch(image)\n315\n>>>\nham_patches = image_patch .find(\u2019ham \u2019)\n316\n>>>\nmiddle_ham_patch = get_middle_patch ( ham_patches )\n317\n>>>\nreturn\nmiddle_ham_patch\n318\n\"\"\"\n319\nreturn\nget_middle_patch ( list_patches )\n320\n321\n31\n322 def\nget_patch_closest_to_anchor_object ( list_patches : List[ ImagePatch ], anchor_object :\nImagePatch) -> ImagePatch:\n323\n\"\"\"\n324\nReturns\nthe\nobject\nfrom\nlist_patches\nthat is the\nclosest\nto the\nanchor_object .\n325\n326\nParameters\n327\n----------\n328\nlist_patches : List[ImagePatch]\n329\nanchor_object : ImagePatch\n330\n331\nExamples\n332\n--------\n333\n# Foo\nnext to bar\n334\n>>> def\nexecute_command (image):\n335\n>>>\nimage_patch = ImagePatch(image)\n336\n>>>\nfoo_patches = image_patch .find(\u2019foo \u2019)\n337\n>>>\nbar_patches = image_patch .find(\u2019bar \u2019)\n338\n>>>\nbar_patch = bar_patches [0]\n339\n>>>\nfoo_next_to_bar_patch = get_patch_closest_to_anchor_object (foo_patches ,\nbar_patch)\n340\n>>>\nreturn\nfoo_next_to_bar_patch\n341\n\"\"\"\n342\nreturn\nget_patch_closest_to_anchor_object (list_patches , anchor_object )\n343\n344\n345 INSERT_IN_CONTEXT_EXAMPLES_HERE\n346\n347 Write a function\nusing\nPython\nand the\nImagePatch\nclass (above) that\ncould be\nexecuted\nto\nprovide\nan answer\nto the\nquery.\n348\n349 Consider\nthe\nfollowing\nguidelines:\n350 - Use\nbase\nPython (comparison , sorting) for\nbasic\nlogical\noperations , left/right/up/down ,\nmath , etc.\n351 - Make\nsure to always\nreturn\nan\nImagePatch\nobject.\n352 - Make\nsure\nthat\nfor all\npossible\ncontrol\nflows , the\nprogram\nalways\nreturns\nan\nImagePatch\nobject.\n353 - ImagePatch\nclass\nuses\nleft\nand\nright to denote\nhorizontal\nedges.\n354 - ImagePatch\nclass\nuses\nbottom\nand top to denote\nvertical\nedges.\n355\n356 INSERT_PREVIOUS_CODE_AND_ERROR_HERE\n357\n358 # INSERT_QUERY_HERE\n32\nA.4.3\nNExT-QA - ViperGPT API\n1 import\nmath\n2\n3 class\nImagePatch:\n4\n\"\"\"A Python\nclass\ncontaining a crop of an image\ncentered\naround a particular\nobject , as\nwell as\nrelevant\ninformation .\n5\nAttributes\n6\n----------\n7\ncropped_image : array_like\n8\nAn array -like of the\ncropped\nimage\ntaken\nfrom\nthe\noriginal\nimage.\n9\nleft , lower , right , upper : int\n10\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s\nbounding\nbox in the\noriginal\nimage.\n11\n12\nMethods\n13\n-------\n14\nfind( object_name: str)->List[ImagePatch ]\n15\nReturns a list of new\nImagePatch\nobjects\ncontaining\ncrops of the\nimage\ncentered\naround\nany\nobjects\nfound in the\n16\nimage\nmatching\nthe\nobject_name .\n17\nexists( object_name : str)->bool\n18\nReturns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n19\nverify_property (property: str)->bool\n20\nReturns\nTrue if the\nproperty\nis met , and\nFalse\notherwise.\n21\nbest_text_match (option_list: List[str], prefix: str)->str\n22\nReturns\nthe\nstring\nthat\nbest\nmatches\nthe\nimage.\n23\nsimple_query (question: str=None)->str\n24\nReturns\nthe\nanswer\nto a basic\nquestion\nasked\nabout\nthe\nimage. If no\nquestion\nis\nprovided , returns\nthe\nanswer to \"What is this ?\".\n25\ncompute_depth () ->float\n26\nReturns\nthe\nmedian\ndepth of the\nimage\ncrop.\n27\ncrop(left: int , lower: int , right: int , upper: int)->ImagePatch\n28\nReturns a new\nImagePatch\nobject\ncontaining a crop of the\nimage at the\ngiven\ncoordinates .\n29\n\"\"\"\n30\n31\ndef\n__init__(self , image , left: int = None , lower: int = None , right: int = None , upper:\nint = None):\n32\n\"\"\" Initializes\nan\nImagePatch\nobject by\ncropping\nthe\nimage at the\ngiven\ncoordinates\nand\nstores\nthe\ncoordinates\nas\n33\nattributes. If no\ncoordinates\nare\nprovided , the\nimage is left\nunmodified , and the\ncoordinates\nare set to the\n34\ndimensions\nof the\nimage.\n35\nParameters\n36\n-------\n37\nimage : array_like\n38\nAn array -like of the\noriginal\nimage.\n39\nleft , lower , right , upper : int\n40\nAn int\ndescribing\nthe\nposition\nof the (left/lower/right/upper) border of the\ncrop \u2019s bounding\nbox in the\noriginal\nimage.\n41\n\"\"\"\n42\nif left is None\nand\nright is None\nand\nupper is None\nand\nlower is None:\n43\nself. cropped_image = image\n44\nself.left = 0\n45\nself.lower = 0\n46\nself.right = image.shape [2]\n# width\n47\nself.upper = image.shape [1]\n# height\n48\nelse:\n49\nself. cropped_image = image [:, lower:upper , left:right]\n50\nself.left = left\n51\nself.upper = upper\n52\nself.right = right\n53\nself.lower = lower\n54\n55\nself.width = self. cropped_image .shape [2]\n56\nself.height = self. cropped_image .shape [1]\n33\n57\n58\nself. horizontal_center = (self.left + self.right) / 2\n59\nself. vertical_center = (self.lower + self.upper) / 2\n60\n61\ndef\nfind(self , object_name: str) -> List[ ImagePatch ]:\n62\n\"\"\" Returns a list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\nif any are\nfound.\n63\nOtherwise , returns\nan empty\nlist.\n64\nParameters\n65\n----------\n66\nobject_name : str\n67\nthe\nname of the\nobject to be found\n68\n69\nReturns\n70\n-------\n71\nList[ImagePatch]\n72\na list of\nImagePatch\nobjects\nmatching\nobject_name\ncontained\nin the\ncrop\n73\n74\nExamples\n75\n--------\n76\n>>> # return\nthe foo\n77\n>>> def\nexecute_command (image) -> List[ ImagePatch ]:\n78\n>>>\nimage_patch = ImagePatch (image)\n79\n>>>\nfoo_patches = image_patch .find (\" foo \")\n80\n>>>\nreturn\nfoo_patches\n81\n\"\"\"\n82\nreturn\nfind_in_image (self.cropped_image , object_name )\n83\n84\ndef\nexists(self , object_name : str) -> bool:\n85\n\"\"\" Returns\nTrue if the\nobject\nspecified\nby\nobject_name\nis found in the image , and\nFalse\notherwise.\n86\nParameters\n87\n-------\n88\nobject_name : str\n89\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n90\n91\nExamples\n92\n-------\n93\n>>> # Are\nthere\nboth\nfoos\nand\ngarply\nbars in the\nphoto?\n94\n>>> def\nexecute_command (image)->str:\n95\n>>>\nimage_patch = ImagePatch (image)\n96\n>>>\nis_foo = image_patch .exists (\" foo \")\n97\n>>>\nis_garply_bar = image_patch .exists (\" garply\nbar \")\n98\n>>>\nreturn\nbool_to_yesno (is_foo\nand\nis_garply_bar )\n99\n\"\"\"\n100\nreturn\nlen(self.find( object_name )) > 0\n101\n102\ndef\nverify_property (self , object_name : str , visual_property : str) -> bool:\n103\n\"\"\" Returns\nTrue if the\nobject\npossesses\nthe\nvisual\nproperty , and\nFalse\notherwise.\n104\nDiffers\nfrom \u2019exists \u2019 in that it\npresupposes\nthe\nexistence\nof the\nobject\nspecified\nby object_name , instead\nchecking\nwhether\nthe\nobject\npossesses\nthe\nproperty.\n105\nParameters\n106\n-------\n107\nobject_name : str\n108\nA string\ndescribing\nthe\nname of the\nobject to be found in the\nimage.\n109\nvisual_property : str\n110\nA string\ndescribing\nthe\nsimple\nvisual\nproperty (e.g., color , shape , material) to\nbe\nchecked.\n111\n112\nExamples\n113\n-------\n114\n>>> # Do the\nletters\nhave\nblue\ncolor?\n115\n>>> def\nexecute_command (image) -> str:\n116\n>>>\nimage_patch = ImagePatch (image)\n117\n>>>\nletters_patches = image_patch .find (\" letters \")\n118\n>>>\n# Question\nassumes\nonly\none\nletter\npatch\n119\n>>>\nreturn\nbool_to_yesno ( letters_patches [0]. verify_property (\" letters\", \"blue \"))\n120\n\"\"\"\n34\n121\nreturn\nverify_property (self.cropped_image , object_name , property)\n122\n123\ndef\nbest_text_match (self , option_list : List[str ]) -> str:\n124\n\"\"\" Returns\nthe\nstring\nthat\nbest\nmatches\nthe\nimage.\n125\nParameters\n126\n-------\n127\noption_list : str\n128\nA list\nwith\nthe\nnames of the\ndifferent\noptions\n129\nprefix : str\n130\nA string\nwith\nthe\nprefixes\nto append to the\noptions\n131\n132\nExamples\n133\n-------\n134\n>>> # Is the foo\ngold or white?\n135\n>>> def\nexecute_command (image)->str:\n136\n>>>\nimage_patch = ImagePatch (image)\n137\n>>>\nfoo_patches = image_patch .find (\" foo \")\n138\n>>>\n# Question\nassumes\none foo\npatch\n139\n>>>\nreturn\nfoo_patches [0]. best_text_match ([\" gold\", \"white \"])\n140\n\"\"\"\n141\nreturn\nbest_text_match (self.cropped_image , option_list )\n142\n143\ndef\nsimple_query (self , question: str = None) -> str:\n144\n\"\"\" Returns\nthe\nanswer\nto a basic\nquestion\nasked\nabout\nthe\nimage. If no\nquestion\nis\nprovided , returns\nthe\nanswer\n145\nto \"What is this ?\". The\nquestions\nare\nabout\nbasic\nperception , and are not\nmeant to\nbe used\nfor\ncomplex\nreasoning\n146\nor\nexternal\nknowledge.\n147\nParameters\n148\n-------\n149\nquestion : str\n150\nA string\ndescribing\nthe\nquestion\nto be asked.\n151\n152\nExamples\n153\n-------\n154\n155\n>>> # Which\nkind of baz is not\nfredding?\n156\n>>> def\nexecute_command (image) -> str:\n157\n>>>\nimage_patch = ImagePatch (image)\n158\n>>>\nbaz_patches = image_patch .find (\" baz \")\n159\n>>>\nfor\nbaz_patch\nin\nbaz_patches :\n160\n>>>\nif not\nbaz_patch. verify_property (\" baz\", \"fredding \"):\n161\n>>>\nreturn\nbaz_patch. simple_query (\" What is this\nbaz ?\")\n162\n163\n>>> # What\ncolor is the foo?\n164\n>>> def\nexecute_command (image) -> str:\n165\n>>>\nimage_patch = ImagePatch (image)\n166\n>>>\nfoo_patches = image_patch .find (\" foo \")\n167\n>>>\nfoo_patch = foo_patches [0]\n168\n>>>\nreturn\nfoo_patch. simple_query (\" What is the\ncolor ?\")\n169\n170\n>>> # Is the\nsecond\nbar\nfrom\nthe\nleft\nquuxy?\n171\n>>> def\nexecute_command (image) -> str:\n172\n>>>\nimage_patch = ImagePatch (image)\n173\n>>>\nbar_patches = image_patch .find (\" bar \")\n174\n>>>\nbar_patches.sort(key=lambda x: x. horizontal_center )\n175\n>>>\nbar_patch = bar_patches [1]\n176\n>>>\nreturn\nbar_patch. simple_query (\"Is the bar\nquuxy ?\")\n177\n\"\"\"\n178\nreturn\nsimple_query (self.cropped_image , question)\n179\n180\ndef\ncompute_depth (self):\n181\n\"\"\" Returns\nthe\nmedian\ndepth of the\nimage\ncrop\n182\nParameters\n183\n----------\n184\nReturns\n185\n-------\n186\nfloat\n35\n187\nthe\nmedian\ndepth of the\nimage\ncrop\n188\n189\nExamples\n190\n--------\n191\n>>> # the bar\nfurthest\naway\n192\n>>> def\nexecute_command (image)->ImagePatch:\n193\n>>>\nimage_patch = ImagePatch (image)\n194\n>>>\nbar_patches = image_patch .find (\" bar \")\n195\n>>>\nbar_patches.sort(key=lambda\nbar: bar. compute_depth ())\n196\n>>>\nreturn\nbar_patches [-1]\n197\n\"\"\"\n198\ndepth_map = compute_depth (self. cropped_image )\n199\nreturn\ndepth_map.median ()\n200\n201\ndef\ncrop(self , left: int , lower: int , right: int , upper: int) -> ImagePatch:\n202\n\"\"\" Returns a new\nImagePatch\ncropped\nfrom\nthe\ncurrent\nImagePatch .\n203\nParameters\n204\n-------\n205\nleft , lower , right , upper : int\n206\nThe (left/lower/right/upper)most\npixel of the\ncropped\nimage.\n207\n-------\n208\n\"\"\"\n209\nreturn\nImagePatch(self.cropped_image , left , lower , right , upper)\n210\n211\ndef\noverlaps_with (self , left , lower , right , upper):\n212\n\"\"\" Returns\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one ,\n213\nelse\nFalse.\n214\nParameters\n215\n----------\n216\nleft , lower , right , upper : int\n217\nthe (left/lower/right/upper) border of the\ncrop to be\nchecked\n218\n219\nReturns\n220\n-------\n221\nbool\n222\nTrue if a crop\nwith\nthe\ngiven\ncoordinates\noverlaps\nwith\nthis one , else\nFalse\n223\n224\nExamples\n225\n--------\n226\n>>> # black\nfoo on top of the qux\n227\n>>> def\nexecute_command (image) -> ImagePatch:\n228\n>>>\nimage_patch = ImagePatch (image)\n229\n>>>\nqux_patches = image_patch .find (\" qux \")\n230\n>>>\nqux_patch = qux_patches [0]\n231\n>>>\nfoo_patches = image_patch .find (\" black\nfoo \")\n232\n>>>\nfor foo in\nfoo_patches :\n233\n>>>\nif foo. vertical_center\n> qux_patch. vertical_center\n234\n>>>\nreturn\nfoo\n235\n\"\"\"\n236\nreturn\nself.left\n<= right\nand\nself.right\n>= left\nand\nself.lower\n<= upper\nand\nself.\nupper\n>= lower\n237\n238\n239 class\nVideoSegment :\n240\n\"\"\"A Python\nclass\ncontaining a set of frames\nrepresented\nas\nImagePatch\nobjects , as well\nas\nrelevant\ninformation .\n241\nAttributes\n242\n----------\n243\nvideo : torch.Tensor\n244\nA tensor of the\noriginal\nvideo.\n245\nstart : int\n246\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n247\nend : int\n248\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n249\nnum_frames ->int\n250\nAn int\ncontaining\nthe\nnumber of\nframes in the\nvideo\nsegment.\n36\n251\n252\nMethods\n253\n-------\n254\nframe_iterator ->Iterator[ImagePatch]\n255\ntrim(start , end)->VideoSegment\n256\nReturns a new\nVideoSegment\ncontaining a trimmed\nversion\nof the\noriginal\nvideo at the [\nstart , end] segment.\n257\nselect_answer (info , question , options)->str\n258\nReturns\nthe\nanswer\nto the\nquestion\ngiven\nthe\noptions\nand\nadditional\ninformation .\n259\n\"\"\"\n260\n261\ndef\n__init__(self , video: torch.Tensor , start: int = None , end: int = None , parent_start\n=0, queues=None):\n262\n\"\"\" Initializes a VideoSegment\nobject by\ntrimming\nthe\nvideo at the\ngiven [start , end]\ntimes\nand\nstores\nthe\n263\nstart\nand end\ntimes as\nattributes . If no times\nare\nprovided , the\nvideo is left\nunmodified , and the\ntimes\nare\n264\nset to the\nbeginning\nand end of the\nvideo.\n265\n266\nParameters\n267\n-------\n268\nvideo : torch.Tensor\n269\nA tensor of the\noriginal\nvideo.\n270\nstart : int\n271\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n272\nend : int\n273\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n274\n\"\"\"\n275\n276\nif start is None\nand end is None:\n277\nself. trimmed_video = video\n278\nself.start = 0\n279\nself.end = video.shape [0] # duration\n280\nelse:\n281\nself. trimmed_video = video[start:end]\n282\nif start is None:\n283\nstart = 0\n284\nif end is None:\n285\nend = video.shape [0]\n286\nself.start = start + parent_start\n287\nself.end = end + parent_start\n288\n289\nself.num_frames = self. trimmed_video .shape [0]\n290\n291\ndef\nframe_iterator (self) -> Iterator[ImagePatch ]:\n292\n\"\"\" Returns\nan\niterator\nover\nthe\nframes in the\nvideo\nsegment.\"\"\"\n293\nfor i in range(self.num_frames ):\n294\nyield\nImagePatch(self. trimmed_video [i], self.start + i)\n295\n296\ndef\ntrim(self , start: Union[int , None] = None , end: Union[int , None] = None) ->\nVideoSegment :\n297\n\"\"\" Returns a new\nVideoSegment\ncontaining a trimmed\nversion\nof the\noriginal\nvideo at\nthe [start , end]\n298\nsegment.\n299\n300\nParameters\n301\n----------\n302\nstart : Union[int , None]\n303\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n304\nend : Union[int , None]\n305\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n306\n307\nExamples\n308\n--------\n37\n309\n>>> # Return\nthe\nsecond\nhalf of the\nvideo\n310\n>>> def\nexecute_command (video):\n311\n>>> video_segment = VideoSegment (video)\n312\n>>> video_second_half = video_segment .trim( video_segment .num_frames\n// 2,\nvideo_segment .num_frames)\n313\n>>> return\nvideo_second_half\n314\n\"\"\"\n315\nif start is not\nNone:\n316\nstart = max(start , 0)\n317\nif end is not\nNone:\n318\nend = min(end , self.num_frames )\n319\n320\nreturn\nVideoSegment (self.trimmed_video , start , end , self.start)\n321\n322\ndef\nselect_answer (self , info: dict , question: str , options: List[str ]) -> str:\n323\nreturn\nselect_answer (self.trimmed_video , info , question , options)\n324\n325\ndef\n__repr__(self):\n326\nreturn \" VideoSegment ({}, {})\".format(self.start , self.end)\n327\n328\ndef\nsimple_query (self , question) -> str:\n329\n\"\"\" Ask a simple\nquestion\nabout\nthe\nvideo.\n330\n331\nExamples\n332\n--------\n333\n# why\ndoes X happen?\n334\n# possible_answers : [\u2019answer1 \u2019, \u2019answer2 \u2019, \u2019answer3 \u2019, \u2019answer4 \u2019, \u2019answer5 \u2019]\n335\ndef\nexecute_command (video , question , possible_answers ) ->[str , dict ]:\n336\n# Create a video\nsegment\nobject\n337\nvideo_segment = VideoSegment (video)\n338\n# The\nquestion\nis simple , so just\nask\n339\ninfo = video_segment . simple_query (\" why\ndoes X happen ?\")\n340\n# Choose\nthe\nanswer\namong\ngiven\npossible\nanswers\n341\nanswer = select_answer (info , question , possible_answers )\n342\nreturn\nanswer\n343\n\"\"\"\n344\nanswer = simple_query (question)\n345\nreturn\nanswer\n346\n347\n348 def\nbest_image_match ( list_patches : List[ ImagePatch], content: List[str], return_index =False)\n-> Union[ImagePatch , int]:\n349\n\"\"\" Returns\nthe\npatch\nmost\nlikely to\ncontain\nthe\ncontent.\n350\nParameters\n351\n----------\n352\nlist_patches : List[ImagePatch]\n353\ncontent : List[str]\n354\nthe\nobject of\ninterest\n355\nreturn_index : bool\n356\nif True , returns\nthe\nindex of the\npatch\nmost\nlikely to\ncontain\nthe\nobject\n357\n358\nReturns\n359\n-------\n360\nint\n361\nPatch\nmost\nlikely to\ncontain\nthe\nobject\n362\n\"\"\"\n363\nreturn\nbest_image_match (list_patches , content , return_index )\n364\n365\n366 def\ndistance(patch_a: ImagePatch , patch_b: ImagePatch ) -> float:\n367\n\"\"\"\n368\nReturns\nthe\ndistance\nbetween\nthe\nedges of two\nImagePatches . If the\npatches\noverlap , it\nreturns a negative\ndistance\n369\ncorresponding\nto the\nnegative\nintersection\nover\nunion.\n370\n371\nParameters\n372\n----------\n373\npatch_a : ImagePatch\n38\n374\npatch_b : ImagePatch\n375\n376\nExamples\n377\n--------\n378\n# Return\nthe qux\nthat is\nclosest\nto the foo\n379\n>>> def\nexecute_command (image):\n380\n>>>\nimage_patch = ImagePatch(image)\n381\n>>>\nqux_patches = image_patch .find(\u2019qux \u2019)\n382\n>>>\nfoo_patches = image_patch .find(\u2019foo \u2019)\n383\n>>>\nfoo_patch = foo_patches [0]\n384\n>>>\nqux_patches.sort(key=lambda x: distance(x, foo_patch))\n385\n>>>\nreturn\nqux_patches [0]\n386\n\"\"\"\n387\nreturn\ndistance(patch_a , patch_b)\n388\n389\n390 def\nbool_to_yesno ( bool_answer : bool) -> str:\n391\nreturn \"yes\" if\nbool_answer\nelse \"no\"\n392\n393\n394 def\nselect_answer (info: str , question: question , possible_answers : str) -> str:\n395\n\"\"\" Given an info , question\nand\npossible\nanswers , select\nthe\ncorrect\nanswer.\n396\n397\nExamples\n398\n--------\n399\n# what\ndoes\nman do at the end of the\nvideo\n400\n# possible_answers : [\u2019answer1 \u2019, \u2019answer2 \u2019, \u2019answer3 \u2019, \u2019answer4 \u2019, \u2019answer5 \u2019]\n401\ndef\nexecute_command (video , question , possible_answers ) ->[str , dict ]:\n402\n# Create a video\nsegment\nobject\n403\nvideo_segment = VideoSegment (video)\n404\n# Caption\nlast\nframe of the\nvideo (end of video)\n405\nlast_frame = ImagePatch(video_segment ,\n-1)\n406\nlast_caption = last_frame. simple_query (\" What is this ?\")\n407\nmen = last_frame.find (\"man\")\n408\nif len(men) == 0:\n409\nmen = [last_frame]\n410\nman = men [0]\n411\nman_action = man. simple_query (\" What is the man\ndoing ?\")\n412\n# Answer\nthe\nquestion. Remember\nto create\nthe\ninfo\ndictionary\n413\ninfo = {\n414\n\"Caption\nof last\nframe \": last_caption ,\n415\n\"Man\nlooks\nlike he is doing \":\nman_action\n416\n}\n417\nanswer = video_segment . select_answer (info , question , possible_answers )\n418\nreturn\nanswer , info\n419\n\"\"\"\n420\n421\n422 INSERT_IN_CONTEXT_EXAMPLES_HERE\n423\n424\n425 Write a function\nusing\nPython\nand the\nVideoSegment\nclass (above) that\ncould be\nexecuted\nto\nprovide\nan answer\nto the\nquery.\n426\n427 Consider\nthe\nfollowing\nguidelines:\n428 - Use\nbase\nPython (comparison , sorting) for\nbasic\nlogical\noperations , left/right/up/down ,\nmath , etc.\n429\n430 INSERT_PREVIOUS_CODE_AND_ERROR_HERE\n431\n432 # INSERT_QUERY_HERE\n39\nA.4.4\nNExT-QA - Abstract API\n1 import\nmath\n2\n3\n4 class\nVideoSegment :\n5\n\"\"\"A Python\nclass\ncontaining a video , as well as\nrelevant\ninformation .\n6\nAttributes\n7\n----------\n8\nvideo : np.ndarray\n9\nA tensor of the\noriginal\nvideo.\n10\nstart : int\n11\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n12\nend : int\n13\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n14\nnum_frames ->int\n15\nAn int\ncontaining\nthe\nnumber of\nframes in the\nvideo\nsegment.\n16\n17\nMethods\n18\n-------\n19\ntrim(start , end)->VideoSegment\n20\nReturns a new\nVideoSegment\ncontaining a trimmed\nversion\nof the\noriginal\nvideo at the [\nstart , end] segment.\n21\nselect_answer (info , question , possible_answers )->str\n22\nReturns\nthe\nanswer\nto the\nquestion\ngiven\nthe\npossible\nanswers\nand\nadditional\ninformation\n.\n23\n\"\"\"\n24\n25\ndef\n__init__(self , video: np.ndarray , start: int = None , end: int = None , parent_start\n=0, queues=None):\n26\n\"\"\" Initializes a VideoSegment\nobject by\ntrimming\nthe\nvideo at the\ngiven [start , end]\ntimes\nand\nstores\nthe\n27\nstart\nand end\ntimes as\nattributes . If no times\nare\nprovided , the\nvideo is left\nunmodified , and the\ntimes\nare\n28\nset to the\nbeginning\nand end of the\nvideo.\n29\n30\nParameters\n31\n-------\n32\nvideo : np.ndarray\n33\nA tensor of the\noriginal\nvideo.\n34\nstart : int\n35\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n36\nend : int\n37\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n38\n\"\"\"\n39\n40\nif start is None\nand end is None:\n41\nself. trimmed_video = video\n42\nself.start = 0\n43\nself.end = video.shape [0] # duration\n44\nelse:\n45\nself. trimmed_video = video[start:end]\n46\nif start is None:\n47\nstart = 0\n48\nif end is None:\n49\nend = video.shape [0]\n50\nself.start = start + parent_start\n51\nself.end = end + parent_start\n52\n53\nself.num_frames = self. trimmed_video .shape [0]\n54\n55\ndef\ntrim(self , start: Union[int , None] = None , end: Union[int , None] = None) ->\nVideoSegment :\n40\n56\n\"\"\" Returns a new\nVideoSegment\ncontaining a trimmed\nversion\nof the\noriginal\nvideo at\nthe [start , end]\n57\nsegment.\n58\n59\nParameters\n60\n----------\n61\nstart : Union[int , None]\n62\nAn int\ndescribing\nthe\nstarting\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n63\nend : Union[int , None]\n64\nAn int\ndescribing\nthe\nending\nframe in this\nvideo\nsegment\nwith\nrespect\nto the\noriginal\nvideo.\n65\n66\nExamples\n67\n--------\n68\n>>> # Return\nthe\nsecond\nhalf of the\nvideo\n69\n>>> def\nexecute_command (video):\n70\n>>>\nvideo_segment = VideoSegment (video)\n71\n>>>\nvideo_second_half = video_segment .trim( video_segment .num_frames\n// 2,\nvideo_segment .num_frames)\n72\n>>>\nreturn\nvideo_second_half\n73\n\"\"\"\n74\nif start is not\nNone:\n75\nstart = max(start , 0)\n76\nif end is not\nNone:\n77\nend = min(end , self.num_frames )\n78\n79\nreturn\nVideoSegment (self.trimmed_video , start , end , self.start)\n80\n81\ndef\nget_video_segment_of_event (self , event) -> VideoSegment :\n82\nreturn\nget_video_segment_of_event (event)\n83\n84\ndef\nget_video_segment_before_event (self , event) -> VideoSegment :\n85\nreturn\nget_video_segment_before_event (event)\n86\n87\ndef\nget_video_segment_after_event (self , event) -> VideoSegment :\n88\nreturn\nget_video_segment_after_event (event)\n89\n90\ndef\ncaption_video (self , question) -> str:\n91\nreturn\ncaption_video (question)\n92\n93\ndef\nsimple_query (self , question) -> str:\n94\n\"\"\" Ask a simple\nquestion\nabout\nthe\nvideo.\n95\n96\nExamples\n97\n--------\n98\n# why\ndoes X happen?\n99\n# possible_answers : [\u2019answer1 \u2019, \u2019answer2 \u2019, \u2019answer3 \u2019, \u2019answer4 \u2019, \u2019answer5 \u2019]\n100\ndef\nexecute_command (video , question , possible_answers ) ->[str , dict ]:\n101\n# Create a video\nsegment\nobject\n102\nvideo_segment = VideoSegment (video)\n103\n# The\nquestion\nis simple , so just\nask\n104\ninfo = video_segment . simple_query (\" why\ndoes X happen ?\")\n105\n# Choose\nthe\nanswer\namong\ngiven\npossible\nanswers\n106\nanswer = select_answer (info , question , possible_answers )\n107\nreturn\nanswer\n108\n\"\"\"\n109\nanswer = simple_query (question)\n110\nreturn\nanswer\n111\n112\n113 def\nselect_answer (info: str , question: question , possible_answers : str) -> str:\n114\n\"\"\" Given an info , question\nand\npossible\nanswers , select\nthe\ncorrect\nanswer.\n115\n116\nExamples\n117\n--------\n118\n# what\ndoes\nperson A do after\nevent X?\n119\n# possible_answers : [\u2019answer1 \u2019, \u2019answer2 \u2019, \u2019answer3 \u2019, \u2019answer4 \u2019, \u2019answer5 \u2019]\n41\n120\ndef\nexecute_command (video , question , possible_answers ) ->[str , dict ]:\n121\n# Create a video\nsegment\nobject\n122\nvideo_segment = VideoSegment (video)\n123\n# Get\nvideo\nsegment\nafter\nevent X\n124\nvideo_segment_after = video_segment . get_video_segment_after_event (\" event X\")\n125\n# Ask\nwhat\nthe\nperson A is doing\n126\ninfo = video_segment_after . caption_video (\" What is person A doing ?\")\n127\n# Choose\nthe\nanswer\namong\ngiven\npossible\nanswers\n128\nanswer = select_answer (info , question , possible_answers )\n129\nreturn\nanswer\n130\n\"\"\"\n131\n132\n133 INSERT_IN_CONTEXT_EXAMPLES_HERE\n134\n135 Write a function\nusing\nPython\nand the\nVideoSegment\nclass (above) that\ncould be\nexecuted\nto\nprovide\nan answer\nto the\nquery.\n136\n137 Consider\nthe\nfollowing\nguidelines:\n138 - Use\nbase\nPython (comparison , sorting) for\nbasic\nlogical\noperations , left/right/up/down ,\nmath , etc.\n139 - The\ninput to your\nprogram\nis a video , question\nand\npossible\nanswers.\n140 - Always\nstart\nyour\nfunction\nby\ncreating a \u2018video_segment = VideoSegment (video)\u2018 object.\n141\n142 INSERT_PREVIOUS_CODE_AND_ERROR_HERE\n143\n144 # INSERT_QUERY_HERE\n42\n"
  }
]