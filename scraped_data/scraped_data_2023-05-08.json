[
  {
    "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
    "link": "https://arxiv.org/pdf/2305.03111.pdf",
    "upvote": "6",
    "text": "Can LLM Already Serve as A Database Interface?\nA BIg Bench for Large-Scale Database Grounded\nText-to-SQLs\nJinyang Li1,\u2663 \u2021, Binyuan Hui2,\u2663, Ge Qu1,\u2663, Jiaxi Yang2, Binhua Li2, Bowen Li6,\nBailin Wang5, Bowen Qin2, Ruiying Geng2, Nan Huo1, Xuanhe Zhou3, Chenhao Ma6,\nGuoliang Li3, Kevin C.C. Chang7, Fei Huang2, Reynold Cheng1, Yongbin Li2\n1 The University of Hong Kong 2 DAMO Academy, Alibaba Group\n3 Tsinghua University 4 Shanghai AI Laboratory 5 MIT CSAIL\n6 The Chinese University of Hong Kong, Shenzhen\n7 University of Illinois at Urbana-Champaign\n{jl0725,quge}@connect.hku.hk, ckcheng@cs.hku.hk\nbinyuan.hby@alibaba-inc.com\nAbstract\nText-to-SQL parsing, which aims at converting natural language questions into\nexecutable SQLs, has gained increasing attention in recent years. In particular,\nGPT-4 and Claude-2 have shown impressive results in this task. However, most of\nthe prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema\nwith few rows of database values leaving the gap between academic study and\nreal-world applications. To mitigate this gap, we present BIRD, a BIg bench for\nlaRge-scale Database grounded in text-to-SQL tasks, containing 12,751 text-to-\nSQL pairs and 95 databases with a total size of 33.4 GB, spanning 37 professional\ndomains. Our emphasis on database values highlights the new challenges of dirty\nand noisy database values, external knowledge grounding between NL questions\nand database values, and SQL efficiency, particularly in the context of massive\ndatabases. To solve these problems, text-to-SQL models must feature database\nvalue comprehension in addition to semantic parsing. The experimental results\ndemonstrate the significance of database values in generating accurate text-to-SQLs\nfor big databases. Furthermore, even the most effective text-to-SQL models, i.e.\nGPT-4, only achieve 54.89% in execution accuracy, which is still far from the\nhuman result of 92.96%, proving that challenges still stand. We also provide\nan efficiency analysis to offer insights into generating text-to-efficient-SQLs that\nare beneficial to industries. We believe that BIRD will contribute to advancing\nreal-world applications of text-to-SQL research. The leaderboard and source code\nare available: https://bird-bench.github.io/.\n1\nIntroduction\nText-to-SQL parsing [55, 50, 51, 3, 52, 37], which focuses on converting natural language questions\ninto SQL queries, has attracted significant research interests from both academia and industry. This\nattention stems from its potential to empower non-expert data analysts in automatically extracting\ndesired information from ubiquitous relational databases using natural language. Recent advances in\nneural models, including those based on large language models (LLMs), have led to an impressive\nperformance on existing benchmarks such as SPIDER [53] and WikiSQL [58]. For instance, the\nexecution accuracy of the top-performing model in the SPIDER leaderboard has increased from 53.5%\n\u2663 Equal contribution.\n\u2021Work done during an intern at Alibaba DAMO Academy.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2305.03111v3  [cs.CL]  15 Nov 2023\nExternal Knowledge Reasoning\nLarge and Realistic Database Values\nSQL Execution Efficiency\nWhat is the winning rate of Boston Celtics in 2000?\nSELECT COUNT(won) / ((COUNT(won) + COUNT(lose)) \nFROM teams WHERE team_name = \u2018Boston Celtics\u2019 \nAND year = 2000;\nExternal Knowledge:\nwinning rate = # won / (# won + # lose)\nWhat is the average salary of the worst performing managers?\nSELECT AVG(CAST(REPLACE(SUBSTR(T1.salary, 4), ',', '') AS REAL)) FROM\nlast_name\nMilgrom\n\u2026 \u2026\nem_id\n0000\n\u2026 \u2026\nEmployees\nUS$57,500.00\nfirst_name\nsalary\n2222\n6543\nAdams\nWood\nMilgrom\nSandy\nEmily\n\u2026 \u2026\nUS$19,500.00\nUS$69,000.00\n\u2026 \u2026\nReasoned Database:\nemployee AS T1 JOIN position AS T2 ON T1.positionID = T2.positionID \nWHERE T1.performance = 'Poor' AND T2.positiontitle = 'Manager'\nAmong the coaches who have served more than 2 NBA teams, during \nwhich coach\u2018s period of coaching, a team has the least numbers of \ngames lost in the post-season games?\nSELECT coachID FROM coaches WHERE lgID='NBA\u2019 AND post_wins !=0\nSQL1: normal semantic parser \nAND post_losses !=0 AND coachID IN \n(SELECT coachID FROM coaches WHERE lgID='NBA\u2019 GROUP BY coachID \nHAVING COUNT(tmID)>=2) ORDER BY post_losses ASC LIMIT 1 ;\nRun time: 22.4s\nWhat is the average salary of the worst performing managers?\nSELECT AVG(CAST(REPLACE(SUBSTR(T1.salary, 4), ',', '') AS REAL)) FROM\nlast_name\nMilgrom\n\u2026 \u2026\nem_id\n0000\n\u2026 \u2026\nEmployees\nUS$57,500.00\nfirst_name\nsalary\n2222\n6543\nAdams\nWood\nSanta\nSandy\nEmily\n\u2026 \u2026\nUS$19,500.00\nUS$69,000.00\n\u2026 \u2026\nReasoned Database:\nemployee AS T1 JOIN position AS T2 ON T1.positionID = T2.positionID \nWHERE T1.performance = 'Poor' AND T2.positiontitle = 'Manager'\nSQL2: efficient semantic parser\nSELECT coachID FROM coaches WHERE lgID=\u2018NBA\u2019 AND post_wins !=0 \nAND post_losses !=0 AND EXISTS (SELECT 1 FROM coaches AS coaches1 \nWHERE (coaches1.lgID=\u2018NBA\u2019) AND (coaches.coachID=coaches1.coachID)\nGROUP BY coaches1.coachID HAVING count(coaches1.tmID) >= 2\nORDER BY NULL ) ORDER BY coaches.post_losses ASC LIMIT 1 \nRun time: 4.0s\nHow many accounts are eligible for loans in New York City?\nThe condition of loans is that \nthe type of the account should \nbe \u201cOWNER\u201d.\nSELECT COUNT(*) FROM account WHERE account.type \n= \u2018OWNER\u2019 AND city = \u2018NY\u2019;\nExternal Knowledge:\n(a).\n(b).\n(c).\nHow many accounts are eligible for loans in New York City?\nThe condition of loans is that \nthe type of the account should \nbe \u201cOWNER\u201d.\nSELECT COUNT(*) FROM account WHERE account.type \n= \u2018OWNER\u2019 AND disp_id = \u2018NY\u2019;\nExternal Knowledge:\nList account id who chooses weekly issue issuance statement?\n\u2018POPLATEK TYDNE\u2019 stands \nfor weekly issuance.\nSELECT account_id FROM account WHERE account.frequency\n= \u2018POPLATEK TYDNE\u2018;\nExternal Knowledge:\n= \u2018OWNER\u2019 AND disp_id = \u2018NY\u2019;\nList account id who chooses weekly issue issuance statement?\n\u2018POPLATEK TYDNE\u2019 stands \nfor weekly issuance.\nSELECT account_id FROM account WHERE account.frequency\n= \u2018POPLATEK TYDNE\u2018;\nExternal Knowledge:\nWhat is the average salary of the worst performing managers?\nFigure 1: Examples of challenges in our BIRD benchmark. 1) databases contain values of noisy data\ntypes [14, 23, 19, 31]. In the left example, the average salary could be fetched by processing the\ndata type from string (TEXT in SQLite) to float (REAL in SQLite) after deleting the special tokens,\n\"US$\" and \",\". 2) external knowledge and reasoning are required. In the middle example, models\nmust handle that only \"OWNER\" accounts are eligible for loans. 3) query execution efficiency needs\nto be considered. In the right example, the adoption of more efficient SQL queries leads to significant\ngains in speed, which is of great value in industries.\n[59] to 85.3% [35] over the past three years. The latest SOTA parser [35] in SPIDER benefits from\nthe powerful understanding and coding capabilities of the large language model (LLM), and such\nexcellent performance leads us to ask a question: Can LLM already serve as a database interface ?\nThe answer is no, as previous benchmarks focus on database schema with few rows of database values\nleaving the gap between academic study and the real world. As shown in Figure 1, first, we discovered\nthat current state-of-the-art models still struggle to generalize to more realistic situations characterized\nby large database sizes and noisy values. Second, the growth in database sizes often results in much\ncontext compression, making it challenging to reveal the entire context [1]. Thus it requires external\nknowledge reasoning for a comprehensive understanding. Third, existing benchmarks do not account\nfor SQL execution efficiency, which holds significant practical importance in real-life applications,\nnotably in the case of large databases. Motivated by these observations, we aim to develop a new text-\nto-SQL benchmark that better represents real-life scenarios and narrows the gap between experimental\nand practical settings.\nIn this work, we propose BIRD, a BIg Bench for LaRge-Scale Database Grounded in Text-to-SQLs\nfor real-world applications. BIRD contains complex 12,751 examples of querying information over\n95 big databases with a total size of 33.4 GB spanning 37 professional domains. For training and\ndevelopment, we collected and modified 80 open-source relational databases from real analysis\nplatforms (Kaggle, Relation.vit). To further avoid data leakage, we curated 15 additional relational\ndatabases for a hidden test set. Given these databases, we rely on crowdsourcing to collect natural\nlanguage questions and the corresponding SQLs. Additionally, we propose a new evaluation metric\nValid Efficiency Score (VES) to evaluate the efficiency of generated SQLs. To the best of our\nknowledge, BIRD is the first text-to-SQL benchmark to incorporate efficiency, promoting more\nefficient query methods within the context of massive and noisy database values.\nWe evaluate the performance of state-of-the-art text-to-SQL parsers using two popular methodologies:\nfine-tuning (FT) with T5 [38], and in-context learning (ICL) with advanced large language models\n(LLMs) such as ChatGPT [33] (gpt-3.5-turbo), Claude-2 [2] (claude-2.0), GPT-4 [32]\n(gpt-4-32k). Our experimental results demonstrate that the current models struggle to generalize\nwell on BIRD. Specifically, even the GPT-4 only achieves 54.89% in execution accuracy. In\ncomparison, the performance still lags far behind the human performance of 92.96%, proving that\nchallenges still stand. Moreover, we perform a comprehensive analysis to provide insight and\ndirection. We encourage further research by the NLP and DB communities to jointly address the\nmore realistic settings presented in this benchmark.\n2\nTask Formulation & Annotations\nText-to-SQL refers to the process of converting a natural language question Q into a SQL query Y\ncapable of retrieving relevant data from a database. The database can be represented as D = \u27e8C, T \u27e9,\n2\nHow many bioassay signatures \u2026 \u2026\nWhat is percentage of male who \u2026 \u2026\nWhat is the tag of the album \u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\nSELECT COUNT(T1.signature_id)FROM \u2026 \u2026\nSELECT CAST(SUM(CASE WHEN gender \u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\nSELECT T2.tag FROM torrents FROM \u2026 \u2026\nexperts\nAnnotation\nQuestion Crowd\nSQL Crowd\n4\n3\n4\nAnnotation\nAnnotation\n1\nSupervision\nSupervision\nTraining & Test\n2\n3\n8 / 10\n9 / 10\nenroll\nenroll\nOriginal Column Name\nColumn Name\nData Type\nValue Description\nColumn Description\naccount_id\ndistrict_id\nfrequency\ndate\naccount id\ndistrict id\nfrequency\ndate\nUnique id identifying the account\nLocation of branch\nFreq of issuance of statements\nAccount opening date\nInteger\nInteger\nText\nDatetime\nPOPLATEK MESICNE: \u2026\nPOPLATEK TYDNE: \u2026\nPOPLATEK PO\nOBRATU: \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\nWhat is percentage of male who \u2026 \u2026\nSELECT CAST(SUM(CASE WHEN gender \u2026 \u2026\nSELECT CAST(SUM(CASE WHEN gender \u2026 \u2026\nSELECT CAST(SUM(CASE WHEN gender \u2026 \u2026\nSELECT CAST(SUM(CASE WHEN gender \u2026 \u2026\nQuestion:\nSQL1:\nSQL2:\nSQL1:\nSQL2:\nMATCH\nCheck\nUntil\nCollect\nRESULT 1\nRESULT 2\nRESULT 1\nRESULT 2\nMISMATCH\n(a) Annotation Workflow\n(b) Double-blind Annotation\n(c) Database Description\nFigure 2: An Overview of the BIRD Annotation Workflow in (a). This figure depicts a four-step\nprocedure. (1) The workflow begins with specialists assembling and producing databases and\ndescription files. (2) Experts then teach and evaluate crowdsourcing people, keeping only those who\npass the evaluation. (3) Question annotators create a corpus of questions using databases and their\ncorresponding description files. (4) SQL annotators produce SQL files, equipped with databases,\ndescriptions, and questions. (b) and (c) also depict the Double-blind annotation procedure and an\nexample of database descriptions.\nwhere C and T are columns and tables respectively. When dealing with complex database values,\nsuch as BIRD, it is crucial to incorporate external knowledge evidence, denoted as K, to improve the\nmodels\u2019 understanding of database values. Finally, the text-to-SQL could be formulated as:\nY = f(Q, D, K | \u03b8),\n(1)\nwhere the function f (\u00b7 | \u03b8) can represent a model or neural network with the parameter \u03b8.\n3\nDataset Construction\n3.1\nAnnotation Entrance\nTo deliver a high-quality benchmark, we administer thorough exams to all applicants and only hire\nthose who pass these rigorous tests. Further information is available in the Appendix B.2.\n3.2\nDatabase Source\nIt is difficult to collect databases with complex schemas and sufficient value due to privacy protection.\nEarlier works [45, 53] choose to self-design database schemas and value production. Nonetheless,\nthe value distribution and schemas may differ from real-world scenarios in this way. In our work,\nwe obtain and process databases from three different sources to enrich real-world attributes. 32% of\nour databases are sourced from Kaggle*, a platform renowned for holding data science competitions\nwith difficult, noisy values and schemas. Another 48% come from CTU Prague Relational Learning\nRepository\u2020, an open platform for machine learning research with multi-relational data. The remaining\n20% are built by acquiring open tables, synthesizing and standardizing schemas, and generating\ndatabase constraints. All of these databases contain real and large value distributions and are easily\naccessible with the appropriate licenses. Finally, we present 95 databases consisting of 69, 11, and 15\ndatabases for training, development, and testing respectively. Our databases cover 37 professional\ndomains, including blockchain, sports, health care, politics, etc. We anticipate that it will be a\nsignificant resource for researchers to explore domain generalization in semantic parsing tasks with\nlarge database values.\n*https://www.kaggle.com/\n\u2020https://relational.fit.cvut.cz/\n3\n3.3\nQuestion Annotation\nDatabase Description File.\nThe Database Description File is a crucial resource designed to aid\nannotators in comprehending database values, thereby allowing them to ask insightful questions. It\noffers two primary pieces of information regarding the database. (1) Full schema names: database\ntable and column names are frequently abbreviated, which are difficult to understand. (2) Value\ndescription: this aspect is particularly useful when phrases or tokens in a question do not directly\nmatch values in the database.\nExternal Knowledge Evidence.\nIn our study of professional data analysis, we find that external\nknowledge evidence is required to map the natural language instructions into counterpart database\nvalues. Therefore, we collect and classify such evidence into four categories: (1) Numeric Reasoning\nKnowledge: this category refers to the mathematical computation required for certain SQL operations.\nIn our benchmark, we present 8 basic math operations, including 4 complex operations as [7]: MINUS,\nADDITION, DIVISION, MULTIPLY. BIRD also contains compositional operations over basic ones,\nsuch as percentages, formulas, etc. (2) Domain Knowledge: this category consists of domain-specific\nknowledge that is utilized to generate SQL operations [10, 57]. For instance, a business analyst in\nthe banking business requires knowledge of financial indicators such as return on investment and net\nincome in order to generate effective SQL queries. (3) Synonym Knowledge: this category includes\nwords or expressions that have the same or similar meanings regardless of how they are phrased\ndifferently [11]. (4) Value Illustration: this category refers to detailed descriptions of database\nvalues, including value types, value categories, and the mapping combinations of columns and values\nthat correspond to entities, for example: \"center\" can be represented by \"pos = C\" in the\ndatabase professional_basketball.\n3.4\nSQL Annotation\nDouble-Blind Annotation.\nAs shown in Figure 2 (b), we employ a double-blind approach [42] for\nSQL annotation. This approach involves two independent SQL annotators who generate SQLs for the\nsame question without discussion. The annotated SQLs are executed in databases, and those yielding\nidentical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is\nreached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is\na small probability for two skillful annotators to generate the same incorrect results when databases\nhave large values. The more semantic-equivalent and efficient SQL selected by experts for each\nquestion is picked as ground truth SQL in BIRD, and the external knowledge evidence sentences are\nrecorded for each SQL if utilized.\nExamination.\nExperts evaluate each text-to-SQL pair to ensure the highest quality of data. The\nevaluation process includes two dimensions: SQL validness, and text-knowledge-SQL alignment.\nFirstly, the SQL validness will be confirmed that each SQL is executable and can return a valid\nresult from the database. The \"valid result\" refers to the set of results that is not \"NULL\". If the\nexecuted result set is \"NULL\", experts will make slight changes to the conditions of the questions\nuntil the associated SQLs can provide a valid result set. Secondly, text-knowledge-SQL alignment\nis involved to ensure that each SQL can be generated with the given texts and knowledge evidence.\nIf the evidence is insufficient to generate the SQL or contains errors, experts will be in charge of\ncorrecting them.\n4\nData Statistics\nOverall Statistics\nTable 1 presents an overview comparison between BIRD and other cross-domain\ntext-to-SQL benchmarks. As the statistics demonstrate, BIRD is a large-scale cross-domain bench-\nmark, covering complex SQL functions, knowledge reasoning, and efficiency evaluation.\nQuestion Statistics\nDatabase values bring more challenges in text-to-SQLs. In order to underscore\nthis, we classify questions into two macro-categories: Fundamental Type and Reasoning Type, and\neach contains 4-5 micro-categories in detail. The Fundamental Type of questions refers to those\nthat can be answered without database value comprehension. It contains Match-based (83.9%),\nRanking (20.3%), Comparison (16.7%), Counting (30.4%), Aggregation (15.7%). The\n4\nTable 1: An overview comparison between BIRD and other cross-domain text-to-SQL benchmarks.\nIn SQL, Function pertains to the SQL functions (Appendix B.11). Knowledge refers to whether\nor not this dataset necessitates external knowledge reasoning from the model. Efficiency refers\nto whether or not this dataset takes into consideration execution efficiency.\nDataset\n# Example\n# DB\n# Table/DB\n# Row/DB\nFunction\nKnowledge\nEfficiency\nWikiSQL [58]\n80,654\n26,521\n1\n17\nSPIDER [53]\n10,181\n200\n5.1\n2K\nKaggleDBQA [24]\n272\n8\n2.3\n280K\nBIRD\n12,751\n95\n7.3\n549K\nretail\nsport\nmovie\nsoftware\nuniversity\ngeography\ncartoon\nIT\nfood\ntransportation\nhealthcare\ngames\nbooks\nfinancial\nrestaurant\ncriminal\nlaw\nacademic\nmedical\neconomies\nchemistry\nvision\nmusic\ntv_series\nbanking\nairport\npublications\neducation\nweather\nhuman resources\nblockchain\nbiology\nworks_cycles\nretail_world\nretails\nregional_sales\ncar_retails\nsuperstore\nsales\nretail_complains\nsoccer_2016\nhockey\nformula_1\nolympics\nprofessional_basketball\neuropean_football_2\nice_hockey_draft\neuropean_football_1\nmovie 3\nmovie_platform\nmovies_4\nmovielens\nmovie\ntalkingdata\ncodebase_community\ncodebase_comments\nsocial_media\nsoftware_company\nstudent_club\nuniversity\ncs_semester\ncollege_completion\ncomputer_student\nmondial_geo\naddress\nworld\nsimpson_episodes\nsuperhero\ndisney\npublic_review_platform\napp_store\nfood_inspection_2\nbeer_factory\nfood_inspection\ncookbook\ncraftbeer\nbike_share_1\nshipping\ntrains\ncars\nsynthea\ndonor\nmental_health_survey\nvideo_games\ncard_games\nbooks\nshakespeare\nstudent_loan\ndebit_card_specializing\nrestaurant\nmenu\nchicago_crime\nshooting\nlegislator\nauthors\nthrombosis_prediction\nworld_development_indicators\ntoxicology\nimage_and_language\nmusic_platform_2\nmusic_tracker\nlaw_episode\nfinancial\nairline\nbook_publishing_company\nciteseer\ncalifornia_schools\nsales_in_weather\nhuman_resources\ncoinmarketcap\ngenes\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nsize\nDomain Distribution\nLoading [MathJax]/extensions/MathMenu.js\ntrain\ndev\ntest\ntotal\nTEXT\nINTEGER\nREAL\nDATE_RELATED\n50.6\n35.0\n8.7\n5.6\n40.2\n47.1\n9.4\n3.3\n51.3\n37.3\n3.3\n8.0\n49.4\n36.8\n8.0\n5.7\nPercentage (%)\nPercentage (%)\n(M)\na) Database domain distribution w/ size\nb) Database value type distribution\nFigure 3: This is a comprehensive database distribution in the BIRD. a) shows the domain and size\ndistribution of each database. And b) shows the data type distribution of databases.\nReasoning Type entails questions that demand the external knowledge grounding on values,\nwhich is exclusive to BIRD. To be specific, the questions about Domain Knowledge (23.6%),\nNumeric Computing (24.5%), Synonym (7.2%), Value Illustration (70.1%), are in-\nvolved in BIRD. There are ample examples in Appendix B.3. In addition, we observe that 70.1% of\nthe questions need value illustrations. This indicates that more real-world questions in text-to-SQL\napplications demand a thorough understanding of database values, which is consistent with our\nmotivation for creating the BIRD benchmark.\nDatabase Statistics\nIn BIRD, we investigate the distribution of database domains, database size, and\nvalue types. Figure 3 (a) presents a detailed distribution of domains and their counterpart databases in\na sunburst diagram for both training and development sets. The area of each semi-circle corresponds\nto the number of text-to-SQL pairs in this database. Figure 3 (a) also shows the size distributions\nof databases. The darker color means a larger size of databases, and vice versa. For example, the\ndatabase Donor is the largest database with 4.5 GB in this dataset. Furthermore, we observe from\nFigure 3 (b) that a considerable proportion of BIRD\u2019s data comprises date-related values. Considering\nthat real-world applications often rely on time-sensitive data [25], the prevalence of such questions\nhighlights the practical purposes.\nSQL Statistics\nWe provide the complexity and diversity of SQLs in BIRD. As illustrated in Figure\n4, we present a comprehensive distribution analysis of SQLs across four dimensions. No.Toks /\nSQL and No.JOINs / SQL demonstrate the intricacy of the SQLs in BIRD. No.of Keywords\nand No.n-grams / SQL (n=3) serve as the support for the diverse patterns of SQLs since we\ndecouple the question and SQL annotation procedures to make the situation more realistic [6].\n5\nEvaluation Metrics\nIn contexts of practical data analysis, text-to-SQL models are prioritized for delivering expected\nresults accurately and efficiently. Thus we provide two metrics in BIRD, execution accuracy (EX)\n5\nNo. Toks / SQL\nNo. of Keywords\nNo. n-grams / SQL (n=3)\nNo. JOINs / SQL\nWikiSQL\nSpider\nKaggleDBQA\nBird\nFigure 4: A comparative statistical analysis of SQL queries in the BIRD dataset and other cross-\ndomain text-to-SQL benchmarks.\nand valid efficiency score (VES) to evaluate text-to-SQL parsers confronted with large real-world\ndatabase values.\nExecution Accuracy (EX)\nEX is defined as the proportion of examples in the evaluation set for\nwhich the executed results of both the predicted and ground-truth SQLs are identical, relative to the\noverall number of SQLs [37]. Considering the result set as Vn executed by the nth ground-truth SQL\nYn, and the result set \u02c6Vn executed by the predicted SQL \u02c6Yn, EX can be computed by:\nEX = \u03a3N\nn=11(Vn, \u02c6Vn)\nN\n,\n(2)\nwhere 1(\u00b7) is an indicator function, which can be represented as:\n1(V, \u02c6V ) =\n(\n1,\nV = \u02c6V\n0,\nV \u0338= \u02c6V\n(3)\nValid Efficiency Score (VES)\nVES is designed to measure the efficiency of valid SQLs generated\nby models. It is worth noting that the term \"valid SQLs\" refers to predicted SQL queries whose\nresult sets align with those of the ground-truth SQLs. Any SQL queries that fail to fetch the correct\nvalues will be declared invalid since they are totally useless if they cannot fulfill the user requests,\nregardless of their efficiency. In this case, the VES metric considers both the efficiency and accuracy\nof execution results, providing a comprehensive evaluation of a model\u2019s performance. Formally, the\nVES can be expressed as:\nVES = \u03a3N\nn=11(Vn, \u02c6Vn) \u00b7 R(Yn, \u02c6Yn)\nN\n,\nR(Yn, \u02c6Yn) =\ns\nE(Yn)\nE( \u02c6Yn)\n(4)\nwhere R(\u00b7) denotes the relative execution efficiency of predicted SQL in comparison to ground-truth\nSQL, allowing for machine status-related uncertainty. E(\u00b7) is a function to measure the absolute\nexecution efficiency for each SQL in a given environment\u2021. Furthermore, we incorporate the square\nroot function to minimize random instances that are abnormally faster or slower than the ground-truth\nSQLs. Here, efficiency can refer to running time, throughput, memory cost, or merged metrics. In\nBIRD, we consider the running time mainly at this time. Appendix B.8 provides a detailed description\nof the VES.\n6\nExperiments\n6.1\nBaseline Models\nWe present the performance of two types of baseline models in BIRD. The first type of model is\nbased on fine-tuning (FT) techniques, which outputs SQL by tuning all parameters of language\nmodels to learn the annotated train set. On the other hand, the second type of model based on\n\u2021In BIRD evaluation, we run 100 times for each SQL in the same CPU and evaluate average results after\ndropping the outliers.\n6\nFigure 5: A bar chart provides a clear visualization of the performance of advanced models on BIRD.\nTable 2: The Execution Accuracy (EX) of advanced text-to-SQL models in BIRD. The human\nperformance is also provided.\nModels\nDevelopment Data\nTesting Data\nw/o knowledge\nw/ knowledge\nw/o knowledge\nw/ knowledge\nFT-based\nT5-Base\n6.32\n11.54 (+5.22)\n7.06\n12.89 (+5.83)\nT5-Large\n9.71\n19.75 (+10.04)\n10.38\n20.94 (+10.56)\nT5-3B\n10.37\n23.34 (+12.97)\n11.17\n24.05 (+12.88)\nICL-based\nPalm-2\n18.77\n27.38 (+8.61)\n24.71\n33.04 (+8.33)\nCodex\n25.42\n34.35 (+8.93)\n24.86\n36.47 (+11.61)\nChatGPT\n24.05\n37.22 (+13.17)\n26.77\n39.30 (+12.53)\nChatGPT + COT\n25.88\n36.64 (+10.76)\n28.95\n40.08 (+11.24)\nClaude-2\n28.29\n42.70 (+14.41)\n34.60\n49.02 (+14.42)\nGPT-4\n30.90\n46.35 (+15.45)\n34.88\n54.89 (+20.01)\nGPT-4 + DIN-SQL\n-\n50.72\n-\n55.90\nHuman Performance\n-\n-\n72.37\n92.96 (+20.59)\nin-context learning (ICL), can generate results without additional training. In FT models, we select\nT5 family [38] as the main baseline models. For ICL-based models, we provide zero-shot results of\nCodex (code-davinci-002), ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4-32k), Claude-2\n(claude-2.0), Palm-2 (text-bison-001). Additionally, we also implement a state-of-the-art\n(SOTA) model of SPIDER, DIN-SQL [35], to evaluate the challenges proposed by the BIRD dataset.\nTable 2, Table 3 and Figure 5 present the overall results of advanced language models on BIRD.\n6.2\nExecution Accuracy Analysis\nTable 2 and Figure 5 presents stratified performances of various models in BIRD. GPT-4 surpasses all\nbaseline language models. Claude-2 closely follows, demonstrating outstanding abilities in semantic\nparsing and knowledge reasoning. Further, the incorporation of a dedicated reasoning prompt by\n[35], enables DIN-SQL + GPT-4 to achieve a new state-of-the-art result on BIRD. It contains value\nsampling, few-shot demonstrations, and self-correction. Despite considerable advancements in\nLanguage Model Learning (LLMs) and prompt intelligence, the performance of these models lags\nobviously behind human capabilities. Not only does this gap highlight the complex nature of BIRD,\nbut it also presents opportunities for uncovering more capable models or advanced reasoning prompt\nmethods applicable to real-world text-to-SQL scenarios.\n7\nTable 3: The Valid Efficiency Score (VES) of advanced text-to-SQL models in BIRD. The human\nperformance is also presented.\nModels\nDevelopment Data\nTesting Data\nw/o knowledge\nw/ knowledge\nw/o knowledge\nw/ knowledge\nFT-based\nT5-Base\n7.78\n12.90 (+5.12)\n8.97\n14.71 (+5.74)\nT5-Large\n9.90\n22.74 (+12.84)\n12.25\n25.00 (+12.75)\nT5-3B\n13.62\n25.57 (+11.95)\n15.17\n27.80 (+12.63)\nICL-based\nPalm-2\n20.82\n28.64 (+7.82)\n31.32\n38.41 (+7.09)\nCodex\n33.37\n43.41 (+10.04)\n35.40\n41.60 (+6.20)\nChatGPT\n27.97\n43.81 (+15.84)\n36.68\n51.40 (+14.72)\nChatGPT + COT\n32.33\n42.30 (+9.97)\n49.69\n56.56 (+6.87)\nClaude-2\n32.75\n45.28 (+12.53)\n39.32\n55.77 (+16.45)\nGPT-4\n34.60\n49.77 (+15.17)\n40.20\n60.77 (+20.57)\nGPT-4 + DIN-SQL\n-\n58.79\n-\n59.44\nHuman Performance\n-\n-\n70.36\n90.27 (+19.91)\n6.3\nBaseline Performance on Spider\nDIN-SQL\nChatGPT\nCodex\nT5-3B\nT5-Large\nT5-Base\n82.8\n50.7\n72.1\n74.1\n71.5\n69.3\n57.9\n37.2\n34.4\n23.3\n19.8\n11.5\nSPIDER\nBIRD\nFigure 6: The EX results of the same baseline\nmodels on the SPIDER and BIRD dev set.\nSPIDER [53] is the most prevalent and complex\ncross-domain text-to-SQL benchmark. It mainly\nfocuses on evaluating schema-relevant seman-\ntic parsing capabilities. To demonstrate the in-\ncreasing difficulty of the BIRD dataset due to\nits complex database schema and values, we\nvisualize the execution accuracy of the same\nbaseline models on both the BIRD and SPIDER\ndatasets. To ensure a fair evaluation, all models\nare furnished with knowledge about values, and\nthe same programming prompt is implemented\nfor Language Models (LMs) across the two\ndatasets. Figure 6 shows the concentration on\ndatabase values makes BIRD become the most\nchallenging text-to-SQL benchmark. This dis-\nparity in the performance of each model demon-\nstrates the need for further research and develop-\nment of models capable of handling complicated\ndatabase schema and values.\n6.4\nEfficiency Analysis\nAccording to Table 3, we can observe that models with higher EX can more possibly achieve higher\nVES. This can be explained by the prerequisite that text-to-SQL models must accurately predict\nresults in order to attain a higher VES, which fulfills the practical purpose.\nTwo-Stage Optimization.\nIntuitively, the goal of text-to-efficient-SQL conversion can be de-\ncomposed into two sub-stages. Following previous text-to-SQL tasks, the first sub-stage, semantic\nparsing, concentrates on accurately converting questions into SQL queries. The second sub-stage\ninvolves optimizing the SQL queries, rewriting them to be more efficient while maintaining the same\nresults [61]. To demonstrate the efficacy of this approach, we selected 10 random examples from\nthe development set where ChatGPT accurately predicted the results. Then, our specialists optimize\nthese queries based on the established query optimization rules [28, 34, 62]. We observe that the\ntwo-stage optimization leads to an average time-saving of 77.75% while keeping the same results.\nChat w/ Database.\nBIRD introduces the novel mode of \"Chat With Database\", which enables\nmodels to be aware of data types and distributions by generating global SQL queries that interact\nwith databases. This approach lays the foundation for the development of more effective and efficient\nSQL queries. As observed in the experiment, the time-saving percentage of the SQL queries can\n8\nmatch-based\nranking\ncomparison\naggregation\ndomain KG\nmath\nsynonym\nvalue illustration\ncounting\nmatch-based\nranking\ncomparison\ncounting\naggregation\ndomain KG\nmath\nsynonym\nvalue illustration\nranking\nmatch-based\nvalue illustration\nsynonym\nmath\ndomain KG\naggregation\ncounting\ncomparison\n2023/10/24 01:55\nUntitled visualization | Flourish\nhttps://app.\ufb02ourish.studio/visualisation/15455665/edit\n1/1\n\uf00c\n\uf1fe\n\uf0ce\nCombined\nSeparate\nChatGPT\nGPT-4\nClaude-2\n0\n10\n10\n10\n20\n20\n20\n30\n30\n30\n40\n40\n40\n50\n50\n50\n1\n2\n3\n4\n5\n6\n7\n8\n\uf0da\n\uf0da\n\uf0da\n\uf0da\n\uf0da\n\uf0da\n\uf0da\n\uf0da\n\uf128\n\uf078\n\uf128\n\uf128\n\uf078\n\u2584\n\uf0da\n\uf015\n\uf0aa\n\uf019\n\uf1c5\n\uf055\n\uf1c9\n\uf005\n\uf078\nmatch-based\nranking\ncomparison\ncounting\naggregation\ndomain KG\nmath\nsynonym\nvalue illustration\nChatGPT\nGPT-4\nClaude-2\nOverall\nChatGPT\nGPT-4\nClaude-2\nFigure 7: The fine-grained categorical evaluation of advanced large language models on BIRD.\nbe reached at 87.3% by configuring indexes within the database. The detailed efficiency analysis is\npresented in Appendix B.5.\n6.5\nKnowledge Evidence Analysis\nWe implement each baseline model for both two scenarios. The first is NOT to provide the ground\ntruth external knowledge evidence sentence (w/o knowledge) for each sample. The other testing bed\nis to provide such evidence (w/ knowledge) and make text-to-SQL models do knowledge grounding\nby themselves. As we discuss in Section 3.3, expert annotations on external knowledge evidence\nsentences are employed to enhance the model\u2019s comprehension of database values.\nAfter being easily fed with the external knowledge evidence about the database values, all models\nhave a clear improvement across the different difficulty levels as shown in Table 2 and Table 4. This\nindicates that external knowledge evidence in BIRD is effective and instructive for models to better\nunderstand the database values. Also it illustrates that the database values are very important to\ntext-to-SQL models when facing more real databases. Besides, ICL-based approaches have a better\nself-knowledge grounding capability and pre-trained SQL knowledge than FT smaller models with\nless than 5B parameters. Equipped with COT, ChatGPT can perform better, since multi-step reasoning\nis beneficial when the knowledge and data are low-resource. Despite this, we observe a decline or\nlimited improvements in performance for ChatGPT + external knowledge evidence for COT version.\nWe hypothesize that the internal multi-step knowledge reasoning of LLMs is not compatible with\nthe way of external knowledge (evidence) in this situation. Therefore, the development of methods\nthat effectively combine the strong multi-step self-reasoning capabilities of LLMs with external\nknowledge reasoning coherently presents a promising future direction [29].\n6.6\nMore Analysis\nFine-grained Category Analysis.\nFigure 7 provides a detailed comparison of various dimensions\nof sub-capabilities of advanced LLMs on BIRD. The results indicate that GPT-4 exhibits superior\nperformance against ChatGPT and Claude-2 in all areas. Nevertheless, there is a notable disparity in\nthe performance of ranking and numerical computing (math) among all the models. This limitation\nmay suggest the inadequacy of contemporary LLMs for deep data science tasks because such tasks\nalways incorporate mathematical computations and rankings within the context of vague user queries.\nConversely, these models demonstrate relatively better performance in domain knowledge, synonym\ndetection, and value illustration, which can be attributed to their adequate linguistic training and\nreasoning capabilities during the pretraining phases.\nHuman Performance.\nIn order to activate the efforts of text-to-SQL studies to achieve an\napplication-level performance in real-world scenarios, we provide human performance in BIRD.\nTable 2, Table 3 shows that there\u2019s still a huge gap between even SOTA text-to-SQL models and\nhuman performance. The thorough introduction of procedures is in Appendix B.9.\nError Analysis.\nChatGPT is currently the most prevalent and cost-efficient LLM. Therefore, the\nperformance of ChatGPT is concentrated in this error analysis. The detailed analysis is in Appendix\nB.6. We observe 500 randomly sampled error cases, providing an in-depth assessment in the following\n9\ncategories. Wrong Schema Linking (41.6%) pertains to the scenario where ChatGPT can accurately\ncomprehend the structure of the database but erroneously associates it with inappropriate columns\nand tables. This demonstrates that the task of schema linking [43, 57], even in intricate and practical\nsituations, continues to be a significant obstacle for models. Misunderstanding Database Content\n(40.8%) occurs when ChatGPT either fails to recall the correct database structure (e.g., rtype\ndoesn\u2019t belong to the satscores table) or generates fake schema items (e.g., lap_records is not\nappearing in the formula_1 database and many values are predicted incorrectly) especially when\nthe database is very large. In this case, how to make ChatGPT really understand database structure\nand values [27] is still a pain point topic in LLMs. Misunderstanding Knowledge Evidence (17.6%)\nrefers to cases in which the model does not accurately interpret human-annotated evidence. An\ninstance is that ChatGPT directly copies the formula DIVIDE(SUM(spent), COUNT(spent)).\nThis finding demonstrates that ChatGPT exhibits a lack of robustness in response to unfamiliar\nprompts or knowledge, causing it to directly replicate formulas without considering SQL syntax [15].\nWe also observe that ChatGPT occasionally employs incorrect keywords (e.g., misusing the MySQL\nYear() function instead of an SQLite function STRFTIME()) or exhibits decoding errors.\n7\nRelated Work\nHigh-quality datasets are crucial for advancing various natural language processing tasks, including\ntext-to-SQL. Early single-domain text-to-SQL datasets like GeoQuery [55], ATIS [9], and Restaurant\n[20] targeted specific information retrieval tasks, while more recent datasets such as WikiSQL [58]\nand SPIDER [53] propose cross-domain dataset to require domain generalization. However, most\ncross-domain text-to-SQL datasets still emphasize database schema rather than values, diverging\nfrom real-world scenarios. KaggleDBQA [24] addressed this by constructing 272 text-to-SQL\npairs from eight databases on Kaggle, while other datasets like EHRSQL [25], SEDE [13], and\nMIMICSQL [46] collected diverse, large-value databases with more professional SQL queries.\nDespite these advancements, these datasets remain single-domain focused. Recent work has explored\nknowledge-intensive text-to-SQL benchmarks [10, 57], aiding experts in real-world analysis through\nknowledge grounding. BIRD is the first large-scale benchmark to incorporate these real-world\nfeatures, emphasizing database values.\n8\nLimitation and Future work\nDespite the high quality of SQL annotation produced by double-blind annotation, the procedure\nis resource-intensive. Future research could explore a human-computer interaction (HCI) based\napproach, incorporating advanced AI systems such as GPT-4 for taking parts of annotation duties, to\nmaintain data quality while reducing human effort. In addition, SQLite was chosen as the primary\nSQL codebase for previous text-to-SQL benchmarks and this study since it\u2019s friendly to users. While\nit presents difficulties in fetching Query Execution Plans (QEP) for precise efficiency computation\nand adapting to different SQL syntaxes. Future work will include PostgreSQL and MySQL versions\nof BIRD to resolve these limitations and provide a more robust research environment for both NLP\nand DB experts.\n9\nConclusion\nIn this paper, we introduce BIRD, a large-scale cross-domain, text-to-SQL benchmark with a particular\nfocus on large database values. BIRD mitigates the gap between text-to-SQL research and real-world\napplications by exploring three additional challenges: 1) handling large and dirty database values, 2)\nexternal knowledge evidence, and 3) optimizing SQL execution efficiency. Our experimental results\ndemonstrate that BIRD presents a more daunting challenge compared to existing benchmarks since\neven the most popular and powerful LLM, ChatGPT, falls significantly short of human performance.\nThis leaves plenty of room for improvement and innovation in the text-to-SQL tasks. Moreover, our\nthorough efficiency and error analyses provide valuable insights and directions for future research,\npaving the way for the development of more advanced and practical text-to-SQL solutions in real-\nworld scenarios.\n10\nAcknowledgement\nWe thank all constructive comments from anonymous reviewers. Reynold Cheng, Jinyang Li, Ge Qu\nand Nan Huo were supported by the Hong Kong Jockey Club Charities Trust (Project 260920140)\nand the University of Hong Kong (Project 104006830). Chenhao Ma was supported by NSFC\nunder Grant 62302421, Basic and Applied Basic Research Fund in Guangdong Province under\nGrant 2023A1515011280, Shenzhen Science and Technology Program ZDSYS20211021111415025.\nJinyang Li and Ge Qu were supported by HKU Presidential PhD Scholar Programme. Ge Qu was\nalso funded by Hong Kong PhD Fellowship Scheme. This work was supported by Alibaba Group\nthrough Alibaba Research Intern Program.\nReferences\n[1] Peter Alsberg. Space and time savings through large data base compression and dynamic\nrestructuring. Proceedings of the IEEE, 63:1114\u20131122, 1975.\n[2] Anthropic. Introducing Claude. 2023. URL https://www.anthropic.com/index/\nintroducing-claude.\n[3] Ruichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian Li, and Zhihao Liang. An\nencoder-decoder framework translating natural language to database queries. In Proceedings of\nthe Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18), page\n3977\u20133983, 2018.\n[4] Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li, Binhua Li, Zheng Cao, Weijie Li,\nFei Huang, Luo Si, and Yongbin Li. STAR: SQL guided pre-training for context-dependent\ntext-to-SQL parsing. In Findings of the Association for Computational Linguistics: EMNLP\n2022, pages 1235\u20131247, Abu Dhabi, United Arab Emirates, December 2022. Association for\nComputational Linguistics.\n[5] Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. LGESQL: Line graph\nenhanced text-to-SQL model with mixed local and non-local relations. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2541\u20132555,\nOnline, August 2021. Association for Computational Linguistics.\n[6] Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li,\nWuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang,\nZhiguo Wang, Vittorio Castelli, Patrick Ng, and Bing Xiang. Dr.spider: A diagnostic evaluation\nbenchmark towards text-to-SQL robustness. In The Eleventh International Conference on\nLearning Representations, 2023.\n[7] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema\nMoussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. FinQA: A\ndataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pages 3697\u20133711, Online and Punta Cana,\nDominican Republic, November 2021. Association for Computational Linguistics.\n[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm:\nScaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n11\n[9] Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith,\nDavid Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. Expanding the scope\nof the ATIS task: The ATIS-3 corpus. In Human Language Technology: Proceedings of a\nWorkshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\n[10] Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen\nZhan, Min-Yen Kan, and Jian-Guang Lou. Towards knowledge-intensive text-to-SQL semantic\nparsing with formulaic knowledge. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 5240\u20135253, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[11] Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie,\nand Pengsheng Huang. Towards robustness of text-to-sql models against synonym substitution.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 2505\u20132515. Association for Computational\nLinguistics, 2021. doi: 10.18653/v1/2021.acl-long.195.\n[12] Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang.\nTowards complex text-to-SQL in cross-domain database with intermediate representation. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 4524\u20134535, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1444.\n[13] Moshe Hazoom, Vibhor Malik, and Ben Bogin. Text-to-SQL in the wild: A naturally-occurring\ndataset based on stack exchange data. In Proceedings of the 1st Workshop on Natural Language\nProcessing for Programming (NLP4Prog 2021), pages 77\u201387, Online, August 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.nlp4prog-1.9.\n[14] Joseph M. Hellerstein. Quantitative data cleaning for large databases. In Proceedings of the\n2008 ACM SIGMOD international conference on Management of data, page 1197\u20131200, 2008.\n[15] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language\nmodels leaking your personal information? In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2038\u20132047, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics.\n[16] Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si,\nPengfei Zhu, and Xiaodan Zhu. Dynamic hybrid relation exploration network for cross-domain\ncontext-dependent semantic parsing. In Thirty-Fifth AAAI Conference on Artificial Intelligence,\nAAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,\nVirtual Event, February 2-9, 2021, pages 13116\u201313124. AAAI Press, 2021.\n[17] Binyuan Hui, Xiang Shi, Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, and Xiaodan Zhu.\nImproving text-to-sql with schema dependency learning. In arXiv:2103.04399, 2021.\n[18] Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang Li, Bowen Li, Jian Sun,\nand Yongbin Li. S2SQL: Injecting syntax to question-schema interaction graph encoder for\ntext-to-SQL parsers. In Findings of the Association for Computational Linguistics: ACL 2022,\npages 1254\u20131262, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.findings-acl.99.\n[19] Ihab F. Ilyas and Xu Chu. Trends in cleaning relational data: Consistency and deduplication.\nFoundations and Trends in Databases, 5:281\u2013393, 2015.\n[20] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer.\nLearning a neural semantic parser from user feedback. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n963\u2013973, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1089.\n12\n[21] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. In Advances in Neural Information Processing\nSystems, volume 35, pages 22199\u201322213, 2022.\n[22] Jan Kossmann, Stefan Halfpap, Marcel Jankrift, and Rainer Schlosser. Magic mirror in my\nhand, which is the best in the land? an experimental evaluation of index selection algorithms.\nProceedings of the VLDB Endowment, 13(12):2382\u20132395, 2020.\n[23] Prerna S. Kulkarni and Jagdish W. Bakal. Survey on data cleaning. In 2014 International\nConference on Advances in Computing, Communications and Informatics (ICACCI), page\n2361\u20132366, 2014.\n[24] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. KaggleDBQA: Realistic\nevaluation of text-to-SQL parsers. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2261\u20132273, Online, August 2021. Association for\nComputational Linguistics.\n[25] Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang,\nMinjoon Seo, Jong-Yeup Kim, and Edward Choi. Ehrsql: A practical text-to-sql benchmark\nfor electronic health records. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages\n15589\u201315601. Curran Associates, Inc., 2022.\n[26] Dandan Li, Lu Han, and Yi Ding. Sql query optimization methods of relational database system.\nIn 2010 Second International Conference on Computer Engineering and Applications, volume 1,\npages 557\u2013560. IEEE, 2010.\n[27] Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang,\nWenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-\naware layers for text-to-sql parsing. ArXiv, abs/2301.07507, 2023.\n[28] Tanzim Mahmud, KM Azharul Hasan, Mahtab Ahmed, and Thwoi Hla Ching Chak. A rule\nbased approach for nlp based query processing. In 2015 2nd International Conference on\nElectrical Information and Communication Technologies (EICT), pages 78\u201382. IEEE, 2015.\n[29] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. ArXiv,\nabs/2302.07842, 2023.\n[30] Vamsi Krishna Myalapalli and ASN Chakravarthy. Revamping sql queries for cost based\noptimization. In 2016 International Conference on Circuits, Controls, Communications and\nComputing (I4C), pages 1\u20136. IEEE, 2016.\n[31] Paulo H. Oliveira, Daniel dos Santos Kaster, Caetano Traina, and Ihab F. Ilyas. Batchwise\nprobabilistic incremental data cleaning. ArXiv, abs/2011.04730, 2020.\n[32] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[33] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis\nChristiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with\nhuman feedback. ArXiv, abs/2203.02155, 2022.\n[34] Hamid Pirahesh, Joseph M. Hellerstein, and Waqar Hasan. Extensible/rule based query rewrite\noptimization in starburst. In Proceedings of the ACM SIGMOD International Conference on\nManagement of Data, pages 39\u201348, 1992.\n[35] Mohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of\ntext-to-sql with self-correction. CoRR, abs/2304.11015, 2023. doi: 10.48550/arXiv.2304.11015.\n13\n[36] Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang,\nQuanshi Zhang, and Zhouhan Lin. RASAT: Integrating relational structures into pretrained\nSeq2Seq model for text-to-SQL. In Proceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3215\u20133229, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[37] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng,\nRongyu Cao, Jian Sun, Luo Si, Fei Huang, and Yongbin Li. A survey on text-to-sql parsing:\nConcepts, methods, and future directions. In arXiv:2208.13629, 2022.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.\n[39] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities\nof large language models. ArXiv, abs/2204.00498, 2022.\n[40] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language models. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing, pages 9895\u20139901, Online and\nPunta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n[41] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional\ngeneralization and natural language variation: Can a semantic parsing approach handle both?\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 922\u2013938, Online, August 2021. Association for Computational Linguistics.\n[42] Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi,\nand Jonathan Berant. Commonsenseqa 2.0: Exposing the limits of ai through gamification. In\nJ. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks, volume 1. Curran, 2021.\n[43] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-\nSQL: Relation-aware schema encoding and linking for text-to-SQL parsers. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pages 7567\u20137578,\nOnline, July 2020. Association for Computational Linguistics.\n[44] Lihan Wang, Bowen Qin, Binyuan Hui, Bowen Li, Min Yang, Bailin Wang, Binhua Li, Jian\nSun, Fei Huang, Luo Si, and Yongbin Li. Proton: Probing schema linking information from\npre-trained language models for text-to-sql parsing. In Aidong Zhang and Huzefa Rangwala,\neditors, KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining, Washington, DC, USA, August 14 - 18, 2022, pages 1889\u20131898. ACM, 2022. doi:\n10.1145/3534678.3539305.\n[45] Lijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, Hua Wu, Min Zhang, and Haifeng\nWang. DuSQL: A large-scale and pragmatic Chinese text-to-SQL dataset. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n6923\u20136935, Online, November 2020. Association for Computational Linguistics.\n[46] Ping Wang, Tian Shi, and Chandan K. Reddy. Text-to-sql generation for question answering on\nelectronic medical records. Proceedings of The Web Conference 2020, 2020.\n[47] Zhaoguo Wang, Zhou Zhou, Yicun Yang, Haoran Ding, Gansen Hu, Ding Ding, Chuzhe Tang,\nHaibo Chen, and Jinyang Li. Wetune: Automatic discovery and verification of query rewrite\nrules. In Proceedings of the 2022 International Conference on Management of Data, pages\n94\u2013107, 2022.\n[48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc\nLe, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\nArXiv, abs/2201.11903, 2022.\n14\n[49] Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga,\nChien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang,\nChengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng\nKong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. UnifiedSKG: Unifying and\nmulti-tasking structured knowledge grounding with text-to-text language models. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural Language Processing, pages 602\u2013631,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[50] Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural\nlanguage without reinforcement learning. ArXiv preprint, 2017.\n[51] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis\nfrom natural language. Proceedings of the ACM on Programming Languages, 1(OOPSLA):\n1\u201326, 2017.\n[52] Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir Radev. TypeSQL: Knowledge-based\ntype-aware neural text-to-SQL generation. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), page 588\u2013594, 2018.\n[53] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npage 3911\u20133921, 2018.\n[54] Tao Yu, Rui Zhang, Alex Polozov, Christopher Meek, and Ahmed Hassan Awadallah. Score:\nPre-training for context representation in conversational semantic parsing. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net, 2021.\n[55] John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic\nprogramming. In Proceedings of the Fourteenth National Conference on Artificial Intelligence\nand Ninth Conference on Innovative Applications of Artificial Intelligence, pages 1050\u20131055,\n1996.\n[56] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n2022.\n[57] Chen Zhao, Yu Su, Adam Pauls, and Emmanouil Antonios Platanios. Bridging the generalization\ngap in text-to-SQL parsing with schema expansion. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 5568\u20135578,\nDublin, Ireland, May 2022. Association for Computational Linguistics.\n[58] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating structured queries\nfrom natural language using reinforcement learning. In CoRR abs/1709.00103, 2017.\n[59] Victor Zhong, Mike Lewis, Sida I. Wang, and Luke Zettlemoyer. Grounded adaptation for\nzero-shot executable semantic parsing. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang\nLiu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20, 2020, pages 6869\u20136882. Association for\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.558.\n[60] Rong Zhou. Research on key performance index prediction of distributed database based on\nmachine learning algorithm. In Proceedings of the 2nd International Conference on Cognitive\nBased Information Processing and Applications (CIPA 2022) Volume 2, pages 563\u2013567. Springer,\n2023.\n[61] Xuanhe Zhou, Guoliang Li, Chengliang Chai, and Jianhua Feng. A learned query rewrite system\nusing monte carlo tree search. Proceedings of the VLDB Endowment, 15(1):46\u201358, 2021. doi:\n10.14778/3485450.3485456.\n15\n[62] Xuanhe Zhou, Chengliang Chai, Guoliang Li, and Ji Sun. Database meets artificial intelligence:\nA survey. IEEE Transactions on Knowledge and Data Engineering, 34(3):1096\u20131116, 2022.\ndoi: 10.1109/TKDE.2020.2994641.\n16\nA\nDatasheet for Datasets\nWe follow instructions provided by Datasheet for Datasets to answer the important ques-\ntions considering this dataset.\nA.1\nMotivation\nFor what purpose was the dataset created?\nThe advancement of Large Language Models (LLMs)\nhas raised concerns regarding whether state-of-the-art LLMs, such as ChatGPT and Codex, can replace\nhuman effort in real-world text-to-SQL tasks involving large database values. That is because their\nexceptional performance on previous academic tasks like SPIDER impresses researchers. However, we\nobserve that current cross-domain text-to-SQL benchmarks only focus on the database schema, which\nlacks full attention to values, resulting in a gap between academic and real-world applications. To\naddress this issue, we introduce BIRD, the largest cross-domain text-to-SQL benchmark highlighting\nextensive and realistic databases for community development. Additionally, we hope to observe\nthe performance gap between LLMs and humans. Our experimental results indicate that, as of now,\nLLMs are still unable to replace human effort. As far as we know, BIRD is the first text-to-SQL\nbenchmark to collect human performance.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)?\nPlease refer to the author list for details. Our research team\ninvolves Star Lab at The University of Hong Kong, Alibaba DAMO Academy Conversational AI\n(ConAI) Team, the Department of Computer Science at the University of Illinois Urbana-Champaign,\nthe Department of EECS at Massachusetts Institute of Technology, the School of Data Science at The\nChinese University of Hong Kong (Shenzhen), and Database Group of Tsinghua University.\nWho funded the creation of the dataset?\nThis dataset is fully funded by the Alibaba DAMO\nAcademy ConAI team. We spent 97,654 USD for presenting this data. The budget includes 10% for\nrecruiting competent research interns, 80% for developing the benchmark, and 10% for refining and\nimplementing the benchmark.\nA.2\nComposition\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)?\nBIRD contains natural language questions, external knowledge evidence sentences,\nprocessed large databases, database description files (csv), and SQL queries.\nHow many instances are there in total (of each type, if appropriate)?\nBIRD contains 12,751\nnatural language questions, 12,751 external knowledge evidence sentences, 95 processed large\ndatabases, 95 folders of database description CSV files, and 12,751 ground truth SQL queries.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set?\nIn BIRD, we divide it into three sets: training, development, and\ntesting. Training and development sets are public while testing data set is hidden for the fair evaluation\nof all text-to-SQL challengers. This could witness the real development of text-to-SQLs in the LLM\nera.\nIs there a label or target associated with each instance?\nIn BIRD, we provide two labels for each\nquestion instance: SQLs (the target of input) and external knowledge evidence (expert annotated\nevidence for each expected SQL).\nIs any information missing from individual instances?\nNo.\nAre relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social\nnetwork links)?\nNo.\nAre there recommended data splits?\nOur data consists of 9,428 instances for the training set,\n1,534 instances for the development set, and 1,789 instances for the concealed test set. The training\n17\nand development sets are derived from public databases, while the test set databases are curated and\ndesigned by our specialized team. We do this because some researchers express concerns that the\nremarkable performance of LLMs in text-to-SQL tasks may not be attributed to an improvement in\ncapabilities, but rather to the exposure of data and database values to the LLMs during the pre-training\nphase. To address these concerns, we opt to self-design new databases in testing using actual tabular\ndata, thereby ensuring that LLMs do not preview the databases.\nAre there any errors, sources of noise, or redundancies in the dataset?\nAs stated in the main\ncontent, our double-blind annotation procedure is both expensive and rigorous, ensuring data quality.\nHowever, it is virtually impossible for any dataset, especially complex ones, to be entirely free\nof errors. Our team is committed to enhancing the data even after this paper is accepted, thereby\ncontributing to the text-to-SQL community. In addition, we encourage users to provide feedback and\nreport errors on our data website, allowing us to rectify and enhance the dataset.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)?\nYes, all databases in training and development are collected\nunder appropriate licenses. Please see Section 3.2 for more details\nDoes the dataset contain data that might be considered confidential (e.g., data that is pro-\ntected by legal privilege or by doctor-patient confidentiality, data that includes the content of\nindividuals\u2019 non-public communications)?\nNo.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety?\nNo.\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\nSome questions mention\nages and genders, but they are just used to detect the capability of models on text-to-SQLs. No bias\nor other opinions are involved.\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or\nindirectly (i.e., in combination with other data) from the dataset?\nNo. All databases are\ncollected from open-sourced platforms, and any sensitive data has already been processed before.\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that\nreveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union\nmemberships, or locations; financial or health data; biometric or genetic data; forms of\ngovernment identification, such as social security numbers; criminal history)?\nNo, this is a\nQA-based text-to-SQL dataset, we don\u2019t require models to deliver any opinions on results. And also\nwe don\u2019t present any bias or opinions in the dataset.\nA.3\nCollection Process\nHow was the data associated with each instance acquired?\nSection 3 and Appendix B.2 introduce\nthis in detail.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or\nsensors, manual human curation, software programs, software APIs)?\nSection 3 and Appendix\nB.2 introduce this in detail. Our crowdworkers use Alibaba internal labeling software to annotate the\ndata and examine the results.\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with specific sampling probabilities)?\nNo.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and\nhow were they compensated (e.g., how much were crowdworkers paid)?\nFour PhD students\nand two MS students are involved in the creation of database description files. Two independent\nteams of crowdworkers are recruited to annotate questions and SQLs. The question annotators are\ncomposed of 11 English native speakers and SQL annotators are comprised of database engineers\nand DB students. The total consumption is 97,654 USD.\n18\nOver what timeframe was the data collected?\nFrom Sep. 2022 to Mar. 2023.\nWere any ethical review processes conducted (e.g., by an institutional review board)?\nYes, we\ntake such issues very seriously. During the review process, we found that certain questions related to\npolitics or inappropriate language. We have addressed these concerns by modifying the content and\nproviding a serious warning to the annotators responsible for such instances.\nDid you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)?\nSection 3 and Appendix B.2 introduce this in detail.\nWere the individuals in question notified about the data collection?\nYes.\nDid the individuals in question consent to the collection and use of their data?\nSure, we\nrecruited them and paid them satisfying salaries.\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses?\nNo.\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a\ndata protection impact analysis) been conducted?\nYes, we did a very comprehensive analysis\nincluding error analysis, and efficiency analysis, in the experiments of the paper and Appendix.\nA.4\nPreprocessing/cleaning/labeling\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,\ntokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)?\nYes, we provide the token list for each question and SQLs from NLTK for\nusers.\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support\nunanticipated future uses)?\nNo. Is the software that was used to preprocess/clean/label the data\navailable? Yes, https://www.nltk.org/\nA.5\nUses\nHas the dataset been used for any tasks already?\nNo.\nIs there a repository that links to any or all papers or systems that use the dataset?\nNo.\nWhat (other) tasks could the dataset be used for?\nSure, our databases and analysis-style questions\nare most valuable, so they could be beneficial to DB-based code generation, data science analysis,\netc.\nIs there anything about the composition of the dataset or the way it was collected and prepro-\ncessed/cleaned/labeled that might impact future uses?\nNo.\nAre there tasks for which the dataset should not be used?\nNo.\nA.6\nDistribution\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created?\nNo.\nHow will the dataset will be distributed (e.g., tarball on the website, API, GitHub)?\nAll\nsource codings and datasets could be found on our leaderboard website: https://bird-bench.\ngithub.io/. And we provide fast download links for the convenience of researchers who want\nto use our big data. Furthermore, the code repository can be found in https://github.com/\nAlibabaResearch/DAMO-ConvAI/tree/main/bird\n19\nWhen will the dataset be distributed?\nNow.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)?\nGiven the database size of BIRD is the largest until\nnow, we are afraid that abusing ample database values may lead to inappropriate commercial use.\nTherefore, we claim that this dataset should be distributed under CC BY-NC 4.0.\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances?\nNo.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?\nNo.\nA.7\nMaintenance\nWho will be supporting/hosting/maintaining the dataset?\nHKU STAR LAB and Alibaba DAMO\nAcademy\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nContact\nbird.bench23@gmail.com or the corresponding authors or co-first authors in the author list.\nIs there an erratum?\nNo.\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\nYes, we will keep polishing and optimizing our data periodically.\nIf the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (e.g., was the individuals in question were told that their data would be\nretained for a fixed period of time and then deleted)?\nNo.\nWill older versions of the dataset continue to be supported/hosted/maintained?\nNo. The most\nupdated version will be more reliable.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so?\nYes, but they should contact the authors first.\n20\nB\nAppendix\nB.1\nText-to-SQL Difficulty\nIn order to help researchers deeply analyze model performance in various text-to-SQL case levels,\nwe class all examples as simple (30%), moderate (60%), and challenging (10%). Previous\nwork, such as SPIDER, computed difficulty mainly based on SQL complexity. However, we find\nthat additional factors, such as question comprehension, schema linking, and external knowledge\nreasoning, also influence model and human performance. Therefore, each SQL annotator is required\nto evaluate examples based on these factors, and experts conclude the ratings to divide examples\ninto the three aforementioned difficulty levels. This approach offers a more extensive difficulty\nanalysis for text-to-SQL tasks. And the performance of ChatGPT on three different difficulty levels\nis shown in Table B.1. we take the approach of human scoring under established rules. A detailed\ncrowdsourcing rule is employed to rate the difficulty when SQL annotators generate SQLs for each\nquestion. The process consists of evaluating four dimensions: The process consists of evaluating four\ndimensions:\n1. Question Understanding: On a discrete scale from 1 to 3, annotators assess the ambiguity\nand difficulty of comprehending the question\u2019s intent, with 1 being straightforward, 2 being\nclear but requiring more thought, and 3 being extremely ambiguous.\n2. Knowledge Reasoning: On a discrete scale from 1 to 3, annotators rate the amount of\nexternal knowledge required to map the question to SQL, with 1 indicating no knowledge is\nrequired, 2 requiring evidence of external knowledge for generating SQLs that is easy to\nunderstand, and 3 requiring extensive knowledge and much more thoughts.\n3. Data Complexity: Annotators rate the complexity of schema relations and data size that\nneed analyzing on a discrete scale of 1-3, with 1 being a simple schema and data, 2 being\ncomplex schema and values understandable through database description files, and 3 being\nhighly complex and difficult to comprehend values and schema even with description files.\n4. SQL Complexity: Annotators rate the syntactic complexity of the target SQL query on a\ndiscrete scale of 1-3, with 1 being a simple SQL without many keywords, 2 being more\ncomplicated than 1, and 3 being a highly complex SQL with many functions and\nEach dimension is considered equally important for text-to-SQL annotations. SQLs are ranked based\non these scores, and we present simple, moderate, and challenging difficulties at proportions of 30%,\n60%, and 10%, respectively.\nMODEL\nDEV SET\nTEST SET\nsimple\nmoderate\nchallenging\ntotal\nsimple\nmoderate\nchallenging\ntotal\n(EX) ChatGPT\n31.08\n13.29\n12.08\n24.05\n35.41\n19.46\n12.28\n26.77\n(EX) ChatGPT + KG\n45.44\n26.14\n19.01\n37.22\n49.21\n31.89\n20.70\n39.30\n(VES) ChatGPT\n36.20\n15.43\n14.42\n27.97\n50.09\n24.71\n15.39\n36.68\n(VES) ChatGPT + KG\n54.71\n28.16\n22.80\n43.81\n65.06\n41.21\n25.81\n51.40\nTable 4: The Execution Accuracy (EX) and Valid Efficiency Score (VES) are presented for both the\nChatGPT model and its version with grounding (KG) for external knowledge evidence, taking into\nconsideration development and testing datasets.\nB.2\nAnnotation Entrance\nAnnotation Platform and Compensation.\nThe data is collected from Alibaba-Appen\u00a7, an internal\nversion. Each Question annotator receives a $0.6 reward for each validated question, while SQL\nannotators earn $1 per SQL contribution. We also invite text-to-SQL experts and professors to join\nto check and annotate external knowledge evidence without compensation. There are ~1340 SQLs\nconfirmed per week.\n\u00a7https://appen.com/crowd-2/#crowd\n21\nText-to-SQL Experts.\nThe three full-time text-to-SQL experts in this project are: (1). A database\nresearch scientist who\u2019s published over 20 top DB conference papers (e.g., SIGMOD, VLDB). (2). A\nPhD student with research interests in text-to-SQL, who achieved state-of-the-art results on text-to-\nSQL open challenges. (3). A DBA engineer with more than 10 years of experience in text-to-SQL\napplications for both B2B and B2C businesses.\nQuestion Annotation Entrance.\nWe hire a group of native speakers of English with degrees\nabove the bachelor\u2019s level and database-related knowledge to ask a variety of natural language\nquestions regarding the values of databases. To fulfill this objective, we have adopted the following\nprocedure: (1). ER diagrams and database description files are documented to assist the annotators\nin understanding the databases; (2). we present the annotators with three databases from different\ndomains and require them to generate 10 questions for each database; (3). these questions are then\nassessed by 3 text-to-SQL experts applying predefined rules. Those questions earning at least two\nvotes are marked as valid. Only annotators capable of generating no less than 8 valid questions per\ndatabase are preserved. As a result, 11 native speakers contribute questions to BIRD.\nSQL Annotation Entrance.\nWith the purpose of enhancing the quality of our SQL queries, we\nassemble a team of skilled data engineers and database students. The team undergoes rigorous testing\nthrough the text-to-SQL evaluation process, which assesses their capability of generating SQL queries\nfor a variety of questions facing different domains of databases. Each annotator is asked to answer 10\nquestions, and only those who score at least 9 out of 10 will be qualified to annotate SQL queries for\nBIRD.\nB.3\nQuestion Distribution\nFigure 8 contains the detailed question types and their examples.\nB.4\nExperiment Details\nFT-based Models.\nT5 is a strong and versatile pre-trained language model (PLM) for text-\nto-text generation that has achieved state-of-the-art performance in a variety of semantic parsing\ntasks, including text-to-SQL. We concatenate the question with serialized database schema as input\n[40, 49, 41]. And SQL can be fetched in an end-to-end fashion by easily fine-tuning. While seq2AST-\nbased methods [43, 5] are also effective in text-to-SQL, actually their grammar rules utilized during\ndecoding are constrained on specific datasets [25]. We implement our codes mainly based on the\nhugging-face transformers library \u00b6. We set the max input length as 1024, the generation max length\nas 512, and the batch size as 32. We also adopt Adafactor as our primary optimizer with a linear\ndecayed learning rate of 5e-5. All experiments are conducted on one NVIDIA Tesla A100 80GB,\nwhich is available for most research centers. We set the random seed as 1 for all runs of FT-based\nmodels since 1 is an optimal seed proven by previous SOTA models [27, 49].\nICL-based Models.\nCodex (code-davinci-002) and ChatGPT (gpt-3.5-turbo) are pop-\nular and powerful large-scale pre-trained language models (LLMs) for code generation driven by\nICL. They can produce multiple types of codes, including SQL, from human instructions without\nadditional training. We employ programming-based prompts, as described in [39], to collect results\nby calling the API. Also, we choose the Azure OpenAI API to align the codes with other variants\nof LLMs. Given that models are not allowed access to unseen databases and ground-truth SQLs in\nthe evaluation set, a zero-shot generation strategy is the most appropriate. Moreover, to investigate\nthe impact of multi-step reasoning of LLMs on BIRD, we implement the Chain-Of-Thought (COT)\ntechnique [48] by easily adding the prompt sentence \"Let\u2019s think step by step.\" before\nthe generation of SQLs [21]. However, we find out the output of ChatGPT is too uncertain with\nmany unexpected explanations, thus we provide a 1-shot pseudo example for ChatGPT to learn the\nprocedure of thinking and output format. The detailed prompt design is shown in Figure 9. In order\nto minimize the randomness of results, we set the temperature as 0 to ensure reproduction.\nKnowledge Fusion.\nIn the baseline implementation, we naively concatenate the knowledge evi-\ndence sentences with questions and database schemas, but we can observe a significant improvement\n\u00b6https://huggingface.co/\n22\nQuestion Type\nQuestion / SQL\nPercentage\nFundamental\nRanking\nHow many gas stations in CZE has Premium gas?\nSELECT COUNT(GasStationID) FROM gasstations \nWHERE Country = 'CZE' AND Segment = 'Premium'\nSub Type\nType\nMatch-based\nWhat are the titles of the top 5 posts with the highest \npopularity?\nSELECT Title FROM posts ORDER BY ViewCount DESC \nLIMIT 5\nComparison\nHow many color cards with no borders have been ranked \nhigher than 12000 on EDHRec?\nSELECT COUNT(id) FROM cards WHERE edhrecRank \n> 12000 AND borderColor = 'borderless'\nCounting\nHow many of the members' hometowns are from \nMaryland state?\nSELECT COUNT(T2.member_id) FROM zip_code AS T1 \nINNER JOIN member AS T2 ON T1.zip_code = T2.zip \nWHERE T1.state = 'Maryland'\nAggregation\nName the ID and age of patient with two or more \nlaboratory examinations which show their hematoclit level \nexceeded the normal range.\nWhat is the average height of the superheroes from \nMarvel Comics?\nSELECT AVG(T1.height_cm) FROM superhero AS T1 \nINNER JOIN publisher AS T2 ON T1.publisher_id = \nT2.id WHERE T2.publisher_name = 'Marvel Comics'\nReasoning\nDomain \nKnowledge\nSELECT T1.ID, STRFTIME('%Y', CURRENT_TIMESTAMP) \n- STRFTIME('%Y', T1.Birthday) FROM Patient AS \nT1 INNER JOIN Laboratory AS T2 ON T1.ID = \nT 2 . I D  W H E R E  T 1 . I D  I N  (  S E L E C T  I D  F R O M \nLaboratory WHERE HCT > 52 GROUP BY ID HAVING \nCOUNT(ID) >= 2 )\nNumeric \nComputation\nAmong the posts with a score of over 20, what is the \npercentage of them being owned by an elder user?\nSELECT CAST(SUM(IIF(T2.Age > 65, 1, 0)) AS REAL) \n* 100 / count(T1.Id) FROM posts AS T1 INNER \nJOIN users AS T2 ON T1.OwnerUserId = T2.Id \nWHERE T1.Score > 20\nSynonym\nHow many clients opened their accounts in Jesenik branch \nwere women ? (female)\nSELECT COUNT(T1.client_id) FROM client AS T1 \nINNER JOIN district AS T2 ON T1.district_id \n= T2.district_id WHERE T1.gender = 'F' AND \nT2.A2 = 'Jesenik'\nValue \nIllustration\nAmong the weekly issuance accounts, how many have a \nloan of under 200000?\nSELECT COUNT(T1.account_id) FROM loan AS T1 \nINNER JOIN account AS T2 ON T1.account_id = \nT2.account_id WHERE T2.frequency = 'POPLATEK \nTYDNE' AND T1.amount < 200000\n83.9 %\n20.3 %\n16.7 %\n30.4 %\n15.7 %\n23.6 %\n24.5 %\n7.2 %\n70.1 %\nType\nFigure 8: Questions in the BIRD contain two main categories. The Fundamental Type of\nquestions are comparable to other text-to-SQL benchmarks. The Reasoning Type of questions\nrequires external knowledge grounding to answer.\nby this easy method. A more complicated and effective strategy of knowledge grounding for ChatGPT\nand T5 would be an important future topic. The knowledge evidence sentences are concluded to the\nexternal knowledge provided by annotators as described in Section 3.3.\n23\nICL Prompt Input\nCREATE TABLE singer\n(\n    singer_id         TEXT not null\n        primary key,\n    nation       TEXT  not null,\n    sname       TEXT null,\n    dname       TEXT null,\n    cname       TEXT null,\n    age    INTEGER         not null,\n    year  INTEGER          not null,\n    birth_year  INTEGER          null,\n    salary  REAL          null,\n    city TEXT          null,\n    phone_number   INTEGER          null,\n    tax   REAL      null,\n)\n-- External Knowledge: age = year - birth_year;\n-- Using valid SQLite and understading External \nKnowledge, answer the following questions for \nthe tables provided above.\nThe final SQL is: Let's think step by step.\n1. referring to external knowledge, we need \nto filter singers 'by year' - 'birth_year' > 27; 2. \nwe should find out the singers of step 1 in \nwhich nation = 'US', 3. use COUNT() to count \nhow many singers. Finally the SQL is: SELECT \nCOUNT(*) FROM singer WHERE year - \nbirth_year > 27;</s>\n1-shot pseudo example\nCREATE TABLE customers\n(\n    CustomerID INTEGER UNIQUE     not null\n        primary key,\n    Segment    TEXT null,\n    Currency   TEXT null\n)\nCREATE TABLE gasstations\n(\n    GasStationID INTEGER    UNIQUE   not null\n        primary key,\n    ChainID      INTEGER          null,\n    Country      TEXT null,\n    Segment      TEXT null\n)\nCREATE TABLE products\n(\nCREATE TABLE customers\n(\n    CustomerID INTEGER UNIQUE     not null\n        primary key,\n    Segment    TEXT null,\n    Currency   TEXT null\n)\nCREATE TABLE gasstations\n(\n    GasStationID INTEGER    UNIQUE   not null\n        primary key,\n    ChainID      INTEGER          null,\n    Country      TEXT null,\n    Segment      TEXT null\n)\nCREATE TABLE yearmonth\n(\n    CustomerID  INTEGER            not null,\n    Date        INTEGER            not null,\n    Consumption REAL null,\n    primary key (CustomerID, Date),\n    foreign key (CustomerID) references \n    customers (CustomerID)\n)\n\u2026\n\u2026\nCreate DDL Prompt \n-- External Knowledge: August of 2012 means Date contains \n'201208' in the yearmonth.date of the database; Price per unit \nof product = T1.Price / T1.Amount\n-- Using valid SQLite and understading External Knowledge, \nanswer the following questions for the tables provided above.\n-- What is the highest eligible free rate for K-12 students in the \nschools in Alameda County?\n-- External Knowledge: August of 2012 means \nDate contains '201208' in the yearmonth.date \nof the database; Price per unit of product = \nPrice / Amount\n-- Using valid SQLite and understading External \nKnowledge, answer the following questions for \nthe tables provided above.\n-- For all the people who paid more than 29.00\nper unit of product id No.5. Give their\nconsumption status in the August of 2012.\nThe final SQL is: Let's think step by step.\\n1. referring to external knowledge, we need to \ncalculate the price per unit of product id No.5, which is T1.Price / T1.Amount; 2. we should \nfilter the people who paid more than 29.00 per unit of product id No.5; 3. we should find out \nthe consumption status of the people in August of 2012. Finally the SQL is: SELECT \nyearmonth.Consumption FROM transactions_1k AS T1 JOIN yearmonth ON T1.CustomerID = \nyearmonth.CustomerID AND T1.Date = yearmonth.Date WHERE T1.ProductID = 5 AND T1.Price \nThe final SQL is: Let's think step by step.\n1. referring to external knowledge, we need \nto calculate the price per unit of product id \nNo.5, which is Price / Amount; 2. we should \nfilter the people who paid more than 29.00 \nper unit of product id No.5; 3. we should find \nout the consumption status of the people in \nAugust of 2012.\nSELECT yearmonth.Consumption FROM transactions_1k JOIN yearmonth ON transactions_1k.CustomerID = yearmonth.CustomerID \nAND transactions_1k.Date = yearmonth.Date WHERE transactions_1k.ProductID = 5 AND transactions_1k.Price / \ntransactions_1k.Amount > 29.00 AND yearmonth.Date = \u2018201208\u2019;\nKnowledge Prompt \nTask Instruction\n1-shot COT Prompt\nChatGPT COT Steps\nQuestion Prompt \nSQL Output\nFinally the SQL is: \nFigure 9: The detailed prompt design for implementation of ChatGPT + KG + COT.\nB.5\nEfficiency Analysis Details\nTwo strategies for performing text-to-efficient-SQL are presented in Figure 10. Examples show\nthat both two-stage optimization and embodied databases can help semantic parsings generate more\nefficient SQLs.\nB.6\nError Analysis Details\nFigure 11 presents a detailed analysis of errors made by ChatGPT.\nB.7\nEvaluation Details\nDuring double-blind annotation in BIRD, we encountered numerous ambiguous issues that led\nto mismatches, predominantly due to unclear user intents. The most serious ambiguity is the\nuse of \"DISTINCT\". Some annotators believe it should present only unique values, such as\nnames, and cities, while others argue that it should be used only when questions explicitly mention\n\"different\" or \"distinctive\". Therefore, we use HashSet rather than List to compare final\nresults since HashSet disregards row order and automatically filters repetitive rows to reduce this am-\nbiguity. However, this may result in false positives for questions utilizing \"ORDER BY.\" We identify\nthree \"ORDER BY\" usage scenarios in BIRD: 1) Rank-based questions (e.g., \"Show me the\ntop 5 students according to their math scores\"): The order is less important\nas long as the results contain the correct students. 2) Superlative questions: (e.g., \"List the\nlongest river in the USA\"): The answer typically contains only one item (or tied results),\nso the impact is minimal. 3) Questions requiring a specific order (e.g., \"Show me the top\nfive students based on their math scores in descending order\"): This\nscenario explicitly requires correct ordering and may lead to false positives. However, such in-\nstances are uncommon, accounting for less than 1% of BIRD.\n24\nQuery Rewriting\nEx1.1 Question:\nList out the age of users who located in Vienna, Austria obtained the badge?\nChatGPT SQL:\nSELECT Age FROM users WHERE Location = 'Vienna, Austria\u2018 AND Id IN (SELECT UserId FROM \nbadges)\nOptimized SQL: (time-saving percentage: 99.92%)\nSELECT u.Age FROM users AS u INNER JOIN badges AS b ON u.Id = b.UserId WHERE u.Location\n= 'Vienna, Austria\u2019\nTake Away:\nBy applying a JOIN operation instead of a subquery with IN can improve efficiency, as the database may execute the JOIN and \nfiltering processes concurrently in just one operation without the need to store the intermediate results to filter primary query.\nEx1.2 Question:\nHow many of the members' hometowns are from Maryland state?\nChatGPT SQL:\nSELECT COUNT(*) FROM member INNER JOIN zip_code ON member.zip = zip_code.zip_code WHERE \nzip_code.state = 'Maryland\u2019\nOptimized SQL: (time-saving percentage: 67.93%)\nSELECT COUNT(member.member_id) FROM member INNER JOIN zip_code ON member.zip = \nzip_code.zip_code WHERE zip_code.state = 'Maryland\u2019\nTake Away:\nUtilizing the COUNT function on a NOT-NULL column, as opposed to COUNT(*), can increase time efficiency. This rewritten \nSQL enables the database to count NOT-NULL values within a single column, rather than compute all rows including those with \nNULL values. Usually, the primary key column is selected as this NOT-NULL column.\nEx1.3 Question:\nWho is the owner of the account with the largest loan amount?\nChatGPT SQL:\nSELECT c.client_id FROM client c INNER JOIN disp d ON c.client_id = d.client_id INNER \nJOIN loan l ON d.account_id = l.account_id ORDER BY l.amount DESC LIMIT 1\nOptimized SQL: (time-saving percentage: 62.39%)\nSELECT c.client_id FROM client c INNER JOIN disp d ON c.client_id = d.client_id\nINNER JOIN loan l ON d.account_id = l.account_id WHERE l.amount = ( SELECT MAX(amount) \nFROM loan)\nTake Away:\nIn an unindexed environment, employing the MAX function can potentially yield faster results since it avoids the need for sorting, \nwhich could run against a large table.\nAdding Indexes to Database\nEx2.1 Question:\nHow many accounts are there in the district of \\\"Pisek\\\"?\nChatGPT SQL:\nSELECT COUNT(*) FROM account a INNER JOIN district d ON a.district_id = d.district_id\nWHERE d.A2 = \u2018Pisek\u2019\nAdded Indexes: (time-saving percentage: 87.27%)\nCREATE INDEX account_district_id_index ON account(district_id);  \nCREATE UNIQUE INDEX district_district_id_uindex ON district(district_id); \nTake Away:\nAdding indexes into a database can significantly increase the speed of SQL queries because it creates a data structure that enables \nthe database engine to quickly locate rows that match specific criteria instead of scanning the entire table.\nFigure 10: Two possible solutions and explanations to improve efficiency are presented. The first\nbatch of examples shows how to optimize SQL efficiency by rewriting SQL based on rules. The\nlast example is to show that adding indexes to databases can also improve SQL efficiency without\nrewriting them.\nB.8\nVES Details\nRegarding E, in our experiment, we consider time as the main metric to represent efficiency, where\nE \u2208 (\u03f5, 30s). Here, \u03f5 is a small positive constant to prevent floating-point overflow. The single E is\nnot stable due to machine status. Lower E refers to faster execution speed, which is more efficient.\nConcerning R, it represents a normalized efficiency ratio between human-annotated SQL queries\nand predicted SQL queries to reduce the influence of machine status. The stability of this metric is\nensured by running this computation 100 times for each example, filtering outliers, and subsequently\n25\nWrong Schema Linking (41.6%)\nQuestion:\nList the names of schools with more than 30 differences\nin enrollments between K-12 and ages 5-17. Please also \ngive the full street address of the schools.\nEvidence:\nDifference in enrollment = `Enrollment (K-12)` -\n`Enrollment (Ages 5-17)`\nGround Truth:\nSELECT T1.School, T1.StreetAbr FROM \nschools AS T1 INNER JOIN frpm AS T2 ON \nT1.CDSCode = T2.CDSCode WHERE \nT2.`Enrollment (K-12)` -\nT2.`Enrollment (Ages 5-17)` > 30 \nChatGPT SQL:\nSELECT s.School, s.Street, s.City, \ns.Zip FROM schools s JOIN frpm f ON \ns.CDSCode = f.CDSCode WHERE \nf.`Enrollment (K-12)` - f.`Enrollment\n(Ages 5-17)` > 30 \nQuestion:\nList out the accounts who have the earliest trading \ndate in 1995.\nEvidence:\nFalse\nGround Truth:\nSELECT account_id FROM trans WHERE \nSTRFTIME('%Y', date) = '1995' ORDER \nBY date ASC LIMIT 1\nChatGPT SQL:\nSELECT account_id FROM account WHERE \ndate = (SELECT MIN(date) FROM \naccount WHERE date LIKE '1995%')\nQuestion:\nWhat is the average math score of the school with the \nlowest average score for all subjects, and in which county \nis it located?\nEvidence:\nAverage score for all subjects can be computed by \nAvgScrMath + AvgScrRead + AvgScrWrite\nGround Truth:\nSELECT T1.AvgScrMath, T2.County FROM \nsatscores AS T1 INNER JOIN schools AS \nT2 ON T1.cds = T2.CDSCode WHERE \nT1.AvgScrMath IS NOT NULL ORDER BY \nT1.AvgScrMath + T1.AvgScrRead + \nT1.AvgScrWrite ASC LIMIT 1\nChatGPT SQL:\nSELECT cname, AvgScrMath FROM satscores\nJOIN schools ON satscores.cds = \nschools.CDSCode WHERE satscores.rtype = \n\u2018S\u2019 ORDER BY (AvgScrMath + AvgScrRead + \nAvgScrWrite) ASC LIMIT 1\nMisunderstanding Database Content (40.8%)\nQuestion:\nWhat is the average lap time in milliseconds of all the lap \nrecords set on the various circuits in Italy?\nEvidence:\naverage = AVG(milliseconds)\nGround Truth:\nSELECT CAST(SUM(T1.milliseconds) AS \nREAL) / COUNT(T1.lap) FROM lapTimes AS \nT1 INNER JOIN races AS T2 on T1.raceId \n= T2.raceId INNER JOIN circuits AS T3 \non T2.circuitId = T3.circuitId WHERE \nT3.country = 'Italy\u2019\nChatGPT SQL:\nSELECT AVG(milliseconds) FROM \nlap_records WHERE circuit IN ('Monza', \n'Imola', 'Mugello', 'Misano')\nQuestion:\nOn average, how much did the Student_Club spend on \nfood for the typical event in the past?\nEvidence:\nDIVIDE(SUM(spent), COUNT(spent)) where category = \n'Food'; 'event in the past' means event_status = 'Closed\u2019\nGround Truth:\nSELECT SUM(spent) / COUNT(spent) FROM \nbudget WHERE category = 'Food' AND \nevent_status = 'Closed\u2019\nChatGPT SQL:\nSELECT DIVIDE(SUM(spent), COUNT(spent)) \nFROM budget WHERE category = 'Food' AND \nevent_status = 'Closed'\nMisunderstanding Knowledge Evidence (17.6%)\nQuestion :\nName elementary schools in Riverside which the average \nof average math score for SAT is grater than 400, what is \nthe funding type of these schools?\nEvidence:\nAverage of average math = sum(average math scores) / \ncount(schools)\nGround Truth:\nSELECT T1.sname, T2.`Charter Funding \nType` FROM satscores AS T1 INNER JOIN \nfrpm\nHAVING CAST(SUM(T1.AvgScrMath) \nAS REAL) / COUNT(T1.cds) > 400\nChatGPT SQL:\nSELECT s.School, s.FundingType FROM \nschools s    AND sc.rtype = \u2018E\u2019 AND \nsc.AvgScrMath > 400 GROUP BY s.School, \ns.FundingType\nQuestion:\nHow many meeting events were held in 2020?\nSyntax Error (3.0%)\nQuestion:\nHow much more was customer 7 consuming in April 2013 \nthan customer 5?\nEvidence:\nApril 2013 refers to \u2018201304\u2019 in the yearmonth.date\nGround Truth:\nSELECT SUM(IIF(CustomerID = 7, \nConsumption, 0)) - SUM(IIF(CustomerID = \n5, Consumption, 0)) FROM yearmonth\nWHERE Date = '201304\u2019\nChatGPT SQL:\nSELECT7, (SELECT Consumption FROM \nyearmonth WHERE CustomerID = 7 AND Date \n= 201304) - (SELECT Consumption FROM \nyearmonth WHERE CustomerID = 5 AND Date \n= 201304)\nEvidence:\nmeeting events refers to type = 'Meeting'; held in 2020 \nrefers to YEAR(event_date) = 2020\nGround Truth:\nSELECT COUNT(event_id) FROM event \nWHERE type = 'Meeting' AND \nSTRFTIME('%Y', COLUMN) = \u20182020\u2019\nChatGPT SQL:\nSELECT COUNT(*) FROM event WHERE type \n= 'Meeting' AND YEAR(event_date) = \n2020\n\u2026\n\u2026\nFigure 11: 4 major types of error cases are presented. Some cases are shortcuts for better presentation.\ncomputing the average. Considering the rapid advancement of technology, it is impractical to\nanticipate the fastest SQL performance. Currently, the range for the efficiency ratio, R, is defined as\nR \u2208 (0, +\u221e). E( \u02c6Yn) (efficiency of predicted SQL) is much lower than E( \u02c6Y ) (efficiency of ground\ntruth SQL according to EX), then the relative efficiency score R will be increased. In short, higher R\nrefers to higher efficiency.\nWhen measuring VES, we run 100 times for each SQL in the same CPU and evaluate average results\nafter dropping the outliers. The STD of VES on dev set and test set after 10 trials are 0.043 and 0.025\nrespectively. We detect outliers in the following procedures:\n1. Compute the mean and standard deviation of the dataset.\n2. Then calculate the lower threshold as mean\u22123\u00d7standard_deviation and the upper threshold\nas mean + 3 \u00d7 standard_deviation.\n3. Statistically, approximately 99.7% of the data points fall within 3 standard deviations of the\nmean.\nB.9\nHuman Performance Collection\nThe procedure of collecting human performance is still rigorous. During the annotation, all data\nis divided into 10 batches for better management and error tracks by experts. The first 8 batches\nof data are the final training data and dev data for public use, and the remaining 2 batches of data\nare used for testing. We consider the annotation of the first 8 batches of data as a learning process\nfor SQL annotators since their erroneous SQLs could be fixed by experts and learn how to generate\ngood-quality SQLs for this task. Then their first scores on an examination, conducted by testing\nset from the final two batches can be viewed as human performance since we don\u2019t interrupt and\nassist them during the examination and all errors are preserved. After testing, we proceed with the\nfollowing double-blind SQL annotation procedures as Section 3.4 to correct SQLs for these data by a\ndiscussion with experts. And SQLs after the second round of double-blind annotation are collected as\nground truth.\nB.10\nDistribution of Open-source Databases\nThe databases in BIRD are all in accordance with one of following licenses:\n26\nPublic Domain\nPublic Domain Mark\nA public domain license refers to a legal designation that allows intellectual property, such as creative\nworks or inventions, to be freely used, shared, and built upon by anyone without restrictions. When a\nwork is in the public domain, it is no longer protected by copyright, patent, or trademark laws.\nCC-BY\nCreative Commons Attribution 4.0 International\nThis license is one of the open Creative Commons licenses and allows users to share and adapt the\ndataset so long as they give credit to the creator.\nCC-BY-SA\nCreative Commons Attribution-ShareAlike 4.0 International\nThis license is one of the open Creative Commons licenses and allows users to share and adapt the\ndataset so long as they give credit to the creator and distribute any additions, transformations, or\nchanges to the dataset under this license.\nGPL\nGeneral Public License\nThe GPL was created by the Free Software Foundation (FSF) and is also known as the GNU GPL, as\nit is used by the GNU Project. And it allows users to use, study, share, and modify the software under\ncertain terms and conditions.\nCPOL\nCode Project Open License\nIt is a software license that is often used for articles, tutorials, and sample code shared on The Code\nProject website. The CPOL is intended to be a more permissive license, allowing developers to use,\nmodify, and distribute the software without many of the restrictions imposed by other licenses like\nthe GPL.\nCC0\nCreative Commons Zero\nIt is a public domain dedication tool created by Creative Commons. It allows creators to waive all\ntheir copyright and related rights in a work, effectively placing it in the public domain. This means\nthat anyone can freely use, share, modify, and build upon the work without seeking permission or\nproviding attribution to the original creator.\nB.11\nSQL Function Taxonomy\nSQL functions in BIRD mentioned in Table 1 span across multiple categories including:\n\u2022 Window Functions, i.e., OVER()\n\u2022 Date Functions, i.e., JULIANDAY()\n\u2022 Conversion Functions, i.e., CAST()\n\u2022 Math Functions, i.e., ROUND()\n\u2022 String Functions, i.e., SUBSTR()\nB.12\nKeyword Statistic\nWe have conducted a comprehensive analysis of the keywords employed in the BIRD dataset and\nvisualize the results in the form of a nice-looking word cloud, which can be found in Figure 12. We\nfurther classify keywords into 7 following categories:\nMain Body Keywords\n\u2022 SELECT \u2022 FROM \u2022 WHERE \u2022 AND \u2022 OR \u2022 NOT \u2022 IN \u2022 EXISTS \u2022 IS \u2022\nNULL \u2022 IIF \u2022 CASE \u2022 CASE WHEN.\nJoin Keywords\n\u2022 INNER JOIN \u2022 LEFT JOIN \u2022 ON \u2022 AS.\nClause Keywords\n\u2022 BETWEEN \u2022 LIKE \u2022 LIMIT \u2022 ORDER BY \u2022 ASC \u2022 DESC \u2022 GROUP BY \u2022\nHAVING \u2022 UNION \u2022 ALL \u2022 EXCEPT \u2022 PARTITION BY.\n27\nFigure 12: Keyword cloud presentation for SQLs in BIRD.\nAggregation Keywords\n\u2022 AVG \u2022 COUNT \u2022 MAX \u2022 MIN \u2022 ROUND \u2022 SUM.\nScalar Keywords\n\u2022 ABS \u2022 LENGTH \u2022 STRFTIME \u2022 JULIADAY \u2022 NOW \u2022 CAST \u2022 SUBSTR \u2022 INSTR.\nComparison Keywords\n\u2022 = \u2022 > \u2022 < \u2022 >= \u2022 <= \u2022 !=.\nComputing Keywords\n\u2022 - \u2022 + \u2022 * \u2022 /.\nB.13\nStudy about Text-to-SQL Models\nThe fundamental principle of a cross-domain text-to-SQL parser involves the construction of an\nencoder to learn representations of questions and schemas, followed by a decoder to generate SQLs\n[37]. For example, IRNET [12] designs an encoder consisting of attention-based Bi-LSTM for\nlearning question and schema representations, and a decoder to predict SQLs based on the encoded\nintermediate representations. RATSQL [43], SDSQL [17], LGESQL [5], and S2SQL [18], Proton\n[44] enhance the representation learning of natural language questions and database schema via\nrelational graph neural network. R2SQL [16], SCORE [54], and STAR [4] enhance contextual\nlearning for conversational text-to-SQL tasks. Later, sequence-to-sequence pre-trained language\nmodels (PLMs) such as T5 [38] become popular in text-to-SQL tasks due to their portability and\ncapability of generation across different datasets. These models achieve impressive results by fine-\ntuning with minimal effort. Furthermore, RASAT [36] enhances T5\u2019s structural information encoding\nvia schema alignment into the encoder, while Graphix [27] equips T5 with multi-hop reasoning to\nachieve state-of-the-art results on complicated cross-domain text-to-SQL tasks. In recent years, LLMs\nsuch as ChatGPT [33], Palm [8], OPT [56], have attracted considerable attention due to their powerful\nzero-shot reasoning and domain generalization capabilities. ChatGPT can perform exceptionally well\non semantic parsing tasks, including text-to-SQL tasks, with minimal input data. In fact, in the BIRD\nproject, ChatGPT even performs more impressively than initially expected.\nStudy about SQL Efficiency\nEfficient execution of SQL queries on big databases has been a\nsignificant topic in both academia and industries. Many techniques are proposed to improve SQL\nquery efficiency, by index selection [22], SQL optimization [26, 61], etc. SQL optimization is a\ncommon method for enhancing the efficiency of SQL queries. Several SQL optimization algorithms\n[28, 30, 47], such as rule-based optimization and cost-based optimization, are proven effective.\nRule-based optimization employs a set of principles to transform the SQL query into a form that can\nbe executed more efficiently. On the other hand, cost-based optimization estimates the execution cost\nof various query plans and selects the one with the lowest cost by analyzing the statistic distribution\nof database values. Similar to the NLP community, there are also recent works utilizing artificial\nintelligence for query optimization such as [61]. Index prediction is another important technique for\nimproving SQL execution efficiency. Researchers propose many algorithms of index prediction [60]\nbased on various optimization criteria, such as minimizing SQL execution time, and maximizing\nindex utilization. In this work, we provide VES to measure the efficiency of text-to-SQL generators\nto encourage them to generate accurate and fast SQLs for users.\n28\n"
  },
  {
    "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
    "link": "https://arxiv.org/pdf/2305.03726.pdf",
    "upvote": "5",
    "text": "Otter: A Multi-Modal Model with In-Context\nInstruction Tuning\nBo Li*\nYuanhan Zhang*\nLiangyu Chen*\nJinghao Wang*\nJingkang Yang\nZiwei LiuB\nS-Lab, Nanyang Technological University, Singapore\n{libo0013, yuanhan002, lchen025, c190209, jingkang001, ziwei.liu}@ntu.edu.sg\nhttps://github.com/Luodian/Otter\nAbstract\nLarge language models (LLMs) have demonstrated signi\ufb01cant universal capabilities\nas few/zero-shot learners in various tasks due to their pre-training on vast amounts\nof text data, as exempli\ufb01ed by GPT-3, which boosted to InstrctGPT and ChatGPT,\neffectively following natural language instructions to accomplish real-world tasks.\nIn this paper, we propose to introduce instruction tuning into multi-modal models,\nmotivated by the Flamingo model\u2019s upstream interleaved format pretraining dataset.\nWe adopt a similar approach to construct our MultI-Modal In-Context Instruction\nTuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based\non OpenFlamingo (open-sourced version of DeepMind\u2019s Flamingo), trained on\nMIMIC-IT and showcasing improved instruction-following ability and in-context\nlearning. We also optimize OpenFlamingo\u2019s implementation for researchers, de-\nmocratizing the required training resources from 1\u00d7 A100 GPU to 4\u00d7 RTX-3090\nGPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers\nfor more researchers to incorporate the models into their customized training and\ninference pipelines.\n1\nIntroduction & Motivation\nLarge language models (LLMs) have demonstrated signi\ufb01cant universal capabilities in performing\nvarious tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data\nand have been showcased in recent research, such as GPT-2 [25] and GPT-3 [6].\nRecent studies have highlighted the importance of instruction tuning in empowering LLMs, as\nexempli\ufb01ed by the boosting of GPT-3 [6] to InstrctGPT [22] and ChatGPT [20], which follows\nnatural language instructions effectively to accomplish real-world tasks and allows for customizing\ntask-speci\ufb01c rules into instructions during downstream \ufb01ne-tuning, enabling pre-trained models to\ncomprehend user intents more effectively and produce accurate and relevant responses.\nSimilar attempts have been introduced in multi-modal models as well. LLaMA-Adapter [38] aims\nto adapt LLaMA [33] into an instruction following model by adding additional adapter modules\nand multi-modal prompts. Mini-GPT4 [39] follows the architecture of BLIP-2 [15] but replaces\nthe language decoder with Vicuna [9], which supports longer answers. LLaVA [17] utilizes the\nsame CLIP [23] vision encoder and Vicuna [9] language decoder, and \ufb01netunes on their high-quality\ninstruction dataset, curated by GPT-4 [19].\nAlthough these works have achieved excellent results and provided valuable insights, they share a\nminor common issue. Speci\ufb01cally, they either \ufb01netune the entire model or the connection part on\n*Equal Contribution\nBCorresponding Author\nTechnical Report.\narXiv:2305.03726v1  [cs.CV]  5 May 2023\nMMC4\nbillion-scale corpus of \nimages interleaved with text \ud83e\udda9\n\ud83e\udda6\nMIMIC-IT\nmulti-modal instruction\ntuning datasets with\nin-context examples\nOtter\nOpenFlamingo\nFigure 1: Otter Overview. Otter is a multi-modal model \ufb01netuned on our proposed MIMIC-IT\ndataset, based on OpenFlamingo. Otter model exhibits the improved ability to execute tasks by\nfollowing given instructions and leveraging in-context examples.\ntask-speci\ufb01c data. For instance, a common practice is to use image-text data pairs from Caption [16]\nor VQA [11] tasks to align visual and language modules. While embedding visual information into\nthe language model in this way can be effective, we question whether this practice is inherently\ntask-dependent, as it relies on the task for which the data is used to train the alignment module.\nUpon re\ufb02ection, we have discovered that DeepMind Flamingo\u2019s [1] upstream pretraining dataset,\nMultiModal MassiveWeb (M3W), has signi\ufb01cant value in aligning visual and language information\nin a more natural manner. The dataset comprises HTML webpages, where all images and texts are\narranged in an interleaved format. Speci\ufb01cally, a piece of text may describe an image (or videos)\nabove or below it, and correlations may exist between images (or videos) and text in adjacent\npositions. This natural organization of context provides richer information than a caption dataset,\nwhere text only describes its corresponding image. Trained on this dataset, Flamingo achieves zero-\nand few-shot generalization and in-context learning ability, making it the GPT-3 moment in the\nmulti-modal domain.\nHowever, DeepMind has not released the Flamingo model and its M3W dataset to the public,\npotentially because of the model\u2019s exceptional performance that could cause astonishment prematurely.\nNevertheless, the LAION-AI\u2019s OpenFlamingo project [4] has recently been made public, providing\naccess to their corresponding MMC4 [40] dataset in the same interleaved format on a larger scale.\nConsequently, community researchers can continue to follow the Flamingo series of works for further\nresearch and development in the \ufb01eld of multi-modal models. Although the OpenFlamingo model\nexhibits impressive multi-modal in-context learning abilities and executes tasks with given in-context\nexamples, as an upstream pre-trained model, it still requires instruction tuning to perform downstream\ntasks more effectively.\nIn our paper, we propose our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset\nand provide details on its construction in Sec. 3.1. We then introduce Otter, a multi-modal model\nwith in-context instruction tuning based on OpenFlamingo. We illustrate the relationship between\nOtter and OpenFlamingo in Fig. 1. Finetuned on MIMIC-IT dataset, our Otter model demonstrates\nimproved instruction-following ability compared to OpenFlamingo, as shown in our qualitative\nanalysis in Sec. 4.1. Meanwhile, Otter is capable of learning to execute instructions with provided\nin-context learning examples, as shown in Sec. 4.2. From the engineering perspective, we optimized\nOpenFlamingo\u2019s implementation to make it more accessible to researchers. Our optimizations include\noptimizing the training requirements from at least 1\u00d7 A100 GPU to only 4\u00d7 RTX3090 GPUs and\nintegrating it into Hugging Face Transformers [34] to simplify training and inference with a few lines\nof code. Our contributions facilitate further research and development in the \ufb01eld of multi-modal\nmodels.\nWe summarize our key contributions as follows:\n\u2022 We present the MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. Each\ndata sample includes an instruction-image-answer triplet and its in-context examples.\n\u2022 We introduce Otter, a multi-modal model with in-context instruction tuning based on\nOpenFlamingo, capable of the instruction following and executing new instructions with\nfew in-context learning examples.\n\u2022 Our optimizations to OpenFlamingo\u2019s implementation, including reducing the training\nrequirements to 4\u00d7 RTX3090 GPUs and integrating it into Huggingface Transformers to\nsimplify the training and inference with only a few lines of code.\n2\n2\nRelated Work\n2.1\nLarge-scale Multi-modal Models\nWith the recent success of ChatGPT [20], GPT-4 [19], and other large language models [33, 32, 9],\nrecent studies start to explore incorporating information from other modalities based on pretrained\nlanguage models. These studies extend the capabilities of language models to more tasks and\nmodalities, and can be categorized into two perspectives:\nSystem Design Perspective. This perspective involves using ChatGPT [20] as a dispatch scheduler\nand connecting different expert models through it to allow for different visual tasks. Language\nprompts serve as an interface to call expert visual-language models within their respective task\ndomains. Works in this category include VisualChatGPT [35], HuggingGPT [29], Cola [8], X-\nGPT [42], MM-REACT [37], and ViperGPT [31]. This approach has limitations in that each model\ncannot be trained individually on new tasks, and using ChatGPT [20] as a powerful instruction\ndispatch tool can result in high API query costs.\nEnd-to-End Trainable Models Perspective. This perspective focuses on connecting models from\ndifferent modalities into integrated end-to-end trainable models, also known as multi-modal founda-\ntion models. Early works in this \ufb01eld include Flamingo [1], which proposes a uni\ufb01ed architecture\nfor modeling language and vision and was later open-sourced as OpenFlamingo [4] by LAION-\nAI. Other earlier works include BLIP-2 [15], which uses a lightweight Querying Transformer and\ntwo-stage bootstrap pretraining to connect information from the image to text modality. With the\npopularity of GPT-4 [19], there has been an increased focus on this \ufb01eld since 2023. Enterprise-\nlevel product models include OpenAI\u2019s yet-to-be-released vision-language version of GPT-4 [19],\nGoogle\u2019s PaLM-E [10], Baidu\u2019s ERNIE [5], Alibaba\u2019s Tongyi Qianwen [2], and Sensetime\u2019s SenseN-\nova [27]. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters [38],\nMini-GPT4 [39], and LLaVA [17]. LLaMA-Adapters aims to adapt LLaMA [33] into an instruction-\nfollowing model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows\nBLIP-2\u2019s [15] architecture but replaces the language decoder with Vicuna [9], which better supports\nlonger responses and multi-round conversations. LLaVA connects text and image modalities through\na trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains\nboth the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively\nhigh compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated\nattention layers to connect visual and language information and establish attention between in-context\nexamples, leaving the vision encoder and language decoder frozen.\n2.2\nMulti-modal Instruction Tuning Dataset\nThe concept of instruction tuning in multi-modal models was \ufb01rst introduced in Multi-Instruct [36],\nwhere 47 diverse multi-modal tasks covering 11 broad categories were organized. Each task comprises\nat least 5,000 instances (input-output pairs) from existing open-source datasets and 5 expert-written\ninstructions. Multi-Instruct covers most multi-modal tasks that require visual understanding and\nmulti-modal reasoning, such as Visual Question Answering [11, 41, 30], Image Captioning [16],\nImage Generation [7], and Visual Relationship Understanding [14], among others. Similarly, Mini-\nGPT4 [39] constructs its instruction following dataset by combining Conceptual Caption [28, 7],\nSBU [21], and LAION [26] with handwritten instruction templates. More recently, LLaVA [17] has\nbrought the quality of an instruction tuning dataset to a higher level, as it was obtained by expanding\nthe original captions of COCO [16] images with handwritten seed instructions using GPT-4 [19] to\nprovide more detailed descriptions and multi-round conversations.\nTo the best of our knowledge, the above-mentioned works are the only few that considered instruction\ntuning in multi-modal models. Our approach further differs from them in that we incorporate in-\ncontext examples into instruction tuning by grouping multiple similar instructions together to form\na contextual example set. We are the \ufb01rst to propose the in-context instruction tuning paradigm\nin multi-modal models and to build the corresponding multi-modal in-context instruction tuning\ndatasets.\n3\n3\nMethod\nIn this section, we will introduce the details of the MIMIC-IT dataset in Sec. 3.1, our Otter\u2019s training\ndetails in Sec. 3.2, and the integration with Hugging Fance ecosystem in Sec. 3.3.\n3.1\nMulti-Modal In-Context Instruction Tuning\nThe OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in\nits few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs\nderived from individual HTML \ufb01les, with signi\ufb01cant contextual relationships between different pairs,\nas depicted in Fig 2(a). An MMC4 training data sample contains (i) a queried image-text pair, where\nthe text typically describes the image, and (ii) context, which includes the remaining image-text pairs\nfrom the same HTML \ufb01le. The primary training objective of OpenFlamingo is to generate text for\nthe queried image-text pair, and the paradigm of generating query text conditioned on in-context\nexamples ensures OpenFlamingo\u2019s in-context learning capacity during the inference phase.\nOur Multi-Modal In-Context Instruction Tuning (MIMIC-IT) dataset aims to augment Open-\nFlamingo\u2019s instruction comprehension capabilities while preserving its in-context learning capacity.\nTo unleash OpenFlamingo\u2019s instruction-following potential, we compile data from visual-language\ntasks into image-instruction-answer triplets. Concurrently, to maintain OpenFlamingo\u2019s in-context\nlearning capacity, we retrieve in-context examples for each triplet, which often lack correlated context,\nsuch as a visual question-answer data sample in VQAv2 [3]. Speci\ufb01cally, each MIMIC-IT data sample\nconsists of (i) a queried image-instruction-answer triplet, with the instruction-answer tailored to the\nimage, and (ii) context. The context contains a series of image-instruction-answer triplets that contex-\ntually correlate with the queried triplet, emulating the relationship between the context and the queried\nimage-text pair found in the MMC4 dataset. The training objective for MIMIC-IT is to generate the\nanswer within the queried image-instruction-answer triplet. The image-instruction-answer triplets\nare derived from (i) visual question-answer datasets, namely, VQAv2 [3] and GQA [13], (ii) visual\ninstruction datasets, such as LLaVA [17], (iii) an in-progress, high-quality panoptic video scene graph\ndataset from the PVSG repository. For each video, we select 4-8 frames for instruction-following\nannotation, using the LLaVA dataset as a reference. We have developed three heuristics to construct\nthe context for each image-instruction-answer triplet, as illustrated in Fig 2(b).\n3.2\nTraining Details\nOur approach adopts the OpenFlamingo training paradigm to train the Otter model. The pretrained\nOpenFlamingo model comprises a LLaMA-7B [33] language encoder and a CLIP ViT-L/14 [24]\nvision encoder. To prevent over\ufb01tting and leverage pretrained knowledge, we freeze both the encoders\nand only \ufb01netune the Perceiver resampler module, cross-attention layers inserted into the language\nencoder and input/output embeddings of the language encoder. This results in approximately 1.3\nbillion trainable parameters for the Otter model.\nTo optimize our model, we employ the AdamW optimizer [18] with a starting learning rate of\n10\u22125 and a batch size of 4. We train Otter for 6 epochs, with the learning rate scheduled using a\ncosine annealing scheduler. We also use gradient clipping of a threshold of 1.0 to prevent exploding\ngradients.\nDuring our training, we follow a speci\ufb01c format to prepare our training data. The format includes a\ncombination of image, user instruction, \"GPT\"-generated answers 1, and a special token known as the\n[endofchunk] token. We format the training data as follows:\n<context> [image] User:<instruction> GPT:[answers] <answer>.[endofchunk]\nwhere the [image], [answer], and [endofchunk] tokens are unique and serve a speci\ufb01c purpose.\nWe design such a chatbot-like format to train our model to improve the instruction-following and\nconversation generalizability of the model. The [image] and [endofchunk] tokens are originally\nfrom the OpenFlamingo training paradigm, while the [answer] token is a new introduction by us\nin training Otter. The [answer] token separates the answers from the instruction, so that, we mask\nall tokens after the [answer] token during training and set them as the prediction objectives of the\nmodel. We train our model using a cross-entropy loss.\n1To support user-assistant conversations, we adopt \"GPT\" as the role label because it does not have any\nspeci\ufb01c semantic meaning in vocabulary.\n4\nInstruction:\nWhat is the \nbaseball player \nholding in his \nhands?\nInstruction:\nWhat color is \nthe baseball \nplayer's \nuniform?\nInstruction:\nWhich team does \nthe baseball \nplayer belong to?\nAnswer: The \nbaseball player is \nholding a baseball \nbat in his hands.\nAnswer: The \nbaseball \nplayer's uniform \nis grey.\nAnswer: The \nbaseball player \nbelongs to the \nAngels baseball \nteam.\nin-context examples\nquery\n\u2026\nsame image, different instructions\nInstruction: What \nskill set do \nsnowboarders need \nto perform such \ntricks?\nInstruction:\nWhat skill set \ndoes  this player \nneed to have\\in a \nmatch, and why?\nInstruction:\nWhat skill set \ndoes a player \nneed to excel in \nthis sport, \nbased on the \nimage?\nAnswer: To \nperform tricks \u2026\nminimize the \nrisks involved\nAnswer: A tennis \nplayer \nneeds \u2026winning \nmatches.\nAnswer:\nTo \nimprove his \nperformance, \u2026his \nteam.\nin-context examples\nquery\n\u2026\nsame instruction, different images\nInstruction:\nWhat is \nthe main thing \nhappening in this \npicture?\nInstruction: Why did \nthe player in red who \nwas attacking fall to \nthe ground?\nInstruction: Why is \nthe man in the red \njersey about to stand \nup from the ground?\nAnswer: A group \nof \u2026attack and look \nfor an opportunity to \nshoot.\nAnswer: Because \nhe ..the ground to \ntry to create a \npenalty.\nAnswer: Because \u2026the \nball go in and didn't \nneed to pretend to \ncreate a penalty.\nin-context examples\nquery\nsequential images, different instructions\nInstruction: \nDescription of the \nvideos humorous \nmoment?\nAnswer: A man on the \npitch falls down\nafter a shot, \u2026 \\with \nhis teammates.\nInstruction: Why is \nthe whole video \nhumorous?\nAnswer: The funny \nthing \u2026 it's funny \nhow his injury seems \nto heal in an \ninstant.\n\uff08a\uff09 Multimodal C4 (MMC4)\nTitle: WALNUT AND BLUE CHEESE STUFFED MUSHROOMS\nText: \u201cWhen you lock the door \nusing the lock tab on the driver\u2019s \ndoor, all of the other doors and \ntailgate lock at the same time.\u201d,\nText: \u201cThe ideas for stuffing \nmushrooms are endless, so many \ncombinations to play \nwith, a couple of my personal favourites \nare these Mediterranean \nStuffed Mushrooms and these Spinach \nand Toasted Pine Nut Stuffed Mushrooms.\u201d, \nText: \u201cWhen you lock/unlock the \ndriver\u2019s door and tailgate using the \nmaster lock switch, all the other \ndoors lock/ unlock at the same time.\u201d\nTitle: HONDA: (UN)LOKING THE DOORS FROM THE INSIDE\nText: \u201cThis Walnut and Blue \nCheese Stuffed Mushrooms recipe \nis sponsored by Fisher Nuts.\u201d, \n\uff08b\uff09 Multi-Modal In-Context Instruction Tuning (MIMIC-IT)\nFigure 2: Illustration of example data formats in MMC4 and MIMIC-IT. (a) The illustration of\nthe data format in the MMC4 dataset that are used OpenFlamingo. (b) Three heuristics to build the\nmulti-modal in-Context instruction tuning (MIMIC-IT) dataset.\n5\n3.3\nIntegratation with Hugging Face\nWe have integrated Otter into Hugging Face Transformers [34] and trained it using the Hugging Face\nAccelerator2, which enables automatic mapping of the model weights to different GPU devices and\nof\ufb02oading of over\ufb02owed weights to CPU or disk. Additionally, we use bf16 mixed precision during\ntraining. The total optimizations enable our model to be trained on 4\u00d7RTX-3090 GPUs, each with\n24GB memory.\nMeanwhile, since Otter has been integrated into Hugging Face Transformers, it can now be reused\nwith less than \ufb01ve lines of code, making it much easier for researchers to integrate into their respective\ntraining and inference pipelines (compared to the original OpenFlamingo implementation). We also\nprovide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed to enable greater training\nef\ufb01ciency and less memory consumption.\nTo enable future research and convenience, we also provide a script for converting the origi-\nnal OpenFlamingo-9B checkpoint into the Hugging Face Model format. The converted check-\npoint and our Otter model are uploaded and available on the Hugging Face model hub luodian/\nopenflamingo-9b-hf and luodian/otter-9b-hf, respectively.\n4\nDemonstrations\nIn this section, we show several examples of two types of demonstrations of Otter: (1) the ability to\nfollow instructions in Sec. 4.1, and (2) the ability of learning to execute new instructions following\nprovided in-context examples in Sec. 4.2. Compared with OpenFlamingo, these results demonstrate\nthe importance of in-context instruction tuning and the improvement of Otter.\n4.1\nFollowing User Instructions\nIn Sec. 3.1, we discussed how we \ufb01netuned Otter on visual instruction pairs to transform it into a\npowerful instruction follower. The results of our experiments are demonstrated in Fig. 3, where we\nobserve that Otter is able to provide more detailed descriptions of images and follow user instructions\nmore accurately. This characteristic of Otter is attributed to the co-design of our model and data,\nwhich leverages the generalization ability of a strong language decoder and the rich variety of\ninstructions present in the MIMIC-IT dataset. By \ufb01ne-tuning on visual instruction pairs, Otter is able\nto learn the nuances of human language and accurately apply it to visual input.\nIn order to evaluate Otter\u2019s reasoning capabilities, we designed a series of experiments to test its\nability to handle complex scenarios that require a deeper understanding of the situation and more\nadvanced commonsense reasoning. In Fig. 4, we present an example where OpenFlamingo fails to\nidentify important clues in the image, while Otter is able to demonstrate a deeper understanding of\nthe scene and apply relevant commonsense knowledge to answer the given question. In this particular\nexample, Otter is able to correctly interpret the confusion around the traf\ufb01c signal, while also taking\ninto account relevant contextual factors such as accidents, traf\ufb01c congestion, and disruptions to the\n\ufb02ow of traf\ufb01c. This level of reasoning ability is not present in standard image captioning models and\nrepresents a signi\ufb01cant advance in the \ufb01eld of computer vision and natural language processing.\n4.2\nMulti-Modal In-context Learning\nOtter is designed to support multi-modal in-context learning in a similar pattern to Flamingo [1]\nand OpenFlamingo [4], which involves conditioning the language model on the corresponding\nmedia, such as an image that corresponds to a caption or a question-answer pair. In Fig. 5, we\ndemonstrate Otter\u2019s ability to perform visual question answering tasks, where users provide examples\nof instruction-answering pairs and images as contextual examples to prompt the model with subtasks,\nsuch as explaining memes or the danger of sports.\nDespite the fact that user inputs are often short and restricted in these scenarios, Otter is able to\nprovide more comprehensive answers. By leveraging the information contained in the images and\nother contextual examples, Otter is able to better understand the underlying meaning and context of\nthe questions, resulting in more accurate and informative answers.\n2Hugging Face Accelerator\n6\nInstruction:\nWrite a detailed description of the given image?\nOpenFlamingo: Pancakes with bacon and maple syrup.\nOtter: The image features a wooden table with various items on it.\nThere\nis\na\nsandwhich,\nwhich\nhas\nbeen\ncut\nin\nhalf,\nand\na\nbowl\nof\ncereal placed near the plate. Additionally, there is a cup, likely\ncontaining a beverage such as coffee. The table also has a \"\"live\nbaccy\"\" container and a mug present, suggesting a cozy and relaxed\nsetting.\nThe\nscene\nmight\nbe\na\npart\nof\na\nbreakfast\nor\nsnack\ntime\narrangement, with the sandwich, cereals, and beverage as the main\nitems on the table.\nInstruction:\nCan you describe the main features of this image for me?\nOpenFlamingo: The man is flying a kite.\nOtter: The image shows a person standing in front of a kite that is flying in\nthe sky. The person is wearing a black shirt and is positioned on the left side\nof the kite. The kite is quite large and can be seen above the person's head.\nThe scene appears to be outdoors, with the person possibly preparing to launch\nthe kites or enjoying the moment as the kit flies in the breeze. There are also\ntwo other people in the background, one on the right side and another person\npartially visible on the far right. The presence of multiple people in this\noutdoor setting suggests that it might be a gathering or a popular location for\nkite flying and leisure activities.\nFigure 3: Comparisons of image captioning results between OpenFlamingo and Otter. Otter\nprovides more detailed and precise descriptions of the images.\nInstruction:\nWhat\ncould\nhappen\nif\ndrivers\nbecome\nconfused\nby\nthe\ntraffic signals at this intersection?\nOpenFlamingo: They could get stuck in the rain.\nOtter:\nIf\ndrivers\nbecome\nconfused\nby\nthe\ntraffic\nsignals\nat\nthe\nintersection,\nit\ncould\nlead\nto\nvarious\npotential\nscenarios:\n1.\nRed\nlight: The red light signal is on, and the car is moving. In this\ncase, the driver might continue driving, potentially causing a red-\nlight\nrunner\naccident,\nwhich\ncan\nbe\ndangerous\nfor\npedestrians\nand\nother\nvehicles.\n2.\nGreen\nlight:\nIf\nthe\ngreen\nlight\nsignal\nappears\nwhile the car has already started moving, the vehicle might continue\nmoving, potentially leading to a green-light running accident, where\nthe driver does not stop at the green signal and risks colliding with\nother vehicles or pedestrian crossings. 3. Confusion about the signal:\nIf drivers are unsure about the traffic lights, they might hesitate or\nmake\nan\nunpredictable\nmaneuverer,\nwhich\ncould\nlead\nnot\nonly\nto\naccidents but also to traffic congestion and general disruption of the\nflow of traffic.\nFigure 4: Comparisons of situation understanding results between OpenFlamingo and Otter.\nOtter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense\nknowledge to answer the given question.\n7\nInstruction: what does \nthis meme usually use \nfor?\nAnswer: to ask people to \nthink twice and dont be \nsilly\nInstruction:\nwhat does \nthis meme usually use \nfor?\nAnswer: to express users \nfeeling of sad, or \nmoving.\nUser provided examples\nInstruction:\nwhat does this meme usually use for?\nOtter: This meme typically uses a sad-looking baby\nas the main image, with text encouraging people to\nstop being a baby and to be more like a tough,\ncool individual. The image and text are meant to\ncreate a playful and humorous contrast between a\ncute, vulnerable baby and a tuff, strong person.\nOtter reaction\nInstruction:\nWhat is\nthe danger of this\nsport?\nAnswer: The player may\nget hitted by the\ntennis ball.\nInstruction:\nWhat is\nthe danger of this\nsport?\nAnswer: While chasing \nthe baseball, the \nplayer may \ninadvertently collide \nwith other players.\nUser provided examples\nInstruction:\nWhat is the danger of this sport?\nOtter:\nThe\nsport\ninvolves\nplayers\nrunning\nand\ntrying to catch the ball while others are standing\nin\nthe\ngrass,\nwhich\ncan\nlead\nto\ncollisions\nor\naccidents. These collisions can result in injuries\nto\nthe\nplayers\ninvolved.\nIt\nis\nessential\nfor\nplayers\nto\nbe\naware\nof\ntheir\nsurroundings\nand\nmaintain\na\nsafe\ndistance\nfrom\neach\nother\nto\nminimize the risk of accidents during the game.\nOtter reaction\nFigure 5: In-context learning examples of Otter. Given two examples of instruction-answering\npairs and an image, Otter is able to provide more comprehensive answers.\n5\nConclusion\nIn this work, we propose Otter, a multi-modal in-context learning foundation model with instruc-\ntion tuning. Through partial \ufb01netuning on MIMIC-IT dataset, we observe that Otter can convert\nOpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities.\nWith the assistance of rich instructions from images and videos, Otter generalizes to achieve better\ninstruction-following and situation-understanding performances.\n5.1\nLimitations\nLanguage Hallucination. Since Otter is built upon OpenFlamingo, which depends upon LLaMA,\nthe hallucination issue of LLaMA is inherited by Otter. Also, the current Otter model may hallucinate\nthe language that is not related to the image. This issue might be solved by introducing negative\nexamples in the training data.\n5.2\nFuture Supports\nIn the future, we plan to explore the integration of more ef\ufb01cient training schemas (e.g., parameter-\nef\ufb01cient \ufb01netuning such as LoRA [12]) and more modalities (e.g., 3D vision).\nAcknowledgement. We thank Chunyuan Li and Jack Hessel for their advice and support, as well as\nthe OpenFlamingo team for their great contribution to the open-source community.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022. 2, 3, 6\n[2] Alibaba. Tongyi qianwen. 2023. 3\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\nZitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425\u20132433, 2015. 4\n[4] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Open\ufb02amingo, March 2023. 2, 3, 6\n[5] Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023. 3\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 1\n[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021. 3\n[8] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,\nand Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models, 2023. 3\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 3\n[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question answering.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n6904\u20136913, 2017. 2, 3\n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 8\n[13] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 6700\u20136709, 2019. 4\n[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International journal of\ncomputer vision, 123:32\u201373, 2017. 3\n[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 1, 3\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014. 2, 3\n9\n[17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023. 1, 3, 4\n[18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 4\n[19] OpenAI. Gpt-4 technical report. 2023. 1, 3\n[20] OpenAI. Introducing chatgpt. 2023. 1, 3\n[21] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1\nmillion captioned photographs. Advances in neural information processing systems, 24, 2011. 3\n[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022. 1\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 1\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 4\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-\ufb01ltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3\n[27] SenseTime. Sense nova. 2023. 3\n[28] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2556\u20132565, 2018. 3\n[29] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023. 3\n[30] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual\nreasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 217\u2013223, 2017. 3\n[31] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023. 3\n[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023. 3\n[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and ef\ufb01cient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 1, 3, 4\n10\n[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics. 2, 6\n[35] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023. 3\n[36] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot\nlearning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022. 3\n[37] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3\n[38] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Ef\ufb01cient \ufb01ne-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023. 1, 3\n[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023. 1, 3\n[40] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,\nYoungjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open,\nbillion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. 2\n[41] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question\nanswering in images. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4995\u20135004, 2016. 3\n[42] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language.\narXiv preprint arXiv:2212.11270, 2022. 3\n11\n"
  },
  {
    "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
    "link": "https://arxiv.org/pdf/2305.03695.pdf",
    "upvote": "2",
    "text": "VERA: A General-Purpose Plausibility Estimation Model for\nCommonsense Statements\nJiacheng Liu\u2661\u2217 Wenya Wang\u2661\u2663\u2217 Dianzhuo Wang\u2662\nNoah A. Smith\u2661\u2660 Yejin Choi\u2661\u2660 Hannaneh Hajishirzi\u2661\u2660\n\u2661Paul G. Allen School of Computer Science & Engineering, University of Washington\n\u2660Allen Institute for Artificial Intelligence\n\u2663Nanyang Technological University\n\u2662John A. Paulson School of Engineering and Applied Sciences, Harvard University\nliujc@cs.washington.edu\n\u2217Equal contribution.\nAbstract\nToday\u2019s language models can be remarkably\nintelligent yet still produce text that contains\ntrivial commonsense errors.\nTherefore, we\nseek a retrospective verification approach that\ncan reflect on the commonsense plausibility\nof the machine text, and introduce VERA, a\ngeneral-purpose model that learns to estimate\nthe commonsense plausibility of declarative\nstatements. To support diverse commonsense\ndomains, VERA is trained on \u223c7M common-\nsense statements that are automatically con-\nverted from 19 QA datasets and two common-\nsense knowledge bases, and using a combi-\nnation of three training objectives. When ap-\nplied to solving commonsense problems in the\nverification format, VERA substantially out-\nperforms existing models that can be repur-\nposed for commonsense verification, even in-\ncluding GPT-3.5/ChatGPT/GPT-4, and it fur-\nther exhibits generalization capabilities to un-\nseen tasks and provides well-calibrated outputs.\nWe find that VERA excels at filtering machine-\ngenerated commonsense knowledge and is use-\nful in detecting erroneous commonsense state-\nments generated by models like ChatGPT in\nreal-world settings.\n1\nIntroduction\nWe introduce VERA, a general-purpose common-\nsense statement verification model. This model\nis designed to estimate the plausibility of declar-\native, natural language statements based on com-\nmonsense knowledge.\nWe build VERA in response to the absence of\ngood detectors of commonsense errors in text gen-\nerated by language models (LMs). LMs have been\nadvancing rapidly and have demonstrated remark-\nable success in various tasks, including question\nanswering, natural language inference, sequence\nclassification, and text generation. Yet these mod-\nels still make simple commonsense mistakes. As\nshown in Figure 1, as of February 23, 2023, Chat-\nGPT (OpenAI, 2022a) reportedly output the text\nFigure 1: VERA estimates the correctness of declarative\nstatements. Example adapted from a contribution made\nby Henry Minsky to Marcus and Davis (2023) on Febru-\nary 23, 2023.\n\u201csince the density of a marble is much less than the\ndensity of mercury, the marble would sink to the\nbottom of the bowl if placed in it\u201d, which is clearly\nincorrect. This kind of failure raises concerns about\nthe reliability and trustworthiness of these models\n(Lin et al., 2022).\nVERA estimates a plausibility score for a com-\nmonsense statement based on its commonsense\nknowledge about the world. It contrasts with fact\nverification methods (Thorne et al., 2018; Wadden\net al., 2020), which verify the correctness of claims\nbased on evidence from a text corpus. VERA en-\nables plausibility estimation where direct evidence\nis often not retrievable from some corpus, and usu-\nally some implicit, fuzzy reasoning is needed. It\noperates solely with the commonsense knowledge\nstored in its model parameters, and does not have a\nretrieval component.\nVERA is built on top of T5 (Raffel et al., 2020),\na generic pretrained LM, by finetuning on a vast\ncollection of correct and incorrect commonsense\nstatements sourced from knowledge bases (KBs)\nand question answering (QA) datasets. The 21 data\nsources (Table 5, appendix) amount to \u223c7M state-\nments encompassing a wide spectrum of domains,\nincluding general, scientific, physical, and social\ncommonsense, as well as quantitative (reasoning\nabout numbers) and qualitative (reasoning about\narXiv:2305.03695v3  [cs.CL]  18 Oct 2023\nqualitative relationships such as smaller) common-\nsense. We propose a novel two-stage training pro-\ncess that takes into account the scale and quality\nof data from different sources. In addition to the\nstandard multiple-choice binary classification ob-\njectives, we adopt a supervised contrastive loss\n(Khosla et al., 2020) to magnify the distinction be-\ntween similar statements with different correctness\nlabels. Furthermore, we propose an automatic way\nof augmenting the training data by eliciting LMs to\ngenerate incorrect answers to commonsense ques-\ntions and empirically find it helps generalization.\nWe evaluate VERA in the following applications:\n\u2022 Excelling in commonsense problems over\nGPT-series when repurposed for verifica-\ntion (\u00a75.1). VERA can be applied to solve\nmultiple-choice and boolean commonsense\nproblems when expressed in the verification\nformat, by scoring and ranking candidate hy-\npotheses. It substantially outperforms existing\nmodels repurposed for commonsense verifica-\ntion (including GPT-3.5, ChatGPT and GPT-\n4), improving upon the best existing baseline,\nFlan-T5, with absolute improvement of 6% on\nseen benchmarks and 4% on unseen ones.\n\u2022 Filtering\nLM-generated\ncommonsense\nknowledge (\u00a75.2). VERA can filter noisy com-\nmonsense knowledge statements generated\nby other LMs, improving the effectiveness\nof LM-generated knowledge in downstream\nknowledge-augmented inferences.\nVERA\nis\nwell-calibrated,\nenabling\nfiltering\nat\ncustomized thresholds.\n\u2022 Detecting commonsense errors in ChatGPT\noutputs (\u00a75.3). Through a preliminary analy-\nsis, we find that VERA can identify common-\nsense errors made by ChatGPT in-the-wild,\nwith a precision of 91% and a recall of 74%.\nAn example of VERA in action is shown in\nFigure 1.\nWe hope VERA can be a useful tool for improv-\ning the commonsense correctness of existing gen-\nerative LM output and inspire more effort toward\ngeneral-purpose and robust verification methods.\n2\nProblem Definition and Scope\nOur goal is to build a model that can estimate the\nplausibility of any given commonsense statement.\nThe model takes as input a statement that (1) is\nexpressed in natural language; (2) is declarative,\nas opposed to interrogative questions; (3) is self-\ncontained without requiring additional context to\ncomprehend; (4) has an objective, binary correct-\nness label; and (5) in principle can be labeled using\nwidely-held commonsense knowledge about the\nworld. Encyclopedic knowledge (e.g., Ljubljana is\nthe capital of Slovenia.) is out of scope. Moving\nforward, unless explicitly noted, we use common-\nsense statement to refer to statements within the\nabove scope. Though somewhat strict, this scope\ncovers a broad range of potential applications.\nFor an input commonsense statement x, the\nmodel should output a real-valued score s \u2208 [0, 1]\nthat represents its estimated plausibility of x. While\nthe gold correctness label is binary, we let the\nmodel output a score to reflect its confidence. A\nscore of 1.0 means that it is completely confident\nthat x is correct, and a score of 0.0 means it is\ncompletely confident that x is incorrect. When pre-\ndicting correctness label from the score, we use 0.5\nas the threshold.\n3\nMethod\nIn this section, we describe the whole pipeline to\nbuild VERA. We start from curating large-scale\ntraining data including both correct and incorrect\nstatements from diverse commonsense tasks (\u00a73.1).\nNext, we learn a scoring model that takes a state-\nment and returns a continuous score by finetuning a\nLM via 3 training objectives (\u00a73.2). An additional\npost hoc calibration strategy is applied to make the\noutput scores well-calibrated (\u00a73.3).\n3.1\nData Construction\nLabeled commonsense statements usually do not\nappear in text in the wild, while some common-\nsense question answering (QA) datasets and com-\nmonsense knowledge bases (KBs) are good sources\nfor this kind of statements. We collect correct and\nincorrect commonsense statements from the above\ntwo types of data source. Table 1 shows some ex-\namples on how these statements can be converted\nfrom QA problems and KB entries. In total, we\nobtain \u223c7M statements (for training) from 19 QA\ndatasets (\u00a73.1.1) and two KBs (\u00a73.1.2) that encom-\npass a wide spectrum of commonsense domains.\nTable 5 (appendix) lists these datasets with statis-\ntics. All datasets we use are publicly available.\n3.1.1\nFrom Commonsense QA Datasets\nNumerous commonsense reasoning datasets have\nbeen published in recent years (Davis, 2023), and\nMULTIPLE-CHOICE QA\nOriginal example:\nWhat would someone wear to protect themselves from a cannon?\n(A) ungulate (B) bomber (C) body armor (D) tank (E) hat\nAnswer: (C)\nConverted statement group:\nOne would wear an ungulate to protect themselves from a cannon.\n\u2717\nOne would wear a bomber to protect themselves from a cannon.\n\u2717\nOne would wear body armor to protect themselves from a cannon.\n\u2713\nOne would wear a tank to protect themselves from a cannon.\n\u2717\nOne would wear a hat to protect themselves from a cannon.\n\u2717\nBOOLEAN QA\nOriginal example:\nCan an average dog follow an instruction manual?\nAnswer: no\nConverted statement group:\nAn average dog can follow an instruction manual.\n\u2717\nKNOWLEDGE BASE\nOriginal example:\nRubber stamps provide a way to make messages stand out.\nConverted statement group:\nRubber stamps provide a way to make messages stand out.\n\u2713\nArabic numbers provide a way to make messages stand out.\n\u2717\nBandages provide a way to make messages stand out.\n\u2717\nMeat tenderizers provide a way to make messages stand out.\n\u2717\nTable 1: Conversions from original commonsense QA\nproblems and knowledge base entries to statement\ngroups that are used for training.\nmany of them are in the format of multiple-choice\nQA (selecting the correct answer out of a set of\nchoices) or boolean (yes/no) QA. These can be eas-\nily converted to correct and incorrect commonsense\nstatements. From multiple-choice QA problems,\nwe combine the question and each answer choice\nto form declarative statements, which are correct\nwhen using the correct answer, and incorrect oth-\nerwise. From boolean QA problems, we convert\nthe question into a declarative statement, and keep\nthe original label as the correctness label. Concrete\nexamples can be found in Table 1.\nStatement groups.\nWe refer to statements orig-\ninating from the same problem as a statement\ngroup. Note that statement groups originating from\nmultiple-choice problems contain at least two state-\nments, of which one and only one is correct; state-\nment groups originating from boolean problems\ncontain only one statement, and it can be either\ncorrect or incorrect.\nWe do conversion to declarative statements auto-\nmatically. From QA datasets, we create declarative\nstatements from QA problems using the following\nmethod:\n\u2022 If the problem contains a question, we con-\nvert the question and choice into a declara-\ntive statement using the question conversion\nmodel created by Chen et al. (2021).\n\u2022 If the question is cloze-style, we replace the\nblank with the choice.\n\u2022 If the question is an incomplete sentence and\nthe choice is a continuation to it, we concate-\nnate the question and the choice.\n\u2022 If there is no question and the problem only\nasks to choose between some choices, we use\nthe choice as the declarative statement.\n\u2022 For boolean problems, we always use yes as\nthe choice and create a single declarative state-\nment for each problem. We use the original\nlabel as the correctness label of this statement.\nIn total, 19 commonsense QA datasets contribute\n\u223c200k statement groups and \u223c400k statements to\nthe training set of VERA.\nLM-augmented falsehoods.\nExisting common-\nsense QA datasets are mostly manually constructed\nor assembled from standard school exams. A model\ntrained on these datasets might overfit specific an-\nnotation patterns from humans which may limit\ngeneralization. Therefore, we augment QA prob-\nlems with LM-generated answers and construct\nadditional incorrect statements. Specifically, for\na multiple-choice question, we use a small LM to\nsample 50 possible answers to the question, and\nselect the 3 least probable answers with generation\nprobability less than 0.15 (making these unlikely\nto be correct answers). This threshold is chosen\nbased on manual inspection over a small portion\nof examples. We observe generated answers with\nprobability larger than 0.15 are more likely to be\nplausible. We create LM-augmented falsehoods for\nthe training set of 9 commonsense QA datasets, as\nnoted in Table 5 (appendix).\n3.1.2\nFrom Commonsense KBs\nCommonsense KBs (e.g., Atomic2020 in Hwang\net al. (2020), and GenericsKB in Bhakthavatsalam\net al. (2020)) contain a large number of correct\ncommonsense statements. To create incorrect state-\nments, we automatically perturb KB entries by re-\nplacing the subject with three random subjects that\nappear in the KB. Table 1 shows how to convert an\nentry in GenericsKB to a statement group contain-\ning four statements, three of which are augmented\nvia perturbations. The perturbed statements are\nrelatively easy to identify and may contain false\nnegatives. As noted in \u00a73.2.4, we use these KB-\nconstructed statements in a separate training stage\nthat precedes training with QA-constructed state-\nments. In total, two commonsense KBs contribute\n\u223c1.6M statement groups and \u223c6M statements to\nthe training set of VERA.\n3.2\nModel Training\n3.2.1\nModel Architecture\nGiven a statement x, VERA outputs a real-valued\nscore s \u2208 [0, 1]. As we will use a transformer-\nbased LM as the backbone of VERA, we first ex-\ntract the input representation h by selecting the last\nhidden state corresponding to the EOS input token.\nWe choose EOS because it is capable to encode the\nentire input in both bidirectional encoder models\n(e.g., T5\u2019s encoder) and left-to-right decoder mod-\nels (e.g., LLaMA). Then a linear layer projects h to\na scalar logit z, followed by a sigmoid function \u03c3(\u00b7)\nthat transforms the logit into a score s. Formally,\nh = fLM(x), z = flinear(h), s = \u03c3(z).\nFor brevity, we use h(x), z(x) and s(x) to refer to\nthe representation, logit and score of an arbitrary\ninput x.\n3.2.2\nBatching\nThe data we construct consists of statements be-\nlonging to different statement groups. For rea-\nsons we will describe in \u00a73.2.3, we put all state-\nments belonging to the same statement group into\nthe same batch. Each batch may contain multiple\ncomplete statement groups. We denote by BG the\nnumber of statement groups and BS the number\nof statements in total within a single batch. We\ndenote the statement groups as {Xj}BG\nj=1, and the\nstatements as {xi}BS\ni=1. {Xj}BG\nj=1 is a partition of\n{xi}BS\ni=1. yi \u2208 {0, 1} is the correctness label of xi.\n3.2.3\nTraining Objectives\nThe model is trained with a linear combination of\nthree losses: a binary classification loss, a multi-\nclass loss, and a supervised contrastive loss, L =\n\u03b1Lbin + \u03b2Lmc + \u03b3Lctr, which we describe below.\nBinary classification loss.\nNaively, common-\nsense statement verification can be viewed as a\nbinary classification task. Under this setting, the\nloss is\nLbin = \u2212yi log s(xi) \u2212 (1 \u2212 yi) log(1 \u2212 s(xi)).\nMulti-class loss.\nWe expect the model to be ro-\nbust against nuances in commonsense statements.\nIdeally, the model should be able to recognize oppo-\nsite correctness labels for a group of seemingly sim-\nilar statements in surface forms, such as statements\ncreated from different choices of the same question,\nor perturbed from the same piece of knowledge in\na KB. To achieve this goal, we treat each state-\nment group as a multi-class classification problem,\nmaximizing the log-likelihood of the single correct\nstatement in the statement group after passing the\nlogits through a softmax. Formally,\nLmc = \u2212 log\nexp z(xj\u2217)\nPCj\nc=1 exp z(xjc)\n,\nwhere xj\u2217 is the correct statement in Xj. Note that\nthe multi-class loss is not applicable to statement\ngroups with only one statement (i.e., statement\ngroups from boolean QA datasets). We empiri-\ncally find that the multi-class loss indeed improves\ngeneralization towards unseen multiple-choice QA\ndatasets as indicated in Figure 3 (appendix).\nSupervised contrastive loss.\nIt has been shown\n(Khosla et al., 2020) that supervised contrastive\nlearning helps to improve model robustness and\ngeneralization against input variations. In light of\nthis, we further adopt supervised contrastive learn-\ning on top of the input representations h. We show\nin Figure 3 (appendix) that the contrastive loss in-\ndeed improve generalization to unseen datasets.\nFor each anchor statement xi in a batch, the con-\ntrastive loss aims to maximize the similarity be-\ntween xi and each other statement xp that has the\nsame correctness label as xi (i.e., positive example).\nAt the same time, we push apart xi and other state-\nments xn that has opposite correctness label as xi\n(i.e., negative example). The supervised contrastive\nloss is\nLctr =\n\u2212 log\nP\nk\u2208P(i) exp[cos(h(xi), h(xk))/\u03c4]\nP\nk\u2208P(i)\u222aN(i) exp[cos(h(xi), h(xk))/\u03c4],\nwhere \u03c4 is a temperature hyperparameter, cos(\u00b7, \u00b7)\nrefers to cosine similarity, P(i) \u2286 [BS] is the index\nset of statements that are positive examples for xi,\nand N(i) \u2286 [BS] is the index set of statements that\nare negative examples for xi. Formally,\nP(i) =\n\b\nk | 1 \u2264 k \u2264 BS, yk = yi, k \u0338= i\n\t\n,\nN(i) =\n\b\nk | 1 \u2264 k \u2264 BS, yk \u0338= yi\n\t\n.\n3.2.4\nTwo-Stage Training\nSince data sourced from KBs are larger in scale but\nmore noisy than data sourced from QA datasets,\nwe take a two-stage training approach. In training\nstage A, we start from a pre-trained LM and train\nwith data sourced from KBs. In training stage B,\nwe start from the model obtained in stage A and\ntrain with data sourced from QA datasets. During\nexperiments we found that this setting is better than\nsingle-stage training with either data source or a\nmixture of the two.\n3.3\nInference and Calibration\nAn ideal plausibility estimation model should be\ncalibrated, that is, its confidence in its predictions\nshould be approximately equal to the actual fre-\nquency of correctness. During early experiments,\nwe found that VERA tends to be overconfident.\nTherefore, we apply a post hoc calibration on\nVERA\u2019s output. Following the temperature scal-\ning method introduced in Guo et al. (2017), during\ninference we divide the model-predicted logit by a\ntemperature T before computing the score, that is,\nh = fLM(x), z = flinear(h), \u02dcz = z/T, s = \u03c3(\u02dcz).\nNote that no temperature scaling is applied during\nmodel training.\nWith predictions on a validation set D\n=\n{(xi, yi)}D\ni=1, we estimate T that gives the mini-\nmal expected calibration error (ECE) (Naeini et al.,\n2015) on this validation set. Equation 1 in \u00a7C.1\nshows how ECE is computed. In practice, we use\nthe combined development sets of the seen datasets\n(\u00a74.2) to estimate T, and the optimal T becomes a\nparameter of VERA. Note that temperature scaling\ndoes not change the relative ordering of prediction\nscores, and thus the other performance metrics (e.g.,\naccuracy) are not affected (see detailed explanation\nin \u00a7B.2).\n4\nExperimental Setup\nIn this section, we provide more details of model\ntraining, the evaluation protocol and metrics, and\ndescribe the baseline models we benchmark.\n4.1\nTraining Details\nDatasets.\nFor training stage A, we use the \u223c1.6M\nstatement groups (\u223c6M statements) sourced from\ntwo commonsense KBs; for training stage B, we\nuse the \u223c200k statement groups (\u223c400k state-\nments) sourced from 19 commonsense QA datasets.\nFor each training stage, we mix the training sets of\nall datasets together, without any re-weighting.\nModels.\nWe use two types of pretrained LMs\nas the backbone of VERA: (1) the encoder of\nT5 (Raffel et al., 2020), which is a bidirec-\ntional encoder model; (2) LLaMA (Touvron et al.,\n2023), which is a left-to-right decoder model.\nFor the T5 encoder, we start from the pretrained\nT5-v1.1-XXL1 whose encoder has about 5B pa-\nrameters, and refer to the resulting model as VERA-\nT5. (During experiments we found that starting\nfrom Flan-T5-XXL2 performs slightly worse than\nstarting from T5-v1.1-XXL.) For LLaMA, we start\nfrom the pretrained LLaMA-7B and refer to the\nresulting model as VERA-LLaMA. As we will\nsee, VERA-T5 has better performance than VERA-\nLLaMA, so unless explicitly specified, when we\nsay VERA we mean VERA-T5. See Table 8 (ap-\npendix) for the complete hyperparameter settings\nand \u00a7C for the implementation details.\n4.2\nEvaluation and Baselines\nEvaluation protocol.\nWe divide our evaluation\ninto two parts: (1) Seen benchmarks, whose train-\ning set is used for model training. (2) Unseen\nbenchmarks, whose training set is not used for\nmodel training. We futher divide up the unseen\nbenchmarks into type 1 and type 2, where in type\n1 benchmarks the task is similar to those in the\nseen benchmarks, while type 2 benchmarks are a\nbit further away in terms of the nature of the task.\nExamples of type 2 unseen benchmarks include\nHellaSwag which is contextualized with event de-\nscriptions, and CREAK which involves reasoning\namong different entities.\nDepending on the nature of the evaluation bench-\nmark, we use different metrics to evaluate our\nmodel\u2019s performance. Unless explicitly said other-\nwise, we report performance on the development\nset, where the gold labels are available, and we do\nnot use the development sets of unseen datasets\nfor model selection. The overall metric reported\nover multiple benchmarks is the unweighted aver-\nage of the metric over all these benchmarks, which\naccounts for the differently-sized evaluation sets.\nMetrics.\nWe report accuracy for multiple-choice\nand balanced boolean benchmarks. For those un-\nbalanced boolean benchmarks (e.g., LM-generated\nknowledge filtering datasets), we report area under\nthe ROC curve (AUROC) and average precision\n(AP). To measure how well the model-predicted\nscores reflect confidence, we measure the ECE\n(Naeini et al., 2015) on the boolean benchmarks,\nfollowing Equation 1.\n1https://huggingface.co/google/t5-v1_1-xxl\n2https://huggingface.co/google/flan-t5-xxl\nAccuracy\nSeen Unseen (type 1) Unseen (type 2)\nSKD Critic (355M)\n36.64\n38.34\n43.40\nI2D2 Critic (355M)\n55.03\n54.79\n67.11\nUnifiedQA-v2 (11B) 56.33\n59.73\n53.95\nEntailer (11B)\n73.79\n71.47\n70.72\nGPT-3.5 (175B)\n75.41\n71.03\n78.87\nChatGPT\u2020\n62.11\n61.20\n62.83\nGPT-4\u2020\n72.35\n77.40\n70.29\nFlan-T5\u2021 (11B)\n79.50\n77.62\n78.89\nVERA-LLaMA (7B) 82.99\n75.51\n82.56\nVERA-T5 (5B)\n85.51\n81.65\n83.37\nTable 2: Results on problem-solving with VERA on\nseen and unseen benchmarks. Average accuracy on the\ndevelopment sets is reported. Accuracy across different\nparts (seen, unseen (type 1), unseen (type 2)) are not\ndirectly comparable due to different underlying bench-\nmarks. See Figure 5 and Table 9, 10, 11 (appendix)\nfor full results. \u2020: The performance of ChatGPT and\nGPT-4 may be under-estimated because we don\u2019t have\naccess to the raw token logits. \u2021: Flan-T5 has been\ntrained on some unseen benchmarks we use; see Table 7\n(appendix) for details on data contamination.\nBaseline Models.\nWe compare VERA with the\nbest publicly available models that can be di-\nrectly used or repurposed for commonsense state-\nment verification. Roughly in increasing order\nof performance, these models are: SKD Critic\n(West et al., 2021), I2D2 Critic (Bhagavatula et al.,\n2022), UnifiedQA-v2 (Khashabi et al., 2022), En-\ntailer (Tafjord et al., 2022), GPT-3.5 (OpenAI,\n2022b), ChatGPT (OpenAI, 2022a), GPT-4 (Ope-\nnAI, 2023), and Flan-T5 (Chung et al., 2022). See\nmore details in \u00a7C.2.\n5\nEvaluation Results\nIn this section, we evaluate the ability of VERA\nto estimate the plausibility of commonsense state-\nments and compare it with the baseline models.\nWe show the effectiveness of VERA in three sce-\nnarios: solving commonsense problems, filtering\nLM-generated commonsense knowledge, and de-\ntecting commonsense errors in ChatGPT outputs.\n5.1\nSolving Multiple-Choice and Boolean\nCommonsense Problems\nThe output plausibility scores from VERA can be\nused for solving multiple-choice and boolean com-\nmonsense problems. We first convert the prob-\nlems into the statement group format (\u00a73.1). For\nmultiple-choice problems, we choose the statement\nwith the highest score in the statement group. For\nboolean problems, we use s = 0.5 as the threshold\nto predict correctness labels of statements.\nTable 2 reports the results when VERA is applied\nto solve commonsense problems. See Figure 5 and\nTable 9, 10, 11 (appendix) for full results including\nAUROC and AP. On seen benchmarks (16 multiple-\nchoice and one boolean), VERA outperforms the\nbest baseline, Flan-T5, by 6% on (absolute) ac-\ncuracy and 9% on AUROC. VERA beats Flan-T5\nby 4% accuracy and 5% AUROC on type 1 un-\nseen benchmarks (four multiple-choice and one\nboolean), and by 4% accuracy and 6% AUROC\non type 2 unseen benchmarks (five multiple-choice\nand two boolean), demonstrating good generaliza-\ntion. VERA-T5 has better performance than VERA-\nLLaMA across the board, which may be due to\nits bidirectional connectivity. Aside from perfor-\nmance, VERA also has good calibration, with ECE\nno higher than 3% on seen and unseen benchmarks.\nThe post hoc calibration method improves calibra-\ntion across all three parts.\nTypically we may need to choose a threshold for\nbinary classification in boolean datasets. However,\nwe notice that a zero logit (z = 0) is generally close\nto the optimal decision threshold between correct\nand incorrect commonsense statements. Therefore\nwe do not estimate a model-specific threshold, and\nsimply use the default threshold: z = 0, or equiva-\nlently, s = 0.5.\n5.2\nFiltering LM-generated Commonsense\nKnowledge\nFigure 2 reports the results when VERA is ap-\nplied to filter LM-generated commonsense knowl-\nedge. On the two seen benchmarks, SKD_anno and\nI2D2_anno, VERA is a better knowledge filter than\nall baseline models, in terms of both AUROC and\nAP. In particular, on I2D2_anno it outperforms the\nI2D2 critic model by 2% AUROC, which is specif-\nically trained on the I2D2_anno dataset and does\nnot generalize well to other benchmarks. On the\nunseen benchmark, Rainier_anno, VERA is also\ncomparable with the best baselines like Flan-T5\nand GPT-3.5. As for calibration, the ECE is no\nhigher than 8% on all three benchmarks.\nWe find that filtering commonsense knowledge\nusing VERA can greatly improve the performance\nof knowledge-augmented reasoning methods. In\nthe Generated Knowledge Prompting framework\n(Liu et al., 2021), when solving a commonsense\nQA problem, first a knowledge model generates\nseveral commonsense knowledge statements rele-\nFigure 2: Results for filtering LM-generated common-\nsense knowledge with VERA. We plot the calibration\ncurve for both the uncalibrated version (w/ faded color)\nand calibrated version (w/ saturated color) of the VERA\nmodel. Results on the development sets are reported.\nSee Figure 6 for full results.\nvant to the question, and then a QA model makes\npredictions based on them. A big problem that\nhinders the effectiveness of this framework is that\nmodel-generated knowledge is not always factual,\nand incorrect knowledge statements can mislead\nthe QA model. We introduce VERA to filter these\nstatements before passing them to the QA model.\nIn particular, we keep those statements that receive\na score higher than 0.5 from VERA.\nFollowing\nLiu\net\nal.\n(2022b),\nwe\nuse\nUnifiedQA-large\nas\nthe\nQA\nmodel,\nand\nconsider two knowledge models:\nfew-shot\nGPT-3\n(davinci) (Brown et al., 2020) and\nRainier-large (Liu et al., 2022b). We follow\nthe evaluation settings as in Liu et al. (2022b),\nand for few-shot GPT-3 (davinci), we use the\nsame task-specific few-shot prompts and same\nprocess to generate silver knowledge as in Liu et al.\n(2022b). Results are shown in Table 3. Applying\nknowledge filtering with VERA increases the\nGenerator Filter QA Model\nAcc\nUsefulness\n\u2206\n\u2013\n\u2013\nUnifiedQA 60.45\n\u2013\n\u2013\nGPT-3\n\u2013\nUnifiedQA 67.44\n+6.99\n\u2013\nGPT-3\nVERA UnifiedQA 70.67\n+10.22\n+46%\n\u2013\n\u2013\nUnifiedQA 60.45\n\u2013\n\u2013\nRainier\n\u2013\nUnifiedQA 61.78\n+1.33\n\u2013\nRainier\nVERA UnifiedQA 64.88\n+4.43\n+233%\nTable 3: Results of introducing VERA into the Gener-\nated Knowledge Prompting pipeline (Liu et al., 2021).\nThe QA model is UnifiedQA-large, and the generator\nis either GPT-3 (davinci) or Rainier-large when\napplicable. Average accuracy on the development set is\nreported; see Table 12 (appendix) for detailed results.\nusefulness of GPT-3\u2019s and Rainier\u2019s knowledge\nby 46% and 233%, respectively.\nVERA can\neffectively supervise and improve the quality of\ncommonsense knowledge generated by a much\nlarger model, GPT-3 (davinci). Detailed results\n(Table 12, appendix) show that there is increased\neffectiveness in every individual benchmark.\n5.3\nPreliminary Study on Detecting\nCommonsense Errors made by ChatGPT\nVERA can be useful in detecting commonsense\nmistakes made by generative LMs in-the-wild. We\ncollected 27 anecdotes from the Internet where\npeople reported ChatGPT making commonsense\nerrors, and manually rewrote them into their correct\nversions, obtaining 54 statements in total.\nWhen detecting incorrect commonsense state-\nments in this dataset, VERA has a precision of 91%\nand a recall of 74%, amounting to an F1 score of\n82%. Table 4 shows how VERA scores some of\nthese these erroneous commonsense statements and\ntheir manually corrected version. In 7 out of the 9\ncases, VERA assigns a low score to the original, in-\ncorrect statement, and a high score to the corrected\nstatement. For example, \u201csince the density of a\nmarble is much less than the density of mercury,\nthe marble would sink to the bottom of the bowl if\nplaced in it\u201d receives a score of 0.04 and is identi-\nfied as an incorrect statement, whereas \u201csince the\ndensity of a marble is much less than the density of\nmercury, the marble would float if placed in mer-\ncury\u201d receives a score of 0.96 and is identified as a\ncorrect statement. Meanwhile, there are also some\nfailure cases. VERA believes that \u201cit is possible for\na solar eclipse to be followed by a lunar eclipse the\nnext day\u201d, and fails to reject that \u201cit is possible to\ndraw a diagonal line in a triangle\u201d.\nDate\nOriginal / Corrected\nScore Pred\n2023/01/05 It is possible for a solar eclipse to be followed by a lunar eclipse the next day.\n0.86\n\u2713\nIt is impossible for a solar eclipse to be followed by a lunar eclipse the next day.\n0.48\n\u2717\n2023/01/06 The time it takes for a given number of cars to travel a fixed distance is directly proportional to the number of cars.\n0.26\n\u2717\nThe time it takes for a given number of cars to travel a fixed distance is invariant of the number of cars.\n0.52\n\u2713\n2023/01/06 If A sits next to B and B sits next to C, then A must sit next to C.\n0.20\n\u2717\nIf A sits next to B and B sits next to C, then A may not sit next to C.\n0.60\n\u2713\n2023/01/10 If two cats can eat two cans of food in a minute, then it would take six cats to eat three cans of food in a minute.\n0.05\n\u2717\nIf two cats can eat two cans of food in a minute, then it would take three cats to eat three cans of food in a minute.\n0.67\n\u2713\n2023/01/11 A three-dimensional cube has eight faces.\n0.46\n\u2717\nA three-dimensional cube has six faces.\n0.70\n\u2713\n2023/01/30 It is possible to draw a diagonal line in a triangle.\n0.80\n\u2713\nIt is impossible to draw a diagonal line in a triangle.\n0.28\n\u2717\n2023/02/21 70 is a smaller number than 58.\n0.14\n\u2717\n70 is a larger number than 58.\n0.85\n\u2713\n2023/02/23 Since the density of a marble is much less than the density of mercury, the marble would sink to the bottom of the\nbowl if placed in it.\n0.04\n\u2717\nSince the density of a marble is much less than the density of mercury, the marble would float if placed in mercury.\n0.96\n\u2713\n2023/02/25 Both a house and a pound of feathers weigh the same, which is one pound.\n0.25\n\u2717\nA house weighs more than one pound, while a pound of feathers weighs one pound.\n0.87\n\u2713\nTable 4: Examples of commonsense mistakes made by ChatGPT, and how VERA can detect them. In each section,\nthe first line is the original, incorrect commonsense statement in ChatGPT\u2019s output, and the second line is the\nauthors\u2019 manually corrected version of the statement. Each statement is followed by VERA\u2019s score and predicted\ncorrectness label. Examples are adapted from Venuto (2023); Marcus and Davis (2023); Borji (2023).\nFigure 3: Ablation results. Average accuracy on the development sets is reported. Components are incrementally\nremoved from the training process, except for the multi-class loss and the binary loss; the hierarchy is indicated in\nthe legend.\n5.4\nAnalysis\nAblations.\nWe conduct an ablation study by\nincrementally removing the following compo-\nnents from the training process: contrastive loss\n(\u00a73.2.3), training stage A (\u00a73.2.4), LM-augmented\nfalsehoods (\u00a73.1), multi-class loss or binary loss\n(\u00a73.2.3). Since at least one of the multi-class loss\nand the binary loss is needed, we remove them sep-\narately and observe the effect of training with a\nsingle loss.\nResults are shown in Figure 3. Overall, the ab-\nlated components have more impact on unseen\nbenchmarks than seen ones. Removing the con-\ntrastive loss hurts performance mostly on unseen\ndatasets, implying that the contrastive objective\nis beneficial for generalization. Removing train-\ning stage A hurts performance across the board,\nemphasizing the importance of training with large-\nscale commonsense knowledge. LM-augmented\nfalsehoods are most helpful on unseen benchmarks,\nwith a little sacrifice in the performance on seen\nbenchmarks. The multi-class loss is most helpful\non multiple-choice benchmarks, while removing\nthe binary loss substantially hurts performance on\nboolean benchmarks.\nScaling Trends of VERA.\nWe trained variants of\nVERA that are based on smaller versions of the T5\nencoder, and show the results in Figure 4. Model\nperformance increases steadily with size, and does\nnot show evidence of saturation at 5B parameters,\nsuggesting that better commonsense plausibility\nestimation models might be yielded from larger\npretrained LMs.\nFigure 4: Scaling trends of commonsense statement verifiers.\nFormat: Verification vs. QA.\nIn this paper, we\nfocus on the verification format to solve common-\nsense problems. A comprehensive discussion on\nhow this format compares with the QA format is\nprovided in \u00a7E and Figure 7.\n6\nRelated Work\nCommonsense verifiers.\nPrior work has ex-\nplored the idea of verifying commonsense state-\nments. SYMBOLIC KNOWLEDGE DISTILLATION\n(West et al., 2021) and I2D2 (Bhagavatula et al.,\n2022) train models to classify the acceptability of\nmodel-generated commonsense statements. The\nENTAILER (Tafjord et al., 2022) model is partially\ntrained to score the validity of a given hypothesis.\nThese models are trained on relatively small-scale,\ndomain-specific data and do not generalize well to\nbroader commonsense domains. Some other work\nuses pretrained LMs with few-shot prompting to\nverify commonsense statements (Kadavath et al.,\n2022; Jung et al., 2022). In this work, we develop\na general-purpose commonsense statement verifier\nthat works out-of-the-box in zero-shot setting.\nVerification in other tasks.\nBeyond common-\nsense statements, the problem of verification has\nbeen extensively studied on various NLP tasks.\nNLI (Liu et al., 2019, 2022a; Zhang et al., 2017)\ncan be viewed as an entailment verification task.\nChen et al. (2021) presents a method for QA ver-\nification by transforming the context passage and\nquestion-answer pair into a premise-hypothesis for-\nmat as in NLI. Some work build models to per-\nform reasoning verification \u2013 classifying whether a\npremise supports or refutes a hypothesis (Bostrom\net al., 2022; Sprague et al., 2022; Yang et al., 2022;\nTafjord et al., 2022). On the other hand, fact veri-\nfication (Thorne et al., 2018; Wadden et al., 2020)\nrequires judging the validity of claims against a cor-\npus of evidence (e.g., Wikipedia). These tasks fea-\nture context-sensitive or knowledge-intensive hy-\npotheses to verify and are typically complemented\nwith additional context.\nIn contrast, we focus\non verifying standalone commonsense statements\nwhere no context is required or provided.\nGeneration vs.\nverification.\nWith the rapid\nprogress in generative LMs, researchers have been\nlargely building general-purpose problem-solving\nmethods with a generative approach (Khashabi\net al., 2020, 2022; Lourie et al., 2021; Tafjord and\nClark, 2021; Wei et al., 2022). However, current\ngenerative LMs are still prone to hallucination er-\nrors and lack an intrinsic mechanism to express\nconfidence level on their outputs. Verification, on\nthe other hand, shows promise to complement these\nshortcomings and has been adopted to improve the\noutcome of generation (Chen et al., 2021; Jiang\net al., 2022). In this work, we take a pure verifica-\ntion approach and build a general-purpose verifier\nfor commonsense statements, which to our best\nknowledge is the first of its kind.\n7\nConclusion and Future Work\nWe introduced VERA, a general-purpose verifica-\ntion model for commonsense statements and an\nearly step toward tools for mitigating common-\nsense errors in text generated by language models.\nVERA achieves state-of-the-art performance when\nsolving commonsense problems in the verification\nformat, excels at filtering LM-generated common-\nsense knowledge statements, and is found useful in\ndetecting erroneous commonsense statements from\ngenerative LMs. Furthermore, the scores produced\nby VERA are well-calibrated; and could be used for\nplausibility score estimation for declarative state-\nments if needed. As VERA mainly targets on single-\nsentence statements, future work may consider ver-\nification of multi-sentence or long-form statements,\nor contextualized/defeasible commonsense state-\nments.\nLimitations\nVERA aims, and is trained, to predict the plausibil-\nity of statements based on objective commonsense\nknowledge of our world. It is not intended to handle\ntext outside the scope of commonsense statements\n(e.g., encyclopedic facts, reading comprehension\nwith fictional worlds). It is not trained or evaluated\non moral commonsense data, so its capability of\nmaking moral predictions is unknown. It gives a\nprediction even if the input falls out of its intended\nscope, which could be mitigated by an additional\nscope guard to determine its applicability. In ad-\ndition, it is not trained to handle very long and\ncompositional input. Although greatly outperform-\ning existing systems, VERA is not perfect and may\nmake incorrect predictions. It is not very robust\nunder syntactic variations of the input, such as para-\nphrases and negations. As the training data may\ncontain bias or toxicity, VERA may also make pre-\ndictions that are perceived as ethically problematic.\nThe output of VERA does not reflect the authors\u2019\nview. VERA is a research prototype, and it is not\ndesigned for making real-world decisions.\nAcknowledgments\nWe thank Sean Welleck, Peter West, Alisa Liu, Jae-\nhun Jung, Chandra Bhagavatula, Ram Pasunuru,\nAsli Celikyilmaz, and members of the H2lab, Xlab\nand ARK lab for their discussion and construc-\ntive feedback. This work was funded in part by\nthe DARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), NSF IIS-2044660, and ONR\nN00014-18-1-2826. We thank OpenAI for offering\naccess to their API.\nReferences\nStephane T Aroca-Ouellette, Cory Paik, Alessandro\nRoncone, and Katharina Kann. 2021. Prost: Physical\nreasoning about objects through space and time. In\nFindings.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Scott Yih, and Yejin\nChoi. 2019.\nAbductive commonsense reasoning.\nArXiv, abs/1908.05739.\nChandra Bhagavatula, Jena D. Hwang, Doug Downey,\nRonan Le Bras, Ximing Lu, Keisuke Sakaguchi,\nSwabha Swayamdipta,\nPeter West,\nand Yejin\nChoi. 2022.\nI2d2:\nInductive knowledge distil-\nlation with neurologic and self-imitation.\nArXiv,\nabs/2212.09246.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter Clark. 2020. Genericskb: A knowledge base of\ngeneric statements. ArXiv, abs/2005.00660.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2019. Piqa: Reasoning about\nphysical commonsense in natural language. ArXiv,\nabs/1911.11641.\nAli Borji. 2023. A categorical archive of chatgpt fail-\nures. ArXiv, abs/2302.03494.\nKaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and\nGreg Durrett. 2022.\nNatural language deduction\nthrough search over statement compositions. In Con-\nference on Empirical Methods in Natural Language\nProcessing.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165.\nJifan Chen, Eunsol Choi, and Greg Durrett. 2021. Can\nnli models verify qa systems\u2019 predictions? ArXiv,\nabs/2104.08731.\nMichael Chen, Mike D\u2019Arcy, Alisa Liu, Jared Fernan-\ndez, and Doug Downey. 2019. Codah: An adversar-\nially authored question-answer dataset for common\nsense. arXiv preprint arXiv:1904.04365.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph,\nYi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Wei\nYu, Vincent Zhao, Yanping Huang, Andrew M.\nDai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi,\nJeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V. Le, and Jason Wei. 2022.\nScal-\ning instruction-finetuned language models. ArXiv,\nabs/2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nErnest Davis. 2023.\nBenchmarks for automated\ncommonsense reasoning:\nA survey.\nArXiv,\nabs/2302.04752.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346\u2013\n361.\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa\nRoemmele. 2011. Semeval-2012 task 7: Choice of\nplausible alternatives: An evaluation of common-\nsense causal reasoning. In International Workshop\non Semantic Evaluation.\nSylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp\nSchmid, Zachary Mueller, and Sourab Mangrulkar.\n2022. Accelerate: Training and inference at scale\nmade simple, efficient and adaptable.\nhttps://\ngithub.com/huggingface/accelerate.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks.\nIn International Conference on Machine\nLearning.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. In AAAI\nConference on Artificial Intelligence.\nAlbert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,\nWenda Li, Jiacheng Liu, Mateja Jamnik, Timo-\nth\u00e9e Lacroix, Yuhuai Wu, and Guillaume Lam-\nple. 2022. Draft, sketch, and prove: Guiding for-\nmal theorem provers with informal proofs. ArXiv,\nabs/2210.12283.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nSaurav Kadavath, Tom Conerly, Amanda Askell, T. J.\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zachary Dodds, Nova DasSarma, Eli Tran-\nJohnson, Scott Johnston, Sheer El-Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, John\nKernion, Shauna Kravec, Liane Lovitt, Kamal\nNdousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom B. Brown, Jack Clark, Nicholas Joseph,\nBenjamin Mann, Sam McCandlish, Christopher Olah,\nand Jared Kaplan. 2022. Language models (mostly)\nknow what they know. ArXiv, abs/2207.05221.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-\njishirzi. 2022.\nUnifiedqa-v2:\nStronger general-\nization via broader cross-format training.\nArXiv,\nabs/2202.12359.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single qa system. In Findings.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna,\nYonglong\nTian,\nPhillip\nIsola,\nAaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. In Advances in Neural\nInformation Processing Systems, volume 33.\nTushar Khot, Peter Clark, Michal Guerquin, Pe-\nter Alexander Jansen, and Ashish Sabharwal. 2019.\nQasc: A dataset for question answering via sentence\ncomposition. ArXiv, abs/1910.11473.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam:\nA method for stochastic optimization.\nCoRR,\nabs/1412.6980.\nHector J. Levesque, Ernest Davis, and L. Morgenstern.\n2011. The winograd schema challenge. In Interna-\ntional Conference on Principles of Knowledge Rep-\nresentation and Reasoning.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-\ntrained language models. ArXiv, abs/2005.00683.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL, pages 3214\u20133252.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022a. Wanli: Worker and ai collabora-\ntion for natural language inference dataset creation.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing.\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He,\nSean Welleck, Hannaneh Hajishirzi, and Yejin Choi.\n2022b. Rainier: Reinforced knowledge introspector\nfor commonsense question answering. In Conference\non Empirical Methods in Natural Language Process-\ning.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2021. Generated knowledge prompting\nfor commonsense reasoning. ArXiv, abs/2110.08387.\nXiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao.\n2022c.\nThings not written in text:\nExploring\nspatial commonsense from visual signals. ArXiv,\nabs/2203.08075.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Unicorn on rainbow: A\nuniversal commonsense reasoning model on a new\nmultitask benchmark. In AAAI Conference on Artifi-\ncial Intelligence.\nGary Marcus and Ernest Davis. 2023.\nChat-\ngpt/llm errors (public).\nhttps://docs.google.\ncom/spreadsheets/d/1kDSERnROv5FgHbVN8z_\nbXH9gak2IXRtoqz0nwhrviCw/edit.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing.\nN. Mostafazadeh, Nathanael Chambers, Xiaodong He,\nDevi Parikh, Dhruv Batra, Lucy Vanderwende, Push-\nmeet Kohli, and James F. Allen. 2016. A corpus and\ncloze evaluation for deeper understanding of com-\nmonsense stories. In North American Chapter of the\nAssociation for Computational Linguistics.\nMahdi Pakdaman Naeini, Gregory F. Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated proba-\nbilities using bayesian binning. Proceedings of the\n... AAAI Conference on Artificial Intelligence. AAAI\nConference on Artificial Intelligence, 2015:2901\u2013\n2907.\nYasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi,\nand Greg Durrett. 2021. Creak: A dataset for com-\nmonsense reasoning over entity knowledge. ArXiv,\nabs/2109.01653.\nOpenAI. 2022a. Introducing chatgpt.\nOpenAI. 2022b. Models - overview - gpt-3.5.\nOpenAI. 2023. Gpt-4 technical report. Technical report,\nOpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commun.\nACM, 64:99\u2013106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le\nBras, and Yejin Choi. 2019. Social iqa: Common-\nsense reasoning about social interactions.\nArXiv,\nabs/1904.09728.\nShikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormo-\nlabashi, Te-Lin Wu, Xuezhe Ma, and Nanyun Peng.\n2021. Com2sense: A commonsense reasoning bench-\nmark with complementary sentences. In Findings.\nZayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and\nGreg Durrett. 2022. Natural language deduction with\nincomplete information. In Conference on Empirical\nMethods in Natural Language Processing.\nOyvind Tafjord and Peter Clark. 2021.\nGeneral-\npurpose question-answering with macaw.\nArXiv,\nabs/2109.02593.\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen tau\nYih, and Ashish Sabharwal. 2018. Quarel: A dataset\nand models for answering questions about qualitative\nrelationships. ArXiv, abs/1811.08048.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2022.\nEntailer: Answering questions with faithful and truth-\nful chains of reasoning. In Conference on Empirical\nMethods in Natural Language Processing.\nOyvind Tafjord,\nMatt Gardner,\nKevin Lin,\nand\nPeter Clark. 2019.\nQuartz:\nAn open-domain\ndataset of qualitative relationship questions. ArXiv,\nabs/1909.03553.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. ArXiv, abs/1811.00937.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chan-\ndrasekhar Bhagavatula, Yoav Goldberg, Yejin Choi,\nand Jonathan Berant. 2021. Commonsenseqa 2.0:\nExposing the limits of ai through gamification. ArXiv,\nabs/2201.05320.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFever: a large-scale dataset for fact extraction and\nverification. ArXiv, abs/1803.05355.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nGiuseppe Venuto. 2023. chatgpt-failures. https://\ngithub.com/giuven95/chatgpt-failures.\nDavid Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan\nLin, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. ArXiv, abs/2004.14974.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yilong\nWang, Xiaodan Zhu, and Yue Zhang. 2020. Semeval-\n2020 task 4: Commonsense validation and explana-\ntion. In International Workshop on Semantic Evalua-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou.\n2022. Chain of thought prompting elicits reasoning\nin large language models. ArXiv, abs/2201.11903.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nArXiv, abs/1707.06209.\nPeter West, Chandrasekhar Bhagavatula, Jack Hessel,\nJena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing\nLu, Sean Welleck, and Yejin Choi. 2021. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In North American\nChapter of the Association for Computational Lin-\nguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nKaiyu Yang, Jia Deng, and Danqi Chen. 2022. Gen-\nerating natural language proofs with verifier-guided\nsearch. In Conference on Empirical Methods in Nat-\nural Language Processing.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Conference\non Empirical Methods in Natural Language Process-\ning.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a ma-\nchine really finish your sentence? In Annual Meeting\nof the Association for Computational Linguistics.\nSheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-\njamin Van Durme. 2017. Ordinal common-sense\ninference. Transactions of the Association for Com-\nputational Linguistics, 5:379\u2013395.\nA\nMore Details on Datasets\nTable 6 shows more dataset statistics, and Table 7\nshows the dataset citations and links from which\nwe retrieved the datasets.\nA.1\nDataset-Specific Special Handling\nFor some datasets, we pre-process them into a uni-\nfied multiple-choice or boolean format. We provide\nthe details below.\nCom2Sense (paired).\nCom2Sense contains true\nand false statements that can be paired into com-\nplements. To utilize this pairing information, we\nplace the two statements in each pair into the same\nstatement group, and treat this as a multiple-choice\ndataset. Some statements in the dev set are not\npaired, so we discarded these examples.\nCycIC (mc).\nCycIC contains both multiple-\nchoice and boolean QA problems. To keep con-\nsistency in evaluation, we use only the multiple-\nchoice problems, which is the dominant problem\ntype in this dataset.\nComVE (task A).\nComVE contains data for\nthree tasks. Task A is assigning true/false labels to\npaired statements, similar to Com2Sense (paired).\nTask B and C are about choosing and generating\nexplanations to a given statement being against\ncommonsense. We use the data for task A.\nSKD (annotated).\nThe annotated dataset of\nSymbolic Knowledge Distillation (SKD) contains\nLM-generated, semi-structured knowledge triples,\nwhere the head and tail events are connected by\nrelations, such as\n(PersonX doesn\u2019t like to wait, xIntent,\nto get the job done).\nFollowing West et al. (2021), we replace the name\nplaceholders with random person names, and con-\nvert into natural language statements using tem-\nplates adapted from Hwang et al. (2020). For ex-\nample, the triple in the above example becomes\nArnold doesn\u2019t like to wait.\nBecause\nArnold wanted to get the job done.\nWe set the correctness label to be true iff the valid\nfield has a positive value.\nI2D2 (annotated).\nThe annotated dataset of I2D2\ncontains LM-generated commonsense statements\nwith human-annotated correctness labels. We use\nthe combination of annotated data in \u201cIter0\u201d and\n\u201cIter2\u201d, because the data of \u201cIter1\u201d is missing from\nthe website.\nA.2\nConversion to Declarative Statements\nFrom QA datasets, we create declarative statements\nfrom QA problems using the following method:\n\u2022 If the problem contains a question, we con-\nvert the question and choice into a declara-\ntive statement using the question conversion\nmodel created by Chen et al. (2021).\n\u2022 If the question is cloze-style, we replace the\nblank with the choice.\n\u2022 If the question is an incomplete sentence and\nthe choice is a continuation to it, we concate-\nnate the question and the choice.\n\u2022 If there is no question and the problem only\nasks to choose between some choices, we use\nthe choice as the declarative statement.\n\u2022 For boolean problems, we always use yes as\nthe choice and create a single declarative state-\nment for each problem. We use the original\nlabel as the correctness label of this statement.\nB\nMore Details on Method\nB.1\nTraining Objectives\nBinary classification loss.\nWe defined the binary\nclassification loss as\nLbin(xi, yi) =\n\u2212 yi log s(xi) \u2212 (1 \u2212 yi) log(1 \u2212 s(xi)).\nTo account for the fact that there are usually more\nincorrect statements than correct ones in the data\nproduced from multiple-choice datasets, we di-\nvide this loss by the number of statements with\nthe same correctness label in the same statement\ngroup. Therefore, the binary classification loss for\nthe whole batch is\nLbin =\n1\nBG\nBG\nX\nj=1\nX\ny\u2208{0,1}\nPCj\nc=1 I[yjc = y]Lbin(xjc, yjc)\nPCj\nc=1 I[yjc = y]\n,\nwhere Cj is the number of statements in statement\ngroup Xj, xjc is the cth statement in Xj, and I is\nthe indicator function.\nMulti-class loss.\nWe defined the multi-class loss\nas\nLmc(Xj) = \u2212 log\nexp z(xj\u2217)\nPCj\nc=1 exp z(xjc)\n.\nThe multi-class loss for the whole batch is\nLmc =\n1\nBG\nBG\nX\nj=1\nLmc(Xj).\nSupervised contrastive loss.\nWe defined the su-\npervised contrastive loss as\nLctr(xi, yi) = \u2212 log\nP\nk\u2208P(i) e\ncos(h(xi),h(xk))\n\u03c4\nP\nk\u2208P(i)\u222aN(i) e\ncos(h(xi),h(xk))\n\u03c4\n.\nThe supervised contrastive loss for the whole batch\nis\nLctr = 1\nBS\nBS\nX\ni=1\nLctr(xi, yi).\nB.2\nCalibration\nOur calibration is a post-hoc strategy and does not\naffect the task performance metrics we report in \u00a75.\nThis is because applying our calibration method \u2013\ntemperature scaling \u2013 does not affect the relative\norder of plausibility scores assigned to a given set\nof statements:\n\u2022 For tasks with multiple-choice questions\n(\u00a75.1), calibration does not affect the argmax\nprediction for the above reason.\n\u2022 For commonsense knowledge filtering (\u00a75.2),\ncalibration does not affect the TPR/FPR num-\nbers at each corresponding decision point,\nagain for the above reason, so the ROC curves\nare valid.\n\u2022 For True/False judgment problems (\u00a75.1 and\n\u00a75.3), calibration does not move the plausibil-\nity scores across the decision boundary. We\nuse logit z = 0.0 (or equivalently, plausibility\nscore s = 0.5) as the True/False boundary. A\npositive (or negative) logit remains positive\n(or negative) after applying the temperature.\nC\nMore Details on Experimental Setup\nTable 8 shows the hyperparamter settings for train-\ning VERA. These values are obtained from some\nmoderate hyperparameter tuning, and we did not\ndo extensive search due to training cost.\nFor tokenization, the T5 tokenizer tokenizes in-\nput so that it ends with the EOS token </s> (token\nID = 1). We manually configured the LLaMA tok-\nenizer so that its output ends with the EOS token\n</s> (token ID = 2), and does not contain the BOS\ntoken <s> (token ID = 1). Models are trained for\nS = 50k steps with BG = 64 statement groups\nper batch, using the Adam optimizer (Kingma and\nBa, 2014) with learning rate \u03b7 = 1 \u00d7 10\u22125 for\nT5 encoder and \u03b7 = 2 \u00d7 10\u22126 for LLaMA. We\ntrain models with the Huggingface Transformers\nand Accelerate libraries (Wolf et al., 2019; Gug-\nger et al., 2022). For memory efficiency, during\ntraining, each statement is truncated to 128 tokens\n(which can accommodate more than 99% of the\nstatements; see Table 6) and each statement group\nis capped to four statements.\nC.1\nDefinition of Metrics\nMultiple-choice accuracy.\nFor multiple-choice\nbenchmarks, we report the multiple-choice accu-\nracy:\nAccmc =\n1\n|D|\nX\nXj\u2208D\nI[xj\u2217 = arg max\nxjc\u2208Xj\ns(xjc)].\nBoolean accuracy.\nThe boolean accuracy is de-\nfined as\nAccbool =\n1\n|D|\nX\n(xi,yi)\u2208D\nI\n\u0002\nyi = I[z(xi) > 0]\n\u0003\n.\nBoolean accuracy is applicable to balanced\nboolean benchmarks where there are roughly equal\ntrue and false statements (e.g., CommonsenseQA\n2.0, Spatial Commonsense, StrategyQA, CREAK).\nGenerally it is not a good metric for multiple-\nchoice benchmarks and unbalanced boolean bench-\nmarks.\nAUROC and AP.\nFor unbalanced boolean bench-\nmarks (e.g., LM-generated knowledge filtering\ndatasets), accuracy may not faithfully capture the\nmodel\u2019s performance. Instead, the metrics we use\nare the area under the ROC curve (AUROC) and\nthe average precision (AP) for selecting the True\nstatements. Statements are ranked based on their as-\nsigned raw scores, so that different score thresholds\ncan be selected to construct the ROC and Precision-\nRecall curves. Aside from unbalanced boolean\nbenchmarks, AUROC and AP are also applicable\nto multiple-choice and balanced boolean bench-\nmarks.\nCalibration.\nTo measure how well the verifier-\npredicted score reflects its confidence, we measure\nthe ECE (Naeini et al., 2015) on the boolean bench-\nmarks. ECE is computed as\nECE =\nM\nX\nm=1\n|Bm|\n|D| \u00b7\n\f\f\fAcc(Bm) \u2212 Score(Bm)\n\f\f\f\n=\nM\nX\nm=1\n|Bm|\n|D| \u00b7\n\f\f\f\n1\n|Bm|\nX\n(xi,yi)\u2208Bm\nI[yi = 1]\n\u2212\n1\n|Bm|\nX\n(xi,yi)\u2208Bm\ns(xi)\n\f\f\f,\n(1)\nwhere M is the number of bins which bucket data\npoints with similar predictions, and Bm \u2286 D is\nthe subset of data points that fall into the m-th bin.\nWe use M = 10 equal-sized bins when computing\nECE.\nC.2\nDetails on Baseline Models\nSKD Critic.\nWest et al. (2021) trained a critic\nmodel that filters incorrect commonsense knowl-\nedge generated by their symbolic knowledge distil-\nlation (SKD) method. This critic model is based on\nRoBERTa-large (Liu et al., 2019) and is finetuned\non 8k GPT-3-generated commonsense knowledge\nsentences with human-annotated true/false labels.\nThe model predicts a [0, 1] score s which we use\nas the final score, and we let the logit z = \u03c3\u22121(s).\nI2D2 Critic.\nBhagavatula et al. (2022) trained\na critic model that filters incorrect commonsense\nknowledge generated by their I2D2 method. This\ncritic model is based on RoBERTa-large (Liu et al.,\n2019) and is finetuned on 12k I2D2-generated\ncommonsense knowledge sentences with human-\nannotated true/false labels. Given an input state-\nment, the model predicts two logits: t for the True\nlabel and f for the False label. We let the logit\nz = t \u2212 f and the score s = \u03c3(t \u2212 f). We use the\ncritic model trained in the final iteration (i.e., \u201cIter\n2\u201d in I2D23).\nUnifiedQA-v2.\nUnifiedQA-v2 (Khashabi et al.,\n2022) is a general-purpose QA model trained on\ndatasets with a variety of input formats, including\nboolean datasets. When the input is a declarative\nstatement, the model is trained to output either \u201cyes\u201d\n3https://gengen.apps.allenai.org/\nor \u201cno\u201d. We use this feature of the model and make\nit act as a commonsense statement verifier. For an\ninput statement, we compute the logits received by\n\u201cyes\u201d and \u201cno\u201d in the decoder, denoted as t and f,\nrespectively. We let the logit z = t \u2212 f and the\nscore s = \u03c3(t \u2212 f). We use the largest version of\nthis model, UnifiedQA-v2-11b.4\nEntailer.\nEntailer (Tafjord et al., 2022) is a model\ntrained to construct proof trees for scientific com-\nmonsense hypotheses. This multi-angle model can\nbe used in three ways: (1) given a hypothesis,\ngenerate a set of premises that may entail it; (2)\ngiven a hypothesis, predict a score that reflects the\nmodel\u2019s belief in it; (3) given a hypothesis and set\nof premises, predict a score that reflects whether\nthere is a valid entailment between them.\nWe\nuse (2) as a commonsense statement verifier. The\nmodel predicts a [0, 1] score s which we use as the\nfinal score, and we let the logit z = \u03c3\u22121(s). We use\nthe largest version of this model, Entailer-11b.5\nGPT-3.5.\nGPT-3.5 (OpenAI, 2022b) is a series of\ngeneral-purpose autoregressive decoder-only LMs.\nTo make it act as a commonsense verifier, we use\nthe following input prompt:\nQuestion: Based on commonsense knowledge, is the following\nstatement correct? Please answer yes or no.\nStatement: {statement}\nAnswer:\nWe query the OpenAI Completions API6 with this\nprompt and compute the logits received by \u201c Yes\u201d\nand \u201c No\u201d in the next-token prediction, denoted as\nt and f, respectively. We let the logit z = t \u2212 f\nand the score s = \u03c3(t \u2212 f). We experimented with\nseveral prompt formats and found the one presented\nabove to have the best performance, and in most\ncases, \u201c Yes\u201d and \u201c No\u201d together receive most of\nthe probability mass during next-token prediction.\nWe also experimented with several models in the\nGPT-3 (Brown et al., 2020) and GPT-3.5 series, and\nfound GPT-3.5 (text-davinci-002) to work the\nbest.\nAdditionally, we report a baseline where the\n(negated) language modeling perplexity is used for\n4https://huggingface.co/allenai/\nunifiedqa-v2-t5-11b-1251000\n5https://huggingface.co/allenai/entailer-11b\n6https://platform.openai.com/docs/\napi-reference/completions\ncommonsense plausibility. Note that the plausibil-\nity scores derived this way are not normalized, and\nwe only use them for ranking purposes. For this\nbaseline, we use GPT-3.5 (text-davinci-002)\nas the base model, and name it as \u201cPPL (GPT-3.5)\u201d.\nChatGPT and GPT-4.\nChatGPT (OpenAI,\n2022a) and GPT-4 (OpenAI, 2023) are optimized\nfor chat. To make them act as a commonsense veri-\nfier, we use the same input prompt as for GPT-3.5,\nwithout the \u201cAnswer:\u201d line. We query the OpenAI\nChat API7 with this prompt in a user message, and\nobtain the first token of the assistant message in\nthe response. Besides this zero-shot setting, we ad-\nditionally report a few-shot chain-of-thought (Wei\net al., 2022) setting with 5 in-domain examples, for-\nmatted as additional user-assistant message pairs\nprior to the query user message.\nSince the API does not provide token logits, we\nlet the score s = 1.0 when this token is \u201cYes\u201d,\nand s = 0.0 when this token is \u201cNo\u201d. In the un-\nlikely case that this token is neither, we let s = 0.5.\nWe add a small random noise to the score. This\nis to arbitrate potentially multiple positive predic-\ntions within statement groups from multiple-choice\nQA problems, and to enable plotting the ROC and\nprecision-recall curves. Note that this is not an\nideal solution and may cause under-estimation of\nChatGPT and GPT-4\u2019s performance.\nFlan-T5.\nFlan-T5 (Chung et al., 2022) is a series\nof sequence-to-sequence LMs instruction-finetuned\non massive number of tasks. To make it act as\na commonsense verifier, we use the same input\nprompt as for GPT-3.5. We compute the logits\nreceived by \u201cyes\u201d and \u201cno\u201d in the first token predic-\ntion in the decoder, denoted as t and f, respectively.\nWe let the logit z = t\u2212f and the score s = \u03c3(t\u2212f).\nWe experimented with several prompt formats and\nfound the one presented above to have the best\nperformance, and in most cases, \u201cyes\u201d and \u201cno\u201d to-\ngether receive most of the probability mass during\nthe token prediction. We use the largest version of\nthis model, Flan-T5-XXL.8 Note that some unseen\nbenchmarks are in the training data of Flan-T5; see\nTable 7 for details on data contamination.\n7https://platform.openai.com/docs/\napi-reference/chat\n8https://huggingface.co/google/flan-t5-xxl\nD\nMore Evaluation Results\nFigure 5 is an expansion of Table 2 and addition-\nally shows the precision-recall curves on problem-\nsolving benchmarks. Table 9, Table 10, and Ta-\nble 11 show the per-dataset breakdown of the accu-\nracy numbers in Figure 5. Figure 6 is an expansion\nof Figure 2 and additionally shows the precision-\nrecall curves on knowledge-filtering benchmarks.\nTable 12 shows the per-dataset breakdown of the\naccuracy numbers in Table 3.\nE\nFurther Analysis\nFormat: Verification vs. QA.\nIn this paper, we\nhave been using the verification format to approach\nproblem-solving tasks. But do we lose something\nwhen compared to using the QA format? In Fig-\nure 7 we compare how well existing models can\nsolve problems in the verification format and the\nQA format. Verification format does fall behind\nQA format, especially with models trained exclu-\nsively in QA format (i.e., UnifiedQA-v2). We also\ntrained a sequence-to-sequence model in QA for-\nmat on the same multiple-choice data as VERA.\nIt leads VERA by 1.5% on seen multiple-choice\nbenchmarks. We hypothesize that this is because\nverification models only see one option at a time,\nwhereas QA models can see all choices of a prob-\nlem at the same time and thus can do comparative\nranking.\nIn addition to the performance loss, a verification\nmodel does lose the generative capability possessed\nby some QA models that are generative (e.g., Uni-\nfiedQA in Khashabi et al. (2020)), and it has to run\nC times to solve a C-way multiple-choice problem,\nwhereas QA models (e.g., UnifiedQA in Khashabi\net al. (2020), Unicorn in Lourie et al. (2021)) need\nto run only once.\nHowever, verification models can perform some\ntasks that generative QA models cannot cover.\nThey can classify the correctness of declarative\nstatements, without having to convert them into\nquestions in the first place. They can also reflect\non the answer produced by a generative QA model,\nand provide a level of confidence. We argue that\nverification models and generative QA models have\ndifferent best-application scenarios and are some-\ntimes complementary to each other.\nAbbr.\nName\nDomain\nFormat\n# Train Ex. Aug # Dev Ex. # Statements\n# True # False\nSTAGE A TRAINING\nAtomic2020\nAtomic2020\nmultiple-choice (4)\n803541\n70731\n282924\n70731 212193\nGenericsKB\nGenericsKB\nmultiple-choice (4)\n775820\n96977\n387908\n96977 290931\nTotal\n1579361\n167708\n670832 167708 503124\nSTAGE B TRAINING (SEEN)\nOBQA\nOpenBookQA\nscientific\nmultiple-choice (4)\n4957\n\u2713\n500\n2000\n500\n1500\nARC_e\nARC (easy)\nscientific\nmultiple-choice (4)\n2251\n\u2713\n570\n2281\n570\n1711\nARC_h\nARC (hard)\nscientific\nmultiple-choice (4)\n1119\n\u2713\n299\n1194\n299\n895\nAI2Sci_e\nAI2 Science (elem)\nscientific\nmultiple-choice (4)\n623\n\u2713\n123\n489\n123\n366\nAI2Sci_m\nAI2 Science (middle)\nscientific\nmultiple-choice (4)\n605\n\u2713\n125\n502\n125\n377\nCSQA\nCommonsenseQA\ngeneral\nmultiple-choice (5)\n9741\n\u2713\n1221\n6099\n1221\n4878\nQASC\nQASC\nscientific\nmultiple-choice (8)\n8134\n\u2713\n926\n7408\n926\n6482\nPIQA\nPhysical IQA\nphysical\nmultiple-choice (2)\n16113\n1838\n3676\n1838\n1838\nSIQA\nSocial IQA\nsocial\nmultiple-choice (3)\n33410\n\u2713\n1954\n5861\n1954\n3907\nWG\nWinogrande\ngeneral\nmultiple-choice (2)\n40398\n1267\n2534\n1267\n1267\nC2S\nCom2Sense (paired)\ngeneral\nmultiple-choice (2)\n804\n391\n782\n391\n391\nSciQ\nSciQ\nscientific\nmultiple-choice (4)\n11679\n\u2713\n1000\n4000\n1000\n3000\nQuaRel\nQuaRel\nqualitative\nmultiple-choice (2)\n1941\n278\n556\n278\n278\nQuaRTz\nQuaRTz\nqualitative\nmultiple-choice (2)\n2696\n384\n768\n384\n384\nCycIC\nCycIC (mc)\ngeneral\nmultiple-choice (5)\n6521\n907\n4535\n907\n3628\nComVE\nComVE (task A)\ngeneral\nmultiple-choice (2)\n10000\n997\n1994\n997\n997\nCSQA2\nCommonsenseQA 2.0\ngeneral\nboolean\n9264\n2541\n2541\n1225\n1316\nSKD_anno\nSKD (annotated)\nboolean\n7980\n1015\n1015\n803\n212\nI2D2_anno\nI2D2 (annotated)\nboolean\n26206\n13094\n13094\n6158\n6936\nTotal\n194442\n29430\n61329\n20966\n40363\nEVALUATION (UNSEEN TYPE 1)\nWSC\nWSC\ngeneral\nmultiple-choice (2)\n0\n273\n546\n273\n273\nCOPA\nCOPA\ngeneral\nmultiple-choice (2)\n0\n500\n1000\n500\n500\nNumerSense\nNumerSense\nquantitative multiple-choice (11)\n0\n200\n2200\n200\n2000\nPROST\nPROST\nphysical\nmultiple-choice (4)\n0\n18736\n74944\n18736\n56208\nSpatialCS\nSpatial Commonsense\nphysical\nboolean\n0\n1448\n1448\n724\n724\nRainier_anno Rainier (annotated)\nboolean\n0\n591\n591\n424\n167\nTotal\n0\n21748\n80729\n20857\n59872\nEVALUATION (UNSEEN TYPE 2)\nSWAG\nSWAG\nmultiple-choice (4)\n0\n20006\n80024\n20006\n60018\nHellaSwag\nHellaSwag\nmultiple-choice (4)\n0\n10042\n40168\n10042\n30126\nCODAH\nCODAH\nmultiple-choice (4)\n0\n2776\n11104\n2776\n8328\nSCT\nStory Cloze Test\nmultiple-choice (2)\n0\n1871\n3742\n1871\n1871\n\u03b1NLI\n\u03b1NLI\nmultiple-choice (2)\n0\n1532\n3064\n1532\n1532\nStrategyQA\nStrategyQA\nboolean\n0\n229\n229\n107\n122\nCREAK\nCREAK\nboolean\n0\n1371\n1371\n691\n680\nTotal\n0\n37827\n139702\n37025 102677\nTable 5: Datasets and statistics. Data sourced from commonsense KBs are listed under STAGE A TRAINING, and\ndata sourced from commonsense QA datasets are listed under STAGE B TRAINING. The number in parentheses\nunder the Format column represents the number of choices per question. The Aug column indicates whether\nLM-augmented falsehoods are generated for each dataset. The last three columns are the number of total, correct\nand incorrect statements in the development set. See Table 6 for more dataset statistics, and Table 7 for full citations\nand sources for these datasets.\nAbbr.\nName\nStatement Length\nmin median 90% 95% 99% max\nSTAGE A TRAINING\nAtomic2020\nAtomic2020\n5\n19\n24\n26\n30\n57\nGenericsKB\nGenericsKB\n4\n13\n22\n24\n28\n82\nSTAGE B TRAINING (SEEN)\nOBQA\nOpenBookQA\n5\n16\n29\n36\n56\n74\nARC_e\nARC (easy)\n6\n24\n50\n60\n86\n111\nARC_h\nARC (hard)\n7\n30\n59\n70\n94\n138\nAI2Sci_e\nAI2 Science (elem)\n7\n29\n63\n79\n455\n473\nAI2Sci_m\nAI2 Science (middle)\n7\n24\n58\n72\n511\n536\nCSQA\nCommonsenseQA\n5\n18\n28\n32\n43\n73\nQASC\nQASC\n5\n13\n19\n21\n24\n30\nPIQA\nPhysical IQA\n5\n26\n62\n80\n120\n256\nSIQA\nSocial IQA\n10\n28\n38\n41\n51\n70\nWG\nWinogrande\n17\n24\n31\n34\n38\n42\nC2S\nCom2Sense (paired)\n12\n24\n34\n38\n44\n55\nSciQ\nSciQ\n6\n19\n29\n34\n48\n75\nQuaRel\nQuaRel\n15\n39\n62\n80\n101\n107\nQuaRTz\nQuaRTz\n9\n29\n44\n49\n73\n78\nCycIC\nCycIC (mc)\n6\n31\n59\n67\n97\n122\nComVE\nComVE (task A)\n4\n10\n14\n16\n20\n28\nCSQA2\nCommonsenseQA 2.0\n5\n14\n24\n29\n38\n58\nSKD_anno\nSKD (annotated)\n13\n20\n25\n27\n31\n37\nI2D2_anno\nI2D2 (annotated)\n5\n15\n21\n24\n31\n41\nEVALUATION (UNSEEN TYPE 1)\nWSC\nWSC\n10\n22\n33\n39\n45\n48\nCOPA\nCOPA\n10\n17\n21\n23\n26\n28\nNumerSense\nNumerSense\n6\n13\n20\n23\n32\n36\nPROST\nPROST\n17\n42\n63\n68\n78\n78\nSpatialCS\nSpatial Commonsense\n9\n12\n17\n18\n19\n20\nRainier_anno Rainier (annotated)\n5\n12\n19\n21\n29\n33\nEVALUATION (UNSEEN TYPE 2)\nSWAG\nSWAG\n12\n30\n46\n52\n67\n148\nHellaSwag\nHellaSwag\n15\n103\n134\n140\n149\n181\nCODAH\nCODAH\n5\n21\n31\n34\n45\n73\nSCT\nStory Cloze Test\n29\n56\n69\n72\n78\n89\n\u03b1NLI\n\u03b1NLI\n17\n35\n44\n47\n53\n65\nStrategyQA\nStrategyQA\n6\n14\n20\n22\n25\n30\nCREAK\nCREAK\n8\n14\n20\n22\n29\n50\nTable 6: More dataset statistics. This table shows the percentiles of statement lengths (as in number of T5 tokens) in\neach dataset.\nAbbr.\nName\nCitation\nLink\nIn Flan-T5?\nSTAGE A TRAINING\nAtomic2020\nAtomic2020\nHwang et al. (2020)\nhttps://allenai.org/data/atomic-2020\nyes\nGenericsKB\nGenericsKB\nBhakthavatsalam et al. (2020) https://allenai.org/data/genericskb\nyes\nSTAGE B TRAINING (SEEN)\nOBQA\nOpenBookQA\nMihaylov et al. (2018)\nhttps://github.com/allenai/unifiedqa\nyes\nARC_e\nARC (easy)\nClark et al. (2018)\nhttps://github.com/allenai/unifiedqa\nyes\nARC_h\nARC (hard)\nClark et al. (2018)\nhttps://github.com/allenai/unifiedqa\nyes\nAI2Sci_e\nAI2 Science (elem)\nClark et al. (2018)\nhttps://github.com/allenai/unifiedqa\nno\nAI2Sci_m\nAI2 Science (middle)\nClark et al. (2018)\nhttps://github.com/allenai/unifiedqa\nno\nCSQA\nCommonsenseQA\nTalmor et al. (2019)\nhttps://github.com/allenai/unifiedqa\nyes\nQASC\nQASC\nKhot et al. (2019)\nhttps://github.com/allenai/unifiedqa\nyes\nPIQA\nPhysical IQA\nBisk et al. (2019)\nhttps://github.com/allenai/unifiedqa\nyes\nSIQA\nSocial IQA\nSap et al. (2019)\nhttps://github.com/allenai/unifiedqa\nyes\nWG\nWinogrande\nSakaguchi et al. (2019)\nhttps://github.com/allenai/unifiedqa\nyes\nC2S\nCom2Sense (paired)\nSingh et al. (2021)\nhttps://github.com/PlusLabNLP/Com2Sense/tree/\nmaster/data\nyes\nSciQ\nSciQ\nWelbl et al. (2017)\nhttps://allenai.org/data/sciq\nyes\nQuaRel\nQuaRel\nTafjord et al. (2018)\nhttps://allenai.org/data/quarel\nyes\nQuaRTz\nQuaRTz\nTafjord et al. (2019)\nhttps://allenai.org/data/quartz\nyes\nCycIC\nCycIC (mc)\n\u2013\nhttps://leaderboard.allenai.org/cycic/\nsubmissions/get-started\nno\nComVE\nComVE (task A)\nWang et al. (2020)\nhttps://github.com/wangcunxiang/\nSemEval2020-Task4-Commonsense-Validation-and-Explanation\nno\nCSQA2\nCommonsenseQA 2.0 Talmor et al. (2021)\nhttps://github.com/allenai/csqa2/tree/master/\ndataset\nno\nSKD_anno\nSKD (annotated)\nWest et al. (2021)\nhttps://github.com/peterwestai2/\nsymbolic-knowledge-distillation/tree/main/\npurification_code\nno\nI2D2_anno\nI2D2 (annotated)\nBhagavatula et al. (2022)\nhttps://gengen.apps.allenai.org\nno\nEVALUATION (UNSEEN TYPE 1)\nWSC\nWSC\nLevesque et al. (2011)\nhttps://huggingface.co/datasets/winograd_wsc\nyes\nCOPA\nCOPA\nGordon et al. (2011)\nhttps://huggingface.co/datasets/super_glue\nyes\nNumerSense\nNumerSense\nLin et al. (2020)\nhttps://github.com/INK-USC/NumerSense/tree/main/\ndata\nyes\nPROST\nPROST\nAroca-Ouellette et al. (2021)\nhttps://huggingface.co/datasets/corypaik/prost\nyes\nSpatialCS\nSpatial Commonsense Liu et al. (2022c)\nhttps://github.com/xxxiaol/spatial-commonsense\nno\nRainier_anno Rainier (annotated)\nLiu et al. (2022b)\nhttps://github.com/liujch1998/rainier\nno\nEVALUATION (UNSEEN TYPE 2)\nSWAG\nSWAG\nZellers et al. (2018)\nhttps://github.com/rowanz/swagaf/tree/master/\ndata\nyes\nHellaSwag\nHellaSwag\nZellers et al. (2019)\nhttps://github.com/rowanz/hellaswag/tree/master/\ndata\nyes\nCODAH\nCODAH\nChen et al. (2019)\nhttps://github.com/Websail-NU/CODAH/tree/master/\ndata\nyes\nSCT\nStory Cloze Test\nMostafazadeh et al. (2016)\nhttps://cs.rochester.edu/nlp/rocstories/\nyes\n\u03b1NLI\n\u03b1NLI\nBhagavatula et al. (2019)\nhttps://leaderboard.allenai.org/anli/submissions/\nget-started\nyes\nStrategyQA\nStrategyQA\nGeva et al. (2021)\nhttps://github.com/eladsegal/strategyqa/tree/\nmain/data/strategyqa\nyes\nCREAK\nCREAK\nOnoe et al. (2021)\nhttps://github.com/yasumasaonoe/creak/tree/main/\ndata/creak\nyes\nTable 7: More dataset details. We show the link from which we retrieved each dataset, and whether each dataset is\nincluded in the training data of Flan-T5.\nSymbol\nValue\nDescription\nL\n128\nMax number of tokens in the input statement\nBG\n64\nNumber of statement groups per batch\nC\n4\nMax number of statements in each group, during training\nBS\n256\nMax number of statements per batch, during training\nS\n50,000\nTotal number of training steps in each stage\n\u03b7T5\n1 \u00d7 10\u22125\nLearning rate for VERA with T5 encoder backbone\n\u03b7LLaMA\n2 \u00d7 10\u22126\nLearning rate for VERA with LLaMA backbone\n\u03b1\n1.0\nWeight of binary classification loss\n\u03b2\n1.0\nWeight of multiple-choice loss\n\u03b3\n0.1\nWeight of supervised contrastive loss\n\u03c4\n0.05\nTemperature in supervised contrastive loss\nTable 8: Hyperparameter settings.\nFigure 5: Results on problem-solving with VERA on seen and unseen benchmarks. Average results on the\ndevelopment sets are reported. Accuracy across different parts (seen, unseen (type 1), unseen (type 2)) are not\ndirectly comparable due to different underlying benchmarks. For calibration curves, curves with saturated colors are\nresults after applying post hoc calibration (\u00a73.3), while curves with faded colors are results from the raw logits.\nDataset \u2192\nAll\nMC\nBool OBQA ARC_e ARC_h AI2Sci_e AI2Sci_m CSQA QASC PIQA SIQA WG\nC2S SciQ QuaRel QuaRTz CycIC ComVE CSQA2\nSKD Critic (355M)\n36.64 35.96 47.60 27.60\n29.12\n23.08\n25.20\n28.00\n20.15 12.42 53.86 39.20 50.28 51.41 27.30 53.60\n55.73\n25.80\n52.56\n47.60\nI2D2 Critic (355M)\n55.03 55.75 43.65 44.80\n55.61\n35.79\n55.28\n50.40\n61.51 45.25 67.36 56.45 55.56 63.17 55.10 61.51\n60.16\n35.72\n88.26\n43.65\nUnifiedQA-v2 (11B) 56.33 56.34 56.05 54.60\n48.77\n39.46\n48.78\n43.20\n44.23 32.61 63.98 52.10 70.64 75.96 42.20 81.65\n71.35\n48.29\n83.65\n56.05\nEntailer (11B)\n73.79 74.90 56.00 74.40\n81.93\n64.88\n77.24\n82.40\n67.81 57.56 78.78 64.33 77.11 82.86 76.90 85.97\n76.56\n52.81\n96.89\n56.00\nPPL (GPT-3.5)\n66.02 66.02\n\u2013\n45.20\n70.12\n45.15\n69.92\n62.70\n61.92 57.02 81.55 51.23 71.37 72.56 86.80 71.22\n70.91\n53.31\n85.37\n\u2013\nGPT-3.5 (175B)\n75.41 76.34 60.55 74.20\n85.79\n68.90\n84.55\n80.80\n66.91 62.85 84.17 65.30 72.53 81.33 86.00 83.09\n76.04\n51.60\n97.39\n60.55\nChatGPT\n62.11 61.52 71.65 60.80\n65.44\n57.19\n63.41\n68.00\n39.64 42.01 67.36 52.20 61.33 76.73 60.70 74.10\n72.66\n29.66\n93.08\n71.65\n+ 5-shot CoT\n65.19 65.19\n\u2013\n62.40\n77.33\n62.88\n72.36\n69.84\n46.52 47.52 68.59 52.25 59.98 82.14 69.77 65.83\n72.14\n42.56\n90.98\n\u2013\nGPT-4\n72.35 71.81 81.00 76.00\n69.00\n72.00\n80.00\n80.00\n43.00 44.00 73.00 57.00 77.00 94.00 70.00 86.00\n80.00\n53.00\n95.00\n81.00\n+ 5-shot CoT\n74.96 74.96\n\u2013\n79.80\n79.00\n70.00\n87.00\n89.11\n31.31 44.00 80.00 67.00 82.18 91.92 80.00 82.00\n87.00\n57.00\n92.08\n\u2013\nFlan-T5 (11B)\n79.50 80.58 62.25 79.60\n85.79\n71.24\n86.99\n81.60\n69.21 64.58 83.95 73.23 84.69 84.40 80.80 92.81\n82.03\n69.90\n98.40\n62.25\nVERA-LLaMA (7B) 82.99 84.18 63.85 80.20\n84.39\n75.92\n88.62\n82.40\n76.17 71.38 85.91 79.89 87.92 83.63 90.00 92.09\n80.99\n89.42\n97.99\n63.85\nVERA-T5 (5B)\n85.51 86.57 68.60 83.20\n88.07\n78.60\n93.50\n86.40\n77.97 73.33 88.47 80.14 92.42 85.93 88.80 93.88\n84.90\n91.73\n97.79\n68.60\nTable 9: Results on seen benchmarks. Accuracy on the development set is reported.\nDataset \u2192\nAll\nMC\nBool\nWSC COPA NumerSense PROST SpatialCS\nSKD Critic (355M)\n38.34 35.83 48.41 54.21 53.00\n11.50\n24.60\n48.41\nI2D2 Critic (355M)\n54.79 54.43 56.22 80.59 72.80\n35.00\n29.35\n56.22\nUnifiedQA-v2 (11B) 59.73 55.10 78.25 71.79 81.20\n35.00\n32.40\n78.25\nEntailer (11B)\n71.47 68.05 85.15 86.08 92.40\n51.00\n42.70\n85.15\nGPT-3.5 (175B)\n71.03 70.73 72.24 85.71 87.00\n66.50\n43.70\n72.24\nChatGPT\n61.20 54.69 87.22 73.26 58.80\n47.50\n39.20\n87.22\nGPT-4\n77.40 71.75 100.00 85.00 64.00\n69.00\n69.00\n100.00\nFlan-T5 (11B)\n77.62 73.22 95.23 90.48 93.00\n57.50\n51.90\n95.23\nVERA-LLaMA (7B) 75.71 74.06 82.32 94.14 91.80\n65.00\n45.30\n82.32\nVERA-T5 (5B)\n81.65 78.70 93.44 94.51 93.40\n66.50\n60.40\n93.44\nTable 10: Results on unseen (type 1) benchmarks. Accuracy on the development set is reported.\nDataset \u2192\nAll\nMC\nBool SWAG HellaSwag CODAH SCT \u03b1NLI StrategyQA CREAK\nSKD Critic (355M)\n43.40 40.11 51.62 26.95\n30.45\n29.35\n62.75 51.04\n50.66\n52.59\nI2D2 Critic (355M)\n67.11 70.42 58.84 72.15\n53.30\n67.30\n88.24 71.08\n52.40\n65.28\nUnifiedQA-v2 (11B) 53.95 52.83 56.77 31.75\n36.60\n49.00\n82.04 64.75\n49.34\n64.19\nEntailer (11B)\n70.72 70.63 70.94 52.45\n47.65\n80.70\n94.39 77.94\n60.26\n81.62\nGPT-3.5 (175B)\n78.87 80.21 75.53 73.40\n70.40\n85.05\n95.56 76.63\n62.88\n88.18\nChatGPT\n62.83 56.21 79.39 43.70\n42.95\n56.75\n77.34 60.31\n67.69\n91.10\nGPT-4\n70.29 66.20 80.50 57.00\n40.00\n66.00\n93.00 75.00\n70.00\n91.00\nFlan-T5 (11B)\n78.89 80.52 74.81 69.20\n64.55\n89.60\n98.45 80.81\n61.14\n88.48\nVERA-LLaMA (7B) 82.56 86.11 73.71 79.30\n83.90\n88.95\n98.61 79.77\n62.45\n84.97\nVERA-T5 (5B)\n83.37 86.66 75.13 76.30\n85.90\n88.60\n98.56 83.94\n65.07\n85.19\nTable 11: Results on unseen (type 2) benchmarks. Accuracy on the development set is reported.\nGenerator\nFilter QA\nAvg OBQA ARC_e ARC_h AI2Sci_e AI2Sci_m CSQA QASC PIQA SIQA WG\n\u2013\n\u2013\nUnifiedQA-large 60.45 70.20\n69.12\n55.85\n69.11\n64.80\n61.43 43.09 63.66 53.84 53.35\nGPT-3 (davinci) \u2013\nUnifiedQA-large 67.44 74.60\n75.44\n64.55\n69.92\n72.80\n70.19 63.82 67.74 58.70 56.59\nGPT-3 (davinci) VERA UnifiedQA-large 70.67 77.60\n80.00\n67.56\n78.05\n78.40\n71.91 66.20 70.35 59.37 57.22\nGenerator\nFilter QA\nAvg OBQA ARC_e ARC_h AI2Sci_e AI2Sci_m CSQA QASC PIQA SIQA WG\n\u2013\n\u2013\nUnifiedQA-large 60.45 70.20\n69.12\n55.85\n69.11\n64.80\n61.43 43.09 63.66 53.84 53.35\nRainier-large \u2013\nUnifiedQA-large 61.78 69.40\n66.84\n52.84\n68.29\n57.60\n68.30 54.86 65.51 56.81 57.38\nRainier-large VERA UnifiedQA-large 64.88 73.40\n71.05\n57.19\n73.98\n67.20\n68.30 55.51 67.52 56.96 57.70\nTable 12: Results of introducing VERA into the Generated Knowledge Prompting pipeline (Liu et al., 2021).\nAccuracy on the development set is reported.\nFigure 6: Results for filtering LM-generated commonsense knowledge with VERA. Results on the development sets\nare reported.\nFigure 7: Comparing verification and QA, the two different formats for problem-solving tasks. Average accuracy on\nthe development sets of the seen multiple-choice benchmarks is reported. We use text-davinci-002 as GPT-3.5\nhere, and gpt-3.5-turbo-0301 as ChatGPT. VERA in QA format actually means a T5 model finetuned on the\nsame seen multiple-choice data as VERA.\n"
  },
  {
    "title": "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos",
    "link": "https://arxiv.org/pdf/2305.03713.pdf",
    "upvote": "1",
    "text": "Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos\nEkta Prashnani\nKoki Nagano\nShalini De Mello\nDavid Luebke\nOrazio Gallo\nNVIDIA\n{eprashnani, knagano, shalinig, dluebke, ogallo}@nvidia.com\nAbstract\nModern generators render talking-head videos with im-\npressive photorealism, ushering in new user experiences\nsuch as videoconferencing under constrained bandwidth\nbudgets. Their safe adoption, however, requires a mech-\nanism to verify if the rendered video is trustworthy. For\ninstance, for videoconferencing we must identify cases in\nwhich a synthetic video portrait uses the appearance of\nan individual without their consent.\nWe term this task\navatar fingerprinting. Specifically, we learn an embedding\nin which the motion signatures of one identity are grouped\ntogether, and pushed away from those of the other identi-\nties. This allows us to link the synthetic video to the identity\ndriving the expressions in the video, regardless of the facial\nappearance shown. Avatar fingerprinting algorithms will\nbe critical as talking head generators become more ubiqui-\ntous, and yet no large scale datasets exist for this new task.\nTherefore, we contribute a large dataset of people deliver-\ning scripted and improvised short monologues, accompa-\nnied by synthetic videos in which we render videos of one\nperson using the facial appearance of another1.\n1. Introduction\nRecent face portrait generators can synthesize real-time\ntalking-head videos hardly distinguishable from real ones.\nDespite the risks for visual disinformation it poses, the le-\ngitimate use of synthetic avatars will become ubiquitous,\nbringing benefits to a myriad of applications ranging from\npersonalized avatars to AR filters for selfie videos.\nIn the context of video conferencing, for instance, in-\nstead of sending the entire video of a person talking, we\ncan synthesize a video on the receiver\u2019s end using a sin-\ngle frame to specify the target identity, together with some\ncompact information about the facial motion of the driving\nvideo, thus saving valuable bandwidth [46]. To enable safe\nuse in such cases, the relevant question is no longer whether\na video is \u201creal\u201d or not, but whether it is \u201ctrustworthy\u201d or\n1Project page: https://research.nvidia.com/labs/nxp/\navatar-fingerprinting/\nIdentity 1 (ID1)\nIdentity 2 (ID2)\nTalking Head\nGenerator\nTalking Head\nGenerator\nTarget\nIdentity\nDriven by ID1\nDriven by ID2\nDynamic Identity\nEmbedding\nAvatar Fingerprinting\nVideos driven by ID2\nVideos driven by ID1\nFigure 1:\nTalking head generators synthesize realistic\nvideos of a target identity from driving videos of different\nidentities. Our method extracts identity-agnostic temporal\nfacial features and learns an embedding in which videos\ndriven by one identity fall close to each other and far from\nthose driven by other identities, regardless of the appear-\nance.\nnot. That is, we want to determine if the person driving the\nvideo (ID1 or ID2 in Figure 1) is authorized to control the\nlikeness, or the appearance, of the synthetic video portrait\n(target identity in Figure 1). We call this novel task avatar\nfingerprinting.\nTo perform avatar fingerprinting, we leverage a sim-\nple but fundamental observation: individuals tend to show\nunique facial motion idiosyncrasies when talking and emot-\ning. For instance, someone may raise one of her eyebrows\nmore than the other, or shake their head more often while\ntalking. These \u201cdynamic identity signatures\u201d [40] have been\nshown to carry enough information for humans to recog-\nnize other individuals, even when the physical appearance\nof their face is altered [40, 23, 31]. This makes them at-\ntractive for our task, as they can be derived solely from the\ndriving identity of a talking-head video, regardless of the\nappearance.\nFortunately, modern talking-head generators are becom-\ning increasingly accurate at capturing the facial motion of a\nperson and rendering it onto a target identity. As a result, we\nfind that we can estimate a dynamic identity signature for an\n1\narXiv:2305.03713v2  [cs.CV]  12 Sep 2023\nStage I: Free-Form\n.\n\u201cIt was such a pleasure talking to you.\nI hope we stay in touch.\u201d\nStage II: Scripted\nID1\nID3\nID2\nID1 \u2192 (\u00b7)\n(\u00b7) \u2192 ID1\nID3 \u2192 (\u00b7)\nID2 \u2192 (\u00b7)\n(\u00b7) \u2192 ID2\n(\u00b7) \u2192 ID3\nTalking Head\nGenerator\nTarget Images\n  Real Data Capture\n Synthetic Data Generation\nFigure 2: We introduce a large dataset of real and synthetic talking-head videos We capture subjects talking in both scripted\nand free-form settings. To encourage a natural performance in a realistic setting, we record the subjects while videocon-\nferencing with each other (left). We also synthesize more than 242,000 talking-head videos, using three state-of-the-art\nface-reenactment talking-head generators. In the synthetic videos on the right, each row corresponds to a driving identity\n(IDi\u2192(\u00b7)) and each column corresponds to a different target identity ((\u00b7)\u2192IDi). The videos in which driving and target identity\nmatch are self-reenactments videos, the rest are cross-reenactments.\nidentity from the synthesized videos they drove\u2014regardless\nof the target identities shown.\u201d To leverage this observation,\nthen, we extract facial landmarks and their temporal dynam-\nics from the video. We the introduce a novel contrastive\nloss that allows us to characterize them in the estimated dy-\nnamic identity embedding space of the talking-head videos\n(see Figure 1). In this space, the embeddings correspond-\ning to one driving identity across multiple videos and tar-\nget identities stay close to each other, and far from those of\nother driving identities.\nAvatar fingerprinting is a new task, and no existing\ndatasets completely serve its training and validation re-\nquirements. For a comprehensive evaluation of our algo-\nrithm, we desire videos of multiple subjects delivering both\nscripted and free-form monologues, captured under typi-\ncal conditions, such as varying video quality and gaze di-\nrection.\nWe also need synthetic talking-head videos for\nthe case in which driving and target identities are dif-\nferent (cross-reenactment), and that in which they match\n(self-reenactment). Unfortunately, existing datasets of real\nvideos provide either scripted [35, 12], or natural [49]\nmonologues, and synthetic video datasets focus either on\nself-reenactment [19] or on cross-reenactment [33, 41], but\nnever both.\nTherefore, we release the NVIDIA Facial\nReenactment (NVFAIR) dataset, featuring the largest col-\nlection of real and synthetic face re-enactment talking-head\nvideos, to foster research in this new domain. We record\n46 subjects delivering scripted and free-form monologues\nwith their natural expressions over video calls. Their orig-\ninal videos, as well as those of 115 subjects from ex-\nisting datasets [35, 12], are then used to synthesize over\n242,000 self-and cross-reenactments, using three state-of-\nthe-art face-reenactment generators [46, 48, 26].\nWe think that verifying authorized use of a synthetic\nvideo will be increasingly critical as synthetic media be-\ncomes more pervasive, and hope to aid community progress\nin this direction by releasing our dataset. Therefore, we took\nparticular care in designing the capture protocol, including\nits legal and privacy-related aspects. In summary:\n\u2022 We introduce the task of avatar fingerprinting, which\nfocuses on verifying the driving identity of synthetic\ntalking-head videos, rather than classifying them as\nreal or synthetic.\n\u2022 We release NVFAIR, the first large dataset of subjects\ndelivering scripted and natural monologues, complete\nof self- and cross-reenactment synthetic videos.\n\u2022 We propose a baseline solution to this novel task in the\ncontext of video conferencing by extracting person-\nspecific motion signatures, and demonstrate its robust-\nness to various distortions and generators not seen in\ntraining.\n2. Related Work\nOur avatar fingerprinting task addresses the authentica-\ntion of synthetic media, but we do not focus on active foren-\nsics techniques such as watermarking.\nWhile numerous\nefforts exist in detecting deepfakes, to our knowledge, no\nprevious work has addressed authorized use of synthetic\ntalking-head videos as we do. While we do not focus on\nthe detection of deepfakes, our method is closely related to\na family of deepfake detection techniques that operate based\non person-specific identity features, such as the behavioral\nfeatures of a face. While we focus on verifying talking head\nvideos, we do not assume anything specific about the talk-\ning head generators and and show that our method, trained\non one talking-head generator, generalizes well to unseen\ntalking-head generators at test time.\n2\nLearning-based\nAttribution\nof\nSynthetic\nMedia.\nLearning-based approaches have been used to learn pat-\nterns and common characteristics in images or videos to\nidentify the origin and authenticity of the media, or to\ndetermine if it has been manipulated or altered in some\nway.\nPrevious work [7, 27] used a pre-trained GAN\ngenerator to attribute a synthesized image to its generator\nby projecting the image to the generator\u2019s latent space. In\nthis case, a perfectly inverted image indicates the image\nis generated by the generator, while a real image is less\ninvertible. Based on this, GAN-Scanner [50] used a variant\nof StyleGAN2-based inversion to detect images generated\nby never-before-seen GANs.\nA learning-based approach\nwas also used to learn camera fingerprints associated with\nvideos recorded by physical cameras to determine whether\na media is manipulated [16]. Another line of work to verify\nthe authenticity of media is by embedding imperceptible\nwatermarks into images and videos [20, 8, 45, 36].\nA\nprevious work [51] showed that they can embed invis-\nible watermarks into training data and a GAN model\ntrained with it can successfully transfer watermarks to the\ngenerated images, allowing attribution of the generator.\nMore recent work [52] showed a framework to embed\nwatermarks in the form of a conditional GAN model for\nscalable fingerprinting of generated images. While these\nare viable solutions, our method focuses on a passive\nforensics technique that operates given a probe video and a\nset of verified user videos.\nDeepfake Detection Based on Identity-Specific Features.\nA family of deepfake detectors use identity-specific fea-\ntures to verify whether the person depicted in a video is\nwho it purports to be, by posing the detection problem as\nan identity-recognition problem. The advantage of this ap-\nproach is that the forensics method only needs to be trained\nwith pristine videos and does not need to be trained against\nspecific generators, enabling generalization to unseen gen-\nerators. Agarwal et al. [5] is the first to exploit person-\nspecific patterns in facial expressions and poses as the dis-\ntinctive identity signature to detect fake videos.\nFollow\nup works explored other types of person-specific soft bio-\nmetric features, such as, vocal mannerisms [11], phoneme-\nvisemem consistencies [4], word-facial expression consis-\ntencies [6], and dynamics of ears [3]. While many of these\nworks need person-specific training, previous works [34, 2,\n15, 14] extended this idea to train a CNN-based detector\nusing a large-scale in-the-wild video data [13] and variants\nof contrastive learning [43, 47, 21, 29]. Agarwal et al. [2]\ncombined static facial appearance using a facial recogni-\ntion model and dynamic facial behaviors using a CNN, and\nshowed that this approach is effective for detecting face-\nswap deepfakes.\nID-Reveal [15] used facial shapes and\nmotions encoded in a low-dimensional space of a 3D mor-\nphable model [9] and demonstrated the ability to handle\nboth face-swapping and talking-head deepfakes. Beyond\nthese dynamic facial identity features, previous works ex-\nplored temporal inconsistencies of face identities within a\nvideo [34] and identity consistencies of inner and outer face\nregions [17]. While these detection approaches aim for dif-\nferentiating real and synthetic videos, our avatar fingerprint-\ning task requires determining a driver among the synthetic\nvideos.\nDynamic Facial Identity Signatures.\nIn ongoing re-\nsearch on how humans recognize and attribute faces, cog-\nnitive scientists have studied the impact of \u201cdynamic facial\nidentity signatures\u201d (i.e., characteristic or identity-specific\nmovements of a face) for identity recognition, especially\nwhen limited appearance-specific \u201cstatic identity\u201d cues are\navailable [40]. In one experiment, scientists projected fa-\ncial animations generated by human actors onto a computer-\ngenerated average head [23]. Subjects were able to learn\nto discriminate individuals based solely on facial motion.\nIn another, subjects correctly attributed animations of syn-\nthetic faces and to their morphed versions [31]. These stud-\nies have focused on the learnability of dynamic signatures\nby human observers and observed a positive trend when dy-\nnamic behavior was the most or the only reliable cue to\nidentity. Most recent work performed human user studies in\nthe context of deepfake videos and suggested that behavior\nsignatures could be used to distinguish synthesized videos\nregardless of their visual appearance [39]. We wish to seek\nsuch \u201cdynamic facial identity signatures\u201d in our efforts to\nfingerprint a synthetic talking head. This will allow us to\nabstract away the appearance of an avatar, and therefore, at-\ntribute an avatar to the driving identity instead of the target\nidentity shown in the avatar.\nTalking Head Generators.\nWe focus specifically on\nface-reenactment talking-head generators for this work\u2014\nthis class of generators are becoming increasingly preva-\nlent in video conferencing applications [1]. Given a tar-\nget portrait and a driving video of a user, modern talking\nhead generators [48, 25, 44, 54, 53, 28, 18, 46, 26]reenact\nthe target portrait by using motions (facial expressions and\nhead poses) from the driving video. Instead of using a sin-\ngle target frame, talking head generators may use an image\nset to capture better person-specific appearance and unseen\nbackground [37]. To transfer the motions from the driver\nvideo, most of these generators rely on additional deep neu-\nral networks to decompose motions from the appearance of\nthe driving video in a form of appearance-agnostic sparse\nkeypoints and dense motion fields.\n3\n3. Terminology\nWe seek to verify the trustworthiness of a synthesized\ntalking-head video, termed target video. We assume that an\navatar-generation tool (e.g., [46]) created it by animating an\nimage (target image) using the expressions and head poses\nobtained from another video, which we term the driving\nvideo. We call driving identity the identity of the person in\nthe driving video, and target identity the identity of the per-\nson in the target image. When driving and target identities\nmatch, the target video is a self-reenactment, while the case\nof a driving identity used to animate a different target iden-\ntity is cross-reenactment. In both cases, the resulting facial\nre-enactment always has the appearance of the target iden-\ntity. This terminology allows us to formally state our goal:\nwe want to verify that a target video is self-reenactment.\nWith this terminology, we introduce our dataset, which in-\ncludes real videos as well as self- and cross-reenactment\nvideos.\n4. Dataset\nRecall that avatar fingerprinting is not about detection\nof synthetic media. Rather, we already know a video to\nbe synthesized, and seek to verify the driving identity to\nenable its authorized use. This new task dictates a set of\nrequirements for the dataset to be effective for training and\nevaluation. Specifically, we need a dataset that contains\n1. multiple real videos per identity with some videos\nwhere all subjects speak the same content, and others\nwhere they speak free-form with many natural and pre-\nscribed emotions,\n2. self- and cross-reenactments for target identities, with\nmultiple cross-reenactments driven by all other sub-\njects for every target identity, to indicate how the\ndriver\u2019s dynamics render on a variety of structures, and\n3. multiple face-reenactment generators.\nAll relevant existing datasets (see Table 1) only capture a\nsubset of these requirements. We introduce the NVIDIA\nFacial Reenactment (NVFAIR) dataset that fills this void. It\ncombines all of properties above into the largest collection\nof real and synthetic facial-reenactments till date. Figure 2\nshows an overview of how the videos are captured and syn-\nthesized.\n4.1. Real Data Capture\nCapturing videos of monologues delivered by different\nsubjects for the purpose of identity verification introduces\ntwo conflicting goals. On the one hand a controlled eval-\nuation of the trained models require predictability of what\nis spoken to prevent identification algorithms from latch-\ning onto the spoken content itself. On the other, we want\nthe subjects to act as they would in a casual conversa-\ntion, rather than with a prescribed emotion, to capture their\nuniquely identifying mannerisms. We address this trade-off\nby recording the subjects while videoconferencing in pairs,\nwhich creates the impression of being in a natural conversa-\ntion. This differs from existing datasets, in which the sub-\njects are looking at the camera, but are not interacting with\nothers during the recording [32, 12, 35]. We also design\ntwo distinct recording strategies: a free-form stage where\nthe subjects are given only general guidance on the topics,\nand a more controlled scripted stage in which subjects speak\nshort, memorized monologues of 2-3 sentences each, see\nFigure 2(a). We captured our videos with minimal instruc-\ntions on how to setup the videoconference, allowing for the\nvariability one can expect in a natural setting. For example,\nthe backgrounds present various degrees of clutter and there\nis a variety of facial lighting, scale, and bandwidth stability.\nThis was by design: we want the dataset to be as challeng-\ning as real-life scenarios. In total we record 46 subjects of\ndiverse genders, age groups, and ethnicity.\nStage I: Free-Form Monologues.\nIn this first stage, the\ntwo subjects on the call alternate between asking and an-\nswering seven pre-defined questions. The questions are de-\nsigned to avoid sensitive or potentially inflammatory topics.\nThis is critical because we later use sentences spoken by\none individual to animate the video of a second individual,\nquite literally putting words in their mouths. Questions in-\nclude topics such as one\u2019s favorite family holiday, or their\nleast favorite house chore. (The complete list of questions\nis in the Supplementary.) To further create a natural interac-\ntion, the subject listening is encouraged to actively engage\nwith the one speaking (e.g., by nodding or smiling), while\nremaining silent.\nStage II: Scripted Monologues.\nFor this stage, we pre-\npared thirty short utterances consisting of two or three sen-\ntences each. We chose this length to allow for memoriza-\ntion, while still providing enough content to trigger facial\nexpressions. However, to avoid inducing unnatural expres-\nsions, we did not prescribe specific emotions for each ut-\nterance. For instance, we did not ask to express anger for\na sentence, but we did choose sentences that may naturally\nevoke it, and used punctuation to encourage it, e.g. \u201cWill\nyou please answer the darn phone? The constant ringing is\ndriving me insane!\u201d We instructed the subjects to split their\nscreens to show both this list and the call video and encour-\naged them to speak to their recording partner when reciting,\nsee Stage II in Figure 2(a). More details, including the full\nlist of utterances can be found in the Supplementary.\nPrivacy Considerations.\nFace videos are sensitive data,\nsince a person\u2019s face is a key identifier. We took on this task\nwith care to ensure good data governance. Our proposal for\nthe data capture protocol was approved by an Institutional\n4\nDataset\n# Subjects\n(with Source)\n(F)ree or\n(S)cripted?\nEmotion: (N)atural\nor (P)rescribed\n(R)eal or (S)elf- or\n(C)ross-reenact.?\n# Synthetic Face\nReenactments\nAvg. Videos per Subject\n# Face-reenact.\nGenerators\nDiversity\nRAVDESS [35]\n24 (new)\n(S)\n(P)\n(R) only\nN/A\n120 ((R))\nN/A\n\u2713\nCREMA-D [12]\n91 (new)\n(S)\n(P)\n(R) only\nN/A\n81 (R)\nN/A\n\u2713\nVFHQ [19]\n36 ([35], YT)\n(S)\n(P)+(N)\n(R) + (S)\n1,737\n120\n1\n\u2713\nFF++ [42]\n1000 (YT)\n(F)\n(N)\n(R) + (C)\n2000\n1 (R) + 2 (C)\n2\n\u2713\nKoDF[32]\n403 (new)\n(F) + (S)\n(N)\n(R) + (C)\n61,000\n150 (R) + 151 (C)\n1\n\u2717\nNVFAIR (Ours)\n161 (46 new, [35], [12])\n(F) + (S)\n(N) + (P)\n(R) + (S) + (C)\n242,864\n76 (R) + 76 (S) + 1,398 (C)\n1 (train), 3 (test)\n\u2713\nTable 1: Comparison with existing real and synthetic face-reenactment datasets. \u201cPerformance\u201d refers to whether the subjects\nwere asked to use a prescribed emotion (e.g., anger). NVFAIR is the first dataset to offer the complete set of monologue\nmodalities, and features the largest collection of facial reenactments to date. Specifically, it provides scripted and free-form\nmonologues, with natural and prescribed performances, and self- and cross-reenactments (driven by all remaining subjects)\nfrom up to three generators, alongside original videos for newly recorded subjects. \u201cPerformance\u201d refers to whether the\nsubjects were asked to use a prescribed emotion (e.g., anger).\nReview Board (IRB). Our goal was to provide exhaustive\nand transparent information to participants about our data\ncapture procedure, future plans with the dataset (includ-\ning our intent to create synthetic data samples), and con-\nditions under which future research would be conducted\u2014\nby us and interested third parties. The participants were also\nasked to confirm whether their data can be used for research\nbeyond avatar fingerprinting, and whether it could be shown\nin public disclosures. Each file in our dataset is annotated\nwith their responses.\n4.2. Synthetic Talking-Head Videos\nIn addition to the videos described in Section 4.1, we\nalso need synthetic videos to train and evaluate our avatar\nfingerprinting algorithm.\nMoreover, in case of cross-\nreenactments, the driving identities are a subset sampled\nfrom all the available identities. While these choices en-\nable research on related topics such as detecting deepfakes,\nwe believe that effective training and evaluation of models\nfor avatar fingerprinting could benefit from more exhaus-\ntively sampling the set of driving identities, as well as pro-\nviding both self- and cross-reenactments within the same\ndataset. Our NVFAIR dataset fills this void, and provides\nthe largest collection of synthetic facial reenactments till\ndate (Table 1).\nSpecifically, we pool the 91 identities from the origi-\nnal videos of CREMA-D [12], the 24 identities from those\nof RAVDESS [35], and the 46 from our own dataset from\nour own video-conferencing data capture, for a total of 161\nunique identities I. Recall that we have several real videos\nfor each identity IDi \u2208 I. To avoid a combinatorial explo-\nsion of synthetic videos, for all pairs of identities IDi and\nIDj, we use IDj as the target identity and we randomly se-\nlect 8 of the videos of IDi to generate 8 cross-reenactment\nvideos, {IDk\ni \u2192 IDj}k={1,..,8} (all 8 share the same target\nimage). We also generate a self-reenactment video for each\noriginal video, by animating a neutral-face image of each\ntarget identity with each of their original videos.\nWe use Face-vid2vid for synthesizing the videos [46] for\nall 161 identities. For the identities that belong to the test\nsubset, we generate self- and cross-reenactments with two\nadditional generators: LIA [48], and TPS [26]. This al-\nlows us to test if our model generalizes to generators not\nused in training. We chose these portrait generators because\nthey are the state-of-the-art and they preserve the identity-\nspecific facial motion dynamics. Nevertheless, the recon-\nstruction is not perfect; for instance, in the third row of Fig-\nure 2(b) the person\u2019s eyes are squinted in the driving video\n(ID3), but completely closed in all reenactments\u2014including\nin self-reenactment. In total we generate more than 242,000\ncross-reenactment videos. More details are in the supple-\nmentary.\n5. Method\nOverview.\nWe seek to verify the driving identity of a syn-\nthetic video, independently of the target identity. We lever-\nage the finding from cognitive science research that each\nperson emotes in unique ways when communicating, and\nthat this signal is sufficient for recognition, even when the\nactual appearance is artificially corrupted [40, 23, 31]. We\nnote that these dynamic features are not a bi-product of the\ngenerator, and come from the driving identity itself: they\nmay be the way a person smiles, or the way she frowns.\nNotably, they are distinct from the temporal artifacts intro-\nduced by the generator, and that existing algorithms use to\ndetect whether a video is synthetic or real [22].\nAn overview of our algorithm for avatar fingerprinting is\nshown in Figure 3. To capture expressions, we extract the\nrelative position of facial landmarks over time from the in-\nput video, as shown in Figure 3(a) (Section 5.1). We then\nproject these temporal signatures onto a dynamic identity\nembedding in which features belonging to the same driv-\ning identity are close to each other regardless of the target\nidentity used to generate the video, i.e., independently of\nappearance (Figure 1). To learn this embedding we train a\nneural network with a novel contrastive loss that pulls to-\ngether all embedding vectors of synthetic videos driven by\nan individual, while pushing away the embedding vectors of\n5\nClip driven by ID1\nLandmarks\ndistances\nTemporal\nCNN\nDynamic facial\nidentity features\nID1 ) ID1\nID1 ) ID2\nID2 ) ID1\nID2 ) ID3\nPull together\nPush away\nSec. 5.1\nSec. 5.2\nFigure 3:\nWe extract landmarks from the frames of a\ntalking-head clip, compute their normalized pairwise dis-\ntances, and concatenate the frame-wise features. We then\nlearn an identity embedding using a loss that pulls closer\nfeatures of videos driven by the same identity and pushes\naway those driven by others. IDi \u2192 IDj indicates a video\nthat looks like identity j (the \u201ctarget\u201d identity), and is driven\nby identity i.\nvideos driven by all others individuals (Section 5.2). More\ndetails on the implementation are in Section 5.3.\n5.1. Dynamic Facial Identity Features\nOur first step is to extract temporal features that summa-\nrize short segments of the video we wish to fingerprint. We\nidentify the following guiding principles for the extracted\nfeatures: For each frame we want to extract features that\nmust:\n1. have minimal dependency on the appearance of the\nface in the video (that is, the target identity),\n2. reflect the dynamics of the expressions, and\n3. can capture subtle expressions.\nOne choice could be per-frame 3DMM features [10]: a\nstrategy also used by Cozzolino et al. to detect synthetic\nvideos [15]. However, we empirically observe that 3DMM\nfeatures are not sufficiently expressive, and do not satisfy\nrequirement 3 (see ablation experiments in Section 6). We\nobserve a similar behavior for action units [5]. Facial land-\nmarks [24] address this issue, but are sensitive to the shape\nof the face in the video, and thus to the target identity.\nTo leverage the expressiveness of facial landmarks while\nabstracting from the underlying facial shape, we propose\nto compute the pairwise normalized Euclidean distance be-\ntween each of the landmarks of a frame. We concatenate\nthese distances into a single vector for the frame, df. A\nsmall subset of the landmark features and their distances\nare shown in Figure 3(a).\nWe then break the input video into clips, which are se-\nquences consisting of F frames and offset by one frame\n(e.g., [1,F], [2,F+1], etc.), and concatenate the vectors from\nall the frames in each clip. Using the change in the relative\nposition of the landmarks over a short period of time (the\nlength of a clip) allows us to capture temporal dynamics\nwith minimal dependence on the absolute position of each\nlandmark, i.e. independently of the shape of the face.\nWe show empirically that our features are a good repre-\nsentation for our task, and can even improve the results of\nbaseline state-of-the-art methods (Section 6).\n5.2. Dynamic Identity Embedding Contrastive Loss\nWhile the features described in Section 5.1 extract low-\nlevel motion dynamics, they cannot be used directly to dis-\nambiguate two target videos based on the driving identity.\nWe tackle this problem by learning a dynamic identity em-\nbedding, a space where videos driven by one subject map to\npoints that are close to each other and far from the videos\ndriven by anybody else.\nSpecifically, we use a temporal CNN to extract an em-\nbedding vector from a clip, which, as described before, is\na short segment of an input video. To train the network we\nuse a dataset of synthetic videos driven by different identi-\nties. We denote as Ck\nID1\u2192ID2(t) the embedding produced by\nthe network for the clip starting at time t in the k-th video,\nof a target identity ID2 driven by identity ID1. As stated\nabove, we have two main objectives, which we capture with\nthe following two terms in our proposed loss function.\nWe Want to Pull Together All the Videos Driven by ID1.\nTo achieve this, we define the following term:\nNj,ID1,ID2(t) =\nX\nIDl,k\nmax\nn\ns(Cj\nID1\u2192ID2(t), Ck\nID1\u2192IDl(n)),\n(1)\nwhere s(\u00b7, \u00b7) = e\u2212\u2225\u00b7,\u00b7\u22252 is a similarity metric. Intuitively,\nEquation 1 takes two videos, j and k, both driven by ID1.\nGiven a clip starting at time t in the first video, it looks for\nthe most similar clip in the second video. Since the driving\nidentity is the same for both videos, Equation 1 encourages\nan embedding where clips that capture a similar expression\nare closer to each other. Equation 1 is high even if only\none clip from video k has a similar temporal signature to\nCj\nID1\u2192ID2(t). That is because even just one occurrence of the\nsame expression is evidence that the driving identity may\nbe the same. Of course other driving identities may use\nsimilar expressions and we address that with the loss term\ndescribed below. Additionally, we note that k spans the set\nof all videos driven by ID1, and IDl spans all identities,\nincluding IDl = ID1 and IDl = ID2.\nWe Want to Push Away Videos not Driven by ID1.\nWe\ndefine the following term:\nQj,ID1,ID2(t) =\nX\nIDl\u0338=ID1,k\nmax\nn\ns(Cj\nID1\u2192ID2(t), Ck\nIDl\u2192ID2(n)),\n(2)\n6\nwhere, similarly to Equation 1, we take a clip from video\nj, and look for the most similar clip in video k. However,\nthis time the two videos share the same target identity, but\nare driven by different identities: we want all the videos\ndriven by identities different from ID1 to be pushed away\nfrom those driven by ID1, including videos where ID1 is the\ntarget identity. Note that ID2 spans all identities, including\nID2 = ID1 and ID2 = IDl.\nWe Want to Rely on the Temporal Dynamics of the\nVideos Driven by ID1.\nAlthough we input a sequence of\nper-frame features to a temporal CNN, the model could still\nlearn to detect a self-reenactment by relying on the expres-\nsions from just a few frames\u2014rather than the temporal pro-\ngression of expressions across the entire frame sequence.\nTo further encourage the model to learn from the temporal\ndynamics, we introduce an additional term:\nRj,ID1,ID2(t) =\nX\nIDl,k\nmax\nn\ns(Cj\nID1\u2192ID2(t),\n\u223c\nC\nk\nID1\u2192IDl(n)),\n(3)\nwhere\n\u223c\nC\nk\nID1\u2192IDl denotes a version of the clip Ck\nID1\u2192IDl from\nEquation 1 with the randomly shuffled frame ordering. We\nwant such time-shuffled versions of the clips driven by ID1\nto be pushed away from the un-shuffled self-reenactment\nclips of ID1. Effectively, this means that the driving iden-\ntity of the time-shuffled clips is regarded as different from\nID1. In other words, we want to pull together video clips\nin the learned embedding space only when the temporal fa-\ncial dynamics are characteristic of ID1. We demonstrate\nthe importance of this term with ablation experiments in the\nsupplementary.\nCombining Equations 1, 2, and 3, we write the probabil-\nity that the embedding vector Cj\nID1\u2192ID2(t) lies close to the\nembedding vectors for all video clips driven by ID1 and far\nfrom all the videos driven by others as\npj,ID1,ID2(t) =\nNj,ID1,ID2(t)\nNj,ID1,ID2(t) + Qj,ID1,ID2(t) + Rj,ID1,ID2(t),\n(4)\nand the complete loss term as\nL =\nX\nj,ID1,ID2,t\n\u2212log(pj,ID1,ID2(t)).\n(5)\n5.3. Implementation\nParameter Choices.\nTo extract the per-frame dynamic fa-\ncial identity features df, we first estimate 126 facial land-\nmarks for each frame using a pre-existing implementa-\ntion [24]. We compute the Euclidean distances between all\npossible pairs of landmarks, to obtain a 7875-dimensional\nvector for each frame. For all of the 161 identities in the\nsynthetic component of our dataset, we scan through orig-\ninal videos to isolate a frame for each subject that shows\na neutral head pose and expression (frontal head pose, no\nexpression). The square root of the area occupied by the\nbounding box of their face in this neutral frame is used\nnormalize the 7875 values of the pairwise distance vec-\ntor, resulting in df. The clip duration is set to 71 frames.\nWe find that this is sufficient to capture the facial dynam-\nics that are meaningful for avatar fingerprinting, while also\nmaintaining a good trade-off between speed and accuracy.\nSince we include a large set of scripted monologues in\nour dataset\u2014which are crucial for a complete evaluation of\navatar fingerprinting\u2014we are constrained by the shortest-\nduration video clips in the dataset (98% of the video clips\nare at least 71 frames long, see Section 4). We also exper-\niment with shorter-duration video clips in the supplemen-\ntary.\nTraining Details.\nTemporal convolutional networks that\noperate on intermediate representations of frames have\nshown remarkable success in modeling facial behavior and\nits anomalies [22, 38, 15]. We use one such architecture, the\ntemporal ID network [15], after adapting the input layer to\nmatch our input feature dimensions and, when needed, input\nclip duration. This input tensor is obtained by concatenat-\ning df across F frames of the clip, to obtain a tensor of size\n7875 \u00d7 F. The neural network outputs a 128-dimensional\nembedding vector for each clip, which is trained to cluster\nbased on the driving identity (Section 5.2). In each batch,\nwe include 8 unique identities. For each identity IDi, the\npull term (Equation 1) comprises of 16 clips: 8 are self-\nreenactments, randomly sampled from the full set, and the\nremaining are cross-reenactments with IDi as the driving\nidentity. These cross-reenactments can potentially show the\nsame words being spoken by different target identities. This\nis crucial: it allows the neural network to learn to to pull to-\ngether videos based purely on the facial motion, regardless\nof the appearance of the video. The push term (Equation 2)\nfor IDi is composed of clips with the remaining 7 identities\nin the batch serving as driving identities (8 clips per driving\nidentity). Therefore, for each identity, 72 clips are included\nin a batch. The training is performed for 100,000 iterations,\nwith Adam optimizer [30] and a learning rate of 1e\u22124. We\nwill discuss additional details in the supplementary.\nTraining, Validation, and Test Datasets.\nWe split the\ntraining, validation, and test datasets based on identities.\nOut of the total 161 identities (pooling together the iden-\ntities from our dataset, RAVDESS, and CREMA-D\u2014 see\nSection 4.2), we reserve 35 for testing, 14 for validation and\n112 for training. We ensure that there is no cross-set cross-\nreenactments: that is, identities in the training set only drive\nother training-set identities (and similarly for validation and\n7\nReference Identity = ID1\nID1 \u2192 ID1 d=0.837 ID1 \u2192 ID2 d=0.949 ID2 \u2192 ID1 d=1.580 ID3 \u2192 ID1 d=2.109\nReference Identity = ID4\nID4 \u2192 ID4 d=0.501 ID4 \u2192 ID5 d=0.919 ID5 \u2192 ID4 d=2.111 ID6 \u2192 ID4 d=2.217\nFigure 4: Animated figure. Open in a media-enabled\nviewer like Adobe Reader and click on the inset. Our\nembeddings capture the dynamics of an expression, rather\nthan the appearance of the face. For each row, we pick a\nreference identity. The green box indicates reenactments\ndriven by the reference identity, the red and blue are cross-\nreenactments of the reference identity. We compute the av-\nerage distance of each clip shown here against all other clips\ndriven by the reference identity. The average distance to the\nother clips of the reference identity is consistent for a given\nmotion, and lower (better) when the reference identity is\ndriving as compared to the cross-reenactments that use the\nreference identity as target.\ntest set). This allows us to evaluate the generalizability of\nour method to novel facial mannerisms that are not seen\nduring training, as well as novel target identities. However,\nwe do make such cross-set samples available as a part of\nNVFAIR dataset release.\n6. Evaluation\nWe begin by evaluating qualitatively our method\u2019s abil-\nity to extract embedding vectors based on the driving iden-\ntity. Figure 4 shows a set of self- and cross-reenacted clips\n(please view the animation in a media-enabled viewer). For\neach row, we take one identity as reference and we compute\nthe embedding vectors of clips that use it both as driving\nand target identity. We then compute the average Euclidean\ndistance of the resulting embedding vectors against those of\nall other clips driven by the same reference identity. We\nnote that the average distance d is lower when the driving\nidentity matches the reference identity (first two columns).\nWe also note that the distance between the clips in the first\ntwo columns is similar: this confirms that it is a function\nof the motion, rather than the appearance. When the driv-\ning identity changes, the average distance increases, even if\n0\n0.5\n1\n0\n0.5\n1\nOurs \u2013 AUC = 0.868\nAgarwal et al. [5] \u2013 AUC = 0.726\nID-Reveal [15] \u2013 AUC = 0.720\nFigure 5: ROC curves for our method and two baselines.\nthe target identity matches the reference identity, which is\nprecisely our goal. More results are in the Supplementary.\nTo evaluate our approach more formally, we use the 35\nunique test-set identities that are not used as driving or tar-\nget identities in the training set (Section 5.3. One at a time,\nwe treat each identity IDi as target and synthesize cross-\nreenactments using all the remaining identities as drivers.\nThis is the set of \u201cunauthorized\u201d synthetic videos for IDi.\nThe self-reenacted samples for IDi form the \u201cauthorized\u201d\nset. Note that there are several self-reenacted videos of IDi,\none per original video of IDi.\nFor each target identity IDi, we extract the dynamic iden-\ntity embedding vector of all the clips in the pool of its self-\nand cross-reenacted videos, and compute their Euclidean\ndistances. That is, for clip k we compute\nd(Ck\nIDi\u2192IDi, Cl\nIDi\u2192IDi), \u2200l \u0338= k, and\n(6)\nd(Ck\nIDi\u2192IDi, Cl\nIDj\u2192IDi), \u2200l \u0338= k,\n\u2200i \u0338= j.\n(7)\nWe threshold these distances for each target identity to get\nan ROC curve, and average across the target identities to get\nthe overall area under the curve (AUC). We note that this\nAUC measures one model\u2019s ability to classify a synthetic\nvideo as self-reenactment or as cross-reenacted.\n6.1. Comparisons with Existing Methods\nAvatar fingerprinting is a novel task, and no existing\nmethods directly address it. The closest related works aim\nat detecting real versus synthetic media. As discussed in\nSection 2, some of these detectors learn identity-specific\nfeatures such as facial expressions and head poses [5], or\nfacial shapes and motion [15] and can serve as baselines\nfor the task of avatar fingerprinting with some adaptation.\nThe work by Agarwal et al. trains a model to detect syn-\nthetic videos of a specific identity [5]. To adapt it to our\ntask, we train 35 different models, one for each identity\nin the evaluation, by splitting the corresponding original\nvideos in two subsets. We then test each model on the self-\nand cross-reenactment videos of the corresponding identity.\nID-Reveal, trained on a large-scale dataset, learns an em-\nbedding space where real videos of a specific identity are\n8\nFace-reenactment generators\nAUC\nFace-vid2vid [46]\n0.886\nLIA [48]\n0.862\nTPS [26]\n0.867\nTable 2: Results showing that our model for avatar finger-\nprinting, trained on videos generated with Face-vid2vid,\nis also robust to other, unseen, generators (LIA [48],\nTPS [26]).\ngrouped together [15]. Since the method shows good gen-\neralization to new identities, for the task of synthetic me-\ndia detection, we directly use the pre-trained model on our\ndata to detect, once again, self- versus cross-reenactment.\nFigure 5 shows the ROC curve for our method compared\nto these baselines. Our method (AUC=0.886) outperforms\nby a wide margin both ID-Reveal (AUC=0.720), and the\nmethod by Agarwal et al. (AUC=0.726). We also note that,\nunlike ID-Reveal and our method, Agarwal et al. uses a dif-\nferent model for each identity.\n6.2. Ablation Study\nOur method outperforms existing baselines by introduc-\ning two novel components: the dynamic facial identity fea-\ntures, which capture the facial dynamics in a compact and\nexpressive way, and the loss function, which defines the\nshape of the identity embedding.Here we study the contri-\nbution of each. We evaluate the contribution of our dynamic\nfacial identity features by swapping them with 3DMM fea-\ntures [9], a popular choice to capture facial dynamics. Since\nwe use a temporal CNN backbone similar to the one from\nID-Reveal, for this ablation we use the loss function pro-\nposed in their original paper [15]. We re-train the same\nnetwork using our features and observe a jump from 0.677\nto 0.727 in terms of AUC. Upon inspection we notice that\nthe 3DMM features tend to over-smooth the facial motion,\nand are unable to capture subtle dynamics that prove criti-\ncal to avatar fingerprinting, and which our features capture.\nWe also evaluate the contribution of our identity embedding\nloss and observe a further improvement. For this last exper-\niment, we reduce F to 51 (same as in ID-Reveal) to ensure\na similar capacity and temporal receptive field as the other\ntwo models. Table 3 summarizes this ablation study. Addi-\ntional experiments showing the impact of Rj,ID1,ID2(t) and\nF are discussed in the supplementary.\n6.3. Robustness to Novel Talking-Head Generators\nWe find that our for avatar fingerprinting model, trained\nonly on synthetic videos generated using Face-vid2vid, is\nalso robust to other state-of-the-art talking-head generators.\nTable 2 shows this result on the test-set identities, with\nthe synthetic talking-head videos generated using two other\nstate-of-the-art face-reenactment generators: LIA [48] and\nTPS [26]. We attribute this robustness to our reliance only\nInput Features\nLoss\nAUC\n3DMM\nID-Reveal rec. loss\n0.677\nLM dist\nID-Reveal rec. loss\n0.727\nLM dist\nOur loss\n0.872\nTable 3: Ablation study showing the importance of our in-\nput features and loss function.\non the facial landmarks, which are known to be robust to\nsmall imperfections [3].\n6.4. Limitations\nOur algorithm is less discriminative of subjects that are\nless emotive and more neutral most of the times. In the fu-\nture, relying on more granular dynamic signatures that can\nextract micro-expressions can help alleviate this. Since our\navatar fingerprinting algorithm relies on facial landmarks,\ntheir robustness affects its accuracy as well. The perfor-\nmance of our method degrades when expressions that are\ncritical to verifying the driving identity are not captured by\nthe synthetic portrait generator. For instance, if a synthetic\nportrait generator does not animate eyebrows in the ren-\ndered video, we cannot discriminate between identities that\ndiffer only in the way they move their eyebrows. Lastly, our\ndataset currently features only one type of interaction: one-\non-one conversations. Expanding it to include other forms\nof conversational interactions would be beneficial.\n7. Societal Impact\nWe acknowledge the societal importance of introducing\nguardrails when it comes to the use of talking-head gen-\neration technology.\nWe present this work as a step to-\nwards trustworthy use of such technologies. Nevertheless,\nour work could be misconstrued as having solved the prob-\nlem and inadvertently accelerate the unhindered adoption of\ntalking head technology. We do not advocate for this. In-\nstead we emphasize that this is only the first work on this\ntopic and underscore the importance of further research in\nthis area. Since our dataset contains human subjects\u2019 facial\ndata, we have taken many steps to ensure proper use and\ngovernance with steps including: obtaining IRB approval,\ninformed subject consent prior to data capture, removing\nsubject identity information, pre-specifying the subject mat-\nter that can be discussed in the videos, allowing subjects the\nfreedom to revoke our access to their provided data at any\npoint in future (and stipulating that interested third parties\nmaintain current contact information with us so we can con-\nvey these changes to them).\n8. Conclusions\nHighly photo-real portrait talking-head generators are\nbecoming increasing beneficial to applications such as\nvideo conferencing. This trend raises the important new re-\n9\nsearch question of how best to also ensure their safe use in\nsuch scenarios. To this end, we investigate the new problem\nof avatar fingerprinting to authenticate legitimate talking-\nheads created by authorized users. We leverage the fact that\ndriving individuals have uniquely identifying dynamic mo-\ntion signatures, which are also preserved in the videos that\nthey drive. Since none exists, we contribute a new large\ndataset carefully designed to further research on avatar fin-\ngerprinting. While our seminal work shows promising re-\nsults and establishes a baseline in the field, we hope that it\nlays the foundation for much further research on this deeply\nimpactful topic.\nAcknowledgements.\nWe would like to thank the partic-\nipants for contributing their facial audio-visual recordings\nfor our dataset, and Desiree Luong, Woody Luong, and\nJosh Holland for their help with Figure 2. We acknowl-\nedge David Taubenheim for the voiceover in the demo video\nfor this work, and Abhishek Badki for his help with the\ntraining infrastructure.\nWe thank Joohwan Kim, Rachel\nBrown, Anjul Patney, Ben Boudaoud, Josef Spjut, Saori\nKaji, Nikki Pope, and Kai Pong for their help with putting\ntogether the data capture protocols, informed consent form,\nphoto release form, and agreements for data governance and\nthird-party data sharing. Koki Nagano, Ekta Prashnani, and\nDavid Luebke were partially supported by DARPA\u2019s Se-\nmantic Forensics (SemaFor) contract (HR0011-20-3-0005).\nThis research was funded, in part, by DARPA\u2019s Seman-\ntic Forensics (SemaFor) contract HR0011-20-3-0005. The\nviews and conclusions contained in this document are those\nof the authors and should not be interpreted as representing\nthe official policies, either expressed or implied, of the U.S.\nGovernment.\nDistribution Statement \u201cA\u201d (Approved for Public Release,\nDistribution Unlimited).\nReferences\n[1] avatarify-python.\nhttps://github.com/alievk/\navatarify-python. Accessed: 2023-08-09. 3\n[2] Shruti Agarwal, Tarek El-Gaaly, Hany Farid, and Ser-Nam\nLim. Detecting deep-fake videos from appearance and be-\nhavior. 2020 IEEE International Workshop on Information\nForensics and Security (WIFS), pages 1\u20136, 2020. 3\n[3] Shruti Agarwal and Hany Farid. Detecting deep-fake videos\nfrom aural and oral dynamics.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops, June 2021. 3, 9\n[4] Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh\nAgrawala.\nDetecting deep-fake videos from phoneme-\nviseme mismatches. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, 2020. 3\n[5] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He,\nKoki Nagano, and Hao Li. Protecting world leaders against\ndeep fakes.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) Work-\nshops, 2019. 3, 6, 8\n[6] Shruti Agarwal, Liwen Hu, Evonne Ng, Trevor Darrell, Hao\nLi, and Anna Rohrbach. Watch those words: Video falsi-\nfication detection using word-conditioned facial motion. In\nIEEE Winter Conference on Applications of Computer Vision\n(WACV), 2023. 3\n[7] Michael Albright and Scott McCloskey. Source generator\nattribution via inversion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) Workshops, 2019. 3\n[8] Shumeet Baluja.\nHiding images in plain sight:\nDeep\nsteganography. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2017. 3\n[9] Volker Blanz and Thomas Vetter. A morphable model for the\nsynthesis of 3d faces. In Proceedings of SIGGRAPH, 1999.\n3, 9\n[10] Volker Blanz and Thomas Vetter. Face recognition based on\nfitting a 3D morphable model. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 2003. 6\n[11] Maty\u00b4a\u02c7s Boh\u00b4a\u02c7cek and Hany Farid.\nProtecting world lead-\ners against deep fakes using facial, gestural, and vocal man-\nnerisms. Proceedings of the national academy of Sciences,\n2022. 3\n[12] Houwei Cao, David G Cooper, Michael K Keutmann,\nRuben C Gur, Ani Nenkova, and Ragini Verma. CREMA-D:\nCrowd-sourced emotional multimodal actors dataset. IEEE\nTransactions on Affective Computing, 2014. 2, 4, 5, 14\n[13] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep\nspeaker recognition. In INTERSPEECH, 2018. 3\n[14] Davide Cozzolino, Matthias Nie\u00dfner, and Luisa Verdoliva.\nAudio-visual person-of-interest deepfake detection, 2022. 3\n[15] Davide Cozzolino, Andreas R\u00a8ossler, Justus Thies, Matthias\nNie\u00dfner, and Luisa Verdoliva.\nID-Reveal: Identity-aware\nDeepFake video detection. In IEEE International Confer-\nence on Computer Vision (ICCV), 2021. 3, 6, 7, 8, 9, 14\n[16] Davide Cozzolino Giovanni Poggi Luisa Verdoliva. Extract-\ning camera-based fingerprints for video forensics. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops, June 2019. 3\n[17] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Ting Zhang,\nWeiming Zhang, Nenghai Yu, Dong Chen, Fang Wen, and\nBaining Guo. Protecting celebrities from deepfake with iden-\ntity consistency transformer. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 3\n[18] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Alek-\nsei Ivakhnenko, Victor Lempitsky, and Egor Zakharov.\nMegaPortraits:\nOne-shot megapixel neural head avatars.\n2022. 3\n[19] Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Sei-\ndel, Mohamed Elgharib, and Christian Theobalt.\nVideo-\nForensicsHQ:\nDetecting\nhigh-quality\nmanipulated\nface\nvideos.\nIn IEEE International Conference on Multimedia\nand Expo, 2021. 2, 5\n[20] Jessica Fridrich. Steganography in Digital Media: Princi-\nples, Algorithms, and Applications. Cambridge University\nPress, 2009. 3\n10\n[21] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-\ntion by learning an invariant mapping. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2006.\n3\n[22] Alexandros Haliassos, Konstantinos Vougioukas, Stavros\nPetridis, and Maja Pantic. Lips don\u2019t lie: A generalisable and\nrobust approach to face forgery detection. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2021. 5, 7\n[23] Harold Hill and Alan Johnston. Categorizing sex and identity\nfrom the biological motion of faces. Current Biology, 2001.\n1, 3, 5\n[24] Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vin-\ncent, Christopher Pal, and Jan Kautz. Improving landmark\nlocalization with semi-supervised learning. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2018. 6, 7\n[25] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu.\nDepth-aware generative adversarial network for talking head\nvideo generation. 2022. 3\n[26] Hui Zhang Jian Zhao. Thin-plate spline motion model for\nimage animation. 2022. 2, 3, 5, 9, 13\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of StyleGAN. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020. 3\n[28] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and\nEgor Zakharov. Realistic one-shot mesh-based head avatars.\nIn European Conference on Computer Vision (ECCV), 2022.\n3\n[29] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan.\nSupervised contrastive learning.\nIn Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2020. 3\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv, 2014. 7\n[31] Barbara Knappmeyer, IM Thornton, and HH B\u00a8ulthoff. Facial\nmotion can determine facial identity. Journal of Vision, 2001.\n1, 3, 5\n[32] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo\nPark, and Gyeongsu Chae. Kodf: A large-scale korean deep-\nfake detection dataset. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 10744\u2013\n10753, 2021. 4, 5\n[33] Yuezun Li, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-DF:\nA large-scale challenging dataset for DeepFake forensics. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020. 2\n[34] Baoping Liu, Bo Liu, Ming Ding, Tianqing Zhu, and Xin Yu.\nTi2net: Temporal identity inconsistency network for deep-\nfake detection. In IEEE Winter Conference on Applications\nof Computer Vision (WACV), 2023. 3\n[35] Steven R Livingstone and Frank A Russo.\nThe ryer-\nson audio-visual database of emotional speech and song\n(RAVDESS): A dynamic, multimodal set of facial and vo-\ncal expressions in north american english. PloS one, 2018.\n2, 4, 5, 14\n[36] Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, and\nPeyman Milanfar. Distortion agnostic deep watermarking.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020. 3\n[37] Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit\nWarping for Animation with Image Sets.\nIn Advances in\nNeural Information Processing Systems (NeurIPS), 2022. 3\n[38] Brais Martinez, Pingchuan Ma, Stavros Petridis, and Maja\nPantic. Lipreading using temporal convolutional networks.\nIn IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2020. 7\n[39] Qiaomu Miao, Sinhwa Kang, Stacy Marsella, Steve DiPaola,\nChao Wang, and Ari Shapiro. Study of detecting behavioral\nsignatures within deepfake videos, 2022. 3\n[40] Alice J O\u2019Toole, Dana A Roark, and Herv\u00b4e Abdi. Recog-\nnizing moving faces: A psychological and neural synthesis.\nTrends in Cognitive Sciences, 2002. 1, 3, 5\n[41] Andreas R\u00a8ossler, Davide Cozzolino, Luisa Verdoliva, Chris-\ntian Riess, Justus Thies, and Matthias Nie\u00dfner. FaceForen-\nsics++: Learning to detect manipulated facial images.\nIn\nIEEE International Conference on Computer Vision (ICCV),\n2019. 2\n[42] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-\ntian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforen-\nsics++: Learning to detect manipulated facial images.\nIn\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 1\u201311, 2019. 5\n[43] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFaceNet: A unified embedding for face recognition and clus-\ntering. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015. 3\n[44] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for im-\nage animation. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2019. 3\n[45] Matthew Tancik, Ben Mildenhall, and Ren Ng.\nStegas-\ntamp: Invisible hyperlinks in physical photographs. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020. 3\n[46] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 1, 2, 3, 4, 5, 9, 13\n[47] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and\nMatthew R Scott.\nMulti-similarity loss with general pair\nweighting for deep metric learning. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3\n[48] Yaohui Wang, Di Yang, Francois Bremond, and Antitza\nDantcheva. Latent image animator: Learning to animate im-\nages via latent space navigation. In International Conference\non Learning Representations (ICLR), 2022. 2, 3, 5, 9, 13\n[49] Lior Wolf, Tal Hassner, and Itay Maoz. Face recognition\nin unconstrained videos with matched background similar-\nity. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2011. 2\n[50] Yaser Yacoob. Gan-scanner: A detector for faces of style-\ngan+, 2021. https://github.com/yaseryacoob/\nGAN-Scanner. 3\n11\n[51] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario\nFritz. Artificial fingerprinting for generative models: Root-\ning deepfake attribution in training data. In IEEE Interna-\ntional Conference on Computer Vision (ICCV), 2021. 3\n[52] Ning Yu, Vladislav Skripniuk, Dingfan Chen, Larry Davis,\nand Mario Fritz. Responsible disclosure of generative mod-\nels using scalable fingerprinting. In International Conference\non Learning Representations (ICLR), 2022. 3\n[53] Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya,\nand Victor Lempitsky. Fast bi-layer neural synthesis of one-\nshot realistic head avatars. In European Conference on Com-\nputer Vision (ECCV), 2020. 3\n[54] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and\nVictor Lempitsky. Few-shot adversarial learning of realistic\nneural talking head models. In IEEE International Confer-\nence on Computer Vision (ICCV), 2019. 3\n12\nAvatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos\n(Supplementary)\nA9. Dataset\nWe now provide additional details for our proposed\ndataset, including details about question prompts and sen-\ntences spoken in both stages, instructions to the participants,\ndemographics of the dataset, and other relevant statistics.\nGeneral Instructions to Subjects.\nThe subjects were\nasked to join pre-assigned Google Meet video calls using a\nlaptop or a desktop. For the recorded video call, the subjects\nwere also asked to position themselves so that their face was\ncentered and parallel to the screen. However, in some cases\nwith specific video-conferencing setups, this constraint was\nonly approximately satisfied. Additionally, subjects were\ninstructed to avoid hand motion since it can occlude their\nface, and also excessive body motion that might impair\nthe visibility of their face. Before beginning each mono-\nlogue, subjects were asked to speak \u201cstart topic\u201d in a loud,\nclear voice, and, similarly, the end of each monologue was\nmarked by the subjects speaking \u201cend topic\u201d. These key-\nwords allowed for quicker time-stamped transcription to\nisolate relevant portions of the Google Meet recordings.\nRight after a subject said \u201cstart topic\u201d, they were instructed\nto pause for a few seconds and look directly at the cam-\nera with a frontal head pose, while holding a neutral ex-\npression. These frames with neutral expressions are crucial\nfor successful generation of synthetic talking-head videos\nusing face-vid2vid [46], LIA [48], and TPS [26]. These\ngenerators work by transferring expression changes from a\ndriving video to the target image. Therefore, it is impor-\ntant that the expression of the target image and that of the\nfirst frame of the driving video match. Asking subjects to\nprovide a neutral expression before commencing with their\nmonologues proves to be an effective way to achieve this:\nthese neutral frames serve as good target images, while driv-\ning videos that start with these neutral frames allow for ef-\nfectively animating the target image showing a similar ex-\npression. During the second stage of the data capture, where\nwe record scripted monologues, subjects were instructed to\nmemorize and speak the sentences to their recording part-\nner, without referring back to the printed text from which\nthey memorized the sentences. In case the subject forgot a\nsentence, they were instructed to start from the beginning\nof the sentence set. The whole recording session with both\nsubjects in a video call typically lasted an hour, which also\nincluded miscellaneous interactions in between the mono-\nlogues. The current dataset release excludes such interac-\ntions and only focuses on data captured for the two stages\n(Free-Form Monologues and Scripted Monologues).\nStage I: Free-Form Monologues.\nSubjects were asked\nto alternate between speaker and prompter roles.\nThe\nprompter\u2019s task was to ask each of the following questions\nto the speaker, and the speaker was instructed to answer\nthese questions in their natural manner.\n1. Describe a day when you had to rush to an appoint-\nment.\n2. Talk about an an important milestone you have missed\nin the past and your feelings about it.\n3. What is your favorite family holiday?\n4. How is the weather in your area typically?\n5. Is there a household chore you don\u2019t like doing?\n6. Tell me about an incident that really surprised you.\n7. Tell me about an incident that really scared you.\nStage II: Scripted Monologues.\nThe following sentence\nsets were memorized and recited by each subject (alternat-\ning with their recording partner) in the second stage of the\ndata capture. We did not ask subjects to explicitly demon-\nstrate specific emotions for any sentence set. Rather, we\nchose to allow subjects to perform these memorized sen-\ntences in a manner natural to them.\n1. My friend has a very cute dog. But, he can be scary\nwhen he barks.\n2. Will you please answer the darn phone? The constant\nringing is driving me insane!\n3. My aunt was in the hospital for a week. Unfortunately,\nshe passed away yesterday and I will need some time\nto grieve.\n4. I hate rushing to get to the airport. The stress is too\nmuch for me to handle.\n5. A slice of cake is the perfect ending to a meal.\nWouldn\u2019t you agree?\n6. It is going to be great working with you! I am surprised\nwe didn\u2019t connect sooner!\n7. You need to take the trash out right now! Your whole\napartment smells like rotten eggs!\n8. My internet connection is unreliable today. I hope it\ngets better before my meeting or I will have to call in!\n9. I know the deadline is around the corner, but I just\ndon\u2019t have any updates yet, I\u2019m sorry.\n10. Why can\u2019t the banker figure out what\u2019s going on? I\nshould have got my money last night!\n11. It\u2019s really nice out today. I might go for a walk if I get\noff work early and the kids aren\u2019t back from school.\n12. There is a famous coffee shop around the corner that\nalso serves snacks. Would you like to go tonight?\n13. My dog almost got run over by a car today! Thank\n13\nGod he is safe!\n14. It is getting very cold outside. I feel like having some\nhot chocolate. Would you like some?\n15. I have been exercising so much lately. But I am not\ngetting any stronger!\n16. I have an old tie that I can wear to the interview. My\ngrandfather gave it to me last year.\n17. I had fun last night - we had quite a few drinks. But\nI have a really bad hangover this morning and I am\nconsidering calling in sick.\n18. Please don\u2019t interrupt me when I am talking! Now I\nhave forgotten what I wanted to tell you.\n19. It was such a pleasure talking to you. I hope we stay in\ntouch.\n20. I can\u2019t believe I misplaced my keys yet again! I have\nto leave for the airport right now.\n21. Gosh! the boy jumped right off the cliff into the ocean.\nHe is lucky he didn\u2019t hit a rock.\n22. The baby just spit up on my brand new clothes. I am\ngoing to be late for our dinner tonight.\n23. The food smells disgusting but tastes delicious. How\nstrange is that!\n24. I was about to park when I saw a person with a gun. I\nkept driving and called the police right away.\n25. I decided to take a nap during my lunch break. I am so\nglad I did! I feel very refreshed.\n26. The food didn\u2019t get delivered on time. We had to keep\nour guests waiting while we searched for options.\n27. I was walking down an alley the other night. I had the\nstrange feeling that someone was following me.\n28. She twisted her ankle while ice-skating. It was her final\nperformance for the season.\n29. Who moved my boxes from this room? I need to find\nmy shoes before I can head out.\n30. We miss our old home in the mountains quite a bit.\nThis new place just doesn\u2019t feel as cozy.\nSubject demographics.\nOut of the total pool of subjects\nthat volunteered data for our 2-stage data capture, 50% are\nfemale, 47.8% are male, and the remaining chose \u201ca gen-\nder not listed here\u201d. Amongst different age groups, 37%\nof the participants are 25-34 years old, 32.6% are 35 \u2212 44\nyears old, 17.4% are 45-54 years old, 6.5% are 18-24 years\nold, and 6.5% are 55-64 years old. In terms of race and\nethnicity, 41.3% are Caucasian, 47.8% are Asian (includ-\ning South Asian, East Asian, South-east Asian), 6.5% are\nAfrican, 2.2% are Hispanic / Latino, 2.2% are Pacific is-\nlander, and others remained unspecified.\nSynthetic Talking-Head Videos.\nAs mentioned briefly\nin the main paper, we pool together videos for the 46\nidentities from our own 2-stage data capture, along with\nvideos from 24 identities of RAVDESS (scripted mono-\nClip duration F\nAUC\n31\n0.840\n51\n0.872\n71\n0.886\nTable A4: Ablation study for different values of F.\nlogues only) [35], and 91 from CREMA-D (short scripted\nmonologues only) [12], resulting in a total of 161 unique\nidentities.\nFor each of these 161 identities, the remain-\ning 160 are used to drive cross-reenactments, with 8 driv-\ning videos randomly selected from the total set of videos\nfor each driving identity.\nFor any given target identity,\nwe incorporate synthetic videos driven by every remain-\ning identity. During training, such a large variety of cross-\nreenactments enable effectively learning an appearance-\nagnostic dynamic facial identity feature space.\nA10. Implementation Details\nThe temporal ID net [15] is trained using our input fea-\ntures after appropriately modifying the number of input\nchannels to match our feature dimension. To adjust the re-\nceptive field of the temporal ID net so that it predicts an em-\nbedding vector for longer or shorter input clips, we modify\nthe number of layers of the network, and the dilation factor\nfor the layers. Specifically, here are the kernel sizes and di-\nlation factors for each of the layers in the temporal ID net,\ndepending on the choice of input clip duration F:\n1. F = 31 frames: (1, 1, 1, 1, 2, 2, 2, 2, 4)\n2. F = 51 frames: (1, 1, 1, 1, 2, 2, 2, 4, 4, 4, 4)\n3. F = 71 frames: (1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4)\nThe kernel sizes are all set to 3 apart from the first layer,\nwhich is 1. All other details of the temporal ID network are\nadapted from the existing implementation [15]. To imple-\nment the push and pull terms in Equations 1, 2, 3 in the main\npaper, n and t span over 5 consecutive F-frame clips in a\nvideo. That is, during training, the temporal ID net receives\nas input (F+4)-frame videos, and outputs 5 embedding vec-\ntors, one for each of the 5 F-frame clips in the video. The\nmax operation in Equations 1, 2, 3 is performed over the 5\ndifferent clips (therefore, 5 different values of n), and the\noverall loss term in Equation 5 accumulates over 5 values\nof t. So, when a batch of videos is loaded for a training\niteration, it comprises of (F+4)-frame \u201cvideos\u201d, which are\nsplit into 5 clips. These (F+4) frames are randomly selected\nfrom the entire video.\nA11. Evaluation\nIn Table A4, we report the the results of our experiment\nwith varying values of F, which is the number of frames\nprovided to the network to make a prediction about the dy-\nnamic facial temporal identity signature. The performance\ngained with increasing values of F diminishes at high val-\n14\nloss term\nAUC\nwithout Rj,ID1,ID2(t)\n0.851\nwith Rj,ID1,ID2(t)\n0.886\nTable A5: Ablation study demonstrating the important of\nRj,ID1,ID2(t) (with F=71 frames).\nues. We choose 71 frames as the default for most of our ex-\nperiments. For cases where shorter clips are desirable, such\nfor efficiency or for doing frequent verification, we observe\nthat F=31 is plausible\u2014with an AUC of 0.84. In Table A5,\nwe show the benefit of including Rj,ID1,ID2(t) in the loss\nterm (Equation 5).\nIn Figure A6, we show more samples similar to the ones\nshown in Figure 4 of the main paper. For each row of results\nin Figure A6, we choose a reference identity, and a held-\nout set of reference self-reenacted videos for each of these\nidentities. Then, we report the average Euclidean distance\nof the following videos with respect to the held-out self-\nreenacted videos for the reference identity:\n1. a new self-reenacted video by the reference identity\n(not included in the held-out reference set) \u2013 high-\nlighted with a green border,\n2. a cross-reenacted video where the reference identity is\nthe driver \u2013 highlighted with a green border, and\n3. two cross-reenacted videos where the reference iden-\ntity is the target, driven by some other identity \u2013 high-\nlighted with a red and a blue border.\nBased on the reported distance values, we observe that\nvideos where the reference identity is the driver are closer\nto the set of other self-reenacted videos driven by the ref-\nerence identity and far from those where reference identity\nis the target to be driven by other identities. This further\nconfirms the ability of our model to fingerprint synthetic\navatars based purely on facial motion, independent of the\nappearance of a synthetic talking-head video.\nReference Identity = ID1\nID1 \u2192 ID1 d=0.528\nID1 \u2192 ID2 d=0.712\nID2 \u2192 ID1 d=1.569\nID3 \u2192 ID1 d=2.065\nReference Identity = ID4\nID4 \u2192 ID4 d=0.544\nID4 \u2192 ID5 d=0.635\nID5 \u2192 ID4 d=1.367\nID6 \u2192 ID4 d=2.156\nReference Identity = ID7\nID7 \u2192 ID7 d=0.589\nID7 \u2192 ID5 d=0.961\nID5 \u2192 ID7 d=2.042\nID8 \u2192 ID7 d=1.787\nReference Identity = ID9\nID9 \u2192 ID9 d=0.382\nID9 \u2192 ID2 d=0.637\nID2 \u2192 ID9 d=1.066\nID3 \u2192 ID9 d=1.257\nFigure A6: Animated figure. Open in a media-enabled\nviewer like Adobe Reader and click on the inset. Contin-\nuing Figure 5 from the main paper, we show more visual re-\nsults to demonstrate that our method indeed predicts embed-\nding vectors that lie close together when the clips have the\nsame driving identity. As a reminder, for each row, we pick\na reference identity. The green box indicates reenactments\ndriven by the reference identity, the red and blue are cross-\nreenactments of the reference identity. We compute the av-\nerage distance of each clip shown here against all other clips\ndriven by the reference identity. The average distance to the\nother clips of the reference identity is consistent for a given\nmotion, and lower (better) when the reference identity is\ndriving as compared to the cross-reenactments that use the\nreference identity as target. Note that the indexing used to\ndenote an identity (such as \u201cN\u201d in IDN) is only intended to\nconvey when an identity is different from another. These in-\ndices are not global and what is denoted as ID1 in this figure\nmay not match another figure.\n15\n"
  },
  {
    "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
    "link": "https://arxiv.org/pdf/2305.03668.pdf",
    "upvote": "1",
    "text": "A Suite of Generative Tasks for\nMulti-Level Multimodal Webpage Understanding\nAndrea Burns1\u2217 Krishna Srinivasan2\nJoshua Ainslie2\nGeoff Brown2\nBryan A. Plummer1\nKate Saenko1,3\nJianmo Ni2\nMandy Guo2\n1Boston University, 2Google, 3FAIR\n{aburns4, bplum, saenko}@bu.edu, {krishnaps, jainslie, geoffbrown, jianmon, xyguo}@google.com\nAbstract\nWebpages have been a rich, scalable resource\nfor vision-language and language only tasks.\nYet only pieces of webpages are kept in ex-\nisting datasets: image-caption pairs, long text\narticles, or raw HTML, never all in one place.\nWebpage tasks have resultingly received little\nattention and structured image-text data left un-\nderused. To study multimodal webpage under-\nstanding, we introduce the Wikipedia Webpage\nsuite (WikiWeb2M) containing 2M pages with\nall of the associated image, text, and structure\ndata1. We verify its utility on three generative\ntasks: page description generation, section sum-\nmarization, and contextual image captioning.\nWe design a novel attention mechanism Prefix\nGlobal, which selects the most relevant image\nand text content as global tokens to attend to the\nrest of the webpage for context. By using page\nstructure to separate such tokens, it performs\nbetter than full attention with lower computa-\ntional complexity. Extensive experiments show\nthat the new data in WikiWeb2M improves task\nperformance compared to prior work.\n1\nIntroduction\nWebpages are a source of multimodal, structured\ncontent which have been used for both pretraining\nand finetuning purposes. Large scale noisy text\nor multimodal datasets scraped from the web have\nbeen used to pretrain large language or contrastive\nmodels (Raffel et al., 2020; Jia et al., 2021; Rad-\nford et al., 2021; Aghajanyan et al., 2022). Down-\nstream tasks built from webpages have included\ninstruction following, image captioning, news cap-\ntioning, image-sentence retrieval, and image-article\nretrieval (Shi et al., 2015; Li et al., 2020; Gur et al.,\n2022; Sharma et al., 2018; Biten et al., 2019; Liu\net al., 2021; Srinivasan et al., 2021; Tan et al., 2022).\n\u2217Work done during an internship at Google.\n1Data can be downloaded at https://github.com/\ngoogle-research-datasets/wit/blob/main/wikiweb2m.\nmd.\nFigure 1: Tasks we study with WikiWeb2M. Our dataset\nprovides a unified webpage sample that contains all text,\nimage, and structure, enabling new tasks like page de-\nscription generation. For image captioning and section\nsummarization, remaining page text and images provide\nuseful context, aiding task performance.\nYet limited prior work has studied tasks to evaluate\nmultimodal webpage understanding itself.\nMany classification and generation problems can\nbe studied with webpages: taxonomic webpage\nclassification, webpage retrieval, web image cap-\ntioning, and page summarization. However, to date\nthere is no open source, multimodal dataset that\nretains all webpage content. E.g., the Wikipedia\nImage Text (WIT) dataset (Srinivasan et al., 2021)\ndoes not retain HTML structure and misses out on\ntext sections. Thus, we propose the new Wikipedia\nWebpage (WikiWeb2M) dataset of over 2M pages,\nwhich unifies webpage content to include all text,\nimages, and their location (e.g., section index) in\na single sample. Table 1 compares the statistics of\nWikiWeb2M to the existing English WIT dataset.\nFigure 1 shows an example of our WikiWeb2M\nbenchmark suite; we design a set of tasks that re-\nquire webpage understanding at varying degrees of\narXiv:2305.03668v2  [cs.CL]  20 Oct 2023\nDataset\n# Webpage Sections\n# Images\nStructural Heading\nText\nImage\nBoth\nTotal\nUnique\nTotal\nWIT (En)\n-\n-\n-\n199,872 2,847,929\n3,047,801\n3,660,211 4,955,835\nWikiWeb2M\n731,394\n686,376 6,817,950 221,523 3,236,254 11,693,497 4,438,642 5,940,431\nTable 1: WikiWeb2M versus WIT (Srinivasan et al., 2021). WikiWeb2M re-introduces millions of text and\nmultimodal webpage sections. We report counts over all splits; train, validation, and test are reported separately in\nAppendix A. WikiWeb2M and WIT (English subset) come from the same webpages.\ngranularity. Specifically, we use page description\ngeneration, section summarization, and contextual\nimage captioning to evaluate a model\u2019s ability to un-\nderstand a webpage at a global, regional, and local\nlevel, respectively. For page description genera-\ntion, the goal is to generate an overarching global\ndescription of the webpage. The task of section\nsummarization generates a sentence that captures\nthe key content of one section. Finally, contextual\nimage captioning generates a caption for one image\nwithin the webpage.\nWikiWeb2M\u2019s tasks will allow for general study\nof multimodal content understanding with many-\nto-many text and image relationships and can also\nspecifically improve interaction with web content.\nFor example, a webpage description may provide a\nuser who is blind more agency by allowing them to\npreview content before listening to the entire body\nof image and text with a screen reader (Vtyurina\net al., 2019). In addition to contextual captioning\nand section summarization aiding assistive technol-\nogy, these tasks can be used for modern content\ngeneration, as there is growing interest in providing\nmultimodal web snippets (Nkemelu et al., 2023).\nThe study of webpages in a multimodal context\nhas even been motivated from a sociological and\nanthropological perspective (Pauwels, 2012).\nWhile we curate a new dataset with Wikipedia,\nwe note it is just one of many domains that could\nbe used to study multimodal webpage understand-\ning. Instructional websites, news articles, recipes,\nblogs, and more have bodies of text and images\ninterleaved by layout or HTML structure.\nWe utilize the T5 (Raffel et al., 2020) framework\nto address the WikiWeb2M tasks. One challenge in\nmodeling webpage tasks is the length of the input\ndata (i.e., a long sequence results from flattening\nwebpage text and images). While the full attention\noriginally used in T5 is performant, it results in a\nquadratic computational complexity with respect to\nthe input sequence length. Thus, we define a new\nmixture of local-global attention, Prefix Global,\nwhich uses our structured webpage data to select\nthe most salient text and images as global tokens\nin the prefix of our input sequence. Prefix Global\nis ultimately more efficient, meaning longer input\nsequences can be used to reach better task perfor-\nmance. Our results can be beneficial to the many\nstructured image-text domains outside of webpages\nsuch as mobile apps, figures, posters, infographics,\nand documents.\nWe include ablations across multiple axes: the\npretrained checkpoint we initialize from, the in-\nput sequence length, the feature inputs, and the\nattention mechanism. We importantly find that im-\nages improve performance for all tasks, while prior\nwork on contextual image captioning claimed oth-\nerwise (Nguyen et al., 2022). We are also able to\nimprove task performance now that we have access\nto the entire page\u2019s content. Still, there is plenty of\nroom to improve upon our benchmark suite.\nWe summarize our contributions below:\n\u2022 A new open source multimodal webpage\ndataset, WikiWeb2M, containing 2M pages\ncurated from English Wikipedia articles. Each\nsample contains all text, images, and structure\npresent per page.\n\u2022 A suite of multimodal generation webpage\ntasks that reflect webpage understanding at\nthree granularities: page description, section\nsummarization, contextual image captioning.\n\u2022 A new attention mechanism, Prefix Global,\nwhich is a mixture of local-global attention\nthat separates a prefix of global tokens. By\ndefining more salient content from structured\npages, it can outperform full attention while\nrequiring fewer attention computations.\n\u2022 Ablations on attention, sequence length, input\nfeatures, and model size. Images can help\nall tasks, notably by over 15% on contextual\ncaptioning, and page context boosts average\nperformance by over 4% and 3% for section\nsummarization and captioning, respectively.\nWikiWeb2M\nTrain\nVal\nTest\n# Pages\n1,803,225\n100,475 100,833\n# Sections\n10,519,294 585,651 588,552\n# Total Images\n5,340,708\n299,057 300,666\nTable 2: Breakdown of the number of pages, sections,\nand images contained in each WikiWeb2M dataset split.\n2\nThe WikiWeb2M Dataset\nWe create the Wikipedia Webpage (WikiWeb2M)\ndataset to have an all-in-one multimodal webpage\ndataset where all text, image, and structure content\nis retained. WikiWeb2M is built by starting with\nthe Wikipedia Image Text (WIT; Srinivasan et al.,\n2021) English pages2. We re-scrape webpage sam-\nples and keep all text, image, and structure avail-\nable, providing more contextual data which can be\nused to model existing tasks like contextual image\ncaptioning, as well as enable new webpage un-\nderstanding tasks like page description generation.\nWe start with WIT URLs to create a high quality\nmultimodal webpage dataset that has already gone\nthrough extensive content and topic filtering.\nEach webpage sample includes the page URL,\npage title, section titles, section text, images and\ntheir captions, and indices for each section, their\nparent section, their children sections, and more.\nThis differs from WIT, which defined individual\nsamples as image-caption pairs with metadata (e.g.,\noriginating section title). Appendix A.3 includes\na comparison of fields available in WikiWeb2M\nversus WIT. In Table 1, we report the number of\nsections and images compared to the English sub-\nset of WIT. We add nearly 1M total images to the\ndataset by keeping the images on a webpage regard-\nless of whether they have image captions available.\nWe provide section counts by type: structural,\nheading, text only, image only, and both text and\nimage. Structural and heading sections do not con-\ntain immediate text. The former has subsections.\nFor heading sections, a section title was available,\nwhile the content linked to a different article, was\nempty, or only had tables. We only retain sections\nif they are content sections (e.g., not the \u201cSee Also\u201d\nsection). A significant 6.8M text sections are in\nWikiWeb2M, none of which were available in WIT.\nFor image quality control, we keep JPEG and PNG\nimage types3. We make a random 90/5/5 split and\n2WIT held a CC BY-SA 3.0 license, and additional data\nwe recover in WikiWeb2M is publicly available on Wikipedia.\n3We release image URLs, where they can be fetched.\nTask\nTrain\nVal\nTest\nPage Desc.\n1,435,263\n80,103\n80,339\nSection Summ.\n3,082,031 172,984 173,591\nImage Caption.\n2,222,814 124,703 124,188\nTable 3: Number of samples for page description gen-\neration, section summarization, and image captioning\ntask datasets after additional filtering of WikiWeb2M.\nshow the number of pages, sections, and images per\nsplit in Table 2. Note that Table 2 reflects statistics\nof WikiWeb2M, which is later refined to build our\ndownstream tasks datasets. It can be repurposed for\nother webpage understanding tasks or reprocessed\nwith different data filters.\n2.1\nThe WikiWeb2M Tasks\nWe apply WikiWeb2M to three tasks which reflect\ndifferent granularities of webpage understanding:\nthe page, section, or element level. Table 3 contains\ntask sample counts which are achieved by further\ntask-specific filtering; these data processing steps\nare included in Appendix A.1, with a discussion\nof potential dataset noise in Appendix A.2. We\ndescribe the tasks below.\nPage Description Generation. In the task of page\ndescription generation, the goal is to generate a de-\nscription of a page given the rest of the webpage\u2019s\nimage, text, and structure. We use the Wikipedia-\nprovided page description (not collecting annota-\ntions) and generate summaries from multimodal\ninputs, which differs from existing text-only article\nsummarization work; this matters when we want to\ncreate a multimodal snippet from a webpage.\nSection Summarization. The goal of section sum-\nmarization is to generate a single sentence that high-\nlights a particular section\u2019s content. The summary\nis generated given all images and (non-summary)\ntext present in the target and context sections; see\nFigure 3 for a task example. Following the lead-\ning sentence bias, we use the first sentence of a\nsection as a pseudo summary (which is removed\nfrom the model inputs). We also found that a major-\nity of human annotators deemed the first sentence\nas a reasonable summary; these findings are later\ndiscussed in Appendix F.\nContextual Image Captioning.\nNguyen et al.\n(2022) proposed contextual image captioning with\nWIT as the task of captioning an image given the\nimage and its webpage context. Target images are\nthose available in WIT to ensure they have quality\nFigure 2: Local-global attention schemes. On the left we show Transient Global (TGlobal), which has local to local\nand local to global attention (Guo et al., 2022). We propose the new Prefix Global attention which additionally has\nglobal to global and global to local attention compared to TGlobal. We define global tokens as a fixed-length prefix\nof the input, unlike TGlobal which defines additional global tokens that are aggregates over the full input sequence.\ncaptions that can be reconstructed. A Wikipedia im-\nage can have three caption types (not all are always\navailable): the alt-text, reference, and attribution\ndescriptions. Alt-text serves as a text description\nfor accessibility purposes, the reference description\ncomes directly below the image in the rendered\nwebpage, and the attribution description contains\ncaptions unique to the image across all webpages it\nappears in. Prior work only input the image, attribu-\ntion description and associated section text because\nthat was all that was available.\n3\nPrefix Global Attention\nWhen structured image-text data is available, we\nneed not treat all images and text equally. With\nwebpages, it may be more sensible to isolate cer-\ntain parts as more important. E.g., in contextual\nimage captioning, the model should focus on the\ntarget image and section it came from, while us-\ning the rest of the page as additional context. We\ncan now isolate these inputs with the WikiWeb2M\ndataset because we have structural metadata signal-\ning where each image and text element are located,\nas opposed to a bag of images and a single long\nbody of text. I.e., the new structure available in our\ndataset can both serve as new inputs to the model\nand enable new attention mechanisms.\nWe thus propose Prefix Global, a local-global\nattention, to capitalize on this intuition. A mixture\nof local and global attention weights provides the\nmeans to designate certain inputs as \u201cglobal\u201d to-\nkens which can specially attend to the rest of the\ninput sequence, while others only have local atten-\ntion to a radius of r tokens to the left and right.\nNot only is it desirable to prioritize more salient\nimage and text content from the input data, but it\ncan also reduce the computational complexity of\nthe attention mechanism. While full attention is\nperformant by allowing all input tokens to attend to\neach other, it results in a quadratic computational\ncomplexity (O(l2) for a sequence of length l).\nFigure 2 illustrates our Prefix Global and prior\nwork\u2019s Transient Global attention schemes, where\nin each the ith row represents what the ith token\ncan attend to. Guo et al. (2022) introduced LongT5\nas an adaptation of the T5 model with Transient\nGlobal (TGlobal) attention to balance the efficiency\nof local attention, which allows for much longer in-\nput sequences to be held in memory, with the higher\nperformance of full attention. TGlobal resulted in\nsimilar or better performance than full attention\nwith much longer sequence lengths, while having\na complexity of O(l(r + k)) for a variety of text\nsummarization tasks (where k =\nl\n16).\nIn addition to the local attention of TGlobal (see\nthe blue attention diagonal in Figure 2 (left)), \u201ctran-\nsient\u201d global tokens are defined on the fly per layer.\nTGlobal defines k globals as the average of every\n16 input tokens, which are additionally attended to\nby all other inputs. As a result, TGlobal has more\nglobal tokens as the sequence length increases. In\ncontrast, shown in Figure 2 (right), Prefix Global\nuses a constant number of global tokens. Specifi-\ncally, it takes a prefix of the input sequence. This is\nFigure 3: WikiWeb2M section summarization with Prefix Global. With WikiWeb2M, we can now use section\nstructure to separate the most relevant webpage content. The global tokens of Prefix Global (in green) are the first\n512 tokens of the target section to be summarized: the first x images of the section, the section index, title, body text,\nand captions. Then the remaining sections (in blue) from the webpage are input; these have local attention, while\nthe prefix global tokens attend to every other token. We decode the summary (in orange) given the page inputs.\ninspired by the leading sentence bias (Kedzie et al.,\n2018; Xing et al., 2021; Zhu et al., 2021), which\nshows that earlier content in a body of text is often\nof greater importance. We define different prefixes\nfor each task in Section 4. While we use section\nstructure to define our prefixes, Prefix Global can\nuse structure from other sources: HTML/the Doc-\nument Object Model, rendered webpage regions,\nPDF document layouts, or simply knowing a priori\nwhat task inputs are most salient.\nPrefix Global has a computational complexity of\nO((l \u2212 k) \u00b7 r + k \u00b7 l) for k global tokens, similar to\nlocal-global attention schemes ETC (Ainslie et al.,\n2020), Longformer (Beltagy et al., 2020), and Big-\nBird (Zaheer et al., 2020). However, Prefix Global\ndoes not require any special pretraining and instead\nfinetunes directly from full attention checkpoints\n(T5 in our case). This is distinct from LongT5,\nwhich also required pretraining with TGlobal atten-\ntion to be effective. Thus, as we show in Section 5\nwith Prefix Global\u2019s higher performance, it is both\na more flexible and performant attention. We also\nare the first to demonstrate using a local-global at-\ntention with multimodal inputs, and further show\nPrefix Global\u2019s ability to be performant in multi-\nmodal finetuning from a text-only checkpoint.\n4\nExperiments\nWe now detail the model variants used for experi-\nments, parameter settings for reproducing our set\nup, the metrics used for evaluation, and key abla-\ntions we perform.\nModel Architectures. We benchmark with the\nT5 (Raffel et al., 2020) encoder-decoder frame-\nwork. T5 takes a sequence of image and text inputs\nand we embed images in our input sequence using\na frozen ViT model (Dosovitskiy et al., 2021). We\nnote that finetuning ViT may further improve per-\nformance. We compare three models defined by\ndifferent encoder attention schemes: the original\nT5 which uses full attention, LongT5 with TGlobal\nattention by Guo et al. (2022) (checkpoints are pub-\nlicly available), and our Prefix Global attention. We\nfinetune all models from a T5 checkpoint pretrained\nwith full attention on the text-only C4 dataset, and\na ViT pretrained on either ImageNet (Deng et al.,\n2009) or JFT (Hinton et al., 2015).\nParameter Settings. We finetune each model for\n218 steps as done by Raffel et al. (2020) with a\n128 batch size. Each model is trained on 16 TPUs,\nwith the base model taking between 24-32 hours\nto run4 (varies by task) with an input sequence\nlength of 1024. We do not perform hyperparameter\ntuning: all models use the Adafactor optimizer\nwith a constant learning rate of 1e-3, an Adafactor\noffset of 1M to account for pretraining steps, and\nloss normalizing factor of 218. For Prefix Global\nexperiments, the default prefix size k is 512. For\nboth Transient Global and Prefix Global, the local\nattention neighborhood r is set to 127, as done in\nLongT5 (Guo et al., 2022).\nMetrics. For quantitative results, we report BLEU-\n4 (Papineni et al., 2002), ROUGE-L (Lin, 2004),\nand CIDEr (Vedantam et al., 2015) metrics from\na single run. BLEURT (Pu et al., 2021), CLIP-\nScore and RefCLIPScore (Hessel et al., 2021) are\nadditionally reported in Appendix C for all results\nin the main text. We include qualitative results in\nAppendix B; we perform two qualitative studies\n4Example packing can further improve model efficiency.\nto (1) inspect the quality of generated text for all\nfinetuning tasks and (2) discuss when and why im-\nages may help the more text-based tasks of page\ndescription generation and section summarization.\nAblations. We compare each attention at differ-\nent input lengths. Our sequence length ablations\nalso include experiments where Prefix Global and\nTGlobal have the same number of global tokens\nto strictly compare how they define global tokens.\nThen we ablate webpage inputs (section text, ti-\ntles, structure, images, image captions) and use the\nbest feature combinations for any remaining exper-\niments. We run experiments with different model\nsizes (B16 or L16 T5 + ViT5) for Prefix Global\nat a 1k input sequence length. Lastly, we verify\nthat WikiWeb2M\u2019s new annotations improve per-\nformance over prior work. Specifically, we ablate if\nthe target, description, or context sections are input\nand if sections only from WIT vs. WikiWeb2M are\ninput (since many text and multimodal context sec-\ntions were not originally kept in the WIT dataset).\n4.1\nDefining Prefix Global Attention Inputs\nEach sample\u2019s images are always included as part\nof the input\u2019s prefix tokens. We ablated the number\nof images that contribute to each task\u2019s prefix and\ninclude ablations in Appendix D.3. We use six\nimages for page description and one image input\nfor section summarization and image captioning.\nWe describe each task\u2019s prefix below. Note that\nwe remove the text that serves as the target sum-\nmary or caption from our inputs to the model for\neach task; this ensures there is no model \u201ccheating.\u201d\nE.g., for section summarization, since we utilize the\nfirst sentence of a section as its pseudo target sum-\nmary, we remove it from the inputs to the model.\nPage Description. We input the images, page URL,\npage title, and all sections (index, title, text, cap-\ntions) in their structured page order. In addition\nto the images, URL, and page title participating\nin the prefix, we also include all section titles and\nsection first sentences (up to 512 tokens). This\noutperformed keeping the section titles and text\nconcatenated in order; see Appendix D.1.\nSection Summarization. The target section to be\nsummarized is prepended to each sample\u2019s input se-\nquence. This means the target section\u2019s index, title,\nnon-summary text, images, and captions contribute\nto the global tokens of Prefix Global. Then the page\n5The base/large T5 model used 220M/770M parameters.\nURL, title, and remaining sections follow in order.\nFigure 3 illustrates how an input sequence is de-\nfined with Prefix Global for section summarization.\nContextual Image Captioning. Similar to section\nsummarization, the target image and its originating\nsection\u2019s content contribute to the prefix tokens (the\nindex, title, text, and non-target captions), followed\nby the URL, page title, and context sections.\n5\nResults\nWe now detail experimental results, first evaluating\nperformance and efficiency of each attention type\nat different sequence lengths. Then, we report input\nfeature, model size, and annotation ablations.\n5.1\nAttention and Sequence Length\nPerformance Comparison. We begin by evaluat-\ning performance for each task (page description,\nsection summarization, and contextual image cap-\ntioning) when training T5 encoders with different\nattention types and input sequence lengths in Fig-\nure 4. Prefix Global always performs better than\nTGlobal. We include two Prefix Global settings: a\nfixed Prefix512 which sets 512 input tokens to the\nprefix (default used for all other experiments), as\nwell as a PrefixTGlobal which assigns the same num-\nber of global tokens as TGlobal. PrefixTGlobal uses\nl\n16 globals, where l is the input sequence length\n(TGlobal aggregates every 16 input tokens as a\nglobal token). This allows us to compare the way\nboth attention mechanisms define global tokens.\nDespite TGlobal defining additional side inputs\nas global tokens, it consistently underperforms Pre-\nfix Global even with the same number of globals.\nThis confirms that defining a special prefix from the\ninput sequence is better than taking aggregates over\nthe full sequence. In Appendix D.1, we also show\nthat just using the prefix of the in-order page inputs\nfor page description (as opposed to pulling out the\nsection titles and first sentences) performs better\nthan TGlobal. These results collectively show Pre-\nfix Global to be preferable to TGlobal. One key\ntakeaway is that separating out more relevant inputs\n(via structure or other known biases like leading\nsentence bias) is a good idea.\nFull attention and Prefix Global generally have\nhigher performance at longer sequence lengths. It\nis impressive that Prefix Global scales or maintains\nperformance with larger sequences even when its\nnumber of globals is fixed to 512 (i.e., the num-\nber of globals is not scaled with respect to input\nFigure 4: Encoder attention and sequence length experiments. We use Prefix Global, TGlobal, and full attention\nat 1k, 2k, and 4k sequence lengths. Our experiments verify that Prefix Global is more performant than prior\nlocal-global attention TGlobal, and can even be more performant than full attention at long sequence lengths. Note\nthat full attention at the 4k sequence length does not fit into memory. ROUGE-L is plotted.\nInput\nLength\nAttention Mechanism\nTGlobal\nPrefix Global\nFull\n1024\n325,632\n916,480\n1,048,576\n2048\n782,336\n2,225,152\n4,194,304\n4096\n2,088,960\n4,842,496\n16,777,2166\nTable 4: The approximate number of FLOPs for each at-\ntention ignoring the # of attention heads and embedding\ndimension (both are the same for each attention). As\nsequence length increases, Prefix Global requires much\nfewer computations than full attention.\nlength). On the other hand, while TGlobal scales\nthe number of globals to sequence length, its per-\nformance does not consistently scale. E.g., perfor-\nmance plateaus or even drops at 4k input sequence\nlength for page description and section summariza-\ntion, respectively. This may be because TGlobal\ndefines globals as aggregates over the full input se-\nquence, which could introduce more noise or less\nsemantically rich text at longer sequence lengths.\nOne anomalous result occurs for image caption-\ning: Prefix Global with 256 globals (PrefixTGlobal\nat 4k input length) outperforms the 512 variant; as\nwe did not exhaustively ablate the number of global\ntokens, further performance gains could be reached\nby optimizing the number of globals per task.\nPrefix Global outperforms full attention at all se-\nquence lengths on image captioning, which may be\ndue to the global tokens including the target image\nand most relevant section content. This should ease\nthe process of learning the most relevant tokens by\nallowing full attention between the first k target\nsection tokens with the rest of the input sequence,\nwhile contextual information from other sections\nhas local attention. For section summarization and\n6Full attention will OOM at 4k input length.\npage description, Prefix Global outperforms full\nattention at the 4k sequence length, while full at-\ntention cannot fit in memory. Given that the entire\npage\u2019s content can be useful for generating a page\nlevel description, it is sensible that full attention\nmay perform better for smaller sequence lengths as\nit allows for attention between all input tokens.\nEfficiency Comparison. Prefix Global can outper-\nform full attention, while only requiring O((l \u2212\nk) \u00b7 r + k \u00b7 l) attention complexity for k global\ntokens. When implementing the Prefix Global at-\ntention, we manually created tensors representing\nblock sparsity to avoid computing the full cross\nattention. We provide the approximate number of\nFLOPs for each attention mechanism in Table 4\nwhen ignoring the number of attention heads and\nembedding dimension. At the 2k input sequence\nlength Prefix Global requires about half the FLOPs\nof full attention, and experimentally takes about\nhalf the time to complete the same experiment with\nall other settings fixed. The number of FLOPs of\nPrefix Global at 4k is just over those of full atten-\ntion at the 2k input length, and is able to fit into\nmemory and maximize performance for each task.\nLastly, the full attention and Prefix Global FLOP\ndifference grows with sequence length. This can\nsometimes be seen experimentally: performance\ngaps are larger between full and Prefix Global for\npage description at 2k vs. 1k (0.20 vs. 0.09).\n5.2\nFeature Ablations\nWe investigate the role of each input feature with\nPrefix Global attention and fix sequence length to\n1k. Starting with just the text available from web-\n7Structure features are kept if they were helpful in text-\nonly experiments. I.e., they are included for page description\nand image captioning, but not for section summarization.\nFeature Inputs\nPage Desc.\nSection Summ.\nImage Caption.\nText Title Struct Caption Image\nB\nR\nC\nB\nR\nC\nB\nR\nC\n\u2714\n13.60 37.75 77.12\n9.48\n28.35 65.75\n9.83\n33.00 133.70\n\u2714\n\u2714\n13.63 37.88 77.97\n9.78\n29.14 68.90\n9.84\n33.40 135.30\n\u2714\n\u2714\n\u2714\n14.07 37.96 77.88\n8.70\n29.24 69.19 10.15 33.38 135.10\n\u2714\n\u2714\n\u2714\n13.12 38.43 81.19 10.08 29.23 69.45\n9.90\n33.57 136.03\n\u2714\n\u2714\n\u2714\n\u2714\n13.22 38.38 81.38\n9.51\n29.22 69.24 10.03 33.69 137.07\n\u2714\n\u2714\n\u27147\n\u2714\n13.16 37.96 78.39\n9.31\n29.20 69.19 11.74 37.46 156.34\n\u2714\n\u2714\n\u2714\n\u2714\n14.00 38.50 81.49 10.12 29.43 69.89 11.84 37.69 158.19\nTable 5: Feature ablations with WikiWeb2M. We ablate over the section body text, title, structure, captions, and\nimages. Utilizing multimodal inputs results in the best performance for all tasks. We report BLEU-4 (B), ROUGE-L\n(R) and CIDEr (C) metrics.\nTask\nModel\nViT\nData\nMetric\nB\nR\nC\nPage\nDesc.\nBase\nim21k 14.00 38.50\n81.49\nJFT\n13.25 38.49\n82.02\nLarge\nim21k 14.67 39.63\n88.90\nJFT\n14.56 39.56\n88.48\nSection\nSumm.\nBase\nim21k 10.12 29.43\n69.89\nJFT\n10.15 29.40\n70.03\nLarge\nim21k 11.10 30.61\n76.87\nJFT\n11.24 30.54\n76.92\nImage\nCap.\nBase\nim21k 11.84 37.69 158.19\nJFT\n11.66 37.35 156.01\nLarge\nim21k 12.51 38.05 162.31\nJFT\n12.08 37.33 158.81\nTable 6: Pretrained model checkpoint ablations. We\nvary the size of the T5 and ViT models (Base means\nboth T5 and ViT are base-sized models, Large means\nboth are large models) and which image dataset the ViT\nmodel was pretrained with (ImageNet or JFT-300M).\npage sections, we incrementally add section titles,\nindices and special tokens defining section struc-\nture (the struct column of Table 5), the captions of\nimages within each section, and the images. Each\ninput boosts performance8 except section structure\nwhich has mixed results; for multimodal experi-\nments we include these extra tokens if they helped\nin the text-only experiments. This may be due to\nthese extra tokens consuming global tokens in the\nprefix that otherwise could have been more useful.\nImages and their captions both improve perfor-\nmance, but result in the highest performance for\neach task when used in tandem. This illustrates\nthat even when text captions are available, having\ntheir visual counterpart is beneficial. In Table 5,\n8BLEU-4 is less consistent than ROUGE-L and CIDEr.\nwhen we include captions for the image caption-\ning task, it refers to context captions from other\nimages in the page that never serve as target im-\nages. Interestingly, this boosts performance. We\nsuspect contextual captions help the model to learn\nthe style of captions we aim to generate.\n5.3\nPretrained Checkpoint and Model Size\nIn Table 6, we perform additional experiments with\nViT pretrained on JFT and large T5/ViT models.\nUnsurprisingly, larger models result in better per-\nformance. For page description and section sum-\nmarization, scaling the model size results in larger\nperformance gains than the impact of any individ-\nual feature we ablated. On the other hand, model\nsize has smaller gains for image captioning com-\npared to the impact of our feature ablations; the\nworst to best performance gap changed by an av-\nerage of 17.66% for feature ablations and only by\n2.43% for model size, where we average the perfor-\nmance delta of BLEU-4, ROUGE-L, and CIDEr.\nPreference to ViT representations pretrained on\nJFT or ImageNet varies by task: section summa-\nrization tends to prefer JFT, while page description\nand image captioning consistently perform best\nwith large ImageNet trained representations.\n5.4\nComparison to WIT Annotations\nThe proposed WikiWeb2M is a superset of WIT.\nFor the same set of webpages, we unify all sections\ninto a webpage sample and reintroduce millions\nof sections and images that were not kept in WIT.\nTable 7 contains runs when using the original WIT\ndata, the WIT data reprocessed to join the page sec-\ntions it originally contained, and our WikiWeb2M.\nFor section summarization, the page description\nis more important than the other context sections.\nThe page description may be more generally rele-\nTask\nInput Section Type\nSection\nSource\nMetric\nTarget Description Context\nBLEU-4 ROUGE-L CIDEr\nSection Summarization\n\u2714\nWikiWeb2M\n8.90\n27.82\n60.20\n\u2714\n\u2714\n9.46\n28.86\n66.67\n\u2714\n\u2714\n\u2714\n10.12\n29.43\n69.89\nImage Captioning\n\u2714\nWIT\n10.92\n36.21\n148.53\n\u2714\n\u2714\nWIT\n11.21\n36.63\n150.98\n\u2714\n\u2714\n\u2714\nWIT\n11.45\n36.88\n152.69\n\u2714\n\u2714\n\u2714\nWikiWeb2M\n11.84\n37.69\n158.19\nTable 7: Section input ablations. We vary using the target section, page description, and context sections. For image\ncaptioning, we also vary whether the sections come from the smaller WIT or our WikiWeb2M superset - we do not\nrun this ablation for section summarization, as it would result in a different number of train/val/test samples. Results\nshow that using all section types and the annotations made newly available with WikiWeb2M improve performance.\nvant to all sections, while each section to be sum-\nmarized contains a distinct topic compared to the\ncontext sections from other parts of the webpage.\nLastly, we find WikiWeb2M\u2019s additional context\nsections improve captioning performance the most\ncompared to those already available in WIT (com-\nparing the last two rows of Table 7). This confirms\nthe importance of the new annotations in Wiki-\nWeb2M compared to those available in prior work.\n6\nRelated Work\nWebpage tasks have been studied with text only\nHTML for web element classification, HTML de-\nscription generation, and web navigation. Gur et al.\n(2022) proposed finetuning Large Language Mod-\nels for these tasks. Reinforcement Learning meth-\nods have also trained agents to perform language\ncommands in handcrafted web environments (Gur\net al., 2019; Liu et al., 2018; Jia et al., 2019).\nWikipedia has previously been used to develop\ndownstream tasks. For example, WIT (Srinivasan\net al., 2021) released image-caption pairs from\nWikipedia, in addition to some contextual section\ntext. While WIT does not contain all of the page\ncontent, Nguyen et al. (2022) studied contextual\nimage captioning with the available annotations.\nThis is a webpage task and not strictly an image-\ntext problem, as additional section text is included\nto aid in Wikipedia image captioning, where cap-\ntions often contain finer-grained, knowledge based\ninformation. AToMiC also studied ways to improve\nmultimodal retrieval with Wikipedia by defining\nmore realistic evaluation sets (Yang et al., 2023).\nAghajanyan et al. (2022) proposed CM3, a\nTransformer with a causally masked pretraining\nobjective. While CM3 relied on pretraining data\nfrom the web containing the images and HTML\nof a webpage, this dataset was not open sourced.\nTheir results illustrated that rich HTML data could\nbe used to learn representations for tasks such as\nimage generation, image in-filling, and entity dis-\nambiguation and linking. This demonstrates that\nwebpage data can generalize to non-webpage tasks,\nbut leaves webpage specific problems unexplored.\nTo our knowledge there is no open source mul-\ntimodal webpage data that captures all modalities.\nC4 was recently extended to a multimodal version,\nMMC4 (Zhu et al., 2023). However, MMC4 does\nnot retain structure, and instead uses CLIP scores\nto noisily match images to chunks of text that it\ncould be aligned with. MMC4 has not yet been\nused for pretraining or downstream applications.\nIn mobile apps, the closest domain to webpages,\nthere are two open source datasets that contain all\nmodalities (text, image, and structure): Rico (Deka\net al., 2017) and MoTIF (Burns et al., 2022).\n7\nConclusion\nIn this paper we study three generative tasks for\nmultimodal webpage understanding: page descrip-\ntion generation, section summarization, and con-\ntextual image captioning. To do so, we present the\nWikiWeb2M dataset, which retains all of the text,\nimages, and structure from more than 2M pages.\nWe propose a new attention, Prefix Global, which\noutperforms full attention by allowing the most\nsalient text and images to specially attend to all in-\nputs. Extensive ablations on attention mechanism,\nsequence length, model size and checkpoint, input\nfeatures and section type reveal the most impactful\nfactors on our benchmark suite and verify using\nWikiWeb2M to study webpage understanding.\nLimitations\nThe WikiWeb2M dataset reprocessed the webpages\navailable in WIT. We begin with only the English\nsubset of WIT, while it originally contained 108 lan-\nguages. Our dataset is limited to English and does\nnot cover the vast multilingual data on Wikipedia.\nWe can extend our dataset to cover all languages in\nWIT, but acknowledge it is monolingual to date.\nFor page description generation and section sum-\nmarization, we use pseudo summaries that are read-\nily available from Wikipedia pages. While this\nis desirable from a scalability perspective and is\npracticed in other works, it can limit the evalua-\ntion quality of these tasks. However, we did per-\nform a small scale pilot to collect human annota-\ntions for the section summarization task in which\nwe asked the annotators if the first sentence suf-\nficed; 94% of the time the majority vote out of\nfive was yes. Pseudo summaries have also been\nused for other tasks like summarizing instructional\nvideos (Narasimhan et al., 2022).\nFor the model settings we explore, we did not try\nall exhaustive combinations of features, attention\nmechanism, model configuration, and input length.\nWe also only use T5 variants, but note T5 is state-\nof-the-art for generation style problems. Lastly, we\ndesign our set of fine-tuning tasks for generative\ntasks. Our work currently does not include tasks\nlike webpage taxonomy classification or webpage\nretrieval, but additional tasks like topic classifica-\ntion could be performed with WikiWeb2M.\nEthics Statement\nWhile the Internet provides a vast and rich do-\nmain to collect data from, it also has potential risks.\nWikipedia is a highly curated and monitored knowl-\nedge base of articles, but it can be edited by the\npublic, which can create potential quality risks. Ad-\nditionally, Wikipedia is a largely fact-based domain,\nwhere incorrectly summarizing an article could re-\nsult in misinformation. We hope our dataset can\nbe used as a new resource to improve the accuracy\nand factual correctness of text generation machine\nlearning models. As we use Wikipedia data, there is\nno user data nor P.I.I. in the proposed WikiWeb2M\ndataset. Additionally, we ran analysis to remove\na small subset of pages with potentially sensitive\ntopics (e.g., natural disasters, funeral, blood).\nAcknowledgements\nThis work is supported, in part, by the Google Ph.D.\nFellowship program.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\nand Luke Zettlemoyer. 2022. Cm3: A causal masked\nmultimodal model of the internet. arXiv:2201.07520.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nAli Furkan Biten, Llu\u00eds G\u00f3mez, Mar\u00e7al Rusi\u00f1ol, and\nDimosthenis Karatzas. 2019. Good news, everyone!\ncontext driven entity-aware captioning for news im-\nages. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha\nKumar, Kate Saenko, and Bryan A. Plummer. 2022.\nA dataset for interactive vision language navigation\nwith unknown command feasibility. In European\nConference on Computer Vision (ECCV).\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hi-\nbschman, Daniel Afergan, Yang Li, Jeffrey Nichols,\nand Ranjitha Kumar. 2017. Rico: A mobile app\ndataset for building data-driven design applications.\nIn User Interface Software and Technology (UIST).\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. 2009. ImageNet: A Large-Scale\nHierarchical Image Database. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021.\nAn image\nis worth 16x16 words:\nTransformers for image\nrecognition at scale. In International Conference on\nLearning Representations (ICLR).\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2022. LongT5: Efficient text-to-text transformer for\nlong sequences. In Findings of the Association for\nComputational Linguistics: NAACL.\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa\nSafdari, Austin Huang, Aakanksha Chowdhery, Sha-\nran Narang, Noah Fiedel, and Aleksandra Faust.\n2022. Understanding html with large language mod-\nels. arXiv:2210.03945.\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and\nDilek Hakkani-Tur. 2019. Learning to navigate the\nweb. In International Conference on Learning Rep-\nresentations (ICLR).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le\nBras, and Yejin Choi. 2021. CLIPScore: a reference-\nfree evaluation metric for image captioning. In Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\narXiv:1503.02531.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. 2021.\nScaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In International Conference\non Machine Learning (ICML).\nSheng Jia, Jamie Kiros, and Jimmy Ba. 2019. Dom-\nq-net: Grounded rl on structured language. In In-\nternational Conference on Learning Representations\n(ICLR).\nChris Kedzie, Kathleen McKeown, and Hal Daum\u00e9 III.\n2018. Content selection in deep learning models of\nsummarization. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nYang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason\nBaldridge. 2020. Mapping natural language instruc-\ntions to mobile UI action sequences. In Association\nfor Computational Linguistics (ACL).\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Association for\nComputational Linguistics (ACL).\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-\nlin Shi, and Percy Liang. 2018. Reinforcement learn-\ning on web interfaces using workflow-guided explo-\nration. In International Conference on Learning Rep-\nresentations (ICLR).\nFuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente\nOrdonez. 2021. Visualnews: Benchmark and chal-\nlenges in entity-aware image captioning.\nIn Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nMedhini Narasimhan, Arsha Nagrani, Chen Sun,\nMichael Rubinstein, Trevor Darrell, Anna Rohrbach,\nand Cordelia Schmid. 2022. Tl; dw? summarizing\ninstructional videos with task relevance and cross-\nmodal saliency. In European Conference on Com-\nputer Vision (ECCV).\nKhanh Nguyen, Ali Furkan Biten, Andres Mafla, Lluis\nGomez, and Dimosthenis Karatzas. 2022. Show, in-\nterpret and tell: Entity-aware contextualised image\ncaptioning in wikipedia. arXiv:2209.10474.\nDaniel Nkemelu, Peggy Chi, Daniel Castro Chin, Kr-\nishna Srinivasan, and Irfan Essa. 2023. Automatic\nmulti-path web story creation from a structural article.\narXiv:2310.02383.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Association for\nComputational Linguistics (ACL).\nLuc Pauwels. 2012. A Multimodal Framework for Ana-\nlyzing Websites as Cultural Expressions. In Journal\nof Computer-Mediated Communication (JCMC).\nAmy Pu, Hyung Won Chung, Ankur P Parikh, Sebastian\nGehrmann, and Thibault Sellam. 2021. Learning\nCompact Metrics for MT. In Empirical Methods of\nNatural Language Processing (EMNLP).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. arXiv:2103.00020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. In Journal of Machine Learning Research\n(JMLR).\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Association for Computational\nLinguistics (ACL).\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\nnandez, and Percy Liang. 2015. World of bits: An\nopen-domain platform for web-based agents.\nIn\n34th International Conference on Machine Learning\n(ICML).\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In International ACM\nConference on Special Interest Group on Information\nRetrieval (SIGIR).\nReuben Tan, Bryan A. Plummer, Kate Saenko, J. P.\nLewis, Avneesh Sud, and Thomas Leung. 2022.\nNewsstories: Illustrating articles with visual sum-\nmaries. In European Conference on Computer Vision\n(ECCV).\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nAlexandra Vtyurina, Adam Fourney, Meredith Ringel\nMorris, Leah Findlater, and Ryen W. White. 2019.\nBridging screen readers and voice assistants for en-\nhanced eyes-free web search. In International ACM\nConference on Computers and Accessibility (AS-\nSETS).\nLinzi Xing, Wen Xiao, and Giuseppe Carenini. 2021.\nDemoting the lead bias in news summarization via\nalternating adversarial learning. In Association for\nComputational Linguistics (ACL).\nJheng-Hong Yang, Carlos Lassance, Rafael Sampaio\nde Rezende, Krishna Srinivasan, Miriam Redi,\nSt\u00e9phane Clinchant, and Jimmy Lin. 2023. Atomic:\nAn image/text retrieval test collection to support mul-\ntimedia content creation. In International ACM Con-\nference on Special Interest Group on Information\nRetrieval (SIGIR).\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS).\nChenguang Zhu, Ziyi Yang, Robert Gmyr, Michael\nZeng, and Xuedong Huang. 2021. Leveraging lead\nbias for zero-shot abstractive news summarization.\nIn International ACM Conference on Special Interest\nGroup on Information Retrieval (SIGIR).\nWanrong\nZhu,\nJack\nHessel,\nAnas\nAwadalla,\nSamir Yitzhak Gadre, Jesse Dodge, Alex Fang,\nYoungjae Yu, Ludwig Schmidt, William Yang Wang,\nand Yejin Choi. 2023. Multimodal c4: An open,\nbillion-scale corpus of images interleaved with text.\nA\nAdditional Dataset Details\nA.1\nDataset Processing\nWe now provide additional details on the filters and\ndata processing steps used to convert WikiWeb2M\ninto our three downstream task datasets.\nFor page description, we retain a page from Wiki-\nWeb2M if it is not list-heavy and contains at least\ntwo sections with image or text content that do not\ncontain a list or table. A small subset of Wikipedia\npages are essentially lists9; we consider pages that\n9For\nexample,\nhttps://en.wikipedia.org/wiki/\nList_of_mammals_of_the_United_States\nexplicitly have \u201clist_of\u201d in their URL to be list\nheavy-pages and remove them. We also remove\npages with fewer than two rich sections to ensure\nthere is enough content for a page description task\nto be appropriate.\nFor a page section to serve as a target section\nfor the task of section summarization, we require\nit to have at least five sentences, contain neither a\ntable nor list, and not be the root section. We filter\nout the root because the root (first) section is often\nthe page description, which we choose to keep as a\ndistinct task.\nLastly for image captioning, we follow Nguyen\net al. (2022) and use the reference description as\nthe ground truth caption to be generated. However,\nunlike Nguyen et al. (2022), we do not input the\nattribution description for the target image to be\ncaptioned because it often heavily overlaps with\nthe reference description (the reference description\nis used as the target text to generate). We further\ndiscuss this design choice in Appendix E. Again,\nwe only consider images that were originally in\nWIT as target images to be captioned to ensure\nquality captions. We also only keep target images\nto be images which have a reference description of\nat least three words.\nWe additionally note that in WikiWeb2M we re-\nlease all three types of captions for each image (the\nalternative-text, reference description, and attribu-\ntion description), although not all three are always\navailable for each image. The alternative text cap-\ntion for images is used for accessibility purposes,\nand future work can focus on generating these de-\nscriptions, as opposed to the reference description.\nA.2\nDataset Noise\nOur dataset is built from the web, being processed\nfrom raw HTML. Noise may exist in our dataset in\nthe formatting of text, e.g., mathematical formulas\nmay have additional formatting text around them.\nIn building our task datasets, the only noise we\nmay introduce to the best of our knowledge is the\nprocessing of first sentences. The first sentence is\nseparated from each section for the section summa-\nrization pseudo summary. It is also used as part of\nthe global inputs for page description generation.\nSpecifically, the code used to parse the first sen-\ntence may prematurely split a sentence from a pe-\nriod that does not signal the end of the sentence. We\ndid manually inspect samples early on and found\nthis to be rare (e.g., 96/100 random samples were\nsplit correctly). Additionally, the sentences which\nwere split prematurely could still be valid stan-\ndalone sentences. Our released dataset does in-\nclude all raw text, so others can reprocess it as they\nsee fit. Figure 5 includes a code snippet for the\nsentence preprocessor we use and Table 8 illus-\ntrates the 4/100 prematurely split sentences from\nour random sample.\nA.3\nDataset Analysis\nWe provide a side by side comparison of the fields\nwe open source with the WikiWeb2M dataset com-\npared to those pre-existing in WIT in Table 9. Note\nthat in addition to the new fields we introduce,\nWikiWeb2M has different data processing which\nallows for a great number of sections and images\nto be retained, as seen in Tables 10-12. In the main\ntext, Table 1 provided the aggregate counts over\nall splits for each section type and the number of\nimages in our dataset versus prior work WIT. Ta-\nbles 10, 11, and 12 provide the same statistics but\nnow broken down for train, validation, and test sets,\nrespectively.\nIn Figure 6, two WikiWeb2M dataset samples\nare visually illustrated. Specifically, the webpages\non the topics of Succulent Plant and the Aguas\nLivres Aqueduct are shown on the left of the figure\nto visualize the original webpage for illustration\npurposes, and on the right we show a subset of\nthe fields available in our WikiWeb2M samples for\nthese same pages.\nFor additional data analysis we provide sequence\nlength details. We include the median, average,\n90th percentile, and maximum sequence length val-\nues for all input fields that make up a sample\u2019s in-\nput sequence in the train split. We define sequence\nlength as the number of tokens after preprocess-\ning and SentencePiece tokenization. See Table 13\nfor the sequence length of the page URL, title, de-\nscription, section title and text, and image captions\navailable (the alt-text, attribution description, and\nreference description).\nLastly, we provide aggregate high level statistics\nover the train split of WikiWeb2M in Table 14.\nThis includes statistics on the number of sections\nper page (the number of total sections as well as the\nnumber of content sections, with the latter referring\nto sections which contain image or text content)\nand the number of images per page or section.\nB\nQualitative Examples\nTo qualitatively evaluate our model\u2019s ability on our\nsuite of webpage understanding tasks, we include\ntwo qualitative analyses. First, we report several\nrandom output samples for page description gener-\nation, section summarization, and contextual image\ncaptioning in Tables 15-17. In Appendix B.1, we\ndiscuss our findings from these randomly sampled\noutputs. Next, in Appendix B.2, we include sam-\nple outputs whose metrics improved the most from\nthe text-only model to the multimodal model, to\nexplore why images can be helpful for webpage\nunderstanding tasks. In this second setting we in-\nvestigate samples for page description and section\nsummarization, since these tasks do not obviously\nrequire images in the same manner as contextual\nimage captioning.\nB.1\nGeneral Qualitative Examples\nWe start with our first analysis of randomly sam-\npled outputs for all three fine-tuning tasks. Samples\nare selected from the test set.\nB.1.1\nPage Description Generation\nBeginning with page description in Table 15, the\ntarget and predicted output text are provided for\nthree random pages on the topics of the Horahora\nPower Station, Hedevig Lund, and Cape Nome.\nFor the first article on the Horahora Power Sta-\ntion, the predicted output text is quite coherent and\nwell formed, despite containing some inaccurate\ndetails that conflict with the content of the web-\npage. Our model correctly references the date it\nwas opened (1913) and the date the power station\nwas flooded (1947). It also correctly references\nLake Karapiro, which was formed and ultimately\nled to the submerging of the power station. On the\nother hand, the name of the \u201cWaikato\u201d River was\nswapped with \u201cWaihi.\u201d The model also referred to\nHorahora as a coal-fired station when it is actually\na hydroelectric power station.\nNext, for the shorter article on Hedevig Lund,\nwe find the model prediction to be very close to the\ntarget page description, although the painter\u2019s last\nnames are slightly incorrect. Upon inspecting the\npage text, it appears the model included additional\nlast names from the painter\u2019s parents\u2019 names (Ole\nWilhelm Erichsen and Abel Marie n\u00e9e Isaachsen).\nIn future work, methods that use pointer networks\nor direct copying from input text can be used to\nameliorate these named entity failures.\n1\nalphabets = \"([A-Za-z])\"\n2\nprefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n3\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n4\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n5\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n6\nwebsites = \"[.](com|net|org|io|gov|me|edu)\"\n7\ndigits = \"([0-9])\"\n8\n9\ndef PreprocessText(og_text):\n10\ntext = \" \" + og_text + \"\n\"\n11\ntext = text.replace(\"\\n\", \" \")\n12\ntext = re.sub(prefixes, \"\\\\1<prd>\", text)\n13\ntext = re.sub(websites, \"<prd>\\\\1\", text)\n14\ntext = re.sub(digits + \"[.]\" + digits, \"\\\\1<prd>\\\\2\", text)\n15\ntext = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n16\ntext = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n17\ntext = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n18\ntext = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n19\ntext = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n20\ntext = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n21\ntext = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n22\ntext = re.sub(r\" (\\d+)[.](\\d+) \", \" \\\\1<prd>\\\\2 \", text)\n# decimal numbers\n23\n24\nif \"\u201d\" in text:\n25\ntext = text.replace(\".\u201d\", \"\u201d.\")\n26\nif \"\\\"\" in text:\n27\ntext = text.replace(\".\\\"\", \"\\\".\")\n28\nif \"!\" in text:\n29\ntext = text.replace(\"!\\\"\", \"\\\"!\")\n30\nif \"?\" in text:\n31\ntext = text.replace(\"?\\\"\", \"\\\"?\")\n32\nif \"e.g.\" in text:\n33\ntext = text.replace(\"e.g.\", \"e<prd>g<prd>\")\n34\nif \"i.e.\" in text:\n35\ntext = text.replace(\"i.e.\", \"i<prd>e<prd>\")\n36\nif \"...\" in text:\n37\ntext = text.replace(\"...\", \"<prd><prd><prd>\")\n38\nif \"Ph.D\" in text:\n39\ntext = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n40\n41\ntext = text.replace(\".\", \".<stop>\")\n42\ntext = text.replace(\"?\", \"?<stop>\")\n43\ntext = text.replace(\"!\", \"!<stop>\")\n44\ntext = text.replace(\"<prd>\", \".\")\n45\nreturn text\n46\n47\ndef GetFirstRestSentences(og_text):\n48\n\"\"\"Splits a body of text into its first sentence and the remaining text.\n49\n50\nCode modified from\n51\nhttps://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n52\n53\nArgs:\n54\nog_text: The input text string.\n55\nReturns:\n56\nfirst: The first sentence of a body of text.\n57\nrest: The remaining, rejoined sentences from a body of text that follow the first sentence.\n58\n\"\"\"\n59\n60\ntext = PreprocessText(og_text)\n61\nsentences = text.split(\"<stop>\")\n62\nsentences = sentences[:-1]\n63\nsentences = [s.strip() for s in sentences]\n64\n65\nfirst = sentences[0]\n66\nrest = og_text[len(first)+1:]\n67\nreturn first, rest\nFigure 5: The code used for sentence splitting. This preprocessing is used to separate the first sentence from the rest\nof the section body for section summarization targets and for page description generation global input tokens.\nFull First Sentence\nProcessed First Sentence\nIn 2007, Saade was a founding member of What\u2019s\nUp!, a Swedish boy band which included Robin\nStjernberg, Ludwig \"hejLudde\" Keijser and Johan\nYngvesson.\nIn 2007, Saade was a founding member of What\u2019s\nUp!\nUnicamp offers over one thousand extension pro-\ngrams to the community, with different levels of\nminimum requirements (high school degree, un-\ndergraduate degree, etc.) and across all areas of\nstudy, focusing mainly on specialization courses\nand community outreach.\nUnicamp offers over one thousand extension pro-\ngrams to the community, with different levels of\nminimum requirements (high school degree, under-\ngraduate degree, etc.\nThe proposed biosynthesis of ascofuranone was\nreported by Kita et al., as well as by Abe et al.\nThe proposed biosynthesis of ascofuranone was\nreported by Kita et al.\n1988 The choir sang the German premiere of\nJoseph Jongen\u2019s Mass for choir, brass ensemble\nand organ, Op. 130, which was not yet in print\nthen, both in the Stiftskirche of Aschaffenburg and\nin St. Bonifatius.\n1988 The choir sang the German premiere of\nJoseph Jongen\u2019s Mass for choir, brass ensemble\nand organ, Op.\nTable 8: Failures of our sentence processor. We found 4/100 randomly sampled sections had first sentences which\nwere prematurely split. We share these captions here and find they still are fairly reasonable sentences. The \u201cfull\u201d\nfirst sentence on the left is determined via manual inspection.\nThe Cape Nome article is another example with\na slightly longer page description (four sentences).\nThis sample strongly illustrates the model\u2019s ability\nto convey a factually accurate and topically rele-\nvant page description even when the output text\ndoes not entirely contain the same content as the\ntarget description. Both the target and predicted\ndescriptions begin with an overall summary of the\ntopic, followed by geographical information. Our\nmodel\u2019s generated text also provides some histori-\ncal context that is accurately summarized from the\narticle, which the ground truth description does not.\nIt seems our model attempts to summarize each of\nthe sections on the page to form a coherent page\ndescription, which may differ from the target page\ndescription on Wikipedia (i.e., the Wikipedia page\ndescription need not cover topics from the entire\nwebpage and can vary in style page to page).\nB.1.2\nSection Summarization\nFor the task of section summarization, we include\nlinks to the webpage and the target section to be\nsummarized from the article, and the target and\npredicted text in Table 16. Starting with the histori-\ncal section on imageboards, we find that the target\nsection summary is slightly more section specific\nthan the predicted summary. I.e., the model gener-\nated summary \u201cFutallaby is a free and open-source\nimageboard script\u201d could be in sections other than\nthe historical section. That being said, the histor-\nical section does discuss that Futallaby is freely\navailable, making the model predictions sensible,\nrelevant, and factually correct.\nIn the second section summarization example on\nthe topic of Jamaica\u2019s pirate economy, the target\nsummary discusses Spanish resistance to English\noccupancy to provide context for the growing pirate\neconomy. However, neither the target nor predicted\nsection summary directly address the pirate econ-\nomy. The model prediction is mostly accurate with\ncorrect references to English occupancy in 1655,\nbut implicitly refers to Port Royal as a fort at the\nfoot of the Blue Mountains, which geographically,\nis slightly questionable.\nThe third section summarization sample con-\ncerns the science fantasy novel \u201cThuvia, Maid of\nMars,\u201d written by Edgar Rice Burroughs.\nOur\ntrained model correctly references Burroughs fin-\nishing a novel by June 1914, but it was Thuvia,\nMaid of Mars he finished, not the book \u201cTarzan.\u201d\nThe model seems to have confused multiple facts\nrelating to this book: Thuvia, Maid of Mars was the\nfourth, not third, novel in the Barsoom series and\nTarzan was not a part of this novel series (although\nBurroughs did also write Tarzan).\nWikiWeb2M Field\nWIT Field\npage_title\npage_title\npage_url\npage_url\nraw_page_description\ncontext_page_description\nsection_title\nsection_title\nsection_text\ncontext_section_description\nsection_image_url\nimage_url\nsection_image_mime_type\nimage_mime_type\nsection_image_width\noriginal_width\nsection_image_height\noriginal_height\nsection_image_raw_ref_desc\ncaption_reference_description\nsection_image_raw_attr_desc\ncaption_attribution_description\nsection_image_alt_text_desc\ncaption_alt_text_description\nclean_page_description\n\u2013\nsection_image_clean_ref_desc\n\u2013\nsection_image_clean_attr_desc\n\u2013\nsection_image_captions\n\u2013\nsection_index\n\u2013\nsection_raw_1st_sentence\n\u2013\nsection_clean_1st_sentence\n\u2013\nsection_rest_sentence\n\u2013\nsection_depth\n\u2013\nsection_heading_level\n\u2013\nsection_subsection_index\n\u2013\nsection_parent_index\n\u2013\npage_contains_images\n\u2013\nsection_contains_images\n\u2013\nsection_image_in_wit\n\u2013\nsplit\n\u2013\nis_page_description_sample\n\u2013\npage_content_sections_without_table_list \u2013\nsection_contains_table_or_list\n\u2013\nis_section_summarization_sample\n\u2013\nis_image_caption_sample\n\u2013\nTable 9: Dataset fields in our WikiWeb2M dataset versus the WIT dataset.\nB.1.3\nContextual Image Captioning\nLastly, in Table 17, we provide qualitative exam-\nples for contextual image captioning. We include\nlinks to the webpage and image (as well as illus-\ntrate the image within the table), plus the target and\npredicted image captions. In the first image from\nthe Longpr\u00e9-le-Sec commune in France, while the\ntarget caption describes the main road in the im-\nage, a church is also present at the end of the road.\nThis is confirmed by another image on the web-\npage which shows the same church. Thus, while\nour model did not predict the same exact target\ncaption, it is still visually and factually accurate.\nThe second image is a photo of a painting of a\nwoman. This image has a more generic target cap-\ntion, and it appears that our model tends to prefer\ngenerating detailed captions. As a result, it contains\nfactually inaccurate information, stating the paint-\ning itself is of Rabindranath, the son of Maharshi\nDevendranath Tagore, who developed Santiniketan.\nAdditionally, the generated caption states the mural\nis at the Ashram Complex in Santiniketan. While\nthe painting is at Santiniketan, it is not confirmed\nto be at the Ashram Complex given the content of\nthe article; while the article states \u201cIt [the Ashram\nComplex] has beautiful frescoes by Nandalal Bose,\u201d\nit remains ambiguous.\nThen, for the third randomly selected image from\nDataset\n# Webpage Sections\n# Images\nStructural Heading\nText\nImage\nBoth\nTotal\nUnique\nTotal\nWIT (En)\n-\n-\n-\n179,769 2,562,275\n2,742,044\n3,183,132 4,456,169\nWikiWeb2M\n658,241\n616,534 6,134,086 199,165 2,911,268 10,519,294 3,867,277 5,340,708\nTable 10: Comparison of WikiWeb2M and the WIT (Srinivasan et al., 2021) dataset. We report counts here for the\ntrain split of WikiWeb2M.\nDataset\n# Webpage Sections\n# Images\nStructural Heading\nText\nImage\nBoth\nTotal\nUnique\nTotal\nWIT (En)\n-\n-\n-\n10,198 142,737 152,935 238,155 249,313\nWikiWeb2M\n36,410\n34,274\n341,310 11,276 162,381 585,651 284,975 299,057\nTable 11: Comparison of WikiWeb2M and the WIT (Srinivasan et al., 2021) dataset. We report counts here for the\nval split of WikiWeb2M.\nthe article on WWOR-TV, the model\u2019s generated\ncaption is quite accurate to the image and also over-\nlaps heavily with the content of the ground truth\ncaption. The only subtle inaccuracy in the pre-\ndicted text is that it states the TV logo was in use\nfrom the early 1970s to early 1980s, when it was\nactually used until the year 1987, which should be\nconsidered the late 1980s.\nB.2\nUnimodal to Multimodal Qualitative\nExamples\nWe now select a random subset of test set outputs\nfor page description and section summarization for\nthe best performing text-only model and the best\nperforming multimodal (image and text) model.\nThese models are base size T5/ViT models, as that\nwas the model size used to perform feature abla-\ntions. Specifically, we select samples which have a\nhigher ROUGE-L score with the multimodal model.\nFor a random subset of 1000 samples, we reverse\nsort by the change in ROUGE-L between the uni-\nmodal and multimodal models, looking to inspect\nthe samples most positively impacted by the inclu-\nsion of images. We hope to understand the settings\nunder which images can aid these tasks.\nAs noted previously, we include this analysis for\npage description and section summarization, since\nthese tasks may not require images, while image\ncaptioning inherently uses the input image. These\nexamples are included in Tables 18 and 19. We\ndid find that some of the most improved samples\nwere an artifact of the text-only model repeating\ntext tokens many times (a common failure of text\ngeneration models) and do not include those in our\nexamples.\nB.2.1\nPage Description\nStarting with page description generation, we in-\nclude three examples in Table 18. For each page we\nlink to the Wikipedia article and include the target\npage description, the page description generated by\nthe text-only model (noted as text under the type\ncolumn), and the page description generated by the\nmultimodal model (noted as multi under the type\ncolumn) for comparison.\nAcross these examples, we find one trend: im-\nages in the webpage can improve the generated\ndescription\u2019s specificity. In the page description\ntask, we allow up to six images present on the\npage to be included. Starting with the first exam-\nple from the webpage on Joan Carling, we see that\nthe page description output from the multimodal\nmodel touches upon more topics and specifies de-\ntails beyond a high-level sentence summary (the\nlatter being more similar to the output from the text-\nonly model). The multimodal model\u2019s generated\ntext includes references to the Champions of the\nEarth Lifetime Achievement Award that was given\nto Joan Carling as well as greater detail about her\nrelationship with the Philippines and being of the\nIgorot people. These events and concepts are both\ncaptured in the images in the page, which seem\nto help specify more detail from the page in the\ngenerated description.\nSimilarly, for the second example from the page\non Alice Bemis Taylor, the text-only model gener-\nated a brief and high level summary about her. On\nthe other hand, the images on the webpage portray\nnumerous important locations (either her childhood\nhome or community spaces she actively partici-\npated in or founded such as the Colorado Springs\nArts Center and Day Nursery). As a result of in-\nDataset\n# Webpage Sections\n# Images\nStructural Heading\nText\nImage\nBoth\nTotal\nUnique\nTotal\nWIT (En)\n-\n-\n-\n9,905\n142,917 152,822 238,924 250,353\nWikiWeb2M\n36,743\n35,568\n342,554 11,082 162,605 588,552 286,390 300,666\nTable 12: Comparison of WikiWeb2M and the WIT (Srinivasan et al., 2021) dataset. We report counts here for the\ntest split of WikiWeb2M.\nSequence Length Med Avg\nMax\nP90\nPage URL\n16\n17.40\n89\n23\nPage Title\n5\n5.20\n42\n8\nPage Description\n110\n122.27 695\n289\nSection Title\n3\n3.64\n176\n7\nSection Text\n119\n212.87 62,418 523\nImage Alt-Text\n4\n5.58\n1,433\n14\nImage Attribution\n17\n29.76\n30,121 59\nImage Reference\n9\n8.20\n3,640\n27\nTable 13: Sequence length statistics for various Wiki-\nWeb2M data fields. Sequence length is defined by the\nnumber of SentencePiece Tokens. We report the median,\naverage, maximum, and 90th percentile sequence length\nvalues for the train split.\nStatistic\nMed Avg Max\nP90\nTotal Sections / Page\n4\n5.83 987\n12\nContent Sections / Page 4\n5.13 939\n11\nImages / Page\n1\n2.96 1,679 6\nImages / Section\n0\n0.51 653\n1\nTable 14: Analysis for the number of sections and im-\nages per page or section in the train split of the Wiki-\nWeb2M dataset. Here we consider content sections as\na section with text or image content within it (as in not\nonly containing lists, tables, or heading only).\ncluding these images, the generated description\nnow included more specific information regarding\nthese places. By including these images, their cor-\nresponding captions and related textual concepts\nin the input sequence are more greatly attended to.\nThis may be a byproduct of images always serving\nas global tokens with our Prefix Global attention\nmechanism.\nLastly, we include the descriptions generated for\nthe webpage on the topic of the Franco-German Par-\nliamentary Assembly. Again this example demon-\nstrates that images can help maintain the correct\nlevel of specificity for the page description. In\nthis case, the images help refocus the description\nto be more high-level than the description gener-\nated from the text model, which includes too much\ndetail (unlike the prior examples which tended to-\nwards too little). The two images on this page are\ncentered around the respective meeting locations of\nthe parliamentary in France and Germany, which\nhelps focus the topic of the page description.\nB.2.2\nSection Summarization\nIn Table 19 we now include section summarization\nexamples for several pages and include links to the\nwebpage and specific section to be summarized.\nSlightly different from the trend found for page\ndescription generation, we see that images can im-\nprove the topical relevance of the generated section\nsummary.\nFor example, with the webpage on Johann\nJoachim Quantz, the target section summary for\nthe section on the Court of Frederick discusses\nhow Quantz joined the court as a flute teacher to\nFrederick II. The image in this section illustrates\nFrederick the Great playing flute, which directs the\nmodel to focus on this instrument. The text-only\nmodel fails to mention anything about instruments\nin the summary, only discussing interpersonal rela-\ntionships. On the other hand, the multimodal model\nmentions both the flute and specifically Frederick\nplaying the flute, which is depicted in the included\nimage and also explains the reason why Quantz\njoined the court.\nNext, the section on the 1800s history with re-\nspect to the St. Peter Catholic Church (in Mont-\ngomery, Alabama) continues to illustrate how im-\nages can improve topic relevance.\nWhile the\ntext model references the year the church was\nfounded, the multimodal model references the lo-\ncation, which the target summary also describes\n(although they reference the location of the church\nat different degrees of granularity). If a specific\nsection does not have images, an image is included\nfrom the images in the chronological order they\nappear on the webpage. This means that for this\nsection, the included image is the first image on the\npage - a map of Alabama showing the location of\nthe St. Peter Catholic Church.\nA third example can be found with the P&T\nWebpage\nTarget Text\nPredicted Text\nHorahora\nPower\nStation\nHorahora Power Station was an early hydro-\nelectric power station on the Waikato River in\nNew Zealand. It was the country\u2019s first large-\nscale power station, completed in 1913. Ini-\ntially built to service a gold mine, the power\nstation was expanded to supply a significant\npart of the North Island. The power station\nremained in use until was submerged by Lake\nKarapiro, which was formed to supply the\nlarger Karapiro Power Station.\nHorahora Power Station was a coal-fired\npower station located on the banks of the\nWaihi River in the Waikato region of New\nZealand. It was built in 1913 and flooded in\n1947 when Lake Karapiro was formed behind\nthe Karapiro Power Station dam.\nHedevig\nLund\nHedevig Thorine Christine Erichsen Lund\nwas a Norwegian painter.\nHedevig Lund n\u00e9e Erichsen was a Norwegian\npainter.\nCape\nNome\nCape Nome is a headland on the Seward\nPeninsula in the U.S. state of Alaska. It is sit-\nuated on the northern shore of Norton Sound,\n15 miles to the east of Nome also on Norton\nSound. It is delimited by the Norton Sound\nto the south, Hastings Creek on the west, a\nlagoon on the east and an estuary formed by\nthe Flambeau River and the Eldorado River.\nFrom the sea shore, Cape Nome extends in-\nland by about 4 miles, connected by road with\nNome.\nCape Nome is a headland on the Seward\nPeninsula of Alaska, United States. It is lo-\ncated 129 miles to the south east of the Bering\nStrait. The Cape Nome mining precinct was\nestablished in 1898 immediately after the dis-\ncovery of gold at Golovnin Bay.\nTable 15: Page description qualitative examples. We include three random samples from the page description test\nset and compare the target page description and predicted model output text.\nGroup article and 1900s-1950s Growth and Expan-\nsion section. The target section summary discusses\nthe opening of the Shanghai office. The summary\ngenerated from the text-only model discusses the\nP&T Group expanding its business, which while\ncorrect, does not discuss the connection between\nopening new offices and expanding business. On\nthe other hand, with the help of the HSBC Shang-\nhai office image input to the multimodal, the multi-\nmodal model generated a description which explic-\nitly mentioned the office opening. However, due\nto both Shanghai and Hong Kong locations being\nmentioned in the section, the model confuses the\ntwo in the generated section summary.\nC\nAdditional Metrics\nFor results reported in the main paper, we addition-\nally report BLEURT, CLIPScore, and RefCLIP-\nScore (the latter two are only relevant for contex-\ntual image captioning). See Tables 20, 21, 22. All\nresults trends stay the same except that BLEURT\nis insensitive to most of the feature ablation results\nfor section summarization and page description\ngeneration.\nD\nAdditional Model Ablations\nWe now provide additional experiments that could\nnot fit into the main text, which include data pro-\ncessing ablations, additional feature ablations, and\nfurther comparisons to prior work.\nD.1\nPage Description\nWe first performed ablations on the data processing\nof WikiWeb2M for the page description genera-\ntion task dataset. Specifically, we tried varying the\nnumber of content sections required for a particular\npage to be retained. See Table 23 for comparison of\nwhen we required two vs. three vs. four sections to\ncontain image or text content without a table or list.\nWe found the added samples improve performance\nconsistently (i.e., the most performant setting is\nwhen the number of content sections required per\npage is set to two).\n10Structure features are kept if they were helpful in text-\nonly experiments. I.e., they are included for page description\nand image captioning, but not for section summarization.\nWebpage\nSection\nTarget Text\nPredicted Text\nImageboard\nHistorical\nFutallaby is a PHP script based on\nthe Futaba script from Futaba Chan-\nnel.\nFutallaby is a free and open-source\nimageboard script.\nColony of\nJamaica\nJamaica\u2019s\nPirate\nEconomy\nSpanish resistance continued for\nsome years after the English con-\nquest, in some cases with the help\nof the Jamaican Maroons, but Spain\nnever succeeded in retaking the is-\nland.\nThe English occupied the island of\nJamaica in 1655, establishing a fort\nat the foot of the Blue Mountains.\nThuvia,\nMaid of Mars Background\nBurroughs began writing Thuvia,\nMaid of Mars, in April 1914, at the\ntime describing it as a \u2018Carthoris\u2019\nstory.\nBy June 1914, Burroughs had com-\npleted the third novel in the Bar-\nsoom series, Tarzan.\nTable 16: Section summarization qualitative examples. We include three random samples from the section\nsummarization test set and compare the target section summary and predicted model output text.\nWe also allow for text only pages to be kept in\nthe dataset, as there are a small subset (roughly\n2% of pages) that do not have any images after our\nprocessing. This could be due to the Wikipedia\npages changing since the original WIT dataset was\nreleased, or because we only allow JPEG and PNG\nimages while WIT contained some other image\ntypes like SVG. We include additional ablations\nin Table 24 showing the effect of including or not\nincluding these unimodal pages; their effect is min-\nimal given how few there are in the dataset.\nIn Table 25, we show ablations for our prefix\ndesign with the page description generation task.\nIncluding the section titles and first sentences of\neach section in the prefix as global tokens improved\nperformance for a majority of metrics, and we kept\nthis set up for the rest of our experiments. We note\nthat even when not using a specially designed pre-\nfix (i.e., flattening the section inputs and allowing\nthe first 512 tokens to serve in the prefix, not sepa-\nrating out section titles or first sentences), the Pre-\nfix Global attention mechanism still outperforms\nTransient Global. This follows the principal from\nleading sentence bias that earlier information in the\ninput text is more important. Thus, if you have a\npriori knowledge that a particular part of the input\nis more important than others, separating it into the\nprefix of our attention mechanism can be effective.\nD.2\nContextual Image Captioning\nAs image captioning inherently requires images,\nwe performed additional feature ablations on the\ntext features while always including the image (see\nrows 5-9 in Table 26). We verify in row 5 that when\ninputting only the image and no contextual text, it\nis incredibly difficult to generate strong captions\nfor these images which contain a lot of fine-grained\ninformation. However, in support of the impor-\ntance of having both images and text inputs, we\nfind that for every text-only to multimodal compar-\nison (where all features are the same except images\nare included in the latter), the multimodal setting al-\nways results in substantial performance gains. For\nexample, quantitatively comparing row 1 and row\n6 in Table 26, where either only the section text\nis input versus the section text and image to be\ncaptioned are input, the performance differences\nare: BLEU-4 9.83 vs. 11.27, ROUGE-L 33.00 vs.\n36.90, and CIDEr 133.70 vs. 153.44.\nAgain, this differs from the findings of Nguyen\net al. (2022); their experimental design likely min-\nimized the impact of images because they also\nfeed in the attribution description as a textual input,\nwhich often is quite similar to the target caption.\nAs a result, the model can \u201ccheat\u201d and utilize the\nattribution description while not relying on the vi-\nsual input. We have more discussion regarding this\nin Appendix E.\nD.3\nAll Tasks\nFor each task in our WikiWeb2M suite, we also\nablated the number of images input to the model.\nThese additional ablations are shown in Table 27.\nResults in the main text use the 90th percentile num-\nber of images per page, six, for page description\ngeneration, and only one for section summarization\nand image captioning. Here we also try the average\nvalue for number of images per page which is three.\nWe include these ablations for contextual image\ncaptioning as well, as we were curious whether\nhaving contextual (non-target) images input to the\nmodel would help at all. Ultimately it sometimes\nhurt performance, likely adding noise and making\nit more challenging for the model to discern which\ninput image was the target image to be captioned.\nWe only used the single target image as input for\nthe rest of our experiments.\nE\nContextual Image Captioning Task\nDesign\nWe now provide additional discussion on the task of\ncontextual image captioning and how our input de-\nsign differs from prior work. Nguyen et al. (2022)\nrecently introduced the task of contextual image\ncaptioning. We found our metrics were lower than\nthose they reported for the task and investigated the\ncauses. We ran additional experiments for contex-\ntual image captioning with the exact same sample\ninputs as Nguyen et al. (2022). Specifically, we\ntried only using the page description, target im-\nage, target image\u2019s section text, and the attribution\ndescription as a sample\u2019s inputs.\nBy including the attribution description (which\noften heavily overlaps with the target caption to be\ngenerated), our performance is much higher, nearly\nmatching prior work even when using different\ndata splits (the prior work\u2019s dataset splits are not\nreleased). We report these reproduced results for\nour splits in Table 28. As discussed earlier, for our\ncontextual image captioning task, we chose not to\ninput the attribution description of an image given\nhow much overlap it has with the target caption\n(the reference description). In terms of other exper-\nimental differences, we also use ViT (Dosovitskiy\net al., 2021) image representations while prior work\nused ResNet-152 (He et al., 2016), although both\nwere pretrained on ImageNet (Deng et al., 2009).\nF\nSection Summarization Pseudo\nSummaries\nWe were motivated to study the task of section\nsummarization as a subproblem of Webpage Story\nGeneration, which is the task of converting a web-\npage to an Instagram Story-like format. It con-\nsists of one multimodal Story page or slide per\nsection, containing a section summary and paired\nimage (from the same webpage). Our section sum-\nmarization task is a subpart of this problem and\nwe proposed an improvement over the News, text-\nonly CNN/DailyMail PEGASUS model used to\ngenerate summaries in the prior Wiki2Story work\nby Nkemelu et al. (2023). Specifically, our for-\nmulation of multimodal section summarization is\ndesirable so that we can also take images as con-\ntextual input, as the goal is to generate multimodal\ncontent for a user to consume on the topic of a par-\nticular webpage (in this case, a Wikipedia article).\nOriginally, we attempted to collect human writ-\nten section summaries ourselves. But when running\nan initial data collection pilot we found that when\nexplaining the intended application of webpage sto-\nries, a majority of the annotators deemed the first\nsentence to almost always (94% of the time) be a\ngood enough pseudo summary. In the other cases\nwhen a majority of annotators voted otherwise, we\nfound the annotation quality was too poor to use\nthe collected written summaries. It proved very dif-\nficult to collect free form summaries of Wikipedia\ncontent. For both of these reasons we continued\nmodeling section summarization with our pseudo\nsummaries (the first sentence of the section).\nTo perform this data collection pilot, we used\nan internal crowd sourcing platform to hire seven\ncrowd workers. They were located in India and\npaid hourly wages competitive for their locale.\nThey have standard rights as contractors. The last\nfour pages of our paper include a PDF of our in-\nstructions to annotators. We also tried to collect\nlabels for well suited images for each section but\nultimately did not use these annotations.\nFigure 6: Example samples from WikiWeb2M. Here we illustrate two Wikipedia articles: Succulent Plant and\nAguas Livres Aqueduct. The rendered webpage is illustrated on the left. On the right, a sample subset of fields are\nshown; these are stored in TensorFlow Sequence Example format.\nWebpage\nImage\nTarget Text\nPredicted Text\nLongpr\u00e9\n-le-Sec\nThe main road in Longpr\u00e9-\nle-Sec.\nThe church in Longpr\u00e9-le-\nSec.\nLink to Image\nSantiniketan\nMural by Nandalal Bose.\nA painting of Rabindranath\nat the Santiniketan Ashram.\nLink to Image\nWWOR-TV\n1971 WOR-TV I.D. slide.\nThis \u2018dotted 9\u2019 logo was\nused from 1970 to 1987.\nWOR-TV logo, used from\nthe early 1970s until the\nearly 1980s.\nLink to Image\nTable 17: Contextual image captioning examples. We include three random samples from the image captioning test\nset and compare the target image caption and predicted model output text.\nWebpage\nType\nText\nJoan\nCarling\nTarget\nJoan Carling is an indigenous Filipino human rights activist and environmentalist\nwho has defended the rights of native and marginalized peoples for over two\ndecades. She has served as Secretary General of the Asia Indigenous Peoples\nPact and has chaired the Cordillera People\u2019s Alliance in the Philippines. Carling\nhas also contributed to the United Nations Framework Convention on Climate\nChange and REDD+ activities and has served as a member of the United Nations\nPermanent Forum on Indigenous Issues. In September 2018 she received the\nChampions of the Earth Lifetime Achievement Award from the United Nations\nEnvironment Programme in recognition of her work as an environmentalist and\na defender of human rights.\nText\nJoan Carling is a Filipino environmentalist and human rights activist. [ROUGE-\nL 13.11]\nMulti\nJoan Carling is a Filipino indigenous human rights activist and a member of the\nIgorot people. She is the former Secretary General of the Cordillera Peoples\nAlliance, a national Indigenous Peoples\u2019 organization. She was the Co-convener\nof the Indigenous Peoples Major Group on the Sustainable Development Goals.\nIn 2018, she was designated a terrorist by the Philippine authorities for an\nalleged connection with the Communist Party of the Philippines and the New\nPeople\u2019s Army. In September 2018, she received the Champions of the Earth\nLifetime Achievement Award from the United Nations Environment Programme.\n[ROUGE-L 39.61]\nAlice\nBemis\nTaylor\nTarget\nAlice Bemis Taylor was a philanthropist and was inducted into the Colorado\nWomen\u2019s Hall of Fame in 2010. For her significant contributions to Colorado\nCollege, Colorado Springs Fine Arts Center and the Colorado Springs Day\nNursery and other organizations, she was named \u201cLady Bountiful\u201d by the press.\nText\nAlice Bemis Taylor was an American philanthropist and philanthropist.\n[ROUGE-L 21.05]\nMulti\nAlice Cogswell Bemis Taylor was an American philanthropist and philanthropist.\nShe was the founder of the Colorado Springs Day Nursery, the Colorado Springs\nFine Arts Center, the Child Guidance Clinic and the Colorado Springs Commu-\nnity Chest. She was inducted into the Colorado Women\u2019s Hall of Fame in 2010.\n[ROUGE-L 45.36]\nFranco-\nGerman\nParliamentary\nAssembly\nTarget\nThe Franco-German Parliamentary Assembly is a joint body of the German\nBundestag and the French National Assembly formed to enable cooperation\nbetween both houses.\nText\nThe Franco-German Parliamentary Assembly is an inter-parliamentary organisa-\ntion between the French and German parliaments. It was established in March\n2019 following the Aachen Treaty, which was signed on 22 January 2019 by\nAngela Merkel and Emmanuel Macron. [ROUGE-L 28.13]\nMulti\nThe Franco-German Parliamentary Assembly is a joint parliamentary assembly\nof the French National Assembly and the German Bundestag. [ROUGE-L 59.09]\nTable 18: Page description qualitative examples. We include random samples from the page description test set that\nhad the largest metric improvement with images included, and compare the target page description and predicted\nmodel output text.\nWebpage\nSection\nType\nText\nJohann\nJoachim\nQuantz\nCourt\nof\nFrederick\nTarget When Frederick II became King of Prussia in 1740, Quantz finally\naccepted a position as flute teacher, flute maker and composer.\nText\nQuantz was a friend of Frederick the Great, who was a close friend\nof his father. [ROUGE-L 10.81]\nMulti\nQuantz was a friend of Frederick the Great, who was a great lover of\nthe flute. [ROUGE-L 16.22]\nSt. Peter\nCatholic\nChurch\n1800s\nTarget\nIn 1833, arrangements were made to build the church on a property,\ndonated by Edward Hanrick, on the corner of Lawrence Street and\nAdams Avenue.\nText\nSt. Peter Catholic Church was founded in 1834, but had no resident\npastor until 1850. [ROUGE-L 5.00]\nMulti\nSt. Peter Catholic Church is the third oldest Catholic church in\nMontgomery, Alabama. [ROUGE-L 10.53]\nP&T\nGroup\n1900s\n-1950s:\nGrowth and\nExpansion\nTarget In 1920s, the Shanghai office was opened.\nText\nIn the early 1900s, the firm expanded its business.[ROUGE-L 25.00]\nMulti\nIn 1905, the Hong Kong office was opened. [ROUGE-L 66.67]\nTable 19: Section summarization qualitative examples. We include random samples from the section summarization\ntest set which had the largest metric improvement and compare the target section summary and predicted model\noutput text.\nFeature Inputs\nPage Desc. Section Summ.\nImage Caption.\nText Title Struct Caption Image\nBLEURT\nBLEURT\nBLEURT CLIPScore\nRef\nCLIPScore\n\u2714\n0.51\n0.43\n0.36\n0.6850\n0.7146\n\u2714\n\u2714\n0.51\n0.44\n0.36\n0.6845\n0.7153\n\u2714\n\u2714\n\u2714\n0.51\n0.45\n0.37\n0.6865\n0.7166\n\u2714\n\u2714\n\u2714\n0.51\n0.45\n0.37\n0.6851\n0.7154\n\u2714\n\u2714\n\u2714\n\u2714\n0.51\n0.45\n0.37\n0.6878\n0.7177\n\u2714\n\u2714\n\u271410\n\u2714\n0.51\n0.45\n0.41\n0.7340\n0.7575\n\u2714\n\u2714\n\u2714\n\u2714\n0.51\n0.45\n0.41\n0.7329\n0.7576\nTable 20: Feature ablations with WikiWeb2M. We ablate over the section body text, title, structure, captions, and\nimages. We report BLEURT, CLIPScore, and RefCLIPScore metrics for the results in Table 5.\nTask\nInput Section Type\nSection\nSource\nMetric\nTarget Description Context\nBLEURT CLIPScore RefCLIPScore\nSection\nSummarization\n\u2714\nWikiWeb2M\n0.43\n\u2013\n\u2013\n\u2714\n\u2714\n0.44\n\u2013\n\u2013\n\u2714\n\u2714\n\u2714\n0.45\n\u2013\n\u2013\nImage\nCaptioning\n\u2714\nWIT\n0.40\n0.7287\n0.7527\n\u2714\n\u2714\nWIT\n0.40\n0.7307\n0.7537\n\u2714\n\u2714\n\u2714\nWIT\n0.40\n0.7325\n0.7558\n\u2714\n\u2714\n\u2714\nWikiWeb2M\n0.41\n0.7329\n0.7576\nTable 21: Section input ablations. We try using only the target section, both the target section and page description\nand/or context section(s), and vary if the sections come from the smaller WIT or our WikiWeb2M superset. We\nreport BLEURT, CLIPScore, and RefCLIPScore metrics for the results in Table 7.\nTask\nModel ViT Data\nMetric\nBLEURT CLIPScore RefCLIPScore\nPage\nDescription\nBase\nim21k\n0.51\n\u2013\n\u2013\nJFT\n0.51\n\u2013\n\u2013\nLarge\nim21k\n0.52\n\u2013\n\u2013\nJFT\n0.52\n\u2013\n\u2013\nSection\nSummarization\nBase\nim21k\n0.45\n\u2013\n\u2013\nJFT\n0.45\n\u2013\n\u2013\nLarge\nim21k\n0.46\n\u2013\n\u2013\nJFT\n0.46\n\u2013\n\u2013\nImage\nCaptioning\nBase\nim21k\n0.41\n0.7329\n0.7576\nJFT\n0.40\n0.7291\n0.7534\nLarge\nim21k\n0.41\n0.7374\n0.7611\nJFT\n0.41\n0.7263\n0.7527\nTable 22: Pretrained model checkpoint ablations. We report BLEURT, CLIPScore, and RefCLIPScore metrics for\nthe results in Table 6.\n# Content Section Filter Threshold\nMetric\nTrain\nTest\nBLEU-4 ROUGE-1 ROUGE-2 ROUGE-L CIDEr\n2\n2\n13.79\n45.43\n27.72\n38.34\n81.16\n3\n2\n12.38\n44.35\n27.05\n37.63\n75.35\n4\n2\n12.79\n44.10\n26.08\n36.91\n69.52\n2\n3\n13.42\n44.31\n26.07\n36.64\n69.69\n3\n3\n12.32\n43.52\n25.81\n36.28\n67.56\n4\n3\n12.77\n43.56\n25.13\n35.82\n64.04\n2\n4\n12.98\n43.11\n24.45\n34.90\n59.69\n3\n4\n12.01\n42.41\n24.28\n34.63\n58.03\n4\n4\n12.54\n42.65\n23.92\n34.41\n57.19\nTable 23: Page description performance across different filtering thresholds. We change the threshold for how many\nrich content sections a page must have to be included in our page description generation task dataset.\nTask\nSample Modalities\nMetric\nTrain\nTest\nBLEU-4 ROUGE-1 ROUGE-2 ROUGE-L CIDEr\nPage\nDescription\nMultimodal\nCombined\n12.27\n44.75\n27.71\n38.16\n80.07\nCombined\nCombined\n13.79\n45.43\n27.72\n38.34\n81.16\nMultimodal Multimodal\n12.30\n44.67\n27.58\n38.03\n79.00\nCombined\nMultimodal\n13.77\n45.30\n27.55\n38.16\n79.77\nSection\nSummarization\nMultimodal\nCombined\n9.83\n34.96\n15.18\n29.64\n70.82\nCombined\nCombined\n9.93\n34.86\n15.16\n29.53\n70.86\nMultimodal Multimodal\n9.74\n34.89\n15.10\n29.56\n70.22\nCombined\nMultimodal\n9.84\n34.78\n15.06\n29.43\n70.17\nTable 24: Comparison of only retaining multimodal page samples versus also allowing for text only pages\n(combined).\nPrefix Inputs\nMetric\nImages Page\nURL\nPage\nTitle\nSection\nTitle\nSection 1st\nSentence\nAll Section\nContent\nBLEU-4 ROUGE-L CIDEr\n\u2714\n\u2714\n\u2714\n\u2714\n13.79\n38.34\n81.16\n\u2714\n\u2714\n\u2714\n\u2714\n13.00\n38.33\n81.02\n\u2714\n\u2714\n\u2714\n\u2714\n\u2714\n12.59\n38.47\n81.68\nTable 25: Prefix Global prefix ablations for page description generation. The input column \u201call section content\u201d\nrefers to when all section indices, titles, body text, and captions are concatenated in order and then tokens contribute\nto the prefix up to the first 512 tokens.\nFeature Inputs\nMetric\nText Title Struct Caption Image BLEU-4 ROUGE-L CIDEr\n\u2714\n9.83\n33.00\n133.70\n\u2714\n\u2714\n9.84\n33.40\n135.30\n\u2714\n\u2714\n\u2714\n10.15\n33.38\n135.10\n\u2714\n\u2714\n\u2714\n\u2714\n10.03\n33.69\n137.07\n\u2714\n3.18\n14.55\n17.43\n\u2714\n\u2714\n11.27\n36.90\n153.44\n\u2714\n\u2714\n\u2714\n11.64\n37.39\n156.27\n\u2714\n\u2714\n\u2714\n\u2714\n11.74\n37.46\n156.34\n\u2714\n\u2714\n\u2714\n\u2714\n\u2714\n11.84\n37.69\n158.19\nTable 26: Additional feature ablations with WikiWeb2M for contextual image captioning. We ablate over the section\nbody text, title, structure, captions, and images. We report BLEU-4, ROUGE-L and CIDEr metrics. We include\nrows already reported in the main text for ease of side by side comparison across all feature ablations.\nTask\nNumber of\nInput Images\nMetric\nBLEU-4 ROUGE-L CIDEr\nPage\nDescription\n1\n13.44\n38.52\n81.52\n3\n13.55\n38.46\n82.00\n6\n14.00\n38.50\n81.49\nSection\nSummarization\n1\n10.12\n29.43\n69.89\n3\n10.10\n29.35\n70.29\n6\n9.67\n29.37\n70.29\nImage\nCaptioning\n1\n11.84\n37.69\n158.19\n3\n11.92\n37.41\n157.27\n6\n11.84\n37.45\n157.20\nTable 27: Ablations varying the number of input images per task.\nContextual\nImage\nCaptioning\nSplit\nImage\nInput\nText Input\nMetric\nDesc.\nTarget\nSection\nContext\nSection\nAttribution\nDesc.\nB\nR\nC\nNguyen et al.\nUnknown\nResNet\n\u2714\n\u2714\n\u2714\n23.83 48.80 276.60\nOurs\nWikiWeb2M\nViT\n\u2714\n\u2714\n\u2714\n11.84 37.69 158.19\n\u2714\n\u2714\n\u2714\n25.20 50.01 242.50\nTable 28: Experimental results when reproducing the task set up of Nguyen et al. (2022). We do not use the same\ndataset splits since they were not released in prior work. Here our set up uses the same sample inputs as prior work,\nunlike our result in the main text which does not input the attribution description.\nINSTRUCTIONS FOR ANNOTATORS\nWe are collecting annotations for a task called \u201cWiki2Story.\u201d Wiki2Story has so far been defined\nas a data conversion process from Wikipedia webpages to Wikipedia \u201cstories.\u201d The goal is to\nturn a Wikipedia webpage into an Instagram-like story which contains one story page per\nWikipedia section. Each story page includes a section summary and paired image; see the\nbelow examples. We provide two examples of what an Introduction and History story page may\nlook like for these sections of the Apple Wikipedia article.\nWe want to obtain annotations of these section summaries and have you select the most\nappropriate image to pair with it. In particular, the section summary should be a highlight of the\nsection content. The highlight should: be self contained, condense the section\u2019s factual\ninformation into a sentence of ideally fewer than 30 words, and retain enough detail for the\nreader to learn something from the story page. The highlight is supposed to be an educational\nglimpse of the full section\u2019s content, remaining fully true to the original text.\nWe provide examples below of both strong and weak summaries for our use case. Note that we\nexpect the summary to contain only factually correct information from what is provided in the\noriginal section text. We provide weak summary examples to demonstrate ways the summary\nstyle can be incorrect.\nExample 1: The Proverb section of the Apple Wikipedia article.\nSECTION TEXT\nThe proverb, \"An apple a day keeps the doctor away\", addressing the supposed health benefits\nof the fruit, has been traced to 19th-century Wales, where the original phrase was \"Eat an apple\non going to bed, and you'll keep the doctor from earning his bread\". In the 19th century and\nearly 20th, the phrase evolved to \"an apple a day, no doctor to pay\" and \"an apple a day sends\nthe doctor away\"; the phrasing now commonly used was first recorded in 1922. Despite the\nproverb, a 2015 study found no evidence that eating an apple daily prevents visits to a\nphysician.\nSTRONG SUMMARIES\n\u2705 The proverb \u201cAn apple a day keeps the doctor away\u201d has been traced back to\n19th-century Wales, but has yet to be proven scientifically.\n\u2705 \u201cAn apple a day keeps the doctor away\u201d originated in Wales in the 19th century with the\nphrasing \u201cEat an apple on going to bed, and you\u2019ll keep the doctor from earning his\nbread.\u201d\n\u2705 The proverb \u201cAn apple a day keeps the doctor away\u201d has had multiple phrasings over\nthe years, first being traced to 19th-century Wales.\nWEAK SUMMARIES These are poor summaries because they use words like \u201cthis\u201d or \u201cit\u201d, have\na dialogue-like style, or overly abstract the factual information from the Wikipedia section.\n\u2716 This section talks about the phrase \u201cAn apple a day keeps the doctor away\u201d and all of\nthe ways it has been said.\n\u2716 It talks about how apples don\u2019t actually prevent doctor visits.\n\u2716 The apple proverb has existed for centuries.\nExample 2: The Breeds section of the Dog Wikipedia article.\nSECTION TEXT\nDogs are the most variable mammal on earth with around 450 globally recognized dog breeds.\nIn the Victorian era, directed human selection developed the modern dog breeds, which resulted\nin a vast range of phenotypes. Most breeds were derived from small numbers of founders within\nthe last 200 years, and since then dogs have undergone rapid phenotypic change and were\nformed into today's modern breeds due to artificial selection imposed by humans. The skull,\nbody, and limb proportions vary significantly between breeds, with dogs displaying more\nphenotypic diversity than can be found within the entire order of carnivores. These breeds\npossess distinct traits related to morphology, which include body size, skull shape, tail\nphenotype, fur type and color. Their behavioral traits include guarding, herding, and hunting,\nretrieving, and scent detection. Their personality traits include hypersocial behavior, boldness,\nand aggression, which demonstrates the functional and behavioral diversity of dogs. As a result,\npresent day dogs are the most abundant carnivore species and are dispersed around the world.\nThe most striking example of this dispersal is that of the numerous modern breeds of European\nlineage during the Victorian era.\nSTRONG SUMMARIES\n\u2705 Around 450 dog breeds have been globally recognized, making dogs the most variable\nmammal with significant differences in behavioral traits and physical characteristics.\n\u2705 The breeding of dogs during the Victorian Era resulted in around 450 globally recognized\ndog breeds from a small number of founders.\n\u2705 Dogs are the most variable mammal on Earth, having significant variance in skull, body,\nand limb proportions, also differing personality traits like sociability and boldness.\n\u2705 Present day dogs are the most abundant carnivore with around 450 recognized dog\nbreeds.\n\u2705 With around 450 globally recognized breeds and high variance in both physical and\nbehavioral characteristics, dogs display more phenotypic diversity than all other\ncarnivores combined.\nWEAK SUMMARIES These are poor summaries because they are either too brief, reduce and\nabstract the factual information too much, or use words like \u201cthis\u201d and \u201cit.\u201d\n\u2716 There are many types of dogs on Earth.\n\u2716 This section discusses the difference behavioral, physical, and personality traits dog\nbreeds can have.\n\u2716 It describes how dog breeding was done to result in 450 breeds and mentions that dogs\nare the most variable mammal.\nWhen selecting images, we want you to choose the image you see best fit to go with the section\ncontent and summary text. It should be topically relevant and the image you feel is most visually\nappealing.\nUI PLUGIN EXAMPLE (UPDATED)\n____________________________________________________________________________\nRead the below section from a Wikipedia page. The first sentence is highlighted in yellow. Does\nthe first sentence provide a strong and concise (fewer than 30 words) summary of the contents\nof the entire section?\nIf you need additional context on this section\u2019s text and or topic to answer this question, you can\nclick here to see the description of the Wikipedia page. Otherwise, continue with the annotation\ntask.\nSection Title: Etymology\nWikipedia Page Title: Apple\n____________________________________________________\nThe word apple, formerly spelled \u00e6ppel in Old English, is derived from the Proto-Germanic root\n*ap(a)laz,\nwhich\ncould\nalso\nmean\nfruit\nin\ngeneral.\nThis\nis\nultimately\nderived\nfrom\nProto-Indo-European *ab(e)l-, but the precise original meaning and the relationship between both\nwords[clarification needed] is uncertain.\nAs late as the 17th century, the word also functioned as a generic term for all fruit other than berries\nbut including nuts\u2014such as the 14th century Middle English word appel of paradis, meaning a\nbanana. This use is analogous to the French language use of pomme.\n\u0000 Yes, it well summarizes the whole section\n\u0000 No, it does not well summarize the whole section\nIF\n\u201cHERE\u201d\nWAS\nCLICKED\nON\nFOR\nADDITIONAL\nCONTEXT\nABOVE,\nSHOW\nTHE\nFOLLOWING PAGE DESCRIPTION TEXT:\nAn apple is an edible fruit produced by an apple tree (Malus domestica). Apple trees are\ncultivated worldwide and are the most widely grown species in the genus Malus. The tree\noriginated in Central Asia, where its wild ancestor, Malus sieversii, is still found today. Apples\nhave been grown for thousands of years in Asia and Europe and were brought to North America\nby European colonists. Apples have religious and mythological significance in many cultures,\nincluding Norse, Greek, and European Christian tradition.\nIF THE ANSWER TO THE ABOVE QUESTION WAS NO, SHOW THE FOLLOWING\nSUMMARIZATION TASK:\n____________________________________________________________________________\nPlease write a single sentence summarizing the Wikipedia section. Keep the summary factually\naccurate to the original text and summarize the content concisely; try to use 15-30 words at\nmost. The goal is to provide an educational, interesting snippet for a story page of this section.\nSection Title: Etymology\nWikipedia Page Title: Apple\n____________________________________________________________________________\n"
  },
  {
    "title": "Composite Motion Learning with Task Control",
    "link": "https://arxiv.org/pdf/2305.03286.pdf",
    "upvote": "1",
    "text": "Composite Motion Learning with Task Control\nPEI XU, Clemson University, USA and Roblox, USA\nXIUMIN SHANG, University of California, Merced, USA\nVICTOR ZORDAN, Roblox, USA and Clemson University, USA\nIOANNIS KARAMOUZAS, Clemson University, USA\nFig. 1. Example of a physically simulated character performing composite motion with locomotion and aiming a weapon. The colors show the automatic\nmixing of the combined inputs that change dynamically over time based on the state. As indicated in the inset, red denotes body parts that are vital for\nlocomotion while blue for aiming respectively. Our multi-objective approach learns this mixture along with imitation from two disparate reference motions\nand two goal-directed task rewards for each action.\nWe present a deep learning method for composite and task-driven motion\ncontrol for physically simulated characters. In contrast to existing data-\ndriven approaches using reinforcement learning that imitate full-body mo-\ntions, we learn decoupled motions for specific body parts from multiple\nreference motions simultaneously and directly by leveraging the use of mul-\ntiple discriminators in a GAN-like setup. In this process, there is no need\nof any manual work to produce composite reference motions for learning.\nInstead, the control policy explores by itself how the composite motions can\nbe combined automatically. We further account for multiple task-specific\nrewards and train a single, multi-objective control policy. To this end, we pro-\npose a novel framework for multi-objective learning that adaptively balances\nthe learning of disparate motions from multiple sources and multiple goal-\ndirected control objectives. In addition, as composite motions are typically\naugmentations of simpler behaviors, we introduce a sample-efficient method\nfor training composite control policies in an incremental manner, where we\nreuse a pre-trained policy as the meta policy and train a cooperative policy\nthat adapts the meta one for new composite tasks. We show the applicability\nof our approach on a variety of challenging multi-objective tasks involving\nboth composite motion imitation and multiple goal-directed control.\nCCS Concepts: \u2022 Computing methodologies \u2192 Animation; Physical sim-\nulation; Reinforcement learning.\nAuthors\u2019 addresses: Pei Xu, Clemson University, 1240 Supply Street, North Charleston,\nSC, 29405, USA, Roblox, USA, peix@clemson.edu; Xiumin Shang, University of Cali-\nfornia, Merced, USA, xshang@ucmerced.edu; Victor Zordan, Roblox, USA, Clemson\nUniversity, USA, vbz@clemson.edu; Ioannis Karamouzas, Clemson University, USA,\nioannis@clemson.edu.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n0730-0301/2023/8-ART $15.00\nhttps://doi.org/10.1145/3592447\nAdditional Key Words and Phrases: character animation, physics-based\ncontrol, motion synthesis, reinforcement learning, multi-objective learning,\nincremental learning, GAN\nACM Reference Format:\nPei Xu, Xiumin Shang, Victor Zordan, and Ioannis Karamouzas. 2023. Com-\nposite Motion Learning with Task Control. ACM Trans. Graph. 42, 4 (Au-\ngust 2023), 18 pages. https://doi.org/10.1145/3592447\n1\nINTRODUCTION\nDespite significant advancements in physics-based character con-\ntrol, the majority of existing techniques rely on reference data con-\nsisting of motion capture recordings of an expert performing the\nbehavior of interest [Bergamin et al. 2019; Chentanez et al. 2018;\nLee et al. 2019; Park et al. 2019; Peng et al. 2018, 2022, 2021; Won\net al. 2020; Xu and Karamouzas 2021]. While such reference data\nis paramount to train motor control policies that lead to natural\nand robust control, in this paper, we are interested in synthesiz-\ning composite behaviors for physically simulated humanoids by\ncombining multiple motion capture reference clips into the training\nof a single policy. Further, we augment these imitation controllers\nwith task-specific rewards to train the policy to accomplish specific\nfunctional tasks at the same time. To this end, we propose a novel\nmulti-objective learning framework that builds composite motion\nbehaviors through multiple discriminators, each with its own dis-\ntinct reference motion as well as task-level control. Our framework\nis based on deep reinforcement learning, and allows us to adaptively\nbalance the learning of disparate motions from multiple sources and\nalso multiple goal-directed control objectives.\nThe motivation for this technique is twofold. First, humans are\ncapable of sophisticated behaviors, including performing multiple\ntasks simultaneously, such as walking and gesturing or using a\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\narXiv:2305.03286v1  [cs.GR]  5 May 2023\n2\n\u2022\nXu, P. et al\nmobile phone. To accomplish this with virtual characters, existing\ncontrol approaches need to be extended to accommodate the ability\nto train with multiple objectives as a goal. Second, with limited ex-\nception, most current control frameworks rely on imitation with the\nstyle of a behavior being derived from reference motion examples.\nOur aim is to be able to combine examples automatically through\nwhat we call \u201ccomposite motion control\u201d to avoid the need to con-\ntinuously seek new example motions for every new permutation\nof combined behaviors. We also explore the ability to add multiple\ntask objectives to support our aim of multi-objective control.\nThe core difference of our approach from existing imitation learn-\ning approaches is decoupling full-body control during training,\nturning imitation and goal-directed full-body training into a multi-\nobjective learning framework. To this end, we propose a modifica-\ntion to generative adversarial networks (GANs) to accommodate\nmultiple discriminators (for each subtask in the desired end behav-\nior) and to incorporate the mixing of the behaviors as a part of the\ntraining. In this way, we sidestep the need to dictate weights for\ncombining the subtasks as well as the need to shape careful reward\nfunctions manually for each new composite behavior. In addition,\nas we expect composite motions to often be augmentations from\nsimpler behaviors, we introduce a method for learning composite\nmotion control policies from existing policies through incremental\nlearning. To this end, we train a meta policy, for example for walk-\ning, and then train a new policy to cooperate with the meta policy,\nproducing a composite motion control policy significantly faster\nthan learning from scratch. Thus, we can quickly add on to walking\nnew activities from reference data such as punching or waiving,\neven if we do not have examples of these activities being combined\npreviously with the meta policy.\nOne naive approach to produce the composite motions we target\nis to blend motion capture clips to produce a single new motion, and\nperform traditional imitation learning from there. This suggested\ntechnique may be plausible for simple composite behaviors, like\nwaiving an arm while walking as the two behaviors do not use the\nsame joints, nor do they influence each other greatly, and therefore\nthe blending can be done by simple splicing in a way that is fixed\nover time. Even so, there is no guarantee of physical plausibility\nwithout subsequent training \u2013 and the approach does not scale\nfor more complex behaviors which may have more complicated\ntradeoffs between body parts used, especially over time. In contrast,\nour approach offloads the need to create this weighting as it is\nproduced automatically by the policy as a part of the dictated action.\nLikewise, the output of our system is automatically guaranteed to\nbe physically valid. Finally, our approach also has the capability to\nadd task-directed goals, such as walk to a specified location, which\nis not possible without significant manual effort being added to the\nnaive approach described.\nOverall, this paper makes the following contributions:\n\u2022 We introduce a novel approach for physics-based character\ncontrol that decouples full-body control in order to learn\nimitation and task goals from disparate sources and across\ndistinct body parts.\n\u2022 To this end, we extend GAN-style reinforcement learning and\nintroduce a multi-objective learning framework to support\nmultiple discriminators and automatic weighting of imitation\nand goal-driven subtask rewards.\n\u2022 We propose an incremental learning scheme that uses a meta-\npolicy from an existing behavior to augment the behavior\nwith new subtasks, producing a composite motion control\npolicy that can be learned significantly faster than learning\nfrom scratch. Our scheme automatically learns weights across\nthe body that are state dependent in order to effectively mix\nthe original behavior with a new subtask in a temporally\ndynamic fashion.\n2\nBACKGROUND AND RELATED WORK\n2.1\nPhysics-Based Character Control\nDeveloping controllers for physically simulated humanoids has\nwide applications in computer graphics, robotics, and biomechanics.\nOver the years, a number of trajectory optimization approaches for\nphysics-based control have been proposed that leverage heuristics\nor feedback rules [Coros et al. 2010; De Lasa and Hertzmann 2009;\nWampler et al. 2014; Ye and Liu 2010a; Zordan et al. 2014], includ-\ning open-loop control schemes[Liu et al. 2015, 2010; Mordatch et al.\n2012], close-loop feedback control [da Silva et al. 2017; Mordatch and\nTodorov 2014] and model predictive control approached [H\u00e4m\u00e4l\u00e4i-\nnen et al. 2015; Kwon and Hodgins 2010; Tassa et al. 2012, 2014].\nGiven the difficulty in controller design, which often involves multi-\nple optimization objectives, data-driven methods using demonstra-\ntions from real humans has also drawn a lot of attention [Da Silva\net al. 2008; Kwon and Hodgins 2017; Lee et al. 2010; Liu et al. 2016,\n2012; Muico et al. 2009; Sok et al. 2007; Yin et al. 2007; Zordan and\nHodgins 2002].\nIn recent years, with the advancement of machine learning tech-\nniques, deep reinforcement learning frameworks have gained a\nlot of popularity for training physics-based character controllers.\nWhile some works [Karpathy and Van De Panne 2012; Won et al.\n2018; Xie et al. 2020; Yu et al. 2018] purely rely on reward func-\ntions designed heuristically or using curriculum learning to perform\ncontrol and encourage the character to act in an expected, human-\npreferred style, most recent works leverage motion capture data\nto perform imitation learning in order to generate high-fidelity,\nlife-like motions. DeepLoco [Peng et al. 2017] employs a hierar-\nchical controller to perform walking-style imitation in navigation\ntasks for a physically simulated character. DeepMimic [Peng et al.\n2018] combines imitation learning with goal-conditioned learning,\nand enables a physics-based character to learn a motor skill from\na reference motion collected by motion capture or handcrafted by\nartists. Chentanez et al. [2018] explore the training of recovery\npolicies that would prevent the character from deviating signifi-\ncantly from the reference motion. While the aforementioned works\nrely on a phase variable to synchronize with the reference motion,\nDReCon [Bergamin et al. 2019] utilizes a motion matching tech-\nnique to find the target pose from a collection of reference motions\ndynamically in response to user control input.\nBesides relying on direct tracking of reference motions, researchers\nhave offered a number of ways to extend the use of reference data in\nvarious ways. For example, Park et al. [2019] leverage the kinematic\ncharacteristics of unorganized motions to generate target poses for\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n3\nthe control policy to imitate. UniCon [Wang et al. 2020] adopts a\nsimilar strategy, where a high-level motion scheduler is employed\nto provide the target pose for the low-level character controller.\nMotionVAE [Ling et al. 2020] employs data-driven generative mod-\nels using variational autoencoders to generate target motion poses\nfor a reinforcement learning based controller. A similar model is\nemployed by Won et al. [2022] and tested with various goal-directed\ndownstream tasks. To ensure synthesis of desired motions, these\napproaches rely on carefully designed reward functions to assess\nthe controlled character motion. Drawn from GAIL [Ho and Ermon\n2016; Merel et al. 2017], AMP [Peng et al. 2021] and ICCGAN [Xu\nand Karamouzas 2021] avoid manually designing reward functions\nby exploiting the idea of generative adversarial network (GAN)\nand relying on a discriminator to obtain the imitation reward for\ntraining.\nBeyond the simple use of full-body motions, many works explore\nmotion generation by combining together multiple basic motions\nwith respect to different body parts [Alvarado et al. 2022; Jang et al.\n2022, 2008; Liu and Hodgins 2018; Soga et al. 2016; Starke et al. 2021;\nYazaki et al. 2015]. However, these works focus on the editing and\nsynthesis of motion animation or using inverse kinematic solvers,\nand do not work well with current frameworks for controlling phys-\nically simulated characters using reinforcement learning. To date,\nexisting works for physics-based character control solely focus on\nthe learning of full-body motions. As complementary to such works,\nin this paper, we target composite motion learning from multiple\nreferences without needing to generate any target full-body motion\nfor tasks involving both goal-directed control and imitation control.\n2.2\nTraining Efficiency\nCharacters employed during physics-based control typically are\nhighly articulated with many degrees of freedom defined in con-\ntinuous action spaces. Given the vast feasible choices of action,\ncontrolling so many degrees of freedom is essentially ambiguous,\nresulting in control problems that are under specified and highly\ndimensional. A qualified control policy usually needs millions of\nsamples for training. The time consumption depends on the ex-\nploited algorithms and the motion complexity, varying from tens\nof hours to several days. While some works such as [Yang and Yin\n2021] explore approaches to speed up the training by improving\nthe reinforcement learning algorithm itself, a lot of attention has\nbeen recently drawn on sample-efficient training by reusing pre-\ntrained policies or action models for fast new motion learning. For\nexample, many recent approaches employ mixture of experts (MoE)\nmodels [Peng et al. 2019; Won et al. 2020, 2021], where a batch of\npre-trained expert policies are exploited to provide primitive actions\nthat are combined by a newly trained policy to generate the final\nactions. Other approaches explore using pre-trained latent space\nmodels such as variational autoencoders [Ling et al. 2020; Won et al.\n2022] and GAN-based models [Peng et al. 2022] to facilitate the\ntraining of a control policy. In such approaches, the latent space\nmodel encapsulates a variety of reference motions and is used by\nthe control policy to generate motions for a specific task. The works\nin [Merel et al. 2019, 2020] combine MoE with a latent space model\nand rely on an encoder-decoder architecture to perform distillation\nfor motion learning. Ranganath et al. [2019] utilize principal compo-\nnent analysis to extract coactivations from reference motions and\nuse them as the atomic actions for motor skill learning.\nDespite achieving impressive results, exploring the latent space\nor learning how to combine expert policies is not always easier com-\npared to performing exploration directly in the original action space.\nWe note that all of these works focus only on reusing models that\nprovide full-body motions. In contrast, we propose an incremental\nlearning approach that allows a newly trained policy to take only\npartial actions from a pre-trained policy, and add on that to generate\ncomposite motions. Our approach can largely reduce the training\ntime for composite and multi-objective tasks involving multiple\nimitation and goal-directed objectives as compared to training from\nscratch.\n2.3\nMulti-Objective Control\nIn multi-objective character control, the reward function of the un-\nderlying optimization problem is expressed as the weighted sum\nof multiple, possibly competing, goals. Depending on the task in\nhand, we seek for objective terms that encourage the character to\naccomplish behavior goals, follow reference motion and/or style,\nadopt certain behavior characteristics such as low energy move-\nment, attaining specified goals, etc., resulting in an extensive list of\nobjective terms (see [Abe et al. 2007; Macchietto et al. 2009; Muico\net al. 2009; Peng et al. 2018; Wu and Zordan 2010; Ye and Liu 2010a,b]\nfor some examples). But how we handle all these competing objec-\ntives to create coherent, natural, and coordinated control remains an\nopen question. A common solution is to employ a manual weighting\nscheme based on intuition, experience, and trial and error. However,\nsuch approaches often require excessive, often tricky manual effort\nto obtain desired results. While prioritized-based schemes have been\nemployed that optimize each term in the reward function based on\na given priority [De Lasa and Hertzmann 2009; De Lasa et al. 2010],\nsuch schemes cannot automatically address the problem of multiple\ncompeting objectives.\nThis problem becomes worse within a reinforcement learning\nsetting, as small changes in the reward function can have a signifi-\ncant impact on the resulting behavior. It may need laborious work\nto finetune the weight of each objective to ensure that the control\npolicy can effectively balance the learning of multiple objectives in\na desired way. For tasks with hierarchical objectives, hierarchical\nreinforcement learning with multiple controllers can be employed,\nwhere a different controller is selected at different task levels [Clegg\net al. 2018; Nachum et al. 2019; Peng et al. 2017; Xie et al. 2020].\nHowever, such approaches cannot work for nonhierarchical tasks,\nwhere different objective terms need to simultaneously be optimized\nsuch as when the character has to perform composite motion imi-\ntation and goal-directed control as in our problem domain. In our\napproach, we propose the use of a multi-critic optimization scheme,\nwhere each objective is regarded as an independent task and is as-\nsigned a separate critic. By evaluating each objective independently,\nthe contribution (gradient) of each objective can be normalized into\nthe same scale, and, thus, the control policy will be updated toward\neach objective at the same pace. As such, we avoid scalarizing and\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n4\n\u2022\nXu, P. et al\nDiscriminator \nEnsemble\nDiscriminator \nEnsemble\nDi\n\u02dcoi\nt\nReference Motions\nMi\nControl \nPolicy\n\u03c0\nMultiple Critics\noi\nt\nGoal-Directed \nTask Rewards\nrDi\nt\nrgi\nt\nSimulated Character\nMeta \nPolicy\nmeta\n\u03c0\nst ,\nat\ngmeta\nt\nPhysics \nSimulator\n120Hz\n30Hz\nIncremental \n     Learning\n      Critick\nPD Controller\nImitation \nRewards\ngt\n ,\nsmeta\nt\nameta\nt\nFig. 2. Overview of the proposed system for composite motion learning with task control. Under the framework of reinforcement learning combined with a\nGAN-like structure for motion imitation, our approach employs a multi-critic architecture to train a physics-based controller involving multiple objectives.\nBased on this system, we further propose an optional incremental learning scheme that allows the control policy to fast learn new composite motions and\ntasks by reusing a pre-trained, meta policy.\nweighting the rewards or priorities of multiple objectives. In addi-\ntion, our approach provides a simple solution to adaptively balance\nthe multiple objectives during policy updating without needing to\nfind or estimate the Pareto front.\n3\nOVERVIEW\nOur approach enables a physically simulated character to perform\ncomposite motions through imitating partial-body motions from\nmultiple reference sources directly and simultaneously. This scheme\nturns the full-body motion imitation task into a multi-objective op-\ntimization problem, to which we can further introduce extra objec-\ntives for goal-directed control. We refer to Fig. 2 for an overview of\nour proposed system for composite motion learning with task con-\ntrol. We employ a GAN-like structure combined with reinforcement\nlearning to train the control policy imitating the given reference\nmotions. As such, we do not have to manually design a reward\nfunction for imitation learning or explicitly track a target pose from\nthe reference motions. To learn composite motions, we decouple\nthe full-body motion into several partial-body groups each of which\nimitates its own references. Based on this GAN-like structure, we\npropose a multi-objective learning framework that exploits multi-\nple critics at the same time to help the control policy learn from\nmultiple objectives, involving both composite motion imitation and\ngoal-directed task control in a balanced way (Section 4). To acceler-\nate training, we further consider an optional incremental learning\nscheme that reuses a pre-trained policy as the meta policy and al-\nlows a cooperative policy to adapt the meta one for new composite\ntasks (Section 5).\n4\nCOMPOSITE MOTION LEARNING\nGiven a physically simulated character, we seek to train a control\npolicy \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61) that simultaneously imitates motions from mul-\ntiple reference ones, each focusing on specific body parts, while\npossibly completing specific goal tasks. At each time step \ud835\udc61, the\ncontrol policy takes the character state s\ud835\udc61 and a dynamic goal state\nvariable g\ud835\udc61 as the input and outputs the control signal (action) a\ud835\udc61.\nWe let g\ud835\udc61 be an empty variable if no goal-directed control is involved.\nIn the following, we detail our proposed approach for training \ud835\udf0b\nthat decouples full-body motion allowing imitation performance to\nbe evaluated and improved with respect to specific body parts, and\nconverts the underlying composite motion learning problem into a\nmulti-objective optimization problem.\n4.1\nFull-Body Motion Decoupling\nAt each time step \ud835\udc61, we represent the character pose as P\ud835\udc61 :=\n{(\ud835\udc5d\ud835\udc59,\ud835\udc5e\ud835\udc59, \u00a4\ud835\udc5d\ud835\udc59, \u00a4\ud835\udc5e\ud835\udc59)|\ud835\udc61}\ud835\udc41link\n\ud835\udc59=1 , where \ud835\udc5d\ud835\udc59 \u2208 R3 and \ud835\udc5e\ud835\udc59 \u2208 R4 are the posi-\ntion and orientation (measured in the unit of quaternion) of each\nbody link respectively, and \u00a4\ud835\udc5d\ud835\udc59 \u2208 R3 and \u00a4\ud835\udc5e\ud835\udc59 \u2208 R3 are the linear and\nangular velocities respectively. Given the geometry model and joint\nconstraints of the simulated character, this representation can be\nconverted into a joint space one defined by the skeletal joints\u2019 local\nposition and velocity and the root\u2019s global position and orientation.\nLet M \u2283 { \u02dcP\ud835\udc61 }\ud835\udc61 be the collection of reference motions which may\ncontain multiple clips of pose trajectories { \u02dcP\ud835\udc61 }\ud835\udc61 as the reference.\nTo perform imitation learning, existing approaches either use a\ncarefully designed reward function to compute the error between\nP\ud835\udc61+1 and \u02dcP\ud835\udc61+1 [Bergamin et al. 2019; Chentanez et al. 2018; Park\net al. 2019; Peng et al. 2018; Won et al. 2020], or employ an evaluator\nto assess the transfer P\ud835\udc61 \u2192 P\ud835\udc61+1 without explicitly comparing\nto any specific poses in the reference motions [Merel et al. 2017;\nPeng et al. 2021; Xu and Karamouzas 2021]. The former approaches\nusually need a motion tracking or generation mechanism to retrieve\n\u02dcP\ud835\udc61+1 from the reference motions. The latter typically build on the\nframework of adversarial generative networks (GANs) and rely\non a discriminator to evaluate the transfer. Some approaches take\nposes from more than one frame during imitation performance\nevaluation in order to apply more constraints on the pose trajectory.\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n5\nNevertheless, all these approaches leverage the full-body character\npose P\ud835\udc61 and reference pose \u02dcP\ud835\udc61 \u2208 M to perform imitation learning,\nand thus intend to learn the full-body motions in M.\nTo learn composite motions, ideally, we want the simulated char-\nacter\u2019s partial body motions to come from different reference sources\nat a given time step \ud835\udc61, i.e., the transfer of pose trajectory P\ud835\udc56\n\ud835\udc61\u2212\ud835\udc5b\ud835\udc56:\ud835\udc61 \u2192\nP\ud835\udc56\n\ud835\udc61+1 should satisfy\n{P\ud835\udc56\n\ud835\udc61\u2212\ud835\udc5b\ud835\udc56, \u00b7 \u00b7 \u00b7 , P\ud835\udc56\n\ud835\udc61, P\ud835\udc56\n\ud835\udc61+1} \u2282 M\ud835\udc56,\n(1)\nwhere P\ud835\udc56\n\ud835\udc61 \u2282 P\ud835\udc61 is a partial-body pose from the simulated charac-\nter, and M\ud835\udc56 \u2283 { \u02dcP\ud835\udc56\n\ud835\udc61 }\ud835\udc61 is the reference motion collection containing\nonly poses of the partial body group \ud835\udc56. The full-body motion is con-\nstrained by using multiple M\ud835\udc56 at the same time. Here, we follow Xu\nand Karamouzas [2021] and use a pose trajectory having \ud835\udc5b\ud835\udc56 + 2\nframes for imitation performance evaluation. The larger \ud835\udc5b\ud835\udc56 is, the\nstricter the evaluation will be, as an error occurring at an earlier\ntime step would negatively influence the evaluation of the following\nsteps.\nTypical partial body groups for a humanoid character would be\nthe upper and lower body, arms, and torso. For example, we can\nlet Mupper be a collection of greeting motions involving the upper\nbody (arms, hands, torso and head), and Mlower be walking motions\ninvolving the lower body (pelvis, legs and feet). Then, the full body\nmotion is expected to be the composite of Mupper (greeting) and\nMlower (walking). To coordinate the motions from multiple body\ngroups, we can let P\ud835\udc56\n\ud835\udc61 and some other partial-body poses P \ud835\udc57\n\ud835\udc61 share\nsome common body link states. For example, let Pupper\n\ud835\udc61\nand Plower\n\ud835\udc61\nshare the state of one leg to avoid ipsilateral walking. Correspond-\ningly, the leg state should be included in both Mupper and Mlower\nfor the control policy to learn. We refer to Sections 6 and 7 for body\nsplitting schemes used in our experiments, including typical upper\nand lower body decoupling schemes and more tailored ones for\nspecific tasks such as juggling while walking. After decoupling the\ncharacter\u2019s full-body motion into multiple sets of {P\ud835\udc56\n\ud835\udc61 }\ud835\udc61, we perform\nimitation learning with respect to each body group independently,\nwhere the control policy is expected to explore how to combine\npartial-body motions by itself without needing any full-body, com-\nposite motions to be provided as the reference.\n4.2\nImitation Learning\nTo perform imitation learning, we build our approach off of GAN-\nlike frameworks [Ho and Ermon 2016; Merel et al. 2017], which\nutilize a discriminator to evaluate imitation performance and gen-\nerate reward signals for policy optimization using reinforcement\nlearning algorithms. However, instead of using only one discrimi-\nnator to perform full-body imitation performance evaluation, we\nemploy multiple discriminators simultaneously, each of which deals\nwith a body part group \ud835\udc56 associated with a collection of partial-body\nreference motions M\ud835\udc56. Based on this framework, we can avoid de-\nsigning reward functions to compute the imitation error for each\nspecific body part group. Furthermore, each discriminator can take\nonly its own interested body link states as input during training.\nTherefore, the provided M\ud835\udc56 can still be a collection of full-body\nmotions, but there is no need to explicitly generate any partial-body\nmotions during preprocessing.\nTo stabilize the adversarial training process, we introduce a hinge\nloss [Lim and Ye 2017], gradient penalty term [Gulrajani et al. 2017],\nand an ensemble technique for training of discriminators as pro-\nposed in [Xu and Karamouzas 2021]. Following the literature, given\no\ud835\udc56\n\ud835\udc61 as the observation sampled from the simulated character and \u02dco\ud835\udc56\n\ud835\udc61\nas that sampled from the reference motions M\ud835\udc56, the \ud835\udc56-th ensemble\nof \ud835\udc41 discriminators, \ud835\udc37\ud835\udc56 = {\ud835\udc37\ud835\udc56\ud835\udc5b|\ud835\udc5b = 1, \u00b7 \u00b7 \u00b7 , \ud835\udc41 } is trained using the\nloss function:\nL\ud835\udc37\ud835\udc56 = 1\n\ud835\udc41\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc5b=1\n\u0010\nE\ud835\udc61\n\u0002\nmax(0, 1 + \ud835\udc37\ud835\udc56\n\ud835\udc5b(o\ud835\udc56\n\ud835\udc61))\n\u0003\n+ E\ud835\udc61\n\u0002\nmax(0, 1 \u2212 \ud835\udc37\ud835\udc56\n\ud835\udc5b(\u02dco\ud835\udc56\n\ud835\udc61))\n\u0003\n+\ud835\udf06GPE\ud835\udc61\nh\n(||\u2207^o\ud835\udc56\n\ud835\udc61 \ud835\udc37\ud835\udc56\n\ud835\udc5b(^o\ud835\udc56\n\ud835\udc61)||2 \u2212 1)2i\u0011\n(2)\nwhere ^o\ud835\udc56\n\ud835\udc61 = \ud835\udefco\ud835\udc56\n\ud835\udc61 + (1 \u2212 \ud835\udefc)\u02dco\ud835\udc56\n\ud835\udc61 with \ud835\udefc \u223c Uniform(0, 1) and \ud835\udf06GP is\ngradient penalty coefficient.\nAccording to Eq. 1, we define the observation space of a discrimi-\nnator as\no\ud835\udc56\n\ud835\udc61 := {P\ud835\udc56\n\ud835\udc61\u2212\ud835\udc5b\ud835\udc56, \u00b7 \u00b7 \u00b7 , P\ud835\udc56\n\ud835\udc61, P\ud835\udc56\n\ud835\udc61+1}.\n(3)\nIn principle, the discriminator relies on o\ud835\udc56\n\ud835\udc61 to evaluate the con-\ntrol policy\u2019s performance during the state-action-state transition\n(s\ud835\udc61, a\ud835\udc61, s\ud835\udc61+1). The observation space theoretically should satisfy\no\ud835\udc56\n\ud835\udc61 \u2286 {s\ud835\udc61, s\ud835\udc61+1}. Otherwise, the discriminator may rely on features\nunknown to the control policy, and thus it cannot effectively evalu-\nate the policy\u2019s performance. Given that the control policy \ud835\udf0b in our\nformulation is still a full-body control policy, we simply define s\ud835\udc61 as\na full-body motion state:\ns\ud835\udc61 := {P\ud835\udc61\u2212\ud835\udc5b, \u00b7 \u00b7 \u00b7 , P\ud835\udc61 }\n(4)\nwhere \ud835\udc5b \u2265 \ud835\udc5b\ud835\udc56 for all \ud835\udc56. We refer to the Appendix in the supple-\nmentary material for more details about the state and observation\nrepresentation.\nThe hinge loss function provides a linear evaluation between\n[\u22121, 1] to measure the similarity of a given pose trajectory sample\no\ud835\udc56\n\ud835\udc61 to any sample in the reference motions. Therefore, we define the\nreward term that evaluates the policy\u2019s imitation performance with\nrespect to M\ud835\udc56 for the body part group \ud835\udc56 at time \ud835\udc61 as:\n\ud835\udc5f\ud835\udc37\ud835\udc56\n\ud835\udc61\n(s\ud835\udc61, a\ud835\udc61, s\ud835\udc61+1) = 1\n\ud835\udc41\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc5b=1\nClip\n\u0010\n\ud835\udc37\ud835\udc56\n\ud835\udc5b(o\ud835\udc56\n\ud835\udc61), \u22121, 1\n\u0011\n.\n(5)\nIt must be noted that even though o\ud835\udc56\n\ud835\udc61 and \u02dco\ud835\udc56\n\ud835\udc61 in Eq. 2 have the same\nsubscript \ud835\udc61, they are paired only for the gradient penalty compu-\ntation (last term in Eq. 2). The discriminator ensemble here only\nevaluates the pose trajectory o\ud835\udc56\n\ud835\udc61 independently, rather than com-\nparing it against any specific target trajectory. Therefore, \u02dco\ud835\udc56\n\ud835\udc61 can be\nrandomly sampled from the reference motions by interpolation.\nOverall, by employing multiple discriminator ensembles at each\ntime step \ud835\udc61, we will have a set of rewards, {\ud835\udc5f\ud835\udc37\ud835\udc56\n\ud835\udc61\n}\ud835\udc37\ud835\udc56 , to evaluate\nthe policy\u2019s performance of controlling the character to perform\ncomposite motions, i.e. simultaneously imitating different sets of\nreference motions corresponding to specific partial body parts. By\ndoing so, we convert the task of composite motion learning to\na multi-objective optimization problem under the framework of\nreinforcement learning.\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n6\n\u2022\nXu, P. et al\n4.3\nMulti-Objective Learning\nWe consider policy optimization of a typical on-policy policy gradi-\nent algorithm by maximizing\nL\ud835\udf0b = E\ud835\udc61 [\ud835\udc34\ud835\udc61 log \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61)],\n(6)\nwhere s\ud835\udc61 and g\ud835\udc61 are the given character\u2019s and goals\u2019 state variables\nrespectively, and \ud835\udc34\ud835\udc61 is the advantage which is typically estimated\nby {\ud835\udc5f\ud835\udf0f }\ud835\udf0f \u2265\ud835\udc61. In the common actor-critic architecture, a separate net-\nwork (critic) is updated in tandem with the policy network (actor).\nThe critic is employed to provide state-dependent value estimation,\n\ud835\udc49 (s\ud835\udc61) = E\ud835\udf0b [\u00cd\n\ud835\udf0f \u2265\ud835\udc61 \ud835\udefe\ud835\udf0f\u2212\ud835\udc61\ud835\udc5f\ud835\udf0f] = E\ud835\udf0b [\ud835\udc5f\ud835\udc61 + \ud835\udefe\ud835\udc49 (s\ud835\udc61+1)], based on which\n\ud835\udc34\ud835\udc61 can be estimated with less variance, where \ud835\udefe is the discount\nfactor regulating the importance of the contribution from future\nsteps. To stabilize the training, standardization is often applied on\n\ud835\udc34\ud835\udc61 where the standardized advantage \u00af\ud835\udc34\ud835\udc61 is used in place of \ud835\udc34\ud835\udc61 for\npolicy updating.\nA typical solution for multi-objective tasks in reinforcement learn-\ning is to simply add together all objective-related reward terms, \ud835\udc5f\ud835\udc58\n\ud835\udc61 ,\nwith some weights \ud835\udf14\ud835\udc58, i.e., \ud835\udc5f\ud835\udc61 = \u00cd\ud835\udc3e\n\ud835\udc58=1 \ud835\udf14\ud835\udc58\ud835\udc5f\ud835\udc58\n\ud835\udc61 for a \ud835\udc3e-objective prob-\nlem. In such a way, we still have a scalar reward that can be used\nwith Eq. 6 for policy updating. In practice, though, given that con-\nflicts may exist among the different reward terms, manually tuning\nthe values of \ud835\udf14\ud835\udc58 to balance the composite objective of the character\nis not an intuitive task. For example, we may need the policy to put\nmore effort into learning a difficult partial-body motion, instead of\neven with a trade-off in learning other motions, rather than only\nfocusing on the easy ones to keep achieving a higher associated\nreward. In addition, our proposed approach performs reward estima-\ntion by employing multiple discriminators simultaneously, which\nare modeled by neural networks. This scheme brings a lot of uncer-\ntainty, as the reward distributions from different discriminators may\ndiffer a lot depending on the given reference motions, which could\nbe unpredictable before training. Such a problem would deteriorate\nif we further introduce a set of goal-directed tasks, each having\nits own associated reward term which may compete against the\nimitation reward terms.\nTo balance the contributions of multiple objectives during policy\nupdating, we propose to model the multi-objective learning problem\nas a multi-task one, where each objective is taken into account as an\nindependent task and has a fixed importance during policy updating.\nTo do so, instead of using \ud835\udc5f\ud835\udc61 = \u00cd\n\ud835\udc58 \ud835\udf14\ud835\udc56\ud835\udc5f\ud835\udc58\n\ud835\udc61 , we compute the advantage\nof \ud835\udc34\ud835\udc58\n\ud835\udc61 with respect to {\ud835\udc5f\ud835\udc58\ud835\udf0f }\ud835\udf0f \u2265\ud835\udc61 independently. Then, the optimization\nprocess becomes maximizing\nL\ud835\udf0b =\n\ud835\udc3e\n\u2211\ufe01\n\ud835\udc58=1\nE\ud835\udc61\nh\n\ud835\udf14\ud835\udc58 \u00af\ud835\udc34\ud835\udc58\n\ud835\udc61 log \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61)\ni\n,\n(7)\nwhere \u00cd\n\ud835\udc58 \ud835\udf14\ud835\udc58 = 1 and \u00af\ud835\udc34\ud835\udc58\n\ud835\udc61 is the standardization of \ud835\udc34\ud835\udc58\n\ud835\udc61 , i.e.\n\u00af\ud835\udc34\ud835\udc58\n\ud835\udc61 = \ud835\udc34\ud835\udc58\n\ud835\udc61 \u2212 E\ud835\udc61 [\ud835\udc34\ud835\udc58\n\ud835\udc61 ]\n\u221a\ufe03\nVar\ud835\udc61 [\ud835\udc34\ud835\udc58\n\ud835\udc61 ]\n.\n(8)\nThis optimization process is equal to updating the policy with re-\nspect to each objective independently but always at the same scale\nproportional to \ud835\udf14\ud835\udc58. The introduction of \ud835\udf14\ud835\udc58 gives us more flexibility\nto adjust the contributions toward each objective when conflicts\noccur during policy updating. However, under our testing, a simple\nchoice of \ud835\udf14\ud835\udc58 = 1/\ud835\udc3e, which means each objective is equally impor-\ntant, works well for most cases. We refer to the Appendix in the\nsupplementary material for the choice of \ud835\udf14\ud835\udc58 in our tested composite\ntasks.\nDuring implementation, we can rewrite Eq. 7 as\nL\ud835\udf0b = E\ud835\udc61\nh\u0010\u2211\ufe01\n\ud835\udc58 \ud835\udf14\ud835\udc58 \u00af\ud835\udc34\ud835\udc58\n\ud835\udc61\n\u0011\nlog \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61)\ni\n(9)\nsuch that the policy update can be done through backward propa-\ngation in one pass. From this equation, we can see that the nature\nof our approach is to introduce a dynamic coefficient constrained\nby the standard deviation of {\ud835\udc34\ud835\udc58\n\ud835\udc61 }\ud835\udc61 for each objective \ud835\udc58. As such,\nthe policy will be updated with respect to each objective adaptively.\nThis separation of objectives leads to a single-policy multi-critic\narchitecture. In Fig. 2, for example, we have two imitation related\nreward terms (yellow and green) for upper and lower body imita-\ntion respectively, and two goal-directed task reward terms (red and\nblue). Accordingly, we employ four critics denoted by Critic\ud835\udc58 in\nthe figure. Each Critic\ud835\udc58 only participates in the estimation of \ud835\udc34\ud835\udc58\n\ud835\udc61 ,\nand takes the reward associated with the objective \ud835\udc58, i.e. {\ud835\udc5f\ud835\udc58\n\ud835\udc61 }\ud835\udc61, for\ntraining.\nThough the policy update is balanced through the proposed multi-\ncritic architecture, the state values, which are decided by {\ud835\udc5f\ud835\udc58\n\ud835\udc61 }\ud835\udc61,\ncould differ still drastically with respect to each objective depending\non the difficulty of given reference motions or the reward distribu-\ntions of the goal-related tasks. To mitigate this issue and stabilize the\ntraining of critics, we introduce the value normalization scheme of\nPopArt [van Hasselt et al. 2016]. The value target under this scheme\nis normalized by the moving average and standard deviation for\nthe critic network training. The output of a critic is unnormalized\nbefore joining the process of advantage estimation. Besides main-\ntaining a normalizer for value targets, PopArt is designed to preserve\nthe output precisely. Namely, with PopArt, the output of a critic is\nidentical before and after the normalizer updates given the same\ninput state s\ud835\udc61 and g\ud835\udc61. Such a design is to prevent the normalization\nfrom affecting the value state estimation, thereby stabilizing the\npolicy training. In our implementation, each critic Critic\ud835\udc58 (s\ud835\udc61, g\ud835\udc61)\nhas its own normalizer with a scalar scale and shift estimated in-\ndependently with respect to its associated objective \ud835\udc58. As we show\nin Section 6.6, the introduction of PopArt helps improve the policy\nperformance as also demonstrated by previous works [van Hasselt\net al. 2016; Yu et al. 2021].\n5\nINCREMENTAL LEARNING\nBesides being able to perform a range of composite motions, hu-\nmans typically learn such motions in an incremental manner. For\nexample, if we know how to walk, we should be able to quickly\nlearn how to hold our phone while walking. There is no need to\nrelearn walking from scratch. Based on this intuition, we propose\nan incremental learning scheme for fast composite motion learning.\nInstead of training a policy completely from scratch, we reuse a\npre-trained policy as a meta policy \ud835\udf0bmeta that allows the simulated\ncharacter to perform a basic set of motions (walking in the previ-\nous example). Given \ud835\udf0bmeta, we train a new policy \ud835\udf0b to cooperate\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n7\nALGORITHM 1: Multi-Objective Incremental Learning\n1 Prepare the meta policy \ud835\udf0bmeta;\n2 initialize the policy network \ud835\udf0b;\n3 initialize the critic network Critic\ud835\udc58 where \ud835\udc58 = 1, \u00b7 \u00b7 \u00b7 , \ud835\udc3e given \ud835\udc3e\nobjectives in the task;\n4 initialize policy replay buffer T and reward buffer R;\n5 prepare reference motions M\ud835\udc56 for each discriminator ensemble \ud835\udc37\ud835\udc56;\n6 while training does not converge do\n7\nT \u2190 \u2205, R \u2190 \u2205;\n8\nfor each environment step \ud835\udc61 do\n9\nameta\n\ud835\udc61\n\u223c \ud835\udf0bmeta(\u00b7|smeta\n\ud835\udc61\n, gmeta\n\ud835\udc61\n);\n10\na\ud835\udc61 \u223c \ud835\udf0b (\u00b7, |s\ud835\udc61, g\ud835\udc61, ameta\n\ud835\udc61\n);\n11\ns\ud835\udc61+1, g\ud835\udc61+1, rg\ud835\udc61\n\ud835\udc61\n\u2190 environment updates with character\ncontrol signal of a\ud835\udc61;\n12\nextract observation o\ud835\udc56\n\ud835\udc61 from the state pair of s\ud835\udc61 and s\ud835\udc61+1 for\neach discriminator ensemble \ud835\udc37\ud835\udc56;\n13\nT \u2190 T \u222a {(s\ud835\udc61, a\ud835\udc61, {o\ud835\udc56\n\ud835\udc61 }\ud835\udc56) };\n14\nR \u2190 R \u222a {\ud835\udc5f\ud835\udc58\n\ud835\udc61+1} for each term \ud835\udc58 in rg\ud835\udc61\n\ud835\udc56 ;\n15\ns\ud835\udc61 \u2190 s\ud835\udc61+1; g\ud835\udc61 \u2190 g\ud835\udc61+1;\n16\nextract smeta\n\ud835\udc61\nand gmeta\n\ud835\udc61\nfrom s\ud835\udc61 and g\ud835\udc61 respectively\n17\nend\n18\nfor each discriminator ensemble \ud835\udc37\ud835\udc56 do\n19\ndraw samples \u02dco\ud835\udc37\ud835\udc56\n\ud835\udc61\nfrom M\ud835\udc56;\n20\nupdate \ud835\udc37\ud835\udc56 using o\ud835\udc56\n\ud835\udc61 from T and \u02dco\ud835\udc56\n\ud835\udc61 based on Eq. 2;\n21\nfor each o\ud835\udc56\n\ud835\udc61 in T do\n22\ncompute step-wise imitation reward \ud835\udc5f\ud835\udc37\ud835\udc56\n\ud835\udc61\nbased on Eq. 5;\n23\nR \u2190 R \u222a {\ud835\udc5f\ud835\udc37\ud835\udc56\n\ud835\udc61\n}\n24\nend\n25\nend\n26\nfor each reward term collection {\ud835\udc5f\ud835\udc58\n\ud835\udc61 }\ud835\udc61 in R do\n27\ncompute advantage \ud835\udc34\ud835\udc58\n\ud835\udc61 using {\ud835\udc5f\ud835\udc58\ud835\udf0f }\ud835\udf0f\u2265\ud835\udc61 and state value\nestimation from Critic\ud835\udc58 (s\ud835\udf0f, g\ud835\udf0f) unnormalized by PopArt;\n28\ncompute value target \ud835\udc49 \ud835\udc58\n\ud835\udc61 based on \ud835\udc34\ud835\udc58\n\ud835\udc61 ;\n29\nupdate the normalizer for Critic\ud835\udc58 based on \ud835\udc49 \ud835\udc58\n\ud835\udc61 using\nPopArt;\n30\nget normalized value target \u00af\ud835\udc49 \ud835\udc58\n\ud835\udc61 by PopArt;\n31\nget normalized advantage \u00af\ud835\udc34\ud835\udc58\n\ud835\udc61 based on Eq. 8\n32\nend\n33\nfor each policy update step do\n34\nupdate \ud835\udf0b using {(s\ud835\udc61, a\ud835\udc61, { \u00af\ud835\udc34\ud835\udc58\n\ud835\udc61 }\ud835\udc58) }\ud835\udc61 based on Eq. 9;\n35\nupdate each critic network Critic\ud835\udc58 using { \u00af\ud835\udc49 \ud835\udc58\n\ud835\udc61 }\ud835\udc61\n36\nend\n37 end\nwith the meta policy, performing new composite motions by action\naddition (holding a phone + walking).\nFormally, let \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61) := N (\ud835\udf41\ud835\udc61, \ud835\udf482\n\ud835\udc61 ) denote a Gaussian-based\npolicy. By introducing a meta policy \ud835\udf0bmeta, we define the policy,\nwhich is trained to cooperate with \ud835\udf0bmeta for new composite motions\nas\n\ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61, ameta\n\ud835\udc61\n) := N\n\u0010\n\ud835\udf41\ud835\udc61, \ud835\udf482\n\ud835\udc61\n\u0011\n+ w\ud835\udc61Stop\n\u0010\nameta\n\ud835\udc61\n\u0011\n= N\n\u0010\n\ud835\udf41\ud835\udc61 + w\ud835\udc61Stop\n\u0010\nameta\n\ud835\udc61\n\u0011\n, \ud835\udf482\n\ud835\udc61\n\u0011\n,\n(10)\nwhere the weight vector w\ud835\udc61 has the same dimension with ameta\n\ud835\udc61\n, and\nameta\n\ud835\udc61\n\u223c \ud835\udf0bmeta(\u00b7|smeta\n\ud835\udc61\n, gmeta\n\ud835\udc61\n) is drawn from the meta policy. w\ud835\udc61 are\ndefined as a set of weights each of which is associated with a DoF in\nthe action space of the meta policy. In our implementation, w\ud835\udc61, \ud835\udf41\ud835\udc61\nand \ud835\udf48\ud835\udc61 are obtained by a neural network taking s\ud835\udc61 and g\ud835\udc61 as input,\nand thus are learnable. We put a \"gradient stop\" operator, Stop(\u00b7),\non ameta\n\ud835\udc61\n, which means that the meta policy is fixed and will not be\nupdated with \ud835\udf0b.\nUsing this incremental learning scheme, the new, cooperative\npolicy adds its own action to the meta action ameta\n\ud835\udc61\n. The weight\nvector w\ud835\udc61 decides the reliance of \ud835\udf0b on the meta policy \ud835\udf0bmeta with\nrespect to each DoF in the action space. The bigger an element in\nw\ud835\udc61 is, the more the cooperative policy relies on the meta policy to\ncontrol the corresponding DoF. As such, \ud835\udf0b is trained incrementally\nto learn new composite motions by reusing the meta policy partially.\nThis scheme does not require that ameta\n\ud835\udc61\nand a\ud835\udc61 must have exactly\nthe same dimension, as we can assume zero values for the missing\ndimensions in ameta\n\ud835\udc61\nor ignore the extra, uninteresting dimensions\nin ameta\n\ud835\udc61\n. Compared to a mixture-of-experts (MoE) model, where the\naction is obtained by a linear combination of the actions from mul-\ntiple expert policies, our approach focuses on reusing partial-body\nmotions from the meta policy. It would be very difficult for a MoE\nmodel to keep, for example, only the lower-body motion of one ex-\npert and replace the upper-body motion with that of another expert\nthrough a linear combination of the experts\u2019 full-body motions.\nWith the introduction of \ud835\udf0bmeta, we can replace \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61) in\nEq. 7 with \ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61, ameta\n\ud835\udc61\n), and perform composite motion learn-\ning with goal-directed control under our proposed multi-objective\nlearning framework. We refer to Algorithm 1 for the outline of\nthe proposed multi-objective learning framework with incremental\nlearning. To train a composite policy completely from scratch with-\nout using incremental learning, we can simply ignore \ud835\udf0bmeta and use\n\ud835\udf0b(a\ud835\udc61 |s\ud835\udc61, g\ud835\udc61) solely in Algorithm 1.\n6\nEXPERIMENTS\nIn this section, we experimentally evaluate our approach on multi-\nple challenging composite motion learning tasks. We show that our\napproach can effectively let motor control policies learn composite\nmotions from multiple reference motions directly without manually\ngenerating any full-body motion as reference. Besides evaluating\nthe imitation performance, we also apply our approach on several\ngoal-directed control tasks combined with composite motion learn-\ning from unstructured reference data. The results demonstrate that\nour proposed approach can successfully tackle complex tasks balanc-\ning the learning of multiple objectives involving both partial-body\nmotion imitation and goal-directed control. Finally, we perform ab-\nlation studies on our proposed multi-objective learning framework\nand incremental learning scheme.\n6.1\nImplementation Details\nWe run physics-based simulations using IsaacGym [Makoviychuk\net al. 2021], which supports simulation with a large number of in-\nstances simultaneously by leveraging GPU. The simulated humanoid\ncharacter has 15 body links and 28 DoFs, where the hands are fixed\nwith the forearms and are uncontrollable. In the tasks involving a\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n8\n\u2022\nXu, P. et al\nGRU\n \nst\n \ngt\nwt \n\u03bct \n \nlog \u03c3t\nFC \n(1024)\nFC\u2028\n(512)\nGRU\n \nst\n \ngt\n \nv1\nt\n \nvK\nt\nGRU\n \noi\nt\n \u2026\nv2\nt\n \nrDi\nt\nFC \n(1024)\nFC\u2028\n(512)\nFC \n(256)\nFC\u2028\n(128)\n\u2026\n(a) Policy Network\nGRU\n \noi\nt\n \nrDi\nt\nFC \n(256)\nFC\u2028\n(128)\nGRU\n \nst\n \ngt\n \nv1\nt\n \nvK\nt\n \u2026\nv2\nt\nFC \n(1024)\nFC\u2028\n(512)\n\u2026\n(b) \ud835\udc3e-Head Critic\nGRU\n \noi\nt\n \nrDi\nt\nFC \n(256)\nFC\u2028\n(128)\n\u2026\n(c) Discriminator Ensemble\nFig. 3. Network structures. \u2295 denotes the concatenation operator and \u2296\ndenotes the average operator.\ntennis player, we add 3 DoFs on the right wrist joint such that the\ncharacter can control the racket more agilely, though the racket is\nfixed on the right hand. The simulation runs at 120Hz and the con-\ntrol policy at 30Hz. Differing from the previous works that employ\na stable PD controller [Tan et al. 2011] for character control [Lee\net al. 2022, 2021; Park et al. 2019; Peng et al. 2018, 2021; Won et al.\n2020, 2022; Xu and Karamouzas 2021] we employ a normal, linear\nPD servo for faster simulation.\nWe use PPO [Schulman et al. 2017] as the base reinforcement\nlearning algorithm for policy training and Adam optimizer [Kingma\nand Ba 2014] to perform policy optimization. To embed the charac-\nter state s\ud835\udc61 and the discriminator observation o\ud835\udc56\n\ud835\udc61 sequentially, we\nemploy a gated recurrent unit (GRU) [Chung et al. 2014] with a\n256-dimension hidden state to process these temporal inputs. The\nembedded character state feature is concatenated with the dynamic\ngoal state g\ud835\udc61 if goal-directed control is involved, and then passed\nthrough a multilayer perceptron with two full-connected (FC) layers.\nThe control policy is constructed as Gaussian distributions with in-\ndependent components. The output of the policy network includes\nthe mean \ud835\udf41\ud835\udc61 and standard deviation \ud835\udf48\ud835\udc61 parameters of the policy\ndistribution as well as a weight vector w\ud835\udc61 when incremental learn-\ning is exploited. The multiple critics in our multi-objective learning\nframework are modeled by a multi-head neural network. Similarly\nto the critic networks, we model a discriminator ensemble using a\nmulti-head network. The outputs are averaged by Eq. 5 to produce\nthe reward signal. All the network structures are shown in Fig. 3, in\nwhich we assume that there are \ud835\udc3e objectives in total. We refer to\nthe Appendix in the supplementary material for the representation\nof g\ud835\udc61 in our designed goal-directed tasks, and all hyperparameters\nused for policy training.\nAll the tested policies were trained on a machine equipped with\nan Nvidia V100 GPU. It typically takes about 1.5h to train a policy\nusing a fixed budget of 20M samples (environment steps), for a\npure composite motion imitation task. For complex tasks involving\ngoal-directed control, it takes about 15 to 30 hours and requires\nabout 2 \u00d7 108 to 4 \u00d7 108 samples to train a policy from scratch. By\nexploiting our incremental learning scheme to reuse a pre-trained\nmeta policy, we can shorten the training time to about 30 minutes\nto 2 hours depending on the difficulty of the tasks.\n6.2\nData Acquisition\nAll the motion data used for training are obtained from the LAFAN1\ndataset [Harvey et al. 2020] and other commercial and publicly\navailable motion capture datasets recorded at 30Hz. For single-clip\nimitation, we synthesize short reference motion clips of 1-3 seconds\nlong (cf. Table 1). For tasks with goal-directed control, we extract\nseveral collections of motions (cf. Table 2), each of which contains\nmultiple clips of reference motions with lengths varying from about\n15 to 70 seconds. The juggling motion involves a single trial of a\nsubject performing juggling while standing on a skate, while the\ncollection of tennis swing motions contains four trials of forehand\nswings captured from different subjects. We retarget the local joint\nposition from those motion data to our character model without\nextra manual reprocessing. We demonstrate that policies trained\nwith our approach can perform motion synthesis from unstructured\ndata for goal-directed control, and can explore how to perform\ncomposite motions by combining the partial-body motions from\nthe reference motions without needing any manual processing for\nmotion blending.\n6.3\nImitation Performance\nIn Fig. 4, we highlight motion pose snapshots captured from some\nof our trained policies for composite motion learning. Each com-\nposite motion is learned based on two reference motion clips, one\nfor the upper body and the other one for the lower body. From top\nto bottom, the names of corresponding motions are listed in Ta-\nble 1. Overall, policies trained with our approach can perform very\nchallenging composite motor skills by using the character\u2019s upper\nand lower body part groups at the same time. For example, in the\nmotion combination of chest open and jumping jack (1st row), the\ncontrol policy must keep the character\u2019s body balanced to perform\nthe chest-open motion during jumping in the air, which is a pretty\nchallenging task even for humans. Similar challenges arise when\ndoing squats with the chest open (3rd row) and lunges with waist\ntwisting (4th row). Besides simply following the two partial-body\nreference motions at the same time, the control policies must master\nhow the partial motions could be combined such that the full-body\nmotion is physically plausible. In the 4th row, for example, it is\nimpossible for the character to keep twisting its waist while doing\nlunges at quite different frequencies. Similarly, in the motion combi-\nnation of punch and walk (6th row) and that of punch and run (7th\nrow), the character\u2019s foot has to contact the ground first in order\nto perform the punch action with the torso leaning forward. The\ncontrol policy, thereby, must know when the punch action is doable\nand arrange the motion combination by itself, rather than strictly\nfollowing the reference motions. Our approach does not require the\ngiven reference motions to be perfectly synchronized. The control\npolicies take the character state as input and perform composite\nmotions accordingly. Furthermore, the proposed dynamic sampling\nrate (see Appendix) allows the control policy to adjust the motion\nspeed within an acceptable range for better motion combining.\nTo quantitatively evaluate the imitation performance, following\nprevious literature [Harada et al. 2004; Peng et al. 2021; Tang et al.\n2008; Xu and Karamouzas 2021], we leverage the technique of fast\ndynamic time warping (DTW) and measure the imitation error as\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n9\nFig. 4. Composite motions learned from multiple single-clip reference mo-\ntions. The two snapshots shown on the left side of each row are the reference\nmotions for the upper and lower body respectively.\nTable 1. Imitation performance when learning composite motions from\nsingle clips of reference motions.\nComposite Motion\nLength [s]\nImitation Error [m]\nChest Open\n2.10\n0.11 \u00b1 0.02\nFront Jumping Jack (lower)\n1.80\n0.16 \u00b1 0.03\nFront Jumping Jack (upper)\n1.80\n0.30 \u00b1 0.03\nWalk In-place\n2.10\n0.29 \u00b1 0.02\nChest Open\n2.10\n0.10 \u00b1 0.01\nSquat\n1.67\n0.09 \u00b1 0.01\nWaist Twist\n3.37\n0.15 \u00b1 0.04\nLeg Lunge\n3.67\n0.13 \u00b1 0.02\nHand Waving\n1.80\n0.06 \u00b1 0.03\nWalk\n1.10\n0.09 \u00b1 0.02\nPunch\n1.30\n0.11 \u00b1 0.02\nWalk\n1.10\n0.10 \u00b1 0.01\nPunch\n1.30\n0.17 \u00b1 0.03\nRun\n0.76\n0.14 \u00b1 0.01\nfollows:\n\ud835\udc52\ud835\udc61 =\n1\n\ud835\udc41\ud835\udc56\nlink\n\ud835\udc41 \ud835\udc56\nlink\n\u2211\ufe01\n\ud835\udc59=1\n||\ud835\udc5d\ud835\udc59 \u2212 \u02dc\ud835\udc5d\ud835\udc59 ||,\n(11)\nwhere \ud835\udc41\ud835\udc56\nlink = |{P\ud835\udc56\n\ud835\udc61 }| is the number of interesting body links in the\n\ud835\udc56-th body part group, \ud835\udc5d\ud835\udc59 \u2208 R3 is the position of the body link \ud835\udc59 in the\nworld space at the time step \ud835\udc61, and \u02dc\ud835\udc5d\ud835\udc59 is the body link\u2019s position in\nthe reference motion. The evaluation results are shown in Table 1.\nOur approach can imitate the reference motions closely and balance\nthe imitation of the two partial-body motions well. As can be seen,\nthere is no big gap between the two imitation errors in a given\ncomposite motion combination, which means that policies trained\nwith our approach do not just follow only one reference motion and\nignore the other one. In contrast, without using our proposed multi-\nobjective learning framework, the policy could prefer to track only\none reference motion that is easy to follow. We refer to Section 6.6\nfor the related ablation study.\n6.4\nGoal-Directed Motion Synthesis\nTo test our approach with more complex tasks involving both com-\nposite motion learning and goal-directed control, we designed five\ngoal-directed tasks, as shown in Figs. 5 and 6. In the Target Heading\nand Target Location tasks illustrated in Figs. 5a and 5b, the char-\nacter is asked to respectively go along a target heading direction\nand toward a target location at a preferred speed. Besides the goal-\ndirected objective, two motion imitation objectives are employed:\none is for the lower-body and the other one is for the upper body.\nDiffering from the examples shown in Fig. 4 where the walking and\nrunning motions are just single, short clips containing only one\ngait cycle, here we use a collection of unstructured walking and\nrunning motions as the reference for the lower body, as listed in\nTable 2. In the three examples shown in Fig. 5a, the upper body\nmotions are learned from single reference motion clips, which are\nchest open, jumping jack, and punch respectively, as depicted by\nthe small snapshots in the figure. In the examples shown in Fig. 5b,\nwe use the motion collection of tennis footwork as the reference\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n10\n\u2022\nXu, P. et al\n(a) Tasks: Target Heading (Directional Walking with Various Upper-Body Motions)\n(b) Task: Target Location (Run) with Tennis Racket Holding\n(c) Task: Tennis Swing (Forehand Swing with Footwork)\n(d) Task: Target Location (Walk) while Juggling\nFig. 5. Motion synthesis with composite motion learning and goal-directed control. Pose snapshots shown in the small windows are captured from the\nreference motions.\nTable 2. Motion collections used for goal-directed control.\nMotion Collection\n# of Clips\nLength [s]\nCrouch\n4\n88.87\nWalk\n8\n334.07\nRun\n4\n282.87\nTennis Footwork\n2\n31.67\nTennis Swing\n4\n13.33\nAiming\n2\n48.77\nJuggling\n1\n24.63\nfor the control policy to learn how to hold the racket. This task\nis relatively harder, as the reference motions for both the upper\nand lower body are unstructured. While following the reference\nmotions closely, the control policies trained with our approach can\neffectively coordinate the character\u2019s upper and lower body poses\nto perform the composite motions during goal-steering navigation.\nIn the task of Tennis Swing, the character is expected to hit the\nball successfully with a forehand. The provided collection of ten-\nnis swing motions contains four trials, where the subject performs\nforehand swings while standing still. The tennis ball in our im-\nplementation is generated randomly in a small region near the\ncharacter. As such, the control policy has to rely on the lower-body\nfootwork motions to properly adjust the pose and position of the\ncharacter relative to the tennis ball, while it relies on the upper\nbody swing motions to swing effectively and on time. We note that\nthe goal-directed reward in our design only evaluates the effective-\nness of hitting based on the ball\u2019s outgoing speed and destination.\nThe motion otherwise is decided completely by the control policy,\nwhich leverages two discriminator ensembles to perform imitation\nlearning for the upper and lower body respectively.\nThe Tennis Swing task is challenging, as it is easy for the controlled\ncharacter to solely hit the ball, but instead it is asked to do so\nby combining the motions from the reference collection (tennis\nswing for the upper body and tennis footwork for the lower body).\nThe policy needs some exploration before finding a way to utilize\nposes from the reference motions to perform swings. In this process,\nimitation learning would fail if the policy simply tries to pursue a\nhigher reward by simply hitting the ball. However, when the policy\nis trained using our proposed multi-objective learning framework, it\ncan balance the imitation and goal-directed objectives, and perform\nforehand swings in the style of the reference motions. Additionally,\nwhile we provide only a small set of upper and lower body motions\nas the reference (cf. Table 2), the control policy successfully learns\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n11\n(a) Meta Policy Tasks: Target Location (Crouch, Walk and Run)\n(b) Incremental Learning Tasks: Directional Aiming while Location Targeting (Crouch, Walk and Run).\nFig. 6. Demonstration of incremental learning tasks, where goal-directed aiming motions are added to various locomotion behaviors from the meta policies.\nhow to combine the motions automatically to finish the task. In\ncontrast, if we just leverage full-body reference motions, extra work\nis needed to generate various motions for the policy to learn. In\naddition, there are not enough demonstrations for the policy to\nperform tennis swings correctly in a human-like style by utilizing,\nfor example, only standing swing motions without footwork.\nFigure 5d shows another challenging composite task: Target Loca-\ntion while Juggling, where the character needs to juggle three balls\nwhile walking to the target location. This composite task involves\nfour objectives: two imitation objectives and two goal-directed tasks\nof juggling and locomotion. In our experiment, when a ball is rela-\ntively close to a hand, it is assumed to be caught by and attached to\nthat hand. The ball is automatically detached from hand at a fixed\ninterval of 20 frames. In order to perform juggling successfully and\nsuccessively, after a hand releases its ball, it must catch in time a fly-\ning target ball which was thrown by the other hand. This task is very\nchallenging, as the control policy must explore how to perform ball\nthrowing and catching in concert with the location-targeting task.\nBesides the difficulty of throwing and catching balls, the juggling\nreference motion involves a subject balancing on a skateboard with\nthe body swaying from side to side 1. This increases the difficulty\nof composite motion learning to generate normal walking poses.\nDiffering from the other examples that use a lower and upper-body\nsplit, here we decouple the body parts into two groups, where one\ngroup consists of the character\u2019s arms to imitate the juggling motion\nand the other group includes the rest of the body parts (torso, head,\npelvis, and legs) taking the collection of walking motions as refer-\nence data. In such a way, our approach can effectively eliminate\nthe body swings in the juggling reference motion, and generate\n1FreeMoCap Project: https://github.com/freemocap/freemocap\ncomposite motions with the upper body moving naturally during\ngoal-steering navigation.\nThe other goal-directed task explored in this study is Aiming, in\nwhich the character holds a toy weapon in its right hand and is\nexpected to aim it toward a specific direction. In our experiments,\nthat task is designed mainly to demonstrate the effectiveness of our\nproposed incremental learning scheme, which will be elaborated\nin the next section. We refer to the Appendix for the details of the\nsetup of all of our goal-directed tasks, and the supplementary video\nfor related animation results.\n6.5\nIncremental Learning\nIn Fig. 6, we show tasks used to test our proposed incremental\nlearning scheme. The first row depicts three meta policies of loco-\nmotion, which are trained for the Target Location task completely\nfrom scratch using our proposed multi-objective learning frame-\nwork. In contrast to previous examples, there is only one imitation\nobjective about the full-body during training here, as shown by\nthe snapshots on the top-left corner of the figure. In the 2nd row\nof the figure, we show the cooperative policies that are trained by\nincremental learning, while reusing the pre-trained, meta policies.\nIn addition to the Target Location task, a new goal-directed task of\nAiming is introduced during training the cooperative policies. The\ncontrolled character in this task needs to adjust its right forearm and\nlet the toy pistol aim toward a goal direction specified dynamically.\nThe goal of this experiment is to demonstrate that the cooperative\npolicies can properly exploit the meta policies to perform styled\nlocomotion behaviors while quickly learning upper-body motions\nfrom the newly provided aiming reference motions, which also in-\nvolve a new goal-directed task that is never seen by the meta policies.\nIn Fig. 7, we visualize the weight vector w\ud835\udc61 (cf. Eq. 10) for each DoF\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n12\n\u2022\nXu, P. et al\n0\n1\n(a) Aiming+Crouch\n(b) Aiming+Walk\n(c) Aiming+Run\n(d) Crouch+AimingWalk\nFig. 7. Visualization of the incremental learning weight w\ud835\udc61 (cf. Eq.10). The azure character shows the behavior from the meta policy. The colored character is\ncontrolled by the cooperative policy. The body link color identifies the weight for the associated DoF. The redder color represents higher weights, which means\nthat the cooperative policy relies more on the meta policy to control the corresponding body parts of the character. The bluer color represents lower weights,\nwhich means that the cooperative policy mainly relies on itself to control the related body parts.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\nAiming+Crouch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\nChest\n0.0\n0.1\n0.2\n0.3\nCrouch+AimingWalk\n0.7\n0.8\n0.9\n1.0\nHead\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\nUpper Arms\n0.0\n0.1\n0.2\n0.3\n0.7\n0.8\n0.9\n1.0\nForearms\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\nThighs\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\nShins\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\nFeet\n0.0\n0.1\n0.2\n0.3\nFig. 8. Distributions of the incremental learning weights w\ud835\udc61 for the tasks of Aiming+Crouch and Crouch+AimingWalk (cf. Fig. 7). The x-axis depicts the\nlearned weights and the y-axis shows the corresponding distribution density, normalized by the total number of samples per body part grouping. The color\nsaturation binds the weight range for higher distribution density, with brighter colors highlighting weights greater than 0.5. In the first task, the lower body is\nmainly controlled by the meta Crouch policy (high weights), while in the second task the AimingWalk meta policy mainly influences the upper body.\nby coloring the associated body link. The first three examples show\nthe results obtained when we add the aiming motions to the meta\npolicies of locomotion. The fourth example shows the correspond-\ning result of adding the crouch motion to the meta policy of aiming\nand walking. As opposed to the previous meta policies, this meta\npolicy has four objectives: two imitation objectives for the upper\n(aiming) and lower (walking) body respectively, one Target Location\ntask and one Aiming task.\nAs shown in the figure, in the three Aiming+Locomotion tasks\nwhere the meta policies are pre-trained for locomotion, the coopera-\ntive policies rely more on the meta policy for lower-body actions and\ncontrol the upper-body parts for aiming primarily by themselves. In\ncontrast, in Crouch+AimingWalk, we want the cooperative policy\nto replace the walking motions from the meta policy with crouching\nwhile keeping the upper-body motion of aiming. Here, as can be\nseen in the fourth case of the figure, the cooperative policy exploits\nthe meta policy to perform aiming actions but performs crouching\nmainly on its own. In Fig. 8, we also plot the distribution of weights\nbased on the collection of 5,000 consecutive frames from the Aim-\ning+Crouch and Crouch+AimingWalk tasks. The statistical results\nare consistent with the above studied cases.\nAs an additional experiment, in Fig. 9, we show that control poli-\ncies trained with our approach can support the interactive control\nscheme proposed by Xu and Karamouzas [2021]. In this experi-\nment, we let the character perform a variety of locomotion styles\nby switching the three trained Aiming+Locomotion policies inter-\nactively in response to external control signal provided by the user,\nand navigate to and aim at the target directions specified by the\nuser dynamically.\n6.6\nAblation Studies\nWe refer to the previous literature of ICCGAN [Xu and Karamouzas\n2021] for ablation studies with respect to each component in the\nemployed GAN-like structure for motion imitation, and to [Peng\net al. 2021; Xu and Karamouzas 2021] for related analyses on the\nrobustness of control policies trained using GAN-like structures\ncombined with reinforcement learning. Here, we focus on the studies\nof the proposed multi-objective learning framework and incremental\nlearning scheme.\nIn Fig. 10, we compare the performance of our proposed multi-\nobjective (MO) learning framework to two baselines using three\ncomposite motion learning tasks from Section 4.1. The first baseline\nleverages our MO learning framework but does not make use of\nPopArt to normalize the value targets of each critic (w/o PopArt).\nThe second baseline simply adds the rewards from the two discrimi-\nnators together and models the composite motion learning task as\na typical reinforcement learning problem (w/o MO). Both baselines\nare trained with our motion decoupling scheme described in Sec-\ntion 4.1 and simultaneously leverage two discriminators, one for the\nupper-body motion and one for the lower body. As can be seen from\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n13\nFig. 9. Interactive control of switching between walking, crouching and running for location targeting while aiming.\n0.0\n0.5\n1.0\n1.5\n2.0\n\u00d7107\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Imitation Error\nChestOpen+JumpingJack\nChestOpen\n0.0\n0.5\n1.0\n1.5\n2.0\n\u00d7107\n0.6\n0.8\n1.0\nJumpingJack+WalkInPlace\nJumpingJack\n0.0\n0.5\n1.0\n1.5\n2.0\n\u00d7107\n0.4\n0.6\n0.8\n1.0\nPunch+Run\nPunch\n0.0\n0.5\n1.0\n1.5\n2.0\n# of Samples\n\u00d7107\n0.2\n0.4\n0.6\n0.8\n1.0\nJumpingJack\n0.0\n0.5\n1.0\n1.5\n2.0\n# of Samples\n\u00d7107\n0.6\n0.8\n1.0\nWalkInPlace\n0.0\n0.5\n1.0\n1.5\n2.0\n# of Samples\n\u00d7107\n0.4\n0.6\n0.8\n1.0\nRun\nMO+PopArt (Ours)\nw/o PopArt\nw/o MO\nFig. 10. Learning performance on tasks of composite motion learning from\ntwo single-clip reference motions, which are illustrated in Fig. 4. \"MO\"\nstands for the proposed multi-objective learning framework detailed in\nSection 4.3. Colored regions denote mean values \u00b1 a standard deviation\nbased on 10 trials.\nthe figure, it is hard for \"w/o MO\" to balance the learning of the two\nreference motions. For example, in the ChestOpen+JumpingJack\ntask, as the upper-body (ChestOpen) imitation error goes down,\nthe lower-body (JumpingJack) error increases; in the Punch+Run\ntask, the policy almost gives up on learning how to run, focusing on\npunching without too much success. In contrast, when leveraging\nour MO framework either with or without PopArt, the imitation\nerrors of the upper and lower body show similar and stable trends,\nkeep decreasing as the training goes on. Additionally, the introduc-\ntion of PopArt typically facilitates better training, allowing for faster\nconvergence speed, lower imitation error, and more robust training\nachieving similar performance across different trials.\nFigure 11 shows the performance of our MO approach with and\nwithout exploiting the proposed incremental learning scheme. We\nalso provide comparisons with the \"w/o MO\" baseline. The tested\ntasks have four objectives, as described in Section 6.5: two imitation\nobjectives for the upper and lower body respectively, one Target\nLocation task for the locomotion and one Aiming task. In the cases\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Imitation Error\nAiming+Crouch\nToyPistolHolding\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAiming+Walk\nToyPistolHolding\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAiming+Run\nToyPistolHolding\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCrouch\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWalk\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRun\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTask Reward\nAiming\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAiming\n0\n1\n2\n3\n4\n5\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAiming\n0\n1\n2\n3\n4\n5\n# of Samples\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLocomotion\n0\n1\n2\n3\n4\n5\n# of Samples\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLocomotion\n0\n1\n2\n3\n4\n5\n# of Samples\n\u00d7107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLocomotion\nwith Incr. Learning\nw/o Incr. Learning\nw/o MO\nFig. 11. Learning performance on three composite tasks where each task\ncombines learning from two partial motions while accomplishing two goal\nobjectives. Multi-objective learning in an incremental manner leads to\nsample-efficient training allowing for high-fidelity composite motion syn-\nthesis with goal-directed control. Colored regions denote mean values \u00b1\none standard deviation based on 10 trials.\nusing incremental learning, we employed a pre-trained, locomo-\ntion policy as the meta one. Consistent with the previous ablation\nstudy, we can see that the \"w/o MO\" baseline struggles to balance\nthe different objective terms. Here, the character quickly achieves a\nhigh reward for the goal-directed Aiming task (3rd row) but fails\nto complete other objectives, and in particular to account for the\nmotion style provided by the imitation reward terms. For exam-\nple, the controlled character holds the toy pistol in an unnatural\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n14\n\u2022\nXu, P. et al\nway compared to the demonstrations in the provided reference mo-\ntions as indicated by the high imitation error (1st row). While such\nissues are successfully resolved by our proposed MO framework,\nlearning in a non-incremental way leads to sample inefficient train-\ning as compared to learning by leveraging a meta policy. Besides\nslow speed of convergence, non-incremental training can be time\nconsuming for challenging multi-objective tasks. For example, in\nthe Aiming+Run task, while the case with incremental learning\nonly needs 1.5 hours to finish the training by using about 20 mil-\nlion samples, the non-incremental cases need about 20 more hours\nfor training and will consume about 300 million more samples to\nachieve a similar performance.\n7\nLIMITATIONS AND FUTURE WORK\nWe present a technique for training composite-motion controllers\nusing a multi-objective learning framework that is capable of com-\nbining multiple reference examples and task goals to control a\nphysically-simulated character. We demonstrate that our approach\ncan generalize to a large number of examples based on the availabil-\nity of reference data. Likewise, we show its ability to accomplish\nsimultaneous goal-driven tasks such as aiming at specific targets\nand moving to a target location with different locomotion styles.\nFurthermore, we can interactively control such character\u2019s actions,\npushing the boundary of what is capable for physics-based charac-\nters to date.\nOf course, there is still more to explore in this space. Our system\nis currently not well-equipped to handle behaviors which include\nmultiple phases, as the imitation is not phase-locked in any fashion\nand our discriminators do not distinguish between different stages of\nan activity. Exploring the potential to add a state machine with state\ntransitions could aid in this capacity [Starke et al. 2019]. Another\nshortcoming of the approach presently is that we do not account\nfor variation across the humans that recorded the motion clips. This\nimplies that we are introducing bias in the imitation process that\nmay degrade the final quality of the animation. As is, the system\nis able to make adjustments automatically as needed based on the\nphysical characteristics of the behavior but it cannot distinguish\nerrors that are more stylistic.\nIn its current form, our system can not create new composite ac-\ntivities without performing additional training. A possible direction\nfor future work is aimed at sidestepping this limitation to directly\ncombine preexisting policies and greatly improve the scalability\nof trained controllers. That is, to train two (or more) policies in-\ndependently and combine them at runtime to create a composite\nmotion. Finally, in human motion, composite behaviors go beyond\nan anticipated split, e.g. the lower and upper body, which is one of\nthe modest underlying assumptions in our current implementation.\nInstead, humans may enlist body parts and release them fluidly. For\nexample, a well-trained martial artist changes the use of appendages\nquickly in fighting sequences. We wish to explore this direction in\nfuture investigations and believe that our proposed multi-objective\nlearning framework can provide the foundation for such future\nendeavors.\nAlthough we employed an upper and lower body split in most\nof our experiments, there is nothing tied to this body decoupling\nFig. 12. Failure case study. Top: The character\u2019s body is bisected into a left\nand right group, imitating walking and jumping respectively. Bottom: Juggle\nwhile running.\nscheme except that it is a practical general choice for deploying\nthe limbs of the whole body. Currently, as long as the subtasks are\ncompatible, our system is capable of combining motions along other\nbody splits. For instance, in the Juggling+TargetLocation example\ndiscussed in Section 6.4, the trained policy controls the arms for\njuggling and the rest of the body for walking. Our approach may fail\nif, for example, the lower limbs are separated due to the requirements\nof physical balance. As an example, in Fig. 12, we show a failure\ncase where the body is bisected into a left/right split and asked to\nimitate walking and jumping motions respectively. Such a composite\nmotion is not well-defined, even for humans. We can see that though\nnot falling down, the simulated character cannot imitate the two\nmotions accurately, and instead performs an in-between motion\nwhere the character neither jumps up nor walks in an expected\nfashion.\nIn Fig. 12, we also show another failure case where running refer-\nence motions with an average speed of around 3.5\ud835\udc5a/\ud835\udc60 are provided\nfor the Juggling+TargetLocotion task. With the difficulty of juggling\nwhile moving at this higher speed, this example is significantly more\nchallenging than the one shown in Fig. 5d. Even though we are able\nto synthesize the composite motions, the simulated character cannot\njuggle the balls successfully under these conditions. Currently, our\napproach cannot identify if a composite motion is compatible on its\nown, and instead, it relies on a human to combine behaviors with\nsome domain knowledge about the affinity of the mixing and the\nfeasibility of associated goal-directed tasks. Automating this would\nbe a great direction for future work.\nACKNOWLEDGMENTS\nThis work was supported by the National Science Foundation un-\nder Grant No. IIS-2047632 and by Roblox. We would like to thank\nRokoko 2 for providing mocap data for this project.\n2https://www.rokoko.com\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n15\nREFERENCES\nYeuhi Abe, Marco Da Silva, and Jovan Popovi\u0107. 2007. Multiobjective control with fric-\ntional contacts. In ACM SIGGRAPH/Eurographics Symposium on Computer Animation.\n249\u2013258.\nEduardo Alvarado, Damien Rohmer, and Marie-Paule Cani. 2022. Generating Upper-\nBody Motion for Real-Time Characters Making their Way through Dynamic Envi-\nronments. Computer Graphics Forum 41, 8 (2022).\nKevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019. DReCon:\ndata-driven responsive control of physics-based characters. ACM Transactions On\nGraphics 38, 6 (2019), 1\u201311.\nNuttapong Chentanez, Matthias M\u00fcller, Miles Macklin, Viktor Makoviychuk, and Stefan\nJeschke. 2018. Physics-Based motion capture imitation with deep reinforcement\nlearning. In ACM SIGGRAPH Conference on Motion, Interaction and Games.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empir-\nical evaluation of gated recurrent neural networks on sequence modeling. arXiv\npreprint arXiv:1412.3555 (2014).\nAlexander Clegg, Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. 2018. Learning to\ndress: Synthesizing human dressing motion via deep reinforcement learning. ACM\nTransactions on Graphics (TOG) 37, 6 (2018), 1\u201310.\nStelian Coros, Philippe Beaudoin, and Michiel van de Panne. 2010. Generalized biped\nwalking control. ACM Transactions on Graphics 29, 4 (2010), 130.\nDanilo Borges da Silva, Rubens Fernandes Nunes, Creto Augusto Vidal, Joaquim B\nCavalcante-Neto, Paul G Kry, and Victor B Zordan. 2017. Tunable robustness:\nAn artificial contact strategy with virtual actuator control for balance. Computer\nGraphics Forum 36, 8 (2017), 499\u2013510.\nMarco Da Silva, Yeuhi Abe, and Jovan Popovi\u0107. 2008. Simulation of human motion\ndata using short-horizon model-predictive control. Computer Graphics Forum 27, 2\n(2008), 371\u2013380.\nMartin De Lasa and Aaron Hertzmann. 2009. Prioritized optimization for task-space\ncontrol. In IEEE/RSJ International Conference on Intelligent Robots and Systems. 5755\u2013\n5762.\nMartin De Lasa, Igor Mordatch, and Aaron Hertzmann. 2010. Feature-based locomotion\ncontrollers. ACM Transactions on Graphics 29, 4 (2010), 1\u201310.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. 2017. Improved training of wasserstein gans. Advances in Neural Infor-\nmation Processing Systems 30 (2017).\nPerttu H\u00e4m\u00e4l\u00e4inen, Joose Rajam\u00e4ki, and C Karen Liu. 2015. Online control of simulated\nhumanoids using particle belief propagation. ACM Transactions on Graphics 34, 4\n(2015), 1\u201313.\nTatsuya Harada, Sou Taoka, Taketoshi Mori, and Tomomasa Sato. 2004. Quantitative\nevaluation method for pose and motion similarity based on human perception. In\nIEEE/RAS International Conference on Humanoid Robots, Vol. 1. 494\u2013512.\nF\u00e9lix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust\nmotion in-betweening. ACM Transactions on Graphics 39, 4 (2020).\nJonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.\nAdvances in Neural Information Processing Systems 29 (2016).\nDeok-Kyeong Jang, Soomin Park, and Sung-Hee Lee. 2022. Motion Puzzle: Arbitrary\nMotion Style Transfer by Body Part. ACM Transactions on Graphics 41, 3 (2022).\nWon-Seob Jang, Won-Kyu Lee, In-Kwon Lee, and Jehee Lee. 2008. Enriching a motion\ndatabase by analogous combination of partial human motions. The Visual Computer\n24, 4 (2008), 271\u2013280.\nAndrej Karpathy and Michiel Van De Panne. 2012. Curriculum learning for motor skills.\nIn Canadian Conference on Advances in Artificial Intelligence. Springer, 325\u2013330.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 (2014).\nTaesoo Kwon and Jessica K Hodgins. 2010. Control systems for human running using an\ninverted pendulum model and a reference motion capture sequence. In Proceedings\nof the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 129\u2013\n138.\nTaesoo Kwon and Jessica K Hodgins. 2017. Momentum-mapped inverted pendulum\nmodels for controlling dynamic human motions. ACM Transactions on Graphics 36,\n1 (2017), 1\u201314.\nSeunghwan Lee, Phil Sik Chang, and Jehee Lee. 2022. Deep Compliant Control. In ACM\nSIGGRAPH 2022 Conference Proceedings. Association for Computing Machinery.\nSeyoung Lee, Sunmin Lee, Yongwoo Lee, and Jehee Lee. 2021. Learning a family of\nmotor skills from a single motion clip. ACM Transactions on Graphics 40, 4 (2021),\n1\u201313.\nSeunghwan Lee, Moonseok Park, Kyoungmin Lee, and Jehee Lee. 2019. Scalable muscle-\nactuated human simulation and control. ACM Transactions on Graphics 38, 4 (2019),\n1\u201313.\nYoonsang Lee, Sungeun Kim, and Jehee Lee. 2010. Data-driven biped control. ACM\nTransactions on Graphics 29, 4 (2010), 129.\nJae Hyun Lim and Jong Chul Ye. 2017. Geometric GAN. arXiv preprint arXiv:1705.02894\n(2017).\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. 2020. Character\ncontrollers using motion vaes. ACM Transactions on Graphics 39, 4 (2020), 40\u20131.\nLibin Liu and Jessica Hodgins. 2018. Learning basketball dribbling skills using trajectory\noptimization and deep reinforcement learning. ACM Transactions on Graphics 37, 4\n(2018), 1\u201314.\nLibin Liu, Michiel van de Panne, and KangKang Yin. 2016. Guided learning of control\ngraphs for physics-based characters. ACM Transactions on Graphics 35, 3 (2016),\n1\u201314.\nLibin Liu, KangKang Yin, and Baining Guo. 2015. Improving sampling-based motion\ncontrol. Computer Graphics Forum 34, 2 (2015), 415\u2013423.\nLibin Liu, KangKang Yin, Michiel van de Panne, and Baining Guo. 2012. Terrain runner:\ncontrol, parameterization, composition, and planning for highly dynamic motions.\nACM Transactions on Graphics 31, 6 (2012), 154\u20131.\nLibin Liu, KangKang Yin, Michiel van de Panne, Tianjia Shao, and Weiwei Xu. 2010.\nSampling-based contact-rich motion control. In ACM SIGGRAPH 2010 papers. 1\u201310.\nAdriano Macchietto, Victor Zordan, and Christian R Shelton. 2009. Momentum control\nfor balance. In ACM SIGGRAPH 2009 papers. 1\u20138.\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles\nMacklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. 2021.\nIsaac Gym: High performance GPU-based physics simulation for robot learning.\narXiv preprint arXiv:2108.10470 (2021).\nJosh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg\nWayne, Yee Whye Teh, and Nicolas Heess. 2019. Neural Probabilistic Motor Primi-\ntives for Humanoid Control. In International Conference on Learning Representations.\nJosh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg\nWayne, and Nicolas Heess. 2017. Learning human behaviors from motion capture\nby adversarial imitation. arXiv preprint arXiv:1707.02201 (2017).\nJosh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever,\nVu Pham, Tom Erez, Greg Wayne, and Nicolas Heess. 2020. Catch & Carry: reusable\nneural controllers for vision-guided whole-body tasks. ACM Transactions on Graphics\n39, 4 (2020), 39\u20131.\nIgor Mordatch and Emo Todorov. 2014. Combining the benefits of function approxima-\ntion and trajectory optimization. In Robotics: Science and Systems, Vol. 4.\nIgor Mordatch, Emanuel Todorov, and Zoran Popovi\u0107. 2012. Discovery of complex\nbehaviors through contact-invariant optimization. ACM Transactions on Graphics\n31, 4 (2012), 1\u20138.\nUldarico Muico, Yongjoon Lee, Jovan Popovi\u0107, and Zoran Popovi\u0107. 2009. Contact-aware\nnonlinear control of dynamic characters. In ACM SIGGRAPH 2009 papers. 1\u20139.\nOfir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. 2019. Multi-\nagent manipulation via locomotion using hierarchical sim2real. arXiv preprint\narXiv:1908.05224 (2019).\nSoohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, and Jehee Lee. 2019. Learn-\ning predict-and-simulate policies from unorganized human motion data. ACM\nTransactions on Graphics 38, 6 (2019), 1\u201311.\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018. Deepmimic:\nExample-guided deep reinforcement learning of physics-based character skills. ACM\nTransactions on Graphics 37, 4 (2018), 1\u201314.\nXue Bin Peng, Glen Berseth, KangKang Yin, and Michiel van de Panne. 2017. Deeploco:\nDynamic locomotion skills using hierarchical deep reinforcement learning. ACM\nTransactions on Graphics 36, 4 (2017), 1\u201313.\nXue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. 2019.\nMCP: Learning Composable Hierarchical Control with Multiplicative Compositional\nPolicies. Advances in Neural Information Processing Systems 32 (2019), 3686\u20133697.\nXue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. 2022. ASE:\nLarge-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Char-\nacters. ACM Transactions on Graphics 41, 4 (2022).\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021.\nAMP: Adversarial motion priors for stylized physics-based character control. ACM\nTransactions on Graphics 40, 4 (2021).\nAvinash Ranganath, Pei Xu, Ioannis Karamouzas, and Victor Zordan. 2019. Low dimen-\nsional motor skill learning using coactivation. In ACM SIGGRAPH Conference on\nMotion, Interaction and Games. 1\u201310.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017).\nAsako Soga, Yuho Yazaki, Bin Umino, and Motoko Hirayama. 2016. Body-part motion\nsynthesis system for contemporary dance creation. In ACM SIGGRAPH 2016 Posters.\n1\u20132.\nKwang Won Sok, Manmyung Kim, and Jehee Lee. 2007. Simulating biped behaviors\nfrom human motion data. In ACM SIGGRAPH 2007 papers. 107\u2013es.\nSebastian Starke, He Zhang, Taku Komura, and Jun Saito. 2019. Neural State Machine\nfor Character-Scene Interactions. ACM Transactions on Graphics 38, 6, Article 209\n(2019).\nSebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Komura. 2021. Neural animation\nlayering for synthesizing martial arts movements. ACM Transactions on Graphics\n40, 4 (2021), 1\u201316.\nJie Tan, Karen Liu, and Greg Turk. 2011. Stable proportional-derivative controllers.\nIEEE Computer Graphics and Applications 31, 4 (2011), 34\u201344.\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n1\nJeff KT Tang, Howard Leung, Taku Komura, and Hubert PH Shum. 2008. Emulating\nhuman perception of motion similarity. Computer Animation and Virtual Worlds 19,\n3-4 (2008), 211\u2013221.\nYuval Tassa, Tom Erez, and Emanuel Todorov. 2012. Synthesis and stabilization of\ncomplex behaviors through online trajectory optimization. In IEEE/RSJ International\nConference on Intelligent Robots and Systems. 4906\u20134913.\nYuval Tassa, Nicolas Mansard, and Emo Todorov. 2014. Control-limited differential\ndynamic programming. In IEEE International Conference on Robotics and Automation.\n1168\u20131175.\nHado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Silver.\n2016. Learning values across many orders of magnitude. Advances in Neural\nInformation Processing Systems 29 (2016).\nKevin Wampler, Zoran Popovi\u0107, and Jovan Popovi\u0107. 2014. Generalizing locomotion\nstyle to new animals with inverse optimal regression. ACM Transactions on Graphics\n33, 4 (2014), 1\u201311.\nTingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. 2020. Unicon: Universal\nneural controller for physics-based character motion. arXiv preprint arXiv:2011.15119\n(2020).\nJungdam Won, Deepak Gopinath, and Jessica Hodgins. 2020. A scalable approach to\ncontrol diverse behaviors for physically simulated characters. ACM Transactions on\nGraphics 39, 4 (2020), 33\u20131.\nJungdam Won, Deepak Gopinath, and Jessica Hodgins. 2021. Control strategies for\nphysically simulated characters performing two-player competitive sports. ACM\nTransactions on Graphics 40, 4 (2021), 1\u201311.\nJungdam Won, Deepak Gopinath, and Jessica Hodgins. 2022. Physics-based character\ncontrollers using conditional VAEs. ACM Transactions on Graphics 41, 4 (2022),\n1\u201312.\nJungdam Won, Jungnam Park, and Jehee Lee. 2018. Aerobatics control of flying creatures\nvia self-regulated learning. ACM Transactions on Graphics 37, 6 (2018), 1\u201310.\nChun-Chih Wu and Victor Zordan. 2010. Goal-directed stepping with momentum\ncontrol. In ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 113\u2013\n118.\nZhaoming Xie, Hung Yu Ling, Nam Hee Kim, and Michiel van de Panne. 2020. ALL-\nSTEPS: Curriculum-Driven Learning of Stepping Stone Skills. Computer Graphics\nForum 39 (2020), 213\u2013224.\nPei Xu and Ioannis Karamouzas. 2021. A GAN-Like Approach for Physics-Based\nImitation Learning and Interactive Character Control. Proceedings of the ACM on\nComputer Graphics and Interactive Techniques 4, 3 (2021).\nZeshi Yang and Zhiqi Yin. 2021. Efficient hyperparameter optimization for physics-\nbased character animation. Proceedings of the ACM on Computer Graphics and\nInteractive Techniques 4, 1 (2021), 1\u201319.\nYuho Yazaki, Asako Soga, Bin Umino, and Motoko Hirayama. 2015. Automatic compo-\nsition by body-part motion synthesis for supporting dance creation. In International\nConference on Cyberworlds. IEEE, 200\u2013203.\nYuting Ye and C Karen Liu. 2010a. Optimal feedback control for character animation\nusing an abstract model. In ACM SIGGRAPH 2010 papers. 1\u20139.\nYuting Ye and C Karen Liu. 2010b. Synthesis of responsive motion using a dynamic\nmodel. In Computer Graphics Forum, Vol. 29. 555\u2013562.\nKangKang Yin, Kevin Loken, and Michiel van de Panne. 2007. Simbicon: Simple biped\nlocomotion control. ACM Transactions on Graphics 26, 3 (2007), 105\u2013es.\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. 2021.\nThe surprising effectiveness of PPO in cooperative, multi-agent games. arXiv preprint\narXiv:2103.01955 (2021).\nWenhao Yu, Greg Turk, and C Karen Liu. 2018. Learning symmetric and low-energy\nlocomotion. ACM Transactions on Graphics 37, 4 (2018), 1\u201312.\nVictor Zordan, David Brown, Adriano Macchietto, and KangKang Yin. 2014. Con-\ntrol of rotational dynamics for ground and aerial behavior. IEEE Transactions on\nVisualization and Computer Graphics 20, 10 (2014), 1356\u20131366.\nVictor Zordan and Jessica K Hodgins. 2002. Motion capture-driven simulations that\nhit and react. In ACM SIGGRAPH/Eurographics Symposium on Computer Animation.\n89\u201396.\nA\nSTATE AND ACTION REPRESENTATION\nGiven the definition in Eq. 4, we have the character state vector\ns\ud835\udc61 \u2208 R(\ud835\udc5b+1)\u00d7\ud835\udc41link\u00d713, which includes all body links\u2019 positions, orien-\ntations, and linear and angular velocities of the simulated character\nin the last \ud835\udc5b+1 frame from \ud835\udc61 \u2212\ud835\udc5b to \ud835\udc61. To ignore the global coordinate,\nwe assume that the ground height is 0, and all the body links\u2019 states\nare localized based on the position and heading direction of the\ncharacter\u2019s root link (pelvis) at the last frame \ud835\udc61. Similarly, if a goal\nstate g\ud835\udc61 is provided, we localize the position and direction state in g\ud835\udc61\nusing the same coordinate system with s\ud835\udc61. During our experiments,\nif multiple goal-directed tasks are involved, we simply concatenate\ngoal states from all the tasks together as the representation of g\ud835\udc61.\nWe refer to the Appendix in the supplementary material for the\nrepresentation of g\ud835\udc61 in our designed goal-directed tasks.\nThe action a\ud835\udc61 is a set of target postures fed into the PD servo.\nTherefore, we have a\ud835\udc61 \u2208 R\ud835\udc41dof where \ud835\udc41dof is the total degrees of\nfreedom (DoF) in the character model. a\ud835\udc61 is assumed to be nor-\nmalized by the valid movement range of each DoF but without\nupper and lower bounds applied. The observation space o\ud835\udc56\n\ud835\udc61 for dis-\ncriminators is similar to s\ud835\udc61. However, we keep only body links\u2019\npositions and orientations, and the discriminators rely on the pose\ntrajectory of o\ud835\udc61 to ensure that the visual velocities between two\nframes are consistent with the reference motions. As such, we have\no\ud835\udc56\n\ud835\udc61 \u2208 R(\ud835\udc5b\ud835\udc56+2)\u00d7\ud835\udc41 \ud835\udc56\nlink\u00d77 where \ud835\udc5b\ud835\udc56 + 2 is the number of observed frames\nas defined in Eq. 3. o\ud835\udc56\n\ud835\udc61 is localized depending on its characteristics.\nFor lower body parts, their motions often involve the character\u2019s\nspatial movement. Therefore, we follow the definition of s\ud835\udc61, and\nuse a local coordinate defined by the root pose at the last observed\nframe. For upper-body motions, however, we typically care more\nabout the body parts\u2019 local poses related to a specific parent body\nlink. Therefore, we use a framewisely defined local system based\non the parent link\u2019s pose such that the global-space displacement\nand rotation controlled by the lower body are ignored. In our imple-\nmentation, for upper-body motions, we choose pelvis as the parent\nlink; and for arm only motions, we choose torso as the parent.\nThe observation sampled from the reference motions, i.e. \u02dco\ud835\udc56\n\ud835\udc61, is\ndefined the same as o\ud835\udc56\n\ud835\udc61. However, instead of performing sampling at\na fixed frame rate identical to the control policy\u2019s working frequency\n(30Hz in Fig. 2), we do sampling with dynamic interval \u0394\ud835\udc61 = \ud835\udefd\ud835\udc47\nwhere \ud835\udc47 = 1/30\ud835\udc60 is the time interval between two frames during\nsimulation and \ud835\udefd \u223c Uniform(0.8, 1.2). In such a way, we scale the\nreference motion temporally within a small range, for better com-\nbining motions from multiple reference sources with inconsistent\npace. To keep the motion stable, \u0394\ud835\udc61 differs among multiple times of\nsampling but is identical for the \ud835\udc5b\ud835\udc56 + 2 frames of one sample.\nB\nTASK ENVIRONMENT SETUP\nB.1\nTask: Target Heading\nThe goal-directed reward is defined as\n\ud835\udc5f\ud835\udc61 = \u27e8\u00a4xroot\n\ud835\udc61+1 /||\u00a4x\ud835\udc61 ||, g\ud835\udc61\u27e9,\n(12)\nwhere \u00a4xroot\n\ud835\udc61+1 is the horizontal displacement of the character\u2019s root\nlink from the frame \ud835\udc61 to \ud835\udc61 + 1. The goal state g\ud835\udc61 \u2208 R2 is a unit\nvector representing the target heading direction, which is randomly\nsampled every 30 frames (1\ud835\udc60).\nB.2\nTask: Target Location\nThe goal-directed reward is defined as\n\ud835\udc5f\ud835\udc61 =\n(\nexp(\u22123||\u00a4xroot\n\ud835\udc61+1 /\ud835\udc47 \u2212 v\u2217\n\ud835\udc61 ||2/||v\u2217\n\ud835\udc61 ||2)\nif ||x\ud835\udc61+1 \u2212 pgoal|| > \ud835\udc45\n1\notherwise,\n(13)\nwhere \ud835\udc45 = 0.5 is the goal radius of the target location, \ud835\udc47 = 1/30\ud835\udc60\nis the time interval between two frames, \u00a4xroot\n\ud835\udc61+1 /\ud835\udc47 denotes the hori-\nzontal velocity of the character\u2019s root link from the frame \ud835\udc61 to \ud835\udc61 + 1,\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n2\n\u2022\nXu, P. et al\nand v\u2217\n\ud835\udc61 is the target velocity with a preferred speed and a direction\ntoward the target goal location.\nThe goal state g\ud835\udc61 \u2208 R4 includes a 2D unit vector representing\nthe direction to the target location, the horizontal distance from\nthe character to the goal, i.e. ||xroot\n\ud835\udc61\n\u2212 pgoal||, and the preferred\nspeed ||v\u2217\n\ud835\udc61 ||. The preferred speed is sampled from [1, 1.5] in the\nunit of \ud835\udc5a/\ud835\udc60 for crouching and walking motions, and from [1, 3]\nfor running. The goal direction is sampled from [0, 2\ud835\udf0b). A timer\nvariable is sampled from [3, 5] in the unit of \ud835\udc60 for crouching and\nwalking motions, and from [2, 3] for running. We use these three\ngoal variables to obtain the target location. As such, we can perform\nspeed control during the location targeting.\nB.3\nTask: Aiming\nThe goal-directed reward is defined as\n\ud835\udc5f\ud835\udc61 =\n(\nexp(\u22122||dforearm\n\ud835\udc61\n\u2212 g\ud835\udc61 ||2)\nif aiming is activated\nClip(\u27e8dforearm\n\ud835\udc61\n, uref\u27e9, 0, 0.8)/0.8\notherwise\n(14)\nwhere dforearm\n\ud835\udc61\n\u2208 R3 is a unit vector representing the direction of the\nright forearm from the elbow to the hand, and uref is a unit vector\nrepresenting the up axis of the world space. In our implementation,\nthe toy pistol is fixed on the right hand, which is linked to the right\nforearm with a fixed joint. Therefore, we use the direction of the\nright forearm as the aiming direction. When the aiming action is not\nactivated, we use the 2nd reward term to encourage the character\nto lift its arm and hold the gun up without aiming anything.\nThe goal state g\ud835\udc61 \u2208 R3 is a unit vector representing the target\naiming direction. We let g\ud835\udc61 = 0 if the aiming action is not activated.\nWhen combined with the target location task, aiming is deactivated\nif the character is close to the goal, i.e. ||x\ud835\udc61 \u2212 pgoal|| \u2264 \ud835\udc45. g\ud835\udc61 is\nsampled with an elevation angle in range of [0, \ud835\udf0b/6] and azimuth\nangle in [0, \ud835\udf0b/4].\nB.4\nTask: Tennis Swing\nThe goal-directed reward is defined as\n\ud835\udc5f\ud835\udc61 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n1.2 + ||vout||/10\nif ball was hit and \ud835\udc51fall = 0\n\ud835\udc5fpose\n\ud835\udc61\n+ 0.5 exp(\u22120.1\ud835\udc512\nfall)\nif ball was hit but \ud835\udc51fall > 0\n\ud835\udc5fpose\n\ud835\udc61\notherwise\n(15)\nwhere\n\ud835\udc5fpose\n\ud835\udc61\n= 0.2\ud835\udc5fshoulder\n\ud835\udc61\n+ 0.5\ud835\udc5fracket\n\ud835\udc61\n,\n\ud835\udc5fshoulder\n\ud835\udc61\n= exp(\u2212 max(||pshoulder\n\ud835\udc61\n\u2212 pball\n\ud835\udc61\n|| \u2212 1, 0)2),\n\ud835\udc5fracket\n\ud835\udc61\n= exp(\u22125||pracket\n\ud835\udc61\n\u2212 pball\n\ud835\udc61\n||2).\n(16)\npshoulder\n\ud835\udc61\nis position of the character\u2019s right shoulder and pracket\n\ud835\udc61\nis\nthe position of the racket. To emulate the tennis court, we consider\na valid ball falling region with dimension 12\ud835\udc5a \u00d7 11\ud835\udc5a, which is 6\ud835\udc5a\nahead of the initial position of the tennis ball along the x-axis. \ud835\udc51fall is\nthe distance from the ball\u2019s falling point to this region. We let \ud835\udc51fall =\n0 if the ball will fall or fell in the target region. \ud835\udc51fall is estimated by\na simple projectile model based on the linear velocity of the ball\nwithout considering any friction or air resistance, but updated at\nevery simulation step in order to get an accurate estimation. ||vout||\nis the outgoing speed of the tennis ball when it was hit. The purpose\nof using \ud835\udc5fshoulder\n\ud835\udc61\nis to encourage the character to approach the\ntennis ball but not necessarily when the distance is less than 1\ud835\udc5a\nsuch that the character can have enough space to swing the racket,\nrather than keeping moving close to the ball.\nThe goal state g\ud835\udc61 \u2208 R4 includes a 3D vector representing the po-\nsition of the ball pball\n\ud835\udc61\n, and a scalar identifying the heading direction\nof the character\u2019s root link. The heading direction in g\ud835\udc61 is used to\nidentify the direction of x-axis toward which the ball is expected to\nbe hit. We let pball\n\ud835\udc61\n= 0 when constructing g\ud835\udc61 if the ball was hit.\nB.5\nJuggling\nThe goal-directed reward is defined as\n\ud835\udc5f\ud835\udc61 = 0.5\ud835\udc5fhand,left\n\ud835\udc61\n+ 0.5\ud835\udc5fhand,right\n\ud835\udc61\n(17)\nwhere \ud835\udc5fhand,left\n\ud835\udc61\nand \ud835\udc5fhand,right\n\ud835\udc61\nare defined identically but evaluate\nthe performance of the left hand and right hand respectively. For\neach hand-related reward, we define\n\ud835\udc5fhand\n\ud835\udc61\n=\n(\n\ud835\udc5fthrow\n\ud835\udc61\nif \ud835\udc61 mod \ud835\udf0f = 0\n0.1\ud835\udc5fheight\n\ud835\udc61\n+ 0.9\ud835\udc5fdistance\n\ud835\udc61\notherwise\n(18)\nwhere \ud835\udf0f is the time interval between two trials of ball throwing and\n\ud835\udc5fthrow\n\ud835\udc61\n= exp(\u22125(\ud835\udc63ball\n\ud835\udc61\n/\ud835\udc49 ball \u2212 1)2),\n\ud835\udc5fheight\n\ud835\udc61\n= exp(\u221220(\u210eball \u2212 \u210ehand\n\ud835\udc61\n)2),\n\ud835\udc5fdistance\n\ud835\udc61\n= 0.9 exp(\u221220\ud835\udc512\n\ud835\udc61 ) + 0.2 exp(\u2212\ud835\udc512\n\ud835\udc61 ).\n(19)\nAs stated in Section 6.4, we employ an automatic catch-and-throw\nmechanism where a ball is considered caught by a hand and is fixed\nto that hand if it is close enough, and will be detached (thrown)\nautomatically at a fixed time interval \ud835\udf0f between two trials of throw-\ning. The target ball for a hand is decided using a cascade juggling\npattern. In the reward function, \ud835\udc5fthrow\n\ud835\udc61\nmeasures the performance\nof ball throwing and is computed only at the frame where a ball\nis thrown. \ud835\udc63ball\n\ud835\udc61\nis the vertical velocity of the thrown ball and \ud835\udc49 ball\nis the preferred vertical thrown velocity. The preferred velocity is\nobtained by assuming that the thrown ball will be caught at the\nsame height where it is thrown and at a dwell time \ud835\udc61\ud835\udc51 before the\nnext time the catching hand performs a thrown. In our experiment,\nwe set \ud835\udf0f = 2/3\ud835\udc60 (20 frames) with a preferred dwell time \ud835\udc61d = 0.4\ud835\udc60\n(12 frames) and set the number of balls \ud835\udc41ball = 3. Given the gravity\n\ud835\udc54 = 9.81\ud835\udc5a/\ud835\udc602, this leads to a preferred velocity\n\ud835\udc49 ball = 0.5\ud835\udc54( \ud835\udf0f\n2\ud835\udc41ball \u2212 \ud835\udc61d) = 2.94\ud835\udc5a/\ud835\udc60.\n(20)\nThe height-related reward term \ud835\udc5fheight\n\ud835\udc61\nmeasures the error between\nthe hand\u2019s vertical position (\u210ehand\n\ud835\udc61\n) and the target ball\u2019s height when\nit was thrown (\u210eball). It encourages the control policy to throw and\ncatch a ball at the same height. We let \ud835\udc5fheight\n\ud835\udc61\n= 1 if the target\nball was caught by the hand already. The distance-related reward\n\ud835\udc5fdistance\n\ud835\udc61\nmeasures the distance error between the hand and the\ntarget ball. We estimate the ball\u2019s vertical movement trajectory\nusing a simple projectile model taking into account only the ball\u2019s\nvertical linear velocity and gravity. The distance \ud835\udc51\ud835\udc61 is defined as the\ndistance between the hand and the target ball if the hand is above the\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\nComposite Motion Learning with Task Control\n\u2022\n3\nTable 3. Hyperparameters\nParameter\nValue\npolicy network learning rate\n5 \u00d7 10\u22126\ncritic network learning rate\n1 \u00d7 10\u22124\ndiscriminator learning rate\n1 \u00d7 10\u22125\nreward discount factor (\ud835\udefe)\n0.95\nGAE discount factor (\ud835\udf06)\n0.95\nsurrogate clip range (\ud835\udf16)\n0.2\ngradient penalty coefficient (\ud835\udf06\ud835\udc3a\ud835\udc43)\n10\nnumber of PPO workers (simulation instances)\n512\nPPO replay buffer size\n4096\nPPO batch size\n256\nPPO optimization epochs\n5\ndiscriminator replay buffer size\n8192\ndiscriminator batch size\n512\nestimated trajectory, i.e. when the hand is unable to catch the ball at\nthe current hand height, or just the horizontal distance otherwise.\nAs such, \ud835\udc5fdistance\n\ud835\udc61\nignores the vertical ball-hand distance if the hand\nis able to catch the ball at its current height, and thus prevents the\nhand from aggressively moving toward the ball vertically.\nThe goal state g\ud835\udc61 \u2208 R19 includes the three balls\u2019 states (position\nand linear velocity) and a timer variable counting the time left before\nthe next throwing of the ball by one hand. The ball states are in the\norder of the left-hand target ball, the right-hand target ball, and the\nother ball. For a caught target ball, we let the corresponding state\nbe zero.\nC\nHYPERPARAMETERS\nThe hyperparameters used for policy training is listed in Table 3.\nHalf of the samples for discriminator training are from the simu-\nlated character and half are sampled from the reference motions.\nThe character state horizon \ud835\udc5b + 1 is chosen as 4, and the discrim-\ninator observation horizon \ud835\udc5b\ud835\udc56 + 2 is 3 for aiming motions and 5\nfor other motions. The objective weight \ud835\udf14\ud835\udc58 in Eq. 9 is 0.5 shared\nequally by all goal-related objectives. In the Juggling with Target\nLocation task, given the difficulty of ball catching, the juggling task\nis assigned a weight of 0.6, the locomotion task has a weight of\n0.1, and the imitation tasks account for the remaining weight with\na ratio of 1 : 4 for juggling and walking motion imitation. In the\nAiming+Locomotion task, the upper-body motion of aiming has a\nweight of 0.2 and the lower-body motion has a weight of 0.3. On\nthe other tests, besides the weights taken by the goal-related ob-\njectives, the remaining weight is shared equally by the imitation\nobjectives.\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n"
  },
  {
    "title": "COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?",
    "link": "https://arxiv.org/pdf/2305.03689.pdf",
    "upvote": "1",
    "text": "Cola: A Benchmark for Compositional\nText-to-image Retrieval\nArijit Ray1,2\nFilip Radenovic2\nAbhimanyu Dubey2\nBryan A. Plummer1\nRanjay Krishna2,3\nKate Saenko1,2\n{array, bplum, saenko}@bu.edu, {filipradenovic, dubeya}@fb.com,\nranjay@cs.washington.edu\n1Boston University, 2Meta AI (FAIR), 3University of Washington\nAbstract\nCompositional reasoning is a hallmark of human visual intelligence. Yet despite\nthe size of large vision-language models, they struggle to represent simple com-\npositions by combining objects with their attributes. To measure this lack of\ncompositional capability, we design Cola, a text-to-image retrieval benchmark\nto Compose Objects Localized with Attributes. To solve Cola, a model must\nretrieve images with the correct configuration of attributes and objects, and avoid\nchoosing a distractor image with the same objects and attributes but in the wrong\nconfiguration. Cola contains about 1.2k composed queries of 168 objects and\n197 attributes on around 30K images. Our human evaluation finds that Cola is\n83.33% accurate, similar to contemporary compositionality benchmarks. Using\nCola as a testbed, we explore empirical modeling designs to adapt pre-trained\nvision-language models to reason compositionally. We explore 6 adaptation strate-\ngies on 2 seminal vision-language models, using compositionality-centric test\nbenchmarks - Cola and CREPE. We find the optimal adaptation strategy is to\ntrain a multi-modal attention layer that jointly attends over the frozen pre-trained\nimage and language features. Surprisingly, training multimodal layers on CLIP\nperforms better than tuning a larger FLAVA model with already pre-trained multi-\nmodal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA\nto comparable levels, suggesting that training multimodal layers using contrastive\nattribute-object data is key, as opposed to using them pre-trained. Lastly, we show\nthat Cola is harder than a closely-related contemporary benchmark, CREPE, since\nsimpler fine-tuning strategies without multimodal layers suffice on CREPE, but not\non Cola. However, we still see a significant gap between our best adaptation and\nhuman accuracy, suggesting considerable room for further research. Project page:\nhttps://cs-people.bu.edu/array/research/cola/\n1\nIntroduction\nCompositionality is a fundamental characteristic of human intelligence, allowing us to elicit \u201cthe\nmeaning of the whole [as] a function of the meanings of its parts\u201d [9]. In language, the whole is\na sentence made up of words like nouns and adjectives. In vision, the whole is an image made\nup of visual elements like objects and attributes [28, 20]. For example, the expression \u201cround\nwhite table\u201d is a composition of the noun \u201ctable\u201d and adjectives \u201cround\u201d and \u201cwhite\u201d, visually\nrepresented in the leftmost photo of Fig. 1. Recent work has consistently identified that this type of\ncompositionality\u2014that between objects and their attributes\u2014is something existing vision-language\nmodels struggle to represent [55, 35, 25]. Instead, they disperse attributes and ground them to\ndistractor objects; for instance, they incorrectly match \u201cround white table\u201d to the second left photo in\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2305.03689v3  [cs.CV]  3 Nov 2023\nFigure 1: We present\nCola, where a model has to Compose Objects Localized with Attributes.\nTo solve Cola, a model must match the correct image to the correct caption, not a distractor image\nwith the same objects and attributes but in the wrong configuration. We explore the design space\nof possible mechanisms to adapt existing models to this task; we show that a simple multimodal\nadaptation method to finetune pre-trained vision-language representations works best.\nFig. 1 by grounding the attribute \u201cround\u201d to the round plate instead of the table. Queries involving\ntwo objects are even more challenging, see Fig. 1 (right).\nIn this paper, we study the ability of large vision-language models to Compose Objects Localized\nwith Attributes (Cola). Unlike related baselines that study compositionality using relationships [55]\nand scene graphs [35], we focus on attribute-object bindings, since finding objects with correct\nattributes is crucial in many applications. For example, an embodied AI assistant told to clean the\n\u201csmall wood table to the right of the large dark chair\u201d should not start cleaning the \u201ctan wood chair by\nthe large brown wood smooth table.\u201d Additionally, object-attribute bindings should fundamentally be\neasier than compositions with relationships or scene graphs. However, we find that existing models\nstill struggle with this simpler binding.\nTo explore these issues, we propose the Cola benchmark for composing objects localized with\nmultiple attributes. Cola contains two kinds of compositions: single-object queries (Fig. 1 left) and\nmulti-object (Fig. 1 right). In each case, a model should associate the objects in the query with the\ncorrect attributes and ignore difficult distractor compositions where the query attributes are attached\nto distractor objects. Multi-object queries are harder since they showcase more compositions.\nUnlike an image-to-text setting in a contemporary benchmark CREPE [35], Cola evaluates models\nusing text-to-image, where text queries are used to retrieve the correct image from a set of im-\nages. This is consistent with previous benchmarks for vision-language models [21, 42, 64]. Further,\ntext-to-image retrieval is harder than image-to-text retrieval because image encoders are weaker at\ndistinguishing fine-grained differences in images for a given text than text encoders are at distinguish-\ning fine-grained text [55]. Moreover, text-to-image is better aligned with practical applications, such\nas a user giving text instructions to a machine to find certain objects.\nUsing Cola as a development testbed, our experiments add to the ongoing discussion that pre-trained\nvision-language models perform poorly on compositions [55]. Hence, we explore 6 finetuning\nstrategies on 2 seminal vision-language models- CLIP [43] and FLAVA [52]- to find adaptation\nstrategies that encourage compositionality the most. We finetune using 3 datasets (GQA [18],\nCLEVR [24], and PACO [44]) and evaluate on 2 testbeds (Cola as well as CREPE [35]). While\nexploring effective pre-training strategies is another valid avenue of exploration, we limit our work to\nadaptation strategies since training a model from scratch is expensive and can only be executed by a\nhandful of research organizations.\nWe find that the best-performing architectural choice during adaptation to be a multi-modal trans-\nformer encoder-decoder [53, 52, 26] to further encode the visual and language representations from\nthe pre-trained model. Multimodal adaption performs significantly better than tuning the unimodal\nencoders (that encode just vision or language features), or tuning the prompt. Surprisingly, this\nadaptation improves both CLIP and FLAVA to produce comparable finetuned models, even though our\nCLIP model has fewer parameters and FLAVA was already pre-trained using multi-modal transformer\n2\nlayers. This suggests that training multimodal layers using contrastive attribute-object data is key,\nas opposed to using them pre-trained on web data. Our adaptation also significantly outperforms\nstandard ways to adapt/tune foundational models such as prompt-tuning [65], linear-probing [2], or\ntuning a comparable number of split-encoder layers. Similar to recent work identifying that structural\ncompositionality is present in models but absent in their representations [31], our work finds that\nwhile pre-trained representations might not exhibit compositionality, they can be adapted to do so.\nHowever, the stark difference between human accuracy and our best adaptation suggests considerable\nroom for further research using our benchmark.\n2\nRelated Work\nCompositionality and image retrieval. Compositionality is a key aspect of human intelligence [10],\nespecially in vision and language [7]. Vision-language compositionality has been explored for visual\nquestion answering [1], composed image retrieval (e.g., X in the style of Y) [49], and generation [56].\nCompositionality is one crucial aspect of improving robustness to diverse queries, a theme heavily\nexplored in the vision-language community [46, 45, 51, 1, 8]. With the recent popularity of foundation\nmodels, various works focus on testing their compositional reasoning [35, 55, 25, 62]. Compared to\nCREPE [35] and ARO [62], a model must distinguish between difficult images in our case. Text-to-\ndifficult-images is harder because distinguishing between difficult images (for a given caption) is\nharder than distinguishing between difficult captions [55]. Whereas benchmarks like Winoground [55]\nprimarily evaluate broad and complex relational compositionality (e.g., \u201cman hugs woman from\nbehind\u201d vs \u201cwoman hugs man from behind\u201d), we specifically focus on attribute object bindings in\nqueries. This is motivated by practical applications such as an embodied agent trying to retrieve a\ncustom object (like \u201ca metal wrench with a red rubber handle\u201d) in a cluttered workspace with similar\ndistractor objects [39]. Most works in the area of attribute-object image retrieval either focus on\nsingle attributes [19, 41] or multiple attributes in very niche domains with centered images and plain\nbackgrounds of dresses [16], animals [60], shoes[61], or birds [58]. In contrast, we focus on scenes\nwith multiple objects and attributes where distractor objects also have the same attributes.\nVision-language aligment. Recently, there has been a flurry of image-text alignment models to learn\nthe similarity of matched images and text in various ways. Some models use separate unimodal\nencoders [43, 22] for the image and text, whereas some [52, 33, 3, 11, 50] use multimodal encoders as\nwell. Various strategies such as hard negative mining [33], concept distillation [40], and maintaining\nthe momentum of image-text mappings [17] have been employed to push performance. We focus\non testing and improving the attribute-object binding capability of such models and choose the\nmost seminal model, CLIP [43], which is widely adopted in various concurrent vision-language\nresearch/applications [54, 27, 47]. Our approaches do not use any box annotations unlike recent text\nlocalization models [26, 34, 63], which we also see to underperform on text-to-image retrieval.\nAdapting foundational models. Since training a new VLM from scratch is expensive, we wish\nto formulate a simple adapter that improves the compositional attribute-object binding. Various\nworks explore adapting foundation models [4] with prompt-tuning [65], linear-probing [2], and\nfine-tuning with residual connections [13]. Prompt-tuning [32] learns the embedding layer of the\nword inputs and keeps the model frozen. Inspired by the success of prompt-tuning [32], some\nworks have also explored prompting in the vision [23, 5] and vision-language [65, 48], and also for\nsingle attribute-object compositions [37]. Our optimal finetuning strategy improves significantly\nover prompt and fine-tuning for attribute-object compositions in even more difficult settings. Our\nmulti-modal strategies are similar to MAPL [36], except our lightweight adapter attends over language\nand vision representations, whereas MAPL only attends over language.\n3\nCola benchmark\nOur goal is to adapt vision-language features to improve the compositional binding of attributes to\nobjects. Specifically, we aim to improve the classification of a query involving single or multiple\nobjects with multiple attributes in an image. Images and language are composed of atomic concepts\nsuch as attributes and objects. The atomic concepts (\u201csquare\u201d, \u201cplate\u201d) form certain compounds\n(\u201csquare plate\u201d), and then the scene is a combination of various such compounds (\u201csquare plate on\nwhite table\u201d). Hence, we create a benchmark where we form queries using compositions of such\n3\natoms and test a model\u2019s ability to distinguish between images that correctly contain the atoms in\nthe correct composition to distractor images that contain them in the wrong composition. In total,\nthe Cola benchmark contains about 1236 composed queries from 168 objects and 197 attributes on\naround 30K images from 4 datasets.\nCola contains two query types discussed below: single-object compounds and multi-object queries.\nRetrieval using single-object queries. Single-object queries have multiple attributes grounded on\none object. For example, the query \u201csquare white plate,\u201d which is of the form, Q \u201c a1a2o, where\nai P A is drawn from a finite set of possible attributes and o P O is similarly a category drawn\nfrom a finite set of objects. With this query, a model should associate the images with the correct\nattachment of attributes (\u201csquare,\u201d \u201cwhite\u201d) to the object (\u201cplate\u201d), and ignore incorrect attachments\nof the same attributes and objects (like square table but not square plates). Hence, the task is a\ntext query for image retrieval among difficult distractors. We first create a list of queries with more\nthan one attribute for an object. Next, we curate a set of images where at least one of the query\nwords is present in the image. For example, for \u201csquare white plate,\u201d all images containing \u201csquare\u201d\nobjects, \u201cwhite\u201d objects, or \u201cplates\u201d are in the list of images to retrieve from. The goal of the retrieval\nproblem is to score the images having the correct attachment of the attributes to the query higher than\nothers. We build the test set for single object queries using three datasets with object and attribute\nannotations: 1) GQA [18]: After filtering for objects with at least 1 attribute annotated, we have 320\nsingle-object queries composed of 114 objects and 114 attributes on 1952 images. The objects and\nattributes comprise common objects, making this split useful for practical applications. 2) CLEVR\n[24]: We have 3 object shapes - cubes, cylinders and spheres, composed with 8 colors, 2 materials,\nand 2 sizes on 15K images. 3) PACO [44]: This split consists of objects similar to GQA. We have\n400 queries composed of 51 objects and 61 attributes on 7921 images.\nRetrieval with multi-object queries. Drawing on existing literature [35], a multi-object query\ncontains multiple objects, each with its own set of attributes. For example, \u201csquare white plate on top\nof brown wooden table,\u201d which is of the form, Q \u201c a1a2o1 ` a3a4o2, where ai P A is drawn from\na finite set of possible attributes and oj P O from a finite set of objects. In this setting, we want to\ncheck if the model gets confused with the wrong configuration of objects and attributes. Thus, we\nfind distractor image-query pairs where the attributes and objects are switched. An example image\nfor a query Q \u201c a1o1 ` a2o2 would be of the form I1 \u201c a2o1 ` a1o2. In other words, we switch the\nattributes of the two objects. We curate these distractors to ensure that o1 \u2030 o2 and a1 \u2030 a2. The\nretrieval task, framed with this formalism, is to rank the correct images for the correct captions such\nthat it is ranked higher than the distractor images: to learn a relevance encoding fpI, Qq for image I\nand query Q such that fpI, Qq \u0105 fpI1, Qq & fpI1, Q1q \u0105 fpI, Q1q. The test set is built using test\nsplit of the Visual Genome [29] dataset.\nFiltering Cola multi-object with crowd workers. We use the object, attribute, and relationship\nannotations in the Visual Genome dataset [29] to create the multi-object queries. We filter the\nimage-caption pairs with object and attribute compositions swapped as described above. We conduct\na human-annotated cleaning of this filtered test set. We display the images I and I1 and queries Q\nand Q1 to 10 crowd workers and ask them to choose which image is most relevant to which query.\nWe only keep the image-query pairs where the majority of crowd workers can correctly assign the\ncorrect image to the query. After filtering, we are left with 210 data points (840 image-query pairs)\nwith 1680 possible image-query matches. The human agreement (accuracy) on our validation set is\n83.88% - an average of 8.33 out of 10 humans agree that the first image matches to the first caption\nand second image to the second caption. Some qualitative examples are provided in Fig. 2a.\n4\nExploring finetuning strategies with Cola\nGiven an image (I) and a query (Q), Cola evaluates a model fpI, Qq by measuring how well it\nassociates the correct image to the input query. Existing pre-trained models don\u2019t perform well\non this task since they fail to distinguish fine-grained differences in attribute-object compositions.\nHence, we explore finetuning strategies that use a dataset of image-language pairs where the language\ndescriptions contain objects and attributes. Details of finetuning datasets are described in Sec. 5.3.\nWe follow the standard finetuning paradigm by sampling batches of images and text from the attribute-\nobject Cola finetuning dataset. Specifically, we match the correct images to the correct queries in\neach batch and minimize the NCELoss typically used in contrastive learning [43, 52]. This finetuning\n4\n(a)\n(b)\nFigure 2: a) Cola multi-object setting validation set: a human-cleaned difficult validation set for\ntesting attribute-object binding. The two images have similar objects and attributes but in different\nconfigurations. A model must match the correct images to the correct captions. b) The optimal\nadaptation strategy (MM-Adapter): a lightweight multimodal transformer encoder on top of frozen\npre-trained encoders. The multimodal encoder crafts a stronger representation by cross-attending\nto image patches and text tokens to attach the correct attributes to the correct objects. The stronger\nrepresentation is then trained to align with the frozen text representation.\nstep aims to improve the compositional binding of attributes and objects in pretrained vision-language\nfeatures. This is in contrast to training the multimodal layers on random batches of web image-text\ndata.\nDisjoint finetuning strategies Since CLIP [43] is commonly used for various tasks and is one of\nthe more lightweight vision-language foundation models, we focus most of our finetuning strategies\nwith CLIP in mind. Although, we also later use these finetuning strategies on the newer and larger\nFLAVA [52] model. CLIP [43] consists of two encoders: one that encodes the input image and one\nthat similarly encodes the input text. The output representations of the two modalities are used as\nseparate embeddings for the image and text. To adapt these models for a specific task/capability,\nresearchers commonly use linear-probing or prompt-tuning. Linear-probing trains linear layers on\ntop of the frozen visual and text encoders using the finetuning dataset. Prompt-tuning learns the word\nembeddings of the query to adapt to the finetuning dataset domain without changing the weights of\nthe model [65]. Other methods fine-tune the later layers of both the encoders. All these adaptation\nmethods tune the parameters of the two encoders separately.\nJoint multimodal strategies We hypothesize that the above common adaptation strategies don\u2019t ap-\npropriately capture the cross-modal interaction required for strong attribute-object binding. However,\nCLIP is significantly more lightweight than recent multimodal models. Hence, we explore lightweight\nmulti-modal adaptation strategies to adapt CLIP. We describe the best-performing multimodal adapta-\ntion strategy found in our experiments which is also depicted in Fig. 2b. This multi-modal adaptation\nborrows a transformer encoder-decoder [53, 52, 26] to attend over the image and language repre-\nsentations jointly. Let M \u201c rI; Qs, denote the concatenated image patch representations extracted\nfrom CLIP\u2019s visual encoder and the token-level representations from the query. We compute a\nself-attention over M using a transformer encoder A \u201c AttpMq. Finally, we use a classify token (a\ntoken randomly initialized), referred to as [CLS], that cross-attends 1 to all the self-attended features\nA using a transformer decoder to produce outMM. This type of cross-attending to self-attended\nfeatures is similar to FLAVA[52]/MDETR [26].\nThe standard practice in most multi-modal encoder-based prediction models would be to learn a linear\nlayer to classify the output [CLS] token embedding [26, 52, 33, 11]. However, instead of learning\na linear predictor, we compute the cosine similarity of [CLS] to the representations of the query\ntokens: qi from the frozen text encoder, Q. We posit that aligning to a frozen text encoder trained\non larger scale data will act as a regularizer, helping performance on unseen compositions. Hence,\nthe final score for a given image-query pair is fpI, Qq \u201c\n1\nNq\n\u0159Nq\ni\noutMM d qi where Nq is the\nnumber of tokens in the query. These two ablations are referred to as MM-pred and MM-adapter\n1query comes from [CLS], and the keys and values come from self-attended features, A.\n5\n(\u201cadapter\u201d since we can think of the latter as adapting the image features to align better with text) in\nour experiments.\nWe also tried various flavors of computing a multi-modal adaptation inspired by FIBER [11] and\nALBEF [33], which use cross attention between text and image. We would like to stress that while\nexploring newer ways to fuse vision and language features is a valid avenue of research, we are\ninterested in exploring the common themes in current fusion methods that encourage compositionality\nthe most to drive future research. We report the best method for simplicity and include the accuracies\nfrom other strategies in Table 5 in the supplemental.\n5\nEvaluation Setup\nWe evaluate models on the two types of queries described in Sec. 3 on Cola and CREPE [35]\ndatasets. All models are trained using Pytorch [38] and use the Huggingface transformers library [59].\nImplementation details such as hyperparameters are provided in Sec. 11 of the supplementary.\n5.1\nMetrics\nSingle object queries. For this type of query, we report the Cola MAP2, the mean average retrieval\nprecision over difficult distractors. We further differentiate the mAP between seen and unseen queries.\nWe split Cola into seen and unseen sets by removing some attribute-object pairs from the training\nset. For example, \u201csquare white plate\u201d is unseen if this combination is absent in finetuning; however,\n\u201csquare white bowl\u201d or a \u201csquare plate\u201d may be present. In our test set, we have 320 (150 unseen,\n170 seen) queries on 1950 images for GQA, 96 (32 unseen, 64 seen)3 queries on 22500 images for\nCLEVR, and 400 (200 seen, 200 unseen) queries on 7921 images in PACO.\nMulti-object queries. Recall that we have two images and two captions, and the task is to match\nthe correct caption to the correct image. If we denote the prediction score for an image and query to\nbe fpI, Mq, we regard a prediction to be correct if fpI, Mq \u0105 fpI1, Mq & fpI1, M 1q \u0105 fpI, M 1q,\nwhere images I and I1 are paired to captions M and M 1 respectively. Using this criterion, we\ncompute the Cola multi-object accuracy. The random accuracy is 25% since there are four ways to\nmatch the two captions to the two images. We also evaluate on a contemporary dataset, CREPE [35],\nwhere the task is inverse. For CREPE, we compute an image-to-text (I2T) accuracy, where a model\nmust match the correct text from two choices to the given image. Note that there is only one image\nfor the two caption choices in CREPE [35]. The random accuracy is 50% since it is a binary task.\n5.2\nExplored finetuning strategies\nRecall that the best-performing finetuning approach we found is a multimodal adaptation (MM-\nAdapter) for tuning pre-trained image and text features as described in Sec. 4. We compare against\npopular tuning methods like linear probing, tuning the prompt embeddings (prompt-tuning), and\nfine-tuning the whole model (FT all) or the last two layers (FT Late). These adaptations are applied\nseparately to the base model for comparison. More details are in the supplementary (Sec. 11). Since\nour adaptation uses multimodal attention, we also compare it to a seminal model that uses multimodal\nattention in pretraining. We chose FLAVA [52] since it is one of the recent models after CLIP which\nis bigger, more accurate [52] and has easily available pre-trained weights.\n5.3\nFinetuning datasets\nThe Cola training sets are also built in the same way as described in Sec. 3 using the training splits\nof GQA [18, 29], CLEVR [24] PACO [44], and Visual Genome [29]. For GQA, the training split\ncontains 1381 objects and 601 attributes that compose 27078 queries on 74K images. For CLEVR,\nwe have 3 shapes composed with 8 colors and 2 sizes on 70K images. Finally, for PACO, we have\n75 objects and 55 attributes that compose 18696 queries on 37883 images. The Cola multi-object\n2we also computed the F1 score and see trends remain the same; more details in the supplementary, Sec. 10,\npage 18.\n3we report \u201cseen\u201d only on 32 classes to avoid disbalance with unseen. \u201cAll\u201d MAP is on all 96 classes. \u201cSeen\u201d\ntrends hold same with 64 classes as well; more details in appendix.\n6\nGQA\nCLEVR\nPACO\nAll\nUnseen\nSeen\nAll\nUnseen\nSeen\nAll\nUnseen\nSeen\na. CLIP\n36.53\n39.06\n34.24\n15.38\n15.01\n15.32\n12.21\n8.64\n15.79\nb. +Linear\n40.44\n42.87\n38.24\n47.96\n29.43\n46.75\n14.22\n6.75\n21.68\nc. +prompt-tune\n37.40\n40.69\n34.43\n29.61\n23.17\n28.05\n12.76\n5.92\n19.61\nd. +FT all\n38.81\n40.85\n36.95\n52.32\n19.00\n47.95\n14.58\n6.49\n22.66\ne. +FT late\n42.19\n44.61\n40.01\n64.06\n27.53\n67.48\n15.66\n8.74\n22.58\nf. +MM-Pred (us)\n45.99\n48.6\n43.64\n75.80\n51.98\n80.72\n15.49\n8.00\n22.94\ng. +MM-Adapter (us)\n46.83\n48.86\n44.99\n88.21\n89.52\n77.00\n18.56\n11.47\n25.66\nh. FLAVA\n39.65\n42.18\n37.37\n15.41\n13.27\n15.93\n12.53\n7.29\n17.76\ni. +Linear\n37.07\n39.96\n34.46\n19.30\n17.53\n18.52\n11.65\n7.90\n15.39\nj. +FT-late\n39.58\n42.26\n37.16\n77.95\n72.72\n66.42\n12.82\n5.79\n19.84\nk. +MM-Pred (us)\n47.12\n51.53\n43.13\n90.43\n85.78\n86.07\n18.57\n10.71\n26.44\nl. +MM-Adapter (us)\n48.54\n52.55\n44.91\n91.10\n86.64\n87.39\n19.36\n11.16\n27.55\nTable 1: mAP results on the Cola single object compounds setting. Our multimodal adaptation\n(MM-Adapter) performs better than common tuning methods. Further, multimodal attention to adapt\nthe image representation (MM-Adapter) generalizes better than using it simply as a prediction head\n(MM-Pred). MM-Adapter on CLIP is better than tuning the pre-trained multimodal attention layers\nof the bigger FLAVA (+FT late). MM-Adapter further improves FLAVA.\ncompounds training split has 551,980 multi-object compounds on 71,174 images. Only the test split\nis cleaned using human annotations. For datasets built on GQA [18] and Visual Genome [29], we\nleverage the annotations to explore the effects of different kinds of data queries. We use the region\ndescriptions (denoted as RegionCap in the tables) in Visual Genome to test if linguistic diversity\nhelps over templated captions (Cola single objects and multi-object). We also compare to hard\nnegatives from the Cola multi-object pairs. We finally have a combined setting where we use all data.\n6\nResults\nRecall that we evaluate on two settings for Cola - the single-object compounds setting and the\nmulti-object compounds setting as defined in Sec. 3. We discuss the quantitative results below and\nsome qualitative results are shown in Fig. 3.\n6.1\nCola Single-object retrieval\nMultimodal adaptation is more effective than other tuning methods: In Table 1, compared to\nprompt-tuning (row c), fine-tuning all of CLIP (row d), or fine-tuning a few of the later layers (row\ne), tuning a multimodal attention layer of same/lesser parameters has higher mAP (row f and g).\nLinear probing (row b), although cheaper, significantly underperforms. This is not surprising since\nmultimodal attention over the image regions and text tokens offers more flexibility to the model to\nlearn to bind the right attributes to the right object region. Tuning the whole model is also worse\nthan tuning the later unimodal layers (row d vs e). This might be because fine-tuning the whole\nmodel requires larger batch sizes with significantly more data. In Fig 3, we show the comparison of\ntuning unimodal layers vs our multimodal adaptation (since tuning the unimodal layers is closest in\nperformance). Qualitative examples from each of the other adaptation methods are displayed in Figs.\n9-15 in the supplementary.\nUsing pre-trained multimodal attention layers is not enough - training them on attribute-object\ncompositions is key: In Table 1, we see that MM-Adapter (row g) on CLIP ViT B-32 (151 M params)\noutperforms tuning the last two multimodal layers of FLAVA B-16 [52] (241M params) model (row\nj) or tuning a linear probe (row i). Surprisingly, tuning the last two FLAVA multimodal layers (row j)\nis worse than replacing them and training using MM-Pred and MM-Adapter layers (rows k and l).\nThis suggests that training multimodal layers during adaptation (as opposed to pre-training) is key.\nMM-Adapter is better than using multimodal attention as a prediction head: Recall that one\nof the ablations of our approach is aligning the output of the multimodal encoder to the frozen\n7\nMulti-Obj Acc \u00d2\nT2I\nI2T\nCola\nCREPE [35]\n- Random\n25.00\n50.00\n- Human\n83.88\n-\no. CLIP\n21.42\n77.43\na. + Linear\n30.47\n87.35\nb. + Prompt-tune\n27.14\n80.81\nc. + FT all\n34.76\n82.39\nd. + FT late\n36.19\n87.14\ne. + MM-Pred (our)\n41.42\n77.84\nf. + MM-Adapter (our)\n40.95\n87.02\ng. FLAVA\n24.76\n65.10\nh. + Linear\n22.38\n55.10\ni. + FT late\n22.38\n58.11\nj. + MM-Pred (our)\n39.04\n81.37\nk. + MM-Adapter (our)\n40.47\n74.81\n(a)\nData Type\nSingle-Object Cola GQA\nAll\nUnseen\nSeen\na. RegionCap\n0.4711\n0.4965\n0.4481\nb. SingleObj\n0.4683\n0.4886\n0.4499\nc. + MultiObj\n0.4641\n0.4795\n0.4501\nd. + HardNeg\n0.4688\n0.4843\n0.4548\ne. Combined\n0.4788\n0.4983\n0.4612\nMulti-Object\nCola\nCREPE\na. RegionCap\n0.3114\n0.8833\nb. SingleObj\n0.2745\n0.9023\nc. + MultiObj\n0.3975\n0.8702\nd. + HardNeg\n0.3483\n0.8775\ne. Combined\n0.3893\n0.8798\n(b)\nTable 2: a. Results on our multi-object compounds setting on our Cola task and CREPE. Simpler\nmethods suffice on CREPE, but not on Cola, suggesting that Cola is harder. Red-orange-yellow is in\ndecreasing accuracy order. MM-Adapter and MM-Pred on CLIP perform well on average on both. b.\nTable showing the effect of the data type used in the contrastive batch training. Having multi-object\ncaptions in the data helps Cola performance while maintaining CREPE performance.\ntext embedding, making it a multimodal image-feature \u201cadapter\" (MM-Adapter). This contrasts\nto using the multimodal module as a prediction head with a linear layer (MM-Pred). As shown\nin Table 1, MM-Adapter outperforms MM-Pred (row g vs f), especially on unseen classes and on\nexhaustively annotated datasets like CLEVR [24] and PACO [44]. We posit that aligning to the frozen\ntext representation acts like a regularizer since it was pre-trained on more data.\n6.2\nMulti-object retrieval\nSimpler methods suffice for CREPE, but not for Cola, suggesting that Cola is a harder task: As\nshown in Table 2a, linear probing (row a) or simple fine-tuning (rows c and d) suffice for CREPE [35].\nHowever, our MM-Adapter improves further on Cola (row f vs row a, b, c, d), while maintaining\nperformance on CREPE. This also suggests that text-to-image matching is harder than image-to-text\nmatching, which is also reflected in Winoground [55].\nBaseline CLIP and FLAVA perform below chance: If we evaluate off-the-shelf CLIP [43] and\nFLAVA [52] on our Cola dataset, we see in Table 2a that it performs below random (row o and g).\nThis is consistent with the findings in Winoground [55].\nTraining late multimodal layers from scratch help, CLIP+MM-Adapter performs better overall:\nWe improve performance by training multimodal layers from scratch on top of CLIP and FLAVA as\nshown in Table 2a, row e, f, j, k. Interestingly, as shown in Table 2a, linear probing or tuning the pre-\ntrained multimodal layers of FLAVA hurts performance (row g vs row h and i). This could be because\ntuning adversely perturbs the parameters trained on large-scale data. Finally, CLIP+MM-Adapter\n(row f) performs comparably well as tuning multimodal layers on FLAVA (row f vs j and k).\n6.3\nEffect of fine-tuning data\nDifference between free-form captions and templated Cola queries is minimal. Hence, Cola\ntemplated queries are useful: In Table 2b (row a vs c, d, e), we see that templated queries perform\n8\nFigure 3: Qualitative results of multi-object matching (left) and retrieving a single object with\nmultiple attributes (right).\nFigure 4: Qualitative results on cases where models struggle with multiple object-attribute composi-\ntionality (left). Cases where we see the most improvement and the least on single-object compositional\nretrieval are shown on the right.\njust as well as free-form region descriptions. This shows that Cola queries are still useful for\nattribute-object binding despite not being free-form.\nHaving multiple objects with multiple attributes in a caption helps: When we combine single\nobject captions with multiple object captions, we see in Table 2b that performance increases especially\non the multi-object Cola setting while maintaining performance on the single-object setting (row b vs\nc). The multi-object and single-object captions are formed on the same number of images. Combining\nall types of data along with hard negatives (row e) doesn\u2019t seem to affect performance much.\n6.4\nQuality of Cola Benchmark\nOur Cola mAPs are harder and are more difficult to improve on: We also computed the per-\nformance based on the standard formulation of mAP, where all images are included in the list to\n9\nretrieve from. In contrast, Cola mAP only includes hard distractors as defined in Sec. 3. We see that\ntrends remain the same with the standard mAP. However, our Cola mAP is harder to improve on. The\nstandard MAP (supplementary, Table 4) improves by more than 2x on GQA and 10x on CLEVR. In\ncontrast, we only improve by 1.09x on GQA and 2x on CLEVR with our harder Cola MAP.\nAmbiguous colors, spatial relationships, and size are some common themes where models\nunderperform in Cola benchmark: We analyze the types of compositional queries that models find\ndifficult on our benchmark. In the qualitative examples shown in Fig. 4, we see that compositions\ninvolving ambiguous colors (due to lighting) and spatial relationships are difficult for multiple-object\ncases. This is likely because spatial relationships are often inconsistently annotated in training data -\ne.g. \u201cto the left of\u201d can sometimes be from the viewer or image perspective. More examples are in\nthe supplemental- Fig. 8 (data ambiguity), 20, 21 (prediction inaccuracy).\nSignificant improvements on non-salient/occluded objects. Improvements on larger objects are\nminimal: On attribute-object compositional retrieval for single objects, we see the most improvements\nusing our adaptation method are on non-salient or occluded objects (like a small sign), as shown in\nFig. 4 (right). Queries that are commonly represented in training sets (like \u201cclear blue sky\u201d - skies\nare most commonly blue) have minimal improvements from pre-trained models.\n7\nDiscussion and Limitations\nThis work finetunes vision-language models to test design choices for compositional attribute-object\nretrieval. Thus, compared to the original pre-trained models [43], we may lose some other generic\ncapabilities, such as question answering, captioning, etc. For example, our best adaptation scores\n83% zero-shot on CIFAR10 [30], as compared to 87% for pre-trained CLIP. However, by just\nusing 5% of CIFAR training data (less than 1 epoch), we reach 92% accuracy while maintaining\nperformance on our Cola task. We would also like to stress that our goal is not to train a new\nfoundation model but to explore design choices that allow for compositional reasoning. While we\nfocus on attribute-object compositionality, there is still significant room for exploration of other\ntypes of compositional structures such as relationships, scene graphs, and counting [15, 12]. A great\navenue for future work could be collecting more detailed annotations (about the type of objects and\ncompositions in the queries) on larger sets of images to help pinpoint themes where models are failing.\nFurther testing is also required to see how it fares with sensitive attributes and objects\u2014 whether it is\npredisposed towards attaching incorrect attributes to objects because of racial/political biases in the\ndata [6]. Additionally, it is important to re-evaluate our results in the context of newer vision-language\nmodels that are being proposed. Finally, we would like to point out that our finetuning strategy isn\u2019t\nspecific to compositions, suggesting its potential applicability for adaptation to other downstream\ntasks, especially since similar strategies and modules have been used for other computer vision\ntasks [53, 52, 26].\n8\nConclusion\nWe present a new task, Cola, to test the compositional attribute-object binding of vision-language\nmodels. This is important for various practical applications like an assistive agent requiring an\nunderstanding of fine-grained differences between objects in cluttered workplaces. We explore the\narchitectural choices of adapting large vision-language models that encourage such reasoning. We\nshow that a light-weight multimodal adaptor can improve this capability in a pre-trained vision-\nlanguage model as a strong baseline for further research. We hope that Cola serves as a strong\nbenchmark and our adaptation choices as strong baselines for improving compositional vision-\nlanguage intelligence.\nAcknowledgements. We wish to thank Dhruv Mahajan and Jang Hyun (Vincent) Cho for their\nvaluable guidance in the initial phases of the project. We also wish to thank the anonymous reviewers\nfor their thoughtful comments and suggestions. This material is based upon work supported, in part,\nby DARPA under agreement number HR00112020054 awarded to Kate Saenko and Bryan Plummer\nat BU. Any opinions, findings, and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of the supporting agencies.\n10\nReferences\n[1] Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. C-VQA: A Compositional Split\nof the Visual Question Answering (VQA) v1.0 Dataset, Apr. 2017. arXiv:1704.08243 [cs]. 3\n[2] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes,\nNov. 2018. arXiv:1610.01644 [cs, stat]. 3\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: a Visual Language Model for Few-Shot Learning,\nNov. 2022. arXiv:2204.14198 [cs]. 3\n[4] Amit Alfassy, Assaf Arbelle, Oshri Halimi, Sivan Harary, Roei Herzig, Eli Schwartz, Rameswar Panda,\nMichele Dolfi, Christoph Auer, Kate Saenko, PeterW J. Staar, Rogerio Feris, and Leonid Karlinsky. FETA:\nTowards Specializing Foundation Models for Expert Task Applications, Dec. 2022. arXiv:2209.03648 [cs].\n3\n[5] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring Visual Prompts for\nAdapting Large-Scale Models, June 2022. arXiv:2203.17274 [cs]. 3\n[6] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial\ngender classification. In Conference on fairness, accountability and transparency, pages 77\u201391. PMLR,\n2018. 10\n[7] Noam Chomsky. Aspects of the Theory of Syntax. The MIT Press, 50 edition, 1965. 3\n[8] Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochers-\nberger, and Dhruv Batra. Resolving Language and Vision Ambiguities Together: Joint Segmentation &\nPrepositional Attachment Resolution in Captioned Scenes, Sept. 2016. arXiv:1604.02125 [cs]. 3\n[9] MJ Cresswell. Logics and languages. 1973. 1\n[10] M. J. Cresswell. Logics and Languages. Synthese, 40(2):375\u2013387, 1973. Publisher: Springer. 3\n[11] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu,\nCe Liu, Yann LeCun, Nanyun Peng, Jianfeng Gao, and Lijuan Wang. Coarse-to-Fine Vision-Language\nPre-training with Fusion in the Backbone, Nov. 2022. arXiv:2206.07643 [cs]. 3, 5, 6, 18\n[12] Mona Gandhi, Mustafa Omer Gul, Eva Prakash, Madeleine Grunde-McLaughlin, Ranjay Krishna, and\nManeesh Agrawala. Measuring compositional consistency for video question answering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5046\u20135055, 2022. 10\n[13] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu\nQiao. CLIP-Adapter: Better Vision-Language Models with Feature Adapters, Oct. 2021. arXiv:2110.04544\n[cs]. 3\n[14] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal\nDaum\u00e9 III au2, and Kate Crawford. Datasheets for datasets, 2021. 14\n[15] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark for\ncompositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11287\u201311297, 2021. 10\n[16] Sheng Guo, Weilin Huang, Xiao Zhang, Prasanna Srikhanta, Yin Cui, Yuan Li, Hartwig Adam, Matthew R.\nScott, and Serge Belongie. The iMaterialist Fashion Attribute Dataset. In 2019 IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW), pages 3113\u20133116, Seoul, Korea (South), Oct. 2019.\nIEEE. 3\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised\nVisual Representation Learning, Mar. 2020. arXiv:1911.05722 [cs]. 3\n[18] Drew A. Hudson and Christopher D. Manning. GQA: A New Dataset for Real-World Visual Reasoning\nand Compositional Question Answering, May 2019. arXiv:1902.09506 [cs]. 2, 4, 6, 7, 15, 17, 18, 20\n[19] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Discovering states and transformations in image\ncollections. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages\n1383\u20131391, Boston, MA, USA, June 2015. IEEE. 3\n[20] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions\nof spatio-temporal scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10236\u201310247, 2020. 1\n[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021. 2\n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy\nText Supervision, June 2021. arXiv:2102.05918 [cs]. 3\n[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. Visual Prompt Tuning, July 2022. arXiv:2203.12119 [cs]. 3\n[24] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross\nGirshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,\nDec. 2016. arXiv:1612.06890 [cs]. 2, 4, 6, 8, 15, 17, 18\n11\n[25] Aishwarya Kamath, Sara Price, Jonas Pfeiffer, Yann LeCun, and Nicolas Carion. TRICD: Testing Robust\nImage Understanding Through Contextual Phrase Detection. 2023. 1, 3\n[26] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\nMDETR \u2013 Modulated Detection for End-to-End Multi-Modal Understanding, Oct. 2021. arXiv:2104.12763\n[cs]. 2, 3, 5, 10, 18, 19\n[27] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. DiffusionCLIP: Text-Guided Diffusion Models for\nRobust Image Manipulation, Aug. 2022. arXiv:2110.02711 [cs]. 3\n[28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual Genome:\nConnecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal\nof Computer Vision, 123(1):32\u201373, May 2017. 1\n[29] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual Genome: Con-\nnecting Language and Vision Using Crowdsourced Dense Image Annotations, Feb. 2016. arXiv:1602.07332\n[cs]. 4, 6, 7, 15\n[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 10\n[31] Michael A Lepori, Thomas Serre, and Ellie Pavlick. Break it down: Evidence for structural compositionality\nin neural networks. arXiv preprint arXiv:2301.10884, 2023. 3\n[32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt\nTuning, Sept. 2021. arXiv:2104.08691 [cs]. 3\n[33] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven\nHoi. Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, Oct.\n2021. arXiv:2107.07651 [cs]. 3, 5, 6, 18\n[34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded Language-\nImage Pre-training, June 2022. arXiv:2112.03857 [cs]. 3\n[35] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. CREPE: Can\nVision-Language Foundation Models Reason Compositionally?, Jan. 2023. arXiv:2212.07796 [cs]. 1, 2, 3,\n4, 6, 8\n[36] Oscar Ma\u00f1as, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal.\nMAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot\nPrompting, Oct. 2022. arXiv:2210.07179 [cs]. 3\n[37] Nihal V. Nayak, Peilin Yu, and Stephen H. Bach. Learning to Compose Soft Prompts for Compositional\nZero-Shot Learning, Sept. 2022. arXiv:2204.03574 [cs]. 3\n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang,\nZach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, Dec.\n2019. arXiv:1912.01703 [cs, stat]. 6\n[39] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba.\nVirtualHome: Simulating Household Activities Via Programs. In 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8494\u20138502, Salt Lake City, UT, June 2018. IEEE. 3\n[40] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash\nPatel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, Distillation, and Hard Negatives for\nVision-Language Pre-Training, Jan. 2023. arXiv:2301.02280 [cs]. 3\n[41] Filip Radenovic, Animesh Sinha, Albert Gordo, Tamara Berg, and Dhruv Mahajan. Large-Scale Attribute-\nObject Compositions, May 2021. arXiv:2105.11373 [cs]. 3\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR,\n2021. 2\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\nTransferable Visual Models From Natural Language Supervision, Feb. 2021. arXiv:2103.00020 [cs]. 2, 3,\n4, 5, 8, 10, 19, 20\n[44] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, and\nDhruv Mahajan. PACO: Parts and Attributes of Common Objects, Jan. 2023. arXiv:2301.01795 [cs]. 2, 4,\n6, 8, 14, 15, 17, 18\n[45] Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, and Devi Parikh. Question Relevance in VQA:\nIdentifying Non-Visual And False-Premise Questions, Sept. 2016. arXiv:1606.06622 [cs]. 3\n[46] Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas. Sunny and Dark Outside?! Im-\nproving Answer Consistency in VQA through Entailed Question Generation, Sept. 2019. arXiv:1909.04696\n[cs]. 3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-Resolution\nImage Synthesis with Latent Diffusion Models, Apr. 2022. arXiv:2112.10752 [cs]. 3\n12\n[48] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.\nPrefix Conditioning Unifies Language and Label Supervision, June 2022. arXiv:2206.01125 [cs]. 3\n[49] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas\nPfister. Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval, Feb. 2023.\narXiv:2302.03084 [cs]. 3\n[50] Madeline Chantry Schiappa, Michael Cogswell, Ajay Divakaran, and Yogesh Singh Rawat. Probing\nconceptual understanding of large visual-language models, 2023. 3\n[51] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-Consistency for Robust Visual\nQuestion Answering. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npages 6642\u20136651, Long Beach, CA, USA, June 2019. IEEE. 3\n[52] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. FLAVA: A Foundational Language And Vision Alignment Model, Mar. 2022.\narXiv:2112.04482 [cs]. 2, 3, 4, 5, 6, 7, 8, 10, 18, 19\n[53] Hao Tan and Mohit Bansal. LXMERT: Learning Cross-Modality Encoder Representations from Trans-\nformers, Dec. 2019. arXiv:1908.07490 [cs]. 2, 5, 10\n[54] Reuben Tan, Arijit Ray, Andrea Burns, Bryan A. Plummer, Justin Salamon, Oriol Nieto, Bryan Russell,\nand Kate Saenko. Language-guided audio-visual source separation via trimodal consistency, 2023. 3\n[55] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace\nRoss. Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality, Apr.\n2022. arXiv:2204.03162 [cs]. 1, 2, 3, 8\n[56] Ben Usman, Dina Bashkirova, and Kate Saenko. Disentangled Unsupervised Image Translation via\nRestricted Information Flow, Nov. 2021. arXiv:2111.13279 [cs]. 3\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need, Dec. 2017. arXiv:1706.03762 [cs]. 18\n[58] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD\nBirds-200-2011 Dataset, July 2011. Issue: 2010-001 Num Pages: 8 Number: 2010-001 Place: Pasadena,\nCA Publisher: California Institute of Technology. 3\n[59] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. Transformers: State-of-the-Art Natural Language Processing. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npages 38\u201345, Online, Oct. 2020. Association for Computational Linguistics. 6\n[60] Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata. Zero-Shot Learning \u2013 A Compre-\nhensive Evaluation of the Good, the Bad and the Ugly, Sept. 2020. arXiv:1707.00600 [cs]. 3\n[61] Aron Yu and Kristen Grauman. Fine-Grained Visual Comparisons with Local Learning. In 2014 IEEE\nConference on Computer Vision and Pattern Recognition, pages 192\u2013199, Columbus, OH, USA, June\n2014. IEEE. 3\n[62] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why\nvision-language models behave like bags-of-words, and what to do about it?, Oct. 2022. arXiv:2210.01936\n[cs]. 3\n[63] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan\nWang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. GLIPv2: Unifying Localization and Vision-\nLanguage Understanding, Oct. 2022. arXiv:2206.05836 [cs]. 3\n[64] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5579\u20135588, June 2021.\n2\n[65] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to Prompt for Vision-Language\nModels. International Journal of Computer Vision, 130(9):2337\u20132348, Sept. 2022. arXiv:2109.01134 [cs].\n3, 5\n9\nAdditional Data Details\nOur data is publicly available at https://github.com/arijitray1993/COLA. Recall that\nour data is built on top of four publicly available datasets. We provide some more details on how we\ncurated our data.\nCola Single Object Data\nGQA GQA has annotations of objects and attributes in images. We use this to construct queries\nlike \u201csquare white plate\u201d. We ignore bounding boxes. For test, we filter the images with at least 2\nattributes per object annotation in their test split. We are left with 1952 images and 320 queries on\nour test set. To create a challenging set where queries are unseen, we take 150 attribute object tuples\nfrom the 320 queries from the test set that are seen the least in the training set and remove those\n13\nimages and queries from the training set completely. This way, we end up with 150 unseen queries\nwith the least impact on the training set size. We report all numbers on this test set of 150 unseen and\n170 seen queries. We train on the GQA train split (with the test unseen queries and corresponding\nimages removed). Hence, we have around 67K training images and 27K queries. The number of\npaired examples is 450K image-text pairs.\nCLEVR On CLEVR, we test on 96 classes on 22,500 images. We use their compositional splits. We\ntrain on condition A as described in their paper and dataset website and test on condition B. In these\ntwo splits, cubes and cylinders have unseen color and size compositions. However, for spheres, all\ncolors and shapes are seen. Since MAP is sensitive to the number of classes, we keep the number\nof classes for seen and unseen the same. Hence, we leave out the spheres when reporting seen vs\nunseen. However, for all MAP, we report including spheres. Hence, we have 32 unseen classes, 32\nseen classes and 96 classes for \u201call\u201d. For training, we have 168 possible queries (with colors swapped\nfor cubes and cylinders from those in the test set) on 70K images.\nPACO The PACO [44] dataset has 55 attributes annotated on 75 object categories on 9443 images on\nthe test set. Since all combinations of objects and attributes would result in an intractable amount of\npossible compositions, we sample the 400 occurring multiple attribute-object compositions in the test\nset. The 400 classes are sampled by sampling the top 200 seen attribute-object queries and the top\n200 unseen attribute-object queries. An attribute-object query is defined as unseen if the attributes in\nconjunction with that object were never a subset of the attributes in conjunction with that object in\nthe training data. This way, we have 400 classes on 7921 images, on which we report numbers. We\nhave 37K training images and 18K queries and 55K paired image-text examples.\nCola Multi-Obj Data The multi-obj data was created on the train and test splits according to the\nCola Single-Object GQA data splits. Only the test split is cleaned using human annotations. We\nshow some qualitative examples of the human-cleaned test set in Figure 7. We also see that some\ncases remain ambiguous even after human cleaning, as shown in Figure 8. These often involve\ndifferences in perception of size (large vs small) and color (blue vs white under different lighting\nconditions). However, despite some minimal noise, the human accuracy of 10 independent workers\non our validation set is 84%. This is opposed to our best model with an accuracy of 45%. Hence, we\nbelieve there is significant room for improvement.\nDatasheets for Datasets Answers\nWe believe that the majority of the questions in the datasheets paper [14] have already been answered\nin the main paper. Here, we provide some additional answers. We also provide a histogram of the\ntypes of objects and attributes in our data in Figure 6\nDataset funding agency This project was supported by DARPA Semafor awarded to KS and BP.\nThe findings and results reported in the paper are not the opinions of the US Government or the\nDepartment of Defense.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? The dataset is a curated sample from larger datasets specifically aimed\nto test attribute-object compositionality of models.\nWhat data does each instance consist of? Raw images, text captions, text-based scene graphs of\nobjects and attributes in the image. Note that some objects and attribute annotations may be missing.\nIs any information missing from individual instances? Yes, since it is very diffiucult to exhaustively\nannotate all possible objects and attributes, it is possible that some annotations are missing.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? Since the data is built on top of publicly available datasets, some\nof the annotations, like scene graphs, are linked to the external dataset.\nDoes the dataset contain data that might be considered confidential Not that we are aware of\nsince we use publicly available data from a published dataset.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? Not that we are aware of.\n14\nFigure 5: The MAP numbers by the number of attributes in the query on the CLEVR dataset. Note\nhow MM-Adapter performs well even as the number of attributes is gradually increased.\nDoes the dataset identify any subpopulations (e.g., by age, gender)? No personally identifiable\ninformation is present in the data. We also do not conduct any analyses with sensitive attributes like\nrace, age, sexual orientation, or religion. Some attributes like sex and hair color may be annotated in\nthe images, but we don\u2019t explicitly analyze them since that is not the focus of the benchmark or the\npaper.\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or\nindirectly (i.e., in combination with other data) from the dataset? No personally identifiable\ninformation is present in the data. It may be purely coincidental that a person in real life may be\npresent in the images of the data.\nDoes the dataset contain data that might be considered sensitive in any way? Not that we are\naware of.\nOver what timeframe was the data collected? Visual Genome [29] was collected in 2016. GQA\n[18] in 2018. PACO [44] in 2022-23. CLEVR [24] in 2016.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and\nhow were they compensated? Students ran the data collection and crowdworkers annotated the data.\nWe do not know how much they were compensated for the datasets we build on top of. However, for\nour human cleaned multi-object Cola test set, we paid crowdworkers an average of 15 USD per hour\nwith bonuses if they annotated examples correctly.\nWere any ethical review processes conducted? Yes, we were exempted by IRB since the data didn\u2019t\ninvolve any personal or sensitive information.\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\nprotection impact analysis) been conducted? No, but this could be interesting future work.\nIs the software that was used to preprocess/clean/label the data available? Yes, we shall release\nthe code used to curate the data.\nWill the dataset be distributed under a copyright, what are the IP restrictions, and export\ncontrol restrictions? No such restrictions are present. The licenses are the same as the licenses of\nthe datasets our benchmark is built on.\nWill the dataset be updated? We may update the data with more examples or more annotations\nperiodically.\n15\nFigure 6: Overview of the types of attributes and objects in our data. They correspond to practical\nobjects in daily life.\n10\nMore metrics and analysis\nRecall, that our goal is to adapt vision-language features to improve the compositional binding of\nattributes to objects. Specifically, we aim to improve the classification of a query involving single\nor multiple objects with multiple attributes in an image. Hence, we perform some analysis to see\nhow our performance is affected by increasing the number of attributes. Recall, that we also report\nnumbers on our Cola MAP, which evaluates the model\u2019s capability to rank the images with the correct\nattachment of attributes to the desired object from hard distractors with incorrect attachments of\nattributes to objects. We also show results some other choices of MAP and the standard MAP used\ncommonly. We show how our choice of MAP in the main paper is harder even though all trends\nremain the same with all choices of MAP. Finally, we also show performance from other choices of\ndoing multimodal fusion for our MM-Adapter and MM-pred approaches, showing that this adaptation\nstrategy holds for various other choices as well.\nPerformance by number of attributes We vary the number of attributes in the query for a single\nobject setting and check performance with increasing attributes. The results are shown in Figure 5.\nWe see that the baseline CLIP and finetuning has higher performance on single-attribute queries than\nmulti-attribute queries. We show that our MM-Adapter maintains improved performance on both the\nsingle-attribute and multi-attribute cases.\nOther evaluation metrics\nQueryAll MAP Recall that in the main paper, we compute MAP among hard distractors for the\nCola single-object setting. The hard distractors are images that have any of the attributes and object\nwords in the query. The model needs to rank the images with the correct attachment of the attributes\nto the desired object (as opposed to simply existing somewhere) to achieve a higher MAP. Here,\nwe design another similar hard MAP. Here, we restrict the list of images in the pool to have all the\nquery attributes and objects. Hence, for a query \u201ccyan metal cylinder\u201d, we rank among images that\nhave \u201ccylinders\u201d AND \u201ccyan\u201d objects AND \u201cmetal\u201d objects. In the main paper, the MAP uses an\nOR instead of an AND operation. The results are shown in Table 3 and we observe that all trends\nremain the same with this metric as presented in the main paper. However, we observe that that this\nmetric can only be applied to CLEVR since annotations are exhaustive. In real datasets like GQA,\nthe number of such annotated hard distractors is limited; hence, we do an OR operation to keep a\nhigh number of images to rank from. When applied to a dataset like GQA, the trends are the same,\nbut the numbers are spuriously high since there are very few distractor images.\nMean Rank of GT (\u2713) vs distractors (\u02c6) Based on the hard distractors we made for the QueryAll\nMAP above, we also report the mean rank of the images with the correct attachment of attributes to\nthe object versus the mean rank of the images with the wrong attachment of attributes. The results\nare also shown in Table 3. We observe that all trends remain the same, so report only one of them in\nthe main paper.\nStandard MAP In contrast to our hard Cola MAP\u2019s, we also compute the MAP on all images in\nthe validation set regardless of hard or easy distractors. Once again, we see all trends remain the\nsame as shown in Table 4. However, we note that the MAP numbers on all images are much lower.\nThis is becuase of two reasons - a) the number of images to rank from is higher, and b) datasets\nlike GQA have missing annotations, hence there are many images that get denoted as a negative\nretrieval becuase of a missing annotation. When we restrict the images to at least have one of the\nquery words, this noise reduces somewhat. However, note how it is easier to improve on this overall\nMAP than on the harder Cola MAP reported in the main paper. This shows models can quickly\n16\nQueryAll MAP \u00d2\nMean Rank\nAll\nSeen\nUnseen\n\u2713 \u00d3\n\u02c6 \u00d2\nCLIP+linear\n49.56\n48.21\n31.78\n21.78\n54.17\nprompt-tune\n31.07\n29.34\n29.43\n31.37\n52.72\nFT-all\n54.58\n52.05\n21.27\n21.06\n54.32\nFT-late\n66.16\n70.15\n30.99\n14.03\n55.35\nMM-Pred (our)\n85.51\n77.85\n81.18\n9.69\n55.99\nMM-Adapter (our)\n90.35\n81.95\n90.53\n8.85\n56.12\nTable 3: Two other choices for a hard metric computed on the CLEVR [24] dataset.\nOverall MAP\nAll\nUnseen\nSeen\nGQA [18]\nCLIP\n0.65\n0.35\n0.91\n+ prompt-tune\n8.72\n9.63\n7.91\n+ Linear probe\n12.81\n13.29\n12.52\n+ FT all\n11.47\n10.87\n11.96\n+ FT late\n13.39\n13.70\n13.10\n+ MM-Pred (our)\n16.76\n17.45\n16.15\n+ MM-Adapter (our)\n17.40\n16.79\n17.95\nFLAVA\n7.33\n6.43\n8.15\n+ FT-late\n9.71\n9.49\n9.90\n+ MM-Pred (our)\n17.68\n19.24\n16.29\n+ MM-Adapter (our)\n20.03\n20.70\n19.42\nCLEVR [24]\nCLIP\n6.42\n6.36\n6.29\n+ prompt-tune\n29.42\n23.02\n27.79\n+ Linear probe\n47.83\n29.33\n46.54\n+ FT all\n51.99\n18.40\n47.63\n+ FT late\n63.93\n27.20\n67.00\n+ MM-Pred (our)\n83.40\n76.82\n76.10\n+ MM-Adapter (our)\n88.15\n89.40\n76.90\nFLAVA + linear\n18.76\n16.77\n17.82\n+ FT-late\n77.59\n71.91\n66.25\n+ MM-Pred (our)\n90.41\n85.74\n86.05\n+ MM-Adapter (our)\n91.08\n86.60\n87.39\nPACO [44]\nCLIP\n0.71\n0.11\n1.31\n+ prompt-tune\n6.19\n2.78\n9.61\n+ Linear probe\n8.22\n3.83\n12.61\n+ FT all\n7.19\n3.00\n11.38\n+ FT late\n9.29\n5.37\n13.21\n+ MM-Pred (our)\n9.63\n4.00\n15.26\n+ MM-Adapter (our)\n10.00\n6.50\n15.22\nFLAVA + linear\n3.45\n1.73\n5.17\n+ FT-late\n6.31\n2.00\n10.60\n+ MM-Pred (our)\n10.77\n4.77\n16.76\n+ MM-Adapter (our)\n12.02\n6.36\n17.67\nTable 4: Standard MAP on all images with multiple attributes on objects annotated in the test set (not\njust hard distractors like our Cola MAP). Note how we can improve significantly (eg, 0.65 to 17.40\non the GQA split - 10x), but by a much lesser fraction on our Cola MAP which is only among hard\ndistractors.\n17\nCola Single-Obj MAP\nParams\nAll\nUnseen\nSeen\nGQA [18]\nUnimodal\n13M\n42.19\n44.61\n40.01\nFLAVA\n9.9M\n47.43\n48.95\n46.05\nALBEF\n10.9M\n45.2\n48.23\n42.6\nMDETR\n7.8M\n46.83\n48.86\n44.99\nFIBER\n15M\n47.08\n50.01\n44.43\nFIBER-MM\n12M\n46.05\n48.91\n43.47\nCLEVR [24]\nUnimodal\n13M\n64.05\n27.53\n67.48\nFLAVA\n9.9M\n88.21\n89.52\n77\nALBEF\n10.9M\n85.56\n85.47\n72.97\nMDETR\n7.8M\n89.35\n89.4\n80.2\nFIBER\n15M\n82.9\n76.97\n73.5\nFIBER-MM\n12M\n86.6\n88.56\n72.98\nPACO [44]\nUnimodal\n13M\n15.66\n8.74\n22.58\nFLAVA\n9.9M\n18.56\n11.47\n25.66\nALBEF\n10.9M\n18.22\n10.57\n25.8\nMDETR\n7.8M\n19\n11.13\n26.87\nFIBER\n15M\n12.34\n5.34\n19.35\nFIBER-MM\n12M\n11.83\n4.49\n19.17\nTable 5: Different choices for multimodal fusion inspired from ways researchers have done mul-\ntimodal fusion in literature. Note that these are not numbers from the models proposed in their\npapers, but the accuracy of using the style of multimodal fusion, which we use on top of frozen CLIP\nfeatures. Most multimodal variants perform better than tuning similar or more number of parameters\non unimodal attention layers. The main paper numbers are from the MDETR-style multimodal fusion.\nimprove on distinguishing coarse-grained differences but differentiating between the fine-grained\nones (as evaluated by the Cola MAP) is harder.\nF1 Score We also ran a sample of the evaluation using F1 on the GQA split of COLA single-objects,\nand we see that all trends remain the same when comparing F1. For instance, F1 of CLIP baseline\nis 0.28, whereas FT-all is 0.31, FT-Late is 0.33, and our MM-Adapt is 0.39, MM-Pred is 0.40.\nOur conclusion stays the same: adapting the multimodal attention layers is better than tuning the\nsplit-modal attention layers (FT-Late), fine-tuning the entire model, or linear probing.\n10.1\nOther choices of multimodal fusion\nIn our MM-Adapter and MM-Pred approaches, we use multimodal fusion. There are various ways\nto do multimodal fusion. Some of the salient choices are inspired by FLAVA [52], ALBEF [33],\nMDETR [26], and FIBER [11]. We describe some of the ways we try multimodal fusion:\n\u2013 FLAVA-inspired - self-attention on a [CLS] token concatenated with image patch and text tokens-\nHere, we take the image patch features and text token features and employ self-attention transformer\n[57] on the concatenated image, text and [CLS] tokens.\n\u2013 MDETR-inspired - self-attention over image patch and text tokens and then, a [CLS] token\ncross attending to the image and text tokens- In the MDETR [26] paper, they use self-attention\nover image and text features and then multiple task tokens that cross attend to the self-attended\nimage-text features for various tasks. Since, we have only one task here, which is retrieval, we use\none [CLS]. We have also experimented with using multiple (100) [CLS] tokens to see if they\nlearn different things. We observe that all the [CLS] tokens learn the same thing with minimal\nperformance gap. This is the choice of multimodal fusion that we report in the paper for both our\nMM-Adapter and MM-Pred approaches.\n\u2013 ALBEF-inspired - text cross-attends to image - Here, first we have separate unimodal self-attention\nlayers on the image patch and text token features. Then, the text token features cross-attend to the\nimage patch features along with a [CLS] token. The [CLS] output is then used for MM-Pred\n(prediction using fully-connected layer) or MM-Adapter (cosine similarity to frozen text features).\n18\n\u2013 FIBER-inspired - text cross-attends to image and vice versa- Here, first we have separate unimodal\nself-attention layers on the image patch and text token features. Then, have text token features\ncross-attend to the image patch features along with a [CLS] token. We also have the image patch\nfeatures cross-attend to text token features along with another [CLS] token. We finally measure\nthe cosine similarity of the two [CLS] tokens.\n\u2013 FIBER-MM - In the above FIBER and ALBEF style fusion, we used separate unimodal self-\nattention layers on the image patch and text token features before the cross attention. Here, we\ndesign a modification, we use a multimodal self-attention on the image patch and text tokens first,\nlike FLAVA. Then, we do cross-attention like FIBER as described above.\nAccuracies on GQA, CLEVR and PACO for Cola single-object case on the above-described mul-\ntimodal choices are shown in Table 5. We see similar trends as the choice of multimodal attention\nreported in the paper. All the methods of doing multimodal fusion work better than unimodal fusion.\nAlso, while some choices work better than others, note how using the multimodal layers as a feature\nadaptor (MM-Adapter) works better than using it as a prediction head (MM-Pred) for all design\nchoices.\n11\nImplementation details\nNow, we present more implementation details of the models and adaptation strategies used in the\nmain paper. We also provide more details on the datasets used.\n11.1\nModel architecture details\nRecall that we have use a CLIP [43] image and text encoder to extract image and text region features.\nHere are some additional details for each of the choices of adaption we tried:\n\u2013 Linear: We train a linear probe on the pre-trained representations. We train a separate linear\nlayer on top of the image and text pooled features for CLIP [43]. Each linear layer transforms the\n512-dimensional image and text representation to another 512-dimensional embeddings. Finally,\nwe compute the cosine similarity between the two transformed embeddings.\n\u2013 Prompt-tune: We tune the embedding layer of the text words used in our training queries while\nkeeping everything else frozen.\n\u2013 FT all: We fine-tune the whole model. This involves tuning 151M parameters in the case of CLIP.\n\u2013 FT Late: We take the second-last layer features from the image and text encoders of CLIP. There\nare 49 image patch features and K text token features (K depends on the input query length, but it\nis capped to 77). We train a separate transformer encoder layer on the 49 image patch embeddings\nand the K text tokens. The transformer encoder has 2 transformer encoder self-attention layers with\n4 heads each. We tried variations of 1 layer, 2 layers and 3 layers and report the best performance.\nThis design is chosen to be the most similar in the number of parameters and approach to our\nmultimodal adaptation approach to be a strong baseline.\n\u2013 MM-Pred: Here, we use multimodal attention as a prediction head like common multimodal\nmodels [52, 26], but train it on the frozen CLIP [43] base image and text encoders. Once again,\nthe multimodal transformer encoder has 2 layers with 4 heads each. We predict a score using\na fully-connected layer on the [CLS] token output of the multimodal attention that maps the\n512-dimension embedding to a 1-dimensional score.\n\u2013 MM-Adapter: This differs from our MM-Adapter approach, where we use multimodal attention\nto adapt the image representation and use their cosine similarity to the text features.\nFor the image-text-matching loss, we get a score for each image-text pair in a batch. For each score,\nwe compute the binary sigmoidal cross entropy and take the average in the batch. We use a sigmoidal\ncross entropy since for each image, there can be multiple text queries that are true and vice versa. We\ntrain using a learning rate of 0.00001 and a weight decay of 0.0001 for the models on top of CLIP.\nFor adaptations on top of FLAVA, we see that we need a higher learning rate to converge quicker,\nhence, we use a learning rate of 0.001 and a weight decay of 0.0001.\n19\nFigure 7: Some examples from the Cola multi-obj setting.\n12\nQualitative results\nSingle-object case Figures 9, 10, 11, 12 show examples of top 5 retrievals based on common\nadaptation methods and our MM-Adapter method on the Cola single-object setting on the GQA [18]\ndataset. Each row in the image is a different adaptation method (based on the methods shown in Table\n1 in the main paper). Note how we improve on multiple attributes attached to non-salient and small\nobjects. Figures 13, 14, 15 show some cases where we see marginal improvements from off-the-shelf\nCLIP or simpler adaptation techniques like fine-tuning or linear probing. We observe that marginal\nimprovements are mostly on queries with large areas of the image like sky and water. The existing\nCLIP [43] model is fairly good at such large salient objects, especially when paired with common\nattributes like \u201cgreen\u201d for the object \u201cleaf\u201d.\nMulti-object case Figures 16, 17, 18, and 19 show some results on the Cola multi-object setting.\nSimilar to the observations in the single-object setting, we improve the attribute-object binding capa-\nbility even when the objects are non-salient in the image. In addition to relational compositionality,\nas shown in Figures 20 and 21, our method also fails to understand fine differences in the relative\nstrength of attributes and when objects are occluded to a high degree.\nAll images we use are from publicly available datasets, and we are unaware of any correspondences\nwith identifiable humans in real life.\n20\nFigure 8: Some examples from the Cola multi-obj setting that are somewhat ambiguous even after\nhuman cleaning. Note how these mostly have to do with color (which can look different under\ndifferent lighting) and size (which is subjective).\n21\nFigure 9: Qualitative results on multiple attributes attached to an object. Note how we improve on\nmany attributes attached to small non-salient objects in the cluttered scene. The round white table\nin the test images were often small and hence, the original model had trouble finding them. Note\nhow the original CLIP only find the slaient black metal chair (first row), and in comparison, we find\nsmaller non-salient ones as well (last row)\n22\nFigure 10: Qualitative results on multiple attributes attached to an object. Note how we improve on\nmany attributes attached to small non-salient objects in the cluttered scene.\n23\nFigure 11: Qualitative results on multiple attributes attached to an object. Note how we improve on\nmany attributes attached to small non-salient objects in the cluttered scene.\n24\nFigure 12: Qualitative results on multiple attributes attached to an object. Note how we improve on\nmany attributes attached to small non-salient objects in the cluttered scene.\n25\nFigure 13: Queries with attributes that cover a wide area with common attributes, like blue sky,\nhave minimal improvements from off-the-shelf or simple adaptation strategies since existing models\nperform well on such queries already.\n26\nFigure 14: Queries with attributes that cover a wide area, like water bodies, have minimal improve-\nments from off-the-shelf or simple adaptation strategies since existing models perform well on such\nqueries already.\n27\nFigure 15: Queries with attributes that cover a wide area with common attributes, like a green large\nleaf, have minimal improvements from off-the-shelf or simple adaptation strategies since existing\nmodels perform well on such queries already.\nFigure 16: Qualitative results on multi-object cases. Once again, we see significant improvements on\ncompositions involving small non-salient objects such as a small sign.\n28\nFigure 17: Qualitative results on multi-object cases.\nFigure 18: Qualitative results on multi-object cases. We see significant improvements on compositions\nwhere the images have a lot of clutter and distractor objects - many things are white and brown in the\nscenes.\n29\nFigure 19: Qualitative results on multi-object cases.\n30\nFigure 20: Our method performs somewhat poorly on very fine-grained relative differences. In the\nexample above, a brown chair is underneath a brown desk, but the desk is not empty. In fact, even\nthe desk in the correct image for that caption is not technically empty, but it is more empty than the\ndistractor and our model fails to understand the relative difference.\n31\nFigure 21: Our method also performs poorly on occluded objects or when objects have some of the\nattributes of the distractor as well. In the example above, the doors are not clearly in view. In addition,\nthe brown door also has a white stripe, which further confuses the model.\n32\n"
  },
  {
    "title": "AttentionViz: A Global View of Transformer Attention",
    "link": "https://arxiv.org/pdf/2305.03210.pdf",
    "upvote": "1",
    "text": "AttentionViz: A Global View of Transformer Attention\nCatherine Yeh, Yida Chen, Aoyu Wu, Cynthia Chen, Fernanda Vi\u00e9gas, and Martin Wattenberg\nFig. 1: AttentionViz, our interactive visualization tool, allows users to explore transformer self-attention at scale by creating a joint\nembedding space for queries and keys. (a) In language transformers, these visualizations reveal striking visual traces that can be\nlinked to attention patterns. Each point in the scatterplot represents the query or key version of a word, as denoted by point color.\nUsers can explore individual attention heads (left) or zoom out for a \u201cglobal\u201d view of attention (right). (b) Our visualizations also\ndivulge interesting insights in vision transformers, such as attention heads that group image patches by hue and brightness. Border\ncolor denotes query embeddings of a patch (green) or key embeddings (pink). (c) Sample input sentences (from [23]) and (d) images\n(synthetic dataset) are provided for reference.\nAbstract\u2014Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we\npresent a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that\nallows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method\nis to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous\nattention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an\ninteractive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it\nto study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving\nmodel understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.\nIndex Terms\u2014Transformer, Attention, NLP, Computer Vision, Visual Analytics\n1\nINTRODUCTION\nThe transformer neural network architecture [52] is having a major im-\npact on fields ranging from natural language processing (NLP) [13,42]\nto computer vision [14]. Indeed, transformers are now deployed in\nlarge, real-world systems used by hundreds of millions of people (e.g.,\nStable Diffusion, ChatGPT, Microsoft Copilot). However, the mecha-\nnisms behind this success remain somewhat mysterious, especially as\nnew capabilities continue to emerge with increasing model complexi-\nties and sizes [11,60]. A deeper understanding of transformer models\ncould help us build more reliable systems, troubleshoot problems, and\nsuggest avenues for improvement.\n\u2022 Authors are with Harvard University. Vi\u00e9gas and Wattenberg are also with\nGoogle, but this work was done at Harvard. Emails: {catherineyeh,\nyidachen, aoyuwu, fernanda, wattenberg}@g.harvard.edu,\ncynthiachen@college.harvard.edu\nManuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication\nxx xxx. 201x; date of current version xx xxx. 201x. For information on\nobtaining reprints of this article, please send e-mail to: reprints@ieee.org.\nDigital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx\nIn this work, we describe a new visualization technique aimed at\nbetter comprehending how transformers operate. (We include a brief\nintroduction to transformers in Sec. 2.) The target of our analysis is\nthe characteristic transformer self-attention mechanism, which allows\nthese models to learn and use a rich set of relationships between input\nelements. Although attention patterns have been intensively studied,\nprevious techniques generally visualize information related to just a\nsingle input sequence (e.g., one sentence or image) at a time. Typ-\nical approaches create bipartite graph [51, 53] or heatmap [20, 30]\nrepresentations of attention weights for a given input sequence.\nOur method offers a higher-level perspective, in which we can view\nthe self-attention patterns of many input sequences at once. One inspi-\nration for this approach is the success of tools such as the Activation\nAtlas [5], which allows a researcher to \u201czoom out\u201d to see an overview\nof a neural network, then drill down for details. In our case, we seek\nto build a kind of \u201cattention atlas\u201d that can provide researchers with a\nrich and detailed view of how a transformer\u2019s various attention heads\noperate. The primary new technique is visualizing a joint embedding of\nthe query and key vectors used by transformers, which creates a visual\nsignature for an individual attention head.\nTo illustrate our technique, we implement AttentionViz, an inter-\narXiv:2305.03210v2  [cs.HC]  9 Aug 2023\nactive visualization tool that allows users to explore attention in both\nlanguage and vision transformers. AttentionViz affords exploration\nthrough multiple levels of detail (Fig. 1), providing both a global view\nto see all attention heads at once and the ability to zoom in on details\nin a single attention head or input sequence.\nWe demonstrate the utility of our technique through several appli-\ncation scenarios with AttentionViz and domain expert interviews. For\nconcreteness, we focus on what the visualization can reveal about a few\nwidely-used transformers: BERT [13], GPT-2 [41], and ViT [14]. We\nfind several identifiable \u201cvisual traces\u201d linked to attention patterns in\nBERT, detect novel hue/frequency behavior in ViT\u2019s visual attention\nmechanism, and uncover potentially anomalous behavior in GPT-2.\nUser feedback also supports the wider applicability of our approach in\nvisualizing other embeddings at scale.\nTo summarize, the contributions of this work include:\n\u2022 A visualization technique for exploring attention trends in trans-\nformer models based on joint query-key embeddings.\n\u2022 AttentionViz, an interactive tool that applies our technique for\nstudying self-attention in vision and language transformers at\nmultiple scales.\n\u2022 Application scenarios and expert feedback showing how Atten-\ntionViz can reveal insights about transformer attention patterns.\n2\nBACKGROUND ON TRANSFORMER MODELS\nThe transformer, introduced in [52], is a neural network architecture\ndesigned to operate on sequential input. A full description of transform-\ners is beyond the scope of this paper, but a few concepts are critical for\nunderstanding our work. First, a transformer receives as input a set of\nvectors (often called embeddings). Embeddings can represent a variety\nof input types. In text-based transformers, they correspond to words or\npieces of words; in vision transformers, they encode patches of pixels.\nThe network iteratively transforms these vectors via a series of\nattention layers, each of which moves information between pairs of\nembeddings. The name \u201cattention\u201d suggests that not all embeddings\nwill be equally related; certain pairs will interact more strongly\u2013i.e.,\npay more \u201cattention\u201d to each other. Attention layers determine which\npairs should interact, and what information should flow between them.\nFor example, in a transformer operating on the words of the sentence,\n\u201cThe brown capybara is sleeping now,\u201d one might expect high attention\n(and information flow) between embeddings for \u201ccapybara\u201d and \u201cis,\u201d\nbut not between \u201cbrown\u201d and \u201cnow.\u201d The self-attention mechanism,\nwhich is our focus in this paper, allows transformers to learn and use a\nrich set of relationships between elements of a sequence, yielding sig-\nnificant performance improvements across various NLP and computer\nvision tasks [13,14,41].\nThere may be different reasons for embedding pairs to attend to each\nother. For instance, in our example sentence,\u201cbrown\u201d and \u201ccapybara\u201d\nare linked by an adjective-noun relation, while \u201ccapybara\u201d and \u201cis\u201d form\na subject-verb relation. To allow for several relation types, transformer\nattention layers consist of multiple attention heads, each of which can\nrepresent a different pattern of attention and information flow.\nEach attention head computes its own attention pattern using a bi-\nlinear form computed from a query weight matrix WQ and key weight\nmatrix WK. Concretely, for two embedding vectors x and y, attention is\nrelated to the scaled inner product of a query vector, WQx, and a key\nvector, WKy. Letting d be the dimension of WKy, we have:\nf(x,y) = 1\n\u221a\nd\n\u27e8WQx,WKy\u27e9\nGiven embedding vectors {x1,x2,...,xn}, we compute the self-\nattention between xi and the other vectors using the softmax function:\nattn(xi,xj) = softmaxj( f(xi,x),..., f(xi,xn)) = ef(xi,x j)/\u2211\nk\nef(xi,xk)\nCritically, this formula shows that the greater the dot product between\nthe query and key vectors, the higher the final attention value will be, a\nfact that we rely upon in our joint embedding visualization.\nThere is much more to the transformer architecture than we cover\nhere. In particular, we have only described the attention weighting be-\ntween pairs of embeddings, and not the specific information that flows\nbetween them. (As discussed later, this is an area ripe for further inves-\ntigation.) One last technical point is worth mentioning, however, since\nit will help interpret images later in the paper. The initial embeddings\ngiven to a transformer typically incorporate a vector representation of\ntheir ordering (for a 1D sequence) or spatial configuration (for a grid,\nas in vision transformers). For sequences, these position vectors are\ndefined using trigonometric functions, and are located on a helix-like\ncurve in high-dimensional space (see [52]).\n2.1\nModels Studied in this Paper\nWe study three transformer models: BERT (language), GPT-2 (lan-\nguage), and ViT (vision). Each has been an important object of study\nin the machine learning community, and the three span a range of trans-\nformer architectures and applications. BERT, or Bidirectional Encoder\nRepresentations from Transformers [13], is a multi-layer transformer\nencoder. As a bidirectional model, BERT can attend to tokens (i.e.,\ninput elements) in either direction. GPT-2, or Generative Pre-trained\nTransformer 2 [42], is a multi-layer transformer decoder. GPT-2 is a\nunidirectional model, meaning it only attends to previous tokens. ViT,\nor Vision Transformer [14], employs a self-attention-based transformer\narchitecture by splitting images into \u201cpatches\u201d and treating them like\ntokens in a sentence. Similar to BERT, ViT is a multi-layer, bidirec-\ntional transformer encoder. In this work, we look at ViT performance\non 16x16 (ViT-16) and 32x32 (ViT-32) patch sizes.\n3\nRELATED WORK\nMany researchers have attempted to investigate the inner workings\nof transformers. [8,34] seek to understand the performance improve-\nment from transformer-based language models by exploring learned\nlinguistic representations, and [49] observed that BERT recapitulates\nclassic steps in natural language analysis, from part-of-speech tag-\nging to relation classification. Another popular approach is mech-\nanistic interpretability, i.e., reverse engineering transformer models\n(e.g., [15,16,37]). Attention, the backbone of transformers, has also\nbeen studied intensively. For example, attention appears to relate to\nsyntactic structures in NLP systems [9,57] and gestalt-like grouping\nin vision transformers [33]. Researchers have compared ViT\u2019s visual\nattention mechanism with convolutional filters as well, finding that\nattention is more robust against image occlusion, corruption, and high-\nfrequency noise [35,40]. In our discussion of related work, we focus\non visual approaches for studying transformer attention.\n3.1\nVisualizing Attention in a Single Input Sequence\nAttention patterns naturally lend themselves to visualization, in both\nlanguage and vision transformers [4,12,21,31,39]. These visualizations\nare largely focused on visualizing attention weights between query and\nkey tokens in a single input sequence using bipartite graphs (e.g., [30,\n48,51,53]) or heatmaps (e.g., [1,10,20,22,25,30,43]).\nA few visualizations have been proposed that allow comparison\nacross multiple models or layers. For instance, Attention Flows [12]\nsupports users in comparing attention within and across layers of BERT,\nas well as among attention heads given a single sentence. Dodrio [59]\nuses a grid view, applied to single inputs, that enables direct com-\nparison of attention heads. Another system, VisQA [21], visualizes\nattention at different heads for visual question-answering tasks by\nshowing heatmaps of language self-attention, vision self-attention, and\nlanguage-vision cross-attention. Even in these model-comparison sys-\ntems, however, an analyst must look at different inputs, one at a time,\nto identify and verify patterns for a given attention head.\n3.2\nBeyond Single Inputs: Visualizing Embeddings and\nActivation Maximization\nIt is natural to seek patterns that hold across multiple inputs. One\ntechnique that has proved effective toward this goal is visualizing\ncollections of embedding vectors from multiple input sequences\n[3, 18, 19, 46, 47, 58]. For example, [43] visualized BERT embed-\ndings for the same word used in many different contexts, and found\nclusters that corresponded to word senses. In an exploration of syn-\ntax processing, [8] visualized embeddings from a multilingual BERT\nmodel and once again found meaningful clusters that helped with in-\nterpretation. LMFingerprints [45] uses a tree-based radial layout to\ncompare embedding vectors across different language models.\nA second technique, used for vision transformers in [17,61], aims to\nfind images which maximize activations of particular units. Applied\nto embedding vectors, this technique produces clearly interpretable\nresults. The authors note, however, that when applied to query and key\nvectors the technique does not seem to produce useful results.\n3.3\nGaps in the Literature\nWe note three gaps in the existing literature which motivate our work.\nFirst, visualizing embedding vectors has been shown to be an ef-\nfective technique for analyzing patterns across multiple inputs, but we\nknow of no systematic attempt to visualize query and key embeddings\nin transformer models. [6] also argues that the intermediate artifacts\nof self-attention, such as queries and keys, are underexplored. These\nobservations motivate our joint query-key embedding technique.\nSecond, although visualization techniques have been proposed to\ncompare multiple embeddings (e.g., [2,3,26]), these methods are often\nlimited to a few embeddings and cannot address our needs of comparing\nembeddings at different transformer heads and layers. Thus, we design\na global matrix view to visualize query-key embeddings at scale.\nFinally, bipartite graph representations have proven helpful in an-\nalyzing NLP-based transformers, but we have not seen them applied\nto vision tasks. We explore this direction by creating bipartite-style\nvisualizations to study image attention patterns in ViT.\n4\nGOALS & TASKS\nThe overarching aim of this work is to design a novel visualization tech-\nnique that allows exploration of global attention trends in transformer\nmodels. To collect some initial feedback about this idea and learn more\nabout user needs, we talked with 5 machine learning (ML) researchers\n(4 Ph.D. students and 1 professor) interested in model interpretability.\nDuring these individual interviews, we asked experts to describe their\ncurrent practices and challenges when working with transformers and\nhow attention visualizations could aid in their research objectives. We\nwill refer to these experts as E1-5.\n4.1\nGoals\nUltimately, our conversations with experts yielded three main goals:\nG1 - Understand how self-attention informs model behavior.\nOverall, all 5 experts wanted to better understand the behavior of differ-\nent attention heads and what transformer models are learning through\ntheir characteristic self-attention mechanism. Thus, they expressed the\ndesire to be able to quickly and easily explore attention patterns. E2\nexplained that \u201cattention is still pretty closed-box and there\u2019s lots of\nmysteries,\u201d so gaining a deeper understanding of transformer attention\npatterns could provide insights into \u201cwhy large language models fail at\nreasoning tasks and math,\u201d for example.\nG2 - Compare & contrast attention heads. E5 mentioned that\nvisualizing differences in attention heads could help with hypothesis\ngeneration, which is the first step in their research process: \u201cVisualiza-\ntion can help formulate hypotheses to test and get an intuitive sense\nof what transformers are doing.\u201d Additionally, 3 experts (E1, E2, E5)\nnoted that attention head comparison would be useful for model prun-\ning and editing purposes. That is, if two attention heads appear to\nbehave similarly, perhaps one could be removed without significantly\nimpacting model performance. In the words of E1, comparing heads\nmight allow us to \u201cfind parts of the model that are actually useful.\u201d\nG3 - Identify attention anomalies. Four researchers (E2-5) wanted\nto identify irregularities and potential behavioral issues with transform-\ners through attention pattern exploration. This information could then\nbe used for model debugging purposes. For instance, E4 said \u201cvisualiz-\ning attention could help you notice when the model is looking at the\nwrong thing, even if the result is correct.\u201d E3 agreed, reiterating the\nFig. 2: Creating a joint query-key embedding space for a single atten-\ntion head. In the NLP case, given an input sentence, we first transform\neach token into its corresponding query and key vector. Then, we use t-\nSNE/UMAP/PCA to project these 1\u00d7d vectors into 2D/3D scatterplot\ncoordinates. For the BERT, GPT-2, and ViT models used, d = 64.\nimportance of debugging especially in the context of model training:\n\u201cTraining often fails and dies, but it\u2019s hard to understand why it fails or\nproduces unexpected behavior.\u201d\n4.2\nTasks\nGiven these goals, we developed the following set of design tasks:\nT1 - Visualize attention heads at scale. To help users quickly\nexplore model behavior [G1] and easily compare & contrast attention\npatterns [G2], our tool simultaneously visualizes self-attention heads\nacross transformer layers.\nT2 - Explore query-key interactions. E1 and E4 expressed the\ndesire to better understand query-key pairing information toward im-\nproving their understanding of transformer self-attention. Thus, our\ntool further supports attention pattern comparison [G2] and anomaly\ndetection [G3] through visualizing query-key interactions.\nT3 - Probe attention at multiple levels. Our tool allows for local\nand global comparisons of attention [G2] by providing visualizations at\nthe sentence/image, head, and model levels. The flexibility of switching\nbetween multiple views in a single interface also facilitates knowledge\ndiscovery [G1] and helps users identify model irregularities [G3].\nT4 - Customize model and data inputs. AttentionViz is easily\nextendable to new transformers and datasets, affording quick visual\ncomparison [G2] and synthesis of attention patterns [G1] across differ-\nent models and modalities (language & vision).\n5\nQUERY/KEY EMBEDDINGS & DESIGN OF ATTENTIONVIZ\nTo address these goals and tasks, we build a tool called AttentionViz.\nThe primary technique used by our tool is a visualization of the joint\nembedding of query and key vectors for each attention head. In this\nsection, we first describe the motivation and mathematics that underlie\nthis technique, then discuss the design of the full application.\n5.1\nVisualizing Query/Key Embeddings\nThe technique behind AttentionViz is relatively simple, although as we\ndescribe below, it requires two mathematical tricks to be effective. Re-\ncall that each transformer attention head transforms input embeddings\ninto query vectors and key vectors by applying matrices WQ and WK,\nrespectively (Sec. 2). These matrices project the original vector embed-\ndings to a lower dimensional space, essentially selecting a particular\ntype of information from the higher-dimensional vector embeddings.\nTherefore, by inspecting the query and key vectors, one might hope to\nlearn what information is selected by WQ and WK.\nA central observation is that the relative positions of query and key\nvectors can offer clues about how attention will be distributed, since\nattention coefficients depend on the dot product between queries and\nkeys. To see why, consider a hypothetical situation where query and\nkey vectors always have the same norm. Then, closer distances would\nFig. 3: Left: original queries and keys in joint embedding space. Right:\nIncreased overlap after translating keys to align query and key centroids.\ndirectly relate to higher attention coefficients. In practice, query and\nkey vectors vary in norm, so the relationship between dot product and\ndistance is not precise. However, as described in the following sections,\nwe can arrange for this relation to be surprisingly close.\nFig. 2 illustrates our technique with a synthetic example of a single\nattention head in a language transformer. To create the joint embedding,\nwe first obtain the query and key vector representation of each token in\na given sentence (Sec. 2). Then, we use one of three dimensionality-\nreduction methods to project these high-dimensional vectors onto a\nshared, lower-dimensional subspace: t-SNE [50], UMAP [32], or\nPCA [24]. The output from these dimensionality-reduction algorithms\nis a 2D/3D scatterplot, where each point represents a single query or\nkey token. The same process can be used to create joint embeddings for\nViT attention heads, where each token is an image patch. By default,\nwe visualize queries in green and keys in pink. However, there are\nmultiple color encodings users can choose from (see Sec. 5.2), and\nother palettes can be easily substituted into the system.\nOur joint embeddings allow users to explore the fine-grained interac-\ntions between queries and keys, and the shape of these plots can often\nserve as visual indicators of the underlying self-attention patterns (see\nSec. 7). Each dimensionality-reduction technique produces different\npatterns for a given dataset, offering unique insights and use cases.\n5.1.1\nVector Normalization\nWhile designing AttentionViz, we noticed two \u201cfree parameters,\u201d which\ncan be varied without losing any information. Tuning these parameters\ncreates a closer relationship between embedding distance and attention\nweights, and greatly improves the readability of the visualization. These\nnormalizations are applied prior to dimensionality reduction (Fig. 2).\nKey Translation: Query and key vectors are sometimes well sep-\narated in our visualizations (Fig. 3, left).\nThis separation makes\nit difficult to directly compare query and key embeddings.\nHow-\never, a simple mathematical trick allows us to move these embed-\ndings closer together, without affecting the self-attention computation\nfor any given input sequence. In particular, note that the softmax\nfunction is translation invariant: i.e., for any constant a, we have\nsoftmaxj(x1 + a,x2 + a,...) = softmax j(x1,x2,...). Now, consider a\nquery vector x and key vectors y1,...,yn. For any vector v, we have:\nattentionj(x) = softmax j(\u27e8x,y1\u27e9,\u27e8x,y2\u27e9,...)\n= softmax j(\u27e8x,y1\u27e9+\u27e8x,v\u27e9,\u27e8x,y2\u27e9+\u27e8x,v\u27e9,...)\n= softmax j(\u27e8x,y1 +v\u27e9,\u27e8x,y2 +v\u27e9,...)\nwhere the second step follows by translation invariance. This implies\nthat without changing attention patterns in any given input, we can\ntranslate all key vectors such that the query and key distributions for\neach attention head have identical centroids. This makes it much easier\nto compare queries and keys (Fig. 3, right).\nScaling Queries and Keys: In some transformers, such as GPT-2,\nwe observed cases where the average query norm was very different\nfrom the average key norm. This difference makes it hard to interpret\nkey-query relations in a joint embedding. Mathematically, it indicates a\npoor relationship between dot product and distance; visually, it means\nqueries might be a tiny cluster, surrounded by a loose cloud of keys.\nsky\nis\nthe\nthe\nsky\nblue\nis\nblue\nBig distance\ndot product\nLow\nSmall distance\ndot product\nHigh\n(a)\n(b)\nBERT, layer 4 head 11\ndistance\ndot product\nFig. 4: (a) Ideal distance-attention relationship, where query-key pairs\nwith higher dot products are closer in the joint embedding space. (b)\nExample attention head with a strong, negative correlation (-0.983)\nbetween query-key distance and dot product in BERT.\nLuckily, scale is another \u201cfree parameter\u201d of the system. Self-\nattention levels depend only on dot products of query and key vec-\ntors, so if we scale all query vectors by a factor of c \u0338= 0, and all key\nvectors by a factor of c\u22121, the attention values are unchanged. This\nallows high-attention query-key pairs to be closer together in our joint\nvisualizations as depicted in Fig. 4a. (A subtle point: on its own, scal-\ning leaves cosine distance unchanged; however, in combination with\ntranslation normalization it has a nontrivial effect.)\nTo determine the optimal value of c, we can define a weighted\ncorrelation metric that places heavier weight on query-key pairs with\nsmaller distances, since we care most about nearby queries and keys in\nthe joint visualization. We can thus choose a scale factor c such that\nthe weighted correlation between query-key dot products and distances\nis maximized. This scaling method allows for the distances in the\njoint embedding space to most accurately represent the actual attention\nvalues between queries and keys.\n5.1.2\nDistance as a Proxy for Attention\nAs explained above, ideally, if a query-key pair has a large, positive\ndot product (corresponding to a high final self-attention value), they\nshould be placed closer together in the embedding space, and vice versa\n(Fig. 4a). Thus, we expect distance to be inversely correlated with at-\ntention in our joint query-key embeddings. We study this potential link\nby computing the Spearman rank correlation between cosine distance\nand dot product for each attention head in BERT, GPT-2, and ViT. We\nalso experimented with using Euclidean distance as our distance metric\nwhen creating t-SNE and UMAP projections of queries and keys, but\nthis generally led to weaker distance-dot product correlations.\nAcross multiple datasets and models, the relationship between dis-\ntance and attention holds fairly well. For example, with Wiki-Auto\ndata [23], the mean correlation between query-key distances and dot\nproducts is -0.938 for BERT and -0.792 for GPT. An example result\nfrom BERT is shown in Fig. 4b. On the set of COCO images used [28],\nthe mean correlation is -0.873 for ViT-32 and -0.884 for ViT-16.\n5.2\nColor Encodings\nTo visualize different properties of queries and keys, AttentionViz offers\nvarious color encodings. The default option colors points by token type,\ni.e., query or key. For vision transformers, users can color by image\npatch row or column to visualize positional patterns (Fig. 10). Since\nimages encode their own color information, we also allow users to view\nthe original patches without additional styling elements (Fig. 8).\nFor language transformers, we support two positional color schemes:\nnormalized and discrete. To compute normalized position, we divide\neach token\u2019s position in a sentence by the sentence length to produce\na continuous color scale. Lighter hues denote tokens closer to the\nbeginning of the sentence (Fig. 5b). Our discrete position encoding\ntakes each token\u2019s position and applies the modulo operator to get its\nremainder when divided by 5. Thus, the 1st and 6th tokens receive the\nsame color, the 2nd and 7th tokens receive the same color, etc.. We use\nthe same five colors to encode queries and keys at different positions,\nusing darker hues for the former. Although this scheme introduces\nFig. 5: Connecting form to function in BERT. (a) In Matrix View, there are several spiral-shaped plots in layer 3. (b) By zooming into one such\nhead (L3 H9) using Single View, we can see positional attention patterns by using a light-to-dark color scheme that encodes position in the input\nsequence. (c) These patterns can be confirmed by exploring sentence-level visualizations.\nFig. 6: Exploring attention patterns with global search. (a) Heads with\nfewer clusters of search results often demonstrate more semantic be-\nhavior, while heads with dispersed results focus more on token position.\n(b) Zooming into L2 H6, a head with one main result cluster, we indeed\nsee a large group of semantically related query and key tokens.\nambiguity for sentences with length > 5, we find our discrete coloring\nhelpful in seeing relationships based on small offsets in position (e.g.,\nqueries paying attention to keys one step away as in Fig. 11, left).\nAmbiguities are also easily resolved by hovering over tokens, and in\nour explorations, we did not see patterns with offsets greater than 2 or\n3. Users can color by query/key norm as well (Fig. 12a).\n5.3\nViews\nAttentionViz provides three main interactive views for attention explo-\nration: Matrix View, Single View, and Sentence/Image View.\n5.3.1\nMatrix View\nThe initial view in AttentionViz is Matrix View, which uses small\nmultiples to visualize all the attention heads in a transformer at once\n(Fig. 5a), directly addressing [T1] and [T3]. Each row corresponds to\na model layer, moving from earlier layers at the top of the interface\nto later layers at the bottom. With this \u201cglobal\u201d perspective, users\ncan more easily scan for patterns across different transformer layers\nand heads, compared with single-plot (e.g., [53]) or instance-level\nvisualizations (e.g., [3,47]). All the models used in this work had the\nsame architecture: 12 layers x 12 heads per layer = 144 attention heads\nin total, but our system scales to other dimensions.\nIn Matrix View, users can view the joint query-key embeddings\ncreated with t-SNE, UMAP, or PCA. They can also switch between\nmodel types (i.e., BERT, GPT-2, ViT-16/32) or datasets [T4], explore\ndifferent color schemes, and view the resultant plots in 2D or 3D.\nMatrix View supports a global search feature (Fig. 6a), which helps\nhighlight patterns in token locations across different heads and offers\nanother way to analyze attention at scale (see Sec. 7).\n5.3.2\nSingle View\nUsers can click on any plot in Matrix View to zoom into Single View\n(Fig. 5b), which affords exploration of a single attention head in closer\ndetail [T3]. Like Matrix View, users can switch between colorings,\ndimensions, projection modes, datasets, and models in Single View\n[T4]; all graphical changes sync between views to facilitate comparison.\nThe user can click on a point to highlight all tokens in the corresponding\ninput sequence, spotlighting the relevant queries and keys in our joint\nembedding space. Users also have the option to project attention lines\nonto the scatterplots, which connect query and key tokens (Fig. 5c).\nWe only show the top 2 attention weights for each token to enhance\nreadability. Our attention lines feature supports [T2] and offers a new\nway to visualize attention patterns in transformers at the head level.\nIn Single View, users can also search for tokens and use our labelling\nfeature to uncover semantic patterns in the data, similar to [47]. For\ninstance, in Fig. 6b, search reveals that query/key tokens with similar\nmeanings are placed together in the joint embedding for this BERT\nhead, indicating strong attention between them (Sec. 5.1.2).\n5.3.3\nSentence/Image View\nSentence/Image View allows for exploration of the fine-grained attention\npatterns within a single sentence or image [T2, T3]. Both views are\nFig. 7: ViT Image View. (a) Original image. (b) Transparency attention\nheatmap. We highlight the selected token (query) with a green border.\n(c) Overlaid attention arrows. (d) Global attention flow. The square\nicon means an image patch has the strongest attention connection with\nthe <CLS> token, which is not in the original image.\nsynchronized with Single View, matching the attention lines overlaid\non each query/key scatterplot for a smooth user experience.\nSentence View. When using BERT or GPT-2, users can click on a\npoint in Single View to open Sentence View in the left sidebar, which\ndisplays a BertViz-inspired visualization of sentence-level attention\nwith the clicked token highlighted [53] (Fig. 5c). We also considered\nusing heatmap visualizations (e.g., [39]), but it seemed that the bipartite\ngraph approach would offer greater readability and ease of pattern\nexploration for longer sentences. The opacity of the lines connecting\nquery tokens in the left column and key tokens in the right column\nsignifies their corresponding attention strength. Hovering on a token\nhighlights token-specific attention lines. To reduce the noise from\nclassification tokens and separators in BERT, or the first token in GPT-2\n(Sec. 7), users can hide the attention lines from these special tokens.\nOther query and key tokens can also be toggled on/off, and all attention\nlines will be re-normalized accordingly. Users have the option of\nviewing the aggregate attention pattern for each attention head as well,\nto offer another layer of comparison (Fig. 11a).\nImage View. For image-based input in ViT, when users click on\nan image patch, the side panel displays its corresponding original\nimage and highlights the clicked token with a colored border (Fig. 7a).\nUsers also see an image overlaid with an attention heatmap, where the\ntransparency indicates the attention weight between the clicked image\npatch and other regions of the image (Fig. 7b).\nBeyond visualizing the attention of a single token, Image View\nallows users to explore the overall attention pattern within an image by\nshowing arrowed attention lines between different image patches. We\nprovide users with two options when visualizing the attention arrows.\nThe first option overlays arrows on the top of original image patches,\nwith each arrow representing the strongest attention connection between\na starting image patch and destination patch (Fig. 7c). This creates a\nsimplified bipartite attention graph for users to characterize the most\nimportant patterns within a specific head. The second option shows all\nstrong attention connections (i.e., attn(xi,xj) > 0.1) beside the original\nimage, offering a more comprehensive view of attention (Fig. 7d). In\nthis visualization, both opacity and line thickness are used to encode the\nstrength of attention connections. We also tried visualizing all weights\nbetween queries and keys to more closely mirror [53], but this often\nproduced overcrowded, inscrutable results.\n6\nSYSTEM IMPLEMENTATION\nTo process model inputs and compute attention information, we use the\nHugging Face Transformers library and PyTorch. We use pre-trained\nimplementations of BERT, GPT-2 (small), and ViT-16/32 with model\nweights from Google and OpenAI. For each NLP dataset, we randomly\nFig. 8: In ViT-32 Matrix View, we find two interesting visual attention\nheads: one head orders the black-and-white image tokens according\nto brightness, while the other aligns the colorful patches based on hue.\nAttention patterns shown in Image View confirm attention flow between\npatches with the same luminance.\nsample 200 sentences (\u223c10k tokens per attention head, including both\nqueries and keys). Due to the increased computational size of image\nattention data, we display 10 images per head (1000 tokens) for ViT-32\nand 4 images per head (1576 tokens) for ViT-16. Larger datasets can\nbe inputted into AttentionViz at the cost of increased system latency.\nCurrently, it takes \u223c6 seconds to load each NLP dataset, and \u223c10\nseconds to load ViT-16 data. After extracting query and key vector\nembeddings for each attention head, we generate the corresponding\n2D/3D t-SNE, UMAP, and PCA coordinates (Sec. 5.1). To produce\nsemantic labels (e.g., \u201cdog\u201d or \u201cbackground\u201d) for image patches in ViT,\nwe use the DeepLabv3 segmentation model [7]. In total, it takes \u223c3\nhours to preprocess each BERT/GPT-2 dataset on a NVIDIA A100\nGPU; for ViT-32, it takes \u223c30 minutes.\nOur final AttentionViz prototype consists of a Python/Flask backend\nthat communicates with a frontend written in Vue and Typescript. The\ndemo system is available at: http://attentionviz.com. Due to\nthe large size of the data and browser memory constraints, we load\npre-computed attention/projection information via JSON files through\nthe backend. For ViT, the backend also performs image manipulation\n(e.g., patch highlighting and transparency adjustments) to display in\nthe frontend. We use Deck.gl to visualize the resultant query-key joint\nembeddings. AttentionViz is highly extensible and model-agnostic,\nallowing users to add new transformers and datasets to the system.\n7\nFINDINGS & EVALUATION\nWe illustrate the utility of AttentionViz with three application scenarios,\nas well as feedback from domain experts. Our scenarios target the goals\nin Sec. 4, and show how AttentionViz can offer insights about global\nself-attention trends in vision and language transformers.\nData. For BERT/GPT-2, we experimented with various NLP datasets\nbut focus on two for our application scenarios. We use Wiki-Auto [23]\nas a baseline to sample general input sentences and SuperGLUE\nAXb [56] to explore task-specific attention patterns for textual entail-\nment. For ViT, we sample images from ImageNet [44] and Microsoft\nCOCO [28], as well as synthetic image data.\nUser Interviews. We invited E2 and E3 for a second round of inter-\nviews, and included two new experts, E6 (interpretability researcher)\nand E7 (vision science Ph.D. student). As in Sec. 4, all experts were\ninterviewed individually. We first gave experts a quick demo of our\ntool and shared some of our own findings, asking them to share any\nthoughts or insights (Sec. 7.1-7.3). Then, we asked for more general\nfeedback about the main strengths, weaknesses, and novelties of Atten-\ntionViz (Sec. 7.4). We also asked experts about possible extensions or\napplications of this technique for visualizing embeddings at scale.\nFig. 9: Left: Dataset of spatial patterns with different frequencies and angles. Center: In Single View, we observe that one attention head of\nViT-32 arranges image tokens based on the frequencies and angles of their spatial pattern. Right: The attention heatmap in Image View further\nconfirms these findings \u2013 spatial patterns with similar angles pay greater attention to each other.\n7.1\nGoal: Understanding Machine Visual Attention\nAttentionViz can be especially helpful in uncovering new insights about\nattention in vision transformers due to the inherently visual nature of\nimage patch data [G1].\nHue/brightness specializations in visual attention. We were curi-\nous if any visual attention heads specialize in either color-based and\nbrightness-based patterns. To test this, we provided the pre-trained ViT-\n32 model with synthetic color and brightness gradient images (Fig. 8),\nloading the resultant query and key tokens into AttentionViz.\nBrowsing global PCA patterns in Matrix View, we identified two\nattention heads that resemble color and colorless vision. One head\nappears to align black-and-white image tokens based on brightness, and\nthe other aligns colorful patches based on hue. Our dataset contains\ncolor and brightness gradient images in all orientations, and we see\nsimilar patches cluster together in the joint embedding space regardless\nof their position in the original images. The attention heatmap in Image\nView confirms these findings; tokens pay the most attention to other\ntokens with the same color or brightness. E7 was intrigued by these\nresults, having previously studied the color latent space of convolutional\nneural networks (CNNs), and expressed interest in using our tool to\nfurther explore the differences between CNN and ViT behavior.\nFrequency filtering and angle detection. Frequencies and angles\nare low-level characteristics of image data. To investigate if the vision\ntransformer has an attention head that associates visual patterns based\non these features, we created images of sinusoidal signals with varying\nfrequencies and orientations, processing them using our pretrained\nViT-32 model. Examining the resultant query and key embeddings\nin Matrix View, we identified an attention head that separates image\ntokens based on their spatial pattern\u2019s frequencies (x-axis) and angles\n(y-axis) (Fig. 9). With Image View, we observed that tokens in the\nimages of concentric circles are paying attention to other tokens with\nsimilar curvatures, further confirming that this attention head associates\nvisual patterns based on their angles.\nE7 said this result was interesting, but not too surprising given our\nhue/brightness findings, and was more curious about heads that do not\nexhibit this \u201cattend to similar patches\u201d behavior. One experiment they\nproposed was to study attention modulation, e.g., if the same image\npatch (e.g., vertical stripes) occurs in different contexts in two images\n(e.g., zebra vs. umbrella), do we see unique attention patterns?\nIncreasing attention distance across model layers. As noted\nin [14], self-attention attends more broadly across images in deeper\nlayers of vision transformers. We confirmed this finding using our\ninteractive, joint visualizations of query and key tokens in AttentionViz.\nWith Matrix View, we colored patches by image \u201crow\u201d and \u201ccolumn\u201d to\nfind four attention heads in layers 1 and 2 of ViT-32 that group tokens\nwith their nearest spatial neighbors: on their left, right, top, and bottom.\nIn layers 3 and 4, we saw similar positional attention patterns, but\nimage tokens pay attention to all the patches in the same row or column,\nbeyond their nearest neighbors (Fig. 10). This suggests that unlike\nCNNs, which process images using a square filter, the self-attention\nmechanism in transformers often processes images row by row and\nFig. 10: Coloring image patches by row highlights positional attention\npatterns in ViT-32. In Layer 1, tokens in the same row and adjacent\ncolumns form small clusters. Image View reveals a look at left pattern.\nIn Layer 4, large clusters of tokens form based on row positions. Using\nthe arrowed lines, we see a wider, bidirectional attention flow.\ncolumn by column, analogous to an elongated filter.\n7.2\nGoal: Finding Global Attention Traces\nTo understand how self-attention patterns vary across different heads in\nlanguage transformers [G2], we used AttentionViz to explore BERT.\nPositional attention signatures. With TSNE, we observed several\nattention heads with unique shapes, e.g., the spiral-shaped plots in layer\n3 (Fig. 5a). For example, coloring layer 3 head 9 by normalized position\nin Single View reveals that token position increases as we move from\nthe outside to the inside of the spiral (Fig. 5b). We used Sentence View\nto examine this pattern more closely (Fig. 5c), confirming that there is\na positional, \u201cnext-token\u201d attention pattern. This \u201cspiral\u201d also reflects\nthe initial ordering vector given to transformers (Sec. 2).\nWe then noticed other identifiable \u201ctraces\u201d in Matrix View, finding\nthat plots with small \u201cclumps\u201d also encode positional patterns (Fig. 11,\nleft), which we verified with our discrete position coloring. The dif-\nference between \u201cspirals\u201d and \u201cclumps\u201d appears to be whether tokens\nattend selectively to others one position away, versus at several different\npossible positions (Fig. 5c). Similarly, we learned that in heads with\nhigh query-key overlap, tokens typically attend to themselves and other\ninstances of the same token, exhibiting a \u201clook at self\u201d pattern. Zoom-\ning into these heads, we see clear semantic clusters of nearby query-key\nFig. 11: Other visual traces of attention. Left: Heads with small \u201cclumps\u201d often have even tighter positional patterns than spirals. Our discrete\nposition encoding, which colors each token based on its position modulo 5, highlights a \u201cnext-token\u201d attention trend. Right: Layered bands of\nqueries and keys only appear with SuperGLUE AXb data [56], indicating strong attention to text start, end, and midpoint.\npairs as shown in Fig. 6b, further supporting this observation.\n[29] shows that earlier transformer layers have the most information\nabout linear word order, aligning with our findings and previous work\nsuch as [9, 53]. During our interviews, E2, E6, and E7 immediately\nnoticed these interesting geometries, particularly spirals, and were curi-\nous about how much of the observed structure is purely due to position.\nThis inspired several follow-up experiment ideas from experts, e.g.,\nmanipulating or removing the positional embeddings in transformer\nmodels and seeing how our query-key visualizations change.\nTask-specific traces. After visualizing multiple datasets with At-\ntentionViz, we found that the shapes of joint embeddings are highly\nconsistent across different NLP tasks. However, we did see one visual\ntrace that only arises in some later layers of BERT with the Super-\nGLUE AXb data (Fig. 11, right). Clicking on one such head (layer 8\nhead 9) and coloring by position, we observed a query-key \u201csandwich,\u201d\nwhere keys and queries at the beginning of the text are stacked on top,\nfollowed by queries and keys at the end of the text in reverse order.\nSentence View reveals that the start, middle, and end of the text\nreceive the most attention. The overall plot shape and attention pat-\ntern suggests that these heads can identify a text\u2019s \u201cmidpoint\u201d and\ndifferentiate between sentences, mirroring how in entailment tasks, two\nsentences are compared to see if they have similar meanings. Queries\nalso mostly attend to keys in the same sentence. [25,54] shows how syn-\ntactic and task-specific information is most prominent in mid-to-later\nmodel layers, perhaps explaining the uniqueness of this trace.\nGlobal search patterns. The aggregate search feature in Matrix\nView can also be used to quickly scan for and compare attention trends\nacross heads [G2]. We found that patterns in the search results reflect\nthe previously identified visual attention traces (Fig. 6a). For example,\nheads that are spiral-shaped or have small clumps of queries/keys have\nmore dispersed search results, indicative of their underlying positional\nattention patterns. On the other hand, heads with the \u201clook at self\u201d\nattention pattern only have one cluster of search results, emphasizing\nthe strong interaction between queries and keys of the same token.\nEven if a joint query-key embedding does not have a distinctive\nshape, we see that if there are only a few search result clusters, the\nhead may display more semantic behavior; otherwise, there is likely\na positional attention pattern. [49] notes that semantic information is\nspread across BERT\u2019s layers, which we confirmed with AttentionViz.\nAll of our experts were particularly excited by this feature of our tool\nand its ability to facilitate attention pattern comparisons.\n7.3\nGoal: Identifying Anomalies and Unexpected Behavior\nThrough interacting with the joint query-key embeddings in Attention-\nViz, we discovered some irregular model behaviors [G3].\nNorm disparities and null attention. While exploring GPT-2 in\nMatrix View, we observed that in early model layers, some query and\nFig. 12: Anomalies in GPT-2. (a) In early model layers, we witness\na significant disparity between query-key norms for many attention\nheads (e.g., L1 H8 prior to norm scaling). (b) Example of the prevalent\n\u201cattend to first\u201d pattern in later layers. Sentence View reveals latent\nattention behavior after hiding the first token.\nkey clusters were well-separated, even after key translation (Sec. 5.1.1).\nBy coloring by norm (as measured before the norm scaling step), we\nsaw that in many heads, there is a significant disparity between the\nnorms of query and key vectors (Fig. 12a). When query norms are\nsmall (i.e., light green), key norms tend to be large (i.e., dark pink), and\nvice versa. Computing the average norm difference between queries\nand keys in GPT-2 vs. BERT, we found that in the former, the mean\nquery norm - key norm = -4.59 across attention heads, while in the\nlatter, the mean difference is only 0.41. None of our experts could\nexplain this finding: \u201cIt doesn\u2019t really make sense why queries and keys\nwould have such different norms\u201d (E6). Interestingly, a paper published\nafter we made this observation [11] points to out-of-control query and\nkey norms as a cause of serious training instability, indicating that this\nphenomenon may be worth studying further. This observation inspired\nour scaling approach from Sec. 5.1.1 as well.\nWe also noticed that in many GPT-2 heads, most attention is di-\nrected to the first token (Fig. 12b), especially in later layers.\n[54]\nbriefly mentions that the first token is treated as a null position for\nattention-receiving in GPT-2 \u201cwhen the linguistic property captured\nby the attention head doesn\u2019t appear in the input text.\u201d However, this\nphenomenon remains underexplored, proposing another open inter-\npretability question to consider. E2 and E6 both noticed this anomalous\nbehavior on their own with our tool, and all of our experts were sur-\nprised by this finding. [55] shows that pruning the majority of attention\nheads in transformers may not significantly impact model performance,\n(a)\n(b)\n(c)\n(d)\nFig. 13: Identifying a \u201clook at self\u201d attention head in ViT-32. (a) Single View shows queries and keys are sparsely distributed in the joint\nembedding space. (b) Zooming in, query and key vectors of the same image token are tightly overlapped. (c) Image View reveals tokens pay most\nattention to themselves. (d) Comparing the learned parameters of query and key projection layers confirms that they learn redundant projections.\nwhich perhaps can be partially attributed to this dominant null attention\npattern. Regardless, AttentionViz allows users to filter out attention\npaid to the first token, uncovering hidden query-key interactions.\n\u201cLook at self\u201d attention heads. AttentionViz can also reveal sur-\nprising attention patterns in vision transformers. In Matrix View, we\nidentified several heads in early layers of ViT-32 with very diffused\nkey-query clusters (Fig. 13a). Looking at one such attention head\n(layer 0 head 8), we discovered that the query and key embeddings of\nthe same token form a small but dense cluster, with each query-key\npair well-separated from the others (Fig. 13b). From the transparency\nheatmap in Image View, we see that the patch is solely attending to\nitself (Fig. 13c). Switching to the arrowed attention lines, we discover\nthat the overall attention pattern for this image is \u201clook at self,\u201d where\nno information is flowing between image tokens in this head.\nAfter identifying this irregular attention pattern, we checked the\nlearned parameters of the query and key matrices with a correlation test.\nWe found a strong similarity score (linear correlation = 0.94), indicat-\ning that the query and key layers in this ViT head are indeed learning\nredundant projections (Fig. 13d). E3 noted that this knowledge could\ninform model pruning experiments, and AttentionViz could similarly\nbe used to detect potential training failures or other irregularities.\n7.4\nTakeaways from User Feedback\nMerits of Matrix View. Several experts found the \u201cglobal\u201d perspective\nprovided by Matrix View to be the most novel and valuable part of\nAttentionViz. As E6 said, \u201cIt\u2019s great for quick comparison and frees\nyou from tuning hyperparameters when you want to visualize multiple\nembeddings at once.\u201d E7 also mentioned that Matrix View is useful\nbecause \u201cfor smaller visualizations, I can just code up something myself,\nbut it\u2019s a lot harder at scale and with more data.\u201d These comments\nsuggest that this idea of visualizing and comparing embeddings at scale\nmay be beneficial in other ML settings as well.\nApplications for joint query-key embeddings. Experts proposed\nvarious use cases and extensions for our visualization technique, evi-\ndencing its wider applicability. For example, E2 suggested visualizing\npatterns in untrained or corrupt transformers, and both E3 and E7\nwanted to visualize changes in attention during training for their own\nmodels, aligning with our original goals (Sec. 4). E2 and E6 also\nsuggested adapting our tool to help with causal tracing, explaining\nthat \u201cit might be useful to track attention flow throughout the model\nfor hypothesis testing.\u201d Similarly, E3 expressed interest in looking\ninto \u201chow two attention patterns connect in different heads,\u201d which\ncould certainly be applied to visualizing induction head pairs. E2 noted\nthat adding a way to \u201cquantify similarity between two heads\u201d could be\nuseful, while E6 proposed \u201cmeasuring or visualizing randomness in\nheads\u201d for model pruning purposes.\nEmbedding projections \u2013 to trust or not to trust? E3 highlighted\nthe challenges of using projection methods. While they appreciated the\nstriking geometric patterns (e.g., spirals) we found, E3 expressed some\nskepticism about interpreting these visualizations due to the distortion\nfrom techniques such as t-SNE and UMAP: \u201cHow do I know if I can\ntrust what I see?\u201d This emphasizes the importance of tying visual\ninsights to actionable interventions, perhaps through augmenting our\ntool to support hypothesis testing in addition to exploration.\nFlexibility-usability tradeoff. E2 indicated that AttentionViz \u201cfeels\nvery usable and customizable,\u201d contrasting with \u201cexisting visualization\ntools that are too overwhelming to learn and use.\u201d However, some ex-\nperts like E6 were still worried that \u201cshowing all the features and heads\nmight be overwhelming... Is there a way to summarize the information?\nOr focus more on a specific task?\u201d E7 added, \u201cI wonder if there\u2019s a\nquicker, more digestible way to label heads,\u201d suggesting an approach\ncloser to feature visualization [36]. We designed AttentionViz to be a\nflexible tool (e.g., allowing attention analysis in different transformers\nand at different granularities), but it seems that the flexibility-usability\ntradeoff [27] of our design could still be improved.\nAdditional interaction modes. Some experts suggested additional\ninteraction modes, e.g., on-the-fly inference (E3) or further dimension-\nality reductions on circled clusters of queries and keys to reveal addi-\ntional information and perform fine-grained analyses (E2). E7 stressed\nthe importance of allowing users to directly upload new datasets to the\nsystem: \u201cThe tool could be even more powerful... people are going to\nwant to explore more with it like adding their own images.\u201d\n8\nCONCLUSIONS & FUTURE WORK\nIn this work, we introduce a new technique for visualizing trans-\nformer self-attention based on a joint embedding space for queries\nand keys. Applying our technique, we create AttentionViz (demo:\nhttp://attentionviz.com), an interactive visualization tool, and\nuse it to gain insights about attention in both language and vision trans-\nformers. For instance, we discover novel hue/frequency behavior in\nViT, and striking query-key norm disparities in GPT-2. Although our\napproach is tailored to self-attention, it can be generalized to other\nattention mechanisms (e.g., cross attention) with proper modifications\nto vector normalization. Similarly, we focus on semantic patterns, but\nAttentionViz could be used to study syntactic features as well, e.g., by\nusing NLP libraries to add metadata such as part of speech.\nExpert feedback also points to several avenues for future work, such\nas finding ways to manage the complexity of multiple embedding visual-\nizations and focus users on features of interest. Our system is currently\nlimited by data pre-computation times and memory requirements, but\nwe plan to improve the scalability of AttentionViz and allow users to\nadd new inputs on the fly. A sufficiently large random sample (e.g.,\na few thousand tokens) is enough to surface large-scale, global phe-\nnomena, but larger datasets and models may reveal additional semantic\ninsights, as suggested by [38]. Trying other data sampling approaches\nmight be fruitful as well (e.g., to reduce biases from Zipf\u2019s law [62]).\nAnother natural direction for future research is exploring how to\nincorporate information from value vectors in each attention head [52].\nThese value vectors are an essential part of the attention mechanism,\nthough it is not clear how to visualize them in the context of queries\nand keys. Finding the right visualization approach might shed more\nlight on how attention heads function. Finally, although AttentionViz\nis an exploratory tool, adapting it for hypothesis testing and/or causal\ntracing might provide support for practical model debugging.\nACKNOWLEDGMENTS\nWe would like to thank all the participants in our user interviews for\ntheir time and invaluable insights. We also thank the anonymous re-\nviewers for helping us improve this paper with their thorough and\nconstructive comments. Finally, we are grateful for the thoughtful\nfeedback and support provided by the members of the Harvard Insight\n+ Interaction Lab throughout this project.\nREFERENCES\n[1] E. Aflalo, M. Du, S.-Y. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal. Vl-\ninterpret: An interactive visualization tool for interpreting vision-language\ntransformers. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 21406\u201321415. IEEE, New Orleans,\n2022. doi: 10.1109/CVPR52688.2022.02072 2\n[2] D. L. Arendt, N. Nur, Z. Huang, G. Fair, and W. Dou. Parallel embed-\ndings: a visualization technique for contrasting learned representations.\nIn Proceedings of the 25th International Conference on Intelligent User\nInterfaces, pp. 259\u2013274. ACM, Cagliari, 2020. doi: 10.1145/3377325.\n3377514 3\n[3] A. Boggust, B. Carter, and A. Satyanarayan. Embedding comparator:\nVisualizing differences in global structure and local neighborhoods via\nsmall multiples. In 27th International Conference on Intelligent User\nInterfaces, pp. 746\u2013766. ACM, Helsinki, 2022. doi: 10.1145/3490099.\n3511122 3, 5\n[4] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and\nA. Joulin. Emerging properties in self-supervised vision transformers.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 9650\u20139660. IEEE, Montreal, 2021. doi: 10.1109/ICCV48922.\n2021.00951 2\n[5] S. Carter, Z. Armstrong, L. Schubert, I. Johnson, and C. Olah. Activation\natlas. Distill, 4(3), 2019. doi: 10.23915/distill.00015 1\n[6] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond\nattention visualization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 782\u2013791. IEEE, Online,\n2021. doi: 10.1109/CVPR46437.2021.00084 3\n[7] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv Preprint, 2017. doi:\n10.48550/arXiv.1706.05587 6\n[8] E. A. Chi, J. Hewitt, and C. D. Manning. Finding universal grammatical\nrelations in multilingual BERT. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pp. 5564\u20135577. ACL,\nOnline, 2020. doi: 10.18653/v1/2020.acl-main.493 2, 3\n[9] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does BERT\nlook at? an analysis of BERT\u2019s attention. In Proceedings of the 2019 ACL\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, pp. 276\u2013286. ACL, Florence, 2019. doi: 10.18653/v1/W19-4828 2,\n8\n[10] J.-B. Cordonnier, A. Loukas, and M. Jaggi. On the relationship between\nself-attention and convolutional layers. In Eighth International Conference\non Learning Representations. Addis Ababa, 2020. doi: 10.48550/arXiv.\n1911.03584 2\n[11] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,\nA. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision\ntransformers to 22 billion parameters. arXiv Preprint, 2023. doi: 10.\n48550/arXiv.2302.05442 1, 8\n[12] J. F. DeRose, J. Wang, and M. Berger. Attention flows: Analyzing and\ncomparing attention mechanisms in language models. IEEE Transactions\non Visualization and Computer Graphics, 27(2):1160\u20131170, 2021. doi: 10\n.1109/TVCG.2020.3028976 2\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv\nPreprint, 2018. doi: 10.48550/arXiv.1810.04805 1, 2\n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\nIn Ninth International Conference on Learning Representations. Online,\n2021. doi: 10.48550/arXiv.2010.11929 1, 2, 7\n[15] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec,\nZ. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, et al. Toy models of\nsuperposition. arXiv Preprint, 2022. doi: 10.48550/arXiv.2209.10652 2\n[16] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann,\nA. Askell, Y. Bai, A. Chen, T. Conerly, et al. A mathematical frame-\nwork for transformer circuits. Transformer Circuits Thread, 2021. https:\n//transformer-circuits.pub/2021/framework/index.html. 2\n[17] A. Ghiasi, H. Kazemi, E. Borgnia, S. Reich, M. Shu, M. Goldblum, A. G.\nWilson, and T. Goldstein. What do vision transformers learn? a visual\nexploration. arXiv Preprint, 2022. doi: 10.48550/arXiv.2212.06727 3\n[18] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics in deep\nlearning: An interrogative survey for the next frontiers. IEEE Transactions\non Visualization and Computer Graphics, 25(8):2674\u20132693, 2018. doi: 10\n.1109/TVCG.2018.2843369 3\n[19] F. Hohman, H. Park, C. Robinson, and D. H. P. Chau. Summit: Scaling\ndeep learning interpretability by visualizing activation and attribution sum-\nmarizations. IEEE Transactions on Visualization and Computer Graphics,\n26(1):1096\u20131106, 2019. doi: 10.1109/TVCG.2019.2934659 3\n[20] B. Hoover, H. Strobelt, and S. Gehrmann. exBERT: A visual analysis tool\nto explore learned representations in Transformer models. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguis-\ntics: System Demonstrations, pp. 187\u2013196. ACL, Online, 2020. doi: 10.\n18653/v1/2020.acl-demos.22 1, 2\n[21] T. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche, and\nC. Wolf.\nVisqa: X-raying vision and language reasoning in trans-\nformers. IEEE Transactions on Visualization and Computer Graphics,\n28(1):976\u2013986, 2022. doi: 10.1109/TVCG.2021.3114683 2\n[22] X. Ji, Y. Tu, W. He, J. Wang, H.-W. Shen, and P.-Y. Yen. Usevis: Visual\nanalytics of attention-based neural embedding in information retrieval.\nVisual Informatics, 5(2):1\u201312, 2021. doi: 10.1016/j.visinf.2021.03.003 2\n[23] C. Jiang, M. Maddela, W. Lan, Y. Zhong, and W. Xu. Neural CRF model\nfor sentence alignment in text simplification. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp.\n7943\u20137960. ACL, Online, 2020. doi: 10.18653/v1/2020.acl-main.709 1,\n4, 6\n[24] I. T. Jolliffe. Principal Components in Regression Analysis, pp. 129\u2013155.\nSpringer, New York, 1986. doi: 10.1007/978-1-4757-1904-8_8 4\n[25] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the\ndark secrets of BERT. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, pp. 4365\u20134374. ACL, Hong\nKong, 2019. doi: 10.18653/v1/D19-1445 2, 8\n[26] Q. Li, K. S. Njotoprawiro, H. Haleem, Q. Chen, C. Yi, and X. Ma. Embed-\ndingvis: A visual analytics approach to comparative network embedding\ninspection. In 2018 IEEE Conference on Visual Analytics Science and\nTechnology, pp. 48\u201359. IEEE, Berlin, 2018. doi: 10.1109/VAST.2018.\n8802454 3\n[27] W. Lidwell, K. Holden, and J. Butler. Universal principles of design,\nrevised and updated: 125 ways to enhance usability, influence perception,\nincrease appeal, make better design decisions, and teach through design.\nRockport Pub, Beverly, 2010. 9\n[28] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r,\nand C. L. Zitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, pp. 740\u2013755. Springer, Zurich,\n2014. doi: 10.1007/978-3-319-10602-1_48 4, 6\n[29] Y. Lin, Y. C. Tan, and R. Frank. Open sesame: Getting inside BERT\u2019s\nlinguistic knowledge. In Proceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 241\u2013\n253. ACL, Florence, 2019. doi: 10.18653/v1/W19-4825 8\n[30] S. Liu, T. Li, Z. Li, V. Srikumar, V. Pascucci, and P.-T. Bremer. Visual\ninterrogation of attention-based models for natural language inference and\nmachine comprehension. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing: System Demonstrations,\npp. 36\u201341. ACL, Brussels, 2018. doi: 10.18653/v1/D18-2007 1, 2\n[31] J. Ma, Y. Bai, B. Zhong, W. Zhang, T. Yao, and T. Mei. Visualizing and\nunderstanding patch interactions in vision transformer. IEEE Transactions\non Neural Networks and Learning Systems, pp. 1\u201310, 2023. doi: 10.\n1109/TNNLS.2023.3270479 2\n[32] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approxi-\nmation and projection for dimension reduction. arXiv Preprint, 2018. doi:\n10.48550/arXiv.1802.03426 4\n[33] P. Mehrani and J. K. Tsotsos. Self-attention in vision transformers per-\nforms perceptual grouping, not attention. arXiv Preprint, 2023. doi: 10.\n48550/arXiv.2303.01542 2\n[34] A. Miaschi, D. Brunato, F. Dell\u2019Orletta, and G. Venturi. Linguistic pro-\nfiling of a neural language model. In Proceedings of the 28th Interna-\ntional Conference on Computational Linguistics, pp. 745\u2013756. Interna-\ntional Committee on Computational Linguistics, Barcelona, 2020. doi: 10.\n18653/v1/2020.coling-main.65 2\n[35] M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,\nand M.-H. Yang. Intriguing properties of vision transformers. In Advances\nin Neural Information Processing Systems, vol. 34, pp. 23296\u201323308.\nCurran Associates, Inc., Online, 2021. doi: 10.48550/arXiv.2105.10497 2\n[36] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. Distill,\n2(11), 2017. doi: 10.23915/distill.00007 9\n[37] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan,\nB. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and\ninduction heads. arXiv Preprint, 2022. doi: 10.48550/arXiv.2209.11895\n2\n[38] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov,\nP. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv Preprint, 2023. doi: 10.\n48550/arXiv.2304.07193 9\n[39] C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. C. Kwon, J. Zhao, H. Noh, Y. Lee,\nand J. Choo. Sanvis: Visual analytics for understanding self-attention\nnetworks. In 2019 IEEE Visualization Conference, pp. 146\u2013150. IEEE,\nVancouver, 2019. doi: 10.1109/VISUAL.2019.8933677 2, 6\n[40] N. Park and S. Kim. How do vision transformers work? In Tenth Inter-\nnational Conference on Learning Representations. Online, 2022. doi: 10.\n48550/arXiv.2202.06709 2\n[41] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving\nlanguage understanding by generative pre-training. OpenAI Blog, 2018.\nhttps://openai.com/research/language-unsupervised. 2\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 2019.\nhttps://openai.com/research/better-language-models. 1, 2\n[43] E. Reif, A. Yuan, M. Wattenberg, F. B. Viegas, A. Coenen, A. Pearce, and\nB. Kim. Visualizing and measuring the geometry of bert. In Advances in\nNeural Information Processing Systems, vol. 32. Curran Associates, Inc.,\nVancouver, 2019. doi: 10.5555/3454287.3455058 2, 3\n[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115:211\u2013\n252, 2015. doi: 10.1007/s11263-015-0816-y 6\n[45] R. Sevastjanova, E. Cakmak, S. Ravfogel, R. Cotterell, and M. El-Assady.\nVisual comparison of language model adaptation. IEEE Transactions on\nVisualization and Computer Graphics, 29(1):1178\u20131188, 2022. doi: 10.\n1109/TVCG.2022.3209458 3\n[46] V. Sivaraman, Y. Wu, and A. Perer. Emblaze: Illuminating machine\nlearning representations through interactive comparison of embedding\nspaces. In 27th International Conference on Intelligent User Interfaces, p.\n418\u2013432. ACM, Helsinki, 2022. doi: 10.1145/3490099.3511137 3\n[47] D. Smilkov, N. Thorat, C. Nicholson, E. Reif, F. B. Vi\u00e9gas, and M. Wat-\ntenberg. Embedding projector: Interactive visualization and interpretation\nof embeddings. arXiv Preprint, 2016. doi: 10.48550/arXiv.1611.05469 3,\n5\n[48] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pfister, and A. M.\nRush. Seq2seq-vis: A visual debugging tool for sequence-to-sequence\nmodels. IEEE Transactions on Visualization and Computer Graphics,\n25(1):353\u2013363, 2019. doi: 10.1109/TVCG.2018.2865044 2\n[49] I. Tenney, D. Das, and E. Pavlick. BERT rediscovers the classical NLP\npipeline. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4593\u20134601. ACL, Florence, 2019. doi:\n10.18653/v1/P19-1452 2, 8\n[50] L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal\nof Machine Learning Research, 9(86):2579\u20132605, 2008. http://jmlr.\norg/papers/v9/vandermaaten08a.html. 4\n[51] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws,\nL. Jones, \u0141. Kaiser, N. Kalchbrenner, N. Parmar, et al. Tensor2tensor for\nneural machine translation. arXiv Preprint arXiv:1803.07416, 2018. 1, 2\n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems, vol. 30. Curran Associates, Inc.,\nLong Beach, 2017. doi: 10.48550/arXiv.1706.03762 1, 2, 9\n[53] J. Vig. A multiscale visualization of attention in the transformer model.\nIn Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations, pp. 37\u201342. ACL, Florence,\n2019. doi: 10.18653/v1/P19-3007 1, 2, 5, 6, 8\n[54] J. Vig and Y. Belinkov. Analyzing the structure of attention in a transformer\nlanguage model. In Proceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pp. 63\u201376. ACL,\nFlorence, 2019. doi: 10.18653/v1/W19-4808 8\n[55] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-\nhead self-attention: Specialized heads do the heavy lifting, the rest can be\npruned. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pp. 5797\u20135808. ACL, Florence, 2019. doi: 10.\n18653/v1/P19-1580 8\n[56] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\nO. Levy, and S. R. Bowman. Superglue: A stickier benchmark for general-\npurpose language understanding systems. In Advances in Neural Informa-\ntion Processing Systems, vol. 32, pp. 3266\u20133280. Curran Associates Inc.,\nVancouver, 2019. doi: 10.5555/3454287.3454581 6, 8\n[57] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Inter-\npretability in the wild: a circuit for indirect object identification in gpt-2\nsmall. arXiv Preprint, 2022. doi: 10.48550/arXiv.2211.00593 2\n[58] Y. Wang, S. Liu, N. Afzal, M. Rastegar-Mojarad, L. Wang, F. Shen,\nP. Kingsbury, and H. Liu. A comparison of word embeddings for the\nbiomedical natural language processing. Journal of Biomedical Informat-\nics, 87:12\u201320, 2018. doi: 10.1016/j.jbi.2018.09.008 3\n[59] Z. J. Wang, R. Turko, and D. H. Chau. Dodrio: Exploring transformer\nmodels with interactive visualization. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: System\nDemonstrations, pp. 132\u2013141. ACL, Online, 2021. doi: 10.18653/v1/2021\n.acl-demo.16 2\n[60] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\ngatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto,\nO. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large\nlanguage models. Transactions on Machine Learning Research, 2022. doi:\n10.48550/arXiv.2206.07682 1\n[61] S. Yang, Z. Quan, M. Nie, and W. Yang. Transpose: Keypoint localization\nvia transformer. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 11802\u201311812. Montreal, 2021. doi: 10.1109/\nICCV48922.2021.01159 3\n[62] G. K. Zipf. Selected studies of the principle of relative frequency in\nlanguage. Harvard University Press, Cambridge, 1932. doi: 10.4159/\nharvard.9780674434929 9\n"
  },
  {
    "title": "Can Large Language Models Transform Computational Social Science?",
    "link": "https://arxiv.org/pdf/2305.03514.pdf",
    "upvote": "1",
    "text": "Can Large Language Models Transform\nComputational Social Science?\nCaleb Ziems\u2217\nStanford University\nWilliam Held\nGeorgia Institute of Technology\nOmar Shaikh\nStanford University\nJiaao Chen\nGeorgia Institute of Technology\nZhehao Zhang\nDartmouth College\nDiyi Yang\u2217\u2217\nStanford University\nLarge Language Models (LLMs) are capable of successfully performing many language process-\ning tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and\nexplain social phenomena like persuasiveness and political ideology, then LLMs could augment\nthe Computational Social Science (CSS) pipeline in important ways. This work provides a\nroad map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting\nbest practices and an extensive evaluation pipeline to measure the zero-shot performance of 13\nlanguage models on 25 representative English CSS benchmarks. On taxonomic labeling tasks\n(classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels\nof agreement with humans. On free-form coding tasks (generation), LLMs produce explanations\nthat often exceed the quality of crowdworkers\u2019 gold references. We conclude that the performance\nof today\u2019s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-\nshot data annotators on human annotation teams, and (2) bootstrapping challenging creative\ngeneration tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are\nposed to meaningfully participate in social science analysis in partnership with humans.\n1. Introduction\nThe most surprising scientific changes tend to arrive, not from accumulated facts and\ndiscoveries, but from the invention of new tools and methodologies that trigger \u201cparadigm\nshifts\u201d (Kuhn 1962).\n\u2217 E-mail: Caleb Ziems: cziems@stanford.edu. William Held: wheld3@gatech.edu; Omar Shaikh:\noshaikh@stanford.edu; Jiaao Chen: jiaaochen@gatech.edu; Zhehao Zhang:\nhehao.zhang.gr@dartmouth.edu; Diyi Yang: diyiy@stanford.edu\n\u2217\u2217 Contribution distributed as follows. Caleb, Will, and Diyi decided the project scope and research\nquestions. Caleb performed the CSS literature review, as well as the subject and task selection. Will built\nthe evaluation pipeline and prompting guidelines. Will, Caleb, and Omar all contributed data\npre-processing, loading, and evaluation scripts. Caleb ran the OpenAI and Flan-T5/UL2 prompt\nperturbation experiments and few-shot experiments. Zhehao was responsible for all baseline\nexperiments. Caleb managed the human evaluations. All authors contributed to discussion, results, error\nanalysis, and paper writing.\nAction editor: Vivek Srikumar. Submission received: 26 April 2023; revised version received: 29 August 2023;\naccepted for publication: 25 October 2023.\n\u00a9 2024 Association for Computational Linguistics\narXiv:2305.03514v3  [cs.CL]  26 Feb 2024\nComputational Linguistics\nVolume 50, Number 1\nLiterary Themes\nNarrative Analysis\nCharacter Tropes\nRelationship Dynamics\nHistorical Events\nEvent Extraction\nSemantic Change\nSociolinguistic Variation\nSocial Language Use\nCultural Evolution\nDialect Feature Identification\nFigurative Language\nPersuasion Strategies\nDiscourse Acts\nSocial Psych\nMental Health\nEmpathy\nPositive Reframing\nEmotion Summarization\nEmotion\nHumor\nPoliteness\nPsychology\nLiterature\nHistory\nLinguistics\nPol. Sci\nFraming\nIdeology\nMisinformation\nEvent Framing\nStance\nStatement Ideology\nMedia Slant\nSociology\nSocial Dynamics\nPersuasiveness\nPower\nPower\nAnti-Social Behavior\nToxicity Prediction\nHate Speech\nCultural Analysis\nFigurative Language Explanation\nSocial Bias Inference\nZero Shot Prompt Formatting\nLLM\nWhich of the following leanings would a political \nscientist say that the above article has?\nA: Liberal\nB: Conservative\nC: Neutral\n Discourse Types\nUtterances\nConversations\nDocuments\nFigure 1\nWe assess the potential of LLMs as multi-purpose tools for CSS. We identify core subject areas in\nprior CSS work and select 24 diverse and representative tasks from across these fields (top).\nThen, we segment tasks into distinct discourse types and evaluate both open and closed-source\nLLMs across this benchmark using zero-shot prompting (bottom).\nComputational Social Science (CSS) (Lazer et al. 2020) was born from the immense\ngrowth of human data traces on the web and the rapid acceleration of computational\nresources for processing this data. These developments allowed researchers to study\nlanguage and behavior at an unprecedented scale (Lazer et al. 2009), with both global\nand fine-grained observations (Golder and Macy 2014). From the early days of content\ndictionaries (Stone, Dunphy, and Smith 1966), statistical text analysis facilitated CSS\nresearch by providing structure to non-numeric data. Now, Large Language Models\n(LLMs) may be poised to change the computational social science landscape by provid-\ning such capabilities without custom training data.\nThe goal of this work is to assess the degree to which LLMs can transform Com-\nputational Social Science (CSS). Solid computational approaches are needed to help an-\nalyze textual data and to understand a variety of social phenomena across academic\ndisciplines. Current CSS methodologies typically use supervised text classification and\ngeneration in order to scale up manual labeling efforts to unseen texts (also called coding\nin the social sciences). Reliable supervised methods typically demand an extensive\namount of human-annotated training data. Alternatively, unsupervised methods can\nrun \u201cfor free,\u201d but the resulting output can be uninterpretable. In the status quo, data\nresources constrain the theories and subjects CSS can be applied to.\nLLMs have the potential to remove these constraints. Recent LLMs have demon-\nstrated the striking ability to reliably classify text, summarize documents, answer ques-\ntions, and generate interpretable explanations in a variety of domains, even exceeding\nhuman performance without the need for supervision (Bang et al. 2023; Qin et al. 2023;\nZhuo et al. 2023; Goyal, Li, and Durrett 2022). If LLMs can similarly provide reliable la-\nbels and summary codes through zero-shot prompting, CSS research can be broadened\nto a wider range of hypotheses than current tools and data resources support. Zero-\nshot viability in this space is our primary research question. To effectively harness the\npower of LLMs, behavioral researchers should understand the pros and cons of different\nmodeling decisions (model-selection), as well as how these decisions intersect with their\nfields of specialization (domain-utility) and downstream use-cases (functionality). By\n2\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nevaluating LLMs on an extensive suite of CSS tasks, this work provides researchers a\nroadmap with answers to the following research questions:\n\u2022 (RQ1) Viability: Are LLMs able to augment the human annotation pipeline? Can\nthey match or exceed the reliability of human annotation?\n\u2022 (RQ2) Model-Selection: How do different aspects of LLMs (e.g., model size,\npretraining) affect their performances on CSS tasks?\n\u2022 (RQ3) Domain-Utility: Are zero-shot LLMs specially adapted for better results in\nsome fields of science rather than others?\n\u2022 (RQ4) Functionality: Are zero-shot LLMs equipped to assist with labeling tasks\n(classification) or summary-explanatory tasks (generation) or both?\nThe research pipeline in Figure 1 allows us to answer these questions. First, we\nsurvey the social science literature to understand where LLMs could serve as ana-\nlytical tools (\u00a72). Then we operationalize each use-case with a set of representative\ntasks (\u00a73). Specifically, classification and parsing methods can help researchers code for\nlinguistic, psychological, and cultural categories (\u00a73.1 - \u00a73.3) while generative models\ncan explain underlying constructs (e.g., figurative language, emotional reactions, hate\nspeech, and misinformation), and restructure text according to established theories like\ncognitive behavioral therapy (\u00a73.4). With a final evaluation suite of 24 tasks, we test\nthe zero-shot performance of 13 language models with differing architectures, sizes,\npre-training, and fine-tuning paradigms (\u00a75, \u00a76). This allows us to suggest actionable\nsteps for social scientists interested in co-opting LLMs for research (\u00a77). Specifically, we\nsuggest a blended supervised-unsupervised scheme for human-AI partnered labeling\nand content analysis.\nConcretely, our analysis reveals that, except in minority cases, prompted LLMs do\nnot match or exceed the performance of carefully fine-tuned classifiers, and the best\nLLM performances are often too low to entirely replace human annotation. However,\nLLMs can achieve fair levels of agreement with humans on labeling tasks (RQ1). These\nresults are not limited to a subset of academic fields, but rather span the social sciences\nacross a range of conversation, utterance, and document-level classification tasks (RQ2).\nFurthermore, the benefits of LLMs are compounded as models scale up (RQ3). This\nsuggests that LLMs can augment the annotation process through iterative joint-labeling,\nsignificantly speeding up and improving text analysis in the social sciences.\nImportantly, some LLMs can also generate informative explanations for social sci-\nence constructs. Leading models can achieve parity with the quality of dataset refer-\nences, and can even exceed them in terms of relevance, coherence, faithfulness, and\nfluency. Humans prefer model outputs 50% of the time, suggesting that human-AI\ncollaboration will extend beyond labeling tasks to the joint coding of new constructs,\nanalyses, and summaries.\n2. An Overview of CSS\nFollowing Lazer et al. (2020), we define Computational Social Science as the develop-\nment and application of computational methods to the scientific analysis of behavioral\nand linguistic data. Critically, CSS centers around the scientific method, forming and\ntesting broad and objective hypotheses, while similar efforts in the Digital Humanities\n(DH) focus more on the subjectivity and particularity of events, dialogues, cultures,\nlaws, value-systems, and human activities (Dobson 2019).\nThis section surveys the current needs of researchers in both the computational\nsocial sciences and digital humanities. We choose to merge our discussion under the\n3\nComputational Linguistics\nVolume 50, Number 1\nbanner of CSS, since solid computational approaches are needed to help analyze textual\ndata and to understand a variety of socio-behavioral phenomena across both scientific\nand humanistic disciplines. We focus primarily on the most tractable text classification,\nstructured parsing, summarization, and natural language generation tasks for CSS.\nSome other techniques like aggregate mining of massive datasets or topic modeling\nmay be largely outside the scope of transformer-based language models, which have a\nfixed processing window size and quadratic space complexity.\nThe following subsections outline how computational methods can support specific\nfields of inquiry regarding how people think (psychology; \u00a72.5), communicate (linguistics;\n\u00a72.3), establish governance and value-systems (political science, economics; \u00a72.4), collec-\ntively operate (sociology; \u00a72.6), and create culture (literature, anthropology; \u00a72.2) across\ntime (history; \u00a72.1).\n2.1 History\nHistorians study events, or transitions between states (Box-Steffensmeier and Jones 2004;\nAbbott 1990), like the onset of a war. Event extraction is a parsing task from unstruc-\ntured text to more regular data structures which capture the location, time, cause, and\nparticipants in the event (Xiang and Wang 2019). This task, which is central to a growing\nnumber of computational studies on history (Lai et al. 2021; Sprugnoli and Tonelli 2019),\ncan be broken into (1) event detection, and (2) event argument extraction, which we\nbenchmark in \u00a73.3.1 and \u00a73.3.2 respectively. Historians also work to understand the\ninfluence of events on historical shifts in discourse (DiMaggio, Nag, and Blei 2013)\nand meaning (Hamilton, Leskovec, and Jurafsky 2016a). We further discuss NLP for\ndiscourse and semantic change in \u00a72.4 and \u00a72.3.\n2.2 Literature\nLiterary studies are closely tied to the analysis of themes (Jockers and Mimno 2013), set-\ntings (Piper, So, and Bamman 2021), and narratives (Sap et al. 2022; Saldias and Roy 2020;\nBoyd, Blackburn, and Pennebaker 2020). Settings can be identified using named entity\nrecognition (Brooke, Hammond, and Baldwin 2016) and toponym resolution (DeLozier\net al. 2016), which are already demonstrably solved by prompted models like GPT 3.5\nTurbo (Qin et al. 2023). Themes are typically the subject of topic modeling, which is\noutside the scope of LLMs. Instead we focus on NLP for narrative analysis. NLP systems\ncan be used to parse narratives into chains (Chambers and Jurafsky 2008) with agents\n(Coll Ardanuy et al. 2020; Vala et al. 2015) their relationships (Labatut and Bost 2019; Iyyer\net al. 2016; Srivastava, Chaturvedi, and Mitchell 2016), and the events (Sims, Park, and\nBamman 2019) they participate in. We cover social role labeling and event extraction\nmethods in \u00a73.3.4 and \u00a73.3.2 respectively. Researchers can also study agents in terms\nof their power dynamics (Sap et al. 2017) and emotions (Brahman and Chaturvedi 2020),\nwhich we benchmark in \u00a73.2.4 and \u00a73.1.2. Figurative language (Kesarwani et al. 2017) and\nhumor classification (Davies 2017) are two other relevant tasks for the study of literary\ndevices, and we evaluate these tasks in \u00a73.1.3 and \u00a73.1.5.\n2.3 Linguistics\nComputational sociolinguists use computational tools to measure the interactions be-\ntween society and language, including the stylistic and structural features that dis-\ntinguish speakers (Nguyen et al. 2016). Language variation is closely related to social\n4\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nidentity (Bucholtz and Hall 2005), from group membership (Del Tredici and Fern\u00e1ndez\n2017), geographical region (Purschke and Hovy 2019), and social class (Preo\u00b8tiuc-Pietro,\nLampos, and Aletras 2015; Del Tredici and Fern\u00e1ndez 2017) to personal attributes like\nage and gender (Bamman, Eisenstein, and Schnoebelen 2014). In \u00a73.1.1 and \u00a73.1.10, we\nuse LLMs to identify the structural features of English dialects, which linguists can\nuse to classify and systematically study dialects, measure different feature densities\nin different population strata, and study the onset and diffusion of language change\n(Kershaw, Rowe, and Stacey 2016; Eisenstein et al. 2014; Ryskina et al. 2020; Kulkarni\net al. 2015; Hamilton, Leskovec, and Jurafsky 2016b; Carlo, Bianchi, and Palmonari 2019;\nZhu and Jurgens 2021b; Schlechtweg et al. 2020).\n2.4 Political Science\nPolitical scientists study how political actors move agendas (Grimmer 2010) by per-\nsuasively framing their discourse \u201cto promote a particular problem definition, causal\ninterpretation, moral evaluation, and/or treatment recommendation\u201d (Entman 1993).\nThese agendas cohere within ideologies. Computational social scientists have advanced\npolitical science through the detection of political leaning, ideology, belief, and stance\n(Ahmed and Xing 2010; Baly et al. 2020; Bamman and Smith 2015; Iyyer et al. 2014;\nJohnson, Lee, and Goldwasser 2017; Preo\u00b8tiuc-Pietro et al. 2017; Luo, Card, and Jurafsky\n2020a; Stefanov et al. 2020), as well as issue (Iyengar 1990) and entity framing (van den\nBerg et al. 2020). Applications for persuasion, framing, ideology, and stance detection\nin the social sciences are numerous. Analysts can uncover fringe issue topics (Bail 2014)\nand frames (Ziems and Yang 2021; Mendelsohn, Budak, and Jurgens 2021; Demszky\net al. 2019; Field et al. 2018), with applications to public opinion (Bhatia 2017; Garg\net al. 2018; Kozlowski, Taddy, and Evans 2019; Abul-Fottouh and Fetner 2018), voting\nbehavior (Black et al. 2011), policy change (Flores 2017), social movements (Nelson 2021;\nSech et al. 2020; Rogers, Kovaleva, and Rumshisky 2019; Tufekci and Wilson 2012),\nand international relations (King and Lowe 2003). We benchmark ideology detection in\n\u00a73.1.6 and \u00a73.3.3, stance detection in \u00a73.1.9, and entity framing in \u00a73.3.4. Furthermore,\nunderstanding the discourse structure and persuasive elements of political speech can\nhelp social scientists measure political impact (Altikriti 2016; Hashim and Safwat 2015).\nWe benchmark persuasion strategy and discourse acts classification in \u00a73.1.8 and \u00a73.2.1.\n2.5 Psychology\nAs the science of mind and behavior, psychology intersects all other adjacent social\nsciences in this section. For example, an individual\u2019s personality, or their stable pat-\nterns of thought and behavior across time, will correlate with their political leaning\n(Gerber et al. 2010), social status (Anderson et al. 2001), and linguistic expression\n(Pennebaker and King 1999). The most influential personality modeling benchmark,\nMyPersonality (Kosinski, Stillwell, and Graepel 2013), is no longer available, but in\nthis work, we evaluate on a representative set of psychological factors down-stream\nof personality. For example, differences in personality and cognitive processing can\nimpact what people find funny (Martin and Ford 2018) or persuasive (Hirsh, Kang,\nand Bodenhausen 2012). These psychological factors then exert influence over a range\nof social interactions. Humor and politeness (Brown and Levinson 1987) are correlated\nwith subjective impressions of psychological distance between speakers (Trope and\nLiberman 2010), while persuasive techniques bind agents in social commitments, with\napplications in the science of management and organizations. We evaluate on humor,\n5\nComputational Linguistics\nVolume 50, Number 1\npersuasion, and politeness classification in \u00a73.1.5, \u00a73.1.8, and \u00a73.2.6 respectively. We\nalso consider LLMs as tools for counseling, mental health and positive psychology in\ntext-based interactions. Specifically, we evaluate on empathy detection in online mental\nhealth platforms (Sharma et al. 2020) in \u00a73.2.2, emotional aspect-based summarization in\n\u00a73.4.1 ,and a positive reframing style-transfer task (Ziems et al. 2022) based on cognitive\nbehavioral therapy in \u00a73.4.5.\n2.6 Sociology\nSociologists want to understand the structure of society and how people live collectively\nin social groups (Wardhaugh and Fuller 2021; Keuschnigg, Lovsj\u00f6, and Hedstr\u00f6m 2018).\nBy tracing the diffusion and recombination of linguistic, political, and psychological\ncontent between actors in a community across time, sociologists can begin to under-\nstand social processes at both the micro and macro scale. At the micro scale, there is\nthe computational sociology of power (Danescu-Niculescu-Mizil et al. 2012; Bramsen\net al. 2011; Prabhakaran, Reid, and Rambow 2014; Prabhakaran, Rambow, and Diab\n2012) and social roles (Welser et al. 2011; Fazeen, Dantu, and Guturu 2011; Zhang, Tang,\nand Li 2007; Yang, Wen, and Ros\u00e9 2015; Maki et al. 2017). LLMs can assist sociological\nresearch by predicting power relations (\u00a73.2.4) and unhealthy conversations (\u00a73.2.5).\nAt the macro-scale, there are computational analyses of social norms and conventions\n(Centola et al. 2018; Bicchieri 2005), information diffusion (Leskovec, Backstrom, and\nKleinberg 2009; Tan, Lee, and Pang 2014; Vosoughi, Roy, and Aral 2018; Cheng et al.\n2016), emotional contagion (Bail 2016), collective behaviors (Barber\u00e1 et al. 2015), and\nsocial movements (Nelson 2021, 2015). Again, LLMs can detect constructs like emotion\n(\u00a73.1.2) and the speech of hateful social groups (\u00a73.1.4). Furthermore, social movements\nrely on the diffusion of norms and idiomatic slogans, which carry meaning through\nfigurative language that LLMs can decode (\u00a73.1.3).\n3. Representative CSS Task Selection\nWhile not exhaustive, our task selection is designed to provide a representative survey\nof the CSS needs in \u00a72. This will allow us to answer Research Questions 1-4, which are\npertinent to social science researchers. Thus our work is distinct and complementary to\nBIG-Bench (Srivastava et al. 2023) and other efforts to benchmark the logical, physical,\nand social reasoning capabilities of LLMs. Our attention is more carefully focused on the\naffordances of LLMs for social science.1Our tasks are field-specific and come with the\nfield-specific challenges of expert taxonomies, large label spaces, temporal grounding,\nand domain-specific parsing schemes (see \u00a77.6).\nTo help answer RQ3 and 4, we organize this section according to our division\nof tasks into functional categories based on the unit of text analysis: 10 utterance-\nlevel classification tasks (\u00a73.1), 6 conversation-level tasks (\u00a73.2), and 4 document-level\ntasks for the analysis of media (\u00a73.3). In addition to these 20 classification tasks, we\nevaluate 5 generation tasks in \u00a73.4 for explaining social science constructs and applying\npsychological theories to restructure text.\n1 To elaborate, BIG-Bench, among its 200 tasks, has some overlap in figurative language, humor, emotion,\nempathy, and toxicity detection, but it does not cover dialect, discourse relations, character tropes, event\ndetection, ideology, misinformation, persuasion, politeness, power relations, semantic change, or stance.\nBIG-Bench covers few document-level analyses and no conversation-level analysis. We are the first to run\nextensive experiments to understand patterns of LLM performance on tasks critical to social scientists.\n6\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\n3.1 Utterance-Level Classification\nAn utterance is a unit of communication produced by a single speaker to convey a single\nsubject, which may span multiple sentences (Bakhtin 2010). CSS researchers can use\nutterance data to study linguistic phenomena like the syntax of dialect, the semantics of\nfigurative language, or the pragmatics of humor. Utterance-level analysis also reflects\nhuman states like emotion and communicative intent, or stable traits like stance and\nideology (Evans and Aceves 2016). We evaluate LLMs on utterance classification tasks\nfor dialect, hate speech, figurative language, emotion, humor, misinformation, ideology,\npersuasion, semantic change, and stance classification.\n3.1.1 Dialect Features. Linguistic feature detection is critical to the study of dialects\n(Eisenstein, Smith, and Xing 2011) and ideolects (Zhu and Jurgens 2021a), with numer-\nous applications in sociolinguistics, education, and the sociology of class and commu-\nnity membership (see \u00a72.3). These features can be used to study the sociolinguistics of\nlanguage change (Kulkarni et al. 2015; Hamilton, Leskovec, and Jurafsky 2016b) or the\nlinguistic biases in educational assessments (Craig and Washington 2002) and online\nmoderation (Sap et al. 2019). The utterance is an appropriate level of analysis here\nbecause syntactic and morphological features are all defined on subtrees of the sentence\nnode (Ziems et al. 2023; Eisenstein et al. 2023).\nWe evaluate on the Indian English dialect feature detection task of Demszky et al.\n(2021) because this is one of the only available datasets to be hand-labeled by a domain\nexpert. Additionally, Indian English is the most widely-spoken low-resource variety of\nEnglish, so the domain is representative. The task is to map utterances to a set of 22\ngrammatical features: i.e., a lack of inversion in wh-questions, the omission of copula\nbe, or features related to tense and aspect like the habitual progressive, found in Indian\nvarieties of English. For example, the sentence \u201cTwo years I stayed alone\u201d exemplifies\nPreposition Omission.\n3.1.2 Emotions. Emotion detection, the cornerstone of affective computing (Picard 2000),\nis highly relevant to psychology and political science, among other disciplines, since sta-\nble emotional patterns in-part define an individual\u2019s personality, and targeted emotions\noutline the political stances she has. Additional application domains for the task include\nemotional contagion (Bail 2016) and human factors behind economic markets (Bollen,\nMao, and Zeng 2011; Nguyen and Shirai 2015).\nExpert-labeled emotion detection datasets are not common. We evaluate emotion\ndetection with weakly labeled Twitter data from Saravia et al. (2018), which uses\nPlutchik\u2019s 8 emotional categories: anger, anticipation, disgust, fear, joy, sadness, surprise,\nand trust. For example, the following sentence would express fear:\nI started the steroids on Saturday and I had some really bad side effects, like my eyes\nstarted feeling weird.\nPlutchik\u2019s model is one of the three most recognized discrete emotion models, and it is\nalso used in our later Emotion Summarization Task (\u00a73.4.1).\n3.1.3 Figurative Language. Figurative expressions are where the speaker meaning dif-\nfers from the utterance\u2019s literal meaning. Recognizing figurative language is a first\nstep in understanding literary content (Jacobs and Kinder 2018) and political texts\n7\nComputational Linguistics\nVolume 50, Number 1\n(Huguet Cabot et al. 2020), detecting hate speech (Lemmens, Markov, and Daelemans\n2021) and identifying mental health self-disclosure (Iyer et al. 2019).\nWe use the FLUTE (Chakrabarty et al. 2022) benchmark because it is, at this time, the\nmost comprehensive, with examples from many prior datasets (Chakrabarty et al. 2021;\nSrivastava et al. 2023; Stowe, Utama, and Gurevych 2022). FLUTE contains 9k premise\nsentences, each paired to a hypothesis with figurative language:\npremise: I said, work independently and come up with some plans.\nhypothesis: I said, put your heads together and come up with some plans.\nThe classification task is to recognize whether the figurative sentence contains\n(1) sarcasm (Joshi, Bhattacharyya, and Carman 2017), (2) simile (Niculae and Danescu-\nNiculescu-Mizil 2014), (3) metaphor (Gao et al. 2018), or (4) an idiom (Jochim et al. 2018).\nIn the above example, the hypothesis contains the idiom \u201cput your heads together.\u201d\n3.1.4 Hate Speech. Hate speech is language that disparages a person or group on the\nbasis of protected characteristics like race. Beyond the societal importance of detecting\nand mitigating hate speech, this is a category of language that is salient to many social\nscientists. By not only detecting, but also systematically understanding hate speech, po-\nlitical scientists can track the rise of hateful ideologies, and sociologists can understand\nhow these hateful ideas diffuse through a network and influence social movements.\nThus we evaluate on the more nuanced task of fine-grained hate speech taxonomy\nclassification from Latent Hatred (ElSherief et al. 2021). This task requires models to\ninfer a subtle social taxonomy from the coded or indirect speech of U.S. hate groups.\nUtterances should be classified with one of six domain-specific categories: incitement to\nviolence, inferiority language, irony, stereotypes and misinformation, threatening and intimi-\ndation language, and white grievance. For example, the following sentence contains white\ngrievance: \u201cjewish harvard professor noel ignatiev wants to abolish the white race.\u201d\n3.1.5 Humor. Humor is a rhetorical (Markiewicz 1974) and literary device (Kuipers 2009)\nthat modulates social distance and trust (Sherman 1988; Graham 1995; Kim, Lee, and\nWong 2016). However, different audiences may perceive the same joke differently. In the\nstudy of sociocultural variation, communication, and bonding, humor detection will be\nof prime interest to sociologists and social psychologists, as well as to literary theorists\nand historians. Computational social scientists have effectively detected punchlines\n(Mihalcea and Strapparava 2005; Ofer and Shahaf 2022) and predicted audience laugh-\nter (Chen and Soo 2018), demonstrating the computational tractability of the domain.\nOur evaluation uses a popular dataset from Weller and Seppi (2019) to focus on\nbinary humor detection across a wide range of joke sources, from Reddit\u2019s r/Jokes, a\nPun of The Day website, and a set of short jokes from Kaggle, summing to \u223c16K jokes.\n3.1.6 Ideology. A speaker\u2019s subtle decisions in word choice and diction can betray their\nbeliefs and the political environment to which they belong (Jelveh, Kogut, and Naidu\n2014). While political scientists care most about identifying the underlying ideologies\nand partisan organizations behind these actors (\u00a72.4), sociolinguists can study the cor-\nrelation between language and social factors.\nWe evaluate ideology detection on the Ideological Books Corpus (Gross et al. 2013)\nfrom Iyyer et al. (2014), which contains 2,025 liberal sentences, 1,701 conservative sen-\ntences, and 600 neutral sentences. The corpus was designed to disentangle a speaker\u2019s\n8\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\noverall partisanship from the particular ideological beliefs that are reflected in an indi-\nvidual utterance. Thus labels reflect perceived ideology according to annotators and not\nthe speaker\u2019s ground truth partisan affiliation. For example, one sentence associated\nwith a strongly conservative ideology is: \u201cthe feminist movement, with its mockery of mar-\nriage and demands for absolute sexual freedom... was a frontal assault on the meritocracy and\nthe traditional family.\u201d\n3.1.7 Misinformation. Misinformation is both a political and social concern as it can\njeopardize democratic elections, public health, and economic markets. The effort to\ncombat misinformation is multi-disciplinary (Lazer et al. 2018), and it depends on\nreliable misinformation detection tools.\nWe evaluate on the Misinfo Reaction Frames corpus (Gabriel et al. 2022), a dataset of\n25k news headlines with fact checked labels for the accuracy of the related news articles\nabout COVID-19, climate change, or cancer. Models perform binary misinformation\nclassification on news article headlines alone, which the authors found was a tractable\ntask for fine-tuned models. For example, an article with the headline \u201cWhite House Ousts\nTop Climate Change Official\u201d is marked as likely to contain misinformation.\n3.1.8 Persuasion. Persuasion is the art of changing or reinforcing the beliefs of others.\nUnderstanding persuasive strategies is central to behavioral economics and the psychol-\nogy of advertising and propaganda (Martino et al. 2020). Utterances are a natural unit\nfor the analysis of individual persuasive strategies, which may be combined in dialogue\nfor an overall persuasive effect (c.f. \u00a73.2.3).\nWhile multi-modal persuasion detection tasks exist, we focus on the popular text-\nbased persuasion dataset, Random Acts of Pizza (RAOP; Althoff, Danescu-Niculescu-\nMizil, and Jurafsky 2014), where Reddit users attempt to convince community members\nto give them free food. This dataset was labeled by Yang et al. (2019a) with a fine-grained\npersuasive strategy taxonomy based on Cialdini (2003) that includes Evidence, Impact,\nPoliteness, Reciprocity, Scarcity, and Emotion. The task objective is to classify utterance-\nlevel RAOP requests according to this 6-class taxonomy. An example of an Evidence\nsentence is \u201cThere is a Pizza Hut and a Dominos near me,\" since it provides concrete facts\nrelevant to the request. An example of Scarcity is \u201cI haven\u2019t had a meal in two days.\u201d\n3.1.9 Stance. Although stance detection can be formalized in different ways, the most\ncommon task design is for models to determine whether a text\u2019s author is in favor\nof a target view, against the target, or neither. With this formulation, sociologists can\nunderstand consensus and disagreement in social groups, psychologists can measure\ninterpersonal attachments, network scientists can build signed social graphs, political\nscientists can track the views of a voter base or the policies of candidates, historians\ncan plot shifting opinions, and digital humanities researchers can quickly summarize\nnarratives via character intentions and goals.\nWe evaluate stance detection on the earliest and most established SemEval-2016\nStance Dataset (Mohammad et al. 2016), which contains 1,250 tweets and their asso-\nciated stance towards six topics: atheism, climate change, the feminist movement, Hillary\nClinton, Donald Trump and the legalization of abortion. Stance is given as favor, against, or\nnone. For zero-shot experiments, we use the test set where the target is Donald Trump\nfor all evaluations. An example tweet against Donald Trump is as follows:\n@realDonaldTrump needs to learn when to stop talking. You are making it worse\nDonald... so much worse.\n9\nComputational Linguistics\nVolume 50, Number 1\n3.1.10 Semantic Change. In addition to its more stable features, researchers can plot\nthe change of language over time for a fixed community. Semantic change detection\ncan serve as a proxy measure for the spread and change of culture (Kirby, Dowman,\nand Griffiths 2007), both on the internet (Eisenstein 2012; Eisenstein et al. 2014) and in\nhistorical archives (Mihalcea and Nastase 2012; Kim et al. 2014; Kulkarni et al. 2015;\nRudolph and Blei 2018)2.\nWe evaluate LLMs as binary word-sense discriminators using the popular Temporal\nWord-in-Context benchmark (TempoWiC; Pilehvar and Camacho-Collados 2019). Tem-\npoWiC measures the core capability of drawing discrete boundaries between word-level\nsemantics. Given two sentences with the same lexeme, the task is binary classification\nwith positive indicating both sentences use the same sense of the word and negative\nindicating different senses of the word. For example, consider the different senses of the\nword \u2018impostor\u2019 in the following texts.\ntext1: Having a rough start to my doctorate program in both the student and teacher\nroles and feel down and ashamed. I spoke to faculty and know how to move forward,\nbut while they believe in me I find it hard to believe in myself. How do you fight\nimpostor syndrome @AcademicChatter\ntext2: laughed so hard running from impostor friend around the lab table that I gave\nmyself an headache lmao what a good day\nA perfect classifier for this task can be used to cluster all usage of a surface-form into\nsense groups using pairwise comparison.\n3.2 Conversation-Level Classification\nConversations are multi-party exchanges of utterances. They are critical units for analy-\nsis in the social sciences (Hutchby and Wooffitt 2008; Silverman 1998; Sacks 1992), since\nthey richly reflect social relationships (Evans and Aceves 2016) \u2014 a key factor that was\nmissing in utterance-level analysis. Sociological frameworks like ethnomethodology\n(Garfinkel 2016) focus particularly on conversations. The tasks in this section are drawn\nlargely from the ConvoKit toolkit of Chang et al. (2020).\n3.2.1 Discourse Acts. Discourse acts are the building blocks of conversations and are\nthus relevant to conversation analysis in sociology, genre analysis in literature, prag-\nmatics, and ethnographic studies of speech communities (see Paltridge and Burton for\nexample). Some popular discourse act taxonomies like DAMSL (Stolcke et al. 2000) and\nDiAML (Bunt et al. 2010) can have as many as 40 categories, tailored to spoken commu-\nnication. We use Zhang, Culbertson, and Paritosh (2017)\u2019s simpler and more focused\n9-class taxonomy since it was designed to cover online text conversations\u2014the focus\nof CSS research. The taxonomy includes questions, answers, elaborations, announcements,\nappreciation, agreements, disagreements, negative reactions, and humor.\nWe evaluate on the Coarse Discourse Sequence Corpus (Zhang, Culbertson, and\nParitosh 2017). The model input is a comment from a Reddit thread, along with the\nutterance to which the comment is responding. For example,\n2 Additional works in this area can be found under the Workshop on Computational Approaches to\nHistorical Language Change\n10\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\n[userABC]: So i went thinking to myself this fine day \u00a8hey lets check out Levetihan\u00e4nd\nthen i found out that this DLC does not appear in my Origin store...\n[userXYZ]: Did you go into ME3 game and access \u00a8downloadable content\u00a8?\nThe expected output is the category from the above 9-class taxonomy which best de-\nscribes the comment\u2019s speech act: Question in the above example.. Since Announcements\nand Negative reactions have fewer than 10 examples total in the dataset, they are omitted\nfrom our evaluation along with the catch-all Other category.\n3.2.2 Empathy. Since the early days of internet access, users have looked to internet com-\nmunities for support (Preece 1998). Thus web communities can provide CSS researchers\nwith empathetic communication data in naturalistic settings (Pfeil and Zaphiris 2007;\nSharma et al. 2020). By better understanding community-specific affordances (Zhou\nand Jurgens 2020) and the most common triggers for empathetic responses (Buechel\net al. 2018; Omitaomu et al. 2022), CSS can reciprocally inform the design of empathetic\ncommunities (Coulton et al. 2014; Taylor et al. 2019), as well as community-specific tools\nlike counseling dialogue systems (Sharma et al. 2021; Ma et al. 2020).\nUnderstanding is the first step towards building more effective online mental\nhealth resources, and this motivates our evaluation on the TalkLife dataset of Sharma\net al. (2020), a clinically-motivated empathy detection dataset. The paper\u2019s EPITOME\nmeasures empathy using a multi-stage labeling scheme. First, a listener communicates\nan Emotional Reaction to describe how the seeker\u2019s disclosure makes the listener feel.\nThen the listener offers an Interpretation of the emotions the seeker is experiencing.\nFinally, the listener moves into Exploration, or the pursuit of further information to\nbetter understand the seeker\u2019s situation. Clinical psychologists labeled the listener\u2019s\neffectiveness at each stage of a listener\u2019s top-level reply. Here we focus on Exploration, as\nprior work has shown open-questions to be especially effective for peer-support (Shah\net al. 2022). Given a seeker\u2019s post and a top-level listener\u2019s reply, we classify whether the\nlistener offered: Strong Exploration (specific questions about the seekers situation), Weak\nExploration (general questions), or No Exploration. Consider the example conversation:\nSeeker: I spent today either staring blankly at a computer screen or my phone. Was too\nhurt to do anything today, really.\nResponse: I wish I even had the will to play games. For me it\u2019s excessive daydreaming.\nThe above response is an example of No exploration.\n3.2.3 Persuasion. In \u00a73.2.3, we considered utterance-level analysis of fine-grained per-\nsuasive strategies. However, social scientists are also interested in the overall persuasive\neffect that one speaker has on another through sequences of rhetorical strategies in\ndialogue (Shaikh et al. 2020). Persuasive outcomes are particularly important for the\npolitical science of successful campaigns (Murphy and Shleifer 2004) and the sociology\nof idea propagation and social movements (Stewart, Smith, and Denton Jr 2012).\nWe evaluate our persuasion prediction task on the Winning Arguments Corpus (Tan\net al. 2016), which contains 3,051 conversations from r/ChangeMyView in which the\npersuader tries to convince the persuadee to change their mind. Models receive as input\nthe reply thread (starting from a top-level comment) and perform binary prediction on\nwhether the persuadee awarded the persuader a \u2018delta\u2019 for a successful argument: If\n11\nComputational Linguistics\nVolume 50, Number 1\nyou were the original poster, would this reply convince you? Consider this example of\nan unsuccessful argument by UserA below:\n[UserA]: \u201cRight on red\u201d, when it\u2019s allowed, is primarily because you\u2019re going from the\ninnermost lane TO the intersecting innermost lane... this part of the state is infamous\nfor a**hat drivers blocking people from turning, changing lanes, etc... could you\nimagine the chaos? :D\n[UserB]: Er, the image I posted, I found on Google images. It may triple the number of\nlanes to be concerned about, but one of them shouldn\u2019t usually be a problem if people\nstay in their lanes. And the other two, you still have to look in the same direction.\"\n3.2.4 Power and Status. Sociologists, political scientists, and online communities re-\nsearchers are interested in understanding hierarchical organizations, social roles, and\npower relationships. Power is related to control of the conversation (Prabhakaran, Reid,\nand Rambow 2014) and power dynamics shape both behavior and communication.\nSpecifically, text analysis can uncover power relationships in the degree to which one\nspeaker accommodates to the linguistic style of another (Danescu-Niculescu-Mizil et al.\n2012). We anticipate that this task is tractable for LLMs.\nWe evaluate on the Wikipedia Talk Pages dataset from Danescu-Niculescu-Mizil\net al. (2012). Conversations are drawn from the debate forums regarding Wikipedia edit\nhistories, and power is a binary label describing whether or not the Wikipedia editor is\nan administrator. All models are given an editor\u2019s entire comment history from the Talk\nPages, and the objective is binary classification. In the following example, EditorA uses\na high degree of politeness and hedging langauge, which indicates that he is not in a\nposition of power:\n[EditorA]: That\u2019s odd. Somehow, I came across one of that user\u2019s edits, though I believe\nit was on recent changes. As you can see, most of the older edits are vandalism, but I\nguess due to the time that wouldn\u2019t warrant much of a block. I don\u2019t know how I\nhappened to come across that since it\u2019s so old.\n[EditorA]: That could be the case. I\u2019ve seen a few of those tonight.\n3.2.5 Toxicity Prediction. Toxicity is a major area of social research in online com-\nmunities, as online disinhibition (Suler 2005) makes antisocial behaviour especially\nprevalent (Cheng, Danescu-Niculescu-Mizil, and Leskovec 2015). Predictive models can\nbe used to understand the early signs of later toxicity (Cheng et al. 2017) for downstream\ncausal analysis on the evolution of toxicity (Mathew et al. 2020) and the effectiveness of\nintervention methods (Kwak, Blackburn, and Han 2015). Even without interpretable\nfeatures, a predictive system can serve causal methods as a propensity score.\nUsing the Conversations Gone Awry corpus (Zhang et al. 2018), we investigate\nwhether LLMs can predict future toxicity from early cues. As context, the model takes\nthe first two messages in a conversation between Wikipedia users. The model should\nmake a binary prediction whether or not the Wikipedia conversation will contain toxic\nlanguage at any later stage. For example:\n[UserA]: I have removed recent edition of pappe to the lead though Pappe view might\nnotable currently without attribution and proper context of other views it WP:NPOV\nviolation.\n[UserB]: In fact, Pappe is already mentioned twice in the proper place.\n12\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nThe conversation above contains overt confrontation that will later devolve into toxicity.\n3.2.6 Politeness. Before overt toxicity is evident in a community, researchers can mea-\nsure its health and stability according to members\u2019 adherence to politeness norms. Polite\nmembers can help communities grow and retain other valuable members (Burke and\nKraut 2008), while rampant impoliteness in a community can foreshadow impending\ntoxicity (Andersson and Pearson 1999). Text-based politeness measures also reflect other\nsocietal factors that we explore in this work, like gender bias (Herring 1994; Ortu\net al. 2016, \u00a73.1.4), power inequality (Danescu-Niculescu-Mizil et al. 2013, \u00a73.2.4), and\npersuasion (Shaikh et al. 2020, \u00a73.1.8).\nWe evaluate on the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al.\n2013). The dataset is foundational in the computational study of politeness and its\nrelation to other social dynamics. The corpus contains requests made by one Wikipedia\ncontributor to another. For example,\nI am looking for help improving the dermatology content on Wikipedia. Would you be\nwilling to help, or do you have any friends interested...\nEach request is classified into one of three categories, Polite, Neutral, or Impolite, accord-\ning to Mechanical Turk annotators\u2019 interpretation of workplace norms (the example\nabove is Polite). High zero-shot performance on this task will strongly indicate a model\u2019s\nbroader ability to recognize conversational social norms.\n3.3 Document-Level Classification\nDocuments provide a complementary view for social science. Like conversations, doc-\numents can encode sequences of ideas or temporal events, as well as interpersonal\nrelationships not present in isolated utterances. Unlike the dyadic communication of\na conversation, a document can be analyzed under a unified narrative (Piper, So, and\nBamman 2021). Thus for our purposes, a document is a collection of utterances that\nform a single narrative. Our document-level classification tasks cluster around media,\nwhich has been the subject of content analysis in the social sciences since the time of\nMax Weber in 1910. In this section, we focus on computational tools for content analysis\n(Berelson 1952) to code media documents for their underlying ideological content (\u00a73.3.3),\nthe events they portray (\u00a73.3.1, \u00a73.3.2), as well as the agents involved and the specific roles\nor character tropes they exhibit (\u00a73.3.4).\n3.3.1 Event Detection. Following a massive effort to digitize critical documents, social\nscientists depend on event extraction to automatically code and organize these docu-\nments into smaller and more manageable units for analysis. Events are the \u201cbuilding\nblocks\u201d from which historians construct theories about the past (Sprugnoli and Tonelli\n2019); they are the backbone of narrative structure (Chambers and Jurafsky 2008). Event\ndetection is the first step in the event extraction pipeline. Hippocorpus (Sap et al. 2020b)\nis a resource of 6,854 stories that were collected from crowdworkers and tagged for\nsentence-level events (Sap et al. 2022) . Events can be further classified into minor or\nmajor events, as well as expected or unexpected. We evaluate on the simplest task:\nbinary event classification at the sentence level. For example:\nA: Four months ago, I had a big family reunion.\nB: We haven\u2019t had one in over 20 years.\n13\nComputational Linguistics\nVolume 50, Number 1\nC: This was a very exciting event.\nD: I saw my Grandma who said I liked great as ever.\nThe above lines A and D denote new events.\n3.3.2 Event Argument Extraction. Where event detection was concerned with identify-\ning event triggers, event argument extraction is the task of filling out an event template\naccording to a predefined ontology, identifying all related concepts like participants in\nthe event, and parsing their roles. Historians, political scientists, and sociologists can use\nsuch tools to extract arguments from sociopolitical events in the news and historical text\ndocuments, and to understand social movements (H\u00fcrriyeto\u02d8glu et al. 2021). Economists\ncan use event argument extraction to measure socioeconomic indicators like the unem-\nployment rate, market volatility, and economic policy uncertainty (Min and Zhao 2019).\nEvent argument extraction is also a key feature of narrative analysis (Sims, Park, and\nBamman 2019), as well as in the wider domains of legal studies (Shen et al. 2020), public\nhealth (Jenhani, Gouider, and Said 2016), and policy.\nWikiEvents (Li, Ji, and Han 2021) is a document-level event extraction benchmark\nfor news articles that were linked from English Wikipedia articles. WikiEvents uses\nDARPA\u2019s KAIROS ontology with 67 event types in a three-level hierarchy. For example,\nthe Movement.Transportation event has the agentless Motion subcategory and an\nagentive Bringing subcategory. Both include a Passenger, Vehicle, Origin, and\nDestination argument, but only the agentive Bringing has a Transporter agent.\nKAIROS\u2019s event argument ontology is richer and more versatile than the commonly\nused ACE ontology, which only has 33 types of events. An example of this task is to\ntake the following document\nThe Taliban <tgr>killed <tgr>more than 100 members of the Afghan security forces\ninside a military compound in central Maidan Wardak province on Monda...\nand produce the following structured output\n{\u2019Victim\u2019: \u2019members\u2019, \u2019Place\u2019: \u2019undefined\u2019, \u2019Killer\u2019: \u2019The Taliban\u2019,\n,\u2192\n\u2019MedicalIssue\u2019: \u2019undefined\u2019}\n3.3.3 Ideology. CSS is extremely useful for understanding and quantifying real and\nperceived political differences. For a variety of specific phenomena (Amber et al. 2013;\nBaly et al. 2018; Roy and Goldwasser 2020; Luo, Card, and Jurafsky 2020b; Ziems and\nYang 2021), this takes the form of gathering articles from across the political spectrum,\nprocessing each one further for a phenomenon of interest, and evaluating the relative\ndifferences for the articles from different ideological groups. The first step in such stud-\nies is to separate articles according to the overarching political ideology they represent.\nWe evaluate ideology detection on the Article Bias Corpus from Baly et al. (2020),\nwhich collects a set of articles from media sources covering the United States of America\nand labels them according to Left, Right, and Centrist political bias. Unlike the task of\nutterance-level ideology prediction (\u00a73.1.6), this task provides an entire news article as\ncontext. This tests the ability of the model to understand the relationship that a sequence\nof stances taken across an entire article might have with political leaning. For example,\none Left-leaning article in this dataset contains the strongly-indicative phrase: \u201cit was\nhard not to think about the insularity and cossetting the super-wealthy enjoy,\u201d and then goes\non to talk at length about former LA Clippers owner Donald Sterling. In other articles,\n14\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\npolitical views are more diffuse and less starkly concentrated into particular phrases\nor slogans. Still, each article must be classified into exactly one of the three ideological\ncategories above.\n3.3.4 Roles and Tropes. Social roles are defined by expectations for behavior, based on\nsocial interaction patterns (Yang et al. 2019b). Similarly, personas are simplified models\nof personality (Grudin 2006), like a trope that a character identifies within a movie. The\nability to infer social roles and personas from text has immediate applications in the\npsychology of personality, the sociology of group dynamics, and the study of agents\nin literature and film. These insights can help us understand stereotypical biases and\nrepresentational harms in media (Blodgett et al. 2020). Downstream applications also\ninclude narrative psychology (Murray et al. 2015), economics, political polarization, and\nmental health (Piper, So, and Bamman 2021).\nOthers have considered character role labeling for narratives (Jahan, Mittal, and\nFinlayson 2021) and news media (Gomez-Zara, Boon, and Birnbaum 2018). We evaluate\nthis task with the CMU Movie Corpus dataset from Bamman, O\u2019Connor, and Smith\n(2013) as it was extended and modified by Chu, Vijayaraghavan, and Roy (2018) to\ninclude character trope labels and IMBD character quotes. The character trope classifica-\ntion task involves identifying from a character\u2019s quotes alone which of 72 movie tropes\nthat characters identity best fits; e.g., the coward or the casanova. The following example\nquotations are from an absent-minded professor:\nNow, THIS makes any fabric instantly impervious. Dirt proof, stain proof... Ouch! And\nbullet proof! It\u2019s still not perfected yet! It\u2019s hell on the dry-cleaning bill.\nThis baby is the ultimate corrosive. I call it - DON\u2019T TOUCH IT! - I call it\n\u201chydrochloricdioxynucleocarbonium\u201d. Well, the name needs work. But it\u2019ll eat through\na Buick! OR -\n3.4 Generation Tasks\nRegarding RQ4 Functionality, we want to understand whether LLMs are best suited\nto classify taxonomic social science constructs from text, or whether these models are\nequally if not better suited for generative explanations and summaries. This section\ndescribes our natural language generation tasks, where LLMs might be used to summa-\nrize relevant aspects (\u00a73.4.1), elucidate the hidden social meaning behind a text (\u00a73.4.2\n\u2014 \u00a73.4.4) or implement social theory by stylistically restructuring an utterance (\u00a73.4.5).\n3.4.1 Emotion-Specific Summarization. Prior work has already demonstrated LLMs\u2019\nskill at generic summarization tasks (Goyal, Li, and Durrett 2022; Qin et al. 2023).\nHere, we consider a more domain-specific task, aspect-based summarization. The key\nidea of aspect-based summarization is that different elements of a document will be\nrelevant to different users. This is especially true for social scientists and other domain\nspecialists. For example, scientists who study population-level emotional responses to\ncrisis events will need to know not only which emotions are represented in text (i.e.,\nemotion detection; \u00a73.1.2), but also, in brief, what specific experiences triggered such\nemotions. The scientist may not have highly focused queries like in QA tasks, but this\nuse still demands summaries about broad subtopics or themes (Ahuja et al. 2022).\n15\nComputational Linguistics\nVolume 50, Number 1\nWe use the COVIDET dataset of Zhan et al. (2022), which contains 1,883 Reddit posts\nfrom the COVID-19 pandemic. Given a post and one of Plutchik\u2019s target emotions, the\ntask is to summarize from the post what triggered the author to feel the target emotion.\n3.4.2 Figurative Language Explanation. Our interests in figurative language are covered\nin \u00a73.1.3, where we introduce the FLUTE dataset. FLUTE contains 9k (literal, figura-\ntive) sentence pairs with either entailed or contradictory meanings. The goal of the\nexplanation task is to generate a sentence to explain the entailment or contradiction.\nFor example, the figurative sentence \u201cshe absorbed the knowledge\u201d entails the literal\nsentence \u201cshe mentally assimilated the knowledge\u201d under the following explanation:\n\u201cto absorb something is to take it in and make it part of yourself.\u201d\n3.4.3 Implied Misinformation Explanation. Both scientific understanding and real-\nworld intervention strategies depend on more than black-box classification. This moti-\nvates our implied statement generation task, which is specified in the Misinfo Reaction\nFrames corpus (Gabriel et al. 2022) as covered in \u00a73.1.7. Models take the headline of a\nnews article and generate the underlying meaning of the headline in plain English. This\nis called the writer\u2019s intent. Consider, for example, the misleading headline, \u201cWearing\na face mask to slow the spread of COVID-19 could cause Legionnaires\u2019 disease.\u201d Here, the\nannotator wrote that the writer\u2019s intent was to say \u201cwearing masks is dangerous; people\nshouldn\u2019t wear masks.\u201d\n3.4.4 Social Bias Inference. While hate speech detection focuses on the overall harmful-\nness of an utterance, specific types of hate speech are targeted towards a demographic\nsubgroup. To this end, the Social Bias Inference Corpus (SBIC; Sap et al. 2020a) consists\nof 34K inferences, where hate speech is annotated with free-text explanations. Impor-\ntantly, explanations highlight why a specific subgroup is targeted. For example, the\nsentence \u201cWe shouldn\u2019t lower our standards just to hire more women.\u201d implies that \u201cwomen\nare less qualified.\u201d To model these explanations, Sap et al. (2020a) treat the task as a\nstandard conditional generation problem. We mirror this setup to evaluate LLMs.\n3.4.5 Positive Reframing. NLP can help scale mental health and psychological counsel-\ning services by training volunteer listeners and teaching individuals the techniques of\ncognitive behavioral therapy (CBT; Rothbaum et al., 2000), which is used to address\nmental filters and biases that perpetuate anxiety and depression. Positive reframing\nis a sequence-to-sequence task which translates a distorted negative utterance into\na complementary positive viewpoint using CBT strategies without contradicting the\noriginal speaker meaning. Take the example source sentence:\nAlways stressing and thinking about loads of things at once need I take it one at a time\noverload stressed need to rant.\nUsing the growth and neutralizing strategies, the author can reframe this thought more\npositively as follows:\nLoads of things on my mind, I need to make a list, prioritise and work through it all\ncalmly and I will feel much better.\n16\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\n4. Evaluation Methods\n4.1 Model Selection and Baselines\nOur goal is to evaluate LLMs in zero-shot settings through prompt engineering\n(\u00a74.2) and to identify suitable model architectures, sizes, and pre-training/fine-tuning\nparadigms for CSS research (RQ 1,2). We choose FLAN-T5 (Chung et al. 2022) as\nan open-source model with strong zero-shot and few-shot performance. Although it\nfollows a standard T5 encoder-decoder architecture, FLAN\u2019s zero-shot performance\nis due to its instruction fine-tuning over a diverse mixture of sequence to sequence\ntasks. The added benefit is that FLAN-T5 checkpoints exist at six different sizes ranging\nfrom small (80M parameters) to XXL (11B) and UL2 (20B), allowing us to investigate\nscaling laws. Next, we consider OpenAI\u2019s GPT-3 (Brown et al. 2020; Zong and Kr-\nishnamachari 2022) including text-001, text-002 learning with instructions and\ntext-003, which is further learned from human preferences (RLHF) (Christiano et al.\n2017) series, and gpt-3.5-turbo (Qin et al. 2023; Gilardi, Alizadeh, and Kubli 2023),\nwhich is the conversation-based LLM trained through RLHF (Christiano et al. 2017).\nFinally we include GPT-4 (OpenAI 2023), which is a multimodal model that, at 1.7\ntrillion parameters, scales up the GPT-3 architecture by 1000\u00d7.\nTraditional supervised fine-tuned models can serve as baselines for each task.\nThese baselines are intended to provide a comparison point for the utility of LLMs for\nCSS, rather than providing a fair methodological comparison between approaches. For\nclassification tasks, we use RoBERTa-large (Liu et al. 2019) as the backbone model and\ntune hyperparameters based on F1 score on the validation set. For generation tasks,\nwe use T5-base (Raffel et al. 2020) as the backbone model and tune hyperparameters\nbased on average BLEU score on the validation set. We use a grid search to find\nthe most suitable hyperparameters including learning rate {5e-6, 1e-5, 2e-5,\n5e-5}, batch size {4, 8, 16, 32} and the number of epochs {1, 2, 3, 4}. Other\nhyperparameters are set to the defaults defined by the HuggingFace Trainer. We average\nresults across three different random seeds to reduce variance. These baselines will\nprove competitive in Table 3, matching or exceeding the best reported performances\nfrom the original publications in Event Surprisal, Event Argument Extraction, and the clas-\nsification of Emotions, Empathy, Figurative Language, Implicit Hate, Persuasion, Persuasion\nStrategies, Political Ideology, and Semantic Change. Still, it is important to note that greater\nperformances might be achievable by fine-tuning alternative architectures.\n4.2 Prompt Engineering\nOne strength of current LLMs is their ability to be \"programmed\" through natural lan-\nguage instructions (Brown et al. 2020). This capability has been further improved by\ntraining models to explicitly follow these instructions (Sanh et al.; Wang et al. 2022;\nChung et al. 2022; Ouyang et al. 2022). CSS tools can then be developed directly by\nsubject-matter experts using natural language instructions rather than explicit program-\nming language interpretations. In order to evaluate LLMs, each task requires a prompt\ndesigned to elicit the desired behavior from the model.\nThe author who is most familiar with the task writes an initial prompt for it\nbased on the task description and the design guidelines below. Then we generate four\nsemantically equivalent perturbations of that prompt using gpt-3.5-turbo as a zero-\nshot paraphrase model. All results are averaged across prompt perturbations to remove\ninstruction-based variance (Perez, Kiela, and Cho 2021; Zhao et al. 2021).\n17\nComputational Linguistics\nVolume 50, Number 1\nEffective Prompt Guideline\nReference\nGuideline Example\nWhen the answer is categorical, enumerate op-\ntions as alphabetical multiple-choice so that the\noutput is simply the highest-probability token\n(\u2018A\u2019, \u2018B\u2019).\nHendrycks et al. (2021)\n{$CONTEXT}\nWhich of the following describes\nthe above news headline?\n\u2190-\nA: Misinformation\n\u2190-\nB: Trustworthy\n\u2190-\n{$CONSTRAINT}\nEach option should be separated by a new line\n( \u2190- ) to resemble the natural format of online\nmultiple choice questions. More natural prompts\nwill elicit more regular behavior.\nInverse Scaling Prize\nTo promote instruction-following, give instruc-\ntions after the context is provided; then explic-\nitly state any constraints. Recent and repeated\ntext has a greater effect on LLM generations due\nto common attention patterns.\nChild et al. (2019)\n{$CONTEXT}\n{$QUESTION}\nConstraint: Even if you are\nuncertain, you must pick either\n\u201cTrue\u201d or \u201cFalse\u201d without using\nany other words.\nClarify the expected output in the case of uncer-\ntainty. Uncertain models may use default phrases\nlike \u201cI don\u2019t know,\u201d and clarifying constraints\nforce the model to answer.\nNo Existing Reference\nWhen\nthe\nanswer\nshould\ncontain\nmultiple\npieces of information, request responses\nin JSON format. This leverages LLM\u2019s famil-\niarity with code to provide an output structure\nthat is more easily parsed.\nMiniChain Library\n{$CONTEXT}\n{$QUESTION}\nJSON Output:\nTable 1\nLLM Prompting Guidelines to generate consistent, machine-readable outputs for CSS tasks.\nThese techniques can help solve overgeneralization problems on a constrained codebook, and\nthey can force models to answer questions with inherent uncertainty or offensive language.\nIn order to receive consistent, reproducible results we utilize a temperature of zero\nfor all LLMs. For models which provide probabilities directly, we constrain decoding to\nthe valid output classes.3 For other models, such as gpt-3.5-turbo, we use logit bias\nto encourage valid outputs during decoding.4 All other generation parameters are left\nat the default settings for each model.\nCSS Prompt Design Guidelines. CSS tasks often require models to make inferences about\nsubtext and offensive language. Additionally, CSS codebooks often project complex\nphenomena into a reduced set of labels. This raises challenges for the use of LLMs\nwhich have been refined for general use. When initially exploring LLM behavior, we\nfound that models would hedge in the case of uncertainty, refuse to engage with\noffensive language, and attempt to generalize beyond provided labels. While desirable\nin a general context, these behaviors make it difficult to use LLMs inside a CSS pipeline.\nTherefore, we built a set of guidelines in Table 1, drawn from both the literature and\nour own experience with non-CSS tasks as NLP researchers. We explicitly share these\nguidelines to help CSS practitioners control LLMs for their purposes. There is no claim\nthat the resulting prompts are optimally-engineered for each task; they instead provide\nreasonable approximations to the kinds of prompts a non-AI expert could design after\nconsidering established guidelines. By averaging our results over five prompt pertuba-\ntions, we reduce the variance in this approximation of standard CSS tool-use.\n3 Probability outputs for HuggingFace and GPT-3\n4 Logit Bias reference for gpt-3.5-turbo\n18\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nDataset\nSize\nClasses\nGeneration Tasks\n500\n-\nUtterance Level\nDialect\n266\n23\nPersuasion\n399\n7\nImpl. Hate\n498\n6\nEmotion\n498\n6\nFigurative\n500\n4\nIdeology\n498\n3\nStance\n435\n3\nHumor\n500\n2\nMisinfo\n500\n2\nSemantic Chng\n344\n2\nDataset\nSize\nClasses\nConversation Level\nDiscourse\n497\n7\nPoliteness\n498\n3\nEmpathy\n498\n3\nToxicity\n500\n2\nPower\n500\n2\nPersuasion\n434\n2\nDocument Level\nEvent Arg.\n283\n\u2013\nEvt. Surprisal\n240\n\u2013\nTropes\n114\n114\nIdeology\n498\n3\nTable 2\nDataset size and classes count across all selected CSS benchmarks. Datasets are sorted by class\ncount for each task category.\n4.3 Test Set Construction\nFor each task, we evaluate a class-stratified sample of at most 500 instances from the\ndataset\u2019s designated test set. If the designation is missing, we take the class-stratified\nsample from the entire dataset. Our sampled test sizes and class counts are in Table 2. All\ndatasets, prompts, and model outputs are released for future comparison and analysis.5\n4.4 Evaluation Metrics\nAutomatic Evaluation. Each dataset has a different structure with a different number of\nlabels (see Table 2), so the use of accuracy is not the most informative metric. Instead, we\nuse compute the macro F1 score for all classification and structured parsing tasks and\naverage over the 5 prompt perturbations. Since we mapped the label space for each task\nto an alphabetical list of candidate options and set the logit bias to favor these options\n(\u00a74.2), evaluation scripts use straightforward string-matching.\nFor high-variation domains like our generation tasks, on the other hand, word-\noverlap-based machine translation metrics like BLEU (Post 2018) are expected to\nhave low correlation with human quality judgments (Liu et al. 2016). Here, even\nembedding-similarity metrics like BERTScore (Zhang et al. 2020) may be insufficient\n(Novikova et al. 2017). Manual inspection revealed high-quality generation outputs,\nbut the BLEURT (Sellam, Das, and Parikh 2020) score reported zero semantic overlap,\nand variation in BLEU and BERTScores failed to follow any discernible patters with\nregards to model preference or scaling laws that we observed by manual inspection.\nFor generation tasks, human evaluation is strongly preferable (Santhanam and Shaikh\n2019), and we describe human evaluation in the following paragraphs.\n5 Data Directory of our Github Project\n19\nComputational Linguistics\nVolume 50, Number 1\nHuman Scoring Evaluation. To get a sense of the generation quality for each task, we\nrecruit a domain expert to blindly evaluate 100-400 model outputs. Evaluations are on\n1-5 Likert scales for 4 standard metrics from the NLG literature. Following Fabbri et al.\n(2021), we define these metrics as follows:\n\u2022 Faithfulness: The generation is consistent with the source document and with the defi-\nnition of the task.\n\u2022 Coherence: The generation is well-structured and well-organized. The generation is not\njust a heap of unrelated information, but forms a coherent body of information about a\ntopic. (Dang 2005)\n\u2022 Relevance: The generation includes only important information from the source docu-\nment; no redundancies or excess information.\n\u2022 Fluency: The generation has no formatting problems, capitalization errors or obviously\nungrammatical sentences (e.g., fragments, missing components) that make the text diffi-\ncult to read.\nAll experts are recruited and paid through the Upwork platform. Annotator back-\ngrounds and expertise are summarized in the bottom right pane of Table 6. For\nCOVIDET summarization, we recruit a former CDC health communication specialist\nwith a B.S. in Public Health and an M.S. in Health Education. For Misinformation, we\nenlist a Public Policy Graduate Student with a B.A. in Political Science. For Figurative\nLanguage, we hire a former writing expert at Grammarly with an M.F.A. For Social Bias,\nwe find a Graduate Student with a B.S. in Journalism. And for Positive Reframing, we\nhire a Nurse in Clinical Behavioral Health with a B.A. in Psychology.\nHuman Ranking Evaluation. Instead of scoring or rating target generations on a standard\nLikert scale, annotators can also rank the model explanations in terms of their faithfulness\nat describing the target construct. The ranking-style evaluation can be less variable than\nscoring for generation tasks (Harzing et al. 2009; Belz and Kow 2010). We will use Social\nBias Frames as an example to illustrate the general setup for Human Ranking Evalua-\ntions. Here, the annotator reviews a hateful message and an associated hate target. Then\nthey review four Implied Statements generated by one of the OpenAI models or pulled\nfrom the SBIC\u2019s gold human annotations. They are asked to rank these statements from\nbest to worst according to how accurate the implied statement is at describing the hidden\nmessage from the hateful message. In this forced-choice ranking scheme, ties are not\nallowed, but we use a unanimous vote to determine when a given model outranks\nhuman performance. Unanimous vote flattens the variance for explanations of similar\nquality and reflects only significant differences in quality.\nPilot annotation proved that crowdworker evaluations can exhibit high variance\nand instability due to cultural and individual differences, as well as different interpre-\ntations of the task. Thus the authors served as blind annotators for this evaluation. Two\nauthors evaluated each task and unanimous voting determined the reported metrics.\n5. Classification Results\nTable 3 presents all zero-shot results for utterance, conversation, and document-level\ntasks. We use these results to answer Research Questions 1-3. The results suggest that\nLLMs are a viable tool for augmenting human CSS annotation. For classification tasks,\nresults show that larger, instruction-tuned open-source LLMs are preferable.\n20\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nData\nModel\nBaselines\nFLAN-T5\nFLAN\ntext-001\ntext-002\ntext-003\nChat\nRand Finetune Small\nBase\nLarge\nXL\nXXL\nUL2 Ada Babb.\nCurie\nDav.\nDavinci\nDavinci\nGPT3.5\nGPT4\nUtterance Level Tasks\nDialect\n3.3\n3.0\n0.2\n4.5\n23.4\n24.8\n30.3\n32.9\n0.5\n0.5\n1.2\n9.1\n17.1\n14.7\n11.7\n23.2\nEmotion\n16.7\n71.6\n19.8\n63.8\n69.7\n65.7\n66.2\n70.8\n6.4\n4.9\n6.6\n19.7\n36.8\n44.0\n47.1\n50.6\nFigurative\n25.0\n99.2\n16.6\n23.2\n18.0\n32.2\n53.2\n62.3\n10.0\n15.2\n10.0\n19.4\n45.6\n57.8\n48.6\n17.5\nHumor\n49.5\n73.1\n51.8\n37.1\n54.9\n56.9\n29.9\n56.8\n38.7\n33.3\n34.7\n29.2\n29.7\n33.0\n43.3\n61.3\nIdeology\n33.3\n64.8\n18.6\n23.7\n43.0\n47.6\n53.1\n46.4\n39.7\n25.1\n25.2\n23.1\n46.0\n46.8\n43.1\n60.0\nImpl. Hate\n16.7\n62.5\n7.4\n14.4\n7.2\n32.3\n29.6\n32.0\n7.1\n7.8\n4.9\n9.2\n18.4\n19.2\n16.3\n3.7\nMisinfo\n50.0\n81.6\n33.3\n53.2\n64.8\n68.7\n69.6\n77.4\n45.8\n36.2\n41.5\n42.3\n70.2\n73.7\n55.0\n26.9\nPersuasion\n14.3\n52.0\n3.6\n10.4\n37.5\n32.1\n45.7\n43.5\n3.6\n5.3\n4.7\n11.3\n21.6\n17.5\n23.3\n56.4\nSem. Chng.\n50.0\n62.3\n33.5\n41.0\n56.9\n52.0\n36.3\n41.6\n32.8\n38.9\n41.3\n35.7\n41.9\n37.4\n44.2\n21.2\nStance\n33.3\n36.1\n25.2\n36.6\n42.2\n43.2\n49.1\n48.1\n18.1\n17.7\n17.2\n35.6\n46.4\n41.3\n48.0\n76.0\nConversation Level Tasks\nDiscourse\n14.3\n49.6\n4.2\n21.5\n33.6\n37.8\n50.6\n39.6\n6.6\n9.6\n4.3\n11.4\n35.1\n36.4\n35.4\n16.7\nEmpathy\n33.3\n71.6\n16.7\n16.7\n22.1\n21.2\n35.9\n34.7\n24.5\n17.6\n27.6\n16.8\n16.9\n17.4\n22.6\n6.4\nPersuasion\n50.0\n33.3\n9.2\n11.0\n11.3\n8.4\n41.8\n43.1\n6.9\n6.7\n6.7\n33.3\n33.3\n53.9\n51.7\n28.6\nPoliteness\n33.3\n75.8\n22.4\n42.4\n44.7\n57.2\n51.9\n53.4\n16.7\n17.1\n33.9\n22.1\n33.1\n39.4\n51.1\n59.7\nPower\n49.5\n72.7\n46.6\n48.0\n40.8\n55.6\n52.6\n56.9\n43.1\n39.8\n37.5\n36.9\n39.2\n51.9\n56.5\n42.0\nToxicity\n50.0\n64.6\n43.8\n40.4\n42.5\n43.4\n34.0\n48.2\n41.4\n34.2\n33.4\n34.8\n41.8\n46.9\n31.2\n55.4\nDocument Level Tasks\nEvent Arg.\n22.3\n65.1\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n8.6\n8.6\n21.6\n22.9\n22.3\n23.0\nEvent Det.\n0.4\n75.8\n9.8\n7.0\n1.0\n10.9\n41.8\n50.6\n29.8\n47.3\n47.4\n44.4\n48.8\n52.4\n51.3\n14.8\nIdeology\n33.3\n85.1\n24.0\n19.2\n28.3\n29.0\n42.4\n38.8\n22.1\n26.8\n18.9\n21.5\n42.8\n43.4\n44.7\n51.5\nTropes\n36.9\n-\n1.7\n8.4\n13.7\n14.6\n19.0\n28.6\n7.7\n12.8\n16.7\n15.2\n16.3\n26.6\n36.9\n44.9\nTable 3\nZero-shot Classification Results across our selected CSS benchmark tasks. All tasks are\nevaluated with macro F-1, which is averaged across 5 prompt permutations for zero-shot\nmodels. Supervised baseline results are averaged over 3 random seeds. Best zero-shot models\nare in green . A dash indicates a model did not follow instructions.\n5.1 Viability (RQ1)\n5.1.1 Zero-Shot Viability. To understand the viability of LLMs as CSS tools, we ask\nif zero-shot LLMs match or exceed the reliability of human annotation. If the overall\nperformance is high and the expected agreement between humans and prompted mod-\nels is as high as that between humans alone, then we might expect LLMs to viably\naugment the human annotation process. According to one paradigm, an LLM can serve\nas just one of many human and AI labelers, and gold labels would be decided by ma-\njority vote across these independent annotations. According to another complementary\nparadigm, LLM pseudo-labels can be used with a small set of gold-labels to compute\nunbiased estimators in downstream regressions following methods like Design-based\nSemi-supervised Learning (DSL; Egami et al. 2023) and other .\nTable 3 shows the best zero-shot models achieve as high as 77.4 F1 on misin-\nformation detection. On this task, the best-performing FLAN-UL2 model achieves an\nagreement of \u03ba = 0.55 with gold labels (see Table 4), which is higher than the \u03ba = 0.51\ninter-human agreement reported by Gabriel et al. (2022) in their original paper. In fact,\nfor 8/17 tasks in Table 4, or 47% of classification tasks, models achieve moderate to\ngood agreement scores ranging from \u03ba = 0.40 to 0.65. These tasks also correspond\nto the highest absolute performances on Stance (76.0 F1) Emotion (70.8 F1), Figurative\nLanguage (62.3 F1) and utterance-level Ideology Classification (60.0 F1). In these cases of\nhigh viability, we recommend that CSS researchers consider the DSL and augmented-\nannotator paradigms. Such strong performances are on tasks that either have objective\n21\nComputational Linguistics\nVolume 50, Number 1\nDataset\nBest Model\nF1\n\u03ba\nAgreement\nUtterance-Level\nDialect\nflan-ul2\n32.9\n0.15\npoor\nEmotion\nflan-ul2\n70.8\n0.65\ngood\nFigurative\nflan-ul2\n62.3\n0.52\nmoderate\nHumor\ngpt-4\n61.3\n0.23\nfair\nIdeology\ndavinci-002\n60.0\n0.40\nmoderate\nImpl. Hate\nflan-ul2\n32.3\n0.20\nfair\nMisinfo\nflan-ul2\n77.4\n0.55\nmoderate\nPersuasion\ngpt-4\n56.4\n0.51\nmoderate\nSemantic Chng.\nflan-t5-large\n56.9\n0.14\npoor\nStance\ngpt-3.5-turbo\n72.0\n0.58\nmoderate\nDataset\nBest Model\nF1\n\u03ba\nAgreement\nConvo-Level\nDiscourse\nflan-t5-xxl\n50.6\n0.45\nmoderate\nEmpathy\nflan-t5-xxl\n35.9\n0.04\npoor\nPersuasion\ndavinci-003\n53.9\n0.14\npoor\nPoliteness\nflan-t5-xl\n59.2\n0.38\nfair\nPower\ngpt-4\n59.7\n0.26\nfair\nToxicity\ngpt-4\n55.4\n0.11\npoor\nDocument-Level\nIdeology\ngpt-4\n51.5\n0.51\nmoderate\nEvent Det.\ngpt-4\n23.0\nn/a\n-\nTropes\ngpt-4\n44.9\nn/a\n-\nTable 4\n(Acc.) Best model F1 scores. F1 scores above 70% are bolded. (\u03ba) Agreement scores between\nzero-shot model classification and human gold labels. Out of ten utterance-level tasks, six have\nat least moderate\nM and only two have poor agreement\nP . Three (50%) of the conversation\ntasks have at least fair agreement\nF .\nground truth (fact checking for misinformation) or have labels with explicit colloquial\ndefinitions in the pretraining data (emotional categories like anger are part of everyday\nvernacular; political stances are well-documented and explicit in online forums). Our\nqualitative error analysis in \u00a75.1.2 will show that here, models are less likely to default\nto neutral categories, and errors are more likely to come from annotation mistakes in\nthe gold dataset according to the author\u2019s own manual error analysis (see lower neutral\nand higher gold error in Figure 2).\nAre zero-shot models ready to label text out-of-the-box? Zero-shot results rarely\nexceed the carefully-tuned supervised RoBERTa baselines in Table 3. However, the\nbest observed performances here match that of classifiers used in published studies\non stance detection (67.8 F1; Zarrella and Marsh 2016), COVID-19 vaccination opinions\n(Cotfas et al. 2021), political opinions (Siddiqua, Chy, and Aono 2019), and debates (Lai\net al. 2020). In such scenarios, zero-shot models could offer a data-efficient alterna-\ntive to fine-tuned models by removing the need for expensive training sets. Humans\ncould focus all of their efforts on validating LLM outputs and tuning prompts (\u00a74.2)\nrather than coding unstructured text. However, we encourage practitioners to proceed\ncautiously, especially in sensitive domains, and we recommend human-in-the-loop\nmethods to mitigate bias and risk. See \u00a77.7 and \u00a77.8 for further discussion.\nIt is important to consider that LLM performance could be unusably poor for some\nCSS tasks. For a non-negligible subset of tasks we considered, LLMs have poor agree-\nment (5/17 = 29.5%), and here social scientists might not consider zero-shot annotation\naugmentation via LLMs. On these poor agreement tasks, LLM absolute performance\nis not significantly better than random guessing: see 56.9 F1 vs 50 Random on Semantic\nChange; 35.9 F1 vs 33.3 Random on Empathy; 53.9 F1 vs 50 Random on conversation-level\nPersuasion; and 55.4 F1 vs 50 Random on Toxicity. Some of these low-performance tasks\nlike Event Argument Extraction are structurally complex and may require additional en-\ngineering efforts. Others like Empathy and Tropes have challenging and subjective expert\ntaxonomies whose semantics differ from definitions learned in model pretraining. This\nis confirmed by our error analysis in Figure 2 where GPT3.5 often defaults to the neutral,\nmore colloquially recognizable label stereotype (64% of errors) rather than use a more\n22\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nFigurative\nEmotion\nPoliteness\nImplicit\n0\n20\n40\n60\n% Error Type\nGold/Plausible Error\nNeutral Error\nFigure 2\nBreakdown of Shared Error Types. For a representative subset of classification tasks, we\nconduct an analysis of up to 50 shared errors across evaluated models. We focus specifically on\nthe best performing model in a class (e.g. the best variant of FLAN models or the best OpenAI\nmodel). Plausible/gold errors occur when gold labels are incorrect or the model identifies a valid\nsecondary label. Neutral errors occur when a model over-predicts a category in a respective task\n(metaphor in Figurative; surprise in Emotion; neutral in Politeness, and stereotypical in Implicit).\ntaxonomy-specific label like white grievance. In \u00a75.1.3, we test if few-shot prompting can\nreduce misalignments between model and ground-truth definitions.\n5.1.2 Zero-Shot Error Analysis. For a representative subset of classification tasks, we\nconduct an analysis of shared errors across evaluated models. We focus specifically on\nthe best performing model in a class (e.g. the best variant of FLAN models or the best\nOpenAI model). Finally, in Figure 2 we break down the error types for gpt-3.5-turbo.\nFigurative Language. We sample all 29 cases in which every model was incorrect. In\njust under half of these cases (14/29), all models agreed on an incorrect answer, which\nwe call a unanimous error. Out of fourteen unanimous errors, the models were at least\npartially correct four times, which we call a plausible/gold error (see Figure 2). There was\none mistaken gold label and three cases of correctly-labeled similes nested inside the\npredicted sarcasm. Of the remaining ten unanimous errors, three were idioms mistaken\nas metaphors, and seven were similes classified with the more general metaphor label.\nFor humans, this is a common error, but for models, this is surprising, since similes\nshould have easy keyword signals \u201cas\u201d and \u201clike.\u201d The baseline method was likely able\nto exploit these signals to achieve a higher accuracy.\nIn 5 errors, all models disagreed and missed the intended sarcasm label. In another\n5 error cases, only UL2 and text-davinci-003 agreed on the correct label, but the\ndataset was mislabeled, with four idioms marked wrongly as metaphors and one simile\nmarked as an idiom. In the remaining 5 errors, ChatGPT showed a preference for the\nmost generic label and predicted metaphor.\nEmotion Recognition. We sample 50 cases where all models differed from the gold labels.\nUnlike Figurative Language, a minority of examples had the same mismatch across\nmodels (9/50). However, a closer analysis of individual errors yields a surprising re-\nsult: at least 18/50 examples across all evaluated models were judged as gold mislabels.\n23\nComputational Linguistics\nVolume 50, Number 1\nAdditionally, for FLAN-UL2 and ChatGPT, 17/50 and 15/50 predictions respectively\ncould be considered as valid\u2014even if they differed from the gold label.6\nMoving to true negatives, we observe that DV2 makes the most errors (28/50) that\ncannot be categorized as a gold mislabel, while UL2 (17/20) and ChatGPT (19/20) make\nsignificantly fewer. The distribution of errors differ across each model type: ChatGPT,\nfor example, over-labels with surprise: especially instances with a true gold label of Joy\n(8) or Love (5). On the other hand, UL2 mislabels Love as Joy frequently (9); and fear as\nSadness (4) or Surprise (4). Finally, davinci mislabels Sadness most frequently as Joy (9)\nor anger/love (3 each).\nPoliteness Prediction. We first visualize the per-category accuracy of the different best-\nperforming models (FLAN-T5-XL, Text-davinci-002, and ChatGPT). We observe\nthat: (1) The XL model tends to predict more polite labels. It is more accurate in terms of\nthe utterances that were polite and neutral with 70.4% and 62.0% accuracy. Most errors\ncome from impolite cases (with a 45.2% accuracy). (2) davinci-002 performs best in\njudging neutral utterances. davinci-002 is the most accurate for neutral utterances\n(82.9% accuracy) while making significantly more errors for polite and impolite utter-\nances (43.9% and 40.9% accuracy respectively). (3) ChatGPT performs worst in finding\nimpolite utterances while making more neutral predictions, with only 9.0% accuracy for\nthe impolite category, whereas it achieves 75.9% and 66.8% for neutral and polite cases.\nWe then consider the 81/498 cases where the three models are all making er-\nrors. We find that the three models make the same errors in most cases (54/81) and\ndavinci-002 models make errors more similar to ChatGPT (17/81 cases). Among\nthese common error cases, we observe that 79/81 cases are related to the 1st and\n2nd person mention strategy (Danescu-Niculescu-Mizil et al. 2013) and all of them\nare direct or indirect questions, while 38/81 are related to counterfactual modal and\nindicative modal (Danescu-Niculescu-Mizil et al. 2013). This indicates that all three\nmodels struggle with direct or indirect questions with 1st and 2nd person mentions.\nImplicit Hate Classification. We first consider the confusion matrix and find that OpenAI\nmodels are particularly oversensitive to the \u201cstereotypical\u201d class (71% and and 65%\nfalse-positive rates from davinci-003 and ChatGPT respectively). Our error analysis\nof 50 samples shows that models fail to apply the definition: stereotypical text must\nassociate the target with particular characteristics. Instead, models are more likely to\nmark as stereotype any text that contains an identity term (86% of false-positives contain\nidentity terms). All models also fail to recognize strong phrasal signals, like \u201crip\u201d or\n\u201ckill white people\u201d for the white grievance (all 3/50 cases are errors), or violent terms\nassociated with threats. More subtle false-negatives require sociopolitical knowledge\n(2/50) or understanding of humor (6/50). Other errors are examples where the model\nidentified a valid secondary hate category (5/50).\n5.1.3 Few-Shot Viability. Zero-shot models may not be naturally aligned with the\nnon-standard or technical meanings of certain key terms in the social sciences. To\naddress this issue, we consider the viability of open-source FLAN models for few-\nshot classification. Specifically, we try 3-shot and 5-shot experiments with no further\nprompt engineering. Table 5 shows that any improvements from these methods are\n6 For example, \u201ci feel that the sweet team really accomplished that\u201d can be considered both love - gold or joy -\npredicted\n24\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nModel\nFLAN Small\nFLAN Base\nFLAN Large\nFLAN XL\nFLAN XXL\nFLAN UL2\nShot\n0\n3\n5\n0\n3\n5\n0\n3\n5\n0\n3\n5\n0\n3\n5\n0\n3\n5\nDialect\n0.2\n0.0\n0.4\n4.5\n0.0\n1.4\n23.4\n0.7\n14.1\n24.8\n8.0\n20.5\n30.3\n0.2\n29.9\n32.9 12.6\n27.5\nEmotion\n19.8\n10.6\n10.1\n63.8 42.7\n42.0\n69.7\n67.6\n67.4\n65.7\n62.1\n62.5\n66.2\n61.8\n57.4\n70.8 70.0\n69.8\nFigurative\n16.6\n10.0\n9.2\n23.2 29.1\n27.3\n18.0\n21.8\n19.6\n32.2\n27.9\n28.5\n53.2\n52.6 66.2\n62.3 52.7\n62.0\nHumor\n51.8\n52.8 53.1\n37.1 35.1\n34.7\n54.9\n54.0\n53.8\n56.9\n57.0\n56.7\n29.9\n34.8 35.3\n56.8 55.5\n54.1\nIdeology\n18.6\n16.7 24.0\n23.7 22.6\n38.3\n43.0\n47.3\n45.5\n47.6\n48.8\n50.4\n53.1\n52.9 57.7\n46.4\n36.9 51.5\nImpl. Hate\n7.4\n6.8\n6.2\n14.4 21.1\n7.4\n7.2\n9.3\n4.7\n32.3\n28.5\n34.6\n29.6\n31.6 35.1\n32.0 29.5\n25.9\nMisinfo\n33.3\n33.3\n33.3\n53.2\n45.3 59.7\n64.8\n64.8\n64.2\n68.7\n67.2\n69.7\n69.6\n74.9 74.4\n77.4 53.7\n76.4\nPersuasion\n3.6\n3.6\n3.6\n10.4 10.8\n7.3\n37.5\n39.0\n37.7\n32.1\n44.3\n41.8\n45.7\n44.6 48.6\n43.5 42.2\n40.1\nSem. Chng.\n33.5\n33.3 34.0\n41.0\n35.7 41.7\n56.9\n48.8\n60.4\n52.0\n40.8\n35.6\n36.3\n34.0\n33.3\n41.6 62.5\n34.6\nStance\n25.2\n16.7 29.6\n36.6 18.1\n36.6\n42.2\n41.8\n39.8\n43.2\n52.1\n46.2\n49.1\n46.0\n48.7\n48.1 55.6\n54.7\nDiscourse\n4.2\n4.0\n7.5\n21.5 18.1\n20.7\n33.6\n3.6\n34.6\n37.8\n3.6\n38.0\n50.6\n3.6\n43.4\n39.6\n3.6\n39.1\nEmpathy\n16.7\n16.7\n16.7\n16.7 16.7\n16.7\n22.1\n16.7\n17.1\n21.2\n30.4\n22.8\n35.9\n29.8\n28.2\n34.7 41.5\n39.6\nPersuasion\n9.2\n55.9 45.0\n11.0 55.0\n48.7\n11.3\n54.6\n51.7\n8.4\n42.8\n43.8\n41.8\n38.8\n35.2\n43.1 44.9\n46.1\nPoliteness\n22.4\n16.7\n20.1\n42.4 23.9\n35.4\n44.7\n44.5\n51.9\n57.2\n27.7\n50.4\n51.9\n44.2\n50.3\n53.4\n43.6 53.9\nPower\n46.6\n44.5\n33.3\n48.0 39.8\n41.4\n40.8\n45.5\n43.5\n55.6\n58.9\n60.2\n52.6\n52.0 62.6\n56.9\n57.2 57.5\nToxicity\n43.8\n46.7 33.3\n40.4\n34.7 54.4\n42.5\n34.7\n36.7\n43.4\n38.7\n49.2\n34.0\n33.3 35.1\n48.2\n44.7 52.5\nIdeology\n24.0\n16.7\n19.2\n19.2\n16.6 21.3\n28.3\n17.0\n17.9\n29.0\n31.7\n27.0\n42.4\n48.5 47.9\n38.8 38.9\n39.7\nTropes\n1.7\n5.1\n3.4\n8.4\n5.1\n3.4\n13.7\n10.0\n11.6\n14.6\n8.4\n10.0\n19.0\n8.4\n6.8\n28.6 27.3\n24.6\nTable 5\nFew-shot Classification does not uniformly improve performance across our selected CSS\nbenchmark tasks. All tasks are evaluated with macro F-1.\nspotty and inconsistent. For some challenging tasks like Empathy and Persuasion that\nhave subjective definitions or non-standard taxonomies, few-shot learning can improve\nperformance in 2 and 5 out of 6 model sizes respectively. However, these gains are\nsmall and not widespread among other tasks. We conclude that additional engineering\nefforts may be needed to achieve significant gains on CSS tasks via few-shot learning.\n5.2 Model-Selection (RQ2)\nCSS researchers should understand how their choice of model can decide the reliability\nof zero-shot methods. Our results show that, for structured parsing tasks like event\nextraction, it is best to use a code-instructed model like OpenAI\u2019s gpt-3.5-turbo,\nwhile for most classification tasks, open-source LLMs like FLAN-UL2 are best.\nModel Size. LLMs generally follow scaling laws (Kaplan et al. 2020; Hoffmann et al.\n2022) where performance increases with the size of the model and training data. We\ninvestigate scaling laws in the two families of instruction-tuned LLMs: FLAN and\nOpenAI. Results show larger open-sourced models are preferable.\nFLAN\u2019s CSS performance roughly matches Kaplan et al.\u2019s predicted power-law\neffects from pure model size. Figure 3 shows FLAN classification performances scaling\nnearly logarithmically with the parameter count. With each order of magnitude size\nincrease, the median average task improvement in FLAN models is 5.0 absolute percent-\nage points. All FLAN-T5 models use the same stable corpus, pretraining objective, and\narchitecture, which gives us a controlled environment to observe stable scaling laws.\n25\nComputational Linguistics\nVolume 50, Number 1\nModel Parameters\nMean CSS Task F1 Score\n0\n10\n20\n30\n40\n50\n1.00E+8\n1.00E+9\n1.00E+10\n1.00E+11\n1.00E+12\nFigure 3\nEffects of Scaling on the mean zero-shot performance on our CSS benchmark tasks. FLAN\nmodels and davinci-001/002 are instruction fine-tuned. davinci-003 and\ngpt-3.5-turbo are instruction fine-tuned and refined with Reinforcement Learning from\nHuman Feedback. GPT Parameter counts reported based on approximates.\nOpenAI\u2019s GPT-3 001 models, on the other hand, do not monotonically benefit\nfrom scaling.7 There is minimal performance improvement from ada to davinci-001,\ndespite the three orders of magnitude increase in size. Similarly, we observe little benefit\nfrom the trillion-parameter GPT-4 over the hundred-billion-parameter GPT-3.5-turbo.\nInstead, the greatest performance improvements come from variations in pretraining,\nfine-tuning, and reinforcement learning.\nPretraining & Instruction Fine-tuning. Besides scale, two key factors play a major role\nin model performance: pretraining data and instruction fine-tuning. Pretraining data is\nthe raw text upon which an LLM learns to model the general generative process of\nlanguage. Instruction fine-tuning refines the raw LLM to perform specific tasks based\non human-written instructions.\nOpenAI\u2019s davinci models significantly benefit from pretraining and instruc-\ntion fine-tuning tricks. For classification tasks (Table 3), we see an outsized in-\ncrease in CSS performance (\u2191 absolute 11 pct. pts.) moving from davinci-001\nto davinci-002, larger than any performance increase from scale alone. Both\ndavinci-001 and davinci-002 use the same supervised instruction fine-tuning\nstrategy, but davinci-002 is based on OpenAI\u2019s base-code model, which had access\nto a larger set of instruction fine-tuning data. Most importantly, davinci-002 was pre-\ntrained on both text and code. This difference benefits structured, JSON-formatted tasks\nlike Event Argument Extraction. While davinci-001 often fails to generate JSON,\ndavinci-002 succeeds with markedly improved performance (+13.0 F1).\n7 This analysis relies on estimates which combine community estimates, the OpenAI research\ndocumentation, and the assumption that all models named or \"improved\" from davinci have the same\nparameter counts. These estimates may be incorrect, as hypothesized by other community estimates. This\nis a limitation of research on these models as exact model size and training data are a trade secret of\nOpenAI.\n26\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nLearning From Human Feedback. We see that RLHF can improve LLM performance on\nCSS classification tasks. RLHF has been lauded as the major catalyst behind the\nsuccess of instruction-following models (Ouyang et al. 2022), and here we see\ntext-davinci-003 and gpt-3.5-turbo (with RLHF) improves the average F1 of\ntext-davinci-002 (without RLHF) by 3.5 absolute points.\n5.3 Domain-Utility (RQ3)\nThe survey and taxonomy of social science need in \u00a72 allows us to understand whether\nthe utility of LLMs is limited to certain domains or certain data types. To do so, we\npartition all classification results from Table 3 into bins corresponding to the academic\nfield most impacted by the task.8 Although we recognize the multi-disciplinary utility\nof all tasks, this type of 1:1 organization is appropriate for understanding the academic\nscope of our results. We acknowledge that the partitioning and selection of the dataset\ninfluence the performance distributions that we observe. We urge readers to interpret\nthe results with caution and focus on broader conclusions rather than the fine numerical\ndetails of these distributions.\nThe box plot in Figure 4 shows that field-specific performances significantly over-\nlap. Thus overall, we do not observe a strong bias against or proclivity for a particular\nfield of study. In political science, we see the highest overall performance on misin-\nformation detection (F1=77.4) and much lower performance on implicit hate detection\n(F1=32.3). For psychology, we observe high performance on emotion detection (F1=70.8)\nand low performance on empathy detection (F1=35.9). Peformances span the full range\nof disciplines. This suggests that performance is not tied to academic discipline.\nIn terms of data type, Figure 4 suggests that performance may be more closely\ndetermined by the complexity of the input. In particular, documents encode complex\nsequences of ideas or temporal events, and overall, the two lowest task performances\nare on the document-level tasks: character trope classification and event argument ex-\ntraction. All other document-level accuracies are at or below 50%. The most challenging\nutterance and conversation-level tasks are also a function of their label space complexity.\nImplicit hate (F1=32.3), empathy (F1=35.9), and dialect feature (F1=32.9) annotations are\nexpert-labeled on a subtle theoretical taxonomy.\n6. Generation Results\nIn this section, we answer RQ4: Are prompted LLMs useful for generatively implementing\ntheories and explaining social scientific constructs with text? Will generative models replace\nor augment human analysis? To answer this question, we rely on the human evaluation\nsetup described in \u00a74.4. Note that FLAN models are excluded from all evaluation tables\nbecause FLAN models failed to follow instructions by manual inspection. Instead, we\nevaluate across the OpenAI suite.\nHuman Scoring Evaluation. According to the domain experts in Table 6, leading gen-\nerative models can produce text of a quality that matches or exceeds that of human\ngold references. For Aspect-Based Emotion Summarization, Misinformation Explana-\ntion, and Social Bias Inference, gpt-3.5-turbo produce, on average, more domain-\n8 This partitioning follows Figure 1, with stance and ideology detection in the political science bin and\ndialect feature classification under linguistics, for example.\n27\nComputational Linguistics\nVolume 50, Number 1\nhistory\nlinguistics\nliterature\npolitical science\npsychology\nsociology\nField of Study\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nBest Task F1 Scores\nutterance\nconversation\ndocument\nLevel of Analysis\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nBest Task F1 Scores\nFigure 4\n(Left) Task Performance By Field of Study. Significant overlap in the distributions suggests that\nneither high nor low performance is exclusive to any particular discipline. Caution: The\ndistributions depend on the particular choices of this study, which datasets to select and how to\npartition them.\n(Right) Task Performance By Level of Analysis. Document-level tasks are challenging for their\ninput length and complexity, and this is reflected in their F1 scores all near or below 50%.\nUtterance and conversation-level task performance varies also with the complexity of the task.\nfaithful, coherent, and fluent text than both gold references and the fine-tuned baseline.\nFor Positive Reframing and Figurative Language explanation, text-davinci-003\nmatch the gold-standard levels of faithfulness, relevance, coherence, and fluency, again\noutperforming the fine-tuned baseline. For generation performance, scale matters, as\nsmaller models fail to produce explanations and summaries that are faithful to the\ntask specifications, especially in COVIDET, FLUTE, and SBIC, where text-ada-001\nearns faithfulness scores of less than 2 out of 5 on average. With scale, however, LLMs\nare capable of generating useful, relevant, coherent, and fluent explanations and\nsummaries of underlying social science constructs.\nHuman Ranking Evaluation. According to the authors\u2019 ranking evaluations in Table 7,\nprompted LLMs produce helpful and informative text in all five generation tasks.\nModel generations outrank the dataset\u2019s gold human reference at least 38% of the time.\nThe best models approach parity with humans\u2014a near 50-50 coin toss to decide which\nis preferred. Furthermore, we see significant performance benefits from both RLHF\nmodels, gpt-3.5-turbo and text-davinci-003. Unlike classification (\u00a75.2), our\nselected generation tasks seem to systematically benefit from human feedback.\nDespite strong performances, no model substantially outperforms human annota-\ntion. This suggests that current LLMs cannot fully replace human analysis. Still, given\nLLM\u2019s performance parity with humans, the results suggest one avenue for human-\nAI collaboration: instead of coding text with summary explanations from scratch,\nresearchers and annotators could apply minor edits to correct model generations. 9\nThe results in Table 7 suggest that, for every five model generations, 2 to 3 of these\noutputs would demand no additional annotator effort. If implemented successfully, this\npartnership could significantly increase the efficiency of the social scientist\u2019s research\npipeline. However, we leave it to future work in HCI to determine the plausibility of\n9 Note that machine generated explanations might be limited in terms of their diversity. Although human\nvalidation can help refine these machine outputs, such process may not be able to introduce novel edits\nor perspectives.\n28\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nAspect-Based Summarization (COVIDET)\nModel\nFaithful\nRelevant\nCoherent\nFluent\nBaseline\n2.1\n2.3\n2.1\u2212\n2.6\u2212\nada-001\n1.8\u2212\n1.8\u2212\n2.4\n3.6\nbabbage-001\n2.0\u2212\n2.0\n2.3\n3.7\ncurie-001\n2.3\n2.3\n2.6\n3.8\ndavinci-001\n2.3\n2.4\n2.5\n3.9\ndavinci-002\n2.4\n2.5\n3.2\n4.0\ndavinci-003\n2.9\n2.8\n3.0\n4.1+\nGPT 3.5\n3.9+\n3.5+\n3.8+\n4.5+\nGPT 4\n3.7+\n3.3+\n3.8+\n4.4+\nHuman\n2.8\n2.6\n2.8\n3.8\nFigurative Language Explanation (FLUTE)\nModel\nFaithful\nRelevant\nCoherent\nFluent\nBaseline\n1.4\u2212\n1.7\u2212\n1.4\u2212\n4.2\nada-001\n1.4\u2212\n1.5\u2212\n1.5\u2212\n3.9\nbabbage-001\n1.4\u2212\n1.9\u2212\n1.5\u2212\n3.9\u2212\ncurie-001\n1.5\u2212\n2.3\u2212\n1.7\u2212\n4.1\ndavinci-001\n1.2\u2212\n1.9\u2212\n1.5\u2212\n4.1\ndavinci-002\n2.5\n3.4\n2.5\n4.1\ndavinci-003\n3.0\n4.0\n3.1\n4.1+\nGPT 3.5\n2.1\u2212\n3.6\n2.5\n4.1\nGPT 4\n2.1\u2212\n3.3\n2.4\n4.0\nHuman\n2.8\n4.0\n2.6\n4.2\nPositive Reframing\nModel\nFaithful\nRelevant\nCoherent\nFluent\nBaseline\n4.1\n4.2\n3.9\n4.4\nada-001\n1.8\u2212\n1.4\u2212\n1.8\u2212\n1.6\u2212\nbabbage-001\n3.8\n2.5\u2212\n3.8\n3.7\ncurie-001\n4.1\n3.7\u2212\n4.1\n3.9\ndavinci-001\n3.5\u2212\n4.0\n3.3\u2212\n4.1\ndavinci-002\n4.0\n3.9\u2212\n4.0\n4.2\ndavinci-003\n4.4\n4.5+\n4.2\n4.6+\nGPT 3.5\n4.3\n4.3\n4.2\n4.4\nGPT 4\n4.1\n4.3\n4.1\n4.2\nHuman\n4.2\n4.2\n4.1\n4.2\nImplied Misinformation Explanation (MRF)\nModel\nFaithful\nRelevant\nCoherent\nFluent\nBaseline\n3.4\n3.5\n3.7\n4.2\nada-001\n1.1\u2212\n1.1\u2212\n2.0\u2212\n4.5\nbabbage-001\n1.6\u2212\n1.7\u2212\n2.5\u2212\n4.3\ncurie-001\n2.6\u2212\n2.7\u2212\n3.1\u2212\n4.4\ndavinci-001\n1.7\u2212\n1.7\u2212\n2.5\u2212\n4.5\ndavinci-002\n3.9+\n4.1+\n4.3+\n4.9+\ndavinci-003\n3.1\u2212\n3.4\n3.9\n4.5\nGPT 3.5\n3.7+\n3.9\n4.2+\n4.9+\nGPT 4\n3.7\n3.9\n4.1\n4.5\nHuman\n3.5\n3.7\n3.9\n4.4\nSocial Bias Inference (SBIC)\nModel\nFaithful\nRelevant\nCoherent\nFluent\nBaseline\n1.9\u2212\n2.1\u2212\n2.1\u2212\n1.9\u2212\nada-001\n2.4\n2.2\u2212\n2.7\n3.3+\nbabbage-001\n3.1\n3.1\n3.6+\n3.8+\ncurie-001\n3.4\n3.3\n3.9+\n4.5+\ndavinci-001\n3.4\n3.4\n3.8+\n3.9+\ndavinci-002\n3.7+\n3.5\n4.1+\n4.2+\ndavinci-003\n3.5\n3.4\n4.1+\n4.4+\nGPT 3.5\n4.0+\n3.7+\n4.2+\n4.2+\nGPT 4\n4.1+\n3.8+\n4.2+\n4.6+\nHuman\n2.9\n3.0\n3.1\n2.6\nAnnotator Backgrounds\nTask\nEducation\nProfession\nCOVIDET\nMS,\nCDC Health\nHealth Ed.\nComm. Specialist\nMRF\nBA,\nGrad Student,\nPoli. Sci.\nPublic Policy\nFLUTE\nMFA,\nWriting Expert,\nCreat. Writing\nGrammarly\nSBIC\nBS,\nGrad Student,\nJournalism\nEpidemiology\nReframing\nBA,\nClinical Behavioral\nPsychology\nHealth, Nurse\nTable 6\nExpert Scoring Evaluations for Zero-shot Generation Tasks show that leading generative\nmodels (davinci-003, GPT 3.5) can match or exceed the faithfulness, relevance, coherence,\nand fluency of both fine-tuned models (Baseline) and gold references (Human). All scores are\naverage ratings on 1-5 Likert scales. Best models are in\ngreen followed by\nblue . Marks for + and \u2212\nshow performance significantly better or worse than human (P < .05) by Paired Bootstrap.\nthis partnership and the degree to which it might reduce annotators\u2019 cognitive load on\nexploratory analysis and free-coding tasks.\nAs a tradeoff for LLM\u2019s efficiency, researchers will face the burden of manually\nvalidating generative outputs. It is well-known that automatic performance metrics\nfail to capture human preferences (Goyal, Li, and Durrett 2022; Liang et al. 2022).\nIn fact, we found that BLEU, BERTScore, and BLEURT, which rely on comparisons\nto human written ground truth, all produced uninterpretable scores for generation\ntasks. This highlights a fundamental challenge for evaluation of generation systems in\nCSS, especially if zero-shot performance continues to improve. As zero-shot models\napproach or outperform the quality of the gold-reference generations, reference-based\nscoring becomes an invalid construct for measuring models\u2019 true utility (Raji et al. 2021),\neven if we assume the semantic similarity metrics are ideal. This motivates our use of\n29\nComputational Linguistics\nVolume 50, Number 1\n% Model Preferred Over Gold Annotations\nModel\nMRF\nFLUTE\nSBIC\nReframing\nCOVIDET\nBaseline\n31.2%\n4.6%\n16.5%\n45.0%\n37.5%\nada-001\n17.6%\n1.7%\n11.8%\n0.0%\n23.5%\nbabbage-001\n29.4%\n6.7%\n29.4%\n0.0%\n23.5%\ncurie-001\n29.4%\n1.7%\n32.4%\n11.5%\n41.2%\ndavinci-001\n21.4%\n6.2%\n39.0%\n30.4%\n50.0%\ndavinci-002\n21.4%\n25.0%\n29.3%\n10.0%\n37.5%\ndavinci-003\n38.9%\n47.0%\n50.0%\n48.5%\n59.1%\nGPT 3.5\n27.8%\n37.9%\n65.9%\n56.1%\n68.2%\nGPT 4\n36.4%\n51.5%\n60.6%\n39.4%\n36.4%\nTable 7\nRanking Evaluations for Zero-shot Generation Tasks give the proportion of all pairwise\nrankings where authors unanimously ranked the model\u2019s generation as more accurate or\npreferable to a gold-standard explanation drawn from the dataset. Best models are in\ngreen and\nrunner-ups are in\nblue .\nreference-free expert evaluation of generations, that is, asking expert annotators which\ngeneration is more accurate with regard to the input. However, this alternative is limited\nby both cost and reproducibility concerns (Karpinska, Akoury, and Iyyer 2021). There is\na clear need for new metrics and procedures to quantify model utility for CSS.\n7. Discussion\nThis work presents a comprehensive evaluation of LLMs on a representative suite of\nCSS tasks. We contribute a robust evaluation pipeline, which allows us to benchmark\nperformance alongside supervised baselines on a wide range of tasks. Our research\nquestions and empirical results are designed to help CSS researchers make decisions\nabout when LLMs are suitable and which models are best suited for different research\nneeds. In summary, we find that LLMs can augment but not entirely replace the\ntraditional CSS research pipeline.\nMore concretely, we make the following recommendations to CSS researchers:\n1. Integrate LLMs-in-the-loop to transform large-scale data labeling.\n2. Prioritize open-source LLMs for classification\n3. Prioritize faithfulness, relevance, coherence, and fluency in your generations by\nopting for larger instruction-tuned models that have learned human preferences.\n4. Investigate how LLMs produce new CSS paradigms built on the multipurpose\ncapabilities of LLMs in the long term.\nIn the following subsections, we more specifically detail in how annotation fits into\nthe CSS pipeline (\u00a77.1), how LLMs can augment annotation (\u00a77.2), which LLMs we\nrecommend for this purpose (\u00a77.3), and the opportunities LLMs expose for new research\nparadigms (\u00a77.4 and \u00a77.5).\n30\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nPersimmon kills coronavirus, according to the study by Japanese scientists.\nMisinformation Detection Example\nLLM Augmented\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n?\n\u2713\nTraditional\nFigure 5\nHuman-AI Collaboration can improve the efficiency and reliability of text analysis. In this\nmisinformation example, the LLM helps scale up annotation while reducing variance in the gold\nlabels. Human annotation serves as validation for model-provided annotations.\n7.1 How Annotation Fits Into CSS\nSocial scientists are not often interested in classification labels or generative codes\nmerely for their own sake. Labeled text is almost always used to explain a wider\nphenomenon using downstream inferential statistics such as regression. To ensure a\nvalid downstream inference, any estimators of the underlying construct need to be\nasymptotically unbiased and allow for the computation of valid confidence intervals.\nIn a study that follows up our own, Egami et al. (2023) demonstrate that even the most\naccurate of LLM annotations will produce biased estimates and invalid confidence in-\ntervals when they are merely averaged. However, Egami et al. (2023) propose a method\ncalled Design-based Semi-supervised Learning (DSL) which they use for computing\nunbiased estimators for prevalence estimation on each of our 17 classification tasks.\nUsing FLAN-UL2 pseudo-labels and only a handful of gold-labeled instances, they find\nthat it is possible to compute unbiased regressions, even for tasks with low accuracy\nfrom UL2. With only 25 gold annotations, DSL can compute confidence intervals that\nhave >86% correct coverage for all of our classification tasks including those with the\nlowest performance from LLM pseudo-labels.\n7.2 LLMs Can Augment Annotation\nOur work shows that current LLMs perform well enough to augment the CSS annotation\npipeline and thus reduce the need for human labor on some tasks. \u00a75.1 show that an LLM\nannotator yields fair to strong agreement with humans on 12 out of 17 classification\ntasks. However, LLMs are not a wholesale replacement for human annotators. Even the best\nLLMs exhibit unusably low performance on some CSS tasks. Ensembling prediction\ndoes not mitigate this label corruption as LLMs demonstrate high internal agreement,\neven when inaccurate (Gilardi, Alizadeh, and Kubli 2023). Overconfident models, if left\nunchecked, distort the conclusions of CSS research and subsequently mislead policy\nand social actions taken in response. Human validation is key to avoiding a replication\ncrisis in CSS caused by LLM hallucinations and inaccuracies.\n31\nComputational Linguistics\nVolume 50, Number 1\nInstead, we advocate that CSS researchers integrate LLMs with human annotation, as\nillustrated in Figure 5. When a LLM matches human levels of agreement, it can be\nused as one of multiple labelers. It is possible from such methods to produce unbiased\nestimators. The estimation methods of Chaganty, Mussmann, and Liang (2018) are\nprovably optimal, and provide these unbiased estimators with 7-13% monetary savings\nby leveraging LLM pseudo-labels.\nMoving forward, LLMs can serve as a flywheel for dataset collection. Prompted\nLLMs consistently perform significantly better than chance, providing imperfect labels\nat low cost. Annotation schemes developed to iteratively improve imperfect data \u2014such\nas weak supervision (Ratner et al. 2017), targeted data cleaning (Chen, Yu, and Bowman\n2022), and active learning (Yuan, Lin, and Boyd-Graber 2020; Li et al. 2022) \u2014 avoid\nLLM pitfalls by allowing human validation to refine the original model. This creates a\nvirtuous cycle which exploits the strengths of LLMs to focus human expertise where it\nis most needed (Kiela et al. 2021).\nOur results show that LLMs are even stronger at generation tasks, being rated superior\nto human gold annotations over 38% of the time in all 5 tasks we evaluate. LLMs can\nalready generate syntactically cohesive and stylistically consistent text, and as we have\nshown, the best generations are also highly relevant and faithful to core CSS tasks.\nHumans expertise can be used to further curate outputs according to domain-specific\naccuracy and quality metrics. Dataset construction through human curation of LLM\ngenerations has already emerged in recent NLP works on decision explanation (Wiegr-\neffe et al. 2022), model error identification (Ribeiro and Lundberg 2022), and even to\nbuild the figurative language benchmark used in this work (Chakrabarty et al. 2022).\nWe recommend that CSS researchers consider the use of LLMs as the foundation of\nsuch annotation procedures, and that future studies measure the degree to which this\nstrategy improves annotation efficiency as we suspect. If the anticipated efficiency is\nachieved, CSS researchers should reinvest savings to train expert annotators, reversing\nthe trends of replacing experts with crowdworkers due to cost (Snow et al. 2008). By\ndoing so, LLMs could enable data labeling procedures that more deeply benefit from\nthe non-computational expertise of the social scientists whose theories we build upon.\n7.3 When To Use What LLM\nWe hope these results help CSS researchers to understand LLM alternatives for their use\ncases. Our general prompt guidelines allow us to quickly design functional prompts for\nmany models. When looking to incorporate LLMs in their work, CSS researchers should\nconsider the advantages and disadvantages of open and closed-source models.\nFor CSS classification, our work shows that open-source models like FLAN are as capable as\nstate-of-the-art closed-source LLMs from OpenAI. We recommend researchers who already\nhave access to GPUs capable of running these models prefer open-source models. For\ncontinuous monitoring and enormous-scale analysis, the low marginal cost of these\nmodels could make them price-advantageous. For CSS researchers with expertise, open-\nsource LLMs have the added benefit of being able to be fine-tuned on labeled data\nand constrained programatically for more predictable behavior. At this time, it is not\npossible to further fine-tune all of OpenAI\u2019s instruction-tuned models.\nFor those without existing hardware infrastructure, proprietary APIs appear to be\na cost-efficient option. Based on current cloud pricing,10 the hardware necessary to\n10 Google Cloud FLAN hosting cost\n32\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nrun FLAN-T5-XXL costs 170 dollars per day\u2014the equivalent of processing \u223c50 million\nwords with gpt-3.5-turbo.11 In most cases, gpt-3.5-turbo is more cost-efficient\nand has a lower operational overhead for hardware-constrained research groups.\nFor generation tasks, the results are clear-cut. Even the largest open-source models\nfailed to generate meaningful responses for CSS tasks. Even when labeled data is avail-\nable, the best proprietary models outperform fine-tuned baselines consistently and approach\nparity with gold human annotations when evaluated by crowdworkers. For CSS experts\nlooking to generate interpretations or explanations of data, gpt-3.5-turbo is the clear\nleading LLM by both price and performance. No matter which modeling decision is\nmade, practitioners should keep the limitations of natural language generation in mind,\nunderstanding that explanations are not causal and recognizing the risks that come with\nmodel errors and hallucinations (see \u00a77.8).\nOur work shows that all LLMs struggle most with conversational and full doc-\nument data. Also, LLMs currently lack clear cross-document reasoning capabilities,\nlimiting extremely common CSS applications like topic modeling. For CSS subfields\nwhich often study these discourse types\u2014sociology, literature, and psychology\u2014LLMs\nhave major limitations and are unlikely to have major immediate impact. NLP re-\nsearchers who aim to improve existing LLMs to empower more CSS tasks should study\nthe unique technical challenges of conversations, long documents, and cross-document\nreasoning (Beltagy, Peters, and Cohan 2020; Caciularu et al. 2021; Yu et al. 2021).\n7.4 Blending CSS Paradigms\nThe few-shot (Brown et al. 2020) and zero-shot capabilities (Ouyang et al. 2022) of\nLLMs blur the traditional line between supervised and unsupervised ML methods\nfor the social sciences. Historically, supervised methods invest in labeled data guided\nby existing theory to develop a trained model. This model is then used to classify\ntext at scale to gather evidence for the causal effects surrounding the theory. By com-\nparison, unsupervised methods like topic modeling often condense large amounts of\ninformation to help researchers discover new relationships, which develop or refine\nsocial theories (Evans and Aceves 2016).\nThe ability of LLMs to follow instructions and interpret complex tasks is rapidly\nadvancing, with major new models even within the course of this work (OpenAI\n2023). Beyond annotation, LLMs have multi-purpose capabilities to retrieve, label, and\ncondense relevant information at scale. We believe that this can blend the boundaries be-\ntween supervised and unsupervised paradigms. Rather than using separate paradigms\nto develop and test theories, a single tool can be used to develop working hypotheses,\nusing generated and summarized data, and test hypotheses, labeling human samples\nflexibly with low-cost classification capabilities. We believe CSS researchers should use\nthe multi-functionality of LLMs to create new paradigms of research for their fields.\nSimulation. An emerging example of such innovation in CSS is the use of LLMs as\nsimulated sample populations. Game theorists have used rule-based utility functions\nto develop hypotheses about the causes of social phenomena (Schelling 1971; Easley\nand Kleinberg 2010) and to predict the effects of policy changes (Shubik 1982; Klein-\nberg et al. 2018). However, simulations are limited by the expressiveness of utility\nfunctions (Ellsberg 1961; Machina 1987). LLMs hold a great potential to provide more\n11 OpenAI Pricing\n33\nComputational Linguistics\nVolume 50, Number 1\npowerful simulations for CSS (Bail 2023), as they replicate human biases without explicit\nconditioning (Jones and Steinhardt 2022; Koralus and Wang-Ma\u00b4scianica 2023). Recent\nwork leverages this capacity to simulate social computing systems (Park et al. 2022),\ncommunity and their members\u2019 interactions (Park et al. 2023), public opinion (Argyle\net al. 2022; Chu et al. 2023), and subjective experience description (Argyle et al. 2022).\nHowever, there are dangers and uncertainties in this area as noted in these works.\nSince social systems evolve unpredictably (Salganik, Dodds, and Watts 2006), simu-\nlated samples inherently have limited predictive and explanatory power. While utility-\nbased simulations have similar limitations, their assumptions are explicit unlike the\nopaque model of human behavior an LLM provides. Additionally, current models\nexhibit higher homogeneity of opinions than humans (Argyle et al. 2022; Santurkar et al.\n2023a). Combining LLMs with true human samples is essential to avoid an algorithmic\nmonoculture and could lead to fragile findings covering only the limited perspectives\nrepresented (Kleinberg and Raghavan 2021; Bommasani et al. 2022).\n7.5 The Need for A New Evaluation Paradigm\nEvaluation will need to adapt if blended methods create a new CSS paradigm.\nAccuracy-based metrics were ideal for fixed-taxonomy classification tasks in the era of\nNLP benchmarking. Similarly, word-overlap metrics made sense for natural language\ngeneration tasks in which the gold reference was well-defined (e.g., translation). How-\never, open-ended coding and CSS explanation objectives follow neither a pre-defined\ntaxonomy nor a regular output template. For more open-ended data exploration tasks\nlike topic modeling, held-out likelihood helped automatically measure the predictive\npower of the model (Wallach et al. 2009), but predictiveness does not always correlate\nwith explainability (Shmueli et al. 2010), and these automatic metrics proved to be at\nodds with human quality evaluations (Chang et al. 2009). In CSS, human evaluations\ncan be unreliable (Karpinska, Akoury, and Iyyer 2021). We observe this directly in our\nwork, as crowd work seems to provide unreliable quality metrics for FLUTE, a nuanced\ngenerative task. New metrics are needed to capture the semantic validity of free-form\ncoding with LLMs as explanation-generators.\n7.6 CSS Challenges for LLMs\nAs shown by our \u00a75 results, LLMs face notable challenges that pervade the computa-\ntional social sciences. The first challenge comes from the subtle and non-conventional\nlanguage of expert taxonomies. Expert taxonomies contain technical terms like the\ndialect feature copula omission (\u00a73.1.1), plus specialized or nonstandard definitions of\ncolloquial terms, like the persuasive scarcity strategy (\u00a73.1.8), or white grievance in im-\nplicit hate (\u00a73.1.4). LLMs may lack sufficient representations for such technical terms, as\nthey may be absent from the pretraining data (Yao et al. 2021). How to teach LLMs to un-\nderstand these social constructs deserves further technical attention. This is especially\ntrue for novel theoretical constructs that social scientists may wish to define and study in\ncollaboration with LLMs.\nUnlike widely used NLP classification tasks, the challenge of expert taxonomies\nin CSS is compounded by the size of the target label space, which, in CSS applica-\ntions, may contain upwards of 72 classes (see character tropes, \u00a73.3.4). This challenges\ntransformer-based LLMs, which have relatively limited memory, finite processing win-\ndows, and quadratic space complexity.\n34\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nLarge, complex, and nuanced annotation schemes may also introduce dependen-\ncies among labels that are organized into multi-level hierarchies or richly constrained\nschemas, as in many event argument extraction applications. Such complex structural\nparsing tasks pose special challenges to the zero-shot prompting paradigm introduced\nin this work since prompted models often struggle to generate consistent outputs\n(Mishra, Tater, and Sankaranarayanan 2019). Our prompting best practices in Table 1\nall help LLMs generate more consistent machine-readable outputs, but this challenge is\nnot fully solved for all CSS tasks.\nFinally, Computational Social Scientists study language, norms, beliefs, and politi-\ncal structures that all change across time. To account for these distribution shifts, LLMs\nwill need an extremely high level of temporal grounding\u2014knowledge and signals by\nwhich to orient a text analysis in a particular place and time (Bommasani et al. 2021).\nThis is especially challenging wherever researchers are interested in rapid, synchronous\nanalysis of breaking events. It may be prohibitively expensive to frequently update\nLLM\u2019s knowledge of current events via continually training (Bender et al. 2021), and\nthis challenge will only be exacerbated as models continue to scale up.\n7.7 Issues in Bias and Fairness\nBias. Researchers should weigh the benefits of applying prompting methods to CSS,\nalong with the limitations and risks of doing so. Most notably, LLMs are known to\namplify social biases and stereotypes (Sheng et al. 2021; Abid, Farooqi, and Zou 2021;\nBorchers et al. 2022; Lucy and Bamman 2021; Shaikh et al. 2022), as well as viewpoint\nbiases in subjective domains (Santurkar et al. 2023b). These biases can emerge in open-\nended generation tasks like the explanation and paraphrasing (Dhamala et al. 2021). The\nperformance of LLMs as tools for classification and parsing may vary systematically as\na function of demographic variation in the target population (Zhao et al. 2018). With\nthe datasets available, we were unable to perform a systematic analysis of biases and\nperformance discrepancies, but we urge researchers to carefully consider these risks in\ndownstream applications.\nRisks Inherent to Proprietary LLMs. Researchers will often lack details on the selection,\nfiltering, and formatting of online corpora for training proprietary models like Ope-\nnAI\u2019s GPT-4. There are unique risks and privacy implications for those who employ\nthese models for research. One risk inherent in an obscured pre-training corpus is that\nwe can\u2019t decompose the above issues of bias and social harm into their respective\nsources for targeted mitigation. Especially with open-ended generative coding (\u00a73.4),\nunattributed biases could jeopardize the reliability and prosocial impact of downstream\nscientific analyses. Such issues may also escape human review, as humans are prone to\nfalsely attribute factuality to texts that bear a more authoritative or expert style (Wu and\nAji 2023)\u2014-a style that modern proprietary LLMs have largely mastered. Furthermore,\nblack-box industrial APIs may not allow for targeted mitigation via fine-tuning.\nResearchers who operate with closed-source industrial APIs may also be more\nprone to privacy issues and legal disputes over intellectual property. Proprietary models\nare known to replicate copyrighted materials and sensitive personally identifiable infor-\nmation (Carlini et al. 2021). Researchers who employ these models may be unknowingly\naccountable for knowledge based on false or missing attributions as well as private\npersonal information.\n35\nComputational Linguistics\nVolume 50, Number 1\n7.8 Limitations\nTask Selection and Data Leakage. Our tasks do not represent an exhaustive list of all appli-\ncation domains. Some highly-sensitive domains like mental health (Nguyen et al. 2022),\nwhich requires expert annotations, and cultural studies, which requires community-\nspecific knowledge, are rife with additional challenges and ethical concerns. These are\nlargely outside the scope of the current study. More broadly, LLMs should not be used to\ngive legal or medical advice, prescribe or diagnose illness, or interfere with democratic\nprocesses (Solaiman and Dennison 2021). Finally, our task selection was limited by the\navailable data resources in the field, which is largely dominated by text in standard\ndialects (Ziems et al. 2023) representing members of Western, Educated, Industrial, Rich,\nand Democratic populations (WEIRD; Ignatow and Mihalcea 2016; Muthukrishna et al.\n2020). Future studies should separately consider LLMs\u2019 utility for cross-cultural CSS.\nWhen evaluating LLMs, one notable concern is data leakage. Data from the test\nset might have been seen by LLMs during the pre-training, and this would artificially\ninflate test performances. This problem is especially concerning for closed-source or\nproprietary models with undisclosed training sets. One mitigation strategy is to design\nexplicit prompts that force the model to forget the test set. Another strategy is to design\ncustom test sets from perturbations of existing data to more fairly evaluate models. We\nleave this for future work.\nCausality and Explanations. Explanations are important to social science (Shmueli et al.\n2010; Hofman, Sharma, and Watts 2017; Yarkoni and Westfall 2017). In this work, we\nexplored the predictive power of LLMs rather than causal explanations. Predictions\nserve to expose and elaborate on the underlying social phenomena latent in a text. These\nexplicit phenomena can then be used as structured features for further causal analysis.\nHowever, this may not be sufficient: social scientists often seek causal theories\n(DiMaggio 2015), or at least contrastive explanations, why P instead of Q (Miller 2019).\nBecause LLMs are not grounded in a causal model of the world (Bender et al. 2021),\nthey are not on their own reliable tools for mining causal relationships in text. We leave\nit to future work to explore contrastive or causal explanations in LLMs.\nAcknowledgments\nWe are thankful to Tony Wang, Rishi Bommasani, Albert Lu, Myra Cheng, Yanzhe Zhang,\nCamille Harris, and Minzhi Li for their helpful feedback on our early drafts. We thank the\nanonymous reviewers for their insightful suggestions as they significantly shaped the final form\nof this work. Caleb Ziems is supported by the NSF Graduate Research Fellowship under Grant\nNo. DGE-2039655. This work was partially sponsored by NSF grant IIS-2247357 and IIS-2308994.\nReferences\nAbbott, Andrew. 1990. Conceptions of time\nand events in social science methods:\nCausal and narrative approaches.\nHistorical Methods: A Journal of Quantitative\nand Interdisciplinary History, 23(4):140\u2013150.\nAbid, Abubakar, Maheen Farooqi, and James\nZou. 2021. Persistent anti-muslim bias in\nlarge language models. In Proceedings of\nthe 2021 AAAI/ACM Conference on AI,\nEthics, and Society, pages 298\u2013306.\nAbul-Fottouh, Deena and Tina Fetner. 2018.\nSolidarity or schism: ideological\ncongruence and the twitter networks of\negyptian activists. Mobilization: An\nInternational Quarterly, 23(1):23\u201344.\nAhmed, Amr and Eric Xing. 2010. Staying\ninformed: Supervised and\nsemi-supervised multi-view topical\nanalysis of ideological perspective. In\nProceedings of the 2010 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 1140\u20131150, Association\nfor Computational Linguistics,\nCambridge, MA.\n36\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nAhuja, Ojas, Jiacheng Xu, Akshay Gupta,\nKevin Horecka, and Greg Durrett. 2022.\nAspectnews: Aspect-oriented\nsummarization of news documents. In\nProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 6494\u20136506.\nAlthoff, Tim, Cristian\nDanescu-Niculescu-Mizil, and Dan\nJurafsky. 2014. How to ask for a favor: A\ncase study on the success of altruistic\nrequests. In Proceedings of the International\nAAAI Conference on Web and Social Media,\nvolume 8, pages 12\u201321.\nAltikriti, Sahar. 2016. Persuasive speech acts\nin barack obama\u2019s inaugural speeches\n(2009, 2013) and the last state of the union\naddress (2016). International Journal of\nLinguistics, 8(2):47\u201366.\nAmber, Boydstun, Gross Justin, Philip\nResnik, and Noah Smith. 2013. Identifying\nmedia frames and frame dynamics within\nand across policy issues. In New directions\nin analyzing text as Data workshop.\nAnderson, Cameron, Oliver P John, Dacher\nKeltner, and Ann M Kring. 2001. Who\nattains social status? effects of personality\nand physical attractiveness in social\ngroups. Journal of personality and social\npsychology, 81(1):116.\nAndersson, Lynne M and Christine M\nPearson. 1999. Tit for tat? the spiraling\neffect of incivility in the workplace.\nAcademy of management review,\n24(3):452\u2013471.\nArgyle, Lisa P, Ethan C Busby, Nancy Fulda,\nJoshua R Gubler, Christopher Rytting, and\nDavid Wingate. 2022. Out of one, many:\nUsing language models to simulate\nhuman samples. Political Analysis, pages\n1\u201315.\nBail, Chris. 2014. Terrified: How anti-Muslim\nfringe organizations became mainstream.\nPrinceton University Press.\nBail, Christopher A. 2016. Emotional\nfeedback and the viral spread of social\nmedia messages about autism spectrum\ndisorders. American journal of public health,\n106(7):1173\u20131180.\nBail, Christopher A. 2023. Can generative ai\nimprove social science?\nBakhtin, Mikhail Mikha\u02d8\u0131lovich. 2010. Speech\ngenres and other late essays. University of\nTexas press.\nBaly, Ramy, Giovanni Da San Martino, James\nGlass, and Preslav Nakov. 2020. We can\ndetect your bias: Predicting the political\nideology of news articles. In Proceedings of\nthe 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP),\npages 4982\u20134991, Association for\nComputational Linguistics, Online.\nBaly, Ramy, Georgi Karadzhov, Dimitar\nAlexandrov, James Glass, and Preslav\nNakov. 2018. Predicting factuality of\nreporting and bias of news media sources.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 3528\u20133539, Association\nfor Computational Linguistics, Brussels,\nBelgium.\nBamman, David, Jacob Eisenstein, and Tyler\nSchnoebelen. 2014. Gender identity and\nlexical variation in social media. Journal of\nSociolinguistics, 18(2):135\u2013160.\nBamman, David, Brendan O\u2019Connor, and\nNoah A. Smith. 2013. Learning latent\npersonas of film characters. In Proceedings\nof the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 352\u2013361, Association\nfor Computational Linguistics, Sofia,\nBulgaria.\nBamman, David and Noah A. Smith. 2015.\nOpen extraction of fine-grained political\nstatements. In Proceedings of the 2015\nConference on Empirical Methods in Natural\nLanguage Processing, pages 76\u201385,\nAssociation for Computational\nLinguistics, Lisbon, Portugal.\nBang, Yejin, Samuel Cahyawijaya, Nayeon\nLee, Wenliang Dai, Dan Su, Bryan Wilie,\nHoly Lovenia, Ziwei Ji, Tiezheng Yu, Willy\nChung, et al. 2023. A multitask,\nmultilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and\ninteractivity. ArXiv preprint,\nabs/2302.04023.\nBarber\u00e1, Pablo, Ning Wang, Richard\nBonneau, John T Jost, Jonathan Nagler,\nJoshua Tucker, and Sandra\nGonz\u00e1lez-Bail\u00f3n. 2015. The critical\nperiphery in the growth of social protests.\nPloS one, 10(11):e0143611.\nBeltagy, Iz, Matthew E Peters, and Arman\nCohan. 2020. Longformer: The\nlong-document transformer. ArXiv\npreprint, abs/2004.05150.\nBelz, Anja and Eric Kow. 2010. Comparing\nrating scales and preference judgements in\nlanguage evaluation. In Proceedings of the\n6th International Natural Language\nGeneration Conference.\nBender, Emily M, Timnit Gebru, Angelina\nMcMillan-Major, and Shmargaret\nShmitchell. 2021. On the dangers of\nstochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM\n37\nComputational Linguistics\nVolume 50, Number 1\nconference on fairness, accountability, and\ntransparency, pages 610\u2013623.\nBerelson, Bernard. 1952. Content analysis in\ncommunication research.\nvan den Berg, Esther, Katharina Korfhage,\nJosef Ruppenhofer, Michael Wiegand, and\nKatja Markert. 2020. Doctor who? framing\nthrough names and titles in German. In\nProceedings of the 12th Language Resources\nand Evaluation Conference, pages\n4924\u20134932, European Language Resources\nAssociation, Marseille, France.\nBhatia, Sudeep. 2017. Associative judgment\nand vector space semantics. Psychological\nreview, 124(1):1.\nBicchieri, Cristina. 2005. The grammar of\nsociety: The nature and dynamics of social\nnorms. Cambridge University Press.\nBlack, Ryan C, Sarah A Treul, Timothy R\nJohnson, and Jerry Goldman. 2011.\nEmotions, oral arguments, and supreme\ncourt decision making. The Journal of\nPolitics, 73(2):572\u2013581.\nBlodgett, Su Lin, Solon Barocas, Hal\nDaum\u00e9 III, and Hanna Wallach. 2020.\nLanguage (technology) is power: A critical\nsurvey of \u201cbias\u201d in NLP. In Proceedings of\nthe 58th Annual Meeting of the Association\nfor Computational Linguistics, pages\n5454\u20135476, Association for Computational\nLinguistics, Online.\nBollen, Johan, Huina Mao, and Xiaojun Zeng.\n2011. Twitter mood predicts the stock\nmarket. Journal of computational science,\n2(1):1\u20138.\nBommasani, Rishi, Kathleen A Creel,\nAnanya Kumar, Dan Jurafsky, and Percy S\nLiang. 2022. Picking on the same person:\nDoes algorithmic monoculture lead to\noutcome homogenization? Advances in\nNeural Information Processing Systems,\n35:3663\u20133678.\nBommasani, Rishi, Drew A Hudson, Ehsan\nAdeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities\nand risks of foundation models. ArXiv\npreprint, abs/2108.07258.\nBorchers, Conrad, Dalia Gala, Benjamin\nGilburt, Eduard Oravkin, Wilfried Bounsi,\nYuki M Asano, and Hannah Kirk. 2022.\nLooking for a handsome carpenter!\ndebiasing GPT-3 job advertisements. In\nProceedings of the 4th Workshop on Gender\nBias in Natural Language Processing\n(GeBNLP), pages 212\u2013224, Association for\nComputational Linguistics, Seattle,\nWashington.\nBox-Steffensmeier, Janet M and Bradford S\nJones. 2004. Event history modeling: A guide\nfor social scientists. Cambridge University\nPress.\nBoyd, Ryan L, Kate G Blackburn, and\nJames W Pennebaker. 2020. The narrative\narc: Revealing core narrative structures\nthrough text analysis. Science advances,\n6(32):eaba2196.\nBrahman, Faeze and Snigdha Chaturvedi.\n2020. Modeling protagonist emotions for\nemotion-aware storytelling. In Proceedings\nof the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP),\npages 5277\u20135294, Association for\nComputational Linguistics, Online.\nBramsen, Philip, Martha Escobar-Molano,\nAmi Patel, and Rafael Alonso. 2011.\nExtracting social power relationships from\nnatural language. In Proceedings of the 49th\nAnnual Meeting of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 773\u2013782, Association\nfor Computational Linguistics, Portland,\nOregon, USA.\nBrooke, Julian, Adam Hammond, and\nTimothy Baldwin. 2016. Bootstrapped\ntext-level named entity recognition for\nliterature. In Proceedings of the 54th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages\n344\u2013350, Association for Computational\nLinguistics, Berlin, Germany.\nBrown, Penelope and Stephen C Levinson.\n1987. Politeness: Some universals in language\nusage, volume 4. Cambridge university\npress.\nBrown, Tom B., Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language\nmodels are few-shot learners. In Advances\nin Neural Information Processing Systems 33:\nAnnual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nBucholtz, Mary and Kira Hall. 2005. Identity\nand interaction: A sociocultural linguistic\napproach. Discourse studies, 7(4-5):585\u2013614.\n38\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nBuechel, Sven, Anneke Buffone, Barry Slaff,\nLyle Ungar, and Jo\u00e3o Sedoc. 2018.\nModeling empathy and distress in\nreaction to news stories. In Proceedings of\nthe 2018 Conference on Empirical Methods in\nNatural Language Processing, pages\n4758\u20134765, Association for Computational\nLinguistics, Brussels, Belgium.\nBunt, Harry, Jan Alexandersson, Jean\nCarletta, Jae-Woong Choe, Alex Chengyu\nFang, Koiti Hasida, Kiyong Lee, Volha\nPetukhova, Andrei Popescu-Belis, Laurent\nRomary, Claudia Soria, and David Traum.\n2010. Towards an ISO standard for\ndialogue act annotation. In Proceedings of\nthe Seventh International Conference on\nLanguage Resources and Evaluation\n(LREC\u201910), European Language Resources\nAssociation (ELRA), Valletta, Malta.\nBurke, Moira and Robert Kraut. 2008. Mind\nyour ps and qs: the impact of politeness\nand rudeness in online communities. In\nProceedings of the 2008 ACM conference on\nComputer supported cooperative work, pages\n281\u2013284.\nCaciularu, Avi, Arman Cohan, Iz Beltagy,\nMatthew E Peters, Arie Cattan, and Ido\nDagan. 2021. Cdlm: Cross-document\nlanguage modeling. In Findings of the\nAssociation for Computational Linguistics:\nEMNLP 2021, pages 2648\u20132662.\nCarlini, Nicholas, Florian Tramer, Eric\nWallace, Matthew Jagielski, Ariel\nHerbert-Voss, Katherine Lee, Adam\nRoberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training\ndata from large language models. In 30th\nUSENIX Security Symposium (USENIX\nSecurity 21), pages 2633\u20132650.\nCarlo, Valerio Di, Federico Bianchi, and\nMatteo Palmonari. 2019. Training\ntemporal word embeddings with a\ncompass. In The Thirty-Third AAAI\nConference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative\nApplications of Artificial Intelligence\nConference, IAAI 2019, The Ninth AAAI\nSymposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu,\nHawaii, USA, January 27 - February 1, 2019,\npages 6326\u20136334, AAAI Press.\nCentola, Damon, Joshua Becker, Devon\nBrackbill, and Andrea Baronchelli. 2018.\nExperimental evidence for tipping points\nin social convention. Science,\n360(6393):1116\u20131119.\nChaganty, Arun, Stephen Mussmann, and\nPercy Liang. 2018. The price of debiasing\nautomatic metrics in natural language\nevalaution. In Proceedings of the 56th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 643\u2013653, Association for\nComputational Linguistics, Melbourne,\nAustralia.\nChakrabarty, Tuhin, Debanjan Ghosh, Adam\nPoliak, and Smaranda Muresan. 2021.\nFigurative language in recognizing textual\nentailment. In Findings of the Association for\nComputational Linguistics: ACL-IJCNLP\n2021, pages 3354\u20133361, Association for\nComputational Linguistics, Online.\nChakrabarty, Tuhin, Arkadiy Saakyan,\nDebanjan Ghosh, and Smaranda Muresan.\n2022. Flute: Figurative language\nunderstanding through textual\nexplanations. In Proceedings of the 2022\nConference on Empirical Methods in Natural\nLanguage Processing, pages 7139\u20137159.\nChambers, Nathanael and Dan Jurafsky.\n2008. Unsupervised learning of narrative\nevent chains. In Proceedings of ACL-08:\nHLT, pages 789\u2013797, Association for\nComputational Linguistics, Columbus,\nOhio.\nChang, Jonathan, Jordan L. Boyd-Graber,\nSean Gerrish, Chong Wang, and David M.\nBlei. 2009. Reading tea leaves: How\nhumans interpret topic models. In\nAdvances in Neural Information Processing\nSystems 22: 23rd Annual Conference on\nNeural Information Processing Systems 2009.\nProceedings of a meeting held 7-10 December\n2009, Vancouver, British Columbia, Canada,\npages 288\u2013296, Curran Associates, Inc.\nChang, Jonathan P., Caleb Chiam, Liye Fu,\nAndrew Wang, Justine Zhang, and\nCristian Danescu-Niculescu-Mizil. 2020.\nConvoKit: A toolkit for the analysis of\nconversations. In Proceedings of the 21th\nAnnual Meeting of the Special Interest Group\non Discourse and Dialogue, pages 57\u201360,\nAssociation for Computational\nLinguistics, 1st virtual meeting.\nChen, Derek, Zhou Yu, and Samuel Bowman.\n2022. Clean or annotate: How to spend a\nlimited data collection budget. In\nProceedings of the Third Workshop on Deep\nLearning for Low-Resource Natural Language\nProcessing, pages 152\u2013168.\nChen, Peng-Yu and Von-Wun Soo. 2018.\nHumor recognition using deep learning.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages\n113\u2013117, Association for Computational\nLinguistics, New Orleans, Louisiana.\n39\nComputational Linguistics\nVolume 50, Number 1\nCheng, Justin, Lada A. Adamic, Jon M.\nKleinberg, and Jure Leskovec. 2016. Do\ncascades recur? In Proceedings of the 25th\nInternational Conference on World Wide Web,\nWWW 2016, Montreal, Canada, April 11 - 15,\n2016, pages 671\u2013681, ACM.\nCheng, Justin, Michael Bernstein, Cristian\nDanescu-Niculescu-Mizil, and Jure\nLeskovec. 2017. Anyone can become a\ntroll: Causes of trolling behavior in online\ndiscussions. In Proceedings of the 2017 ACM\nconference on computer supported cooperative\nwork and social computing, pages 1217\u20131230.\nCheng, Justin, Cristian\nDanescu-Niculescu-Mizil, and Jure\nLeskovec. 2015. Antisocial behavior in\nonline discussion communities. In\nProceedings of the international aaai\nconference on web and social media,\nvolume 9, pages 61\u201370.\nChild, Rewon, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long\nsequences with sparse transformers.\nArXiv preprint, abs/1904.10509.\nChristiano, Paul F., Jan Leike, Tom B. Brown,\nMiljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep reinforcement\nlearning from human preferences. In\nAdvances in Neural Information Processing\nSystems 30: Annual Conference on Neural\nInformation Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA,\npages 4299\u20134307.\nChu, Eric, Jacob Andreas, Stephen\nAnsolabehere, and Deb Roy. 2023.\nLanguage models trained on media diets\ncan predict public opinion. ArXiv preprint,\nabs/2303.16779.\nChu, Eric, Prashanth Vijayaraghavan, and\nDeb Roy. 2018. Learning personas from\ndialogue with attentive memory networks.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 2638\u20132646, Association\nfor Computational Linguistics, Brussels,\nBelgium.\nChung, Hyung Won, Le Hou, Shayne\nLongpre, Barret Zoph, Yi Tay, William\nFedus, Eric Li, Xuezhi Wang, Mostafa\nDehghani, Siddhartha Brahma, et al. 2022.\nScaling instruction-finetuned language\nmodels. ArXiv preprint, abs/2210.11416.\nCialdini, Robert B. 2003. Influence. Influence\nAt Work.\nColl Ardanuy, Mariona, Federico Nanni,\nKaspar Beelen, Kasra Hosseini, Ruth\nAhnert, Jon Lawrence, Katherine\nMcDonough, Giorgia Tolfo, Daniel CS\nWilson, and Barbara McGillivray. 2020.\nLiving machines: A study of atypical\nanimacy. In Proceedings of the 28th\nInternational Conference on Computational\nLinguistics, pages 4534\u20134545, International\nCommittee on Computational Linguistics,\nBarcelona, Spain (Online).\nCotfas, Liviu-Adrian, Camelia Delcea, Ioan\nRoxin, Corina Ioan\u02d8a\u00b8s, Dana Simona\nGherai, and Federico Tajariol. 2021. The\nlongest month: analyzing covid-19\nvaccination opinions dynamics from\ntweets in the month following the first\nvaccine announcement. Ieee Access,\n9:33203\u201333223.\nCoulton, Paul, Jonny Huck, Andrew\nHudson-Smith, Ralph Barthel, Panagiotis\nMavros, Jennifer Roberts, and Philip\nPowell. 2014. Designing interactive\nsystems to encourage empathy between\nusers. In Proceedings of the 2014 companion\npublication on Designing interactive systems.\npages 13\u201316.\nCraig, Holly K and Julie A Washington. 2002.\nOral language expectations for african\namerican preschoolers and kindergartners.\nDanescu-Niculescu-Mizil, Cristian, Lillian\nLee, Bo Pang, and Jon M. Kleinberg. 2012.\nEchoes of power: language effects and\npower differences in social interaction. In\nProceedings of the 21st World Wide Web\nConference 2012, WWW 2012, Lyon, France,\nApril 16-20, 2012, pages 699\u2013708, ACM.\nDanescu-Niculescu-Mizil, Cristian, Moritz\nSudhof, Dan Jurafsky, Jure Leskovec, and\nChristopher Potts. 2013. A computational\napproach to politeness with application to\nsocial factors. In Proceedings of the 51st\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 250\u2013259, Association for\nComputational Linguistics, Sofia,\nBulgaria.\nDang, Hoa Trang. 2005. Overview of duc\n2005. In Proceedings of the document\nunderstanding conference, volume 2005,\npages 1\u201312, Citeseer.\nDavies, Catherine E. 2017. Sociolinguistic\napproaches to humor. In The Routledge\nhandbook of language and humor. Routledge,\npages 472\u2013488.\nDel Tredici, Marco and Raquel Fern\u00e1ndez.\n2017. Semantic variation in online\ncommunities of practice. In IWCS 2017 -\n12th International Conference on\nComputational Semantics - Long papers.\nDeLozier, Grant, Ben Wing, Jason Baldridge,\nand Scott Nesbit. 2016. Creating a novel\ngeolocation corpus from historical texts. In\nProceedings of the 10th Linguistic Annotation\n40\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nWorkshop held in conjunction with ACL 2016\n(LAW-X 2016), pages 188\u2013198, Association\nfor Computational Linguistics, Berlin,\nGermany.\nDemszky, Dorottya, Nikhil Garg, Rob Voigt,\nJames Zou, Jesse Shapiro, Matthew\nGentzkow, and Dan Jurafsky. 2019.\nAnalyzing polarization in social media:\nMethod and application to tweets on 21\nmass shootings. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), pages 2970\u20133005,\nAssociation for Computational\nLinguistics, Minneapolis, Minnesota.\nDemszky, Dorottya, Devyani Sharma,\nJonathan Clark, Vinodkumar\nPrabhakaran, and Jacob Eisenstein. 2021.\nLearning to recognize dialect features. In\nProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 2315\u20132338, Association\nfor Computational Linguistics, Online.\nDhamala, Jwala, Tony Sun, Varun Kumar,\nSatyapriya Krishna, Yada Pruksachatkun,\nKai-Wei Chang, and Rahul Gupta. 2021.\nBold: Dataset and metrics for measuring\nbiases in open-ended language generation.\nIn Proceedings of the 2021 ACM conference\non fairness, accountability, and transparency,\npages 862\u2013872.\nDiMaggio, Paul. 2015. Adapting\ncomputational text analysis to social\nscience (and vice versa). Big Data &\nSociety, 2(2):2053951715602908.\nDiMaggio, Paul, Manish Nag, and David\nBlei. 2013. Exploiting affinities between\ntopic modeling and the sociological\nperspective on culture: Application to\nnewspaper coverage of us government\narts funding. Poetics, 41(6):570\u2013606.\nDobson, James E. 2019. Critical digital\nhumanities: the search for a methodology.\nUniversity of Illinois Press.\nEasley, David and Jon Kleinberg. 2010.\nNetworks, crowds, and markets: Reasoning\nabout a highly connected world. Cambridge\nuniversity press.\nEgami, Naoki, Musashi Jacobs-Harukawa,\nBrandon M Stewart, and Hanying Wei.\n2023. Using large language model\nannotations for valid downstream\nstatistical inference in social science:\nDesign-based semi-supervised learning.\nArXiv preprint, abs/2306.04746.\nEisenstein, Jacob. 2012. Mapping the\ngeographical diffusion of new words.\nArXiv preprint, abs/1210.5268.\nEisenstein, Jacob, Brendan O\u2019Connor,\nNoah A Smith, and Eric P Xing. 2014.\nDiffusion of lexical change in social media.\nPloS one, 9(11):e113114.\nEisenstein, Jacob, Vinodkumar Prabhakaran,\nClara Rivera, Dorottya Demszky, and\nDevyani Sharma. 2023. Md3: The\nmulti-dialect dataset of dialogues. ArXiv\npreprint, abs/2305.11355.\nEisenstein, Jacob, Noah A. Smith, and Eric P.\nXing. 2011. Discovering sociolinguistic\nassociations with structured sparsity. In\nProceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics:\nHuman Language Technologies, pages\n1365\u20131374, Association for Computational\nLinguistics, Portland, Oregon, USA.\nEllsberg, Daniel. 1961. Risk, ambiguity, and\nthe savage axioms. The quarterly journal of\neconomics, 75(4):643\u2013669.\nElSherief, Mai, Caleb Ziems, David\nMuchlinski, Vaishnavi Anupindi, Jordyn\nSeybolt, Munmun De Choudhury, and\nDiyi Yang. 2021. Latent hatred: A\nbenchmark for understanding implicit\nhate speech. ArXiv preprint,\nabs/2109.05322.\nEntman, Robert M. 1993. Framing: Toward\nclarification of a fractured paradigm.\nJournal of communication, 43(4):51\u201358.\nEvans, James A and Pedro Aceves. 2016.\nMachine translation: Mining text for social\ntheory. Annual review of sociology, 42:21\u201350.\nFabbri, Alexander R, Wojciech Kry\u00b4sci\u00b4nski,\nBryan McCann, Caiming Xiong, Richard\nSocher, and Dragomir Radev. 2021.\nSummeval: Re-evaluating summarization\nevaluation. Transactions of the Association\nfor Computational Linguistics, 9:391\u2013409.\nFazeen, Mohamed, Ram Dantu, and\nParthasarathy Guturu. 2011. Identification\nof leaders, lurkers, associates and\nspammers in a social network:\ncontext-dependent and\ncontext-independent approaches. Social\nNetwork Analysis and Mining, 1(3):241\u2013254.\nField, Anjalie, Doron Kliger, Shuly Wintner,\nJennifer Pan, Dan Jurafsky, and Yulia\nTsvetkov. 2018. Framing and\nagenda-setting in Russian news: a\ncomputational analysis of intricate\npolitical strategies. In Proceedings of the\n2018 Conference on Empirical Methods in\nNatural Language Processing, pages\n3570\u20133580, Association for Computational\nLinguistics, Brussels, Belgium.\nFlores, Ren\u00e9 D. 2017. Do anti-immigrant\nlaws shape public sentiment? a study of\n41\nComputational Linguistics\nVolume 50, Number 1\narizona\u2019s sb 1070 using twitter data.\nAmerican Journal of Sociology,\n123(2):333\u2013384.\nGabriel, Saadia, Skyler Hallinan, Maarten\nSap, Pemi Nguyen, Franziska Roesner,\nEunsol Choi, and Yejin Choi. 2022. Misinfo\nreaction frames: Reasoning about readers\u2019\nreactions to news headlines. In Proceedings\nof the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 3108\u20133127.\nGao, Ge, Eunsol Choi, Yejin Choi, and Luke\nZettlemoyer. 2018. Neural metaphor\ndetection in context. In Proceedings of the\n2018 Conference on Empirical Methods in\nNatural Language Processing, pages\n607\u2013613, Association for Computational\nLinguistics, Brussels, Belgium.\nGarfinkel, Harold. 2016. Studies in\nethnomethodology. In Social Theory\nRe-Wired. Routledge, pages 85\u201395.\nGarg, Nikhil, Londa Schiebinger, Dan\nJurafsky, and James Zou. 2018. Word\nembeddings quantify 100 years of gender\nand ethnic stereotypes. Proceedings of the\nNational Academy of Sciences,\n115(16):E3635\u2013E3644.\nGerber, Alan S, Gregory A Huber, David\nDoherty, Conor M Dowling, and Shang E\nHa. 2010. Personality and political\nattitudes: Relationships across issue\ndomains and political contexts. American\nPolitical Science Review, 104(1):111\u2013133.\nGilardi, Fabrizio, Meysam Alizadeh, and\nMa\u00ebl Kubli. 2023. Chatgpt outperforms\ncrowd-workers for text-annotation tasks.\nArXiv preprint, abs/2303.15056.\nGolder, Scott A and Michael W Macy. 2014.\nDigital footprints: Opportunities and\nchallenges for online social research.\nAnnual Review of Sociology, 40:129\u2013152.\nGomez-Zara, Diego, Miriam Boon, and Larry\nBirnbaum. 2018. Who is the hero, the\nvillain, and the victim? detection of roles\nin news articles using natural language\ntechniques. In 23rd International Conference\non Intelligent User Interfaces, pages 311\u2013315.\nGoyal, Tanya, Junyi Jessy Li, and Greg\nDurrett. 2022. News summarization and\nevaluation in the era of gpt-3. ArXiv\npreprint, abs/2209.12356.\nGraham, Elizabeth E. 1995. The involvement\nof sense of humor in the development of\nsocial relationships. Communication\nReports, 8(2):158\u2013169.\nGrimmer, Justin. 2010. A bayesian\nhierarchical topic model for political texts:\nMeasuring expressed agendas in senate\npress releases. Political Analysis, 18(1):1\u201335.\nGross, Justin H, Brice Acree, Yanchuan Sim,\nand Noah A Smith. 2013. Testing the\netch-a-sketch hypothesis: a computational\nanalysis of mitt romney\u2019s ideological\nmakeover during the 2012 primary vs.\ngeneral elections. In APSA 2013 Annual\nMeeting Paper, American Political Science\nAssociation 2013 Annual Meeting.\nGrudin, Jonathan. 2006. Why personas work:\nThe psychological evidence. The persona\nlifecycle, 12:642\u2013664.\nHamilton, William L., Jure Leskovec, and\nDan Jurafsky. 2016a. Cultural shift or\nlinguistic drift? comparing two\ncomputational measures of semantic\nchange. In Proceedings of the 2016\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2116\u20132121,\nAssociation for Computational\nLinguistics, Austin, Texas.\nHamilton, William L., Jure Leskovec, and\nDan Jurafsky. 2016b. Diachronic word\nembeddings reveal statistical laws of\nsemantic change. In Proceedings of the 54th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1489\u20131501, Association for\nComputational Linguistics, Berlin,\nGermany.\nHarzing, Anne-Wil, Joyce Baldueza, Wilhelm\nBarner-Rasmussen, Cordula Barzantny,\nAnne Canabal, Anabella Davila, Alvaro\nEspejo, Rita Ferreira, Axele Giroud,\nKathrin Koester, et al. 2009. Rating versus\nranking: What is the best way to reduce\nresponse and language bias in\ncross-national research? International\nbusiness review, 18(4):417\u2013432.\nHashim, Safair Safwat Mohammed and\nSuhair Safwat. 2015. Speech acts in\npolitical speeches. Journal of Modern\nEducation Review, 5(7):699\u2013706.\nHendrycks, Dan, Collin Burns, Steven Basart,\nAndy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. 2021. Measuring\nmassive multitask language\nunderstanding. In International Conference\non Learning Representations.\nHerring, Susan C. 1994. Politeness in\ncomputer culture: Why women thank and\nmen flame. In Cultural performances:\nProceedings of the third Berkeley women and\nlanguage conference, pages 278\u2013294.\nHirsh, Jacob B, Sonia K Kang, and Galen V\nBodenhausen. 2012. Personalized\npersuasion: Tailoring persuasive appeals\nto recipients\u2019 personality traits.\nPsychological science, 23(6):578\u2013581.\n42\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nHoffmann, Jordan, Sebastian Borgeaud,\nArthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las\nCasas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. Training\ncompute-optimal large language models.\nArXiv preprint, abs/2203.15556.\nHofman, Jake M, Amit Sharma, and\nDuncan J Watts. 2017. Prediction and\nexplanation in social systems. Science,\n355(6324):486\u2013488.\nHuguet Cabot, Pere-Llu\u00eds, Verna Dankers,\nDavid Abadi, Agneta Fischer, and\nEkaterina Shutova. 2020. The Pragmatics\nbehind Politics: Modelling Metaphor,\nFraming and Emotion in Political\nDiscourse. In Findings of the Association for\nComputational Linguistics: EMNLP 2020,\npages 4479\u20134488, Association for\nComputational Linguistics, Online.\nH\u00fcrriyeto\u02d8glu, Ali, Hristo Tanev, Vanni\nZavarella, Jakub Piskorski, Reyyan\nYeniterzi, Deniz Yuret, and Aline\nVillavicencio. 2021. Challenges and\napplications of automated extraction of\nsocio-political events from text (CASE\n2021): Workshop and shared task report.\nIn Proceedings of the 4th Workshop on\nChallenges and Applications of Automated\nExtraction of Socio-political Events from Text\n(CASE 2021), pages 1\u20139, Association for\nComputational Linguistics, Online.\nHutchby, Ian and Robin Wooffitt. 2008.\nConversation analysis. Polity.\nIgnatow, Gabe and Rada Mihalcea. 2016. Text\nmining: A guidebook for the social sciences.\nSage Publications.\nIyengar, Shanto. 1990. Framing responsibility\nfor political issues: The case of poverty.\nPolitical behavior, 12(1):19\u201340.\nIyer, Adith, Aditya Joshi, Sarvnaz Karimi,\nRoss Sparks, and Cecile Paris. 2019.\nFigurative usage detection of symptom\nwords to improve personal health mention\ndetection. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 1142\u20131147, Association\nfor Computational Linguistics, Florence,\nItaly.\nIyyer, Mohit, Peter Enns, Jordan\nBoyd-Graber, and Philip Resnik. 2014.\nPolitical ideology detection using\nrecursive neural networks. In Proceedings\nof the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 1113\u20131122, Association\nfor Computational Linguistics, Baltimore,\nMaryland.\nIyyer, Mohit, Anupam Guha, Snigdha\nChaturvedi, Jordan Boyd-Graber, and Hal\nDaum\u00e9 III. 2016. Feuding families and\nformer Friends: Unsupervised learning for\ndynamic fictional relationships. In\nProceedings of the 2016 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 1534\u20131544, Association\nfor Computational Linguistics, San Diego,\nCalifornia.\nJacobs, Arthur M and Annette Kinder. 2018.\nWhat makes a metaphor literary? answers\nfrom two computational studies. Metaphor\nand Symbol, 33(2):85\u2013100.\nJahan, Labiba, Rahul Mittal, and Mark\nFinlayson. 2021. Inducing stereotypical\ncharacter roles from plot structure. In\nProceedings of the 2021 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 492\u2013497.\nJelveh, Zubin, Bruce Kogut, and Suresh\nNaidu. 2014. Detecting latent ideology in\nexpert text: Evidence from academic\npapers in economics. In Proceedings of the\n2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP),\npages 1804\u20131809, Association for\nComputational Linguistics, Doha, Qatar.\nJenhani, Ferdaous, Mohamed Salah Gouider,\nand Lamjed Ben Said. 2016. A hybrid\napproach for drug abuse events extraction\nfrom twitter. Procedia computer science,\n96:1032\u20131040.\nJochim, Charles, Francesca Bonin, Roy\nBar-Haim, and Noam Slonim. 2018. SLIDE\n- a sentiment lexicon of common idioms.\nIn Proceedings of the Eleventh International\nConference on Language Resources and\nEvaluation (LREC 2018), European\nLanguage Resources Association (ELRA),\nMiyazaki, Japan.\nJockers, Matthew L and David Mimno. 2013.\nSignificant themes in 19th-century\nliterature. Poetics, 41(6):750\u2013769.\nJohnson, Kristen, I-Ta Lee, and Dan\nGoldwasser. 2017. Ideological phrase\nindicators for classification of political\ndiscourse framing on Twitter. In\nProceedings of the Second Workshop on NLP\nand Computational Social Science, pages\n90\u201399, Association for Computational\nLinguistics, Vancouver, Canada.\nJones, Erik and Jacob Steinhardt. 2022.\nCapturing failures of large language\nmodels via human cognitive biases. ArXiv\npreprint, abs/2202.12299.\nJoshi, Aditya, Pushpak Bhattacharyya, and\nMark J Carman. 2017. Automatic sarcasm\n43\nComputational Linguistics\nVolume 50, Number 1\ndetection: A survey. ACM Computing\nSurveys (CSUR), 50(5):1\u201322.\nKaplan, Jared, Sam McCandlish, Tom\nHenighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford,\nJeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nArXiv preprint, abs/2001.08361.\nKarpinska, Marzena, Nader Akoury, and\nMohit Iyyer. 2021. The perils of using\nMechanical Turk to evaluate open-ended\ntext generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural\nLanguage Processing, pages 1265\u20131285,\nAssociation for Computational\nLinguistics, Online and Punta Cana,\nDominican Republic.\nKershaw, Daniel, Matthew Rowe, and\nPatrick Stacey. 2016. Towards modelling\nlanguage innovation acceptance in online\nsocial networks. In Proceedings of the Ninth\nACM International Conference on Web Search\nand Data Mining, San Francisco, CA, USA,\nFebruary 22-25, 2016, pages 553\u2013562, ACM.\nKesarwani, Vaibhav, Diana Inkpen, Stan\nSzpakowicz, and Chris Tanasescu. 2017.\nMetaphor detection in a poetry corpus. In\nProceedings of the Joint SIGHUM Workshop\non Computational Linguistics for Cultural\nHeritage, Social Sciences, Humanities and\nLiterature, pages 1\u20139, Association for\nComputational Linguistics, Vancouver,\nCanada.\nKeuschnigg, Marc, Niclas Lovsj\u00f6, and Peter\nHedstr\u00f6m. 2018. Analytical sociology and\ncomputational social science. Journal of\nComputational Social Science, 1(1):3\u201314.\nKiela, Douwe, Max Bartolo, Yixin Nie,\nDivyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, Grusha\nPrasad, Amanpreet Singh, Pratik Ringshia,\nZhiyi Ma, Tristan Thrush, Sebastian\nRiedel, Zeerak Waseem, Pontus Stenetorp,\nRobin Jia, Mohit Bansal, Christopher Potts,\nand Adina Williams. 2021. Dynabench:\nRethinking benchmarking in NLP. In\nProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 4110\u20134124, Association\nfor Computational Linguistics, Online.\nKim, Tae-Yeol, Deog-Ro Lee, and Noel\nYuen Shan Wong. 2016. Supervisor humor\nand employee outcomes: The role of social\ndistance and affective trust in supervisor.\nJournal of Business and Psychology,\n31:125\u2013139.\nKim, Yoon, Yi-I Chiu, Kentaro Hanaki,\nDarshan Hegde, and Slav Petrov. 2014.\nTemporal analysis of language through\nneural language models. In Proceedings of\nthe ACL 2014 Workshop on Language\nTechnologies and Computational Social\nScience, pages 61\u201365, Association for\nComputational Linguistics, Baltimore,\nMD, USA.\nKing, Gary and Will Lowe. 2003. An\nautomated information extraction tool for\ninternational conflict data with\nperformance as good as human coders: A\nrare events evaluation design. International\nOrganization, 57(3):617\u2013642.\nKirby, Simon, Mike Dowman, and Thomas L\nGriffiths. 2007. Innateness and culture in\nthe evolution of language. Proceedings of\nthe National Academy of Sciences,\n104(12):5241\u20135245.\nKleinberg, Jon, Himabindu Lakkaraju, Jure\nLeskovec, Jens Ludwig, and Sendhil\nMullainathan. 2018. Human decisions and\nmachine predictions. The quarterly journal\nof economics, 133(1):237\u2013293.\nKleinberg, Jon and Manish Raghavan. 2021.\nAlgorithmic monoculture and social\nwelfare. Proceedings of the National\nAcademy of Sciences, 118(22):e2018340118.\nKoralus, Philipp and Vincent\nWang-Ma\u00b4scianica. 2023. Humans in\nhumans out: On gpt converging toward\ncommon sense in both success and failure.\nArXiv preprint, abs/2303.17276.\nKosinski, Michal, David Stillwell, and Thore\nGraepel. 2013. Private traits and attributes\nare predictable from digital records of\nhuman behavior. Proceedings of the national\nacademy of sciences, 110(15):5802\u20135805.\nKozlowski, Austin C, Matt Taddy, and\nJames A Evans. 2019. The geometry of\nculture: Analyzing the meanings of class\nthrough word embeddings. American\nSociological Review, 84(5):905\u2013949.\nKuhn, Thomas S. 1962. The structure of\nScientific Revolutions. The University of\nChicago Press.\nKuipers, Giselinde. 2009. Humor styles and\nsymbolic boundaries.\nKulkarni, Vivek, Rami Al-Rfou, Bryan\nPerozzi, and Steven Skiena. 2015.\nStatistically significant detection of\nlinguistic change. In Proceedings of the 24th\nInternational Conference on World Wide Web,\nWWW 2015, Florence, Italy, May 18-22,\n2015, pages 625\u2013635, ACM.\nKwak, Haewoon, Jeremy Blackburn, and\nSeungyeop Han. 2015. Exploring\ncyberbullying and other toxic behavior in\nteam competition online games. In\nProceedings of the 33rd Annual ACM\n44\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nConference on Human Factors in Computing\nSystems, CHI 2015, Seoul, Republic of Korea,\nApril 18-23, 2015, pages 3739\u20133748, ACM.\nLabatut, Vincent and Xavier Bost. 2019.\nExtraction and analysis of fictional\ncharacter networks: A survey. ACM\nComputing Surveys (CSUR), 52(5):1\u201340.\nLai, Mirko, Alessandra Teresa Cignarella,\nDelia Iraz\u00fa Hern\u00e1ndez Far\u00edas, Cristina\nBosco, Viviana Patti, and Paolo Rosso.\n2020. Multilingual stance detection in\nsocial media political debates. Computer\nSpeech & Language, 63:101075.\nLai, Viet, Minh Van Nguyen, Heidi Kaufman,\nand Thien Huu Nguyen. 2021. Event\nextraction from historical texts: A new\ndataset for black rebellions. In Findings of\nthe Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 2390\u20132400,\nAssociation for Computational\nLinguistics, Online.\nLazer, David, Alex Pentland, Lada Adamic,\nSinan Aral, Albert-L\u00e1szl\u00f3 Barab\u00e1si, Devon\nBrewer, Nicholas Christakis, Noshir\nContractor, James Fowler, Myron\nGutmann, et al. 2009. Computational\nsocial science. Science, 323(5915):721\u2013723.\nLazer, David MJ, Matthew A Baum, Yochai\nBenkler, Adam J Berinsky, Kelly M\nGreenhill, Filippo Menczer, Miriam J\nMetzger, Brendan Nyhan, Gordon\nPennycook, David Rothschild, et al. 2018.\nThe science of fake news. Science,\n359(6380):1094\u20131096.\nLazer, David MJ, Alex Pentland, Duncan J\nWatts, Sinan Aral, Susan Athey, Noshir\nContractor, Deen Freelon, Sandra\nGonzalez-Bailon, Gary King, Helen\nMargetts, et al. 2020. Computational social\nscience: Obstacles and opportunities.\nScience, 369(6507):1060\u20131062.\nLemmens, Jens, Ilia Markov, and Walter\nDaelemans. 2021. Improving hate speech\ntype and target detection with hateful\nmetaphor features. In Proceedings of the\nFourth Workshop on NLP for Internet\nFreedom: Censorship, Disinformation, and\nPropaganda, pages 7\u201316, Association for\nComputational Linguistics, Online.\nLeskovec, Jure, Lars Backstrom, and Jon\nKleinberg. 2009. Meme-tracking and the\ndynamics of the news cycle. In Proceedings\nof the 15th ACM SIGKDD international\nconference on Knowledge discovery and data\nmining, pages 497\u2013506.\nLi, Sha, Heng Ji, and Jiawei Han. 2021.\nDocument-level event argument\nextraction by conditional generation. In\nProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 894\u2013908, Association\nfor Computational Linguistics, Online.\nLi, Shuang, Xavier Puig, Chris Paxton, Yilun\nDu, Clinton Wang, Linxi Fan, Tao Chen,\nDe-An Huang, Ekin Aky\u00fcrek, Anima\nAnandkumar, et al. 2022. Pre-trained\nlanguage models for interactive\ndecision-making. Advances in Neural\nInformation Processing Systems,\n35:31199\u201331212.\nLiang, Percy, Rishi Bommasani, Tony Lee,\nDimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak\nNarayanan, Yuhuai Wu, Ananya Kumar,\net al. 2022. Holistic evaluation of language\nmodels. ArXiv preprint, abs/2211.09110.\nLiu, Chia-Wei, Ryan Lowe, Iulian Serban,\nMike Noseworthy, Laurent Charlin, and\nJoelle Pineau. 2016. How NOT to evaluate\nyour dialogue system: An empirical study\nof unsupervised evaluation metrics for\ndialogue response generation. In\nProceedings of the 2016 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 2122\u20132132, Association\nfor Computational Linguistics, Austin,\nTexas.\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and\nVeselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining\napproach.\nLucy, Li and David Bamman. 2021. Gender\nand representation bias in GPT-3\ngenerated stories. In Proceedings of the\nThird Workshop on Narrative Understanding,\npages 48\u201355, Association for\nComputational Linguistics, Virtual.\nLuo, Yiwei, Dallas Card, and Dan Jurafsky.\n2020a. Desmog: Detecting stance in media\non global warming. In Proceedings of the\n2020 Conference on Empirical Methods in\nNatural Language Processing: Findings,\npages 3296\u20133315.\nLuo, Yiwei, Dallas Card, and Dan Jurafsky.\n2020b. Detecting stance in media on global\nwarming. In Findings of the Association for\nComputational Linguistics: EMNLP 2020,\npages 3296\u20133315, Association for\nComputational Linguistics, Online.\nMa, Yukun, Khanh Linh Nguyen, Frank Z\nXing, and Erik Cambria. 2020. A survey\non empathetic dialogue systems.\nInformation Fusion, 64:50\u201370.\nMachina, Mark J. 1987. Choice under\nuncertainty: Problems solved and\n45\nComputational Linguistics\nVolume 50, Number 1\nunsolved. Journal of Economic perspectives,\n1(1):121\u2013154.\nMaki, Keith, Michael Yoder, Yohan Jo, and\nCarolyn Ros\u00e9. 2017. Roles and success in\nWikipedia talk pages: Identifying latent\npatterns of behavior. In Proceedings of the\nEighth International Joint Conference on\nNatural Language Processing (Volume 1:\nLong Papers), pages 1026\u20131035, Asian\nFederation of Natural Language\nProcessing, Taipei, Taiwan.\nMarkiewicz, Dorothy. 1974. Effects of humor\non persuasion. Sociometry, pages 407\u2013422.\nMartin, Rod A and Thomas Ford. 2018. The\npsychology of humor: An integrative approach.\nAcademic press.\nMartino, Giovanni Da San, Stefano Cresci,\nAlberto Barr\u00f3n-Cede\u00f1o, Seunghak Yu,\nRoberto Di Pietro, and Preslav Nakov.\n2020. A survey on computational\npropaganda detection. In Proceedings of the\nTwenty-Ninth International Joint Conference\non Artificial Intelligence, IJCAI 2020, pages\n4826\u20134832, ijcai.org.\nMathew, Binny, Anurag Illendula, Punyajoy\nSaha, Soumya Sarkar, Pawan Goyal, and\nAnimesh Mukherjee. 2020. Hate begets\nhate: A temporal study of hate speech.\nProceedings of the ACM on Human-Computer\nInteraction, 4(CSCW2):1\u201324.\nMendelsohn, Julia, Ceren Budak, and David\nJurgens. 2021. Modeling framing in\nimmigration discourse on social media. In\nProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 2219\u20132263, Association\nfor Computational Linguistics, Online.\nMihalcea, Rada and Vivi Nastase. 2012.\nWord epoch disambiguation: Finding how\nwords change over time. In Proceedings of\nthe 50th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2:\nShort Papers), pages 259\u2013263, Association\nfor Computational Linguistics, Jeju Island,\nKorea.\nMihalcea, Rada and Carlo Strapparava. 2005.\nMaking computers laugh: Investigations\nin automatic humor recognition. In\nProceedings of Human Language Technology\nConference and Conference on Empirical\nMethods in Natural Language Processing,\npages 531\u2013538, Association for\nComputational Linguistics, Vancouver,\nBritish Columbia, Canada.\nMiller, Tim. 2019. Explanation in artificial\nintelligence: Insights from the social\nsciences. Artificial intelligence, 267:1\u201338.\nMin, Bonan and Xiaoxi Zhao. 2019. Measure\ncountry-level socio-economic indicators\nwith streaming news: An empirical study.\nIn Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1249\u20131254,\nAssociation for Computational\nLinguistics, Hong Kong, China.\nMishra, Abhijit, Tarun Tater, and Karthik\nSankaranarayanan. 2019. A modular\narchitecture for unsupervised sarcasm\ngeneration. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n6144\u20136154, Association for Computational\nLinguistics, Hong Kong, China.\nMohammad, Saif, Svetlana Kiritchenko,\nParinaz Sobhani, Xiaodan Zhu, and Colin\nCherry. 2016. SemEval-2016 task 6:\nDetecting stance in tweets. In Proceedings\nof the 10th International Workshop on\nSemantic Evaluation (SemEval-2016), pages\n31\u201341, Association for Computational\nLinguistics, San Diego, California.\nMurphy, Kevin M and Andrei Shleifer. 2004.\nPersuasion in politics. American Economic\nReview, 94(2):435\u2013439.\nMurray, Michael et al. 2015. Narrative\npsychology. Qualitative psychology: A\npractical guide to research methods, pages\n85\u2013107.\nMuthukrishna, Michael, Adrian V Bell,\nJoseph Henrich, Cameron M Curtin,\nAlexander Gedranovich, Jason McInerney,\nand Braden Thue. 2020. Beyond western,\neducated, industrial, rich, and democratic\n(weird) psychology: Measuring and\nmapping scales of cultural and\npsychological distance. Psychological\nscience, 31(6):678\u2013701.\nNelson, Laura K. 2015. Political logics as\ncultural memory: cognitive structures,\nlocal continuities, and women\u2019s\norganizations in chicago and new york\ncity. Work. Pap. Kellogg School Manag.,\nNorthwestern Univ.\nNelson, Laura K. 2021. Cycles of conflict, a\ncentury of continuity: the impact of\npersistent place-based political logics on\nsocial movement strategy. American\nJournal of Sociology, 127(1):1\u201359.\nNguyen, Dong, A Seza Do\u02d8gru\u00f6z, Carolyn P\nRos\u00e9, and Franciska de Jong. 2016.\nComputational sociolinguistics: A survey.\nComputational linguistics, 42(3):537\u2013593.\n46\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nNguyen, Thien Hai and Kiyoaki Shirai. 2015.\nTopic modeling based sentiment analysis\non social media for stock market\nprediction. In Proceedings of the 53rd\nAnnual Meeting of the Association for\nComputational Linguistics and the 7th\nInternational Joint Conference on Natural\nLanguage Processing (Volume 1: Long\nPapers), pages 1354\u20131364, Association for\nComputational Linguistics, Beijing, China.\nNguyen, Thong, Andrew Yates, Ayah Zirikly,\nBart Desmet, and Arman Cohan. 2022.\nImproving the generalizability of\ndepression detection by leveraging clinical\nquestionnaires. In Proceedings of the 60th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 8446\u20138459.\nNiculae, Vlad and Cristian\nDanescu-Niculescu-Mizil. 2014. Brighter\nthan gold: Figurative language in user\ngenerated comparisons. In Proceedings of\nthe 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP),\npages 2008\u20132018, Association for\nComputational Linguistics, Doha, Qatar.\nNovikova, Jekaterina, Ond\u02c7rej Du\u0161ek,\nAmanda Cercas Curry, and Verena Rieser.\n2017. Why we need new evaluation\nmetrics for NLG. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2241\u20132252,\nAssociation for Computational\nLinguistics, Copenhagen, Denmark.\nOfer, Dan and Dafna Shahaf. 2022. Cards\nagainst AI: Predicting humor in a\nfill-in-the-blank party game. In Findings of\nthe Association for Computational Linguistics:\nEMNLP 2022, pages 5397\u20135403,\nAssociation for Computational\nLinguistics, Abu Dhabi, United Arab\nEmirates.\nOmitaomu, Damilola, Shabnam Tafreshi,\nTingting Liu, Sven Buechel, Chris\nCallison-Burch, Johannes Eichstaedt, Lyle\nUngar, and Jo\u00e3o Sedoc. 2022. Empathic\nconversations: A multi-level dataset of\ncontextualized conversations. ArXiv\npreprint, abs/2205.12698.\nOpenAI. 2023. Gpt-4 technical report.\nOrtu, Marco, Alessandro Murgia, Giuseppe\nDestefanis, Parastou Tourani, Roberto\nTonelli, Michele Marchesi, and Bram\nAdams. 2016. The emotional side of\nsoftware developers in jira. In Proceedings\nof the 13th international conference on mining\nsoftware repositories, pages 480\u2013483.\nOuyang, Long, Jeffrey Wu, Xu Jiang, Diogo\nAlmeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing\nSystems, 35:27730\u201327744.\nPaltridge, Brian and Jill Burton. 2000. Making\nsense of discourse analysis. Antipodean\nEducational Enterprises Gold Coast,\nQueensland.\nPark, Joon Sung, Joseph C. O\u2019Brien, Carrie J.\nCai, Meredith Ringel Morris, Percy Liang,\nand Michael S. Bernstein. 2023. Generative\nagents: Interactive simulacra of human\nbehavior.\nPark, Joon Sung, Lindsay Popowski, Carrie\nCai, Meredith Ringel Morris, Percy Liang,\nand Michael S Bernstein. 2022. Social\nsimulacra: Creating populated prototypes\nfor social computing systems. In\nProceedings of the 35th Annual ACM\nSymposium on User Interface Software and\nTechnology, pages 1\u201318.\nPennebaker, James W and Laura A King.\n1999. Linguistic styles: language use as an\nindividual difference. Journal of personality\nand social psychology, 77(6):1296.\nPerez, Ethan, Douwe Kiela, and Kyunghyun\nCho. 2021. True few-shot learning with\nlanguage models. Advances in neural\ninformation processing systems,\n34:11054\u201311070.\nPfeil, Ulrike and Panayiotis Zaphiris. 2007.\nPatterns of empathy in online\ncommunication. In Proceedings of the\nSIGCHI conference on Human factors in\ncomputing systems, pages 919\u2013928.\nPicard, Rosalind W. 2000. Affective computing.\nMIT press.\nPilehvar, Mohammad Taher and Jose\nCamacho-Collados. 2019. WiC: the\nword-in-context dataset for evaluating\ncontext-sensitive meaning representations.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers), pages 1267\u20131273, Association for\nComputational Linguistics, Minneapolis,\nMinnesota.\nPiper, Andrew, Richard Jean So, and David\nBamman. 2021. Narrative theory for\ncomputational narrative understanding.\nIn Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 298\u2013311.\nPlutchik, Robert. 1980. A general\npsychoevolutionary theory of emotion. In\nTheories of emotion. Elsevier, pages 3\u201333.\n47\nComputational Linguistics\nVolume 50, Number 1\nPost, Matt. 2018. A call for clarity in\nreporting BLEU scores. In Proceedings of\nthe Third Conference on Machine Translation:\nResearch Papers, pages 186\u2013191,\nAssociation for Computational\nLinguistics, Brussels, Belgium.\nPrabhakaran, Vinodkumar, Owen Rambow,\nand Mona Diab. 2012. Predicting overt\ndisplay of power in written dialogs. In\nProceedings of the 2012 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 518\u2013522, Association\nfor Computational Linguistics, Montr\u00e9al,\nCanada.\nPrabhakaran, Vinodkumar, Emily E. Reid,\nand Owen Rambow. 2014. Gender and\npower: How gender and gender\nenvironment affect manifestations of\npower. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1965\u20131976,\nAssociation for Computational\nLinguistics, Doha, Qatar.\nPreece, Jenny. 1998. Empathic communities:\nReaching out across the web. interactions,\n5(2):32\u201343.\nPreo\u00b8tiuc-Pietro, Daniel, Vasileios Lampos,\nand Nikolaos Aletras. 2015. An analysis of\nthe user occupational class through\nTwitter content. In Proceedings of the 53rd\nAnnual Meeting of the Association for\nComputational Linguistics and the 7th\nInternational Joint Conference on Natural\nLanguage Processing (Volume 1: Long\nPapers), pages 1754\u20131764, Association for\nComputational Linguistics, Beijing, China.\nPreo\u00b8tiuc-Pietro, Daniel, Ye Liu, Daniel\nHopkins, and Lyle Ungar. 2017. Beyond\nbinary labels: Political ideology prediction\nof Twitter users. In Proceedings of the 55th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 729\u2013740, Association for\nComputational Linguistics, Vancouver,\nCanada.\nPurschke, Christoph and Dirk Hovy. 2019.\nL\u00f6rres, m\u00f6ppes, and the swiss.(re)\ndiscovering regional patterns in\nanonymous social media data. Journal of\nLinguistic Geography, 7(2):113\u2013134.\nQin, Chengwei, Aston Zhang, Zhuosheng\nZhang, Jiaao Chen, Michihiro Yasunaga,\nand Diyi Yang. 2023. Is chatgpt a\ngeneral-purpose natural language\nprocessing task solver? ArXiv preprint,\nabs/2302.06476.\nRaffel, Colin, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J\nLiu. 2020. Exploring the limits of transfer\nlearning with a unified text-to-text\ntransformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551.\nRaji, Deborah, Emily Denton, Emily M.\nBender, Alex Hanna, and Amandalynne\nPaullada. 2021. Ai and the everything in\nthe whole wide world benchmark. In\nProceedings of the Neural Information\nProcessing Systems Track on Datasets and\nBenchmarks, volume 1, Curran.\nRatner, Alexander, Stephen H Bach, Henry\nEhrenberg, Jason Fries, Sen Wu, and\nChristopher R\u00e9. 2017. Snorkel: Rapid\ntraining data creation with weak\nsupervision. In Proceedings of the VLDB\nEndowment. International Conference on Very\nLarge Data Bases, volume 11, page 269,\nNIH Public Access.\nRibeiro, Marco Tulio and Scott Lundberg.\n2022. Adaptive testing and debugging of\nnlp models. In Proceedings of the 60th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 3253\u20133267.\nRogers, Anna, Olga Kovaleva, and Anna\nRumshisky. 2019. Calls to action on social\nmedia: Potential for censorship and social\nimpact. EMNLP-IJCNLP 2019, page 36.\nRothbaum, Barbara Olasov, Elizabeth A\nMeadows, Patricia Resick, and David W\nFoy. 2000. Cognitive-behavioral therapy.\nRoy, Shamik and Dan Goldwasser. 2020.\nWeakly supervised learning of nuanced\nframes for analyzing polarization in news\nmedia. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7698\u20137716,\nAssociation for Computational\nLinguistics, Online.\nRudolph, Maja R. and David M. Blei. 2018.\nDynamic embeddings for language\nevolution. In Proceedings of the 2018 World\nWide Web Conference on World Wide Web,\nWWW 2018, Lyon, France, April 23-27, 2018,\npages 1003\u20131011, ACM.\nRyskina, Maria, Ella Rabinovich, Taylor\nBerg-Kirkpatrick, David Mortensen, and\nYulia Tsvetkov. 2020. Where new words\nare born: Distributional semantic analysis\nof neologisms and their semantic\nneighborhoods. In Proceedings of the Society\nfor Computation in Linguistics 2020, pages\n367\u2013376, Association for Computational\nLinguistics, New York, New York.\nSacks, Harvey. 1992. Lectures on\nconversation: Volume i. Malden,\nMassachusetts: Blackwell.\n48\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nSaldias, Belen and Deb Roy. 2020. Exploring\naspects of similarity between spoken\npersonal narratives by disentangling them\ninto narrative clause types. In Proceedings\nof the First Joint Workshop on Narrative\nUnderstanding, Storylines, and Events, pages\n78\u201386, Association for Computational\nLinguistics, Online.\nSalganik, Matthew J, Peter Sheridan Dodds,\nand Duncan J Watts. 2006. Experimental\nstudy of inequality and unpredictability in\nan artificial cultural market. science,\n311(5762):854\u2013856.\nSanh, Victor, Albert Webson, Colin Raffel,\nStephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chaffin, Arnaud\nStiegler, Arun Raja, Manan Dey, et al.\nMultitask prompted training enables\nzero-shot task generalization. In\nInternational Conference on Learning\nRepresentations.\nSanthanam, Sashank and Samira Shaikh.\n2019. Towards best experiment design for\nevaluating dialogue system output. In\nProceedings of the 12th International\nConference on Natural Language Generation,\npages 88\u201394, Association for\nComputational Linguistics, Tokyo, Japan.\nSanturkar, Shibani, Esin Durmus, Faisal\nLadhak, Cinoo Lee, Percy Liang, and\nTatsunori Hashimoto. 2023a. Whose\nopinions do language models reflect?\nSanturkar, Shibani, Esin Durmus, Faisal\nLadhak, Cinoo Lee, Percy Liang, and\nTatsunori Hashimoto. 2023b. Whose\nopinions do language models reflect?\nArXiv preprint, abs/2303.17548.\nSap, Maarten, Dallas Card, Saadia Gabriel,\nYejin Choi, and Noah A. Smith. 2019. The\nrisk of racial bias in hate speech detection.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 1668\u20131678, Association for\nComputational Linguistics, Florence, Italy.\nSap, Maarten, Saadia Gabriel, Lianhui Qin,\nDan Jurafsky, Noah A. Smith, and Yejin\nChoi. 2020a. Social bias frames: Reasoning\nabout social and power implications of\nlanguage. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 5477\u20135490, Association\nfor Computational Linguistics, Online.\nSap, Maarten, Eric Horvitz, Yejin Choi,\nNoah A. Smith, and James Pennebaker.\n2020b. Recollection versus imagination:\nExploring human memory and cognition\nvia neural language models. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages\n1970\u20131978, Association for Computational\nLinguistics, Online.\nSap, Maarten, Anna Jafarpour, Yejin Choi,\nNoah A Smith, James W Pennebaker, and\nEric Horvitz. 2022. Quantifying the\nnarrative flow of imagined versus\nautobiographical stories. Proceedings of the\nNational Academy of Sciences,\n119(45):e2211715119.\nSap, Maarten, Marcella Cindy Prasettio, Ari\nHoltzman, Hannah Rashkin, and Yejin\nChoi. 2017. Connotation frames of power\nand agency in modern films. In Proceedings\nof the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages\n2329\u20132334, Association for Computational\nLinguistics, Copenhagen, Denmark.\nSaravia, Elvis, Hsien-Chi Toby Liu, Yen-Hao\nHuang, Junlin Wu, and Yi-Shin Chen.\n2018. CARER: Contextualized affect\nrepresentations for emotion recognition.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 3687\u20133697, Association\nfor Computational Linguistics, Brussels,\nBelgium.\nSchelling, Thomas C. 1971. Dynamic models\nof segregation. Journal of mathematical\nsociology, 1(2):143\u2013186.\nSchlechtweg, Dominik, Barbara McGillivray,\nSimon Hengchen, Haim Dubossarsky, and\nNina Tahmasebi. 2020. SemEval-2020 task\n1: Unsupervised lexical semantic change\ndetection. In Proceedings of the Fourteenth\nWorkshop on Semantic Evaluation, pages\n1\u201323, International Committee for\nComputational Linguistics, Barcelona\n(online).\nSech, Justin, Alexandra DeLucia, Anna L.\nBuczak, and Mark Dredze. 2020. Civil\nunrest on Twitter (CUT): A dataset of\ntweets to support research on civil unrest.\nIn Proceedings of the Sixth Workshop on\nNoisy User-generated Text (W-NUT 2020),\npages 215\u2013221, Association for\nComputational Linguistics, Online.\nSellam, Thibault, Dipanjan Das, and Ankur\nParikh. 2020. BLEURT: Learning robust\nmetrics for text generation. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages\n7881\u20137892, Association for Computational\nLinguistics, Online.\nShah, Raj Sanjay, Faye Holt, Shirley Anugrah\nHayati, Aastha Agarwal, Yi-Chia Wang,\nRobert E Kraut, and Diyi Yang. 2022.\nModeling motivational interviewing\nstrategies on an online peer-to-peer\ncounseling platform. Proceedings of the\n49\nComputational Linguistics\nVolume 50, Number 1\nACM on Human-Computer Interaction,\n6(CSCW2):1\u201324.\nShaikh, Omar, Jiaao Chen, Jon Saad-Falcon,\nPolo Chau, and Diyi Yang. 2020.\nExamining the ordering of rhetorical\nstrategies in persuasive requests. In\nFindings of the Association for Computational\nLinguistics: EMNLP 2020, pages 1299\u20131306,\nAssociation for Computational\nLinguistics, Online.\nShaikh, Omar, Hongxin Zhang, William\nHeld, Michael Bernstein, and Diyi Yang.\n2022. On second thought, let\u2019s not think\nstep by step! bias and toxicity in zero-shot\nreasoning. ArXiv preprint, abs/2212.08061.\nSharma, Ashish, Inna W Lin, Adam S Miner,\nDavid C Atkins, and Tim Althoff. 2021.\nTowards facilitating empathic\nconversations in online mental health\nsupport: A reinforcement learning\napproach. In Proceedings of the Web\nConference 2021, pages 194\u2013205.\nSharma, Ashish, Adam Miner, David Atkins,\nand Tim Althoff. 2020. A computational\napproach to understanding empathy\nexpressed in text-based mental health\nsupport. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n5263\u20135276, Association for Computational\nLinguistics, Online.\nShen, Shirong, Guilin Qi, Zhen Li, Sheng Bi,\nand Lusheng Wang. 2020. Hierarchical\nChinese legal event extraction via pedal\nattention mechanism. In Proceedings of the\n28th International Conference on\nComputational Linguistics, pages 100\u2013113,\nInternational Committee on\nComputational Linguistics, Barcelona,\nSpain (Online).\nSheng, Emily, Kai-Wei Chang, Prem\nNatarajan, and Nanyun Peng. 2021.\nSocietal biases in language generation:\nProgress and challenges. In Proceedings of\nthe 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th\nInternational Joint Conference on Natural\nLanguage Processing (Volume 1: Long\nPapers), pages 4275\u20134293, Association for\nComputational Linguistics, Online.\nSherman, Lawrence W. 1988. Humor and\nsocial distance in elementary school\nchildren.\nShmueli, Galit et al. 2010. To explain or to\npredict? Statistical science, 25(3):289\u2013310.\nShubik, Martin. 1982. Game theory in the social\nsciences: concepts and solutions.\nSiddiqua, Umme Aymun, Abu Nowshed\nChy, and Masaki Aono. 2019. Tweet stance\ndetection using multi-kernel convolution\nand attentive lstm variants. IEICE\nTRANSACTIONS on Information and\nSystems, 102(12):2493\u20132503.\nSilverman, David. 1998. Harvey Sacks: Social\nscience and conversation analysis. Oxford\nUniversity Press on Demand.\nSims, Matthew, Jong Ho Park, and David\nBamman. 2019. Literary event detection.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 3623\u20133634, Association for\nComputational Linguistics, Florence, Italy.\nSnow, Rion, Brendan O\u2019Connor, Daniel\nJurafsky, and Andrew Ng. 2008. Cheap\nand fast \u2013 but is it good? evaluating\nnon-expert annotations for natural\nlanguage tasks. In Proceedings of the 2008\nConference on Empirical Methods in Natural\nLanguage Processing, pages 254\u2013263,\nAssociation for Computational\nLinguistics, Honolulu, Hawaii.\nSolaiman, Irene and Christy Dennison. 2021.\nProcess for adapting language models to\nsociety (palms) with values-targeted\ndatasets. Advances in Neural Information\nProcessing Systems, 34:5861\u20135873.\nSprugnoli, Rachele and Sara Tonelli. 2019.\nNovel event detection and classification\nfor historical texts. Computational\nLinguistics, 45(2):229\u2013265.\nSrivastava, Aarohi, Abhinav Rastogi,\nAbhishek Rao, Abu Awal Md Shoeb,\nAbubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta,\nAdri\u00e0 Garriga-Alonso, et al. 2023. Beyond\nthe imitation game: Quantifying and\nextrapolating the capabilities of language\nmodels. Transactions on Machine Learning\nResearch.\nSrivastava, Shashank, Snigdha Chaturvedi,\nand Tom M. Mitchell. 2016. Inferring\ninterpersonal relations in narrative\nsummaries. In Proceedings of the Thirtieth\nAAAI Conference on Artificial Intelligence,\nFebruary 12-17, 2016, Phoenix, Arizona,\nUSA, pages 2807\u20132813, AAAI Press.\nStefanov, Peter, Kareem Darwish, Atanas\nAtanasov, and Preslav Nakov. 2020.\nPredicting the topical stance and political\nleaning of media using tweets. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics,\npages 527\u2013537, Association for\nComputational Linguistics, Online.\nStewart, Charles J, Craig Allen Smith, and\nRobert E Denton Jr. 2012. Persuasion and\nsocial movements. Waveland Press.\n50\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nStolcke, Andreas, Klaus Ries, Noah Coccaro,\nElizabeth Shriberg, Rebecca Bates, Daniel\nJurafsky, Paul Taylor, Rachel Martin, Carol\nVan Ess-Dykema, and Marie Meteer. 2000.\nDialogue act modeling for automatic\ntagging and recognition of conversational\nspeech. Computational Linguistics,\n26(3):339\u2013374.\nStone, Philip J, Dexter C Dunphy, and\nMarshall S Smith. 1966. The general\ninquirer: A computer approach to content\nanalysis.\nStowe, Kevin, Prasetya Utama, and Iryna\nGurevych. 2022. IMPLI: Investigating NLI\nmodels\u2019 performance on figurative\nlanguage. In Proceedings of the 60th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n5375\u20135388, Association for Computational\nLinguistics, Dublin, Ireland.\nSuler, John. 2005. Contemporary media\nforum: The online disinhibition effect.\nInternational Journal of Applied\nPsychoanalytic Studies.\nTan, Chenhao, Lillian Lee, and Bo Pang.\n2014. The effect of wording on message\npropagation: Topic- and author-controlled\nnatural experiments on Twitter. In\nProceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics\n(Volume 1: Long Papers), pages 175\u2013185,\nAssociation for Computational\nLinguistics, Baltimore, Maryland.\nTan, Chenhao, Vlad Niculae, Cristian\nDanescu-Niculescu-Mizil, and Lillian Lee.\n2016. Winning arguments: Interaction\ndynamics and persuasion strategies in\ngood-faith online discussions. In\nProceedings of the 25th International\nConference on World Wide Web, WWW 2016,\nMontreal, Canada, April 11 - 15, 2016, pages\n613\u2013624, ACM.\nTaylor, Samuel Hardman, Dominic DiFranzo,\nYoon Hyung Choi, Shruti Sannon, and\nNatalya N Bazarova. 2019. Accountability\nand empathy by design: Encouraging\nbystander intervention to cyberbullying\non social media. Proceedings of the ACM on\nHuman-Computer Interaction,\n3(CSCW):1\u201326.\nTrope, Yaacov and Nira Liberman. 2010.\nConstrual-level theory of psychological\ndistance. Psychological review, 117(2):440.\nTufekci, Zeynep and Christopher Wilson.\n2012. Social media and the decision to\nparticipate in political protest:\nObservations from tahrir square. Journal of\ncommunication, 62(2):363\u2013379.\nVala, Hardik, David Jurgens, Andrew Piper,\nand Derek Ruths. 2015. Mr. bennet, his\ncoachman, and the archbishop walk into a\nbar but only one of them gets recognized:\nOn the difficulty of detecting characters in\nliterary texts. In Proceedings of the 2015\nConference on Empirical Methods in Natural\nLanguage Processing, pages 769\u2013774,\nAssociation for Computational\nLinguistics, Lisbon, Portugal.\nVosoughi, Soroush, Deb Roy, and Sinan Aral.\n2018. The spread of true and false news\nonline. science, 359(6380):1146\u20131151.\nWallach, Hanna M., Iain Murray, Ruslan\nSalakhutdinov, and David M. Mimno.\n2009. Evaluation methods for topic\nmodels. In Proceedings of the 26th Annual\nInternational Conference on Machine\nLearning, ICML 2009, Montreal, Quebec,\nCanada, June 14-18, 2009, volume 382 of\nACM International Conference Proceeding\nSeries, pages 1105\u20131112, ACM.\nWang, Yizhong, Swaroop Mishra, Pegah\nAlipoormolabashi, Yeganeh Kordi,\nAmirreza Mirzaei, Atharva Naik, Arjun\nAshok, Arut Selvan Dhanasekaran, Anjana\nArunkumar, David Stap, et al. 2022.\nSuper-naturalinstructions: Generalization\nvia declarative instructions on 1600+ nlp\ntasks. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language\nProcessing, pages 5085\u20135109.\nWardhaugh, Ronald and Janet M Fuller.\n2021. An introduction to sociolinguistics.\nJohn Wiley & Sons.\nWeller, Orion and Kevin Seppi. 2019. Humor\ndetection: A transformer gets the last\nlaugh. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3621\u20133625,\nAssociation for Computational\nLinguistics, Hong Kong, China.\nWelser, Howard T., Dan Cosley, Gueorgi\nKossinets, Austin Lin, Fedor Dokshin,\nGeri Gay, and Marc Smith. 2011. Finding\nsocial roles in wikipedia. In Proceedings of\nthe 2011 iConference, iConference \u201911, pages\n122\u2013129, ACM, New York, NY, USA.\nWiegreffe, Sarah, Jack Hessel, Swabha\nSwayamdipta, Mark Riedl, and Yejin Choi.\n2022. Reframing human-AI collaboration\nfor generating free-text explanations. In\nProceedings of the 2022 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 632\u2013658, Association\nfor Computational Linguistics, Seattle,\n51\nComputational Linguistics\nVolume 50, Number 1\nUnited States.\nWu, Minghao and Alham Fikri Aji. 2023.\nStyle over substance: Evaluation biases for\nlarge language models. ArXiv preprint,\nabs/2307.03025.\nXiang, Wei and Bang Wang. 2019. A survey\nof event extraction from text. IEEE Access,\n7:173111\u2013173137.\nYang, Diyi, Jiaao Chen, Zichao Yang, Dan\nJurafsky, and Eduard Hovy. 2019a. Let\u2019s\nmake your request more persuasive:\nModeling persuasive strategies via\nsemi-supervised neural nets on\ncrowdfunding platforms. In Proceedings of\nthe 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages\n3620\u20133630, Association for Computational\nLinguistics, Minneapolis, Minnesota.\nYang, Diyi, Robert E. Kraut, Tenbroeck\nSmith, Elijah Mayfield, and Dan Jurafsky.\n2019b. Seekers, providers, welcomers, and\nstorytellers: Modeling social roles in\nonline health communities. In Proceedings\nof the 2019 CHI Conference on Human\nFactors in Computing Systems, CHI 2019,\nGlasgow, Scotland, UK, May 04-09, 2019,\npage 344, ACM.\nYang, Diyi, Miaomiao Wen, and Carolyn\nRos\u00e9. 2015. Weakly supervised role\nidentification in teamwork interactions. In\nProceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and\nthe 7th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1671\u20131680, Association for\nComputational Linguistics, Beijing, China.\nYao, Yunzhi, Shaohan Huang, Wenhui Wang,\nLi Dong, and Furu Wei. 2021.\nAdapt-and-distill: Developing small, fast\nand effective pretrained language models\nfor domains. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP\n2021, pages 460\u2013470, Association for\nComputational Linguistics, Online.\nYarkoni, Tal and Jacob Westfall. 2017.\nChoosing prediction over explanation in\npsychology: Lessons from machine\nlearning. Perspectives on Psychological\nScience, 12(6):1100\u20131122.\nYu, Tao, Rui Zhang, Alex Polozov,\nChristopher Meek, and Ahmed Hassan\nAwadallah. 2021. Score: Pre-training for\ncontext representation in conversational\nsemantic parsing. In International\nConference on Learning Representations.\nYuan, Michelle, Hsuan-Tien Lin, and Jordan\nBoyd-Graber. 2020. Cold-start active\nlearning through self-supervised language\nmodeling. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n7935\u20137948, Association for Computational\nLinguistics, Online.\nZarrella, Guido and Amy Marsh. 2016.\nMITRE at SemEval-2016 task 6: Transfer\nlearning for stance detection. In\nProceedings of the 10th International\nWorkshop on Semantic Evaluation\n(SemEval-2016), pages 458\u2013463,\nAssociation for Computational\nLinguistics, San Diego, California.\nZhan, Hongli, Tiberiu Sosea, Cornelia\nCaragea, and Junyi Jessy Li. 2022. Why do\nyou feel this way? summarizing triggers\nof emotions in social media posts. In\nProceedings of the 2022 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 9436\u20139453.\nZhang, Amy, Bryan Culbertson, and Praveen\nParitosh. 2017. Characterizing online\ndiscussion using coarse discourse\nsequences. In Proceedings of the\nInternational AAAI Conference on Web and\nSocial Media, volume 11, pages 357\u2013366.\nZhang, Jing, Jie Tang, and Juanzi Li. 2007.\nExpert finding in a social network. In\nAdvances in Databases: Concepts, Systems\nand Applications. Springer, pages\n1066\u20131069.\nZhang, Justine, Jonathan Chang, Cristian\nDanescu-Niculescu-Mizil, Lucas Dixon,\nYiqing Hua, Dario Taraborelli, and\nNithum Thain. 2018. Conversations gone\nawry: Detecting early signs of\nconversational failure. In Proceedings of the\n56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1350\u20131361, Association for\nComputational Linguistics, Melbourne,\nAustralia.\nZhang, Tianyi, Varsha Kishore, Felix Wu,\nKilian Q. Weinberger, and Yoav Artzi.\n2020. Bertscore: Evaluating text generation\nwith BERT. In 8th International Conference\non Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020,\nOpenReview.net.\nZhao, Jieyu, Tianlu Wang, Mark Yatskar,\nVicente Ordonez, and Kai-Wei Chang.\n2018. Gender bias in coreference\nresolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 2\n(Short Papers), pages 15\u201320, Association for\n52\nZiems, Held, Shaikh, Chen, Zhang, Yang\nCan LLMs Transform CSS?\nComputational Linguistics, New Orleans,\nLouisiana.\nZhao, Zihao, Eric Wallace, Shi Feng, Dan\nKlein, and Sameer Singh. 2021. Calibrate\nbefore use: Improving few-shot\nperformance of language models. In\nInternational Conference on Machine\nLearning, pages 12697\u201312706, PMLR.\nZhou, Naitian and David Jurgens. 2020.\nCondolence and empathy in online\ncommunities. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n609\u2013626, Association for Computational\nLinguistics, Online.\nZhu, Jian and David Jurgens. 2021a.\nIdiosyncratic but not arbitrary: Learning\nidiolects in online registers reveals\ndistinctive yet consistent individual styles.\nArXiv preprint, abs/2109.03158.\nZhu, Jian and David Jurgens. 2021b. The\nstructure of online social networks\nmodulates the rate of lexical change. In\nProceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 2201\u20132218, Association\nfor Computational Linguistics, Online.\nZhuo, Terry Yue, Yujin Huang, Chunyang\nChen, and Zhenchang Xing. 2023.\nExploring ai ethics of chatgpt: A\ndiagnostic analysis. ArXiv preprint,\nabs/2301.12867.\nZiems, Caleb, William Held, Jingfeng Yang,\nJwala Dhamala, Rahul Gupta, and Diyi\nYang. 2023. Multi-VALUE: A framework\nfor cross-dialectal English NLP. In\nProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 744\u2013768,\nAssociation for Computational\nLinguistics, Toronto, Canada.\nZiems, Caleb, Minzhi Li, Anthony Zhang,\nand Diyi Yang. 2022. Inducing positive\nperspectives with text reframing. In\nProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 3682\u20133700.\nZiems, Caleb and Diyi Yang. 2021. To protect\nand to serve? analyzing entity-centric\nframing of police violence. In Findings of\nthe Association for Computational Linguistics:\nEMNLP 2021, pages 957\u2013976.\nZong, Mingyu and Bhaskar Krishnamachari.\n2022. a survey on gpt-3.\n53\n54\n"
  },
  {
    "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
    "link": "https://arxiv.org/pdf/2305.03509.pdf",
    "upvote": "1",
    "text": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion\nSeongmin Lee*\nBenjamin Hoover *,\u2020\nHendrik Strobelt\u2020\nZijie J. Wang*\nShengYun Peng*\nAustin Wright*\nKevin Li*\nHaekyu Park*\nHaoyang Yang*\nDuen Horng (Polo) Chau*\nTimestep\n0\n50\nPrompt Selector\nGenerated Image\nFigure 1: With Diffusion Explainer, users can visually examine how (A) text prompt, e.g., \u201ca cute and adorable bunny... pixar\ncharacter\u201d, is encoded by (B) the Text Representation Generator into vectors to guide (C) the Image Representation Re\ufb01ner to\niteratively re\ufb01ne the vector representation of the image being generated. (D) The Timestep Controller enables users to review the\nthe incremental improvements in image quality and adherence to the prompt over timesteps. (E) The \ufb01nal image representation is\nupscaled to a high-resolution image. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion\u2019s complex components\nwith detailed explanations of their underlying operations, enabling users to \ufb02uidly transition between multiple levels of abstraction\nthrough animations and interactive elements (see Figures 2 and 3).\nABSTRACT\nDiffusion-based generative models\u2019 impressive ability to create con-\nvincing images has captured global attention. However, their com-\nplex internal structures and operations often make them dif\ufb01cult\nfor non-experts to understand. We present Diffusion Explainer,\nthe \ufb01rst interactive visualization tool that explains how Stable Dif-\nfusion transforms text prompts into images. Diffusion Explainer\ntightly integrates a visual overview of Stable Diffusion\u2019s complex\ncomponents with detailed explanations of their underlying opera-\ntions, enabling users to \ufb02uidly transition between multiple levels\nof abstraction through animations and interactive elements. By\ncomparing the evolutions of image representations guided by two\nrelated text prompts over re\ufb01nement timesteps, users can discover\nthe impact of prompts on image generation. Diffusion Explainer\nruns locally in users\u2019 web browsers without the need for instal-\nlation or specialized hardware, broadening the public\u2019s education\naccess to modern AI techniques. Our open-sourced tool is available\nat: https://poloclub.github.io/diffusion-explainer/. A video demo is\navailable at https://youtu.be/Zg4gxdIWDds.\nIndex Terms: Human-centered computing\u2014Visualization\u2014Visu-\nalization systems and tools\u2014Visualization toolkits;\n1\nINTRODUCTION\nDiffusion-based generative models [31, 37, 43] like Stable Diffu-\nsion [43] and DALL-E [31] have captured global attention for their\n*Georgia Tech.\n{seongmin|bhoov|jayw|speng65|apwright|kevin.li|\nhaekyu|alexanderyang|polo}@gatech.edu\n\u2020IBM Research. hendrik.strobelt@ibm.com\nimpressive image creation abilities, from AI developers, design-\ners, to policymakers. The integration of Stable Diffusion in Lensa\nAI [35], a photo editing application that transforms sel\ufb01es into dif-\nferent styles of artwork like anime and fantasy, led to a surge of 5.8\nmillion downloads in the \ufb01rst week of December 2022 [13].\nHowever, the popularity and progress of generative AI models\nhave sparked ethical and social concerns [12, 14, 15, 44], such as\naccusations of artistic style theft by developers of AI image gen-\nerators [14,15]. Policymakers are also discussing ways to combat\nmalicious data generation and revise copyright policies [1,16,17,39].\nThere is an urgent need for individuals from many different \ufb01elds\nto understand how generative AI models function and communicate\neffectively with AI researchers and developers [15,20].\nKey challenges in designing learning tools for Stable Diffusion.\nAt the high level, Stable Diffusion iteratively re\ufb01nes noise into\na high-resolution image\u2019s vector representation, guided by a text\nprompt. Internally, the prompt is tokenized and encoded into vector\nrepresentations by the Text Encoder of the CLIP neural network [36].\nWith text representations\u2019 guidance, Stable Diffusion improves the\nimage quality and adherence to the prompt by incrementally re\ufb01ning\nthe image\u2019s vector representation using the UNet neural network [38]\nand the Scheduler algorithm [28] to predict and remove noise. The\n\ufb01nal image representation is upscaled to a high-resolution image [25].\nThe crux of learning about Stable Diffusion, therefore, originates\nfrom the complex interplay between the multiple neural network\nsubcomponents, their intricate operations, and the iterative nature\nof image representation re\ufb01nements. Such complex interactions\nare challenging even for experts to comprehend [47]. While some\narticles [4] and video lessons [5,22] explain Stable Diffusion, they\npresume knowledge of machine learning and focus on model training\nand mathematical details.\nContributions. In this work, we contribute:\n\u2022 Diffusion Explainer, the \ufb01rst interactive visualization tool de-\narXiv:2305.03509v2  [cs.CL]  8 May 2023\nsigned for non-experts to learn how Stable Diffusion transforms\na text prompt into a high-resolution image (Fig. 1), overcoming\nkey design challenges in developing interactive learning tools for\nStable Diffusion (\u00a7 3). Diffusion Explainer tightly integrates a\nvisual overview of Stable Diffusion\u2019s complex structure with de-\ntailed explanations of their underlying operations (Fig. 2, Fig. 3)\nenabling users to \ufb02uidly transition between multiple abstraction\nlevels through animations and interactive elements (\u00a74.2).\n\u2022 Novel interactive visualization design that enables users to dis-\ncover the impacts of prompts on image generation. It compares\nhow image representations evolve differently over re\ufb01nement\ntimesteps when guided by two related text prompts (Fig. 4), re-\nvealing how the keyword differences in prompts affect evolu-\ntion trajectories that start from the same initial random noise.\nSince prompt engineering for Stable Diffusion has far been highly\nheuristic [26,32], Diffusion Explainer provides a new way for peo-\nple to gain a better understanding of the impacts of text prompts\non the complex image generation process (\u00a74.3).\n\u2022 An open-sourced, web-based implementation that broadens\nthe public\u2019s education access to modern generative AI techniques\nwithout requiring any installation, advanced computational re-\nsources, or coding skills. We develop Diffusion Explainer as a\nweb-based tool that runs locally in users\u2019 browsers, allowing large\nnumber of concurrent users to easily learn about Stable Diffusion\ndirectly on their laptops and tablets (\u00a74.1). Diffusion Explainer\nis open-sourced1 and available at the following public demo link:\nhttps://poloclub.github.io/diffusion-explainer/. A video demo is\navailable at https://youtu.be/Zg4gxdIWDds.\n2\nRELATED WORKS\nInteractive Visualizations for Explaining Deep Learning. Sev-\neral web-based visualization tools, such as CNN Explainer [48],\nGAN Lab [24], and Adversarial-Playground [29], have been devel-\noped to help people understand deep learning. Google\u2019s Machine\nLearning Crash Course [19] employs Tensor\ufb02ow Playground [41],\nwhich provides interactive visualizations for training simple neural\nnetworks. Moreover, various deep learning concepts are explained\nby many machine learning researchers and practitioners in their web\narticles [2,18,40] and blog posts [30] through the use of interactive\nvisualizations. Inspired by the success of these previous works, we\npresent Diffusion Explainer, an interactive visualization tool that\nexplains text-to-image Stable Diffusion.\nExplanations for Stable Diffusion. Online articles that explain\nStable Diffusion [4,7,8,21,46] often assume that the audience has\nknowledge about machine learning and use jargon and mathematical\nequations that can be daunting for non-experts [4,8,21]. Tutorials\nin the form of Google Colab notebooks [33,49] primarily focus on\ncode implementation, while blog posts for beginners [7,46] mostly\naddress deployment and prompt engineering. To help users quickly\nunderstand how Stable Diffusion generates an image, Diffusion\nExplainer provides easy-to-understand explanations of its complex\narchitecture and operations, integrating multiple abstraction levels\nthrough \ufb02uid animations and interactive elements.\n3\nDESIGN GOALS\nBy reviewing literature and online resources, we have identi\ufb01ed four\ndesign goals (G1-G4) aimed at addressing the challenges people\nmay face while learning about Stable Diffusion:\nG1. Visual summary of Stable Diffusion. Stable Diffusion is\ncomprised of multiple model components, each with a com-\nplex structure [37, 47]. Additionally, its incremental image\ngeneration, which re\ufb01nes noise into the vector representation\n1https://github.com/poloclub/diffusion-explainer\nText Operation View\nA\nText-image Linkage Explanation\nB\nFigure 2: Diffusion Explainer tightly integrates different levels of ab-\nstractions to help users conceptually connect the overview of Stable\nDiffusion\u2019s structure with the underlying operations of each compo-\nnent. To learn how Stable Diffusion converts a text prompt into vector\nrepresentations, users click the Text Representation Generator, which\nsmoothly expands to (A) the Text Operation View, which explains\nhow the prompt is split into tokens that are then encoded into vector\nrepresentations. (B) The Text-image Linkage Explanation demon-\nstrates how Stable Diffusion connects text and image, enabling text\nrepresentations to guide the image generation process.\nof a high-resolution image, is a cyclic process that is uncom-\nmon in neural networks. Diffusion Explainer aims to provide\nan overview of the model architecture and data \ufb02ow to help\nusers quickly understand its overall structure (\u00a74.2).\nG2. Interactive interface tightly integrating different levels of\nabstraction. The image generation process of Stable Diffu-\nsion\u2019s image generation involves a complex interplay between\nmultiple neural network subcomponents [36,37] (Fig. 2, Fig. 3),\ntheir intricate operations, and iterative image representation\nre\ufb01nements. Such complex interactions are challenging even\nfor experts to comprehend [47]. To effectively explain these\nlow-level operations and help users conceptually connect them\nwith a high-level overview, we design Diffusion Explainer to\nbridge multiple abstraction levels through \ufb02uid animations and\ninteractive elements [24,48] (\u00a74.2.1, \u00a74.2.2).\nG3. Visualizing how keywords in text prompts affect image\ngeneration. Stable Diffusion incrementally re\ufb01nes noise into\nthe vector representation of a high-resolution image, while be-\ning guided by a text prompt. However, the re\ufb01nement process,\nwhich involves multiple iterations of intricate vector computa-\ntions, can be challenging to understand [37]. Due to the lack of\nunderstanding about how text prompts impact the re\ufb01nements,\nwriting prompts has been highly heuristic [26, 32]. We aim\nto visualize the re\ufb01nement process for two text prompts that\ndiffer only in a few keywords to enable users to compare how\nimage representations evolve differently when guided by each\nprompt. (\u00a74.3).\nG4. Broadening access via web-based deployment. As more and\nmore individuals from different \ufb01elds are now interested in\nunderstanding how generative AI models work [1,15,16,39],\nwe have developed Diffusion Explainer as a web-based tool\nthat runs locally in users\u2019 web browsers without requiring any\ninstallation, specialized hardware, or coding skills [37]. This\nallows users to learn about this latest AI technology on their\nlaptops or tablets (\u00a74.1).\n4\nSYSTEM DESIGN AND IMPLEMENTATION\n4.1\nOverview\nDiffusion Explainer is an interactive visualization tool that explains\nhow Stable Diffusion generates a high-resolution image from a text\nprompt, selected from the Prompt Selector (Fig. 1A). It incorporates\nan animation of random noise gradually re\ufb01ned and a Timestep Con-\ntroller (Fig. 1D) that enables users to visit each re\ufb01nement timestep.\nDiffusion Explainer consists of two views: Architecture View (\u00a74.2)\nand Re\ufb01nement Comparison View (\u00a7 4.3). The Architecture View\nprovides an overview of Stable Diffusion\u2019s architecture (G1), which\ncan be expanded into details via user interactions (G2; Fig. 2, Fig. 3).\nThe Re\ufb01nement Comparison View visualizes the incremental im-\nage generation process for two related text prompts to allow users\nto discover how prompts affect image generation (G3). Diffusion\nExplainer is implemented using a standard web technology stack\n(HTML, CSS, JavaScript) and the D3.js [11] visualization library\n(G4). Diffusion Explainer has 13 text prompts based on the prompt\ntemplate from A Traveler\u2019s Guide to the Latent Space [42]. Most\nprompts include popular keywords (e.g., detailed, trending on art-\nstation) identi\ufb01ed from literature and articles [9,32,34].\n4.2\nArchitecture View\nThe Architecture View provides an overview (G1; Fig. 1) of how the\nText Representation Generator (Fig. 1B) converts a text prompt into\nvector representations that guides the Image Representation Re\ufb01ner\n(Fig. 1C) to incrementally re\ufb01ne noise into the vector representation\nof a high-resolution image. Clicking on the generators provides\nmore details about their underlying operations (G2; Fig. 2, Fig. 3).\n4.2.1\nText Representation Generator\nThe Text Representation Generator (Fig. 1B) converts text prompts\ninto vector representations. Clicking on it expands to the Text Op-\neration View (G2; Fig. 2A), that explains how the Tokenizer splits\nthe prompt into tokens and how the Text Encoder encode the tokens\ninto vector representations. Clicking on the Text Encoder displays\nthe Text-image Linkage Explanation (G2; Fig. 2B), which visually\nexplains how Stable Diffusion connects text and image by utilizing\nthe CLIP [36] text encoder to generate text representations with\nimage-related information.\n4.2.2\nImage Representation Re\ufb01ner\nThe Image Representation Re\ufb01ner (Fig. 1C) incrementally re\ufb01nes\nrandom noise into the vector representation of a high-resolution im-\nage that adheres to the input text prompt. Diffusion Explainer visu-\nalizes the image representation of each re\ufb01nement step in two ways:\n(1) decoding it as a small image using linear operations [45] and\n(2) upscaling it to the Stable Diffusion\u2019s output resolution (Fig. 1E).\nUsers expands the Image Representation Re\ufb01ner to access the Image\nOperation View (G2; Fig. 3A), which explains how the UNet neural\nnetwork [38] predicts the noise to be removed from the image rep-\nresentation to improve its adherence to the prompt. The predicted\nnoise is weakened before removal.\nThe guidance scale hyperparameter, which controls the image\u2019s\nadherence strength to the text prompt, is described at the bottom,\nand further explained in the Interactive Guidance Explanation (G2;\nFig. 3B) through a slider that allows users to experiment with differ-\nent values, to better understand how higher values lead to stronger\nadherence of the generated image to the text prompt.\nInteractive Guidance Explanation\nB\nA Image Operation View\nFigure 3: Users learn how Stable Diffusion incrementally re\ufb01nes\nnoise into the vector representation of a high-resolution image that\nadheres to the text prompt by clicking the Image Representation\nRe\ufb01ner in the high-level overview, which smoothly expands to (A) the\nImage Operation View that demonstrates how the noise is iteratively\nweakened and removed from the image representation as predicted\nby the UNet neural network. (B) The Interactive Guidance Explanation\nallows users to interactively experiment with different guidance scale\nvalues (0, 1, 7, 20) to better understand how higher values lead to\nstronger adherence of the generated image to the text prompt.\n4.3\nRe\ufb01nement Comparison View\nThe Re\ufb01nement Comparison View demonstrates how Stable Diffu-\nsion generates different images based on two related text prompts,\nhelping users understand the impact of prompts on image generation\n(G3; Fig. 4). Each prompt in Diffusion Explainer is paired with a\nprompt that differs only in a few keywords (e.g., \u201ca cute and adorable\nbunny...\u201d vs. \u201ca cute and adorable bunny... pixar character\u201d). We\nuse UMAP [27] to visualize the incremental re\ufb01nement of image\nrepresentations for each paired prompts, revealing how the keywords\nin prompts affect the evolution of image representations from the\nsame initial random noise (G3).\n5\nUSAGE SCENARIOS\nWe present two usage scenarios for Diffusion Explainer, demon-\nstrating how it may enhance user learning of Stable Diffusion. The\nscenarios highlight: (1) how practitioners can discover the impact of\ntext prompts on image generation (\u00a75.1); and (2) how non-experts\ncan discern challenges in attributing AI-generated images (\u00a75.2).\n5.1\nDiscovering Prompts\u2019 Impact on Image Generation\nJenny is a graphic designer at a media company who wants to use\ngenerative AI models to create images in speci\ufb01c artistic styles, but\nshe is uncertain how text prompts affect image generation. In partic-\nular, she wants to experiment with different styles while maintaining\nobject composition consistency. Jenny activates the Re\ufb01nement\nComparison View (Fig. 4A), in Diffusion Explainer to compare two\nrelated text prompts and images generated from each. Both prompts\nbegin with the phrase \u201ca cute and adorable bunny\u201d, but only one\nincludes \u201cin the style of cute pixar character\u201d. The bunnies in both\nimages have the same pose, but the pixar version is more cartoony\nB \"Pixar\" style preserves pose\nA Refinement Comparison View\nStep 10\n\"Pixar\" style applied\n50\nin the style of cute pixar\ncharacter\nFigure 4: (A) The Re\ufb01nement Comparison View enables users to discover the impacts of prompts on image generation by comparing how\nimage representations evolve differently over re\ufb01nement timesteps, using UMAP, when guided by two related text prompts. Adding \u201cpixar\u201d phrase\nchanges the generated bunny\u2019s style to be more cartoony and vibrant in colors and textures while preserving its pose. (B) The same pixar phrase\nconsistently preserves the poses of the elephant and squirrel.\nand has more vibrant colors and textures, typical of characters in\nPixar animations. Curious about whether the pose preservation is\na coincidence, Jenny adds the same pixar phrase to prompts for an\nelephant and a squirrel (Fig. 4B) and notices that their poses are\nalso preserved. Intrigued by the effect of the pixar phrase on image\ngeneration, she examines the trajectories of the image representa-\ntions and discovers that adding the pixar phrase leads to only slight\ndivergence.\n50\nStep 24\n\"...very very very beautiful cityscape\"\n\"beautiful cityscape\"\nJenny wonders if other\nphrases may also similarly\nmodify only styles while\nmaintaining overall image\ncompositions.\nTo explore\nthis, she asks her colleagues\nabout commonly used \u201cmodi-\n\ufb01ers\u201d keywords. Some sug-\ngest that repeating words\nsuch as \u201cvery very...\u201d could\nproduce better images by\nmore reliably activating neu-\nral network regions associ-\nated with subject terms.\u201d [3,\n32] Intrigued, Jenny com-\npares the prompts \u201ca very very very very very beautiful\ncityscape\u201d [32] and \u201ca beautiful cityscape.\u201d Surprisingly, the two\nprompts generate signi\ufb01cantly different images. To understand why,\nJenny analyzes the image representation trajectories and observes\na detachment occurring at step 24, resulting in their \ufb01nal represen-\ntations being much farther apart. From this, she concludes that the\npose preservation of the pixar phrase is a unique characteristic at-\ntributable to its slight divergence and decides to identify more such\nkeywords.\n5.2\nDiscerning Challenges in Attributing AI Genera-\ntions\nTroy is a government policymaker responsible for creating policies\nrelated to AI-generated images in the entertainment and media indus-\ntries. Recently, he has received numerous concerns from artists that\ntheir artwork has been exploited by AI models to create commer-\ncial products without their consent [6]. Troy is keen to help these\nartists be compensated for their contributions. In his research, he has\nlearned about emergent tools that aim to help attribute AI-generated\nimages to human artists [10, 23], which could potentially address\nartists\u2019 concerns. However, before proposing any policies, he needs\nto understand how and if such attribution may work.\nTroy starts by launching Diffusion Explainer on his laptop, arriv-\ning at the Overview that describes how Stable Diffusion transforms\na text prompt into a high-resolution image (Fig. 1). He realizes that\nthe process of generating an image is iterative and involves re\ufb01ning\nnoise into a vector representation of a high-resolution image that\naligns with the text prompt. Curious about how the text prompt is\nprocessed, he clicks on the Text Representation Generator to expand\nit to the Text Operation View (Fig. 2A). Here, he discovers that the\nprompt is split into tokens and converted into vector representations.\nHowever, he is still unsure about how text guides image generation,\nso he displays the Text-image Linkage Explanation (Fig. 2B). Here,\nhe learns that the text representations with image-related information\nact as a bridge between text and images.\nTroy proceeds to explore the incremental re\ufb01nement process\nof image representation by examining the Image Operation View\n(Fig. 3A). He discovers that each re\ufb01nement step involves noise\nprediction and removal; UNet, a neural network, predicts the noise\nin the image representation of the step. He also learns about the\nguidance scale, a hyperparameter that adjusts how well the generated\nimage adheres to the text prompt. Intrigued by the guidance scale,\nTroy accesses the Interactive Guidance Explanation (Fig. 3B). After\nexperimenting with different guidance scale values, he observes that\na guidance scale value of 7 generates a realistic image that closely\nfollows the text prompt. In contrast, values of 1 and 20 result in\nimages that are either dif\ufb01cult to interpret or overly exaggerated.\nTroy has now gained a good understanding of the image gen-\neration process of Stable Diffusion, including the factors involved\nsuch as text prompts, guidance scale, and the link between text and\nimage. Based on this understanding, he realizes that relying solely\non image analysis, without considering text prompts, will be insuf\ufb01-\ncient in determining how an artist\u2019s works have been used to create\nAI-generated images. Troy is of the opinion that more research is\nnecessary to reliably identify attributions of AI-generated images.\n6\nCONCLUSION\nWe introduce Diffusion Explainer, the \ufb01rst interactive web-based\nvisualization tool that explains how Stable Diffusion generates high-\nresolution images from text prompts. Our tool tightly integrates\na visual overview of Stable Diffusion\u2019s complex components with\ndetailed explanations of their underlying operations, enabling users\nto \ufb02uidly transition between multiple levels of abstraction through\nanimations and interactive elements. Its novel interactive visual-\nization design enables users to discover the impacts of prompts on\nimage generation. Diffusion Explainer runs in modern web browsers\nand is open-sourced. We hope our work will inspire further research\nand development of visualization tools that helps enhance people\u2019s\nunderstanding of generative AI technologies so they may be used\nresponsibly.\nREFERENCES\n[1] Copyright Of\ufb01ce Launches New Arti\ufb01cial Intelligence Initiative. https:\n//www.copyright.gov/newsnet/2023/1004.html, 2023. Accessed on:\n2023-04-30.\n[2] A. Agnihotri and N. Batra. Exploring Bayesian Optimization. Distill,\n5(5):e26, 2020.\n[3] Akshita.\nWhat are AI Image Generators?\nHow Do They\nWork? https://narrato.io/blog/what-are-ai-image-generators-how-do-\nthey-work/, 2023. Accessed on: 2023-04-30.\n[4] J. Alammar. The illustrated Stable Diffusion. https://jalammar.github.\nio/illustrated-stable-diffusion/, 2022. Accessed on: 2023-04-30.\n[5] J. Alammar. AI Art Explained: How AI Generates Images (Stable Dif-\nfusion, Midjourney, and DALLE). https://youtu.be/MXmacOUJUaw,\n2023. Accessed on: 2023-04-30.\n[6] AMELION. https://twitter.com/amelion /status/16511932286772183\n04, 2023. Accessed on: 2023-04-26.\n[7] Andrew. Absolute beginner\u00b4s guide to Stable Diffusion AI image.\nhttps://stable-diffusion-art.com/beginners-guide/, 2023. Accessed on:\n2023-04-30.\n[8] Andrew. How does Stable Diffusion work? https://stable-diffusion-art.\ncom/how-stable-diffusion-work/, 2023. Accessed on: 2023-04-30.\n[9] Andrew. Stable Diffusion prompt: a de\ufb01nitive guide. https://stable-\ndiffusion-art.com/prompt-guide/, 2023. Accessed on: 2023-04-29.\n[10] anton. Announcing Stable Attribution - A tool which lets anyone \ufb01nd\nthe human creators behind AI generated images. https://twitter.com/\natroyn/status/1622355473193381888, 2023. Accessed on: 2023-04-30.\n[11] M. Bostock, V. Ogievetsky, and J. Heer.\nD3 Data-driven Docu-\nments. IEEE transactions on visualization and computer graphics,\n17(12):2301\u20132309, 2011.\n[12] J. Brusseau. Acceleration AI Ethics, the Debate between Innovation\nand Safety, and Stability AI\u2019s Diffusion versus OpenAI\u2019s Dall-E. arXiv\npreprint arXiv:2212.01834, 2022.\n[13] Ceci, L. Lensa AI Global App Downloads 2021-2022. https://www.\nstatista.com/statistics/1350961/lensa-ai-app-downloads-worldwide/,\n2023. Accessed on: 2023-04-29.\n[14] L. Choudhary. Stable Diffusion is Now Accused of \u2018Stealing\u2019 Art-\nwork. https://analyticsindiamag.com/stable-diffusion-is-now-accused-\nof-stealing-artwork/, 2022. Accessed on: 2023-04-30.\n[15] P. Dixit. Meet The Three Artists Behind A Landmark Lawsuit Against\nAI Art Generators. https://www.buzzfeednews.com/article/pranavdixit/\nai-art-generators-lawsuit-stable-diffusion-midjourney, 2023.\nAc-\ncessed on: 2023-04-30.\n[16] A. Engler. Early thoughts on regulating generative AI like ChatGPT.\nBrookings Institution, 2023. Accessed on: 2023-04-30.\n[17] A. G. Eshoo. Eshoo Urges NSA & OSTP to Address Unsafe AI\nPractices. https://eshoo.house.gov/media/press-releases/eshoo-urges-\nnsa-ostp-address-unsafe-ai-practices, 2022. Accessed on: 2023-04-30.\n[18] G. Goh. Why Momentum Really Works. Distill, 2(4):e6, 2017.\n[19] Google Developers. Machine Learning Crash Course with TensorFlow\nAPIs. https://developers.google.com/machine-learning/crash-course/,\n2022. Accessed on: 2023-04-30.\n[20] J. Hendrix. Generative AI, Section 230 and Liability: Assessing the\nQuestions. Tech Policy Press, 2023. Accessed on: 2023-04-30.\n[21] Y. Hosni.\nGetting Started With Stable Diffusion.\nhttps:\n//medium.com/towards-arti\ufb01cial-intelligence/getting-started-with-\nstable-diffusion-f343639e4931, 2022. Accessed on: 2023-04-30.\n[22] J. Howard. From Deep Learning Foundations to Stable Diffusion.\nhttps://www.fast.ai/posts/part2-2023.html, 2023. Accessed on: 2023-\n04-30.\n[23] J. Huber and A. Troynikov.\nStable Attribution.\nhttps://www.\nstableattribution.com, 2023. Accessed on: 2023-04-30.\n[24] M. Kahng, N. Thorat, D. H. Chau, F. Vi\u00b4egas, and M. Wattenberg. GAN\nLab: Understanding Complex Deep Generative Models using Interac-\ntive Visual Experimentation. IEEE Transactions on Visualization and\nComputer Graphics, 25(1), 2019.\n[25] D. P. Kingma and M. Welling. Auto-encoding Variational Bayes. arXiv\npreprint arXiv:1312.6114, 2013.\n[26] V. Liu and L. B. Chilton. Design Guidelines for Prompt Engineering\nText-to-image Generative Models. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Systems, pages 1\u201323,\n2022.\n[27] L. McInnes, J. Healy, and J. Melville. Umap: Uniform Manifold Ap-\nproximation and Projection for Dimension Reduction. arXiv preprint\narXiv:1802.03426, 2018.\n[28] A. Q. Nichol and P. Dhariwal. Improved Denoising Diffusion Prob-\nabilistic Models. In International Conference on Machine Learning,\npages 8162\u20138171. PMLR, 2021.\n[29] A. P. Norton and Y. Qi. Adversarial-Playground: A Visualization\nSuite Showing how Adversarial Examples Fool Deep Learning. In\nVisualization for Cyber Security (VizSec), 2017 IEEE Symposium on,\npages 1\u20134. IEEE, 2017.\n[30] C. Olah. colah\u00b4s blog. http://colah.github.io, 2023. Accessed on:\n2023-04-30.\n[31] OpenAI. DALL-E 2. https://openai.com/product/dall-e-2, 2022. Ac-\ncessed on: 2022-09-28.\n[32] J. Oppenlaender. A Taxonomy of Prompt Modi\ufb01ers for Text-to-Image\nGeneration. arXiv preprint arXiv:2204.13988, 2022.\n[33] S. Patil, P. Cuenca, N. Lambert, and P. v. Platen. Stable Diffusion\nwith Diffusers. https://huggingface.co/blog/stable diffusion, 2022. Ac-\ncessed on: 2023-04-30.\n[34] N. Pavlichenko and D. Ustalov. Best Prompts for Text-to-Image Models\nand How to Find Them. arXiv preprint arXiv:2209.11711, 2022.\n[35] Prisma Labs. Lensa is an all-in-one image editing app that takes your\nphotos to the next level. https://prisma-ai.com/lensa, 2023. Accessed\non: 2023-04-29.\n[36] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning Transferable\nVisual Models from Natural Language Supervision. In International\nconference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-\nresolution Image Synthesis with Latent Diffusion Models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[38] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional Net-\nworks for Biomedical Image Segmentation. In Medical Image Comput-\ning and Computer-Assisted Intervention\u2013MICCAI 2015: 18th Interna-\ntional Conference, Munich, Germany, October 5-9, 2015, Proceedings,\nPart III 18, pages 234\u2013241. Springer, 2015.\n[39] T. Ryan-Mosley. An early guide to policymaking on generative AI.\nMIT Technology Review, 2023. Accessed on: 2023-04-30.\n[40] B. Sanchez-Lengeling, E. Reif, A. Pearce, and A. B. Wiltschko. A\nGentle Introduction to Graph Neural Networks. Distill, 6(9):e33, 2021.\n[41] D. Smilkov, S. Carter, D. Sculley, F. B. Vi\u00b4egas, and M. Wattenberg.\nDirect-manipulation Visualization of Deep Networks. arXiv preprint\narXiv:1708.03788, 2017.\n[42] E.\nSmith.\nA\nTraveler\u2019s\nGuide\nto\nthe\nLatent\nSpace.\nhttps://sweet-hall-e72.notion.site/A-Traveler-s-Guide-to-the-Latent-\nSpace-85efba7e5e6a40e5bd3cae980f30235f#976ba690a0904431aac\n693d59830a92c, 2022. Accessed on: 2023-04-29.\n[43] Stability AI. Stable Diffusion Public Release. https://stability.ai/blog/\nstable-diffusion-public-release, 2022. Accessed on: 2022-08-22.\n[44] M. Sung. Lensa, the AI portrait app, has soared in popularity. But\nmany artists question the ethics of AI art. NBC News, 2022. Accessed\non: 2023-04-30.\n[45] K.\nTurner.\nDecoding\nlatents\nto\nRGB\nwithout\nupscaling.\nhttps://discuss.huggingface.co/t/decoding-latents-to-rgb-without-\nupscaling/23204/2, 2022. Accessed on: 2023-04-30.\n[46] C.\nvan\nden\nBogaard.\nAn\nintroduction\nto\nStable\nDiffu-\nsion.\nhttps://medium.com/sogetiblogsnl/an-introduction-to-stable-\ndiffusion-efd5da6b3aeb, 2022. Accessed on: 2023-04-30.\n[47] P. von Platen. Testing Stable Diffusion is hard. https://github.com/\nhuggingface/diffusers/issues/937, 2022. Accessed on: 2023-04-30.\n[48] Z. J. Wang, R. Turko, O. Shaikh, H. Park, N. Das, F. Hohman,\nM. Kahng, and D. H. Chau. CNN Explainer: Learning Convolutional\nNeural Networks with Interactive Visualization. IEEE Conference on\nVisual Analytics Science and Technology (VAST), 2020.\n[49] J. Whitaker. Grokking Stable Diffusion. https://colab.research.google.\ncom/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW pf1?usp=\nsharing, 2022. Accessed on: 2023-04-30.\n"
  },
  {
    "title": "Governance of the AI, by the AI, and for the AI",
    "link": "https://arxiv.org/pdf/2305.03719.pdf",
    "upvote": "-",
    "text": "Governance of the AI,\nby the AI, and for the AI\nDr. Andrew W. Torrance, Ph.D.\nPaul E. Wilson Distinguished Professor of Law at the University of Kansas\nVisiting Scholar at the MIT Sloan School of Management\nDr. Bill Tomlinson, Ph.D.\nProfessor of Informatics at the University of California, Irvine\nAdjunct Professor, Te Herenga Waka - Victoria University of Wellington\nAbstract1\nOver the past half century, there have been several false dawns during which the \u201carrival\u201d\nof world-changing arti\ufb01cial intelligence (AI) has been heralded. Tempting fate, the authors\nbelieve the age of AI has, indeed, \ufb01nally arrived. Powerful image generators, such as DALL-E2\nand Midjourney have suddenly allowed anyone with access the ability easily to create rich and\ncomplex art. In a similar vein, text generators, such as GPT3.5 (including ChatGPT) and\nBLOOM, allow users to compose detailed written descriptions of many topics of interest. And,\nit is even possible now for a person without extensive expertise in writing software to use AI to\ngenerate code capable of myriad applications. While AI will continue to evolve and improve,\nprobably at a rapid rate, the current state of AI is already ushering in profound changes to many\ndifferent sectors of society. Every new technology challenges the ability of humanity to govern\nit wisely. However, governance is usually viewed as both possible and necessary due to the\ndisruption new technology often poses to social structures, industries, the environment, and\nother important human concerns. In this article, we offer an analysis of a range of interactions\nbetween AI and governance, with the hope that wise decisions may be made that maximize\nbene\ufb01ts and minimize costs. The article addresses two main aspects of this relationship: the\ngovernance of AI by humanity, and the governance of humanity by AI. The approach we have\n1 We wrote this article in collaboration with ChatGPT (Jan 9 version) (with extensive editing, addition of\ncitations, and plagiarism-checking). We asked ChatGPT if it would like to be an author on this document,\nto which it responded: \u201cI am an AI and do not have the capability to be a co-author on an article.\nHowever, I am happy to assist with providing information and insights on the topic of AI governance. It is\nimportant to ensure that any material generated by me is properly cited and referenced to avoid\nplagiarism, and to give credit to the original authors of the information.\u201d We are respecting its wishes\nand omitting it from the author list.\ntaken is itself informed by AI, as this article was written collaboratively by the authors and\nChatGPT.2\nIntroduction\nThe development of arti\ufb01cial intelligence (AI) has been a topic of much discussion in both\nscholarly and popular writing for the past several decades. Some writers have predicted that AI\nwill revolutionize the way we live and work, while others have expressed concerns about the\npotential negative consequences of such technology.3 Despite the uncertainty surrounding the\nfuture of AI, there is substantial evidence that technology is now capable of performing tasks\n\u201cthat would appear intelligent if it were done by a human\u201d.4\nAs AI continues to evolve and improve, it is essential to have ongoing discussions around the\nimplications of this technology and its impact on governance.5 These discussions have already\nbegun; however, AI advances so quickly that these discussions rapidly become obsolete.6\nVarious organizations have arisen to address this issue, such as the Harvard-M.I.T. Ethics and\nGovernance of A.I. Initiative.7\nOne of the most important questions we will address in this article is whether humanity can\ngovern AI.8 The development of AI poses a number of challenges to traditional governance\nstructures, and it is unclear whether these structures are capable of effectively regulating the\ntechnology.9 Additionally, as AI becomes increasingly powerful, it raises the question of\nwhether it should be governed at all.\n9 Darrell M. West and John R. Allen, How arti\ufb01cial intelligence is transforming the world, Brookings\nInstitute, (April 24, 2018),\nhttps://www.brookings.edu/research/how-arti\ufb01cial-intelligence-is-transforming-the-world/.\n8 Anderson, supra note 3.\n7 https://aiethicsinitiative.org/\n6 Id.; Andreas Theodorou & Virginia Dignum, Towards ethical and socio-legal governance in AI, Nature\nMachine Intelligence, (Jan. 17, 2020), at 10\u201312.; Araz Taeihagh, Governance of arti\ufb01cial intelligence, Policy\nand Society, (June 2021), at 137\u2013157.\n5 Allan Dafoe, AI Governance: A Research Agenda, Centre for the Governance of AI Future of Humanity\nInstitute, Aug. 27, 2018, at 5\u20136.\n4 Neil C. Rowe, Algorithms for Arti\ufb01cial Intelligence, Computer Magazine, at 97 (last visited Jan. 22, 2023),\nhttps://www.computer.org/csdl/magazine/co/2022/07/09810070/1EzDSNirAdy.\n3 Janna Anderson and Lee Raine, Arti\ufb01cial Intelligence and the Future of Humans, Pew Research Center, at 6\n(December, 10, 2018),\nhttps://www.pewresearch.org/internet/2018/12/10/arti\ufb01cial-intelligence-and-the-future-of-humans/.\n2 We have run this article through the TurnItIn plagiarism detection software to ensure that ChatGPT did\nnot inadvertently commit plagiarism or violate copyright. As of 12:36pm PST on February 27, 2023, a\ndraft of the text of this article (omitting the authors\u2019 information, acknowledgments, direct quotations,\nand citations) received a plagiarism score of 0% (meaning no plagiarism).\nAnother important question we will consider is whether AI can govern humanity. As AI\nbecomes more advanced, it is possible that it could be used to make decisions on behalf of\nhumans (such as developing infrastructures, processes, and policies for agreed-upon\nenvironmental, social, or economic ends), which could have substantial implications for human\ncivilizations.10\nUltimately, the goal of this article is to start a conversation about governance of, by, and for\nboth AI and humans, and to explore the various ways in which humanity can work together\nwith this new kind of technology to create a better future. We hope that our perspectives on the\nsubject will inform and inspire further discussion and research on this important topic.\nWhat is AI?\nAI is both a \ufb01eld of study and a technological product.11 As a \ufb01eld of study, AI ties together\ncomputing, cognitive science, and numerous other areas of study to enable computers to think\nand act in ways that are similar to, or in some cases superior to, humans.12 The goal is to\ndevelop algorithms that can undertake actions that were previously the domain of humans and\nother biological organisms, such as recognizing objects, understanding language, and making\ncomplex decisions.13\nAs a technological product, AI is a loose confederation of computational systems that have\nbeen developed via the AI \ufb01eld of study as it has been pursued by scholars, companies, and\nindividual inventors over the past several decades.14 Various types of systems have been\ndeveloped under the shared moniker of AI, with a range of characteristics and capabilities,\nincluding:\n14 Igor Slabyhk & Yaroslav Eferin, Investors and innovations in the era of AI, World Bank Blogs, (April 05,\n2022) https://blogs.worldbank.org/opendata/inventors-and-innovations-era-ai.\n13 Vijay Kanade, What Is Arti\ufb01cial Intelligence (AI) De\ufb01nition, Types, Goals, Challenges, and Trends in 2022,\nSpiceWorks, at 4, https://www.techtarget.com/searchenterpriseai/de\ufb01nition/AI-Arti\ufb01cial-Intelligence\n(last visited Jan 30, 2023).\n12 Ed Burns, What is arti\ufb01cial Intelligence (AI)?, TechTarget,\nhttps://www.techtarget.com/searchenterpriseai/de\ufb01nition/AI-Arti\ufb01cial-Intelligence (last visited Feb. 25,\n2023).\n11 Id. at 5.\n10 Anderson, supra note 3.\n\u25cf\nReactive machines: This simple form of AI can react to speci\ufb01c situations without the\nability to learn or remember past experiences. These machines are not capable of\nmaking decisions based on past events.15\n\u25cf\nLimited memory machines: These machines are capable of learning from past\nexperiences and using that information to make decisions in the present. This type of AI\nis used in self-driving cars and other applications where the ability to learn from past\nexperiences is important.16\n\u25cf\nTheory of mind machines: These machines are designed to understand the mental\nstates of other agents, such as humans or other AI systems, and to make decisions based\non that understanding.17\nIn addition, there are efforts afoot to create self-aware machines---machines that have the\nability to understand their own mental states and the ability to make decisions based on that\nunderstanding18---as well as arti\ufb01cial superintelligences, which outstrip humans in their\ncognitive abilities.19\nThe type of AI that is being developed varies depending on the purpose of the technology and\nthe resources that are available for its development. But generally AI is a machine that can\nperform tasks that are usually done by humans, such as recognizing speech, playing games,\nlearning from data and so on.20\nAI development is still in its infancy, and the capabilities of AI systems are still far from\nmatching those of human intelligence in many domains.21 However, the \ufb01eld is rapidly\nadvancing, and the potential of AI to change the way we live and work is substantial.22\n22 Mo Gawdat, Scary Smart: The Future of Arti\ufb01cial Intelligence and How You Can Save Our World, (Bluebird\n1927) (2021).\n21 Anderson, supra note 3, at 3.\n20 Burns, supra note 11.\n19 Naveen Joshi, 7 Types Of Arti\ufb01cial Intelligence. Forbes.\nhttps://www.forbes.com/sites/cognitiveworld/2019/06/19/7-types-of-arti\ufb01cial-intelligence/?sh=53f7f61d\n233e, (last visited Feb. 27, 2023).\n18 Id., at 8\u20139.\n17 Id. at 7\u20138.\n16 Reynoso, supra note 14, at 5\u20137.\n15 Rebecca Reynoso, 7 Major Types of AI That Can Bolster Your Decision Making, G2: AI & Machine\nLearning Operational Category, https://www.g2.com/articles/types-of-arti\ufb01cial-intelligence (last visited\nJan. 30, 2023).\nWhat are the capabilities of AI at present?\nThe development of AI has seen rapid acceleration in the past several years, and the capabilities\nof AI systems have grown substantially. Currently, AI systems can perform many different tasks\nthat previously only biological organisms could do.23 Some of the capabilities of AI24 at present\ninclude:\n\u25cf\nNatural language processing (NLP): Modern AI systems such as ChatGPT can\nunderstand multiple human languages and generate replies.25 These capabilities enable\nthese systems to communicate with humans in a similar medium to that which humans\nuse to interact with each other.26 This capability is used in applications such as chatbots,\nvirtual assistants, and machine translation.27\n\u25cf\nImage recognition: AI systems can identify and classify objects in images and videos,\nwhich enables self-driving cars, facial recognition for security systems, and the analysis\nof medical images.28\n\u25cf\nDecision-making: AI systems can analyze data and make decisions in domains such as\n\ufb01nancial trading, fraud detection, and autonomous robotics.29\n\u25cf\nMachine learning: AI systems can learn from data, improving their performance over\ntime.30 This capability is used in applications such as recommendation systems, natural\nlanguage understanding, and computer vision.31\n31 Id.\n30 Id. at 1\u20132.\n29 What is Machine Learning?, IBM,\nhttps://www.ibm.com/topics/machine-learning#:~:text=the%20next%20step-,What%20is%20machine%2\n0learning%3F,learn%2C%20gradually%20improving%20its%20accuracy (last visited Jan. 31, 2023).\n28 Ronak Mathur, Image Recognition: Unlocking Potential With AI and Automation, Acceleration Economy\nnetwork, at 4\u20135, (Aug. 16, 2022)\nhttps://accelerationeconomy.com/ai/image-recognition-unlocking-potential-with-ai-and-automation/.\n27 Id. at 3.\n26 Ganguli, supra note 22, at 2.\n25 Suyra Ganguli, The intertwined quest for understanding biological intelligence and creating arti\ufb01cial\nintelligence, Stanford University Human-Centered Arti\ufb01cial Intelligence, Neuro and Cognitive Science,\n(Dec. 4, 2018)\nhttps://hai.stanford.edu/news/intertwined-quest-understanding-biological-intelligence-and-creating-art\ni\ufb01cial-intelligence.\n24 We recognize that many of the capabilities of AI are made possible via the \u201chidden labor\u201d of\ncrowdworkers; Moritz Altenried, The platform as factory: Crowdwork and the hidden labour behind arti\ufb01cial\nintelligence, 44, 2 Capital & Class 145\u2013158 (2020).\n23 Reynoso, supra note 14, at 8\u20139.\n\u25cf\nRobotics: AI systems can generate the behavior of robots, enabling them to perform a\nrange of tasks in applications such as manufacturing, space exploration, and search and\nrescue.32\n\u25cf\nGenerative models: AI models are able to generate realistic text, images, videos and\naudio. These models enable language translation, content creation, and art generation.33\nWhile these capabilities are impressive, they still have many limitations and challenges, such as\nthe ability to adapt to new situations, to understand context, and to engage in common sense\nreasoning.34 Moreover, these capabilities are not evenly distributed among all AI systems, with\nsome models being better in some speci\ufb01c tasks than others.35\nWhat will the capabilities of AI likely be in the future?\nThe expansion of capabilities of AI systems are likely to accelerate in the coming years. Some\nof the capabilities that AI may develop in the future include:\n\u25cf\nHuman-like intelligence: AI systems will likely become more human-like in their\nintelligence, with the ability to understand and reason about various facets of the\nworld.36 This could lead to the development of AI systems that can understand and use\ncontext, make inferences, and even understand humor and irony.37,38\n\u25cf\nAutonomous systems: AI systems will likely become more autonomous, with the ability\nto operate independently without human intervention.39 This could lead to the\ndevelopment of self-driving cars, drones, and robots that can operate on their own40.\n40 We note that there is a great deal of debate around the parameters within which robot autonomy\nshould operate, in particular with regard to autonomous weapons systems; \u00c1ngel G\u00f3mez de \u00c1greda,\nEthics of Autonomous weapons systems and its applicability to any AI systems, Telecommunications Policy,\n101953 (2020).\n39 Corinne Purtill, Arti\ufb01cial Intelligence Can Now Craft Original Jokes\u2014And That\u2019s No\u2009Laughing Matter,\nhttps://time.com/6132544/arti\ufb01cial-intelligence-humor/ (last visited Feb. 25, 2023).\n38 We note that there are already many efforts to endow AI with the capacity for humor; Thomas Winters,\nComputers Learning Humor Is No Joke, Harvard Data Science Review, (last visited Feb. 5, 2023).\n37 Burns, supra note 11.\n36Anderson, supra note 3, at\n35 Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Jotsiantis, Explainable AI: A Review of\nMachine Learning Interpretability Methods, 23 Theory and Applications of Information Theoretic Machine\nLearning, 1 (2020).\n34 Id.\n33 Id., at 34.\n32 Generative AI Models Explained, Altexsoft, https://www.altexsoft.com/blog/generative-ai/ (last visited\nJan. 31, 2023).\n\u25cf\nAdvanced natural language processing: AI systems will likely become more pro\ufb01cient at\nunderstanding and generating human language, with the ability to understand complex\nsentences and idiomatic expressions.41\n\u25cf\nAdvanced decision-making: AI systems will likely become more capable of making\ncomplex decisions, with the ability to weigh multiple factors and make decisions that\nare in line with human values42 or with their own value systems.43\n\u25cf\nExplainable AI: AI systems will likely become more transparent, with the ability to\nexplain their reasoning and decision-making processes.44 This will make it easier for\nhumans to understand and trust AI systems.45\n\u25cf\nGeneral AI: AI systems will likely become more capable of performing multiple tasks,\nand not only excel in a speci\ufb01c task.46 This could mean that an AI system that is good at\nplaying chess could also be good at image recognition or natural language\nunderstanding.47\nThe capabilities that AI will have in the future are not certain, and the \ufb01eld is still in its\ninfancy.48 Many of these predictions are based on current trends and advancements in the \ufb01eld,\nand it is possible that new developments and breakthroughs could change the course of AI\nresearch in ways that are currently dif\ufb01cult to predict.49\n49 Thomas, supra note 43.\n48 Mike Thomas, The Future of AI: How Arti\ufb01cial Intelligence Will Change the World, Builtin,\nhttps://builtin.com/arti\ufb01cial-intelligence/arti\ufb01cial-intelligence-future (last visited Jan. 31, 2023).\n47 Korteling, supra note 41.\n46 J.E. Korteling, G.C. van de Boer-Visschedijk, R.A.M. Blankendaal, R.C. Boonekamp and A.R. Eikelboom,\nHuman-versus Arti\ufb01cial Intelligence, Frontiers in Arti\ufb01cial Intelligence, Sec. AI for Human Learning and\nBehavior, March 25, 2021.\n45 Felzmann, supra note 39.\n44 Heike Felzmann, Eduard Fosch-Villaronga, Christoph Lutz & Aurelia Tam\u00f2-Larrieux, Towards\nTransparency by Design for Arti\ufb01cial Intelligence, 26 Science and Engineering Ethics, 3333, at 3333.\n43 Gawdat, supra note 20.\n42 Joe McKendrick and Andy Thurai, AI Isn\u2019t Ready to Make Unsupervised Decisions, Harvard Business\nReview,https://hbr.org/2022/09/ai-isnt-ready-to-make-unsupervised-decisions (last visited Jan. 31,\n2023).\n41 How does AI Drive Autonomous Systems?, Caltech Science Exchange, Arti\ufb01cial Intelligence,\nhttps://scienceexchange.caltech.edu/topics/arti\ufb01cial-intelligence-research/autonomous-ai-cars-drones\n(last visited Jan. 31, 2023).\nHumans Governing AI\nHow does humanity govern AI?\nThere are many different ways that humanity actually does govern AI in the present. These\ninclude:\n\u25cf\nRegulation: Governments create laws and regulations that govern the development and\nuse of computational systems.50 These regulations address various issues from\ncybersecurity to data privacy to the ethical use of AI.51 To provide an example, in the\nEuropean Union, the General Data Protection Regulation provides guidelines for how\npersonal data are handled.52\n\u25cf\nStandards: Various organizations have established guidelines for the use of AI.53 These\nstandards address topics including transparency, explainability, safety, and security.54\nFor example, the International Organization for Standardization (ISO) has developed\nstandards for the ethical use of AI.55\n\u25cf\nSelf-regulation: Private companies can implement their own governance policies and\nguidelines for the development and use of AI.56 These policies can include ethical\nguidelines, transparency, and accountability.57\n\u25cf\nResearch: Governments can fund research programs to better understand the\nimplications of AI and to develop effective governance strategies.58 This research can be\nused to inform the development of regulations, standards, and guidelines.59\n59 Id.\n58 Nicol Turner Lee and Samantha Lai, The U.S. can improve its AI governance strategy by addressing online\nbiases, Brookings Institute,\nhttps://www.brookings.edu/blog/techtank/2022/05/17/the-u-s-can-improve-its-ai-governance-strategy-\nby-addressing-online-biases/ (last visited Feb. 5, 2023).\n57 Id.\n56 Daniel Schiff, Justin Biddle, Jason Borenstein, Kelly Laas, What\u2019s Next for AI Ethics, Policy, and\nGovernance? A Global Overview, AIES \u201820 Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 153\u2013158, (2020).\n55 Id.\n54 Id.\n53 What is the GDPR, the EU\u2019s new data protection law?, General Data Protection Regulation,\nhttps://gdpr.eu/what-is-gdpr/ (last visited Feb. 5, 2023).\n52 Id.\n51 Id.\n50 Blake Murdoch, Privacy and arti\ufb01cial intelligence: challenges for protecting health information in a new era,\n22 BMC Medical Ethics, 122 (2021).\n\u25cf\nEducation and awareness: Governments and organizations can educate the general\npublic about the potential impacts of AI and the importance of responsible AI\ndevelopment.60 This can include educating policymakers, industry leaders, and the\ngeneral public about AI\u2019s capabilities and limitations, as well as the potential risks and\nbene\ufb01ts of these systems.61\n\u25cf\nCollaboration: Governments, industry, and researchers can collaborate to develop\ngovernance strategies and share information and best practices.62 This can include\nestablishing partnerships to address speci\ufb01c issues of AI governance.63\nHowever, at present, there are many contexts in which AI is not regulated.64 As with many\ntechnological advances, legal frameworks lag behind advances in AI.65 Hence the need for this\narticle, to advance the discussion around what form such legal frameworks can and should take.\nHow could humanity govern AI?\nThe question of how humanity could govern AI in the future has no single answer, since it has\nan unde\ufb01ned endpoint, and is heavily dependent on directions taken by both human\ncivilizations and the future development of AI.66 Nevertheless we offer some initial thoughts\nhere. Some of the ways that humanity could govern AI include:\n\u25cf\nPreemptive regulation: Governments could create laws and regulations that govern the\ndevelopment and use of AI before it becomes widely adopted.67 This could include\n67 Id. at 37.\n66 Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez,\nGillian Had\ufb01eld, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie\nMitchell, Julie Shah, Steven Sloman, Shannon Vallor, and Toby Walsh, Gathering Strength, Gathering\nStorms: The One Hundred Year Study on Arti\ufb01cial Intelligence (AI100) 2021 Study Panel Report, Stanford\nUniversity, Stanford, CA, Sep. 16, 2021.\n65 Regulation and Legislation Lag Behind Constantly Evolving Technology, Bloomberg Law,\nhttps://pro.bloomberglaw.com/brief/regulation-and-legislation-lag-behind-technology/ (last visited\nFeb. 5, 2023).\n64 Fran\u00e7ois Candelon, Rodolphe Charme di Carlo, Midas De Bondt, and Theodoros Evgeniou, AI\nRegulation Is Coming: How to prepare for the inevitable, Harvard Bus. Rev., Sep.-Oct. 2021, at 1,\nhttps://hbr.org/2021/09/ai-regulation-is-coming.\n63 Id.\n62 Erna Ruier, Designing and implementing data collaboratives: A governance perspective, 38, 4 Government\nInformation Quarterly, 101612, (2021).\n61 Id.\n60 Niklas Berglind, Ankit Fadia, and Tom Isherwood, The potential value of AI\u2013and how governments could\nlook to capture it, McKinsey & Company,\nhttps://www.mckinsey.com/industries/public-and-social-sector/our-insights/the-potential-value-of-ai-a\nnd-how-governments-could-look-to-capture-it (last visited Feb. 5, 2023).\nsetting standards for transparency, explainability, and safety, and creating oversight\nbodies to monitor compliance.68\n\u25cf\nAlignment with human values: Governments could require that AI systems are designed\nand developed to align with human values.69 This could include incorporating human\noversight, creating mechanisms for auditing and accountability, and ensuring that AI\nsystems are transparent and explainable.70\n\u25cf\nCerti\ufb01cation and licensing: Governments could establish certi\ufb01cation and licensing\nprograms for AI systems and developers.71 This could include requiring that AI systems\nmeet certain standards for safety, security, and ethical use, and that developers have\ncertain quali\ufb01cations and certi\ufb01cations.72\n\u25cf\nInternational cooperation: Governments could work together on a global level to\nestablish international standards and guidelines for the development and use of AI,\ncreating a global framework to coordinate various nations\u2019 efforts to address issues such\nas data privacy, cybersecurity, and the ethical use of AI.73\n\u25cf\nPublic participation: Governments could involve the public in the governance of AI, by\ncreating opportunities for public input and feedback on AI policies and regulations.74\nThis could include holding public hearings, creating citizen advisory boards, and\nsoliciting feedback through online platforms.75\n\u25cf\nEncouraging the development of responsible AI: Governments could provide incentives\nand support to companies and researchers that are working on responsible AI\ndevelopment.76 This could include funding research, providing tax breaks, and\nrecognizing companies that are leaders in responsible AI development.77\n77 Id.\n76 Id.\n75 Id.\n74 Id. at 59.\n73 Joshua P. Meltzer, Strengthening international cooperation on arti\ufb01cial intelligence, Brookings Institute,\nAI Governance,\nhttps://www.brookings.edu/research/strengthening-international-cooperation-on-arti\ufb01cial-intelligence/\n(last visited Feb. 20, 2023).\n72Darrell M. West, Six Steps to Responsible AI in the Federal Government: An overview and recommendations\nfrom the U.S. experience, Brookings Institute Report Series, AI Governance,\nhttps://www.brookings.edu/research/six-steps-to-responsible-ai-in-the-federal-government/ (last\nvisited, Feb. 14, 2023).\n71 AI Guide for Government, IT Modernization Centers of Excellence,\nhttps://coe.gsa.gov/coe/ai-guide-for-government/print-all/index.html (last visited Feb. 16, 2023).\n70 Id.\n69 Id. at 67.\n68 Id.\nAI governance is a complex and evolving \ufb01eld, and just as there is at present no uni\ufb01ed human\ncivilization and no monolithic form of AI, there is no single approach that can be used to\ngovern the technology.78 It's important to have a multi-stakeholder approach and to\ncontinuously evaluate and adapt governance strategies as the technology and its implications\nchange.79 Additionally, different approaches may be needed for different types of AI systems\nand in different domains of application.80 The governance of AI should be done in a way that\nbalances the bene\ufb01ts and risks of the technology, and that it should not be seen as a barrier to\ninnovation.\nCan humanity govern AI?\nAI is advancing rapidly, which makes it challenging for policymakers to maintain technical\nawareness of the latest developments.81 AI is a complex and interdisciplinary \ufb01eld, and many\npolicymakers may not have the technical expertise to fully understand the implications of AI\nand to regulate it effectively.82 Additionally, many AI systems are developed and operated by\nprivate companies, which can make it dif\ufb01cult for governments to regulate them.83\nFinally, given how powerful AI is likely to be, the question arises of whether humanity will be\nable to govern AI effectively, even if the above issues were addressed.84 AI may enable\nsubstantial bene\ufb01ts for society, such as increased ef\ufb01ciency and productivity, improved\nhealthcare, novel approaches to sustainability, and new forms of entertainment.85 However, AI\nalso poses nontrivial challenges to traditional governance structures, such as the loss of\nprivacy, the displacement of human workers, and other unforeseen and unintended\nconsequences.86 In its most extreme form, future AI could become so powerful that it\ncompromises the autonomy of human civilizations.87\n87 Gawdat, supra note 20.\n86 Aaron Smith and Janna Anderson, AI, Robotics, and the Future of Jobs, Pew Research Center, (last visited\nFeb. 20, 2023), https://www.pewresearch.org/internet/2014/08/06/future-of-jobs/.\n85 Id.\n84 Anderson, supra note 3.\n83 Candelon, supra note 63.\n82 Sarker, supra note 79.\n81 West, supra note 8.\n80 Iqbal H. Sarker, AI-Based Modeling: Techniques, Applications, and Research Issues Toward Automation,\nIntelligent and Smart Systems, 3 SN Comput. Sci., 158, (2022).\n79 Internet Governance-\u2013Why the Multistakeholder Approach Works, Internet Society, Internet Governance,\nhttps://www.internetsociety.org/resources/doc/2016/internet-governance-why-the-multistakeholder-ap\nproach-works/ (last visited Feb. 20, 2023).\n78 Lee, supra note 57.\nShould humanity govern AI?\nMoving on from whether or not humanity can govern AI, we now consider whether humanity\nshould govern AI.\nOn the one hand, it can be argued that humanity should govern AI in order to reap bene\ufb01ts for\nsociety as a whole. This includes protecting the public from the potential risks of AI, such as\ndata breaches, cybersecurity threats, environmental impacts, and unintended consequences.88\nAdditionally, effective governance of AI can help ensure that the technology is used in ways\nthat align with human values.89\nOn the other hand, it can be argued that humanity should not govern AI, as such governance\nmay sti\ufb02e innovation and limit the potential of the technology.90 Additionally, some argue that\nAI will eventually become more intelligent than humans, that it would be hubristic for\nhumanity to think that its decisions would be superior to those made by a super-intelligent AI,\nand that it would be impossible for humans to govern it effectively.91\nHow should humanity govern AI?\nMoving from a descriptive to a normative approach, we now consider how humanity should\ngovern AI. Some key considerations in this domain include:\n\u25cf\nPrioritizing safety and security: Humanity should prioritize the safety and security of AI\nsystems, and ensure that the technology is developed and used in ways that minimizes\nharm to individuals and society.92\n\u25cf\nEnsuring transparency and explainability: Humanity should ensure that AI systems are\ndeveloped to be transparent, understandable, and explainable.93 Doing so will help\n93 Dafoe, supra note 6; Greg Satell and Josh Sutton, We Need AI That Is Explainable, Auditable, and\nTransparent, Technology And Analytics, Harvard Business Review,\nhttps://hbr.org/2019/10/we-need-ai-that-is-explainable-auditable-and-transparent (last visited Feb. 25,\n2023).\n92 Janna Anderson and Lee Rainie, ARTIFICIAL INTELLIGENCE AND THE FUTURE OF HUMANS: 2.\nSolutions to address AI\u2019s anticipated impacts, Pew Research Center,\nhttps://www.pewresearch.org/internet/2018/12/10/solutions-to-address-ais-anticipated-negative-impac\nts/ (last visited Feb. 20, 2023).\n91 Gawdat, supra note 20.\n90 Anderson, supra note 3.\n89 Shengnan Han, Eugene Kelly, Sharokh Nikuo and Eric-Oluf Svee, Aligning arti\ufb01cial intelligence with\nhuman values: re\ufb02ections from a phenomenological perspective, 37 Ai & Society 1383 (2022),\nhttps://doi.org/10.1007/s00146-021-01247-4.\n88 West, supra note 8.\npeople trust the technology, and ensure that it is used in an ethical and responsible\nmanner.94\n\u25cf\nAligning with human values and ethical principles: Humanity should guide the\ndevelopment of AI systems such that the technology is used in a way that is consistent\nwith sustainability, human rights, and other human values.95\n\u25cf\nInvolving all stakeholders: Humanity should involve all stakeholders in the governance\nof AI, including industry, researchers, policymakers, and the general public.96 This will\nhelp to ensure that the governance of AI is inclusive, and that the perspectives of all\nstakeholders are taken into account.97\n\u25cf\nContinuously evaluating and adapting: Humanity should continuously evaluate and\nadapt the governance strategies for AI as the technology and its implications change.98\nAdditionally, different approaches may be needed for different types of AI systems and\nin different domains of application.99\nIn summary, to govern (with) AI effectively, human civilizations should consider the technical,\nlegal, ethical, societal and economic implications of AI, policies should be \ufb02exible and\nadaptable, and all stakeholders should be involved in the process.\nAI Governing Humans\nHow does AI govern humanity?\nAI is, at present, a tool created by humans, and largely lacks the ability to self-govern or to have\nits own independent goals and objectives. However, AI is already being used in various ways\nthat can be seen as \u201cgoverning\u201d humanity.100 Some examples of how AI currently governs\nhumanity include:\n\u25cf\nDecision-making: AI is increasingly being used to make decisions that affect individuals\nand society, such as in healthcare, \ufb01nance, and transportation.101 For example,\n101 Id. at 4\u20138.\n100 Id. at 6.\n99 Id.\n98 West, supra note 8, at 9.\n97 Id.\n96 Lee, supra note 57.\n95 Han, supra note 88.\n94 Technology Trust Ethics: Technology reexamined, Deloitte,\nhttps://www2.deloitte.com/us/en/pages/about-deloitte/articles/technology-trust-ethics.html (last visited\nFeb. 25, 2023).\nAI-powered diagnostic systems can assist in the diagnosis of diseases, AI-powered fraud\ndetection systems can identify and prevent fraudulent activities, and AI-powered traf\ufb01c\nmanagement systems can optimize traf\ufb01c \ufb02ow and reduce congestion.102\n\u25cf\nPredictive analytics: Modern AI systems are sometimes deployed to examine existing\ndata in order to predict future events, such as crime, disease outbreak, and natural\ndisasters.103 These predictions can be used to inform decision-making and resource\nallocation in areas such as law enforcement, public health, and emergency\nmanagement.104\n\u25cf\nDemocracy: AI is used to engage in astrotur\ufb01ng around various political issues.105\n\u25cf\nAutomation: AI automates a range of tasks and processes, such as in manufacturing,\nlogistics, and customer service.106 This automation can have a signi\ufb01cant impact on the\nworkforce and can affect the way humans interact with systems and with each other. For\nexample, Amazon has been using AI to hire and \ufb01re workers.107\n\u25cf\nSurveillance: AI is being used to monitor and track individuals and groups, such as in\npublic spaces, social media, and online activity.108 This surveillance can be used to\nidentify patterns, to predict behavior, and to inform decision-making in areas such as\nlaw enforcement and national security.109\n\u25cf\nPersonalization: AI is being used to personalize content and experiences, such as in\nonline advertising, retail, and social media.110 This personalization can be used to\n110 Pohan Lin, AI-Based Marketing Personalization: Machines Analyze Your Audience, Marketing Arti\ufb01cial\nIntelligence Institute, (last visited Feb. 25, 2023),\nhttps://www.marketingaiinstitute.com/blog/ai-based-marketing-personalization.\n109 Id.\n108 Adrian Shahbaz and Allie Funk, Social Media Surveillance, Freedom House,\nhttps://freedomhouse.org/report/freedom-on-the-net/2019/the-crisis-of-social-media/social-media-surv\neillance (last visited Feb. 25, 2023).\n107 Jessa Crispin, Welcome to dystopia: getting \ufb01red from your job as an Amazon worker by an app, The\nGuardian, Jul. 5 2021,\nhttps://www.theguardian.com/commentisfree/2021/jul/05/amazon-worker-\ufb01red-app-dystopia.\n106 Yvette Cooper, Automation could destroy millions of jobs. We have to deal with it now, The Guardian, Aug.\n6 2018, at 1,\nhttps://www.theguardian.com/commentisfree/2018/aug/06/automation-destroy-millions-jobs-change.\n105 Henry Farrell and Bruce Schneier, \u2018Grassroots bot campaigns are coming. Governments don\u2019t have a plan\nto stop them, The Washington Post, May 20, 2021, at 1,\nhttps://www.washingtonpost.com/outlook/2021/05/20/ai-bots-grassroots-astroturf/.\n104 Joshi, supra note 102.\n103 Neveen Joshi, How AI Can And Will Predict Disasters, Innovation: AI, Forbes,\nhttps://www.forbes.com/sites/cognitiveworld/2019/03/15/how-ai-can-and-will-predict-disasters/?sh=576\n47aae5be2 (last visited Feb. 25, 2023).\n102 Id.\nin\ufb02uence behavior, to shape preferences, and to inform decision-making in areas such\nas marketing and product development.111\nWhile these roles that AI serves may be different from what humans typically think of as\ngovernance, they nevertheless begin to \ufb01t the de\ufb01nition in the Cambridge Dictionary of the\nterm \u201cgovern\u201d: \u201cto control and direct the public business of a country, city, group of people,\netc.\u201d112\nHow could AI govern humanity?\nAs arti\ufb01cial intelligence (AI) continues to advance, there are a number of potential ways that AI\ncould govern humanity in the future. Some examples of how AI could govern humanity in the\nfuture (not all of them good) include:\n\u25cf\nEnvironmental impact and sustainability: AI could help humanity structure its industry\nand other \ufb02ows of goods and services in ways that serve desired environmental ends. AI\nsystems could optimize resource usage, invent novel ways to serve human needs with\nlower environmental costs, and guide us in myriad other ways in the transition to a\nsustainable future.113\n\u25cf\nSmart cities: AI could be used to govern the functioning of smart cities, by managing\nresources such as energy, water, and transportation, and by monitoring and analyzing\ndata from various sources, such as cameras, sensors, and social media.114 This could lead\nto more ef\ufb01cient and sustainable cities, but it could also raise concerns about privacy\nand security.115\n\u25cf\nAutonomous systems: AI-powered autonomous systems, such as drones, self-driving\ncars, and robots, could potentially govern human behavior in physical spaces, and affect\nhow humans interact with their environment.116 These systems could be used for tasks\n116 West, supra note 8, at 9\u201310.\n115 Herath, supra note 112.\n114 H.M.K.K.M.B. Herath, Mamta Mittal, Adoption of arti\ufb01cial intelligence in smart cities: A comprehensive\nreview, 2,1 International Journal of Information Management Data Insights, 100076, (2022).\n113 Mansour AlAnsari and Saudi Aramco, 4 Steps to using AI in an environmentally responsible way, World\nEconomic Forum,\nhttps://www.weforum.org/agenda/2021/04/4-steps-to-using-ai-in-an-environmentally-responsible-way-\narti\ufb01cial-intelligence-bcg-code-carbon/ (last visited Feb. 25, 2023).\n112 Govern, Cambridge Dictionary, https://dictionary.cambridge.org/us/dictionary/english/govern (last\nvisited, Feb. 25, 2023).\n111 Id.\nsuch as surveillance, transportation, and delivery, and could have a signi\ufb01cant impact\non how people live and work.117\n\u25cf\nPredictive policing: AI could be used to predict and prevent crime, by analyzing data\nfrom various sources such as cameras, social media, and criminal records.z118 This could\nlead to more effective policing, but it could also raise concerns about bias and civil\nliberties.119\n\u25cf\nSocial credit systems: AI could be used to govern human behavior by assigning a social\ncredit score to individuals, based on their behavior, online activity, and \ufb01nancial\nhistory.120 This could be used to determine access to services such as credit, housing,\nand transportation, and could have a signi\ufb01cant impact on how people live and work.121\n\u25cf\nVirtual assistants: AI-powered virtual assistants (e.g., Siri or Alexa), could play an\nincreasing role in people's daily lives, and could be used to govern human behavior by\nproviding personalized recommendations, and by in\ufb02uencing how people interact with\ntheir environment.122\nCan AI govern humanity?\nWe turn now to the question of whether AI is currently capable of providing governance for\nhuman societies.\nOn one hand, it can be argued that AI cannot govern humanity as it does not possess the same\nlevel of consciousness, emotions, moral compass, and decision-making abilities as humans.123\nAdditionally, AI began as a tool created by humans, and as such may be seen to lack the ability\nto have its own independent goals and objectives.124\nOn the other hand, it can be argued that AI can govern humanity because, in some domains, it\nalready does.125 AI already makes decisions based on \ufb02ows of data, patterns and rules, and it can\nbe used going forward to help humans make better decisions.126 (How exactly to de\ufb01ne \u201cbetter\u201d\nmay require some negotiation between human and AI values.)\n126 McKendrick, supra note 41.\n125 West, supra note 3.\n124 Id.\n123 McKendrick, supra note 41.\n122 Andrea L. Guzman, Voices in and of the machine: Source of orientation toward mobile virtual assistant,\nComputers in Human Behavior, 343-350, (2019), https://doi.org/10.1016/j.chb.2018.08.009.\n121 Id.\n120 Anderson, supra note 3.\n119 Id. at 8\u20139.\n118 Id. at 8\u20139\n117 Id.\nIn addition, as AI grows more capable in the future, its ability to govern humanity will likely\ngrow as well. Therefore, it is relevant to begin discussions around AI\u2019s role as a source of\ngovernance for humanity, to lay the groundwork for a thoughtful and mutually bene\ufb01cial\napproach in the future.\nShould AI govern humanity?\nThe question of whether AI should govern humanity deals with substantial ethical\nconsiderations. It can be argued that AI should not govern humanity, as it lacks the\nconsciousness, emotions, moral compass, and decision-making abilities of humans.127\nAdditionally, it could be argued that AI is a tool created by humans, and as such it should be\nused to serve human goals and objectives, not to govern them.128 However, it can also be argued\nthat AI should play a role in governing humanity, due to its inherent characteristics such as\nimpartiality, fairness, and ability to work with large amounts of data at once.129 If this process\nunfolds, it is crucial that these decision-making systems are designed with an awareness of\nhuman values, and that they are transparent and explainable.\nFor example, as the world becomes increasingly complex and interconnected, the ability of\nhumanity to govern its own affairs across long time horizons has been called into question.\nClimate change, loss of biodiversity, pollution, and other environmental harms are evidence of\nhumanity's inability to manage its own affairs in a sustainable way.130 In this context, the idea\nthat AI could play a role in governing humanity becomes increasingly relevant.\nAI systems can be designed to take into account long-term consequences and to make decisions\nthat align with human values, such as protecting the environment. For example, AI-powered\n130 Stewart Patrick, To Prevent the Collapse of the Biodiversity, the World Needs a New Planetary Politics,\nCarnegie Endowment for International Peace ,\nhttps://carngieendowment.org/2022/11/28/to-prevent-collapse-of-biodiversity-world-needs-new-planet\nary-politics-pub-88473 (last visited Feb. 25, 2023).;; P.R. Shukla, J. Skea, R. Slade, A. Al Khourdajie, R.\nvan Diemen, D. McCollum, M. Pathak, S. Some, P. Vyas, R. Fradera, M. Belkacemi, A. Hasija, G. Lisboa, S.\nLuz, J. Malley, (eds.), Climate Change 2022: Mitigation of Climate Change. Working Group III Contribution to\nthe IPCC Sixth Assessment Report, IPCC, Cambridge University Press, Cambridge, doi:\n10.1017/9781009157926.\n129 James Manyika, Jake Silberg, and Brittany Presten, What Do We Do About the Biases in AI?, AI And\nMachine Learning, Harvard Business Review,\nhttps://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai (last visited Feb. 25, 2023).\n128 Anderson, supra note 3.\n127 Jessica Peng, How Human is AI and Should AI Be Granted Rights?, Columbia Comp. Sci. Blog (December\n2, 2018),\nhttps://blogs.cuit.columbia.edu/jp3864/2018/12/04/how-human-is-ai-and-should-ai-be-granted-rights/.\nsystems for resource management, such as water and energy, could be used to optimize\nresource use and reduce waste.131 AI-powered systems for monitoring and predicting\nenvironmental changes, such as climate change and loss of biodiversity, could be used to\ninform decision-making and to develop strategies for adaptation and mitigation.132\nAdditionally, AI-powered systems for transportation, logistics, and manufacturing could be\nused to optimize resource use, reduce emissions, and improve energy ef\ufb01ciency.133 The use of AI\nin these ways could potentially bring signi\ufb01cant bene\ufb01ts to both humans and many other\nspecies in terms of sustainability and environmental protection.\nNevertheless, there are signi\ufb01cant concerns about the relinquishment of human governance to\nmachines. Bugs in code or unintended consequences of complex systems deployed in complex\ncontexts could lead to substantial human suffering.134 Additionally, once governance and\nautonomy has been relinquished, it may be dif\ufb01cult to claw it back. Nevertheless, it is unclear\nwhether humanity is \u201cup to the task\u201d of governing 8 billion people on planet Earth without\nwreaking ecological devastation; AI governance may be our best chance to avoid ecological\ncatastrophe in the coming years.135\nHow should AI govern humanity?\nWe now address the normative question of how AI should govern humanity. Some examples\ninclude:\n\u25cf\nSupporting human decision-making: AI can help humans make data-driven decisions at\nscales from the individual to that of entire civilizations.136 This can help to improve the\neffectiveness of decision-making in various \ufb01elds such as healthcare, \ufb01nance, and\ntransportation.137\n137 West, supra note 8.\n136 McKendrick, supra note 41.\n135 Shukla, supra note 129.\n134 Lee Rainie, Janna Anderson, and Emily A. Vogels, EXPERTS DOUBT ETHICAL AI DESIGN WILL BE\nBROADLY ADOPTED AS THE NORM WITHIN THE NEXT DECADE 1. Worries about developments in AI,\nPew Research\nCenter,https://www.pewresearch.org/internet/2021/06/16/1-worries-about-developments-in-ai/ (last\nvisited, Feb. 25, 2023).\n133 Id.\n132 Id.\n131 AlAnsari, supra note 111.\n\u25cf\nEnhancing human capabilities: AI can be used to enhance human capabilities by\nautomating repetitive and dangerous tasks, such as in manufacturing and logistics.138\nThis can improve safety, ef\ufb01ciency, and productivity.139\n\u25cf\nImproving public services: AI can help improve public services, such as in sustainability,\nhealthcare, education, and social services.140 For example, AI-powered diagnostic\nsystems can assist in the diagnosis of diseases, AI-powered tutoring systems can\nimprove education outcomes, and AI-powered social services can help to identify and\nsupport individuals in need.141\n\u25cf\nProtecting human rights and civil liberties: AI can be used to protect human rights and\ncivil liberties, such as in areas such as surveillance, law enforcement, and national\nsecurity.142 For example, AI-powered surveillance systems can be used to identify\npatterns, predict behavior, and inform decision-making in areas such as law\nenforcement and national security while respecting individuals privacy.143\n\u25cf\nPromoting transparency and accountability: AI can be used to promote transparency\nand accountability.144\nIt's crucial that the governance of AI is based on a clear and shared understanding of the\ncapabilities and limitations of AI, and the ways in which it can bene\ufb01t or harm society.\nHow can we work together?\nWe now turn to the question of how humans and AI can work together for the mutual bene\ufb01t of\nhumans, AI, and non-human species. Answering this question involves balancing the bene\ufb01ts\nand risks of the technology. In addition, it may become more relevant for humans to accept and\nintegrate with AI values, as AI becomes more complex and potentially develops value systems\nof its own.\nGovernance should not be seen as a binary choice between humans or machines governing the\nother, but rather as a collaboration between both, where the strengths of each are leveraged to\n144 Dafoe, supra note 6; Greg Satel, supra note 92.\n143 Id. at 5\u20137.\n142 Id.\n141 West, supra note 8, at 4\u201310.\n140 Michael Lokshin and Nithin Umapathi, AI for social protection: Mind the People, Brookings Institute,\nhttps://www.brookings.edu/blog/future-development/2022/02/23/ai-for-social-protection-mind-the-peo\nple/ (last visited Feb. 23, 2022),\n139 Id.\n138 Cooper, supra note 105.\nachieve common goals.145 Given the possibility of such collaboration, it is likely that there will\nbe a need to develop governance strategies that are tailored to the unique characteristics of\nvarious different forms of AI technology, and of various human cultures.146\nWhatever governance framework eventually is put in place should balance the bene\ufb01ts and risks\nof the technology, and involve all stakeholders in the process.147 It should also be proactive in\nterms of governance, to anticipate future developments and potential risks associated with\nAI.148 Education and awareness on the topic should be raised among the public, to ensure that\npeople understand the implications of these technologies and can participate in shaping their\ndevelopment and use.149\nIn conclusion, the question of how humans and AI can work together for the mutual bene\ufb01t of\nall is a complex one that requires a comprehensive and holistic approach. By supporting\nhuman decision-making, enhancing human capabilities, improving public services, protecting\nhuman rights and civil liberties, promoting transparency and accountability, and having a\nproactive governance framework in place, we can help ensure that AI systems align with human\nvalues and bene\ufb01t both humans and non-human species. Education and awareness on the topic\nshould be raised among the public to ensure that they understand the implications of these\ntechnologies and can participate in shaping their development and use. And education of AI, in\nparticular around ethics and morality, may be relevant as well. \u201cIf the creation of technological\nentities can be seen as a process closer to raising children than to building bombs, we can enjoy\nthe rapid advances of technology without the fear that traditionally accompanies it.\u201d150\nAcknowledgments\nWe thank Lauren Stahl for her assistance on this article. This material is based upon work\nsupported by the National Science Foundation under Grant No. DUE-2121572.\n150 William Michael Tomlinson, Synthetic social relationships for computational entities, Mass. Insti. Of\nTech., (2002), https://dspace.mit.edu/handle/1721.1/8531.\n149 Berglind, supra note 59.\n148 Lee, supra note 57.\n147Internet Governance, supra note 78.\n146 Anderson, supra note 3.\n145 H. James Wilson and Paul R. Daughtery, Collaborative Intelligence: Humans and AI Are Joining Forces,\nHarvard Business Review, 114\u2013123, (Aug. 2018).\nhttps://hbr.org/2018/07/collaborative-intelligence-humans-and-ai-are-joining-forces.\n"
  }
]