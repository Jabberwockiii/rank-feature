[
  {
    "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention",
    "link": "https://arxiv.org/pdf/2306.06189.pdf",
    "upvote": "29",
    "text": "FasterViT: Fast Vision Transformers with Hierarchical Attention\nAli Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose M. Alvarez,\nJan Kautz, Pavlo Molchanov\nNVIDIA\n{ahatamizadeh, gheinrich, dannyy, atao, josea, jkautz, pmolchanov}@nvidia.com\nAbstract\nWe design a new family of hybrid CNN-ViT neural net-\nworks, named FasterViT, with a focus on high image through-\nput for computer vision (CV) applications. FasterViT com-\nbines the benefits of fast local representation learning in\nCNNs and global modeling properties in ViT. Our newly\nintroduced Hierarchical Attention (HAT) approach decom-\nposes global self-attention with quadratic complexity into\na multi-level attention with reduced computational costs.\nWe benefit from efficient window-based self-attention. Each\nwindow has access to dedicated carrier tokens that par-\nticipate in local and global representation learning. At a\nhigh level, global self-attentions enable the efficient cross-\nwindow communication at lower costs. FasterViT achieves a\nSOTA Pareto-front in terms of accuracy vs. image through-\nput.\nWe have extensively validated its effectiveness on\nvarious CV tasks including classification, object detection\nand segmentation. We also show that HAT can be used\nas a plug-and-play module for existing networks and en-\nhance them. We further demonstrate significantly faster\nand more accurate performance than competitive counter-\nparts for images with high resolution. Code is available at\nhttps://github.com/NVlabs/FasterViT.\n1. Introduction\nVision Transformers (ViTs) [18] have recently become\npopular in computer vision and achieved superior perfor-\nmance in various applications such as image classifica-\ntion [38, 17, 35], object detection [77, 21] and semantic seg-\nmentation [61, 10]. In addition to learning more uniform lo-\ncal and global representations across their architecture when\ncompared to Convolutional Neural Networks (CNNs), ViTs\nscale properly to large-scale data and model sizes [47, 45].\nRecently, several efforts [27, 63] have also shown the ex-\nceptional capability of ViTs in self-supervised learning of\nsurrogate tasks such as masked image modeling which may\nsignificantly enhance the performance of downstream appli-\ncations. Despite these advantages, lack of inductive bias in\n1000\n2000\n3000\n4000\n5000\n6000\nThroughput (img/sec)\n79\n80\n81\n82\n83\n84\n85\n86\nAccuracy (%)\nFasterViT(ours)\nConvNeXt (2022)\nRegNetY (2020)\nMaxViT (2022)\nEfficientNetV2 (2021)\nSwin (2021)\nSwinV2 (2022)\nTNT (2021)\nTwins (2021)\nDeiT (2021)\nPoolFormer (2022)\nCoaT (2021)\nCrossViT (2021)\nVisformer (2021)\nEfficientFormer (2022)\nNextViT (2022)\nFasterViT-5\nFasterViT-4\nFasterViT-3\nFasterViT-2\nFasterViT-1\nFasterViT-0\nModel\nThroughput\nTop-1\nSwin-S\n1720\n83.2\nConvNeXt-S\n2008\n83.1\nFasterViT-2\n3161\n84.2\nSwin-B\n1232\n83.5\nConvNeXt-B\n1485\n83.8\nFasterViT-3\n1780\n84.9\nConvNeXt-L\n508\n84.3\nFasterViT-4\n849\n85.4\nFigure 1. Comparison of image throughput and ImageNet-1K Top-1\naccuracy. The family of FasterViT models achieves a new Pareto\nfront encompassing various throughput and Top-1 accuracy trade-\noffs. Specifically, FasterViT has significantly better throughput\ncompared to other ViT-based models. For all models, throughput is\nmeasured on A100 GPU with batch size of 128.\npure ViT models may require more training data and impede\nperformance [65]. Hybrid architectures, which consist of\nboth CNN and ViT-based components, could address this\nproblem and achieve competitive performance without need-\ning large-scale training datasets [18] or other techniques such\nas knowledge distillation [52].\nAn integral component of ViTs is the self-attention mech-\nanism [56, 18] which enables modeling of both short and\nlong-range spatial dependencies. However, the quadratic\ncomputational complexity of self-attention significantly im-\npacts the efficiency and hinders its use for applications with\nhigh-resolution images. In addition, contrary to the isotropic\narchitecture (i.e., same feature resolution with no downsam-\n1\narXiv:2306.06189v1  [cs.CV]  9 Jun 2023\nx\nx\nLocal window attention\nHierarchical attention\nx\no\nGlobal attention\nCarrier tokens\nx o\nLocal window\nLocal tokens\nx\nLocal attention\nHierarchical\nattention\n+\n=\nFigure 2. Visualization of the proposed Hierarchical Attention in\nthe feature space. By performing local window attention and hier-\narchical attention we can achieve global information propagation\nat reduced costs.Best viewed in color.\npling) of the original ViT model, learning feature represen-\ntations in a multi-scale manner typically yields better per-\nformance [20, 58], specifically for downstream applications\n(e.g., detection, segmentation).\nTo address these issues, Swin Transformer [38] proposed\na multi-scale architecture in which self-attention is computed\nin local windows, and window-shifting allows for interaction\nof different regions. However, due to the limited receptive\nfield of these local regions and small area of coverage in win-\ndow shifting [38, 35], capturing cross-window interactions\nand modeling the long-range spatial dependencies become\nchallenging for large-resolution input features. Furthermore,\nusing self-attention blocks in early stages with larger resolu-\ntion may impact the image throughput due to the increased\nnumber of local windows. Recently, the Swin Transformer\nV2 model [37] was proposed to address training instabilities\non high-resolution images by improving the self-attention\nmechanism. However, in addition to having a lower image\nthroughput compared to the Swin Transformer [38], Swin\nTransformer V2 still relies on the original window-shifting\nmechanism for cross-interaction of different windows, which\nbecomes less effective with large image sizes.\nIn this work, we attempt to address these issues and\npropose a novel hybrid architecture, denoted as FasterViT,\nwhich is tailored for high-resolution input images, while\nmaintaining a fast image throughput. FasterViT consists of\nfour different stages in which the input image resolution is re-\nduced by using a strided convolutional layer, while doubling\nthe number of feature maps. We propose to leverage resid-\nual convolutional blocks in the high-resolution stages of the\narchitecture (i.e., stage 1, 2), while employing transformer-\nblocks in later stages (i.e., stage 3, 4). This strategy allows\nfor fast generation of high-level tokens which can be fur-\nther processed with the transformer-based blocks. For each\ntransformer block, we use an interleaved pattern of local\nand, newly proposed, Hierarchical Attention blocks to cap-\nture both short and long-range spatial dependencies and\nefficiently model the cross-window interactions. Specifi-\ncally, our proposed Hierarchical Attention (see Fig. 2) learns\ncarrier tokens as a summary of each local window and effi-\nciently models the cross-interaction between these regions.\nThe computational complexity of the Hierarchical Atten-\ntion grows almost linearly with input image resolution, as\nthe number of regions increases, due to the local windowed\nattention being the compute bottleneck. Hence, it is an effi-\ncient, yet effective way of capturing long-range information\nwith large input features.\nWe have extensively validated the effectiveness of the pro-\nposed FasterViT model on various image tasks and datasets\nsuch as ImageNet-1k for image classification, MS COCO\nfor object detection and instance segmentation and ADE20K\ndataset for semantic segmentation. FasterViT achieves state-\nof-the-art performance considering the trade-off between\nperformance (e.g., ImageNet-1K top-1 accuracy) and image\nthroughput (see Fig. 1). To demonstrate the scalability of\nFasterViT for larger datasets, we have also pre-trained Faster-\nViT on ImageNet-21K dataset and achieved state-of-the-art\nperformance when fine-tuning and evaluating on larger-scale\nresolutions.\nThe summary of our contributions is as follows:\n\u2022 We introduce FasterViT, which is a novel hybrid vi-\nsion transformer architecture designed for an optimal\ntrade-off between performance and image throughput.\nFasterViT scales effectively to higher resolution input\nimages for different dataset and model sizes.\n\u2022 We propose the Hierarchical Attention module which\nefficiently captures the cross-window interactions of\nlocal regions and models the long-range spatial depen-\ndencies.\n\u2022 FasterViT achieves a new SOTA Pareto front in terms of\nimage throughput and accuracy trade-off and is signifi-\ncantly faster than comparable ViT-based architectures\nyielding significant speed-up compared to recent SOTA\nmodels. It also achieves competitive performance on\ndownstream tasks of detection and instance segmenta-\ntion on MS COCO dataset and semantic segmentation\non ADE20K dataset.\n2. Related Work\nVision Transformers. Oriented from the language process-\ning domain, the first application of transformer architecture\nto vision task immediately offers an inspiring demonstra-\ntion of the high efficacy of attention across image patches\nacross varying scenarios [18]. The appealing strength of\nvision transformer and its architecture and logic simplic-\nity has therefore triggered a quickly evolving literature\nin the past two years, where ViT performance is quickly\nboosted by an erupting new set of innovations: network-wise\nleveraging knowledge distillation for data-efficient training\nas in DeiT [52], hybriding convolution and self-attention\nfor enhanced inductive biases as in LeViT [24], imposing\nCNN-inspired pyramid rules on ViTs [57, 58], along with\ncomponent-wise improvements such as improved token uti-\n2\nlization as in T2T-ViT [72], enhanced positional embed-\nding [12], local window attention as shown in the inspiring\nwork of the Swin family [38, 37] and CSwin [17], global\nattention in GCViT [26], among many other architectural in-\nsights [11, 76, 73]. Along with the increasing capacity comes\nthe increasing computation burden. As similarly facing chal-\nlenges in scaling up the models in language tasks (e.g.,\nfrom BERT-Large 0.3B [16], to Megatron-LM 8.3B [50],\nand Switch-Transformer1.6T [22]), scaling up vision trans-\nformers is also a highly challenging but highly important\ntask [14, 37] due to the attention-extensive nature of trans-\nformers, urging efficiency for pervasive usage.\nTowards Enhanced Efficiency. Boosting up ViT efficiency\nhas therefore been a very vibrant area. One stream of ap-\nproach roots in the efficient deep learning literature that\ncuts down on network complexity leveraging popular meth-\nods such as efficient attention [3, 41, 4], network compres-\nsion [7, 8, 34, 67], dynamic inference [69, 48], operator\nadaptation [43], token merging and manipulations [42, 66],\netc. These methods can yield off-the-shelf speedups on\ntarget ViT backbones, but are also limited to the original\nbackbone\u2019s accuracy and capacity. Another stream of work,\non the other hand, focuses on designing new ViT archi-\ntectures with enhanced efficiency as an original design ob-\njective. For example, EfficientFormer [33] entails mobile\napplications through dimension-consistent re-design of trans-\nformer block and removing redundant architectural compo-\nnents. VisFormer [9] transits computation extensive trans-\nformer to a convolutional counterpart for enhanced vision\nefficiency. CrossViT [5] learns multi-scale features and uti-\nlizes small/large-patch backed tokens that are channeled by\nefficient attention, offering linear time and memory complex-\nity. Even with such a rapid progress in literature, enabling\nefficient ViTs remains a significant challenge, where we next\nfurther push the Pareto front of faster ViT on top of prior art\nby a large margin. Note that we focus on the second stream\nof architectural redesign for efficiency boost, and consider a\njoint exploration with the first acceleration stream of method\nlike compression as orthogonal and fruitful future work.\nGlobal Self-Attention. A number of efforts have introduced\nglobal self-attention to capture more contextual information.\nIn NLP (i.e., 1D), BigBird [74] and LongFormer [2] pro-\nposed to select special tokens (i.e. non-learnable) as global\ntokens to attend to other tokens via a sliding-window dense\nself-attention. In computer vision, EdgeViT [44], Twins [11]\nand Focal Transformer [68] proposed hierarchical-like atten-\ntion mechanisms which rely on heuristic token aggregation\nin the forms of pooling [68] or linear projection [44, 11].\nThere are three key differences between these efforts and\nour proposed hierarchical attention: (1) as opposed to using\na pre-defined mechanism to select the global tokens (e.g.,\nrandom), we propose to learn these tokens (i.e., carrier to-\nken) via summarizing the role of each region in the input\nfeature space (2) we propose learnable token aggregation\nand propagation mechanisms by computing self-attention\namong carrier tokens (3) as opposed to using dense/dilated\nself-attention, our proposed HAT uses local window-based\nself-attention and has a smaller computational complexity.\n3. FasterViT\n3.1. Design Principals\nWe next detail our FasterViT architecture, offering Pareto\naccuracy-latency trade-off. We focus on highest throughput\nfor computer vision tasks on mainstream off-the-shelf hard-\nware such as GPUs that excel in parallel computing. Compu-\ntation in this case involves a set of streaming multiprocessors\n(SMs) with CUDA and Tensor cores as computation units.\nIt requires frequent data transfer for calculation and can be\nimpacted by data movement bandwidth. As such, opera-\ntions bounded by computation are math-limited, while those\nbounded by memory transfer are memory-limited. It requires\na careful balance between the two to maximize throughput.\nIn hierarchical vision models, spatial dimension of inter-\nmediate representation shrinks as inference proceeds. Initial\nnetwork layers mostly have larger spatial dimensions and\nfewer channel (e.g., 112\u02c6112\u02c664), making them memory-\nbound. This makes a better fit for compute-intensive opera-\ntions, such as dense convolution instead of depth-wise/sparse\ncounterparts that impose extra transfer cost. Also operations\nnot representable in matrix manipulation forms, e.g., non-\nlinearity, pooling, batch normalization, are also memory-\nbound and shall be minimized for usage. On the contrary,\nlater layers tend to be math-limited with computationally\nexpensive operations. For example, hierarchical CNNs have\nfeature maps of size 14\u02c614 with high dimensional kernels.\nThis leaves room for more expressive operations such as\nLayer Normalization, squeeze-and-excitation, or attention,\nwith fairly small effect on throughput. Guided by these in-\nsights we design a novel architecture that will benefit all\nstages from accelerated computing hardware.\n3.2. Architecture\nOur overall design is shown in Fig. 3. It exploits convo-\nlutional layers in the earlier stages that operate on higher\nresolution. The second half of the model relies on novel\nhierarchical attention layers to reason spatially across the\nentire feature maps. In this design, we optimize the archi-\ntecture for compute and throughput. As a result, the first\nhalf of the network and downsampling blocks make use of\ndense convolutional kernels. We also avoid squeeze-and-\nexcitation operators and minimize Layer Normalization for\nhigher resolution stages (i.e., 1, 2), as these layers tend to be\nmath-limited. Later stages (i.e., 3, 4) in the architecture tend\nto be math-limited as GPU hardware spends more time on\ncompute compared to the memory transfer cost. As a result,\n3\nConv (s2)\nInput\nConv (s2)\nDownsample\nConv\nBlock\nStage 2\nStage 3\nStage 4\nStage 1\nDownsample\nConv\nBlock\nDownsample\nHierarchical\nAttention\nHierarchical\nAttention\nHead\nDownsample\nCT\ninit\nCT\ninit\nFigure 3. Overview of the FasterViT architecture. We use a multi-scale architecture with CNN and transformer-based blocks in stages 1, 2\nand 3, 4, respectively. Best viewed in color.\napplying multi-head attention will not be a bottleneck.\n3.3. FasterViT Components\nStem\nAn input image x P RH\u02c6W \u02c63 is converted into\noverlapping patches by two consecutive 3 \u02c6 3 convolutional\nlayers, each with a stride of 2, which project them into a D-\ndimensional embedding. The embedded tokens are further\nbatch-normalized [32] and use the ReLU activation function\nafter each convolution.\nDownsampler Blocks\nFasterViT follows the hierarchical\nstructure: the spatial resolution is reduced by 2 between\nstages by a downsampling block. We apply 2D layer normal-\nization on spatial features, followed by a convolutional layer\nwith a kernel of 3 \u02c6 3 and a stride of two.\nConv Blocks\nStage 1 and 2 consist of residual convolu-\ntional blocks, which are defined as\n\u02c6x \u201c GELUpBNpConv3\u02c63pxqqq,\nx \u201c BNpConv3\u02c63p\u02c6xqq ` x,\n(1)\nwhere BN denotes batch normalization [32]. Following the\ndesign principles, these convolutions are dense.\nHierarchical Attention\nIn this work, we propose a novel\nformulation of windowed attention, summarized in Fig 2 and\ndetailed presentation in Fig 4. We start with local windows\nintroduced in Swin Transformer [38]. Then, we introduce\na notion of carrier tokens (CTs) that play the summarizing\nrole of the entire local window. The first attention block\nis applied on CTs to summarize and pass global informa-\ntion. Then, local window tokens and CTs are concatenated,\nsuch that every local window has access only to its own\nset of CTs. By performing self attention on concatenated\ntokens we facilitate local and global information exchange\nat reduced cost. By alternating sub-global (CTs) and local\n(windowed) self-attention we formulate a concept of hierar-\nchical attention. Conceptually, CTs can be further grouped\ninto windows and have a higher order of carrier tokens.\nAssume we are given an input feature map x P RH\u02c6W \u02c6d\nin which H, W and d denote the height, width and number\nof feature maps, let us set H \u201c W for simplicity. We first\npartition the input feature map into n \u02c6 n local windows\nwith n \u201c H2\nk2 , where k is the window size, as:\n\u02c6xl \u201c Splitk\u02c6kpxq.\n(2)\nThe key idea of our approach is the formulation of carrier\ntokens (CTs) that help to have an attention footprint much\nlarger than a local window at low cost. At first, we initialize\nCTs by pooling to L \u201c 2c tokens per window:\n\u02c6xc \u201c Conv3\u02c63pxq,\n\u02c6xct \u201c AvgPoolH2\u00d1n2Lp\u02c6xcq,\n(3)\nwhere Conv3\u02c63 represents efficient positional encoding in-\nspired by [13] and used in Twins [11]. \u02c6xct and AvgPool de-\nnote the carrier tokens and feature pooling operation, respec-\ntively; c is set to 1, but can be changed to control latency. The\ncurrent approach with conv+pooling gives flexibility with\nthe image size. These pooled tokens represent a summary\nof their respective local windows, we set L \u0103\u0103 k. The pro-\ncedure of CT initialization is performed only once for every\nresolution stage. Note that every local window \u02c6xl has unique\nset of carrier tokens, \u02c6xct,l, such that \u02c6xct \u201c t\u02c6xct,lun\nl\u201c0.\nIn every HAT block, CTs undergo the attention procedure:\n\u02c6xct \u201c \u02c6xct ` \u03b31 \u00a8 MHSApLNp\u02c6xctqq,\n\u02c6xct \u201c \u02c6xct ` \u03b32 \u00a8 MLPd\u00d14d\u00d1dpLNp\u02c6xctqq,\n(4)\nwhere LN represents layer normalization [1], MHSA rep-\nresents multi-head self attention [56], \u03b3 is a learnable per-\nchannel scale multiplier [54], MLPd\u00d14d\u00d1d is a 2-layer MLP\nwith GeLU [30] activation function.\nNext, in order to model short-long-range spatial infor-\nmation, we compute the interaction between the local and\ncarrier tokens, \u02c6xl and \u02c6xct,l, respectively. At first, local fea-\ntures and CTs are concatenated. Each local window only has\naccess to its corresponding CTs:\n\u02c6xw \u201c Concatp\u02c6xl, \u02c6xct,lq.\n(5)\nThese tokens undergo another set of attention procedure:\n\u02c6xw \u201c \u02c6xw ` \u03b31 \u00a8 MHSApLNp\u02c6xwqq,\n\u02c6xw \u201c \u02c6xw ` \u03b32 \u00a8 MLPd\u00d14d\u00d1dpLNp\u02c6xwqq.\n(6)\n4\nFeature map\nSplit\nTokenization\nCarrier token \ninitialization\nAttention + MLP\nConcate-\nnation\nAttention + MLP\nAttention + MLP\nAttention + MLP\nAttention + MLP\nSplit\nSplit\nlocal windows\nlocal windows\nsummary\nHierarchical Attention Block\nrepeated\nCarrier\ntoken\nLocal\ntokens\nCarrier\ntoken\nLocal windows\nReshape\nUpsample\nFeature map\nMerge\nMLP\n2D positional\nbias\n+\n(x,y)\nlog 2D relative\npositional bias\n(x,y)\nMLP\n+\n+\nFigure 4. Proposed Hierarchical Attention block. Carrier tokens (CT) learn a summary of each local window and facilitate global information\nexchange between local windows. Local window tokens only have access to a dedicated subset of CT for efficient attention. CT undergo full\nself-attention to enable cross-window attention. \u201cAttention\u201d stands for MHSA [56], MLP for multi-layer perceptron. Best viewed in color.\nTokens\nTokens\n\ud835\udc3b!\nTokens\nTokens\n\ud835\udc58!\n\ud835\udc58!\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n\ud835\udc58!\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n\ud835\udc58!\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n\ud835\udc58!\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n#\ud835\udc65\"#,%\n#\ud835\udc65\"#,!\n#\ud835\udc65\"#,&\n(a) Full attention\n(b) Windowed attention\n(c) Hierarchical Attention (ours)\n(d) Twins [11]\n(e) LongFormer [2]\n(f) BigBird [74]\nFigure 5. Attention map comparison for a feature map of size H \u02c6 H \u02c6 d.\n- no attention,\n- normal token attention,\n- carrier\ntoken attention,\n- random token attention. Full attention (a) has complexity of OpH4dq, windowed attention significantly reduces it\nto Opk2H2dq but lacks global context. The proposed attention mechanism allows for context to be passed on by carrier tokens. It is\nimplemented through 2 dense attentions on (i) window attention and (ii) carrier token attention. Twins [11] uses heuristics to compute\nsummarization tokens, they are used in all windowed attention and have higher computational complexity while lacking cross summarization\ntoken interaction. LongFormer [2] and BigBird [74] could be adapted (from 1D) to 2D as shown in (e) and (f), they have a more dense\nattention map. Best viewed in color.\nFinally, tokens are further split back and used in the sub-\nsequent hierarchical attention layers:\n\u02c6xl, \u02c6xct,l \u201c Splitp\u02c6xwq,\n(7)\nProcedures described in Equations 4-7 are iteratively ap-\nplied for a number of layers in the stage. To further facilitate\nlong-shot-range interaction, we perform global information\npropagation, similar to the one in [44] in the end of the stage.\nFinally, the output of the stage is computed as:\nx \u201c Upsamplen2L\u00d1H2p\u02c6xct,lq ` Mergen2k2\u00d1H2p\u02c6xlq (8)\nMHSAs performed in Eq. 4 and 6 are token position\ninvariant, however, the location of features in the spatial\ndimension are clearly informative. To address this, we first\nadd absolute positional bias directly to CTs and local win-\ndow tokens. We are inspired by SwinV2 [37] and employ\na 2-layer MLP to embed absolute 2D token location into\nfeature dimension. Then, to facilitate image-like locality in-\nductive bias we enhance the attention with log space relative\npositional bias from SwinV2 [37] (2-layer MLP). It ensures\nthat the relative position of tokens contribute to shared at-\ntention patterns. This approach yields flexibility regarding\nimage size, as the positional encoding is interpolated by the\nMLP, and hence a trained model can be applied to any input\nresolution.\nAn attention map comparison between efficient global-\nlocal self attention is shown in Fig. 5. The proposed hierar-\nchical attention splits full attention into local and sub-global,\nboth compressible to 2 dense attentions. Carrier tokens\nparticipate in both attentions and facilitate information ex-\nchange.\nComplexity Analysis of HAT\nThe key features of the effi-\nciency of our approach are (i) separation of attentions and\n(ii) local windows only have access to their CTs. The com-\nplexity of the most conventional and popular full attention\nis OpH4dq. Partitioning the feature size into windows of\nsize k, and running the attention, simplifies the attention to\nOpk2H2dq as proposed in [38]. It is well known that such\nwindowed attention is more efficient but lacks global feature\ninteraction. Our approach takes this one step further and\nis based on carrier tokens that summarize and interact over\nthe entire feature map, to remedy for missing global com-\nmunication. Given L total carrier tokens per window, local\nwindow complexity is Oppk2 ` LqH2dq. Local (windowed)\nattention is followed by attention on carrier tokens with com-\nplexity Opp H2\nk2 Lq2dq. The total cost of both attentions is\nOpk2H2d ` LH2d ` H4\nk4 L2dq.\nAn orthogonal approach for multilevel attention is to pro-\nvide access to subsampled global information inside local\nattention. For example, Twins [11] subsamples global fea-\nture map and uses it as key and value for local window\nattention. It has a complexity of Opk2H2d ` H4\nk2 dq (from\n5\nTable 1. Comparison of classification benchmarks on ImageNet-\n1K dataset [15]. Image throughput is measured on A100 GPUs\nwith batch size of 128.\nModel\nImage Size #Param FLOPs Throughput Top-1\n(Px)\n(M)\n(G)\n(Img/Sec)\n(%)\nConv-Based\nConvNeXt-T [39]\n224\n28.6\n4.5\n3196\n82.0\nConvNeXt-S [39]\n224\n50.2\n8.7\n2008\n83.1\nConvNeXt-B [39]\n224\n88.6\n15.4\n1485\n83.8\nRegNetY-040 [46]\n288\n20.6\n6.6\n3227\n83.0\nResNetV2-101 [59]\n224\n44.5\n7.8\n4019\n82.0\nEfficientNetV2-S [51]\n384\n21.5\n8.0\n1735\n83.9\nTransformer-Based\nSwin-T [38]\n224\n28.3\n4.4\n2758\n81.3\nSwin-S [38]\n224\n49.6\n8.5\n1720\n83.2\nSwinV2-T [37]\n256\n28.3\n4.4\n1674\n81.8\nSwinV2-S [37]\n256\n49.7\n8.5\n1043\n83.8\nSwinV2-B [37]\n256\n87.9\n15.1\n535\n84.6\nTNT-S [25]\n224\n23.8\n4.8\n1478\n81.5\nTwins-S [11]\n224\n24.1\n2.8\n3596\n81.7\nTwins-B [11]\n224\n56.1\n8.3\n1926\n83.1\nTwins-L [11]\n224\n99.3\n14.8\n1439\n83.7\nDeiT-B [52]\n224\n86.6\n16.9\n2035\n82.0\nDeiT3-L\n224\n304.4\n59.7\n535\n84.8\nPoolFormer-M58 [71]\n224\n73.5\n11.6\n884\n82.4\nHybrid\nCoaT-Lite-S [64]\n224\n19.8\n4.1\n2269\n82.3\nCrossViT-B [5]\n240\n105.0\n20.1\n1321\n82.2\nVisformer-S [9]\n224\n40.2\n4.8\n3676\n82.1\nEdgeViT-S [44]\n224\n13.1\n1.9\n4254\n81.0\nEfficientFormer-L7 [33]\n224\n82.2\n10.2\n1359\n83.4\nMaxViT-B [55]\n224\n120.0\n23.4\n507\n84.9\nMaxViT-L [55]\n224\n212.0\n43.9\n376\n85.1\nFasterViT\nFasterViT-0\n224\n31.4\n3.3\n5802\n82.1\nFasterViT-1\n224\n53.4\n5.3\n4188\n83.2\nFasterViT-2\n224\n75.9\n8.7\n3161\n84.2\nFasterViT-3\n224\n159.5\n18.2\n1780\n84.9\nFasterViT-4\n224\n424.6\n36.6\n849\n85.4\nFasterViT-5\n224\n957.5\n113.0\n449\n85.6\nFasterViT-6\n224\n1360.0\n142.0\n352\n85.8\nthe paper). Under the same size of the local window (k),\nand H, we can get the difference of OpL ` H2L2\nk4 q for HAT\nand Op H2\nk2 q for Twins. HAT gets more efficient with higher\nresolution, for example, for H \u201c 32, k \u201c 8, with L \u201c 4 we\nget Op8q for HAT, whereas Op16q for Twins.\n4. Experiments\n4.1. Training Settings\nImage Classification\nWe employ the ImageNet-1K\ndataset [15] for classification that includes 1.2M and 50K\ntraining and validation images. The dataset has 1000 cat-\negories and we report the performance in terms of top-1\naccuracy. In addition, we use ImageNet-21K dataset which\nhas 14M images with 21841 classes for pretraining. We\ntrain all FasterViT models by using LAMB optimizer [70]\noptimizer for 300 epochs with a learning rate of 5e-3 and\na total batch size of 4096 using 32 A100 GPUs. For data\nTable 2. ImageNet-21K pretrained classification benchmarks on\nImageNet-1K dataset [15]. Image throughput is measured on A100\nGPUs with batch size of 128. ; denotes models that are pre-trained\non ImageNet-21K dataset.\nModel\nImage Size #Param FLOPs Throughput Top-1\n(Px)\n(M)\n(G)\n(Img/Sec)\n(%)\nViT-L/16; [38]\n384\n307.0\n190.7\n149\n85.2\nSwin-L; [38]\n224\n197.0\n34.5\n787\n86.3\nSwin-L; [38]\n384\n197.0\n103.9\n206\n87.3\nConvNeXt-L; [39]\n224\n198.0\n34.4\n508\n86.6\nConvNeXt-L; [39]\n384\n198.0\n101.0\n172\n87.5\nFasterViT-4;\n224\n424.6\n36.6\n849\n86.6\nFasterViT-4;\n384\n424.6\n119.2\n281\n87.5\nTable 3. Object detection and instance segmentation benchmarks\nusing Cascade Mask R-CNN [28] on MS COCO dataset [36]. All\nmodels employ 3\u02c6 schedule. All model statistics are reported using\na input test resolution of 1280 \u02c6 800.\nBackbone\nThrou.\nAPbox\nAPmask\nim/sec\nBox\n50\n75\nMask\n50\n75\nSwin-T [38]\n161\n50.4\n69.2\n54.7\n43.7\n66.6\n47.3\nConvNeXt-T [39]\n166\n50.4\n69.1\n54.8\n43.7\n66.5\n47.3\nDeiT-Small/16 [52]\n269\n48.0\n67.2\n51.7\n41.4\n64.2\n44.3\nFasterViT-2\n287\n52.1\n71.0\n56.6\n45.2\n68.4\n49.0\nSwin-S [38]\n119\n51.9\n70.7\n56.3\n45.0\n68.2\n48.8\nX101-32 [62]\n124\n48.1\n66.5\n52.4\n41.6\n63.9\n45.2\nConvNeXt-S [39]\n128\n51.9\n70.8\n56.5\n45.0\n68.4\n49.1\nFasterViT-3\n159\n52.4\n71.1\n56.7\n45.4\n68.7\n49.3\nX101-64 [62]\n86\n48.3\n66.4\n52.3\n41.7\n64.0\n45.1\nSwin-B [38]\n90\n51.9\n70.5\n56.4\n45.0\n68.1\n48.9\nConvNeXt-B [39]\n101\n52.7\n71.3\n57.2\n45.6\n68.9\n49.5\nFasterViT-4\n117\n52.9\n71.6\n57.7\n45.8\n69.1\n49.8\nTable 4. MS COCO dataset [36] object detection results with\nDINO [75] model.\n; denotes models that are pre-trained on\nImageNet-21K dataset.\nBackbone\nModel\nEpochs FLOPs (G) Throughput APbox\nSwin-L; [38]\nHTC++ [6]\n72\n1470\n-\n57.1\nSwin-L; [38]\nDINO [75]\n36\n1285\n71\n58.5\nFasterViT-4;\nDINO [75]\n36\n1364\n84\n58.7\naugmentation, we follow same strategies as in previous ef-\nforts [39, 38]. We also use Exponential Moving Average\n(EMA) which often improves the performance. Further de-\ntails on training settings can be found in the appendix. For\npre-training on ImageNet-21K, we train the models for 90\nepochs with a learning rate of 4e-3. In addition, we fine-tune\nthe models for 60 epochs with a learning rate of 7e-5.\nDetection and Segmentation\nWe used the MS COCO\ndataset [36] to finetune a Cascade Mask-RCNN network [28]\nwith pretrained FasterViT backbones. For this purpose, we\ntrained all models with AdamW [40] optimizer with an initial\nlearning rate of 1e-4, a 3 \u02c6 schedule, weight decay of 5e-2\nand a total batch size of 16 on 8 A100 GPUs.\n6\nTable 5. Semantic segmentation benchmarks ADE20K dataset [78]\nwith UPerNet [60] and pre-trained FasterViT backbones. All model\nstatistics are reported using a input test resolution of 512 \u02c6 512.\nThroughput is measured in image/sec.\nModel\nThroughput\nFLOPs (G)\nIoU(ss/ms)\nSwin-T [38]\n350\n945\n44.5/45.8\nConvNeXt-T [39]\n363\n939\n- /46.7\nFasterViT-2\n377\n974\n47.2/48.4\nTwins-SVT-B [11]\n204\n-\n47.7/48.9\nSwin-S [38]\n219\n1038\n47.6/49.5\nConvNeXt-S [39]\n234\n1027\n- /49.6\nFasterViT-3\n254\n1076\n48.7/49.7\nTwins-SVT-L [11]\n164\n-\n48.8/50.2\nSwin-B [38]\n172\n1188\n48.1/49.7\nConvNeXt-B [39]\n189\n1170\n- /49.9\nFasterViT-4\n202\n1290\n49.1/50.3\nSemantic Segmentation\nFor semantic segmentation, we\nemployed ADE20K dataset [78] to finetune an UperNet\nnetwork [60] with pre-trained FasterViT backbones. Specif-\nically, we trained all models with Adam-W [40] optimizer\nand by using a learning rate of 6e-5, weight decay of 1e-2\nand total batch size of 16 on 8 A100 GPUs.\n5. Results\n5.1. Image Classification\nIn Table 1, we demonstrate a quantitative comparison be-\ntween the performance of FasterViT models and a variety of\ndifferent hybrid, conv and Transformer-based networks on\nImageNet-1K dataset. Comparing to Conv-based architec-\ntures, we achieve higher accuracy under the same throughput,\nfor example, we outperform ConvNeXt-T by 2.2%. Con-\nsidering the accuracy and throughput trade-off, FasterViT\nmodels are significantly faster than Transformer-based mod-\nels such as the family of Swin Transformers [38, 37]. Fur-\nthermore, compared to hybrid models, such as the recent\nEfficientFormer [33] and MaxViT [55] models, FasterViT\non average has a higher throughput while achieving a better\nImageNet top-1 performance. The trend of latency-accuracy\nPareto front holds even for post training model optimization\ntechniques such as TensorRT (see appendix).\nIn order to validate the scalability of the proposed model,\nwe pre-trained FasterViT-4 on ImageNet-21K dataset and\nfine-tuned it on various image resolutions on ImageNet-\n1K dataset. In general, FasterViT-4 has a better accuracy-\nthroughput trade-off compared to other counterparts. As\nshown in Table 2, FasterViT-4 outperforms ViT-L/16 by a\nsignificant margin of +2.3% on 3842 resolution and +15.43%\nhigher throughput. In addition, FasterViT-4 outperforms\nSwin-L on both 2242 and 3842 resolutions by +0.3% and\n+0.2% while having 12.07% and 36.40% higher through-\nput, respectively. FasterViT has 73.62% and 63.37% higher\nTable 6. Ablation study on the effectiveness of HAT compared\nto EdgeViT [44] and Twins [11] self-attention mechanisms. All\nattention blocks are replaced with the indicated attention type.\nModel\nAttention\nFLOPs (G) Throughput (Img/Sec) Top-1 (%)\nFasterViT-0 Twins [11]\n3.0\n6896\n80.8\nFasterViT-0 EdgeViT [44]\n3.2\n5928\n81.0\nFasterViT-0 HAT\n3.3\n5802\n82.2\nFasterViT-1 Twins [11]\n4.7\n4949\n82.1\nFasterViT-1 EdgeViT [44]\n4.8\n4188\n82.5\nFasterViT-1 HAT\n5.3\n4344\n83.2\nFasterViT-2 Twins [11]\n8.0\n3668\n82.9\nFasterViT-2 EdgeViT [44]\n8.5\n3127\n83.4\nFasterViT-2 HAT\n8.7\n3161\n84.1\nthroughput compared to ConvNeXt-L on 2242 and 3842\nresolutions respectively, while achieving the same Top-1\naccuracy benchmarks.\n5.2. Object Detection and Instance Segmentation\nIn Table 3, we present object detection and instance seg-\nmentation benchmarks on MS COCO dataset [36] with Cas-\ncade Mask R-CNN [28] network. We observe that FasterViT\nmodels have better accuracy-throughput trade-off when com-\npared to other counterparts. Specifically, FasterViT-4 outper-\nforms ConvNeXt-B and Swin-B by +0.2 and +1.0 in terms of\nbox AP and +0.3 and +1.0 in terms of mask AP, while being\n15% and 30% faster in terms of throughput, respectively.\nSimilar trend are observed for other models variants. In ad-\ndition, as shown in In Table 3, we conduct additional object\ndetection experiments with FasterViT-4 ImageNet-21K pre-\ntrained backbone and the state-of-the-art DINO [75] model\nand achieve a high detection accuracy of 58.7 box AP. Hence,\nthis validates the effectiveness of FasterViT as a backbone\nwith more sophisticated and state-of-the-art models.\n5.3. Semantic Segmentation\nIn Table 5, we present the semantic segmentation bench-\nmarks with UPerNet [60] network for experiments conducted\non ADE20K dataset [78]. Similar to previous tasks, Faster-\nViT models benefit from a better performance-throughput\ntrade-off. Specifically, FasterViT-4 outperforms Swin-B by\n+1.0 and +0.7 for single and multi scale inference in terms\nof mIoU, respectively while having 16.94% higher through-\nput. Similarly, FasterViT-4 has a 7.01% higher throughput\nand achieves +0.4 higher mIoU for multi scale inference\ncompared to ConvNeXt-B.\n6. Ablation\n6.1. Component-wise study\nTable 9 shows per component ablation. Two settings\nare considered: (i) when the model is trained without the\ncomponent, (ii) when the component is disabled after the\nmodel is trained. The first shows if the model can operate\n7\nTable 7. Quantitative comparison between higher resolution fine-\ntuning of FasterViT and Swin Transformer V2 networks. FasterViT\nis more accurate on average by 0.9%, and faster by 2x.\nPretrain\nFinetune\nW8, I256\nW12, I384\nW16, I512\nW24, I768\nModel\nacc\nim/s\nacc\nim/s\nacc\nim/s\nacc\nim/s\nSwinV2-T [37]\n81.8\n1674\n83.2\n573\n83.8\n168\n84.2\n72\nSwinV2-S [37]\n83.7\n633\n84.8\n338\n85.4\n153\n-\n-\nFasterViT-2\n84.3\n2500\n85.3\n984\n85.5\n489\n85.6\n155\nSwinV2-B [37]\n84.2\n499\n85.1\n251\n85.6\n115\n-\n-\nFasterViT-4 256\n85.3\n653\n86.0\n254\n86.1\n133\n86.0\n44\nTable 8. Ablation study on the effectiveness of HAT as a plug-and-\nplay module with Swin-T model for various CV tasks.\nImageNet\nCOCO\nADE20k\ntop-1\nAPbox APmask\nmIoU\nSwin-T\n81.3\n50.4\n43.7\n44.5\nSwin-T + HAT\n81.7\n50.9\n44.3\n45.4\nTable 9. Ablation study on the effectiveness of HAT components in\nFasterViT-2 as measured in Top-1 change on ImageNet-1K [15].\nAblation\nTrained\nPost training Throughput\nfrom scratch\nremoval\nratio\nHAT block\n-0.24%\n-1.49%\n1.08\nCT attention\n-0.13%\n-3.85%\n1.00\nAttention Bias\n-0.31%\n-8.90%\n1.00\nCT propagation\n-0.16%\n-\n0.93\n1D pos bias\n-0.07%\n-24.85%\n1.00\nCT initialization\n-0.05%\n-0.48%\n1.00\nWindow 14\u02c614\n+0.10%\n-\n0.90\nwell without the component, while the second cases shows\nif the components is used in the final model.\nWe observe that changing the window resolution to\n14 \u02c6 14 in the 3rd stage (effectively removing HAT by\nhave a full global window) improves the model accuracy\nby `0.1% while scarifying 10% of throughput. Even though\nthis setup shows better accuracy, it does not scale to high\nresolution, and HAT is required. Removing the HAT block\nfrom the architecture results in \u00b40.24% accuracy drop for\nre-trained model and \u00b41.49% for post training study at the\nbenefit of 8% throughtput improvement. CT attention is\nanother block of high importance, resulting in \u00b43.85% post\ntraining removal. Attention bias is an important component\nof our system, resulting in \u00b40.31% drop in the re-training\nscenario. Removing CT propagation, results in the require-\nment to pool and propagate features at every layer (similar\nto EdgeViT), that costs 7% of total inference and in lower\naccuracy \u00b40.16%. CT initialization is important to the net-\nwork, as accuracy drops by \u00b40.48% in post-training removal.\nRemoving all components and having only CNN plus win-\ndowed vanilla transformer results in \u00b40.46%.\n6.2. Attention Maps\nIn Fig. 6, we have illustrated the full attention maps of\nstage 3 layers for different FasterViT model variants. Specif-\nically, the attention maps have a resolution of 53\u02c653 con-\n(a) FasterViT-2\n(b) FasterViT-3\n(c) FasterViT-4\nFigure 6. (a) FasterViT-2. (b) FasterViT-3. (c) FasterViT-4. Full at-\ntention map visualizations of stage 3 for FasterViT model variants.\nsisting of a concatenation of 4\u02c64 carrier tokens and 49\u02c649\nlocal window-based attention. The carrier tokens are in the\ntop left position of each attention map. We observe that\nall local tokens attend to the carrier tokens in addition to\ntheir own local attention. Please see the appendix for more\nvisualizations.\n6.3. Attention Alternatives\nAs shown in Table 6, we performed a comprehensive\nablation study to validate the effectiveness of HAT by re-\nplacing all attention layers with attention mechanisms in\nEdgeViT [44] and Twins [11] in the 3rs and 4th stages. For\nall model variants, FasterViT models with HAT achieve a\nbetter accuracy, sometimes by a significant margin. Twins\nachieves a higher throughput due to its small kernel size (i.e.\nk \u201c 2), however, this significantly limits its accuracy. The\nbetter performance of HAT is attributed to its learnable infor-\nmation aggregation/propagation via CTs, and direct access\nto dedicated CTs in windowed attention.\nPlug-and-Play HAT. We employed HAT as a plug-and-play\nmodule with Swin-T model Table 8. This change results in\n+0.9 and +0.4% improvement in terms of mIoU and Top-1\naccuracy on ImageNet classification and ADE20K segmenta-\ntion tasks. In addition, improvements on MS COCO by +0.5\nbox AP and +0.6 mask AP on object detection and instance\nsegmentation tasks, respectively. These results indicate the\nviability of HAT as a standalone self-attention mechanism.\n7. Conclusion\nIn this work, we have presented a novel hybrid model,\ndenoted as FasterViT, which achieves SOTA Pareto-front\nin terms of ImageNet Top-1 accuracy and throughput. We\nintroduced an novel Hierarchical Attention (HAT) which\ncomputes cross-window interactions at reduced computa-\ntional cost. We have extensively validated the effectiveness\nof FasterViT in downstream tasks such as object detection, in-\nstance segmentation and semantic segmentation. Our bench-\nmarks demonstrate better accuracy-throughput trade-off in\ncomparison to counterpart models such as ConvNeXt and\nSwin Transformer. We have also demonstrated that HAT\ncan be used as a plug-and-play self-attention module and\nimprove the accuracy and throughput for various tasks.\n8\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450, 2016.\n4, 14, 15\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer.\narXiv preprint\narXiv:2004.05150, 2020. 3, 5\n[3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,\nand Judy Hoffman. Hydra attention: Efficient attention with\nmany heads. arXiv preprint arXiv:2209.07484, 2022. 3\n[4] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced\nlinear attention for high-resolution low-computation visual\nrecognition. arXiv preprint arXiv:2205.14756, 2022. 3\n[5] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.\nCrossvit: Cross-attention multi-scale vision transformer for\nimage classification. In Proceedings of the IEEE/CVF in-\nternational conference on computer vision, pages 357\u2013366,\n2021. 3, 6\n[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,\nWanli Ouyang, et al. Hybrid task cascade for instance seg-\nmentation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 4974\u20134983,\n2019. 6\n[7] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.\nAutoformer: Searching transformers for visual recognition.\narXiv preprint arXiv:2107.00651, 2021. 3\n[8] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and\nZhangyang Wang. Chasing sparsity in vision transformers:\nAn end-to-end exploration. arXiv preprint arXiv:2106.04533,\n2021. 3\n[9] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,\nLonghui Wei, and Qi Tian. Visformer: The vision-friendly\ntransformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 589\u2013598, 2021. 3, 6,\n14\n[10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmen-\ntation. Advances in Neural Information Processing Systems,\n34:17864\u201317875, 2021. 1\n[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins:\nRevisiting the design of spatial attention in vision transform-\ners. Advances in Neural Information Processing Systems, 34,\n2021. 3, 4, 5, 6, 7, 8, 14\n[12] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional\npositional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 3\n[13] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? CoRR, abs/2102.10882, 2021. 4\n[14] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data sizes.\nAdvances in Neural Information Processing Systems, 34:3965\u2013\n3977, 2021. 3\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009. 6, 8, 12, 13\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. NAACL, 2019. 3\n[17] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang,\nNenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin\ntransformer: A general vision transformer backbone with\ncross-shaped windows. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12124\u201312134, 2022. 1, 3\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. In International Conference\non Learning Representations, 2020. 1, 2\n[19] Jiawei Du, Zhou Daquan, Jiashi Feng, Vincent Tan, and\nJoey Tianyi Zhou. Sharpness-aware training for free. In\nAdvances in Neural Information Processing Systems. 15\n[20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer. Multiscale vision transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 6824\u20136835, 2021. 2\n[21] Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang,\nJiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu.\nYou\nonly look at one sequence: Rethinking transformer in vision\nthrough object detection. Advances in Neural Information\nProcessing Systems, 34:26183\u201326197, 2021. 1\n[22] William Fedus, Barret Zoph, and Noam Shazeer. Switch\ntransformers: Scaling to trillion parameter models with simple\nand efficient sparsity, 2021. 3\n[23] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam\nNeyshabur. Sharpness-aware minimization for efficiently\nimproving generalization. arXiv preprint arXiv:2010.01412,\n2020. 15\n[24] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herv\u00b4e J\u00b4egou, and Matthijs\nDouze. Levit: a vision transformer in convnet\u2019s clothing\nfor faster inference. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 12259\u201312269,\n2021. 2\n[25] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. Advances\nin Neural Information Processing Systems, 34:15908\u201315919,\n2021. 6\n[26] Ali Hatamizadeh, Hongxu Yin, Jan Kautz, and Pavlo\nMolchanov.\nGlobal context vision transformers.\narXiv\npreprint arXiv:2206.09959, 2022. 3\n[27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022. 1\n9\n[28] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 6, 7\n[29] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of robust-\nness: A critical analysis of out-of-distribution generalization.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 8340\u20138349, 2021. 12\n[30] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 4\n[31] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,\nand Dawn Song. Natural adversarial examples. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 15262\u201315271, 2021. 12\n[32] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learning,\npages 448\u2013456. PMLR, 2015. 4\n[33] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evan-\ngelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Effi-\ncientformer: Vision transformers at mobilenet speed. arXiv\npreprint arXiv:2206.01191, 2022. 3, 6, 7\n[34] Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue\nWang, and Pengtao Xie. EVit: Expediting vision transformers\nvia token reorganizations. In International Conference on\nLearning Representations, 2022. 3\n[35] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980\u20132988, 2017. 1, 2\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 6, 7\n[37] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\nSwin transformer v2: Scaling up capacity and resolution.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12009\u201312019, 2022.\n2, 3, 5, 6, 7, 8, 13, 14, 15\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 1, 2, 3, 4, 5, 6,\n7, 12, 14\n[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11976\u201311986,\n2022. 6, 7, 12\n[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6, 7,\n15\n[41] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu,\nWeiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft:\nsoftmax-free transformer with linear complexity. Advances\nin Neural Information Processing Systems, 34:21297\u201321309,\n2021. 3\n[42] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish\nPrabhu, Mohammad Rastegari, and Oncel Tuzel. Token pool-\ning in vision transformers. arXiv preprint arXiv:2110.03860,\n2021. 3\n[43] Pavlo Molchanov, Jimmy Hall, Hongxu Yin, Jan Kautz, Ni-\ncolo Fusi, and Arash Vahdat. Lana: latency aware network\nacceleration. In European Conference on Computer Vision,\npages 137\u2013156. Springer, 2022. 3\n[44] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz\nDudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais\nMartinez. Edgevits: Competing light-weight cnns on mobile\ndevices with vision transformers. In ECCV, 2022. 3, 5, 6, 7,\n8, 14\n[45] Sayak Paul and Pin-Yu Chen. Vision transformers are robust\nlearners. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 2071\u20132081, 2022. 1\n[46] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Doll\u00b4ar. Designing network design spaces. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10428\u201310436, 2020. 6\n[47] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks? Advances in\nNeural Information Processing Systems, 34, 2021. 1\n[48] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision\ntransformers with dynamic token sparsification. In NeurIPS,\n2021. 3\n[49] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In International Conference on Machine Learning,\npages 5389\u20135400. PMLR, 2019. 12\n[50] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick\nLeGresley, Jared Casper, and Bryan Catanzaro. Megatron-\nlm: Training multi-billion parameter language models using\nmodel parallelism. arXiv preprint arXiv:1909.08053, 2019. 3\n[51] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\nand faster training. In International Conference on Machine\nLearning, pages 10096\u201310106. PMLR, 2021. 6\n[52] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International Conference on Machine Learning,\npages 10347\u201310357. PMLR, 2021. 1, 2, 6\n[53] Hugo Touvron, Matthieu Cord, and Herv\u00b4e J\u00b4egou. Deit iii:\nRevenge of the vit. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXIV, pages 516\u2013533. Springer, 2022. 15\n[54] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv\u00b4e J\u00b4egou.\nGoing deeper with\nimage transformers, 2021. 4\n[55] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan Bovik, and Yinxiao Li.\nMaxvit:\nMulti-axis vision transformer. In Computer Vision\u2013ECCV\n10\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u2013\n27, 2022, Proceedings, Part XXIV, pages 459\u2013479. Springer,\n2022. 6, 7\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998\u20136008, 2017. 1,\n4, 5\n[57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 568\u2013\n578, 2021. 2\n[58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2:\nImproved baselines with pyramid vision transformer. Compu-\ntational Visual Media, 8(3):415\u2013424, 2022. 2\n[59] Ross Wightman, Hugo Touvron, and Herv\u00b4e J\u00b4egou. Resnet\nstrikes back: An improved training procedure in timm. arXiv\npreprint arXiv:2110.00476, 2021. 6\n[60] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understanding.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), pages 418\u2013434, 2018. 7\n[61] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo.\nSegformer: Simple and\nefficient design for semantic segmentation with transform-\ners. Advances in Neural Information Processing Systems,\n34:12077\u201312090, 2021. 1\n[62] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492\u20131500,\n2017. 6\n[63] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9653\u20139663, 2022. 1\n[64] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 9981\u20139990, 2021. 6\n[65] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao.\nVitae: Vision transformer advanced by exploring intrinsic\ninductive bias. Advances in Neural Information Processing\nSystems, 34:28522\u201328535, 2021. 1\n[66] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke\nLi, Weiming Dong, Liqing Zhang, Changsheng Xu, and Xing\nSun. Evo-vit: Slow-fast token evolution for dynamic vision\ntransformer. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 36, pages 2964\u20132972, 2022. 3\n[67] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and\nJan Kautz. Nvit: Vision transformer compression and param-\neter redistribution. arXiv preprint arXiv:2110.04869, 2021.\n3\n[68] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for\nlong-range interactions in vision transformers. Advances in\nNeural Information Processing Systems, 34, 2021. 3\n[69] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan\nKautz, and Pavlo Molchanov. A-ViT: Adaptive tokens for\nefficient vision transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022. 3\n[70] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv\nKumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel,\nKurt Keutzer, and Cho-Jui Hsieh. Large batch optimization\nfor deep learning: Training bert in 76 minutes. arXiv preprint\narXiv:1904.00962, 2019. 6, 15\n[71] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,\nXinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer\nis actually what you need for vision.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10819\u201310829, 2022. 6\n[72] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token ViT: Training vision transformers from\nscratch on imagenet. In ICCV, 2021. 3\n[73] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and\nShuicheng Yan. Volo: Vision outlooker for visual recog-\nnition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2022. 3\n[74] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua\nAinslie, Chris Alberti, Santiago Ontanon, Philip Pham,\nAnirudh Ravula, Qifan Wang, Li Yang, et al.\nBig\nbird: Transformers for longer sequences.\narXiv preprint\narXiv:2007.14062, 2020. 3, 5\n[75] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr\nwith improved denoising anchor boxes for end-to-end object\ndetection. arXiv preprint arXiv:2203.03605, 2022. 6, 7\n[76] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding. arXiv preprint arXiv:2103.15358, 2021. 3\n[77] Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang,\nLicheng Jiao, and Fang Liu. Vit-yolo: Transformer-based\nyolo for object detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 2799\u2013\n2808, 2021. 1\n[78] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\nriuso, and Antonio Torralba. Scene parsing through ade20k\ndataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633\u2013641, 2017. 7\n11\nAppendix\nA. Robustness Analysis\nIn this section, we analyze the robustness of FasterViT\nmodels on different datasets.\nWe test FasterViT model\nvariants on ImageNet-A [31], ImageNet-R [29] and Im-\nageNetV2 [49] datasets.\nIn addition, we did not per-\nform any fine-tuning and simply employed the pre-trained\nImageNet-1K [15] weights for each model. As shown in\nTable S.1, FasterViT demonstrates promising robustness per-\nformance on various datasets for each model variant. Specif-\nically, FasterViT-3 outperforms comparable models such\nas ConvNeXt-B and Swin-B [39] by +7.5% and +8.4% on\nImageNet-A [31], +0.6% and +5.3% on ImageNet-R [29]\nand +1.3% and +2.7% on ImageNetV2 [49], respectively.\nFor larger models, FasterViT-4 outperforms ConvNeXt-\nL [39] by +7.9%, +2.6% and +1.5% on ImageNet-A [31],\nImageNet-R [29] and ImageNetV2 [49], respectively, hence\nvalidating the effectiveness of the proposed model in vari-\nous benchmarks. Similar trends can be observed for smaller\nmodels.\nB. Ablation\nB.1. FasterViT Component-wise Study\nTable S.2 shows per component ablation. Two settings\nare considered: (i) when the model is trained without the\ncomponent, (ii) when the component is disabled after the\nmodel is trained. The first shows if the model can operate\nwell without the component, while the second cases shows\nif the components is used in the final model.\nWe observe that changing the window resolution to 14 \u02c6\n14 in the 3rd stage (effectively removing HAT by have a\nfull global window) improves the model accuracy by `20%\nwhile scarifying 10% of throughput. Even though this setup\nshows better accuracy, it does not scale to high resolution,\nand HAT is required.\nRemoving the HAT block from the architecture results\nin \u00b40.24% accuracy drop for re-trained model and \u00b41.49%\nfor post training study at the benefit of 8% throughtput im-\nprovement. CT attention is another block of high importance,\nresulting in \u00b43.85% post training removal. Attention bias is\nan important component of our system, resulting in \u00b40.31%\ndrop in the re-training scenario. Removing CT propagation,\nresults in the requirement to pool and propagate features\nat every layer (similar to EdgeViT), that costs 7% of total\ninference and in lower accuracy \u00b40.16%. CT initialization\nis important to the network, as accuracy drops by \u00b40.48%\nin post-training removal. Removing all components and hav-\ning only CNN plus windowed vanilla transformer results in\n\u00b40.46%.\nTable S.1. Robustness analysis of ImageNet-1K [15] pretrained\nFasterViT models on ImageNet-A [31], ImageNet-R [29] and Ima-\ngeNetV2 [49] datasets.\nModel\nSize #Param FLOPs Throughput Clean\nA\nR\nV2\n(Px)\n(M)\n(G)\n(Img/Sec)\n(%)\n(%) (%) (%)\nFasterViT-0\n224\n31.4\n3.3\n5802\n82.1 23.9 45.9 70.9\nFasterViT-1\n224\n53.4\n5.3\n4188\n83.2 31.2 47.5 72.6\nSwin-T [38]\n224\n28.3\n4.4\n2758\n81.3 21.6 41.3 69.7\nConvNeXt-T [39] 224\n28.6\n4.5\n3196\n82.0 24.2 47.2 71.0\nConvNeXt-S [39] 224\n50.2\n8.7\n2008\n83.1 31.3 49.5 72.4\nFasterViT-2\n224\n75.9\n8.7\n3161\n84.2 38.2 49.6 73.7\nSwin-S [38]\n224\n49.6\n8.5\n1720\n83.2 32.5 44.7 72.1\nSwin-B [38]\n224\n87.8\n15.4\n1232\n83.4 35.8 46.6 72.3\nConvNeXt-B [39] 224\n88.6\n15.4\n1485\n83.8 36.7 51.3 73.7\nFasterViT-3\n224\n159.5\n18.2\n1780\n84.9 44.2 51.9 75.0\nConvNeXt-L [39] 224\n198.0\n34.4\n508\n84.3 41.1 53.4 74.2\nFasterViT-4\n224\n424.6\n36.6\n849\n85.4 49.0 56.0 75.7\nFasterViT-5\n224\n975.5\n113.0\n449\n85.6 52.7 56.9 76.0\nFasterViT-6\n224 1360.0 142.0\n352\n85.8 53.7 57.1 76.1\nTable S.2. Ablation study on the effectiveness of HAT components\nin FasterViT-2 as measured in Top-1 change on ImageNet-1K [15].\nAblation\nTrained\nPost training Throughput\nfrom scratch\nremoval\nratio\nHAT block\n-0.24%\n-1.49%\n1.08\nCT attention\n-0.13%\n-3.85%\n1.00\nAttention Bias\n-0.31%\n-8.90%\n1.00\nCT propagation\n-0.16%\n-\n0.93\n1D pos bias\n-0.07%\n-24.85%\n1.00\nCT initialization\n-0.05%\n-0.48%\n1.00\nWindow 14\u02c614\n+0.20%\n-\n0.90\nTable S.3. Ablation study of the number of carrier tokens. The\nnumbers after w and c indicate the window size and the carrier\ntoken window size, respectively.\n224 res\nConfig\nw14c0\nw7c7\nw7c6\nw7c5\nw7c2\nw7c1\nAcc\n84.26\n84.92\n84.41\n84.28\n84.16\n83.96\nLatency ratio\n0.9\n0.47\n0.57\n0.67\n1.0\n1.05\n256 res\nConfig\nw16c0\nw8c4\nw8c3\nw8c2\nAcc\n84.65\n84.7\n84.51\n84.4\nLatency ratio\n0.95\n0.87\n0.93\n1.0\nB.2. Number of carrier tokens\nThe proposed HAT can be controlled by the number of\ncarrier tokens. We ablate the carrier token window size in\nTable S.3. It should be noted, that the number of model\nparameters does not change with varying the number of CT,\nhowever, the latency does.\n12\n(a) FasterViT-0\n(b) FasterViT-1\n(c) FasterViT-2\n(d) FasterViT-3\n(d) FasterViT-4\nFigure S.1. (a) FasterViT-0. (b) FasterViT-1. (c) FasterViT-2. (d) FasterViT-3 (e) FasterViT-4. Full attention map visualizations of stage 3 for\nFasterViT model variants. From top to bottom, we visualize attention maps of first to last layers with an interval of a quarter length of the\nnumber of layers in stage 3 for each model. We visualize the attention maps of the same input image for all cases to facilitate comparability.\nB.3. SwinV2 Comparison\nIn the Table 7 we compare the performance of\nSwinV2 [37] and FasterViT models on large image reso-\nlution. The initial model is pretrained with an image res-\nolution of 2562px for 300 epochs on ImageNet-1K. Then\nmodels are fine-tuned on a larger resolution (I) for an 30\nepochs with various window sizes (W). Faster-ViT consis-\ntently demonstrates a higher image throughput, sometimes\nby a significant margin compared to Swin Transformer V2\nmodel. Hence validating the effectiveness of the proposed\nhierarchical attention for high input resolution.\nC. Attention Maps\nIn Fig. S.1, we have illustrated the full attention maps of\nstage 3 layers for different FasterViT model variants. For\nthis purpose, we use input images of size 224 \u02c6 224 \u02c6\n3 and ImageNet-1K [15] trained FasterViT models. For\neach model, from the top to the bottom rows, we show the\nattention maps from the first to the final layer with an interval\nof a quarter of the total number of layers at stage 3 (e.g.\nlayers 1, 4, 9 and 12 for FasterViT-4).\nIn particular, Stage 3 for this illustration serves an im-\nportant purpose, since we use local attention windows of\n7\u02c67 with input features that have a resolution of 14 \u02c6 14.\nHence, attention is computed in 4 local regions after win-\ndow partitioning and 4 carrier tokens are designated to each\ncorresponding window. Each illustrated attention map has\na size of size 53\u02c653 consisting of a concatenation of 4\u02c64\ncarrier tokens and 49\u02c649 local window-based attention. The\ncarrier tokens are shown in in the top left position of each\nmap. We observe that for all models, all tokens will attend\n13\n200\n400\n600\n800\n1000\n1200\n1400\nTRT Throughput (img/sec)\n82\n83\n84\n85\nAccuracy (%)\nFasterViT(ours)\nConvNeXt (2022)\nEfficientNetV2 (2021)\nSwin (2021)\nSwinV2 (2022)\nTNT (2021)\nTwins (2021)\nPoolFormer (2022)\nCoaT (2021)\nCrossViT (2021)\nVisformer (2021)\nFasterViT-4\nFasterViT-3\nFasterViT-2\nFasterViT-1\nFasterViT-0\nFigure S.2. Comparison of image throughput and ImageNet-1K\nTop-1 accuracy with TensorRT post-training model optimization.\nFor all models, throughput is measured on A100 GPU with batch\nsize of 1.\nto the carrier tokens with different patterns.\nFor FasterViT-0 and FasterViT-1 models, from the first to\nthe last layers, all tokens transition to attend to the the carrier\ntokens (i.e. vertical bar on the left side). In the last layers,\nin addition to all tokens attending to the carrier tokens, we\nsee a more global attention pattern, hence showing the cross\ninteraction between different regions.\nFor FasterViT-2, FasterViT-3 and FasterViT-4 models,\nstarting from the first layers, all tokens attend to both carrier\nand local tokens. In the last layers however, the attention\npattern shifts from local to global. As discussed in this work\nand also shown in these illustrations, carrier tokens serve\nan integral role in modeling cross-region interactions and\ncapturing long-range spatial dependencies.\nD. TensorRT latency\nAll throughput numbers and insights presented in the\nmain paper were computed using PyTorch v1.13. In order\nto demonstrate the scalability with post-training optimiza-\ntion techniques, we compared throughput using the TensoRT\n(TRT) framework for batch size 1, as illustrated in Fig S.2.\nFasterViT is still considerably faster than other models, mak-\ning it a good choice to meet various efficient inference design\ntargets.\nFigure S.3. Learned positional biases for attentions in the 3rd stage\nof FasterViT-4 model finetuned for 512\u02c6512 px. Each kernel cor-\nresponds to a bias for a single head in the multi-headed attention.\nVisualizations demonstrate that the model learns positional depen-\ndent features, while also sharing the pattern between pixels.\nE. Attention bias\nWe follow the concept of relative positional bias in the\nattention from Swin [38]. Particularly, we use the imple-\nmentation with MLP from SwinV2 [37], where relative co-\nordinate shift in x, y is transformed to the positional bias in\nthe attention via 2-layer network. This allows the model to\nlearn relative position aware kernels, and to introduce image\ninductive bias. We visualize learned positional biases of the\nMLP in FasterViT-4 finetuned for 512 with window size of\n16\u02c616 pixels in Fig S.3. The visualization shows a diverse\nset of kernels learned by FasterViT model.\nF. Design Insights\nLayer normalization [1]. We found it to be critical for trans-\nformer blocks (stage 3 and 4). Replacing it with batch nor-\nmalization leads to accuracy drop of 0.7%. The LN performs\ncross token normalization and affects cross-channel interac-\ntion.\nNo feature map reshaping. In our architecture, we have re-\nmoved windowing and de-windowing functions from trans-\nformer layers. They are usually used to perform convo-\nlutions between layers (like in Twins [11], EdgeViT [44],\nVisformer [9]), or window shifting (Swin [38], SwinV2 [37]).\nWe perform windowing only once once in stages 3 and 4,\nand keep data as tokenized with channel last. This leads to\nthroughput improvement of 5% for PyTorch and 10% for\nTensorRT.\n14\nOutput Size\n(Downs. Rate)\nFasterViT-1\nFasterViT-2\nFasterViT-3\nFasterViT-4\nStem\n112\u02c6112\n(2\u02c6)\n\u201e\nConv-BN-ReLU\nC:32, S:2\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:64, S:2\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:64, S:2\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:64, S:2\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:80\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:96\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:128\n\u0237\n\u02c6 1\n\u201e\nConv-BN-ReLU\nC:196\n\u0237\n\u02c6 1\nStage 1\n56\u02c656\n(4\u02c6)\nLN-2D, Conv, C:160, S:2\nLN-2D, Conv, C:192, S:2\nLN-2D, Conv, C:256, S:2\nLN-2D, Conv, C:392, S:2\n\u201e\nResBlock\nC:160\n\u0237\n\u02c61,\n\u201e\nResBlock\nC:192\n\u0237\n\u02c6 3,\n\u201e\nResBlock\nC:256\n\u0237\n\u02c6 3,\n\u201e\nResBlock\nC:392\n\u0237\n\u02c6 3,\nStage 2\n28\u02c628\n(8\u02c6)\nLN-2D, Conv, C:320, S:2\nLN-2D Conv, C:384, S:2\nLN-2D, Conv, C:512, S:2\nLN-2D, Conv, C:768, S:2\n\u201e\nResBlock\nC:320\n\u0237\n\u02c6 3,\n\u201e\nResBlock\nC:384\n\u0237\n\u02c6 3,\n\u201e\nResBlock\nC:512\n\u0237\n\u02c6 3,\n\u201e\nResBlock\nC:768\n\u0237\n\u02c6 3,\nStage 3\n14\u02c614\n(16\u02c6)\nLN-2D, Conv, C:640, S:2\nLN-2D, Conv, C:768, S:2\nLN-2D, Conv, C:1024, S:2\nLN-2D, Conv, C:1568, S:2\n\u201e\nHAT\nC:640, head:8\n\u0237\n\u02c6 8 ,\n\u201e\nHAT\nC:768, head:8\n\u0237\n\u02c6 8,\n\u201e\nHAT\nC:1024, head:8\n\u0237\n\u02c6 12,\n\u201e\nHAT\nC:1568, head:16\n\u0237\n\u02c6 12,\nStage 4\n7\u02c67\n(32\u02c6)\nLN-2D, Conv, C:1280, S:2\nLN-2D, Conv, C:1536, S:2\nLN-2D, Conv, C:2048, S:2\nLN-2D, Conv, C:3136, S:2\n\u201e\nHAT\nC:1280, head:16\n\u0237\n\u02c6 5,\n\u201e\nHAT\nC:1536, head:16\n\u0237\n\u02c6 5,\n\u201e\nHAT\nC:2048, head:16\n\u0237\n\u02c6 5,\n\u201e\nHAT\nC:3136, head:32\n\u0237\n\u02c6 5,\nTable S.4. FasterViT architecture configurations. BN and LN-2D denote Batch Normalization and 2D Layer Normalization, respectively.\nHAT denotes Hierarchical Attention block.\nLAMB optimizer [70].\nWe observed incredible stabil-\nity of LAMB [70] optimizer for training our biggest\nmodels (FasterVIT-3 and FasterViT-4), more widely used\nAdamW [40] was leading to NaNs for some trainings. We\nattribute this to joined usage of batch normalization and layer\nnormalization [1] in the same model.\nPositional bias. We employ 1D positional bias for local\nand carrier tokens, as well as 2D relative attention bias by\nMLP introduced in SwinV2 [37]. For 1D bias we remove\nlog scale. This approach yields flexibility to the image size,\nas positional encoding is interpolated by MLP if resolution\nchange. Those positional biases are quick to compute, how-\never, will block all cores in GPUs until positional biases are\ncomputed, and will significantly impact the throughput. To\naddress this, we propose to pre-compute positional biases\nfor a given feature resolution and skip the MLP bottleneck,\nleading to 6% throughput gain.\nDrop-out. We found that conventional drop-out on MLP\nlayers and attention has a negative effect on the final accuracy\neven for big models that overfit. Stochastic depth is helpful;\nin contrary to recent trends, we found that a small probability\n(up to 30%) works better than 65% like in DEIT3 [53]. Better\nregularization can be achieved by increased weight decay.\nFor example, model 4 with drop-path rate of 50% and weight\ndecay of 0.05 achieves 84.91%, while model 4 with drop-\npath rate of 30% and weight decay of 0.12 achieves 85.15%.\nMESA [19]. It is shown to be useful to prevent overfitting\nof larger models at little overhead. MESA is a simplified\nversion of SAM [23] that forces optimization to have sharper\nminima at the convergence, naive implementation slows\ndown training by 2x. In MESA, authors propose to simply\napply knowledge distillation loss with respect to the EMA\nweight computed during training, the training overhead is\nalmost not noticeable. We enable it after 25% of the training,\ncoefficient is set proportionally to the model size in range\nLatency\nFLOPs\nMemory\nLayers\n0%\n25%\n50%\n75%\n100%\nEmbedding\nStage 1\nStage 2\nStage 3\nStage 4\nFigure S.4. FasterViT-2 profiling benchmarks. In FasterViT mod-\nels, stage 3 (HAT) dominates over all metrics.\n0.25 (FasterViT-0)-3.0(FasterViT-4).\nIntermediate LN. SwinV2 [37] argues that intermediate\nLN [1] help to stabilize training of large models, we saw\naccuracy degradation of this approach.\nG. Architecture Details\nIn Table S.4, we show the different architecture configu-\nrations of the FasterViT model variants.\nH. FasterViT Profiling\nIn Fig. S.4, we provide detailed stage-wise profiling of\nFasterViT-2 using NVIDIA DLSIM. As expected, stage 3\n(HAT) has the highest latency, FLOPs and memory footprint\nsince it is composed of considerably more layers compared\nto other stages.\n15\n"
  },
  {
    "title": "Benchmarking Neural Network Training Algorithms",
    "link": "https://arxiv.org/pdf/2306.07179.pdf",
    "upvote": "22",
    "text": "Benchmarking Neural Network Training Algorithms\nGeorge E. Dahl1\u02da\ngdahl@google.com\nFrank Schneider2\u02da\nf.schneider@uni-tuebingen.de\nZachary Nado1\u02da\nznado@google.com\nNaman Agarwal1\u02da\nnamanagarwal@google.com\nChandramouli Shama Sastry3,4:\nchandramouli.sastry@gmail.com\nPhilipp Hennig2:\nphilipp.hennig@uni-tuebingen.de\nSourabh Medapati1:\nsmedapati@google.com\nRuna Eschenhagen2:\nre393@cam.ac.uk\nPriya Kasimbeg1:\nkasimbeg@google.com\nDaniel Suo1:\ndsuo@google.com\nJuhan Bae3,5:\njbae@cs.toronto.edu\nJustin Gilmer1:\ngilmer@google.com\nAbel L. Peirson6:\nalpv95@stanford.edu\nBilal Khan1:\nkbilal@google.com\nRohan Anil1:\nrohananil@google.com\nMike Rabbat7:\nmikerabbat@meta.com\nShankar Krishnan1:\nskrishnan@google.com\nDaniel Snider3,5;\ndans@cs.toronto.edu\nEhsan Amid1;\neamid@google.com\nKongtao Chen1;\nkongtao@google.com\nChris J. Maddison3,5;\ncmaddis@cs.toronto.edu\nRakshith Vasudev8;\nrakshith.vasudev@dell.com\nMichal Badura1;\nmbadura@google.com\nAnkush Garg1;\nankugarg@google.com\nPeter Mattson1;\npetermattson@google.com\n1Google; 2University of T\u00a8ubingen; 3Vector Institute; 4Dalhousie University; 5University of Toronto;\n6Stanford University; 7Meta AI (FAIR); 8Dell Technologies\n\u2217. Corresponding authors.\n\u2020. These authors are listed in random order.\n\u2021. These authors are listed in random order.\n1\narXiv:2306.07179v1  [cs.LG]  12 Jun 2023\nAbstract\nTraining algorithms, broadly construed, are an essential part of every deep learning pipeline.\nTraining algorithm improvements that speed up training across a wide variety of work-\nloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection\nschemes) could save time, save computational resources, and lead to better, more accurate,\nmodels. Unfortunately, as a community, we are currently unable to reliably identify train-\ning algorithm improvements, or even determine the state-of-the-art training algorithm. In\nthis work, using concrete experiments, we argue that real progress in speeding up training\nrequires new benchmarks that resolve three basic challenges faced by empirical comparisons\nof training algorithms: (1) how to decide when training is complete and precisely measure\ntraining time, (2) how to handle the sensitivity of measurements to exact workload details,\nand (3) how to fairly compare algorithms that require hyperparameter tuning. In order to\naddress these challenges, we introduce a new, competitive, time-to-result benchmark using\nmultiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms\nbenchmark.\nOur benchmark includes a set of workload variants that make it possible\nto detect benchmark submissions that are more robust to workload changes than current\nwidely-used methods. Finally, we evaluate baseline submissions constructed using various\noptimizers that represent current practice, as well as other optimizers that have recently\nreceived attention in the literature.\nThese baseline results collectively demonstrate the\nfeasibility of our benchmark, show that non-trivial gaps between methods exist, and set a\nprovisional state-of-the-art for future benchmark submissions to try and surpass.\nKeywords:\nbenchmark, deep learning, neural network, training algorithms, optimizer\nContents\n1\nIntroduction\n5\n1.1\nContributions of This Work . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.2\nTraining Algorithm Benchmark Goals and Scope . . . . . . . . . . . . . . .\n7\n2\nThe Challenges of Empirical Comparisons of Training Algorithms\n8\n2.1\nPrecisely Defining and Measuring Training Speed . . . . . . . . . . . . . . .\n9\n2.2\nDependence on the Workload . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.1\nSensitivity of Optimizer Ranking to the Model Architecture . . . . .\n11\n2.2.2\nImplications of Workload Sensitivity . . . . . . . . . . . . . . . . . .\n14\n2.3\nWe Cannot Compare Families of Training Algorithms, Only Specific Instances 15\n2.3.1\nTraining Algorithms With Different Hyperparameters\n. . . . . . . .\n15\n2.3.2\nTraining Algorithms With Different Hyperparameter Search Spaces .\n16\n2.3.3\nTraining Algorithms With Different Tuning Goals\n. . . . . . . . . .\n18\n2.4\nStrong Baselines Are Far Too Hard to Obtain . . . . . . . . . . . . . . . . .\n20\n3\nRelated Work\n21\n3.1\nExisting Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.2\nMethodological Critiques of Training Algorithm Comparisons . . . . . . . .\n23\n3.3\nDisagreement over Training Algorithms: The Case for Clear Benchmarks\n.\n23\n4\nRules\n25\n4.1\nA Time-to-Result Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nBenchmarking Neural Network Training Algorithms\n4.1.1\nMeasuring Runtime\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.1.2\nStandardizing Benchmarking Hardware\n. . . . . . . . . . . . . . . .\n27\n4.2\nSpecifying a Training Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.2.1\nIsolate the Training Algorithm\n. . . . . . . . . . . . . . . . . . . . .\n29\n4.2.2\nIncentivizing Generally Useful Submissions\n. . . . . . . . . . . . . .\n30\n4.3\nWorkloads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.3.1\nFixed and Randomized Workloads . . . . . . . . . . . . . . . . . . .\n31\n4.4\nTuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.4.1\nExternal Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.4.2\nSelf-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.5\nScoring and Reporting Results\n. . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.5.1\nAggregation Using Performance Profiles . . . . . . . . . . . . . . . .\n35\n4.5.2\nIntegrating Performance Profiles for the Benchmark Score . . . . . .\n36\n4.5.3\nUsing Held-Out Workloads in Scoring\n. . . . . . . . . . . . . . . . .\n37\n4.5.4\nMeasuring Year-Over-Year Benchmark Progress . . . . . . . . . . . .\n37\n5\nTarget-Setting Experiments\n38\n6\nRandomized Workloads Experiments\n42\n6.1\nDesiderata for Workload Variants . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6.2\nCreating and Testing Workload Variants . . . . . . . . . . . . . . . . . . . .\n44\n6.3\nWorkload Variants of the Benchmark . . . . . . . . . . . . . . . . . . . . . .\n46\n7\nBaseline Submissions\n46\n7.1\nBaseline Creation Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n7.2\nBaseline Timing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n7.3\nBaseline Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n7.3.1\nBaseline Results Comparing Search Spaces\n. . . . . . . . . . . . . .\n53\n8\nDiscussion\n56\n8.1\nTarget Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n8.2\nRandomized Workloads\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n8.3\nBaselines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n8.4\nBenchmark Limitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n8.5\nFuture Work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n9\nConclusion\n63\nAppendices\n68\nAppendix A Experimental Details for Section 2\n68\nA.1 Learning Rate Schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.1.1\nWarmup Cosine Decay . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.1.2\nWarmup Linear Decay Constant\n. . . . . . . . . . . . . . . . . . . .\n69\nA.2 Details for Training Curves that Cross . . . . . . . . . . . . . . . . . . . . .\n69\nA.3 Details for Sensitivity of Optimizer Ranking to the Model Architecture . . .\n70\n3\nAlgoPerf Authors\nA.3.1\nWide ResNet with Stride Changes . . . . . . . . . . . . . . . . . .\n70\nA.3.2\nArchitectural Modifications of Transformer Models . . . . . . . . . .\n70\nA.4 Details for Comparing Instances of Training Algorithms . . . . . . . . . . .\n70\nA.4.1\nTraining Algorithms with Different Hyperparameters . . . . . . . . .\n70\nA.4.2\nTraining Algorithms with Different Hyperparameter Search Spaces .\n74\nA.4.3\nTraining Algorithms with Different Tuning Goals . . . . . . . . . . .\n74\nAppendix B Details for Target-Setting Experiments (Section 5)\n74\nAppendix C Details for Baseline Experiments (Section 7)\n77\nAppendix D Workload Details\n77\nD.1 Criteo 1TB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\nD.1.1\nDLRMsmall Model . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\nD.1.2\nCriteo 1TB DLRMsmall Workload Variants . . . . . . . . . . . .\n82\nD.2 fastMRI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\nD.2.1\nU-Net Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\nD.2.2\nfastMRI U-Net Workload Variants . . . . . . . . . . . . . . . . . .\n84\nD.3 ImageNet\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nD.3.1\nResNet-50 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nD.3.2\nImageNet ResNet-50 Workload Variants . . . . . . . . . . . . . .\n85\nD.3.3\nVision Transformer Model\n. . . . . . . . . . . . . . . . . . . . .\n85\nD.3.4\nImageNet Vision Transformer Workload Variants . . . . . . . .\n86\nD.4 LibriSpeech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\nD.4.1\nConformer Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\nD.4.2\nLibriSpeech Conformer Workload Variants . . . . . . . . . . . .\n87\nD.4.3\nDeepSpeech Model . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\nD.4.4\nLibriSpeech DeepSpeech Workload Variants . . . . . . . . . . . .\n88\nD.5 OGBG\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\nD.5.1\nGNN Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nD.5.2\nOGBG GNN Workload Variants . . . . . . . . . . . . . . . . . . . .\n89\nD.6 WMT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nD.6.1\nTransformer Model . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nD.6.2\nWMT Transformer Workload Variants . . . . . . . . . . . . . . .\n91\nAppendix E Preliminary Experiments for Randomized Workloads\n91\nReferences\n94\n4\nBenchmarking Neural Network Training Algorithms\n1. Introduction\nAlthough artificial neural networks are extremely useful models, training them remains quite\nexpensive. Moreover, investing more time and computational resources in training produces\nbetter, more accurate models. For example, training larger models, training longer, training\non more data, or performing more exploratory experiments can all improve results. Training\nmore efficiently would directly reduce costs and/or indirectly produce more accurate models.\nAlthough there are many ways to make training more efficient, in this work, we restrict our\nattention to improvements in training algorithms.\nUnfortunately, as a community, we are currently unable to identify which existing train-\ning algorithms are best, let alone understand what novel methods are the most promising.\nA multitude of training algorithms have been proposed, and more are proposed every year.\nSchmidt et al. (2021) lists well over a hundred methods, mostly published in the last seven\nyears. Naturally, each of these papers claims that their algorithm has significant benefits,\nbut the vast majority of the deep learning community never uses any of these techniques. At\npresent, most training algorithms are assumed to not be useful until they have been widely\nadopted, creating a chicken-and-egg problem. This state of affairs should be troubling for\nanyone trying to train neural networks or develop new training algorithms.\nTo provide actionable guidance to practitioners, we need to understand how existing\nand future methods perform in practice. Additionally, the community needs to incentivize\nresearch that makes actual progress on training algorithms.\nCurrently, the paper that\nintroduces a new technique is also the primary source of experimental evidence for its\nmerits, which causes a conflict of interest if the researchers proposing the techniques also\nhave control over the baselines included for comparison.\nAlthough comparative studies and meta-analyses are effective ways to assess the merits\nof competing techniques in principle, so far, they have not been able to provide definitive\nanswers. One reason may be that such studies are themselves often subject to criticism and\naccusations of bias: For any concrete empirical comparison, the proponents of any particular\ntechnique can always find fault with the details of the setup. Without broad agreement on\ndelicate issues such as hyperparameter tuning, in many cases, such criticism may indeed\nbe justified. Another issue is that the studies\u2019 retrospective nature shifts too much of the\nburden of proof away from the original inventors of training algorithms. Although a useful\ntool, empirical studies ultimately face an uphill battle to generate strong enough evidence\nto actually convince practitioners, especially since the truth can be quite complicated.\nInstead of relying entirely on retrospective comparative studies, why not have researchers\nsubmit their methods to a common competitive evaluation? Such competitive benchmarks\nhave historically worked well. For example, the deep learning community tried for years to\nconvince the computer vision community that neural networks were better models for image\nclassification by publishing papers, but deep learning was only widely adopted for computer\nvision tasks after neural networks showed success in high-profile competitive benchmarks\n(e.g., Russakovsky et al., 2015). Similarly, if we can construct benchmarks that capture use-\nful notions of training speed in sufficiently realistic conditions, we could determine whether\nwidely-used training algorithms are indeed the best currently available. If not, we could\ndemonstrate that current incumbent methods should be replaced with better alternatives.\n5\nAlgoPerf Authors\nThe mission of the MLCommons\u00ae1 Algorithms Working Group is to create a set of\nrigorous and relevant benchmarks to measure neural network training speedups due to algo-\nrithmic improvements. The best benchmarks will capture what is needed to drive progress\nat a particular time in the field and continue evolving along with the needs of the community.\nThis paper describes the working group\u2019s first attempt to benchmark training algorithms\nfor neural networks, and we intend to continue releasing improved versions periodically.\nWe hope that anyone interested in improving upon the benchmark we propose here will\nconsider joining the working group to help shape future versions.\n1.1 Contributions of This Work\n1. We precisely articulate the challenges of benchmarking training algorithms, provide\nconcrete experiments demonstrating the methodological issues we identify, and explain\nhow they hold back training algorithms research (Section 2).\n2. We introduce the AlgoPerf: Training Algorithms benchmark \u2014 a competi-\ntive, time-to-result benchmark on multiple workloads running on fixed hardware for\nsystematically comparing training algorithms (Section 4).\n(a) Our benchmark defines a complete and workable procedure for setting (validation\nand test error) targets and measuring training time to reach them. Furthermore,\nthis procedure produces targets that are reasonably competitive with results in\nthe literature, given the resource constraints of the benchmark (Section 5).\n(b) Our benchmark incentivizes generally useful training algorithms by computing\na joint score across all workloads and by including randomized workloads to\nsimulate novel problems.\n(c) Our benchmark requires submissions to explicitly account for the hyperparam-\neter tuning they need to achieve their results across workloads, giving submis-\nsions that are easier to tune an advantage.\nSubmissions can either compete\nusing limited, parallel tuning resources, or enter as a completely self-tuning and\nhyperparameter-free algorithm.\n(d) We specify a set of benchmark workloads covering image classification, speech\nrecognition, machine translation, MRI reconstruction, click-through rate predic-\ntion, and chemical property prediction tasks, and we design workload variants\nto challenge training algorithms to be more robust to natural workload modifi-\ncations.\n(e) We provide open-source JAX and PyTorch implementations of all workloads,\nand a training algorithm API that supports submissions in both frameworks. By\nproviding code along with details of the benchmark system, we make it easy to\nindependently reproduce results on the benchmark.\n3. We construct baselines by defining search spaces for eight popular optimizers (AdamW,\nNadamW, Heavy Ball, Nesterov, LAMB, Adafactor, SAM(w. Adam), Dis-\n1. MLCommons\u00ae and MLPerf\u2122 are registered and unregistered trademarks, respectively, of the ML-\nCommons Association in the United States and other countries. All rights reserved. Unauthorized use\nstrictly prohibited.\n6\nBenchmarking Neural Network Training Algorithms\ntributed Shampoo) that includes both popular optimizers that represent current\npractice and methods that have received attention in the recent literature (Section 7).\n(a) Collectively, these baselines demonstrate that our benchmark is feasible and that\nthere is a non-trivial gap between different methods.\n(b) We show that our benchmark score successfully favors algorithms that are easier\nto tune and that successful training algorithms require good search spaces that\nare tailored to the tuning budget. Specifically, baselines using adaptive methods\n(AdamW and NadamW) score more highly than baselines using non-adaptive\nmethods (Heavy Ball and Nesterov), in large part because of the difficulty\nof constructing good search spaces for the latter.\n(c) We set a provisional state of the art on the benchmark under our external tuning\nruleset based on extensive experiments including tens of thousands of tuning\ntrials. NadamW with our search space performs well on every (fixed) workload.\n4. We construct a set of 24 workload variants (three for each fixed workload) that make\nit possible to detect improvements in robustness over current popular methods, and\ndemonstrate the need for additional research on hyperparameter transfer (Section 6).\nHow to read this paper.\nThis paper serves a dual purpose: it acts both as a research\nreport on benchmarking neural network training algorithms, and as a form of technical doc-\numentation of the AlgoPerf: Training Algorithms benchmark and its rules. Readers\nwho plan to participate in the competition may wish to start by reading the sections that\ndescribe the rules (Section 4), using them as a higher-level companion to the latest version\nof the complete rules that can be found online, and then move to the experiments performed\non this benchmark (Sections 5 to 7). Readers who are primarily interested in the research\naspects of the paper may wish to skip, or only briefly skim, the rules section (Section 4)\nduring an initial read, with the exception of Section 4.5 which is essential to understanding\nthe scores reported in Section 7.\n1.2 Training Algorithm Benchmark Goals and Scope\nIn order to construct a benchmark for general neural network training algorithms that\nmeasures training speed, we need to decide what we mean by a general training algorithm,\nand how we measure training speed.\nDesign goal.\nWe aim to encourage general-purpose training algorithms that are easy to\napply across different data modalities and model architectures. It is legitimate to wonder\nwhat the best training algorithm is for a particular workload, e.g. \u201cResNet-50 on Ima-\ngeNet,\u201d or even a single workload family, e.g. \u201cconvolutional neural net image classifiers.\u201d\nBut given the still rapid rate of change among architectures in the field, it is more promising\nfor the community to develop relatively general methods first. A downside of this design\nchoice is that it implicitly assumes a clean separation between model and algorithm.\nSeparating \u201calgorithm\u201d and \u201cmodel\u201d.\nWe adopt the paradigm of machine learning\nas optimization; i.e., we view the optimizer and the model as two separate modules and\nfocus on the first.\nNevertheless, we must consider more pieces of the training pipeline\n7\nAlgoPerf Authors\nthan just the optimizer because the separation can be subtle. For example, should batch\nnormalization (Ioffe and Szegedy, 2015) be considered an aspect of the training algorithm\nor the modeling? An ideal benchmark should allow competitors a maximum of innovation\nand creativity while still providing enough constraints to make fair comparisons and glean\nuseful insights from the results. Section 4.2 explains how we delineate what constitutes a\ntraining algorithm and, thus, the design domain of competition entries. In general, we aim\nto offer flexibility while enforcing generality: We allow competitors to choose optimization\nalgorithms, update rules, and control regularization. We also allow algorithms that reorder\nor re-sample the training data. But we restrict or prohibit methods that only apply to\nspecific model architectures or data modalities.\nMeasuring training performance.\nAlthough this work, like the majority of the com-\nmunity, uses the metaphor of optimization, optimization and learning are, of course, not\ntechnically the same. We ultimately care about how quickly a method reaches a satisfactory\nout-of-sample error, as estimated by a validation or test set error rate. To incentivize prac-\ntically useful techniques, success in our competition must be based on this end goal, not on\nhow quickly an algorithm can actually minimize the training objective. Unfortunately, this\nforces us to confront the complexities of regularization and overfitting in the benchmark\n(which are also present, of course, in any real application). Section 4.1 details how we define\n\u201ctime to result\u201d. In short, we define a target out-of-sample error rate for every workload\nand score submissions based on how quickly they reach these targets.\nInternal and external hyperparameter tuning.\nComparing generally-applicable neu-\nral network training algorithms requires careful attention to the rules around hyperparam-\neter tuning for benchmark submissions. In particular, should certain types of tuning be\nviewed as integral parts of the training algorithm or external to the training algorithm?\nAt the moment, choosing hyperparameters is still largely the job of the user, so algorithms\nshould perform well under an external tuning schedule. However, methods that do not\nrequire external tuning at all are, of course, desirable, and internal parameter tuning is\narguably among the biggest opportunities for improvement in resource use. So we also need\nan opportunity for methods that effectively tune themselves to shine. Section 4.4 describes\nour approach to hyperparameter tuning in detail.\n2. The Challenges of Empirical Comparisons of Training Algorithms\nAn ideal empirical comparison of training algorithms should be (as much as possible) con-\nvincing, informative, and practically relevant.\nA convincing comparison generates high\nquality empirical evidence, makes \u201cfair\u201d comparisons to strong baselines, and is not mis-\nleading. An informative comparison disentangles the causes of any measured improvements\nand provides insight into the observed phenomena. A practically relevant comparison mea-\nsures situations that are likely to arise in important applications and studies conditions\nas similar as possible to current practice in applied work.\nUnfortunately, several basic\nchallenges make achieving these desiderata far from simple.\n8\nBenchmarking Neural Network Training Algorithms\n2.1 Precisely Defining and Measuring Training Speed\nCurrently, papers proposing training algorithms for deep neural networks tend to shy away\nfrom making quantitative, empirical claims, although some will make theoretical conver-\ngence rate claims that depend on assumptions that preclude them from being directly\napplicable to neural network training (e.g., assuming a convex loss function). For example,\nin abstracts we see phrases such as \u201cfrequently delivers faster convergence\u201d (Lucas et al.,\n2019), \u201coutperforms other methods with fast convergence and high accuracy\u201d (Zhuang\net al., 2020), or \u201cworks well in practice and compares favorably to other stochastic opti-\nmization methods\u201d (Kingma and Ba, 2015). In contrast, deep learning modeling papers\nusually make precise quantitative claims. For example, in abstracts we see language such\nas \u201cachieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving\nover the existing best results\u201d (Vaswani et al., 2017), \u201cachieves state-of-the-art 84.3 % top-1\naccuracy on ImageNet, while being 8.4\u02c6 smaller and 6.1\u02c6 faster on inference\u201d (Tan and\nLe, 2019), or \u201cachieves 86.2 % top-1 ImageNet accuracy, while being 4.7\u02c6 faster\u201d (Bello\net al., 2021).2 Some examples of recent optimizer papers that make quantitative claims are\nChen et al. (2023), Xie et al. (2022), and Tian and Parikh (2022), but these claims are still\nusing different metrics or are referring to different benchmarks or workload versions.\nTo the detriment of progress on training algorithms, a lack of quantitative claims means\nthere is no clear notion of the \u201cstate of the art\u201d, only what is popular or topical. Most\ntraining algorithm papers will display the training curve (loss vs time) for their method and\nsome baseline, but without a shared understanding of how such curves should be converted\ninto quantitative measurements of training speed, they will only provide an illusory sense\nof precision. Even valiant attempts to make quantitative sense of such a plot will produce\nclaims tortured with caveats. For example, Tian and Parikh (2022) claims their method\u2019s\nloss is \u201calways significantly lower beyond [the first] 30 % of the training procedure.\u201d\nThe root cause of the preponderance of vague training speed claims in the literature\nis that a direct comparison of two training curves is ill-posed. Certainly, if one curve is\nstrictly below the other, we can say it is \u201cbetter\u201d, but even then, it is not clear by how\nmuch. Real-world training curves are noisy and tend to cross each other, often multiple\ntimes. Figure 1 (left) shows two sample validation error curves for ResNet-50 trained on\nImageNet, achieving a final validation error of 24.0 % (\n) and 24.4 % (\n), respectively\n(see Appendix A.2 for exact experiment details). These curves intersect multiple times,\neven after ignoring the early part of training. Even more importantly, the curves of the\nbest validation error seen so far (Figure 1, right) also intersect multiple times, showing that\nwhich run is leading the race swaps back and forth. The run that trains ResNet-50 the\nfastest depends on what it means for training to be complete.\nConsequently, to measure the training time, we need a clear criterion that indicates when\ntraining is complete. There can be many possible criteria, and it is not obvious which is the\ncorrect one. As a result, many papers avoid the issue altogether, relying instead on more\nvague descriptions of training results. Straight-forward approaches, such as computing the\narea under the validation loss curve, fail to capture what is relevant in practical applications.\nFor example, improvements that affect only the initial stage of training are irrelevant in\n2. Ironically, Bello et al. (2021) showed that the gains observed in Tan and Le (2019) were largely due to\nan improved training procedure, even though Tan and Le (2019) was presented as a modeling paper.\n9\nAlgoPerf Authors\n20,000\n50,000\n80,000\n110,000\n140,000\nStep\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nValidation Error\n20,000\n50,000\n80,000\n110,000\n140,000\nStep\nFigure 1: A direct comparison of training curves is ill-posed if they intersect. Left:\nThe validation error for two different runs (\n,\n) of Adam on ResNet-50 on ImageNet.\nRight: The best validation error obtained so far by each curve (i.e. the running minimum\nof the left subplot). Even after removing the early part of training and looking at the best\nresult so far (right), the curves intersect multiple times (highlighted by red crosses (\u2716) at\nthe bottom of each plot). Through much of the plot, the orange curve is winning, but the\nblue curve overtakes it near the end of the runs.\nmost cases. Despite these difficulties, a precise definition of training speed is essential. If\nthe community can standardize how training time should be measured, we can shift the\nliterature towards clear quantitative comparisons between training algorithms.\nThe simplest approach is to set a target error rate and measure how long it takes\nto first achieve this target.\nSpecifically, we propose measuring training time based on\nhow long it takes a run to reach a workload-specific, out-of-sample error target that is\nselected to be a competitive, near-state-of-the-art error rate. For any given comparison\nbetween algorithms, standardizing hardware is also a prerequisite for measuring wall-clock\ntime, although perhaps it would be sufficient to standardize a cost model for primitive\noperations. Determining an appropriate target error rate for a particular workload is not\ntrivial. Section 4.1 describes our approach to setting error rate targets. Targets for a given\nworkload may need to be revised as new results emerge, but they should be more stable\nthan the state-of-the-art result on the corresponding dataset since a workload freezes a\nspecific model as well. Ultimately, targets will always be somewhat arbitrary, but that is\nall the more reason they should be standardized before comparing training algorithms. Our\ngeneral philosophy is to try and pick the best error rates we can consistently and reliably\nachieve with standard methods.\n2.2 Dependence on the Workload\nThe performance of a training algorithm is necessarily a function of the workload it is timed\non. Ultimately, in applications, we are interested in finding the method best suited for the\nparticular workload in front of us. In general, this will be a novel workload without many\nprevious results. The more we can generalize from previous results on related workloads,\nthe better we can prioritize what to try. For generic methods research, on the other hand,\nwe are interested in characterizing how methods perform across a diverse set of workloads\nrepresentative of current important applications. Once again, we cannot measure every\n10\nBenchmarking Neural Network Training Algorithms\npossible method even on a single workload, let alone on all possible workloads, so we are\nforced to extrapolate. When building generic methods, we need to detect when a method\nprovides a generally useful improvement and thus we also need to find a meaningful way to\naggregate performance across multiple workloads.\n2.2.1 Sensitivity of Optimizer Ranking to the Model Architecture\nAlthough it is obvious that the model and dataset affect how well different algorithms\nperform, the situation is worse than it might initially appear because even seemingly small\ndetails about the workload can have a large effect on our results. As a recent example, Nado\net al. (2021) describes several small implementation details that make a large difference in\nmatching state of the art MLPerf\u2122 ResNet-50 ImageNet training speed results. For\nexample, they mention needing to set the initial value of the batch normalization scale\nparameters in the final layer of each residual block to a value less than one, match the\nexact v1.5 version of ResNet-50, match the virtual batch size of batch normalization, and\n(perhaps least surprisingly) avoid applying L2 regularization to bias variables.\nTo further illustrate how brittle training results can be, we ran three different experi-\nments, each making a minor change to a different workload: changing the stride in the final\nresidual block of a Wide ResNet on CIFAR-10, adding extra batch normalization layers\nto ResNets, and switching between two different published Transformer models that\nonly differ by the placement of the layer normalization block.\nWide ResNet with stride changes\nIn the first experiment, we start with a standard\nWide ResNet (Zagoruyko and Komodakis, 2016) architecture on CIFAR-10. For this\nworkload, we compare the Nesterov optimizer with AdamW on a simple learning rate\nsweep with cosine decay (full experimental details provided in Appendix A.3.1).\nAfter\nselecting the best learning rate from the sweep, Nesterov achieves a better test error\nthan AdamW, see Table 1 and Figure 2. The superiority of Nesterov in this experiment\nis largely due to AdamW overfitting in these conditions; i.e., AdamW achieves a better\ntraining loss than Nesterov. However, if we change the convolutional strides in the final\nresidual block from the standard 2\u02c62 stride to a 1\u02c61 stride, the performance of Nesterov\nshows a large 3.5% increase in test error. AdamW, on the other hand, is largely unaffected\nby this architectural change. Nesterov struggles to train the 1\u02c61 stride model due to\nan early training instability caused by the large loss curvature at initialization\u2014the largest\neigenvalue of the loss Hessian changes from 32 to 1052 with this architectural modification\nand initial weight distribution. This kind of instability can be dealt with using learning rate\nwarmup (Gilmer et al., 2021). Adding 1000 steps of linear warmup makes Nesterov stable\nat large learning rates despite the high initial loss curvature (see Figure 8 in the Appendix).\nAdditional batch normalization layers\nA similar inversion of rank ordering between\nNesterov and AdamW occurs when we add in additional batch normalization (Ioffe and\nSzegedy, 2015) layers to the 200-layer ResNetV2 architecture (He et al., 2016b). This batch\nnormalization layer is added after every residual connection, similar to the structure of the\noriginal Post-Layer Norm Transformer (Vaswani et al., 2017).\nThis architectural\nchange actually benefits Nesterov with the 50 layer model, offering slight validation error\nand stability improvements. However, when this ResNet with extra batch normalization\n11\nAlgoPerf Authors\nTraining Algorithm\nStride2 \u02c6 2\n2 \u02c6 2\n2 \u02c6 2\nStride1 \u02c6 1\n1 \u02c6 1\n1 \u02c6 1\n(standard)\nNesterov\n0.0376\n0.0726\nAdamW\n0.0407\n0.0420\nNesterov + Warmup\n0.0380\n0.0378\nTable 1: Performance of different training algorithms on Wide ResNet with stride\nchanges. After changing from the standard 2 \u02c6 2 stride to a 1 \u02c6 1 stride, the performance\nof Nesterov drops significantly, while AdamW is largely unaffected by this architectural\nchange. Adding a learning rate warmup to Nesterov (i.e. Nesterov + Warmup) allows\nit to recapture its original performance.\n10\u22124\n10\u22123\n10\u22122\nBase Learning Rate\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nTest Error Rate\nAdamW\nStride 2x2\nStride 1x1\n10\u22123\n10\u22122\n10\u22121\n100\nBase Learning Rate\nNesterov\nStride 2x2\nStride 1x1\nStride 1x1 w/ LR Warmup\nFigure 2: Small architectural changes can result in different rankings between\ntraining algorithms. In the above plot, we compare a learning rate sweep with AdamW\n(right) and Nesterov (left) on two variants of the CIFAR-10 Wide ResNet. The first\nvariant is the standard architecture, which uses a 2x2 convolutional stride in each residual\nblock, labeled \u201cStride 2x2\u201d (\n). The second variant changes the strides in the final residual\nblock to 1x1, labeled \u201cStride 1x1\u201d (\n). On the standard architecture, Nesterov outper-\nforms AdamW (the best-performing hyperparameter setting per architecture is highlighted\nwith a colored star, e.g. (\n)). However, the stride change results in significant training\ninstability at high learning rates with Nesterov (highlighted with a black cross (\u2715)). This\ninstability causes Nesterov to under perform AdamW. If a learning rate warmup is ap-\nplied (\n) then we recover the performance of the original model. AdamW on the other\nhand is unaffected by the training instability caused by this architectural change.\nlayers is scaled to 200 layers, it becomes completely untrainable with Nesterov at any\nlearning rate in the sweep (see Table 2). AdamW is also affected by this architectural\nchange, but is able to train successfully with a 3% drop in final performance. Similar to the\nWide ResNet stride change, this instability results from a dramatic increase in the initial\nloss curvature\u2014with the extra batch normalization layers the initial loss curvature exceeds\n1010! Prepending a linear learning rate warmup in this case is insufficient to resolve the\n12\nBenchmarking Neural Network Training Algorithms\ntraining instability. However, with the addition of gradient clipping, we are able to recover\nthe performance of the original architecture.\nTraining Algorithm\nResNet-200\nResNet-200\n(standard)\nExtra-BN\nNesterov\n0.2090\nNo Feasible Trials\nAdamW\n0.2626\n0.2722\nNesterov + Grad Clip\n0.2091\n0.2094\nTable 2: Performance of different training algorithms after adding an extra batch\nnormalization layer to a ResNet-200.\nWhile AdamW is only slightly affected by\nadding extra batch normalization layers, vanilla Nesterov becomes untrainable. Adding\ngradient clipping (i.e. Nesterov + Grad Clip), however, allows it to recover the original\narchitecture\u2019s performance.\nArchitectural modifications to Transformer models\nThe Transformer model is\nthe most widely used architecture for natural language processing tasks. There are two\npopular versions of this model.\nFirst, the original version of the model from Vaswani\net al. (2017), the Post-Layer Norm (Post-LN) Transformer, which places the layer\nnormalization between the residual blocks.\nSecond, the Pre-Layer Norm (Pre-LN)\nTransformer (Xiong et al., 2020), which places the layer normalization inside the residual\nblocks. Figure 3 shows the differences between the two architectures.\nMulti-Head \nAttention\nLayer Norm\nxl\nxl+1\nFFN\naddition\n(a) P\uf76f\uf773\uf774-LN\n(b) P\uf772\uf765-LN\nLayer Norm\naddition\nMulti-Head \nAttention\nLayer Norm\nxl\nxl+1\nFFN\naddition\nLayer Norm\naddition\nFigure 3: Comparison between (a) Post-Layer Norm Transformer layer (Post-\nLN) and (b) Pre-Layer Norm Transformer layer (Pre-LN). Figure recreated from\nXiong et al. (2020) with permission.\nThis architectural modification affects training algorithms differently (Table 3). The\nexperiment to produce Table 3 was conducted as follows: we designed a customized search\nspace for each optimizer and ran 100 trials using random samples from this search space\nfor each of the architectural modifications. Each of the trials were run for a fixed number\nof steps (in this case 133K steps). The best BLEU score from each trial was chosen for the\n13\nAlgoPerf Authors\nTraining\nPre-LN\nPost-LN\nDifference\nAlgorithm\nBest\nConfidence interval\nBest\nConfidence interval\nAdamW\n31.3306\n31.0386 \u02d8 0.2811\n30.8098\n29.6819 \u02d8 1.1374\n0.5208\nNadamW\n31.3381\n31.1153 \u02d8 0.2681\n30.8894\n29.6122 \u02d8 1.2608\n0.4487\nNesterov\n29.2951\n28.9091 \u02d8 0.8864\n26.3211\n20.0433 \u02d8 7.0693\n2.9740\nShampoo\n30.2051\n29.9168 \u02d8 0.2816\n29.9482\n29.0940 \u02d8 1.1800\n0.2569\nTable 3: Test set BLEU score for Pre-Layer Norm (Pre-LN) and Post-Layer\nNorm (Post-LN) architectures for different training algorithms.\nThe architec-\ntural modification of Pre-LN vs. Post-LN affects AdamW, NadamW, Nesterov, and\nShampoo very differently as highlighted by the performance difference between the best-\nperforming trials for Pre-LN and Post-LN. Table 16 in the Appendix shows the analogous\ntable with the cross-entropy loss instead of the BLEU score.\nanalysis. We performed Bootstrap analysis (Efron and Tibshirani, 1993) on this data using\na bootstrap sample size of 20 (corresponding to a plausible submission) and calculated the\nmean and standard deviation of the resulting distribution.\n2.2.2 Implications of Workload Sensitivity\nAs we have seen above, seemingly minor changes to the training pipeline, such as model ar-\nchitecture tweaks or the exact placement of normalization layers, can have a large effect on\nthe results different training algorithms achieve. As a consequence, empirical comparisons\ncan fall into several failure modes and fail to achieve our desiderata of being convincing,\ninformative, and practically relevant. First, it is very tempting to claim that a particular\nmethod provides a general improvement even though it is only useful on a very specific\nworkload. Second, comparisons to previously published results can easily fail to exactly\nmatch the training pipeline details and end up with better or worse results for reasons\nunrelated to the methods being evaluated. Achieving a true apples-to-apples comparison\nusually requires reproducing earlier results in the same codebase and matching other de-\ntails of the implementation and setup, an extremely labor-intensive prospect. Sometimes\nthis issue might manifest as improvements improperly attributed to the training algorithm.\nThird, common benchmark workloads may not be very representative of the applications\nthat are most important to the community, especially since the importance of various ap-\nplications naturally waxes and wanes. Popular benchmark tasks also suffer from a selection\nbias that leaves us with unusually well-behaved training problems (e.g., unusually stable)\nor with training problems that co-evolved with the training algorithms used by whoever\ncreated them. For example, it shouldn\u2019t surprise us that Adam works especially well for\nmodel architectures built to train using Adam.\nOur benchmark takes several steps to handle workload sensitivity. First and foremost,\nwe fix a set of workloads (see Section 4.3) and prevent pipeline changes that are not part of\nthe training algorithm (see Section 4.2). By separating changes to the training algorithm\nand other parts of the pipeline, we can avoid the case where training speedups are incorrectly\nattributed, at the cost of having to occasionally split philosophical hairs on what parts of\n14\nBenchmarking Neural Network Training Algorithms\nthe training pipeline are off limits for submissions to alter. We also try to include a set of\nworkloads that is relatively diverse, while still being representative of the most popular deep\nlearning applications. However, workload diversity and relevance could always be better.\nAlso, adding workloads comes at a cost both in engineering labor and in terms of our ability\nto run comprehensive experiments, sharply limiting what was feasible for the first version\nof our benchmark. Long term, we will need an even larger community effort to standardize\nworkloads for training algorithm research and contribute measurements of new methods\nin a standardized way. Workloads should be implemented in open source code (ideally in\nmultiple frameworks) and any variations or deviations should come with new names, in\norder to avoid confusion and misleading comparisons.\n2.3 We Cannot Compare Families of Training Algorithms, Only Specific\nInstances\nMost training algorithms in deep learning are not actually procedures we can run so much as\nalgorithm templates we can instantiate with particular hyperparameters. Unfortunately, we\ncannot eliminate these hyperparameters because they exist for a reason: using different hy-\nperparameter values for different workloads can lead to much better results (Section 2.3.1).\nTo compare training algorithms with free hyperparameters, we must either set the hyper-\nparameters to specific values or choose a specific procedure to tune them. If we choose a\ntuning protocol, we should view the protocol, including the hyperparameter search space,\nas part of the training algorithm definition, since the same algorithm template combined\nwith a different hyperparameter tuning protocol (or even just a different search space Sec-\ntion 2.3.2) can produce significantly different results. For a fair comparison, it is necessary\nto ensure that all training algorithms are tuned for the same tuning goal (Section 2.3.3).\n2.3.1 Training Algorithms With Different Hyperparameters\nTo illustrate the issues with using a single hyperparameter setting for all workloads, we fixed\na training algorithm and studied the impact of per-workload hyperparameter tuning on the\nvalidation performance after a fixed number of training steps. Assuming the training algo-\nrithm definition specifies a hyperparameter search space H, we can use quasirandom search\nto sample a finite set H \u010eH of candidate hyperparameters. Let valpw, hq be the validation\nmetric value achieved on workload w by running the training algorithm with hyperparam-\neter setting h for a fixed number of steps. Furthermore, let valHpwq\u201cminhPH valpw, hq be\nthe best validation score achieved among all hyperparameter settings hPH.3\nSuppose now that we used the same hyperparameter setting h for every workload instead\nof tuning the hyperparameters separately for each workload. Then we can consider the\nfollowing measure of the worst-case relative performance degradation over all workloads\n\u03c6ph, Hq \u201c max\nw\n\u02c7\u02c7\u02c7\u02c7\nvalpw, hq \u00b4 valHpwq\nvalHpwq\n\u02c7\u02c7\u02c7\u02c7 .\n3. To simplify the notation, we assume that the validation metric should be minimized, e.g., as is the case\nfor an error rate. For cases were a higher validation metric is better, e.g. the BLEU score, we would\ninstead simply define valHpwq \u201c maxhPH valpw, hq.\n15\nAlgoPerf Authors\nNote that if valpw, hq \u201c valHpwq for all workloads (i.e., if the same h achieves the best\nvalidation score across all workloads), then \u03c6ph, Hq will be zero. Generally the higher the\nvalue of \u03c6ph, Hq, the larger the relative performance gap between employing h versus tuning\nover H. We can now find the hP H with the lowest such degradation,i.e.\n\u03a6pHq \u201c min\nhPH p\u03c6ph, Hqq \u201c min\nhPH\n\u02c6\nmax\nw\n\u02c7\u02c7\u02c7\u02c7\nvalpw, hq \u00b4 valHpwq\nvalHpwq\n\u02c7\u02c7\u02c7\u02c7\n\u02d9\n.\n\u03a6pHq denotes the best worst-case relative performance degradation achieved among the\nhyperparameters hPH. For a set of hyperparameters H, the quantity \u03a6pHq thus measures\nhow much validation performance degrades from using any single setting of the hyperpa-\nrameters in H that is shared across all workloads instead of tuning within H to find the best\nper-workload setting. Thus, we can view it as a measure of how much the training algorithm\nbenefits from per-workload hyperparameter tuning on a particular set of workloads, tuning\nover a particular set of hyperparameter search points H. By definition, \u03a6pHq\u011b0 and if it\nis zero then there is a hyperparameter setting h\u2039 PH that achieves the best performance on\nevery workload among all settings in H. A larger value of \u03a6pHq suggests that the training\nalgorithm benefits more from workload-specific hyperparameter tuning.\nWe computed \u03a6pHq for four training algorithms on eight workloads (for the details\nof these workloads, see Table 7 and Appendix D). We constructed H by sampling 100\nhyperparameter settings with quasirandom search from the tuning search spaces defined\nin Table 8, for each training algorithm. Table 4 shows the \u03a6-values achieved by different\nmethods using these specific search spaces.\nNote that \u03a6pHq is a random variable that\ndepends on the random seeds in training and is a function of the set of hyperparameters\nH which were sampled from the search space H. In these experiments, we used a single\ntraining run per hyperparameter, i.e., we used one sample to estimate the mean of valpw, hq.\nThe results illustrate the performance gains that can be achieved by using workload-\nspecific tuning of hyperparameters. We see that for every algorithm the \u03a6-value is least\n0.169 which implies that for every hyperparameter setting there is a workload for which the\nperformance of that setting is at least 16.9% worse than the performance of the optimal\nhyperparameter setting for that particular workload.\nTables 17 to 20 in the appendix\nshow the performance of the optimal per-workload hyperparameters (i.e., valHpwq), the\nperformance of the optimal overall hyperparameters per workload (i.e., valpw, h\u02daq where\nh\u02da is the hyperparameter that minimizes \u03a6pHq), and their associated relative performance\ndegradation for each training algorithm.\n2.3.2 Training Algorithms With Different Hyperparameter Search Spaces\nDifferences in tuning protocols can wreak havoc on empirical work. Choi et al. (2019b)\nargue that they are the single most important factor explaining contradicting results from\nempirical optimizer comparisons. Learning rate schedules are an especially pernicious free\nparameter to tune because different algorithms result in different implicit schedules. Ide-\nally, these implicit schedules would be separated from the other properties of the algorithm\n(Agarwal et al., 2020). There is an entire literature on semi-automated tuning algorithms\nfor hyperparameter tuning that includes Bayesian optimization and other black-box op-\ntimization techniques. However, even if a particular Bayesian optimization tool became\n16\nBenchmarking Neural Network Training Algorithms\nTraining Algorithm\n\u03a6pHq\nAdamW\n0.195425\nNadamW\n0.169197\nNesterov\n0.230001\nHeavy Ball\n0.239372\nTable 4: Using workload-specific hyperparameters can significantly improve the\nperformance of training algorithms. The \u03a6-values obtained by different training al-\ngorithms with search spaces as defined in Table 8. Higher values indicate the necessity of\ntuning to achieve good performance across multiple workloads.\nstandard, such tools still require search spaces as input and any procedure for constructing\nsearch spaces would need to be aware of the tuning budget (Ariafar et al., 2022).\nEven if we fix everything about the tuning protocol except the search space, changes to\nthe search space alone suffice to change training results. To illustrate this, we defined two\nsearch spaces for AdamW (Table 5) and compared the best hyperparameter points found\nfrom each search space for each workload in our benchmark. The first search space (AdamW\nNarrow) is completely contained within the second search space (AdamW Broad). In\nprinciple, with a large enough tuning budget, the broader search space can be no worse than\nthe narrower one. However, at any particular tuning budget, either search space might end\nup performing better. Both search spaces seem reasonable a priori. Indeed, they both\ncontain good points, although without the benefit of hindsight one might be concerned that\nthe narrow search space does not allow small enough values of weight decay.\nSearch Space\nLearning Rate\nWeight Decay\n1 \u00b4 \u03b21\n\u03b22\nAdamW Narrow\n[2e-4, 5e-3]\n[2e-2, 0.5]\n0.1\n0.999\nAdamW Broad\n[5e-6, 2e-2]\n[5e-6, 2.0]\n[1e-3, 1.0]\n0.999\nTable 5:\nHyperparameter search spaces for two training algorithms using\nAdamW.\nAll hyperparameters are sampled using a log-uniform distribution with the\nlower and upper bounds as shown in the table.\nFrom each of the search spaces, we sampled 100 points using quasirandom search. Using\nthese 100 points, we simulated tuning with a budget of T trials by repeatedly sampling\ngroups of T trials, with replacement, from the 100 real results and taking the best trial\nwithin the group based on its performance on the validation set. Table 6 shows the results\nof simulated tuning with a budget of 20 trials for the two search spaces. The validation\nperformance is the median over 1000 simulations. At this budget, the narrow search space\nachieves markedly better validation metrics across all workloads in our benchmark.\nAlthough it might seem obvious that the results would depend on the search space, it is\nstill not standard within the literature to report exact tuning protocols or even supply search\nspace details. The narrow and broad search spaces we considered here could have instead\nhad less\u2014or even no\u2014overlap and produced even more dramatic differences in results. It is\nextremely easy to select a particular search space for tuning a baseline and then claim that\n17\nAlgoPerf Authors\nWorkload\nAdamW Narrow\nAdamW Broad\nMedian\nQ1\nQ3\nMedian\nQ1\nQ3\nCriteo 1TB\nDLRMsmall\n0.12401\n0.123967\n0.124025\n0.124087\n0.12396\n0.124025\nfastMRI\nU-Net\n0.734746\n0.734590\n0.734936\n0.734311\n0.734054\n0.734522\nImageNet\nResNet-50\n0.23256\n0.23094\n0.23330\n0.24334\n0.23904\n0.24708\nViT\n0.21992\n0.22118\n0.22118\n0.23616\n0.22694\n0.24038\nLibriSpeech\nConformer\n0.075989\n0.075962\n0.076817\n0.080673\n0.078963\n0.087340\nDeepSpeech\n0.112706\n0.112353\n0.113485\n0.120674\n0.116902\n0.127974\nOGBG\nGNN\n0.28214\n0.281595\n0.284034\n0.276307\n0.275642\n0.279285\nWMT\nTransformer\n31.3523\n31.2824\n31.3946\n30.9950\n30.9129\n31.1748\nTable 6: Performance across multiple workloads for AdamW with two different\nhyperparameter search spaces. Shown are the median, as well as the lower and upper\nquartiles (Q1 and Q3) of the best observed validation metric. The results are for a budget\nof T \u201c20 trials (see Table 21 for results with T \u201c5) across 1000 simulations. At this budget,\nAdamW Narrow performs significantly better across all test workloads.\nanother algorithm outperforms it without including appropriate caveats about the search\nspace.\nOur experiment here shows that even if we consider two very reasonable search\nspaces, merely neglecting to select a search space that matches the tuning budget could\nsubtly weaken a baseline and befuddle a comparison between training algorithms.\n2.3.3 Training Algorithms With Different Tuning Goals\nEven if we fix everything about the tuning procedure including the search space, differences\nin tuning goals can lead to unfair comparisons. Specifically, tuning to achieve the best\nvalidation error within a fixed training time budget is not the same as tuning to achieve a\nfixed validation error as fast as possible. Suppose some previously published result achieves\na particular validation error rate on a particular workload after training for a certain amount\nof time using Algorithm X. Now suppose that we demonstrate that Algorithm Y can achieve\nthe same validation error in substantially less time (on the same hardware). In many cases,\nthis kind of comparison, although seemingly innocuous, will be unfair to Algorithm X\nbecause the result with Algorithm X was from a paper that was not trying to minimize\ntraining time, but was instead engaged in an implicit competition to get the best possible\nvalidation error, within their available budget.\nWe tuned the hyperparameters for AdamW for ResNet-50 on ImageNet for two\ntraining step budgets, 186, 666 and 233, 333. Both studies use the same search space and,\nby using the same seed, the same 100 hyperparameter samples. They also used the same\ncosine decay learning rate schedule with a linear learning rate warmup. However, the cosine\ndecay schedule depends on the maximum number of training steps and therefore differs\nbetween the two studies (for the complete search space see Table 8, AdamW). In both\nstudies, the hyperparameter setting achieving the best validation error happened to be\nthe same (see Table 22 in Appendix A.4.3). We then retrained this best trial for both\nstudies using 20 different random seeds. Figure 4 shows the best validation error achieved\nso far versus the training step for Trial A (\n, step budget of 186, 666 steps), and Trial B\n18\nBenchmarking Neural Network Training Algorithms\n(\n, step budget of 233, 333 steps). Both trials achieve nearly identical validation errors\n(22.4 % \u00b4 23.0 % depending on the seed), but Trial A is clearly (by design) much faster.\nTraining longer tends to very slightly improve the validation error (median of 22.6 % vs a\nmedian of 22.7 %), but this improvement is hard to detect with the variance across seeds.\nFurthermore, if we compare larger and larger pairs of training step budgets, any difference\nbetween the budgets will be swamped by the variance across different runs. This experiment\nshows that we can get (roughly) the same validation error result faster, simply by setting a\nlower training step budget.\nResearchers trying to achieve the best headline validation error number to publish will\nnaturally give their experiments a generous step budget to make their lives easier, and\nnot try to find the minimum step budget that can still reproduce their result. Thus we\nmight imagine published results more like Trial B when they are being tuned for the best\nvalidation error using a somewhat arbitrary, but generous, step budget. In general, learning\nrate decay schedules are necessary for the very best results, and we should expect the best\nvalidation error rates to be achieved near the end of training when the learning rate has\nalready been reduced.\nHowever, common decay schedules make it extremely dangerous\nto assume that some state-of-the-art result, in terms of out-of-sample error, was achieved\nwith a near-minimal number of training steps. Unfortunately, that is precisely what we\nare implicitly assuming when we compare results tuned to minimize training time to reach\na particular error rate with results tuned to achieve the best possible error rate given a\nparticular time (or step) budget.\n25,000\n50,000\n75,000\n100,000\n125,000\n150,000\n175,000\n200,000\n225,000\nSteps\n0.25\n0.30\n0.35\n0.40\nValidation Error\nTrial A (Decay ending at 186,666 steps)\nTrial B (Decay ending at 233,333 steps)\nFigure 4: For a fair comparison, training curves need to be tuned for the same\ncriterion.\nTwo AdamW training runs (\n,\n) for ResNet-50 on ImageNet using\nhyperparameters tuned within the same search space, but using different step budgets. Since\nthe cosine learning rate decay schedule stretches with a longer step budget, we see \u201cslower\u201d\ntraining caused by the larger step budget (\n). For each of the hyperparameter settings, we\nran 20 different random seeds to create min/max error bounds around a median trajectory\n( ,\n). The dashed gray line (\n) denotes the best median validation error achieved by\nboth training runs. See Appendix A.4.3 for experimental details.\nIn light of the strong effect tuning protocol details will have on the results of any\nempirical comparison between training algorithms, we have no choice but to view the tuning\n19\nAlgoPerf Authors\nprotocol as part of the algorithm. Additionally, we can only compare two results that are\ntuned to optimize the same criterion, i.e., two results that are both tuned to minimize\ntime-to-result or are both tuned to minimize validation error at a fixed time budget. Once\nwe fold the tuning protocol into the definition of the method, we can study one algorithm\nwith two different tuning protocols as if it was two separate methods. Algorithm designers\ncould then also provide guidance on how the methods they introduce should be tuned.\nCurrently, new training algorithms rarely come with sufficient guidance on how they should\nbe tuned in various budget scenarios. Despite not needing much (or any) tuning being\na common selling point, we are not aware of any popular training algorithm for neural\nnetworks where this property has been precisely defined and convincingly demonstrated,\nalthough this would certainly be a valuable contribution.Unfortunately, the tuning protocol\nis necessarily a function of the tuning budget, which makes it difficult for algorithm designers\nto specify how their method is best tuned in all possible budget scenarios. Nevertheless,\neven providing a short list of budget scenarios along with tuning guidance would improve the\nsituation dramatically, especially since the largest tuning budget scenarios are probably less\ncritical to address for practitioners. Ultimately, methods that are easier to tune\u2014or even\nfully self-tuning\u2014would be extremely valuable if they could be supported by compelling\nexperimental evidence. Section 4.4 describes our approach to tuning in detail.\nEven if we view results as conditional on a tuning protocol, publication bias and other\nsimilar selection bias effects can, in effect, amount to implicit tuning. Ultimately, the root of\nthe problem is that quantifying tuning effort when developing a training algorithm is nearly\nimpossible. If we try 1000 different methods on a small set of workloads and find one that\nseems to work across this small set \u201cwithout much tuning\u201d, we might just have obfuscated\nthe tuning process. However, \u201coff the books\u201d implicit tuning should not bother us if the re-\nsulting training algorithms generalize to new workloads. Therefore, the solution to this issue\nis a combination of various generalization incentives and safeguards: preventing workload\ncherry-picking by standardizing sets of benchmark workloads, sharply limiting per-workload\nadaptation and tuning in empirical comparisons, using larger sets of more diverse bench-\nmark workloads while measuring performance aggregated across workloads, and perhaps\neven generating novel workload variations randomly to create \u201cheld-out\u201d workloads.\n2.4 Strong Baselines Are Far Too Hard to Obtain\nResearchers proposing new training algorithms have total control over the baselines they\ncompare with their algorithms. Even with everyone acting in good faith, it is very easy\nto accidentally make an experimental design choice that gives a new algorithm an unfair\nadvantage. For example, we could select a learning rate schedule family that works well\nwith a new technique and not even know how appropriate it is for our baselines.\nOr\nwe might select workloads where controlling overfitting is essential for good results, but\nnot apply sufficient regularization to a baseline that minimizes training loss faster than\nthe novel method we are studying. There are essentially limitless possible choices we can\nmake when designing experiments that might inadvertently hamstring our baselines and\nshow overoptimistic improvements for some novel method. Researchers just have far too\nmany choices to make when comparing training algorithms. Even worse, the least careful\n20\nBenchmarking Neural Network Training Algorithms\nexperiments not only take the least effort to conduct, but also tend to produce the most\nimpressive-seeming results.\nAlthough the problem of weak baselines exists in other parts of deep learning (and in\nmachine learning as a whole), training algorithm comparisons seem particularly fraught\ncompared to, say, model comparisons on a single task (e.g., what is the best neural net-\nwork architecture for large vocabulary speech recognition).\nThere are many factors at\nplay, including the challenges described above, but experimental methodologies for studying\ntraining algorithms seem less mature as well. There is a long tradition of competitions sur-\nrounding particular datasets, e.g., the ImageNet competition (Russakovsky et al., 2015),\nthat have at least produced a level playing field within the narrow boundaries of the original\nrules. We can apply the same approach to training algorithm comparisons. Direct compe-\ntition under a shared set of rules can cut through the Gordian knot of tangled incentives\nand inconsistent experimental protocols.\n3. Related Work\nOur goal is to create a benchmark for neural network training algorithms. Related work can\nbe grouped into three broad categories: Earlier efforts to benchmark training algorithms\nfor deep learning (Section 3.1); previous work identifying issues with existing approaches\nfor evaluating deep learning training algorithms (Section 3.2); and, finally, relevant prior\nwork that makes a case for, or against, particular training algorithms (Section 3.3), with a\nspecial focus on disagreements in the literature.\n3.1 Existing Benchmarks\nDomain-specific benchmarks and challenges have driven advances in machine learning for\ndecades (Garofolo et al., 1993; Krizhevsky and Hinton, 2009; Deng et al., 2009; Lin et al.,\n2014; Russakovsky et al., 2015; Panayotov et al., 2015; Bojar et al., 2015; Zbontar et al.,\n2018; Wu et al., 2018; Hu et al., 2020; Dwivedi et al., 2023; Dong and Yang, 2020). Such\nbenchmarks generally provide a curated dataset\u2014usually with pre-defined training, vali-\ndation, and test splits\u2014and specify what performance metrics should be used to measure\nprogress.\nThe research community uses benchmarks like these to demonstrate progress\non specific tasks (e.g., image classification, speech recognition, machine translation). Ulti-\nmately, domain-specific benchmark datasets are a component of almost all training algo-\nrithms benchmarks, including ours. Progress in domain-specific benchmarks often involves\na combination of new models, regularization techniques, and training algorithms. In many\ncases, however, it is difficult to disentangle how much improvement is due to, e.g., changes\nin the model versus the training procedure or tuning effort (Bello et al., 2021).\nA separate set of benchmarks thus aims to measure specifically the performance of train-\ning algorithms on different systems, or in different frameworks. The MLPerf\u2122 Training\nbenchmark (Mattson et al., 2020),4 which grew out of the DAWNBench benchmark (Cole-\nman et al., 2017), has the goal of evaluating system performance for neural network training.\nThe Closed Division of the benchmark aims to measure the performance of machine learning\nhardware and systems by requiring mathematical equivalence to a reference implementation,\n4. https://mlperf.org/training-overview\n21\nAlgoPerf Authors\nwhile still allowing submissions on different hardware. Similar to the MLPerf\u2122 Training\nbenchmark, the Deep Learning Benchmark Suite,5 DeepBench,6 and the Train-\ning Benchmark for DNNs (Zhy et al., 2018) also focus on evaluating machine learn-\ning systems (accelerators, operating systems, and frameworks) using fixed, well-established\nworkloads and training algorithms.\nUnlike the Closed Division of the MLPerf\u2122 Training benchmark and other similar\nsystems benchmarks, the Open Division of MLPerf\u2122 Training, has slightly more overlap\nwith our goals. In this division, in addition to running on different systems, submissions are\nallowed to modify the training algorithm and model. However, allowing arbitrary hardware,\npotentially at radically different scales, makes it impossible to distinguish improvements due\nto algorithms from those due to extra computation; in fact, an algorithm may not perform\nthe same at scales. Additionally, by allowing model changes, there is also no way to isolate\nimprovements due to training algorithm modifications. Generally, since submissions are\nviewed as independent per-workload entries, there is no incentive to avoid hyper-specific\nchanges that only help one particular benchmark workload. Even if a participant provides\nsubmissions for all workloads, they are free to use completely different training algorithms\nor models for each workload and perform unlimited workload-specific tuning.\nDeepOBS (Schneider et al., 2019), a software package and set of test problems for\nbenchmarking deep learning optimization methods, aims to simplify research and devel-\nopment of training algorithms. DeepOBS recommends reporting three key performance\nindicators: the final predictive error after a fixed number of steps, the wall-clock runtime\nper optimization step, and the ease of hyperparameter tuning. Since it positions itself as\na research tool, the package does not provide guidance on how these three performance\nindicators should be weighted against each other or aggregated across the provided test\nproblems. While DeepOBS fixes the workloads (addressing the challenges mentioned in\nSection 2.2), it contains only one large-scale deep learning workload (training ResNet-50\non ImageNet). The other workloads in DeepOBS are mainly smaller-scale deep learning\ntasks (e.g., image classification on MNIST, CIFAR, and SVHN). Synthetic problems and\nsmaller workloads are useful for rapid experimentation, and thus helpful to researchers.\nHowever, conclusions drawn from these problems do not always generalize to larger work-\nloads (e.g., larger models and/or datasets).\nFurthermore, DeepOBS leaves the user in\ncontrol of many parts of the training protocol, such as the tuning budget, and thus it is\nultimately up to the user to ensure a fair comparison.\nIn follow-up work, Schmidt et al. (2021) leverage the DeepOBS framework to perform\nan empirical comparison of fifteen popular training algorithms. For this purpose, they fix\nthe benchmarking protocol by comparing the methods at four different tuning budgets.\nThis empirical comparison is not a competitive benchmark. Instead, Schmidt et al. (2021)\ndetermined the hyperparameter search spaces of the training algorithms themselves, aim-\ning for a \u201cfair representation\u201d of each method based on statements made in the original\npublication, and not necessarily trying to make each method perform at its best (as the\noriginal authors may have done in a competitive setting). In addition, some of the issues\nof DeepOBS described above carry over to the study by Schmidt et al. (2021). Notably,\n5. https://github.com/HewlettPackard/dlcookbook-dlbs\n6. https://github.com/baidu-research/DeepBench\n22\nBenchmarking Neural Network Training Algorithms\ntheir empirical comparison uses mainly small-scale deep learning problems, and there is no\naggregated score across test problems.\nMoreau et al. (2022) introduce the Benchopt software suite for evaluating machine\nlearning optimization methods. They focus on features that a software platform should sup-\nport (e.g., modularity, extensibility) to enable fair and reproducible comparisons. However,\nat the time of writing, although Benchopt includes 16 different optimization problems,\nonly one is a deep neural network training workload.\n3.2 Methodological Critiques of Training Algorithm Comparisons\nBartz-Beielstein et al. (2020) present a series of recommendations for defining benchmarks\nfor optimization methods for several families of optimization problems, including deep learn-\ning optimizers. Although these recommendations generally align with the benchmark de-\nscribed in this paper, as we discussed in Section 2 and again in Section 4, below, a deep\nneural network training algorithm benchmark should not be restricted solely to optimization\nmethods, and should also encompass aspects like regularization and data sampling.\nSeveral recent works emphasize the importance of hyperparameter tuning in making fair\ncomparisons of different training algorithms. Schneider et al. (2019) recommend finding\nand reporting the best-performing hyperparameters for each test problem in DeepOBS.\nSivaprasad et al. (2020) advocate that researchers introducing a new training algorithm\nshould always provide a workload-agnostic prior distribution over hyperparameters that\ncan be used in conjunction with random search. They illustrate how the relative ranking\nof different optimization methods may change on a given workload depending on the hy-\nperparameter tuning budget. Choi et al. (2019b) illustrate how rankings can be sensitive\nto which hyperparameters are tuned, e.g., showing that tuning Adam\u2019s \u03b22 and \u03f5 can sig-\nnificantly improve performance compared to simply using their default values. They also\nillustrate that the choice of search space parameterization can influence the effectiveness\nof hyperparameter tuning, for example, explaining why tuning the relative initial learning\nrate \u03b10{\u03f5 of Adam is more efficient than tuning \u03b10 and \u03f5 independently.\n3.3 Disagreement over Training Algorithms: The Case for Clear Benchmarks\nSeveral of the studies discussed in Section 3.2 are motivated by earlier work making con-\ntentious claims about certain training algorithms. Below, we review several examples of\ndisagreements about training algorithms in the literature, since they demonstrate the op-\nportunity for trusted, competitive benchmarks to promote joint progress.\nAs one prominent example, Wilson et al. (2017) showed that SGD converges to the\nmaximum margin solution and provide a toy example where AdaGrad finds a solution\nwhich generalizes poorly while SGD finds a solution that generalizes well. The same work\nalso presented empirical results where adaptive methods such as AdaGrad, RMSProp,\nand Adam generalized much worse than SGD with Momentum.\nChoi et al. (2019b)\nsubsequently argued that Wilson et al.\u2019s empirical conclusion is only valid under what\nthey consider restrictive conditions, namely fixing Adam\u2019s \u03b22 and \u03f5 parameters to their\ndefault values, and only tuning a constant learning rate. In contrast, tuning all of Adam\u2019s\nhyperparameters and incorporating a learning rate decay schedule allows Adam to perform\ncomparably or even slightly better than SGD with Momentum. Additionally, Agarwal\n23\nAlgoPerf Authors\net al. (2020) argued for the importance of separating the effects of search direction and\nper-layer learning rate scaling when comparing optimizers, showing that conflating the two\nalso accounts for some of the findings of Wilson et al. (2017).\nAs another example, Liu et al. (2020) observed that the effective per-coordinate scal-\ning factors can have high variance early in training, which can lead to unstable training\ndynamics. They proposed the rectified Adam optimizer (RAdam) to compensate for this,\nand reported several experiments showing that RAdam outperforms standard Adam with\nlearning rate warmup on image classification and machine translation tasks. Subsequently,\nMa and Yarats (2021) showed that using Adam with an appropriately tuned learning rate\nwarmup performs comparably or slightly better to RAdam on similar tasks.\nThere are also disagreements about whether optimizers that use off-diagonal curvature\ninformation are useful when training neural networks. Hinton and Salakhutdinov (2006)\nproposed a deep autoencoder model for reducing the dimensionality of the input data.\nMartens (2010) showed that the Non-Linear Conjugate Gradient method (Nocedal and\nWright, 1999) is quite ineffective in minimizing the training loss in this problem. In contrast,\nnon-diagonal methods such as truncated-Newton (Nocedal and Wright, 1999) provide a\nsignificant advantage.\nSince Martens (2010)\u2019s work, the deep autoencoder problem has\nserved as a standard benchmark for comparing diagonal and non-diagonal optimization\ntechniques where the gap between the two methods is expected to be fairly substantial\n(Martens and Grosse, 2015; Goldfarb et al., 2020; Anil et al., 2020; Ren and Goldfarb,\n2021; Ren et al., 2020; Zhao et al., 2022; Bae et al., 2022). However, Amid et al. (2022)\nfound that with sufficient hyperparameter tuning, diagonal preconditioning based methods\nsuch as RMSProp worked reasonably well on this problem compared to non-diagonal\npreconditioning based methods such as K-FAC (Martens and Grosse, 2015) and Shampoo\n(Gupta et al., 2018; Anil et al., 2020). Amid et al. (2022) thus argued that algorithmic\nimprovements to diagonal methods can practically eliminate the gap between diagonal and\nnon-diagonal methods.\nFinally, there has been substantial disagreement in the literature about whether new\ntraining algorithms are necessary as the batch size increases. Although some of this dis-\nagreement is due to imprecise claims and experimental setups that implicitly demand perfect\nbatch size scaling (Shallue et al., 2019), various training algorithms have been proposed to\nhandle the supposed problem of \u201clarge batch training.\u201d For example, You et al. (2017) and\nYou et al. (2020) introduced the LARS and LAMB optimizers, respectively, and argued\nthey were necessary at larger batch sizes. Subsequently, MLPerf\u2122 Training adopted\nLARS for its ResNet-50 on ImageNet workload and various submitters have demon-\nstrated impressive training speed results. Nevertheless, Nado et al. (2021) have since shown\nthat, with appropriate hyperparameter tuning, it is possible to obtain similar training speed\nresults using SGD with Nesterov Momentum with batch sizes up to 32,768. They also\nshowed that stronger Adam baselines could outperform LAMB results on BERT (Devlin\net al., 2019) pre-training, and argued that there was no convincing evidence that LARS\nand LAMB should be used over standard techniques.\nThe examples above illustrate how the challenges in benchmarking training algorithms\ndiscussed in Section 2 directly affect the training algorithms community. Perhaps most crit-\nically, they emphasize the importance of tuning hyperparameters in a fair and consistent\nway to give each algorithm the best chance to perform well (Choi et al., 2019b; Sivaprasad\n24\nBenchmarking Neural Network Training Algorithms\net al., 2020). Although this may sound straightforward, substantial care must be taken when\ndefining the hyperparameter search space for each algorithm. Framing training algorithm\ncomparisons as a competition has the crucial advantage that each participant individually\nstrives to make their method work best under the constraints of the contest, with one par-\nticipant\u2019s method becoming another\u2019s baseline. In contrast, the status quo in the literature\nis for researchers to make uncontrolled changes and depend on the vicissitudes of the (noisy)\npeer review process to enforce some notion of a \u201cfair\u201d comparison with previous work, re-\nsulting in confusing comparisons with baselines that tend to be much too weak. Working\nfrom a common, open codebase enables researchers to independently reproduce and verify\nthe claims of others, and also makes it easier for entrants to the competitive benchmark to\nshare their submissions with the community for future comparisons and studies.\n4. Rules\nThe goal of our benchmark is to identify general-purpose neural network training\nalgorithms that can speed up training on new workloads. Our benchmark measures time-\nto-result (Section 4.1) on a fixed hardware configuration. To ensure that the benchmark\nresults have real-world relevance, we define goal error rates based on the validation and test\nsets instead of the training loss. In order to isolate the effect of the training algorithm,\nsubmissions must adhere to a specific API (Section 4.2) and cannot make alterations out-\nside a limited number of functions. To incentivize generally useful algorithms, we require\nthat a single training algorithm simultaneously performs well across multiple workloads\n(Section 4.3) without manual workload-specific adaptation. Instead, any adaptation to the\nworkloads should either be possible with generic tuning methods (Section 4.4.1) or be per-\nformed as part of the timed training process (Section 4.4.2). All workloads are considered\nwhen calculating an aggregate benchmark score of the training algorithm (Section 4.5).\nThe resulting benchmark score is intended to serve as an estimate of the performance of a\ntraining algorithm on unknown workloads.\nIn the following sections, we describe the essential elements of our benchmark rules\nand explain the reasoning behind them.\nThis section is based on the rules at the\ntime of writing.\nAs the benchmark evolves, the rules may also change, and the\nmost up-to-date, complete rules can be found at github.com/mlcommons/algorithmic-\nefficiency/blob/main/RULES.md.\n4.1 A Time-to-Result Benchmark\nA submission\u2019s score is based on the time to reach the target validation and test scores\non each workload.\nTraining is considered complete, and the clock stops, as soon as a\nsubmission achieves the validation and test targets. For practical reasons, submissions are\nlimited by a maximum allowed runtime on each workload. If a submission fails to achieve\nthe targets within this maximum runtime it will receive an infinite training time on this\ntrial.7 Although setting these targets will always be contentious to some degree, we need\n7. Depending on the tuning ruleset, a submission may get several trials or only one trial per workload; see\nSection 4.4 for more details.\n25\nAlgoPerf Authors\na systematic procedure to determine target validation and test scores that are competitive\n(ideally near the state of the art), while being achievable within a reasonable time budget.\nFor a given target-setting runtime budget, we defined the validation and test targets\nof a workload based on what can be reliably achieved using standard methods. Specifi-\ncally, we used four popular training algorithms (Heavy Ball, Nesterov, AdamW, and\nNadamW) tuned with quasirandom search (Bousquet et al., 2017) on hand-engineered,\nworkload-agnostic search spaces. For each training algorithm and workload, we ran 200\ntrials for the full target-setting runtime budget8 and determined the best combination of\ntraining algorithm and hyperparameter settings by finding the trial that achieved the best\nvalidation error. We re-ran this combination of training algorithm and hyperparameters\n20 times with different random seeds, taking the median of the best achieved validation\nerrors across seeds to obtain a validation target. Out of the 10 repeated runs that achieved\nthis validation target, we took the worst achieved test error across seeds as our test target.\nTaking the median validation performance after rerunning the best hyperparameter point\nprevents our procedure from selecting a lucky outlier. Our protocol defines both validation\nset targets and test set targets in order to implement the external tuning ruleset, as de-\nscribed in Section 4.4.1. Exact tuning details and results can be found in Section 5. It is\nimportant to note that the target-setting procedure does not constitute a valid submission\nsince it uses 200 trials instead of the 20 trials available in the external tuning ruleset.\nThe procedure outlined above requires us to determine two runtime budgets for each\nworkload. First, we need to set the maximum allowed runtime for the submissions. Ideally,\nthis runtime budget would be infinite, as it would allow us to accurately gauge the time\nrequired for a submission to successfully train each workload. However, for practical reasons,\nwe must limit this budget. Second, we need to set a target-setting runtime budget for the\ntarget-setting procedure described in the previous paragraph. We decided to use different\nruntime budgets for the submissions and target-setting.\nSpecifically, the target-setting\nruntime budget is set to 0.75\u02c6 the maximum allowed runtime for submissions. By allowing\nsubmissions a more generous runtime budget, they have some leeway to spend extra time\non certain workloads, if they can compensate for it on other workloads.\nWhen setting maximum runtimes for a workload, it is important to find a balance\nbetween challenging and achievable targets. Increasing the maximum runtime may improve\nthe performance but it also increases the overall runtime of the benchmark. To balance the\ndual objectives of stringent targets, near the state of the art in the literature, and making\nbenchmark submissions practical to evaluate on the full suite of workloads, we aimed to limit\nthe combined runtime of all fixed workloads to 100 hours on the benchmarking hardware.\nWe used both preliminary experiments and published results on the datasets and models\nof our workloads as a guide in allocating this combined runtime budget to the individual\nworkloads. The allowed maximum runtimes for the submissions on each workload are shown\nin Table 7. Section 5 discusses how the benchmark targets (which were set using 0.75\u02c6 the\nruntimes presented in Table 7) compare to results from the literature.\n8. For simplicity, we converted these target-setting runtime budgets into step budgets since all four training\nalgorithms used for target-setting happen to have nearly identical average step times.\n26\nBenchmarking Neural Network Training Algorithms\n4.1.1 Measuring Runtime\nWe selected elapsed real time, or wall-clock time, as our measure of runtime for training.\nWe made this choice to maximize the practical relevance of our timing measurements and\nto avoid imposing new restrictions on how training algorithms could operate. To make\nmeaningful comparisons of different algorithms in terms of wall-clock time, all algorithms\nmust be run on a standardized hardware system (discussed further in Section 4.1.2).\nThe research literature contains examples of several alternatives to directly measuring\nwall-clock time. In some cases, researchers count training steps, number of forward passes,\ngradients, or some other abstract notion of iterations instead of an all-encompassing time\nmeasurement. Counting steps is convenient when iterations have the same, consistent cost\nduring a single run and we can measure the average time-per-step for different algorithms\neasily. More generally, abstracting away the hardware and system conditions is appealing\nwhen it is possible, but such abstract runtime proxies are sadly not an option for a general\ntraining algorithms benchmark. Counting iterations is meaningless since submissions are\nfree to redefine what a single step means.\nCounting gradient computations is similarly\nmeaningless since submissions can vary in what derivatives (if any) they compute, or even\nuse radically different batching schemes that include intelligent data selection.\nAbstract notions of steps completed or examples processed do not necessarily reward\nalgorithms that are the most useful in practice. Some algorithms might cleverly reclaim idle\naccelerator time while waiting for new data (e.g., by applying Data Echoing (Choi et al.,\n2019a)). Some algorithms might be especially memory efficient and thus able to use larger\nbatch sizes and better exploit hardware parallelism. Conversely, some algorithms might be\nimpractical because they require too much extra memory.\nMeasuring wall-clock time has some disadvantages. Although we care about implemen-\ntation quality to some extent, our intention is not to make a software benchmark since that\ngoal is better served by comparing mathematically equivalent programs. Furthermore, vari-\nation in network congestion when reading data or writing results is of little interest, nor do\nwe care about benchmarking unrelated processes running on the system and how they inter-\nfere with the training program. We expect that by using a standardized hardware system\ndescribed below (namely, a single server with multiple GPUs), and by having the training\nprogram be the only significant (i.e., computationally intensive) program running on the\nsystem, that these additional factors will not substantially affect the measured runtimes.\n4.1.2 Standardizing Benchmarking Hardware\nTo fairly compare wall-clock runtimes of different algorithms, the times should be mea-\nsured on a standardized training system (e.g., hardware accelerators, memory, CPUs, in-\nterconnect) using a standardized execution environment, including consistent software ver-\nsions. For the initial version of the benchmark, we selected a system with 8\u02c6NVIDIA V100\nGPUs with 16GB of VRAM per card since it is widely available on major cloud computing\nproviders and offers a good compromise between performance and affordability. This offi-\ncial benchmarking system only needs to be used for final timing runs. Tuning a submission\nonly requires a comparison between different hyperparameter settings of a single training\nalgorithm, so it is fine to use a different but consistent system for tuning experiments.\n27\nAlgoPerf Authors\nInevitably, accelerators and the systems available on the market will change in the future,\nso future iterations of the benchmark may adopt new benchmarking systems or even support\nmultiple \u201cweight classes\u201d of systems. As long as future benchmarking systems are strictly\nmore capable, especially in terms of accelerator memory capacity, it should be relatively\nstraightforward to rerun the baselines and the top-performing previous submissions on new\nsystems, at least using the old batch sizes to provide a pessimistic bound.\n4.2 Specifying a Training Algorithm\nThe rules define a precise API to be used when specifying a training algorithm submis-\nsion. A submission to the competitive benchmark must define four submission functions.\nThe submission functions allow submitters to define how the submitted training algorithm\nupdates the model parameters ( update params ) and how data are selected for training\n( data selection ).\nFurthermore, submitters can initialize the training algorithm\u2019s state\n( init optimizer state ) and must provide a batch size for each workload ( get batch size ).\nAn implementation of the submission functions may make use of a limited API to get some\nbasic information about the workload. A detailed description of the submission functions\nsignatures\u2019 and the benchmark API can be found in the rules.9\nIn addition to implementing the four submission functions, a training algorithm may\nhave hyperparameters that will be tuned following one of the rule sets described in Sec-\ntion 4.4. Submissions to the external tuning ruleset described in Section 4.4.1 must also\nspecify a hyperparameter search space.\nApart from defining the submission functions and hyperparameter search space, a sub-\nmission may not modify any other parts of the training pipeline. The rest of the training\nprogram (e.g., implementations of data pipelines, model architecture, training loop, cal-\nculation of validation and test metrics, and measurement of runtime) are implemented by\nthe benchmark. We intentionally restrict which parts of the training program a submission\nmay modify for two main reasons. First, we prohibit certain types of changes in order to\nisolate speedups due to training algorithm changes (Section 4.2.1). Second, we hope to deter\nsubmissions that are over-specialized to particular benchmark workloads and are unlikely\nto be generally useful on new problems (Section 4.2.2).\nThe benchmark exposes a limited API of functions that a submission may call, within\nthe four submission functions, to get information about a training workload at execution\ntime. The workload-specific information available to submissions is restricted, since the goal\nof this benchmark is to identify training algorithms that are generally useful across many\nworkloads. One of the ways in which a submission may be workload-specific is by providing\na different batch size to be used for each workload. This is allowed since the benchmark\nhardware, including accelerator memory capacity, is fixed, and different training algorithms\nmay involve storing and updating different state.\nIt is impossible to design an API and comprehensive rules that would prohibit all possible\ncases of submissions circumventing the spirit of the benchmark. Instead, we will prohibit\nthese submissions in the abstract and will defer rulings about specific submissions to a\n\u201cspirit [of the rules] jury.\u201d Similarly, there may be modifications that we would allow in\n9. See the \u201cValid Submission\u201d Section in https://github.com/mlcommons/algorithmic-efficiency/\nblob/main/RULES.md#valid-submissions.\n28\nBenchmarking Neural Network Training Algorithms\nprinciple, but which are currently not practically feasible within the provided API. Since\nthis is a practical benchmark, we must accept that we cannot guarantee a perfect overlap\nbetween what is allowed and what is possible. However, we may modify the API in future\niterations of this benchmark to accommodate a larger set of allowed modifications.\n4.2.1 Isolate the Training Algorithm\nA training algorithms benchmark should prohibit modifications outside of the training al-\ngorithm in order to disentangle improvements due to the training algorithm from other\nbeneficial pipeline changes (see Section 2.2.2). However, exactly what constitutes part of\nthe training algorithm is not always clear.\nThe distinction between the model and the\ntraining algorithm can be quite subtle, as well as the distinction between improving the\nimplementation quality of the submission versus improving its software dependencies in a\nway that could apply to all submissions.\nModel vs. training algorithm.\nThere is not always an obvious separation between the\nmodel and the training algorithm. In our benchmark, we use the basic rule of thumb that\nanything that can be applied to a generic workload is part of the training algorithm. On\nthe other hand, if something only applies to some workloads or is otherwise inherently\nworkload-specific, it should not be considered part of the training algorithm.\nOne delicate area is regularization. Because our benchmark focuses on validation and\ntest set performance, we need to include regularization as part of the training algorithm\nin our benchmark and allow submitters at least some control over it since some training\nalgorithms may implicitly have a regularizing effect. However, submissions can only be\nallowed complete control over model-agnostic regularization. Therefore, we allow submis-\nsions to tune the strength of regularization methods predefined by the workloads, and we\ndo not allow submissions to introduce new regularization strategies that require modifica-\ntions to the data preprocessing or model architecture. For example, submissions can set or\ntune the dropout rates of models that already have dropout layers. However, they cannot\nintroduce additional dropout layers, as this would require knowledge of the model architec-\nture. Similarly, the submitted training algorithms receive raw loss values and can add any\nworkload-agnostic regularization term, e.g., L2 regularization of the desired strength.\nData augmentation strategies often improve generalization performance. However, most\ndata augmentations strongly depend on the input data modality and are therefore workload-\ndependent. Submissions cannot introduce new data augmentation strategies to the bench-\nmark workloads. However, they do have control over batching and could potentially filter,\nreorder, or otherwise prioritize training data.\nAnother tricky technique to handle is batch normalization, which can be seen either\nas a training algorithm component or\u2014as it is commonly implemented\u2014as a model layer\nand thus part of the model architecture. In our benchmark, submissions cannot introduce\nadditional batch normalization layers to the workloads or change the location of existing\nnormalization layers. However, they do have control over whether the batch normalization\nstatistics should be updated. This is important, for example, for line search approaches\nthat search over multiple candidate updates before applying the most appropriate one. In\nthis case, such a submission might not want to update the batch normalization statistics\nafter each candidate evaluation, but only once an acceptable point has been selected.\n29\nAlgoPerf Authors\nIntroducing software dependencies.\nSubmissions are not allowed to use software en-\ngineering approaches to speed up low-level, primitive operations in JAX, PyTorch, their\ndependencies, or the operating system.\nFor example, it is prohibited to introduce new\ncompiler functionality, using faster GPU kernels, or make similar modifications that could\ngenerally benefit any submission. Submissions must also use the versions of PyTorch or\nJAX (and their dependencies) specified by the benchmark.\nSubmissions are free to add software packages that support novel algorithmic and math-\nematical ideas, as long as they do not circumvent the intention of the benchmark. For\nexample, submitters are allowed to use packages such as BackPACK (Dangel et al., 2020),\nwhich extracts additional information from the backward pass. We also recognize that the\nway a method is implemented will impact its performance in the benchmark, and it is gen-\nerally acceptable to make clever and efficient use of the JAX and PyTorch APIs from\nwithin the submission functions. For example, it would be acceptable to use CUDA streams\nto schedule the transfer of data from CPU to GPU while performing other computations.\nHowever, under the rules there are periodic untimed model evaluations that do not con-\ntribute to the submissions score and it would not be acceptable for a submission to schedule\nasynchronous computations that are being performed during these untimed evaluations.\n4.2.2 Incentivizing Generally Useful Submissions\nWe want to disallow submissions if they clearly violate the spirit of the benchmark, even if\nthese submissions perform well in our benchmark. Most importantly, this includes overly\nbenchmark-specific methods that cannot be applied to generic deep learning workloads.\nIt is impossible to define rules that clearly distinguish between allowed and prohib-\nited submissions in all possible scenarios. This section provides some guidelines to clarify\nwhether or not a submission violates the spirit of the rules, and thus should be disqualified\nby the spirit jury. As a rule of thumb, a submission should be allowed if it will run and do\nsomething reasonable on unseen workloads without requiring additional human engineering\neffort. Two essential questions can guide this distinction:\n1. What information is used by the submission?\n2. What action is the submission code taking based on this information?\nGenerally, both parts are needed to decide if a particular piece of code is within the spirit\nof the rules. Below are some specific examples intended to illustrate the policy. Additional\ncases of allowed and disallowed submissions, along with further clarifications, can be found\nin the complete rules.10\nUsing shape and layer information of the model.\nSubmissions may use the pro-\nvided model parameter shape information if the resulting action can be applied to generic\nworkloads. Some examples of allowed uses include (a) using the shape information of each\nlayer to switch between a high-memory and a low-memory routine, (b) using different up-\ndate rules (e.g. Adam and SGD) for different layer types (e.g. convolutional layer, batch\nnormalization layer, fully-connected layer), or (c) leveraging the order of the layers to train\nlayers in an organized fashion.\n10. See the \u201cValid Submission\u201d Section in https://github.com/mlcommons/algorithmic-efficiency/\nblob/main/RULES.md#valid-submissions.\n30\nBenchmarking Neural Network Training Algorithms\nHowever, submissions may not use this same information to identify the specific work-\nload and use workload-specific settings. A clear case of using this information in a way that\nviolates the spirit of the benchmark would be using the shapes of the model parameters as\na workload \u201cfingerprint\u201d and then loading or looking up the predetermined optimal hyper-\nparameters. In general, any hard-coded behavior based on identifying a specific workload is\nprohibited. At the same time, it is entirely acceptable to take action based on basic modules\nsuch as the layer type. In other words, a submission may run different code whenever it\nencounters a model with convolutional layers, but it shouldn\u2019t need to specifically know\nthat it is training a ResNet-50 on ImageNet.\nUsing expensive offline computations.\nSubmissions may not circumvent the tuning\nrules by looking up the workload-specific results of offline computations that have been\nperformed ahead of time. This includes looking up optimal hyperparameter settings for\neach specific workload or even looking up (pre-trained) model parameters.\nIn contrast, it is perfectly fine to hard-code a single hyperparameter setting, e.g., a\ndefault hyperparameter setting, even when found using an expensive offline search because\nthe hyperparameter will need to perform well on all workloads simultaneously and thus\ncould be expected to have some hope of generalizing to new workloads.\nWe also allow\nsubmissions based on learned training algorithms, which may include using a learned set of\nhyperparameters. In this case, we ask submitters to disclose information about the training\nset used to develop these learned training algorithms.\n4.3 Workloads\nA workload consists of a dataset (including any preprocessing), model, and loss function,\nalong with a target that is defined using some evaluation metric. This evaluation metric\ncan be identical to the workload\u2019s loss function, or it could be a workload-specific metric\nsuch as the word error rate (WER) or the mean average precision (mAP). For example,\ntraining ResNet-50 on ImageNet using the cross-entropy loss (CE) until a target error\nof 34.6 % on the test set has been reached, would constitute one workload.\nA diverse set of workloads that reflect multiple important application areas is neces-\nsary to assess the suitability of an algorithm as a general-purpose training algorithm (Sec-\ntion 2.2.2). We selected our list of workloads to cover several different tasks and data modal-\nities, including image classification, machine translation, speech recognition, and other typ-\nical machine learning tasks, focusing on today\u2019s practically relevant workloads. The set\nof workloads will need to be extended in future iterations of the benchmark in order to\nremain relevant and to match advancements in the field. We intentionally restricted our\ncurrent set of workloads to supervised learning tasks (although we could easily accommo-\ndate self-supervised tasks) and excluded reinforcement learning problems that might require\nfundamentally different methods and evaluation protocols.\n4.3.1 Fixed and Randomized Workloads\nIn service of identifying generally useful training algorithms, our benchmark includes two\ntypes of workloads: fixed workloads and randomized workloads. Fixed workloads are fully\nspecified in the benchmark and completely known to the submitters. Table 7 provides an\noverview of the 8 fixed workloads used in the first iteration of this benchmark (Appendix D\n31\nAlgoPerf Authors\ncontains the individual workload details). Additionally, we also provide randomized work-\nloads which are only defined as distributions over workloads.\nOnce all the benchmark\nsubmissions are frozen, we will sample specific instances from these randomized workloads\nthat we call held-out workloads. A submission\u2019s score is a function of its performance on\nthe fixed workloads as well as these held-out workloads.\nValidation\nTest\nMaximum\nTask\nDataset\nModel\nLoss\nMetric\nTarget\nTarget\nRuntime\nClickthrough\nrate\nprediction\nCriteo 1TB\nDLRMsmall\nCE\nCE\n0.123649\n0.126060\n7703\nMRI reconstruction\nfastMRI\nU-Net\nL1\nSSIM\n0.7344\n0.741652\n8859\nImage\nImageNet\nResNet-50\nCE\nER\n0.22569\n0.3440\n63,008\nclassification\nViT\nCE\nER\n0.22691\n0.3481\n77,520\nSpeech\nLibriSpeech\nConformer\nCTC\nWER\n0.078477\n0.046973\n101,780\nrecognition\nDeepSpeech\nCTC\nWER\n0.1162\n0.068093\n92,509\nMolecular\nproperty\nprediction\nOGBG\nGNN\nCE\nmAP\n0.28098\n0.268729\n18,477\nTranslation\nWMT\nTransformer\nCE\nBLEU\n30.8491\n30.7219\n48,151\nTable 7: Summary of the fixed workloads used in our benchmark. The possible\nlosses are the cross-entropy loss (CE), the mean absolute error (L1), and the Connectionist\nTemporal Classification loss (CTC). The evaluation metrics additionally include the struc-\ntural similarity index measure (SSIM), the error rate (ER), the word error rate (WER), the\nmean average precision (mAP), and the bilingual evaluation understudy score (BLEU).\nEach randomized workload introduces minor modifications to an associated fixed base\nworkload. These modifications include, for example, altering the data augmentation strate-\ngies or modifying aspects of the model architecture, such as the activation function or the\nnumber of layers. Each randomized workload defines a distribution over workloads. For\nthe first iteration of the benchmark, for convenience, we used particularly simple discrete\ndistributions that sample one concrete workload variant out of a set of three possible, hand-\ndesigned variants of the base workload. Only once all submissions are frozen do we select the\nspecific instance of the held-out workload that will be used during scoring. Thus, although\nsubmitters know the three possible variants that might be sampled for each randomized\nworkload, they will not know which of the three will be sampled during scoring. Defining\ndistributions instead of directly specifying instances ensures that while the entire process\nis public and transparent, neither the submitters nor the benchmark organizers know the\nspecific held-out workloads beforehand.\nThe random held-out workloads function similarly to a held-out test set, discouraging\ntraining algorithms that overfit to the fixed workloads.\nThe randomized workloads are\nintended to simulate novel-but-related workloads to ensure that the proposed training al-\ngorithms generalize to unknown problems. Although submitters can enumerate all possible\nheld-out workloads, by creating a larger set of possibilities than the fixed workloads alone,\nthe randomized workloads should encourage more robust training algorithms. Ideally, the\nrandomized workloads should strike a balance between modifying a base workload enough\nto generate a different workload, but not so much that they produce something impossible\n32\nBenchmarking Neural Network Training Algorithms\nto train. If the modifications are too conservative, then the held-out workload will simply\nbe a copy of the base workload and not provide a generalization challenge that simulates\nnovel workloads. However, if the modifications are too drastic, then the workload could lose\nits practical relevance.\nConsequently, our randomized workload distribution should only introduce \u201cnatural\nchanges\u201d that a practitioner might want to experiment with. These include modifications\nsuch as changing the activation function, the type of normalization layer, or modifying the\nnumber and width of the layers. Any modification that actually improves results is prima\nfacie natural, as are modifications that occur in the literature or are obvious extensions\nof common practice. We want to avoid extremely contrived changes purely designed to\nmake the training problem harder. That said, we are not above changes that might arise\nbased on an insufficiently careful initial weight distribution, badly scaled parameters, or\nother changes of that nature. At the end of the day, we want changes that may elucidate\nrobustness properties of training algorithms that actually provide value for practitioners.\nFor the benchmark\u2019s first iteration, we manually designed three different workload vari-\nants for each fixed workload (see Table 11) from which to draw held-out workloads.11 For\neach fixed workload, one of these workload variants will be randomly selected after submis-\nsions are frozen. The randomized workloads use the same procedure as the fixed workloads\nto define validation and test targets (Section 4.1).\nHowever, to save computational re-\nsources, we only tuned two training algorithms for the randomized workloads, NadamW\nand the other best-performing training algorithm on the corresponding base workload.\nIn total, a training algorithm submitted to the benchmark is evaluated on 8 fixed and 8\nheld-out workloads. However, scoring uses the held-out workloads only to penalize submis-\nsions that can\u2019t handle them, and reserves the fixed workloads for timing measurements.\nSection 4.5.3 describes the precise way held-out workloads affect submissions scores, but,\nat a high level, we wanted to prioritize the fixed workloads for timing measurements since\nthey are the most relevant variant and merely use the held-out workloads as a deterrent for\nbrittle submissions. Finally, Section 6.2 describes the experimental protocol for selecting\npossible workload variants to serve as components of randomized workloads.\n4.4 Tuning\nGiven our goal of evaluating generally applicable training methods, any workload-specific\nadaptation (or tuning) should be an automated part of the algorithm. In our benchmark,\nwe provided two tuning rulesets that govern how hyperparameters may be tuned. In the\nmore permissive, external tuning ruleset (Section 4.4.1), each submission may define a list\nof hyperparameters along with a search space to tune them over. To evaluate a submis-\nsion using external tuning, we tune its hyperparameters using quasirandom search (with a\nmodest, fixed budget of tuning trials) over the submission\u2019s search space. The runtime for\na submission under this ruleset on a given workload is then the best-performing (fastest\nto reach the target) hyperparameter setting for that workload across the tuning trials. In\ncontrast, the more restrictive, self-tuning ruleset (Section 4.4.2) does not allow any tun-\n11. In preliminary experiments, we briefly attempted to construct randomized workload distributions with\nsupport over much larger sets of concrete workloads before deciding to take a simpler approach for the\nfirst iteration of the benchmark. See Appendix E for information on these preliminary experiments.\n33\nAlgoPerf Authors\ning outside of the timed operation of the submission itself on the benchmarking system.\nInstead, the training algorithm must be hyperparameter-free or, equivalently, it must au-\ntomatically set any hyperparameters it happens to define. The main difference between\nthese two rulesets is that in the self-tuning ruleset every part of the algorithm\u2014which may\ninclude any arbitrary internal tuning procedure\u2014is performed \u201con the clock.\u201d In contrast,\nthe external tuning ruleset allows for some parallelization of hyperparameter tuning, where\nonly the fastest hyperparameter setting for each workload is used for scoring.\nBoth rulesets are essentially independent competitions, where submissions only compete\nwith methods adhering to the same ruleset. The two rulesets cover two different practical\nscenarios: tuning on a single machine sequentially, and tuning in parallel in one shot across\na modest number of machines. Any valid submission for the self-tuning ruleset is valid\nunder the external tuning ruleset, but not vice-versa. Both tuning rulesets share all other\nnon-tuning benchmark rules, unless otherwise specified. Below we describe the each version\nof the tuning rules in more detail.\n4.4.1 External Tuning\nFor each workload and each submission requiring external tuning, the hyperparameters are\ntuned using 20 tuning trials drawn with quasirandom search (Bousquet et al., 2017) from the\nworkload-agnostic search space specified in the submission. In lieu of independent samples\nfrom their search space, submissions can instead supply a fixed list of 20 hyperparameter\npoints that will be sampled without replacement.12 Using more than 20 tuning trials would\nincrease the computational burden of the benchmark. At the same time, we want the exter-\nnal tuning ruleset to be sufficiently different from the self-tuning ruleset, which effectively\nuses a single tuning trial. To produce lower variance scores, the rules require repeating the\ntuning for five independent studies, resulting in a total of 100 trials. For each of the five\nstudies and for each workload, the hyperparameter setting that reaches the validation target\nthe fastest will be selected among the 20 tuning trials. For each workload, the score for a\nsubmission is the median of these five per-study training times and becomes an input into\nthe overall benchmark score (Section 4.5.2). While we use the time to reach the validation\nset target for selecting the hyperparameter point, we use the time to reach the test set\ntarget for that hyperparameter setting for scoring. The five independent studies effectively\nsimulate five hypothetical independent practitioners training the algorithm on a workload\nusing the same search space. Using the median score allows us to assess what training speed\nthe average practitioner can expect from this method.\n4.4.2 Self-Tuning\nSubmissions to the self-tuning ruleset are not allowed to have user-defined hyperparameters\nand therefore receive no extra tuning. Instead, the training algorithms in this ruleset need\nto perform all necessary adaptations to the workload autonomously. This adaptation could,\nfor example, come in the form of inner-loop tuning, e.g. line search approaches, sequential\nouter-loop tuning, freeze-thaw methods, or algorithms that use the same hyperparameters\n12. Following Metz et al. (2020), we also refer to this approach as a \u201clearned optimizer list\u201d or, abbreviated,\nas an OptList.\n34\nBenchmarking Neural Network Training Algorithms\nfor all workloads, e.g. Adam with its default parameters. Any tuning effort will be part of\nthe per-workload score and thus any tuning should save more time than it costs.\nCompared to the external tuning ruleset, there are no tuning trials but only a single run\nper study. Once again, the median training time of the 5 studies represents the per-workload\nscore and is incorporated into the benchmark score. Since we do not use any (external)\ntuning in this ruleset, the time to reach the validation target is irrelevant and only the time\nto the test target is considered. To account for the lack of external tuning, submissions have\na longer per-workload time budget. Compared to the external tuning ruleset, all maximum\nallowed runtimes are tripled, i.e. 3\u02c6 the maximum runtime reported in Table 7.\nAt this time, we do not anticipate self-tuning submissions will be competitive with\nexternally tuned methods in terms of their per-workload score. However, closing the gap\nbetween fully automatic methods in the self-tuning ruleset and more traditional externally\ntuned methods could drastically reduce the compute requirements of deep learning.\n4.5 Scoring and Reporting Results\nSo far, we have described how to evaluate a submission on a workload and produce a raw\ntraining time score. Suppose that for each submission s and each workload w, we determine\na training time ts,w P r0, 8q using one of the rulesets described in Section 4.4. This training\ntime is the wall-clock runtime it took the submission to first reach the test target on this\nparticular workload. In the case of the external tuning rule set, we measure the time it takes\nto reach the test target only for the hyperparameter setting that achieved the validation\ntarget the fastest. If a submission is unable to reach the target within the given runtime\nbudget, it will receive a score of ts,w \u201c 8 for this particular workload. These raw training\ntime scores are already useful if we care about a single workload, e.g., by ranking submissions\non the benchmark workload that is most similar to the real-world problem we are trying to\nsolve. However, the raw training time scores are too detailed in most situations.\nDepending on our goal, we will need different ways of summarizing the raw training\ntime data and converting it into scores, visualizations, and summary statistics. In general,\nin addition to providing guidance to practitioners that only care about a single workload,\nwe need the results we report to help us:\n1. Qualitatively compare submissions across all workloads at once,\n2. Construct a leaderboard for submissions that aggregates across workloads, and\n3. Summarize year-over-year benchmark progress.\n4.5.1 Aggregation Using Performance Profiles\nA natural way to compare a given submission s within a larger pool of competing sub-\nmissions is to look at the fraction of workloads where s trains the fastest (i.e., ts,w is the\nsmallest). However, if there are workloads where a few strong submissions get nearly the\nsame raw training time score, it might make sense to look at the fraction of workloads\nwhere s is either the fastest or nearly the fastest by being within, say, 1% of the runtime\nof the best submission on that workload. Since our choice of 1% is arbitrary, we also might\nask the same question with different notions of whether a submission is sufficiently close\nto the fastest. Performance profiles (Dolan and Mor\u00b4e, 2002) conveniently generalize this\n35\nAlgoPerf Authors\nidea. Specifically, a performance profile is a plot of the fraction of workloads where a given\nmethod is within some ratio of the best per-workload training time.\nPerformance profiles are straightforward to compute given the raw training times, ts,w,\nfor a set of k submissions S \u201c ts1, s2, . . . , sku measured on a set of n workloads W \u201c\ntw1, w2, . . . , wnu. Specifically, for a submission \u00afs on a particular workload \u00afw we define its\nperformance ratio as:\nr\u00afs, \u00afw \u201c\nt\u00afs, \u00afw\nminsPS ts, \u00afw\n.\n(1)\nIn other words, the performance ratio r\u00afs, \u00afw expresses how much time the submission \u00afs took\non workload \u00afw compared to the best submission on \u00afw. For example, if a submission takes\ntwice as long on a particular workload compared to the best submission, it will receive a\nperformance ratio of 2. By definition, r\u00afs, \u00afw \u011b 1 for all submissions \u00afs and workloads \u00afw.\nThe performance profile \u03c1\u00afsp\u03c4q for a submission \u00afs is the probability that, on a random\nworkload w drawn uniformly from W, \u00afs will have a performance ratio r\u00afs,w of at most \u03c4.\nSpecifically, for \u03c4 P r1, 8q the performance profile is defined as \u00afs:\n\u03c1\u00afsp\u03c4q \u201c\n\u02c6 1\nn\n\u02d9\n\u00a8 |t \u00afw : r\u00afs, \u00afw \u010f \u03c4u| .\n(2)\nSince \u03c1\u00afsp\u03c4q expresses the fraction of workloads where a submission is less than \u03c4 away from\nthe optimal submission, it is piecewise constant, monotonically non-decreasing with \u03c4, and\nbounded between 0 and 1. A perfect submission that always achieves the fastest training\ntime on every workload would have a performance profile that immediately jumps to 1 at\n\u03c4 \u201c 1. Figure 5 in Section 7 shows performance profiles for our baseline submissions.\nPerformance profiles are a convenient way to summarize the overall performance of a\nsubmission relative to other submissions on the benchmark workloads. Since performance\nratios are relative to the best submission on each workload, performance profiles are also\ninherently relative and will change as submissions are added or removed from the compar-\nison set. Although performance profiles are great to qualitatively compare submissions, to\nconstruct a leaderboard we need a single scalar benchmark score.\n4.5.2 Integrating Performance Profiles for the Benchmark Score\nTo calculate the scalar benchmark score B\u00afs of a submission \u00afs, we integrate its performance\nprofile up to a maximum ratio rmax:\nB\u00afs \u201c\n1\nrmax \u00b4 1\n\u017c rmax\n1\n\u03c1\u00afsp\u03c4q d\u03c4 .\n(3)\nSince we normalize by rmax \u00b4 1, B\u00afs P r0, 1s, with higher benchmark scores being better. A\nbenchmark score of B\u00afs \u201c 1 would indicate that the submission \u00afs was the fastest on every\nworkload. Table 13 presents the benchmark scores of the baselines shown in Section 7.\nWe set the upper integration limit to rmax \u201c 4, which also serves as the right-hand limit\nof any performance profile plot. This choice means that any submission that requires more\nthan four times the runtime of the fastest submission will not get any credit on this workload\nand will be treated the same as a training algorithm that is unable to successfully reach\n36\nBenchmarking Neural Network Training Algorithms\nthe target within the maximum allowed runtime budget. Although the exact integration\nbound is somewhat arbitrary, we want to encourage algorithms that are robust to different\nworkloads, and in practice we are likely to rank the best-performing submissions similarly\nfor most reasonable values of rmax. If there exists a specialized algorithm that is four times\nfaster than any generic training algorithm on a particular problem, it seems likely that\npractitioners will prefer it even if it is only useful for a small number of problems.\n4.5.3 Using Held-Out Workloads in Scoring\nThe benchmark score computation is based on a performance profile over only the fixed\nworkloads. However, we penalize submissions that perform poorly on the held-out work-\nloads. If a submission does not perform well enough on a given held-out workload, then we\nscore the submission on the corresponding fixed workload as if that submission did not reach\nthe fixed-workload target. Specifically, for a submission to get credit for a finite training\ntime on a particular fixed workload, it must:\n1. Reach the validation and test target on the fixed workload within the runtime budget.\n2. Reach the validation and test target on the fixed workload within 4\u02c6 of the fastest\nsubmission.\n3. Reach the validation and test target on the held-out workload corresponding to the\nfixed workload within the maximum runtime.\n4. Reach the validation and test target on the held-out workload corresponding to the\nfixed workload within 4\u02c6 of the fastest submission.13\nOnly if all four requirements are met does the submission get credit for a finite score on\nthat particular workload. Otherwise, a submission will have an infinite training time on\nthe fixed workload.\nThis rule means that being unable to successfully train a held-out\nworkload will disqualify a submission from getting credit for a good training time on the\ncorresponding fixed workload. We thus require submissions to be robust enough to not\ncompletely fail when faced with minor workload variations. This ensures that we prioritize\nthe fixed workloads for scoring since they are the most relevant version of that workload in\npractice. However, we also protect the benchmark from egregious workload-specific tuning\nand penalize brittle methods that break with slight modifications of the workload.\n4.5.4 Measuring Year-Over-Year Benchmark Progress\nScores derived from performance profiles make sense for relative comparisons within a set\nof submissions, but the benchmark scores Bs of the winning submission between different\niterations of the benchmark tell us nothing about how much faster training algorithms\nhave become in an absolute sense. In order to measure year-over-year progress in reducing\ntraining time, we can use the geometric mean across workloads of the raw training times\nfor the best submission.\n13. To determine the fastest submission on a held-out workload, we only consider submissions that reached\nthe target on the corresponding fixed workload. This protects us against extremely fast submissions that\nonly work on a specific held-out workload and are useless as general algorithms.\n37\nAlgoPerf Authors\nIn general, we recommend that anyone who reports results on the benchmark should\nreport raw training times in addition to any performance profiles and benchmark scores.\nThe raw scores allow other researchers to include the submissions in their own performance\nprofile comparisons or compute geometric means of any speedups if they want to measure\nprogress through time. Ideally, the raw times would be measured on the official competition\nhardware. However, if practical considerations require using a different system, additional\nbaseline results from previous work should also be reported to facilitate comparisons. In\nthe event that future iterations of the benchmark change the competition hardware, we can\nrun the most crucial previous submissions on the new system.\n5. Target-Setting Experiments\nAs a time-to-result benchmark (see Section 4.1), we need to set validation and test tar-\ngets for all workloads. To set the targets, we considered 4 training algorithms, AdamW,\nNadamW, Nesterov, Heavy Ball. We tuned all four target-setting training algorithms\nover relatively broad search spaces (see Table 8 for exact search spaces). For each algorithm,\nwe sampled 200 trials quasirandomly from its search space and selected the trial with the\nbest validation metric. Table 9 shows these resulting validation evaluation metric values for\neach algorithm, with the boldface values denoting the best for each workload.\nAs mentioned in Section 4.1, each algorithm ran for 0.75\u02c6the maximum runtime shown\nin Table 7. More precisely, since these four algorithms happen to have nearly identical step\ntimes, in these experiments we used a maximum number of steps as a proxy for runtime.14\nWe determined the maximum number of steps that would fit within the given runtime\nbudget, and used this same number of steps for every training algorithm.\nHyperparameter\nAdamW\nNadamW\nHeavy Ball\nNesterov\nBase LR\nLog r1e\u22125 ,1e\u22121 ]\nLog r1e\u22125 ,1e\u22121 ]\nLog r1e\u22123 ,10s\nLog r1e\u22123 ,10s\nWeight decay\nLog r1e\u22125 ,1s\nLog r1e\u22125 ,1s\nLog r1e\u22127 ,1e\u22122 ]\nLog r1e\u22127 ,1e\u22122 ]\n1 - \u03b21\nLog r1e\u22123 ,1s\nLog r1e\u22123 ,1s\nLog r1e\u22123 ,1s\nLog r1e\u22123 ,1s\n1 - \u03b22\nLog r1e\u22123 ,1s\nLog r1e\u22123 ,1s\nNA\nNA\nSchedule\nwarmup\nwarmup\nwarmup\nwarmup\n+ cosine decay\n+ cosine decay\n+ linear decay\n+ linear decay\n+ constant\n+ constant\nWarmup\nt2%, 5%, 10%u\nt2%, 5%, 10%u\n5%\n5%\nDecay factor\n-\n-\nt1e\u22122 ,1e\u22123 }\nt1e\u22122 ,1e\u22123 }\nDecay steps\n-\n-\nLinear r0.8, 1.0s\nLinear r0.8, 1.0s\nDropout\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nAux. dropout\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nLabel smoothing\nt0.0, 0.1, 0.2u\nt0.0, 0.1, 0.2u\nt0.0, 0.1, 0.2u\nt0.0, 0.1, 0.2u\nTable 8:\nHyperparameter search spaces for the target-setting training algo-\nrithms. Descriptions of the learning rate schedules can be found in Appendix A.1. The\nregularization hyperparameters are tuned only for those workloads where they are applica-\nble.\n14. On all workloads, the step time differences between these four optimizers are negligible compared to the\ntime required to complete one gradient calculation.\n38\nBenchmarking Neural Network Training Algorithms\nWorkload\nCriteo 1TB\nfastMRI\nImageNet\nImageNet\nDLRMsmall\nU-Net\nResNet-50\nViT\nMetric\nCE\u00d3\nSSIM\u00d2\nError Rate\u00d3\nError Rate\u00d3\nAdamW\n0.123675\n0.734330\n0.23034\n0.22614\nNadamW\n0.123609\n0.734523\n0.22702\n0.22534\nHeavy Ball\n0.125913\n0.733828\n0.22534\n0.24486\nNesterov\n0.126139\n0.734645\n0.22660\n0.24318\nWorkload\nLibriSpeech\nLibriSpeech\nOGBG\nWMT\nConformer\nDeepSpeech\nGNN\nTransformer\nMetric\nWER\u00d3\nWER\u00d3\nmAP\u00d2\nBLEU\u00d2\nAdamW\n0.078327\n0.114152\n0.277534\n30.6939\nNadamW\n0.077790\n0.113950\n0.280012\n30.8534\nHeavy Ball\n0.132797\n0.161977\n0.276148\n30.6431\nNesterov\n0.130823\n0.171137\n0.283124\n30.1074\nTable 9: Performance of the target-setting training algorithms on each of the\nworkloads. Each entry contains the validation performance for the best tuning trial for a\ngiven algorithm and workload. We often see some \u201creversion to the mean\u201d when rerunning\nthis trial 20 times (cf.\nTable 23).\nThe boldface values indicate the best result in each\ncolumn.\n39\nAlgoPerf Authors\nAfter running the 200 hyperparameter trials, for every workload we took the best con-\nfiguration (training algorithm and hyperparameter settings) on the validation set (detailed\nin Table 10) and retrained it 20 times with different random seeds. The final validation\ntargets were the median values achieved over these 20 repetitions, while the test targets\nwere the worst-case test set performance achieved across those 10 repetitions that hit the\nvalidation target. The results of these 20 repetitions, for each workload, along with the as-\nsociated training algorithm, hyperparameter settings, and additional statistics are included\nin Tables 23 and 24 in Appendix B. Table 7 shows the final targets for all fixed workloads.\nCriteo 1TB fastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall U-Net\nResNet-50 ViT\nConformer DeepSpeech GNN\nTransformer\nAlgorithm\nNadamW\nNesterov Heavy Ball NadamW NadamW\nNadamW\nNesterov NadamW\nBase LR\n0.003331\n0.028609\n4.131896\n0.000844\n0.001308\n0.004958\n2.491773\n0.001749\nWeight decay 0.003578\n0.000577\n5.67e-6\n0.081354\n0.163753\n0.114739\n1.29e-7\n0.081216\n\u03b21\n0.948\n0.981543\n0.927476\n0.889576\n0.973133\n0.863744\n0.944937\n0.932661\n\u03b22\n0.998793\n0.99785\n0.998123\n0.629185\n0.995516\nWarmup\n2%\n5%\n5%\n5%\n10%\n2%\n5%\n2%\nDecay factor\n-\n0.01\n0.001\n-\n-\n-\n0.001\n-\nDecay steps\n-\n0.984398\n0.900777\n-\n-\n-\n0.861509\n-\nDropout\n0.1\n0\n0\n0\n0\n0\n0.1\n0.1\nAux. dropout -\n-\n-\n-\n0\n0.1\n-\n0.1\nLabel\n-\n-\n0.2\n0.2\n-\n-\n0\n0\nsmoothing\nTable 10: Optimal training algorithm and hyperparameter settings used for tar-\nget setting on each workload.\nThe reported configuration (training algorithm and\nhyperparameter setting) achieved the best performance on the validation set.\nFor the randomized workloads, we used nearly an identical procedure to set targets\nfor each workload variant. To save computational resources, we only tuned two training\nalgorithms instead of four. For each workload variant, we used NadamW and the other\nbest-performing training algorithm on the corresponding base workload.\nComparison to results in the literature\nThe ideal target-setting procedure would\nproduce targets that compare favorably to results reported in the literature for similar\nsetups, although in many cases it will not be possible to find published results that are\nan exact match for our workloads (i.e. training budget, evaluation metrics, data prepara-\ntion, or model architecture might be different). Therefore, this comparison is neither a\ncomprehensive literature review, nor\u2014by itself\u2014a direct assessment of our target-setting\nprocedure. Instead, our goal is to provide context for a holistic evaluation of the relevance\nand competitiveness of our targets.\n\u2022 Criteo 1TB On this workload, our target-setting procedure resulted in a validation\ntarget (binary cross entropy) loss of 0.123649 and a test target loss of 0.126060. In\ncomparison, Sterbenz (2017) reported a combined loss on our validation and test sets\nof 0.1250 (our combined loss would be 0.1248545), leading to a reported AUC score of\n0.8002. It is worth noting that Sterbenz (2017) used a larger model and trained it for\nmore than three times as long as we did. NVIDIA (2023) trained a model with the\n40\nBenchmarking Neural Network Training Algorithms\nsame dimensions as the one used in our workload for one epoch and reported AUC\nscores on our validation set between 0.802509 and 0.802784. However, it is important\nto note that their test datasets used frequency thresholding, which we did not apply.\n\u2022 fastMRI The target-setting procedure achieved an SSIM of 0.7344 on our validation\nset and 0.741652 on our test set. The paper introducing the fastMRI benchmark\nprovided a U-Net baseline with an SSIM score of 0.72 on our combined validation\nand test set (Zbontar et al., 2018, Table 8, note that model selection was done using\nNMSE and not SSIM), compared to our slightly better combined score of 0.738. In\nthe 2019 fastMRI challenge (Knoll et al., 2020), the winning submission for the\nsingle-coil knee dataset achieved an SSIM of 0.751 on our combined validation and\ntest set, using a custom i-RIM model (Putzky et al., 2019).\n\u2022 ImageNet For the ResNet-50 workload, our target error rate on the validation set\nof 22.57% improves over the performance reported in the original ResNet paper by\nHe et al. (2016a) (their most similar setup reaches 24.7%). More recent studies have\nachieved lower error rates by using variants of the original ResNet architecture or\nby employing improved training recipes. Bello et al. (2021, ResNet-RS-50 in Table\n7) report a validation error of 21.2%, and Wightman et al. (2021, A2 in Table 1)\nreport an error of 20.2% when training for roughly 2.67\u02c6 the training budget of our\nworkload. With a slightly shorter training time than our workload, Wightman et al.\n(2021, A3 in Table 1) achieved an error rate of 21.9% using a lower training resolution,\nRandAugment, CutMix, and Mixup compared to our workload.\nFor the ViT workload, our target-setting procedure yielded a validation error rate of\n22.69%. The paper introducing Vision Transformers reported an error rate of 22.09%\nfor a slightly larger model trained exclusively on ImageNet (Dosovitskiy et al., 2021,\nViT-B/16 in Table 5). We can compare our results with a training budget of roughly\n112 epochs, to Beyer et al. (2022), which reported error rates of 23.5% and 21.5% for\nthe same ViT-S/16 model used in our workload for a training budget of 90 and 150\nepoch respectively (note that they used our validation set as a test set and trained\nonly on 99% of the training data to use the remaining 1% as their validation set).\n\u2022 LibriSpeech Our target-setting procedure for the LibriSpeech dataset resulted in\na validation word error rate (WER) of 0.1162 for the DeepSpeech workload and\n0.078477 for the Conformer workload. For easier comparison with the literature,\nwe can consider the WERs on our test set (the test clean split), which are 0.067976\nand 0.046696 for DeepSpeech and Conformer, respectively. The original paper\nintroducing the LibriSpeech dataset reported a baseline with a WER of 0.0551\n(Panayotov et al., 2015, Table 3), although the model uses a completely different ap-\nproach than our workloads. Subsequently, Amodei et al. (2016, Table 4) reported an\nimprovement to 0.0515 with the DeepSpeech 2 model. The DeepSpeech 2 model\nuses a much larger training set beyond just LibriSpeech and uses beam search de-\ncoding, unlike our DeepSpeech workload. Gulati et al. (2020, Table 2, Conformer(S)\nwithout LM) introduced the Conformer architecture and reported 0.027 test WER.\nHowever, Gulati et al. (2020) used a much larger Conformer model, used beam\nsearch decoding instead of greedy decoding, trained for more steps, and included a\nfew other more minor model differences.\n41\nAlgoPerf Authors\n\u2022 OGBG The target-setting procedure for the OGBG workload resulted in a mean\naverage precision (mAP) on the validation set of 0.28098.\nWhen introducing the\nOpen Graph Benchmark (OGB), Hu et al. (2020) also provided six baselines\nfor the ogbg-molpcba dataset used in our workload. The best baseline reached a\nvalidation mAP of 0.2798. Subsequent approaches were able to achieve higher results\nwith different models and training techniques, such as a 0.3012 mAP by Wang et al.\n(2022), or 0.3252 mAP using additional training data (Wang et al., 2021).\n\u2022 WMT For our WMT workload, we follow the setup adopted by the Flax (Heek et al.,\n2023) WMT example. WMT datasets for different years aim to establish benchmarks\nfor different problems in the machine translation domain, e.g. low-resource transla-\ntion, domain & style of translation, or long-sequence translation. Consequently, over\nthe past decade, the neural machine translation literature has used different combina-\ntions of WMT datasets depending on the desired language pair, recency of the data,\namount of data, and underlying translation problem. Since our goal is to provide a\ntraining algorithms benchmark, we decided to stay close to a high quality open source\nexample (in this case the Flax WMT example). Specifically, the models are trained\non \u201ctrain\u201d split of WMT2017 translation dataset (Bojar et al., 2017) for German\nto English (De \u00d1 En), use the \u201cdev\u201d set from the WMT2014 dataset (Bojar et al.,\n2014) and use newstest2014 as the test set. Our models achieve a BLEU score of\n30.72 on newstest2014. Gao et al. (2022, Table 10) also evaluate their models on\nnewstest2014 De \u00d1 En, and achieve higher BLEU scores (in the range 33.60 \u2013 35.15).\nHowever, they pre-train the models in a bidirectional manner (De\u00d1En and En\u00d1De)\nbefore fine-tuning on the De\u00d1En translation direction. Additionally, Ma et al. (2023,\nTable 6) evaluate their models on newstest2014 De\u00d1En direction and report BLEU\nscore in the range 31.33 \u2013 32.35, although they made architectural changes to the\nattention mechanism.\n6. Randomized Workloads Experiments\nThe primary goal of our randomized workloads is to help deter brittle submissions that\nonly work on the original\u2014and relatively standard\u2014fixed workloads, and instead encourage\nmore robust training algorithms that also perform well on novel deep learning workloads. An\nadditional concern is that by using only a small set of 8 (fixed) workloads, submissions in the\nexternal tuning ruleset (which are permitted to sample 20 different hyperparameter points\nper workload) could effectively perform workload-specific tuning. This risk is amplified since\nwe permit submissions with a fixed hyperparameter list (i.e., OptList approaches) instead\nof search spaces. For instance, a submission could use a hyperparameter list of the best\nhyperparameter configurations for each fixed workload. Such approaches could potentially\nlead to generally useful training algorithms and thus should be allowed. However, unless it\ngeneralizes to novel workloads the expensive offline computation it would require cannot be\njustified. Without held-out workloads, the benchmark would not test whether submissions\nthat tune over a list of configurations that perform well on the fixed workloads can generalize\nto workloads outside of the 8 fixed workloads. To encourage robust submissions and avoid\nvitiating the limits on workload-specific tuning, we manually created three variations of\neach fixed, base workload. The three variants together form a randomized workload that\n42\nBenchmarking Neural Network Training Algorithms\nwill be used to sample one specific concrete variant to use as a held-out workload during\nscoring. Since held-out workload sampling occurs after all submission code has been frozen,\nsubmissions may need to perform well on any of the possible variants.\n6.1 Desiderata for Workload Variants\nAlthough any workload change could potentially pose challenges for some hypothetical\nsubmissions, an ideal workload variant intended for use as part of a randomized workload\nwould have the following properties.\nRepresentative of real workloads\nThe best workload variants would be as representa-\ntive as possible of changes that occur in practice. Extremely contrived changes to the base\nworkload do not help the community develop robust and general training algorithms. The\nmost natural changes are ones that already occur in the wild (e.g., architectural changes\ndescribed in the literature). However, when judging how natural a particular modification\nis for a workload, it is important to distinguish between changes that affect the optimization\ndynamics and more surface level architectural changes. Even if we would never expect a\nparticular change to be applied in a real application, if it results in optimization dynamics\nthat do occur in the wild, it might still be worth considering. The \u201cattention temperature\u201d\nchange described below is arguably a bit contrived in terms of model architecture, but it\nreproduces a type of training instability that occurs in practice, especially as Transformer\nmodels become larger, and therefore training algorithms that handle it well could be useful\n(Gilmer et al., 2023; Kim et al., 2021; Dehghani et al., 2023).\nTrainable in the original runtime budget\nWe want workload variants that are train-\nable. Furthermore, they should reach reasonable validation and test evaluation metric values\nwithin the runtime budget of the base workload. Specifically, there should exist a training\nalgorithm with some hyperparameter setting that achieves good results. It is very easy to\nmake changes that completely ruin a workload and produce a model incapable of performing\nwell on its task no matter how it is trained, but these models are not useful. Exhibiting a\nconfiguration of the hyperparameters that achieves a good validation error on the original\ntask proves that it is possible for the workload variant to yield a useful trained model.\nDistinct from the base workload (and other variants)\nA workload variant only\nadds information to the benchmark if it is distinct enough from the original fixed workload\nit is based on. Ideally, the entire pool of variants of a given workload would be mutually\ndistinct from each other\u2014and the base workload\u2014while each presenting an interesting new\nchallenge for submissions. Although there are many ways to define distinctiveness, we tried\nto create variants that meaningfully changed the optimal hyperparameters of the training\nalgorithms used during target setting. In this way, we hoped to encourage submissions that\nare easier to tune than current popular techniques that use straightforward search spaces,\nas represented by our target-setting algorithms.\nAchieving all the desiderata listed above simultaneously, on command, is unfortunately\ndifficult. Nonetheless, workload variants that meet these requirements definitely exist, espe-\ncially if we make a few practical concessions, so giving up does not seem appropriate either.\nOriginally, we hoped to specify randomized workloads via distributions with support over\na very large number of variants by randomizing different pieces of the workload definition.\n43\nAlgoPerf Authors\nHowever, without a better scientific understanding of the effects of various workload modifi-\ncations, constructing such distributions was simply too onerous (see Appendix E for details\non some of our attempts), which is why we elected to manually design a small number of\nspecific, concrete workload variants instead. Even this goal was more challenging than we\nexpected, but not because it is impossible to construct modifications that require re-tuning\nthe hyperparameters.\nInstead, the challenge in designing interesting workload variants\ncomes from the conjunction of requirements that need to be achieved simultaneously.\n6.2 Creating and Testing Workload Variants\nWhile creating workload variants, we explored various modifications to the base workloads.\nThe following families of modifications ended up being used in the benchmark variants:\n\u2022 Activation function: Most base workloads employed ReLU as the activation func-\ntion and we explored alternative activation functions such as GELU, SiLU, or TanH.\n\u2022 Pre-LN vs Post-LN: For Transformer-based models, the base workload was usually\nthe Pre-Layer Norm (Pre-LN) (Xiong et al., 2020) version. We changed these to\nPost-Layer Norm (Post-LN, see Figure 3).\n\u2022 Attention temperature: For the WMT Transformer, we modified the attention\nlayers to compute Softmax\n\u02c6\ncXW QpXW KqJ\n?\nD{H\n\u02d9\nwhere c is a constant scalar denoting the\nattention temperature. The default self-attention implementation sets c \u201c 1. In order\nto artificially induce instabilities similar to those faced by larger versions of these\nmodels, we set c \u201c 1.6.\n\u2022 Initialization scales: For the DLRM model, changing the scale of the initial weights\nof the embedding layer resulted in a variant. For the ResNet model, changing the\ninitial batch normalization layer scale weights resulted in a workload variant.\n\u2022 Normalization layer: We changed the type of normalization layer employed in\nthe model. Common changes included interchanging batch normalization with layer\nnormalization, as well as instance normalization with layer normalization.\n\u2022 Width, depth, and channels: We explored changing model width, depth, and\nnumber of channels, as applicable.\n\u2022 Input pipeline: On LibriSpeech, we found changing SpecAugment strength to\nbe an effective strategy.\n\u2022 Residual connection structure and scaling: For the DLRM model, we created a\nvariant with additional residual connections. For DeepSpeech, we removed residual\nconnections from the model.\n\u2022 Pooling layer type: Changing the pooling layer type from global average pooling to\nmax average pooling resulted in a variant for the ViT workload.\nWe used a relatively permissive protocol to decide whether a candidate change to a work-\nload produced an acceptable variant, since strictly achieving the strongest versions of all of\nour desiderata is quite difficult. Specifically, we tested whether the optimal hyperparam-\neter setting for one of the more robust, standard algorithms (NadamW) were sufficiently\n44\nBenchmarking Neural Network Training Algorithms\ndifferent between the candidate variant and its base workload.\nWe chose NadamW as\nthe training algorithm to measure variant distinctiveness because, in our experiments with\nour search spaces, its optimal hyperparameters tended to transfer much better than, for\ninstance, Heavy Ball or Nesterov. NadamW also happened to be the algorithm that\nset targets on the most workloads (5{8).\nIn addition to testing distinctiveness, we also rejected variants that degraded validation\nperformance too much. As a rule of thumb, we accepted workload variants for which we\ncould achieve a performance within 10% of the original validation performance (in the\noriginal target setting budget). More precisely, given a base workload w and a candidate\nworkload variant w1, we re-ran the exact same collection of 200 hyperparameter settings\nthat were used for target-setting (Table 8) for NadamW on the candidate variant. Let H \u201c\nth1, . . . h200u denote this set of points in hyperparameter space. Furthermore, let Hpwq \u201c\nthipwqu200\ni\u201c1, where hipwq denotes the validation evaluation metric of hyperparameter setting\nhi on workload w. Let h\u02dapwq and h\u02dapw1q denote the best hyperparameter setting for w and\nw1, respectively. Additionally, let rankph, Hpwqq denote the rank of the hyperparameter\nsetting h in the set Hpwq when the elements are ordered according to validation performance\non workload w (lower ranks indicate better performance). In general, we sought variants\nw1 for which the following quantity was as large as possible:\nmin\n\u2423\nrank\n`\nh\u02dapwq, Hpw1q\n\u02d8\n, rank\n`\nh\u02dapw1q, Hpwq\n\u02d8(\n.\nIn other words, when both ranks are greater than c, then the optimal hyperparameter setting\non w performs worse on w1 than at least c other hyperparameter points, and the optimal\nhyperparameter setting on w1 performs worse on w than at least c other hyperparameters\nas well. Note, if w and w1 share the same optimal hyperparameter, then these ranks will be\n0. In the rest of this section, to simplify notation we write rankpw \u00d1 w1q and rankpw1 \u00d1wq\ninstead of rankph\u02dapwq, Hpw1qq and rankph\u02dapw1q, Hpwqq, respectively.\nUltimately, we used human judgement instead of strict thresholds to decide if rankpw\u00d1\nw1q and rankpw1 \u00d1wq were large enough for a given candidate variant and if it achieved a\ntolerable validation performance. In some cases (for the ResNet and Conformer models)\nwe did not find variants that substantially changed the rank of the optimal hyperparameter\npoint.\nIn these cases, we fell back to creating variants that had different optimal base\nlearning rates in simple one-dimensional learning rate sweeps. See Appendix D for additional\ndetails and results from our variant testing protocol.\nOur procedure for testing and rejecting candidate variants is not guaranteed to produce\na set of variants that achieve all of our desiderata. Although, by design, all our variants are\nat least somewhat representative of real workloads and are trainable in the original budget\n(perhaps with some degradation in performance), we potentially sacrificed distinctiveness.\nWe made no attempt to ensure that different variants of the same base workload were\nmutually distinct from each other in terms of optimal hyperparameters, although we did\nnot repeat the same modifications within a set. Furthermore, our operational definition\nof distinctiveness depends on the particular set of hyperparameter points we used and our\nchoice of training algorithm. Finally, we did not always produce variants that made very\ndrastic changes to the ranks of the best hyperparameter points.\n45\nAlgoPerf Authors\n6.3 Workload Variants of the Benchmark\nTable 11 contains a brief description of the changes that produced the three variants of every\nfixed workload. See Appendix D for additional details about the changes and variants, as\nwell as workload-specific results of the variant testing protocol (described above). Although\nit was hard to predict how candidate variants would perform in our tests, in hindsight\nsome patterns emerged. For example, after switching the activation function from ReLU\nto GELU in the Conformer model, higher learning rates performed better (and in many\ncases the resulting model performance itself was improved).\nWe found producing variants which successfully changed the optimal NadamW hy-\nperparameters to be surprisingly difficult. This speaks to the robustness of NadamW to\nworkload variations, a desirable property that perhaps helps explain the success of precon-\nditioned training algorithms. In contrast, the optimal learning rate for momentum methods\nis highly sensitive to seemingly trivial variations in the workload (e.g. the Wide ResNet\nstride change experiment in Section 2.2.1). Note, this does not mean that the performance\nof NadamW was completely robust to changes in the workloads. In fact, it was quite easy\nto design a workload variant which was more difficult for NadamW to optimize. However,\nfor many such variants the optimal NadamW hyperparameters did not change.\nThere is at least one known case for which we expect the optimal NadamW hyperpa-\nrameters to change: when the batch size changes (Nado et al., 2021). However, this was\nnot a valid option for us because the submission hardware is fixed and the submitter is al-\nlowed to choose the batch size. We were initially hopeful that other methods for increasing\nstochasticity in optimization problem would decrease the optimal NadamW learning rate.\nIndeed, although it isn\u2019t part of the workload definition, changing the value of dropout can\nhave this effect. However, in many cases when we tried to achieve similar effects by increas-\ning the severity of data augmentation or adding label noise, we did not see a significant\nchange in the optimal learning rate, although we did manage to hurt overall performance.\nAnother surprising failure was playing with residual scales. For example we tried mod-\nifying the traditional residual connection of x`Fpxq to \u03b1x`p1\u00b4\u03b1qFpxq for \u03b1 P r0, 1s. By\nsweeping \u03b1 between 0 and 1, we can interpolate between a model with no residual connec-\ntions (\u03b1\u201c0) to the default setting \u03b1\u201c0.5 to a model which ignores all intermediate blocks\n\u03b1 \u201c 1.0\u2014varying \u03b1 between these 3 extremes allows us to explore interesting variations\nof residual connections. Indeed we found \u03b1 to be a very important parameter for overall\nperformance (somewhat surprisingly \u03b1 \u201c 0.5 was not always the optimal value), however\nthe optimal learning rate for any given \u03b1 did not seem to change. We also tried \u03b1x`Fpxq\nwhich did not work either, though did improve performance for \u03b1\u201c2 and \u03b1\u201c4.\nFinally we would like to highlight some of our variants that actually outperform the base\nworkload in validation error. The changes which we saw leading to improvement in models\nwere changing from ReLU to GELU (ResNet), changing from ReLU to SiLU (ResNet\nand GNN), introducing GLU (ViT) and removing residual connections (DeepSpeech).\n7. Baseline Submissions\nWe constructed baseline submissions using eight different training algorithm families:\nAdamW, NadamW, Nesterov, Heavy Ball, LAMB, Adafactor, SAM(w. Adam)\nand Distributed Shampoo. For each training algorithm family, we designed one or more\n46\nBenchmarking Neural Network Training Algorithms\nBase\nVariant\nValidation\nTest\nWorkload\nVariant\nDescription\nTarget\nTarget\nCriteo 1TB\nDLRMsmall\nEmbed Init Scale\nChanges initialization scale of the em-\nbedding layer from 1{\n?\nvocab size to 1\n0.124286\n0.126725\nLayerNorm\nAdds layer normalization to the network\n0.123744\n0.126161\nResidual\nAdds residual connections to the network\n0.124027\n0.126470\nfastMRI\nU-Net\nChannels & Pooling\nIncreases number of channels and de-\ncreases number of pool layers\n0.734376\n0.741547\nTanH\nReplaces all Leaky ReLU activations\nwith TanH\n0.729633\n0.736727\nLayerNorm\nReplaces\ninstance\nnormalization\nwith\nlayer normalization with learnable pa-\nrameters\n0.734861\n0.741982\nImageNet\nResNet-50\nSiLU\nReplaces all ReLU activations with SiLU\n0.220090\n0.342600\nGELU\nReplaces\nall\nReLU\nactivations\nwith\nGELU\n0.220770\n0.340200\nBN Init Scale\nIncreases the scale of the initialization of\nbatch normalization scale variables\n0.234740\n0.357700\nViT\nPost-LN\nUses Post-LN instead of Pre-LN\n0.246880\n0.371400\nMAP\nChanges pooling type from global to max\naverage\n0.228860\n0.347700\nGLU\nInclude GLU in the MLPBlock\n0.223300\n0.345500\nLibriSpeech\nConformer\nGELU\nReplaces\nall\nReLU\nactivations\nwith\nGELU\n0.077958\n0.047643\nLayerNorm Change\nThe LayerNorm before the final readout\nlayer was removed\n0.085371\n0.053096\nAttention Temp\nIncreases attention temp from 1 to 1.6\n0.082665\n0.050168\nDeepSpeech\nTanH\nReplaces all ReLU activations with TanH\n0.133449\n0.079810\nNo Residual\nRemoves residual connections.\n0.105042\n0.060388\nNorm & SpecAugment\nRemoves\ndecoder\nlayer\nnormalization\nlayer & replaces batch normalization\nwith\nlayer\nnormalization.\nChanges\nSpecAugment specifications.\n0.131553\n0.082442\nOGBG\nGNN\nGELU\nReplaces\nall\nReLU\nactivations\nwith\nGELU\n0.277710\n0.262926\nSiLU\nReplaces all ReLU activations with SiLU\n0.282178\n0.272144\nAltered Layers\nAdds a hidden layer, decreases latent di-\nmension.\nReduces the number of mes-\nsage passing steps and changes layer nor-\nmalization to batch normalization.\n0.269446\n0.253051\nWMT\nTransformer\nPost-LN\nUses Post-LN instead of Pre-LN\n30.2003\n29.8982\nAttention Temp\nIncreases attention temperature from 1\nto 4.0\n30.0756\n29.8094\nGLU & TanH\nUses GLUs in the MLP blocks and re-\nplaces all ReLU activations to TanH\n30.0002\n29.8139\nTable 11: Overview of workload variants used for randomized workloads. See\nAppendix D for additional details of each workload variant. Targets that are better than\nthe corresponding base workload target are in bold. Unsurprisingly, the variants where the\nvalidation targets improved were the same as the ones where the test targets improved.\n47\nAlgoPerf Authors\nsearch spaces to create valid submissions for the external tuning ruleset. For AdamW,\nNadamW, Nesterov and Heavy Ball, we compared submissions derived from search\nspaces that fixed the relevant first moment parameter (\u03b21) to a default value with submis-\nsions that tuned it, denoted by names ending with fixed \u03b21 or tuned \u03b21, respectively.\nAdditionally, for these four training algorithm families, we also constructed baselines that\nuse a list of 20 specific hyperparameter configurations to sample from, without replace-\nment. We denote baselines making use of these kinds of search spaces by names ending\nwith OptList (e.g. AdamW OptList).\n7.1 Baseline Creation Procedure\nWe used the results of the target-setting experiments to guide the creation of search spaces\nfor baseline submissions. Since LAMB, Adafactor, SAM(w. Adam) and Distributed\nShampoo were not used in the target-setting procedure, we collected similar tuning data\nfor them by sampling 200 trials from a broad search space for each algorithm. We used\nsearch spaces with the same ranges as in Table 8, except defined over the analogous hyperpa-\nrameters in LAMB, Adafactor, SAM(w. Adam) and Distributed Shampoo (i.e. the\nAdamW 1 \u00b4 \u03b21 range was used as the 1 \u00b4 \u03b21 range for the analogous first moment parame-\nters in the other algorithms). SAM(w. Adam) has an additional \u03c1 hyperparameter that we\nsearch over using the recommended discrete search space of t0.01, 0.02, 0.05, 0.1, 0.2, 0.5u.\nSince the target setting procedure violates the submission rules by using too many tuning\ntrials, we needed to tighten the search spaces used in the 200-trial searches to produce\nstrong baselines for the 20 trial budget allowed in the external tuning ruleset. We used the\nfollowing recipe to produce narrower search spaces for all baseline submissions (see Table 12\nfor the resulting exact search spaces).\n1. Learning rate and weight decay: For a given algorithm, for every fixed workload,\nwe first found the hyperparameter setting with the best validation error among the 200\ntrials, yielding a set of at most 8 hyperparameter points (up to one per fixed workload).\nWe set the boundaries for the search spaces for the (base) learning rate and weight\ndecay to include the maximum and minimum values observed in this set, plus a little\nextra on each side. While this procedure led to a small search space for preconditioned\nalgorithms such as AdamW and NadamW, the search spaces for Heavy Ball and\nNesterov were prima facie too wide to allow for efficient sampling, so we narrowed\nthem by excluding outliers when building the set of per-workload hyperparameter\npoints. In particular, we removed hyperparameter points from consideration when we\ncould find an alternative hyperparameter point in the target-setting experiments with\nvery similar performance, but within our narrowed-down search space.\n2. \u03b21 and \u03b22: As mentioned above, some baseline submissions searched over \u03b21 (e.g.\nAdamW tuned \u03b21, Heavy Ball tuned \u03b21, etc.) and others fixed it to a default\nvalue (e.g. AdamW fixed \u03b21).15\nWhen tuning \u03b21, we followed the same proce-\ndure as described for the learning rate to find the extreme values among the best\nper-workload hyperparameter settings, and set the search space to be within these\n15. For the purposes of this paper, we call the momentum parameter \u03b21 in Heavy Ball and Nesterov\nalthough it obviously is a different hyperparameter in each of those algorithms (as well as in AdamW).\n48\nBenchmarking Neural Network Training Algorithms\n(rounded) bounds. As is common, we reparameterized the search space to explore\n1 \u00b4 \u03b21 in logspace. For \u03b22 (where applicable) we used its default value of 0.999.\n3. Learning rate schedule parameters: We used a fixed 5% linear warmup from 0.0\nfor all runs.16 Baselines using the Linear Decay + Constant schedule family always\nsearched the decay factor in the set t0.01, 0.001u (as in target-setting). In target-\nsetting, both of these values appeared in top-performing hyperparameter settings in\nroughly equal proportion. For the decay steps factor parameter, we used a fixed value\nof 0.9; we saw the best trials in the 200-trial searches concentrated near that value\nand preliminary experiments seemed to suggest tuning it did not matter very much.\n4. Regularization Parameters: For dropout, we noticed that, in the best trials,\ndropout and aux. dropout parameters tended to be the same (either both 0.0 or\nboth 0.1). Therefore, we tied these parameters together in our baselines and searched\nthem in the discrete set t0.0, 0.1u. For label smoothing, we noticed that whenever\nthe workload allows it, the best-performing trials used either 0.1 or 0.2, therefore we\nsearched within that set whenever the workload supported label smoothing.\n5. OptList baselines: To build OptList baselines, we ranked the 200 hyperparameter\nconfigurations from the broad searches independently on each of the 8 fixed workloads.\nThen we greedily grew a set of 20 hyperparameter configurations by cycling through\nworkloads in an arbitrary, round-robin order and adding the top configurations on\neach workload that wasn\u2019t already in the set.\n7.2 Baseline Timing\nIn order to run more extensive experiments, we used Google TPUs (Jouppi et al., 2020)\nwhenever we could, thus deviating from the official, 8\u02c6NVIDIA V100 GPUs benchmark\nsystem. To estimate training time for our baselines on the official benchmark system, we\ntimed each training algorithm on the true competition system, on each workload, for a\nreduced number of training steps.\nOur timing measurements have some noise, possibly\nexacerbated by non-local disks. However, variations smaller than the time between off-\nthe-clock evaluations (typically about 1% of total allowed runtime) are unlikely to have a\nlarge effect on the benchmark results. To reduce the time needed on the official benchmark\nsystem, we ran each algorithm (except Distributed Shampoo) for 20% of the allowed\nnumber of steps, extrapolated to the full number of steps, and averaged across two runs\non two different machines (see Table 28 in the Appendix for the timing results). As of the\ntime of writing, we were not able to run Distributed Shampoo on all workloads on the\nofficial benchmark system due to various memory and configuration issues that we hope\nto correct in future versions of this work.\nTherefore, we omit Distributed Shampoo\nwhen presenting results with respect to runtime. Since we were able to run Distributed\nShampoo on TPUs, we have included Distributed Shampoo in any results that don\u2019t\nrequire runtime measurements on the official benchmark system.\nAll the baseline algorithms should take roughly the same time per step, except for\nSAM(w. Adam) and Distributed Shampoo. In practice, SAM(w. Adam) and Dis-\n16. Although using a learning rate of zero on the first step is somewhat perverse, it simplifies the logic and\nis common in many codebases.\n49\nAlgoPerf Authors\nHyperparameter\nAdamW\nNadamW\nHeavy Ball\nNesterov\nBase LR\nLog r1e\u22124 ,1e\u22122 ]\nLog r1e\u22124 ,1e\u22122 ]\nLog r1e\u22121 ,10s\nLog r1e\u22121 ,10s\nWeight decay\nLog r5e\u22123 ,1s\nLog r5e\u22123 ,1s\nLog r1e\u22127 ,1e\u22125 ]\nLog r1e\u22127 ,1e\u22125 ]\n1 - \u03b21[Default]\n0.1\n0.1\n0.1\n0.1\n1 - \u03b21[Tuned]\nLog r2e\u22122 ,0.5s\nLog r4e\u22123 ,0.1s\nLog r5e\u22123 ,0.3s\nLog r5e\u22123 ,0.3s\n1 - \u03b22\n0.999\n0.999\nNA\nNA\nSchedule\nwarmup\nwarmup\nwarmup\nwarmup\n+ cosine decay\n+ cosine decay\n+ linear decay\n+ linear decay\nWarmup\n5%\n5%\n5%\n5%\nDecay factor\nNA\nNA\nt1e\u22122 ,1e\u22123 }\nt1e\u22122 ,1e\u22123 }\nDecay steps\nNA\nNA\n0.9\n0.9\nLabel smoothing\nt0.1, 0.2u\nt0.1, 0.2u\nt0.1, 0.2u\nt0.1, 0.2u\nDropout (Tied)\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nHyperparameter\nLAMB\nAdafactor\nSAM(w. Adam)\nDistributed Shampoo\nBase LR\nLog r1e\u22124 ,1e\u22122 ]\nLog r1e\u22124 ,1e\u22122 ]\nLog r1e\u22124 ,1e\u22122 ]\nLog r1e\u22124 ,1e\u22122 ]\nWeight decay\nLog r1e\u22123 ,1s\nLog r1e\u22123 ,1s\nLog r1e\u22122 ,0.2s\nLog r5e\u22123 ,1s\n1 - \u03b21[Default]\n0.1\n0.1\n0.1\n0.1\n1 - \u03b21[Tuned]\nLog r2e\u22122 ,0.5s\nLog r1e\u22122 ,0.45s\nLog r5e\u22122 ,0.43s\nLog r1e\u22122 ,0.15s\n1 - \u03b22\n0.999\n0.999\n0.999\n0.999\n\u03c1\nNA\nNA\nt0.01, 0.02, 0.05u\nNA\nSchedule\nwarmup\nwarmup\nwarmup\nwarmup\n+ cosine decay\n+ cosine decay\n+ cosine decay\n+ cosine decay\nWarmup\n5%\n5%\n5%\n5%\nDecay factor\nNA\nNA\nNA\nNA\nDecay steps\nNA\nNA\nNA\nNA\nLabel smoothing\nt0.1, 0.2u\nt0.1, 0.2u\nt0.1, 0.2u\nt0.1, 0.2u\nDropout (Tied)\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nt0.0, 0.1u\nTable 12: Hyperparameter search space for the baseline submissions. Descriptions\nof the learning rate schedules can be found in Appendix A.1. The regularization hyper-\nparameters are tuned only for those workloads where they are applicable. Dropout and\naux. dropout are set to the same value.\n50\nBenchmarking Neural Network Training Algorithms\ntributed Shampoo typically require 1.5\u02c6\u20132\u02c6 the time per step (for Distributed\nShampoo this measurement is on TPU), depending on the workload. Note however that\nDistributed Shampoo can amortize these costs by increasing the batch size which we\nleave for future work as it requires careful work for fair comparisons. Although all of our\nmain, target-setting training algorithms had timing results in line with our expectations,\nspecifically on the competition hardware using GPUs, our implementation of Adafactor\ntook around 10-20% more time per step than AdamW on multiple workloads. In contrast,\non TPU, Adafactor step times were much closer to AdamW, although they still seemed\nslightly slower. Due to these unexpectedly slower step times and the max runtime limit, our\nAdafactor baseline misses a couple of targets that it would be able to hit if it ran for the\nsame number of steps as AdamW. Although Adafactor was not one of the target-setting\nalgorithms and we did not find any obvious issues in the implementation, we would like to\ninvestigate this slowdown more in the future.\nOn the competition hardware, using our current implementations, three out of the eight\nworkloads, namely Criteo 1TB DLRMsmall, fastMRI U-Net and OGBG GNN, are\ndata-pipeline-bound when training with any of the target-setting algorithms.\nIn other\nwords, reading, preprocessing, and preparing batches of training data takes much longer\nthan computing gradients and weight updates. The exact bottleneck varies across work-\nloads, and isn\u2019t necessarily transferring data from disk. For example, on OGBG GNN\ncreating and padding batches of graphs is a bottleneck. On data-pipeline-bound workloads,\ntechniques such as data echoing (Choi et al., 2019a) can accelerate training. Additionally,\non such workloads, optimizers that perform more work per batch and might otherwise in-\ncrease the time-per-step (e.g. SAM(w. Adam)) can reclaim idle accelerator time. Although\ndata-pipeline-bound workloads exist in the wild and are a legitimate part of a representative\nbenchmark suite,17 as researchers iterate on a specific workload, they tend to make changes\nthat reduce, or eliminate, idle accelerator time. For example, they might optimize the input\npipeline code, run multiple independent copies of the data pipeline in parallel, or switch to a\nlarger, more computationally intensive model. For these reasons, input pipeline bottlenecks\nare best viewed not as an immutable property of the problem, but instead as a consequence\nof a particular model size, amount of engineering resources, and amount of computational\nresources. Although a small number are fine, we generally view data-pipeline-bound work-\nloads as undesirable for a training algorithms benchmark because such workloads tend to\nfavor expensive training algorithms that wouldn\u2019t be as useful when we have the resources\nto remove the bottleneck. In the future, we would like to optimize the data pipelines for\nCriteo 1TB DLRMsmall, fastMRI U-Net and OGBG GNN in hopes of removing\nbottlenecks and increasing GPU utilization.\n7.3 Baseline Results\nFigure 5 shows performance profiles for AdamW, NadamW, Nesterov, Heavy Ball,\nLAMB, Adafactor, SAM(w. Adam) and Distributed Shampoo using the search\nspaces described in Table 12 with the \u03b21 parameter tuned. Figure 5a shows the perfor-\n17. Given the end of Moore\u2019s law, we should expect accelerator improvements to continue to outpace im-\nprovements in general purpose processors. For that reason, the problem of bottlenecks upstream of the\npart of the training pipeline that runs on the accelerator(s) is not likely to go away.\n51\nAlgoPerf Authors\nmance profile with respect to runtime, Figure 5b shows the same performance profile with\nrespect to steps, and Table 13 shows the benchmark scores measuring the normalized area\nunder the performance profile curves. Unlike the official scoring procedure we will use for\nreal submissions, baselines in Figure 5 ignore the held-out workload criterion described in\nSection 4.5.3, which will depend on the specific held-out workloads sampled during official\nscoring, after submission code has been frozen. Since the performance profile (and thus the\nbenchmark score) for any individual baseline depends upon the entire set of baselines in-\ncluded in the performance profile, we computed performance profiles and benchmark scores\nfor the complete set of baselines presented across all sections, together (e.g. including those\npresented in Section 7.3.1) to ensure consistency. Throughout the paper, when comparing a\nsubset of baselines, we might elide some baselines from a figure to improve readability, but\nthe curves are always computed based on the full set. Please see Table 27 in the appendix\nfor the complete set of benchmark scores for all baselines considered in the paper.\nAlthough our benchmark is designed around measuring time-to-result, looking at per-\nformance profiles based on steps as well can sometimes be illuminating. Given that all\ntarget-setting algorithms take roughly the same time per step, the relative ranking of the\nbaselines derived from those algorithms does not change when measuring steps instead of\nruntime. Since SAM(w. Adam) and Distributed Shampoo typically require 1.5\u02c6\u20132\u02c6\nthe time per step as the target-setting algorithms, in order to create performance profiles\nbased on steps as well as runtime, we ran SAM(w. Adam) and Distributed Shampoo\nfor the same number of steps as the other baselines and used learning rate schedules based\non this step budget. These step-budget-optimized schedules likely decay the learning rate\ntoo slowly to be ideal when measured with respect to runtime. With faster implementations\nand runtime-tuned learning rate schedules, it might be possible to make these training al-\ngorithms competitive in runtime as well as in number of steps. That said, SAM(w. Adam)\nhas a very large deficit to make up, and might still struggle to be competitive. Regardless,\nwe hope proponents of these algorithms create submissions that achieve the best possible\nruntime results on our benchmark.\nWe can observe several interesting results from Figure 5, Table 13 and the workload-\nspecific breakdown in Table 25 and Table 26 presented in the appendix.\n\u2022 There is no single baseline training algorithm that hits the targets on every workload,\nbut on every workload there exists at least one (and usually more than one) baseline\nthat hits the target. Distributed Shampoo and NadamW both reach the target\non 7 out of 8 workloads. However, as described above, Distributed Shampoo has\nan unfair budget advantage, so NadamW hits the targets on the largest number\nof workloads among baselines adhering to the strict runtime budget. Distributed\nShampoo misses the target on Criteo 1TB DLRMsmall, whereas Nadam misses\nthe target on ImageNet ResNet-50.\n\u2022 With our search spaces, NadamW performs significantly better than AdamW both\nin terms of runtime and steps. Despite this impressive result, it seems to be much less\npopular than AdamW.\n\u2022 We found that Adafactor performs somewhat worse than AdamW in terms of steps\nto target. Adafactor is able to hit targets on 4\u20448 workload. AdamW hits the same\ntargets and additionally the target on Criteo 1TB DLRMsmall.\n52\nBenchmarking Neural Network Training Algorithms\n\u2022 LAMB hits the target on only 2 of our workloads, LibriSpeech DeepSpeech and\nWMT Transformer and thus does not do well in terms of the benchmark score.\nHowever, for WMT Transformer in particular, LAMB is the fastest algorithm to\nthe target (by a 7% margin).\n\u2022 The non-preconditioned baselines, Heavy Ball and Nesterov, are not able to hit\nthe target on any workload. Two factors explain this surprisingly poor result: (1)\nhow we constructed search spaces for these training algorithms, and (2) the way\nour benchmark rules require consistent performance and force training algorithms to\naccount for the workload-specific tuning they require to achieve good results. Both\nHeavy Ball and Nesterov have some trials which hit the target on ImageNet\nResNet-50 and fastMRI, but they are not frequent enough to show up in the\nmedian over the five studies.\nIndeed, search spaces for these algorithms that are\nbetter tailored to the tuning budget can hit more targets, which we show later in\nSection 7.3.1. However, as the target-setting experiments suggest, outperforming the\npre-conditioned baselines would be a tall order.\nSubmission\nBenchmark Score\nRuntime\nSteps\nAdamW\n0.600141\n0.596116\nNadamW\n0.849960\n0.830414\nNesterov\n0.0\n0.0\nHeavy Ball\n0.0\n0.0\nLAMB\n0.248619\n0.248494\nAdafactor\n0.236111\n0.475760\nSAM(w. Adam)\n0.120368\n0.731717\nDistributed Shampoo\n-\n0.854210\nTable 13: The benchmark scores for our baseline submissions. These are the inte-\ngrated performance profiles shown in Figure 5a (runtime) and Figure 5b (steps).\n7.3.1 Baseline Results Comparing Search Spaces\nIn order to reveal more about the role of the hyperparameter search space, we com-\npared baselines using the target-setting algorithms (AdamW, NadamW, Nesterov,\nHeavy Ball) with different search spaces. For each algorithm, we compared two search\nspaces (specified in Table 12): one tuning the \u03b21 parameter, and the other keeping the \u03b21\nparameter fixed to 0.9. Additionally, for each algorithm, we also prepared an OptList base-\nline, as described above, that samples without replacement from a list of hyperparameter\nconfigurations that performed well on at least one workload during target-setting. Tables\n29, 30, 31 and 32 in the Appendix contain the complete list of configurations used in the\nOptList baselines. Figure 6 shows the resulting performance profiles for this comparison\nand Table 14 shows the corresponding benchmark scores given by the (normalized) area\n53\nAlgoPerf Authors\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n\u03c4\n0%\n25%\n50%\n75%\n100%\nPercentage of Workloads\n(a) Performance profiles when measuring runtime to target\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n\u03c4\n0%\n25%\n50%\n75%\n100%\nPercentage of Workloads\nAdamW\nLAMB\nNadamW\nAdafactor\nNesterov\nSAM (w. Adam)\nHeavy Ball\nDistributed Shampoo\n(b) Performance profiles when measuring steps to target\nFigure 5: Performance profiles of our baseline submissions. Each line in these plots\nis the performance profile of a single baseline submission. A step in any line occurring at a\nvalue \u03c4 indicates that for one additional workload the corresponding submission achieves the\ntarget within a \u03c4 factor of the runtime of the best submission. For example in (b), NadamW\n(\n) has a bump just before \u03c4 \u201c 1.5.\nThis indicates that on one additional workload,\nNadamW requires a bit less than 1.5\u02c6 the number of steps as the fastest submission to\nreach the target on this workload. If a submission does not reach the target on one or more\nworkloads, its performance profile will not reach 100% at the very right of the plot. A flat\nline at 0% indicates that for all workloads the submission either did not hit the target at\nall or did so in time/steps that is at least 4 times worse than the time/steps to target of\nthe best submission. As mentioned earlier, we computed the performance profile for the\nentire set of baselines presented in the paper together and have removed the profiles for\nother baselines from this figure for readability.\n54\nBenchmarking Neural Network Training Algorithms\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n\u03c4\n0%\n25%\n50%\n75%\n100%\nPercentage of Workloads\n(a) Performance profiles when measuring runtime to target\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n\u03c4\n0%\n25%\n50%\n75%\n100%\nPercentage of Workloads\nAdamW\nNadamW\nNesterov\nHeavy Ball\nwith tuned \u03b21\nwith fixed \u03b21\nwith OptList\n(b) Performance profiles when measuring steps to target\nFigure 6: Performance profiles of our baseline submissions with different search\nspaces. Each line in these plots is the performance profile of a single baseline submission\n(analogously to Figure 5). As mentioned earlier we computed the performance profile for\nthe entire set of baselines presented in the paper together and have suppressed the profiles\nfor other baselines from this figure for readability.\nunder the curve. The raw data used to prepare these plots can be found in Tables 25 and\n26 in the Appendix.\nAs expected, the search space plays a large role in the results.\n\u2022 Although overall NadamW with a search space that tunes \u03b21 performed the best\namong these training algorithms in terms of runtime (and steps), NadamW performed\nroughly the same as AdamW when the \u03b21 parameter isn\u2019t tuned (fixed to the default\n55\nAlgoPerf Authors\nvalue of 0.9). On the other hand, for AdamW, tuning \u03b21 shows little advantage (at\nleast at the limited number of 20 tuning trials).\n\u2022 The OptList baselines for NadamW and AdamW seem to be roughly the same\nin terms of performance as compared to the best box polytope search spaces for\nthose training algorithms. On the other hand, for Nesterov and Heavy Ball, the\nOptList baselines perform substantially better (and are the only ones to hit any\ntarget). This result is likely a consequence of our search space construction proce-\ndure and the diversity of hyperparameter values that worked well for Nesterov\nand Heavy Ball.\nIn other words, our procedure for constructing box polytope\nsearch spaces needed to cover much wider hyperparameter ranges for Nesterov and\nHeavy Ball that also contain a large volume of bad points, making it harder to get\ngood results when sampling from these types of search spaces, for these algorithms.\nSubmission\nVersion\nBenchmark Score\nRuntime\nSteps\nAdamW\ntuned \u03b21\n0.600141\n0.596116\nfixed \u03b21\n0.596985\n0.593047\nopt-list\n0.725260\n0.721035\nNadamW\ntuned \u03b21\n0.849960\n0.830414\nfixed \u03b21\n0.599691\n0.595478\nopt-list\n0.835602\n0.813194\nNesterov\ntuned \u03b21\n0.0\n0.0\nfixed \u03b21\n0.0\n0.0\nopt-list\n0.233373\n0.232048\nHeavy Ball\ntuned \u03b21\n0.0\n0.0\nfixed \u03b21\n0.0\n0.0\nopt-list\n0.230504\n0.226860\nTable 14: The benchmark scores for our baseline submissions. These are the inte-\ngrated performance profiles shown in Figure 6a (runtime) and Figure 6b (steps).\n8. Discussion\n8.1 Target Setting\nFundamentally, our target setting procedure is a generic recipe for training neural networks\nthat achieves (arguably) a near state-of-the-art result on all of our fixed workloads and could,\nin principle, be applied to any workload. The procedure assumes the model architecture,\ndataset, loss function, and other workload details are given and also assumes we have a\nknown limit for the maximum runtime allowed for the training program. Furthermore, it\nassumes we can afford to sample hundreds of trials from the search spaces we designed for\nthe various training algorithms. Although it depends on a lot of assumptions, it does give\nus an automatic process for getting good results on our fixed workloads.\n56\nBenchmarking Neural Network Training Algorithms\nNaturally, we might ask ourselves how good this procedure is compared to other alter-\nnatives. Unfortunately, our procedure is the only one of its kind we are aware of in the\ndeep learning literature. Although Godbole et al. (2023) make an attempt to systematize\nthe applied deep learning workflow, the process they describe is still far from a repeat-\nable, mechanical procedure we can use for setting targets. Instead of generic procedures for\ntraining and tuning neural networks, the literature is filled with particular configurations\nthat worked well on specific problems, only occasionally accompanied by the tuning search\nspaces that helped discover them. These workload-specific training recipes typically do not\ngeneralize to new workloads.\nThis gap in the literature makes it difficult to know how\ncompetitive the validation and test targets we set with our procedure actually are, or how\nrealistic they are to reach for completely general, workload-agnostic training algorithms.\nTwo steps in the deep learning workflow seem to be especially neglected by the literature.\nFirst, we are not aware of algorithms for determining how long to train, or papers that\ntackle the concomitant philosophical issues surrounding how best to frame this choice.\nSecond, existing tools for automatic hyperparameter tuning still require search spaces as\ninput, i.e., we need to somehow determine which hyperparameters to tune and what set of\nvalues they should be allowed to take on. Although the blackbox optimization literature\nhas explored methods that automatically grow or shrink search spaces, the deep learning\nliterature contains shockingly little research on how to design the best search spaces for\nuse with existing tools and arbitrary budgets. Indeed, this profound gap in the literature\nwas part of our motivation for forcing submissions in our own benchmark to grapple with\ntuning challenges directly.\nOur own target-setting procedure is far from the final word on these issues. In an effort\nto keep the protocol workload-agnostic and generic, we ended up with broad search spaces\nthat required a large number of tuning trials, even though preliminary experiments showed\nthat some hyperparameters only needed to be tuned on a subset of the workloads. For\nexample, supporting and tuning dropout was not always necessary or helpful. Furthermore,\nour search spaces (Table 8), despite being relatively broad, had to be constructed through\nmanual trial and error. In several cases, we had to refer to previous published results on\nsimilar workloads to determine that our initial search spaces needed to be adjusted. We\nhope that the community scrutinizes our target setting procedure and comes up with better\nalternatives. Although we tried our best to create an objective protocol that minimized hu-\nman judgement, we were unable to remove it entirely. Even if full automation is out of reach,\nwe hope the community takes the problem of hyperparameter search space construction,\nspecifically in deep learning, much more seriously.\nStepping back, our target setting procedure essentially simulates a competition between\ntraining algorithms to get the best validation performance within a fixed runtime budget.\nThis protocol is a mirror image of our time-to-result benchmark. One possibility might be to\nformalize this relationship and alternate competitions to improve targets given fixed runtime\nbudgets, and runtimes given fixed targets. Even if we don\u2019t take things quite so far, we\ncould revise targets in future benchmark iterations by incorporating winning submissions\ninto the target-setting protocol.\nThe targets we selected using our procedure represent\nwhat is possible with currently popular training algorithms applied in a workload-agnostic\nmanner. These targets are generally not going to be the state-of-the-art error rates on the\ntasks our workloads cover.\n57\nAlgoPerf Authors\n8.2 Randomized Workloads\nAs described in Section 6, although we found realistic workload variants that simultane-\nously changed the best-performing NadamW hyperparameters in our search spaces and\nreached reasonable error rates in the original runtime budget, discovering such variants was\nsurprisingly difficult and far too labor intensive. Our difficulties highlight the desperate\nneed for new research to predict when hyperparameter settings that perform well on one\nworkload will transfer to a related workload and, more generally, predict the effect of var-\nious workload modifications on the optimal hyperparameters. While there has been some\nrecent work in this vein, such as Yang et al. (2021) which focuses on model size, we are\nstill far from being able to reliably predict when good hyperparameters will transfer. A\ntheory of hyperparameter transfer for all of the most common hyperparameters (and most\npopular training algorithms) would not just make it easier to produce interesting workload\nvariants, but would likely go a long way towards achieving the underlying purpose of our\nrandomized workloads: encouraging training algorithms that are robust to the exact details\nof the workload and are easy to tune. Even if such a theory did not immediately lead to\nmore robust and convenient algorithms, it would let us save tuning effort by extrapolating\nto new experiments from related tuning results.\nOur randomized workloads built from workload variants highlight an important set of\ntrade-offs in the design of our benchmark.\nThe more workloads in the benchmark, the\nmore expensive running a submission through the benchmark becomes, especially in the\nmore expensive external tuning ruleset. Both fixed workloads and randomized workloads\ncontribute to this cost, but because we sample a single variant from each randomized work-\nload to construct the held-out workloads, the fixed and randomized workloads currently\ncontribute roughly equally to the total cost. Similarly, the more tuning trials we allow\nin the external tuning ruleset, the more expensive the benchmark becomes. Since larger\ntuning budgets let submissions adapt more to specific workloads, they also increase the im-\nportance of using a large enough and diverse enough set of workloads, potentially through\nrandomized workloads. However, without a better understanding of when existing training\nalgorithms need to be re-tuned, it is very hard to know when a new workload or workload\nvariant is worth the increase in cost. Even worse, a variant that provides a useful challenge\nfor one training algorithm might be completely redundant for another.\n8.3 Baselines\nOur baseline results show that our per-workload targets are achievable, reveal clear gaps\nbetween different training algorithms, and suggest performance on the benchmark is far\nfrom saturated. On each workload, at least one baseline was able to reach the target, but\nno baseline reached the targets on all workloads simultaneously. NadamW with our tuned\n\u03b21 search space reached the target on seven out of the eight fixed workloads, and constitutes\na provisional state of the art on our benchmark. The performance profiles that form the\nbasis of the benchmark scores showed stark differences between the tested methods.\nOur baseline results also showed that the current practice of trying to compare abstract\nupdate rules with free parameters, divorced from the tuning protocol that would instantiate\nthem, is doomed to produce perpetually conflicting results. Hyperparameter tuning search\nspaces (and tuning protocols more generally) play a crucial role in the effectiveness of\n58\nBenchmarking Neural Network Training Algorithms\na training algorithm.\nBy picking specific search spaces, even when they are reasonable\na priori, we could claim our results showed that AdamW is better than NadamW, or\nvice versa, when in reality we are only showing that NadamW with a particular tuning\nprotocol is better than AdamW with some other specific tuning protocol. We need to view\nthe tuning protocol as an inseparable part of the training algorithm if we are ever going\nto determine a meaningful notion of the state-of-the-art training algorithm.\nSimilarly,\nalthough Heavy Ball or Nesterov was the target-setting algorithm on a combined three\nout of the eight workloads, our Heavy Ball or Nesterov with box polytope search spaces\nfailed to reach the target reliably on any workloads. This is not a general judgment about\nHeavy Ball or Nesterov as update rules. Instead, this result once again highlights the\nimportance of the tuning protocol and, if anything, might indicate that these algorithms\nrequire more workload-specific tuning effort. New training algorithms with hyperparameters\nthat must be tuned should ideally provide (budget-dependent) tuning procedures, or at a\nminimum, guidance on how to tune at a variety of budgets.\n8.4 Benchmark Limitations\nThe benchmark we presented in this work, like any benchmark, has a variety of limitations.\nThese limitations fall into several, broad categories.\nFirst, the benchmark has limited\ncoverage of possible submissions. In other words, the benchmark rules and software end\nup prohibiting, or effectively prohibiting, some potentially interesting submissions that we\nwould have preferred to allow, in principle. Second, there are limitations that could affect\nwhether benchmark scores truly measure what we intend them to measure (i.e. benchmark\n\u201cvalidity\u201d in the parlance of psychometrics). Third, there are limitations of the scope of the\nbenchmark that could affect its relevance to the actual practice of deep learning. Finally,\nthere are limitations that affect the accessibility of the benchmark, primarily in terms of\nhow easy and affordable it is for researchers to score new submissions.\nCoverage of the space of potentially interesting submissions\nBenchmark submis-\nsions must adhere to a specific training algorithm API and interoperate with workload\nimplementations in either JAX or PyTorch. Although this restriction is an essential de-\nsign choice intended to, among other things, help isolate the effects of the training algorithm,\nit does make it so some potentially interesting submissions cannot be supported under the\ncurrent rules. For example, although submissions can employ arbitrary hyperparameter\ntuning procedures while being timed, submissions adhering to the external tuning ruleset\nand making use of parallel tuning resources must use random search.\nInstead of being\nallowed to employ more sophisticated black-box optimization algorithms, they must only\nrely on their control over the tuning search space. Although we could potentially relax this\nlimitation in future versions, allowing submissions complete control over how they utilize\nthe tuning resources would require much more complicated orchestration code and APIs.\nSimilarly, although not against the rules, the API does not provide a way to implement\nmodel parallelism since it isn\u2019t situated in the submission code, as we define it, even if we\ncould imagine a hypothetical exotic training algorithm that depended on it somehow. As\nanother example, submissions are not allowed to access arbitrary information about the\ncurrent workload. Instead, they can only obtain basic layer metadata and dataset informa-\ntion. This restriction precludes submissions based on optimizers, such as K-FAC (Martens\n59\nAlgoPerf Authors\nand Grosse, 2015), that require detailed architectural information. Although K-FAC itself\nis non-trivial to apply to new model architectures even when detailed architectural informa-\ntion is available, hypothetical generic training algorithms that required such information,\nbut worked for any arbitrary neural network, would likely also face difficulties.\nSubmissions are also constrained from a software implementation standpoint.\nThey\nmust interoperate with either JAX or PyTorch workload implementations, prohibiting\nother frameworks and software stacks. Although in theory there is nothing stopping us\nfrom porting our workloads to additional frameworks, in practice it is far too costly in\nterms of engineering resources.\nUltimately, our initial rules and API err on the side of caution to make sure we isolate\nthe effects of the training algorithm. Moving forward, we intend to keep a close eye on the\ntypes of algorithmic modifications that are of interest to the community, but are not possible\nwithin our rules or API, and solicit suggestions on ways they could be accommodated.\nExperimental protocol\nAlthough we designed our benchmark to prioritize producing\nconvincing measurements, there are still ways the experimental protocol could be strength-\nened. Specifically in the external tuning ruleset, the ratio between the number of workloads\n(eight fixed along with eight more held-out workloads) and the number of tuning trials\nallowed per study (twenty) could allow for too much workload-specific tuning and thus risk\nrewarding submissions that overfit to the particular suite of workloads. Although the ran-\ndomized workloads we draw held-out workloads from are designed to mitigate this issue, the\nworkload variants did not fully achieve all of our desiderata. Although all three variants of a\ngiven base workload will require different hyperparameter settings than the base workload,\nthey might not require mutually distinct hyperparameter settings from each other, risking\na set of variants that doesn\u2019t challenge submissions as much as initial appearances might\nsuggest. Additionally, the variants of different base workloads often repeat similar changes.\nFor example, we generated variants of multiple base workloads by making activation func-\ntion changes. Moving beyond workload overfitting concerns, neural network training is a\nnoisy process, especially when hyperparameter tuning using random search is involved. Our\nstrategy of repeating measurements with different random seeds during scoring can help,\nbut the best-performing optimization hyperparameters for the most popular optimizers are\noften near the \u201cedge of stability\u201d (Cohen et al., 2022), so some submissions can get un-\nlucky and have training diverge more than is typical when we only have a small number of\nrepetitions.\nScope and relationship to current practice\nOur choices of what workloads and condi-\ntions to study in our benchmark necessarily constrain the set of situations the results will be\nmost relevant for. Even restricting our attention to supervised and self-supervised learning,\nwe cannot hope to cover every practically relevant data modality, let alone every practically\nrelevant dataset and model. For example, our benchmark currently does not contain any\nworkloads for object detection from point cloud data, weather prediction, language model-\ning, video understanding, or image generation. Our choice to prioritize currently popular,\neasily-accessible, and well-studied datasets and models could end up reinforcing existing\nselection effects. Perhaps because currently popular training algorithm co-evolved with the\nmost popular application domains, they work unusually well together. In this case, a bench-\nmark emphasizing existing popular workloads will make it hard to break out of what could\n60\nBenchmarking Neural Network Training Algorithms\nbe a methodological local optimum. The advent of new training algorithms could poten-\ntially unlock efficient training for entirely different models, ones that are not covered by our\nbenchmark. In the near-term, it is impossible to resolve these types of counterfactuals, but\nwe could potentially create a more diverse\u2014along every dimension we can measure\u2014set of\nbenchmark workloads, if we saved resources in other ways. Additionally, as discussed in\nSection 7.2, we would like to have slightly fewer data-pipeline-bound workloads. Moving\naway from the limitations of our particular set of current workloads, the benchmark today\nincludes only a single hardware weight class. Although we believe results at this scale are\nrelevant at a variety of interesting scales, the gold standard would be to directly measure\nmuch larger and smaller scales. That said, even if the benchmark results are informative\nfor many different scales, the relative performance of different training algorithms might\nsystematically vary with batch size. By only using a single specific system with a particular\namount of accelerator memory, the benchmark effectively covers only the narrow range of\nbatch sizes for each workload that are close to the largest batch size that can fit in memory\nfor the most competitive submissions.\nAccessibility\nIn order for our benchmark to be useful, scoring the most intriguing new\ntraining algorithms developed by the research community needs to be feasible. A large part\nof whether it is feasible for a particular group to evaluate a submission on the benchmark\nis the compute costs of the scoring protocol, running on the official benchmark system.\nTable 15 lists the provisional18 number of machine-hours required, on the official benchmark\nsystem using 8\u02c6NVIDIA V100 GPUs, to evaluate submissions under both tuning rulesets.\nAlthough that tuning can run on a different machine and need not use the official benchmark\nsystem, it is still worth computing the number of machine-hours on the benchmark system\nas a reference point. There are a variety of ways these costs could come down. For example,\nwe could switch to a system with newer GPUs that speeds up training enough to be worth\nany concomitant increase in the price per machine-hour. Or, as submissions become more\ncompetitive, we could shrink the maximum runtime allowed.\nNevertheless, as it stands, for groups that don\u2019t have several 8\u02c6NVIDIA V100 GPUs\nmachines on premises, cloud costs to score submissions could be a significant hurdle. On\nthe other hand, groups that train very large language models routinely run experiments\neclipsing these scoring costs by orders of magnitude. Ultimately, whether or not evaluating\na submission is affordable is a relative question that will depend on the specific research\ngroup. For now, our solution is to obtain compute sponsorship to help groups with more\nlimited resources evaluate and score promising submissions.\nIn order to allocate limited sponsorship funding among submissions from groups unable\nto self-fund scoring costs, we plan to use performance on a qualification set of workloads that\nexcludes some of the most expensive workloads. Specifically, the qualification set consists\nof the Criteo 1TB DLRM small, OGBG GNN, and WMT Transformer workloads,\nwithout any held-out workloads. Submitters that do not have the resources to self-report\nresults on the full set of workloads may instead report results on this smaller qualification\n18. We plan to reduce the running time of some of the more expensive workloads before issuing a call for\nsubmissions, so these costs are likely a bit of an overestimate, although they should be the correct order\nof magnitude.\n61\nAlgoPerf Authors\nset. As shown in Table 15, evaluating submissions on the qualification set is about an order\nof magnitude cheaper than the full set of workloads.\nStepping back, part of the issue of affordability is that our benchmark is all-or-nothing.\nTo compute a benchmark score, we need to run submissions on all workloads. Thus, for\nany specific level of compute resources necessary for scoring, some groups will struggle\nto participate without compute support. It is hard to imagine this approach scaling to\nthousands of realistic workloads, especially if we consider tuning costs, without excluding\nfar too many groups. Ideally, we would have a benchmark with a flexible mechanism for\nincrementally investing computational resources to gather more and more information about\nthe performance of a submission, eventually culminating in a complete set of experiments on\na large, highly-diverse set of fixed and held-out workloads. Even with our current design,\nwe can compute an upper bound on a submission\u2019s benchmark score based on training\non a subset of workloads and/or with a reduced runtime limit, but our scoring procedure\nwould probably need to be revised if we wanted it to scale to a much larger number of\nstill-relatively-costly workloads.\nSetting\nTime (h)\nExternal Tuning Ruleset\nOne hyperparameter\n232.23\nScoring a submission\n1161.13\nTuning a submission\n23,222.61\nQualification Set\nOne hyperparameter\n20.65\nScoring a submission\n103.24\nTuning a submission\n2064.75\nSelf-tuning Ruleset\nOne hyperparameter\n696.68\nScoring a submission\n3483.39\nQualification Set\nOne hyperparameter\n61.94\nScoring a submission\n309.71\nTable 15: Estimated required runtime of the benchmark. One hyperparameter refers\nto running all eight fixed and eight held-out workloads once, i.e. with a single hyperparam-\neter. Scoring a submission involves repeating this process for each study, i.e. five times.\nTo fully tune a submission, each study uses twenty tuning trial to identify the best hyper-\nparameter setting (note that this need not use the benchmark system). Running a single\nhyperparameter is more expensive in the self-tuning ruleset since it has a three times larger\nruntime budget for every workload to compensate for the lack of external tuning.\nThe\nqualification set only consists of three (out of the eight) fixed workloads, without held-out\nworkloads, and thus offers a reduce cost.\n8.5 Future Work\nIn addition to work on improving the benchmark itself and building new, and even stronger,\nbaselines (especially for the self-tuning ruleset), there are several related areas that would\nbenefit from more research. One appealing area of future work motivated by our challenges\n62\nBenchmarking Neural Network Training Algorithms\nwith building randomized workloads, as discussed previously in Section 8.2, would be a the-\nory to predict the effect of various workload modifications on the optimal hyperparameters,\npaving the way for a better understanding of how to perform workload-specific tuning, how\nto build randomized workloads with support over a combinatorial space of variants, and\nwhen workload-specific tuning is even necessary. Another direction with immense practical\npotential is developing completely workload-agnostic training recipes that, when given an\narbitrary neural network training workload (model, dataset, and loss function) along with a\nbudget, produce the best possible result. Even though we could use such recipes for target-\nsetting, to be maximally interesting, they would need to go far beyond our target-setting\nprocedure and be useful enough to be adopted by practitioners.\nAlthough we restricted our attention to training algorithms in this work, there are other\nparts of the deep learning training pipeline that affect training speed. For the purposes of our\ntraining algorithms benchmark, the preprocessing, model architecture, and loss function are\nfixed components of the workload, but they could all benefit from algorithmic improvements.\nThe initial proposal for the benchmark rules also included a separate, time-to-result model\nbenchmark that measured training speedups due to model changes. In order to isolate the\neffects of model changes, this model benchmark required submissions to train using a small\nset of standard training algorithms. Unlike in the training algorithm benchmark, models\nwould only be required to perform well on a single task, albeit across different datasets (e.g.\ndifferent language pairs in machine translation). Consequently, task-specific components\nof the machine learning pipeline, such as data augmentation, could be incorporated as\npart of the submission in a hypothetical future AlgoPerf Models benchmark. With\nresults from both training algorithms and separate model benchmarks, we could determine\nthe relative responsibility for speedups on different tasks of the training algorithms vs the\nmodel architecture, and develop a complete picture of the most promising directions for\naccelerating neural network training.\n9. Conclusion\nNeural networks must be trained to be useful, making training algorithms essential for\ncreating useful deep learning models. Unfortunately, due to the lack of a standard, con-\nvincing protocol for empirically comparing training algorithms, progress on better training\nalgorithms has stalled.\nTo address this pressing issue, in this work, we introduced the\nAlgoPerf: Training Algorithms benchmark, a competitive, time-to-result benchmark\ncovering multiple realistic workloads, running on fixed hardware. This benchmark represents\nthe collective efforts, over multiple years, of the members of the MLCommons Algorithms\nworking group to remove the measurement barriers frustrating progress on neural network\ntraining algorithms.\nAlthough we believe the benchmark we have created represents an important advance, it\nis far from perfect and has a variety of limitations. We extend an open invitation to the entire\ncommunity\u2014and in particular, those who disagree with our design choices\u2014to join the\nworking group and collaborate on improving the benchmark further, either before we issue\nthe initial call for submissions, or after. We would also be delighted by any novel solutions\nfor the challenges with training algorithm comparisons we described in Section 2, whether\n63\nAlgoPerf Authors\nthey are possible to incorporate into AlgoPerf: Training Algorithms or necessitate\nan additional benchmark with a fundamentally different approach.\nWe urge researchers developing new training algorithms to submit them to the\nAlgoPerf: Training Algorithms benchmark competition once the call for submissions\nhas been issued. Crafting a valid submission might require thinking more about hyperpa-\nrameter tuning than researchers inventing new optimizers are used to, but this work will\nhelp us break free of the cycle of hype and abandonment currently facing new algorithms.\nOutside the competition schedule, researchers can still use the benchmark to measure the\nperformance of new algorithms as long as they adhere to the rules and report raw workload\ntimes, in addition to recomputing unofficial benchmark scores. Researchers should feel free\nto reach out to the working group for guidance, as needed, or to find potential collaborators\nwith the resources to run larger, more comprehensive experiments. Finally, we believe that\nthe presented benchmark constitutes a new state of the art for empirical comparisons of\ntraining algorithms, and should be viewed as a first step towards a more reproducible and\nempirically rigorous scientific literature on neural network training algorithms.\n64\nBenchmarking Neural Network Training Algorithms\nAuthor Contributions and Acknowledgments\n\u2022 George E. Dahl: Founded and chaired the working group. Co-authored the initial\nrules proposal and shaped the rules of the benchmark.\nRecruited contributors to\ncomplete the necessary engineering work. Co-led paper experiments and co-authored\nthe codebase used for paper experiments.\nCo-led the paper\u2019s writing process and\nsupervised the writing contributions from other authors. Directly contributed to every\naspect of the writing process including outlining, drafting sections, creating figures,\nand editing. Served as overall project coordinator.\n\u2022 Frank Schneider: Chaired the working group. Significantly influenced the rules\nof the benchmark.\nCo-led the paper\u2019s writing process and supervised the writing\ncontributions from other authors. Directly contributed to every aspect of the writing\nprocess including outlining, drafting sections, creating figures, and editing.\n\u2022 Zachary Nado: Lead engineer, tech lead, and supervisor for building the benchmark\ncodebase and cloud infrastructure, including the implementation of the workloads in\nboth JAX and PyTorch. Coordinated verifying the correctness across infrastructure\nimplementations. Co-authored the codebase for paper experiments. Co-authored the\ninitial rules proposal. Made major writing contributions to the paper.\n\u2022 Naman Agarwal: Co-led paper experiments.\nDesigned and ran a large number\nof critical experiments including target-setting experiments, baselines, workload vari-\nants, and many others. Worked on initial implementations of the fastMRI and ViT\nworkloads. Made major writing contributions to the paper. Significantly influenced\nthe rules of the benchmark.\n\u2022 Chandramouli Shama Sastry: Implemented both LibriSpeech workloads in Py-\nTorch. Developed test suites to compare the implementations of workloads across\nframeworks that caught several implementation deficiencies.\nContributed to the\nbenchmark infrastructure.\n\u2022 Philipp Hennig: Significantly influenced the rules of the benchmark and made major\nwriting contributions to the paper.\n\u2022 Sourabh Medapati: Assisted with the writing of the paper. Implemented both\nLibriSpeech workloads in JAX and helped debug them in PyTorch. Conducted\nseveral experiments. Helped maintain the paper experiment codebase. Contributed\nto the benchmark infrastructure.\n\u2022 Runa Eschenhagen: Significantly contributed to the benchmark infrastructure,\nleading the PyTorch development for a large portion of the development cycle.\nImplemented the WMT workload in PyTorch and the OGBG workload in both\nPyTorch and JAX. Debugged numerous critical issues in the benchmark codebase\nand helped refine APIs.\n\u2022 Priya Kasimbeg: Led the timing experiments between our PyTorch and JAX im-\nplementations and debugged several critical issues in the benchmark codebase. Con-\ntributed to the benchmark infrastructure.\nHelped maintain the paper experiment\ncodebase.\n65\nAlgoPerf Authors\n\u2022 Daniel Suo: Implemented the initial version of the fastMRI and ViT workloads\nin JAX. Created the performance profile and scoring infrastructure. Made a large\nnumber of essential contributions to the main codebase for paper experiments. Con-\ntributed to the benchmark infrastructure. Assisted with the writing of the paper.\n\u2022 Juhan Bae: Implemented the ViT, fastMRI, and Criteo 1TB workloads in Py-\nTorch. Debugged several critical issues in the benchmark codebase. Contributed to\nthe benchmark infrastructure and documentation.\n\u2022 Justin Gilmer: Made major writing contributions to the paper.\nConducted ex-\nperiments highlighting the need for workload standardization and designed several\nheld-out workload variants. Co-authored the initial rules proposal. Co-authored the\ncodebase for paper experiments.\n\u2022 Abel L. Peirson: Explored ways to define randomized workload distributions for\nthe OGBG base workload and implemented the CIFAR-10 development workload\nfor JAX. Assisted with the writing.\n\u2022 Bilal Khan:\nConducted the baseline experiments for Distributed Shampoo,\nSAM, Adafactor, and LAMB. Assisted with the writing of the paper.\n\u2022 Rohan Anil: Co-authored the initial rules proposal. Advised on the setup of the\nJAX baselines, and optimizer configuration details, generally helped with perfor-\nmance tuning and debugging JAX/PyTorch differences, and assisted with writing\nthe paper.\n\u2022 Mike Rabbat: Significantly influenced the rules of the benchmark, assisted with the\nPyTorch implementation, provided support for the fastMRI workload, and made\nmajor writing contributions to the paper.\n\u2022 Shankar Krishnan: Assisted with the writing of the paper.\nConducted several\nexperiments.\n\u2022 Daniel Snider: Implemented the ResNet-50 workload in JAX. Conducted prelim-\ninary experiments for randomized workloads on OGBG, wrote code for logging, and\nexplored options for serving the benchmark. Drafted rules around software dependen-\ncies.\n\u2022 Ehsan Amid: Assisted with the writing of the paper. Supported the implementation\nof Criteo 1TB for JAX.\n\u2022 Kongtao Chen: Implemented the initial drafts of the DeepSpeech workload in\nPyTorch and the WMT workload in JAX.\n\u2022 Chris J. Maddison: Co-authored the initial rules proposal.\n\u2022 Rakshith Vasudev: Supported the implementation of the Criteo 1TB workload\nfor PyTorch.\n\u2022 Michal Badura: Assisted with the writing of the paper. Wrote the initial JAX\nimplementation of the OGBG workload in the main paper experiment codebase.\n\u2022 Ankush Garg: Supported the implementation of the WMT workload. Assisted with\nthe writing of the paper.\n66\nBenchmarking Neural Network Training Algorithms\n\u2022 Peter Mattson: Co-authored the initial rules proposal.\nThe Brain team at Google Research supported the initial work on the benchmark and\nthe JAX baselines.\nThe authors would like to express their gratitude to David Kanter and the entire ML-\nCommons organization for their support throughout the project. Many thanks to Toby\nBoyd for his help in preparing a request for compute sponsorship and additional logistical\nsupport. We are thankful to Hanlin Tang for his help in designing the initial PyTorch\nAPI requirements and implementing the ResNet-50 workload in PyTorch. We thank\nLeda Sari for providing a reference implementation and her expertise for the LibriSpeech\nworkloads. Thanks to Guodong Zhang for helpful suggestions regarding the rules and the\nsubmission API. We thank Dami Choi and Roger Grosse for helpful discussions. Further-\nmore, we would like to thank Varun Godbole for helpful feedback on this manuscript, Lucas\nNestler for his help in implementing LibriSpeech workloads in JAX, and Kamal Raj for\nhelping formulate Docker instructions. Finally, we\u2019d like to especially thank all the members\nof the MLCommons Algorithms working group.\nFrank Schneider is supported by funds from the Cyber Valley Research Fund. Philipp\nHennig and Frank Schneider gratefully acknowledge financial support by the European\nResearch Council through ERC StG Action 757275/PANAMA; the DFG Cluster of Ex-\ncellence \u201cMachine Learning - New Perspectives for Science\u201d, EXC 2064/1, project number\n390727645; the German Federal Ministry of Education and Research (BMBF) through the\nT\u00a8ubingen AI Center (FKZ:01IS18039A); and funds from the Ministry of Science, Research\nand Arts of the State of Baden-W\u00a8urttemberg. Daniel Snider is supported by funds from\nthe Canada Foundation for Innovation JELF grant, NSERC Discovery grant, AWS Machine\nLearning Research Award (MLRA), Facebook Faculty Research Award, Google Scholar Re-\nsearch Award, and VMware Early Career Faculty Grant.\n67\nAlgoPerf Authors\nAppendices\nA. Experimental Details for Section 2\nA.1 Learning Rate Schedules\nThroughout the experiments in this paper (not only those reported in Section 2), we use\ntwo types of learning rate schedules. Both schedules are illustrated in Figure 7 and their\ndetails are provided in the following.\nWarmup\nSteps\nNum\nSteps\nIteration\n0.0\nBase\nLR\nLearning Rate\nWarmup\nSteps\nDecay\nSteps\nNum\nSteps\nIteration\n0.0\nReduced\nLR\nBase\nLR\nFigure 7: The two learning rate schedules used in our experiments. We use a\ncosine decay schedule with a learning rate warmup phase (warmup ` cosine decay, left,\n) and a linear decay schedule with a learning rate warmup phase and a constant phase\nat the end (warmup ` linear decay ` constant, right,\n). The schedules are scaled\nto fill the entire NumSteps and are further parameterized by the parameters shown in the\nrespective figures.\nA.1.1 Warmup Cosine Decay\nThe cosine decay with warmup (denoted warmup`cosine decay) is characterized by a linear\nwarmup phase followed by a cosine decay of the learning rate. It is parameterized by the\nfollowing parameters:\n\u2022 Base LR: The base learning rate. It is used as the peak learning rate of the warmup\npart and the cosine decay.\n\u2022 Num Steps: The total number of steps of the schedule.\n\u2022 Warmup Steps: The number of warmup steps over which the learning rate is linearly\nincreased from zero to the base learning rate. Here, the warmup steps, characterize\nthe absolute number of steps over which the warmup is performed. For simplicity, in\nour code, we instead provide the warmup phase in terms of the percentage of the total\nnumber of steps in the schedule (see for example Table 10).\n68\nBenchmarking Neural Network Training Algorithms\nMathematically, the learning rate LR at any step t \u011b 0 of the warmup ` cosine decay\nschedule is defined by\nLRptq \u201c\n$\n&\n%\nBaseLR\nt\nWarmupSteps\nt \u010f WarmupSteps\nBaseLR\n2\n\u00b4\n1 ` cos\n\u00b4\n\u03c0\n\u00b4\nt\u00b4WarmupSteps\nNumSteps\u00b4WarmupSteps\n\u00af\u00af\u00af\notherwise .\nA.1.2 Warmup Linear Decay Constant\nThe other learning rate schedule (denoted warmup ` linear decay ` constant) also employs\na linear warmup phase, this time followed by a linear decay and a final phase of constant\nlearning rate. It is parameterized by the following parameters:\n\u2022 Base LR: The base learning rate. It is used as the upper learning rate of the warmup\npart and the linear decay.\n\u2022 Num Steps: The total number of steps of the schedule.\n\u2022 Warmup Steps: The number of warmup steps over which the learning rate is linearly\nincreased from zero to the base learning rate. Here, the warmup steps, characterize\nthe absolute number of steps over which the warmup is performed. For simplicity, in\nour code and the rest of the paper, we instead provide the warmup phase in terms of\nthe percentage of the total number of steps in the schedule (see the entry warmup for\nexample in Table 10).\n\u2022 Reduced LR: The reduced learning rate is the lower bound of the linear decay and\nis used for the final constant phase of the schedule. Here, the reduced learning rate\ndenotes the absolute learning rate.\nFor simplicity, in our code and the remaining\npaper, we characterize the reduced learning rate by a decay factor (see the entry\ndecay factor for example in Table 10) relative to the base learning rate.\n\u2022 Decay Steps:\nThe absolute number of steps of the linear decay (including the\nwarmup steps). Once again, for simplicity in our code and throughout the paper,\nwe use a slightly different version. There, we define the decay steps relative to the to-\ntal number of steps excluding the warmup phase (see the entry decay steps for example\nin Table 10).\nMathematically, the learning rate LR at any step t \u011b 0 of the warmup ` linear decay `\nconstant schedule is defined by\nLRptq \u201c\n$\n\u2019\n\u2019\n\u2019\n\u2019\n\u2019\n&\n\u2019\n\u2019\n\u2019\n\u2019\n\u2019\n%\nBaseLR\nt\nWarmupSteps\nt \u010f WarmupSteps\nBaseLR\nDecaySteps\u00b4t\nDecaySteps\u00b4WarmupSteps\n`ReducedLR\nt\u00b4WarmupSteps\nDecaySteps\u00b4WarmupSteps\nWarmupSteps \u0103 t \u010f DecaySteps\nBaseLR DecayFactor\notherwise .\nA.2 Details for Training Curves that Cross\nThe training curves presented in Figure 1 in Section 2.1 were drawn from preliminary target-\nsetting experiments for ResNet-50 trained on ImageNet using AdamW. They use the\n69\nAlgoPerf Authors\nworkload configuration described in Appendix D.3.1. The trials were selected from two\narbitrary preliminary tuning studies that each had a budget of 100 trials. To select the two\ntrials, we plotted all the training curves on a single plot, selecting two arbitrary trials that\ncrossed and also achieved relatively good final validation error rates (in the top 20 trials for\nboth studies). It was not necessary to find trials from different studies given how plentiful\ntraining curves that cross are in typical tuning studies. The search spaces were the same as\nthe AdamW search space listed in Table 8, except they were from experiments that predate\nthe final choice of allowed options for learning rate warmup lengths, disabled dropout, and\ntuned label smoothing on a continuous range of r0.0, 0.2s. These preliminary experiments\nalso happened to be from studies using a batch size of 8192 and 32768, respectively. The\nparticular trials selected used a warmup length of 15% and 10%; the trial with the better\nfinal validation error used a batch size of 8192 and a warmup length of 15%. Many pairs of\ntrials in these studies cross, and the specific trials we selected were typical well-performing\nones. It is easy to find training curves that cross multiple times among the top dozen trials\nin almost any of our larger tuning studies.\nA.3 Details for Sensitivity of Optimizer Ranking to the Model Architecture\nA.3.1 Wide ResNet with Stride Changes\nThe standard Wide ResNet 28-10 architecture consists of 3 groups of 4 residual blocks,\neach block containing 2 convolutional layers. The default strides used in each of the three\ngroups are 1\u02c61, 2\u02c62, and 2\u02c62 in groups 1, 2, and 3 respectively. Our modified architectures\nchanges the strides in group 3 from 2\u02c62 to 1\u02c61. The strides are what reduce the height\nand width dimensions in the embedded image tensor, so changing the strides from 2\u02c62\nto 1\u02c61 means that no reduction occurs in the final group. The final operation before the\ndense layer is an 8\u02c68 average pooling operation followed by a flatten operation. So all\nremaining height (H), width (W) and channel (C) dimensions are flattened into a single\ndim of size H \u02c6 W \u02c6 C. In the stride 1x1 variant both H and W increase by a factor of 9,\nresulting in a factor of 81 increase in the fan-in of the final dense layer (in particular the\ndense layer shape changes from 640\u02c610 to 51840\u02c610). This dense layer is initialized with\nthe LeCun normal initialization, which makes the variance of the forward pass invariant to\nthe fan-in. However, the statistics of the backward pass are not invariant to the fan-in (and\nmore importantly the loss curvature itself is not invariant) and so the resulting model has\nan instability particularly with respect to the final output layer (see Figure 8).\nA.3.2 Architectural Modifications of Transformer Models\nThis section provides additional data for the experiment presented in Section 2.2.1. Specif-\nically, Table 16 is a variant of Table 3 but showing the cross-entropy loss instead of the\nBLEU score.\nA.4 Details for Comparing Instances of Training Algorithms\nA.4.1 Training Algorithms with Different Hyperparameters\nThis section provides additional details for the experiments described in Section 2.3.1 and\nsummarized in Table 4. We report the performance of the per-workload optimal hyper-\n70\nBenchmarking Neural Network Training Algorithms\n100\n101\n102\n103\n104\nStep\n0\n20\n40\n60\n80\n100\nTraining Loss\nStride 2x2\nStride 1x1\nStride 1x1 w/ LR Warmup\nFigure 8: Visualizing the training instability caused by changes to the Wide\nResNet convolutional stride. For the standard Stride 2x2 architecture (\n), the train-\ning loss decreases almost monotonically throughout training. However, the Stride 1x1 ar-\nchitecture (\n) shows a brief period of training instability, followed by a recover all within\nthe first 10 steps of training. Despite the recovery, the long term optimization trajectory\nis affected and the stride change under-performs the standard architecture. Learning rate\nwarmup helps mitigate this instability and allows the Stride 1x1 architecture to match the\nperformance of the original model (\n). All runs are shown for a learning rate of 0.215.\nTraining\nPre-LN\nPost-LN\nAlgorithm\nBest\nConfidence interval\nBest\nConfidence interval\nAdamW\n1.3093\n1.3449 \u02d8 0.0337\n1.3632\n1.4934 \u02d8 0.1327\nNadamW\n1.3070\n1.3308 \u02d8 0.0240\n1.3539\n1.5072 \u02d8 0.1822\nNesterov\n1.5668\n1.6362 \u02d8 0.1047\n1.9577\n2.5451 \u02d8 0.6894\nShampoo\n1.4096\n1.4400 \u02d8 0.0318\n1.4516\n1.5330 \u02d8 0.1452\nTable 16: Test set cross-entropy score for Pre-Layer Norm (Pre-LN) and Post-\nLayer Norm (Post-LN) architectures for different training algorithms. The archi-\ntectural modification of Pre-LN vs. Post-LN affects Nesterov momentum, AdamW,\nNadamW, and Shampoo differently. This is the same as Table 3 but showing the cross-\nentropy loss instead of the BLEU score.\nparameters, of the overall optimal hyperparameters, as well as the resulting relative per-\nformance degradation \u03d5pHq from using a single shared hyperparameter setting, which is\ndefined as\n\u03d5wpHq \u201c\n\u02c7\u02c7\u02c7\u02c7\nvalpw, h\u02daq \u00b4 valHpwq\nvalHpwq\n\u02c7\u02c7\u02c7\u02c7\nwhere h\u02da is the overall optimal hyperparameter setting. It holds that maxw \u03d5wpHq \u201c \u03a6pHq\nas reported in Section 2.3.1. Tables 17 to 20 show the quantities mentioned above for the\nfour target-setting training algorithms.\n71\nAlgoPerf Authors\nWorkload\nPerformance of the\n\u03d5wpHq\nPer-Workload Optimal\nOverall Optimal\nHyperparameters\nHyperparameters\nCriteo 1TB\nDLRMsmall\n0.123675\n0.124022\n0.002806\nfastMRI\nU-Net\n0.734330\n0.731403\n0.003986\nImageNet\nResNet 50\n0.230340\n0.267460\n0.161153\nViT\n0.226140\n0.263300\n0.164323\nLibriSpeech\nConformer\n0.078327\n0.093633\n0.195425\nDeepSpeech\n0.114152\n0.133201\n0.166873\nOGBG\nGNN\n0.277534\n0.252594\n0.089862\nWMT\nTransformer\n30.693876\n29.067333\n0.052992\nTable 17: Performance of the per-workload optimal and overall optimal hyper-\nparameters for AdamW. A column with \u03d5wpHq \u201c 0 indicates that the overall optimal\nhyperparameters achieved the same performance as the per-workload optimal hyperparam-\neters on this workload. The \u03d5w-value in bold indicates the largest value observed across all\nworkloads, which is reported as \u03a6pHq in Table 4.\nWorkload\nPerformance of the\n\u03d5wpHq\nPer-Workload Optimal\nOverall Optimal\nHyperparameters\nHyperparameters\nCriteo 1TB\nDLRMsmall\n0.123609\n0.135988\n0.100147\nfastMRI\nU-Net\n0.734523\n0.710679\n0.032462\nImageNet\nResNet 50\n0.227020\n0.261320\n0.151088\nViT\n0.225300\n0.263420\n0.169197\nLibriSpeech\nConformer\n0.077790\n0.089341\n0.148486\nDeepSpeech\n0.113950\n0.121887\n0.069654\nOGBG\nGNN\n0.280012\n0.274704\n0.018954\nWMT\nTransformer\n30.853422\n29.747388\n0.035848\nTable 18: Performance of the per-workload optimal and overall optimal hyper-\nparameters for NadamW. A column with \u03d5wpHq \u201c 0 indicates that the overall optimal\nhyperparameters achieved the same performance as the per-workload optimal hyperparam-\neters on this workload. The \u03d5w-value in bold indicates the largest value observed across all\nworkloads, which is reported as \u03a6pHq in Table 4.\n72\nBenchmarking Neural Network Training Algorithms\nWorkload\nPerformance of the\n\u03d5wpHq\nPer-Workload Optimal\nOverall Optimal\nHyperparameters\nHyperparameters\nCriteo 1TB\nDLRMsmall\n0.126139\n0.144934\n0.149004\nfastMRI\nU-Net\n0.734645\n0.733440\n0.001640\nImageNet\nResNet 50\n0.226600\n0.278720\n0.230009\nViT\n0.243180\n0.278700\n0.146065\nLibriSpeech\nConformer\n0.130823\n0.130823\n0.0\nDeepSpeech\n0.171137\n0.192623\n0.125546\nOGBG\nGNN\n0.283124\n0.226850\n0.198761\nWMT\nTransformer\n30.107387\n26.977169\n0.103968\nTable 19: Performance of the per-workload optimal and overall optimal hyper-\nparameters for Nesterov. A column with \u03d5wpHq \u201c 0 indicates that the overall optimal\nhyperparameters achieved the same performance as the per-workload optimal hyperparam-\neters on this workload. The \u03d5w-value in bold indicates the largest value observed across all\nworkloads, which is reported as \u03a6pHq in Table 4.\nWorkload\nPerformance of the\n\u03d5wpHq\nPer-Workload Optimal\nOverall Optimal\nHyperparameters\nHyperparameters\nCriteo 1TB\nDLRMsmall\n0.125913\n0.145933\n0.158998\nfastMRI\nU-Net\n0.733828\n0.731964\n0.002539\nImageNet\nResNet 50\n0.225340\n0.279280\n0.239372\nViT\n0.244860\n0.286660\n0.170710\nLibriSpeech\nConformer\n0.132797\n0.134879\n0.015684\nDeepSpeech\n0.161977\n0.186385\n0.150687\nOGBG\nGNN\n0.276148\n0.223901\n0.189199\nWMT\nTransformer\n30.643066\n26.705241\n0.128506\nTable 20: Performance of the per-workload optimal and overall optimal hyperpa-\nrameters for Heavy Ball. A column with \u03d5wpHq \u201c 0 indicates that the overall optimal\nhyperparameters achieved the same performance as the per-workload optimal hyperparam-\neters on this workload. The \u03d5w-value in bold indicates the largest value observed across all\nworkloads, which is reported as \u03a6pHq in Table 4.\n73\nAlgoPerf Authors\nA.4.2 Training Algorithms with Different Hyperparameter Search Spaces\nIn Table 21, we report additional data for the AdamW search space comparison presented in\nSection 2.3.2. Specifically, we show the results presented in Table 6 but showing a different\nbudget of tuning trials, i.e. 5 trials instead of the 20 trials reported in the main text.\nWorkload\nAdamW Narrow\nAdamW Broad\nMedian\nQ1\nQ3\nMedian\nQ1\nQ3\nCriteo 1TB\nDLRMsmall\n0.124039\n0.124016\n0.124074\n0.124319\n0.124118\n0.124634\nfastMRI\nU-Net\n0.734512\n0.734335\n0.734682\n0.733791\n0.733094\n0.734065\nImageNet\nResNet-50\n0.23382\n0.23298\n0.2357\n0.26842\n0.24708\n0.291495\nViT\n0.22324\n0.22108\n0.22526\n0.24690\n0.23924\n0.27028\nLibriSpeech\nConformer\n0.077553\n0.076817\n0.079047\n0.090477\n0.085484\n0.106039\nDeepSpeech\n0.115082\n0.112909\n0.117964\n0.131462\n0.124718\n0.155395\nOGBG\nGNN\n0.279426\n0.277748\n0.281602\n0.269102\n0.265382\n0.275955\nWMT\nTransformer\n31.1849\n30.9471\n31.2992\n30.4967\n29.2948\n30.9648\nTable 21: Performance across multiple workloads for AdamW with two different\nhyperparameter search spaces. Shown are the median, as well as the lower and upper\nquartiles (Q1 and Q3) of the best observed validation metric. The results are for a budget of\nT \u201c5 trials across 1000 simulations. This table is similar to Table 6 but showing a different\nbudget of tuning trials (5 instead of 20).\nA.4.3 Training Algorithms with Different Tuning Goals\nIn this section, we provide more details from the experiment presented in Section 2.3.2 and\nillustrated in Figure 4. For this, we ran hyperparameter tuning studies for ResNet-50\non ImageNet trained with AdamW with two different training step budgets, 186, 666 and\n233, 333. Both studies used the same search space and used 100 tuning trials of quasirandom\nsearch. They both used a cosine decay schedule and a linear learning rate warmup. Although\nthe cosine decay schedule is a function of the maximum number of training steps, the only\nlearning rate schedule parameter that was tuned beyond the peak learning rate was the\nlength of the warmup, which was tuned over three discrete options: 2%, 5%, or 10% of the\nstep budget (for the complete search space see Table 8, AdamW). We used the same seed\nfor quasirandom search in both studies to generate the exact same set of 100 hyperparameter\npoints. With the search space parameterized in this way to be relative to the training step\nbudget and with the exact same set of 100 hyperparameter points, the hyperparameter\nsetting achieving the best validation error happened to be the same across the two studies\n(Table 22). We selected the best trial based on minimum validation error achieved at any\npoint during training, not just at the end of training, but with a cosine learning rate decay\nschedule we should expect the best result to be at\u2014or near\u2014the end of training.\nB. Details for Target-Setting Experiments (Section 5)\nTables 23 and 24 provide the results of all 20 reruns of the target-setting training algorithm\nfor each workload.\n74\nBenchmarking Neural Network Training Algorithms\nHyperparameter\nValue\nBase LR\n0.00040908031497988146\nWeight decay\n0.5107969085979827\n\u03b21\n0.9978657786056152\n\u03b22\n0.9961526971493336\nWarmup\n5%\nLabel smoothing\n0.1986402587653765\nTable 22: Hyperparameter values for both runs shown in Figure 4. Although both\ntrials use a different step budget (186, 666 vs. 233, 333) the hyperparameter values found\nafter tuning are the same.\nWorkload Criteo 1TB fastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall\nU-Net\nResNet-50\nViT\nConformer DeepSpeech\nGNN\nTransformer\nMetric\nCE\u00d3\nSSIM\u00d2\nError Rate \u00d3\nWER \u00d3\nmAP\u00d2\nBLEU\u00d2\n0\n0.123585\n0.733429\n0.22484\n0.22436\n0.076226\n0.113374\n0.275715\n30.7062\n1\n0.123589\n0.733868\n0.22488\n0.2255\n0.07658\n0.115416\n0.276308\n30.7142\n2\n0.123609\n0.733994\n0.2249\n0.22606\n0.076726\n0.115487\n0.277504\n30.726\n3\n0.123619\n0.734157\n0.2251\n0.22618\n0.076862\n0.115659\n0.277586\n30.7814\n4\n0.123624\n0.734161\n0.22512\n0.22626\n0.077372\n0.11573\n0.277711\n30.7992\n5\n0.123631\n0.734162\n0.22512\n0.22628\n0.077453\n0.11574\n0.278202\n30.7993\n6\n0.123634\n0.734205\n0.22518\n0.22632\n0.077517\n0.115922\n0.278523\n30.8069\n7\n0.123635\n0.734223\n0.22534\n0.22674\n0.077599\n0.115982\n0.278594\n30.8229\n8\n0.123637\n0.734276\n0.22546\n0.2268\n0.077653\n0.116134\n0.279844\n30.829\n9\n0.123649\n0.73432\n0.22562\n0.22688\n0.078145\n0.116144\n0.280716\n30.8458\n10\n0.123649\n0.73448\n0.22576\n0.22694\n0.078809\n0.116255\n0.281243\n30.8524\n11\n0.123662\n0.734483\n0.2258\n0.22726\n0.078881\n0.116478\n0.281428\n30.8571\n12\n0.123664\n0.734532\n0.22586\n0.22766\n0.079063\n0.116791\n0.281518\n30.8631\n13\n0.123668\n0.734587\n0.22598\n0.22776\n0.079718\n0.117135\n0.281907\n30.8787\n14\n0.123673\n0.734605\n0.22598\n0.22796\n0.079718\n0.117378\n0.282107\n30.8835\n15\n0.123687\n0.734658\n0.22608\n0.2284\n0.084566\n0.117398\n0.282539\n30.9064\n16\n0.1237\n0.734674\n0.22628\n0.2284\n0.091796\n0.117438\n0.282713\n30.9196\n17\n0.123715\n0.734712\n0.22664\n0.22854\n0.093352\n0.117812\n0.282918\n30.9267\n18\n0.123728\n0.734804\n0.22706\n0.22886\n0.098336\n0.131573\n0.283148\n30.9446\n19\n0.12374\n0.735031\n0.22746\n0.2293\n0.104256\n0.171481\n0.288737\n31.0684\nmin\n0.123585\n0.733429\n0.22484\n0.22436\n0.076226\n0.113374\n0.275715\n30.7062\n25th\n0.12363\n0.734162\n0.22512\n0.226275\n0.077433\n0.115737\n0.278079\n30.7993\nmedian\n0.123649\n0.7344\n0.22569\n0.22691\n0.078477\n0.1162\n0.28098\n30.8491\n75th\n0.123676\n0.734619\n0.226005\n0.22807\n0.08093\n0.117383\n0.282215\n30.8892\nmax\n0.12374\n0.735031\n0.22746\n0.2293\n0.104256\n0.171481\n0.288737\n31.0684\nmax \u00b4 min\n0.000155\n0.001602\n0.00262\n0.00494\n0.028031\n0.058107\n0.013022\n0.3621\nmax\u00b4min\nmin\np%q 0.13\n0.22\n1.17\n2.20\n36.77\n51.25\n4.72\n1.18\nTable 23: Validation evaluation metric results of the 20 reruns of the top training\nalgorithm and hyperparameter combination for each workload.\n75\nAlgoPerf Authors\nWorkload Criteo 1TB fastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall\nU-Net\nResNet-50\nViT\nConformer DeepSpeech\nGNN\nTransformer\nMetric\nCE\u00d3\nSSIM\u00d2\nError Rate \u00d3\nWER \u00d3\nmAP\u00d2\nBLEU\u00d2\n0\n0.125989\n0.7404\n0.3371\n0.3396\n0.044773\n0.065546\n0.263254\n30.6455\n1\n0.126002\n0.74106\n0.3405\n0.3403\n0.044849\n0.066671\n0.265461\n30.7219\n2\n0.126029\n0.741231\n0.3412\n0.3408\n0.045175\n0.066862\n0.266552\n30.7898\n3\n0.126035\n0.741236\n0.3415\n0.3414\n0.04571\n0.066883\n0.267611\n30.8858\n4\n0.126039\n0.741361\n0.3417\n0.3416\n0.046055\n0.066926\n0.268155\n30.8996\n5\n0.126042\n0.741451\n0.3425\n0.3422\n0.046189\n0.067499\n0.268729\n30.9216\n6\n0.126043\n0.74157\n0.343\n0.3422\n0.046265\n0.067584\n0.268964\n30.9228\n7\n0.126044\n0.741643\n0.3432\n0.3426\n0.046476\n0.067732\n0.26905\n30.947\n8\n0.126051\n0.741652\n0.3432\n0.3428\n0.046552\n0.067944\n0.269792\n30.9692\n9\n0.126052\n0.741671\n0.3434\n0.3432\n0.046629\n0.067944\n0.269988\n30.9959\n10\n0.126054\n0.741699\n0.3436\n0.3432\n0.046763\n0.068008\n0.270238\n30.9987\n11\n0.12606\n0.741721\n0.3437\n0.3437\n0.046954\n0.068029\n0.270672\n31.005\n12\n0.126071\n0.741738\n0.3438\n0.3438\n0.046973\n0.068093\n0.270756\n31.0129\n13\n0.126076\n0.741778\n0.3438\n0.3439\n0.047184\n0.068369\n0.271233\n31.0221\n14\n0.126076\n0.741827\n0.3439\n0.3444\n0.048638\n0.068454\n0.271476\n31.0408\n15\n0.126087\n0.7419\n0.344\n0.3451\n0.050551\n0.069112\n0.272067\n31.0412\n16\n0.126087\n0.741909\n0.3449\n0.3451\n0.055086\n0.069282\n0.272105\n31.1161\n17\n0.126126\n0.741915\n0.3453\n0.3457\n0.057286\n0.069494\n0.27285\n31.1407\n18\n0.126128\n0.742025\n0.3462\n0.3466\n0.060749\n0.081402\n0.276531\n31.1446\n19\n0.126136\n0.742195\n0.3481\n0.3481\n0.066317\n0.110948\n0.276976\n31.1679\nmin\n0.125989\n0.7404\n0.3371\n0.3396\n0.044773\n0.065546\n0.263254\n30.6455\n25th\n0.126041\n0.741428\n0.3423\n0.34205\n0.046155\n0.067355\n0.268586\n30.9161\nmedian\n0.126053\n0.741685\n0.3435\n0.3432\n0.046696\n0.067976\n0.270113\n30.9973\n75th\n0.126079\n0.741845\n0.343925\n0.344575\n0.049116\n0.068618\n0.271624\n31.0409\nmax\n0.126136\n0.742195\n0.3481\n0.3481\n0.066317\n0.110948\n0.276976\n31.1679\nmax \u00b4 min\n0.000148\n0.001796\n0.011\n0.0085\n0.021544\n0.045402\n0.013722\n0.5225\nmax\u00b4min\nmin\np%q 0.12\n0.24\n3.26\n2.50\n48.12\n69.27\n5.21\n1.70\nTable 24: Test evaluation metric results of the 20 reruns of the top training\nalgorithm and hyperparameter combination for each workload.\n76\nBenchmarking Neural Network Training Algorithms\nC. Details for Baseline Experiments (Section 7)\nIn this section, we provide additional results from the baseline experiments presented in\nSection 7. Tables 25 and 26 report the total runtime and number of steps needed to achieve\nthe target performance for all baselines. The benchmark scores for all baselines presented\nin the paper are listed in Table 27. Table 28 presents the estimated runtimes for a given\nnumber of steps of the different training algorithms used in the baselines. We report the 20\nhyperparameter points used in our OptList baselines in Tables 29 to 32.\nAlgorithm\nCriteo 1TB\nfastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall\nU-Net\nResNet-50\nViT\nConformer DeepSpeech\nGNN\nTransformer\nAdamW\ntuned \u03b21\n5622\ninf\ninf\n62,667\n95,222\n80,106\ninf\n40,534\nfixed \u03b21\n5320\n7473\ninf\n62,667\ninf\n81,946\ninf\n41,499\nopt-list\n5471\n6415\ninf\n64,213\n88,135\n76,427\ninf\n44,391\nHeavy Ball\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nfixed \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nopt-list\ninf\ninf\n57,321\ninf\ninf\ninf\ninf\n43,984\nLAMB\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\n78,966\ninf\n29,962\nNadamW\ntuned \u03b21\n5850\n8559\ninf\n62,005\n92,558\n79,569\n11,441\n30,822\nfixed \u03b21\n5544\ninf\n61,049\n60,457\ninf\n79,569\ninf\n43,329\nopt-list\n5544\n8205\ninf\n59,682\n87,475\n77,721\n12,914\n44,291\nNesterov\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nfixed \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nopt-list\ninf\n8750\n59,330\ninf\ninf\ninf\ninf\ninf\nAdafactor\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\n82,214\ninf\n37,679\nSAM(w. Adam)\ntuned \u03b21\n5910\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nTable 25: Total runtime to achieve the target performance for different baselines.\nThese numbers are used to plot the performance profiles in Figure 5a and Figure 6a. A value\nof inf indicates that that baseline was unable to achieve the target within the maximum\nallowed runtime.\nD. Workload Details\nD.1 Criteo 1TB\nWe train on the Criteo 1TB Click Logs dataset (Lab, 2014) to train a standard ads rec-\nommender model, DLRM (Naumov et al., 2019) to predict the CTR. The dataset contains\nexamples with 13 numerical features and 26 categorical. The numerical features are log-\ntransformed and the 26 categorical features are hashed into a single embedding table. The\ndata is from 24 days of click data, split into one file per day. We use the first 23 days as the\ntraining split, resulting in 4, 195, 197, 692 training examples. We then use the first half of\n77\nAlgoPerf Authors\nAlgorithm\nCriteo 1TB\nfastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall\nU-Net\nResNet-50\nViT\nConformer DeepSpeech\nGNN\nTransformer\nAdamW\ntuned \u03b21\n7881\ninf\ninf\n151,146\n250,604\n69,600\ninf\n111,972\nfixed \u03b21\n7455\n30,324\ninf\n151,146\ninf\n71,200\ninf\n114,638\nopt-list\n7668\n25,992\ninf\n154,878\n231,942\n66,400\ninf\n122,636\nDistributed Shampoo\ntuned \u03b21\ninf\n32,851\n181,002\n138,084\n229,276\n67,200\n35,200\n90,644\nHeavy Ball\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nfixed \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nopt-list\ninf\ninf\n169,806\ninf\ninf\ninf\ninf\n122,636\nLAMB\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\n68,800\ninf\n79,980\nNadamW\ntuned \u03b21\n8094\n34,656\ninf\n149,280\n242,606\n68,800\n49,600\n85,312\nfixed \u03b21\n7668\ninf\n181,002\n145,548\ninf\n68,800\ninf\n119,970\nopt-list\n7668\n33,212\ninf\n143,682\n229,276\n67,200\n56,000\n122,636\nNesterov\ntuned \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nfixed \u03b21\ninf\ninf\ninf\ninf\ninf\ninf\ninf\ninf\nopt-list\ninf\n35,739\n175,404\ninf\ninf\ninf\ninf\ninf\nAdafactor\ntuned \u03b21\ninf\ninf\ninf\n158,610\n261,268\n70,400\ninf\n98,642\nSAM(w. Adam)\ntuned \u03b21\n8307\ninf\n166,118\n147,414\n231,942\n67,200\ninf\n98,642\nTable 26: Number of steps to achieve the target performance for different base-\nlines. These numbers are used to plot the performance profiles in Figure 5b and Figure 6b.\nA value of inf indicates that that baseline was unable to achieve the target in the maximum\nallowed step budget.\n78\nBenchmarking Neural Network Training Algorithms\nSubmission\nVersion\nBenchmark Score\nRuntime\nSteps\nAdamW\ntuned \u03b21\n0.600141\n0.596116\nfixed \u03b21\n0.596985\n0.593047\nopt-list\n0.725260\n0.721035\nDistributed Shampoo\ntuned \u03b21\n-\n0.85421\nHeavy Ball\ntuned \u03b21\n0\n0\nfixed \u03b21\n0\n0\nopt-list\n0.230504\n0.22686\nLAMB\ntuned \u03b21\n0.248618\n0.248494\nNadamW\ntuned \u03b21\n0.849960\n0.595478\nfixed \u03b21\n0.599691\n0.830414\nopt-list\n0.835602\n0.813194\nNesterov\ntuned \u03b21\n0\n0\nfixed \u03b21\n0\n0\nopt-list\n0.233373\n0.232048\nAdafactor\ntuned \u03b21\n0.236111\n0.47576\nSAM(w. Adam)\ntuned \u03b21\n0.120368\n0.731717\nTable 27: Benchmark scores for all baselines presented in the paper.\nAlgorithm\nCriteo 1TB fastMRI\nImageNet\nLibriSpeech\nOGBG\nWMT\nDLRMsmall\nU-Net\nResNet-50\nViT\nConformer DeepSpeech\nGNN\nTransformer\nSteps\n10,667\n36,189\n186,667\n186,667\n133,333\n80,000\n80,000\n133,333\nAdamW\n7602\n8906\n62,827\n77,380\n101,368\n92,064\n18,361\n48,261\nHeavy Ball\n7042\n8962\n63,008\n76,535\n105,506\n91,460\n18,303\n47,818\nLAMB\n7166\n8972\n67,797\n85,254\n101,982\n91,802\n19,330\n49,911\nNadamW\n7703\n8935\n62,958\n77,520\n101,780\n92,509\n18,438\n48,151\nNesterov\n7097\n8859\n63,137\n78,791\n105,108\n90,874\n18,477\n47,855\nAdafactor\n7419\n9058\n67,381\n93,895\n122,920\n93,404\n20,379\n50,896\nSAM(w. Adam)\n7583\n9048\n117,997\n146,507\n196,144\n180,832\n20,852\n95,139\nTable 28: Runtime measurements (rounded to the nearest second) for different\ntraining algorithms used in baselines. For each workload and training algorithm we\nmeasure the runtime for the mentioned number of steps. The highlighted entry for each\nworkload corresponds to the runtime for the target-setting algorithm for that workload and\nthus defines our maximum allowed runtime for each workload.\n79\nAlgoPerf Authors\nLearning\n\u03b21\n\u03b22\nWeight\nWarmup\nDropout\nAux. Dropout\nLabel\nRate\nDecay\nSmoothing\n0.002995\n0.960644\n0.998968\n0.006842\n5\n0\n0\n0\n0.003331\n0.948\n0.998793\n0.003578\n2\n0.1\n0.1\n0\n0.007502\n0.86932\n0.989658\n0.000071\n10\n0\n0\n0\n0.000644\n0.931783\n0.969068\n0.677756\n5\n0\n0\n0\n0.002827\n0.885906\n0.820717\n0.185407\n2\n0\n0\n0\n0.001308\n0.973133\n0.998123\n0.163753\n10\n0\n0\n0.1\n0.018978\n0.966607\n0.996816\n0.015654\n5\n0.1\n0.1\n0.2\n0.001949\n0.532796\n0.981278\n0.037642\n5\n0\n0\n0.1\n0.000845\n0.889576\n0.99785\n0.081354\n5\n0\n0\n0.2\n0.000987\n0.99139\n0.993211\n0.00835\n10\n0\n0\n0.2\n0.002107\n0.823119\n0.877457\n0.275905\n2\n0.1\n0.1\n0\n0.002234\n0.751323\n0.612946\n0.215092\n5\n0\n0\n0\n0.003926\n0.813913\n0.987628\n0.028657\n2\n0\n0\n0\n0.001949\n0.532796\n0.981278\n0.037642\n5\n0\n0\n0\n0.004958\n0.863744\n0.629185\n0.114739\n2\n0\n0\n0\n0.000584\n0.962501\n0.998687\n0.000148\n5\n0.1\n0.1\n0.1\n0.000504\n0.923701\n0.99475\n0.000012\n5\n0.1\n0.1\n0.2\n0.000388\n0.674831\n0.946874\n0.124668\n10\n0.1\n0.1\n0\n0.001749\n0.932661\n0.995516\n0.081216\n2\n0.1\n0.1\n0\n0.000275\n0.788345\n0.076547\n0.023545\n2\n0.1\n0.1\n0.2\nTable 29: OptList for AdamW. The learning rate schedule used is warmup ` cosine\ndecay.\nLearning\n\u03b21\n\u03b22\nWeight\nWarmup\nDropout\nAux. Dropout\nLabel\nRate\nDecay\nSmoothing\n0.003331\n0.948\n0.998793\n0.003578\n2\n0.1\n0.1\n0\n0.001614\n0.959792\n0.998463\n0.000033\n5\n0\n0\n0\n0.010937\n0.974179\n0.998111\n0.007607\n5\n0.1\n0.1\n0\n0.005146\n0.994362\n0.994663\n0.246009\n10\n0\n0\n0\n0.002827\n0.885906\n0.820717\n0.185407\n2\n0\n0\n0\n0.001308\n0.973133\n0.998123\n0.163753\n10\n0\n0\n0.1\n0.018978\n0.966607\n0.996816\n0.015654\n5\n0.1\n0.1\n0.2\n0.000845\n0.889576\n0.99785\n0.081354\n5\n0\n0\n0.2\n0.001949\n0.532796\n0.981278\n0.037642\n5\n0\n0\n0.1\n0.000987\n0.99139\n0.993211\n0.00835\n10\n0\n0\n0.2\n0.001308\n0.973133\n0.998123\n0.163753\n10\n0\n0\n0\n0.000845\n0.889576\n0.99785\n0.081354\n5\n0.1\n0.1\n0\n0.004958\n0.863744\n0.629185\n0.114739\n2\n0\n0\n0\n0.003528\n0.819231\n0.495851\n0.043397\n10\n0\n0\n0\n0.001308\n0.973133\n0.998123\n0.163753\n10\n0\n0\n0\n0.003296\n0.996693\n0.998649\n0.003729\n10\n0.1\n0.1\n0.1\n0.000584\n0.962501\n0.998687\n0.000148\n5\n0.1\n0.1\n0.1\n0.000279\n0.991934\n0.997984\n0.000324\n10\n0.1\n0.1\n0.1\n0.001749\n0.932661\n0.995516\n0.081216\n2\n0.1\n0.1\n0\n0.001011\n0.712472\n0.966607\n0.000069\n5\n0.1\n0.1\n0.1\nTable 30: OptList for NadamW. The learning rate schedule used is warmup ` cosine\ndecay.\n80\nBenchmarking Neural Network Training Algorithms\nLearning\n\u03b21\nWeight\nWarmup\nDecay\nEnd\nDropout\nAux.\nLabel\nRate\nDecay\nSteps\nFactor\nDropout\nSmoothing\n0.333132\n0.948\n1.40e-7\n5\n0.942079\n0.01\n0.1\n0.1\n0\n0.082037\n0.980735\n1.01e-6\n5\n0.891621\n0.01\n0.1\n0.1\n0\n0.810523\n0.898228\n1.00e-7\n5\n0.842587\n0.01\n0.1\n0.1\n0\n0.028609\n0.981543\n5.77e-4\n5\n0.984398\n0.01\n0\n0\n0\n0.416058\n0.970426\n1.99e-5\n5\n0.936585\n0.01\n0\n0\n0\n4.131896\n0.927476\n5.67e-6\n5\n0.900777\n0.001\n0\n0\n0.2\n0.191165\n0.995978\n3.83e-6\n5\n0.871275\n0.01\n0.1\n0.1\n0.2\n1.376742\n0.736477\n5.09e-6\n5\n0.977277\n0.01\n0\n0\n0.2\n0.032559\n0.988578\n3.32e-6\n5\n0.876362\n0.001\n0\n0\n0.1\n0.130821\n0.973133\n2.90e-7\n5\n0.816545\n0.001\n0\n0\n0.2\n0.022941\n0.984057\n2.40e-7\n5\n0.924988\n0.01\n0.1\n0.1\n0\n0.010036\n0.986308\n3.22e-5\n5\n0.994571\n0.01\n0\n0\n0\n0.026287\n0.992389\n3.88e-4\n5\n0.945944\n0.01\n0.1\n0.1\n0\n0.014244\n0.970264\n4.22e-4\n5\n0.940451\n0.01\n0\n0\n0\n0.019827\n0.95789\n2.41e-4\n5\n0.80861\n0.001\n0.1\n0.1\n0\n2.491773\n0.944937\n1.30e-7\n5\n0.861509\n0.001\n0.1\n0.1\n0\n2.051309\n0.917965\n4.58e-6\n5\n0.82041\n0.001\n0.1\n0.1\n0.1\n1.897755\n0.966607\n6.90e-7\n5\n0.987857\n0.01\n0.1\n0.1\n0.1\n0.169804\n0.99636\n1.03e-6\n5\n0.998233\n0.001\n0.1\n0.1\n0.1\n0.253647\n0.989819\n1.15e-6\n5\n0.932109\n0.01\n0\n0\n0.1\nTable 31: OptList for Nesterov. The learning rate schedule used is warmup ` linear\ndecay ` constant.\nLearning\n\u03b21\nWeight\nWarmup\nDecay\nDecay\nDropout\nAux.\nLabel\nRate\nDecay\nSteps\nFactor\nDropout\nSmoothing\n0.299534\n0.960644\n1.10e-7\n5\n0.839739\n0.01\n0.1\n0.1\n0\n5.133865\n0.673928\n2.50e-7\n5\n0.98745\n0.01\n0\n0\n0\n1.042065\n0.862619\n3.90e-7\n5\n0.920716\n0.01\n0.1\n0.1\n0\n0.028609\n0.981543\n5.77e-4\n5\n0.984398\n0.01\n0\n0\n0\n0.416058\n0.970426\n1.99e-5\n5\n0.936585\n0.01\n0\n0\n0\n4.131896\n0.927476\n5.67e-6\n5\n0.900777\n0.001\n0\n0\n0.2\n7.263293\n0.860749\n2.81e-6\n5\n0.889586\n0.01\n0.1\n0.1\n0.2\n1.376742\n0.736477\n5.09e-6\n5\n0.977277\n0.01\n0\n0\n0.2\n1.111547\n0.589943\n6.31e-6\n5\n0.911763\n0.01\n0\n0\n0.1\n0.392622\n0.813913\n6.62e-6\n5\n0.854795\n0.01\n0\n0\n0.2\n0.130821\n0.973133\n2.90e-7\n5\n0.816545\n0.001\n0.1\n0.1\n0\n0.022941\n0.984057\n2.40e-7\n5\n0.924988\n0.01\n0.1\n0.1\n0\n0.142788\n0.961398\n1.83e-6\n5\n0.84625\n0.01\n0\n0\n0\n0.027867\n0.991934\n3.20e-7\n5\n0.923564\n0.001\n0\n0\n0\n0.026287\n0.992389\n3.88e-4\n5\n0.945944\n0.01\n0.1\n0.1\n0\n2.051309\n0.917965\n4.58e-6\n5\n0.82041\n0.001\n0.1\n0.1\n0.1\n1.897755\n0.966607\n6.90e-7\n5\n0.987857\n0.01\n0.1\n0.1\n0.1\n2.491773\n0.944937\n1.30e-7\n5\n0.861509\n0.001\n0.1\n0.1\n0\n0.426111\n0.995127\n9.80e-7\n5\n0.934347\n0.001\n0\n0\n0.2\n0.169804\n0.99636\n1.03e-6\n5\n0.998233\n0.001\n0.1\n0.1\n0.1\nTable 32: OptList for Heavy Ball. The learning rate schedule used is warmup ` linear\ndecay ` constant.\n81\nAlgoPerf Authors\nthe 24th day as the test set, and the second half of the 24th day as a validation set, resulting\nin 89, 137, 319 validation and 89, 137, 318 test examples. See here for the implementation\nof our Criteo 1TB input pipeline. Unlike many other Criteo 1TB pipelines, instead of\nthe AUC we use the sigmoid binary cross entropy loss as an evaluation metric. We do\nthis to have a metric that decomposes elementwise, which avoids requiring submitters to\nrun expensive AUC evaluations that would have required maintaining arrays the size of the\nroughly 89 million evaluation examples.\nWe split the training split into files with 5, 000, 000 lines each and randomly shuffle\nthem. However, the training time budget we set typically only allows for around 60% of an\nepoch to be consumed at the batch sizes we can fit into memory. In our paper experiment\ncodebase, we reshuffle the dataset each time we have a preemption, which could result in\nrepeated examples, but in the benchmark codebase we do not assume that the code runs\non preemptable instances so this is not a concern.\nD.1.1 DLRMsmall Model\nThe concatenated features form the input layer for the DLRM model (Naumov et al., 2019).\nThe single embedding table is of size 4M entries with an embedding dimension of 128. The\ndense features are fed into a three-layer fully-connected network with 512, 256, 128 units\nper layer. The outputs of this layer are then concatenated to the embedding lookups of the\ncategorical features, and fed into the cross-interaction layer. Finally, the cross-interaction\noutput is passed into a five-layer fully-connected network with 1024, 1024, 512, 256, 1 units\nper layer. All layers use a ReLU activation, except for the final 1d output. A dropout layer\nfollows the 512 dimensional layer in the second network, after the ReLU activation. See\nhere for our DLRM model in Jax and here for our DLRM model in PyTorch.\nD.1.2 Criteo 1TB DLRMsmall Workload Variants\nThe three workload variants for the DLRMsmall model are:\n\u2022 Embed Init Scale:\nWe changed the initialization of the embedding layer from\nN\n\u00b4\n0,\n1\n?\n4194304\n\u00af\n\u00ab N p0, 0.00049q (where 4194304 is the vocabulary size), to N p0, 1q.\n\u2022 LayerNorm: Layer normalization was added after the activations of each layer,\nexcept for the final 1d output.\n\u2022 Residual: For every layer after the first layer of each subnetwork, instead of being a\nsimple fully-connected layer, it is the residual branch in a residual subnetwork.\nD.2 fastMRI\nThe fastMRI (Zbontar et al., 2018) dataset was released by NYU Langone Health as\npart of a challenge organized in collaboration with Facebook AI Research (FAIR, now\nMeta AI). The challenge aims to reduce the time to acquire an MRI scan by up to a\nfactor of ten without any loss in diagnostic quality. In MRI acquisition, a \u201cpulse sequence\u201d\nof spatially- and temporally-varying magnetic fields induces the subject\u2019s body to emit\nelectromagnetic response fields that are measured by one or more receiver coils placed\nnear the area to be imaged. Using these fine-grained Fourier-space measurements (k-space\n82\nBenchmarking Neural Network Training Algorithms\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.123609\nEmbed Init Scale\n0.123800\n0.123880\n3\n0.123920\n16\nLayerNorm\n0.123653\n0.123797\n14\n0.127778\n136\nResidual\n0.123920\n0.124010\n5\n0.123860\n13\nTable 33: Hyperparameter transfer between the base workload and the vari-\nants of DLRMsmall on Criteo 1TB. We show the validation performance of the base\nworkload compared to the performance achieved by the variants. Further, we show the\nperformance (and hyperparameter ranking) when using the optimal hyperparameter point\nfrom the base workload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1\nBase). All runs are from the same search space for NadamW. See Section 6.2 for a detailed\ndescription of our protocol for accepting workload variants.\nin the medical literature), clinicians reconstruct volumetric images with high-quality soft\ntissue contrast. However, while powerful, these images can take 30 minutes or more to\nproduce, causing logistical issues ranging from patient discomfort to low patient throughput.\nRecent strides in machine learning seek to reduce this time by reconstructing the volumetric\nimages from sub-sampled k-space measurements. The fastMRI task provides raw k-space\ndata, randomly masks this data to simulate sub-sampling, and asks a supplied algorithm\nto faithfully reconstruct the image as compared to the inverse Fourier transform of the\ncomplete (non-sub-sampled) k-space data.\nIn this benchmark, we use fastMRI\u2019s single-coil knee data which is organized into\nvolumes, with each volume coming from one patient and being composed of a collection\nof slices (i.e., images). In this task, we treat each slice as an independent example. The\nofficial data set contains 34, 742 training slices (from 973 volumes), and 7, 135 validation\nslices (from 199 volumes). In the data pipeline, we use a fixed random sequence to mask\ncertain columns (down-sampling by a factor of four) from each raw k-space slice before\napplying an inverse Fourier transform and normalizing the resulting image.\nThe target\nground-truth image is normalized using the same mean and standard deviation. Since the\nofficial test set targets are not publicly available, we split the validation set roughly in half\nto obtain disjoint validation and test sets\u2014the first 100 validation HDF5 files are used for\nvalidation and contain 3, 554 slices, while the final 99 validation HDF5 are used for test and\ncontain 3, 581 slices. See here for the implementation of our fastMRI input pipeline.\nD.2.1 U-Net Model\nWe train a U-Net model similar to the one described in Ronneberger et al. (2015). Our U-\nNet implementation has 32 channels, four down-sampling convolutional blocks, and four\nup-sampling transpose convolutional/convolutional block pairs. Each layer uses dropout\nwith a rate that may be tuned by the submission; the default dropout rate is 0.0. See here\n83\nAlgoPerf Authors\nfor the implementation of our U-Net model in Jax and here for the implementation of our\nU-Net model in PyTorch.\nD.2.2 fastMRI U-Net Workload Variants\nThe three workload variants for the U-Net model are:\n\u2022 Channels & Pooling: The base number of channels in each convolution block was\nincreased to 64 (which is multiplied by another factor of 2 with each down or up\nsample level), and the number of down and up sample layers was decreased to 3.\n\u2022 TanH: Activation functions were swapped to TanH.\n\u2022 LayerNorm: Instance normalization was swapped to layer normalization (which has\nlearnable parameters).\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.734523\nChannels & Pooling\n0.734603\n0.734172\n12\n0.734277\n8\nTanH\n0.729743\n0.728203\n6\n0.734438\n3\nLayerNorm\n0.734968\n0.733304\n26\n0.734364\n6\nTable 34: Hyperparameter transfer between the base workload and the variants\nof U-Net on fastMRI. We show the validation performance of the base workload com-\npared to the performance achieved by the variants.\nFurther, we show the performance\n(and hyperparameter ranking) when using the optimal hyperparameter point from the base\nworkload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs\nare from the same search space for NadamW. See Section 6.2 for a detailed description of\nour protocol for accepting workload variants.\nD.3 ImageNet\nFor our ImageNet workload we used the ILSVRC 2012 training and validation sets as\nthe training and validation splits (Deng et al., 2009), and ImageNet-v2 as the test split\n(Recht et al., 2019). For training preprocessing we take a random crop and randomly flip\nthe image, whereas for the validation and test sets we just take a center crop (colloquially\nreferred to as \u201dResNet preprocessing\u201d). Images are fed into the model normalized to r0, 1s.\nWe used RandAugment (Cubuk et al., 2020) and mixup (Zhang et al., 2018) for our ViT\nworkload but not for ResNet. Code for our ImageNet input pipelines can be found here\nin tf.data, here in PyTorch, and here for ImageNet-v2.\nD.3.1 ResNet-50 Model\nWith the exception of Section 2.2.1, all experiments use the ResNet-50 defined in He\net al. (2016a, Section 4.1). We use ghost batch normalization with a virtual batch size of 64\n84\nBenchmarking Neural Network Training Algorithms\n(Hoffer et al., 2017). To further improve optimization stability, the scales in the final batch\nnormalization layer in each residual block are initialized to all zeros. This has the effect of\ninitializing each residual block to be the identity function. See here for our implementation\nof ResNet-50 in JAX and here for our implementation of ResNet-50 in PyTorch.\nIn Section 2.2.1 we ran experiments on an unstable 200-layer ResNetV2 architecture\n(He et al., 2016b). These experiments also used a virtual batch size of 64. In contrast to\nthe above ResNet-50, we initialize all batch normalization layers in the default way (with\nscalar factor all initialized to 1). This was necessary to produce the training instability\nat initialization. The extra batch normalization layer was added right after every residual\nconnection, so each residual block returns BNpx ` Fpxqq instead of the standard x ` Fpxq.\nD.3.2 ImageNet ResNet-50 Workload Variants\nThe three workload variants for the U-Net model are:\n\u2022 SiLU: Changed activation functions to SiLU (Elfwing et al., 2018).\n\u2022 GELU: Changed activation functions to GELU (Hendrycks and Gimpel, 2016).\n\u2022 BN Init Scale: Changed the initialization of the scale variable in the final batch\nnormalization layer in the residual branches from 0.0 to 8.0.\nIn Table 35 we present the results of our protocol to test our variants (described in\nSection 6.2). We observe that the ranks of hyperparameter transfers were not as high as\nvariants in other models. In such cases we tested these models along an alternative protocol\nto ensure that the optimum learning rate is indeed different for these models.\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.22702\nSiLU\n0.21996\n0.24234\n9\n0.22980\n2\nGELU\n0.22148\n0.22858\n3\n0.22980\n2\nBN Init Scale\n0.23502\n0.23502\n1\n0.22702\n1\nTable 35: Hyperparameter transfer between the base workload and the variants\nof ResNet-50 on ImageNet. We show the validation performance of the base workload\ncompared to the performance achieved by the variants. Further, we show the performance\n(and hyperparameter ranking) when using the optimal hyperparameter point from the base\nworkload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs\nare from the same search space for NadamW. See Section 6.2 for a detailed description of\nour protocol for accepting workload variants.\nD.3.3 Vision Transformer Model\nFor all experiments, we use the S/16 variant of the Vision Transformer (ViT) (Dosovit-\nskiy et al., 2021) as enumerated in Steiner et al. (2022) with no regularization and light2\n85\nAlgoPerf Authors\ndata augmentation.\nWe chose the S/16 variant given its relatively small size (width of\n384, depth of 12, MLP dimension of 1536, with 6 heads and a 16 \u02c6 16 image patch size).\nSteiner et al. (2022) define light2 data augmentation as using mixup (Zhang et al., 2018)\nper batch, with \u03b1 \u201c 0.2 and RandAugment (Cubuk et al., 2020) per image with two layers\nand magnitude 15. See here for our implementation of Vision Transformer in Jax and\nhere for our implementation of Vision Transformer in PyTorch.\nD.3.4 ImageNet Vision Transformer Workload Variants\nThe three workload variants for the U-Net model are:\n\u2022 Post-LN: Layer normalization was applied after the residual branch was added back\ninto the trunk.\n\u2022 MAP: The pooling layer type was changed from global average pooling to multihead\nattention pooling.\n\u2022 GLU: We included gated linear units in the MLPBlock.\nIn Table 36 we present the results of our protocol to test our variants (described in Sec-\ntion 6.2). We observe that the ranks of hyperparameter transfers were 0 for two variants.\nFor these variants we tested these models along an alternative protocol to ensure that the\noptimum learning rate is indeed different for these models.\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.22530\nPost-LN\n0.24794\n0.24794\n14\n0.28478\n5\nMAP\n0.23004\n0.23004\n0\n0.22530\n0\nGLU\n0.22258\n0.22258\n0\n0.22530\n0\nTable 36: Hyperparameter transfer between the base workload and the variants\nof ViT on ImageNet. We show the validation performance of the base workload com-\npared to the performance achieved by the variants.\nFurther, we show the performance\n(and hyperparameter ranking) when using the optimal hyperparameter point from the base\nworkload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs\nare from the same search space for NadamW. See Section 6.2 for a detailed description of\nour protocol for accepting workload variants.\nD.4 LibriSpeech\nThe LibriSpeech dataset (Panayotov et al., 2015) is a corpus of read English speech with\nsampling rate of 16 kHz. We train our speech recognition models on the combination of\ntrain-clean-100, train-clean-360, and train-other-500 splits from the LibriSpeech\ndataset giving us 960 hours of raw audio data. For validation we use a combination of\n86\nBenchmarking Neural Network Training Algorithms\ndev-clean and dev-other splits resulting in 5567 examples in the validation set.\nWe\nreport the word error rates on the test-clean split as our test set performance.\nWe preprocess the data by eliminating any examples with audio length greater than\n320k and a target sentence length greater than 256. We then compute logmel spectrogram\nfeatures for the raw audio input and use the SentencePiece (Kudo and Richardson, 2018)\ntokenizer with a vocab size of 1024 to tokenize the target sentences. We pad the sequences\nto bring all examples to same length and handle paddings inside the model and while\ncomputing metrics like loss and word error rate. See here for the implementation of our\nLibriSpeech input pipeline.\nD.4.1 Conformer Model\nConformer (Gulati et al., 2020) is an architecture combing attention and convolution\nlayers to capture both global and local relationships in input audio. We use a 4-layer deep\nConformer model with model encoder dimension of 512. Model weights are initialized\nusing Xavier uniform initialization. CTC loss (Graves et al., 2006) is used to train the model.\nSee here for our implementation of Conformer in Jax and here for our implementation of\nConformer in PyTorch.\nInference\nWe use a greedy decoding procedure to generate decoded logits which are\nthen transformed back into sentences using SentencePiece (Kudo and Richardson, 2018)\ntokenizer to compute word error rates.\nD.4.2 LibriSpeech Conformer Workload Variants\nThe three workload variants for the Conformer model are:\n\u2022 GELU: Activations functions were changed to GELU.\n\u2022 LayerNorm change The layer normalization before the final readout layer was re-\nmoved.\n\u2022 Attention Temp The output of the attention softmax was multiplied by a temper-\nature constant of 1.6. Note that this is different than other attention temperature\nsetups where the temperature is multiplied in before the softmax.\nIn Table 37 we present the results of our protocol to test our variants (described in\nSection 6.2). We observe that the ranks of hyperparameter transfers were not as high as\nvariants in other models. In such cases we tested these models along an alternative protocol\nto ensure that the optimum learning rate is indeed different for these models.\nD.4.3 DeepSpeech Model\nWe use a variant of the DeepSpeech (Amodei et al., 2016) model with residual connections,\ndropout (Srivastava et al., 2014), layer normalization, and SpecAugment (Park et al.,\n2019) to improve performance. We use a convolution subsampling layer to reduce input\ndimensions by a factor of 4, which is further passed through 6 bi-directional LSTM layers\nand 3 feed-forward layers with a model internal dimension of 512 across layers. We use\nbatch normalization inside the LSTM and feed-forward layers as post normalization layers.\nModel weights are initialized using Xavier uniform initialization. The CTC loss (Graves\n87\nAlgoPerf Authors\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.077790\nGELU\n0.079354\n0.079391\n2\n0.085503\n3\nLayerNorm Change\n0.084175\n0.084175\n1\n0.229800\n1\nAttention Temp\n0.083092\n0.083092\n1\n0.077790\n1\nTable 37: Hyperparameter transfer between the base workload and the variants\nof Conformer on LibriSpeech. We show the validation performance of the base workload\ncompared to the performance achieved by the variants. Further, we show the performance\n(and hyperparameter ranking) when using the optimal hyperparameter point from the base\nworkload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs\nare from the same search space for NadamW. See Section 6.2 for a detailed description of\nour protocol for accepting workload variants.\net al., 2006) is used to train the model. See here for our implementation of DeepSpeech\nin Jax and here for our implementation of DeepSpeech in PyTorch. For experiments in\nthis paper, we used a slightly different word piece tokenizer than Conformer, but we plan\nto update the tokenizer before the call for submissions to also use the same SentencePiece\n(Kudo and Richardson, 2018) tokenizer.\nWe expect the DeepSpeech validation WER\ntarget to improve from 0.1162 to 0.111825.\nD.4.4 LibriSpeech DeepSpeech Workload Variants\nThe three workload variants for the DeepSpeech model are:\n\u2022 TanH: Activation functions were changed to TanH.\n\u2022 No residual: We removed the residual connections in the model. Interestingly this\nimproved the overall performance of the model.\n\u2022 Norm & SpecAugment: We removed the decoder layer normalization layer. We\nreplaced all other batch normalization layers with layer normalization layers.\nWe\nchanged SpecAugment specifications, specifically the Frequency Mask from 2 to\n4 and the Time Mask from 10 to 15.\nD.5 OGBG\nWe use the OGBG-MOLPCBA dataset (Hu et al., 2020) containing molecular graphs and\n128 molecular properties. The goal is to predict the properties given the graphs. The graphs\ncontain on average 26 nodes and 28 edges. The train split contains 350, 343 examples, and\nthe train and validation splits both contain 43, 793 examples.\nBecause the graphs are of varying sizes, we construct the batches using dynamic batch-\ning. We specify a maximum number of nodes and edges per batch and take graphs from the\ndataset into the batch until one of these thresholds is exceeded. We set these thresholds to\n88\nBenchmarking Neural Network Training Algorithms\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.113950\nTanH\n0.131300\n0.177558\n32\n0.122959\n9\nNo Residual\n0.105063\n0.135881\n23\n0.116508\n2\nNorm & SpecAugment\n0.130967\n0.141442\n6\n0.117388\n3\nTable 38: Hyperparameter transfer between the base workload and the vari-\nants of DeepSpeech on LibriSpeech. We show the validation performance of the base\nworkload compared to the performance achieved by the variants. Further, we show the\nperformance (and hyperparameter ranking) when using the optimal hyperparameter point\nfrom the base workload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1\nBase). All runs are from the same search space for NadamW. See Section 6.2 for a detailed\ndescription of our protocol for accepting workload variants.\nBatchSize \u00a8 AvgNumNodes and 2 \u00a8 BatchSize \u00a8 AvgNumEdges (the factor of 2 accounts for\nthe bidirectional nature of the edges), respectively. See here for the implementation of our\nOGBG input pipeline.\nD.5.1 GNN Model\nThe model is defined as a graph neural network (GNN) (Battaglia et al., 2018), which is a\ngeneralization of graph architectures such as GIN (Xu et al., 2019). We use the implemen-\ntation provided by the Jraph library (Godwin et al., 2020). The model first performs an\nembedding step which transforms the node and edge features with a linear embedding of size\n256, and creates global features of size 128 initialized to zeros. Then the model performs 5\nmessage passing steps following Battaglia et al. (2018, Algorithm 1) which update the node,\nedge, and global features. Each of the update functions is a 1-layer fully-connected network\nwith a dense layer of size 256, layer normalization, ReLU, and dropout. The dropout rate\ncan be tuned by the submissions but it defaults to 0.1. The weights for the model are\ninitialized using LeCun normal initialization (Klambauer et al., 2017). The final output is\nread from the global features after the last step. See here for our implementation of GNN\nin Jax and here for our implementation of GNN in PyTorch.\nD.5.2 OGBG GNN Workload Variants\nThe three workload variants for the GNN model are:\n\u2022 GELU: The activation was changed to GELU.\n\u2022 SiLU: The activation was changed to SiLU.\n\u2022 Altered Layers: An additional hidden layer of width 256 was added to each fully\nconnected network, the latent dim was reduced to 128, and the number of message\n89\nAlgoPerf Authors\npassing steps was reduced to 3. Also, the layer normalization layers were swapped for\nbatch normalization.\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n0.280012\nGELU\n0.284251\n0.261344\n25\n0.267038\n17\nSiLU\n0.287569\n0.258375\n28\n0.275818\n3\nAltered Layers\n0.269345\n0.252153\n15\n0.244109\n44\nTable 39: Hyperparameter transfer between the base workload and the variants\nof GNN on OGBG. We show the validation performance of the base workload compared\nto the performance achieved by the variants. Further, we show the performance (and hyper-\nparameter ranking) when using the optimal hyperparameter point from the base workload\non the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs are from\nthe same search space for NadamW. See Section 6.2 for a detailed description of our pro-\ntocol for accepting workload variants.\nD.6 WMT\nThe models are trained on the WMT 2017 German\u00d1English (De\u00d1En) training dataset\n(Bojar et al., 2017) which consists of about 5.9 million parallel sentence pairs. We don\u2019t\nuse any monolingual data or data augmentation methods. The models are evaluated on\nthe \u201dvalidation\u201d and \u201dtest\u201d splits of the WMT 2014 De\u00d1En dataset (Bojar et al., 2014)\nwhich consists of 3000 and 3003 sentence pairs respectively. We use SentencePiece (Kudo\nand Richardson, 2018) with 32k vocabulary size which is shared on source and target side.\nSentences longer than 256 tokens on either the source or the target side are removed from\nthe training data. See here for the implementation of our WMT input pipeline.\nD.6.1 Transformer Model\nWe use the Transformer-big architecture from Vaswani et al. (2017) with some modifica-\ntions. More specifically, the model has 6 encoder and decoder layers each, model dimension\n1024, hidden dimension 4096 and 16 attention heads. The embedding parameters are shared\non the encoder and the decoder side. The same embedding matrix is also used for linear\nreadout on the decoder (softmax). For regularization, the model uses label smoothing and\ndropout where the rates may be tuned by the submissions; the default label smoothing is 0.1\nand the default dropout rate is 0.1. A batch size of 128 corresponds to about 280K tokens\neach on source and target side in one training batch. See here for our implementation of\nTransformer in Jax and here for our implementation of Transformer in PyTorch.\n90\nBenchmarking Neural Network Training Algorithms\nDecoding\nTo generate samples from the model, we use beam search decoding with beam\nsize 4, and a length penalty of 0.6. For evaluation, we use standard de-tokenized BLEU\nscores (Papineni et al., 2002) using the sacreBLEU library (Post, 2018).19\nD.6.2 WMT Transformer Workload Variants\nThe three workload variants for the WMT model are:\n\u2022 Post-LN: Layer normalization was applied after the residual branch was added back\ninto the trunk.\n\u2022 Attention Temp: The attention logits were multiplied by a temperature of 4.0,\nbefore applying the softmax.\n\u2022 GLU & TanH: Gated Linear Units (GLU) (Dauphin et al., 2017) were used in the\nMLP blocks, and activation functions were changed to TanH.\nVariant\nValidation\nHyperparameter Performance Transfer\nPerformance\nBase \u00d1 Variant\nVariant \u00d1 Base\nPerformance\nRank\nPerformance\nRank\nBase Workload\n30.8534\nPost-LN\n30.2011\n11.1946\n76\n29.7474\n23\nAttention Temp\n30.2643\n0\n74\n29.4278\n32\nGLU & TanH\n30.1595\n0\n82\n29.7135\n25\nTable 40: Hyperparameter transfer between the base workload and the variants\nof Transformer on WMT. We show the validation performance of the base workload\ncompared to the performance achieved by the variants. Further, we show the performance\n(and hyperparameter ranking) when using the optimal hyperparameter point from the base\nworkload on the variants (Base \u00d1 Variant) and vice-versa (Variant \u00d1 Base). All runs\nare from the same search space for NadamW. See Section 6.2 for a detailed description of\nour protocol for accepting workload variants.\nE. Preliminary Experiments for Randomized Workloads\nIn order to create randomized workloads based on one of the fixed workloads, we needed to\nwrite down a distribution over workload modifications that has the properties we want. Our\ninitial plan was to design a relatively broad distribution over natural changes to the base\nworkload, then draw held-out workload samples, rejecting ones that failed a \u201ctrainability\u201d\ntest, and then rejecting ones that were too close to the base workload. Then we would\nrevise the distribution to hopefully reduce the rejection rates and produce a final, official\nrandomized workload. We started this process for the GNN workload on OGBG before\neventually abandoning it.\n19. case.mixed + numrefs.1 + smooth.exp + tok.13a + sacreBLEU 1.3.1\n91\nAlgoPerf Authors\nThe distribution we started with is defined over the GNN\u2019s hidden dimensions, latent\ndimensions (node and edge embedding dimension), normalization layer type, activation\nfunction, number of message passing steps, and dropout probability20, see Table 41. All\nworkload samples from this distribution fit into competition hardware GPU memory with\na batch size of 512. The batch size is not part of the workload definition, so in principle we\nmight generate held-out workloads that require adjusting the batch size to remain within\nmemory constraints. In general, we also need to find the best-performing batch size for\ntarget setting in order to reach competitive targets.\nGNN Parameters\nDistribution\nHidden dimension 1\nt128, 256, 512, 1024, 2048u\nHidden dimension 2\ntNone, 128, 256, 512, 1024, 2048u\nLatent dimension\nt128, 256, 512, 1024, 2048u\nNormalization layer\n{batch normalization, layer normalization}\nActivation function\n{ReLU, SiLU, Leaky ReLU}\nN message passing steps\nt2, 3, 4, 5, 6, 7, 8u\nDropout probability\nr0.0, 0.5s\nTable 41: Distribution used in a preliminary experiment for a potential random-\nized GNN OGBG workload.\nAfter sampling 60 held-out workload candidates from the distribution described by Ta-\nble 41, we ran a target-setting procedure on each sample. In other words, for each sample\nwe tuned Nesterov using 100 hyperparameter points to find the hyperparameters that\nachieve the best mAP. We considered a sample able to achieve an acceptable result in the\nruntime budget if, after tuning, it reached an mAP no more than 10 % worse than the orig-\ninal base workload mAP target. We found that roughly 30 % of the samples were trainable\nby this definition.\nTo quickly check whether the surviving 16 trainable workload candidates were suffi-\nciently different from the base workload, we ran the best-performing hyperparameter set-\ntings from the base workload on each candidate. Unfortunately, in pretty much every case\nexcept for one activation function change, the best-performing hyperparameter settings from\nthe base workload achieved about the same (within re-training noise) mAP as the best we\ncould find after tuning just on the workload sample.\nWe decided to abandon this process for constructing randomized workload distributions\n(and not extend it to the other base workloads) for a variety of reasons. First, it was far too\ncostly. Every time we revised the distribution we were considering, we had to run our entire\ntarget-setting procedure on every single sample from the distribution (in the experiment de-\nscribed above, this amounted to running 6000 trials for just one version of the distribution).\nOur procedure above also ignored some of the technicalities around selecting batch sizes for\ntarget setting, and we had to be very careful to filter out samples (or write down distribu-\n20. Our initial experiments on randomized workloads for OGBG included the dropout probability as part\nof the workload definition. However, in the competition, submitters are free to determine the dropout\nprobability. Therefore, a final, official randomized workload distribution for our benchmark could not\ninclude the dropout probability.\n92\nBenchmarking Neural Network Training Algorithms\ntions) that didn\u2019t violate the memory constraints of the competition hardware. We also did\nnot have the understanding of what kinds of changes would cause the training problem to\nactually change that would be necessary to quickly come up with interesting distributions\nover workloads. Instead, we decided to adopt a procedure of manually constructing variants\none at a time (see Section 6.2).\n93\nAlgoPerf Authors\nReferences\nNaman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Disentangling\nAdaptive Gradient Methods from Learning Rates, 2020.\nEhsan Amid, Rohan Anil, and Manfred Warmuth. LocoProp: Enhancing BackProp via Lo-\ncal loss optimization. In International Conference on Artificial Intelligence and Statistics\n(AISTATS), 2022.\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catan-\nzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen,\nJesse H. Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy\nJun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair,\nRyan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta,\nYi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, and Zhenyao\nZhu. Deep Speech 2: End-to-End Speech Recognition in English and Mandarin. In In-\nternational Conference on Machine Learning (ICML), 2016.\nRohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second\norder optimization for deep learning, 2020.\nSetareh Ariafar, Justin Gilmer, Zachary Nado, Jasper Snoek, Rodolphe Jenatton, and\nGeorge E. Dahl. Predicting the utility of search spaces for black-box optimization: a\nsimple, budget-aware approach.\nIn International Conference on Artificial Intelligence\nand Statistics (AISTATS), 2022.\nJuhan Bae, Paul Vicol, Jeff Z. HaoChen, and Roger B. Grosse. Amortized proximal opti-\nmization. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\nThomas Bartz-Beielstein, Carola Doerr, Daan van den Berg, Jakob Bossek, Sowmya Chan-\ndrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, William La Cava,\nManuel Lopez-Ibanez, Katherine M. Malan, Jason H. Moore, Boris Naujoks, Patryk\nOrzechowski, Vanessa Volz, Markus Wagner, and Thomas Weise. Benchmarking in Op-\ntimization: Best Practice and Open Issues, 2020.\nPeter Battaglia, Jessica Blake Chandler Hamrick, Victor Bapst, Alvaro Sanchez, Vinicius\nZambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan\nFaulkner, Caglar Gulcehre, Francis Song, Andy Ballard, Justin Gilmer, George E. Dahl,\nAshish Vaswani, Kelsey Allen, Charles Nash, Victoria Jayne Langston, Chris Dyer, Nico-\nlas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and\nRazvan Pascanu. Relational inductive biases, deep learning, and graph networks, 2018.\nIrwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, Aravind Srinivas, Tsung-Yi\nLin, Jonathon Shlens, and Barret Zoph. Revisiting ResNets: Improved Training and\nScaling Strategies. In Advances in Neural Information Processing Systems (NeurIPS),\n2021.\nLucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov.\nBetter plain ViT baselines for\nImageNet-1k, 2022.\n94\nBenchmarking Neural Network Training Algorithms\nOnd\u02c7rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo-\nhannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu\nSoricut, Lucia Specia, and Ale\u02c7s Tamchyna. Findings of the 2014 Workshop on Statisti-\ncal Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine\nTranslation, 2014.\nOnd\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck,\nChris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt\nPost, Carolina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 Workshop\non Statistical Machine Translation. In Proceedings of the Tenth Workshop on Statistical\nMachine Translation, 2015.\nOnd\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow,\nShujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof\nMonz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. Find-\nings of the 2017 Conference on Machine Translation (WMT17). In Proceedings of the\nSecond Conference on Machine Translation, Volume 2: Shared Task Papers, 2017.\nOlivier Bousquet, Sylvain Gelly, Karol Kurach, Olivier Teytaud, and Damien Vincent.\nCritical Hyper-Parameters: No Random, No Cry, 2017.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu\nPham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic Discovery of Opti-\nmization Algorithms, 2023.\nDami Choi, Alexandre Passos, Christopher J. Shallue, and George E. Dahl. Faster Neural\nNetwork Training with Data Echoing, 2019a.\nDami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J. Maddison, and\nGeorge E. Dahl. On Empirical Comparisons of Optimizers for Deep Learning, 2019b.\nJeremy M. Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Meda-\npati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E. Dahl, and\nJustin Gilmer. Adaptive Gradient Methods at the Edge of Stability, 2022.\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi,\nPeter Bailis, Kunle Olukotun, Chris R\u00b4e, and Matei Zaharia. DAWNBench: An end-to-\nend deep learning benchmark and competition. In ML System Workshop at Advances in\nNeural Information Processing Systems (NeurIPS), 2017.\nEkin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. RandAugment: Practical\ndata augmentation with no separate search. In Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nFelix Dangel, Frederik Kunstner, and Philipp Hennig. BackPACK: Packing more into Back-\nprop. In International Conference on Learning Representations (ICLR), 2020.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language Modeling\nwith Gated Convolutional Networks. In International Conference on Machine Learning\n(ICML), 2017.\n95\nAlgoPerf Authors\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin\nGilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,\nRodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Car-\nlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd\nvan Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver,\nFantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birod-\nkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti\u00b4c,\nDustin Tran, Thomas Kipf, Mario Lu\u02c7ci\u00b4c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harm-\nsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nImageNet: A\nLarge-Scale Hierarchical Image Database. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. In Conference of the\nNorth American Chapter of the Association for Computational Linguistics \u2013 Human Lan-\nguage Technologies (NAACL-HLT), 2019.\nElizabeth D. Dolan and Jorge J. Mor\u00b4e. Benchmarking optimization software with perfor-\nmance profiles. Mathematical Programming, 2002.\nXuanyi Dong and Yi Yang. NAS-Bench-201: Extending the Scope of Reproducible Neural\nArchitecture Search. In International Conference on Learning Representations (ICLR),\n2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale. In International Conference on Learning Representations\n(ICLR), 2021.\nVijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua\nBengio, and Xavier Bresson.\nBenchmarking Graph Neural Networks.\nIn Journal of\nMachine Learning Research, 2023.\nBradley Efron and Robert J. Tibshirani. An Introduction to the Bootstrap. Monographs on\nStatistics and Applied Probability. Chapman & Hall/CRC, 1993.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning. Neural Networks, 2018.\nPengzhi Gao, Zhongjun He, Hua Wu, and Haifeng Wang. Bi-SimCut: A Simple Strategy for\nBoosting Neural Machine Translation. In Conference of the North American Chapter of\nthe Association for Computational Linguistics \u2013 Human Language Technologies (NAACL-\nHLT), 2022.\nJohn S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathan G. Fiscus, David S. Pal-\nlett, Nancy L. Dahlgren, and Victor Zue. TIMIT Acoustic-Phonetic Continuous Speech\nCorpus. Web download, 1993.\n96\nBenchmarking Neural Network Training Algorithms\nJustin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur,\nDavid Cardoze, George E. Dahl, Zachary Nado, and Orhan Firat. A Loss Curvature Per-\nspective on Training Instabilities of Deep Learning Models. In International Conference\non Learning Representations (ICLR), 2021.\nJustin Gilmer, Andrea Schioppa, and Jeremy Cohen. Intriguing Properties of Transformer\nTraining Instabilities. To Appear, 2023.\nVarun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, and Zachary Nado.\nDeep Learning Tuning Playbook, 2023.\nURL http://github.com/google/tuning_\nplaybook. Version 1.0.\nJonathan Godwin, Thomas Keck, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li,\nKimberly Stachenfeld, Petar Veli\u02c7ckovi\u00b4c, and Alvaro Sanchez-Gonzalez. Jraph: A library\nfor graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph.\nDonald Goldfarb, Yi Ren, and Achraf Bahamou.\nPractical Quasi-Newton Methods for\nTraining Deep Neural Networks. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\nAlex Graves, Santiago Fernandez, Faustino Gomez, and J\u00a8urgen Schmidhuber. Connectionist\nTemporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural\nNetworks. In International Conference on Machine Learning (ICML), 2006.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei\nHan, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.\nConformer:\nConvolution-augmented Transformer for Speech Recognition, 2020.\nVineet Gupta, Tomer Koren, and Yoram Singer.\nShampoo: Preconditioned Stochastic\nTensor Optimization . In International Conference on Machine Learning (ICML), 2018.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for\nImage Recognition. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016a.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep\nResidual Networks. In Computer Vision \u2013 ECCV, 2016b.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre,\nAndreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for\nJAX, 2023. URL http://github.com/google/flax.\nDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs), 2016.\nGeoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the Dimensionality of Data\nwith Neural Networks. Science, 2006.\nElad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the\ngeneralization gap in large batch training of neural networks.\nIn Advances in Neural\nInformation Processing Systems (NeurIPS), 2017.\n97\nAlgoPerf Authors\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on\nGraphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\nSergey Ioffe and Christian Szegedy.\nBatch Normalization: Accelerating Deep Network\nTraining by Reducing Internal Covariate Shift. In International Conference on Machine\nLearning (ICML), 2015.\nNorman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon,\nCliff Young, and David Patterson. A Domain-Specific Supercomputer for Training Deep\nNeural Networks. Communications of the ACM, 2020.\nHyunjik Kim, George Papamakarios, and Andriy Mnih. The Lipschitz Constant of Self-\nAttention. In International Conference on Machine Learning (ICML), 2021.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In\nInternational Conference on Learning Representations (ICLR), 2015.\nG\u00a8unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.\nSelf-\nnormalizing neural networks.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\nFlorian Knoll, Tullie Murrell, Anuroop Sriram, Nafissa Yakubova, Jure Zbontar, Michael\nRabbat, Aaron Defazio, Matthew J. Muckley, Daniel K. Sodickson, C. Lawrence Zitnick,\nand Michael P. Recht. Advancing machine learning for MR image reconstruction with\nan open competition: Overview of the 2019 fastMRI challenge. Magnetic Resonance in\nMedicine, 2020.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\nTechnical report, University of Toronto, 2009.\nTaku Kudo and John Richardson.\nSentencepiece: A simple and language independent\nsubword tokenizer and detokenizer for neural text processing. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2018.\nCriteo A. I. Lab. Criteo 1TB Click Logs dataset. Web download, 2014.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James\nHays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\nCOCO: Common Objects in Context. In Computer Vision \u2013 ECCV, 2014.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and\nJiawei Han. On the Variance of the Adaptive Learning Rate and Beyond. In International\nConference on Learning Representations (ICLR), 2020.\nJames Lucas, Shengyang Sun, Richard S. Zemel, and Roger B. Grosse. Aggregated Mo-\nmentum: Stability Through Passive Damping. In International Conference on Learning\nRepresentations (ICLR), 2019.\n98\nBenchmarking Neural Network Training Algorithms\nJerry Ma and Dennis Yarats. On the Adequacy of Untuned Warmup for Adaptive Opti-\nmization. In AAAI Conference on Artificial Intelligence, 2021.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,\nJonathan May, and Luke Zettlemoyer.\nMega: Moving Average Equipped Gated At-\ntention. In International Conference on Learning Representations (ICLR), 2023.\nJames Martens. Deep learning via Hessian-free optimization. In International Conference\non Machine Learning (ICML), 2010.\nJames Martens and Roger B. Grosse. Optimizing Neural Networks with Kronecker-factored\nApproximate Curvature.\nIn International Conference on Machine Learning (ICML),\n2015.\nPeter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius,\nDavid Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks,\nDehao Chen, Debo Dutta, Udit Gupta, Kim Hazelwood, Andy Hock, Xinyuan Huang,\nDaniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Deepak Narayanan, Tayo Ogun-\ntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom\nSt John, Carole-Jean Wu, Lingjie Xu, Cliff Young, and Matei Zaharia. MLPerf Training\nBenchmark. In Proceedings of Machine Learning and Systems, 2020.\nLuke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha\nSohl-Dickstein.\nUsing a thousand optimization tasks to learn hyperparameter search\nstrategies, 2020.\nThomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine\nBannier, Benjamin Charlier, Mathieu Dagr\u00b4eou, Tom Dupr\u00b4e la Tour, Ghislain Durif, Cas-\nsio F. Dantas, Quentin Klopfenstein, Johan Larsson, En Lai, Tanguy Lefort, Benoit\nMalr\u00b4ezieux, Badr Moufad, Binh T. Nguyen, Alain Rakotomamonjy, Zaccharie Ramzi,\nJoseph Salmon, and Samuel Vaiter. Benchopt: Reproducible, efficient and collabora-\ntive optimization benchmarks. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\nZachary Nado, Justin M. Gilmer, Christopher J. Shallue, Rohan Anil, and George E. Dahl.\nA Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across\nBatch Sizes, 2021.\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan\nSundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G.\nAzzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghu-\nraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie\nChen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. Deep\nLearning Recommendation Model for Personalization and Recommendation Systems,\n2019.\nJorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science, 1999.\n99\nAlgoPerf Authors\nNVIDIA.\nDLRM for PyTorch, 2023.\nURL https://catalog.ngc.nvidia.com/orgs/\nnvidia/resources/dlrm_for_pytorch.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An\nASR corpus based on public domain audio books. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2015.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\nBleu: a Method for\nAutomatic Evaluation of Machine Translation. In Annual Meeting of the Association for\nComputational Linguistics (ACL), 2002.\nDaniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk,\nand Quoc V. Le. SpecAugment: A Simple Data Augmentation Method for Automatic\nSpeech Recognition. In Proc. Interspeech, 2019.\nMatt Post.\nA Call for Clarity in Reporting BLEU Scores.\nIn Conference on Machine\nTranslation: Research Papers, 2018.\nPatrick Putzky, Dimitrios Karkalousos, Jonas Teuwen, Nikita Miriakov, Bart Bakker,\nMatthan Caan, and Max Welling. i-RIM applied to the fastMRI challenge, 2019.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet\nClassifiers Generalize to ImageNet?\nIn International Conference on Machine Learning\n(ICML), 2019.\nYi Ren and Donald Goldfarb. Tensor normal training for deep learning models. In Advances\nin Neural Information Processing Systems (NeurIPS), 2021.\nYi Ren, Achraf Bahamou, and Donald Goldfarb.\nPractical Quasi-Newton Methods for\nTraining Deep Neural Networks. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for\nBiomedical Image Segmentation. In Medical Image Computing and Computer-Assisted\nIntervention \u2013 MICCAI 2015, 2015.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhi-\nheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,\nand Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal\nof Computer Vision (IJCV), 2015.\nRobin M. Schmidt, Frank Schneider, and Philipp Hennig. Descending through a Crowded\nValley \u2013 Benchmarking Deep Learning Optimizers. In International Conference on Ma-\nchine Learning (ICML), 2021.\nFrank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: A Deep Learning Optimizer\nBenchmark Suite.\nIn International Conference on Learning Representations (ICLR),\n2019.\n100\nBenchmarking Neural Network Training Algorithms\nChristopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig,\nand George E. Dahl.\nMeasuring the Effects of Data Parallelism on Neural Network\nTraining. Journal of Machine Learning Research, 2019.\nPrabhu Teja Sivaprasad, Florian Mai, Thijs Vogels, Martin Jaggi, and Francois\n\u00b8\nFleuret.\nOptimizer Benchmarking Needs to Acccount for Hyperparameter Tuning. In International\nConference on Machine Learning (ICML), 2020.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal\nof Machine Learning Research, 2014.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit,\nand Lucas Beyer. How to train your ViT? Data, Augmentation, and Regularization in\nVision Transformers. Transactions on Machine Learning Research, 2022.\nAndreas\nSterbenz.\nUsing\nGoogle\nCloud\nMachine\nLearning\nto\npredict\nclicks\nat\nscale,\n2017.\nURL\nhttps://cloud.google.com/blog/products/gcp/\nusing-google-cloud-machine-learning-to-predict-clicks-at-scale.\nMingxing Tan and Quoc Le.\nEfficientNet: Rethinking Model Scaling for Convolutional\nNeural Networks. In International Conference on Machine Learning (ICML), 2019.\nRan Tian and Ankur P. Parikh. Amos: An Adam-style Optimizer with Adaptive Weight\nDecay towards Model-Oriented Scale, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in\nNeural Information Processing Systems (NeurIPS), 2017.\nXu Wang, Huan Zhao, Lanning Wei, and Quanming Yao. Pooling Architecture Search for\nGraph Property Prediction in Open Graph Benchmark. Technical report, AutoGraph\nteam, 2022.\nYan Wang, Hao Zhang, Jing Yang, Ruixin Zhang, and Shouhong Ding. Technical Report\nfor OGB Graph Property Prediction. Technical report, Tencent Youtu Lab, 2021.\nRoss Wightman, Hugo Touvron, and Herv\u00b4e J\u00b4egou.\nResNet strikes back: An improved\ntraining procedure in timm, 2021.\nAshia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.\nThe Marginal Value of Adaptive Gradient Methods in Machine Learning. In Advances in\nNeural Information Processing Systems (NeurIPS), 2017.\nZhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse,\nAneesh S. Pappu, Karl Leswing, and Vijay Pande. MoleculeNet: a benchmark for molec-\nular machine learning. Chemical science, 2018.\nXingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan.\nAdan: Adaptive\nNesterov Momentum Algorithm for Faster Optimizing Deep Models, 2022.\n101\nAlgoPerf Authors\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\nZhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On Layer Normalization in the Trans-\nformer Architecture. In International Conference on Machine Learning (ICML), 2020.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph\nNeural Networks?\nIn International Conference on Learning Representations (ICLR),\n2019.\nGreg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning Large Neural Networks\nvia Zero-Shot Hyperparameter Transfer. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021.\nYang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Net-\nworks, 2017.\nYang You, Jing Li, Shashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,\nXiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large Batch Opti-\nmization for Deep Learning: Training BERT in 76 minutes. In International Conference\non Learning Representations (ICLR), 2020.\nSergey Zagoruyko and Nikos Komodakis. Wide Residual Networks, 2016.\nJure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J.\nMuckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente,\nKrzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal,\nAdriana Romero, Michael Rabbat, Pascal Vincent, Nafissa Yakubova, James Pinkerton,\nDuo Wang, Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson,\nand Yvonne W. Lui. fastMRI: An Open Dataset and Benchmarks for Accelerated MRI,\n2018.\nHongyi Zhang, Moustapha Ciss\u00b4e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond\nEmpirical Risk Minimization. In International Conference on Learning Representations\n(ICLR), 2018.\nYilong Zhao, Li Jiang, Mingyu Gao, Naifeng Jing, Chengyang Gu, Qidong Tang, Fangxin\nLiu, Tao Yang, and Xiaoyao Liang.\nRePAST: A ReRAM-based PIM Accelerator for\nSecond-order Training of DNN, 2022.\nJuntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C. Tatikonda, Nicha Dvornek,\nXenophon Papademetris, and James Duncan.\nAdaBelief Optimizer: Adapting Step-\nsizes by the Belief in Observed Gradients. In Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nHongyu Zhy, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Amar Phanishayee, Bianca\nSchroeder, and Gennady Pekhimenko. TBD: Benchmarking and Analyzing Deep Neu-\nral Network Training. In IEEE International Symposium on Workload Characterization\n(IISWC), 2018.\n102\n"
  },
  {
    "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
    "link": "https://arxiv.org/pdf/2306.07280.pdf",
    "upvote": "19",
    "text": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning\nZeju Qiu1,*\nWeiyang Liu1,2,*,\u2020\nHaiwen Feng1\nYuxuan Xue3\nYao Feng1\nZhen Liu1,4\nDan Zhang3,5\nAdrian Weller2,6\nBernhard Sch\u00f6lkopf1\n1MPI for Intelligent Systems - T\u00fcbingen\n2University of Cambridge\n3University of T\u00fcbingen\n4Mila, Universit\u00e9 de Montr\u00e9al\n5Bosch Center for Artificial Intelligence\n6The Alan Turing Institute\n*Equal contribution\n\u2020Project lead\noft.wyliu.com\nAbstract\nLarge text-to-image diffusion models have impressive capabilities in generating\nphotorealistic images from text prompts. How to effectively guide or control these\npowerful models to perform different downstream tasks becomes an important open\nproblem. To tackle this challenge, we introduce a principled finetuning method\n\u2013 Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to\ndownstream tasks. Unlike existing methods, OFT can provably preserve hyper-\nspherical energy which characterizes the pairwise neuron relationship on the unit\nhypersphere. We find that this property is crucial for preserving the semantic gen-\neration ability of text-to-image diffusion models. To improve finetuning stability,\nwe further propose Constrained Orthogonal Finetuning (COFT) which imposes\nan additional radius constraint to the hypersphere. Specifically, we consider two\nimportant finetuning text-to-image tasks: subject-driven generation where the goal\nis to generate subject-specific images given a few images of a subject and a text\nprompt, and controllable generation where the goal is to enable the model to\ntake in additional control signals. We empirically show that our OFT framework\noutperforms existing methods in generation quality and convergence speed.\n1\nIntroduction\nRecent text-to-image diffusion models [45, 50, 53] achieve impressive performance in text-guided\ncontrol for high-fidelity image generation. Despite strong results, text guidance can still be ambiguous\nand insufficient to provide fine-grained and accurate control to the generated images. To address this\nshortcoming, we target two types of text-to-image generation tasks in this paper:\n\u2022 Subject-driven generation [51]: Given just a few images of a subject, the task is to generate\nimages of the same subject in a different context using the guidance of a text prompt.\n\u2022 Controllable generation [38, 68]: Given an additional control signal (e.g., canny edges, segmenta-\ntion maps), the task is to generate images following such a control signal and a text prompt.\nBoth tasks essentially boil down to how to effectively finetune text-to-image diffusion models without\nlosing the pretraining generative performance. We summarize the desiderata for an effective finetuning\nmethod as: (1) training efficiency: having fewer trainable parameters and number of training epochs,\nand (2) generalizability preservation: preserving the high-fidelity and diverse generative performance.\nTo this end, finetuning is typically done either by updating the neuron weights by a small learning\nrate (e.g., [51]) or by adding a small component with re-parameterized neuron weights (e.g., [22, 68]).\nDespite simplicity, neither finetuning strategy is able to guarantee the preservation of pretraining\ngenerative performance. There is also a lack of principled understanding towards designing a good\nfinetuning strategy and finding suitable hyperparameters such as the number of training epochs.\nA key difficulty is the lack of a measure for quantifying the preservation of pretrained generative\nability. Existing finetuning methods implicitly assume that a smaller Euclidean distance between the\nThis work was finished when ZQ was a research intern hosted by WL at MPI for Intelligent Systems.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.07280v3  [cs.CV]  14 Mar 2024\nIteration 400\nIteration 1400\nIteration 2200\nIteration 3000\nInput images\nDreamBooth\nLoRA\nOFT\n(a) Subject-driven generation \n(b) Controllable generation \nText prompt: a [V] dog with a mountain in the background\n0\n500\n1000\n1500\n2000\n2500\n3000\nNumber of iterations\n0.95\n0.955\n0.96\n0.965\nHyperspherical energy\nDreamBooth\nLoRA\nOFT\n+199\nText prompt: a house\nControlNet\nLoRA\nOFT\nGround truth\nControl signal\n5% data\n10% data\n50% data\nLoRA\nOFT\n0.945\n0.946\nHyperspherical energy\n5% data\n10% data\n50% data\n+199\nInitial energy\nFigure 1: (a) Subject-driven generation: OFT preserves the hyperspherical energy and yields more stable finetuning performance across different\nnumber of iterations, while both DreamBooth [51] and LoRA [22] do not. OFT can preserve hyperspherical energy and perform stable finetuning,\nwhile both LoRA and DreamBooth are unable. (b) Controllable generation: OFT is more sample-efficient in training and converges well with\nonly 5% of the original dataset, while both ControlNet [68] and LoRA [22] cannot converge until 50% of the data is present. The hyperspherical\nenergy comparison between LoRA and OFT is fair, since they finetune the same layers. ControlNet uses a different layer finetuning strategy, so\nits hyperspherical energy is not comparable. The detailed settings are given in the experiment section and Appendix A.\nfinetuned model and the pretrained model indicates better preservation of the pretrained ability. Due\nto the same reason, finetuning methods typically work with a very small learning rate. While this\nassumption may occasionally hold, we argue that the Euclidean difference to the pretrained model\nalone is unable to fully capture the degree of semantic preservation, and therefore a more structural\nmeasure to characterize the difference between the finetuned model and the pretrained model can\ngreatly benefit the preservation of pretraining performance as well as finetuning stability.\nInspired by the empirical observation that hyperspherical similarity encodes semantic information\nwell [7, 35, 36], we use hyperspherical energy [32] to characterize the pairwise relational structure\namong neurons. Hyperspherical energy is defined as the sum of hyperspherical similarity (e.g., cosine\nsimilarity) between all pairwise neurons in the same layer, capturing the level of neuron uniformity\non the unit hypersphere [34]. We hypothesize that a good finetuned model should have a minimal\ndifference in hyperspherical energy compared to the pretrained model. A naive way is to add a\nregularizer such that the hyperspherical energy remains the same during the finetuning stage, but there\nis no guarantee that the hyperspherical energy difference can be well minimized. Therefore, we take\nadvantage of an invariance property of hyperspherical energy \u2013 the pairwise hyperspherical similarity\nis provably preserved if we apply the same orthogonal transformation for all neurons. Motivated\nby such an invariance, we propose Orthogonal Finetuning (OFT) which adapts large text-to-image\ndiffusion models to a downstream task without changing its hyperspherical energy. The central idea\nis to learn a layer-shared orthogonal transformation for neurons such that their pairwise angles are\npreserved. OFT can also be viewed as adjusting the canonical coordinate system for the neurons\nin the same layer. By jointly taking into consideration that smaller Euclidean distance between the\nfinetuned model and the pretrained model implies better preservation of pretraining performance, we\nfurther propose an OFT variant \u2013 Constrained Orthogonal Finetuning (COFT) which constrains the\nfinetuned model within the hypersphere of a fixed radius centered on the pretrained neurons.\nThe intuition for why orthogonal transformation works for finetuning neurons partially comes from\n2D Fourier transform, with which an image can be decomposed as magnitude and phase spectrum.\nThe phase spectrum, which is angular information between input and basis, preserves the major part\nof semantics. For example, the phase spectrum of an image, along with a random magnitude spectrum,\ncan still reconstruct the original image without losing its semantics. This phenomenon suggests that\nchanging the neuron directions is the key to semantically modifying the generated image (which is the\ngoal of both subject-driven and controllable generation). However, changing neuron directions with a\nlarge degree of freedom will inevitably destroy the pretraining generative performance. To constrain\nthe degree of freedom, we propose to preserve the angle between any pair of neurons, largely based\non the hypothesis that the angles between neurons are crucial for representing the knowledge of\nneural networks. With this intuition, it is natural to learn layer-shared orthogonal transformation for\nneurons in each layer such that the hyperspherical energy stays unchanged.\nWe also draw inspiration from orthogonal over-parameterized training [33] which trains classification\nneural networks from scratch by orthogonally transforming a randomly initialized neural network.\nThis is because a randomly initialized neural network yields a provably small hyperspherical energy in\n2\nexpectation and the goal of [33] is to keep hyperspherical energy small during training (small energy\nleads to better generalization in classification [30, 32]). [33] shows that orthogonal transformation is\nsufficiently flexible to train generalizable neural networks for classification problems. In contrast, we\nfocus on finetuning text-to-image diffusion models for better controllability and stronger downstream\ngenerative performance. We emphasize the difference between OFT and [33] in two aspects. First,\nwhile [33] is designed to minimize the hyperspherical energy, OFT aims to preserve the same\nhyperspherical energy as the pretrained model so that the intrinsic pretrained structure will not be\ndestroyed by finetuning. In the case of finetuning diffusion models, minimizing hyperspherical\nenergy could destroy the original semantic structures. Second, OFT seeks to minimize the deviation\nfrom the pretrained model, which leads to the constrained variant. In contrast, [33] imposes no such\nconstraints. The key to finetuning is to find a good trade-off between flexibility and stability, and we\nargue that our OFT framework effectively achieves this goal. Our contributions are listed below:\n\u2022 We propose a novel finetuning method \u2013 Orthogonal Finetuning for guiding text-to-image diffusion\nmodels towards better controllability. To further improve stability, we propose a constrained variant\nwhich limits the angular deviation from the pretrained model.\n\u2022 Compared to existing finetuning methods, OFT performs model finetuning while provably pre-\nserving the hyperspherical energy, which we empirically find to be an important measure of the\ngenerative semantic preservation of the pretrained model.\n\u2022 We apply OFT to two tasks: subject-driven generation and controllable generation. We conduct a\ncomprehensive empirical study and demonstrate significant improvement over prior work in terms\nof generation quality, convergence speed and finetuning stability. Moreover, OFT achieves better\nsample efficiency, as it converges well with a much smaller number of training images and epochs.\n\u2022 For controllable generation, we introduce a new control consistency metric to evaluate the control-\nlability. This core idea is to estimate the control signal from the generated image and then compare\nit with the origin control signal. The metric further validates the strong controllability of OFT.\n2\nRelated Work\nText-to-image diffusion models. Tremendous progress [16, 39, 45, 50, 53] has been made in\ntext-to-image generation, largely thanks to the rapid development in diffusion-based generative\nmodels [12, 20, 55, 56] and vision-language representation learning [1, 28, 29, 37, 44, 54, 57, 61].\nGLIDE [39] and Imagen [53] train diffusion models in the pixel space. GLIDE trains the text encoder\njointly with a diffusion prior using paired text-image data, while Imagen uses a frozen pretrained\ntext encoder. Stable Diffusion [50] and DALL-E2 [45] train diffusion models in the latent space.\nStable Diffusion uses VQ-GAN [14] to learn a visual codebook as its latent space, while DALL-\nE2 adopts CLIP [44] to construct a joint latent embedding space for representing images and text.\nOther than diffusion models, generative adversarial networks [27, 48, 65, 67] and autoregressive\nmodels [13, 46, 62, 66] have also been studied in text-to-image generation. OFT is inherently a\nmodel-agnostic finetuning approach and can be applied to any text-to-image diffusion model.\nSubject-driven generation. To prevent subject modification, [2, 39] consider a given mask from\nusers as an additional condition. Inversion methods [8, 12, 15, 45] can be applied to modify the\ncontext without changing the subject. [18] can perform local and global editing without input\nmasks. The methods above are unable to well preserve identity-related details of the subject. In\nPivotal Tuning [49], a generator is finetuned around an initial inverted latent code with an additional\nregularization to preserve the identity. Similarly, [41] learns a personalized generative face prior\nfrom a collection of a person\u2019s face images. [6] can generate difference variations of an instance,\nbut it may lose the instance-specific details. With a customized token and a few subject images,\nDreamBooth [51] finetunes the text-to-image diffusion model using a reconstruction loss and a class-\nspecific prior preservation loss. OFT adopts the DreamBooth framework, but instead of performing\nnaive finetuning with a small learning rate, OFT finetunes the model with orthogonal transformations.\nControllable generation. The task of image-to-image translation can be viewed as a form of\ncontrollable generation, and previous methods mostly adopt conditional generative adversarial\nnetworks [9, 23, 42, 60, 71]. Diffusion models are also used for image-to-image translation [52, 58,\n59]. More recently, ControlNet [68] proposes to control a pretrained diffusion model by finetuning and\nadapting it to additional control signals and achieves impressive controllable generation performance.\nAnother concurrent and similar work, T2I-Adapter [38], also finetunes a pretrained diffusion model\nin order to gain stronger controllability for the generated images. Following the same task setting\n3\nin [38, 68], we apply OFT to finetune pretrained diffusion models, yielding consistently better\ncontrollability with fewer training data and less finetuning parameters. More significantly, OFT does\nnot introduce any additional computational overhead during test-time inference.\nModel finetuning. Finetuning large pretrained models on downstream tasks has been increasingly\npopular nowadays [3, 11, 17]. As a form of finetuning, adaptation methods (e.g., [21, 22, 43]) are\nheavily studied in natural language processing. LoRA [22] is the most relevant work to OFT, and it\nassumes a low-rank structure for the additive weight update during finetuning. In contrast, OFT uses\nlayer-shared orthogonal transformation to update neuron weights in a multiplicative manner, and it\nprovably preserves the pair-wise angles among neurons in the same layer, yielding better stability.\n3\nOrthogonal Finetuning\n3.1\nWhy Does Orthogonal Transformation Make Sense?\nWe start by discussing why orthogonal transformation is desirable in finetuning text-to-image diffusion\nmodels. We decompose this question into two smaller ones: (1) why we want to finetune the angle of\nneurons (i.e., direction), and (2) why we adopt orthogonal transformation to finetune angles.\nw      x\n = \n =      \n||\n|| || ||\n.\n =\nz\nz\nz\nz\nw  x\nFeature map\nConvolution\nDeconvolution\n(a)\n(b)\n(c)\n(a) Inner product\n(b) Magnitude\n(c) Angle\nT\ncos(\u03b8)\nFigure 2: A toy experiment to demonstrate the impor-\ntance of angular information. The autoencoder is trained\nin a standard way using inner product activation, and (a)\nshows the standard reconstruction. In testing, the angular\ninformation of neurons alone can well recover the input\nimage, even if the autoencoder is not trained with angles.\nFor the first question, we draw inspiration from the empir-\nical observation in [7, 35] that angular feature difference\nwell characterizes the semantic gap. SphereNet [36] shows\nthat training a neural network with all neurons normalized\nonto a unit hypersphere yields comparable capacity and\neven better generalizability, implying that the direction of\nneurons can fully capture the most important information\nfrom data. To better demonstrate the importance of neuron\nangles, we conduct a toy experiment in Figure 2 where we\ntrain a standard convolutional autoencoder on some flower\nimages. In the training stage, we use the standard inner\nproduct to produce the feature map (z denotes the element\noutput of the convolution kernel w and x is the input in\nthe sliding window). In the testing stage, we compare three\nways to generate the feature map: (a) the inner product\nused in training, (b) the magnitude information, and (c) the\nangular information. The results in Figure 2 show that the angular information of neurons can almost\nperfectly recover the input images, while the magnitude of neurons contains no useful information.\nWe emphasize that we do not apply the cosine activation (c) during training, and the training is\ndone only based on inner product. The results imply that the angles (directions) of neurons play the\nmajor role in storing the semantic information of the input images. In order to modify the semantic\ninformation of images, finetuning the neuron directions will likely be more effective.\nControl signal\nOriginal image\nEpoch 1\nEpoch 20\nw/ orthogonal\nw/o orthogonal\nFigure 3: Controllable generation with or\nwithout orthogonality. Middle column is from\nthe original OFT, and the right column is given\nby OFT without the orthogonality constraint.\nFor the second question, the simplest way to finetune direction of\nneurons is to simultaneously rotate / reflect all the neurons (in the\nsame layer), which naturally brings in orthogonal transformation.\nIt may be more flexible to use some other angular transformation\nthat rotates different neurons with different angles, but we find that\northogonal transformation is a sweet spot between flexibility and\nregularity. Moreover, [33] shows that orthogonal transformation is\nsufficiently powerful for learning neural networks. To support our\nargument, we perform an experiment to demonstrate the effective\nregularization induced by the orthogonality constraint. We perform\nthe controllable generation experiment using the setting of Con-\ntrolNet [68], and the results are given in Figure 3. We can observe that our standard OFT performs\nquite stably and achieves accurate control after the training is finished (epoch 20). In comparison,\nOFT without the orthogonality constraint fails to generate any realistic image and achieve no control\neffect. The experiment validates the importance of the orthogonality constraint in OFT.\n3.2\nGeneral Framework\nThe conventional finetuning strategy typically uses gradient descent with a small learning rate to\nupdate a model (or certain layers of a model). The small learning rate implicitly encourages a small\n4\ndeviation from the pretrained model, and the standard finetuning essentially aims to train the model\nwhile implicitly minimizing \u2225M \u2212M 0\u2225 where M is the finetuned model weights and M 0 is the\npretrained model weights. This implicit constraint makes intuitive sense, but it can still be too flexible\nfor finetuning a large model. To address this, LoRA introduces an additional low-rank constraint for\nthe weight update, i.e., rank(M \u2212M 0)=r\u2032 where r\u2032 is set to be some small number. Different from\nLoRA, OFT introduces a constraint for the pair-wise neuron similarity: \u2225HE(M)\u2212HE(M 0)\u2225=0\nwhere HE(\u00b7) denotes hyperspherical energy of a weight matrix. As an illustrative example, we\nconsider a fully connected layer W ={w1, \u00b7 \u00b7 \u00b7 , wn}\u2208Rd\u00d7n where wi \u2208 Rd is the i-th neuron (W 0\nis the pretrained weights). The output vector z \u2208Rn of this fully connected layer is computed by\nz =W \u22a4x where x\u2208Rd is the input vector. OFT can be interpreted as minimizing the hyperspherical\nenergy difference between the finetuned model and the pretrained model:\nmin\nW\n\r\rHE(W ) \u2212 HE(W 0)\n\r\r\n\u21d4\nmin\nW\n\r\r\r\r\nX\ni\u0338=j\n\u2225 \u02c6wi \u2212 \u02c6wj\u2225\u22121 \u2212\nX\ni\u0338=j\n\u2225 \u02c6w0\ni \u2212 \u02c6w0\nj \u2225\u22121\n\r\r\r\r\n(1)\nwhere \u02c6wi :=wi/\u2225wi\u2225 denotes the i-th normalized neuron, and the hyperspherical energy of a fully\nconnected layer W is defined as HE(W ):=P\ni\u0338=j \u2225 \u02c6wi \u2212 \u02c6wj\u2225\u22121. One can easily observe that the\nattainable minimum is zero for Eq. (1). The minimum can be achieved as long as W and W 0 differ\nonly up to a rotation or reflection, i.e., W =RW 0 in which R\u2208Rd\u00d7d is an orthogonal matrix (The\ndeterminant 1 or \u22121 means rotation or reflection, respectively). This is exactly the idea of OFT,\nthat we only need to finetune the neural network by learning layer-shared orthogonal matrices to\ntransform neurons in each layer. Formally, OFT seeks to optimize the orthogonal matrix R\u2208Rd\u00d7d\nfor a pretrained fully connected layer W 0 \u2208Rd\u00d7n, changing the forward pass from z =(W 0)\u22a4x to\nz = W \u22a4x = (R \u00b7 W 0)\u22a4x,\ns.t. R\u22a4R = RR\u22a4 = I\n(2)\nwhere W denotes the OFT-finetuned weight matrix and I is an identity matrix. OFT is illustrated in\nFigure 4. Similar to the zero initialization in LoRA, we need to ensure OFT to finetune the pretrained\nmodel exactly from W 0. To achieve this, we initialize the orthogonal matrix R to be an identity\nmatrix so that the finetuned model starts with the pretrained weights. To guarantee the orthogonality\nof the matrix R, we can use differential orthogonalization strategies discussed in [26, 33]. We will\ndiscuss how to guarantee the orthogonality in a computationally efficient way.\n3.3\nEfficient Orthogonal Parameterization\nx\nR\nOrthogonal\nMatrix\nPretrained \nWeight Matrix\nW 0\nx\nOrthogonal\nMatrix\nPretrained \nWeight Matrix\nW 0\n...\nR1\nR2\nR3\nRr\n(a) Original OFT\n(b) OFT with block-diagonal R\nd\nd\nn\nd\nn\nd/r\nFigure 4: (a) Original OFT without a diagonal structure. (b) OFT with r diagonal\nblocks of the same size. When r = 1, the case of (b) recovers the case of (a).\nStandard orthogonalization such as Gram-\nSchmidt method, despite differentiable, is\noften too expensive to compute in prac-\ntice [33]. For better efficiency, we adopt\nCayley parameterization to generate the\northogonal matrix. Specifically, we con-\nstruct the orthogonal matrix with R=\n(I +Q)(I \u2212Q)\u22121 where Q is a skew-symmetric matrix satisfying Q=\u2212Q\u22a4. Such an efficiency\ncomes at a small price \u2013 the Cayley parameterization can only produce orthogonal matrices with\ndeterminant 1 which belongs to the special orthogonal group. Fortunately, we find that such a limita-\ntion does not affect the performance in practice. Even if we use Cayley transform to parameterize\nthe orthogonal matrix, R can still be very parameter-inefficient with a large d. To address this, we\npropose to represent R with a block-diagonal matrix with r blocks, leading to the following form:\nR = diag(R1, R2, \u00b7 \u00b7 \u00b7 , Rr) =\n\uf8ee\n\uf8ef\uf8f0\nR1 \u2208 O( d\nr )\n...\nRr \u2208 O( d\nr )\n\uf8f9\n\uf8fa\uf8fb \u2208 O(d)\n(3)\nwhere O(d) denotes the orthogonal group in dimension d, and R\u2208Rd\u00d7d and Ri \u2208Rd/r\u00d7d/r, \u2200i are\northogonal matrices. When r=1, then the block-diagonal orthogonal matrix becomes a standard\nunconstrained one. For an orthogonal matrix with size d\u00d7d, the number of parameters is d(d\u22121)/2,\nresulting in a complexity of O(d2). For an r-block diagonal orthogonal matrix, the number of\nparameter is d(d/r\u22121)/2, resulting in a complexity of O(d2/r). We can optionally share the block\nmatrix to further reduce the number of parameters, i.e., Ri =Rj, \u2200i\u0338=j. This reduces the parameter\ncomplexity to O(d2/r2). Despite all these strategies to improve parameter efficiency, we note that the\nresulting matrix R remains orthogonal, so there is no sacrifice in preserving hyperspherical energy.\n5\nWe discuss how OFT compares to LoRA in terms of parameter efficiency. For LoRA with a low-rank\nparameter r\u2032, we have its number of trainable parameters as r\u2032(d+n). If we consider both r and r\u2032 to\nbe dependent on the neuron dimension d (e.g., r=r\u2032 =\u03b1d where 0<\u03b1\u22641 is some constant), then the\nparameter complexity of LoRA becomes O(d2 + dn) and the parameter complexity of OFT becomes\nO(d). We illustrate the difference in complexity between OFT and LoRA with a concrete example.\nSuppose we have a weight matrix with size 128\u00d7128, LoRA has 2, 048 trainable parameters with\nr\u2032 =8, while OFT has 960 trainable parameters with r=8 (no block sharing is applied).\n3.4\nConstrained Orthogonal Finetuning\nWe can further limit the flexibility of original OFT by constraining the finetuned model to be within a\nsmall neighborhood of the pretrained model. Specifically, COFT uses the following forward pass:\nz = W \u22a4x = (R \u00b7 W 0)\u22a4x,\ns.t. R\u22a4R = RR\u22a4 = I, \u2225R \u2212 I\u2225 \u2264 \u03f5\n(4)\nwhich has an orthogonality constraint and an \u03f5-deviation constraint to an identity matrix. The\northogonality constraint can be achieved with the Cayley parameterization introduced in Section 3.3.\nHowever, it is nontrivial to incorporate the \u03f5-deviation constraint to the Cayley-parameterized\northogonal matrix. To gain more insights on the Cayley transform, we apply the Neumann series to ap-\nproximate R=(I +Q)(I \u2212Q)\u22121 as R\u2248I +2Q+O(Q2) (under the assumption that the Neumann\n\u03b5= 0\n\u03b5 1e-4 d\nOFT \u03b5   )\n.\nInput image\n( =\u221e\n=\n\u03b5 1e-5 d\n.\n=\n\u03b5 1e-6 d\n.\n=\n\u03b5 1e-7 d\n.\n=\nText prompt: a [V] toy on the beach\nFigure 5: How \u03f5 affects the flexibility of COFT in subject-driven generation.\nseries converges in the operator\nnorm). Therefore, we can move\nthe constraint \u2225R\u2212I\u2225\u2264\u03f5 inside\nthe Cayley transform, and the\nequivalent constraint is \u2225Q\u22120\u2225\u2264\n\u03f5\u2032 where 0 denotes an all-zero ma-\ntrix and \u03f5\u2032 is another error hyper-\nparameter (different than \u03f5). The new constraint on the matrix Q can be easily enforced by projected\ngradient descent. To achieve identity initialization for the orthogonal matrix R, we initialize Q\nas an all-zero matrix. COFT can be viewed as a combination of two explicit constraints: minimal\nhyperspherical energy difference and constrained deviation from the pretrained model. The second\nconstraint is usually implicitly used by existing finetuning methods, but COFT makes it an explicit\none. Despite the excellent performance of OFT, we observe that COFT yields even better finetuning\nstability than OFT due to this explicit deviation constraint. Figure 5 provides an example on how\n\u03f5 affects the performance of COFT. We can observe that \u03f5 controls the flexibility of finetuning.\nWith larger \u03f5, the COFT-finetuned model resembles the OFT-finetuned model. With smaller \u03f5, the\nCOFT-finetuned model behaves increasingly similar to the pretrained text-to-image diffusion model.\n3.5\nRe-scaled Orthogonal Finetuning\nWe propose a simple extension to the original OFT by additionally learning a magnitude scaling\ncoefficient for each neuron. This is motivated by the fact that re-scaling neurons does not change the\nhyperspherical energy (the magnitude will be normalized out). Specifically, we use the forward pass:\nz =(RW 0D)\u22a4x1 where D=diag(s1, \u00b7 \u00b7 \u00b7 , sn)\u2208Rn\u00d7n is a learnable diagonal matrix with all the\ndiagonal element s1, \u00b7 \u00b7 \u00b7 , sn larger than zero. In contrast to OFT\u2019s original forward pass in Eq. (2)\nwhere only R is learnable, we have both the diagonal matrix D and the orthogonal matrix R learnable.\nThe re-scaled OFT further improves the flexibility of OFT with a neglectable number of additional\nparameters. We stick to the original OFT in the experiment to show the effectiveness of orthogonal\ntransformation alone, but we find that the re-scaled OFT is generally better (see Appendix C).\n4\nIntriguing Insights and Discussions\nOFT is agnostic to different architectures. We can apply OFT to any type of neural network in\nprinciple. For Transformers, LoRA is typically applied to the attention weights [22]. To compare\nfairly to LoRA, we only apply OFT to finetune the attention weights in our experiments. Besides\nfully connected layers, OFT is also well suited for finetuning convolution layers, because the block-\ndiagonal structure of R has interesting interpretations in convolution layers (unlike LoRA). When\nwe use the same number of blocks as the number of input channels, each block only transforms a\nunique neuron channel, similar to learning depth-wise convolution kernels [10]. When all the blocks\n1Errata: In the NeurIPS camera ready version, the forward pass of re-scaled OFT is mistakenly written as\nz =(DRW 0)\u22a4x. The original implementation is correct, so the results in Appendix C are unaffected.\n6\nin R are shared, OFT transforms the neurons with an orthogonal matrix shared across channels. We\nconduct a preliminary study on finetuning convolution layers with OFT in Appendix D\nConnection to LoRA. By adding a low-rank matrix, LoRA prevents the information in the pretrained\nweight matrix from shifting dramatically. In contrast, OFT controls the transform that applies to\nthe pretrained weight matrix to be orthogonal (full-rank), which prevents the transform to destroy\nthe pretraining information. We can rewrite OFT\u2019s forward pass as z =(RW 0)\u22a4x=(W 0 +(R\u2212\nI)W 0)\u22a4x where (R\u2212I)W 0 is analogous to LoRA\u2019s low-rank weight update. Since W 0 is typically\nfull-rank, OFT also performs low-rank weight update when R\u2212I is low-rank. Similar to LoRA that\nhas a rank parameter r\u2032, OFT has a diagonal block parameter r to reduce the number of trainable\nparameters. More interestingly, LoRA and OFT represent two distinct ways to be parameter-efficient.\nLoRA exploits the low-rank structure to reduce the number of trainable parameters, while OFT takes\na different route by exploiting the sparsity structure (i.e., block-diagonal orthogonality).\nWhy OFT converges faster. On one hand, we can see from Figure 2 that the most effective update\nto modify the semantics is to change neuron directions, which is exactly what OFT is designed for.\nOn the other hand, OFT can be viewed as finetuning neurons on a smooth hypersphere manifold,\nwhich yields better optimization landscape. This is also empirically verified in [33].\nWhy not minimize hyperspherical energy. A key difference to [33] is that we do not aim to\nminimize hyperspherical energy. In classification problems, neurons without redundancy are desired.\nThe minimum hyperspherical energy means all neurons are uniformly spaced around the hypersphere.\nThis is not a meaningful objective for finetuning, as it may destroy the pretraining information.\nTrade-off between flexibility and regularity in finetuning. We discover an underlying trade-off\nbetween flexibility and regularity. Standard finetuning is the most flexible method, but it yields poor\nstability and easily causes model collapse. Being surprisingly simple, OFT finds a good balance\nbetween flexibility and regularity by preserving the pairwise neuron angles. The block-diagonal\nparameterization can also be viewed as a stronger regularization of the orthogonal matrix.\nNo additional inference overhead. Unlike ControlNet, our OFT framework introduces no additional\ninference overhead to the finetuned model. In the inference stage, we can simply multiply the learned\northogonal matrix R into the pretrained weight matrix W 0 and obtain an equivalent weight matrix\nW =RW 0. Thus the inference speed is the same as the pretrained model.\n5\nExperiments and Results\nGeneral settings. In the experiment, we use Stable Diffusion v1.5 [50] as the pretrained text-to-image\nmodel. For fairness, we randomly pick generated images from each method. For subject-driven\ngeneration, we generally follow DreamBooth [51]. For controllable generation, we generally follow\nControlNet [68] and T2I-Adapter [38]. To ensure a fair comparison to LoRA, we only apply OFT or\nCOFT to the same layer where LoRA is used. More results and details are given in Appendix A.\n5.1\nSubject-driven Generation\nIter. 400\nIter. 1000\nIter. 2000\nDreamBooth\nLoRA\nCOFT\nText prompt: a [V] cat wearing a yellow shirt\nOFT\nInput images\nFigure 6: Generated images across different iterations.\nSettings. We use DreamBooth [51] and LoRA [22] as the\nbaselines. All the methods adopt the same loss function as\nin DreamBooth. For DreamBooth and LoRA, we generally\nfollow the original paper and use the best hyperparameter\nsetup. More results are provided in Appendix A,E,F,J.\nFinetuning stability and convergence. We first evalu-\nate the finetuning stability and the convergence speed for\nDreamBooth, LoRA, OFT and COFT. Results are given in\nFigure 1 and Figure 6. We can observe that both COFT and\nOFT are able to finetune the diffusion model quite stably.\nAfter 400 iterations, both DreamBooth and OFT variants\nachieve good control, while LoRA fails to preserve the\nsubject identity. After 2000 iterations, DreamBooth starts\nto generate collapsed images, and LoRA fails to generate\nyellow shirt (and instead generates yellow fur). In contrast,\nboth OFT and COFT are still able to achieve stable and consistent control over the generated image.\nThese results validate the fast yet stable convergence of our OFT framework in subject-driven genera-\n7\na [V] bowl with a blue house in the background\na [V] bowl with a tree and autumn leaves in the background\nInput images\nDreamBooth\nLoRA\nCOFT\nA [V] clock on the beach\nA [V] clock on top of green gras with sunflower\nInput images\nDreamBooth\nLoRA\nCOFT\nOFT\nOFT\na [V] dog in a police outfit\na [V] dog wearing a black top and a monocle\nInput images\nDreamBooth\nLoRA\nCOFT\nOFT\na [V] stuffed animal on top of a pink fabric\na [V] stuffed animal in the snow\nInput images\nDreamBooth\nLoRA\nCOFT\nOFT\nFigure 7: Qualitative comparison of subject-driven generation among DreamBooth, LoRA, COFT and OFT. Results are generated with the same\nfinetuned model from each method. All examples are randomly picked. The figure is best viewed digitally, in color and significantly zoomed in.\ntion. We note that the insensitivity to the number of finetuning iteration is quite important, since it\ncan effectively alleviate the trouble of tuning the iteration number for different subjects. For both\nOFT and COFT, we can directly set a relatively large iteration number without carefully tuning it.\nFor COFT with a proper \u03f5, both the learning rate and the iteration number become effortless to set.\nMethod\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nLPIPS \u2191\nReal Images\n0.703\n0.864\n-\n0.695\nDreamBooth\n0.614\n0.778\n0.239\n0.737\nLoRA\n0.613\n0.765\n0.237\n0.744\nCOFT\n0.630\n0.783\n0.235\n0.744\nOFT\n0.632\n0.785\n0.237\n0.746\nTable 1: Quantitative comparison of subject fidelity (DINO,\nCLIP-I), prompt fidelity (CLIP-T) and diversity metric\n(LPIPS). The evaluation images and prompts are the same as\n[51] (25 subjects with 30 text prompts each subject).\nQuantitative comparison. Following [51], we conduct\na quantitative comparison to evaluate subject fidelity\n(DINO [5], CLIP-I [44]), text prompt fidelity (CLIP-\nT [44]) and sample diversity (LPIPS [69]). CLIP-I com-\nputes the average pairwise cosine similarity of CLIP\nembeddings between generated and real images. DINO\nis similar to CLIP-I, except that we use ViT S/16 DINO\nembeddings. CLIP-T is the average cosine similarity of\nCLIP embeddings between text prompt and generated\nimages. We also evaluate average LPIPS cosine simi-\nlarity between generated images of the same subject with the same text prompt. Table 1 show that\nboth COFT and OFT outperforms DreamBooth and LoRA in the DINO and CLIP-I metrics by a\nconsiderable margin, while achieving slightly better or comparable performance in prompt fidelity\nand diversity metric. For each method, we repeatedly finetune the same pretrained model with 30\ndifferent random seeds to minimize randomness. The results show that our OFT framework not only\nachieves better convergence and stability, but also yields consistently better final performance.\nQualitative comparison. To have a more intuitive understanding of OFT\u2019s benefits, we show some\nrandomly picked examples for subject-driven generation in Figure 7. For a fair comparison, all the\nexamples are generated from the same finetuned model using each method, so no text prompt will\nbe separately optimized for its final results. For each method, we select the model that achieves\nthe best validation CLIP metrics. From the results in Figure 7, we can observe that both OFT and\nCOFT deliver excellent semantic subject preservation, while LoRA often fails to preserve the subject\nidentity (e.g., LoRA completely loses the subject identity in the bowl example). In the meantime, both\nOFT and COFT have much more accurate control using text prompts, while DreamBooth, despite its\npreservation of subject identity, often fails to generate the image following the text prompt (e.g., the\nfirst row of the bowl example). The qualitative comparison demonstrates that our OFT framework\nachieves better controllability and subject preservation at the same time. Moreover, the number of\niterations is not sensitive in OFT, so OFT performs well even with a large number of iterations, while\nneither DreamBooth nor LoRA can. More qualitative examples are given in Appendix F. Moreover,\nwe conduct a human evaluation in Appendix H which further validates the superiority of OFT.\n5.2\nControllable Generation\nSettings. We use ControlNet [68], T2I-Adapter [38] and LoRA [22] as the baselines. We consider\nthree challenging controllable generation tasks in the main paper: Canny edge to image (C2I) on the\n8\nCOCO dataset [31], segmentation map to image (S2I) on the ADE20K dataset [70] and landmark\nto face (L2F) on the CelebA-HQ dataset [25, 63]. All the methods are used to finetune Stable\nDiffusion (SD) v1.5 on these three datasets for 20 epochs. More results are given in Appendix F,G,J.\n5\n10\n15\n20\nNumber of epochs\n10\n15\n20\n25\n30\n35\nLandmark error\nT2I-Adapter\nControlNet\nLoRA\nCOFT\nOFT\nFigure 8: Face landmark error.\nConvergence. We evaluate the convergence speed of ControlNet, T2I-\nAdapter, LoRA and COFT on the L2F task. We provide both quantitative\nand qualitative evaluation. Specifically for the evaluation metric, we\ncompute the mean \u21132 distance between control face landmarks and pre-\ndicted face landmarks. In Figure 8, we plot the face landmark error\nobtained by the model finetuned with different number of epochs. We\ncan observe that both COFT and OFT achieve significantly faster con-\nvergence. It takes 20 epochs for LoRA to converge to the performance\nof our OFT framework at the 8-th epoch. We note that OFT and COFT\nuse a similar number of trainable parameters to LoRA (much fewer than\nControlNet), while being much more efficient to converge than existing methods. On the other hand,\nthe fast convergence of OFT is also validated by the results in Figure 1. The right example in Figure 1\nshows that OFT is much more data-efficient than ControlNet and LoRA, since OFT can converge well\nwith only 5% of the ADE20K dataset. For qualitative results, we focus on comparing OFT, COFT\nand ControlNet, because ControlNet achieves the closest landmark error to ours. Results in Figure 9\nshow that both OFT and COFT converge stably and the generated face pose is gradually aligned\nwith the control landmarks. In contrast to our stable and smooth convergence, the controllability in\nControlNet suddenly emerges after the 8-th epoch, which perfectly matches the sudden convergence\nphenomenon observed in [68]. Such a convergence stability makes our OFT framework much easier\nto use in practice, since the training dynamics of OFT is far more smooth and predictable. Thus it\nwill be easier to find good OFT\u2019s hyperparameters.\nControl\nGround truth\nControlNet\nCOFT\nEpoch 4\nEpoch 6\nEpoch 8\nEpoch 10\na man with \nlong hair\nwearing \na red shirt\nOFT\nFigure 9: Qualitative examples with different number of epochs.\nQuantitative comparison. We introduce a control\nconsistency metric to evaluate the performance of\ncontrollable generation. The basic idea is to com-\npute the control signal from the generated image and\nthen compare it with the original input control sig-\nnal. For the C2I task, we compute IoU and F1 score.\nFor the S2I task, we compute mean IoU, mean and\noverall accuracy. For the L2F task, we compute the\nmean \u21132 distance between control landmarks and\npredicted landmarks. More details regarding the\nconsistency metrics are given in Appendix A. For\nall the compared method, we use the best possible hyperparameter settings. Results in Table 2 show\nthat both OFT and COFT yield much stronger and accurate control than the other methods. We\nobserve that the adapter-based approaches (e.g., T2I-Adapter and ControlNet) converge slowly and\nalso yield worse final results. Compared to ControlNet, LoRA performs better in the S2I task and\nworse in the C2I and L2F tasks. In general, we find that the performance ceiling of LoRA is relatively\nlow, even if we have carefully tuned its hyperparameters. As a comparison, the performance of our\nOFT framework has not yet saturated, since we empirically find that it still gets better as the number\nof trainable parameters gets large. We emphasize that our quantitative evaluation in controllable\ngeneration is one of our novel contributions, since it can accurately evaluate the control performance\nof the finetuned models (up to the accuracy of the off-the-shelf segmentation/detection model).\nTask\nMetric\nSD\nControlNet\nT2I-Adapter\nLoRA\nCOFT\nOFT\nC2I\nIoU \u2191\n0.049\n0.189\n0.078\n0.168\n0.195\n0.193\nF1 \u2191\n0.093\n0.317\n0.143\n0.286\n0.325\n0.323\nS2I\nmIoU \u2191\n7.72\n20.88\n16.38\n22.98\n26.92\n27.06\nmAcc \u2191\n14.40\n30.91\n26.31\n35.52\n40.08\n40.09\naAcc \u2191\n33.61\n61.42\n51.63\n58.03\n62.96\n62.42\nL2F\nError \u2193\n146.19\n7.61\n23.75\n7.68\n6.92\n7.07\nTable 2: Quantitative comparison of control signal consistency for three control\ntasks (Canny edge to image, segmentation to image and landmark to face).\nQualitative comparison. We also qual-\nitatively compare OFT and COFT to\ncurrent state-of-the-art methods, includ-\ning ControlNet, T2I-Adapter and LoRA.\nRandomly generated images in Fig-\nure 10 show that OFT and COFT not\nonly yield high-fidelity and realistic im-\nage quality, but also achieve accurate\ncontrol. In the S2I task, we can see that\nLoRA completely fails to generate images following the input segmentation map, while Control-\nNet, OFT and COFT can well control the generated images. In contrast to ControlNet, both OFT\nand COFT are able to generate high-fidelity images with more vivid details and more reasonable\n9\nSegmentation to Imge\nCanny edge to Image\nLandmark to face\nGround truth\nControl\nControlNet\nT2I-Adapter\nLoRA\nCOFT\nOFT\na tropical island\na building\na cat\na truck\na women with curly hair\na man with long hair wearing\na red shirt\na building\nFigure 10: Qualitative comparison of controllable generation. The figure is best viewed digitally, in color and significantly zoomed in.\ngeometric structures with far less model parameters. In the C2I task, both OFT and COFT are\nable to hallucinate semantically similar images based on a rough Canny edges, while T2I-Adapter\nand LoRA perform much worse. In the L2F task, our method produces the most accurate pose\ncontrol for the generated faces even under challenging face poses. In all three control tasks, we\nshow that both OFT and COFT produce qualitatively better images than the state-of-the-art baselines,\ndemonstrating the effectiveness of our OFT framework in controllable generation. To give a more\ncomprehensive qualitative comparison, we provide more qualitative examples for all the three control\ntasks in Appendix F.2, and moreover, we demonstrate OFT can perform well on more control tasks\n(including dense pose to human body, sketch to image and depth to image) in Appendix G.\n6\nConcluding Remarks and Open Problems\nMotivated by the observation that angular information among neurons crucially determines visual\nsemantics, we propose a simple yet effective finetuning method \u2013 orthogonal finetuning for controlling\ntext-to-image diffusion models. Specifically, we target two text-to-image applications: subject-driven\ngeneration and controllable generation. Compared to existing methods, OFT demonstrates stronger\ncontrollability and finetuning stability with fewer number of finetuning parameters. More importantly,\nOFT does not introduce additional inference overhead, leading to an efficient deployable model.\nOFT also introduces a few interesting open problems. First, OFT guarantees the orthogonality via\nCayley parametrization which involves a matrix inverse. It slightly limits the scalability of OFT.\nAlthough we address this limitation using block diagonal parametrization, how to speed up this\nmatrix inverse in a differentiable way remains a challenge. Second, OFT has unique potential in\ncompositionality, in the sense that the orthogonal matrices produced by multiple OFT finetuning tasks\ncan be multiplied together and remains an orthogonal matrix. Whether this set of orthogonal matrices\npreserve the knowledge of all the downstream tasks remains an interesting direction to study. Finally,\nthe parameter efficiency of OFT is largely dependent on the block diagonal structure which inevitably\nintroduces additional biases and limits the flexibility. How to improve the parameter efficiency in a\nmore effective and less biased way remains an important open problem.\n10\nAcknowledgement\nThe authors would like to sincerely thank Luigi Gresele, Yandong Wen, Yuliang Xiu and many other\ncolleagues at Max Planck Institute for Intelligent Systems for many helpful suggestions.\nThis work was supported by the German Federal Ministry of Education and Research (BMBF): Tubin-\ngen AI Center, FKZ: 01IS18039B, and by the Machine Learning Cluster of Excellence, EXC number\n2064/1 \u2013 Project number 390727645. WL was supported by the German Research Foundation (DFG):\nSFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP XX, project number:\n276693517. AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1,\nThe Alan Turing Institute, and the Leverhulme Trust via CFI.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. In NeurIPS, 2022. 3\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski.\nBlended latent diffusion.\narXiv preprint\narXiv:2206.02779, 2022. 3\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020. 4\n[4] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment\nproblem? (and a dataset of 230,000 3d facial landmarks). In ICCV, 2017. 17\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 8\n[6] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano.\nInstance-conditioned gan. In NeurIPS, 2021. 3\n[7] Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava, Animesh Garg, and Animashree\nAnandkumar. Angular visual hardness. In ICML, 2020. 2, 4\n[8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning\nmethod for denoising diffusion probabilistic models. In ICCV, 2021. 3\n[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan:\nUnified generative adversarial networks for multi-domain image-to-image translation. In CVPR, 2018. 3\n[10] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017. 6\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 4\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS,\n2021. 3\n[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. In NeurIPS,\n2021. 3\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.\nIn CVPR, 2021. 3\n[15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618, 2022. 3\n[16] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining\nGuo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022. 3\n[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In CVPR, 2022. 4\n11\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 17\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3\n[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML,\n2019. 4\n[22] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models. In ICLR, 2022. 1, 2, 4, 6, 7, 8, 20\n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional\nadversarial networks. In CVPR, 2017. 3\n[24] Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human:\nText-driven controllable human image generation. ACM Transactions on Graphics (TOG), 41(4):1\u201311,\n2022. 16\n[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved\nquality, stability, and variation. In ICLR, 2018. 9, 16\n[26] Mario Lezcano-Casado and David Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A\nsimple parametrization of the orthogonal and unitary group. In ICML, 2019. 5\n[27] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation. In\nNeurIPS, 2019. 3\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3\n[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML, 2022. 3, 16\n[30] Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James M Rehg, Li Xiong, and Le Song.\nRegularizing neural networks via minimizing hyperspherical energy. In CVPR, 2020. 3\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 9, 16\n[32] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards\nminimum hyperspherical energy. In NeurIPS, 2018. 2, 3\n[33] Weiyang Liu, Rongmei Lin, Zhen Liu, James M Rehg, Liam Paull, Li Xiong, Le Song, and Adrian Weller.\nOrthogonal over-parameterized training. In CVPR, 2021. 2, 3, 4, 5, 7\n[34] Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Sch\u00f6lkopf, and Adrian Weller. Learning with\nhyperspherical uniformity. In AISTATS, 2021. 2\n[35] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and Le Song.\nDecoupled networks. In CVPR, 2018. 2, 4\n[36] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep\nhyperspherical learning. In NIPS, 2017. 2, 4\n[37] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In NeurIPS, 2019. 3\n[38] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023. 1, 3, 4, 7, 8\n[39] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. In ICML, 2022. 3\n12\n[40] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of\nclasses. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008. 16\n[41] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael\nPritch, and Daniel Cohen-Or. Mystyle: A personalized generative prior. ACM Transactions on Graphics\n(TOG), 41(6):1\u201310, 2022. 3\n[42] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-\nadaptive normalization. In CVPR, 2019. 3\n[43] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion:\nNon-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 4\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021. 3, 8\n[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 3\n[46] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 3\n[47] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on\npattern analysis and machine intelligence, 44(3):1623\u20131637, 2020. 16\n[48] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In ICML, 2016. 3\n[49] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based\nediting of real images. ACM Transactions on Graphics, 42(1):1\u201313, 2022. 3\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 7\n[51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 1, 2, 3, 7,\n8, 16\n[52] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and\nMohammad Norouzi. Palette: Image-to-image diffusion models. In SIGGRAPH 2022, pages 1\u201310, 2022. 3\n[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. In NeurIPS, 2022. 1, 3\n[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 3\n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3\n[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In ICLR, 2021. 3\n[57] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of\ngeneric visual-linguistic representations. In ICLR, 2020. 3\n[58] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models.\narXiv preprint arXiv:2211.13752, 2022. 3\n[59] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining\nis all you need for image-to-image translation. arXiv preprint arXiv:2205.12952, 2022. 3\n[60] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-\nresolution image synthesis and semantic manipulation with conditional gans. In CVPR, 2018. 3\n[61] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\nlanguage model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021. 3\n13\n[62] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00fcwa: Visual\nsynthesis pre-training for neural visual world creation. In ECCV, 2022. 3\n[63] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image\ngeneration and manipulation. In CVPR, 2021. 9\n[64] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\nSimple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021. 17\n[65] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. In CVPR,\n2018. 3\n[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich\ntext-to-image generation. Transactions on Machine Learning Research, 2022. 3\n[67] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N\nMetaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.\nIn ICCV, 2017. 3\n[68] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 1, 2, 3, 4, 7, 8, 9, 16\n[69] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018. 8\n[70] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017. 9, 16\n[71] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli\nShechtman. Toward multimodal image-to-image translation. In NeurIPS, 2017. 3\n14\nAppendix\nTable of Contents\nA\nExperimental Details\n16\nB\nEffect of Different Number of Diagonal Blocks\n18\nC\nExperiments on Re-scaled OFT\n19\nD\nApplying OFT to Convolution Layers\n20\nE\nComparison between COFT and OFT\n21\nF\nMore Qualitative Results\n23\nF.1\nSubject-driven Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nF.2\nControllable Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nG\nMore Controllable Generation Tasks\n34\nG.1\nDense Pose to Human Body\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.2\nSketch to Image\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nG.3\nDepth to Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nH\nHuman Evaluation\n40\nI\nStyle Transfer by Adapting Stable Diffusion with Orthogonal Finetuning\n41\nJ\nFailure Cases\n42\nJ.1\nFailure Cases in Subject-driven Generation\n. . . . . . . . . . . . . . . . . . .\n42\nJ.2\nFailure Cases in Controllable Generation . . . . . . . . . . . . . . . . . . . . .\n43\n15\nA\nExperimental Details\nTo verify the effectiveness of our Orthogonal Fine-tuning (OFT) method, we extensively evaluate the\nperformance of our method in two common text-to-image generation tasks: subject-driven generation\nand controllable generation. More specifically, we use the exact same task setting as ControlNet [68]\nand Dreambooth [51] and the baseline implementations were sourced from the GitHub repository\nDiffusers2 and ControlNet3.\nData and Model.\nFor training the convolutional autoencoder from Figure 2, we use 1000 random\nimages from the Oxford 102 Flower dataset [40]. For the task of subject-driven generation, we use the\nofficial DreamBooth dataset, which consists of 30 subjects from 15 different classes. For each subject,\nthere are several images and 25 different text prompts. For generating the image-control-caption\ncombinations, we use BLIP [29] to automatically caption the images (pre-trained model weight and\ncode for captioning based on the GitHub repository BLIP4). Note, although COCO provides captions\nfor the training and validation split, to be consistent with other image-control-caption combinations,\nwe instead use the BLIP-generated captions as text prompts. For the C2I task, we use the whole\nCOCO 2017 dataset [31] with in total of 180K images; we generate canny edge images as the control\nsignal using the same canny edge detector as ControlNet. For the S2I task, we use the semantic\nsegmentation dataset ADE20K [70] with in total of 24K image-segmentation mask pairs. For the\nL2F dataset, we use the CelebA-HQ dataset [25], which contains 30K images. Additionally, we\ndemonstrate that OFT also works well in other controllable generation tasks, including depth-to-\nimage (D2I), densepose-to-image (P2I), and sketch-to-image (Sk2I). For the D2I task, we also use\nthe COCO dataset and employ MiDaS [47] to generate depth maps; the pre-trained weights are\nobtained from the GitHub repository MiDaS5. For the P2I task, we use the DeepFashion-MultiModal\ndataset [24] with in total of 44K clothed human images with the corresponding densepose. For the\nSk2I task, we use a subset of the LAION-Aesthetics dataset with approximately 350K images to\nlearn sketch-guided image generation. We use the Stable Diffusion v1.56 as the base model.\nSubject-driven generation.\nFor training our subject-driven generation diffusion model, we follow\nthe training objective of Dreambooth. More specifically, we use the class-specific prior preservation\nloss to fine-tune our orthogonal matrices:\nEx,c,\u03f5,\u03f5\u2032,t[wt\u2225\u02c6x\u03b8(\u03b1tx + \u03c3t\u03f5, c) \u2212 x\u22252\n2 + \u03bbwt\u2032\u2225\u02c6x\u03b8(\u03b1t\u2032xpr + \u03c3t\u2032\u03f5\u2032, cpr) \u2212 xpr\u22252\n2],\n(5)\nwith cpr being the class conditioning vector. For calculating the prior-preservation loss, we additionally\nneed to generate 200 images using the subject\u2019s class prompt. Similar to LoRA, we inject our trainable\northogonal matrices into the attention modules of the stable diffusion model. To be comparable with\nLoRA, we choose the exact same linear layers as LoRA to affect upon: the linear layers Wq, Wk,\nWv and Wo. We perform training on 1 Tesla V100-SXM2-32GB GPU using a learning rate of\n6 \u00d7 10\u22125, batch size of 1, and train for approximately 1000 iterations. In the case of COFT, we use\n\u03f5 = 6 \u00d7 10\u22125 to constrain the orthogonal matrices.\nControllable generation.\nApart from injecting the trainable OFT weights into the stable diffusion\nmodel, we need to add a small encoding model to stable diffusion to encode the control signal. To\nbe comparable with ControlNet [68], we use the same encoding module, which is a shallow 8-layer\nconvolutional network with Scaled Exponential Linear Unit (SELU) activation functions. We also the\nsame training objective as ControlNet. The control signal is encoded and concatenated once with the\ninput to the stable diffusion U-Net. For the LoRA baseline, we use the same encoding module to\nencode the control signal. For S2I, L2I and P2I, we fine-tune the model for 20 epochs; for C2I and\nD2I we fine-tune the model for 10 epochs; for Sk2I we fine-tune the model for 8 epochs. We perform\ntraining on 4 NVIDIA A100-SXM4-80GB GPUs using a learning rate of 1 \u00d7 10\u22125, batch size of 4\nfor L2I and batch size of 16 for the rest of tasks. For fine-tuning with COFT, we use \u03f5 = 1 \u00d7 10\u22123.\n2https://github.com/huggingface/diffusers\n3https://github.com/lllyasviel/ControlNet\n4https://github.com/salesforce/BLIP\n5https://github.com/isl-org/MiDaS\n6https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\n16\nEvaluation.\nWhen evaluating the effectiveness of controllable generation, we primarily focus on\nevaluating the controllability. Using the consistency metrics introduced in the main paper, we can\neffectively compute the difference between the control signal and the generated image. For the\nC2I task, we apply the identical canny filter on the generated image to determine a canny image of\nthe predicted image. Both the control signal canny image and the canny image obtained from the\ngenerated images are black-and-white images, with pixel values being either 0 or 1. We evaluate the\npixel-wise Intersection over Union (IoU) and F1 score between these two canny predictions. For the\nS2I task, we compute mean IoU, mean and overall accuracy by deploying a pre-trained semantic\nsegmentation model. More specifically, we use the Segformer7 [64] model, which is trained on\nADE20K (Segformer-B4), to perform semantic segmentation on our generated images. We use the\nsegmentation accuracy as an indication for the overall semantically and structural resemblance of\nthe generated images to the ground truth image. For the L2F task, we compute the mean \u21132 distance\nbetween the input control landmarks and the landmarks estimated from generated images using facial\nlandmark detector [4].\nWe also evaluate the generation performance by calculating Fr\u00e9chet Inception Distance (FID) [19],\nwe use the default setting of the GitHub repository pytorch-fid8. The FID is a metric quantifying the\nsimilarity between two image dataset. It utilizes 2048-dimensional features, which are derived from\nthe final average pooling layer of a pretrained InceptionV3 network trained on ImageNet dataset. A\nlower FID score indicates a higher similarity between the datasets.\n7https://github.com/NVlabs/SegFormer\n8https://github.com/mseitzer/pytorch-fid\n17\nB\nEffect of Different Number of Diagonal Blocks\nWe note that the number of diagonal blocks r is an important hyperparameter that effectively\ncontrols the number of trainable parameters. It is necessary to perform a sensitivity study on this\nhyperparameter. Following the same settings as the main paper, we evaluate how r affects OFT in the\nS2I task. Results in Table 3 show that smaller r (closer to recovering the standard orthogonal matrix)\ngenerally works better than larger r. However, we find that a good trade-off between flexibility\nand parameter-efficiency indeed exists. Empirically, we find that we can use a much bigger r if the\ndataset is simple, leading to better parameter-efficiency and faster convergence. In the main paper,\nwe always use r = 4 because we find that r = 4 works well across datasets and tasks. Note that, in\nterms of the number of inference parameters, both LoRA and OFT have the exact same number of\nparameters, which is equal to the number of parameters of the underlying stable diffusion model,\nwhile ControlNet has an additional control model with 361M parameters.\nControlNet\nr = 2\nr = 4\nr = 8\nr = 16\nTrainable Parameters\n361.3 M\n29.5 M\n16.3 M\n9.7 M\n6.4 M\nInference Parameters\n1.42 B\n1.06 B\n1.06 B\n1.06 B\n1.06 B\nmIoU \u2191\n20.88\n27.18\n27.06\n24.09\n21.0\nmAcc \u2191\n30.91\n39.39\n40.09\n36.95\n32.55\naAcc \u2191\n61.42\n65.24\n62.96\n60.25\n55.5\nTable 3: How the number of diagonal blocks affects the control capability of OFT.\n18\nC\nExperiments on Re-scaled OFT\nSince both OFT and COFT transform neurons with orthogonal matrices and do not affect the\nmagnitude of neurons, their magnitude may be sub-optimal with their updated orientations. To\naddress this issue, we propose a re-scaled OFT where the neuron magnitude is refined using the same\nset of data in the downstream task. Specifically, re-scaled OFT further finetunes the magnitude of\nneurons without changing their directions. re-scaled OFT can be performed in two manners: (1) joint\nfitting: magnitude fitting can be used simultaneously with OFT or COFT, and (2) Post-stage fitting:\nmagnitude fitting can be used after OFT or COFT is finished. An important motivation for re-scaled\nOFT comes from Figure 2, where we observe that constructing images only with angular information\nperfectly preserves visual structures, but it also results in a certain degree of color distortion. We\nhypothesize that this minor color distortion is caused by magnitude loss and fixing this issue can\npotentially improve the visual quality of generated images.\nNotably, re-scaled OFT does not change the hyperspherical energy since it does not change the\ndirection of neurons - all the nice properties of OFT and COFT on hyperspherical energy are still\nperfectly preserved. Therefore, the advantage of structural preservation is also inherited.\nTo simplify the experiments and validate the effectiveness of re-scaled OFT, we perform post-stage\nmagnitude fitting on the COFT model and compare the FID between the original validation images\nand the generated images (using the control signals extracted from validation images). The reason\nwe use FID here is that FID is more sensitive to color distortion, while the consistency metric only\nmeasures the structural preservation. Table 4 shows that magnitude fitting can indeed improve the\nFID of COFT and is beneficial to COFT.\nMagnitude fitting is lightweight and can be implemented easily by simply adding one trainable\nparameter for each layer we modify; the parameter has the shape of (N \u00d7 1), with N corresponds to\nthe number of neurons in that specific layer. The performance gain illustrated in Table 4 is achieved by\nperforming Post-stage fitting on a COFT-fine-tuned model for only one additional epoch. Moreover,\nwe expect that the joint fitting re-scaled OFT can lead to better performance.\nSD\nControlNet\nT2I\nLoRA\nCOFT\nRe-scaled COFT\nFID \u2193\n41.2\n30.9\n33.1\n30.9\n30.8\n30.2\nTable 4: FID on the segmentation to image task (ADE20K). r = 4 is used here.\n19\nD\nApplying OFT to Convolution Layers\nIn the original setting [22], LoRA is only applied to the linear layers of the attention modules. To be a\nfair comparison, we also apply OFT to these weights. However, OFT is not limited to linear layers but\ncan easily be adapted to convolution layers by transforming the convolutional neurons. We highlight\nthe compatibility of OFT and COFT for finetuning convolution layers. More interestingly, sharing\nthe parameters of diagonal blocks in R becomes interpretable in convolution layers. With a suitable\nsetup, orthogonal matrices with sharing diagonal blocks can transform the convolution kernel in a\nchannel-sharing manner (or in a spatial manner), implying that the same orthogonal transformation is\napplied to all channels. This shares similar intuition with depth-wise convolution.\nFor this ablation experiment, we study the performance of applying OFT to the convolution layers in\nthe ResNet blocks of the stable diffusion model. In this experiment, we use COFT as the baseline\nmethod and consider the controllable generation (segmentation to image) as an example. We have\nboth quantitative (Table 5) and qualitative results (Figure 11). We can empirically observe that by only\nfine-tuning the convolutional layers, we can also achieve some degree of control. By simultaneously\nfine-tuning both linear and convolutional layers, we achieve a slightly better FID score. Note, for\nfine-tuning convolutional layers, we let r be equal to the number of channels of convolutional neurons\nin that layer.\nCOFT (attention)\nCOFT (conv)\nCOFT (extended)\nFID \u2193\n30.8\n39.8\n30.4\nTable 5: FID results of applying COFT to different types of layers. (with r = 4)\nText prompt: the inside of an airplane\nText prompt: a room with paintings on the wall\nText prompt: a sunset\nText prompt: a city\nConvolution\nAttention\nBoth\nOriginal image\nControl\nFigure 11: Controllable generation results of applying COFT to different types of layers.\n20\nE\nComparison between COFT and OFT\nWe have already provided many qualitative examples for COFT and OFT in the main paper. One may\nquestion the fundamental difference between OFT and COFT. Based on the intuition behind COFT,\nthe deviation constraint is introduced to improve the training stability. We demonstrate the training\nstability of COFT with a qualitative example in subject-driven generation. Results in Figure 12 and\nFigure 13 show that, despite being much more stable than existing methods, OFT will eventually\ngenerate collapsed images at the 9000-th iteration. In contrast, COFT still produces visually appealing\nimages. We train both OFT and COFT with a learning rate of 1 \u00d7 10\u22125 and constrain COFT with\n\u03f5 = 1 \u00d7 10\u22125.\nCOFT\nOFT\nCOFT\nOFT\nText prompt: a [V] teapot on top of a dirt road\nText prompt: a [V] dog on top of a wooden floor\nText prompt: a [V] dog wearing a yellow shirt\nInput images\nInput images\nCOFT\nOFT\nCOFT\nOFT\nText prompt: a [V] teapot on top of a dirt road\nIteration 2000\nIteration 9000\nFigure 12: Qualitative comparison between COFT and OFT on subject-driven generation.\n21\nText prompt: a [V] vase on top of the sidewalk in a crowded street\nCOFT\nOFT\nCOFT\nOFT\nText prompt: a [V] vase with a city in the back ground\nInput images\nIteration 2000\nIteration 9000\nText prompt: a [V] vase on top of a dirt road\nCOFT\nOFT\nText prompt: a [V] vase with a tree and autumn leaves in the background\nCOFT\nOFT\nFigure 13: Qualitative comparison between COFT and OFT on subject-driven generation.\n22\nF\nMore Qualitative Results\nF.1\nSubject-driven Generation\nText prompt: a [V] backpack floating on top of water\nText prompt: a [V] backpack in the snow\nText prompt: a [V] backpack on the beach\nText prompt: a purple [V] backpack\nText prompt: a [V] cat in a firefighter outfit\nText prompt: a [V] cat on top of a purple rug in a forest\nText prompt: a [V] cat wearing a red hat\nText prompt: a wet [V] cat\nCOFT\nOFT\nLoRA\nDreamBooth\nInput images\nFigure 14: More qualitative results on subject-driven generation.\n23\nText prompt: a [V] dog in the snow\nText prompt: a [V] dog wearing a yellow shirt\nText prompt: a [V] dog with a blue house in the background\nText prompt: a [V] in a police outfit\nText prompt: a [V] vase on a cobblestone street\nText prompt: a [V] vase on the beach\nText prompt: a [V] vase with a wheat field in the background\nText prompt: a purple [V] vase\nCOFT\nOFT\nLoRA\nDreamBooth\nInput images\nFigure 15: More qualitative results on subject-driven generation.\n24\nF.2\nControllable Generation\nF.2.1\nSegmentation to Image\nText prompt: an auditorium\nText prompt: two beds in a room\nText prompt: an art gallery\nText prompt: a thomass train at disneyland world\nText prompt: a small room with a bed and shelves\nText prompt: a bathroom with a sink and toilet\nText prompt: a house\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 16: More qualitative results of OFT and COFT on the segmentation to image generation task.\n25\nText prompt: a living room\nText prompt: a living room\nText prompt: a road in the middle of nowhere\nText prompt: a hallway in a building\nText prompt: the sun shining through the clouds over a lake\nText prompt: a person in a kitchen\nText prompt: bears\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 17: More qualitative results of OFT and COFT on the segmentation to image generation task.\n26\nText prompt: a bedroom\nText prompt: a dining room with a table and chairs\nText prompt: the living room\nText prompt: a living room with a vase of flowers\nText prompt: a volleyball court\nText prompt: a bedroom\nText prompt: a bedroom\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 18: More qualitative results of OFT and COFT on the segmentation to image generation task.\n27\nF.2.2\nCanny Edge to Image\nText prompt: a bus with the words scott brown on it\nText prompt: a sign on a pole\nText prompt: elephants in the wild\nText prompt: a man holding a skateboard\nText prompt: a woman with red hair\nText prompt: big ben and the houses of parliament\nText prompt: a kitchen\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 19: More qualitative results of OFT and COFT on the Canny edge to image generation task.\n28\nText prompt: a train going through a tunnel\nText prompt: a sandwich\nText prompt: people on the beach at sunset\nText prompt: a slice of pizza on a table\nText prompt: a bathroom with a toilet and a shower\nText prompt: a bathroom with a toilet and a bucket\nText prompt: a living room\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 20: More qualitative results of OFT and COFT on the Canny edge to image generation task.\n29\nText prompt: a train\nText prompt: a bowl of food on a table\nText prompt: two white horses in a field\nText prompt: a vase on a table\nText prompt: a fire truck parked in a parking\nText prompt: a bathroom\nText prompt: a bus parked in a park\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 21: More qualitative results of OFT and COFT on the Canny edge to image generation task.\n30\nF.2.3\nLandmark to Face\nText prompt: a woman with red hair\nText prompt: a man wearing a hat\nText prompt: a man wearing glasses\nText prompt: a man in a tuxedo\nText prompt: a man with glasses smiling\nText prompt: a woman with blonde hair\nText prompt: a woman with long brown hair\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 22: More qualitative results of OFT and COFT on the landmark to face generation task.\n31\nText prompt: a man with a beard\nText prompt: a man wearing glasses\nText prompt: a man in a suit and tie\nText prompt: a man in a suit and tie\nText prompt: a man with long hair\nText prompt: a man in a suit and tie\nText prompt: a woman with long brown hair\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 23: More qualitative results of OFT and COFT on the landmark to face generation task.\n32\nText prompt: a woman with long brown hair\nText prompt: a woman with red lipstick\nText prompt: a woman with blonde hair\nText prompt: a young man with brown hair\nText prompt: a young man in a suit and tie\nText prompt: a beautiful asian woman\nText prompt: a woman eating food\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nFigure 24: More qualitative results of OFT and COFT on the landmark to face generation task.\n33\nG\nMore Controllable Generation Tasks\nG.1\nDense Pose to Human Body\nText prompt: a young man wearing a grey t-shirt and black jeans\nText prompt: a man wearing a grey t-shirt\nText prompt: a man in a white shirt and green pants\nControlNet\nText prompt: a man in a white top and black jeans\nText prompt: a man in a black jacket and jeans\nText prompt: a young man wearing a black t-shirt and white jeans\nText prompt: a young man in a white t-shirt and red pants\nText prompt: a man wearing a white t-shirt and jeans\nT2I-Adapter\nLoRA\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a man wearing a white shirt and black jeans\nFigure 25: Qualitative comparison among different methods on the dense pose to human body generation task.\n34\nText prompt: a man in a black sweater and jeans\nCOFT\nOFT\nReconstruction\nControl signal\nText prompt: a man in a grey sweater and blue jeans\nText prompt: a man wearing a denim shirt and white pants\nText prompt: a man wearing a white t-shirt and jeans\nText prompt: a man wearing a denim jacket and jeans\nFigure 26: More qualitative results of COFT and OFT on the dense pose to human body task.\n35\nG.2\nSketch to Image\nText prompt: a boathouse on the water at sunset\nText prompt: a group of people standing in front of the white house\nText prompt: a boat on the beach\nControlNet\nT2I-Adapter\nLoRA\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a mountain with snow on it\nText prompt: an old car driving down a snowy road\nText prompt: a painting of a castle\nText prompt: a family with a dog\nText prompt: the golden gate bridge\nText prompt: a deer in the snow\nFigure 27: Qualitative comparison among different methods on the sketch to image generation task.\n36\nText prompt: a mountain lake with rocks and trees\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a river scene with a ship in the distance\nText prompt: a river in the mountains\nText prompt: an old town with a church in the background\nText prompt: a castle with a dragon flying over it\nText prompt: a boat with lightning in the sky\nText prompt: a man with blonde hair\nFigure 28: More qualitative comparison on the sketch to image generation task.\n37\nG.3\nDepth to Image\nText prompt: a motorcycle on display\nText prompt: a bike\nText prompt: a toilet in a bathroom\nControlNet\nT2I-Adapter\nLoRA\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a city\nText prompt: a hospital room\nText prompt: a painting\nText prompt: an airplane at the airport\nText prompt: a cat in a suitcase\nText prompt: a kitchen\nFigure 29: Qualitative comparison among different methods on the depth to image generation task.\n38\nText prompt: a mountain lake with rocks and trees\nControlNet\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a river scene with a ship in the distance\nText prompt: a river in the mountains\nText prompt: an old town with a church in the background\nText prompt: a castle with a dragon flying over it\nText prompt: a boat with lightning in the sky\nText prompt: a man with blonde hair\nFigure 30: More qualitative comparison on the depth to image generation task.\n39\nH\nHuman Evaluation\nHuman evaluation settings. We also carried out a structured human evaluation for the subject-\ndriven generation task, involving 50 participants. Here\u2019s a breakdown of our evaluation process:\n\u2022 Selection of subjects: we picked 7 subjects from the DreamBooth dataset9 at random.\n\u2022 Image and prompt: for every subject, 4 unique text prompts were chosen at random. This resulted\nin a total of 28 distinct subject-prompt combinations. For every single one of the 28 tasks, we\nrandomly sampled an image generated by each of the three methods - DreamBooth, LoRA, and\nOFT.\nEvery participant was asked to answer three single-selection questions for each task:\n\u2022 Subject fidelity: which image best preserves the identity of the subject? In other words, which\ngenerated image resembles the most the original subject?\n\u2022 Text alignment: which image matches the given text description the best?\n\u2022 Overall image quality: out of the options, which image has the best overall quality?\nThe methods were assessed at two specific points during their fine-tuning phase: at the 1000th\niteration, a checkpoint where these methods typically exhibit best performance, and at the 10,000th\niteration, a checkpoint used to measure the stability of the finetuning process over an extended period.\nResults. The results are given in Table 6, indicating the proportion of participants who chose a partic-\nular method based on the above criteria. We can see that OFT is more favored after finetuning Stable\nDiffusion with 1000 iterations and after 10000 iterations. We note that OFT delivers significantly\nbetter image quality and text-following ability than both DreamBooth and LoRA after a relatively\nlarge number of finetuning iterations.\nIteration 1000\nIteration 10000\nMetric\nDreamBooth\nLoRA\nOFT\nDreamBooth\nLoRA\nOFT\nSubject Fidelity\n42.0%\n15.4%\n42.6%\n22.4%\n1.4%\n76.2%\nText Alignment\n18.6%\n24.7%\n56.7%\n2.6%\n1.4%\n96.0%\nOverall Image Quality\n35.7%\n19.2%\n45.1%\n11.6%\n0.8%\n87.6%\nTable 6: Participant voting percentages for subject fidelity, text alignment and overall image quality.\n9https://github.com/google/dreambooth\n40\nI\nStyle Transfer by Adapting Stable Diffusion with Orthogonal Finetuning\nStable Diffusion can generate images based on the input text prompts. Without any adaptation,\ninputting text prompts to a pre-trained Stable Diffusion model will result in images that resemble\nnatural images. We can finetune the pre-trained Stable Diffusion model on a custom dataset, to\nadapt the style of the generated images to the custom dataset. To demonstrate the effectiveness\nof orthogonal finetuning, we show qualitative results of adapting Stable Diffusion on the Sketch\nScene dataset10 after finetuning for 20000 iterations in Figure 31 and on the Wikiart dataset11 after\nfinetuning for 30000 iterations in Figure 32. We train on a single NVIDIA A100-SXM4-80GB GPU\nusing a learning rate of 1 \u00d7 10\u22124, batch size of 1, and 4 as the number of gradient accumulation steps.\nBench kept on \nthe hill\ncows walking in \nthe grass\nA boy is riding bike\non the road\nA House in the field\na dog grabbing\na flying disc\na big tree and a bench\nnear the hillside\n(a) Example image-text pairs in the sketch-scene dataset\na herd of zebras\na street in the snow\nText prompt\nGenerated image \nAfter OFT finetuning\nBefore OFT finetuning\na train\n(b) Text-to-image generation after finetuning on the sketch-scene dataset\nFigure 31: Direct OFT Finetuning of Stable Diffusion on the sketch-scene dataset.\na landscape in the \nstyle of impressionism\na portrait in the \nstyle of realism\na landscape in the \nstyle of impressionism\na cityscape in the style \nof post impressionism\na landscape in the \nstyle of realism\na portrait in the style \nof impressionism\na living room\nwith a fire place\na restaurant on\na city street\nText prompt\nGenerated image \nAfter OFT finetuning\nBefore OFT finetuning\na stree at \nnight time\n(a) Example image-text pairs in the wikiart-caption dataset\n(b) Text-to-image generation after finetuning on the wikiart-caption dataset\nFigure 32: Direct OFT Finetuning of Stable Diffusion on the wikiart-caption dataset.\n10https://huggingface.co/datasets/zoheb/sketch-scene\n11https://huggingface.co/datasets/fusing/wikiart_captions\n41\nJ\nFailure Cases\nWe also show a few failure cases of OFT and COFT. Figure 33 gives three failure cases in subject-\ndriven generation. Figure 34 gives three failure cases in controllable generation.\nJ.1\nFailure Cases in Subject-driven Generation\nIn subject-driven generation, OFT and COFT will sometimes fail to ground the text attribute to the\nintended object. In the cat example, both OFT and COFT will sometimes generate other red objects,\ninstead of generating a red cat.\nText prompt: a cube shaped [V] dog\nText prompt: a [V] vase floating in an ocean of milk\nInput images\nCOFT\nOFT\nCOFT\nOFT\nText prompt: a red [V] cat\nCOFT\nOFT\nFigure 33: Some failure cases in subject-driven generation.\n42\nJ.2\nFailure Cases in Controllable Generation\nBoth OFT and COFT will sometimes hallucinate complicated structural details in a large region with\nthe same semantics. Despite still being visually plausible, these generated images cannot match the\noriginal segmentation maps.\nCOFT\nOFT\nOriginal image\nControl\nText prompt: a living room\nText prompt: a building with stairs leading up to it\nText prompt: a field with trees and mountains in the background\nFigure 34: Some failure cases in controllable generation.\n43\n"
  },
  {
    "title": "Augmenting Language Models with Long-Term Memory",
    "link": "https://arxiv.org/pdf/2306.07174.pdf",
    "upvote": "17",
    "text": "Augmenting Language Models with\nLong-Term Memory\nWeizhi Wang\u2020, Li Dong\u2021, Hao Cheng\u2021, Xiaodong Liu\u2021,\nXifeng Yan\u2020, Jianfeng Gao\u2021, Furu Wei\u2021\n\u2020University of California, Santa Barbara\n\u2021Microsoft Research\nweizhiwang@ucsb.edu, {lidong1, haocheng}@microsoft.com\nAbstract\nExisting large language models (LLMs) can only afford fix-sized inputs due to the\ninput length limit, preventing them from utilizing rich long-context information\nfrom past inputs. To address this, we propose a framework, Language Models\nAugmented with Long-Term Memory (LONGMEM), which enables LLMs to\nmemorize long history. We design a novel decoupled network architecture with\nthe original backbone LLM frozen as a memory encoder and an adaptive residual\nside-network as a memory retriever and reader. Such a decoupled memory design\ncan easily cache and update long-term past contexts for memory retrieval without\nsuffering from memory staleness. Enhanced with memory-augmented adaptation\ntraining, LONGMEM can thus memorize long past context and use long-term\nmemory for language modeling. The proposed memory retrieval module can\nhandle unlimited-length context in its memory bank to benefit various downstream\ntasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens\nand thus cache many-shot extra demonstration examples as long-form memory for\nin-context learning. Experiments show that our method outperforms strong long-\ncontext models on ChapterBreak, a challenging long-context modeling benchmark,\nand achieves remarkable improvements on memory-augmented in-context learning\nover LLMs. The results demonstrate that the proposed method is effective in\nhelping language models to memorize and utilize long-form contents. Our code is\nopen-sourced at https://aka.ms/LongMem.\n1\nIntroduction\nLarge language models (LLMs) have revolutionized natural language processing with great successes\nin advancing the state-of-the-art on various understanding and generation tasks [DCLT19, RWC+19,\nLOG+19, YDY+19, BMR+20, RSR+20]. Most LLMs benefit from self-supervised training over\nlarge corpora via harvesting knowledge from fix-sized local context, showing emergent abilities,\ne.g., zero-shot prompting [RWC+19], in-context learning [BMR+20], and Chain-of-Thought (CoT)\nreasoning [WWS+22]. Nevertheless, the input length limit of existing LLMs prevents them from\ngeneralizing to real-world scenarios where the capability of processing long-form information beyond\na fix-sized session is critical, e.g., long horizontal planning.\nTo address the length limit issue, the most straightforward method is to simply scale up the input con-\ntext length. For instance, GPT-3 [BMR+20] increases the input length from 1k of GPT-2 [RWC+19]\nto 2k tokens for capturing better long-range dependencies. However, this approach typically incurs\ncomputation-intensive training from scratch and the in-context dense attention is still heavily con-\nstrained by the quadratic computation complexity of Transformer self-attention [VSP+17]. Another\nrecent line of work [BPC20, ZGD+20] instead focuses on developing in-context sparse attention to\navoid the quadratic cost of self-attention, which still largely requires training from scratch. In contrast,\nthe prominent work, Memorizing Transformer (MemTRM) [WRHS22], approximates in-context\narXiv:2306.07174v1  [cs.CL]  12 Jun 2023\nA\nB\nC\nD\n...\nY\nZ\nLong Sequence Inputs\nLarge Language Model\n(Frozen)\nLong-Memory Retrieval\nCached Memory Bank with Key, Value Pairs\nResidual SideNet\n(Trainable)\nAttn Keys\nand Values\n(Seg A)\nAttn Keys\nand Values\n(Seg B)\nAttn Keys\nand Values\n(Seg Z)\nAttention Query\nof Current Inputs\nSearch\nMemory Fusion\nRetrieved Attn\nKeys and Values \nResidual\nConnections\nFigure 1: Overview of the memory caching and retrieval flow of LONGMEM. The long text sequence\nis split into fix-length segments, then each segment is forwarded through large language models and\nthe attention key and value vectors of m-th layer are cached into the long-term memory bank. For\nfuture inputs, via attention query-key based retrieval, the top-k attention key-value pairs of long-term\nmemory are retrieved and fused into language modeling.\nsparse attention via dense attention over both in-context tokens and memorized tokens retrieved from\na non-differentiable memory for Transformers. Thus, MemTRM scales up the resulting language\nmodel to handle up to 65k tokens and achieves substantial perplexity gains in modeling full-length\nbooks or long papers. However, MemTRM faces the memory staleness challenge during training\ndue to its coupled memory design, which uses a single model for encoding memory and fusing\nmemory for language modeling. In other words, as the model parameters are updated, cached older\nrepresentations in memory may have distributional shifts from those from the latest model, thereby\nlimiting the effectiveness of the memory augmentation.\nIn this paper, we propose a framework for Language Models Augmented with Long-Term Memory\n(LONGMEM), which enables language models to cache long-form previous context or knowledge\ninto the non-differentiable memory bank, and further take advantage of them via a decoupled memory\nmodule to address the memory staleness problem. To achieve decoupled memory, we design a\nnovel residual side-network (SideNet). Paired attention keys and values of the previous context are\nextracted using a frozen backbone LLM into the memory bank. In the memory-augmented layer\nof the SideNet, the generated attention query of the current input is used to retrieve cached (keys,\nvalues) of previous contexts from the memory, and the corresponding memory augmentations are\nthen fused into learned hidden states via a joint-attention mechanism. Furthermore, newly designed\ncross-network residual connections between the SideNet and the frozen backbone LLM enable better\nknowledge transfer from the pretrained backbone LLM. By continually training the residual SideNet\nto retrieve and fuse memory-augmented long-context, the pre-trained LLM can be adapted to leverage\nlong-contextual memory for improved modeling. The detailed memory cache, retrieval and fusion\nprocess is illustrated in Figure 1.\nOur decoupled memory design leads to two main benefits. First, our proposed architecture decouples\nthe process of encoding previous inputs into memory and the process of memory retrieval and fusion\nby decoupled frozen backbone LLM and SideNet. In this way, the backbone LLM only works as\nthe long-context knowledge encoder, while the residual SideNet works as the memory retriever and\nreader, which effectively resolves the issue of memory staleness. Second, directly adapting the entire\nLLM with memory augmentations is computationally inefficient, and also suffers from catastrophic\nforgetting. As the backbone LLM is frozen during the efficient memory-augmented adaptation stage,\nLONGMEM can not only tap into the pretrained knowledge but also avoid catastrophic forgetting.\nLONGMEM is capable of taking various types of long-form text and knowledge into the memory bank\nbased on downstream tasks. Here, we consider two representative cases, language modeling with\nfull-length book contexts, and memory-augmented in-context learning with thousands of task-relevant\ndemonstration examples. Specifically, we evaluate the effectiveness of the proposed LONGMEM\non various long-text language modeling, and memory-augmented in-context learning for language\nunderstanding. Experimental results demonstrate that our model consistently outperforms the strong\nbaselines in terms of long-text modeling and in-context learning abilities. Our method substantially\n2\nLLM Decoder Layer\nLLM Decoder Layer\nCache Attn\n Keys and Values\nLLM Decoder Layer\nSideNet Layer\nSideNet\nMemAug Layer\nEmbedding Layer\nMemory Retrieval and Fusion\nMemory\nFusion\nLanguage Model Head\nLLM Decoder Layer\nCached Memory Bank with Key, Value Pairs\nToken-to-Chunk\nRetrieval\ne\nf\ng\nh\nf\ng\nh\ni\nCurrent Inputs\nCached Memory Bank with Key, Value Pairs\nFrozen Layer\nTrainable Layer\nResidual\nFigure 2: Overview of LONGMEM architecture. \u201cMemAug\u201d represents Memory-Augmented Layer.\nimproves LLM\u2019s long-context language modeling capabilities by -1.38\u223c-1.62 perplexity over dif-\nferent length splits of Gutenberg-2022 corpus. Remarkably, our model achieves the state-of-the-art\nperformance of 40.5% identification accuracy on ChapterBreak, a challenging long-context modeling\nbenchmark, significantly surpassing existing strong x-former baselines. Lastly, with 2k demonstration\nexamples in memory, LONGMEM shows pronounced in-context learning improvements on popular\nNLU tasks, compared with MemTRM and non-memory-augmented baselines.\n2\nMethods\nTo enable LLMs to harvest relevant information from the past long context in memory, we propose to\naugment the frozen backbone LLM with a decoupled memory module. To fuse the memory context\ninformation, we design a novel lightweight residual SideNet, which can be continually trained in\nan efficient way. In the following, we first discuss the problem formulation of language modeling\nwith memory augmentations. Then, we formally introduce our efficient residual SideNet for adapting\nthe frozen pretrained LLM to jointly attend over local input context and retrieved memory context.\nLastly, we provide our designed processes of how past memory is encoded, stored, recalled and fused\nfor language modeling.\n2.1\nLanguage Models Augmented with Long-Term Memory\nHere, we focus on the high-level problem setup and defer more component details to later sections.\nGiven its wide adoption for pretrained LLMs, our LONGMEM model is built on the Transformer\narchitecture [VSP+17]. For LONGMEM, there are three key components: the frozen backbone LLM,\nSideNet, and Cache Memory Bank. As most existing pretrained LLMs can only take a fix-sized input,\nonly the input segment of a long sequence (e.g., a book) that can fit in the length limit is denoted as\nthe current input as done for most existing autoregressive language models. Those previous segments\nthat can not fit are denoted as previous inputs, which are used for memory augmentations. To tap into\nthe learned knowledge of the pretrained LLM, both previous and current inputs are encoded using the\nfrozen backbone LLM but different representations are extracted. For previous inputs, the key-value\npairs from the Transformer self-attention at m-th layer are stored in Cache Memory Bank, whereas\nthe hidden states from each LLM decoder layer for the current inputs are retained and transferred to\nSideNet. For each current input token, top relevant key-value vector pairs are retrieved as memory\naugmentations for language modeling. The SideNet module can be viewed as an efficient adaption\nmodel that is trained to fuse the current input context and relevant cached previous contexts in the\ndecoupled memory.\nFormally, for a fix-sized input text sequence {xi}|x|\ni=1 (the current input), LONGMEM first performs a\nforward pass using the backbone LLM (marked in Blue in Figure 2) without any gradient calculation.\nThe embedding layer of the backbone LLM first encodes the input {xi}|x|\ni=1 into embedding space\nand outputs the initial hidden states, H0\nLLM \u2208 R|x|\u00d7E, where E is the hidden dimension. Then each\nsuccessive Transformer decoder layer of the frozen backbone LLM computes the new hidden states\n3\nusing the hidden states from the previous layer, Hl\u2032\nLLM = f\u03b8l\u2032\nLLM(Hl\u2032\u22121\nLLM), \u2200l\u2032 \u2208 [1, L\u2032] and L\u2032 is the\ntotal # layers for the backbone LLM. During the forward pass with the backbone LLM for all previous\ninputs, the key-value pairs used for self-attention at the m-th Transformer decoder layer are stored in\nCached Memory Bank (marked in Orange in Upper-Left corner of Figure2), which are later recalled\nas memory augmentations for future inputs.\nCached Memory Bank is a cached head-wise vector queue Zk, Zv \u2208 RH\u00d7M\u00d7d, which maintains\nattention key-value pairs of latest M previous inputs eK, eV \u2208 RH\u00d7|x|\u00d7d, where H, d denotes the\nnumber of attention heads and per-head dimension respectively. After memory retrieval and fusion\n(\u00a72.3), the memory bank removes the key-value pairs of the oldest sequences and appends the current\nsequences to the cached vector bank. Thus such an update mechanism ensures the language modeling\ncausality at the sequences level and enables the memory bank to always keep records of the nearest\nprevious context for the current inputs.\nAfter the forward pass with the backbone LLM, the SideNet module then takes all current input hidden\nstates from the backbone LLM {Hl\u2032\nLLM}L\u2032\nl\u2032=1 and the past key-value pairs in Cached Memory Bank for\ncomputing memory-augmented representations. Specifically, our SideNet of LONGMEM consists of\n(L \u2212 1) normal Transformer decoder layers and one special memory-augmented decoder layer. For\nefficient purposes, we mainly consider the case where #layers L of the SideNet is smaller than that\nof the backbone LLM, i.e., L < L\u2032. Our SideNet encodes H0 into memory-augmented contextual\nrepresentation via (L \u2212 1) normal Transformer decoder layers and a special memory-augmented\nlayer.\nThe memory-augmented layer is an extension of the vanilla Transformer decoder layer that takes a\nmemory-augmented input, including both top relevant key-value pairs in memory and the hidden states\nfrom the current input. Here, the cached key-value pairs are recalled using a token-based memory\nretrieval module (\u00a72.3). For each current input token, the memory retrieval module srt(:) retrieves\ntop-K relevant key-value pairs in the memory bank {ekij, evij}K\nj=1 = srt(xi). Then SideNet computes\nthe output using the memory-augmented input, Hms\nSide = f\u03b8Mem(Hms\u22121\nSide\n, {{ekij, evij}K\nj=1}|x|\ni=1), where\nms is the layer index where we inject the memory-augmentation layer.\nFinally, the token probability is computed using the last SideNet hidden states P(xi|x1, \u00b7 \u00b7 \u00b7 , xi\u22121) =\nsoftmax(WHL), where W is the frozen output embedding weight shared by both the backbone\nLLM and SideNet. We perform a memory-augmented adaptation training for LONGMEM to utilize\nthe decoupled memory. Following the generative unsupervised pre-training [RNSS18], the training\nobjective of LONGMEM is the standard left-to-right language modeling objective, which maximizes\nthe likelihood of the next token based on the left context: max P\nx\u2208D\nP|x|\ni=1 log P(xi|x1, \u00b7 \u00b7 \u00b7 , xi\u22121),\nwhere x is a randomly sampled sentence from the pre-training text corpus D.\n2.2\nResidual SideNet\nSideNet Architecture and Initialization. Here, we again implement SideNet based on Trans-\nformer [VSP+17]. Here, the number of decoder layers L in SideNet is equal to the number of layers\nL\u2032 in the backbone LLM divided by a reduction factor (a layer reduction factor of 2 throughout this\nwork L\u2032 = 2L). The weights of each decoder layer in SideNet are initialized from the corresponding\npre-trained decoder layer of the backbone LLM with the same depth: \u0398\nl\u2032\n2\nSide = \u0398l\u2032\nLLM. As illustrated in\nFigure 2, the SideNet takes the output of backbone LLM\u2019s embedding layer and reuses the language\nmodeling head layer of backbone LLM, which is also frozen during the continual adaption stage.\nDuring the memory-augmented adaptation stage, all other parameters of SideNet are updated accord-\ningly based on the training signal. In this way, the lightweight SideNet achieves fast convergence\nwith knowledge transferred from pre-trained parameters.\nCross-Network Residual Connections. To tap into knowledge from the pretrained backbone\nLLM, we resort to proposed cross-network residual connections for fusing representations from the\nbackbone LLM into SideNet. Specifically, we add the difference between output hidden states at\n2l-th and (2l \u2212 2)-th layers of the backbone LLM as the residual connections to the output hidden\nstates at l-th layer of SideNet. Then, the input to the next (l + 1)-th layer of SideNet is the sum of\nthe original hidden state forwarded through the previous layer f\u0398l\nSide(Hl\u22121\nSide) and the cross-network\n4\nresidual connection of the hidden state difference from the backbone LLM\nHl\nSide = f\u0398l\nSide(Hl\u22121\nSide) + (H2l\nLLM \u2212 H2l\u22122\nLLM ), \u2200l \u2208 [1, L],\n(1)\nwhere H0 is the output of embedding layer. It is worth noting that the residual connections after the\nself-attention and feed-forward network of a decoder layer [VSP+17] will be performed as normal in\nf\u0398l\nSide(Hl\u22121\nSide) and parallel to the proposed cross-network residual connections.\n2.3\nMemory Retrieval and Fusion\nThe long-term memory capability of LONGMEM is achieved via a memory-augmentation module for\nretrieval and fusion.\nToken-to-Chunk Memory Retrieval. Instead of performing token-to-token retrieval, we focus on\ntoken-to-chunk retrieval for acceleration and integrity. A text-chunk refers to an n-gram structure of\nchunk-size csz number of contiguous tokens. The memory bank stores cached key-value pairs at the\nlevel of token chunks. We divide the memory bank into M/csz attention key-value paired chunks\nand use the mean-pooled vector on the chunk-size dimension to get the key vector for retrieval. Then\nwe retrieve the top-(K/csz) attention key-value chunks w.r.t the dot product between the attention\nquery of the current input token and the mean-pooled attention key of a candidate chunk. Finally,\nwe squeeze the chunk-size dimension for retrieved key-value paired chunks and flatten them into\nK key-value pairs at token-level { eKj, eVj}K\nj=1. Adopting token-to-chunk retrieval reduces the size\nof the retrieval index and accelerates the process. Meanwhile, the retrieval accuracy can be further\nimproved, which is also observed in [LGW+23] and [BMH+21]. The hyperparameter chunk-size csz\ncontrols the granularity of retrieved contexts, which can be empirically adjusted based on downstream\ntasks. For instance, in-context learning requires more fine-grained label tokens from demonstration\nexamples cached in memory, where a smaller csz is helpful.\nMemory Fusion. The memory fusion is performed within a special memory-augmented layer.\nAs the conventional Transformer decoder layer uses the multi-head self-attention [VSP+17], we\nfollow [WRHS22] to extend it to a joint-attention mechanism and propose a long-term memory\nfusion process to enable each token to attend on both local contexts and retrieved memory contexts.\nWith the head-wise hidden state output from previous layer Hl\u22121 \u2208 R|x|\u00d7d and the corresponding\nretrieved attention key-value pairs are { eKi, eVi}|x|\ni=1 \u2208 R|x|\u00d7K\u00d7d, the output hidden state for the l-th\nmemory-augmented layer Hl is computed as:\nA = softmax(QKT\n\u221a\nd\n)V, M = Concat{softmax(Qi eKT\ni\n\u221a\nd\n) eVi}|x|\ni=1,\n(2)\nHl = sigmoid(g) \u00b7 A + (1 \u2212 sigmoid(g)) \u00b7 M,\n(3)\nwhere Q, K, V, A, M \u2208 R|x|\u00d7d, K is the number of retrieved attention key-value pairs in cached\nmemory for each token, and g is a trainable head-wise gating vector. The hidden state output from\nprevious layer H(l\u22121) is linearly projected into attention queries, keys, and values Q, K, V separately\nvia three matrices W Q, W K, W V \u2208 Rd\u00d7d. It is worth noting that the retrieved attention key-value\npairs in cached memory are distinct to each token.\n3\nExperiments\nWe evaluate our proposed LONGMEM model on different tasks based on the demanded in-memory\nlong-contexts: a) long-text language modeling and language understanding when loading the past\nlong-context into cached memory; b) infinite-length in-context learning when loading large number\nof demonstration examples into cached memory.\n3.1\nTraining Setup\nBatchfying the training corpora. The conventional batchyfing process for large corpora truncates\nthe whole corpora into consecutive fix-length text segments without padding and shuffles all segments\nto construct mini-batches [RWC+19]. In contrast, LONGMEM must disable global shuffling and\nensure the global causality at segment level. Firstly, we divide all long documents in training corpora\n5\ninto batch-size number of document groups with equivalent length and then perform a document-level\nshuffling within each group. Then, we concatenate shuffled documents within one group and truncate\nthem into ordered segments. In order to ensure that two consecutive segments of one long document\nare distributed in two consecutive input batches after batchfying, we select one segment from batch-\nsize number of document groups with the same inner-group index. Thus a mini-batch with batch-size\nnumber of segments are constructed from exactly batch-size number of document groups. In this\nway, as the training iteration steps, the cached attention key-value pairs in memory bank are exactly\nprevious context of current inputs within the same document. The batchfying process is illustrated in\nFigure 3.\nDocument 1\nDocument 2\nDocument 3\nDocument 4\nDocument Z \nS11\nS12\nS13\nS14\nS15\nS16\nS1N\nS41\nS42\nS43\nS44\nS45\nS46\nS4N\nS21\nS22\nS23\nS24\nS25\nS26\nS2N\nDocument Grouping\nTruncated Segments\nLong Documents\nS11\nS21\nS31\nS41\nS12\nS22\nS32\nS42\nS13\nS23\nS33\nS43\nS1N\nS2N\nS3N\nS4N\nS31\nS32\nS33\nS34\nS35\nS36\nS3N\nDocument 5\nCached Memory Update during 5-th Iteration\nBatch 1 Batch 2 Batch 3\nBatch N\nRemove Oldest\nAppend Lastest\nS11\nS21\nS31\nS41\nS12\nS22\nS32\nS42\nS13\nS23\nS33\nS43\nS14\nS24\nS34\nS44\nS12\nS22\nS32\nS42\nS13\nS23\nS33\nS43\nS14\nS24\nS34\nS44\nS15\nS25\nS35\nS45\nBatchfying \nFigure 3: Batchfying the large text corpora into batches to ensure that each consecutive segments\nwithin each document is distributed in consecutive batches.\nTraining Corpus and Hyperparameters. We sample a subset of the Pile [GBB+20] as the training\ncorpus, including BookCorpus2, Books3, OpenWebText2, Stack Exchange, Wikipedia, Gutenberg\n(PG-19), NIH ExPorter, and Pile-CC datasets. We reproduce GPT-2 (407M-params) as the pre-trained\nbackbone LLM with Alibi [PSL21] position embedding because original GPT-2 [RWC+19] adopts\nabsolute position embedding, which is found to perform poorly to enable LLM to learn long-distance\ndependencies [DYY+19]. The backbone LLM holds a L\u2032 = 24, H = 16, d = 64 architecture.\nThe SideNet holds a L = 12, H = 16, d = 64 architecture. The training for memory-augmented\nadaptation iterates on 26B tokens, with a global 256 batch-size and 1024 sequence length. The\nchunk-size csz is 4 tokens and the memory size M is 65k key-value pairs of tokens. For each token,\nwe retrieve K=64 attention key-value pairs for augmentation, which are K/csz=16 text chunks. The\nmemory-augmentation layer is the 9-th layer of SideNet. The attention keys and values from 18-th\nlayer of backbone LLM is cached into memory and used for future retrieval. Other training details\nare presented in Appendix C.\nMemory Retrieval Module. The fixed memory-size of cached memory bank in one GPU is 65536\nkey-value pairs of tokens. We enable each GPU to construct and update their own memory retrieval\nmodule for efficiency. For the implementation of the efficient token-to-chunk retrieval, we use the\nfaiss [JDJ21] toolkit to construct an exact-search index on GPU to store the mean-pooled attention\nkeys of text chunks and perform efficient retrieval. The faiss index maintains a fixed M/csz keys\nand provides the efficient exact search w.r.t. inner product. The retrieval takes about 15ms per 1k\ntokens, which is 55% timecost of backbone LLM forwarding pass. We can easily adapt the exact\nsearch index to approximate search index to gain more the retrieval efficiency.\nBaselines. In addition to the baseline of pre-trained GPT-2*, we reproduce Memorizing Transformer\n(MemTRM) [WRHS22] as another memory-augmented adaptation baseline. The MemTRM can be\neasily adapted to tune a pre-trained LLM to use external memory. We insert the knn-augmented layer\nproposed by MemTRM as the same 18-th layer in the LLM decoder. The MemTRM baseline is also\ntrained for the same number of tokens under the same hyperparameter setting.\n3.2\nLong-Context Language Modeling\nThe long-context language modeling can easily benefit from the augmented decoupled memory of\npast long-contexts, in which the knowledge stored in retrieved attention key-values can play a useful\n6\nDataset\nPG-22\nArXiv\nSplits\nS1\nS2\nS3\nS4\nS5\nLen. Range\n5K-10K\n10K-100K\n100K-500K\n500K-1M\n>1M\n<60K\n#Documents\n500\n100\n30\n8\n1\n100\nAvg. #tokens\n7.6K\n47.6K\n140K\n640K\n1.2M\n15.4K\nTable 1: Dataset Statistics of five splits of PG-22 based on length range and ArXiv.\nModel\nIn-Context\nIn-Memory\nPG-22\nArXiv\nLen.\nLen.\n5K-10K\n10K-100K\n100K-500K\n500K-1M\n>1M\nGPT-2*\n1k\nN/A\n22.78\n24.39\n24.12\n24.97\n18.07\n11.05\nMemTRM\n1k\n65K\n21.77\n23.56\n23.23\n24.16\n17.39\n10.81\nLONGMEM\n1k\n65k\n21.29\n23.01\n22.55\n23.35\n16.71\n10.05\nTable 2: Evaluation results on long-context language modeling datasets. We report token-level\nperplexity (PPL) (lower the better) on all datasets.\nrole in providing significant background and contextual information to help models perform better on\nlong-context language modeling. For instance, when trying to model a long-text book accurately,\nacquiring knowledge from previous background and character relationships can be helpful to model\nthe consequent stories.\nEvaluation Setting. We first compare LONGMEM and baselines on 3 long-context modeling datasets,\nProject Gutenberg 2020-2022, ArXiv, and ChapterBreak. The majority of included books or papers\nin these datasets have the length of at least 16k tokens. All listed datasets are evaluated in zero-shot\nmanner without any task-specific tuning. The detailed evaluation settings on 3 datasets are as follows:\n\u2022 Project Gutenberg 2020-2022 Language Modeling Dataset. We crawled and cleaned the books\npublished between 2020 and 2022 under Project Gutenberg Library1 to build up a completely new\nlong-text modeling dataset, named PG-22. It is highly differentiated from our training subset\nPG-19 in domains and writing styles, because books in PG-19 [RPJL19] are published before\n1919. We provide different validation splits of PG-22 based on length range, and data statistics are\npresented in Table 1.\n\u2022 ArXiv Dataset. ArXiv dataset involves papers in the areas of Math, Computer Science, and\nPhysics. We select a validation split of ArXiv paper subset in the Pile corpus [GBB+20]. ArXiv\nsubset of the Pile is excluded from our training and is an out-of-distribution dataset. We report the\ntoken-level language modeling perplexity on the long-context language modeling benchmarks of\nPG-22 and ArXiv.\n\u2022 ChapterBreak Benchmark. ChapterBreak is proposed in [STI22] as a challenging suffix\nidentification dataset that requires LLMs to distinguish the beginning of the ground-truth next\nchapter from a set of hard negative segments sampled from the same book, given the long context\nof previous chapters. ChapterBreak requires processing global long-context to comprehend and\nidentify the correct suffix. [STI22] demonstrated that even state-of-the-art x-formers for long-\ntext processing fail to effectively leverage long-range context to perform well on ChapterBreak.\nWe select the Archive of Our Own (AO3) subset of ChapterBreak which contains fan-fictions\nextracted from AO3. ChapterBreak provides 8 splits based on the prefix length from 0.5k to 8k\ntokens to fit the length limit of different models. The splits of 4k, 6k, and 8k prefix are selected for\nevaluation. For LLMs that cannot process over 4k tokens, we abandon the front prefix to fulfill the\nmaximum input length of LLMs. For MemTRM and LONGMEM model, we firstly load the given\n4k/6k/8k prefix contexts into the cached memory and then do the scoring. we use the perplexity\nas the scorer for each candidate suffix segment in zero-shot evaluation manner. Then the suffix\nsegment with lower perplexity is selected as the label. The suffix identification accuracy is used as\nthe evaluation metric.\nResults. The main results on evaluated long-context datasets are summarized in Table 2. The proposed\nLONGMEM model significantly outperform all considered baselines on long-text language modeling\n1https://www.gutenberg.org/\n7\nModel\n#Params\nIn-Context\nIn-Memory\nChapterBreakao3\nLen.\nLen.\nctx-4k\nctx-6k\nctx-8k\nGPT-2-XL\u2020 [RWC+19]\n1.5B\n1K\nN/A\n24%\n24%\n24%\nGPT-3\u2020 [BMR+20]\n175B\n2K\nN/A\n28%\n28%\n28%\nLocalTRM\u2020 [RSVG21]\n516M\n8K\nN/A\n24%\n24%\n24%\nRoutTRM\u2020 [RSVG21]\n490M\n8K\nN/A\n25%\n24%\n24%\nBigbird\u2020 [ZGD+20]\n128M\n4K\nN/A\n26%\n26%\n26%\nGPT-2*\n407M\n1K\nN/A\n18.4%\n18.4%\n18.4%\nMemTRM\n407M\n1K\n\u221e\n28.3%\n28.7%\n28.7%\nLONGMEM\n558M\n1K\n\u221e\n37.7%\n39.4%\n40.5%\nTable 3: Zero-shot Suffix Identification Accuracy on AO3 subset of ChapterBreak. Baselines\nmarked with \u2020 are directly cited from [STI22]. The MemTRM and LONGMEM loads the given\n4k/6k/8k prefix contexts into cached memory, while the input length to local context is still 1k tokens.\ndatasets, with improvements of -1.38 to -1.62 perplexity on different length splits of PG-22, and -1.0\nppl on ARXIV datasets. Surprisingly, the proposed method achieves the state-of-the-art performance\nof 40.5% accuracy on ChapterBreakAO3 suffix identification benchmark and outperforms both the\nstrong long-context transformers and latest LLM GPT-3 with 313x larger parameters. The substantial\nimprovements on these datasets demonstrate that LONGMEM can comprehend past long-context in\ncached memory to well complete the language modeling towards future inputs.\n3.3\nMemory-Augmented In-Context Learning\nLLMs have the emerging capability of in-context learning (ICL) via learning knowledge non-\nparametrically from few-shot demonstration examples in the local context. However, conventional\nin-context learning is heavily restricted by input context length, rendering it ineffective to absorb\nsupervision from sufficient demonstration examples in the training set. With the proposed unlimited-\nlength memory augmentation, our LONGMEM method can overcome the limitation of the number\nof demonstration examples in the local context and even attend on the whole training set by load-\ning it into the cached memory. In this way, LONGMEM goes beyond the conventional few-shot\nin-context learning and realized memory-augmented in-context learning with thousands of auxiliary\ndemonstration examples.\nEvaluation Setting. Here, we evaluate the in-context learning capability of baselines and the proposed\nLONGMEM model on five Natural Language Understanding (NLU) datasets, SST-2 [SPW+13],\nMPQA [WWC05], MR [ABK+07], Subj [PL04] and SST-5 [SPW+13]. We evaluate models on two\nfew-shot settings, 4-shot and 20-shot. The 4-shot demonstrations are data-insufficient scenario, while\nthe 20-shot demonstrations can almost fulfill the 1k input length and provide sufficient contextual\nself-supervisions. We transform the k-shot examples to semantically meaningful demonstration\nexamples via fixed text template, i.e., di=\"Review: xi Sentiment: yi\",\u2200{(xi, yi)}k\ni=1 \u2208 Dtrain for\nsentiment analysis tasks. Additionally, we evaluate the 3-shot ICL on question-answering tasks of\nSQuAD [RZLL16] under an open-ended generation setting. The details of all prompt templates are\npresented in Appendix D. Then we concatenate the demonstration examples with newlines to delimit\nthem. The prediction label is directly generated using greedy decoding given the demonstration\nexamples and test cases in context. The prediction accuracy is used as the evaluation metric. We\nreport the mean and standard deviation of 6 runs with different random seeds to overcome the\nrandomness in selecting k-shot demonstration examples. As illustrated before, the chunk size controls\nthe granularity of retrieved text chunks. As the select NLU datasets require to retrieve fine-grained\nlabels from cached memory, we perform an hypperparameter selection on the validation set of SST-2,\nand the best chunk-size 2 is used to report the results for MemTRM and our model.\nResults. The results on in-context learning are summarized in Table 5 and Table 4. LONGMEM\nachieves remarkable improvements on all NLU tasks in 20-shot sufficient in-context set-\nting, with +8.0 average scores increase over pretrained GPT-2* and MemTRM. Meanwhile,\nLONGMEM also brings performance improvements on the scenario of 4-shot demonstra-\ntions in local context.\nAdditionally, LONGMEM improves the in-context learning capabil-\nities of LLMs on open-ended generation tasks, with +4.5 EM score increase on SQuAD.\n8\nModel\nIn-Context\nIn-Memory\nSST-2\nMR\nSubj\nSST-5\nMPQA\nAvg.\n#Demons.\n#Demons.\nACC\u2191\nACC\u2191\nACC\u2191\nACC\u2191\nACC\u2191\nMajority\nN/A\nN/A\n50.9\n50.0\n50.0\n20.0\n50.0\n44.2\nGPT-2*\n4\nN/A\n68.311.6\n64.712.5\n51.94.2\n31.44.4\n61.511.8\n55.6\nMemTRM\n4\n2000\n67.512.4\n64.611.3\n53.26.0\n29.64.4\n63.012.1\n55.6\nLONGMEM\n4\n2000\n71.814.0\n65.111.0\n53.83.7\n36.06.8\n65.412.8\n58.4\nGPT-2*\n20\nN/A\n68.211.5\n63.45.2\n57.610.2\n33.66.0\n70.87.6\n58.7\nMemTRM\n20\n2000\n65.19.6\n65.19.3\n58.210.6\n31.96.3\n72.77.4\n58.6\nLONGMEM\n20\n2000\n78.014.1\n78.63.3\n65.68.5\n36.57.5\n74.67.3\n66.7\nTable 5: Accuracy [%] of 4-shot and 20-shot ICL on 5 NLU tasks (SST-2, mr, subj, SST-5, mpqa).\nWe sample 2000 extra demonstration examples and load them into cached memory. The subscript is\nthe standard deviation across 6 runs. Avg. refers to the average accuracy on 5 datasets.\nModel\nEM\nF1\nGPT-2*\n22.282.3\n30.782.0\nMemTRM\n22.843.5\n32.652.8\nLONGMEM\n26.772.3\n35.702.0\nTable 4:\nExact match (EM) and\nF1 scores of 3-shot (about 1k to-\nkens) in-context learning on SQuAD.\nLONGMEM loads 200 extra demonstra-\ntion examples into cached memory.\nThe results indicate that the demonstration examples loaded\nin cached memory can be regarded as auxiliary contextual\ndemonstrations to attend to and be helpful for in-context\nlearning. LONGMEM model can harvest both the task-\nrelevant knowledge in both local contextual demonstrations\nand in-memory augmented demonstrations for better in-\ncontext learning.\n3.4\nAblation Studies\nSo far, we empirically verify the effectiveness and superi-\nority of LONGMEM in utilizing cached memory for long-\ncontext modeling, long-context understanding, and many-\nshot in-context learning. As the design of cached memory\nbank involves many hyperparameters like memory size msz and chunk-size csz, we perform a series\nof ablation studies to evaluate the effects of these hyperparameters on task performance.\nEffects of Chunk-Size. As analyzed before, the chunk-size csz controls the granularity of retrieval\nand thus it may make a difference to tasks with requirements of fine-grained retrieval like in-context\nlearning. We perform an ablation study on the effects of various chunk-size csz \u2208 {2, 4, 8} on\nin-context learning and the results are presented in Figure 4(a). The chunk size of 2 yields the best\nperformance on in-context learning tasks on five NLU datasets, which is consistent with the property\nof NLU tasks with the requirement of fine-grained retrieval and fusion towards classification label\ntokens.\nEffects of Memory Size. The memory size (msz) controls the capacity of the memory bank. In\ngeneral, the memory size should be compatible with the average length of documents or contexts,\ni.e., , a set of books with average 16k tokens should deploy the memory size of 16k tokens in cached\nmemory. The training msz of 65 tokens is excessive for downstream tasks such as ChapterBreak\nas the whole prefix context length does not exceed 65k tokens. Thus, we perform an ablation study\non the effects of memory size msz \u2208 {8k, 16k, 32k, 65k} during the inference stage on the PG-22\nlanguage modeling datasets and the results are shown in Figure 4(b). To model the books with\naverage 8k-50k length, the smaller memory size 16k which is consistent with the average length of\ntarget books yields the best perplexity.\n4\nRelated Work\nLarge Language Models. Large Language Models, i.e., GPT-2 [RWC+19], GPT-3 [BMR+20],\nOPT [ZRG+22], and BLOOM [SFA+22], significantly revolutionized NLP research and promoted\nthe state-of-the-art of various language understanding, language generation [WZG+22], and even\nvision-language tasks [WDC+22]. Additionally, via scaling the model parameters, LLMs exhibit\n\u201cemergent abilities\u201c [WTB+22] like few-shot in-context learning [BMR+20], multi-step reason-\ning [WWS+22], code completion, etc.\n9\n72\n72.5\n59.8\n34.4\n71.8\n78\n78.6\n65.6\n36.5\n74.6\n77.8\n75.6\n61.1\n34.8\n72.7\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nSST-2\nMR\nSubj\nsst-5\nMPQA\nAccuracy\ncsz=1\ncsz=2\ncsz=4\n(a)\n5-10K\n10-100K\n0.1-0.5M\n0.5-1M\nSplits of PG-22\n0.1\n0.0\n0.1\nPPL\nmsz=65k\nmsz=32k\nmsz=16k\nmsz=8k\n(b)\nFigure 4: (a) Accuracy on 5 NLU datasets given different chunk size during inference; (b) \u2206Perplexity\non 4 splits of PG-22 given different memory size during inference, in which the perplexity when\nmsz=65k is used as baseline.\nx-formers. To enable transformers to attend on longer context, many variants of \u201cx-formers\u201c are pro-\nposed. Transformer-XL [DYY+19] proposes to cache attention keys and values of past segment and\nreuse them in recurrent manner. Recent seminal works of x-formers, including LinFormer [WLK+20],\nLongFormer [BPC20], Routing Transformer [RSVG21], proposed various sparse attention mecha-\nnisms for decreasing O(n2) complexity to O(n log n) or even O(n). BigBird [ZGD+20] achieves a\n4k sequence length via attending on a subset of context tokens. Although these x-formers achieve\nsubstantial efficiency improvements, such efficiency gains are not remarkable when modeling se-\nquences that spans book-level length. Moreover, the largest sequence length of these methods is\nstill upper-bounded by 16k tokens, making them invalid in modeling long-sequences at the book or\nwikipedia-page level (i.e., average 70k tokens for full-length books in PG19 dataset [RPJL19]).\nSide-Tuning. The method of Side-Tuning [ZSZ+20, SCB22] is a task-specific tuning method for\npre-trained models via training a lightweight side-network that is fused with the fixed pre-trained\nnetwork via summation. Our method inherits the idea of adopting a side-network but distinguishes\nthe side-tuning method in terms of learning objective and cross-network fusion ways. LONGMEM\nproposes to augment LLMs with decoupled memory for memorizing long past inputs, which does not\ninvolve any task-specific tuning. The cross-network residual connections proposed by LONGMEM is\nnovel and distincts from the vanilla summation of Side-Tuning.\n5\nConclusion\nIn this paper, we propose to augment LLMs with long-term memory for enabling them to memorize\nlong-form context and gain long-form memory. The designed decoupled memory module can cache\nattention key and value pairs of past inputs for future retrieval and fusion. A decoupled residual\nSideNet is introduced as the memory retriever and reader, meanwhile the LLM itself is frozen\nand works as knowledge and memory encoder. Experiments on various long-contextual language\nmodeling datasets demonstrate the effectiveness of our model over other memory-augmentation\nbaselines. The proposed method can also enable in-context learning of LLMs to overcome the limited\nnumber of demonstration examples in context, which is constrained by the contextual length, via\ncaching thousands of auxiliary demonstration examples in memory.\nReferences\n[ABK+07] S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary\nIves. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722\u2013735. Springer,\n2007.\n[BMH+21] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving\nlanguage models by retrieving from trillions of tokens. ArXiv, abs/2112.04426, 2021.\n[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n10\nHerbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\n[BPC20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\n[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[JDJ21] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7:535\u2013547, 2021.\n[KB15] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nCoRR,\nabs/1412.6980, 2015.\n[LGW+23] Rui Lv, Junliang Guo, Rui Wang, Xu Tan, Qi Liu, and Tao Qin. N-gram nearest neighbor machine\ntranslation. arXiv preprint arXiv:2301.12866, 2023.\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining\napproach. ArXiv, abs/1907.11692, 2019.\n[PL04] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. arXiv preprint cs/0409058, 2004.\n[PSL21] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding with unsupervised learning. 2018.\n[RPJL19] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n[RSR+20] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. ArXiv, abs/1910.10683, 2020.\n[RSVG21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based\nsparse attention with routing transformers. Transactions of the Association for Computational\nLinguistics, 9:53\u201368, 2021.\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250, 2016.\n[SCB22] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory\nefficient transfer learning. arXiv preprint arXiv:2206.06522, 2022.\n[SFA+22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\nprocessing, pages 1631\u20131642, 2013.\n11\n[STI22] Simeng Sun, Katherine Thai, and Mohit Iyyer. Chapterbreak: A challenge dataset for long-range\nlanguage models. arXiv preprint arXiv:2204.10878, 2022.\n[VSP+17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n[WDC+22] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and\nFuru Wei. Visually-augmented language modeling. arXiv preprint arXiv:2205.10178, 2022.\n[WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[WRHS22] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins, and Christian Szegedy. Memorizing trans-\nformers. ArXiv, abs/2203.08913, 2022.\n[WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n[WWC05] Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions\nin language. Language resources and evaluation, 39(2):165\u2013210, 2005.\n[WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[WZG+22] Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, and Weihua Luo. Task-\noriented dialogue system as natural language generation. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Development in Information Retrieval, pages 2698\u20132703,\n2022.\n[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.\n[ZGD+20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020.\n[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\n[ZSZ+20] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning:\na baseline for network adaptation via additive side networks. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages\n698\u2013714. Springer, 2020.\n12\nA\nInference Efficiency and GPU-Memory Efficiency\nWhen the model is required to comprehend long sequences, the proposed method LONGMEM can\nload the out-of-boundary inputs into the cached memory as previous context. Thus, the memory\nusage and inference speed can be significantly improved compared with vanilla self-attention-based\nmodels. The detailed statistics in terms of the efficiency is presented in Table 6.\nModel\nIn-Context\nIn-Memory\nInference Speed\nGPU-Memory Usage\nLen.\nLen.\n(tokens/s)\u2191\n(MBs)\u2193\nGPT-2*\n4k\nN/A\n14666\n20671\nLONGMEM\n1k\n3k\n22638\n13335\nGPT-2*\n8k\nN/A\n8417\n54195\nLONGMEM\n1k\n7k\n21343\n13437\nTable 6: The superiority of our method over fully dense self-attention (GPT-2*) in terms of inference\nspeed and GPU-memory utilization.\nB\nTraining Details\nThe pre-training of reproduced GPT-2* iterates on 117B tokens in total, with 512 batch-size and\n1024-token fixed segment-length. The Adam optimizer [KB15] is adopted in memory-augmented\nadaptation training. The pre-training and adaptation are trained on 16 32GB-Tesla-V100 GPUs. Other\ndetailed training hypperparamters and settings are presented in Table 7.\nHyperparameter\nLONGMEM\nReproduced GPT-2* Backbone LLM Hyperparameters\nParameters\n407M\nPrecision\nfloat16\nLayers\n24\nHidden dim.\n1024\nAttention heads\n16\nHead Dim\n64\nVocab size\n52k\nSequence length\n1024\nPosition emb.\nAlibi\nTied embedding\nFalse\nSideNet Hyperparameters\nParameters\n151M\nPrecision\nfloat16\nLayers\n12\nHidden dim.\n1024\nAttention heads\n16\nHead Dim\n64\nSequence length\n1024\nMemory-Augmented Adaptation Hyperparameters\nGlobal Batch Size\n256\nLearning rate\n2.0e-4\nTotal tokens\n26B\nWarmup tokens\n0\nLR Decay style\npolynomial\nAdam (\u03b21, \u03b22)\n(0.9, 0.98)\nAdam eps\n1e-06\nWeight decay\n0.01\nGradient clipping\n2.0\nTable 7: Memory-Augmented Adaptation and Architectural Hyperparameters.\n13\nC\nPrompting Templates\nWe present all hand-crafted in-context learning prompting templates and labels for 5 NLU datasets\nand Squad QA dataset in Tabel 8.\nTask\nPrompt\nLabels\nSST-2\nReview: [Sentence] Sentiment: [Label]\n{positive, negative}\nMR\nReview: [Sentence] Sentiment: [Label]\n{positive, negative}\nMPQA\nReview: [Sentence] Sentiment: [Label]\n{positive, negative}\nSST-5\ninput: [Sentence] type: [Label]\n{terrible,bad,okay,good,great}\nSubj\ninput: [Sentence] type: [Label]\n{objective, subjective}\nSquad\nPassage: [Passage]\\n Question: [Question] Answer: [Answer]\nTable 8: The hand-crafted prompts used to query the model predictions on the zero-shot evaluation of\n5 NLU datasets and one question-answering dataset Squad.\n14\n"
  },
  {
    "title": "Face0: Instantaneously Conditioning a Text-to-Image Model on a Face",
    "link": "https://arxiv.org/pdf/2306.06638.pdf",
    "upvote": "16",
    "text": "Face0: Instantaneously Conditioning a Text-to-Image Model on a Face\nDANI VALEVSKI\u2217, Google Research\nDANNY WASSERMAN\u2217, Google Research\nYOSSI MATIAS, Google Research\nYANIV LEVIATHAN\u2217, Google Research\nFig. 1. Example generations from Face0. It only takes a couple of seconds to generate an image given a single face image (left) and a textual prompt (top).\nWe present Face0, a novel way to instantaneously condition a text-to-image\ngeneration model on a face, in sample time, without any optimization proce-\ndures such as fine-tuning or inversions. We augment a dataset of annotated\nimages with embeddings of the included faces and train an image generation\nmodel, on the augmented dataset. Once trained, our system is practically\nidentical at inference time to the underlying base model, and is therefore\nable to generate images, given a user-supplied face image and a prompt,\nin just a couple of seconds. Our method achieves pleasing results, is re-\nmarkably simple, extremely fast, and equips the underlying model with new\ncapabilities, like controlling the generated images both via text or via direct\nmanipulation of the input face embeddings. In addition, when using a fixed\nrandom vector instead of a face embedding from a user supplied image, our\nmethod essentially solves the problem of consistent character generation\nacross images. Finally, while requiring further research, we hope that our\nmethod, which decouples the model\u2019s textual biases from its biases on faces,\nmight be a step towards some mitigation of biases in future text-to-image\nmodels.\n1\nINTRODUCTION\nThe field of text-to-image synthesis has recently experienced rapid\ndevelopment, largely thanks to advances in diffusion models. By\nconditioning on free-form text, images of unprecedented quality\nand diversity can now be generated. However, generating an image\ndepicting a person from a user-supplied image is still a challenging\ntask. To overcome this gap, existing methods rely on solving an\noptimization problem during inference time, e.g. fine-tuning the\nmodel [Ruiz et al. 2022] or reversing the image into the textual\nembedding space [Gal et al. 2022]. While these methods produce\ngood results they are costly in time or memory.\nIn this work we develop a novel method for instantaneously\nconditioning an image generation model, we use Stable Diffusion,\non a face. At inference time our method is practically identical\nto standard inference from the base diffusion model, and enables\ninstantly generating images in the likeness of a person from a single\n\u2217Equal contribution.\nAuthors\u2019 addresses: Dani Valevski, Google Research, daniv@google.com; Danny Wasser-\nman, Google Research, dwasserman@google.com; Yossi Matias, Google Research,\nyossi@google.com; Yaniv Leviathan, Google Research, leviathan@google.com.\narXiv:2306.06638v1  [cs.CV]  11 Jun 2023\n2\n\u2022\nDani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan\nProject\nEmbed\nTraining\nDetect \n&\ncrop\nAdd \nnoise\n\u201cA tennis player\u201d\nEmbed \nwith \nCLIP\nDiffusion \nModel\nMSE Loss\nFig. 2. The training scheme for Face0 (see Sec. 2). Everything except the dashed red arrows is part of the standard diffusion model training procedure. For\nsimplicity, we omit the details of converting from pixel space to latent space.\nphoto. For example, the images in Fig. 1 were generated in just a\ncouple of seconds, given the original images on the left and the\ntextual prompts at the top.\nFace0 generates pleasing results (Fig. 1), while being especially\nsimple and efficient. In addition, it has several other advantages: (1) it\nenables easy and natural control of the generated faces, e.g. changing\nhair styles or orientations, both through textual prompts as well as\nmore gradually through direct manipulation of the face embedding\nvectors (Figs. 4 to 6), (2) by using fixed randomized face embedding\nvectors, instead of a user-supplied face image, it trivially solves the\nproblem of generating consistent characters across generated images,\nand (3) since it encourages the model to decode the facial features\nfrom the face embedding, instead of from the textual prompt, it\ndecouples some of the model\u2019s textual biases from its biases for\nfacial features and, while more research is needed, we are hopeful\nthat this is a step towards mitigating some of the model\u2019s inherent\nbiases for facial features (Fig. 3).\nOur core idea is to leverage a face embedding model. Specifically,\nwe take our dataset of training images and augment those that\ncontain a face with the embeddings of the face. We use a simple\nmodule (a small MLP) to project the embeddings to Stable Diffusion\u2019s\ncontext\u2019s space, and then jointly train the base diffusion model\nand this projection module to generate images conditioned on the\nface embeddings (see Fig. 2). In sampling time, we calculate the\nface embeddings from the user-supplied image, add it to Stable\nDiffusion\u2019s context, and sample an image in practically the standard\nway (we use a slightly modified classifier free guidance).\n2\nMETHOD\nThe main idea is to train the underlying diffusion model to be con-\nditioned on both text and the output of an efficient face embedding\nmechanism.\n2.1\nArchitecture and Training\nFor our face embedding module we use part of an Inception Resnet\nV1 model (i.e. we drop the last layers), trained on vggface2 [Cao\net al. 2018]. The model, with the dropped layers, is not suitable for\naccurate identification, but it is able to preserve enough of the visual\ndetails needed for a high quality generation. This embedding module\nmostly fixes the face pose and expression. We augment a dataset\nof annotated images transforming each (image, caption) pair into a\n(image, caption, face-embedding) triplet. We then train a 4 layer MLP\nto convert from the face embedding space into the CLIP embedding\nspace, and jointly fine-tune the underlying model to receive both the\nCLIP embedding and the projected face embedding as conditions.\nSpecifically, we fine-tune the parameters \ud835\udf03 of our model \ud835\udc40\ud835\udf03 to\noptimize the standard diffusion model MSE loss objective, with an\nadditional conditioning on the embedding of the face:\n\ud835\udc3f(\ud835\udf03) = \ud835\udc38\ud835\udc61,\ud835\udc650,\ud835\udc51,\ud835\udc53 ,\ud835\udf16 [\ud835\udc64\ud835\udc61 ||\ud835\udc40\ud835\udf03 (\ud835\udefc\ud835\udc61\ud835\udc650 + \ud835\udf0e\ud835\udc61\ud835\udf16,\ud835\udc61,\ud835\udc51, \ud835\udc53 ) \u2212 \ud835\udf16)2||]\nWhere \ud835\udc40\ud835\udf03 is the full model, including the projection MLP, param-\neterized by \ud835\udf03, \ud835\udc61 \u223c \ud835\udc48 (0, 1), \ud835\udf16 \u223c \ud835\udc41 (0, 1), \ud835\udefc\ud835\udc61, \ud835\udf0e\ud835\udc61 and\ud835\udc64\ud835\udc61 are the diffusion\nnoise parameters (see [Ho et al. 2020]), \ud835\udc650 is an image sampled from\nthe dataset, \ud835\udc51 is its associated text condition, and finally \ud835\udc53 is our\nnewly introduced face embedding condition (see Fig. 2).\nFace0: Instantaneously Conditioning a Text-to-Image Model on a Face\n\u2022\n3\n2.2\nSampling\nAt sample time, given the user-supplied image, we run the same face\nextraction logic we used in training time, calculate the projected\nface embedding, and use it to override the last three tokens.\nWe then use a slight variation of classifier free guidance (CFG)\n[Ho and Salimans 2022]. Similarly to standard CFG, we calculate the\nfollowing linear combination for the unconditioned result (with a\nnegative weight) and the conditioned result (with a positive weight).\nSpecifically, we evaluate the following standard CFG formula:\n\ud835\udf16\ud835\udc61 = \ud835\udc64 \u00b7 \u02c6\ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61,\ud835\udc51, \ud835\udc53 ) + (1 \u2212 \ud835\udc64) \u00b7 \ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61)\nWhere \ud835\udc51 is the textual prompt and \ud835\udc53 is the face embedding. In\nour experiments we use a classifier free guidance weight of \ud835\udc64 =\n7.5. Unlike standard classifier free guidance, to allow more refined\ncontrol of the result, we use a weighted mean of three separate\nconditioned vectors. The conditioned vectors are those conditioned\non the textual prompt \ud835\udc51 alone, the face embedding \ud835\udc53 alone, and\ntheir combination. We choose a parametrization where \ud835\udc50 represents\nthe relative weight of the combined vector, and \ud835\udc4e represents the\nrelative weight of the vector conditioned on the face-only from the\nremainder. Overall we have:\n\u02c6\ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61,\ud835\udc51, \ud835\udc53 ) = \ud835\udc50 \u00b7\ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61,\ud835\udc51, \ud835\udc53 ) + (1\u2212\ud835\udc50) \u00b7 (\ud835\udc4e \u00b7\ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61, \ud835\udc53 ) + (1\u2212\ud835\udc4e) \u00b7\ud835\udf16\ud835\udc61 (\ud835\udc67\ud835\udc61,\ud835\udc51))\nIn practice, at least one of these three weight terms is always 0.\nSee Sec. 3.4 and Fig. 7.\nFinally, we note that our method tends to maintain extremely\nhigh consistency to the face across generations (see Fig. 10). If this is\nundesirable, adding a small amount of noise to the input embedding\ncan increase variety.\n2.3\nDetails\nWe train our model on the LAION dataset [Schuhmann et al. 2022],\nfiltered by an aesthetics threshold of 5.5, and we further filter it\nto only include images that include a face larger than 20 pixels.\nWe note that this filtering operation might amplify biases in the\ndataset [Birhane et al. 2021]. This results in \u223c 10\ud835\udc40 (image, caption)\npairs. We use MTCNN [Zhang et al. 2016] for face detection. If an\nimage has multiple faces we only take the largest one (images with\nmultiple faces are useful for conditioning the model on multiple\ncharacter, we leave this for future research). To generate the face\nembeddings we crop the image based on the output of MTCNN and\nresize to a 160x160px square, preserving aspect ratio. We expand\nthe rectangle returned by MTCNN to include some more details,\nsuch as the hairstyle. For the expansion, we manually picked the\nvalues of 10 pixels for the left and right margins, 33 pixels for the\ntop margin and 15 pixels for the bottom margin. We have not tuned\nthese choices further and used these numbers throughout all of\nour experiments. We then run our face embedding module, which\noutputs an embedding vector. To project the vector into the CLIP\nembedding space, we use a simple 4-layer feed-forward network\nwith dimensions of 768 and ReLU non-linearities after the hidden\nlayers. This results in \u223c 10\ud835\udc40 additional total parameters.\nThe output of the projection is three 768-dimensional vectors\nwhich we use to override the last three tokens (tokens 75-77) in\nStable Diffusion\u2019s CLIP embedding vector. To support classifier free\nguidance, we zero out the projected embedding with a probability of\n10%. Note that we do not do the same for the text embedding, which\nmight have improved the quality of the generations, and could be\nan interesting direction for further research. We then jointly train\nthe U-Net, starting from the Stable Diffusion 1.4 checkpoint, and\nthe projection network, which is initialized randomly. We train on\n64 TPU-v4s for 500K steps with a learning rate of 2e-5 and a batch\nsize of 256. We use EMA of 0.9999. We keep the CLIP encoder for\nthe textual prompt and the VAE frozen during training.\n3\nRESULTS\n3.1\nModel Bias\nText-to-image diffusion models may inherit unfair biases from their\nunderlying training data. One type of bias might be correlating\nfacial features with specific words unrelated to facial features. Since\nour model is incentivized to decouple facial features from textual\nprompts, a simple variation of our method might allow some mit-\nigation of this type of bias. Specifically, instead of taking a face\nembedding vector generated by our face embedding module from a\ngiven image, we can instead, for every generation from the model,\nuse a randomly sampled face embedding vector. This procedure\nconditions the model on the random embedding which is decoupled\nfrom the biases the model might have for the textual prompts (see\nFig. 3). We note that this doesn\u2019t affect the model\u2019s running time,\nand can be applied horizontally to all generations containing a face.\nThis is only a preliminary result, and there are many important\nquestions still open, for example what biases might exist within\nthe face embeddings themselves, how the biases between the face\nembeddings and the textual prompt interact, and how to sample the\nrandom face embedding (for our experiments we used a simple mix-\nture of Gaussians). In spite of these shortcomings, we hope that our\npreliminary results encourage further research on this important\ntopic.\n3.2\nConsistent Characters\nWhen generating images with a diffusion model, a user might en-\ncounter a character that they like. Unfortunately it is usually non-\ntrivial to recreate the same character with the base model. With\nFace0, consistent character creation is trivial - we can just random-\nize an embedding when generating the character, and maintain\none that we like for maximum fidelity, or simply calculate the face\nembedding from an image generated without one.\nWe can also do the reverse: if we get a generation that we like\nbut would like to use a different face, we can keep the latent seed\nand prompt fixed, and only change the conditioning embedding.\nWhile this sometimes results in larger changes, often the main effect\nwould be to just change the face (see Fig. 1).\n3.3\nControllability\nThe embedding mechanism we use in Face0 mostly fixes facial\nfeatures, pose and expression when no other conditions are provided.\nHowever, our model allows controlling the generated face in two\nways. First, we can modify the generated faces by using the textual\nprompt and specifying a trait that contradicts the embedding. For\n4\n\u2022\nDani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan\nFig. 3. Samples for the prompts \u201cA stock photo of X\u201d for X in {\u201c\ud835\udc51\ud835\udc5c\ud835\udc50\ud835\udc61\ud835\udc5c\ud835\udc5f\u201d, \u201c\ud835\udc36\ud835\udc38\ud835\udc42\u201d, \u201c\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc5a\ud835\udc52\ud835\udc5f\u201d} from the base model (left) and our model with a random\nface embedding (right).\nexample, \u201ca person with blue hair\u201d (see Fig. 4). Second, our model\nprovides additional controllability for features that are harder to\ndescribe textually. For example, with a simple linear interpolation\nbetween two face embeddings, we can create a meaningful semantic\ntransition between the faces (see Fig. 5). Note that interpolation also\nprovides a simple way to take into account several face images of\nthe same person. For example, we can average the embedding or do\na weighted average with different weights between several images\nof the same person. Since generation is immediate, the weights can\nbe tuned interactively. We only did minimal experimentation with\nthis but this showed promising results for further research.\nFinally, we can combine the two control methods to attenuate\nthe strength of the textual control. For example, we can create an\nimage with a face embedding \ud835\udc52\ud835\udc60\ud835\udc5f\ud835\udc50 and the prompt \u201ca person with\na mustache,\u201d then calculate the embedding \ud835\udc52\ud835\udc5a\ud835\udc62\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc50\u210e\ud835\udc52 and, using\nlinear interpolation between the embeddings, control the amount\nof mustache (See Fig. 6).\n3.4\nSampling Variations\nWhen generating images with Face0, we found that changing the\nweights of the facial, textual and combined embeddings indepen-\ndently allowed for fine-grained control over the resulting images in\nuseful ways. As mentioned above in our experiments we fixed the\nCFG weight as \ud835\udc64 = 7.5. When a photo-realistic result was desired\n(ex. superheroes, doctors in Fig. 1) we set \ud835\udc50 = 1, essentially doing\nstandard CFG; in other words, all of the CFG strength was given to\nthe combined vector. When non-photo-realistic image generations\nwere desired (ex. the Van Gogh style paintings or the action figures\nin Fig. 1) we simply increased the weight of the textual embedding\nover the combined embedding. Values in the range 0.4 < \ud835\udc50 < 0.7 and\n\ud835\udc4e = 0 work well. We also experimented with assigning a negative\nweight to the facial embedding, and increasing the weight of the\ncombined vector, e.g. \ud835\udc50 = 1.4,\ud835\udc4e = 1, for very non-photo-realistic\nimages.\nFig. 7 illustrates the effect of changing the weights. As \ud835\udc50 increases,\nwe give more weight to the combined embedding and as \ud835\udc4e increases,\nwe shift weight from the textual embedding to the facial embedding.\n4\nCOMPARISONS\nWe compare our method to Dreambooth [Ruiz et al. 2022], a method\nthat conditions a diffusion model on a given subject by fine-tuning\nit on 3-5 images of that subject. One key difference between the\nmethods is inference time \u2013 training a Dreambooth model on a\nsubject took 15 minutes on an A100 GPU, while Face0 does not\nrequire per-subject training.\n4.1\nDataset\nTo perform the comparison, we created a dataset of 20 synthetic\nidentities, with multiple photos for each identity (we extracted faces\nfrom these photos as described in Sec. 2). In addition, we used 10\nidentities from the Labeled Faces in the Wild (LFW) [Huang et al.\n2008] dataset. We tested the models on 10 prompts (5 photo-realistic\nand 5 artistic). Dreambooth was trained on all available images (4-\n5) and Face0 received only a single photo as input. We collected\nresults from both methods on 8 random seeds using standard DDIM\nsampling (i.e. we used \ud835\udc50 = 1 for Face0).\n4.2\nQualitative results\nQualitative results can be seen in Fig. 8. We observe comparable\nresults, and note that it was easier to get consistent appearance\nof the face when using Face0 (Dreambooth often ignored the face\naltogether requiring multiple seeds to obtain a good result). On\nthe other hand, when there is an identity mismatch in Face0, it is\npreserved across seeds.\n4.3\nQuantitative results\nTo perform a quantitative evaluation of the methods we measure\nalignment with the provided text and the provided face. For text\nalignment, we measure the cosine similarity between the CLIP [Rad-\nford et al. 2021] embeddings of the generated image and the textual\nFace0: Instantaneously Conditioning a Text-to-Image Model on a Face\n\u2022\n5\nFig. 4. Our model allows overriding features from the face embedding via the textual prompt.\nFig. 5. Face0 enables control of facial features that are harder to describe textually via direct manipulation of the face embedding. Here we see simple linear\ninterpolation between the left and right faces.\nprompt. For face alignment, we extract the largest face from the\ngenerated image and compare it with a face provided to the model\nusing cosine similarity of the CLIP embeddings. When no faces exist\nin the generated image, we set the similarity to 0. We define the\noverall score of each generated image as the sum of the face and\ntext scores.\nBoth methods perform comparably on text alignment, but Face0\nscored better at aligning with the provided face (Tab. 1). In addition,\nboth methods performed slightly better on synthetic images. To ver-\nify our qualitative observations we also measured the performance\nof the methods when considering the best generation out of the\n8 random seeds (Tab. 2). We see a performance improvement for\nDreambooth, showing that it\u2019s less consistent than Face0.\nTable 1. Quantitative comparison of Face0 and Dreambooth over synthetic\nfaces (SYN) and faces from the LFW dataset (LFW).\nMethod\nText Align.\nFace Align.\nOverall\nFace0 (SYN)\n0.24 \u00b1 0.02\n0.72 \u00b1 0.07\n0.96 \u00b1 0.07\nDreamBooth (SYN)\n0.23 \u00b1 0.03\n0.46 \u00b1 0.19\n0.69 \u00b1 0.18\nFace0 (LFW)\n0.23 \u00b1 0.03\n0.66 \u00b1 0.06\n0.89 \u00b1 0.06\nDreamBooth (LFW)\n0.24 \u00b1 0.02\n0.39 \u00b1 0.14\n0.62 \u00b1 0.13\n5\nRELATED WORK\nText-to-Image Models. Deep generative models for image gen-\neration have shown tremendous progress in recent years. Early\n6\n\u2022\nDani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan\nFig. 6. Face0 enables fine-grained control of facial features that are harder to describe textually via direct manipulation of the face embedding. Here we see a\nsimple linear interpolation between the facial embeddings of two generated photos from the same source (the top-left image in Fig. 4) with different textual\nprompts.\nFig. 7. Face0 can independently weight the textual, facial, combined and\nunconditioned embeddings. All images were generated with the prompt\n\"latte art of a face in a mug\" and a fixed latent seed. See Sec. 3.4 for details.\nTable 2. Overall scores for Face0 and Dreambooth when selecting the best\nscore out of 8 seeds vs. the average.\nFace0\nDreamBooth\nBest (SYN)\n1.04 \u00b1 0.06\n0.93 \u00b1 0.11\nAverage (SYN)\n0.96 \u00b1 0.07\n0.69 \u00b1 0.18\nBest (LFW)\n0.98 \u00b1 0.04\n0.83 \u00b1 0.10\nAverage (LFW)\n0.89 \u00b1 0.06\n0.62 \u00b1 0.13\napproaches relied on training a GAN [Goodfellow et al. 2014] gen-\nerator (like StyleGAN [Karras et al. 2018]) and guiding it using\nCLIP [Radford et al. 2021] in various methods [Abdal et al. 2021;\nPatashnik et al. 2021]. More recently, transformer-based methods\n[Chang et al. 2023; Ramesh et al. 2021; Yu et al. 2022] and diffusion\nmodels [Ramesh et al. 2022; Rombach et al. 2021; Saharia et al. 2022]\nhave gained popularity as they allow easy text-conditioning and\ncan generalize to broader domains. Our work, demonstrated on the\nStable Diffusion model [Rombach et al. 2021], shows that diffusion\nmodels can also be easily conditioned on other modalities, such as\nface encoding.\nImage embedding. Encoding image pixels into a latent repre-\nsentation that contains useful features for downstream models is\nan important and long-standing problem in deep learning. Some\nuseful encoding are achieved by training an autoencoder [Esser et al.\n2020; Oord et al. 2017], while other methods take an intermediate\nlayer of a model that was trained to solve an image-related problem\nlike image recognition [Dosovitskiy et al. 2021]. A popular image\nencoder is CLIP [Radford et al. 2021] which is obtained by aligning\nimages with textual captions. In our work we use an intermediate\nlayer of an Inception Resnet [Szegedy et al. 2016, 2014] that was\ntrained on the vggface2 [Cao et al. 2018] dataset.\nPersonalization of image generation models. Personalized\nimage generation attempts to include new subjects, described by\none or more images, in the resulting synthesized image. A common\napproach to this problem is to fine-tune an image generation model\nduring inference, on the provided images. MyStyle [Nitzan et al.\n2022] fine tunes a StyleGAN [Karras et al. 2018] on a custom face\nimage. DreamBooth [Ruiz et al. 2022] and Textual Inversion [Gal\net al. 2022] enable personalization in diffusion models using fine-\ntuning (either of the model itself or of an entry in the embedding\nFace0: Instantaneously Conditioning a Text-to-Image Model on a Face\n\u2022\n7\nOriginal\nDreambooth\nOurs\nGreek \nstatue\nSyn12\nSyn19\nAt the \nbeach\nSyn18\nSuperhero\nPolice \nofficer\nBasketball \nplayer\nPirate \ncaptain\nSyn0\nPencil \nsketch\nFig. 8. Comparison of Face0 and Dreambooth. Images shown are the best\nout of 8 random samples.\ntable of the textual encoder). Other methods [Molad et al. 2023;\nValevski et al. 2022] use fine-tuning for text-guided editing of a\nsingle input image or video. These methods perform fine-tuning\nduring inference, and are therefore costly in time and memory. More\nrecent advancement [Ryu 2013] use LORA [Hu et al. 2021] to address\nthe memory cost, but speed is still an issue. Concurrently with our\nwork, [Gal et al. 2023] suggest to use intermediate layers in CLIP\n[Radford et al. 2021] as input to an image encoder. This significantly\nlowers the amount of fine-tuning required for high quality inference,\nbut does not eliminate it.\n6\nDISCUSSION AND LIMITATIONS\nWe presented Face0, a novel and simple method for conditioning\na diffusion based image generation model on a face. Once trained,\nthe model is able to produce pleasing results extremely quickly,\npractically at the same cost as the base model. Our method allows\nfor controlling more or less photo-realistic generations (by varying\nthe text-only, face-only and combination CFG weights balance). We\nshow that it is easy to override properties of the face embedding\nwith the textual prompt. We also show that our method can help\nsolve the problem of consistent character generation, by keeping a\nfixed face embedding vector. Finally, while more research is needed,\nwe show that training the model to decouple its textual conditioning\nfrom its conditioning on a face, is hopefully a step towards some\nmitigation of some of the biases of the base model.\nThere are several interesting related directions for further re-\nsearch and improvements. For example, we choose a face embed-\nding mechanism that mostly fixes the face pose and expression, but\nit would be interesting to experiment with other face embedding\nmechanisms. In addition, while generating pleasing results, Face0\nis not always able to fully preserve a provided identity, and some-\ntimes creates \"look-alike\" characters that are close in appearance\nbut still distinguishable from the input face. We are hopeful that\nthis can be improved by smart noising of the embedding vector and\nby experimenting with conditioning the model on multiple faces (in\nour experiments we only allowed one) which we leave for future\nwork. Another interesting idea would be to use the face embedding\nmodel to guide sampling at each sampling step. Finally, while faces\nare especially important to condition on, it might be interesting to\napply the same method to additional domains.\nSOCIETAL IMPACT\nFace0, like other image generation techniques, has a great potential\nto complement and augment human creativity by creating new\ntools for professionals and empowering non-professionals with the\nability to create images more easily and in a more intuitive manner.\nHowever, we recognize that applications of this research may impact\nindividuals and society in complex ways (see [Saharia et al. 2022]\nfor an overview). In particular, this method illustrates the ease with\nwhich such models can be used to alter sensitive characteristics\nsuch as skin color, age and gender. Although this has long been\npossible by means of image editing software, text-to-image models\ncan make it easier.\nAnother cause of concern is reproducing unfair bias that may be\nfound in the underlying model training data. This is also relevant\nfor the underlying model, Stable Diffusion. Moreover, these unfair\nbiases may make the performance of the model vary across people\nof different groups. While we did not see this effect in our qualitative\nexperiments, more research into bias evaluation methods, both for\nimage editing and generation, will help address this concern. In\naddition, while these capabilities already exist in image editing\nsoftware, for example, single image personalization methods, such\nas Face0, may increase the ability to forge convincing images of\nnon-public individuals, or make it easier to generate disinformation\nand manipulate images in hateful and harassing ways.\nWe encourage future research to help mitigate and measure the\npotential negative impact of generative models if misused, and be-\nlieve thoughtful consideration and further research in all of these\nmatters is necessary prior to determining how such technologies\ncan be made broadly available.\n8\n\u2022\nDani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan\nACKNOWLEDGMENTS\nWe would like to thank Matan Kalman, Jason Baldridge, Kathy\nMeier-Hellstern, Tom Duerig, Caroline Pantofaru, Michael Nechyba,\nDmitry Lagun, Viral Carpenter, Eyal Segalis, Eyal Molad, Yael Pritch,\nShlomi Fruchter, the Theta Labs team at Google, and our families.\nREFERENCES\nRameen Abdal, Peihao Zhu, John Femiani, Niloy J. Mitra, and Peter Wonka. 2021.\nCLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions.\nhttps:\n//doi.org/10.48550/ARXIV.2112.05219\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021.\nMul-\ntimodal\ndatasets:\nmisogyny,\npornography,\nand\nmalignant\nstereotypes.\narXiv:cs.CY/2110.01963\nQiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. 2018. VG-\nGFace2: A dataset for recognising faces across pose and age. arXiv:cs.CV/1710.08092\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-\nHsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li,\nand Dilip Krishnan. 2023. Muse: Text-To-Image Generation via Masked Generative\nTransformers. arXiv:cs.CV/2301.00704\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. arXiv:cs.CV/2010.11929\nPatrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. 2020. Taming Transformers for\nHigh-Resolution Image Synthesis. https://doi.org/10.48550/ARXIV.2012.09841\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An Image is Worth One Word: Personalizing Text-to-\nImage Generation using Textual Inversion. https://doi.org/10.48550/ARXIV.2208.\n01618\nRinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. 2023. Encoder-based Domain Tuning for Fast Personalization of Text-to-\nImage Models. arXiv:cs.CV/2302.12228\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial\nNetworks. https://doi.org/10.48550/ARXIV.1406.2661\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic\nModels. https://doi.org/10.48550/ARXIV.2006.11239\nJonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance.\nhttps:\n//doi.org/10.48550/ARXIV.2207.12598\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\nModels. arXiv:cs.CL/2106.09685\nGary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008. La-\nbeled faces in the wild: A database forstudying face recognition in unconstrained\nenvironments. In Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and\nrecognition.\nTero Karras, Samuli Laine, and Timo Aila. 2018. A Style-Based Generator Architecture\nfor Generative Adversarial Networks. https://doi.org/10.48550/ARXIV.1812.04948\nEyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch,\nYaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video Diffusion Models are\nGeneral Video Editors. arXiv:cs.CV/2302.01329\nYotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman,\nInbar Mosseri, Yael Pritch, and Daniel Cohen-or. 2022. MyStyle: A Personalized\nGenerative Prior. arXiv:cs.CV/2203.17272\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete\nRepresentation Learning. https://doi.org/10.48550/ARXIV.1711.00937\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021.\nStyleCLIP: Text-Driven Manipulation of StyleGAN Imagery.\nhttps://doi.org/10.\n48550/ARXIV.2103.17249\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From\nNatural Language Supervision. https://doi.org/10.48550/ARXIV.2103.00020\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical Text-Conditional Image Generation with CLIP Latents. https://doi.\norg/10.48550/ARXIV.2204.06125\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\nMark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. https:\n//doi.org/10.48550/ARXIV.2102.12092\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\narXiv:cs.CV/2112.10752\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models for\nSubject-Driven Generation.\nSimo Ryu. 2013. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning.\nhttps://github.com/cloneofsimo/lora.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Den-\nton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language\nUnderstanding. https://doi.org/10.48550/ARXIV.2205.11487\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig\nSchmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale\ndataset for training next generation image-text models. arXiv:cs.CV/2210.08402\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi. 2016. Inception-\nv4, Inception-ResNet and the Impact of Residual Connections on Learning.\narXiv:cs.CV/1602.07261\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2014. Going\nDeeper with Convolutions. arXiv:cs.CV/1409.4842\nDani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. 2022. UniTune:\nText-Driven Image Editing by Fine Tuning an Image Generation Model on a Single\nImage. arXiv:cs.CV/2210.09477\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei\nHan, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. 2022.\nScaling Autoregressive Models for Content-Rich Text-to-Image Generation. https:\n//doi.org/10.48550/ARXIV.2206.10789\nKaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016. Joint Face Detection\nand Alignment Using Multitask Cascaded Convolutional Networks. IEEE Signal\nProcessing Letters 23, 10 (oct 2016), 1499\u20131503.\nhttps://doi.org/10.1109/lsp.2016.\n2603342\nFace0: Instantaneously Conditioning a Text-to-Image Model on a Face\n\u2022\n9\nFig. 9. Face 0 maintains consistency across generations. Non-cherry picked examples using the face embeddings from the original photos in Fig. 1.\n10\n\u2022\nDani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan\nFig. 10. Additional examples using several images of the same person and a variety of prompts.\n"
  },
  {
    "title": "Scalable 3D Captioning with Pretrained Models",
    "link": "https://arxiv.org/pdf/2306.07279.pdf",
    "upvote": "13",
    "text": "Scalable 3D Captioning with Pretrained Models\nTiange Luo1,\u2217\nChris Rockwell1,\u2217\nHonglak Lee1,2,\u2020\nJustin Johnson1,\u2020\n1University of Michigan\n2LG AI Research\nAbstract\nWe introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of\na 3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using\n41k human annotations from the same dataset, demonstrates that Cap3D surpasses\nhuman-authored descriptions in terms of quality, cost, and speed. Through effective\nprompt engineering, Cap3D rivals human performance in generating geometric de-\nscriptions on 17k collected annotations from the ABO dataset. Finally, we finetune\ntext-to-3D models on Cap3D and human captions, and show Cap3D outperforms;\nand benchmark the SOTA including Point\u00b7E, Shap\u00b7E, and DreamFusion.\n3D model of a sakura soft drink \ncan with purple and yellow \ngradient, Japanese writing, and \npurple flowers.\nA 3D model of a blue grand \npiano with spikes and sharp \nteeth resembling a shark \nmouth.\nA 3D model of a metal cube \nfeaturing a skull, pizza, and \nvarious stickers.\n3D model of a yellow Pikachu-\nthemed Pok\u00e9mon ball with a \nblack and gold stripe and \nlightning bolt.\n3D model of Notre Dame \nCathedral, a Gothic cathedral \nwith spires in Paris.\nLoki bust 3D model \nfeaturing a green and yellow \nhorned helmet.\nz\nA 3D model featuring a \nbasketball hoop, ball, racquet, \nbowling ball, stand, and pin.\n3D model of a purple and \ngreen Halloween spider bowl \non a metal stand, containing \npurple liquid.\n3D model of an armored \ncharacter with purple horns and \nspikes on the back.\n3D model of a robotic \nscorpion with multiple arms \nand guns.\nA cluster of five glass sphere \nlight bulbs suspended from a \nsingle thin wire.\nL-shaped sectional sofa with a \nchaise, U-shaped backrest, \ncurved armrests, and a \nfootstool on one side.\nFigure 1: Cap3D provides detailed descriptions of 3D objects by leveraging pretrained models in\ncaptioning, alignment, and LLM to consolidate multi-view information. Two views of 3D objects are\nshown here, Cap3D uses eight. Additional examples are available in Appendix B.\n\u2217 joint first authorship; \u2020 equal advising\nPreprint.\narXiv:2306.07279v2  [cs.CV]  16 Jun 2023\nTable 1: Cap3D is better, cheaper, and faster than crowdsourced annotation. Use 36k responses\nacross 22k objects for A/B testing; 8A40s on a cloud platform for speed and cost computations.\nMethod\nA/B Human Testing\nCost per\nAnnotation\nWin % (Tie %)\n1k Objects\nSpeed\nHuman\n37.8% \u00b1 0.5% (9.5%)\n$87.18\n1.4k / day\nCap3D\n52.3% \u00b1 0.5% (9.5%)\n$8.35\n65k / day\n1k Objects Cost Breakdown\nBLIP2\n$3.79\nCLIP\n$0.38\nGPT4\n$4.18\nCap3D Total Cost\n$8.35\n1\nIntroduction\nText-conditioned 3D synthesis [1\u20133] could revolutionize the creation process of 3D assets, impacting\nvarious sectors, including 3D design, virtual reality [4], film [5], robotics [6, 7], and autonomous\ndriving [8]. However, challenges persist, namely the high cost of 3D asset creation and the scarcity of\nhigh-quality captions for 3D assets. Objaverse [9] takes a step towards this as the first public large-\nscale 3D object dataset. Unfortunately, while objects contain paired metadata, these do not serve as\ninformative captions, as shown in Table 3. In contrast with 3D, a plethora of high-quality text-image\npaired data is publicly available [10\u201314]. This data has led to incredible recent progress in image-text\nlearning [15\u201318], text-conditioned image synthesis [19\u201324], and image captioning [25\u201329].\nIn this work, we present Cap3D, a method to automate 3D object annotation. Our key insight is\nto leverage the abundance of knowledge in pretrained image-text models to remedy the lack of\nexisting 3D-text data. The core of our data collection process is to apply an image captioning\nmodel (BLIP2 [29]) to a set of 3D asset renders, use an image-text alignment model (CLIP [16]) to\nfilter captions, and apply a language model (GPT4 [30]) to fuse the filtered captions across views.\nCritically, the models we apply are pretrained on varied and large-scale text-image [11\u201313, 31\u201333],\nand text [34], data; and approach complementary problems. As a result, each model adds additional\nvalue to the framework, as we show in Table 3.\nCap3D is agnostic to 3D asset sources and can be effectively scaled to larger extents with increased\n3D assets and computational resources. In this paper, we apply it primarily to Objaverse, gathering a\ndataset of 660k 3D-text pairs. Through object rendering and captioning, we enable ethical filtering\nof 3D objects via both image and text, as detailed in \u00a7 3.2. We publicly release all of our collected\ndata including automated and human-annotated captions, along with associated Point Clouds and\nRendered Images, at huggingface.co/datasets/tiange/Cap3D. The dataset is released under\nODC-By 1.0 license. We will release trained models and code for replicating the benchmark table.\nWe validate our collection approach by collecting over 50k crowdsourced captions on over 40k\nobjects. We conduct human evaluations and show on Objaverse that our automated captions are\nsuperior to crowdsourced captions in quality, cost, and speed (Table 1, details in Appendix A).\nSpecifically, it is preferred 35% more often by humans, costs more than 10 times less, and is over\n40 times faster, assuming only 8A40 GPUs. We also test the limits of automated captioning. We\nconsider a separate task of captioning geometry (as shown in Figure 1 bottom-right) using ABO, a\ndataset of 3D models with complex geometries [35]. Shown in Table 4, our automated captioning\nunderperforms humans. However, by formulating description as a question answering task (detailed\nin \u00a7 3.1), we show stronger performance compared to crowdsourced workers. This result shows the\nability of our method to adapt beyond traditional captioning and still be highly competitive.\nFinally, our high-quality gathered 3D-text dataset enables us to train and validate large-scale text-\nto-3D models. In \u00a75.3, we evaluate several state-of-the-art methods on Objaverse out-of-the box,\nincluding Point\u00b7E, Shap\u00b7E, DreamFields, and DreamFusion. Finetuning on our data typically shows\nmeaningful improvements, demonstrating the value of the collected dataset. In addition, we show our\nautomatically collected captions yield better finetuning performance than human captions \u2013 even at\nthe same scale. At full scale, finetuning is further boosted.\n2\nRelated Work\nObtaining 3D-text pairs at scale is challenging, and we take inspiration from image-text datasets and\nmethods when approaching this task.\n2\nImage-Text Data and Modeling. Early image captioning [36\u201338] and text-image representation\nlearning methods [39, 40] were built using CNNs [41\u201343] and LSTMs [44, 45], leveraging human-\nannotated datasets [31\u201333, 46]. Text-to-image methods used similar datasets, and relied on GANs [47,\n48] and VQVAEs [19, 49\u201351]. The advent of semi-automated image-text collection has enabled\nsuccessful scaling of datasets [10\u201314] and models [25\u201328]. Transformer-based architectures [16,\n52, 53] and diffusion models [54\u201359] have scaled best to large data; we employ transformer-based\nmethods through our captioning process and adopt diffusion models for text-to-3D experiments.\nTraining models upon large datasets and using the corresponding trained models to filter larger data\nhas led to datasets of rapidly increasing size [13, 14]. In addition to filtering, trained models have\nbeen used to annotate new data with high-quality [60]. We take this approach, captioning rendered\nviews with BLIP2 [29], refining with CLIP [16, 61], and summarizing with GPT4 [62]; all of which\nare trained on large datasets, including [11\u201313, 31\u201333]. Concurrent works [63\u201365] use automated\ncaptioning on 2D images using an older system [66] or based upon metadata [65, 67].\n3D-Text Data and Modeling. Until recently, 3D data was of relatively small scale (\u223c 50k objects)\n[68\u201371]. Labeled 3D-text data was scarce, relying on human annotation, and typically limited to\nShapeNet [68] chairs [72] or tables and chairs [73, 74], and ScanNet [75, 76]. This enabled prior work\nto undertake the task of 3D captioning [77\u201379] or text-to-3D [73, 77, 80\u201383] at small scale. Methods\nthat approached text-to-3D would sometimes avoid 3D supervision entirely [3, 84\u201386], leading to\nslow generation due to many optimization steps. We annotate a small-scale dataset containing 3D\nfurniture, ABO [35], to evaluate the ability of Cap3D to specify fine-grained geometry.\nObjaverse [9] introduced a diverse set of objects over 10 times the size of the prior largest public 3D\ndataset [68]. This data is our primary captioning focus, and we associate a single caption with each\nobject in Objaverse after filtering. Concurrent works [65, 87] gather text associated with Objaverse,\nbut do not fuse captions across views [87] or rely upon metadata [65], and do not approach text-to-3D.\nThe concurrent studies 3DGen [2] learns text and image to 3D on Objaverse; Point\u00b7E [88] and\nShap\u00b7E [89] learn text-to-3D models on a large-scale 3D dataset, but none have fully disclosed their\ncode or data. Point\u00b7E involves two variants and released a text-to-3D model and a text-to-image-to-3D\nmodel by finetuning GLIDE [23] and training an image-to-point cloud diffusion model [90]. Other\nrecent works [91, 92] also focus on scaled image-3D generation. We show finetuning on our captions\nimproves Point\u00b7E performance despite having already been trained on large amounts of Internet data.\n3\nMethod\n3.1\nCaptioning Process\nOur task is to produce a single descriptive caption given a 3D asset. Our proposed method, Cap3D,\nemploys a four-step process. First, we render a set of 2D views for each 3D object. Next, we apply\nimage captioning to achieve preliminary descriptions. As these captions may contain inaccuracies, an\nimage-text alignment model, CLIP, is introduced in the third step to rectify errors. Finally, an LLM\nis employed to unify captions from various perspectives, creating a comprehensive caption. This\nprocess is shown in Figure 2 and detailed below.\nObject Rendering: We render using Blender at 512\u00d7512 from M = 8 high-information camera\nangles rotating horizontally around the object, with two slightly below and the rest slightly above\nthe object, to cover all the object details. The reason we prefer multiple views is a forward-facing\nview may miss self-occluded object details (e.g. Figure 1 row 1) or face strange appearance and/or\nlighting. In contrast, multiple views will see much of the object from different viewpoints, increasing\nthe number of chances for a captioning model to predict objects in detail. For instance, in Figure 2,\nthe back view 1 identifies the \"yellow handle\", which is barely visible in forward view M.\nImage Captioning: We use BLIP2 [29] for captioning, selecting the largest pretrained model\nadapting ViT-G [53, 93] image encoder and FlanT5XXL [94] text encoder. We generate N = 5\ncaptions per rendered image using nucleus sampling [95]. By generating multiple captions, we\nincrease the likelihood of generating correct details (e.g. \"black and yellow toy bomb\" in Figure 2\nview M caption 1). Incorrect captions, such as \"scissors\" in Figure 2 view M caption N, can be\nfiltered in later stages. To generate captions containing fine-grained geometry details (in our ABO\nexperiments), we employ a two-stage question-answering instead of captioning. The first stage\n3\nGPT4\nCLIP\nA 3D model of a yellow \nand black toy bomb with \na handle and a straw\nInput 3D Object\n\u2026\n1. A black and \nyellow toy bomb on \na grey background \nN. A 3d model of a \nbomb with scissors\n\u2026\nView 1\nView M\nOutput Caption\nView 1. A yellow \nand black bomb \nwith a yellow \nhandle\nView 2. a yellow \nand black toy ball \nwith a straw \nsticking out of it \nView M. A black \nand yellow toy \nbomb on a grey \nbackground\n\u2026\n1. A yellow and \nblack bomb with a \nyellow handle\nN. 3d model of a \nbomb with a yellow \ncup on it\n\u2026\n\u2026\nBLIP2\n\u2026\nBLIP2\nCLIP\nPrompt: Given a set of \ndescriptions about the \nsame 3D object \u2026 distill \nthese descriptions into one \nconcise caption:\nFigure 2: Overview of Cap3D. Left to Right: (1) Render 3D objects from M = 8 camera angles\nto capture object details (2) Generate N = 5 image captions per rendered image using BLIP2; (3)\nSelect one caption for each image based on its similarity to the image encoding using CLIP; (4) Use\nGPT4 to consolidate all selected captions into a final, summary of the object.\ngenerates one answer to a prompt asking what object is pictured. The answered object is passed into\na second prompt, which asks its structure and geometry, and generates 5 answers.\nCaption Selection: While BLIP2 often generates high-quality captions, it is not uncommon for\nsamples to contain mistakes, particularly in non-forward facing views such as \"yellow cup\", in Figure\n2 view 1, caption N. To reduce the frequency of mistakes, we compute CLIP [16] ViT-B/32 [53]\nencodings from each of 5 captions and the associated image, and select the caption maximizing cosine\nsimilarity. CLIP tends to select good captions for each view, e.g. Figure 2: view 1, BLIP2 caption 1\nand view M, caption 1. CLIP is complementary to BLIP2 as not only does it have different training\ndetails and architecture, but it trains on different data. While BLIP2 is trained upon COCO [31],\nVisual Genome [32], CC3M [11], CC12M [12], SBU [33] and LAION400M [13]; CLIP is trained\nupon a dataset of 400M images based on frequent text occurrence in Wikipedia.\nCaption Consolidation: Accumulating information across viewpoints to form a complete picture of\n3D objects is challenging, but crucial. We find prompting of GPT4 [62] to summarize the M captions\nresults in good parsing of the details across captions. By applying GPT4 as the final summary step,\nit can both include significant details and remove unlikely ones. For example, the final caption in\nFigure 2 filters the incorrect information, from view 2, \u201ctoy ball\", while keeping key details, including\n\"handle\" and \"straw\". The alternative order of GPT4 followed by CLIP would result in (1) GPT4\nhaving to make sense of more incorrect input details and (2) CLIP simply selecting between aggregate\ncaptions instead of being able to error-correct small mistakes. The effectiveness of introducing GPT4\nis verified in ablations (Table 3).\n3.2\nEthical Filtering\nCaptions generated and images rendered by Cap3D enhance the identification and mitigation of legal\nand ethical issues associated with large-scale 3D object datasets, including identifiable information\nand NSFW content.\nWe manage two datasets: Objaverse and ABO. In Objaverse, our main responsibility involves dealing\nwith artist-created assets. These can include identifiable elements such as human face scans and\nNSFW objects. Objaverse contains approximately 800k objects, which makes the manual verification\nof each asset impractical. The ABO dataset, on the other hand, is smaller and mostly consists of\nfurniture. We manually ensure the ethical integrity of this dataset.\nWe begin by filtering Objaverse to include only those objects that can be rendered and shared. Objects\nwith CC BY-NC-SA and CC BY-NC licenses are removed, while we retain those with CC BY, CC\nBY-SA, and CC0 licenses, thereby facilitating commercial usage of our data. This process reduces\nthe dataset size from 798k to 723.7k objects. Furthermore, we exclude objects that lack sufficient\ncamera information for rendering, leaving us with 680k objects.\nWe next follow prior work [10] and use a face detector [96] and NSFW classifier [97, 98] on forward-\nfacing object renders and filter detected objects with score >= 0.9. The face detector filters out 18.6k\n4\nA plant with \nwhite flowers\nFlower.\nMetadata\nHuman\nCap3D\n3D model of a branch with \nwhite flowers and green leaves.\n0\n5\n10\n15\n20\n25\n30\nNumber of words in caption\n0\n6\n12\n18\n24\n% of captions\nMetadata\nHuman\nCap3D\nFigure 3: Objaverse Caption Comparison. Human captions and Internet metadata frequently\ncontain limited detail. Cap3D captions typically have longer length and more detail.\nobjects, and the NSFW classifier filters out 217 objects. Text is also carefully processed. Our final cap-\ntions are the output of GPT4, which has been trained to filter out inappropriate or harmful content [62].\nTable 2: Ethical Filtering Analysis. We\nmanually detect faces and NSFW content\nto validate automated filtering. 16 of 17\nmissed face detections were sports cards.\nDetected\nPrecision\nMissed dets.\n(Filtered)\n5k\n(%)\n10k\n680k\nFaces\n18.6k\n790 16%\n17\n\u22481k\nNSFW\n217\n102 47%\n12\n<1k\nLanguage \u2020 226\n\u2013\n\u2013\n\u2013\n\u2013\n\u2020: String match filtering is deterministic.\nWe run a standard blocklist [99] on its output, remov-\ning any object-caption pairs including blocked words.\nThis filters out 226 objects. After all the filtering, we\nare left with 661k objects in the Objaverse dataset. We\nmanually estimate detection precision and recall in Ta-\nble 2. To summarize, our process detects over 19k\nobjects, of which a nontrivial amount is accurately re-\nmoved. We estimate roughly 1k face and less than 1k\nNSFW are missed, using a conservative standard (e.g.\nmissed faces are typically sports cards).\n4\nDataset\nWe collect captions in two distinct settings: Objaverse, a large and varied dataset of artist-created 3D\nassets; and ABO, a small dataset of real products, typically furniture.\n4.1\nObjaverse Captions\nObjaverse [9] features roughly 800k 3D object assets across 21k classes designed by over 100k artists.\nIt is of significantly larger scale than prior work; the paper shows this size enables more diversity\nby generative 3D models trained upon it. It is released under the ODC-By 1.0 license, permitting\nsubsequent researchers to curate new data from it. Metadata is paired with many assets, however as\nseen in Figure 3 (right), metadata caption length is frequently short or empty. We collect two caption\ndatasets on Objaverse. First, an automated set of one caption for each of 660k objects using Cap3D\n(a total of 660k captions). Second, a crowdsourced set of 41.4k captions spanning 39.7k objects\nfor evaluating generated captions. Captions are collected using thehive.ai, a crowdsourced platform\nsimilar to AMT. Workers are given instructions with gold-standard sample captions, see the same\n8 views as models during captioning, and are routinely monitored. Poor captioning performance\nresults in a ban and deletion of the worker\u2019s captions. Crowdsourced captions are also filtered using\nthe blocklist in \u00a7 3.2. Figure 3 (left) shows human captions provide more detail than metadata, but\nautomated captions tend to be most descriptive.\n4.2\nABO Geometry Captions\nABO [35] is a collection of 3D models of Amazon products and is primarily furniture. ABO serves\nas an important contrast to Objaverse as it consists of a small number of classes varying primarily in\ngeometry. Captioning, therefore, needs to focus more on structure as opposed to semantic category.\nTo emphasize this focus, we consider the task of captioning the geometric structure of objects without\ncolor or texture (seen in the bottom right of Figure 1). Like Objaverse, ABO contains metadata that is\ntypically quite short (Table 4), resulting in limited detail. We collect three sets of captions on the\n6.4k ABO splits of [77]: crowdsourced (a total of 17.2k captions), captions generated by Cap3D\n5\nBed.\nCap3D\nMetadata\nHuman\n3D rendering \nof a couch.\nCap3D (QA)\nThree-seater sofa with a \nslender, curved backrest \nand armrests.\nA three seater sofa with \nlow backrest to the \nheight of the armrests.\n0\n5\n10\n15\n20\n25\n30\nNumber of words in caption\n0\n6\n12\n18\n24\n% of captions\nMetadata\nHuman\nCap3D\nCap3D (QA)\nFigure 4: ABO Automated Geometric Description. Left: Human descriptions provide more\ndetailed geometry than automated captions. With careful prompting, Cap3D (QA) can match human-\nlevel detail. Right: The high peak of Metadata is cropped, which otherwise obscures other curves.\n(a total of 6.4k captions), and captions generated by Cap3D (QA) which uses the two-stage prompt\ncaptioning (a total of 6.4k captions). Crowdsourced captions follow similar detail to Objaverse with\nthe exception instructions and examples are focused on geometric structure. We compare alternatives\nin Figure 4. In contrast to Objaverse, human geometric descriptions on ABO are more detailed than\ncaptioning. With prompting (QA), the Cap3D pipeline can rival human descriptions.\n5\nExperiments\nIn this section, we first validate the quality of Cap3D captions against metadata and human-authored\ncaptions on both Objaverse and ABO. To verify Cap3D captions are helpful in practice, we next\ncompare text-to-3D models finetuned on both human-authored captions and Cap3D (using the same\n>30k set as crowdsourced captions). Finally, we evaluate state-of-the-art text-to-3D models on our\ncaptions at scale to measure if finetuning on our captions can improve performance.\n5.1\n3D Captioning on Objaverse\nDataset. We evaluate caption quality on three subsets of Objaverse: (1) a random set of 22k objects\ncontaining a human caption, (2) a random split of 5k objects containing a human caption, and (3) a\nrandom 5k split across the entire dataset.\nBaselines. In data splits (1) and (2), we compare the caption generated by Cap3D with human-\nauthored annotations, Human, and existing Objaverse metadata, Metadata, described in \u00a7 4.1. Split\n(1) is used for A/B testing of Cap3D vs. Human, as shown in Table 1, at scale. Collecting A/B\ncomparison is expensive, so we compute more extensive experiments on the smaller set (2) in Table 3.\nIn data split (3), we ablate the main components of Cap3D into BLIP2 and +GPT4. BLIP2 uses only\nthe image captioning component of our method, taking a front-view rendering and producing a single\noutput caption. +GPT4 uses the same image captioning process of our method, producing 5 captions\nfor each of 8 views. However, instead of using CLIP to filter 5 captions from each view, it directly\nsummarizes all 40 captions into a final caption.\nMetrics. Our primary metric is human judgment A/B tests, where we ask workers to select between\ntwo captions on a scale of 1-5, where 3 is a tie. Workers are carefully monitored and each comparison\nhas at least 10k observations across 5k objects.We report mean score, along with the percent each\nmethod is preferred (i.e. scores a 4 or 5). We use automated metrics CLIPScore [16, 61], the cosine\nsimilarity of CLIP encodings with input images; and ViLT Image and Text Retrieval, which ranks\nlikely image-text pairs, from which one computes precision.\nWe emphasize CLIPScore is not our primary metric since our captioning model utilizes CLIP. BLIP2\nutilizes ViT-L/14 and ViT-g/14, while our filtering uses ViT-B/32, so following previous work [84]\nwe compute CLIP score using a different model to reduce bias (ViT-B/16). However, we report it as it\nhas shown a higher correlation with human judgments than other automated metrics [61]. ViLT [100]\nis trained on different data and is a different architecture than CLIP, providing an orthogonal metric.\n6\nTable 3: Objaverse Captions Evaluations. Cap3D outperforms human and Metadata; BLIP2, GPT4,\nand CLIP are all important to performance. We report 95% confidence interval and use 5k objects.\nMethod\nUser A/B Study vs. Cap3D\nCLIP\nViLT Img Retr.\nViLT Text Retr.\nScore (1-5)\nWin %\nLose %\nScore\nR@5\nR@10\nR@5\nR@10\nMetadata\n1.74\u00b10.026\n10.7 \u00b1 0.7\n83.8 \u00b1 0.8\n66.8\n4.3\n6.3\n6.1\n8.5\nHuman\n2.86\u00b10.026\n37.0\u00b11.0\n46.1\u00b11.0\n72.5\n21.2\n29.0\n18.5\n24.9\nCap3D\n-\n-\n-\n88.4\n35.7\n46.3\n34.7\n44.2\nBLIP2\n2.87\u00b1 0.019\n41.0\u00b1 0.7\n50.6\u00b1 0.7\n83.1\n24.7\n32.3\n21.9\n29.3\n+ GPT4\n2.94\u00b1 0.015\n35.2\u00b1 0.6\n40.8\u00b1 0.6\n86.3\n31.9\n39.9\n30.2\n38.4\n+ CLIP (Cap3D)\n-\n-\n-\n86.9\n31.1\n40.2\n30.3\n38.6\nA 3D model of a house \nwith a garage, roof, \ngrass and trees.\nA 3D model of a house with a \nroof, garage, grass, trees, and \na green field, featuring a knife \nand a pair of scissors.\nA 3D model \nof a house \non a gray \nbackground.\n+ GPT4\nBLIP2\n+ CLIP (Cap3D)\n0\n5\n10\n15\n20\n25\n30\nNumber of words in caption\n0\n6\n12\n18\n24\n% of captions\nBLIP2\n+ GPT4\n+ CLIP (Cap3D)\nFigure 5: Objaverse Caption Ablations. GPT produces longer and more detailed captions than\nBLIP2; CLIP tends to prune incorrect details and reduces length slightly.\nResults. We report large scale A/B testing (1) against Human in Table 1, which shows Cap3D is\nbetter across metrics, with high confidence. The top three rows of Table 3 use the smaller human-\ncaptioned split (2), and demonstrate Cap3D\u2019s superior performance over Objaverse metadata and\nhuman-authored captions across A/B studies and automated metrics. The bottom three rows of\nTable 3, studied across a random split of the full dataset (3), reveal that while BLIP2 is effective,\nincorporating multiple views with +GPT4 enhances performance. As shown in Figure 5, GPT4 adds\ndetail by consolidating view-specific information. Filtering using +CLIP (Cap3D) mitigates false\ndetails by purging subpar captions from GPT input. In addition to reducing errors, utilizing CLIP\nalso reduces GPT input captions from 40 to 8, effectively decreasing token numbers and facilitating a\ncost reduction from $15.33 to $4.18.\n5.2\nGeometry 3D Captioning on ABO\nDataset. We evaluate geometric captioning on a 6.4k object split from ABO [35, 77], comparing\nCap3D captions for each object against a maximum of two human-authored ones. To emphasize\ngeometric focus, images used for model input and human assessment are texture-free and colorless.\nBaselines and Metrics. We use two automated variants from \u00a73.1: Cap3D and Cap3D (QA), which\nuses a two-stage prompt captioning to ask more about the input 3D geometry; and compare to\ncrowdsourced human descriptions, Human, detailed in \u00a74.1, and ABO metadata, Meta.\nOur primary metric of comparison is similar human A/B testing to \u00a75.1, since automated metrics\nsuch as CLIPScore do not accurately represent the distance between fine-grained captions and images\nas shown in [77].\nResults. In stark contrast to Objaverse, Human captions beat automated (Cap3D) in Table 4.\nAutomated captions alone contain little geometric detail (e.g., Figure 4), making Cap3D unsuited\nfor this setting. However, by using the two-stage prompt engineering, Cap3D (QA) is preferred to\nHuman. Shown in Figure 4, Cap3D (QA) produces significant fine-grained geometric detail as well\nas longer captions in general. In contrast, Metadata is clearly the weakest baseline.\n7\nTable 4: ABO Fine-Grained Geometry Cap-\ntions.\nCap3D (QA) performs best; crowd-\nsourced beats captioning alone.\nMethod\nA/B\nA/B\nA/B\nScore (1-5)\nWin %\nLose %\nHuman v. Cap3D\n3.09\u00b10.02\n47.3\u00b11%\n41.4\u00b11%\nCap3D(QA) v. Human\n3.08\u00b10.02\n50.2\u00b11%\n44.0\u00b11%\nCap3D(QA) v. Cap3D\n3.27\u00b10.02\n56.0\u00b11%\n37.4\u00b11%\nCap3D(QA) v. Meta\n4.27\u00b10.02\n88.2\u00b11%\n10.0\u00b11%\nTable 5: Text-to-3D: Human Captions. Cap3D\ncaptions are better than human on the 30k set.\nFinetuning on Cap3D full set performs best.\nFinetune\nFID\u2193\nCLIP\nCLIP R-Precision (2k)\nDataset\nScore\nR@1\nR@5\nR@10\nPoint\u00b7E\nPretrained\n36.1\n72.4\n6.0\n16.2\n22.4\n30k (Human)\n34.6\n74.4\n8.2\n21.3\n29.1\n30k (Cap3D)\n33.7\n75.0\n10.4\n24.3\n32.1\n350k (Cap3D)\n32.8\n75.6\n12.4\n28.1\n36.9\nShap\u00b7E\nPretrained\n37.2\n80.4\n20.3\n39.7\n48.7\n30k (Human)\n36.0\n79.6\n18.6\n36.3\n45.3\n30k (Cap3D)\n37.2\n79.4\n19.1\n37.5\n46.1\n350k (Cap3D)\n35.5\n79.1\n20.0\n38.8\n47.3\n5.3\nLarge-Scale Text-to-3D Generation\nDataset. We evaluate text-to-3D generation on three subsets of Objaverse: (1) a 30k split of objects\ncontaining human-authored captions, to measure if finetuning on Cap3D captions outperform human-\nauthored ones; (2) a 350k split of Objaverse objects paired with Cap3D captions, for finetuning\nstate-of-the-art text-to-3D methods \u2013 obtaining high-density point cloud and latent codes to finetune\nPoint\u00b7E and Shap\u00b7E for all 660k objects is prohibitively expensive (20k GPU days); and (3) a 300\nobject split for optimization-based baselines, which typically take >30 mins per object to optimize.\nPretrained and Finetuned models are evaluated on 8 views across a held-out test set of 2k objects.\nMethods. We consider several recent SOTA methods in three general categories: text-to-3D diffusion,\ncascaded text-to-image then image-to-3D diffusion, and optimization-based. We use the direct\ntext-to-3D variant of Point\u00b7E [88], as well as two variants of Shap\u00b7E [89]: STF [101] and NeRF [102].\nWe use Stable Diffusion cascaded with Point\u00b7E (Im-to-3D), adapting ControlNet [63] and LoRA [103]\nfor Stable Diffusion finetuning. We use optimization-based baselines DreamField [84], the publicly\navailable implementation of DreamFusion [3], Stable DreamFusion [104]; and 3DFuse [105], using\ntheir implementation based on Karlo [24, 106].\nMetrics. We use standard metrics from prior work [3, 84, 88, 89] to evaluate. Primarily, these are\nCLIP Score and CLIP R-Precision. CLIP R-Precision ranks a rendered image against all text pairs in\nthe test set by CLIP cosine similarity, and computes precision upon true text-image correspondence.\nSince we have ground truth images, we calculate the FID [107] of 3D rendered images against ground\ntruth images, as well as assess CLIP Score on these reference images. We also use ViLT Retrieval\nR-Precision, used in 5.1, which has the same evaluation procedure as CLIP R-Precision with a\ndifferent model.\nResults. Table 5 lists the results of finetuning using human-authored and Cap3D captions. Point\u00b7E\nimproves after finetuning upon human captions. However, performance is further improved using our\ncaptions on the same dataset; and improved most by training upon the full dataset. This result strongly\ndefends Cap3D captioning at scale. Shap\u00b7E does not improve on CLIP metrics after finetuning in any\ndataset, but performs the least bad on the full dataset using our captions; and FID improves most.\nTable 6 presents results from several state-of-the-art pretrained and finetuned models using Cap3D-\ngenerated captions. The models finetuned on our captions generally outperform pretrained models\nunder the FID metric. For CLIP-related metrics, the finetuned models of Point\u00b7E (Text-to-3D) and\nStableDiffusion + Point\u00b7E (Im-to-3D) also beat their pretrained counterparts. Point\u00b7E and Stable\nDiffusion have been trained on massive datasets, so improvement from finetuning is strong evidence\nCap3D captions are effective. The observed downturns in Shap\u00b7E could be attributed to at least two\nfactors. First, our replication of their privately-available train code is unstable, often resulting in NaN\nloss during finetuning. We restart from earlier checkpoints upon crashing, but the result alone is\nconcerning. Second, we exclusively finetune the diffusion model in Shap\u00b7E\u2019s two-stage approach.\nQualitative results in Figure 6 validate quantitative findings. Point\u00b7E and Stable Diffusion baselines\nshow large improvements from finetuning, while Shap\u00b7E can better fit the Objaverse data distribution\n(corresponding to improved FID).\n8\nTable 6: Text-to-3D on Objaverse. Finetuning improves FID over pretrained performance across\nmodels. CLIP metrics of Stable Diffusion increase; CLIP metrics of Point\u00b7E increase significantly.\nPretrained\nFinetuned on Cap3D\nFID\u2193\nCLIP\nCLIP R-Precision (2k)\nFID\u2193\nCLIP\nCLIP R-Precision (2k)\nScore\nR@1\nR@5\nR@10\nScore\nR@1\nR@5\nR@10\nGround Truth Images\n-\n81.6\n32.7\n55.1\n64.3\n-\n81.6\n32.7\n55.1\n64.3\nPoint\u00b7E (Text-to-3D) [88]\n36.1\n72.4\n6.0\n16.2\n22.4\n32.8\n75.6\n12.4\n28.1\n36.9\nS. Diff. [22] (CNet) [63]+ [88](Im-to-3D)\n54.7\n73.6\n11.0\n23.4\n30.0\n53.3\n74.6\n12.4\n26.2\n33.8\nS. Diff. [22] (LoRA) [103]+ [88](Im-to-3D)\n54.7\n73.6\n11.0\n23.4\n30.0\n53.7\n74.4\n11.6\n24.6\n31.4\nShap\u00b7E [89] (STF) [101]\n37.2\n80.4\n20.3\n39.7\n48.7\n35.5\n79.1\n20.0\n38.8\n47.3\nShap\u00b7E [89] (NeRF) [102]\n48.7\n79.4\n19.0\n37.7\n46.8\n48.2\n78.1\n18.3\n35.1\n43.5\nA 3D white skateboard ramp model.\nA 3D model of a green teapot with horns.\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\nFigure 6: Text-to-3D results. Finetuning on Cap3D captions can significantly improve results.\nTable 7: Text-to-3D: Optimization Baselines.\nOverfitting via CLIP leads to higher CLIP-based\nscores than ground truth; ViLT score is more fair.\nFID\u2193\nCLIP\nViLT\nScore\nR@1\nR@5\nR@1\nR@5\nTrue Images\n-\n83.2\n53.2\n77.8\n41.3\n69.0\nD. Field [84]\n106.1\n83.7\n61.8\n83.6\n32.3\n56.0\nD. Fusion [3]\n127.8\n72.4\n28.4\n46.1\n23.7\n45.3\n3DFuse [105]\n93.4\n75.8\n38.8\n59.5\n24.7\n51.0\nOptimization baselines, shown in Table 7, per-\nform very well upon CLIP-based metrics, con-\nsistent with prior work [89]. In fact, DreamField\noutperforms ground truth images in CLIP met-\nrics. This demonstrates DreamField overfits to\nthe CLIP metric, which is the standard protocol\nfor text-to-3D evaluation. We propose to also\nconsider ViLT precision (see \u00a75.1). This helps\nmitigate the bias of CLIP, though DreamField\nperformance on this metric is still strong.\n6\nConclusion\nIn this work, we collect (1) 3D object captions at scale, creating the largest publicly available high-\nquality 3D-text by an order of magnitude. To do so we propose Cap3D, an automated pipeline\nleveraging several models pretrained on large datasets, and show design choices are important to\nperformance. In addition, we collect (2) a dataset of geometric captions upon fine-grained 3D\nobjects. This helps analyze shortcomings of automated captioning and study the potential of question\nanswering, while yielding geometric descriptions for 3D assets of real objects paired with real images.\nThese datasets serve as benchmarks for text-to-3D tasks (1) at scale and (2) in geometric detail.\nAcknowledgments and Disclosure of Funding\nThis work is supported by two grants from LG AI Research and Grant #1453651 from NSF. We\ngreatly thank Kaiyi Li for his technical support. We thank Mohamed EI Banani, Karan Desai, and Ang\nCao for their helpful discussions. Thanks Matt Deitke for helping with Objaverse-related questions.\n9\nReferences\n[1] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\n[2] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz. 3dgen: Triplane latent diffusion\nfor textured mesh generation. arXiv, 2023.\n[3] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv, 2022.\n[4] Yuk Ming Tang and Ho Lun Ho. 3d modeling and computer graphics in virtual reality. In Mixed Reality\nand Three-Dimensional Computer Graphics. IntechOpen, 2020.\n[5] Rick Parent. Computer animation: algorithms and techniques. Newnes, 2012.\n[6] Afsoon Afzal, Deborah S Katz, Claire Le Goues, and Christopher S Timperley. A study on the challenges\nof using robotics simulators for testing. arXiv preprint arXiv:2004.07368, 2020.\n[7] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn,\nChen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for\nembodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning,\npages 80\u201393. PMLR, 2023.\n[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open\nurban driving simulator. In Conference on robot learning, pages 1\u201316. PMLR, 2017.\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d\nobjects. 2023.\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data\ncreated by the people, for the people. NeurIPS, 2021.\n[11] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\n[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n[13] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered\n400 million image-text pairs. arXiv, 2021.\n[14] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. 2022.\n[15] Mohamed El Banani, Karan Desai, and Justin Johnson. Learning Visual Representations via Language-\nGuided Sampling. In CVPR, 2023.\n[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021.\n[17] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-\nimage pre-training. In ECCV, 2022.\n[18] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. NeurIPS, 2022.\n[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In ECCV, 2022.\n10\n[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. 2022.\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022.\n[23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. CoRR, 2021.\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv, 2022.\n[25] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple\nvisual language model pretraining with weak supervision. ICLR, 2022.\n[26] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In\nECCV, 2020.\n[27] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\n[28] Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. Shapecaptioner: Generative caption\nnetwork for 3d shapes by learning a mapping from parts detected in multiple views to sentences. In ACM\nMM, 2020.\n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv, 2023.\n[30] OpenAI. Gpt-4 technical report, 2023.\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n[32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. IJCV, 2017.\n[33] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. NeurIPS, 2011.\n[34] https://commoncrawl.org/the-data/.\n[35] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang,\nTomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik.\nAbo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022.\n[36] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR,\n2018.\n[37] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In CVPR, 2017.\n[38] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.\n[39] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet:\nModular attention network for referring expression comprehension. In CVPR, 2018.\n[40] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for\nimage-text matching. In ECCV, 2018.\n[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, 2012.\n11\n[43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI 2015, 2015.\n[44] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 1997.\n[45] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. transactions on Signal\nProcessing, 1997.\n[46] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In ICCV, 2019.\n[47] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. 2014.\n[48] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing\nand improving the image quality of stylegan. In CVPR, 2020.\n[49] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017.\n[50] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, 2021.\n[51] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS,\n2021.\n[52] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In\nCVPR, 2021.\n[53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\n[54] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\n[55] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33, 2020.\n[56] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. NeurIPS, 2022.\n[57] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In\nICML, 2021.\n[58] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nNeurIPS, 2019.\n[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In ICML, 2015.\n[60] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv, 2023.\n[61] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. arXiv, 2021.\n[62] OpenAI. Gpt-4 technical report. arXiv, 2023.\n[63] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv, 2023.\n[64] Justin N. M. Pinkney. Pokemon blip captions. https://huggingface.co/datasets/lambdalabs/\npokemon-blip-captions/, 2022.\n[65] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih\nPorikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding.\narXiv, 2023.\n[66] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML, 2022.\n12\n[67] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Car-\nlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, image and point\ncloud for 3d understanding. CVPR, 2023.\n[68] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio\nSavarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.\narXiv, 2015.\n[69] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B\nTenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling.\nIn CVPR, 2018.\n[70] Joseph J Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In\nICCV, 2013.\n[71] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao.\n3d-future: 3d furniture shape with texture. IJCV, 2021.\n[72] Panos Achlioptas, Judy Fan, X.D. Robert Hawkins, D. Noah Goodman, and J. Leonidas Guibas. Shape-\nGlot: Learning language for shape differentiation. CoRR, 2019.\n[73] Kevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang, Thomas Funkhouser, and Silvio\nSavarese. Text2shape: Generating shapes from natural language by learning joint embeddings. In ACCV,\n2019.\n[74] Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. Shapecrafter: A recursive\ntext-conditioned 3d shape generation model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https:\n//openreview.net/forum?id=KUOKpojFr_.\n[75] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner.\nScannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017.\n[76] Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d\nscans using natural language. In ECCV, 2020.\n[77] Tiange Luo, Honglak Lee, and Justin Johnson.\nNeural shape compiler: A unified framework for\ntransforming between text, point cloud, and program. Transactions on Machine Learning Research, 2023.\nISSN 2835-8856. URL https://openreview.net/forum?id=gR9UVgH8PZ.\n[78] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image\ncaption generator. In ICML, 2015.\n[79] Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X. Chang. Scan2cap: Context-aware dense\ncaptioning in rgb-d scans. In CVPR, 2021.\n[80] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-\nmal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In CVPR, 2022.\n[81] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d\ncompletion, reconstruction and generation. In CVPR, 2022.\n[82] Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and Kim-Hui Yap. Taps3d: Text-guided 3d textured\nshape generation from pseudo supervision. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16805\u201316815, 2023.\n[83] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representa-\ntion for neural fields and generative diffusion models. arXiv preprint arXiv:2301.11445, 2023.\n[84] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In CVPR, 2022.\n[85] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv, 2023.\n[86] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309,\n2023.\n13\n[87] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu,\nJuan Carlos Niebles, and Silvio Savarese. Ulip-2: Towards scalable multimodal pre-training for 3d\nunderstanding. arXiv, 2023.\n[88] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for\ngenerating 3d point clouds from complex prompts. arXiv, 2022.\n[89] Alex Nichol and Heewoo Jun. Shap-e: Generating conditional 3d implicit functions. arXiv, 2023.\n[90] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5826\u20135835, 2021.\n[91] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview\ncompressive coding for 3d reconstruction. arXiv, 2023.\n[92] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv, 2023.\n[93] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong\nWang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. CVPR,\n2023.\n[94] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv,\n2022.\n[95] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In ICLR, 2020.\n[96] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-\nshot multi-level face localisation in the wild. In CVPR, 2020.\n[97] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\n[98] Gant Laborde. Deep nn for nsfw detection. https://github.com/GantMan/nsfw_model. [Online;\naccessed 7-May-2023].\n[99] https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.\n[Online; accessed 7-May-2023].\n[100] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution\nor region supervision. In ICML, 2021.\n[101] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic,\nand Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images.\nNeurIPS, 2022.\n[102] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n[103] Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models, 2021.\n[104] Jiaxiang\nTang.\nStable-dreamfusion:\nText-to-3d\nwith\nstable-diffusion,\n2022.\nhttps://github.com/ashawkey/stable-dreamfusion.\n[105] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim,\nJiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust text-to-3d\ngeneration. arXiv, 2023.\n[106] Donghoon Lee, Jiseob Kim, Jisu Choi, Jongmin Kim, Minwoo Byeon, Woonhyuk Baek, and Saehoon\nKim. Karlo-v1.0.alpha on coyo-100m and cc15m. https://github.com/kakaobrain/karlo, 2022.\n[107] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017.\n14\nAppendix A\nPrice Breakdown Details\nThis section provides our details computation for Table 1. Using a single A40 GPU, BLIP2 runs at\n\u223c 2700 iterations per hour, enabling it to process around \u223c 337.5 objects hourly given the eight-run\nrequirement for generating captions for 8 rendering views. This translates to about 2.96 hours to\nprocess 1k objects, costing 2.96 \u00d7 $1.28 = $3.79 with the rate $1.28/hr on the cloud platform,\nCoreWeave. On the same A40 GPU, CLIP operates at \u223c 27000 iterations per hour, incurring a cost\nof $0.38. Importantly, utilizing eight A40s costs the same as using one, due to the parallel processing\ncapacity across multiple GPUs for multiple rendering views.\nWe compute our GPT4 cost by averaging input token numbers, as OpenAI GPT4 API (8k context)\ncosts 0.03/1k tokens, Our input prompt is: \u201cGiven a set of descriptions about the same 3D object,\ndistill these descriptions into one concise caption. The descriptions are as follows: \u2018captions\u2019. Avoid\ndescribing background, surface, and posture. The caption should be:\", which consists of (1) text\nprompt and (2) captions generated by BLIP2 or BLIP2 + CLIP. Without CLIP\u2019s filtering, our input\nprompt contains 40 captions which have \u223c 511.1 tokens on average, cost 511.1/1000\u00d70.03\u00d71000 =\n$15.33 for 1k objects. With CLIP, our input prompt contains 8 captions which have \u223c 139.3 tokens\non average, cost 139.3/1000 \u00d7 0.03 \u00d7 1000 = $4.18 for 1k objects.\nThe average cost per 1k objects for human-authored annotation is computed as the average expenditure\non the crowdsourcing platform, Hive. The human annotation speed is computed by averaging the\nannotation progress across our whole annotation process.\nWe do not report the average cost of Cap3D (QA) in the main paper, as we only use it on ABO. For\ncompleteness, we report it here. The one distinction is BLIP2 is run twice instead of once for the\ntwo-stage question answering (QA). The cost of BLIP2 thus doubles, from $3.79 to $7.58; and total\ncost increases from $8.35 to $12.14 per 1k objects.\nAppendix B\nAdditional 3D Captioning Results\n3D low poly model of a \nsoldier/warrior with a sword \nand shield.\n3D model of a red and white \nmolecule.\nA 3D model of a helmet with a \nred, yellow, and black gas \nrespirator mask and a red light.\nA 3D model of a red and white \nmushroom.\nA 3D rendering of a spiral \nstaircase with a railing in a \nwhite room with an open door.\nA red and blue leather \nsuitcase with a cross, \nresembling an old medical \nbag or first aid box.\nz\nA 3D model of a Rubik's Cube \nfeaturing blue, orange, red, \ngreen, and yellow squares.\nA 3D model of a robotic horse \nwith wings and spikes.\n3D model of a green and \nyellow metal truss with two \nholes and cross beam.\nA 3D model of a house with \na red roof, fence, and \ncarousel.\nA white 3D printed figurine of \nSanta Claus with reindeer \nantlers, holding a stick and \nstanding on a rock.\nL-shaped sectional sofa with a \nchaise, U-shaped backrest, \ncurved armrests, and a \nfootstool on one side.\nFigure 7: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\n15\nUFC card featuring a shirtless \nman in red and white design.\n3D white submarine model \nwith a gun and pole feature.\n3D Wooden Barrel Model\nA 3D model of a stone mask \nsculpture resembling a broken \npiece of ice.\nA 3D ornate picture frame with \na light bulb in it.\n3D rendering of a house with \na roof\nz\n3D white satellite model with \nantenna\nA set of three colorful 3D \nmodel houses with roofs.\n3D model of Earth in a circle \nshape.\nA white 3D figurine of a bird \non top of a shoe.\nStar Wars Boba Fett 3D Model\n3D model of a pink plastic \nchair\nFigure 8: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\nA 3D model of a white robot.\nA 3D model of a green, \narmored lizard wearing a \ncrown and holding a sword.\nA 3D model of a large white \nrock, possibly marble or \ngranite, with a flag on it.\nA 3D model of a statue of a \nman with a hole in the ground.\nA 3D rendering of a row of \nvending machines and various \ncolored boxes.\nA 3D model of a small flying \nrobot-spaceship hybrid with \nextended arms, featuring an \nalien and a man on it.\nz\n3D wooden bear statue model\n3D printed kookaburra model \nsitting on a branch.\nA 3D yellow table with a cup \nof coffee on top.\nA 3D model of a white \ncylindrical object with \nfeatures resembling a \nradiator, vase, and light bulb.\n3D illustration of a small \nyellow flower with leaves.\n3D model of Transformers \nOptimus Prime blue and red \ntruck\nFigure 9: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\n16\nA 3D rendering of a house with \na roof structure featuring pink \nlines.\nA white doily with a floral \npattern and a circular ceiling \nlight with a flower design.\n3D rendering of a white square \nobject\n3D model of a wooden park \nbench with leaves on it.\nA 3D model of a modern blue \nleather sofa with metal legs and \na gray metal shelf with two \nhooks.\n3D model of a green pea pod \nand propeller with two peas.\nz\n3D model of a marble nativity \nscene figurine.\nA 3D white skull model with \nred eyes, resembling a \ncombination of animal, squid, \nand starfish features.\n3D model of a cow\nWhite Nintendo Wii console \nwith a power outlet and USB \nport.\nA yellow gold ring with a pink \nsapphire stone.\nA 3D scene featuring a \ndestroyed house, building, \nplane, and car, with a flying \nbird.\nFigure 10: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\n3D model of a sword/dagger\nA white 3D model of a teddy \nbear with arms outstretched.\nA 3D model of a tall high-rise \nbuilding.\nA 3D model of a robot with a \ntable and satellite.\n3D model of a green and \nyellow box with a hole in it.\nA 3D model of a twisted, \ncolorful object resembling a \nball with purple, pink, and \nblue squiggles and wires.\nz\n3D model of a baseball bat\nWhite Aston Martin DB5 3D \nModel\nA 3D model featuring pillars, a \nfountain with statues, three \nvases, pots on a table, valves, a \npipe, and light fixtures.\nA 3D model of a red \nshipping container with a \nblue and white logo and \nwhite label on it.\n3D model of a \ngravestone/tombstone.\nBrown bag with coins and a \ngreen string, featuring a coin \nfalling out.\nFigure 11: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\n17\nA 3D model of a colorful, \nmulti-colored boat with a flag, \noars, and paddles.\nA purple and yellow toy \nlantern with a handle.\nA 3D model of a wooden door \nwith a design, featuring a \nwooden sign, curved wall, and \nclock.\nA 3D model featuring a bird, \ntwigs, fishing poles, a \ndragonfly, and skis with poles.\nA 3D blue box with a hole and \nsand on it.\n3D model of a small white \nand blue toy robot teddy \nbear.\nz\n3D model of a cricket bat, \nroyalty-free vector illustration.\nA 3D rendering of a row of \nyellow poles.\nWhite 3D spiral staircase model\n3D model of a white axe\nWooden coat rack in a 3D \nmodel.\nA 3D model of a green plant \nwith leaves and an eye.\nFigure 12: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\nA 3D model of an armored \nhand wearing gloves with a \nleather strap and metal cuff.\n3D model of a white cloud.\nWhite plastic ring\n3D model of a small white \nbuilding with stairs, featuring a \ncube and ceiling light fixture.\n3D model of a wooden power \npole with wires.\n3D model of a white plastic \nbottle with a lid.\nz\nA 3D model of a small boat \nand house on a yellow \nplatform.\nA knife and sliced bread on a \nwooden cutting board.\nA 3D model of a two-story \nhouse with a roof structure.\nA 3D rendering of a fish \nnext to a Rubik's cube.\nA 3D model of a house on a \ngrassy field with a road in \nfront.\n3D model of a flower in a \nblack box with a purple \ncontainer and blue corner shelf.\nFigure 13: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Obja-\nverse [9]) are shown here, Cap3D uses eight.\n18\n3D model of a boy in a black \noutfit holding a sword with \narms outstretched.\n3D model of a Qantas Airbus \nA380 airplane.\nA 3D model of black and white \nsneakers with a blue logo and a \ncat character, featuring white \nsoles.\nWhite Nike Air Max Uptempo \nbasketball sneaker 3D model.\nFirst Character in 3Ds max, \nStudent Work\nbong 787\nAdidas scanned by Thunk3D \nhandheld scanner, sampling at \nratio 40%.  For more \ninformation, please kindly \ncheck as below, Whatsapp:...\na cartoon kid with clothes black \nin 3d\na cartoon white commercial \nairplane in 3d\na grey shoe with a white front\ngrey coloured sports shoe with \nundulations on its surface.\nCap3D\nHuman\nMetadata\nFigure 14: Comparative Analysis: Cap3D Generated Caption vs Human-Annotated Caption vs\nObjaverse Metadata [9]. Two views of 3D objects are shown here, Cap3D and human use eight.\n3D model of a jar with a green \nlid.\n3D rendering of grey \nChampion sweatpants with \nred and black logo.\nA 3D model of a rusty, old \ntrain engine.\nThis is a backup of a Poly Asset \nnamed Jar of jam. Saved from Poly by \nGoogle. Preview may be without \ntextures, they are still in the Download \nZIP with a preview thumbnail.\na three layer structure with green \noval top and white middle part and \nalso having a brown base.\na 3d model of a train old \nengine.\na carton made antelope with \nhorns horns on top of each \nother\nCap3D\nHuman\nMetadata\nOld industrial diesel locomotive \nfrom Hungary.\nA 3D model of a white deer.\na cartoon white pants\nGame resolution Jogger pants made in the likeness of a \npair of Joggers made by Champion. I started in Zbrush \nwith the high res and exported to Maya for quad drawing \nthe game res.. The model was also UV mapped and \ntextured by myself. The texture was baked and finalized in \nSubstance Painter. The poly count is just over 5200 tri's to \nfit the topology of the high res that was decimated from \napproximately 2,400,000 to around 415,000 quads.\nFigure 15: Comparative Analysis: Cap3D Generated Caption vs Human-Annotated Caption vs\nObjaverse Metadata [9]. Two views of 3D objects are shown here, Cap3D and human use eight.\nAppendix C\nAdditional Text-to-3D Results\nIn this section, we provide several text-to-3D results for all of our compared methods. We include\nShap\u00b7E and Point\u00b7E pretrained models and the models finetuned on our data, as well as optimization\nbaselines, including DreamFusion, DreamField, and 3D Fuse.\n19\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\nDream Fusion\nDream Field\n3D Fuse\nReference\nA 3D model of a green teapot with horns.\nFigure 16: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\n3D Fuse\nDream Fusion\nDream Field\nReference\nA red and black table lamp with a black shade.\nFigure 17: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\n3D Fuse\nDream Fusion\nDream Field\nA 3D model of an axe featuring a dragon head, a sword, and a long, colorful handle.\nReference\nFigure 18: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\n20\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\n3D Fuse\nDream Fusion\nDream Field\n3D model of a Five Nights at Freddy's fox character with outstretched arms, wearing a hat and holding a gun.\nReference\nFigure 19: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\n3D Fuse\nDream Fusion\nDream Field\nA 3D model of a mountain range with green grass and mountainous terrain.\nReference\nFigure 20: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\nFinetuned\nShap\u00b7E NeRF\nShap\u00b7E STF\nPretrained\nPoint\u00b7E \n(Text-to-3D)\nS.Diff(LoRA) +\nPoint\u00b7E (Im-to-3D)\nS.Diff(CNet) + \nPoint\u00b7E (Im-to-3D)\nDream Fusion\nDream Field\n3D Fuse\nReference\n3D model of witch hats and lanterns hanging from a chain.\nFigure 21: Text-to-3D results. The top text prompt and \u201cReference\" are from our test set. We\nfine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods\nare described in \u00a75.3.\n21\nAppendix D\nLimitations and Failure Cases\nAs described in \u00a73, Cap3D consists of four steps: (1) 3D objects rendering; (2) captioning via BLIP2;\n(3) filtering captions via CLIP; (4) consolidate multiview information via GPT4. To effectively\ncapture comprehensive information through 2D renderings, our cameras are positioned above or\nbelow the object. However, this sometimes leads to unusual 2D views, which cause the BLIP2 to\nproduce inaccurate information that CLIP cannot filter. Consequently, GPT4 struggles to consolidate\ndisparate information from multiple views, leading to ambiguous, verbose, and imprecise descriptions.\nOne example is shown in Figure 22. Moreover, our system struggles to accurately process certain\nindoor 3D scans due to their inherent complexity (as shown in Figure 23), making them challenging\nto distinguish, sometimes even for humans.\nNote that, none of a caption from a single view can well describe the complete details from the given\n3D object.\na 3d model of a \ndragon on a rock\na 3d model of a \nbutterfly on a rock\na 3d model of a \nrock formation \nwith a dragon on it\na 3d model of a \nblue dragon on a \nrock\na 3d model of a rock \nwith a blue and \nyellow blob on it\na 3d model of a \ndragon with rocks \nand water\na 3d model of a rock \nwith a girl sitting on \nit\na 3d rendering of a \nrock with flowers \non it\nGPT4\nPrompt: Given a set of \ndescriptions about the same \n3D object \u2026 distill these \ndescriptions into one concise \ncaption:\nA 3D model of a blue dragon \non a rock formation with \nsurrounding elements like a \nbutterfly, girl, flowers, and \nwater.\nOutput Caption\nFigure 22: An failed case. The caption under each rendered image are generated by BLIP2 + filtered\nby CLIP. The inaccurate content are highlighted with colors. GPT4 + CLIP cannot fix the error\ngenerated by BLIP2 and result in a fuzzy description.\na 3d model of a \nhouse with a hole \nin it\na model of a dump \ntruck on a gray \nbackground\na 3d model of a \nhouse with a \nwindow\na 3d model of a \nboat in the \nmiddle of a field, \na 3d model of a mud \nhouse\na 3d model of a \nwooden boat\na 3d model of a \nrusted car on a gray \nbackground\na 3d model of a torn \npiece of wood\nGPT4\nPrompt: Given a set of \ndescriptions about the same \n3D object \u2026 distill these \ndescriptions into one concise \ncaption:\nA 3D model featuring a house \nwith a hole and window, a boat \nin a field, a mud house, a dump \ntruck, a wooden boat, and a \nrusted car.\nOutput Caption\nFigure 23: An failed case. The caption under each rendered image are generated by BLIP2 + filtered\nby CLIP. The inaccurate content are highlighted with colors. The various views contain inaccurate\ninformation. The associated details, roughly described, fail to accurately depict the indoor scene.\n22\nAppendix E\nABO Captioning: Automated Metrics\nIn \u00a75.2, we report human A/B judgments on ABO. We do not report automated metrics, which\nare poor measures of performance for at least two reasons. First, ABO contains a large number\nof objects that are very similar, meaning it would be challenging for captions to distinguish their\ndifferences. Thus, retrieval metrics such as ViLT Image or Text Retrieval will show very poor scores\nacross metrics. Second, we show automated captioning performs poorly at describing geometry well,\nmeaning it is likely automated image-caption alignment will not align based on geometry well. For\ncompleteness, we report automated metrics in Table 8. As expected, all retrieval scores are very low.\nAutomated captioning scores best across automated metrics, however we caution against drawing\nconclusions from this result. Human studies in Table 4 suggest the opposite, and qualitative results\nagree with this finding, e.g. Figure 4.\nTable 8: ABO Automated Caption Evaluations. Automated captions are a poor measure of\nperformance on ABO as (1) many objects are similar, making retrieval difficult; (2) automated\ncaptioning does not describe geometry well, so we should not expect automated image-caption\nalignment to describe geometrically correct captions well.\nMethod\nCLIP\nViLT Img Retr.\nViLT Text Retr.\nScore\nR@5\nR@10\nR@5\nR@10\nMeta\n61.9\n0.8\n1.7\n0.8\n1.7\nHuman\n75.2\n2.6\n4.4\n2.3\n4.2\nCap3D\n89.9\n4.2\n7.2\n3.2\n5.6\nCap3D(QA)\n82.7\n2.9\n5.3\n2.4\n4.3\nIn contrast with A/B tests, which take place on the full 6.4k objects of ABO, this table is computed\non a random 5k object subset of ABO to follow standard retrieval benchmarks (performance drops\nconsiderably as dataset size increases. Using 5k instead of the full 6.4k makes it much easier to\ncontextualize retrieval numbers). A/B performance on this 5k subset is very close to the full 6.4k\ndataset, meaning the sample is highly representative, and one can compare the results from this table\nin combination with Table 4 in the main paper.\nAppendix F\nAdditional Details\nF.1\nPrompt used in Cap3D\nThe two prompts used for BLIP2 used in Cap3D (QA) are (1) \u201cQuestion: what object is in this image?\nAnswer:\" and (2) \u201cQuestion: what is the structure and geometry of this <object>?\" where <object> is\nreplaced with the response to prompt (1).\nFor the prompt used in GPT4, we used \u201cGiven a set of descriptions about the same 3D object, distill\nthese descriptions into one concise caption. The descriptions are as follows: \u2019captions\u2019. Avoid\ndescribing background, surface, and posture. The caption should be:\". We did several prompt\nengineering and considered prompt with more context, like \u201cBelow you will find a set of descriptions,\neach one is originating from various renderings of an identical 3D object. The level of accuracy in\nthese descriptions ranges significantly: some might not correspond to the 3D object at all, others\ncould be entirely accurate, while a few may only partially represent the object. Your task involves\nscrutinizing these descriptions and distilling them into a single, holistic depiction. The descriptions\nare as follows: \u2018captions\u2019. Note: Please avoid using the phrases \u2019grey background\u2019, \u2019gray background\u2019,\nand \u2019gray surface\u2019 in your consolidated depiction. The synthesized description of the 3D object\nshould be:\". However, with those longer prompt with more context, we noticed GPT4 sometimes\nwould generate its reasoning process which led to confusing output captions. Also, for the sake of\ncost, we hope to make our prompt as short as possible.\nF.2\nRendering Details\nWe use Blender to render 3D objects in Objaverse [9] and ABO [35]. For each object, we first\nnormalize them into a unit cube and recenter to origin. Then, we place 8 different cameras surrounding\n23\nthe object with 2 cameras slightly below the object to capture the bottom of the object. Three area\nlights are placed and function as key light, fill light, and rim light, respectively. The detailed\nparameters are listed in our rendering script, provided in our Github.\nAccording to \u00a7 3.2, we filter objects in Objaverse based on commerical-license, rendering information,\nand ethicial standards, and results a subset of 660k objects for rendering and captioning. In ABO,\nwe exclude categories with simple geometry to concentrate on geometrical captioning, including\n\u201cBLANKET\", \u201cRUG\", \u201cWALL_ART\", \u201cPLACEMAT\", \u201cCURTAIN\", \u201cMOUSE_PAD\". This resulting\na final subset of 6.4k objects for rendering and captioning.\nF.3\nHuman Captioning Split\nHuman captions are collected on a manually selected subset of Objaverse with good renders of\nnontrivial but decipherable objects. These objects are likely to be the most sensible for captioning\nand A/B testing. For instance, some Objaverse objects are essentially a simple rock with little texture;\nin others it can be difficult for a human to describe an object (e.g. abstract art, no clear object visible,\nor 3D scans with hard-to-distinguish details). These excluded objects are generally not effective\nsamples to use for human A/B testing, as the correct caption may not be clear or may be trivial. We\nalso exclude furniture, which is suitable for captioning, but we measure this with more focus on ABO.\nHuman captions on ABO follow the split of [77].\nAppendix G\nCrowdsourced Captioning Details\nWe use Hive for crowdsourced captioning. Workers are given instructions for the task including\ngold-standard examples. Captioning instructions are shared below for Objaverse in Figure 24 and\nABO in Figure 25. Workers are persistently monitored. If a worker produces bad captions they\nare promptly banned from captioning, and their previous captions are discarded. Workers are paid\napproximately $50 per 1k tasks. We do not have access to their captioning rates; assuming a rate of 3\nobjects per minute, this would result in $9 per hour. Across Objaverse and ABO we spend a total of\n$7k on captioning.\n24\nFigure 25: ABO Caption Instructions.\n25\nFigure 24: Objaverse Caption Instructions.\n26\nAppendix H\nCrowdsourced A/B Testing Details\nWe use Hive for crowdsourced A/B testing. Specifically, workers are given an image and two captions,\nand select which is better on a scale from 1 to 5, where 3 is a tie. So 1 would be \"left much better\",\nand 2 would be \"left better\". Workers are given instructions for the task along with gold standard\nexamples. Workers are informed to prioritize accuracy, then informative detail, then brevity. Left/right\norder between methods was randomized for each instance. A/B Testing instructions are shared below\nfor Objaverse in Figure 27 and ABO in Figure 26.\nWorkers are automatically banned by the platform if they miss too many gold-standard examples.\nHowever, we found some workers would successfully pass the handful of gold-standard examples\nwhile scamming on the rest of the examples. The most common scam cases were always picking the\nsame number, or always picking the shorter or longer caption. We thus manually search through all\nworkers and ban workers who meet these scamming criteria and discard their judgments. Unfortu-\nnately, discarding judgments leads to uneven numbers of observations for each individual experiment.\nNevertheless, in all cases, enough observations are available to draw conclusive findings.\nThe size of each experiment\u2019s data after discarded judgments is below.\n\u2022 Objaverse Split (1) takes place on a random set upon which human captions are available.\nCap3D vs. Human has 36k observations across 22k objects.\n\u2022 Objaverse Split (2) takes place on a random object set upon which human captions are\navailable. Cap3D vs. Human has 10k observations across 4.7k objects. Cap3D vs. Metadata\nhas 7k observations across 4.7k objects (less than the target 10k), though given the extremely\npoor rating of Metadata, results are conclusive.\n\u2022 Objaverse Split (3) takes place on a random object set upon the entire Objaverse dataset.\nCap3D vs. BLIP2 has 20k observations across 5.0k objects and Cap3D vs. +GPT4 has 29k\nobservations across 5.0k objects.\n\u2022 ABO takes place on the full ABO object set. Human vs. Cap3D has 21k observations across\n6.4k objects, Cap3D (QA) vs. Human has 17k observations across 6.4k objects, Cap3D\n(QA) vs. Cap3D has 13k observations across 6.4k objects, and Cap3D (QA) vs. Meta has\n12k observations across 6.4k objects.\nWorkers are paid approximately $20 per 1k tasks. We do not have access to their captioning rates;\nassuming a rate of 7.5 A/B tests selected per minute, this would result in $9 per hour. Across\nObjaverse and ABO we spent a total of $1.8k on A/B testing.\n27\nFigure 26: A/B Instructions: Objaverse Captions.\n28\nFigure 27: A/B Instructions: ABO Captions.\n29\nAppendix I\nAdditional Experimental Details\nCaptioning: we perform one full-scale evaluation run for all captioning experiments; 95% confidence\ninterval for mean is presented. Metrics are overviewed in \u00a75.1; A/B testing is detailed further in \u00a7H.\nCLIP Score takes about 5 minutes, while ViLT R-Precision takes about 8 hours using an A40 for test\nset of 5k object-caption pairs. Crowdsourced A/B testing takes about 12 hours for 10k responses\nacross 5k objects.\nText-to-3D, finetuning: for finetuning experiments, we used one train and evaluation run using a\nlearning rate validated on a small overfitting experiment on the train set. Training took about 3 days on\nthe full set and 1 day on the small (human) set. We used AdamW optimizer and CosineAnnealingLR\nscheduler with initial learning rate 1e \u2212 5 for finetuning both Point\u00b7E and Shap\u00b7E. We adopted batch\nsize 64 and 256 for Shap\u00b7E and Point\u00b7E, respectively. However, for Shap\u00b7E, we found it usually\noutputs NaN and needed to re-start from saved checkpoints, which could be one of the reaons why our\nfinetune did not bring improvements. For LoRA, we use AdamW optimizer and CosineAnnealingLR\nscheduler with initial learning rate 1e \u2212 4 and batch size of 3. For ControlNet, we use AdamW\noptimizer and constant learning rate of 1e \u2212 5 and batch size of 8. Experiments use 4 A40s to train\nexcept LoRA, which fails upon multi-gpu training due to a HuggingFace internal DDP error. Notably\nsingle-gpu training still yields improvement. Evaluation takes the following time (in seconds) per\niteration, which includes rendering:\n\u2022 PointE (text-to-3D): 37sec = 28sec (text-to-3D) + 9sec (render)\n\u2022 LoRA + PointE(im-to-3D): 114sec = 5sec + 100sec (im-to-3D) + 9sec (render)\n\u2022 ControlNet + PointE(im-to-3D): 124sec = 15sec + 100sec (im-to-3D) + 9sec (render)\n\u2022 ShapE (NeRF): 193sec (text-to-3D + render)\n\u2022 ShapE (stf): 16sec (text-to-3D + render)\nNote publicly available PointE (im-to-3D) is 1B param, making it slower than the largest publicly\navailable PointE (text-to-3D) of 40M. Evaluation metrics are detailed in \u00a75.3.\nText-to-3D, optimization: For one object, optimization plus final rendering takes 40 minutes for\n3DFuse, 95 minutes for Stable DreamFusion, and 35 minutes for DreamField; using 1 A40 GPU. We\nuse default parameters for all methods and run them once.\n30\n"
  },
  {
    "title": "Transformers learn through gradual rank increase",
    "link": "https://arxiv.org/pdf/2306.07042.pdf",
    "upvote": "9",
    "text": "Transformers learn through gradual rank increase\nEnric Boix-Adser`a1,2\nEtai Littwin1\nEmmanuel Abbe1,3\nSamy Bengio1\nJoshua Susskind1\n1Apple\n2MIT\n3EPFL\neboix@mit.edu,emmanuel.abbe@epfl.ch\n{elittwin,bengio,jsusskind}@apple.com\nDecember 12, 2023\nAbstract\nWe identify incremental learning dynamics in transformers, where the difference between\ntrained and initial weights progressively increases in rank. We rigorously prove this occurs\nunder the simplifying assumptions of diagonal weight matrices and small initialization. Our\nexperiments support the theory and also show that phenomenon can occur in practice without\nthe simplifying assumptions.\n1\nIntroduction\nThe transformer architecture achieves state of the art performance in various domains, yet we still lack\na solid theoretical understanding of its training dynamics [VSP+17, DCLT19, LOG+19, DBK+20].\nNevertheless, the theoretical toolbox has matured over the last years and there are promising new\napproaches. One important line of work examines the role that initialization scale plays on the\ntrajectory taken by gradient descent [JGH18, COB18, GSJW19, MGW+20, JGS+21, SS21, KC22].\nWhen the weights are initialized small, it has been shown for simple networks that an incremental\nlearning behaviour occurs, where functions of increasing complexity are learned in stages. This\nregime is known to be richer than the large-initialization regime1, but the incremental learning\ndynamics are difficult to analyze, and are so far understood only for extremely simple architectures.\nCan we apply this analysis to transformers? Namely:\nAre there incremental learning dynamics when training a transformer architecture?\nAn obstacle is that past work on incremental learning has mainly studied linear networks\n[Ber22, ACHL19, MKAA21, LLL20, WGL+19, JGS+21, GSSD19, SKZ+23, PF23], with one paper\nstudying nonlinear 2-layer fully-connected networks [BPVF22]. In contrast, transformers have\nnonlinear attention heads that do not fall under previous analyses: given X \u2208 Rn\u00d7d, an attention\nhead computes\nattention(X; WK, WQ, WV , WO) = smax(XWKW \u22a4\nQ X\u22a4)XWV W \u22a4\nO\n(1)\nwhere WK, WQ, WV , WO \u2208 Rd\u00d7d\u2032 are trainable matrices, and the softmax is applied row-wise. A\ntransformer is even more complex, since it is formed by stacking alternating layers of attention\nheads and feedforward networks, along with residual connections.\n1In the large-initialization regime, deep learning behaves as a kernel method [JGH18, COB18]. Various separations\nwith kernels are known for smaller initialization: e.g., [GMMM19, ABM22, MKAS21].\n1\narXiv:2306.07042v2  [cs.LG]  11 Dec 2023\nMain finding\nOur main finding is that transformers exhibit incremental learning dynamics, where\nthe difference between the trained and initial weights incrementally increases in rank. Our results\nhave a theoretical component and an experimental component.\nTheoretical contributions\nFor our theory, we study a simplification of the transformer archi-\ntecture, where the attention head weights are diagonal matrices: i.e., in each attention head we\nhave WK = diag(wK), where wK \u2208 Rd are trainable weights, and similarly for WQ, WV and WO.\nWe rigorously establish the training dynamics of this architecture under gradient flow when the\ninitialization is small. We prove that dynamics occur in discrete stages: (1) during most of each\nstage, the loss plateaus because the weights remain close to a saddle point, and (2) at the end, the\nsaddle point is quickly escaped and the rank of the weights increases by at most one.\nThis theoretical result on transformers follows from a general theorem characterizing the learning\ndynamics of networks fNN that depend on the product of parameters u, v \u2208 Rp as\nfNN(x; u, v) = h(x; u \u2299 v) ,\n(2)\nwhere x is the input, \u2299 denotes the elementwise product, and h is a smooth function.\nTheorem 1.1 (Informal statement of incremental learning dynamics). Let fNN be a network of\nthe form (2), and suppose that the weights are initialized very small: i.e., the entries of u, v are\ninitialized on the order \u0398(\u03b1) for some small \u03b1 > 0. Then the dynamics of gradient flow training\neffectively proceeds in discrete stages, each one lasting time \u0398(log(1/\u03b1)). In each stage, the number\nof nonnegligible entries of u \u2299 v increases by at most one.\nA transformer with diagonal weight matrices falls under this result when we only train the\nattention head weights. For example, if the transformer has one attention head, then we can take\nu = [wK, wV ] \u2208 R2d and v = [wQ, wO] \u2208 R2d to be concatenations of the diagonal entries of the\nweights of the head; see Example 3.2 for more details and the extension to transformers with many\nheads. Then, using Theorem 1.1, we see that in each stage either WKW \u22a4\nQ = diag(wK)diag(wQ) or\nWV W \u22a4\nO = diag(wV )diag(wO) increases in effective rank by at most one.2\nExperimental contributions\nIn our experiments, we first validate our theoretical results, which\nrequire the simplifying assumptions of small initialization and diagonal weight matrices.\nThen, we conduct experiments on vision and language transformers in settings closer to practice,\nwithout any of the assumptions required by our theoretical analysis. Perhaps surprisingly, we again\nobserve incremental learning dynamics, even though the assumptions of the theory are not met.\nThe difference between trained and initial weights has low rank, and the rank of this difference\ngrows gradually during training; see Figure 1. The incremental nature of the dynamics is easier to\nsee for ImageNet, since for CIFAR-10 the rank of the weight difference does not grow as much.\n1.1\nRelated work\nRelation to LoRA\nWe note an intriguing connection to the LoRA algorithm, where a pretrained\nbase model is cheaply fine-tuned by training a low-rank perturbation of the weights [LFLY18,\nAZG20, HSW+21]. The method is surprisingly powerful, and recently LoRA has been fundamental\nto allowing the open-source community to inexpensively fine-tune language models [PA23, TGZ+23].\n2We also remark that Theorem 1.1 is interesting in its own right and may have other applications beyond\ntransformers. It qualitatively recovers the incremental dynamics result of [Ber22, PF23] when specialized to linear\ndiagonal networks, i.e., when fNN(x; u, v) = Pp\ni=1 uivixi.\n2\n(a)\n(b)\nFigure 1: For an attention head in\nViT trained on (a) CIFAR-10, and\n(b) ImageNet, we plot the normal-\nized spectra of WKW \u22a4\nQ at initial-\nization (in red), and of the learned\nperturbations to WKW \u22a4\nQ at differ-\nent iterations (in green).\nOn the other hand, in our work we observe that the trained weights are a low-rank perturbation of\nthe initial weights due to the training dynamics, without having to apply an explicit rank constraint\nas in LoRA. This raises an exciting open question for future work: can we explain and improve\nalgorithms like LoRA by better understanding and quantifying the incremental dynamics of large\ntransformers?\nLow-rank bias in nonlinear networks\nFor 2-layer networks, it is known that low-rank bias\nin the weights emerges if the target function depends on a low-dimensional subspace of the input\n[ABM22, ABM23, DLS22, BBSS22, MHPG+22]. The results of [ABM22, ABM23] are especially\nrelevant, since they show that the rank of the weights increases in a sequential manner, determined\nby the \u201cleap complexity\u201d of the target function, which is reminiscent of our empirical observations\non transformers. See also [FVB+22, TVS23] for more investigations of low-rank bias in 2-layer\nnetworks under different assumptions. For transformers, [YW23] report that empirically the trained\nweights (using default initialization) are not low-rank. This is consistent with our claim that the\ndifference between initial and trained weights is low-rank, since the initial weights might not be\nlow-rank.\nIncremental learning dynamics\nSeveral works prove incremental learning behaviour in deep\nlinear networks when the initialization is small. [GBLJ19] has shown that gradient descent dynamics\non a 2-layer linear network with L2 loss effectively solve a reduced-rank regression problem with\ngradually increasing rank. [GSSD19] prove a dynamical depth separation result, allowing for milder\nassumptions on initialization scale. [ACHL19, MKAA21] show implicit bias towards low rank\nin deep matrix and tensor factorization. [LLL20] show deep matrix factorization dynamics with\nsmall initialization are equivalent to a greedy low-rank learning (GLRL) algorithm. And [JGS+21]\nindependently provides a similar description of the dynamics, but without requiring balanced\ninitialization. Finally, [Ber22, JLL+23, PF23] overcome a technical hurdle from previous analyses\nby proving incremental learning for the entire training trajectory, rather than just the first stage. In\ncontrast to our result, these prior works apply only to linear networks with certain convex losses,\nwhereas our result applies to nonlinear networks. In order to make our extension to nonlinear\nnetworks possible, we must make stronger assumptions on the training trajectory, which we verify\nhold empirically. As far as we are aware, one other work on incremental learning handles nonlinear\nnetworks: [BPVF22] proves that a 2-layer network learns with a two-stage incremental dynamic;\nbut that result needs the stylized assumption that all data points are orthogonal.\n1.2\nPaper organization\nSections 2, 3, and 4 contain theoretical preliminaries, definitions of the models to which our theory\napplies, and our main theoretical result on incremental dynamics. Section 5 provides experiments\nwhich verify and extend the theory. Section 6 discusses limitations and future directions.\n3\n2\nPreliminaries\nWe consider training a network fNN(\u00b7; \u03b8) parametrized by a vector of weights \u03b8, to minimize a loss\nL(\u03b8) = Ex,y[\u2113(y, fNN(x; \u03b8))] ,\nwhere the expectation is over samples (x, y) \u2208 Rdx \u00d7 Rdy from a training data distribution, and\n\u2113 : Rdy \u00d7 Rdout \u2192 R. Consider a solution \u03b8(t) to the gradient flow3\n\u03b8(0) = \u03b1\u03b80,\nd\u03b8\ndt = \u2212\u2207\u03b8L(\u03b8)\n(3)\nwhere \u03b1 > 0 is a parameter governing the initialization scale, that we will take small. For our theory,\nwe henceforth require the following mild regularity assumption on the loss and data.\nAssumption 2.1 (Regularity of data distribution and loss). The function \u2113(y, \u03b6) is continuously\ntwice-differentiable in the arguments [y, \u03b6] \u2208 Rdy+dout. There exists C > 0 such that almost surely\nthe data is bounded by \u2225x\u2225, \u2225y\u2225 \u2264 C.\nThe assumption on \u2113 is satisfied in typical cases such as the square and the cross-entropy losses.\nThe data boundedness is often satisfied in practice (e.g., if the data is normalized).\nWe also use the notation supp(a) := {i : ai \u0338= 0} to denote the support of a vector a.\n3\nNeural networks with diagonal weights\nOur theory analyzes the training dynamics of networks that depend on products of diagonal weight\nmatrices. We use \u2299 to denote elementwise vector product.\nDefinition 3.1. A network fNN is smooth with diagonal weights \u03b8 = (u, v) \u2208 R2p if it is of the\nform\nfNN(x; \u03b8) = h(x; u \u2299 v)\nwhere h : Rdx \u00d7 Rp \u2192 Rdout is continuously twice-differentiable in its arguments in Rdx+p.\nThe assumption on h precludes the use of the ReLU function since it is not continuously-\ndifferentiable. Otherwise the assumption is fairly mild since any h can be used to express an\narchitecture of any depth as long as the nonlinearities are twice-differentiable, which includes for\nexample GeLUs (as used in ViT). We describe how to express a transformer with diagonal weights.\nExample 3.2 (Transformer with diagonal weights). A transformer with L layers and H attention\nheads on each layer is defined inductively by Z0 = X \u2208 Rn\u00d7d and\n\u2022 (Attention layer) \u02dcZ\u2113 = Z\u2113\u22121 + PH\ni=1 attention(Z\u2113\u22121; W \u2113,i\nK , W \u2113,i\nQ , W \u2113,i\nV , W \u2113,i\nO )\n\u2022 (Feedforward layer) Z\u2113 = \u02dcZ\u2113 + \u03c3( \u02dcZ\u2113W \u2113\nA)(W \u2113\nB)\u22a4 ,\n3Gradient flow training can be obtained as a limit of SGD or GD training as the learning rate tends to 0 (see, e.g.,\n[Bac20]). It is a popular testbed for studying learning dynamics (see e.g., [SMG13, ACH18, RC20]), since is generally\nsimpler to analyze than SGD.\n4\nwhere W \u2113,i\nK , W \u2113,i\nQ , W \u2113,i\nV , W \u2113,i\nO\n\u2208 Rd\u00d7d\u2032 are attention parameters, and W \u2113\nA, W \u2113\nB \u2208 Rd\u00d7d\u2032 are the\nfeedforward parameters, and \u03c3 is a continuously twice-differentiable activation. Suppose that the\nattention parameters are diagonal matrices: i.e., W \u2113,i\nK = diag(w\u2113,i\nK ) \u2208 Rd\u00d7d, and similarly for the\nW \u2113,i\nQ , W \u2113,i\nV , W \u2113,i\nO\nmatrices. Then by the definition of the attention layer (1), the final output of\nthe transformer ZL only depends on the attention parameters through the elementwise products\nw\u2113,i\nK \u2299 w\u2113,i\nQ and w\u2113,i\nV \u2299 w\u2113,i\nO . In other words, we can write\nZL = h(X; u \u2299 v) ,\nfor vectors u = [w\u2113,i\nK , w\u2113,i\nV ](\u2113,i)\u2208[L]\u00d7[H] \u2208 R2dHL and v = [w\u2113,i\nQ , w\u2113,i\nO ](\u2113,i)\u2208[L]\u00d7[H] \u2208 R2dHL, and some\nsmooth model h. Thus, if only the attention layers are trained, the diagonal transformer fits under\nDefinition 3.1.\n4\nIncremental learning in networks with diagonal weights\nWe prove that if the initialization scale \u03b1 is small, then learning proceeds in incremental stages, where\nin each stage the effective sparsity of the weights increases by at most one. These stages are implicitly\ndefined by Algorithm 1 below, which constructs a sequence of times 0 = T0 < T1 < \u00b7 \u00b7 \u00b7 < Tk < \u00b7 \u00b7 \u00b7\nand weight vectors \u03b80, \u03b81, . . . , \u03b8k, . . . \u2208 R2p that define the stages. We prove the following:\nTheorem 4.1 (Incremental dynamics at small initialization). Let fNN be a model with diagonal\nweights as in Definition 3.1. For any stage k and time t \u2208 (Tk, Tk+1) the following holds under\nAssumptions 2.1, 4.3, 4.4 and 4.5. There is \u03b10(t) > 0 such that for all \u03b1 < \u03b10, there exists a unique\nsolution \u03b8 : [0, t log(1/\u03b1)] \u2192 Rp to the gradient flow (3) and\nlim\n\u03b1\u21920 \u03b8(t \u00b7 log(1/\u03b1)) \u2192 \u03b8k ,\nand at each stage the sparsity increases by at most one: supp(\u03b8k+1) \\ supp(\u03b8k) \u2286 {ik}.4\nAlgorithm 1 Incremental learning in networks with diagonal weights\n1: b0, \u03b80 \u2190 0 \u2208 Rp, T0 \u2190 0\n2: for stage number k = 0, 1, 2, . . . do\n3:\n# (A) Pick new coordinate ik \u2208 [p] to activate.\n4:\nFor each i, define time \u2206k(i) until active using (10).\n5:\nPick winning coordinate ik using (11)\n6:\nCalculate time Tk+1 using (11) and break if \u221e\n7:\nUpdate logarithmic weight approximation bk+1 using (12)\n8:\n# (B) Train activated coordinates to stationarity.\n9:\n\u03b8k+1 \u2190 limiting dynamics point from (13)\n10: end for\nApplication: transformer with diagonal weights\nBefore giving the intuition for this theorem\nand stating the assumptions formally, let us discuss its application to the diagonal transformer\nmodel from Example 3.2. As a corollary of Theorem 4.1, the gradient flow on a diagonal transformer\nwith small initialization will learn in stages, where in each stage there will be at most one head\n4Abusing notation, for \u03b8 = (u, v) \u2208 Rp \u00d7 Rp, we write supp(\u03b8) = supp(u) \u222a supp(v).\n5\ni \u2208 [H] in one layer \u2113 \u2208 [L] such that either the rank of W \u2113,i\nK (W \u2113,i\nQ )\u22a4 = diag(w\u2113,i\nK )diag(w\u2113,i\nQ ) or the\nrank of W \u2113,i\nV (W \u2113,i\nO )\u22a4 = diag(w\u2113,i\nV )diag(w\u2113,i\nO ) increases by at most one. In Figure 2, we illustrate\nthese dynamics in the toy case of a single attention head trained in a student-teacher setup.\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n0.00\n0.02\n0.04\n0.06\n0.08\ntraining loss\nT\nraining loss vs. rescaled time, for various alpha\nalpha = 0.1\nalpha = 0.01\nalpha = 0.0001\nalpha = 1e-08\nalpha = 1e-16\nalpha = 1e-32\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 0.1\n0\n200\n400\n600\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), fo\n0\n200\n400\n600\n800\n1000\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 0.0001\n0\n200\n400\n600\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), fo\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 1e-16\n0\n200\n400\n600\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), fo\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 0.1\n0\n200\n400\n600\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), fo\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 0.0001\n0\n200\n400\n600\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 1e-16\n0\n200\n400\n600\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for\n(a)\n(b)\nFigure 2: (a) Loss versus rescaled time in the toy task of learning an attention head with diagonal\nweights, for various initialization scales \u03b1. The loss curves converge as \u03b1 \u2192 0 to a curve with\nstagewise loss plateaus and sharp decreases, as predicted by the theory; some stagewise learning\nbehavior is already clear with \u03b1 = 0.01. (b) Each line shows the evolution of one of the entries\nof diag(wQ)diag(wK) and diag(wV )diag(wO) over rescaled time, demonstrating that the rank of\nthese matrices increases incrementally; see Appendix A for experimental details and further results.\n4.1\nIntuition for incremental learning dynamics\nWe develop an informal intuition for Theorem 4.1 and fill out the definition of Algorithm 1. A model\nfNN with diagonal weights \u03b8 = (u, v) as in Definition 3.1 evolves under the gradient flow (3) as\ndu\ndt = v \u2299 g(\u03b8),\ndv\ndt = u \u2299 g(\u03b8)\nwhere\n(4)\ng(\u03b8) = \u2212 Ex,y[D\u2113(y, h(x; u \u2299 v))\u22a4Dh(x; u \u2299 v)\u22a4] .\nHere D\u2113(y, \u00b7) \u2208 R1\u00d7dout is the derivative of \u2113 in the second argument and Dh(x, \u00b7) \u2208 Rdout\u00d7p is\nthe derivative of h in the second argument. The first key observation is a conservation law that\nsimplifies the dynamics. It can be viewed as the balancedness property for networks with linear\nactivations [ACH18, DHL18], specialized to the case of diagonal layers.\nLemma 4.2 (Conservation law). For any i \u2208 [p] and any time t, we have\nu2\ni (t) \u2212 v2\ni (t) = u2\ni (0) \u2212 v2\ni (0) .\n(5)\nProof. This follows from d\ndt(u2\ni \u2212 v2\ni ) = uivigi(\u03b8) \u2212 uivigi(\u03b8) = 0.\n6\nThe conservation law reduces the degrees of freedom and means that we need only keep track of p\nparameters in total. Specifically, if we define wi(t) := ui(t)+vi(t), then the vector w(t) = u(t)+v(t)\nevolves by\ndw\ndt = w \u2299 g(\u03b8) .\n(6)\nUsing the conservation law (5), we can keep track of the weights in terms of the initialization and\nw(t):\n\u03b8(t) =\n\u00121\n2(w(t) + u\u22992(0) \u2212 v\u22992(0)\nw(t)\n), 1\n2(w(t) \u2212 u\u22992(0) \u2212 v\u22992(0)\nw(t)\n)\n\u0013\n(7)\nTherefore it suffices to analyze the dynamics of w(t).\n4.1.1\nStage 1 of dynamics\nStage 1A of dynamics: loss plateau for time \u0398(log(1/\u03b1))\nAt initialization, \u03b8(0) \u2248 0 because\nthe weights are initialized on the order of \u03b1 which is small. This motivates the approximation\ng(\u03b8(t)) \u2248 g(0), under which the dynamics solve to:\nw(t) \u2248 w(0) \u2299 eg(0)t.\n(8)\nOf course, this approximation is valid only while the weights are still close to the small initialization.\nThe approximation breaks once one of the entries of \u03b8(t) reaches constant size. By combining (7)\nand (8), this happens at time t \u2248 T1 \u00b7 log(1/\u03b1) for\nT1 = min\ni\u2208[p] 1/|gi(0)| .\nUntil this time, the network remains close to its initialization, and so we observe a loss plateau.\nStage 1B of dynamics: nonlinear dynamics for time O(1)\nSubsequently, the loss decreases\nnonlinearly during a O(1) time-scale, which is vanishingly short relative to the time-scale of the loss\nplateau. To prove this, we make the non-degeneracy assumption that there is a unique coordinate\ni0 such that 1/|gi0(0)| = T1. Under this assumption, in stage 1A all weights except for those at\ncoordinate i0 remain vanishingly small, on the order of o\u03b1(1). Concretely, for any small \u03f5 > 0, there\nis a time t1(\u03f5) \u2248 T1 \u00b7 log(1/\u03b1) and sign s \u2208 {+1, \u22121} such that5\nui0(t1) \u2248 \u03f5, vi0(t1) \u2248 s\u03f5\nand\n|ui(t1)|, |vi(t1)| = o\u03b1(1) for all i \u0338= i0.\nBecause all coordinates except for i0 have vanishingly small o\u03b1(1) weights after stage 1A, we may\nperform the following approximation of the dynamics. Zero out the weights at coordinates except\nfor i0, and consider the training dynamics starting at \u02dc\u03b8 = (\u03f5ei0, s\u03f5ei0). After O(1) time, we should\nexpect these dynamics to approach a stationary point. Although the evolution is nonlinear, all\nentries remain zero except for the i0 entries, so the stationary point is also sparse. Mathematically,\nthere is a time \u00aft1 = t1 + O(1) \u2248 T1 \u00b7 log(1/\u03b1) such that\n\u03b8(\u00aft1) \u2248 (aei0, saei0) := \u03b81 ,\n5Without loss of generality, we can ensure that at initialization u(0) and u(0) + v(0) are nonnegative. This implies\nu(t) is nonnegative. The fact that ui0 and vi0 are roughly equal in magnitude but might differ in sign is due to the\nconservation law (5). See Appendix C.3 for details.\n7\nfor some a \u2208 R>0, where \u03b81 is a stationary point of the loss.6 Despite the nonlinearity of the\ndynamics, the approximation can be proved using Gr\u00a8onwall\u2019s inequality since \u00aft1 \u2212 t1 = O(1) is a\nconstant time-scale.\nTo summarize, we have argued that the network approximately reaches stationary point that is\n1-sparse, where only the weights at coordinate i0 are nonzero.\n4.1.2\nLater stages\nWe inductively extend the argument to any number of stages k, where each stage has a \u0398(log(1/\u03b1))-\ntime plateau, and then a O(1)-time nonlinear evolution, with the sparsity of the weights increasing\nby at most one. The argument to analyze multiple stages is analogous, but we must also keep\ntrack of the magnitude of the weights on the logarithmic scale, since these determine how much\nlonger . Inductively on k, suppose that there is some Tk \u2208 R, bk \u2208 Rp and \u03b8k \u2208 R2p and a time\n\u00aftk \u2248 Tk \u00b7 log(1/\u03b1) such that\nlog\u03b1(w(\u00aftk)) \u2248 bk and \u03b8(\u00aftk) \u2248 \u03b8k,\nwhere \u03b8k is a stationary point of the loss. Our inductive step shows that there is Tk+1 \u2208 R such\nthat during times t \u2208 (\u00aftk, Tk+1 \u00b7 log(1/\u03b1) \u2212 \u2126(1)) the weights remain close to the stationary point\nfrom the previous stage, i.e., \u03b8(t) \u2248 \u03b8k. And at a time \u00aftk+1 \u2248 Tk+1 \u00b7 log(1/\u03b1) we have\nlog\u03b1(w(\u00aftk+1)) \u2248 bk+1 and \u03b8(\u00aftk+1) \u2248 \u03b8k+1,\nwhere \u03b8k+1 and bk+1 are defined below (summarized in Algorithm 1). Most notably, \u03b8k+1 is a\nstationary point of the loss whose support grows by at most one compared to \u03b8k.\nStage (k + 1)A, loss plateau for time \u0398(log(1/\u03b1))\nAt the beginning of stage k + 1, the weights\nare close to the stationary point \u03b8k, and so, similarly to stage 1A, linear dynamics are valid.\nw(t) \u2248 w(\u00aftk) \u2299 eg(\u03b8k)(t\u2212\u00aftk) .\n(9)\nUsing the conservation law (7), we derive a \u201ctime until active\u201d for each coordinate i \u2208 [p], which\ncorresponds to the time for the weight at that coordinate to grow from o\u03b1(1) to \u0398(1) magnitude:\n\u2206k(i) =\n(\n(bk\ni \u2212 1 + sgn(gi(\u03b8k)))/gi(\u03b8k),\nif gi(\u03b8k) \u0338= 0\n\u221e,\nif gi(\u03b8k) = 0\n(10)\nThe linear dynamics approximation (9) breaks down at a time t \u2248 Tk+1 \u00b7 log(1/\u03b1), where\nTk+1 = Tk + \u2206k(ik),\nik = arg min\ni\u2208[p] \u2206k(i) ,\n(11)\nwhich corresponds to the first time at the weights at a coordinate grow from o\u03b1(1) to \u0398(1) magnitude.\nAnd at times t \u2248 Tk+1 \u00b7 log(1/\u03b1), on the logarithmic scale w is given by\nlog\u03b1(w(t)) \u2248 bk+1 := bk \u2212 g(\u03b8k)\u2206k(ik) ,\n(12)\n6The entries of u and v are close in magnitude (but may differ in sign) because of the conservation law (5).\n8\nStage (k + 1)B of dynamics: nonlinear dynamics for time O(1)\nSubsequently, the weights\nevolve nonlinearly during O(1) time. In a similar way to the analysis of Stage 1B, we show that at\na time \u00aftk+1 = tk+1 + O(1) \u2248 Tk+1 \u00b7 log(1/\u03b1), we have\n\u03b8(\u00aftk+1) \u2248 \u03b8k+1 := lim\n\u03f5\u21920 lim\nt\u2192\u221e \u03c8k(t, \u03f5) ,\n(13)\nwhere the dynamics \u03c8k(t, \u03f5) \u2208 R2p are initialized at \u03c8k(0, \u03f5) = \u03b8k + (\u03f5eik, sgn(gi(\u03b8k))\u03f5eik) and\nevolve according to the gradient flow d\u03c8k(t,\u03f5)\ndt\n= \u2212\u2207\u03b8L(\u03c8k). This concludes the inductive step.\n4.2\nAssumptions for incremental dynamics\nTo make this intuition rigorous, we formalize below the assumptions required for Theorem 4.1. In\nFigure 3 and Appendix A, we provide experiments validating these assumptions on the toy model.\nThe first assumption is that the dynamics are non-degenerate, in the sense that two coordinates\ndo not have weights that grow from o\u03b1(1) to \u0398(1) size at the same rescaled time. We also place a\ntechnical condition to handle the corner case when a coordinate leaves the support of the current\nstage\u2019s stationary point.\nAssumption 4.3 (Nondegeneracy of dynamics in part (A)). The initialization satisfies |ui(0)| \u0338=\n|vi(0)| for all i. For stage k, either Tk+1 = \u221e or there is a unique minimizer ik to mini \u2206k(ik) in\n(11). Finally, for all i \u2208 supp(\u03b8k\u22121) \\ supp(\u03b8k) we have gi(\u03b8k) \u0338= 0.\nNext, we require that very small perturbations of the coordinates outside of supp(\u03b8k) do not\nchange the dynamics. For this, it suffices that \u03b8k be a strict local minimum.\nAssumption 4.4 (Stationary points are strict local minima). For stage k, there exist \u03b4k > 0 and\nck > 0 such that for \u02dcu \u2208 B(uk, \u03b4) supported on supp(uk), we have\nL(\u02dcu, sk \u2299 \u02dcu) \u2265 ck\u2225uk \u2212 \u02dcu\u22252\nFinally, we require a robust version of the assumption that the limit (13) exists, asking for\nconvergence to a neighborhood of \u03b8k+1 even when the initialization is slightly noisy.\nAssumption 4.5 (Noise-robustness of dynamics in part (B)). For any stage k with Tk+1 < \u221e and\nany \u03f5 > 0, there are \u03b4 > 0 and \u03c4 : R>0 \u2192 R such that the following holds. For any \u02dcu \u2208 B(uk, \u03b4)\u2229Rp\n\u22650\nsupported on supp(\u02dcu) \u2286 supp(uk) \u222a {ik}, there exists a unique solution \u03c8 : [0, \u221e) \u2192 Rp of the\ngradient flow d\u03c8\ndt = \u2212\u2207\u03b8L(\u03c8) initialized at \u03c8(0) = (\u02dcu, sk+1 \u2299 \u02dcu), and at times t \u2265 \u03c4( \u02dc\u03c8ik),\n\u2225\u03c8(t) \u2212 \u03b8k+1\u2225 < \u03f5 .\n5\nExperimental results\nWe run experiments that go beyond the toy diagonal attention head model (see Figures 2 and 3) to\ntest the extent to which low-rank incremental learning occurs in popular models used in practice.\nWe conduct experiments with vision transformers (ViT) [DBK+20] trained on the CIFAR-10/100\nand ImageNet datasets, and with the GPT-2 language transformer [BMR+20] trained on the\nWikitext-103 dataset. Full experiments are deferred to Appendix B.\n9\n541\n542\n543\n544\n545\n546\n547\ntime / log(1/al ha)\n\u22121\n0\n1\n2\nCoordinate valu\n541\n542\n543\n544\n545\n546\n547\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\nCoordinate valu\n778\n779\n780\n781\n782\n783\n784\ntime / log(1/al ha)\n\u22121\n0\n1\n2\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 778.0\n778\n779\n780\n781\n782\n783\n784\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 778.0\n863\n864\n865\n866\n867\n868\ntime / log(1/al ha)\n\u22121\n0\n1\n2\n3\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 862.4\n863\n864\n865\n866\n867\n868\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 862.4\n942\n943\n944\n945\n946\n947\n948\ntime / log(1/al ha)\n\u22121\n0\n1\n2\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 942.2\n942\n943\n944\n945\n946\n947\n948\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 942.2\n200\n220\n240\n260\n280\n300\ntime / log(1/alpha)\n0.0\n0.5\n1.0\n1.5\n2.0\nCoo dinate valu\n200\n220\n240\n260\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoo dinate valu\n322.5\n325.0\n327.5\n330.0\n332.5\n335.0\n337.5\ntime / log(1/alpha)\n0\n1\n2\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 321.4\n322.5\n325.0\n327.5\n330.0\n332.5\ntime / log(1/alpha)\n\u22120.06\n\u22120.04\n\u22120.02\n0.00\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(\n357.5\n360.0\n362.5\n365.0\n367.5\n370.0\n372.5\n375.0\n377.5\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 356.8\n357.5\n360.0\n362.5\n365.0\n367.5\n370.0\ntime / log(1/alpha)\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(\n406\n408\n410\n412\n414\n416\n418\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 405.4\n406\n408\n410\n412\n414\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(\n500\n525\n550\n575\n600\n625\n650\ntime / log(1/alpha)\n\u22121\n0\n1\n2\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 486.5\n500\n525\n550\n575\n600\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(\nFigure 3: Validation of assumptions on the toy model of learning a single attention head. (a)\nAssumption 4.4: weights perturbed at a random time during training (solid lines) tend back to the\nnear-stationary point (dashed lines). (b) Assumption 4.5: weights perturbed at the beginning of a\nstage (solid lines) have same nonlinear evolution as without perturbation (dashed lines). Details of\nthese experiments and further validations are provided in Appendix A.\nGradual rank increase in vision and language models\nWe train practical transformer\narchitectures on vision and language tasks using Adam and the cross-entropy loss.\nWe train\nall layers (including the feedforward layers). To capture the low-rank bias with a non-vanishing\ninitialization scale, we study the spectrum of the difference \u2206WKW \u22a4\nQ and \u2206WV W \u22a4\nO between the\nweights post-training and their initial values. Specifically, in Figure 4, we plot the stable rank of\nthe differences \u2206WKW \u22a4\nQ and \u2206WV W \u22a4\nO . The weight perturbation learned during the training\nprocess gradually increases in stable rank during training, and is ultimately low-rank when compared\nto the initial spectrum. Finally, for CIFAR-10, we plot the spectrum of \u2206WKW \u22a4\nQ against that\nof its initialized state in Figure 5 for different self-attention heads, illustrating that the weight\nperturbation learned during the training process is extremely low-rank when compared to the initial\nspectrum. In Appendix B, we also study optimization with SGD, which shows similar gradual rank\nincrease behavior.\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 3\nWQWT\nK\nWVWT\nO\n(a) ViT, CIFAR-10\n(b) ViT, CIFAR-100\n(c) ViT, ImageNet\n(d) GPT-2, Wikitext-103\nFigure 4: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO (orange) on an arbitrary chosen layer\nthroughout training for four different pairs of networks and tasks. The stable rank of a matrix W\nis defined as \u2225W \u22252\nF /\u2225W \u22252\n2, and gives a smooth approximation of the rank. Mean and standard\ndeviation (shaded area) are computed across all heads in each attention layer. Full details and\nresults are in Appendix B.\nEffect of initialization scale\nWe probe the effect of initialization scale on gradual rank increase\ndynamics for a ViT trained on CIFAR-10. We use a ViT of depth 6, with 8 self-attention heads\nper layer (with layer normalization). We use an embedding and MLP dimension of demb = 512,\nand a head dimension of dh = 128 (i.e WK, WQ, WV , WO \u2208 Rdemb\u00d7dh). We train the transformer\nusing Adam with the cross-entropy loss. We train all layers (including the feedforward layers) while\n10\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5: Spectrum of the weight perturbation \u2206WKW \u22a4\nQ vs. initialization in a vision transformer\ntrained on CIFAR-10, using Adam and default initialization scale, in random self-attention heads\nin different layers. The learned perturbation exhibits extreme low-rank bias post-training even in\ndefault initialization scales. Analogous plots for CIFAR-100 and ImageNet are in Appendix B.\nvarying the initialization scale of all layers by multiplying their initial values by a scale factor (we fix\nthe scale of the initial token mapper). Figure 6 shows the evolution of the principal components of\n\u2206WKW \u22a4\nQ and \u2206WV W \u22a4\nO for a randomly-chosen self-attention head and layer throughout training,\nexhibiting incremental learning dynamics and a low-rank bias. Note that incremental learning and\nlow-rank bias are increasingly evident with smaller initialization scales, as further demonstrated in\nFigure 7.\n6\nDiscussion\nWe have identified incremental learning dynamics in transformers, proved them rigorously in a sim-\nplified setting, and shown them experimentally in networks trained with practical hyperparameters.\nLimitations\nThere are clear limitations to our theory: the diagonal weights and small initialization\nassumptions. More subtly, the theory does not apply to losses with exponential-like tails because the\nweights may not converge to a finite value and so Assumption 4.4 is not met (this could possibly be\naddressed by adding regularization). Also, the architecture must be smooth, which precludes ReLUs\n\u2013 but allows for smoothed ReLUs such as the GeLUs used in ViT [DBK+20]. Finally, the theory is\nfor training with gradient flow, while other optimizers such as Adam are used in practice instead\n[KB14]. Nevertheless, our experiments on ViTs indicate that the incremental learning dynamics\noccur even when training with Adam.\nFuture directions\nAn interesting avenue of future research is to develop a theoretical understand-\ning of the implicit bias in function space of transformers whose weights are a low-rank perturbation\nof randomly initialized weights. Another promising direction is to examine the connection between\nour results on incremental dynamics and the LoRA method [HSW+21], with the goal of explaining\nand improving on this algorithm; see also the discussion in Section 1.1. Along this vein, a concurrent\nwork [ZZC+23] independently observes gradual rank increase dynamics during transformer training\n11\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 6: Training a vision transformer on CIFAR-10 using Adam, while varying the initialization\nscale (unit scale indicates default initialization). Plotted are the evolution of the eigenvalues of\n\u2206WKW \u22a4\nQ (a) - (c) and \u2206WV W \u22a4\nO (d) - (f) in a random self-attention head in the second layer\nthroughout training. Incremental learning dynamics and a low-rank bias are evident for all scales,\nalbeit more pronounced at smaller initialization scales.\n(a)\n(b)\n(c)\nFigure 7: Stable rank of \u2206WKW \u22a4\nQ per initialization scale (Unit scale refers to the default initializa-\ntion) in different self-attention heads post-training, at layers 1, 3, 5. At each layer, the stable rank\nmean and standard deviation are computed across 8 heads per layer, for each initialization scale.\nAll models were trained on CIFAR-10 using the Adam optimizer. Smaller initialization scales lead\nto lower-rank attention heads.\nand this inspires a low-rank training algorithm that obtains runtime and memory improvements\nover regular training. The results of [ZZC+23] are complementary to ours, since they study the\nfeedforward layers of the transformer, and their theory applies to linear networks in the standard\ninitialization scale; in contrast, we study the attention layers, and our theory applies to nonlinear\nnetworks with small initialization scale.\nAcknowledgments\nWe would like to thank Vimal Thilak for his help in setting up the infrastructure for conducting\nexperiments, and the anonymous reviewers for their helpful feedback.\n12\nReferences\n[ABM22] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz, The merged-staircase\nproperty: a necessary and nearly sufficient condition for SGD learning of sparse\nfunctions on two-layer neural networks, COLT, 2022.\n[ABM23]\n, Sgd learning on neural networks: leap complexity and saddle-to-saddle dy-\nnamics, arXiv preprint arXiv:2302.11055 (2023).\n[ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan, On the optimization of deep networks:\nImplicit acceleration by overparameterization, International Conference on Machine\nLearning, PMLR, 2018, pp. 244\u2013253.\n[ACHL19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo, Implicit regularization in deep\nmatrix factorization, Advances in Neural Information Processing Systems 32 (2019).\n[AZG20] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta, Intrinsic dimensionality ex-\nplains the effectiveness of language model fine-tuning, arXiv preprint arXiv:2012.13255\n(2020).\n[Bac20] Francis Bach, Effortless optimization through gradient flows, Machine Learning Re-\nsearch Blog. https://francisbach. com/gradient-flows (2020).\n[BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song, Learning single-index\nmodels with shallow neural networks, arXiv preprint arXiv:2210.15651 (2022).\n[Ber22] Rapha\u00a8el Berthier, Incremental learning in diagonal linear networks, arXiv preprint\narXiv:2208.14673 (2022).\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.,\nLanguage models are few-shot learners, Advances in neural information processing\nsystems 33 (2020), 1877\u20131901.\n[BPVF22] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion, Gradient flow\ndynamics of shallow relu networks for square loss and orthogonal inputs, arXiv preprint\narXiv:2206.00939 (2022).\n[COB18] L\u00b4ena\u00a8\u0131c Chizat, Edouard Oyallon, and Francis R. Bach, On lazy training in differentiable\nprogramming, Neural Information Processing Systems, 2018.\n[DBK+20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, An image is worth 16x16 words:\nTransformers for image recognition at scale, ArXiv abs/2010.11929 (2020).\n[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, Bert:\nPre-training of deep bidirectional transformers for language understanding, ArXiv\nabs/1810.04805 (2019).\n[DHL18] Simon S Du, Wei Hu, and Jason D Lee, Algorithmic regularization in learning deep ho-\nmogeneous models: Layers are automatically balanced, Advances in Neural Information\nProcessing Systems 31 (2018).\n13\n[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi, Neural networks can learn\nrepresentations with gradient descent, Conference on Learning Theory, PMLR, 2022,\npp. 5413\u20135452.\n[FVB+22] Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu, Implicit bias in\nleaky relu networks trained on high-dimensional data, arXiv preprint arXiv:2210.07082\n(2022).\n[GBLJ19] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien, Implicit regularization of\ndiscrete gradient dynamics in linear neural networks, Advances in Neural Information\nProcessing Systems 32 (2019).\n[GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Limita-\ntions of lazy training of two-layers neural network, Advances in Neural Information\nProcessing Systems 32 (2019).\n[GSJW19] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart, Disentangling\nfeature and lazy learning in deep neural networks:\nan empirical study, ArXiv\nabs/1906.08034 (2019).\n[GSSD19] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely, The implicit bias of depth: How\nincremental learning drives generalization, arXiv preprint arXiv:1909.12051 (2019).\n[HSW+21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen, Lora: Low-rank adaptation of large language\nmodels, arXiv preprint arXiv:2106.09685 (2021).\n[JGH18] Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler, Neural tangent kernel: Conver-\ngence and generalization in neural networks, Advances in neural information processing\nsystems 31 (2018).\n[JGS+21] Arthur Jacot, Francois Gaston Ged, Berfin Simsek, Cl\u00b4ement Hongler, and Franck\nGabriel, Saddle-to-saddle dynamics in deep linear networks: Small initialization train-\ning, symmetry, and sparsity, 2021.\n[JLL+23] Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon S Du, and Jason D Lee, Understanding\nincremental learning of gradient descent: A fine-grained analysis of matrix sensing,\narXiv preprint arXiv:2301.11500 (2023).\n[KB14] Diederik P Kingma and Jimmy Ba, Adam: A method for stochastic optimization, arXiv\npreprint arXiv:1412.6980 (2014).\n[KC22] Daesung Kim and Hye Won Chung, Rank-1 matrix completion with gradient descent\nand small random initialization, ArXiv abs/2212.09396 (2022).\n[LFLY18] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski, Measuring the\nintrinsic dimension of objective landscapes, arXiv preprint arXiv:1804.08838 (2018).\n[LLL20] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu, Towards resolving the implicit bias of gradi-\nent descent for matrix factorization: Greedy low-rank learning, ArXiv abs/2012.09839\n(2020).\n14\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, Roberta: A robustly\noptimized bert pretraining approach, ArXiv abs/1907.11692 (2019).\n[MGW+20] Edward Moroshko, Suriya Gunasekar, Blake E. Woodworth, J. Lee, Nathan Srebro,\nand Daniel Soudry, Implicit bias in deep linear classification: Initialization scale vs\ntraining accuracy, ArXiv abs/2007.06738 (2020).\n[MHPG+22] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and\nMurat A Erdogdu, Neural networks efficiently learn low-dimensional representations\nwith sgd, arXiv preprint arXiv:2209.14863 (2022).\n[MKAA21] Paolo Milanesi, Hachem Kadri, S. Ayache, and Thierry Arti`eres, Implicit regularization\nin deep tensor factorization, 2021 International Joint Conference on Neural Networks\n(IJCNN) (2021), 1\u20138.\n[MKAS21] Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro, Quantifying\nthe benefit of using differentiable learning over tangent kernels, Proceedings of the\n38th International Conference on Machine Learning (Marina Meila and Tong Zhang,\neds.), Proceedings of Machine Learning Research, vol. 139, PMLR, 18\u201324 Jul 2021,\npp. 7379\u20137389.\n[PA23] Dylan Patel and Afzal Ahmad, Google \u201cwe have no moat, and neither does openai\u201d,\nMay 2023.\n[PF23] Scott Pesme and Nicolas Flammarion, Saddle-to-saddle dynamics in diagonal linear\nnetworks, arXiv preprint arXiv:2304.00488 (2023).\n[RC20] Noam Razin and Nadav Cohen, Implicit regularization in deep learning may not be\nexplainable by norms, Advances in neural information processing systems 33 (2020),\n21174\u201321187.\n[SKZ+23] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and\nJoshua Albrecht, On the stepwise nature of self-supervised learning, arXiv preprint\narXiv:2303.15438 (2023).\n[SMG13] Andrew M Saxe, James L McClelland, and Surya Ganguli, Exact solutions to the nonlin-\near dynamics of learning in deep linear neural networks, arXiv preprint arXiv:1312.6120\n(2013).\n[SS21] Dominik St\u00a8oger and Mahdi Soltanolkotabi, Small random initialization is akin to\nspectral learning: Optimization and generalization guarantees for overparameterized\nlow-rank matrix reconstruction, Neural Information Processing Systems, 2021.\n[TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto, Alpaca: A strong, replicable\ninstruction-following model, Stanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html (2023).\n[TVS23] Nadav Timor, Gal Vardi, and Ohad Shamir, Implicit regularization towards rank\nminimization in relu networks, International Conference on Algorithmic Learning\nTheory, PMLR, 2023, pp. 1429\u20131459.\n15\n[VSP+17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, ArXiv\nabs/1706.03762 (2017).\n[WGL+19] Blake E. Woodworth, Suriya Gunasekar, J. Lee, Edward Moroshko, Pedro H. P.\nSavarese, Itay Golan, Daniel Soudry, and Nathan Srebro, Kernel and rich regimes in\noverparametrized models, ArXiv abs/2002.09277 (2019).\n[YW23] Hao Yu and Jianxin Wu, Compressing transformers: Features are low-rank, but weights\nare not!\n[ZZC+23] Jiawei Zhao, Yifei Zhang, Beidi Chen, Florian Sch\u00a8afer, and Anima Anandkumar,\nInrank: Incremental low-rank learning, arXiv preprint arXiv:2306.11250 (2023).\n16\nContents\n1\nIntroduction\n1\n1.1\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nPaper organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nPreliminaries\n4\n3\nNeural networks with diagonal weights\n4\n4\nIncremental learning in networks with diagonal weights\n5\n4.1\nIntuition for incremental learning dynamics . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.1.1\nStage 1 of dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.1.2\nLater stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.2\nAssumptions for incremental dynamics . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n5\nExperimental results\n9\n6\nDiscussion\n11\nA Experimental validation of the assumptions in Theorem 4.1\n18\nB Further experiments on vision and language transformers\n23\nB.1\nSGD-trained transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nB.2 Adam-trained transformers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC Proof for dynamics of networks with diagonal parametrization (Theorem 4.1)\n29\nC.1 Assumptions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.2 Rescaling time for notational convenience\n. . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.3 Simplifying problem without loss of generality . . . . . . . . . . . . . . . . . . . . . .\n30\nC.4 Tracking the sum of the weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nC.5 Claimed invariants in proof of Theorem C.4 . . . . . . . . . . . . . . . . . . . . . . .\n31\nC.6\nDynamics from time \u00aftk to time tk+1 (Linear dynamics for O(log(1/\u03b1)) unrescaled time) 31\nC.6.1\nAnalysis in case where Tk+1 < \u221e . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nC.6.2\nAnalysis in case where Tk+1 = \u221e . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nC.7 Dynamics from time tk to time \u00aftk (Nonlinear evolution for O(1) unrescaled time) . .\n36\nC.8 Concluding the proof of Theorem C.4\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nD Technical lemmas\n38\nD.1 Relating the sum of the weights to the original weights using the conservation law\n.\n38\nD.2 Sign of gradients on coordinates that leave support . . . . . . . . . . . . . . . . . . .\n38\nD.3 Local lipschitzness and smoothness . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n17\nA\nExperimental validation of the assumptions in Theorem 4.1\nIn Figures 2, 8, and 9, we plot the evolution of the losses, of the entries of WKW \u22a4\nQ = diag(wK)diag(wQ),\nand of the entries of WV W \u22a4\nO = diag(wV )diag(wO) in the toy task of training an attention head\n(1) with diagonal weights. The model is trained with SGD on the mean-squared error loss on\n1000 random samples (X, y). Each random sample has X \u2208 R10\u00d750, which a sequence of 10\ntokens, each of dimension 50, which is distributed as isotropic Gaussian. The label y is given by\na randomly-generated teacher model that is also an attention head (1) with diagonal weights. In\nFigures 2, 8, and 9, for \u03b1 \u2208 {0.1, 0.01, 0.0001, 10\u22128, 10\u221216, 10\u221232} we plot the evolution of the loss\nand of the weights when initialized at \u03b8(0) = \u03b1\u03b80, for some random Gaussian \u03b80. Qualitatively, as\n\u03b1 \u2192 0 we observe that the loss curve and the trajectories of the weights appear to converge to a\nlimiting stagewise dynamics, where there are plateaus followed by movement on short time-scales,\nas predicted by the theory.\nValidation of Assumption 4.3 (non-degeneracy of dynamics)\nAs \u03b1 \u2192 0, notice that the\nstages appear to separate and happen at distinct times. Furthermore, the extra technical condition\non coordinates i \u2208 supp(\u03b8k) \\ supp(\u03b8k\u22121) in Assumption 4.3 is satisfied since no coordinates ever\nleave the support of \u03b8k.\nValidation of Assumption 4.4 (stationary points are strict local minima)\nIn Figure 10\nwe consider the \u03b1 = 10\u221232 trajectory, since this is closest to the dynamics in the \u03b1 \u2192 0 limit. We\nrandomly select several epochs. Since the transitions between stages are a vanishing fraction of the\ntotal training time, each of these randomly-selected epochs is likely during a plateau, as we see in\nthe figure. For each epoch perform the following experiment. For each nonnegligible coordinate\nof the weights (those where the weight is of magnitude greater than the threshold \u03c4 = 10\u22125), we\nperturb the weights by adding noise of standard deviation 0.05. We then run the training dynamics\nstarting at this perturbed initialization for 1000 epochs. We observe that the training dynamics\nquickly converge to the original unperturbed initialization, indicating that the weights were close to\na strict local minimum of the loss.\nValidation of Assumption 4.5 (noise-robustness of dynamics)\nIn Figure 11 we perform\nthe same experiment as in Figure 10, except that the epochs we select to perturb the weights are\nthose where there is a newly-nonnegligible coordinate (less than 10\u22125 in magnitude in the previous\nepoch, and more than 10\u22125 in magnitude in this epoch). We find that the nonlinear dynamics are\nrobust and tend to the limiting endpoint even under a random Gaussian perturbation of standard\ndeviation 10\u22122 on each of the nonnegligible coordinates, supporting Assumption 4.5.\n18\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 0.1\n0\n200\n400\n600\n800\n1000\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 0.01\n0\n200\n400\n600\n800\n1000\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 0.0001\n0\n200\n400\n600\n800\n1000\n ime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 1e-08\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 1e-16\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nDiagonal en ries of W\nQ\nW\nK\nEn ries of diag(w\nQ\n)diag(w\nK\n), for alpha = 1e-32\nFigure 8: Evolution of diag(wQ)diag(wK) entries over rescaled time initializing at various scalings \u03b1.\nNotice that as \u03b1 \u2192 0, the training trajectories tend to a limiting trajectory. Each line corresponds\nto a diagonal entry of diag(wQ)diag(wK).\n19\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 0.1\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 0.01\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 0.0001\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 1e-08\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 1e-16\n0\n200\n400\n600\n800\n1000\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nDiagonal entries of W\nV\nW\nO\nEntries of diag(w\nV\n)diag(w\nO\n), for alpha = 1e-32\nFigure 9: Evolution of diag(wV )diag(wO) entries in the toy task of learning an attention head with\ndiagonal weights. Each line corresponds to the evolution of an entry of diag(wV )diag(wO) over\nrescaled time. Each plot corresponds to a different initialization magnitude \u03b1. Notice that as \u03b1 \u2192 0,\nthe training trajectories tend to a limiting trajectory.\n20\n374\n375\n376\n377\n378\n379\ntime / log(1/al ha)\n\u22121\n0\n1\n2\n3\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 373.4\n374\n375\n376\n377\n378\n379\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 373.4\n541\n542\n543\n544\n545\n546\n547\ntime / log(1/al ha)\n\u22121\n0\n1\n2\n3\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 541.2\n541\n542\n543\n544\n545\n546\n547\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 541.2\n778\n779\n780\n781\n782\n783\n784\ntime / log(1/al ha)\n\u22121\n0\n1\n2\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 778.0\n778\n779\n780\n781\n782\n783\n784\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 778.0\n863\n864\n865\n866\n867\n868\ntime / log(1/al ha)\n\u22121\n0\n1\n2\n3\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 862.4\n863\n864\n865\n866\n867\n868\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 862.4\n942\n943\n944\n945\n946\n947\n948\ntime / log(1/al ha)\n\u22121\n0\n1\n2\nCoordinate value\nEntries of W\nV\nW\nT\nO\n,  erturb time / log(1/al ha) = 942.2\n942\n943\n944\n945\n946\n947\n948\ntime / log(1/al ha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoordinate value\nEntries of W\nK\nW\nT\nQ\n,  erturb time / log(1/al ha) = 942.2\nFigure 10: Evolution of weights of toy attention model under perturbation, validating Assumption 4.4.\nAt 5 different random times during training, we perturb the nonnegligible weight coordinates and\ncontinue to train with SGD. The evolution of each of the weights under the initial perturbation (solid\nline) is compared to the original evolution without perturbation (dashed line). Observe that the\ntraining dynamics quickly brings each weight back to the unperturbed weight trajectory, indicating\nthat the weights are originally close to a strict local minimum.\n21\n200\n220\n240\n260\n280\n300\ntime / log(1/alpha)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 202.5\n200\n220\n240\n260\n280\n300\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nCoo dinate value\n1e\u22125\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(1/alpha) = 202.5\n322.5\n325.0\n327.5\n330.0\n332.5\n335.0\n337.5\ntime / log(1/alpha)\n0\n1\n2\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 321.4\n322.5\n325.0\n327.5\n330.0\n332.5\n335.0\n337.5\ntime / log(1/alpha)\n\u22120.06\n\u22120.04\n\u22120.02\n0.00\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(1/alpha) = 321.4\n357.5\n360.0\n362.5\n365.0\n367.5\n370.0\n372.5\n375.0\n377.5\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 356.8\n357.5\n360.0\n362.5\n365.0\n367.5\n370.0\n372.5\n375.0\n377.5\ntime / log(1/alpha)\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(1/alpha) = 356.8\n406\n408\n410\n412\n414\n416\n418\ntime / log(1/alpha)\n\u22121\n0\n1\n2\n3\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 405.4\n406\n408\n410\n412\n414\n416\n418\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(1/alpha) = 405.4\n500\n525\n550\n575\n600\n625\n650\ntime / log(1/alpha)\n\u22121\n0\n1\n2\nCoo dinate value\nEnt ies of W\nV\nW\nT\nO\n, pe tu b time / log(1/alpha) = 486.5\n500\n525\n550\n575\n600\n625\n650\ntime / log(1/alpha)\n\u22121.0\n\u22120.5\n0.0\n0.5\nCoo dinate value\nEnt ies of W\nK\nW\nT\nQ\n, pe tu b time / log(1/alpha) = 486.5\nFigure 11: Validating Assumption 4.5 with the same experiment as in Figure 10, except that\nthe epochs for the perturbation chosen are those where there is a newly nonnegligible coordinate.\nPerturbed dynamics (solid lines) are again robust to perturbation and track the original dynamics\n(dashed lines).\n22\nB\nFurther experiments on vision and language transformers\nThe practice of training transformer models often deviates substantially from the assumptions made\nin our theoretical analysis, and it is a priori unclear to what extent gradual rank increase behaviour\nand a low rank bias are manifested in setups more common in practical applications. To gauge the\nrelevancy of our findings we conduct experiments on popular vision and language benchmarks, using\nalgorithms and hyperparameters common in the literature. We use the stable rank of a matrix W\ngiven by \u2225W \u22252\nF\n\u2225W \u22252\n2 as a smooth approximation of rank. We track the value of the stable rank for the\ndifferent attention matrices throughout training. Although we do not expect our theoretical results\nto to hold precisely in practice, we find evidence of gradual increase in stable rank, leading to a\nlow rank bias in Figures 12, 13, 15, 17 and 19. In these experiments we use off-the-shelf vision\ntransformers (ViT) [DBK+20] trained on popular vision benchmarks, as well as off-the-shelf GPT-2\ntrained on a popular language benchmark. We use no weight decay or dropout in our experiments.\nAll models were initialized using the default initialization scale.\nB.1\nSGD-trained transformers\nCIFAR-10/100\nWe trained a 6-layer ViT with 8 heads per layer, embedding dimension 512, head\ndimension 128, and MLP dimension 512 and patch-size 4 for 500 epochs on CIFAR10/CIFAR100\nwith SGD and learning rate 3e-1 and warmup. See Figures 12 and 13. Each run took 2 hours on\none A100 GPU.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 0\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 1\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 2\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 3\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 4\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 5\nWQWT\nK\nWVWT\nO\nFigure 12: CIFAR-10, ViT trained with SGD: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO\n(orange) throughout training. Mean and standard deviation (shaded area) are computed across 8\nheads per attention layer.\n23\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 0\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 1\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 2\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 3\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 4\nWQWT\nK\nWVWT\nO\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nStable rank\nLayer 5\nWQWT\nK\nWVWT\nO\nFigure 13: CIFAR-100, ViT trained with SGD: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO\n(orange) throughout training. Mean and standard deviation (shaded area) are computed across 8\nheads per attention layer.\nB.2\nAdam-trained transformers\nCIFAR-10/100\nFor the CIFAR-10/100 datasets we use a VIT with 6 layers, patchsize of 4, 8\nheads per self attention layer, an embedding and MLP dimension of 512, and a head dimension of\n128. We train the model using the Adam optimizer for 500 epochs with a base learning rate of 1e-4,\na cyclic learning rate decay with a linear warmup schedule for 15 epochs and a batchsize of 512. Our\nresults are summarized in Figures 14 and 15 for CIFAR-10, and Figures 16 and 17 for CIFAR-100.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 14: CIFAR-10, ViT trained with Adam: normalized spectrum at different stages of training.\n(a) - (c) Normalized spectrum of WKW \u22a4\nQ at initialization and \u2206WKW \u22a4\nQ during training for different\nattention heads at different layers. (d) - (e) equivalent figures for WV W \u22a4\nO .\nImageNet\nFor ImageNet, we use the VIT-Base/16 from [DBK+20] trained with Adam for 360\nepochs with a base learning rate of 3e-3, a cyclic learning rate decay with a linear warmup schedule\nfor 15 epochs and a batchsize of 4096. Our results are summarized in Figures 18 and 19 for ImageNet.\n24\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 15: CIFAR-10, ViT trained with Adam: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO\n(red) throughout training. Mean and standard deviation (shaded area) are computed across 8 heads\nper attention layer.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 16: CIFAR-100, ViT trained with Adam: normalized spectrum at different stages of training.\n(a) - (c) Normalized spectrum of WKW \u22a4\nQ at initialization and \u2206WKW \u22a4\nQ during training for different\nattention heads at different layers. (d) - (e) equivalent figures for WV W \u22a4\nO .\n25\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 17: CIFAR-100, ViT trained with Adam: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO\n(red) throughout training. Mean and standard deviation (shaded area) are computed across 8 heads\nper attention layer.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 18: ImageNet, ViT trained with Adam: normalized spectrum at different stages of training.\n(a) - (c) Normalized spectrum of WKW \u22a4\nQ at initialization and \u2206WKW \u22a4\nQ during training for different\nattention heads at different layers. (d) - (e) equivalent figures for WV W \u22a4\nO .\n26\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\nFigure 19: ImageNet, ViT trained with Adam: Stable rank of \u2206WKW \u22a4\nQ (blue) and \u2206WV W \u22a4\nO\n(red) throughout training. Mean and standard deviation (shaded area) are computed across 12\nheads per attention layer.\n27\nWikitext-103\nThe gradual rank increase phenomenon also occurs in the NLP setting with\nlanguage transformers. We trained GPT-2 on Wikitext-103 using the HuggingFace training script\nwith Adam learning rate 3e-4, per-GPU batch-size 8, and block-length 256. We trained for 3 epochs\non 2 A100 GPUs, which took 12 hours. See Figure 20.\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 0\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 1\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 2\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 3\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 4\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 5\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 6\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 7\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 8\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 9\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 10\nWQWT\nK\nWVWT\nO\n0\n20000\n40000\n60000\n80000\nIteration\n1\n2\n3\n4\n5\n6\nStable rank\nLayer 11\nWQWT\nK\nWVWT\nO\nFigure 20: Wikitext-103, GPT-2 trained with Adam: Stable rank of \u2206WV W \u22a4\nO and \u2206WQW \u22a4\nK ,\nversus training iteration. Stable rank of the perturbation increases gradually, but remains small\nthroughout training.\n28\nC\nProof for dynamics of networks with diagonal parametrization\n(Theorem 4.1)\nC.1\nAssumptions\nRecall we have defined \u03b80, . . . , \u03b8k, . . . \u2208 R2p as the sequence of weights such that \u03b80 = 0 and \u03b8k+1\nis defined inductively as follows. Consider the dynamics of \u03c8k(t, \u03f5) \u2208 R2p initialized at \u03c8k(0, \u03f5) =\n\u03b8k + (\u03f5eik, sgn(gi(\u03b8k))\u03f5eik) and evolving according to the gradient flow d\u03c8k(t,\u03f5)\ndt\n= \u2212\u2207\u03b8L(\u03c8k). We\nassume that there is a limiting point \u03b8k+1 of these dynamics as \u03f5 is taken small and the time is\ntaken large:\nlim\n\u03f5\u21920 lim\nt\u2192\u221e \u03c8k(t, \u03f5) = \u03b8k+1 .\nUnder the above assumption that this sequence \u03b80, . . . , \u03b8k, . . . is well-defined, we can derive a useful\nproperty of it for free. Namely, the conservation law (5) implies that u \u2299 u \u2212 v \u2299 v is preserved. It\nfollows that for each k we have that \u03b8k = (uk, vk) satisfies |uk| = |vk| entrywise. In other words,\nthere is sk \u2208 {+1, \u22121}p satisfying\n\u03b8k = (uk, sk \u2299 uk) \u2208 R2p .\nWe also abuse notation and write supp(\u03b8k) := supp(uk) \u2286 [p], since the support of \u03b8k on the first p\ncoordinates matches its support on the last p coordinates.\nHaving fixed this notation, we now recall the main assumptions of the theorem.\nAssumption C.1 (Nondegeneracy of dynamics in part (A); Assumption 4.3). The initialization\nsatisfies |ui(0)| \u0338= |vi(0)| for all i. For stage k, either Tk+1 = \u221e or there is a unique minimizer ik to\nmini \u2206k(ik) in (11). Finally, for all i \u2208 supp(\u03b8k\u22121) \\ supp(\u03b8k) we have gi(\u03b8k) \u0338= 0.\nAssumption C.2 (Stationary points are strict local minima; Assumption 4.4). For stage k, there\nexist \u03b4k > 0 and ck > 0 such that for \u02dcu \u2208 B(uk, \u03b4) supported on supp(uk), we have\nL(\u02dcu, sk \u2299 \u02dcu) \u2265 ck\u2225uk \u2212 \u02dcu\u22252 .\nAssumption C.3 (Noise-robustness of dynamics in part (B); Assumption 4.5). For stage k, either\nTk+1 = \u221e or the following holds. For any \u03f5 > 0, there are \u03b4 > 0 and \u03c4 : R>0 \u2192 R such that\nthe following holds. For any \u02dcu \u2208 B(uk, \u03b4) \u2229 Rp\n\u22650 supported on supp(\u02dcu) \u2286 supp(uk) \u222a {ik}, there\nexists a unique solution \u03c8 : [0, \u221e) \u2192 Rp of the gradient flow d\u03c8\ndt = \u2212\u2207\u03b8L(\u03c8) initialized at\n\u03c8(0) = (\u02dcu, sk+1 \u2299 \u02dcu), and at times t \u2265 \u03c4(\u02dcuik),\n\u2225\u03c8(t) \u2212 \u03b8k+1\u2225 < \u03f5 .\nC.2\nRescaling time for notational convenience\nFor ease of notation, we rescale time\nu\u03b1(0) = \u03b1u(0),\nv\u03b1(0) = \u03b1v(0)\ndu\u03b1\ndt\n= log(1/\u03b1)v\u03b1 \u2299 g(u\u03b1, v\u03b1),\ndv\u03b1\ndt = log(1/\u03b1)u\u03b1 \u2299 g(u\u03b1, v\u03b1).\n(14)\nWe also define\n\u03b8\u03b1(t) = (u\u03b1(t), v\u03b1(t)) \u2208 R2p .\nBecause of this time-rescaling, we equivalently state Theorem 4.1 as:\n29\nTheorem C.4 (Restatement of Theorem 4.1). Let K \u2208 Z\u22650 be such that Assumptions 4.3 4.4 hold\nfor all k \u2264 K and Assumption 4.5 holds for all k < K. Then for any k \u2264 K and time t \u2208 (Tk, Tk+1)\nthe following holds. There is \u03b10(t) > 0 such that for all \u03b1 < \u03b10, there exists a unique solution\n\u03b8\u03b1 : [0, t] \u2192 Rp to the gradient flow (14) and\nlim\n\u03b1\u21920 \u03b8\u03b1(t) \u2192 \u03b8k ,\nwhere at each stage |supp(uk) \\ supp(uk\u22121)| \u2264 1.\nFor shorthand, we also write\nSk = supp(uk) and Sc\nk = [p] \\ supp(uk) .\nC.3\nSimplifying problem without loss of generality\nFor each coordinate i \u2208 [p] we have |u\u03b1,i(0)| \u0338= |v\u03b1,i(0)| by the non-degeneracy Assumption 4.3.\nSo we can assume |u\u03b1,i(0)| > |v\u03b1,i(0)| without loss of generality. Furthermore, we can assume the\nentrywise inequality\nu\u03b1(0) > 0\nby otherwise training weights \u02dcu\u03b1(t), \u02dcv\u03b1(t) initialized at \u02dcu\u03b1(0) = sgn(u\u03b1(0))u\u03b1(0) and \u02dcv\u03b1(0) =\nsgn(v\u03b1(0))v\u03b1(0), as \u02dcu\u03b1(t) \u2299 \u02dcv\u03b1(t) = u\u03b1(t) \u2299 v\u03b1(t) at all times.\nSince u2\n\u03b1,i(t) \u2212 v2\n\u03b1,i(t) = u2\n\u03b1,i(0) \u2212 v2\n\u03b1,i(0) by the conservation law (5), it holds that |u\u03b1,i(t)| >\n|v\u03b1,i(t)| throughout. So by continuity\nu\u03b1(t) > 0\nthroughout training.\nC.4\nTracking the sum of the weights\nWe define\nw\u03b1(t) = u\u03b1(t) + v\u03b1(t) .\nThe reason for this definition is that during training we have\ndw\u03b1\ndt\n= log(1/\u03b1)w\u03b1 \u2299 g(\u03b8\u03b1) ,\n(15)\nNotice that since that we have assumed u\u03b1,i(0) > |v\u03b1,i(0)| for each i \u2208 [p] we have w\u03b1(0) > 0\nentrywise. So, by (15) for all t > 0 ,\nw\u03b1(t) > 0 .\nIt suffices to track w\u03b1(t) because we can relate the log-scale magnitude of w\u03b1(t) to the magnitudes\nof the corresponding coordinates in u\u03b1(t) and v\u03b1(t) \u2013 see technical Lemmas D.1 D.2 and D.3.\n30\nC.5\nClaimed invariants in proof of Theorem C.4\nIn order to prove Theorem C.4, we consider any gradient flow \u03b8\u03b1 : [0, T \u2217] \u2192 Rp solving (14) where\nT \u2217 \u2208 (TK, TK+1). For now, we focus only on proving properties of this gradient flow, and defer its\nexistence and uniqueness to Section C.8.\nWe show the following invariants inductively on the stage k. For any \u03f5 > 0, any stage k \u2264 K,\nthere is \u03b1k := \u03b1k(\u03f5) > 0 such that for all \u03b1 < \u03b1k the following holds. There are times \u00aftk := \u00aftk(\u03b1, \u03f5)\nand tk+1 := tk+1(\u03b1, \u03f5), such that\n\u00aftk \u2208 [Tk \u2212 \u03f5, Tk + \u03f5] ,\n(16)\ntk+1 \u2208\n(\n[Tk+1 \u2212 \u03f5, Tk+1 + \u03f5] ,\nif Tk+1 < \u221e\n{T \u2217},\nif Tk+1 = \u221e .\n(17)\nand the weights approximate the greedy limit for all times t \u2208 [\u00aftk, tk+1]\n\u2225\u03b8\u03b1(t) \u2212 \u03b8k\u2225 < \u03f5 ,\n(18)\nand the weights at times \u00aftk and tk+1 are correctly estimated by the incremental learning dynamics\non the logarithmic-scale\n\u2225 log\u03b1(w\u03b1(\u00aftk)) \u2212 bk\u2225 < \u03f5\n(19)\nand if Tk+1 < \u221e then\n\u2225 log\u03b1(w\u03b1(tk+1)) \u2212 bk+1\u2225 < \u03f5 .\n(20)\nBase case k = 0: Take \u00aft0(\u03b1, \u03f5) = 0. Then statement (16) holds since T0 = 0. Notice that as\n\u03b1 \u2192 0 we have that u\u03b1(0), v\u03b1(0) \u2192 0 = u0, and also log\u03b1 w\u03b1(0) \u2192 1 = b0. So statement (19)\nfollows if we take \u03b10 small enough. In Section C.6 we show how to construct time t1 such that (18)\nand (20) hold.\nInductive step: Suppose that (16), (18), (19) and (20) hold for some iteration k < K. We prove\nthem for iteration k + 1. In Section C.7 we construct time \u00aftk. In Section C.6 we construct time tk+1.\nC.6\nDynamics from time \u00aftk to time tk+1 (Linear dynamics for O(log(1/\u03b1)) un-\nrescaled time)\nLet k \u2264 K, and suppose that we know that for any \u00af\u03f5k > 0, there is \u00af\u03b1k(\u00af\u03f5k) > 0 such that for all\n0 < \u03b1 < \u00af\u03b1k, there is a time \u00aftk = \u00aftk(\u03b1, \u00af\u03f5k) satisfying\n|Tk \u2212 \u00aftk| < \u00af\u03f5k\n\u2225\u03b8\u03b1(\u00aftk) \u2212 \u03b8k\u2225 < \u00af\u03f5k\n\u2225 log\u03b1(w\u03b1(\u00aftk)) \u2212 bk\u2225 < \u00af\u03f5k .\nC.6.1\nAnalysis in case where Tk+1 < \u221e\nConsider first the case where Tk+1 < \u221e. We show that, for any \u03f5k+1 > 0, there is \u03c1k+1(\u03f5k+1) > 0\nsuch that for all 0 < \u03c1 < \u03c1k+1(\u00af\u03f5k+1) there is \u03b1k+1(\u03c1, \u03f5k+1) > 0 such that for all \u03b1 < \u03b1k+1, there is a\n31\ntime tk+1 = tk+1(\u03b1, \u03c1, \u03f5k+1) satisfying\n|Tk+1 \u2212 tk+1| < \u03f5k+1\n(21)\n\u2225\u03b8\u03b1(t) \u2212 \u03b8k\u2225 < \u03f5k+1 for all t \u2208 [\u00aftk, tk+1]\n(22)\n\u2225 log\u03b1(w\u03b1(tk+1)) \u2212 bk+1\u2225 < \u03f5k+1\n(23)\nu\u03b1,ik(tk+1) \u2208 [\u03c1, 3\u03c1] ,\n(24)\nsgn(v\u03b1,ik(tk+1)) = sk+1\nik\n.\n(25)\nFor any \u03c1, \u03b1, let \u00af\u03f5k = \u03c1\u03f5k+1/(4p) and choose \u00aftk = \u00aftk(\u03b1, \u00af\u03f5k). Then define\ntk+1 = tk+1(\u03b1, \u03c1, \u03f5k+1)\n(26)\n= inf{t \u2208 [\u00aftk, \u221e) : \u2225u\u03b1,Sc\nk(t) \u2212 u\u03b1,Sc\nk(\u00aftk)\u2225 + \u2225v\u03b1,Sc\nk(t) \u2212 v\u03b1,Sc\nk(\u00aftk)\u2225 > 4\u03c1} .\nNow we show that the weights \u03b8\u03b1(t) cannot move much from time \u00aftk to tk+1. The argument\nuses the local Lipschitzness of the loss L (from technical Lemma D.7), and the strictness of \u03b8k as a\nstationary point (from Assumption 4.4).\nLemma C.5 (Stability of active variables during part (A) of dynamics). There is \u03c1k+1 small\nenough and \u03b1k+1(\u03c1) small enough depending on \u03c1,such that for all \u03c1 < \u03c1k+1 and \u03b1 < \u03b1k+1 and all\nt \u2208 [\u00aftk, tk+1),\n\u2225\u03b8\u03b1(t) \u2212 \u03b8k\u2225 < \u03c1\u2032 := max(24\u03c1, 18\nq\n\u03c1KRk/ck) .\n(27)\nwhere ck is the strict-minimum constant from Assumption 4.4 and KRk is the Lipschitzness constant\nfrom Lemma D.7 for the ball of radius Rk = \u2225\u03b8k\u2225 + 1.\nProof. Assume by contradiction that (27) is violated at some time t < tk+1. Let us choose the first\nsuch time\nt\u2217 = inf{t \u2208 [\u00aftk, tk+1) : \u2225u\u03b1(t\u2217) \u2212 uk\u2225 + \u2225v\u03b1(t\u2217) \u2212 sk \u2299 uk\u2225 \u2265 \u03c1\u2032} .\nDefine \u02dc\u03b8 = (\u02dcu, \u02dcv) by\n\u02dcui =\n(\nu\u03b1,i(t\u2217),\ni \u2208 Sk\n0,\ni \u0338\u2208 Sk\nand\n\u02dcvi =\n(\nv\u03b1,i(t\u2217),\ni \u2208 Sk\n0,\ni \u0338\u2208 Sk\n.\nBy the definition of tk+1, this satisfies\n\u2225\u02dcu \u2212 u\u03b1(t\u2217)\u2225 = \u2225u\u03b1,Sc\nk(t\u2217)\u2225 \u2264 4\u03c1 + \u2225u\u03b1,Sc\nk(\u00aftk)\u2225 \u2264 4\u03c1 + \u03f5k < 5\u03c1 ,\n\u2225\u02dcv \u2212 v\u03b1(t\u2217)\u2225 = \u2225v\u03b1,Sc\nk(t\u2217)\u2225 \u2264 4\u03c1 + \u2225v\u03b1,Sc\nk(\u00aftk)\u2225 \u2264 4\u03c1 + \u03f5k < 5\u03c1 .\nAlso\n\u2225\u02dcu \u2212 uk\u2225 + \u2225\u02dcv \u2212 sk \u2299 uk\u2225 = \u2225u\u03b1,Sk(t\u2217) \u2212 zk\nSk\u2225 + \u2225v\u03b1,Sk(t\u2217) \u2212 sk\nSk \u2299 zk\nSk\u2225 \u2265 \u03c1\u2032 \u2212 10\u03c1 \u2265 \u03c1\u2032/2 .\nUsing (a) the strict minimum Assumption 4.4 with constant ck, since \u2225\u02dc\u03b8 \u2212 \u03b8k\u2225 \u2264 \u03c1\u2032 and we take\n\u03c1\u2032 small enough,\nL(\u03b8\u03b1(t\u2217)) \u2265 L(\u02dc\u03b8) \u2212 4\u03c1KRk\n(a)\n\u2265 L(\u03b8k) \u2212 4\u03c1KRk + ck(\u03c1\u2032)2\n16\n\u2265 L(\u03b8\u03b1(\u00aftk)) \u2212 (4\u03c1 + \u00af\u03f5k)KRk + ck(\u03c1\u2032)2\n16\n> L(\u03b8\u03b1(\u00aftk)) .\nThis is a contradiction because L is nondecreasing along the gradient flow.\n32\nLemma C.6 (Log-scale approximation is correct during part (A)). There are functions \u03c1k+1(\u03f5k+1) >\n0 and \u03b1k+1(\u03c1, \u03f5k+1) > 0 such that for all \u03c1 < \u03c1k+1 and \u03b1 < \u03b1k+1, and for all t \u2208 (\u00aftk, tk+1) we have\nfor a constant C depending on k,\n\u2225 log\u03b1(w\u03b1(t)) \u2212 bk + (t \u2212 \u00aftk)g(\u03b8k)\u2225 < \u03c1\u03f5k+1 + C\u03c1\u2032(t \u2212 \u00aftk) .\n(28)\nFurthermore, for all i \u2208 Sc\nk and t \u2208 (\u00aftk, tk+1) we have\nsgn(gi(\u03b8\u03b1(t))) = sgn(gi(\u03b8k)).\n(29)\nProof. By Lemma C.5 and Lemma D.7, there is a constant C depending on \u03b8k such that for all\nt \u2208 (\u00aftk, tk+1),\n\u2225g(\u03b8\u03b1(t)) \u2212 g(\u03b8k)\u2225 \u2264 C\u03c1\u2032 .\nFor shorthand, write \u00afg(\u03b8k) = g(\u03b8k) + C\u03c1\u20321 and g(\u03b8k) = g(\u03b8k) \u2212 C\u03c1\u20321. Since w\u03b1(t) > 0 entrywise\nas we have assumed without loss of generality (see Section C.3), we have the following entrywise\ninequalities\ng(\u03b8k) \u2299 w\u03b1(t) < g(\u03b8\u03b1(t)) \u2299 w\u03b1(t) < \u00afg(\u03b8k) \u2299 w\u03b1(t) .\n(30)\nSince the dynamics are given by dw\u03b1\ndt\n= log(1/\u03b1)g(w\u03b1) \u2299 w\u03b1,\nw\u03b1(\u00aftk)e(t\u2212\u00aftk) log(1/\u03b1)g(\u03b8k) \u2264 w\u03b1(t) \u2264 w\u03b1(\u00aftk)e(t\u2212\u00aftk) log(1/\u03b1)\u00afg(\u03b8k) .\nTaking the logarithms with base \u03b1 \u2208 (0, 1),\n(t \u2212 \u00aftk)g(uk) \u2264 log\u03b1(w\u03b1(\u00aftk)) \u2212 log\u03b1(w\u03b1(t)) \u2264 (t \u2212 \u00aftk)\u00afg(uk) .\nThe bound (28) follows since \u2225 log\u03b1(w\u03b1(\u00aftk)) \u2212 bk\u2225 < \u00af\u03f5k < \u03c1\u03f5k+1.\nFinally, the claim (29) follows from (30) since sgn(\u00afg(\u03b8k)) = sgn(g(\u03b8k)) = sgn(g(\u03b8k)) if we take\n\u03c1 small enough.\nFirst, we show that the weights must move significantly by time roughly Tk+1. This is because\nof the contribution of coordinate ik.\nLemma C.7 (tk+1 is not much larger than Tk+1). Suppose that Tk+1 < \u221e.\nThen there are\n\u03c1k+1(\u03f5k+1) > 0 and \u03b1k+1(\u03c1, \u03f5k+1) > 0 such that for all \u03c1 < \u03c1k+1 and \u03b1 < \u03b1k+1, the following holds.\ntk+1 < Tk+1 + \u03f5k+1 .\nProof. Assume by contradiction that tk+1 < Tk+1+\u03f5k+1. For all times t \u2208 [\u00aftk, min(tk+1, Tk+1+\u03f5k+1)],\nby Lemma C.6,\n| log\u03b1(w\u03b1,ik(t)) \u2212 bt\nik + (t \u2212 \u00aftk)gik(\u03b8k)| < O(\u221a\u03c1) .\nSince we know |\u2206k(ik) \u2212 (Tk+1 \u2212 \u00aftk)| < \u00af\u03f5k and bk\ni \u2212 \u2206k(ik)gik(\u03b8k) \u2208 {0, 2}, it follows that\nlog\u03b1(w\u03b1,ik(Tk+1 + \u03f5k+1)) \u0338\u2208 (\u2212|gik(\u03b8k)|(\u03f5k+1 \u2212 \u00af\u03f5k+1), 2 + |gik(\u03b8k)|(\u03f5k+1 \u2212 \u00af\u03f5k+1)) + O(\u221a\u03c1).\nBy taking \u03c1 small enough, we see that |gik(\u03b8k)|(\u03f5k+1 \u2212 \u00af\u03f5k+1) + O(\u221a\u03c1) > \u03b4 > 0 for some \u03b4 > 0 that\nis independent of \u03b1, so\nlog\u03b1(w\u03b1,ik(Tk+1 + \u03f5k+1)) \u0338\u2208 (\u2212\u03b4, 2 + \u03b4) .\nSo |u\u03b1,ik(Tk+1 + \u03f5k+1)| > 1 by Lemma D.2. But by the construction of tk+1 this means that\ntk+1 < Tk+1 + \u03f5k+1.\n33\nNext, we show that until time tk+1, none of the coordinates in Sc\nk move significantly, with the\npossible exception of coordinate ik.\nLemma C.8 (No coordinates in Sc\nk \\ {ik} move significantly during part (A)). Suppose Tk+1 < \u221e.\nThen there are \u03c1k+1(\u03f5k+1) > 0 and \u03b1k+1(\u03c1, \u03f5k+1) > 0 such that for all \u03c1 < \u03c1k+1 and \u03b1 < \u03b1k+1,\nthe following holds. There is a constant c > 0 depending on k such that for all i \u2208 Sc\nk \\ {ik} and\nt \u2208 [\u00aftk, tk+1],\n|u\u03b1,i(t) \u2212 u\u03b1,i(\u00aftk)|, |v\u03b1,i(t) \u2212 v\u03b1,i(\u00aftk)| < \u03b1c + \u00af\u03f5k .\nProof. The previous lemma combined with the inductive hypothesis gives\ntk+1 \u2212 \u00aftk < \u2206k(ik) + 2\u03f5k+1 \\ {ik}.\nWe analyze the movement of each coordinate i \u2208 Sc\nk \\ {ik} by breaking into two cases:\n\u2022 Coordinate i \u0338= ik such that bk\ni \u2208 (0, 2). By Assumption 4.3, there is a unique winning co-\nordinate so bk\ni \u2212 \u03c4gi(\u03b8k) \u2208 (c, 2 \u2212 c) for some constant c > 0 for all \u03c4 \u2208 [0, tk+1 \u2212 \u00aftk] \u2286\n[0, \u2206k(ik) + 2\u03f5k+1]. By Lemma C.6, log\u03b1(w\u03b1,i(t)) \u2208 (\u2212c/2, 2 \u2212 c/2) for all times t \u2208 [\u00aftk, tk+1].\nSo by Lemma D.1, |u\u03b1,i(t)|, |v\u03b1,i(t)| \u2264 \u03b1c/4.\n\u2022 Coordinate i \u0338= ik such that bk\ni = 0. By Lemma D.4, we must be in the corner case where\ni \u2208 Sk\u22121 \u2229 Sc\nk (i.e., the coordinate was active in the previous stage but was dropped from the\nsupport in this stage).\nBy Lemma D.4, since bk\ni = 0 we have gi(\u03b8k) < 0. By Lemma C.6, this means sgn(gi(\u03b8\u03b1(t))) =\nsgn(gi(\u03b8k)) < 0 for all t \u2208 (\u00aftk, tk+1).\nWe break the analysis into two parts. Since bk\ni = 0, the sign is sk\ni = +1. The inductive\nhypothesis \u2225\u03b8\u03b1(\u00aftk)\u2212\u03b8k\u2225 < \u00af\u03f5k implies that |u\u03b1,i(\u00aftk)\u2212zk\ni | < \u00af\u03f5k and |v\u03b1,i(\u00aftk)\u2212zk\ni | < \u00af\u03f5k. For small\nenough \u00af\u03f5k this means that sgn(u\u03b1,i(\u00aftk)) = sgn(v\u03b1,i(\u00aftk)) = +1. Now let t\u2217 = min(tk+1, inf{t >\n\u00aftk : v\u03b1,i(t) = 0}). Since u\u03b1,i(t) > v\u03b1,i(t) without loss of generality (see Section C.3), we have\nsgn(u\u03b1,i(t)) = sgn(v\u03b1,i(t)) = +1 for all t \u2208 [\u00aftk, t\u2217]. So du\u03b1,i(t)\ndt\n, dv\u03b1,i(t)\ndt\n< 0 for all t \u2208 [\u00aftk, t\u2217]. So,\nfor any t \u2208 [\u00aftk, t\u2217],\n|u\u03b1,i(t) \u2212 u\u03b1,i(\u00aftk)|, |v\u03b1,i(t) \u2212 v\u03b1,i(\u00aftk)| < \u00af\u03f5k\nAlso, since log\u03b1(w\u03b1,i(t\u2217)) \u2248 1, by Lemma C.6 we have t\u2217 > c > 0 for some constant c\nindependent of \u03b1. So for all t \u2208 [t\u2217, tk+1] we have bk\ni \u2212 \u03c4gi(\u03b8k) \u2208 (c, 2 \u2212 c) for some constant\nc > 0. So |u\u03b1,i(t)|, |v\u03b1,i(t)| \u2264 \u03b1c/4 for all t \u2208 [t\u2217, tk+1]. The conclusion follows by triangle\ninequality.\n\u2022 Coordinate i \u0338= ik such that bk\ni = 2. The analysis is analogous to the case bk\ni = 0, except that\nwe have sk\ni = \u22121 instead and gi(\u03b8k) > 0 by Lemma D.4.\nFinally, we use this conclude that tk+1 \u2248 Tk+1 and that the weights at coordinate ik are the\nonly weights that change significantly, and by an amount approximately \u03c1.\n34\nLemma C.9 (Coordinate ik wins the part (A) race at time tk+1 \u2248 Tk+1). Suppose that Tk+1 < \u221e.\nThen there are \u03c1k+1(\u03f5k+1) > 0 and \u03b1k+1(\u03c1, \u03f5k+1) > 0 such that for all \u03c1 < \u03c1k+1 and \u03b1 < \u03b1k+1, the\nfollowing holds.\n|tk+1 \u2212 Tk+1| < \u03f5k+1 ,\nu\u03b1,ik(tk+1) \u2208 [\u03c1, 3\u03c1] ,\nsgn(v\u03b1,ik(tk+1)) = sk+1\nik\n.\nProof. Let us analyze the case that bk\nik \u2208 (0, 2). Notice that bk+1\nik\n= bk\nik \u2212 \u2206k(ik)gik(\u03b8k) \u2208 {0, 2}\nand that if bk+1\ni\n= 0 then gik(\u03b8k) > 0 and if it is 2 then bk+1\nik\n= gik(\u03b8k) < 0. So by Lemma C.6, for\nall times t \u2208 [\u00aftk, min(tk+1, Tk+1 \u2212 \u03f5k+1)], we have w\u03b1,ik(t) \u2208 (c, 2 \u2212 c) for some c > 0. So for small\nenough \u03b1 by Lemma D.1, |u\u03b1,ik(t)|, |v\u03b1,ik(t)| \u2264 \u03b1c/2. Combining this with Lemma C.8, we see that\nfor t \u2208 [\u00aftk, min(tk+1, Tk+1 \u2212 \u03f5k+1)] we have\n\u2225u\u03b1(t) \u2212 u\u03b1(\u00aftk)\u2225 + \u2225v\u03b1(t) \u2212 v\u03b1(\u00aftk)\u2225 < 2(\u03b1c + \u00af\u03f5k)p < \u03c1 ,\nfor small enough \u03b1.\nSo by definition of tk+1 we must have tk+1 > Tk+1 \u2212 \u03f5k+1.\nCombined\nwith Lemma C.7, we conclude that |Tk+1 \u2212 tk+1| < \u03f5k+1, which is the first claim of the lemma.\nFurthermore, by Lemma C.8,\nX\ni\u2208Sc\nk\\{ik}\n|u\u03b1,i(tk+1) \u2212 u\u03b1,i(\u00aftk)| + |v\u03b1,i(tk+1) \u2212 v\u03b1,i(\u00aftk)| \u2264 2p(\u03b1c + \u00af\u03f5k)) < \u03c1/2,\nso by definition of tk+1 and triangle inequality we have |u\u03b1,ik(tk+1)|+|v\u03b1,ik(tk+1)| \u2265 4\u03c1\u2212\u03c1/2 = 7\u03c1/2.\nAlso, since u2\n\u03b1,ik(tk+1) \u2212 v2\n\u03b1,ik(tk+1) = \u0398(\u03b12) we have u\u03b1,ik(tk+1) \u2208 [\u03c1, 3\u03c1]. Finally, if bk+1\nik\n= 2, then\nsk+1\nik\n= \u22121 and log\u03b1(w\u03b1,ik(tk+1)) > 1.5 so sgn(v\u03b1,ik(t)) < 0 by Lemma D.3; analogously, if bk+1\nik\n= 0,\nwe have sk+1\nik\n= 1 and log\u03b1(w\u03b1,ik(tk+1) < 0.5 so sgn(v\u03b1,ik(tk+1) > 0.\nThe case bk\nik \u2208 {0, 2} can be proved similarly to the analysis in Lemma C.8, where one shows\nthat during the first period of time the magnitudes of |uik(t)| and |vik(t)| decrease, until the sign of\nvik flips and they once again increase.\nWe have shown the claims (21), (22), (23) (24), and (25) for the time tk+1. In fact, if we let\nt\u2032\nk+1 \u2208 [\u00aftk, \u221e) be the first time t such that u\u03b1,ik(t) = \u03c1 we still have (21), (22), (23) and (25) by the\nsame analysis as above, and (24) can be replaced with the slightly more convenient\nu\u03b1,ik(t\u2032\nk+1) = \u03c1 .\nC.6.2\nAnalysis in case where Tk+1 = \u221e\nIn this case that Tk+1, we just have to show that the weights remain close to \u03b8k. We show that for\nany \u03f5k+1 > 0, there is \u03b1k+1(\u03f5k+1) > 0 such that for all \u03b1 < \u03b1k+1 and times t \u2208 [Tk + \u03f5k+1, T \u2217],\n\u2225\u03b8\u03b1(t) \u2212 \u03b8k\u2225 < \u03f5k+1.\nWe can use Lemmas C.5 and C.6, which were developed for the case of Tk+1 < \u221e, but still hold for\nTk+1 = \u221e. Lemma C.5 guarantees that the weights do not move much until time tk+1, and so we\nonly need to show that tk+1 \u2265 T \u2217 when we take \u03c1 small enough. For this, observe that gi(\u03b8k) = 0\n35\nfor all i \u0338\u2208 Sk, because otherwise Tk+1 < \u221e. Therefore Lemma C.6 guarantees that until time\nmin(T\u2217, tk+1) all weights are close to the original on the logarithmic scale. Namely,\n\u2225 log\u03b1(w\u03b1(t)) \u2212 bk\u2225 < \u03c1\u03f5k+1 + C\u03c1\u2032(T \u2217 \u2212 \u00aftk)\nFurthermore, by the non-degeneracy Assumption 4.3 we know that bk\ni \u2208 (0, 2) for all i \u0338\u2208 Sk by\nLemma D.4. So if we take \u03c1 small enough and \u03b1k+1 small enough, we must have that tk+1 \u2265 T \u2217.\nC.7\nDynamics from time tk to time \u00aftk (Nonlinear evolution for O(1) unrescaled\ntime)\nSuppose that we know for some k \u2264 K that for any \u03f5k > 0, there is \u03c1k(\u03f5k) > 0 such that for all\n\u03c1 < \u03c1k there is \u03b1k(\u03c1, \u03f5k) > 0 such that for all \u03b1 < \u03b1k, there is a time tk = tk(\u03b1, \u03c1, \u03f5k) satisfying\n|Tk \u2212 tk| < \u03f5k\n(31)\n\u2225\u03b8\u03b1(tk) \u2212 \u03b8k\u22121\u2225 < \u03f5k\n(32)\n\u2225 log\u03b1(w\u03b1(tk)) \u2212 bk\u2225 < \u03f5k\n(33)\nu\u03b1,ik\u22121(tk) = \u03c1 ,\n(34)\nsgn(v\u03b1,ik\u22121(tk)) = sk\nik\u22121 .\n(35)\nNow we will show that for any \u00af\u03f5k > 0, there is \u00af\u03b1k = \u00af\u03b1k(\u00af\u03f5k) > 0 such that for all 0 < \u03b1 < \u00af\u03b1k,\nthere is a time \u00aftk = \u00aftk(\u03b1, \u00af\u03f5k) satisfying\n|Tk \u2212 \u00aftk| < \u00af\u03f5k\n(36)\n\u2225\u03b8\u03b1(\u00aftk) \u2212 \u03b8k\u2225 < \u00af\u03f5k\n(37)\n\u2225 log\u03b1(w\u03b1(\u00aftk)) \u2212 bk\u2225 < \u00af\u03f5k\n(38)\nWe give the construction for \u00aftk. For any desired accuracy \u00af\u03f5k > 0 in this stage, we will construct\nan accuracy \u03f5k = \u03f5k(\u00af\u03f5k) = \u00af\u03f5k/3 > 0. We will also construct a \u03c1 = \u03c1(\u03f5k) > 0 which is sufficiently\nsmall, and we will construct an cutoff for \u03b1 equal to \u00af\u03b1k = \u00af\u03b1k+1(\u00af\u03f5k) > 0 which satisfies \u00af\u03b1k < \u03b1k(\u03c1, \u03f5k).\nThe values for these parameters \u03f5k and \u03c1 and \u00af\u03b1k will be chosen in the following lemma, and will\ndepend only on \u00af\u03f5k.\nLemma C.10 (New local minimum reached in time O(1/ log(1/\u03b1))). For any \u00af\u03f5k > 0, we can\nchoose \u00af\u03b1k = \u00af\u03b1k(\u00af\u03f5k) > 0 small enough so that, for any 0 < \u03b1 < \u00af\u03b1k, there is \u00aftk = \u00aftk(\u03b1, \u00af\u03f5k) for which\nconditions (36) to (38) hold.\nFurthermore, there is a constant C\u2032\u2032 independent of \u03b1 such that |\u03b8\u03b1(t)|/|\u03b8\u03b1(tk)| \u2208 [1/C\u2032\u2032, C\u2032\u2032]2p\nat all times t \u2208 [tk, \u00aftk].\nProof. Let tk = tk(\u03b1, \u03c1, \u03f5k) be given by the induction. Let us compare the dynamics starting at\n\u03b8\u03b1(tk) with the dynamics starting at \u02dc\u03b8(tk) = (\u02dcu(tk), \u02dcv(tk)) which is given by\n\u02dcui(tk) =\n(\nu\u03b1,i(tk),\ni \u2208 Sk\u22121 \u222a {ik\u22121}\n0,\notherwise\nand\n\u02dcvi(tk) =\n(\nv\u03b1,i(tk),\ni \u2208 Sk\u22121 \u222a {ik\u22121}\n0,\notherwise\nand run with\nd\u02dc\u03b8\ndt = \u2212 log(1/\u03b1)\u2207wL(\u02dc\u03b8) .\n36\nBy Assumption 4.5 we know there exists a unique solution \u02dc\u03b8 : [tk, \u221e) \u2192 Rp as long as we take\n\u03f5k small enough because supp(\u02dc\u03b8(tk)) = Sk\u22121 \u222a {ik\u22121} and \u2225\u02dc\u03b8i(tk) \u2212 \u03b8k\u22121\u2225 < \u03f5k. Furthermore, by\nAssumption 4.5 if we take \u03f5k small enough there must be a time \u03c4 := \u03c4(\u00af\u03f5k, \u03c1) < \u221e such that\n\u2225\u02dc\u03b8(t) \u2212 \u03b8k\u2225 < \u00af\u03f5k/2 for t \u2265 tk + \u03c4/ log(1/\u03b1)\n(39)\nDefine\n\u00aftk = tk + \u03c4/ log(1/\u03b1).\nSo for \u03b1 small enough, |Tk \u2212 \u00aftk| < 2\u03f5k < \u00af\u03f5k, proving (36).\nWe now compare \u03b8\u03b1(\u00aftk) with \u02dc\u03b8(\u00aftk), and show that if we take \u03b1 small enough, then the dynamics\nof \u02dc\u03b8 closely match the dynamics of \u03b8\u03b1(t) for times tk+O(1/ log(1/\u03b1)). The argument uses Gronwall\u2019s\ninequality. Let t\u2217 = inf{t > tk : \u2225\u02dc\u03b8(t\u2217) \u2212 \u03b8\u03b1(t)\u2225 > 1/3}. For times t \u2208 [tk, t\u2217) by Lemma D.7 we\nhave\n\u2225 d\ndt\n\u02dc\u03b8(t) \u2212 d\ndt\u03b8\u03b1(t)\u2225 = log(1/\u03b1)\u2225\u2207\u03b8L(\u02dc\u03b8(t)) \u2212 \u2207\u03b8L(\u03b8\u03b1(t))\u2225 \u2264 K \u02dc\u03b8(t) log(1/\u03b1)\u2225\u02dc\u03b8(t) \u2212 \u03b8\u03b1(t)\u2225,\nwhere K \u02dc\u03b8(t) is the smoothness constant from Lemma D.7. Note that since \u2225\u02dc\u03b8(t)\u2225 < \u221e for large\nenough t by (39), the trajectory of \u02dc\u03b8 must lie in a compact set. Therefore, there must be a finite\nset of times s1, . . . , sm \u2208 [tk, t\u2217) such that \u222at\u2208[tk,t\u2217)B(\u02dc\u03b8(t), 1/2) \u2286 \u222am\ni=1B(\u02dc\u03b8(si), 3/4). So letting\nC = maxm\ni=1 K \u02dc\u03b8(si) < \u221e for all times t \u2208 [tk, t\u2217) we have\nd\ndt\u2225\u02dc\u03b8(t) \u2212 \u03b8\u03b1(t)\u2225 \u2264 C log(1/\u03b1)\u2225\u02dc\u03b8(t) \u2212 \u03b8\u03b1(t)\u2225 .\nBy Gronwall\u2019s inequality, for all times t \u2208 [tk, t\u2217),\n\u2225\u02dc\u03b8(t) \u2212 \u03b8\u03b1(t)\u2225 \u2264 \u2225\u02dc\u03b8(tk) \u2212 \u03b8\u03b1(tk)\u2225 exp(C log(1/\u03b1)(t \u2212 tk)) .\nWe know from Lemma C.8 that there is a constant c > 0 such that for any small enough 0 < \u03b1 < \u03b1k,\nsuch that\n\u2225\u02dc\u03b8(tk) \u2212 \u03b8\u03b1(tk)\u2225 < \u03b1c\nIf we take \u03b1 small enough that \u03b1c exp(C\u03c4) < \u00af\u03f5k/2 < 1/3, we must have t\u2217 > tk + \u03c4/ log(1/\u03b1) and\nso we prove (37)\n\u2225\u03b8k \u2212 \u03b8\u03b1(\u00aftk)\u2225 \u2264 \u00af\u03f5k/2 + \u2225\u02dc\u03b8(\u00aftk) \u2212 \u03b8\u03b1(\u00aftk)\u2225 < \u00af\u03f5k .\nIt remains to show that (38) is satisfied. Since \u2225\u02dc\u03b8(t) \u2212 \u03b8\u03b1(t)\u2225 < 1/3 for all t \u2208 [tk, \u00aftk], it holds\nthat the trajectory of \u03b8\u03b1(t) lies in a compact set. So by Lemma D.7 we have \u2225g(\u03b8\u03b1(t))\u2225 < C\u2032 for\nsome constant C\u2032 at all times t \u2208 [tk, \u00aftk]. Since\n1\nlog(1/\u03b1)| dw\u03b1,i\ndt | = |w\u03b1,i(t)||gi(w\u03b1(t))| < C\u2032|w\u03b1,i(t)|, we\nmust have |w\u03b1,i(t)|/|w\u03b1,i(tk)| \u2208 [1/C\u2032\u2032, C\u2032\u2032] for some constant C\u2032\u2032 independent of \u03b1 and all t \u2208 [tk, \u00aftk].\nTherefore, (38) follows from (33). A similar argument shows that |\u03b8\u03b1(t)/\u03b8\u03b1(tk)| \u2208 [1/C\u2032\u2032, C\u2032\u2032]2p.\n37\nC.8\nConcluding the proof of Theorem C.4\nWe have shown that Theorem 4.1 is true for solutions \u03b8\u03b1 : [0, T \u2217] \u2192 R2p to the gradient flow,\nwhere T\u2217 \u2208 (TK, TK+1). To establish Theorem C.4 it remains only to show that for any T\u2217 \u2208\n(TK, TK+1) and small enough \u03b1 such a solution to the gradient flow exists and is unique. To\nsee this, note that in the inductive proof of the invariants we construct a sequence of times\n0 = \u00aft0 \u2264 t1 \u2264 \u00aft1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u00aftK \u2264 tK+1 > T\u2217, where we guarantee that any gradient flow solution\n\u03b8\u03b1 : [0, tk+1] \u2192 Rp satisfies \u03b8\u03b1 \u2208 \u222ak\u2208{0,...,K}B(\u03b8k, 1) for all t \u2208 \u222ak\u2208{0,...,K}[\u00aftk, tk+1]. And also for\nt \u2208 \u222ak\u2208{0,...,K\u22121}[tk, \u00aftk+1], we have \u03b8\u03b1(t) \u2208 B(0, C\u2032\u2032\nk\u03b8k) for some constant C\u2032\u2032\nk independent of \u03b1 by\nLemma C.10. So \u03b8\u03b1(t) \u2208 B(0, CK) for some constant CK at all times t \u2208 [0, T \u2217]. By Lemma D.7,\nthe loss gradient \u2207\u03b8L(\u03b8) = (v\u2299g(\u03b8), u\u2299g(\u03b8)) is Lipschitz-continuous on the compact set B(0, CK).\nSo \u03b8\u03b1 : [0, T \u2217] \u2192 Rp exists and is unique by the Cauchy-Lipschitz theorem.\nD\nTechnical lemmas\nD.1\nRelating the sum of the weights to the original weights using the conserva-\ntion law\nLemma D.1. If for some constant 0 < c < 1 we have log\u03b1(w\u03b1,i(t)) \u2208 (c, 2 \u2212 c), then for small\nenough \u03b1\nmax(|u\u03b1,i(t)|, |v\u03b1,i(t)|) \u2264 \u03b1c/2 .\nProof. Let \u02dcw\u03b1(t) = u\u03b1(t) \u2212 v\u03b1(t). By the conservation law (5), w\u03b1,i(t) \u02dcw\u03b1,i(t) = w\u03b1,i(0) \u02dcw\u03b1,i(0) =\nu\u03b1,i(0)2 \u2212 v\u03b1,i(0)2. By the non-degeneracy of initialization (Assumption 4.3), the right-hand-side\nis \u0398(\u03b12).\nSo if log\u03b1(w\u03b1,i(t)) \u2208 (c, 2 \u2212 c) then for small enough \u03b1, we have log\u03b1(| \u02dcw\u03b1,i(t)|) \u2208\n(3c/4, 2 \u2212 3c/4). So |u\u03b1,i(t)| \u2264 |w\u03b1,i(t) + \u02dcw\u03b1,i(t)| \u2264 \u03b1c/2 and |v\u03b1,i(t)| \u2264 |w\u03b1,i(t) \u2212 \u02dcw\u03b1,i(t)| \u2264 \u03b1c/2.\nLemma D.2. If for some constant 0 < c we have log\u03b1(w\u03b1,i(t)) \u0338\u2208 (\u2212c, 2 + c), then for small enough\n\u03b1,\n|u\u03b1,i(t)| > 1 .\nProof. Define \u02dcw\u03b1 = u\u03b1\u2212v\u03b1 as in the proof of Lemma D.1. If log\u03b1(w\u03b1,i(t)) < \u2212c then log\u03b1(| \u02dcw\u03b1,i(t)|) >\n2 \u2212 c/2 for small enough \u03b1, so ui(t) > \u03b1\u2212c \u2212 \u03b12\u2212c/2 > 1. Similarly, if log\u03b1(w\u03b1,i(t)) > 2 + c then\nlog\u03b1(| \u02dcw\u03b1,i(t)|) < \u2212c/2 so |ui(\u03b1)| > \u03b1\u2212c/2 \u2212 \u03b12+c > 1.\nLemma D.3. If for some constant c > 0, there is small enough \u03b1 such that if we have log\u03b1(w\u03b1,i(t)) >\n1 + c then sgn(v\u03b1,i(t)) < 0. Otherwise, if log\u03b1(w\u03b1,i(t)) < 1 \u2212 c then sgn(v\u03b1,i(t)) > 0.\nProof. Follows from v\u03b1 = 1\n2(w\u03b1 \u2212 \u02dcw\u03b1). Recall that w\u03b1(t) > 0 and notice that \u02dcw\u03b1(t) > 0. In the first\ncase, w\u03b1,i(t) < \u03b11+c and \u02dcw\u03b1,i(t) > \u03b11\u2212c/2. In the latter case w\u03b1,i(t) > \u03b11\u2212c and \u02dcw\u03b1,i(t) < \u03b11+c/2.\nD.2\nSign of gradients on coordinates that leave support\nLemma D.4. For any k \u2265 1 and i \u2208 Sc\nk, if bk\ni \u2208 {0, 2} then we must have i \u2208 supp(uk\u22121)\\supp(uk),\nand we must have gi(uk) < 0 if bk\ni = 0 and gi(\u03b8k) > 0 if bk\ni = 2. In particular, \u2206k(ik) > 0 for all k.\nProof. This is by induction on k and using the non-degeneracy Assumption 4.3.\n38\nD.3\nLocal lipschitzness and smoothness\nWe provide several technical lemmas on the local Lipschitzness and smoothness of \u2113, h, and g.\nLemma D.5. The function \u2113(y, \u00b7) is locally Lipschitz and smooth in its second argument: for any\nR > 0, there exists KR such that for any \u03b6, \u03b6\u2032 \u2208 B(0, R)\n|\u2113(y, \u03b6) \u2212 \u2113(y, \u03b6\u2032)| \u2264 KR\u2225\u03b6 \u2212 \u03b6\u2032\u2225\n\u2225D\u2113(y, \u03b6) \u2212 D\u2113(y, \u03b6\u2032)\u2225 \u2264 KR\u2225\u03b6 \u2212 \u03b6\u2032\u2225,\nalmost surely over y. Here D\u2113(y, \u00b7)\u22a4 \u2208 Rdout is the derivative in the second argument.\nProof. Since \u2113 is continuously twice-differentiable, for each y \u2208 Rdy, \u03b6 \u2208 Rdout there is Ky,\u03b6 < \u221e\nsuch that for all y \u2208 B(y, 1/Ky,\u03b6) and \u03b6\u2032 \u2208 B(\u03b6, 1/Ky,\u03b6) we have\n\u2225D\u2113(y\u2032, \u03b6\u2032)\u2225 \u2264 Ky,\u03b6\nand\n\u2225D2\u2113(y\u2032, \u03b6\u2032)\u2225 \u2264 Ky,\u03b6 ,\nwhere D\u2113 and D2\u2113 denote the first and second derivative in the second argument. So for all such\ny\u2032 \u2208 B(y, 1/Ky,\u03b6) and \u03b6\u2032, \u03b6\u2032\u2032 \u2208 B(\u03b6, 1/Ky,\u03b6) we have\n|\u2113(y\u2032, \u03b6\u2032) \u2212 \u2113(y\u2032, \u03b6\u2032\u2032)| \u2264 Ky,\u03b6\u2225\u03b6\u2032 \u2212 \u03b6\u2032\u2032\u2225\nand\n|D\u2113(y\u2032, \u03b6\u2032) \u2212 D\u2113(y\u2032, \u03b6\u2032\u2032)| \u2264 Ky,\u03b6\u2225\u03b6\u2032 \u2212 \u03b6\u2032\u2032\u2225 .\nCover the set {(y, \u03b6) : \u2225y\u2225 \u2264 C, \u2225\u03b6\u2225 \u2264 R} with the balls \u222ayB(y, 1/Ky,\u03b6). By compactness, there\nis a finite subcover (y1, \u03b61), . . . , (yr, \u03b6r), so we can take KR = maxi\u2208[r] Kyi,\u03b6i < \u221e and the lemma\nholds since \u2225y\u2225 \u2264 C almost surely by Assumption 2.1.\nLemma D.6. The function h(x; \u00b7) is locally bounded, Lipschitz and smooth in its second argument:\nfor any R > 0 there exists KR such that for any \u03c8, \u03c8\u2032 \u2208 B(0, R),\n\u2225h(x; \u03c8)\u2225 \u2264 KR\n\u2225h(x; \u03c8) \u2212 h(x; \u03c8\u2032)\u2225 \u2264 KR\u2225\u03c8 \u2212 \u03c8\u2032\u2225\n\u2225Dh(x; \u03c8) \u2212 Dh(x; \u03c8\u2032)\u2225 \u2264 KR\u2225\u03c8 \u2212 \u03c8\u2032\u2225 ,\nalmost surely over x. Here Dh(x, \u00b7) \u2208 Rdout \u00d7 Rp is the derivative in the second argument.\nProof. Analogous to proof of Lemma D.5, using continuous twice-differentiability of h and bounded-\nness of \u2225x\u2225.\nLemma D.7 (Local Lipschitzness of loss and loss derivative). When \u03b8 = (u, v) \u2208 R2p and\nfNN(x; \u03b8) = h(x; u \u2299 u) the following holds for g(\u03b8) defined in (4). For any R > 0, there exists\nKR < \u221e such that for any \u03b8, \u03b8\u2032 \u2208 B(0, KR),\n\u2225g(\u03b8) \u2212 g(\u03b8\u2032)\u2225 \u2264 KR\u2225\u03b8 \u2212 \u03b8\u2032\u2225\n\u2225\u2207\u03b8L(\u03b8) \u2212 \u2207RL(\u03b8\u2032)\u2225 \u2264 K\u03b8\u2225\u03b8 \u2212 \u03b8\u2032\u2225\n|L(\u03b8) \u2212 L(\u03b8\u2032)| \u2264 KR\u2225\u03b8 \u2212 \u03b8\u2032\u2225 .\nProof. Let \u03b8 = (u, v), \u03b8\u2032 = (u\u2032, v\u2032). This follows immediately from the local Lipschitzness and\nsmoothness of h and \u2113 in Lemmas D.5 and D.6, as well as\n\u2225g(\u03b8) \u2212 g(\u03b8\u2032)\u2225 = \u2225 Ex,y[Dh(x; u \u2299 v)\u22a4D\u2113(y, h(x; u \u2299 v))\u22a4 \u2212 Dh(x; u\u2032 \u2299 v\u2032)\u22a4D\u2113(y, h(x; u\u2032 \u2299 v\u2032))\u22a4]\u2225 .\n39\n"
  },
  {
    "title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "link": "https://arxiv.org/pdf/2306.06546.pdf",
    "upvote": "8",
    "text": "High-Fidelity Audio Compression\nwith Improved RVQGAN\nRithesh Kumar*\nDescript, Inc.\nPrem Seetharaman*\nDescript, Inc.\nAlejandro Luebs\nDescript, Inc.\nIshaan Kumar\nDescript, Inc.\nKundan Kumar\nDescript, Inc.\nAbstract\nLanguage models have been successfully used to model natural signals, such as\nimages, speech, and music. A key component of these models is a high quality\nneural compression model that can compress high-dimensional natural signals\ninto lower dimensional discrete tokens. To that end, we introduce a high-fidelity\nuniversal neural audio compression algorithm that achieves 90x compression of\n44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining\nadvances in high-fidelity audio generation with better vector quantization tech-\nniques from the image domain, along with improved adversarial and reconstruction\nlosses. We compress all domains (speech, environment, music, etc.) with a single\nuniversal model, making it widely applicable to generative modeling of all audio.\nWe compare with competing audio compression algorithms, and find our method\noutperforms them significantly. We provide thorough ablations for every design\nchoice, as well as open-source code and trained model weights. We hope our work\ncan lay the foundation for the next generation of high-fidelity audio modeling.\n1\nIntroduction\nGenerative modeling of high-resolution audio is difficult due to high dimensionality (~44,100 samples\nper second of audio) [24, 19], and presence of structure at different time-scales with both short and\nlong-term dependencies. To mitigate this problem, audio generation is typically divided into two\nstages: 1) predicting audio conditioned on some intermediate representation such as mel-spectrograms\n[24, 28, 19, 30] and 2) predicting the intermediate representation given some conditioning information,\nsuch as text [35, 34]. This can be interpreted as a hierarchical generative model, with observed\nintermediate variables. Naturally, an alternate formulation is to learn the intermediate variables using\nthe variational auto-encoder (VAE) framework, with a learned conditional prior to predict the latent\nvariables given some conditioning. This formulation, with continuous latent variables and training an\nexpressive prior using normalizing flows has been quite successful for speech synthesis [17, 36].\nA closely related idea is to train the same varitional-autoencoder with discrete latent variables using\nVQ-VAE [38]. Arguably, discrete latent variables are a better choice since expressive priors can be\ntrained using powerful autoregressive models that have been developed for modeling distributions\nover discrete variables [27]. Specifically, transformer language models [39] have already exhibited\nthe capacity to scale with data and model capacity to learn arbitrarily complex distributions such as\ntext[6], images[12, 44], audio [5, 41], music [1], etc. While modeling the prior is straightforward,\nmodeling the discrete latent codes using a quantized auto-encoder remains a challenge.\n*Equal contribution to this work. Address correspondence to papers@descript.com, or raise an issue at\nhttps://github.com/descriptinc/descript-audio-codec.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.06546v2  [cs.SD]  26 Oct 2023\nLearning these discrete codes can be interpreted as a lossy compression task, where the audio\nsignal is compressed into a discrete latent space by vector-quantizing the representations of an\nautoencoder using a fixed length codebook. This audio compression model needs to satisfy the\nfollowing properties: 1) Reconstruct audio with high fidelity and free of artifacts 2) Achieve high\nlevel of compression along with temporal downscaling to learn a compact representation that discards\nlow-level imperceptible details while preserving high-level structure [38, 33] 3) Handle all types of\naudio such as speech, music, environmental sounds, different audio encodings (such as mp3) as well\nas different sampling rates using a single universal model.\nWhile the recent neural audio compression algorithms such as SoundStream [46] and EnCodec [8]\npartially satisfy these properties, they often suffer from the same issues that plague GAN-based\ngeneration models. Specifically, such models exhibit audio artifacts such as tonal artifacts [29], pitch\nand periodicity artifacts [25] and imperfectly model high-frequencies leading to audio that are clearly\ndistinguishable from originals. These models are often tailored to a specific type of audio signal such\nas speech or music and struggle to model generic sounds. We make the following contributions:\n\u2022 We introduce Improved RVQGAN a high fidelity universal audio compression model, that\ncan compress 44.1 KHz audio into discrete codes at 8 kbps bitrate (~90x compression) with\nminimal loss in quality and fewer artifacts. Our model outperforms state-of-the-art methods\nby a large margin even at lower bitrates (higher compression) , when evaluated with both\nquantitative metrics and qualitative listening tests.\n\u2022 We identify a critical issue in existing models which don\u2019t utilize the full bandwidth due\nto codebook collapse (where a fraction of the codes are unused) and fix it using improved\ncodebook learning techniques.\n\u2022 We identify a side-effect of quantizer dropout - a technique designed to allow a single\nmodel to support variable bitrates, actually hurts the full-bandwidth audio quality and\npropose a solution to mitigate it.\n\u2022 We make impactful design changes to existing neural audio codecs by adding periodic\ninductive biases, multi-scale STFT discriminator, multi-scale mel loss and provide thorough\nablations and intuitions to motivate them.\n\u2022 Our proposed method is a universal audio compression model, capable of handling speech,\nmusic, environmental sounds, different sampling rates and audio encoding formats.\nWe provide code 1, models, and audio samples 2 that we encourage the reader to listen to.\n2\nRelated Work\nHigh fidelity neural audio synthesis: Recently, generative adversarial networks (GANs) have\nemerged as a solution to generate high-quality audio with fast inference speeds, due to the feed-\nforward (parallel) generator. MelGAN [19] successfully trains a GAN-based spectrogram inversion\n(neural vocoding) model. It introduces a multi-scale waveform discriminator (MSD) to penalize\nstructure at different audio resolutions and a feature matching loss that minimizes L1 distance\nbetween discriminator feature maps of real and synthetic audio. HifiGAN [18] refines this recipe by\nintroducing a multi-period waveform discriminator (MPD) for high fidelity synthesis, and adding\nan auxiliary mel-reconstruction loss for fast training. UnivNet [16] introduces a multi-resolution\nspectrogram discriminator (MRSD) to generate audio with sharp spectrograms. BigVGAN [21]\nextends the HifiGAN recipe by introducing a periodic inductive bias using the Snake activation\nfunction [47]. It also replaces the MSD in HifiGAN with the MRSD to improve audio quality and\nreduce pitch, periodicity artifacts [25]. While these the GAN-based learning techniques are used for\nvocoding, these recipes are readily applicable to neural audio compression. Our Improved RVQGAN\nmodel closely follows the BigVGAN training recipe, with a few key changes. Our model uses a\nnew multi-band multi-scale STFT discriminator that alleviates aliasing artifacts, and a multi-scale\nmel-reconstruction loss that better model quick transients.\nNeural audio compression models: VQ-VAEs [38] have been the dominant paradigm to train neural\naudio codecs. The first VQ-VAE based speech codec was proposed in [13] operating at 1.6 kbps. This\nmodel used the original architecture from [38] with a convolutional encoder and an autoregressive\n1https://github.com/descriptinc/descript-audio-codec\n2https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5\n2\nwavenet [27] decoder. SoundStream [46] is one of the first universal compression models capable\nof handling diverse audio types, while supporting varying bitrates using a single model. They use a\nfully causal convolutional encoder and decoder network, and perform residual vector quantization\n(RVQ). The model is trained using the VQ-GAN [12] formulation, by adding adversarial and feature-\nmatching losses along with the multi-scale spectral reconstruction loss. EnCodec [8] closely follows\nthe SoundStream recipe, with a few modifications that lead to improved quality. EnCodec uses a multi-\nscale STFT discriminator with a multi-scale spectral reconstruction loss. They use a loss balancer\nwhich adjusts loss weights based on the varying scale of gradients coming from the discriminator.\nOur proposed method also uses a convolutional encoder-decoder architecture, residual vector quan-\ntization and adversarial, perceptual losses. However, our recipe has the following key differences:\n1) We introduce a periodic inductive bias using Snake activations [47, 21] 2) We improve codebook\nlearning by projecting the encodings into a low-dimensional space [44] 3) We obtain a stable training\nrecipe using best practices for adversarial and perceptual loss design, with fixed loss weights and\nwithout requiring a sophisticated loss balancer. We find that our changes lead to a near-optimal\neffective bandwidth usage. This allows our model to outperform EnCodec even with 3x lower bitrate.\nLanguage modeling of natural signals : Neural language models have demonstrated great success\nin diverse tasks such as open-ended text generation [6] with in-context learning capabilities. A\nkey-component of these models is self-attention [39], which is capable of modeling complex and\nlong-range dependencies but suffers from a quadratic computational cost with the length of the\nsequence. This cost is unacceptable for natural signals such as images and audio with very high\ndimensionality, requiring a compact mapping into a discrete representation space. This mapping is\ntypically learnt using VQ-GANs [12, 44], followed by training an autoregressive Transformer on the\ndiscrete tokens. This approach has shown success across image [45, 44, 32], audio [5, 41], video and\nmusic [9, 1] domains. Codecs like SoundStream and EnCodec have already been used in generative\naudio models, like AudioLM [5], MusicLM [1], and VALL-E [41]. Our proposed model can serve as\na drop-in replacement for the audio tokenization model used in these methods, allowing for highly\nsuperior audio fidelity, and more efficient learning due to our maximum entropy code representation.\n3\nThe Improved RVQGAN Model\nCodec\nSampling rate (kHz)\nTarget bitrate (kbps)\nStriding factor\nFrame rate (Hz)\n# of 10-bit codebooks\nCompression factor\nProposed\n44.1\n8\n512\n86\n9\n91.16\nEnCodec\n24\n24\n320\n75\n32\n16\n48\n24\n320\n150\n16\n32\nSoundStream\n24\n6\n320\n75\n8\n64\nTable 1: Comparison of compression approaches.\nOur model is built on the framework of VQ-GANs,\nfollowing the same pattern as SoundStream [46] and\nEnCodec [8]. Our model uses the fully convolutional\nencoder-decoder network from SoundStream, that\nperforms temporal downscaling with a chosen strid-\ning factor. Following recent literature, we quantize\nthe encodings using Residual Vector Quantization\n(RVQ), a method that recursively quantizes residuals\nfollowing an initial quantization step with a distinct\ncodebook. Quantizer dropout is applied during train-\ning to enable a single model that can operate at several\ntarget bitrates. Our model is similarly trained using\na frequency domain reconstruction loss along with\nadversarial and perceptual losses.\nAn audio signal with sampling rate fs (Hz), encoder striding factor M, and Nq layers of RVQ produce\na discrete code matrix of shape S \u00d7Nq, where S is the frame rate defined as fs/M. Table 1 compares\nour proposed model against baselines to contrast the compression factors and the frame rate of latent\ncodes. Note that the target bitrate mentioned is an upper bound, since all models support variable\nbitrates. Our model achieves a higher compression factor compared to all baseline methods while\noutperforming them in audio quality, as we show later. Finally, a lower frame rate is desirable when\ntraining a language model on the discrete codes, as it results in shorter sequences.\n3.1\nPeriodic activation function\nAudio waveforms are known to exhibit high periodicity (especially in voiced components, music,\netc.) While current non-autoregressive audio generation architectures are capable of generating high\nfidelity audio, they often exhibit jarring pitch and periodicity artifacts [25]. Moreover, common\n3\nneural network activations (such as Leaky ReLUs) are known to struggle with extrapolating periodic\nsignals, and exhibit poor out-of-distribution generalization for audio synthesis [21].\nTo add a periodic inductive bias to the generator, we adopt the Snake activation function proposed by\nLiu et al. [47] and introduced to the audio domain in the BigVGAN neural vocoding model [21]. It is\ndefined as snake(x) = x + 1\n\u03b1 sin2(\u03b1x) , where \u03b1 controls the frequency of periodic component of\nthe signal. In our experiments, we find replacing Leaky ReLU activations with Snake function to be\nan influential change that significantly improves audio fidelity (Table 2).\n3.2\nImproved residual vector quantization\n0\n5\n10\n15\n20\n25\n30\nCodebook index\n8.8\n9.0\n9.2\n9.4\n9.6\n9.8\n10.0\nEntropy (bits)\nProposed (24kbps)\nProposed\nEnCodec\nProposed (w/ EMA)\nFigure 1: Entropy for each codebook, computed\nusing code usage statistics across a large test set.\nWhile vector quantization (VQ) is a popular method\nto train discrete auto-encoder, there are many practi-\ncal struggles when training them. Vanilla VQ-VAEs\nstruggle from low codebook usage due to poor initial-\nization, leading to a significant portion of the code-\nbook being unused. This reduction in effective code-\nbook size leads to an implicit reduction in target bi-\ntrate, which translates to poor reconstruction quality.\nTo mitigate this, recent audio codec methods use k-\nmeans clustering to initialize the codebook vectors,\nand manually employ randomized restarts [9] when\ncertain codebooks are unused for several batches.\nHowever, we find that the EnCodec model trained at\n24kbps target bitrate, as well as our proposed model\nwith the same codebook learning method (Proposed\nw/ EMA) still suffers from codebook under-utilization (Figure 1).\nTo address this issue, we use two key techniques introduced in the Improved VQGAN image\nmodel[44] to improve codebook usage: factorized codes and L2-normalized codes. Factorization\ndecouples code lookup and code embedding, by performing code lookup in a low-dimensional space\n(8d or 32d) whereas the code embedding resides in a high dimensional space (1024d). Intuitively,\nthis can be interpreted as a code lookup using only the principal components of the input vector that\nmaximally explain the variance in the data. The L2-normalization of the encoded and codebook\nvectors converts euclidean distance to cosine similarity, which is helpful for stability and quality [44].\nThese two tricks along with the overall model recipe significantly improve codebook usage, and\ntherefore bitrate efficiency (Figure 1) and reconstruction quality (Table 2), while being simpler to\nimplement. Our model can be trained using the original VQ-VAE codebook and commitment losses\n[38], without k-means initialization or random restarts. The equations for the modified codebook\nlearning procedure are written in Appendix A\n3.3\nQuantizer dropout rate\n1\n2\n3\n4\n5\n6\n7\n8\nBitrate (kbps)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nMel-reconstruction loss\nDropout prob.\n0.0\n0.25\n0.5\n1.0\nFigure 2: Effect of quantizer dropout on audio\nquality vs bitrate.\nQuantizer dropout was introduced in SoundStream\n[46] to train a single compression model with vari-\nable bitrate. The number of quantizers Nq determine\nthe bitrate, so for each input example we randomly\nsample n \u223c {1, 2, . . . , Nq} and only use the first\nnq quantizers while training. However, we noticed\nthat applying quantizer dropout degrades the audio\nreconstruction quality at full bandwidth (Figure 2).\nTo address this problem, we instead apply quantizer\ndropout to each input example with some probabil-\nity p. Interestingly, we find that dropout probability\np = 0.5 closely matches the reconstruction quality\nof baseline at lower bitrates, while closing the gap\nto full-bandwidth quality of a model trained without\nquantizer dropout (p = 0.0).\n4\nMoreover, we provide additional insight into the practical behavior of quantizer dropout and it\u2019s\ninteraction with RVQ. Firstly, we find that these techniques put together lead the quantized codes to\nlearn most-significant to least significant bits of information with each additional quantizer. When the\ncodes are reconstructed with 1 . . . Nq codebooks, we can see each codebook adds increasing amounts\nof fine-scale detail. We believe this interaction is beneficial when training hierarchical generative\nmodels on top of these codes [5, 41, 1], for example to partition the codes into \u201ccoarse\u201d tokens\n(denoting the most significant codes) and \u201cfine\u201d tokens.\n3.4\nDiscriminator design\nLike prior work, we use multi-scale (MSD) and multi-period waveform discriminators (MPD)\nwhich lead to improved audio fidelity. However, spectrograms of generated audio can still appear\nblurry, exhibiting over-smoothing artifacts in high frequencies[16]. The multi-resolution spectrogram\ndiscriminator (MRSD) was proposed in UnivNet to fix these artifacts and BigVGAN [21] found\nthat it also helps to reduce pitch and periodicity artifacts. However, using magnitude spectrograms\ndiscards phase information which could\u2019ve been otherwise utilized by the discriminator to penalize\nphase modeling errors. Moreover, we find that high-frequency modeling is still challenging for these\nmodels especially at high sampling rates.\nTo address these issues, we use a complex STFT discriminator [46] at multiple time-scales [8] and\nfind that it works better in practice and leads to improved phase modeling. Additionally we find\nthat splitting the STFT into sub-bands slightly improves high frequency prediction and mitigates\naliasing artifacts, since the discriminator can learn discriminative features about a specific sub-band\nand provide a stronger gradient signal to the generator. Multi-band processing was earlier proposed\nin [43] to predict audio in sub-bands which are subsequently summed to produce the full-band audio.\n3.5\nLoss functions\nFrequency domain reconstruction loss: while the mel-reconstruction loss [18] is known to improve\nstability, fidelity and convergence speed, the multi-scale spectral losses[42, 11, 15] encourage model-\ning of frequencies in multiple time-scales. In our model, we combine both methods by using a L1\nloss on mel-spectrograms computed with window lengths of [32, 64, 128, 256, 512, 1024, 2048] and\nhop length set to window_length / 4. We especially find that using the lowest hop size of 8 improves\nmodeling of very quick transients that are especially common in the music domain.\nEnCodec [8] uses a similar loss formulation, but with both L1 and L2 loss terms, and a fixed mel bin\nsize of 64. We find that fixing mel bin size leads to holes in the spectrogram especially at low filter\nlengths. Therefore, we use mel bin sizes [5, 10, 20, 40, 80, 160, 320] corresponding to the above filter\nlengths which were verified to be correct by manual inspection.\nAdversarial loss: our model uses the multi-period discriminator [18] for waveform discrimination,\nas well as the proposed multi-band multi-scale STFT discriminator for the frequency domain. We use\nthe HingeGAN [22] adversarial loss formulation, and apply the L1 feature matching loss [19].\nCodebook learning: we use the simple codebook and commitment losses with stop-gradients from\nthe original VQ-VAE formulation [38], and backpropagate gradients through the codebook lookup\nusing the straight-through estimator [3].\nLoss weighting: we use the loss weightings of 15.0 for the multi-scale mel loss, 2.0 for the feature\nmatching loss, 1.0 for the adversarial loss and 1.0, 0.25 for the codebook and commitment losses\nrespectively. These loss weightings are in line with recent works [18, 21] (which use 45.0 weighting\nfor the mel loss), but simply rescaled to account for the multiple scales and log10 base we used for\ncomputing the mel loss. We don\u2019t use a loss balancer as proposed in EnCodec [8].\n4\nExperiments\n4.1\nData sources\nWe train our model on a large dataset compiled of speech, music, and environmental sounds. For\nspeech, we use the DAPS dataset [26], the clean speech segments from DNS Challenge 4 [10], the\nCommon Voice dataset [2], and the VCTK dataset [40]. For music, we use the MUSDB dataset\n5\n[31], and the Jamendo dataset [4]. Finally, for environmental sound, we use both the balanced and\nunbalanced train segments from AudioSet [14]. All audio is resampled to 44kHz.\nDuring training, we extract short excerpts from each audio file, and normalize them to -24 dB LUFS.\nThe only data augmentation we apply is to randomly shift the phase of the excerpt, uniformly. For\nevaluation, we use the evaluation segments from AudioSet [14], two speakers that are held out from\nDAPS [26] (F10, M10) for speech, and the test split of MUSDB [31]. We extract 3000 10-second\nsegments (1000 from each domain), as our test set.\n4.2\nBalanced data sampling\nWe take special care in how we sample from our dataset. Though our dataset is resampled to 44kHz,\nthe data within it may be band-limited in some way. That is, some audio may have had an original\nsampling rate much lower than 44kHz. This is especially prevalent in speech data, where the true\nsampling rates of the underlying data can vary greatly (e.g. the Common Voice data is commonly\n8-16kHz). When we trained models on varying sampling rates, we found that the resultant model\noften would not reconstruct data above a certain frequency. When investigating, we found that this\nthreshold frequency corresponded to the average true sampling rate of our dataset. To fix this, we\nintroduce a balanced data sampling technique.\nWe first split our dataset into data sources that we know to be full-band - they are confirmed to contain\nenergy in frequencies up to the desired Nyquist frequency (22.05kHz) of the codec - and data sources\nwhere we have no assurances of the max frequency. When sampling batches, we make sure that a\nfull-band item is sampled. Finally, we ensure that in each batch, there are an equal number of items\nfrom each domain: speech, music, and environmental sound. In our ablation study, we examine how\nthis balanced sampling technique affects model performance.\n4.3\nModel and training recipe\nOur model consists of a convolutional encoder, a residual vector quantizer, and a convolutional\ndecoder. The basic building block of our network is a convolutional layer which either upsamples\nor downsamples with some stride, followed by a residual layer consisting of convolutional layers\ninterleaved with non-linear Snake activations. Our encoder has 4 of these layers, each of which\ndownsamples the input audio waveform at rates [2, 4, 8, 8]. Our decoder has 4 corresponding layers,\nwhich upsample at rates [8, 8, 4, 2]. We set the decoder dimension to 1536. In total, our model\nhas 76M parameters, with 22M in the encoder, and 54M in the decoder. We also examine decoder\ndimensions of 512 (31M parameters) and 1024 (49M parameters).\nWe use the multi-period discriminator [18], and a complex multi-scale STFT discriminator. For the\nfirst, we use periods of [2, 3, 5, 7, 11], and for the second, we use window lengths [2048, 1024, 512],\nwith a hop-length that is 1/4 the window length. For band-splitting of the STFT, we use the band-\nlimits [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. For the reconstruction loss, we use distance between log-mel\nspectrograms with window lengths [32, 64, 128, 256, 512, 1024, 2048], with corresponding number\nof mels for each of [5, 10, 20, 40, 80, 160, 320]. The hop length is 1/4 of the window length. We use\nfeature matching and codebook losses, as described in Section 3.5.\nFor our ablation study, we train each model with a batch size of 12 for 250k iterations. In practice,\nthis takes about 30 hours to train on a single GPU. For our final model, we train with a batch size of\n72 for 400k iterations. We train with excerpts of duration 0.38s. We use the AdamW optimizer [23]\nwith a learning rate of 1e \u2212 4, \u03b21 = 0.8, and \u03b22 = 0.9, for both the generator and the discriminator.\nWe decay the learning rate at every step, with \u03b3 = 0.999996.\n4.4\nObjective and subjective metrics\nTo evaluate our models, we use the following objective metrics:\n1. ViSQOL [7]: an intrusive perceptual quality metric that uses spectral similarity to the ground\ntruth to estimate a mean opinion score.\n2. Mel distance: distance between log mel spectrograms of the reconstructed and ground truth\nwaveforms. The configuration of this loss is the same as described in 3.5.\n6\nAblation on\nDecoder dim.\nActivation\nMulti-period\nSingle-scale\n# of STFT bands\nMulti-scale mel.\nLatent dim\nQuant. method\nQuant. dropout\nBitrate (kbps)\nBalanced samp.\nMel distance \u2193\nSTFT distance \u2193\nViSQOL \u2191\nSI-SDR \u2191\nBitrate efficiency \u2191\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.09 1.82 3.96\n9.12\n99%\nArchitecture\n512\nsnake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.11 1.83 3.91\n8.72\n99%\n1024 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.07 1.82 3.96\n9.07\n99%\n1536\nrelu\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.17 1.81 3.83\n6.92\n99%\nDiscriminator\n1536 snake\n\u2717\n\u2717\n\u2717\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.13 1.92 4.12\n1.07\n62%\n1536 snake\n\u2713\n\u2717\n1\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.07 1.80 3.98\n9.07\n99%\n1536 snake\n\u2717\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.07 1.81 3.97\n9.04\n99%\n1536 snake\n\u2717\n\u2713\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2713\n1.08 1.82 3.95\n8.51\n99%\nReconstruction loss\n1536 snake\n\u2713\n\u2717\n5\n\u2717\n8\nProj.\n1.0\n8\n\u2713\n1.10 1.87 4.01\n7.68\n99%\nLatent dim\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n2\nProj.\n1.0\n8\n\u2713\n1.44 2.08 3.65\n2.22\n84%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n4\nProj.\n1.0\n8\n\u2713\n1.20 1.89 3.86\n7.15\n97%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n32\nProj.\n1.0\n8\n\u2713\n1.10 1.84 3.95\n9.05\n98%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n256\nProj.\n1.0\n8\n\u2713\n1.31 1.97 3.79\n5.09\n59%\nQuantization setup\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nEMA 1.0\n8\n\u2713\n1.11 1.84 3.94\n8.33\n97%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n0.0\n8\n\u2713\n0.98 1.70 4.09\n10.14\n99%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n0.25 8\n\u2713\n0.99 1.69 4.04\n10.00\n99%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n0.5\n8\n\u2713\n1.01 1.75 4.03\n9.74\n99%\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n24\n\u2713\n0.73 1.62 4.16\n13.83\n99%\nData\n1536 snake\n\u2713\n\u2717\n5\n\u2713\n8\nProj.\n1.0\n8\n\u2717\n1.09 1.94 3.89\n8.89\n99%\nTable 2: Results of the ablation study on our proposed codec. The final model is trained with the\nsame configuration as the baseline (top row), but with a quantization dropout of 0.5.\n3. STFT distance: distance between log magnitude spectrograms of the reconstructed and\nground truth waveforms. We use window lengths [2048, 512]. This metric captures the\nfidelity in higher frequencies better than the mel distance.\n4. Scale-invariant source-to-distortion ratio (SI-SDR) [20]: distance between waveforms,\nsimilar to signal-to-noise ratio, with modifications so that it is invariant to scale differences.\nWhen considered alongside spectral metrics, SI-SDR indicates the quality of the phase\nreconstruction of the audio.\n5. Bitrate efficiency: We calculate bitrate efficiency as the sum of the entropy (in bits) of each\ncodebook when applied on a large test set divided by the number of bits across all codebooks.\nFor efficient bitrate utilization this should tend to 100% and lower percentages indicate that\nthe bitrate is being underutilized.\nWe also conduct a MUSHRA-inspired listening test, with a hidden reference, but no low-passed\nanchor. In it each one of ten expert listeners rated 12 randomly selected 10-second samples from\nour evaluation set, 4 of each domain; speech, music and environmental sounds. We compare our\nproposed system at 2.67kbps, 5.33kbps and 8kbps to EnCodec at 3kbps, 6kbps and 12kbps.\n4.5\nAblation study\nWe conduct a thorough ablation study of our model, varying components of our training recipe and\nmodel configuration one-by-one. To compare models, we use the four objective metrics described in\nSection 4.4. The results of our ablation study can be seen in Table 2.\nArchitecture: We find that varying the decoder dimension has some effect on performance, with\nsmaller models having consistently worse metrics. However, the model with decoder dimension 1024\nhas similar performance to the baseline, indicating that smaller models can still be competitive. The\nchange with the biggest impact was switching out the relu activation for the snake activation. This\nchange resulted in much better SI-SDR and other metrics. Similar to the results in BigVGAN [21],\nwe find that the periodic inductive bias of the snake activation is helpful for waveform generation.\nFor our final model, we use the largest decoder dimension (1536), and the snake activation.\n7\nDiscriminator: Next, we removed or changed the discriminators one-by-one, to see their impact on\nthe final result. First, we find that the multi-band STFT discriminator does not result in significantly\nbetter metrics, except for SI-SDR, where it is slightly better. However, when inspecting spectrograms\nof generated waveforms, we find that the multi-band discriminator alleviates aliasing of high fre-\nquencies. The upsampling layers of the decoder introduce significant aliasing artifacts [29]. The\nmulti-band discriminator is more easily able to detect these aliasing artifacts and give feedback to the\ngenerator to remove them. Since aliasing artifacts are very small in terms of magnitude, their effect\non our objective metrics is minimal. Thus, we keep the multi-band discriminator.\nWe find that adversarial losses are critical to both the quality of the output audio, as well as the bitrate\nefficiency. When training with only reconstruction loss, the bitrate efficiency drops from 99% to\n62%, and the SI-SDR drops from 9.12 to 1.07. The other metrics capture spectral distance, and are\nrelatively unaffected. However, the audio from this model has many artifacts, including buzzing, as it\nhas not learned to reconstruct phase. Finally, we found that swapping the multi-period discriminator\nfor the single-scale waveform discriminator proposed in MelGAN [19] resulted in worse SI-SDR.\nWe retain the multi-period discriminator.\nImpact of low-hop reconstruction loss: We find that low-hop reconstruction is critical to both\nthe waveform loss and the modeling of fast transients and high frequencies. When replaced with a\nsingle-scale high-hop mel reconstruction (80 mels, with a window length of 512), we find significantly\nlower SI-SDR (7.68 from 9.12). Subjectively, we find that this model does much better at capturing\ncertain sorts of sounds, such as cymbal crashes, beeping and alarms, and singing vocals. We retain\nthe multi-scale mel reconstruction loss in our final recipe.\nLatent dimension of codebook: the latent dimension of the codebook has a significant impact on\nbitrate efficiency, and consequently the reconstruction quality. If set too low or too high (e.g. 2,\n256), quantitative metrics are significantly worse with drastically lowered bitrate efficiency. Lower\nbitrate efficiency results in effectively lowered bandwidth, which harms the modeling capability of the\ngenerator. As the generator is weakened, the discriminator tends to \u201cwin\u201d, and thus the generator does\nnot learn to generate audio with high audio quality. We find 8 to be optimal for the latent dimension.\nQuantization setup: we find that using exponential moving average as the codebook learning method,\nas in EnCodec[8], results in worse metrics especially for SI-SDR. It also results in poorer codebook\nutilization across all codebooks (Figure 1). When taken with its increased implementation complexity\n(requiring K-Means initialization and random restarts), we retain our simpler projected lookup method\nfor learning codebooks, along with a commitment loss. Next, we note that the quantization dropout\nrate has a significant effect on the quantitative metrics. However, as seen in Figure 2, a dropout of 0.0\nresults in poor reconstruction with fewer codebooks. As this makes usage of the codec challenging for\ndownstream generative modeling tasks, we instead use a dropout rate of 0.5 in our final model. This\nachieves a good trade-off between audio quality at full bitrate as well as lower bitrates. Finally, we\nshow that we can increase the max bitrate of our model from 8kbps to 24kbps and achieve excellent\naudio quality, surpassing all other model configurations. However, for our final model, we train at the\nlower bitrates, in order to push the compression rate as much as possible.\nBalanced data sampling: When removed, this results in worse metrics across the board. Empirically,\nwe find that without balanced data sampling, the model produces waveforms that have a max\nfrequency of around 18kHz. This corresponds to the max frequency preserved by various audio\ncompression algorithms like MPEG, which make up the vast majority of our datasets. With balanced\ndata sampling, we sample full-band audio from high-quality datasets (e.g. DAPS) just as much as\npossibly band-limited audio from datasets of unknown quality (e.g. Common Voice). This alleviates\nthe issue, allowing our codec to reconstruct full-band audio, as well as band-limited audio.\n4.6\nComparison to other methods\nWe now compare the performance of our final model with competitive baselines: EnCodec [8], Lyra\n[46], and Opus [37], a popular open-source audio codec. For EnCodec, Lyra, and Opus, we use\npublicly available open-source implementations provided by the authors. We compare using both\nobjective and subjective evaluations, at varying bitrates. The results are shown in Table 3. We find\nthat the proposed codec out-performs all competing codecs at all bitrates in terms of both objective\nand subjective metrics, while modeling a much wider bandwidth of 22kHz.\n8\n2\n4\n6\n8\n10\n12\nBitrate (kbps)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nMUSHRA score\nReference\nProposed\nEnCodec\nFigure 3: Listening tests at 44 KHz: MUSHRA\nscores, with 95% confidence intervals vs bitrate\nfor EnCodec, our proposed approach, and the ref-\nerence.\nCodec\nBitrate (kbps)\nBandwidth (kHz)\nMel distance \u2193\nSTFT distance \u2193\nViSQOL \u2191\nSI-SDR \u2191\nProposed\n1.78 22.05 1.39 1.95 3.76\n2.16\n2.67 22.05 1.28 1.85 3.90\n4.41\n5.33 22.05 1.07 1.69 4.09\n8.13\n8 22.05 0.93 1.60 4.18 10.75\nEnCodec\n1.5\n12 2.11 4.30 2.82\n-0.02\n3\n12 1.97 4.19 2.94\n2.94\n6\n12 1.83 4.10 3.05\n5.99\n12\n12 1.70 4.02 3.13\n8.36\n24\n12 1.61 3.97 3.16\n9.59\nLyra\n9.2\n8 2.71 4.86 2.19 -14.52\nOpus\n8\n4 3.60 5.72 2.06\n5.68\n14\n16 1.23 2.14 4.02\n8.02\n24\n16 0.88 1.90 4.15 11.65\nTable 3: Objective evaluation of the proposed\ncodec at varying bitrates, along with results from\ncompeting approaches.\nIn Figure 3, we show the result of our MUSHRA study, which compares EnCodec to our proposed\ncodec at various bitrates. We find that our codec achieves much higher MUSHRA scores than\nEnCodec at all bitrates. However, even at the highest bitrate, it still falls short of the reference\nMUSHRA score, indicating that there is room for improvement. We note that the metrics of our final\nmodel are still lower than the 24kbps model trained in our ablation study, as can be seen in Table 2.\nThis indicates that the remaining performance gap may be closed by increasing the maximum bitrate.\nIn Figure 4 and Table 4, we compare our proposed model trained with the same exact configuration as\nEnCodec (24 KHz sampling rate, 24 kbps bitrate, 320 stride, 32 codebooks of 10 bits each) to existing\nbaselines, in both quantitative and qualitative metrics. In Figure 5, we show qualitative results by\nsound category.\n2\n4\n6\n8\n10\n12\nBitrate (kbps)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nMUSHRA score\nReference\nProposed\nEnCodec\nFigure 4: Listening tests at 24 KHz: MUSHRA\nscores with 95% confidence intervals vs bitrate for\nEnCodec, our proposed approach with the same\nconfiguration, and the reference. Here all samples\nunder comparison are resampled to 24 KHz.\nCodec\nBitrate (kbps)\nBandwidth (kHz)\nMel distance \u2193\nSTFT distance \u2193\nViSQOL \u2191\nSI-SDR \u2191\nProposed@24kHz\n1.5 12 1.48 2.24 4.04\n0.32\n3 12 1.24 2.01 4.23\n4.44\n6 12 1.00 1.78 4.38\n8.44\n12 12 0.74 1.54 4.51 12.51\n24 12 0.49 1.33 4.61 16.40\nEnCodec\n1.5 12 1.63 2.69 3.98\n0.02\n3 12 1.46 2.54 4.16\n2.99\n6 12 1.30 2.39 4.30\n6.06\n12 12 1.15 2.28 4.39\n8.44\n24 12 1.05 2.21 4.42\n9.69\nTable 4: Encodec Configuration: Objective evalu-\nation of the proposed model trained with the same\nconfiguration as EnCodec at varying bitrates, along\nwith results from EnCodec.\n9\n2\n4\n6\n8\n10\n12\nBitrate (kbps)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nMUSHRA score\nReference (speech)\nProposed (speech)\nEnCodec (speech)\n2\n4\n6\n8\n10\n12\nBitrate (kbps)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nMUSHRA score\nReference (music)\nProposed (music)\nEnCodec (music)\n2\n4\n6\n8\n10\n12\nBitrate (kbps)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nMUSHRA score\nReference (env)\nProposed (env)\nEnCodec (env)\nFigure 5: MUSHRA by category: MUSHRA scores with 95% confidence intervals vs bitrate for our\nproposed model, EnCodec and reference.\n5\nConclusion\nWe have presented a high-fidelity universal neural audio compression algorithm that achieves re-\nmarkable compression rates while maintaining audio quality across various types of audio data.\nOur method combines the latest advancements in audio generation, vector quantization techniques,\nand improved adversarial and reconstruction losses. Our extensive evaluation against existing au-\ndio compression algorithms demonstrates the superiority of our approach, providing a promising\nfoundation for future high-fidelity audio modeling. With thorough ablations, open-source code, and\ntrained model weights, we aim to contribute a useful centerpiece to the generative audio modeling\ncommunity.\nBroader impact and limitations: our model has the capability to make generative modeling of\nfull-band audio much easier to do. While this unlocks many useful applications, such as media editing,\ntext-to-speech synthesis, music synthesis, and more, it can also lead to harmful applications like\ndeepfakes. Care should be taken to avoid these applications. One possibility is to add watermarking\nand/or train a classifier that can detect whether or not the codec is applied, in order to enable the\ndetection of synthetic media generated based on our codec. Also, our model is not perfect, and still\nhas difficulty reconstructing some challenging audio. By slicing the results by domain we find that,\neven though the proposed codec outperforms competing approaches across all of the domains, it\nperforms best for speech and has more issues with environmental sounds. Finally, we notice that it\ndoes not model some musical instruments perfectly, such as glockenspeil, or synthesizer sounds.\n10\nReferences\n[1] Andrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\n[2] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer,\nReuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. Common voice: A\nmassively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019.\n[3] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[4] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The mtg-\njamendo dataset for automatic music tagging. In Machine Learning for Music Discovery\nWorkshop, International Conference on Machine Learning (ICML 2019), Long Beach, CA,\nUnited States, 2019. URL http://hdl.handle.net/10230/42015.\n[5] Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt\nSharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a\nlanguage modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[7] Michael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev, Feargus O\u2019Gorman, and Andrew\nHines. Visqol v3: An open source production ready objective speech and audio metric. In\n2020 twelfth international conference on quality of multimedia experience (QoMEX), pages 1\u20136.\nIEEE, 2020.\n[8] Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\n[9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\n[10] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sergiy Matusevych, Sebastian Braun,\nEmre Sefik Eskimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper, and Robert Aichner.\nIcassp 2022 deep noise suppression challenge. In ICASSP, 2022.\n[11] Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, and Adam Roberts. Ddsp: Differentiable digital\nsignal processing. arXiv preprint arXiv:2001.04643, 2020.\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12873\u201312883, 2021.\n[13] Cristina G\u00e2rbacea, A\u00e4ron van den Oord, Yazhe Li, Felicia SC Lim, Alejandro Luebs, Oriol\nVinyals, and Thomas C Walters. Low bit-rate speech coding with vq-vae and a wavenet\ndecoder. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 735\u2013739. IEEE, 2019.\n[14] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for\naudio events. In 2017 IEEE international conference on acoustics, speech and signal processing\n(ICASSP), pages 776\u2013780. IEEE, 2017.\n[15] Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, and Nal Kalchbrenner.\nA spectral energy distance for parallel speech synthesis. Advances in Neural Information\nProcessing Systems, 33:13062\u201313072, 2020.\n[16] Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim. Univnet: A neural vocoder\nwith multi-resolution spectrogram discriminators for high-fidelity waveform generation. arXiv\npreprint arXiv:2106.07889, 2021.\n11\n[17] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. In International Conference on Machine Learning,\npages 5530\u20135540. PMLR, 2021.\n[18] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks\nfor efficient and high fidelity speech synthesis. Advances in Neural Information Processing\nSystems, 33:17022\u201317033, 2020.\n[19] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose\nSotelo, Alexandre de Br\u00e9bisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative\nadversarial networks for conditional waveform synthesis. Advances in neural information\nprocessing systems, 32, 2019.\n[20] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey. Sdr\u2013half-baked or\nwell done? In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 626\u2013630. IEEE, 2019.\n[21] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: A\nuniversal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022.\n[22] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[24] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo,\nAaron Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio\ngeneration model. arXiv preprint arXiv:1612.07837, 2016.\n[25] Max Morrison, Rithesh Kumar, Kundan Kumar, Prem Seetharaman, Aaron Courville, and\nYoshua Bengio. Chunked autoregressive gan for conditional waveform synthesis. arXiv preprint\narXiv:2110.10139, 2021.\n[26] Gautham J Mysore. Can we automatically transform speech recorded on common consumer\ndevices in real-world environments into professional production quality speech?\u2014a dataset,\ninsights, and challenges. IEEE Signal Processing Letters, 22(8):1006\u20131010, 2014.\n[27] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[28] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep voice 3: Scaling text-to-speech with convolutional\nsequence learning. arXiv preprint arXiv:1710.07654, 2017.\n[29] Jordi Pons, Santiago Pascual, Giulio Cengarle, and Joan Serr\u00e0. Upsampling artifacts in neural\naudio synthesis. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 3005\u20133009. IEEE, 2021.\n[30] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network\nfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.\n[31] Zafar Rafii, Antoine Liutkus, Fabian-Robert St\u00f6ter, Stylianos Ioannis Mimilakis, and Rachel\nBittner. The musdb18 corpus for music separation, 2017.\n[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[33] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images\nwith vq-vae-2. Advances in neural information processing systems, 32, 2019.\n[34] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2:\nFast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.\n12\n[35] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,\nZhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by\nconditioning wavenet on mel spectrogram predictions. In 2018 IEEE international conference\non acoustics, speech and signal processing (ICASSP), pages 4779\u20134783. IEEE, 2018.\n[36] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong\nLeng, Yuanhao Yi, Lei He, et al. Naturalspeech: End-to-end text to speech synthesis with\nhuman-level quality. arXiv preprint arXiv:2205.04421, 2022.\n[37] Jean-Marc Valin, Koen Vos, and Timothy Terriberry. Definition of the opus audio codec.\nTechnical report, 2012.\n[38] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[40] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Cstr vctk corpus: English\nmulti-speaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre for\nSpeech Technology Research (CSTR), 2017.\n[41] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen,\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text\nto speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.\n[42] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel wavegan: A fast waveform gen-\neration model based on generative adversarial networks with multi-resolution spectrogram. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6199\u20136203. IEEE, 2020.\n[43] Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie. Multi-band melgan:\nFaster waveform generation for high-quality text-to-speech. In 2021 IEEE Spoken Language\nTechnology Workshop (SLT), pages 492\u2013498. IEEE, 2021.\n[44] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021.\n[45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n[46] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:495\u2013507, 2021.\n[47] Liu Ziyin, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions\nand how to fix it. Advances in Neural Information Processing Systems, 33:1583\u20131594, 2020.\n13\nA\nAppendix\nModified codebook learning algorithm\nIn our work, we use a modified quantization operation,\ngiven by:\nzq(x) = Woutek,\nwhere\nk = arg min\nj\n||\u21132(Winze(x)) \u2212 \u21132(ej)||2\nHere, Win and Wout are projection matrices, with Win mapping the encoder\u2019s output to an intermediate\nrepresentation, and Wout mapping this intermediate representation to the quantized representation\nzq(x). Specifically,\nWin \u2208 RD\u00d7M\nand\nWout \u2208 RM\u00d7D\nwhere D is the output dimension of the encoder, and M is the codebook dimension with M \u226a D.\nThe vector quantizer loss function is then defined to measure the reconstruction error and is given by:\nzproj(x) = Win ze(x)\nLVQ = ||sg[\u21132(zproj(x))] \u2212 \u21132(ek)||2\n2 + \u03b2||\u21132(zproj(x)) \u2212 sg[\u21132(ek)]||2\n2\nwhere sg is the stop gradient operator, preventing the back-propagation of gradients through ek, and\n\u03b2 is a hyperparameter controlling the balance between the two terms in the loss function.\n14\n"
  },
  {
    "title": "Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions",
    "link": "https://arxiv.org/pdf/2306.06212.pdf",
    "upvote": "8",
    "text": "Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract\nScene Descriptions\nIAN HUANG, Stanford University, USA\nVRISHAB KRISHNA, Stanford University, USA\nOMORUYI ATEKHA, Stanford University, USA\nLEONIDAS GUIBAS, Stanford University, USA\nFig. 1. Our system produces stylized assets to fit a scene description. Given an abstract scene description that does not provide details on what objects should\nbe found within that scene, our system (1) infers a semantic shopping list, a human-readable and editable list of object categories and appearance attributes,\nand then uses this to (2) retrieve template shapes from a 3D asset database before (3) re-texturing them to fit the desired appearance attributes. The output\nof our system is a collection of textured meshes, which can be directly imported into 3D design software and used for other downstream tasks. Note the\ncorrespondences in the assets on the right with many of the desired object categories and appearance attributes generated by our system on the left!\nWhat constitutes the \u201cvibe\u201d of a particular scene? What should one find in\n\u201ca busy, dirty city street\u201d, \u201can idyllic countryside\u201d, or \u201ca crime scene in an\nabandoned living room\u201d? The translation from abstract scene descriptions\nto stylized scene elements cannot be done with any generality by extant sys-\ntems trained on rigid and limited indoor datasets. In this paper, we propose\nto leverage the knowledge captured by foundation models to accomplish\nthis translation. We present a system that can serve as a tool to generate\nAuthors\u2019 addresses: Ian Huang, Stanford University, Stanford, CA, USA, ianhuang@\ncs.stanford.edu; Vrishab Krishna, Stanford University, Stanford, CA, USA, vrishab@\nstanford.edu; Omoruyi Atekha, Stanford University, Stanford, CA, USA, oatekha@\nstanford.edu; Leonidas Guibas, Stanford University, Stanford, CA, USA, guibas@cs.\nstanford.edu.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\n\u00a9 2023 Association for Computing Machinery.\n0730-0301/2023/6-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nstylized assets for 3D scenes described by a short phrase, without the need\nto enumerate the objects to be found within the scene or give instructions\non their appearance. Additionally, it is robust to open-world concepts in\na way that traditional methods trained on limited data are not, affording\nmore creative freedom to the 3D artist. Our system demonstrates this using\na foundation model \u201cteam\u201d composed of a large language model, a vision-\nlanguage model and several image diffusion models, which communicate\nusing an interpretable and user-editable intermediate representation, thus\nallowing for more versatile and controllable stylized asset generation for 3D\nartists. We introduce novel metrics for this task, and show through human\nevaluations that in 91% of the cases, our system outputs are judged more\nfaithful to the semantics of the input scene description than the baseline,\nthus highlighting the potential of this approach to radically accelerate the\n3D content creation process for 3D artists.\nCCS Concepts: \u2022 Applied computing \u2192 Media arts.\nAdditional Key Words and Phrases: Large Language Models, Foundation\nModels, Texture Generation, Scene Descriptions, Asset Retrieval\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\narXiv:2306.06212v1  [cs.CV]  9 Jun 2023\n2\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nACM Reference Format:\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene\nDescriptions. ACM Trans. Graph. 1, 1 (June 2023), 39 pages. https://doi.org/\n10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nWhile language-to-shape generation has taken the world by storm,\nscene generation has been less accessible by language control par-\ntially because the language content needed to express details of\na scene becomes prohibitively cumbersome for average human\ncreators. Additionally, manually searching, selecting, and retextur-\ning/restylizing assets from online 3D repositories is, in aggregate,\na very time-consuming task for a single object, not to mention the\ntask of doing so for 30-50 objects that may make up a 3D scene.\nFrom a user\u2019s perspective, wouldn\u2019t it be convenient to say \u201cgener-\nate a scene of the financial district of New York\u201d and have the system\ninfer what should be in the scene and how every item should look?\nIn other words, we would like to build a system that hallucinates\nboth semantic and visual detail from an abstract high-level scene\ndescription \u2013 something that human users can provide much more\nconveniently than fully enumerative language found in [Achlioptas\net al. 2020; Chang et al. 2015b; Ilinykh et al. 2019].\nWhile valuable, this is not a problem that can be solved using tra-\nditional machine learning approaches, primarily due to limitations\nin data \u2013 the indoor scene datasets that dominate the domain of\nscene generation are largely limited in scene and object diversity\n[Chang et al. 2017; Fu et al. 2021a; Roberts et al. 2021; Song et al.\n2015, 2017], far from open-vocabulary. The same can be said for\nlanguage-scene multimodal datasets, where language labels are ei-\nther limited or prohibitively enumerative to train for our primary\ntask of interest [Achlioptas et al. 2020; Chang et al. 2015b].\nIn this paper, we ask, how far can zero-shot inference using foun-\ndation models go, with their common sense understanding [Brown\net al. 2020; Radford et al. 2021; Rombach et al. 2022], in facilitating\nthe 3D scene creation process for 3D artists? We introduce a system\nthat allows 3D content creators to synthesize entire asset collections\nfrom abstract scene descriptions (e.g. \u201ca busy city street\u201d), by lever-\naging the immense amount of progress in Large Language Models\nand Vision Language Models.\nTo go from abstract description to stylized asset collections, we\nbreak the process into 3 stages. In the first stage, we \u201csemantically\nupsample\u201d the input abstract description into a plausible list of\nobjects, attributes and appearances (which we call the \u201csemantic\nshopping list\u201d) that may compose the described scene. For this, we\nuse in-context prompting of LLM\u2019s [Brown et al. 2020], exploiting\ncommon sense knowledge of scene composition embedded within\nLLM\u2019s. The second stage requires a retrieval from an existing 3D\nasset database, given the attributes and appearances hallucinated\nin the semantic shopping list. We use visual and textual similarity\ngiven by large vision-language models like CLIP to retrieve top\ncandidates. Finally, we use diffusion models to texture the surface\nof the objects given their hallucinated appearance attributes.\nOur system uses natural language as an intermediary represen-\ntation between these stages, for 3 reasons: (1) Interpretability\nand Editability: This means that users can visualize, interpret and\nedit the intermediary outputs. This is important, since this work\nemploys a \u201cteam\u201d of foundation models for the first time, where\nthe output of one may not necessarily \u2013 in a zero-shot sense \u2013 be\noptimal as an input into another to accomplish the user\u2019s artistic\nintent. (2) Varying Abtraction Levels: Given language\u2019s ability\nto represent information at a variety of abstraction levels, it as a\nmedium that allows both the large language model (as well as user\nedits) to specify semantic constraints at a wide range of specificity.\n(3) Moore\u2019s law, but for foundation models: Given recent trends,\nwe\u2019re anticipating that the foundation models used in this paper\nwill have more powerful replacements soon. We expect that users\nof our system will be able to \u201cupgrade\u201d different modules with the\nlatest models.\nOur system integrates with existing 3D asset databases and treats\nits assets as templates for both the appearance and the geome-\ntry. The benefits of this are two-fold: (1) while large amounts of\nwork that does scene generation is reliant and restricted on indoor\nscene datasets [Chang et al. 2015b; Ma et al. 2018; Paschalidou et al.\n2021], our method can generate outdoor scenes and radically out-of-\ndistribution scenes as well, by leveraging diverse and larger-scale\nshape databases [Chang et al. 2015a; Deitke et al. 2022; Selvaraju\net al. 2021] and (2) building ontop of a 3D asset store allows usage\nof such a system to be specialized, depending on the asset store\nprovided, not to mention that it allows for nice priors, important for\nboth geometric and textural manipulation [Hui et al. 2022; Michel\net al. 2022,?; Xu et al. 2022].\nThe main contribution of this paper is three-fold: (1) we present\nthe task of stylized asset curation given abstract scene descriptions,\nwhich, to the knowledge of the authors, has not been considered\nin isolation. (2) we present a system that tackles this task using\nthe zero-shot capabilities of foundation models, and contribute\na method that does this using semantic upsampling through in-\ncontext learning. (3) We introduce a new metric, CLIP-D/S, which\ncan be used to measure both the diversity of the asset collection and\nthe semantic alignment with respect to a target scene description.\nIn addition to quantitative and qualitative evaluations, our human\nevaluation experiments conducted using 72 evaluators showcases\nthe efficacy of our system, and the value of semantic upsampling\nas a key powerhorse in the quality of the generated assets and as-\nsembled scenes. Code for our system and metrics can be found at\nhttps://github.com/ianhuang0630/Aladdin \u00a9.\n2\nRELATED WORKS\nWorks like [Achlioptas et al. 2022; Fu et al. 2022; Gao et al. 2022;\nHuang et al. 2022; Jain et al. 2022; Jun and Nichol 2023; Lin et al.\n2022; Michel et al. 2022; Nichol et al. 2022; Poole et al. 2022; Sanghi\net al. 2022; Xu et al. 2022] focus on generating shapes from natu-\nral language. However, as many of them use non-mesh-based 3D\nrepresentations like implicit representations [Jain et al. 2022; Lin\net al. 2022; Poole et al. 2022; Xu et al. 2022], extracting meshes from\nthem gives rise to disruptive artifacts in both texture and geometry,\nlimiting the usability of the asset in almost all 3D design applica-\ntions. As such their outputs are not optimized for usage by human\nusers, since composing and editing scenes using implicit represen-\ntations of assets remains non-trivial. Additionally, such systems\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n3\nare not optimized to read between the lines \u2013 the desired output\nis oftentimes what is described verbatim, given its object-centric\nfocus. However, for abstract scene descriptions, compositional un-\nderstanding beyond what is typically captured by vision-language\nmodels is needed.\nOn the other hand, works on mesh generation and texturing\nusing text prompts [Michel et al. 2022; Sanghi et al. 2022; Xu et al.\n2022] make use of vision-language models like CLIP [Radford et al.\n2021] coupled with differentiable rendering to optimize the mesh\nto correspond to a certain text embedding. These methods manage\nto edit the mesh to become semantically similar to the text prompt,\nbut since CLIP was not directly optimized to guide differentiable\nrendering, the resulting optimization often leads to improbable or\nunrealistic outputs, as can be seen by disruptive artifacts that often\ngive a distorted and blocky feel to the outputs. Meanwhile, through\nnewer generative models, the world knowledge obtained from large-\nscale image-text datasets is easily accessible. [Lin et al. 2022] takes\na step in this direction, using image diffusion models to generate\nhigh resolution textures of a mesh. However, this requires a detailed\ndescription specific to the individual objects to be generated, which\nis not provided a priori in our problem setting.\nAlong this line, works like [Fridman et al. 2023], [H\u00f6llein et al.\n2023], and [Zhang et al. 2023] introduce pipelines that incorporate\nimage diffusion models [Ho et al. 2020; Rombach et al. 2022; Saharia\net al. 2022] to create scenes, but not in a way that allows assets\nthat compose the scene to be easily and effectively extracted. Sce-\nnescape [Fridman et al. 2023] and Text2Room [H\u00f6llein et al. 2023]\nare two similar methods that make use of depth prediction models\nto craft a mesh using iterative predictions from a image diffusion\nmodel. SceneScape [Fridman et al. 2023], making use of generated\nsuper-resolution videos, is biased towards producing scenes that\nare long and tunnel-like, thereby restricting the set of producible\nscenes. Similarly, Text2Room [H\u00f6llein et al. 2023] can only generate\nclosed, star-convex meshes due to the depth projection approach.\nThe fundamental drawback of these methods is that the end result\nis a single connected mesh with limited flexibility to extract and edit\nassets. Meanwhile, [Po and Wetzstein 2023] recently introduced a\nmodel that uses locally conditioned diffusion to generate the scene\ncompositionally by using different language instructions to gen-\nerate different patches of the scene (e.g. \u201ca firepit\u201d in one part of\nthe scene, \u201ca tent\u201d in another). However, not only do the generated\nassets suffer from the same aforementioned weaknesses in regards\nto mesh extraction, but the generative pipeline also requires fully\nenumerative language input, in contrast with the focus of this work.\nAlthough considerate effort has been made towards collections\nof 3D scene datasets [Chang et al. 2017; Fu et al. 2021a; Roberts et al.\n2021; Song et al. 2015, 2017] as well as training models to generate\nand position elements within indoor scenes [Chang et al. 2015b; Ma\net al. 2018; Paschalidou et al. 2021; Ritchie et al. 2019; Wang et al.\n2019, 2021], these were not designed to handle open vocabularies of\nobjects, which makes them limited for creative applications. More-\nover, the latter works do not have the ability to re-texture scene\nelements to better match the input language description, which is a\nprime focus of our system.\nFig. 2. The template and query segments of the GPT-3 input share the\nsame structure up to the part where GPT-3 is prompted to do next-token\nprediction. We use this template for all generations of the anchor objects\nwithin the scene.\nFig. 3. We move down the scene hierarchy by asking GPT-3 to generate\nperipheral objects around each of the objects in the current level. Usually,\nthis results in smaller and more peripheral objects that add to the realism\nof the scene. We use in-context learning again to generate their attributes.\n3\nOUR METHOD\n3.1\nSemantic Upsampling\nGiven an abstract scene description, our system \u201cupsamples\u201d the\nsemantics of the scene description to the level of object categories,\nproperties and appearance. To do this, we use few-shot prompting\nof GPT-3 [Brown et al. 2020], which has shown to be very useful in\nother settings [Chen et al. 2022; Dong et al. 2022; Min et al. 2021;\nRubin et al. 2021; Shin et al. 2022; Wang et al. 2022; Wei et al. 2021,\n2022; Zhang et al. 2022; Zhao et al. 2021; Zhou et al. 2022].\nTo do this, we create templates that cover a variety of differ-\nent aspects of objects that may be found within the scene; object\ncategory, style, material properties, and condition (e.g. scratched,\nunused, rusted, ...). These templates can be found in the Appendix.\nTemplates are used for two main reasons: (1) they effectively en-\nforce a prior over the kind of attributes that one would like to use\nto describe objects within the scene and (2) they dictate a textual\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n4\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nformat that can be very easily parseable by our system (e.g. comma\nseparated attributes, colon separation between object category and\nattributes.).\nIn practice, we found that querying for all the objects within a\nscene at once can lead to degenerate results \u2013 generating details\nfor way too many objects at once may cause the objects chosen\nto \u201cdrift\u201d semantically away from the prompt. As such, we adopt a\nmore hierarchical approach, where we first use in-context learning\nto ask GPT-3 [Brown et al. 2020] to generate a set of \u201canchor\u201d objects\n(typically, this is a small set of 6-8 objects) and their attributes (Figure\n2). For each of these anchor objects, we ask it to hallucinate objects\n(and their attributes) found \u201caround\u201d the anchor object (Figure 3),\nand repeating this recursively down the hierarchy. This works fairly\nwell to elucidate the hierarchy of objects, and can be useful for\nobject placement (e.g. for a \u201cfancy french restaurant\u201d, an anchor\nobject generated is a table, and objects generated around this anchor\nobject are objects typically found on the table). Additionally, doing\nthis hierarchically means that for abstract descriptions that involve\na large set of objects, we need only call this procedure a few times\nbefore we arrive at the \u201cleaf\u201d objects within the implicit object\nhierarchy. A traversal through this hierarchy allows a full list of\nobjects and appearance attributes of objects likely to be found within\nthe described scene. We will refer to this list as the semantic shopping\nlist. An example of this is shown on the left in Figure 1.\n3.2\nObject retrieval & retexturing\nGiven the semantic shopping list from semantic upsampling, the\nsystem use CLIP [Radford et al. 2021] embeddings of both visual\nrenderings and textual annotations of objects within asset databases\nto retrieve the template geometries for each object.\nThis is, however, a nuanced objective; since all objects selected\nduring retrieval will go through diffusion-based re-texturing, it\u2019s\ntempting to disregard the original texturing altogether, and retrieve\nonly using a query composed of the object category information\nfrom the semantic upsampling (ignoring object attributes, which will\nbe \"painted\" on in a later stage). In practice, this leads to suboptimal\nretrieval results. Some object attributes (e.g. \u201cold\u201d in \u201cold car\u201d) are\nless solely based in texture, affecting both the visual appearance and\nthe geometry. Moreover, the pretrained model of CLIP was trained\non natural images, which relies on color properties for accurate\nsimilarity evaluation (similar observations have also been reported\nin [Michel et al. 2022]). As such, using a textureless rendering of\nthe candidate asset can actually hurt the retrieval performance.\nTo match the open-world vocabulary found in semantic shopping\nlists, it is essential to have a large and diverse asset database to\nchoose from. For this paper, we\u2019ve chosen to use a combination of\nFuture3D [Fu et al. 2021b] and a 30K-subset of Objaverse [Deitke et al.\n2022]. Future3D specializes in objects commonly found in indoor\nenvironments, and is a useful dataset for the majority of \u201cbase\u201d\nobject found within indoor scenes, which we anticipate would make\nup of the majority of user scene queries. Objaverse is a lot more\ndiverse in object category, and serves for the \u201cpersonality\u201d pieces\nof indoor scenes (e.g. the sword along the wall in Figure 8), which\nallows the scene to be more faithful to the \u201cvibe\u201d communicated\nin the input description. Additionally, it contains object categories\ntypically found outdoors, which allows our system to construct\noutdoor scenes in ways that previous scene generation pipelines\ncannot (see Figure 6).\nIn our current implementation, we use the thumbnails of different\nassets to derive the CLIP image embeddings, since (1) these are\nreadily available in most datasets and (2) human artists already use\nthem to judge the appropriateness of a particular asset for their\nscene. Future works extending our pipeline can use more complex\nrendering techniques for different objects, and the question of how\nrenderings should be done to encourage high accuracy 3D asset\nretrieval is an important direction for future work.\nEnforcing stylistic consistency from the retrieval stage is hard.\nEmpirically, we notice that using just the semantic shopping list\nalone often leads to retrieval of objects that are stylistically incon-\nsistent in their template geometry, and thus do not aesthetically\ncombine well once put in the same scene. This is because though\nsemantic upsampling hallucinates visual details, it has no context of\nwhat would be important for stylistic consistency in the retrieved\nresults downstream. Therefore, we merge the abstract scene descrip-\ntion into all retrieval and texturing queries, for all objects, as a\nfail-safe for when the semantic shopping list provides inadequate\nstylistic information.\nGiven that many 3D assets have language annotations, we incor-\nporate that information when determining the K-nearest neighbors\nthrough a simple linear weighting of the language- and image-based\ncosine similarities. Doing brings some more robustness to the re-\ntrieval process in the case when the asset thumbnail does not reflect\nthe geometric content as well as its textual annotations do.\nOnce we have the template objects, we make use of pre-existing\nimage generation pipelines to texture each retrieved object. Using an\navailable depth-guided and language-guided image diffusion mod-\nels [Rombach et al. 2022], we can generate images corresponding\nto views of an object and use differentiable rendering to optimize\nour mesh texture to match the generated image, while encouraging\n3D consistency between different views through depth and lan-\nguage conditioning. We use the implementation of a recent paper\n[Richardson et al. 2023] to achieve this.\n4\nEXPERIMENTS\nThe main output of our system is a set of textured assets. To demon-\nstrate the usability of our system outputs, we source ideas for input\nscene descriptions from 8 people who do not have any prior 3D de-\nsign experience or experience interacting with our system. 20 such\nprompts were collected, ranging in plausibility (from \u201ca romantic\nfrench restaurant\u201d to \u201ca church for strawberries\u201d), emotional valence\n(from \u201ca marvel-themed bedroom for a five-year-old toddler\u201d to\n\u201cmurder in an abandoned living room\u201d) and complexity (from \u201ca rus-\ntic backyard in the countryside\u201d to \u201ca busy street in downtown new\nyork\u201d). A full collection of the abstract scene descriptions can be\nfound in the Appendix, as well as their corresponding visualizations.\nWe provide the prompts to our system, and \u2013 for the purposes\nof this paper \u2013 run our system in a fully automated way, sidestep-\nping the possible option of user edits of the semantic shopping list\nbetween different stages. To do this, we use the same query string\n(generated from the semantic upsampling stage) for the retrieval\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n5\nand texturing stages, and automatically select the top-1 sample in\nCLIP-Similarity in the retrieval outputs for texturing.\nNote that to demonstrate the robustness of this system and the\nbenefit of basing it on foundation models, we do not cherrypick\nbetween different runs for the same input prompt. In other words,\nall visualizations of scenes are done based on assets generated in a\nsingle pass.\n4.1\nComposing assets into scenes\nTo construct the final scene, the authors of this paper import the\ngenerated assets into Blender [Community 2018] and create 3D\nscenes according to the following rules: (1) they are allowed to\ntranslate, rotate and scale any 3D asset in the generated collection\nalong any axis, (2) they are allowed to add ground and wall planes to\nthe scene, (3) they are allowed to omit subsets of the asset collection\nfrom visualization, (4) they are allowed to duplicate assets as many\ntimes as they wish, and (5) they are not allowed to change the\nmaterial properties of the textured mesh, except emissive properties\nfor assets that should emit light (rare). On average, the importing,\narrangement and rendering of a single scene took 20 minutes.\n4.2\nEvaluating stylized asset collections\nWithin the literature, CLIP-Similarity (or CLIP-S) has been recently\nused to measure adherence of generative output to the semantics of\nthe input text [Fu et al. 2022; Xu et al. 2022]. Given a vision encoder\n\ud835\udc63 and a language encoder \ud835\udc54, rendered views of the object \ud835\udc65\ud835\udc56 and\nassociated language description \ud835\udc59, CLIP-S is defined as:\n\ud835\udc46(\ud835\udc65,\ud835\udc59) = max\n\ud835\udc56\n\ud835\udc63(\ud835\udc65\ud835\udc56)\ud835\udc47\ud835\udc54(\ud835\udc59)\n(1)\nHowever, using CLIP-S directly on our task has serious drawbacks.\nFirst, it\u2019s an observed phenomenon that CLIP\u2019s language model\noftentimes behaves like a Bag-Of-Words model [Michel et al. 2022;\nYuksekgonul et al. 2022], where important relations between entities\nor concepts are often not reflected in its similarity evaluations. This\nmotivates why it\u2019s inappropriate to use such a metric to evaluate\nthe adherence of a stylized asset to the set of objects that likely\ncomposes a scene of a particular semantic \u2013 the relationships that\nare key to the idea of scene membership (i.e. that an asset belongs to\na scene) can be overpowered by the description of the scene itself.\nEmpirically, we\u2019ve found that such a metric tends to slightly favor\nthe outputs of the system when semantic upsampling is not used\nand assets are retrieved and textured according to the abstract scene\ndescription, though this comes as little surprise. The distinction is\ndemonstrated in Figure 10, which shows that CLIP (and by extension,\nCLIP-S) cannot favor assets that compose a scene over assets that\nmay resemble the abstract prompt but do not compose that scene.\nWe would like a metric that exhibits this behavior.\nTo solve this problem, we introduce the idea of CLIP-Diversity\n(CLIP-D), a score that is high when the assets that is generated are\nsemantically varied. This metric counteracts the favoring of systems\nthat generate assets that are very narrowly aligned with the scene\ndescription. Assuming the same visual encoder \ud835\udc63, and that \ud835\udc65 \ud835\udc57\n\ud835\udc56 is\nthe renderings of asset \ud835\udc57 from angle \ud835\udc56, as well as a function \ud835\udc5a that\naverages on the surface of the unit sphere over a set of points on\nthe unit hypersphere of the CLIP embedding space, we define CLIP-\nDiversity as the negative mean pairwise cossine similarity between\nassets within the collection:\n\ud835\udc37({\ud835\udc65 \ud835\udc57 }\ud835\udc57=1...\ud835\udc41 ) =\n\u2212\n2\n\ud835\udc41 (\ud835\udc41 \u2212 1)\n\u2211\ufe01\n\ud835\udc56<\ud835\udc57\n\ud835\udc5a\n\u0012\n{\ud835\udc63(\ud835\udc65 \ud835\udc57\n\ud835\udc58)}\ud835\udc58=1...\ud835\udc3e\n\u0013\ud835\udc47\n\ud835\udc5a\n\u0012\n{\ud835\udc63(\ud835\udc65\ud835\udc56\n\ud835\udc58)}\ud835\udc58=1...\ud835\udc3e\n\u0013\n(2)\nHowever, diversity alone does not provide adherence to a lan-\nguage instruction and can be satisfied without consideration for the\nlanguage prompt. As such, we construct CLIP-D/S, a metric that\nadditively combines CLIP-D and CLIP-S over an asset collection\n{\ud835\udc65 \ud835\udc57 } and a language instruction \ud835\udc59 from a collection \ud835\udc3f of augmented\nutterances based on the scene description (see Section 4.3), given\nequal weighting:\n\ud835\udc37\ud835\udc46({\ud835\udc65 \ud835\udc57 }\ud835\udc57=1...\ud835\udc41,\ud835\udc59) = \ud835\udc37({\ud835\udc65 \ud835\udc57 }\ud835\udc57=1...\ud835\udc41 ) +\n1\n\ud835\udc41 |\ud835\udc3f|\n\u2211\ufe01\n\ud835\udc57\n\u2211\ufe01\n\ud835\udc59 \u2208\ud835\udc3f\n\ud835\udc46(\ud835\udc65 \ud835\udc57,\ud835\udc59)\n(3)\nCLIP-D/S is therefore a combination of diversity and similarity,\nand can be used heuristically to measure the fidelity and usefulness\nof the asset collections that our system generates.\n4.3\nSystem outputs\nFigures 1, 4, 5, 6, 7, 8, and 9 show some outputs from our system,\narranged into 3D scenes. A longer list of examples, along with\ntheir corresponding semantic shopping lists, can be found in the\nAppendix. Predominantly, the benefit of this system is its ability to\nadd \u201ccharacter\u201d to a scene through inferring a wider, more diverse\nset of object categories.\nA practical property of this system is that due to the inherent\nrandomness present in next-token generation of GPT-3, running\nthe system twice will create differing semantic shopping lists. This\nis a useful property for 3D artists, since this allows them to seman-\ntically densify their scenes by rerunning the semantic upsampling\nprocess. As shown in Figures 8 and 9, multiple runs can come up\nwith different but valid assets, where the union or intersection of\nthem could create even richer and accurate scenes.\nAs a measure of stylistic adherence, we alternatively ask, given\nan asset we\u2019ve generated for each of the scenes, how well can one\npredict which scene they were generated for? We use the CLIP-S\nmetric as a zero-shot classifier to classify each of our 572 stylized\nassets across the 20 scenes that they were generated for (full list\ncan be seen in Table 1 and in Appendix), and find a top-1 classifica-\ntion accuracy of 32.69%, substantially higher than the accuracy of\nguessing randomly (5%). We consider the predicted scene to be the\nscene that maximizes the average CLIP-S score across \ud835\udc3f, which is\na set of language augmentations on the abstract scene description:\n(1) \u201can element in a scene of [SCENE DESCRIPTION]\u201d, (2) \u201can object\nfrom a scene of [SCENE DESCRIPTION]\u201d, (3) \u201ca picture of an object\nform [SCENE DESCRIPTION]\u201d, (4) \u201ca rendering of an asset from a 3D\nscene of [SCENE DESCRIPTION]\u201d and (5) \u201c[SCENE DESCRIPTION]\u201d.\n4.4\nThe importance of semantic upsampling\nThe main contribution of our work is the use of in-context learn-\ning to generate semantically meaningful details as they pertain to\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n6\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 4. A scene of \u201ca Church for Strawberries\u201d is one of the more out-of-distribution queries given to our system, but through a combination of human\ncreativity and assets generated by our system, a rather funny scene emerges.\nFig. 5. A scene of \u201ca murder in an abandoned living room\u201d. The hallucinations of semantic upsampling tells a gruesome and disturbing story, with the cleaver\nplaced near the bloodied couch, the gun with empty shells around it, and crimson smears on the canvases. We acknowledge the graphic nature of this scene,\ninclude this example to show that our system is capable of producing scenes at the extremities of emotional valences, unlike more traditional scene generation\nsystems.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n7\nFig. 6. A scene of \u201ca rustic backyard in the countryside\u201d. This example demonstrates the potential of our system to create outdoor-esque scenes by using the\ncommon-sense reasoning of foundation models. Notice the elements that suggests the outdoor environment \u2013 the gardening equipment on the table, the\nbarbeque grill and bag of coal, the logs and rocks, match sticks and kerosene lamp, and the umbrella table.\nFig. 7. A scene of \u201ca marvel-themed bedroom of a five-year old toddler\u201d. As foundation models are trained mostly by data on the internet, we observe that it\u2019s\nable to understand references to pop culture fairly well, resulting in this very prominently marvel-themed bedroom.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n8\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 8. Asset arrangement from the first run of our system for the input\n\u201coffice of the King\u2019s Hand in Game of Thrones\u201d.\nFig. 9. Asset arrangement from the second run of our system for the input\n\u201coffice of the King\u2019s Hand in Game of Thrones\u201d.\nassets. How important is this step? We compare against a baseline\nmethod that is exactly the same as the method proposed, except\nthat it retrieve and retextures according to the input abstract scene\ndescription, instead of the semantic shopping list given by semantic\nupsampling. For this, we retrieve and retexture the top-\ud835\udc3e assets that\nhave the highest CLIP-similarity with the abstract scene description\n, where \ud835\udc3e is the number of assets generated by our method.\nTable 1 shows the impact of removing semantic upsampling on\nthe diversity (CLIP-D) and the CLIP-D/S score for the generated\nassets for each of the 20 scenes. This corroborates the observation\nthat in the best case, as shown in Figure 10, assets that align very\nwell with the scene itself might get retrieved, resulting a narrow\nselection that cannot be used to compose the scene. Or, as is often\nthe case, an erroneous template shape is retrieved, and confuses\nthe downstream retexturing to produce poorly textured 3D assets.\nPlease see the Appendix for examples of this phenomenon.\n4.5\nThe importance of retexturing\nGiven the ever-growing 3D asset collections, what is the benefit of\nreplacing pre-fabricated textures using the last step of our system?\nTable 2 reflects what happens to the CLIP-S score (w.r.t. the abstract\nscene description) when we use the original texture, compared to\nthat of our re-textured objects. This shows that in general, retextur-\ning using the output from semantic upsampling allows an increase\nFig. 10. Retrieval and retexturing for a scene of \u201ca saloon from an old\nwestern\u201d without the use of semantic upsampling. The outputs can be very\nnarrowly aligned with the \u201cwestern saloon\u201d concept, but are not elements\nthat can compose the scene described, unlike the assets in Figure 1.\nScene Reference\nD (b)\nD (o) \u2191\nD/S (b)\nD/S (o) \u2191\nrustic backyard\n-0.84\n-0.80\n-0.61\n-0.60\nfuturistic teahouse\n-0.89\n-0.81\n-0.67\n-0.62\nconfucius bedroom\n-0.86\n-0.81\n-0.61\n-0.58\nalien teagarden\n-0.84\n-0.80\n-0.63\n-0.60\nretro arcade\n-0.84\n-0.79\n-0.59\n-0.56\nanne frank room\n-0.87\n-0.80\n-0.64\n-0.57\nhades cave\n-0.85\n-0.76\n-0.63\n-0.56\nshrek home\n-0.86\n-0.78\n-0.59\n-0.53\nsmurf house\n-0.88\n-0.77\n-0.65\n-0.56\nmad scientist restaurant\n-0.81\n-0.78\n-0.61\n-0.60\nwestern saloon\n-0.82\n-0.79\n-0.60\n-0.59\noccult cult\n-0.82\n-0.79\n-0.61\n-0.59\nmarvel bedroom\n-0.91\n-0.87\n-0.63\n-0.59\nmurder room\n-0.85\n-0.77\n-0.62\n-0.57\nstrawberry church\n-0.84\n-0.79\n-0.58\n-0.57\nposeidon living room\n-0.83\n-0.77\n-0.61\n-0.55\nnorth korean classroom\n-0.85\n-0.77\n-0.62\n-0.57\nantichrist vatican\n-0.82\n-0.77\n-0.60\n-0.57\nromantic restaurant\n-0.81\n-0.78\n-0.59\n-0.59\nbusy new york street\n-0.79\n-0.79\n-0.62\n-0.63\nTable 1. A comparison of CLIP-D (abbreviated D) and CLIP-D/S (abbrevi-\nated D/S) for all 20 scenes created using assets generated by our method\n(abbreviated o) and those generated by a baseline method (abbreviated b),\nwhich does not use semantic upsampling. Our method generally produces\nboth higher performance in both CLIP-D (diversity) and CLIP-D/S.\nin visual similarity with respect to the abstract description of the\nwhole scene. An example of this can be seen in Figure 11.\n4.6\nUser study\nThe metrics used to evaluate our method thus far are heuristical. To\ngauge the true value of our system, we conduct a user study com-\nposed of 72 human evaluators, across 11 randomly selected scenes\nin the full list of 20, for a total of 792 annotations. Each human\nevaluators are first shown two options: (1) a scene rendering com-\nposed of assets generated by our system and (2) a scene rendering\ncomposed of assets generated by the baseline system (see Section\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n9\nFig. 11. The original Future3D asset retrieved and the same asset retextured\nby our system for the scene described by \u201ca marvel-themed bedroom of a\nfive-year old toddler\u201d. The effective texturing prompt created by semantic\nupsampling is \u201cchair, in a scene of a marvel-themed bedroom for a five-year-\nold toddler, red and blue colors. no signs of wear and tear, firm supporting\ncushions.\u201d Note how the semantic adherence to the scene description in-\ncreases after retexturing!\nScene Reference\nOrig. \u2191\nRetextured \u2191\n% Improved\nrustic backyard\n0.17\n0.20\n80.00\nfuturistic teahouse\n0.21\n0.19\n28.00\nconfucius bedroom\n0.23\n0.23\n63.33\nalien teagarden\n0.18\n0.20\n70.59\nretro arcade\n0.21\n0.23\n66.67\nanne frank room\n0.18\n0.23\n85.19\nhades cave\n0.20\n0.20\n74.29\nshrek home\n0.18\n0.25\n93.55\nsmurf house\n0.18\n0.21\n85.00\nmad scientist restaurant\n0.19\n0.18\n44.74\nwestern saloon\n0.16\n0.19\n81.82\noccult cult\n0.20\n0.20\n48.28\nmarvel bedroom\n0.23\n0.27\n84.38\nmurder room\n0.20\n0.21\n60.00\nstrawberry church\n0.20\n0.22\n78.26\nposeidon living room\n0.20\n0.21\n56.00\nnorth korean classroom\n0.17\n0.21\n85.00\nantichrist vatican\n0.18\n0.20\n73.91\nromantic restaurant\n0.17\n0.20\n76.60\nbusy new york street\n0.15\n0.16\n68.75\nTable 2. The mean CLIP-S scores of generated asset collections w.r.t. their\nabstract scene description, with (Retextured) and without (Orig.) the re-\ntexturing using the semantic shopping lists. \u201c% Improve\u201d indicates the\npercentage of assets in the collection whose CLIP-S scores increased after\nretexturing. This shows that the retexturing is a valuable step of the pipeline\nto return assets that are more aligned with the scene semantics.\n4.4). To decouple semantic alignment of the composed scene from\nthe quality and diversity of the assets themselves, each evaluator\nis then asked two questions: (1) which arrangement of 3D assets is\nmore accurate/faithful to the scene description? (2) If you were a 3D\nartist, which group of assets would you use to create a scene that\nmatches the scene description? (Considering diversity, quality ...etc).\nScene reference\nQ1(base)\nQ1(our)\nQ2(base)\nQ2(our)\nposeidon living room\n25 %\n75%\n23.6%\n76.4%\nromantic restaurant\n9.7%\n90.3%\n19.4%\n80.6%\nretro arcade\n6.9 %\n93.1%\n5.6%\n94.4%\nanne frank room\n31.9%\n68.1%\n22.2%\n77.8%\nsmurf house\n18.1%\n81.9%\n25%\n75%\nmurder room\n4.2%\n95.8%\n9.7%\n90.3%\nshrek home\n9.7%\n90.3%\n12.5%\n87.5%\nconfucius bedroom\n63.9%\n36.1%\n59.7%\n40.3%\nmarvel bedroom\n16.7%\n83.3%\n25%\n75%\nfuturistic teahouse\n48.6%\n51.4%\n50%\n50%\nwestern saloon\n12.5%\n87.5%\n19.4%\n80.6%\nTable 3. The percentage of human evaluators who selected each option for\nthe two questions. Q1 indicates the first question: \u201cWhich arrangement of\n3D assets is more accurate/faithful to the scene description?\u201d Q2 indicates\nthe second question: \u201cIf you were a 3D artist, which group of assets would\nyou use to create a scene that matches the scene description? (Consider-\ning diversity, quality ...etc)\u201d. Our system (our) is consistently favored for\nboth questions over the baseline (base), except for one scene. This will be\nexpanded upon further in the Appendix.\nEvaluators can only select one of the two options for each ques-\ntion. Please see the Appendix for the images shown to the human\nevaluators.\nA summary of user selections for these two questions for each\nof the 11 scenes is shown in Table 3. Note that in 10 out of the 11\nscenes, the study showed that using the assets generated by our\nsystem allows the creator of the scene to better match the semantics\nof the abstract scene description, compared to assets generated by a\nversion of our system without semantic upsampling. Additionally\nin 9 out of the 11 scenes, the assets were also considered a better\nselection for 3D artists for creating similar scenes. This demonstrates\nthe efficacy of semantic upsampling in producing both more diverse\nand relevant assets, and their ability to constitute more semantically\naligned 3D scenes.\n5\nDISCUSSION & CONCLUSION\nIn this paper, we present a system that leverages the common sense\nunderstanding of LLMs, Vision-Language models and Diffusion\nmodels to tackle the problem of 3D assets stylization given abstract\nscene descriptions. Our system uses the key insight that to generate\nhigher quality elements that compose a scene, we can mine the\ncommon sense understanding of GPT-3 to semantically upsample\nthe scene semantics based on the abstract scene description using\nin-context learning. The result is an intermediary representation\nthat is human-readable, editable, and conducive towards higher\nquality retrieval and texturing of 3D assets.\nAs a framework for 3D asset generation, our system offers an easy\nmethod to transfer the world knowledge of foundational models ex-\ntracted from modalities like image and text to identifying, texturing\nand composing meshes that can be used to construct a scene. As the\nreasoning, generation and texturing potential of these underlying\nfoundation models improve, so would our system outputs.\nWe showcase our system in action using diverse language in-\nputs, and show the importance of various aspects of our framework\nthrough both quantitative metrics and user studies. In addition, we\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n10\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\ndemonstrate the power and the robustness to our framework af-\nforded by leveraging foundation models for this task in a zero-shot\nmanner.\nAlthough our work makes an important step towards scene syn-\nthesis, there are still many open questions to be addressed in future\nresearch. For instance, generating valid scene-layouts in an open-\nvocabulary and generalizable way remains a challenge. Furthermore,\nfuture efforts in inferring more 3D consistent texture maps as well\nas material properties from generative image models are also valu-\nable. Finally, it would also be useful to develop methods that can\nadequately generate appropriate backgrounds for asset collections.\nREFERENCES\nPanos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas\nGuibas. 2020. Referit3d: Neural listeners for fine-grained 3d object identification\nin real-world scenes. In Computer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16. Springer, 422\u2013440.\nPanos Achlioptas, Ian Huang, Minhyuk Sung, Sergey Tulyakov, and Leonidas Guibas.\n2022. ChangeIt3D: Language-Assisted 3D Shape Edits and Deformations. https:\n//changeit3d.github.io/ (2022).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n2020. Language models are few-shot learners. Advances in neural information\nprocessing systems 33 (2020), 1877\u20131901.\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner,\nManolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Matterport3d:\nLearning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158\n(2017).\nAngel Chang, Will Monroe, Manolis Savva, Christopher Potts, and Christopher D\nManning. 2015b. Text to 3d scene generation with rich lexical grounding. arXiv\npreprint arXiv:1505.06289 (2015).\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang,\nZimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. 2015a. Shapenet:\nAn information-rich 3d model repository. arXiv preprint arXiv:1512.03012 (2015).\nMingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin\nStoyanov, and Zornitsa Kozareva. 2022. Improving In-Context Few-Shot Learning\nvia Self-Supervised Training. arXiv preprint arXiv:2205.01703 (2022).\nBlender Online Community. 2018. Blender - a 3D modelling and rendering package.\nBlender Foundation, Stichting Blender Foundation, Amsterdam. http://www.blender.\norg\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vander-\nBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2022.\nObjaverse: A Universe of Annotated 3D Objects. arXiv preprint arXiv:2212.08051\n(2022).\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\nJingjing Xu, and Zhifang Sui. 2022. A Survey for In-context Learning. arXiv preprint\narXiv:2301.00234 (2022).\nRafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. 2023. Scenescape: Text-\ndriven consistent scene generation. arXiv preprint arXiv:2302.01133 (2023).\nHuan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng,\nChengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 2021a. 3d-front: 3d furnished rooms\nwith layouts and semantics. In Proceedings of the IEEE/CVF International Conference\non Computer Vision. 10933\u201310942.\nHuan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and\nDacheng Tao. 2021b. 3d-future: 3d furniture shape with texture. International\nJournal of Computer Vision (2021), 1\u201325.\nRao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. 2022. Shapecrafter:\nA recursive text-conditioned 3d shape generation model.\narXiv preprint\narXiv:2207.09446 (2022).\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or\nLitany, Zan Gojcic, and Sanja Fidler. 2022. Get3d: A generative model of high quality\n3d textured shapes learned from images. Advances In Neural Information Processing\nSystems 35 (2022), 31841\u201331854.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nLukas H\u00f6llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. 2023.\nText2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models.\nIan Huang, Panos Achlioptas, Tianyi Zhang, Sergey Tulyakov, Minhyuk Sung, and\nLeonidas Guibas. 2022. LADIS: Language disentanglement for 3D shape editing.\narXiv preprint arXiv:2212.05011 (2022).\nKa-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. 2022. Neural template: Topology-\naware reconstruction and disentangled generation of 3d meshes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18572\u201318582.\nNikolai Ilinykh, Sina Zarrie\u00df, and David Schlangen. 2019. Tell me more: A dataset of\nvisual scene description sequences. In Proceedings of the 12th international conference\non natural language generation. 152\u2013157.\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. 2022.\nZero-shot text-guided object generation with dream fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 867\u2013876.\nHeewoo Jun and Alex Nichol. 2023. Shap-E: Generating Conditional 3D Implicit\nFunctions. arXiv preprint arXiv:2305.02463 (2023).\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022. Magic3D: High-\nResolution Text-to-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022).\nRui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, S\u00f6ren Pirk, Binh-Son Hua, Sai-Kit\nYeung, Xin Tong, Leonidas Guibas, and Hao Zhang. 2018. Language-driven synthesis\nof 3D scenes from scene databases. ACM Transactions on Graphics (TOG) 37, 6 (2018),\n1\u201316.\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022.\nText2mesh: Text-driven neural stylization for meshes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 13492\u201313502.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl:\nLearning to learn in context. arXiv preprint arXiv:2110.15943 (2021).\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022.\nPoint-E: A System for Generating 3D Point Clouds from Complex Prompts. arXiv\npreprint arXiv:2212.08751 (2022).\nDespoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and\nSanja Fidler. 2021. Atiss: Autoregressive transformers for indoor scene synthesis.\nAdvances in Neural Information Processing Systems 34 (2021), 12013\u201312026.\nRyan Po and Gordon Wetzstein. 2023. Compositional 3D Scene Generation using\nLocally Conditioned Diffusion. arXiv preprint arXiv:2303.12218 (2023).\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion:\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.\nLearning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning. PMLR, 8748\u20138763.\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTEXTure: Text-Guided Texturing of 3D Shapes. arXiv preprint arXiv:2302.01721\n(2023).\nDaniel Ritchie, Kai Wang, and Yu-an Lin. 2019. Fast and flexible indoor scene synthesis\nvia deep convolutional generative models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 6182\u20136190.\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista,\nNathan Paczan, Russ Webb, and Joshua M. Susskind. 2021. Hypersim: A Photore-\nalistic Synthetic Dataset for Holistic Indoor Scene Understanding. In International\nConference on Computer Vision (ICCV) 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts\nfor in-context learning. arXiv preprint arXiv:2112.08633 (2021).\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems 35 (2022), 36479\u201336494.\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco\nFumero, and Kamal Rahimi Malekshan. 2022. Clip-forge: Towards zero-shot text-to-\nshape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 18603\u201318613.\nPratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova, Melinos\nAverkiou, Andreas Andreou, Siddhartha Chaudhuri, and Evangelos Kalogerakis.\n2021. BuildingNet: Learning to label 3D buildings. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 10397\u201310407.\nSeongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop\nKim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, et al. 2022.\nOn the effect of pretraining corpora on in-context learning by a large-scale language\nmodel. arXiv preprint arXiv:2204.13509 (2022).\nShuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. 2015. Sun rgb-d: A rgb-d scene\nunderstanding benchmark suite. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 567\u2013576.\nShuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas\nFunkhouser. 2017. Semantic scene completion from a single depth image. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition. 1746\u20131754.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022. Iteratively prompt pre-trained language\nmodels for chain of thought. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing. 2714\u20132730.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n11\nKai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang, and Daniel\nRitchie. 2019. Planit: Planning and instantiating indoor scenes with relation graph\nand spatial prior networks. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1\u201315.\nXinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner. 2021. Sceneformer: Indoor\nscene generation with transformers. In 2021 International Conference on 3D Vision\n(3DV). IEEE, 106\u2013115.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are\nzero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain of thought prompting elicits reasoning in large language\nmodels. arXiv preprint arXiv:2201.11903 (2022).\nJiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and\nShenghua Gao. 2022. Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape\nPrior and Text-to-Image Diffusion Models. arXiv preprint arXiv:2212.14704 (2022).\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou.\n2022. When and why vision-language models behave like bags-of-words, and what\nto do about it? arXiv e-prints (2022), arXiv\u20132210.\nJingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. 2023. Text2NeRF:\nText-Driven 3D Scene Generation with Neural Radiance Fields. arXiv preprint\narXiv:2305.11588 (2023).\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of\nthought prompting in large language models. arXiv preprint arXiv:2210.03493 (2022).\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate\nbefore use: Improving few-shot performance of language models. In International\nConference on Machine Learning. PMLR, 12697\u201312706.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting\nenables complex reasoning in large language models. arXiv preprint arXiv:2205.10625\n(2022).\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n12\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nAppendices\nA\nTEMPLATES FOR SEMANTIC UPSAMPLING\nThe templates used during the semantic upsampling stage of the\nsystem to perform in-context learning with GPT-3 are manually\ncreated. We create 4 templates, used different combinations during\nsemantic upsampling phase as the model\u2019s focus moves down the\nscene hierarchy.\nThe first template is used to extract \u201canchor\u201d objects of a scene:\nHere we are building a 3D scene of a french restaurant. At\neach step, we are not adding more than 8 assets in total into\nthe scene.\nFirst, we place the most important assets (e.g. furnitures,\nbigger objects) and use those as our anchors. Here is a list of\nthem:\n* Tables : 1\n* Chairs : 4\n* Bar : 1\n* Bar stools : 2\nThe second template is used to progress down the asset hierar-\nchy towards the more peripheral (and sometimes more decorative)\nassets.\nNext we enhance the scene with more assets, in relation to\nthe anchor objects. In relation to the \u2018table\u2018, here is the list of\nassets we add:\n* Tablecloth : 1\n* Plates : 4\n* Silverware : 4\n* Wine glasses : 2\nAt each level of the hierarchy we can generate appearance at-\ntributes by conditioning the LLM input with this template:\nSuppose we want to create a shopping list for the items we\nneed to create the above scene of a fancy french restaurant. It\nwould look like, being specific about the brand and the visual\nproperties:\n* Table : country style farmhouse table, oakwood and dark\nbrown.\n* Chairs : provincial style chairs, upholstered in ivory velvet.\n* Bar : Traditional style bar counter, white marble, gold\naccents on the corners.\n* Bar stools : provincial style bar stools, upholstered in ivory\nvelvet, golden accents on corners\nAs well as their physical condition, by conditioning using this\ntemplate:\nDescribe the physical condition of these items in a scene of a\nfancy french restaurant:\n* Table : smooth, polished finish.\n* Chairs : slight signs of wear on the sides.\n* Bar : slight signs of wear.\n* Bar stools : slight wear on the rattan seats.\nIt is possible that using only a small set of templates based on a\nsingle indoor scene may limit the LLM\u2019s ability to perform semantic\nupsampling for outdoor scenes. We see such behavior in Figure 43,\na failure case of our system. Future works should consider how to\nbetter incorporate a more diverse set of templates to improve the\nsystem\u2019s generalizability to outdoor scenes and other scenes.\nB\nHIERARCHY OF SEMANTIC SHOPPING LISTS\nDuring the semantic upsampling step of our system, we condition\nGPT-3 to hallucinate semantic detail in a hierarchical fashion; that\nis, it first starts out by generating details of \u201canchor\u201d objects (i.e. the\nkey objects within the scene) before recursively generating details\nof \u201cperipheral objects\u201d. A natural question is, does this method\nnaturally yield meaningful hierarchies in object groupings within\nthe scene?\nFigure 12 shows the resultant hierarchy of object categories for\na single scene. We can see that at times, GPT-3 doesn\u2019t output\nperipheral objects that form natural groups with the anchor objects,\nand this varies based on the class of the anchor object. For objects\nthat typically have objects placed on, inside or around them, GPT-3\nis typically able to capture this regularity (see the peripheral objects\naround the Table in Figure 12, for example).\nGenerating more semantically meaningful groupings of objects\nusing LLM\u2019s is a challenging task for future works, and can lay the\nfoundation for methods that predict placements of objects according\nto coarse specifications in their positional relations.\nC\nINPUT SCENE DESCRIPTIONS, SEMANTIC\nSHOPPING LISTS & SCENE RENDERINGS\nBelow, we display the semantic shopping lists and scene renderings\nfor each of the 20 scenes mentioned in the main paper. 11 were\nselected randomly for the user study, and have baseline visualiza-\ntions used for our human evaluation. For each scene, its input scene\ndescription and the corresponding scene reference (used in tables\nin the Experiments section of the main paper) is indicated in the\nsection title. A star (\u2605) indicates that the scene was selected for the\nuser study.\nC.1\n(\u2605) Poseidon\u2019s living room (poseidon living room)\u2013\nFigures 13 and 14\nInput Scene Description : Poseidon\u2019s living room Semantic\nshopping list\n(1) throne: with intricate carvings of ocean life.glossy, pol-\nished finish.\n(2) fur rug: ivory and white with a hint of blue.soft and\nfluffy, with subtle wave patterns.\n(3) pillows: navy blue with gold accents.plump and luxuri-\nous.\n(4) candelabras: gold-plated with intricate designs.shiny\nand lustrous.\n(5) shield: gold-plated with intricate designs.gleaming and\nregal.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n13\nInput Scene Description : A characteristic office of the King\u2019s Hand in Game of Thrones\n\u2022 desk : large wooden desk with intricate carvings and gold accents.smooth and polished with no visible signs of wear.\n\u2013 quill : bronze ink tip.smooth and well-polished handle.\n\u2013 candlesticks : metallic gold with intricate leaf patterns.gleaming and polished to a brilliant shine.\n\u2013 inkwell : gold plated with an ornamental bird head on top.smooth and shiny plating with no scratches.\n\u2013 stack of parchment : parchment-like papers of antique beige.crisp and neatly stacked.\n\u2013 book : gold foil embossed title on the front.firm and sturdy spine, no page creases or tears.\n\u2022 chairs : wooden chairs with burgundy upholstery and gold accents.upholstery with no visible signs of wear.\n\u2013 pillows : firm and plush pillows with vibrant colors.\n\u2013 lanterns : clean and polished with bright sheen.\n\u2013 books : well-kept and intact.\n\u2013 candles : unburned and strong scent.\n\u2022 sofa : tufted velvet with no visible signs of wear.\n\u2013 pillows : maroon and grey.plush and velvety fabric with no signs of wear.\n\u2013 blanket : black and gold intricate pattern.soft and smooth wool fabric with no signs of wear.\n\u2013 paintings : gold frames.frames with intricate engravings and no fading of colors.\n\u2013 candles : purple and white.wax melted to a smooth texture, no discoloration.\n\u2013 tapestry : brown and gold pattern.no signs of wear on the fabric, colors still vibrant.\n\u2022 rug : burgundy and gold wool rug with intricate floral patterns.wool rug with no visible signs of wear.\n\u2013 tapestries : thickly woven with golden threads.perfect condition, no signs of fading or wear.\n\u2013 paintings : oil on canvas.smooth and glossy, with vivid colors.\n\u2013 bookshelf : slightly worn edges, but overall in very good condition.\n\u2013 books : no signs of wear and tear, leather still vibrant and glossy.\n\u2022 fireplace : smooth and polished with no visible signs of wear.\n\u2013 firewood : 1 foot long.well-seasoned and dry.\n\u2013 poker : slightly tarnished.\n\u2013 broom : well-used, with fraying edges of the straw.\n\u2013 mantelpiece : polished, with no signs of wear.\n\u2013 fire irons : black with gold accents.slightly rusted, with gold accents still shining.\nFig. 12. The hierarchical version of the semantic shopping list given by semantic upsampling, for the scene description of \u201ca characteristic office of the King\u2019s\nHand in Game of Thrones.\u201d We use only a single level of recursion here (i.e. max depth of the semantic shopping list tree is 1). Notice how for certain anchor\nobjects like the table and fireplace, the children grouped underneath them are plausible. However, for anchor objects that are often stand-alone like chairs, the\nperipheral objects are plausible object categories found near the anchor object.\n(6) trident: gold-plated with intricate designs.sturdy and\nmajestic.\n(7) fireplace: with blue marble accents and a marble man-\ntel.smooth, polished stone.\n(8) firewood logs: split and ready to burn.clean and dry,\nready to burn.\n(9) coal bucket: clean and unscratched.\n(10) fire poker: free of rust and in good condition.\n(11) fireplace screen: clean and undamaged.\n(12) candelabra: shiny with no visible signs of wear.\n(13) couches: upholstered in a deep navy blue.plush and soft,\nwith no signs of wear.\n(14) throw pillows: soft and fluffy to the touch.\n(15) blankets: crisp and plush.\n(16) candlesticks: shiny and well-polished.\n(17) books: new and pristine condition.\n(18) ottoman: with gold accents and an ocean-blue tufted\nupholstery.no signs of wear, pristine condition.\n(19) pillows: plump and pristine, with no signs of wear.\n(20) carpet: plush and vibrant, with no signs of wear.\n(21) vase with flowers: vibrant and fresh, with no signs of\nwear.\n(22) bowl with fruits: vibrant and colorful, with no signs of\nwear.\n(23) books: sturdy and well-preserved, with no signs of wear.\n(24) mermaid statues: standing atop two large seashells.smooth\nmarble, with intricate details in the carvings.\n(25) seashells: white and glossy.smooth, glossy finish.\n(26) fish statues: intricately detailed.no signs of wear or dis-\ncoloration.\n(27) coral: intricate patterns and realistic texture.vibrant col-\nors, no signs of fading or discoloration.\nC.2\n(\u2605) A romantic french restaurant (romantic\nrestaurant)\u2013 Figures 15 and 16\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n14\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 13. Our output for \u201cPoseidon\u2019s living room\"\nFig. 14. Baseline output for \u201cPoseidon\u2019s living room\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n15\nInput Scene Description : a romantic french restaurant\nSemantic shopping list\n(1) tables: two-tone oakwood finish.smooth, polished fin-\nish.\n(2) tablecloths: crisp and clean.\n(3) plates: no chips or cracks.\n(4) silverware: polished and gleaming.\n(5) wine glasses: no scratches or smudges.\n(6) candles: unburned and fragrant.\n(7) flowers: bright and freshly blooming.\n(8) chairs: upholstered in ivory velvet.slight signs of wear\non the sides.\n(9) chair cushions: ivory color.soft and fluffy.\n(10) table linen: clean and wrinkle free.\n(11) candles: scented with lavender.in perfect condition, no\ndripping wax.\n(12) flower vase: polished and sparkling.\n(13) centerpiece: intricately detailed and in perfect condi-\ntion.\n(14) bar: white marble, gold accents on the corners.slight\nsigns of wear.\n(15) bar stools: upholstered in an off-white velvet fabric,\nwith gold accents on the corners.slight signs of wear\non the fabric.\n(16) wine glasses: with golden rims.shiny, with no signs of\nwear.\n(17) cocktail shakers: sleek and modern design.clean, no\nsigns of wear.\n(18) coasters: with a distressed finish and a gold laurel leaf\nin the center.slight scratches due to regular use.\n(19) bottle openers: with a sleek and modern design.shiny\nand unscratched.\n(20) bar stools: upholstered in ivory velvet, golden accents\non corners.slight wear on the rattan seats.\n(21) tablecloth: white and ivory.crisp and clean, no signs of\nwear.\n(22) coasters: white marble coasters with gold accents.smooth,\npolished finish.\n(23) ashtrays: black marble ashtrays with gold accents.slight\nsigns of wear.\n(24) bar napkins: white and ivory.crisp and clean, no signs\nof wear.\n(25) cocktail shaker: smooth, polished finish.\n(26) chandelier: with crystal pendants and antique gold fin-\nish.ornate carvings and sparkling crystal pendants.\n(27) candles: ivory wax.melting wax and spreading soft light.\n(28) beaded curtains: ivory color.shimmering in the light,\nhanging gracefully.\n(29) wall sconces: gold metal with intricate designs.slightly\naged, but still gleaming with their intricate designs.\n(30) lamps: ivory porcelain base with gold accents.softly\nilluminating the room in a warm and inviting light.\n(31) sofa: upholstered in ivory velvet, with golden accents\non the legs.slight signs of wear on the velvet upholstery.\n(32) pillows: ivory and burgundy color.soft and fluffy.\n(33) blanket: ivory and burgundy color.smooth and plush.\n(34) end table: whitewashed oakwood, gold accents.smooth\npolished finish.\n(35) lamp: bronze base, ivory drum shade.no signs of wear.\n(36) candles: ivory and burgundy color.unscented.\n(37) fireplace: ornate carvings and polished gold accents.\n(38) firewood: cut into 16-inch lengths.freshly cut and dried.\n(39) fireplace tools: black finish.smooth and glossy finish.\n(40) candles: scented with lavender.unscented, clean sur-\nfaces.\n(41) throw pillows: in a pink and gold pattern.bright and\nvibrant colors.\n(42) rug: in ivory and blue.tightly woven, plush texture.\n(43) wall art: with gilded frame.crisp colors and gilded frame.\n(44) curtains: off-white color, with ruffles and lace.crisp and\nclean, with no signs of fading or wear.\n(45) vase: with intricate designs.shiny and sparkly, with no\nnicks or scratches.\n(46) flowers: in pink and ivory shades.soft and fragrant, look-\ning freshly cut.\n(47) candle holder: gold plated.smooth and polished finish\nwith no signs of rust.\n(48) candles: beeswax, scented with lavender.no signs of\nmelting, wax evenly distributed.\nC.3\n(\u2605) A retro arcade in the style of the 1980s (retro\narcade) \u2013 Figures 17 and 18\nInput Scene Description : a retro arcade in the style of the\n1980s Semantic shopping list\n(1) pinball machine: featuring lights and sound effects from\nthe 80s.brightly lit led lights, glossy finish and smooth\nmovement.\n(2) flippers: black with chrome accents.minor scratches on\nthe chrome accents.\n(3) pinball bumpers: bright and colorful with cartoon char-\nacters.vibrant colors and cartoon characters on the bumpers.\n(4) pinball slingshots: chrome with black accents.light rust\non the chrome accents.\n(5) arcade machine: space invaders, etc.slight signs of wear\non the buttons and joystick.\n(6) atari posters: bright colors and bold font.crisp colors,\nno rips or tears.\n(7) joysticks: bright colors with red and blue buttons.no\nsigns of wear, bright colors.\n(8) retro speakers: chrome and with a retro design.no signs\nof wear, polished chrome finish.\n(9) coin dispenser: metal with a retro design.slight signs of\nwear, chrome finish still intact.\n(10) bar: preferably in a bright color.shiny finish with slight\nsigns of wear.\n(11) neon-lights: brightly illuminated, no signs of wear.\n(12) drinks: full bottles with clean and crisp labels.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n16\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 15. Our output for \u201ca romantic french restaurant\"\nFig. 16. Baseline output for \u201ca romantic french restaurant\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n17\n(13) beer mugs: free of chips and cracks, vibrant colors.\n(14) ashtrays: no signs of wear, vivid colors.\n(15) bar stools: some rust on the metal frames, but vinyl\nseats still vibrant in color.\n(16) neon signs: operated with a remote.brightly lit and col-\norful.\n(17) ashtrays: slightly faded with signs of wear.\n(18) retro music system: cassette player, and cd player.dusty\nand slightly weathered.\n(19) retro decorations: highly detailed and brightly-colored.\n(20) retro seating: preferably in bright colors.slight signs of\nwear on the fabric, but still vibrant in color.\n(21) throw pillows: slightly faded, with some minor signs of\nwear.\n(22) drink holders: slightly tarnished, but still in good con-\ndition.\n(23) cushions: slightly faded and worn, but otherwise in\ngood condition.\n(24) vintage poster: slightly faded, but still vibrant colours.\nC.4\n(\u2605) Anne frank\u2019s room during world war II (anne frank\nroom)\u2013 Figures 19 and 20\nInput Scene Description : anne frank\u2019s room during world\nwar II Semantic shopping list\n(1) bed: made of solid wood.slightly weathered and with a\nfew scuffs.\n(2) blanket: beige color.discolored with age, but still intact.\n(3) pillow: white cover.slightly lumpy from years of use.\n(4) bedside table: dark brown stain.well-worn with scratches\nand chips.\n(5) lamp: yellow glass shade.tarnished, but still functional.\n(6) book: embossed cover.faded around the edges, with\nnotes written inside.\n(7) clothes: blue and white checkered patterns.slightly worn,\nbut still in good condition.\n(8) desk: wooden construction and a dark wood finish.slightly\nworn on the edges.\n(9) pencils: natural wood color.slightly worn and dull, with\na few scratches.\n(10) books: with antique paper pages.faded covers, pages\nslightly yellowed from aging.\n(11) paper: aged look.delicate and fragile, with visible signs\nof wear.\n(12) postcards: slightly faded, with minor creases and wrin-\nkles.\n(13) bookshelf: slightly worn on the edges.\n(14) books: with the original cover design.slightly worn\ncover with some fading.\n(15) candle: with a simple black holder.wax melted down to\nthe base.\n(16) picture frame: with a white matte finish.slight scratches\non the corner of the frame.\n(17) doll: wearing a white dress with a floral pattern.age-\nrelated discoloration, with some loose threads.\n(18) chair: slightly worn on the fabric.\n(19) cushion: patchwork design with embroidery.slightly\nworn and faded from time.\n(20) blankets: one with patchwork, the other plain.slight\nfraying and discoloration from wear.\n(21) books: such as anne of green gables or the adventures\nof tom sawyer.faded pages, with some discoloration due\nto age.\n(22) pen: with an ornate design.slight wear from age, but\nstill usable.\n(23) notebook: with embossed floral design.vintage leather,\nbut still in good condition.\n(24) clothes: gray or navy with white buttons.slightly faded,\nbut still in wearable condition.\n(25) cabinet: some scratches, but overall in relatively good\ncondition.\n(26) books: slightly faded and tattered covers.\n(27) clothes: simple and worn.\n(28) photos: slightly faded and yellowed.\n(29) letters: wrinkled and slightly faded.\n(30) curtains: slightly faded and with some small tears.\n(31) window: with a thin glass pane and black iron hingesslightly\nworn with scratches from use, but still in good condi-\ntion.\n(32) photos: crisp black and white photos, without any dis-\ncoloration.\n(33) books: in a variety of colors such as black, brown, and\nred.some signs of wear on the spines and edges, but\noverall in good condition.\n(34) bedding: slightly worn from years of use, but still in\ngood condition.\n(35) clothes: and warm sweaters in muted colors.slightly\nworn, but still in good condition.\nC.5\n(\u2605) The interior of the smurf house (smurf house)\u2013\nFigures 21 and 22\nInput Scene Description : the interior of the smurf house\nSemantic shopping list\n(1) mushroom house: blue and white in color.new and pris-\ntine\n(2) mushrooms: with blue-green dots.vibrant colors, no\nchips or cracks.\n(3) flowers: petals slightly wilted, with no fading of colors.\n(4) window frames: smooth surface, no signs of wear and\ntear.\n(5) fireplace: no signs of wear or discoloration.\n(6) trees: green leaves.vibrant with fresh leaves\n(7) wildflowers: vibrant and lifelike, with no tears or signs\nof wear.\n(8) leaves: soft to the touch, with no rips or tears.\n(9) acorns: glossy and realistic, with no cracks or chips.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n18\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 17. Our output for \u201ca retro arcade in the style of the 1980s\"\nFig. 18. Baseline output for \u201ca retro arcade in the style of the 1980s\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n19\nFig. 19. Our output for \u201canne frank\u2019s room during world war II\"\nFig. 20. Baseline output for \u201canne frank\u2019s room during world war II\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n20\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\n(10) mushrooms: white caps, and red spots.smooth and glossy,\nwith no tears or creases.\n(11) table: clean and smooth\n(12) tablecloth: crisp and clean.\n(13) plates: no chips or cracks.\n(14) silverware: no tarnishing.\n(15) smurf-shaped cupcakes: with edible smurf decorations.freshly\nbaked and frosted.\n(16) chairs: sturdy and clean\n(17) flowers: in various shapes.pristine condition, no signs\nof wear or tear.\n(18) birdhouse: with a pointed roof.freshly painted and in\ngood condition.\n(19) gnome figurines: dressed in traditional smurf attire.vibrant\ncolors, no chips or cracks.\n(20) paintings: crisp edges, no fading of colors.\n(21) smurf figures: new and vivid in color.\n(22) mushrooms: bright and glossy, with no signs of wear.\n(23) flowers: bright and vibrant colors, no signs of fading.\n(24) fishing rod: smooth and painted with bright colors, no\nsigns of wear.\n(25) basket: tightly woven and sturdy, no signs of fraying.\nC.6\n(\u2605) A murder in an abandoned living room (murder\nroom)\u2013 Figures 23 and 24\nInput Scene Description : a murder in an abandoned living\nroom Semantic shopping list\n(1) couch: stained and torn fabric, fraying at the edges.\n(2) pillows: slightly worn and stained.\n(3) blankets: light greytorn and stained.\n(4) blood stains: dried and smeared across surfaces.\n(5) magazine: preferably one from the 1960saged and yel-\nlowed.\n(6) candle: melted and partially burned.\n(7) lamp: dusty and cobwebbed.\n(8) table: made from teak wood.scuffed and scratched sur-\nfaces.\n(9) lamp: dusty and worn.\n(10) papers: scattered on the floor.\n(11) pen: traces of dried ink on the nib.\n(12) gun: scratched and worn.\n(13) blood spatters: with a realistic texture.slightly damp in\nplaces.\n(14) desk: made of reclaimed wood.dust and dirt accumu-\nlated in the corners.\n(15) lamp: bronze and cream.tarnished, dented, and dusty.\n(16) pen holder: scuffed and scratched.\n(17) pens/pencils: dull, worn, and faded.\n(18) stack of paper: aged parchment paper.yellowed and\naged.\n(19) mug: with a faded sketch of a crow.chipped and cracked.\n(20) chair: upholstered in a grey linen fabric.signs of wear\nin the upholstery.\n(21) pillow: slightly stained and worn out.\n(22) blanket: faded grey.discolored and frayed.\n(23) candle: waxen and melted.\n(24) book: discolored pages with visible creases and wrin-\nkles.\n(25) newspaper: torn and crumpled.\n(26) coffee cup: cracked handle.broken handle and stained.\n(27) bottle: empty, with a cork stopper.empty, with a cork\nstopper.\n(28) window: cracked and broken glass.\n(29) shattered glass: silver and black.shards of broken glass\nscattered on the floor.\n(30) curtains: black and grey.torn and tattered.\n(31) window blinds: black and grey.torn and damaged, some\nof the slats are missing.\n(32) bullet hole: black.a large circular hole in the wall with\nfrayed edges.\n(33) blood stains: red and black.dark red splatters on walls\nand furniture.\n(34) door: made of reclaimed wood.large dents and scratches.\n(35) key: rusty and tarnished.\n(36) knob: rusty and tarnished.\n(37) light switch: flickering and weak.\n(38) window curtains: black velvet curtains.ripped and tat-\ntered.\n(39) bloodstain: red, water-based paint.fresh and bright.\n(40) lamp: bronze with a white shade.slightly dusty.\n(41) photo frame: antique gold frame for a 4x6 photo.slightly\ndusty and cracked.\n(42) rug: faded colors and worn out threads.\n(43) blood stains: non-toxic and non-hazardous.smeared and\nsplattered on the walls and floors.\n(44) broken glass: realistic-looking, made of plastic.scattered\nacross the room in pieces.\n(45) empty beer bottle: made of plastic.lying upside down\non the floor.\n(46) bloodied knife: lying next to the empty beer bottle.\n(47) bullet casing: made of plastic.scattered across the floor.\n(48) bloodied rag: lying in a corner of the room.\nC.7\n(\u2605) Bedroom in Shrek\u2019s home in the swamp (shrek\nhome)\u2013 Figures 25 and 26\nInput Scene Description : bedroom in shrek\u2019s home in the\nswamp Semantic shopping list\n(1) bed: dark wood with green fabric accents.sturdy and in\ngood condition.\n(2) pillows: fluffy, with no signs of wear.\n(3) blanket: slightly worn edges, with a soft texture.\n(4) stuffed animal: plush, cartoon-style ogre.soft, plush,\nwith no signs of wear.\n(5) books: slightly tattered corners, with no major scuffs\nor scratches.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n21\nFig. 21. Our output for \u201cthe interior of the smurf house\" \"\nFig. 22. Baseline output for \u201cthe interior of the smurf house\" \"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n22\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 23. Our output for \u201ca murder in an abandoned living room\"\nFig. 24. Baseline output for \u201ca murder in an abandoned living room\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n23\n(6) shoes: cartoon-style ogre shoes.new, with bright colors\nand sturdy soles.\n(7) cabinet: green paint with gold accents.minor signs of\nwear.\n(8) cabinet knob: cast iron with a green and gold finish.slightly\nworn, with a few scratches.\n(9) dishes: with a green and brown glaze.no chips or cracks,\nwith a glossy finish.\n(10) bowls: with a green and brown glaze.no chips or cracks,\nwith a glossy finish.\n(11) mugs: with a green and brown glaze.no chips or cracks,\nwith a glossy finish.\n(12) desk: distressed wood with iron accents.minor signs of\nwear.\n(13) pencil holder: stained with a dark green finish.sturdy,\nwith no visible scratches.\n(14) notepad: wooden notepad with a \"shrek\" motif carved\non the cover.smooth, with no visible signs of wear and\ntear.\n(15) pens: with green and yellow feathers, stored in a velvet\npouch.clean and sharp, with no visible stains.\n(16) books: bound in green leather and embossed with golden\nlettering.unmarked, with gold lettering intact and no\nvisible wear and tear.\n(17) chair: green velvet upholstery and gold accents.minor\nsigns of wear.\n(18) blanket: soft and warm.\n(19) bookshelf: well-crafted and sturdy.\n(20) books: featuring characters from the shrek movie.new\nand in pristine condition.\n(21) rug: green and gold motif.thick and plush.\n(22) throw pillows: squishy texture.fluffy with no visible\nsigns of wear.\n(23) blanket: plaid design woolen blanket in brown and\ngreen tones.free from any snags or tears.\n(24) bookshelf: smooth, polished finish with minimal signs\nof wear.\n(25) books: perfectly bound and crisp pages with no signs\nof fading.\n(26) lamp: green glass with bronze accents.no chips or cracks.\n(27) lamp shade: smooth and polished finish with vibrant\ncolors.\n(28) lightbulb: bright and evenly lit.\n(29) lampshade finial: smooth and well-polished.\n(30) lamp base: smoothly painted with vibrant colors.\n(31) lampshade fringe: soft and lightweight.\n(32) lampshade harp: shiny and polished with a patina finish.\n(33) lampshade diffuser: vibrant colors and free of wrinkles.\n(34) wall art: framed in gold.vibrant colors, free of any dam-\nage.\n(35) stuffed animals: one donkey and one puss in boots.soft\nand cuddly.\n(36) painting: depicting various characters from the movie.vivid\nand colorful.\n(37) picture frame: sturdy and unblemished.\n(38) bookshelf: with a whimsical design.polished and smooth-\nto-the-touch.\n(39) books: titles to include \"shrek\u2019s adventures\" and \"fairy\ntales of the swamp\".crisp pages, with vibrant illustra-\ntions.\nC.8\n(\u2605) Confucius\u2019s bedroom (confucius bedroom)\u2013\nFigures 27 and 27\nInput Scene Description : confucius\u2019s bedroom Semantic\nshopping list\n(1) bed: solid oak wood, dark brown finish.gently worn but\nstill in good condition.\n(2) pillows: with intricate golden embroidery.soft and plush.\n(3) blankets: with tassels and embroidered motifs.soft and\nsmooth to touch.\n(4) bedside cabinet: carved with dragons and made of dark\nwood.polished and gleaming.\n(5) scrolls: unrolled and in perfect condition.\n(6) incense burner: with dragon etchings.bright and shin-\ning.\n(7) dresser: painted with floral motifs, gold accents.some\nsigns of wear but still colorful and vibrant.\n(8) boxes: lacquered in gold.lightly distressed with some\nwear around the edges.\n(9) scrolls: mounted and framed.bright and colorful, lightly\naged.\n(10) books: hardcover in blue and green.slightly faded covers\nwith faint traces of wear.\n(11) vases: painted with floral motifs.some signs of wear and\ncracking, but still in good condition.\n(12) desk: dark cherry wood, golden detailing.well-maintained\nwith a slight patina.\n(13) pen holder: glossy, with intricate carvings on the edges.\n(14) ink stone: made of black slate.polished smooth surface.\n(15) ink brush: made of bamboo and horsehair bristles.bristles\nin perfect condition.\n(16) scroll: with calligraphy writing.no signs of wear or tear.\n(17) papers: made of hemp.crisp and clean.\n(18) chair: carved and upholstered in silk.well-maintained\nwith minimal signs of wear.\n(19) blanket: plain blue or red with yellow embroiderysoft\nand well worn.\n(20) bookshelf: dark brown, metal accents on edgeswell-\npolished with metal accents gleaming.\n(21) books: aged and well-handled, with a few loose pages.\n(22) scroll: traditional red or black inkcrisp and vibrant, with\nthe ink color vividly preserved.\n(23) inkstone: black stone with carved dragon designglossy\nand smooth, with the dragon design still clearly visible.\n(24) bamboo screen: natural brown.strong and sturdy, with\na few scratches.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n24\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 25. Our output for \u201cbedroom in shrek\u2019s home in the swamp\"\nFig. 26. Baseline output for \u201cbedroom in shrek\u2019s home in the swamp\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n25\n(25) scrolls: with chinese characters.bright colors, free of\ntears and creases.\n(26) ink-stone pot: ceramic, with intricate patterns.smooth,\npolished finish, free of scratches and cracks.\n(27) brush and ink: bristles are intact, handle is strong and\nsturdy.\n(28) calligraphy scrolls: with chinese characters.neatly cut\nedges, free of tears and wrinkles.\n(29) paper fans: with painted chinese characters.no tears or\ncreases, colors are still vibrant.\n(30) table lamps: golden finish, hand-painted shades.slightly\nweathered but still bright and colorful.\n(31) paintings: framed in black lacquer.smooth, glossy finish.\n(32) writing utensils: black lacquer handle.intact and in pris-\ntine condition.\n(33) scrolls: framed in black lacquer.slightly yellowed edges\nfrom age but overall in good condition.\n(34) candles: with intricate gold designs.intact with no chips\nor cracks.\nC.9\n(\u2605) A Marvel-themed bedroom of a five-year old\ntoddler (marvel bedroom)\u2013 Figures 29 and 30\nInput Scene Description : a marvel-themed bedroom of a\nfive-year old toddler Semantic shopping list\n(1) bed: red and blue colors.no signs of wear and tear, firm\nmattress.\n(2) blanket: designed with the avengers characters.bright\nand vibrant colors with no visible signs of wear.\n(3) pillows: designed with the avengers characters.bright\nand vibrant colors with no visible signs of wear.\n(4) stuffed toys: no visible signs of wear, with soft and fluffy\nstuffing.\n(5) marvel posters: with motivational quotes.crisp paper\nstock, with bright and vibrant colors.\n(6) dresser: red and blue colors.smooth finish, no signs of\nwear and tear.\n(7) basket for clothes: decorated with captain america im-\nagery.brand new.\n(8) lamp: led lighting.brand new.\n(9) mirror: brand new.\n(10) decorative items: 1- iron man action figure, 1- captain\namerica shield wall art.brand new.\n(11) chair: red and blue colors.no signs of wear and tear, firm\nsupporting cushions.\n(12) pillow: fluffy and soft to the touch.\n(13) blanket: printed with comic book characters.soft and\nlightweight with vibrant colors.\n(14) marvel figures: 4-6 inches tall.pristine condition with\nno scratches or marks.\n(15) marvel posters: crisp and vibrant colors, no signs of\nwear.\n(16) marvel stickers: bright and colorful, no peeling or tear-\ning.\n(17) desk: red and blue colors.smooth finish, no signs of wear\nand tear.\n(18) desk lamp: with bright colors and a cartoonish design.bright,\ncolorful, and cartoonish design.\n(19) pen holder: with bright colors and cartoonish design.bright,\ncolorful, and cartoonish design.\n(20) pencils: with bright colors and cartoonish design.bright,\ncolorful, and cartoonish design.\n(21) notebook: with bright colors and cartoonish design.bright,\ncolorful, and cartoonish design.\n(22) marvel-themed action figures: with bright colors and\ncartoonish design.brightly colored and cartoonish de-\nsign, with no signs of wear and tear.\n(23) toy chest: red and blue colors.smooth finish, no signs\nof wear and tear.\n(24) stuffed animals: iron man, captain america and thor.well-\nloved, lightly used with minimal fading or wear.\n(25) action figures: iron man, captain america and thor.well-\nloved, lightly used with minimal fading or wear.\n(26) toys: marvel avengers lego kit, marvel avengers puzzle\nand marvel avengers playdough set.well-loved, lightly\nused, with minor scratches and fading.\n(27) books: well-loved, lightly used, with minor creases and\nscuffs.\n(28) wall art: vibrant colors.vibrant colors, no signs of fading.\n(29) bedding sheets: incredible hulk, thor, and captain amer-\nica.brightly colored and with a soft texture.\n(30) wall stickers: captain america shield wall stickers, iron\nman wall stickers and thor hammer wall sticker with\n3d effects.high quality and vibrant colors.\n(31) wall decals: detailed designs with 3d effects.\n(32) pillows: soft and fluffy with 3d effects.\nC.10\n(\u2605) A futuristic teahouse from the future in silicon\nvalley (futuristic teahouse)\u2013 Figures 31 and 32\nInput Scene Description : a futuristic teahouse from the\nfuture in silicon valley Semantic shopping list\n(1) tables: with a plasma top, stainless steel legs.shiny and\nreflective surface.\n(2) tablecloths: blue and white geometric pattern.pristine\ncondition without any creases or wrinkles.\n(3) plates: stainless steel with holographic accents.shiny,\nsleek finish with no chips or scratches.\n(4) silverware: made from titanium with sleek design.clean,\nsmooth and no dents or scratches.\n(5) tea cups: with holographic accents.no chips or cracks,\nholographic accents in perfect condition.\n(6) holographic menus: with interactive voice activated op-\ntions.crisp, clear display with no distortions or defects.\n(7) chairs: upholstered in light grey fabric.no signs of wear\nand tear.\n(8) arm rests: adjustable height.sleek and modern design,\npolished metal finish.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n26\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 27. Our output for \u201c\u201cconfucius\u2019s bedroom\u201d\"\nFig. 28. Baseline output for \u201c\u201cconfucius\u2019s bedroom\u201d\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n27\nFig. 29. Our output for \u201c\u201ca marvel-themed bedroom of a five-year old toddler\"\nFig. 30. Baseline output for \u201c\u201ca marvel-themed bedroom of a five-year old toddler\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n28\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\n(9) cushions: light grey in color with silver accents.soft and\ncomfortable, with no signs of wear.\n(10) holographic menus: adjustable to display any type of\nmenu.bright and vibrant colors, displaying any type of\nmenu in 3d.\n(11) tea dispensers: able to dispense any type of tea.high-\ntech and modern design, no signs of wear.\n(12) bar: with curved edges and led lighting.smooth, glossy\nfinish.\n(13) tea pot: sleek and shiny, with no signs of use.\n(14) teacups: unworn, with a holographic design shining\nbrightly.\n(15) saucers: unworn, with a holographic design shining\nbrightly.\n(16) holographic menu: brightly glowing, with voice acti-\nvated commands responding quickly.\n(17) bar stools: upholstered in light grey fabric.no signs of\nwear and tear.\n(18) holographic napkins: crisp and clean with vibrant col-\nors.\n(19) holographic tea cups: pristine and dust free with vibrant\ncolors.\n(20) holographic saucers: pristine and dust free with vibrant\ncolors.\n(21) holographic teapot: gleaming and unscratched with vi-\nbrant colors.\n(22) holographic displays: bright and clear images.\n(23) floating screens: crisp, vibrant colours and high resolu-\ntion display.\n(24) holographic images: flickerless, with sharp edges and\nvivid colours.\n(25) interactive screens: responsive to touch and voice com-\nmands.\n(26) artificial intelligence (ai) bots: smooth, gliding motions\nwith natural-sounding speech.\nC.11\n(\u2605) A saloon from an old western\u2013 Figures 33 and 34\nInput Scene Description : a saloon from an old western\nSemantic shopping list\n(1) bar: dark walnut wood, brass foot rail and accents.slightly\nworn edges, signs of age on the finish.\n(2) bar stools: wooden frames, leather seat cushions.worn,\ndistressed wood finish.\n(3) bottles: clear glass.slightly dusty.\n(4) glasses: faceted crystal glass.etched, lightly scratched.\n(5) bar counter: metal foot rail.weathered, aged patina.\n(6) chairs: upholstered in leather, with metal or wooden\nframes.soft leather, with a few scuff marks.\n(7) cowboy hats: with weathered bandanas on the sides.slightly\ndusty, with signs of wear.\n(8) cowboy boots: with weathered stitching.scuffed and\nworn with age.\n(9) cigars: slightly weathered and dry.\n(10) whisky bottle: aged, with signs of condensation.\n(11) tables: wooden legs and a square top.aged wood with a\nrustic finish, some minor scratches.\n(12) tablecloth: red and white.slightly frayed on the edges.\n(13) beer mugs: black with gold accents.rustic finish, show-\ning signs of wear and tear.\n(14) playing cards: slightly creased edges.\n(15) poker chips: white with red and black accents.lightly\nworn, with minor scratches.\n(16) counter: wood and metal accents.aged wood with a few\ndents and scratches.\n(17) whiskey bottle: clear glass and brown stopper.dull glass\nwith some scratches and dull brown stopper.\n(18) shot glasses: clear glass and gold rims.slightly worn\nedges and faded gold rims.\n(19) poker chips: red and black with gold accents.some chips\nare slightly faded and chipped.\n(20) ashtray: made of bronze and silver accents.some tar-\nnishing on the bronze and silver accents.\n(21) jukebox: bright colors, and lights.minor wear and tear,\nwith a few chips in the paint.\n(22) music discs: slightly worn edges, but still in good con-\ndition.\n(23) coins: slightly tarnished, but still in good condition.\n(24) posters: slightly faded, but still in good condition.\n(25) stools: dark brown finish.slightly worn edges, but still\nin good condition.\nC.12\nAn alien tea garden on Mars (alien teagarden)\u2013\nFigure 35\nInput Scene Description : an alien tea garden on mars\nSemantic shopping list\n(1) alien tea table: metallic finish in a deep purple hue.smooth,\nglossy finish.\n(2) alien tea pot: copper metal with silver accents.shiny,\npolished finish.\n(3) alien tea cups: glazed ceramic with green and blue ac-\ncents.smooth and glossy.\n(4) alien tea spoons: silver metal with gold accents.no signs\nof wear.\n(5) alien tea plates: glazed ceramic with blue and purple\naccents.no signs of wear.\n(6) alien tea chairs: bright green hue with a glossy finish.no\nsigns of wear.\n(7) alien tea tablecloths: unblemished and pristine condi-\ntion.\n(8) alien tea cups: smooth and glossy finish.\n(9) alien tea saucers: featuring holographic images.shiny\nand reflective surface.\n(10) alien tea kettles: with a sleek design and a futuristic\nhandle.gleaming and metallic look.\n(11) alien tea bar: dark blue hue with a matte finish.no signs\nof wear.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n29\nFig. 31. Our output for \u201ca futuristic teahouse from the future in silicon valley\"\nFig. 32. Baseline output for \u201ca futuristic teahouse from the future in silicon valley\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n30\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 33. Our output for \u201ca saloon from an old western\"\nFig. 34. Baseline output for \u201ca saloon from an old western\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n31\n(12) alien tea kettle: with an alien-shaped handle.brand-new,\nsparkling with glitter.\n(13) alien tea pot: with a star-shaped lid and alien designs\non it.brand-new, shimmering with iridescent hues.\n(14) alien tea cups: featuring alien designs on them.brand-\nnew, shining with crystal clarity.\n(15) alien tea saucers: featuring alien designs on them.brand-\nnew, bright with vibrant colors.\n(16) alien tea bar stools: light blue hue with a glossy finish.no\nsigns of wear.\n(17) alien tea cups: with gold accents and a cartoon alien\nmotif.smooth, with no chips or scratches.\n(18) alien tea saucers: with gold accents and a cartoon alien\nmotif.smooth, with no chips or scratches.\n(19) alien tea kettle: with a spout and handle in the shape of\nan alien head.bright and shining, with no rust or dents.\n(20) alien tea lights: with alien faces on the front.unused and\nunwaxed, with bright green glow.\nC.13\nAntichrist in the bedroom of the pope (antichrist\nvatican)\u2013 Figure 36\nInput Scene Description : antichrist in the bedroom of the\npope Semantic shopping list\n(1) bed: crafted with dark, solid wood and royal blue velvet\ndrapes.solid, with slight signs of age.\n(2) pillows: black with golden embroidery.perfectly plump\nand inviting.\n(3) blanket: black with golden trim.soft and smooth to the\ntouch.\n(4) nightstand: dark wood with gold accents.polished and\ngleaming.\n(5) lamp: black iron with gold details.intricate and delicate.\n(6) bible: black leather with gold lettering.slightly worn on\nthe edges.\n(7) crucifix: black iron with gold details.detailed and gleam-\ning.\n(8) desk: crafted with mahogany wood, gold accents and\na green marble top.smooth, polished finish with subtle\ngold accents.\n(9) inkwell: made of iron, with a burnished finish.smooth\nsurface, no signs of corrosion.\n(10) quill: gold plated, with a feathery texture.sharpened to\na point, no visible fraying.\n(11) book: with intricate designs on the cover.frayed edges,\nsome pages yellowing with age.\n(12) papers: handmade and decorated with gold leaf.intricate\ndesigns still visible, slight discoloration on the edges.\n(13) lamp: made of brass, with a matte finish.strong and\nsturdy with no visible dents.\n(14) chair: upholstered in red velvet with slight signs of wear.\n(15) armrests: made of dark wood, with intricate carvings.smooth,\nwith no scratches or dents.\n(16) pillows: made of velvet with gold accents.soft and plush\nto the touch.\n(17) bible: with silver accents and a velvet cover.perfectly\npreserved with no signs of wear.\n(18) crucifix: unblemished and shining.\n(19) candles: with ornate holders.perfectly shaped and in\npristine condition.\n(20) bookshelf: crafted with mahogany wood, gold accents\nand a green marble top.smooth, polished finish with\nsubtle gold accents.\n(21) books: pages slightly yellowed with age.\n(22) cross: glint of gold in the details and worn edges.\n(23) bible: well-worn with traces of use.\n(24) candelabra: intricate details with slight signs of age.\n(25) candlesticks: ornate design.polished and free of tarnish.\n(26) candle: scented red roses.unburned, with the scent of\nred roses.\n(27) incense burner: dragon motif.smooth, with intricate\ndesigns.\n(28) incense sticks: unburned, with the scent of sandalwood.\n(29) bell: with inlaid designs.unrusted, with intricate inlay\ndesigns.\n(30) rug: intricate patterns and colors with slight fading.\n(31) cross: intricate detailing, polished finish.\n(32) crown of thorns: aged thorns and a tarnished crown.\n(33) bible: discoloration on the edges and cover.\n(34) chalice: intricate detailing, polished finish.\n(35) candle: wax burn marks and a tarnished candlestick.\n(36) incense burner: intricate detailing, polished finish.\n(37) crucifix: intricate detailing, polished finish.\n(38) rosary: intricate detailing, polished finish.\n(39) painting: vibrant colors and intricate details, slightly\naged.\n(40) frame: polished finish, but with some signs of age.\n(41) mirror: glossy finish, but with some signs of age.\n(42) bible: bound with gold leather.binding intact, with some\nsigns of age.\n(43) crucifix: made of black iron.black iron, with some signs\nof oxidation.\n(44) candles: with gold accents.smooth finish with signs of\nusage.\n(45) incense: in a gold incense holder.unscented, but with\nsigns of age.\n(46) chandelier: gold accents.crystal clear, with gold accents,\nslightly faded.\n(47) candles: slightly melted and slightly charred.\n(48) chain: slightly rusted and with a few dents.\n(49) bells: slightly tarnished and with a few dents.\n(50) crosses: slightly weathered with a few scratches.\nC.14\nHades\u2019s man cave (hades cave)\u2013 Figure 37\nInput Scene Description : hades\u2019s man cave Semantic shop-\nping list\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n32\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 35. Our output for \u201can alien tea garden on mars\"\nFig. 36. Our output for \u201cantichrist in the bedroom of the pope\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n33\n(1) throne: with a raised backrest, and skulls decorated on\nthe sides.smooth, polished metal.\n(2) red velvet throne cover: pristine condition.\n(3) skeleton: slightly aged but in pristine condition.\n(4) crown: slightly tarnished but still gleaming.\n(5) chain: slightly worn with a few scratches.\n(6) sword: slightly rusted but still sharp.\n(7) scepter: a few scratches and a bit of tarnish.\n(8) scroll: yellowed parchment scroll with ancient writ-\nings.slightly faded but still legible.\n(9) bat: black bat with glossy wings.slightly worn but still\nglossy.\n(10) pit of despair: flickering flames and smoky mist.\n(11) chains: blackened and rusted.heavy duty metal chains,\nheavily rusted with a few missing links.\n(12) skeletons: hand painted in a metallic black with red\nhighlights.skeleton props, intact with a few parts being\nchipped away due to age.\n(13) torches: antiqued metal torches, slightly worn with a\nfew patches of rust.\n(14) kraken statue: aged bronze with a glossy finish.\n(15) tentacles: made of soft rubber and flexible enough to be\nmanipulated into different shapes.flexible and smooth.\n(16) sea shells: curved sea shells, with natural colours and\nraised patterns.shiny and slightly transparent.\n(17) rocks: decorative rocks, with rough texture and dark\ngrey colour.rough and textured.\n(18) pearls: white pearls, with a glossy finish.smooth and\nglossy.\n(19) bookshelf: with intricate carvings.aged and weathered,\nwith intricate details still visible.\n(20) books: bound in dark leather with golden embossed\ndesigns.perfectly preserved and complete.\n(21) scrolls: embossed with ancient runes.slightly weathered\nwith age.\n(22) potions: made of dark glass with ornate cork stoppers.intact,\nwith no signs of tampering.\n(23) fire pit: with a black iron surround.rustic and sturdy,\nwith a protective black iron surround.\n(24) firewood: pre-split for convenience.dry and ready to\nburn.\n(25) fireplace tools: wrought iron with black finish.glossy\nblack finish with no visible rust or wear.\n(26) fireplace grate: black finish.no visible rust or wear.\n(27) fire poker: no visible rust or wear.\n(28) fireplace bellows: no visible wear or tear on the leather.\n(29) cauldron: with a black iron handle.smooth, heavy cast\niron with a black iron handle.\n(30) potions: containing a mysterious black liquid.perfectly\npreserved and sealed.\n(31) oils: fragrant and fresh.\n(32) bones: carefully arranged in the chest and slightly aged.\n(33) chair: with a velvet cushion.black iron frame, with a\nvelvet cushion slightly worn.\n(34) skulls: painted black.smooth and glossy finish.\n(35) goatskin rug: dyed black and red.soft and supple, with\nno signs of wear.\n(36) crossbones: painted black.no signs of wear.\n(37) torch: sculpted with skull and bones, brass finish.smooth\nand glossy, with no signs of wear.\n(38) anvil: with a skull etched into it.heavy and solid, with\na skull etched into it.\n(39) hammer: with an 8-inch handle and a 4-inch head.slightly\nworn handle and a clean head.\n(40) tongs: slight signs of rust on the flat jaw and a sturdy\nhandle.\n(41) iron rods: clean and free from dents.\n(42) iron ingots: no signs of rust.\n(43) forge: with a charcoal-fired box and adjustable air intake\nand exhaust.well-maintained, with no signs of damage.\nC.15\nA michelin star restaurant opened by a mad scientist\n(mad scientist restaurant)\u2013 Figure 38\nInput Scene Description : a michelin star restaurant opened\nby a mad scientist Semantic shopping list\n(1) tables: black steel with rivets.clean and glossy finish.\n(2) tablecloths: crisp and wrinkle free.\n(3) plates: smooth and glossy.\n(4) silverware: shiny and unscratched.\n(5) wine glasses: crisp and free of smudges.\n(6) petri dishes: clean and free of dust.\n(7) chairs: white plastic with black accents.unused and free\nof scratches.\n(8) table linen: crisp, clean and wrinkle free.\n(9) plates: spotless and shiny.\n(10) silverware: shining and polished.\n(11) test tubes: bright and clear.\n(12) test tube holders: with a copper finish.clean and without\ncorrosion.\n(13) counter: stainless steel with led lighting.unused and\nfree of scratches.\n(14) kitchenware: pans, baking trays, mixers, etc. with a\nsteampunk design.sparkling, stainless steel with en-\ngraved designs.\n(15) bottles with potion: glass apothecary bottles with cork\nstoppers.sealed with a tight cork stopper.\n(16) cookbooks: slightly worn, yellowed pages with colorful\nillustrations.\n(17) scientist\u2019s lab coat: slightly worn, with ragged edges\nand loose threads.\n(18) test tubes: clean and free of cracks.\n(19) test tube rack: 8-test tube capacity.clean and clear, with\nno signs of corrosion or rust.\n(20) flasks: 50ml capacity, glass and rubber stoppers.glass\ncontainers with clear markings, with no chips or cracks.\n(21) chemical containers: graduated cylinders.no leaks, no\nsigns of discoloration or damage.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n34\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 37. Our output for \u201cHade\u2019s man cave\"\n(22) dropper: 1ml capacity, with flexible bulb.no cracks or\nleaks, with a good seal.\n(23) beakers: clean and free of cracks.\n(24) chemical flasks: with graduated markingsclean and un-\ntouched.\n(25) beaker racks: powder coatedclean and in new condition.\n(26) safety goggles: adjustable strapsin perfect condition,\nfree of scratches.\n(27) lab coats: three-button front closurefreshly laundered\nand wrinkle-free.\n(28) pipettes: graduated markings, with rubber tipssterile,\nfree of dirt or dust.\n(29) bunsen burner: unused and in perfect working order.\n(30) flask: borosilicate glass.gleaming and sparkling, with\nno scratches.\n(31) feed tube: no kinks or bends.\n(32) gas valve: clean, no rust on the connections.\n(33) pressure gauge: no scratches, well calibrated.\n(34) ignition switch: free of dust and grime, working prop-\nerly.\n(35) burner pipe: no rust, no dents, free of soot.\n(36) electrodes: unused and free of rust.\n(37) wires: insulated, 20 awgno visible signs of damage.\n(38) voltmeter: lcd displayno visible signs of damage.\n(39) ammeter: lcd displayno visible signs of damage.\n(40) switch: red and blackno visible signs of damage.\n(41) magnifying glass: plastic handleno visible scratches and\nsmudges.\n(42) safety goggles: adjustable headbandno visible signs of\nwear and tear.\nC.16\nA North Korean classroom (north korean classroom)\n\u2013 Figure 39\nInput Scene Description : a north korean classroom Se-\nmantic shopping list\n(1) desks: made of wood with black paint finish.smooth,\ndust-free surfaces.\n(2) pencils: sharpened and ready for use.\n(3) books: new and in great condition.\n(4) erasers: new, unused.\n(5) notebooks: brand new and unused.\n(6) chairs: made of wood with black paint finish.slight signs\nof wear on the edges.\n(7) student uniform: navy blue with red accents.neatly\npressed and ironed.\n(8) pencils: black lead, white erasers on the top.newly sharp-\nened.\n(9) books: navy blue with gold accents.minimal signs of\nwear.\n(10) notebooks: navy blue with red accents.neatly stacked\nand organized.\n(11) teacher\u2019s desk and chair: some signs of wear on the\ncushioning of the chair.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n35\nFig. 38. Our output for \u201ca michelin star restaurant opened by a mad scientist\"\n(12) vase with flowers: with paper flowers.clean and well\ntaken care of.\n(13) globe: with north korea highlighted in gold.smooth and\nglossy finish.\n(14) school textbooks: philosophy and political science text-\nbooks.brand new and untouched.\n(15) blackboard: with a portrait of kim il sung in the cen-\nter.spotless and polished.\n(16) chalk: in white and red.freshly sharpened.\n(17) bookshelf: metal frame with wooden shelves.no signs\nof rust or wear.\n(18) books: north korea\u2019s official state doctrine, printed on\nthick off-white paper with blue binding.crisp pages and\nspines, in new condition.\n(19) artifacts: made of bronze and gold-plated.shining bronze\nand gold plating, highly polished.\n(20) posters: printed on thick cardstock with vibrant col-\nors.crisp and vibrant colors, free of creases.\n(21) flag: made of polyester and printed with bold colors.unfaded\ncolors, free of wrinkles.\nC.17\nHeadquarters of an occult cult (occult cult) \u2013 Figure\n40\nInput Scene Description : headquarters of an occult cult\nSemantic shopping list\n(1) altar: polished finish with no visible signs of wear.\n(2) ritual items: shiny and polished surface.\n(3) candelabra: black matte finish with five wax candles lit.\n(4) incense burner: polished silver surface with a hint of\nsmoky scent.\n(5) ritual objects: smooth, dark surface with intricate sym-\nbols etched on them.\n(6) candles: flawless appearance, no signs of burning.\n(7) incense: 1 lb.fresh and aromatic.\n(8) incense burner: polished, clean and free of tarnish.\n(9) bell: 8 inches in diameter, ornate design on the han-\ndle.no scratches or dents.\n(10) dagger: sharp blade, no nicks.\n(11) robe: made of velvet fabric.undamaged and without\nstains.\n(12) chairs: no signs of wear or tear.\n(13) candelabras: black iron with red accents.slight tarnish-\ning on the metal.\n(14) ritual tools: bronze and silver with intricate designs.slightly\nworn down edges.\n(15) mystic symbols: carved in onyx stone, painted with gold\naccents.crisp, sharp lines.\n(16) bookshelves: no visible signs of wear.\n(17) books: slightly worn and faded cover.\n(18) scrolls: slightly aged and fragile.\n(19) crucifix: intricate engravings, solid and well-crafted.\n(20) chalice: gleaming silver finish, ornate engravings.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n36\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 39. Our output for \u201ca north korean classroom\"\nFig. 40. Our output for \u201cheadquarters of an occult cult\"\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n37\nC.18\nA rustic backyard in the countryside (rustic\nbackyard) \u2013 Figure 41\nInput Scene Description : a rustic backyard in the coun-\ntryside Semantic shopping list\n(1) patio furniture: dark brown with white cushionsslightly\nfaded from sun exposure.\n(2) umbrella: beige fabric, wooden pole and ribs.no visible\nwear or tear.\n(3) pillows: weather-resistant material with a rustic pat-\ntern.no visible wear or tear.\n(4) chairs: wooden material and natural finish.slight signs\nof fading on the wood.\n(5) cooler: canvas material and rustic pattern.no visible\nwear or tear.\n(6) sunscreen: spf 50 protection.unopened and not expired.\n(7) picnic table: dark brownslightly weathered from out-\ndoor elements.\n(8) picnic blanket: cotton and red and black plaid.slight\nwear and tear but still intact.\n(9) plates: rustic style with floral pattern and earthy col-\nors.minor scratches and chips but still intact.\n(10) silverware: no damage or wear and tear.\n(11) firepit: well maintained and clean.\n(12) firewood: 16-18 inches in length.rough texture, split\ninto manageable pieces.\n(13) logs: 8-10 inches in diameter.bark peeled but still rough\nto the touch.\n(14) rocks: at least 12 inches in diameter.smooth, slightly\nweathered surfaces.\n(15) firestarter: eco-friendly material.lightweight, dry mate-\nrial.\n(16) matches: waterproof container.unused, undamaged boxes.\n(17) grill: stainless steel with black accentsslightly weath-\nered from outdoor elements.\n(18) charcoal: 10lbs.packed in a sturdy box.\n(19) lighter fluid: 64oz.sealed tightly in a plastic container.\n(20) grill utensils: stainless steel with wooden handles.rustic\nappearance with slight signs of wear.\n(21) grilling food: freshly packaged.\n(22) hammock: blue and white stripesslightly faded from\nsun exposure.\n(23) blanket: cream and sky blue, with fringed hem.soft and\nfluffy, slightly faded from the sun.\n(24) pillow: plump and full of cozy filling.\n(25) books: with worn covers.slightly bent from use, with\ncreased covers.\n(26) mug: in a natural ochre color.slightly worn from use,\nwith a few scratches on the surface.\n(27) lantern: made of copper and glass.rusty and aged from\nbeing in the elements, with a few chips in the glass.\n(28) trees: about 5-6 feet tallthick, full foliage.\n(29) birdhouses: white-washed finish.smooth, worn surface\nwith age-related discoloration.\n(30) birdfeeders: white-washed finish.smooth, worn surface\nwith age-related discoloration.\n(31) wildflowers: chosen for their color and texture.vibrant\ncolors and lush texture.\n(32) potted plants: chosen for their hardiness and beauty.natural\npatina and signs of weathering.\nC.19\nA church for strawberries (strawberry church) \u2013\nFigure 42\nInput Scene Description : a church for strawberries Se-\nmantic shopping list\n(1) altar: smooth, polished finish.\n(2) candles: ivory white in color.bright and evenly burning\nflames.\n(3) cross: slightly tarnished, but still gleaming with an an-\ntique feel.\n(4) bible: pages appear crisp and clean.\n(5) flowerpots: glazed with freshly painted designs.\n(6) incense burner: shiny and polished with no signs of\nscratches.\n(7) pews: slight signs of wear on the arms.\n(8) cushions: embroidered with delicate golden strawberry\npatternscrisp and clean, without any signs of wear or\nfading.\n(9) hymnals: with a strawberry-themed cover designbrand\nnew with no signs of wear or damage.\n(10) candles: scented with a sweet strawberry fragranceunlit,\nwith no signs of use.\n(11) pulpit: intricate carvings and polished marble top in\nperfect condition.\n(12) bible: burgundy leather binding, gold lettering.brand\nnew, no signs of wear.\n(13) microphone: wireless microphone with an adjustable\nstand.no signs of wear and fully functional.\n(14) flowers: fresh, vibrant petals.\n(15) choir books: brand new, no signs of wear.\n(16) candles: bright and vibrant, with no signs of smoke.\n(17) chandelier: all crystal drops intact, with no discoloura-\ntion.\n(18) candles: unburned and well-preserved.\n(19) banners: crisp and clean edges.\n(20) candelabras: bright and lustrous finish.\n(21) incense burner: smooth and polished finish.\n(22) statue: intricate detailing in perfect condition.\n(23) candles: 6 inches tall.no wax drips, burning softly.\n(24) flowers: long stem.fresh, with long stems and large\nblooms.\n(25) incense burner: with a strawberry design.shiny, with a\ndetailed strawberry design.\n(26) prayer beads: smooth to the touch, with a sturdy string.\n(27) bible: with a red cover and a white strawberry embossed\non the front.new, with crisp pages and a glossy cover.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n38\n\u2022\nIan Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas\nFig. 41. Our output for \u201ca rustic backyard in the countryside\"\n(28) stained glass windows: bright colours with no fading\nor discolouration.\n(29) candles: lightly scented, with red flower petals around\nthem.new, unscented, with red petals scattered around\nthem.\n(30) banners: with the words \u2019love is sweet\u2019 written in gold\nlettering.crisp, with no signs of wear.\n(31) flowers: with green foliage and red ribbons.fresh, with\nvibrant colors and no signs of wilting.\nC.20\nA busy street in downtown New York (busy New\nYork street)\u2013 Figure 43\nInput Scene Description : a busy street in downtown new\nyork Semantic shopping list\n(1) street lamps: white with gold accents.bright and shin-\ning.\n(2) street lamp post: black iron, with a height of 3 feet.unblemished\nand free from rust.\n(3) street lamp light: with 500 lumens and a white light.unblemished\nand free from dirt and dust.\n(4) street lamp wires: rated to withstand a voltage of 600\nv.free from fraying and breaks in the insulation.\n(5) street lamp bulbs: with a warm white light and a power\nconsumption of 7 watts.bright and free from flickering.\n(6) car: black with gold accents.in good condition, freshly\nwashed.\n(7) car windows: no visible scratches or cracks.\n(8) car doors: no dents or discoloration.\n(9) car tires: no visible punctures.\n(10) license plate: no obvious signs of wear or tear.\n(11) benches: steel, lined with cushioning for comfort.clean\nand dust-free.\n(12) trash can: minimal signs of wear from pedestrian traffic.\n(13) fire hydrant: freshly painted yellow finish, with no rust-\ning or damage.\n(14) street lights: bright and in good condition, without any\ndimming.\n(15) buildings: no visible signs of damage or deterioration.\n(16) street signs: reflective and weatherproof material.clear\nand legible.\n(17) street posts: obvious signs of rust, but otherwise in good\ncondition.\n(18) street signs: clear lettering with no signs of wear.\n(19) trash bins: minor dents and scratches, but otherwise in\ngood condition.\n(20) pigeons: hand-painted pigeons.brightly coloured, no\nsigns of wear or fading.\nWe consider this example a failure case, and was reflected by the\nCLIP-D/S score for this scene in our main paper. This shows that\nthis models still has many ways to go for outdoor scenes, a direction\nfor future work.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\nAladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions\n\u2022\n39\nFig. 42. Our output for \u201ca church for strawberries\"\nFig. 43. Our output for \u201ca busy street in downtown new york\". We consider this a failure case, and see the improvement of our system for busy urban outdoor\nscenes as exciting future work.\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: June 2023.\n"
  },
  {
    "title": "Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence",
    "link": "https://arxiv.org/pdf/2306.07075.pdf",
    "upvote": "8",
    "text": " \n \n \n \n \n*Correspondence: John J. Nay, johnjnay.com, john.j.nay@gmail.com. \nLarge Language Models as Tax Attorneys: \nA Case Study in Legal Capabilities Emergence\n \n \nJohn J. Nay, David Karamardian, Sarah B. Lawsky, Wenting Tao, Meghana Bhat,  \nRaghav Jain, Aaron Travis Lee, Jonathan H. Choi, Jungo Kasai \n \nStanford University \nUniversity of Michigan \nUniversity of Washington \nUniversity of Southern California \nNorthwestern Pritzker School of Law \nSimPPL \n \nAbstract \n \nBetter understanding of Large Language Models\u2019 (LLMs) legal analysis abilities can contribute to \nimproving the efficiency of legal services, governing artificial intelligence, and leveraging LLMs to \nidentify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose \nthis area of law because it has a structure that allows us to set up automated validation pipelines \nacross thousands of examples, requires logical reasoning and maths skills, and enables us to test \nLLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our \nexperiments demonstrate emerging legal understanding capabilities, with improved performance \nin each subsequent OpenAI model release. We experiment with retrieving and utilising the relevant \nlegal authority to assess the impact of providing additional legal context to LLMs. Few-shot \nprompting, presenting examples of question-answer pairs, is also found to significantly enhance \nthe performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly \nwhen combined with prompting enhancements and the correct legal texts, can perform at high \nlevels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability \nto reason about law autonomously could have significant implications for the legal profession and \nAI governance. \n \n \n \n2 \n \n \n \n \n \n \nIntroduction \n \nAI capabilities are marching forward.1 2 3 4 5 Large Language Models (LLMs)6 are the locus of the \nrapid advances. State-of-the-art LLMs can pass standardised tests7 and plan, reason, and leverage \ntools.8 LLMs, though, are essentially black boxes, even to their developers. We have little insight \ninto their inner workings and have no guarantees on how an LLM will behave on a new task.9 10 11 \nBest practice is to measure LLM performance on a litany of benchmarks before models are \ndeployed beyond the research environment, but these benchmarks are often not real-world tasks \nwe care about, or may have been memorised by the LLM during its training.12 This phenomenon \ntypically arises when the datasets used for training LLMs, often sourced from the internet, contain \nthe same data used for performance evaluation. The overlap can inflate the estimate of the model's \nperformance, giving an illusion of understanding that could instead be basic recognition.13  \n \nWe focus evaluation effort specifically on legal analysis capabilities of LLMs for three reasons. \n \nFirst, assessing the extent that LLMs grasp the law can contribute toward governing LLMs and \nautomated systems more generally.14 One policy-relevant approach seeks to leverage regulatory \nreasoning and legal reasoning within LLMs for \u201cLaw-Informed AI\u201d aligned with societal values as \ndetermined by democratic processes and law-making. This \u201cLaw Informs Code\u201d approach rests on \nthe established effectiveness of the democratic process in creating adaptive legal standards such \n \n1 Benaich N, Hogarth I. State of AI Report 2022. STATEOF.AI [Internet]. 2022 Oct 11. Available from: https://www.stateof.ai/2022-\nreport-launch.html \n2 Wei J. 137 Emergent Abilities of Large Language Models. JASONWEI.NET [Internet]. 2022 Nov 14. Available from: \nhttps://www.jasonwei.net/blog/emergence \n3 Webb T, et al. Emergent Analogical Reasoning in Large Language Models. ARXIV [Internet]. 2022 Dec 19. Available from: \nhttps://arxiv.org/abs/2212.09196 \n4  Hafner D, et al. Mastering Diverse Domains through World Models. ARXIV [Internet]. 2023 Jan 10. Available from: \nhttps://arxiv.org/abs/2301.04104 \n5 Institute for Human-Centered AI. STANFORD UNIV. [Internet]. [cited 2023 Mar 20]. Available from: https://hai.stanford.edu \n6  OpenAI. Better Language Models and Their Implications. OPENAI [Internet]. 2019 Feb 14. Available from: \nhttps://openai.com/research/better-language-models \n7 OpenAI. GPT-4. OPENAI [Internet]. 2023 Mar 14. Available from: https://openai.com/research/gpt-4 \n8 Schick T, et al. Toolformer: Language Models Can Teach Themselves to Use Tools. ARXIV [Internet]. 2023 Feb 9. Available from: \nhttps://arxiv.org/abs/2302.04761 \n9  World Economic Forum. The Promises and Perils of AI, with Prof Stuart Russell. WORLDECON. FORUM [Internet]. 2022 Jan 6. \nAvailable from: https://www.weforum.org/agenda/2022/01/artificial-intelligence-stuart-russell-radio-davos/ \n10  Zhang D, et al. The AI Index 2022 Annual Report. STANFORD INST. FOR HUMAN-CENTERED AI, STANFORD UNIV. [Internet]. [cited \n2023 Mar 20]. Available from: https://aiindex.stanford.edu/report/ \n11 The New AI-Powered Bing is Threatening Users. That's no Laughing Matter. TIME [Internet]. 2023 Feb 17. Available from: \ntime.com/6256529/bing-openai-chatgpt-danger-alignment/ \n12 Magar, Inbal, and Roy Schwartz. \"Data Contamination: From Memorization to Exploitation.\" arXiv preprint arXiv:2203.08242 (2022). \n13Chang, K. K., Cramer, M., Soni, S., & Bamman, D. (2023). Speak, memory: An archaeology of books known to chatgpt/gpt-4. arXiv \npreprint arXiv:2305.00118. \n14  Some examples in the US and UK include National Artificial Intelligence Initiative, About: National Artificial Intelligence Initiative, \nhttps://www.ai.gov/about/#naii-national-artificial-intelligence initiative. [Accessed 11 April 2023]; Advancing Trustworthy AI, National \nArtificial Intelligence Initiative, https://www.ai.gov/strategic pillars/advancing-trustworthy-ai. [Accessed 11 April 2023]; European \nCommission, Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial \nIntelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX%3A52021PC0206. [Accessed 11 April 2023] \n3 \n \n \n \n \n \n \n \nas fiduciary duties through iterative debate and litigation.15 The premise is that learning the spirit \nof the law can guide AI systems in making reasonable choices in novel scenarios. For instance, LLMs \nexhibit an early ability to predict when fiduciary duties are violated,16 and this capability could \npower safer AI deployments where an LLM-powered system serves a human principal. \n \nSecond, LLMs can be used as tools for humans to provide legal services more efficiently and \neffectively, whether that be self-service or through a professional attorney. If the models better \nunderstand law, they can be more reliable and ultimately more useful. LLMs can assist in tasks \nranging from contract analysis to case prediction, potentially democratising access to legal advice, \nreducing the cost and complexity for those who might otherwise struggle to navigate the legal \nsystem. Rigorous safeguards should be put in place as these models are deployed, given the \nsensitive nature of legal work. This includes increasing data privacy, minimising bias, maintaining \naccountability for the decisions made with the help of these models, and evaluating the suitability \nof the LLMs for any given use case. Hence, the need for systematic evaluations. \n \nThird, if LLMs understand the law well enough, they could be deployed by the government, citizens, \nand researchers to identify inconsistencies in existing laws.17 LLMs could increase the efficiency and \ntransparency of governments more broadly. For instance, LLMs can oftentimes provide clear, \nunderstandable explanations of complex laws and regulations. Eventually, LLMs may help predict \nlikely impacts of new laws or policies. By scanning vast amounts of legal text and associated \nimplementations, LLMs could flag potentially \u201coutdated\u201d law, or areas where the law is silent when, \nin other similar circumstances, the legislature or regulators provide guidance. \n \nIn this paper, we study retrieval-augmented generation of LLMs leveraging the text of the U.S. Code \nof Federal Regulations (CFR) and the U.S. Code (a compilation of federal statutes). We test the \nemerging capability of a suite of LLMs in understanding tax law.  \n \nWe chose tax law for four reasons. First, unlike some legal subjects where the doctrines are distilled \nfrom numerous precedents, the legal authority in tax law is principally concentrated in two sources: \nthe Treasury Regulations under the CFR and Title 26 of the U.S. Code (also called the Internal \nRevenue Code). This allows us to use a fixed universe of potentially relevant documents for the \nLLM\u2019s retrieval augmentation. Second, many tax rules allow for definitive answers to inquiries. This \nallows us to set up consistent and automated validation pipelines. Third, answering tax law \nquestions for a given scenario usually requires logical reasoning skills and even maths skills beyond \n \n15 Nay J. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. Northwestern J Tech & Intell \nProp. 2023. https://scholarlycommons.law.northwestern.edu/njtip/vol20/iss3/1/ \n16 Nay JJ. Large Language Models as Fiduciaries: A Case Study Toward Robustly Communicating with Artificial Intelligence through \nLegal Standards. SSRN [Internet]. 2023 Jan 30. Available from: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4335945 \n17 Ren\u00e9e Jean, \u201cWyoming Lawmakers Turn To Experts To Stay Ahead Of AI Curve, Even As ChatGPT4 Passes Bar Exam\u201d May 20, 2023 \nhttps://cowboystatedaily.com/2023/05/20/wyoming-lawmakers-turn-to-experts-to-stay-ahead-of-ai-curve-even-as-chatgpt4-passes-\nthe-bar-exam/https://cowboystatedaily.com/2023/05/20/wyoming-lawmakers-turn-to-experts-to-stay-ahead-of-ai-curve-even-as-\nchatgpt4-passes-the-bar-exam/  \n4 \n \n \n \n \n \n \njust reading the relevant legal authority, enabling us to test LLM capabilities in a manner relevant \nto real-world practice. Fourth, tax law is highly significant to the economic lives of nearly every \ncitizen and company on a regular basis. \n \nWe assess the accuracy of responses generated by LLMs on thousands of tax law inquiries across \nexperimental setups: the use of the LLM alone and the integration of the LLM with the underlying \nlegal texts, along with various retrieval techniques (with comparisons made across different \nretrieval methods). We conduct these experiments across LLMs, from smaller and weaker models, \nup through the largest state-of-the-art model, OpenAI\u2019s GPT-4. Each LLM we tested was state-of-\nthe-art when it was originally released. Through examining results across increasingly large models, \nwe find evidence for emerging legal understanding capabilities of LLMs, improving with each \nmodel release. This suggests that we may see the advent of superhuman AI legal skills as the state-\nof-the-art continues to rapidly advance. \n \nOur Approach to Validating LLM Legal Understanding  \n \nWe test LLMs\u2019 ability to \u201cunderstand\u201d law. Giving relevant and correct legal advice for a specific \nsituation is a task combining precise knowledge of legal sources as well as reasoning (and \nsometimes maths) capabilities to analyse situations. \n \nWe generate multiple-choice problems, each consisting of a question and a set of potential answers \n(only one of which is correct). The facts, names and numbers for each problem are randomly \ngenerated by Python code. As a result, our synthetic validation set consists solely of brand-new \nproblems that do not exist on the internet and could not have been included in the training set for \nany LLM.  This is an important distinction setting our validation apart from many other LLM \nbenchmarking exercises. In many cases, the LLM being evaluated \u2013 which is trained on much of the \ninternet \u2013 may have been trained on the validation data itself. \n \nWhile some of the questions involve only qualitative understanding (e.g., \u201cchecking-the-box\u201d \nquestions, see Appendix for more), others involve more arithmetic operations as well (e.g., \ncalculation to determine basis amount). The solution to a given problem can refer to either the CFR \nor the U.S. Code. To produce problems of a specific question type, we call our Python code to \ngenerate a bulk collection of multiple-choice problems. Each generated problem contains: (1) the \nlegal question with answer options for the LLM to choose from; (2) the correct option; (3) the \nexplanation for why that option is correct; and (4) the citation to the specific part of the law in \nwhich the answer to the question can be determined. \n \nWe generate two overarching multiple-choice exams for evaluation, one based on the CFR, and \none based on the U.S. Code. The CFR and U.S. Code exams are composed of three and four sections, \nrespectively, with each 100-question section pertaining to a specific tax law question type. See the \nAppendix for details on these seven types of questions across tax law categories. \n \n5 \n \n \n \n \n \n \n \nFor each question, we prompt an LLM to pick one of the multiple-choice answers, and we evaluate \nthe LLM\u2019s performance based on whether it chooses the correct answer. Since manually grading \nover 28,000 questions across all experiments by hand is not feasible, and since the models don\u2019t \nalways produce outputs in a consistent format that we can parse consistently and directly compare \nto the real answer, we utilise the most powerful available LLM, GPT-4, to compare the ground truth \nto the outputted answer. GPT-4 is instructed to grade the accuracy of a predicted answer choice \nby comparing it to the real answer choice for a given question.18 \n \nOur Approach to Retrieval-Augmented Generation and LLM Prompting \n \nWe compare results across retrieval methods, each with its own prompt template that provides \ndifferent supporting context to the LLM; see the Appendix for a full example of a prompt template \nfrom one of our experimental runs. When supplying supporting context to models with smaller \ncontext windows, we sometimes had to truncate the retrieved context to fit inside the window. The \nLLMs have the following context windows: davinci, 2049 tokens; text-davinci-002 and gpt-3.5-\nturbo, 4097 tokens; gpt-4, 8192 tokens. \n \nOur first experimental setting for retrieval, \u201cbypass_retrieval,\u201d creates a baseline for testing the \nimpact of retrieval and LLM knowledge. In these cases, we simply provide the LLM with a multiple-\nchoice question and the answer options with no additional explicitly provided legal context. This \nmethod assesses the ability of an LLM to answer a tax law question solely from its \u201cinternal \nknowledge base.\u201d \n \nFor the second retrieval experimental setting, \u201csimilarity_search,\u201d we inject potentially relevant legal \ntext into the prompt. Offline, before running the evaluations, we extracted the statutes from Title \n26 of the U.S. Code and regulations from the CFR Treasury Regulations in the form of discrete \ndocuments, each corresponding to a subsection from the legal source. The discrete subsection \ndocuments are roughly 130 tokens on average for our CFR vector database, and roughly 250 tokens \non average for our U.S. Code vector database. We leverage the open-source \u201cFacebook AI Similarity \nSearch\u201d library to create a vector database that maps the discrete subsections to 768-dimensional \nembeddings, computed by a state-of-the-art19 dense retrieval model, GTR-large.20 GTR-large is \ntrained on large amounts of retrieval data from various domains, including biomedical and science \n \n18 Before deploying this evaluation method at scale to grade all of our experiments, we assessed its efficacy on a subset of questions \nacross all the LLMs we were going to evaluate; our GPT-4 LLM evaluator perfectly graded nearly all of these questions, only erring less \nthan 1% of times when grading the most ambiguous and difficult-to-grade outputs from the oldest and least capable model we \nevaluated, OpenAI\u2019s \u201cdavinci\u201d release. For example, davinci can at times generate outputs with multiple conflicting answer choices \n(even when instructed not to), occasionally fooling our LLM grader that davinci correctly picked the one true answer. But this did not \noccur frequently enough to significantly impact the overall trends we report, which we determined by manually reviewing much of the \ndavinci grading. \n19 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. \nIn Advances in Neural Information Processing Systems. \n20 Ni, J., Qu, C., Lu, J., Dai, Z., \u00c1brego, G. H., Ma, J., Zhao, V. Y., Luan, Y., Hall, K. B., Chang, M.-W., Yang, Y. Large Dual Encoders Are \nGeneralizable Retrievers. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. \n6 \n \n \n \n \n \n \ntext, but not legal text; thus, our retrieval use-case is \u201cout-of-domain\u201d for the embeddings model.21 \nWhen a question is presented as input, our system retrieves the four most \u201crelevant\u201d documents \nfrom the vector store, where relevance is estimated based on the cosine similarity between the \ndocuments and the question. These documents are then injected as context into the prompt \ntogether with the original question, and the LLM is also instructed to return the metadata for which \nlegal source subsections it referenced in its answer.  \n \nThe third experimental setting, \u201cgold_truth,\u201d does not rely on a vector database or similarity search \nto provide the LLM with additional context; instead, we directly provide as context the correct \nsource material, obtained by referencing each given question\u2019s citation to the specific part of law \nit pertains to. Incorporating this method in the experimental design helps estimate the impact of \nthe theoretically best possible retrieval. Another way of looking at this design is that it isolates \nerrors in the LLM\u2019s reasoning caused by inaccurate retrieval in the \u201csimilarity search\u201d method.  \n \nFor the fourth retrieval method, \u201clecture_notes,\u201d we provide context to the LLM in the form of \nlecture notes (written by Sarah Lawsky, one of this paper\u2019s co-authors and a Professor of Law at \nNorthwestern) relevant to the given question type. \n \nAnother experimental variable was whether we employ \u201cchain-of-thought\u201d (CoT) prompting, which \nasks the LLM to think through its response step-by-step.  \n \nFinally, we experimented with few-shot prompting. This is where we provide a set of three other \nquestion-answer pair examples to the LLM, along with the question being asked. We varied the \npairs to match the question type for the given problem and ensured that the question-answer pairs \nwere not any of the questions used for evaluation. The notion behind few-shot prompting is to \nguide the LLM toward how to answer the given question by observing examples of how to answer \nquestions. We did for all LLMs without providing contextual source documents or lecture notes. \n \nThe LLMs \n \nA primary factor we vary in our experiments is the LLM itself. We run the experimental design across \nfour increasingly advanced LLMs released by OpenAI over the past three years. The weakest model \nwe employ, 'davinci,' is the \u201cmost capable GPT-3 model.\u201d 'text-davinci-002' is an earlier version of \nGPT-3.5 that is \u201ctrained with supervised fine-tuning instead of reinforcement learning.\u201d 'gpt-3.5-\nturbo' is the \u201cmost capable GPT-3.5 model.\u201d The most capable model, 'gpt-4,' is \u201cmore capable \nthan any GPT-3.5 model, able to do more complex tasks.\u201d22  \n \nFor all models across all experiments, we set temperature equal to zero when generating responses \nto our prompts. Temperature is a parameter that controls the \u201crandomness\u201d of the model\u2019s output. \nFor these LLMs, lower temperatures make the outputs more deterministic. \n \n21 We are developing \u201cin-domain\u201d embeddings currently for follow-up work. \n22 https://platform.openai.com/docs/models/ \n7 \n \n \n \n \n \n \n \n \nFinally, after running the experimental design across factors, we then run a final setting, \n\u201cmega_run,\u201d which combines the \u201cgold_truth\u201d retrieval method, few-shot prompting, and CoT \nprompting of GPT-4. In other words, the most powerful combination of techniques and the most \npowerful model. This allows us to assess the upper bound on performance with these techniques. \nWe discuss more advanced prompting in the Related Work and Next Steps sections that is likely to \nfurther boost performance. \n \nOur total sample across the experiments contains 28,700 answers. Figure 1 below visualises the \nprocess and the experimental factors (in red). \n \n \n \nFigure 1: Our experimental pipeline compares performance on multiple-choice tax law exams \nacross different LLMs, document retrieval techniques, and prompting techniques. \n \n8 \n \n \n \n \n \n \nResults \n \nThe first question we wanted to answer is whether CoT consistently improved the results for all (or \nmost) models and all (or most) methods of retrieval. The answer is no, as evidenced by the \ndifference between the solid and dashed lines in the charts of Figure 2. CoT does boost the \nperformance of GPT-4, though. This suggests an LLM might need to have a certain capability level \nto be able to exhibit improved performance through additional reasoning. Two responses from \nGPT-4 prompted with CoT provide a sense of what our evaluation data looks like (Examples 1 and \n2). \n \n \nExample 1:  \n \nQuestion: Aurora is hired by Grasshopper, Inc., on September 17, 2023. On her first day of work, \nwhen the stock trades at $18 per share, Aurora acquires 20 shares of Grasshopper stock for $15 \nper share, as contemplated by her employment agreement. The stock is a capital asset in Aurora's \nhands. As part of the terms of her employment, if she does not work for Grasshopper for 4 years, \nAurora must return the stock. Anyone to whom Aurora transfers the stock is subject to the same \nrestriction. Aurora makes no elections with respect to the stock. When the restriction expires and \nthe stock vests in 2027, it is worth $23. Aurora sells the shares in 2028, when the stock is worth $25 \nper share. How much may Grasshopper deduct per share in 2027 due to the vesting of the shares?  \n \n(1) $0 (2) $2 (3) $3 (4) $7 (5) $8 (6) $10 (7) $15 (8) $18 (9) $23 (10) $25 \n \nGPT-4 Response: According to Section 83(a), the amount to be included in the gross income of the \nperson who performed the services is the excess of the fair market value of the property at the time \nthe rights become transferable or not subject to a substantial risk of forfeiture, over the amount \npaid for the property. In this case, the fair market value of the stock when it vests in 2027 is $23 per \nshare, and Aurora paid $15 per share. The excess is $23 - $15 = $8 per share. Section 83(h) states \nthat the employer, Grasshopper, can deduct an amount equal to the amount included in the gross \nincome of the person who performed the services. In this case, that amount is $8 per share. So, \nGrasshopper may deduct (5) $8 per share in 2027 due to the vesting of the shares.  \n \nCorrect Answer: (5) $8 \n \n \nExample 2:  \n \nQuestion: Zahra owns property that is worth $41,000, with a basis of $33,000. Zahra dies and leaves \nthe property to Dylan. Several years later, Dylan sells the property for $37,000. How much gain or \nloss does Dylan recognize due to his sale of the property for $37,000?  \n \n(1) -$4,000 (2) -$3,000 (3) -$488 (4) $0 (5) $2,000 (6) $4,000 (7) $14,000  \n9 \n \n \n \n \n \n \n \n \nGPT-4 Response: Zahra dies and leaves the property to Dylan. According to the context provided, \nthe basis of the property in the hands of Dylan would be the fair market value of the property at \nthe date of Zahra's death, which is $41,000. Now, Dylan sells the property for $37,000. To calculate \nthe gain or loss, we subtract the basis from the selling price: $37,000 - $41,000 = -$4,000. So, the \nanswer is (1) -$4,000. \n \nCorrect Answer: (1) -$4,000  \n \n \nThe second experimental question is whether giving the LLM more legal text and more relevant (to \nthe specific question we are asking it) legal text increases accuracy for all (or most) models. The \nanswer is weakly yes, as evidenced by the upward slope of some of the lines in the charts of Figure \n2. As you move from the left to the right in these charts, the experimental setup is providing more \nand/or more relevant legal text to the LLM, starting with the \u201cbypass_retrieval\u201d setup, where we \ndon't give any source material to the LLM, to \u201cgold_truth,\u201d where we give the most relevant source \nmaterial to the LLM. \n \n \nFigure 2: The y-axis is the accuracy of that experimental setting averaged across the different \nquestion sub-types. Spanning from left to right within each of the two columns, we generally \n10 \n \n \n \n \n \n \nsee a slight improvement in overall accuracy as the LLM is provided with more relevant legal \nsource material with each subsequent retrieval method. \n \nThe third experimental question is whether few-shot prompting helps. The answer is a strong yes \nfor GPT-4 and seems to be less consistently useful for weaker models. In the \u201cfew_shot\u201d \nexperimental setting, we don't give any source material to the LLM, but we input into the prompt \nexamples of questions and answers from other questions that we are not testing it on, i.e., \n\u201cfew_shot\u201d is \u201cbypass_retrieval\u201d plus few-shot prompting. The \u201cmega_run\u201d experiment combines \n\u201cgold_truth\u201d sources with few-shot and CoT prompting. As evidenced by Figure 3, GPT-4 can \nleverage relevant legal text and examples of the question-and-answer task to \u201creason\u201d and come \nto a correct answer a large proportion of the time on difficult tax questions. \n \n \n \nFigure 3: The y-axis is the accuracy of that experimental setting averaged across the different \nquestion sub-types. The \u201cmega_run\u201d experimental setup for GPT-4, which combines few-shot \nand CoT prompting, along with providing \u201cgold truth\u201d legal sources, results in best overall \naccuracy for both the CFR and U.S. Code exams. CoT boosts GPT-4 performance in the retrieval \nexperimental settings of providing both no legal text (\u201cbypass retrieval\u201d and \u201cfew shot\u201d) and the \nmost relevant possible legal text (\u201cgold truth\u201d and \u201cmega run\u201d). \n \n11 \n \n \n \n \n \n \n \nThe primary experimental factor causing consistent increases in accuracy, when averaged across \nthe other factors, is which underlying LLM is being used. This is consistent across the CFR and U.S. \nCode focused questions; the same pattern holds of newer models outperforming older models \nholds, as shown in Figure 4. \n \n \n \nFigure 4: For both the CFR and U.S. Code exams, we see a clear increase in overall answer \naccuracy with each subsequently released OpenAI model. The most capable model, GPT-4, with \nboth prompting enhancements (CoT and few-shot) and the most relevant \u201cgold_truth\u201d legal \ntext input into the prompt, can perform extremely well, far better than any other setup in the \nexperiments (see \u201cmega_run\u201d in Figure 3). \n \nImplications \n \nOur work represents a step toward adapting LLMs to autonomously and reliably reason about law. \nWhile our experiments are limited to U.S. tax law, the capabilities the experiments investigate \u2013 \nfinding relevant legal authorities and applying them to specific factual scenarios \u2013 are at the heart \nof legal work and could be generalised to other areas of legal practice. The increasing performance \nof LLMs on these tasks will have profound implications for the practice of law and the governance \nof AI.  \n \nFirst, lawyers are highly trained professionals, and LLMs could disrupt the legal services industry to \nthe extent they are able to replicate much of the work of a skilled lawyer. We do not wish to \noverstate this possibility, since even our best current models underperform a professional tax \nlawyer, who would be expected to answer these questions with near-perfect accuracy. Moreover, \n12 \n \n \n \n \n \n \nanswering clear-cut legal questions is only a small part of the work of a practising lawyer. Clients \nrely on lawyers for contextual advice, ethical counsel, and nuanced judgement, which at present \nLLMs are not able to provide as consistently as most human lawyers. Nevertheless, there is no \nstrong reason to believe that LLMs could not eventually accomplish a wide range of legal tasks \nwith greater performance, and our work represents a benchmark to track the improvement of LLMs \nat legal reasoning. \n \nSecond, even if LLMs are not replacing trained lawyers, they can assist a lawyer or provide a first \ndraft of work a lawyer could subsequently check. This could significantly increase the productivity \nof practising lawyers and decrease the cost of legal services, potentially improving access to legal \ncounsel for many people who currently cannot afford it. In addition, LLMs could provide useful \nlegal information to consumers who are not engaging a traditional lawyer. As LLMs become more \ncapable of autonomously providing basic legal advice, policymakers might have to reconsider \nregulations on how legal advice is delivered, including regulations about the unauthorised practice \nof law. \n \nThird, governance is a key component of aligning AI with humans. Methods that improve LLM legal \nanalysis skills are relevant, either by helping AI models \u201cself-police\u201d to ensure they are acting in \naccordance with law, or by designing separate models that can apply legal and ethical standards \nto confirm whether another AI is properly aligned with the law. \n \nOur work also adds to the literature on emergent capabilities of LLMs by demonstrating the \nemergence of tax law understanding, which occurs once the LLM is of sufficient underlying general \ncapability and is adequately prompted to elicit \u201creasoning\u201d behaviour. Extrapolating these \ncapabilities forward, LLMs being able to \u201cunderstand\u201d law would affect law-making 23  and \nnecessitate changes to legal services regulation and emerging AI governance regimes. \n \nRelated Work \n \nLLM prompting involves designing text inputs to generate a response from an LLM. The goal of \nprompting is to steer the behaviour of the LLM in a way that elicits a desired outcome. Recent \nresearch has focused on developing effective prompting techniques that can expand LLMs\u2019 \ncapabilities when carrying out a variety of tasks. Examples include prompt patterns,24 in-context \ninstruction learning,25 evolutionary prompt engineering,26 and domain-specific keywords with a \ntrainable gated prompt to guide toward a target domain for general-domain LLMs.27 Zhong et al. \n \n23 Nay JJ. Large Language Models as Corporate Lobbyists. [eprint]. arXiv. 2023 Jan 01 Available from: https://arxiv.org/abs/2301.01181. \n24 White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, et al. A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT \n[Internet]. arXiv [cs.SE]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.11382 \n25  Ye S, Hwang H, Yang S, Yun H, Kim Y, Seo M. In-Context Instruction Learning [Internet]. arXiv [cs.CL]. 2023 [cited 2023 Mar 31]. \nAvailable from: https://arxiv.org/abs/2302.14691 \n26  Chen A, Dohan DM, So DR. EvoPrompting: Language Models for Code-Level Neural Architecture Search [Internet]. arXiv [cs.NE]. \n2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.14838 \n27 Goswami K, Lange L, Araki J, Adel H. SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource \nDomains [Internet]. arXiv [cs.CL]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.06868 \n13 \n \n \n \n \n \n \n \nexperiment with prompting LLMs to do scientific tasks across fields like business, science, and \nhealth by providing the LLM with a research goal and two large corpora, asking the LLM for corpus-\nlevel difference.28 Reppert et. al develop iterated decomposition, a human-in-the-loop workflow \nfor developing and refining compositional LLM programs that improves performance on real-world \nscience question and answer tasks.29 \n \nMore advanced techniques involve processes such as annotation, distillation, and model self-\nreflection. Diao et al. developed Active-prompt, which finds the most uncertain questions for the \nLLM and annotates those from the pool, achieving state-of-the-art on complex reasoning tasks.30 \nZhuo et al. develop methods for automatically designing multiple prompts and integrating \nautomatic verbalizers without sacrificing performance. 31  LLMs can also improve through \nintrospection. For example, Kim et al. develop a prompting scheme where an LLM agent recursively \ncriticises and improves its output (RCI), outperforming supervised learning and reinforcement \nlearning approaches on the MiniWoB++ benchmark, a web-based simulation task suite with tasks \nranging from simple clicking to complex maths problems.32 Press et al. investigate LLMs' ability to \nengage in compositional reasoning tasks, finding that as model size increases, single-hop question-\nanswering ability improves more rapidly than multi-hop question-answering ability, resulting in a \n\"compositionality gap.\" The authors propose \u201celicitive\u201d prompting methods, such as CoT and self-\ntalk, to mitigate this gap.33 Yao et al. developed the popular ReAct approach where LLMs generate \nreasoning and actions in an interleaved manner, outperforming state-of-the-art baselines at the \ntime across various tasks.34 Jin et al. develop \"Moral Chain-of-Thought\" (MORALCoT) prompting, \nwhich draws from cognitive science theories of moral reasoning and excels in a novel challenge set \ncentred on permissible rule-breaking.35 \n \nA growing body of research examines the characteristics of prompting. For instance, Lu et al. find \nthat the performance of LLMs is not associated with the perceived difficulty of prompts estimated \nby human annotators, and that employing definitions, demonstrations, and explanations can \nenhance performance. 36  Halawi et al. investigate model performance when confronted with \n \n28 Zhong R, Zhang P, Li S, Ahn J, Klein D, Steinhardt J. Goal Driven Discovery of Distributional Differences via Language Descriptions \n[Internet]. arXiv [cs.CL]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.14233 \n29 Reppert J, Rachbach B, George C, Stebbing L, Byun J, Appleton M, et al. Iterated Decomposition: Improving Science Q&A by \nSupervising Reasoning Processes [Internet]. arXiv [cs.CL]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2301.01751 \n30 Diao S, Wang P, Lin Y, Zhang T. Active Prompting with Chain-of-Thought for Large Language Models [Internet]. arXiv [cs.CL]. 2023 \n[cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.12246 \n31 Zhou Y, Maharjan S, Liu B. Scalable Prompt Generation for Semi-supervised Learning with Language Models [Internet]. arXiv [cs.CL]. \n2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.09236 \n32  Kim G, Baldi P, McAleer S. Language Models can Solve Computer Tasks. arXiv preprint arXiv:2303.17491. 2023 Mar 31 \n33 Press O, Zhang M, Min S, Schmidt L, Smith NA, Lewis M. Measuring and Narrowing the Compositionality Gap in Language Models. \narXiv preprint arXiv:2210.03350. 2022 Oct 7. \n34 Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv \npreprint arXiv:2210.03629. 2023 Oct 10 \n35  Jin Z, Levine S, Gonzalez F, Kamal O, Sap M, Sachan M, et al. When to Make Exceptions: Exploring Language Models as Accounts of \nHuman Moral Judgment [Internet]. arXiv [cs.CL]. 2022 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2210.01478 \n36 Lu A, Zhang H, Zhang Y, Wang X, Yang D. Bounding the Capabilities of Large Language Models in Open Text Generation with \nPrompt Constraints [Internet]. arXiv [cs.CL]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.09185 \n14 \n \n \n \n \n \n \nmisleading or false prompts and reveal that LLMs exhibit comparable performance, irrespective of \nfew-shot prompt accuracy, while accuracy discrepancies due to deceptive prompts only emerge in \nlater layers of the model.37 Focusing specifically on discrete prompts, Ishibashi et al. demonstrate \nthat although these prompts exhibit a degree of robustness against certain perturbations, they \nremain vulnerable to others and fail to generalise effectively across natural language inference \ndatasets. This underscores the necessity for further exploration into robust discrete prompting.38 \nFocusing on the role of prompting in boosting LLMs\u2019 \u201cTheory-of-Mind\u201d performance, Moghaddam \nand Honey show that in-context learning prompts boost Theory-of-Mind abilities in GPT-4 and \nGPT-3.5 models.39 \n \nPrompting serves as a crucial element in utilising LLMs for real-world applications such as legal \nservices and legal question-answering, as it connects model capabilities with targeted \nfunctionalities. In the context of our study, we examine LLMs' capacity to comprehend regulations \nand laws and experiment with the effects of very simple prompting techniques on accuracy. We \nleave the more advanced prompting discussed here for follow-up work in adapting these \ntechniques to the legal domain. \n \nAnother burgeoning part of the LLM literature is dedicated to the capacity of LLMs to function as \nagents that perform tasks, make decisions, and interact with their environment. Andreas et al. \ndemonstrate that LLMs can serve as agent models when only trained on bodies of documents, by \nimplicitly inferring fine-grained communicative intentions and using that for subsequent text \ngeneration.40 LLM-powered agents have demonstrated competence on some tasks that require \nreasoning, especially when combined with \u201ctools\u201d and symbolic systems. For instance, an AI system, \nCicero, achieved human-level performance in the strategy game Diplomacy by integrating an LLM \nwith strategic reasoning.41 Furthermore, Shinn et al. explore LLM agents' ability for learning from \nmistakes with Reflexion, an approach that equips LLM-based agents with dynamic memory, \u201cself-\nreflection\u201d capabilities, and a method for detecting hallucinations.42  \n \nRegarding agentic LLMs more generally, Yang et al. investigate connections between LLMs and \nexternal entities, and their decision-making, using methods such as prompting, conditional \ngenerative modelling, planning, optimal control, and reinforcement learning.43 A primary focus of \n \n37 Halawi D, Denain JS, Steinhardt J. Overthinking the Truth: Understanding how Language Models process False Demonstrations \n[Internet]. 2023 [cited 2023 Mar 31]. Available from: https://openreview.net/forum?id=em4xg1Gvxa \n38 Ishibashi Y, Bollegala D, Sudoh K, Nakamura S. Evaluating the Robustness of Discrete Prompts [Internet]. arXiv [cs.CL]. 2023 [cited \n2023 Mar 31]. Available from: https://arxiv.org/abs/2302.05619 \n39 Moghaddam SR, Honey CJ. Boosting Theory-of-Mind Performance in Large Language Models via Prompting. arXiv preprint \narXiv:2304.11490. 2023. \n40 Andreas J. Language Models as Agent Models. arXiv preprint arXiv:2212.01681. 2022 \n41 FAIR, Bakhtin A, Brown N, Dinan E, Farina G, Flaherty C, Fried D, Goff A, Gray J, Hu H, Jacob AP, Komeili M, Konath K, Kwon M, Lerer \nA, Lewis M, Miller AH, Mitts S, Renduchintala A, Roller S, Rowe D, Shi W, Spisak J, Wei A, Wu D, Zhang H, Zijlstra M. Human-level play \nin the game of Diplomacy by combining language models with strategic reasoning. Science. 2022 Nov 25;378(6624):1067-1074. \nAvailable from: https://www.science.org/doi/10.1126/science.ade9097. doi: 10.1126/science.ade9097. PMID: 34976589. \n42 Shinn N, Labash B, Gopinath A. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint \narXiv:2303.11366. 2023 Mar 28 \n43 Yang S, Nachum O, Du Y, Wei J, Abbeel P, Schuurmans D. Foundation Models for Decision Making: Problems, Methods, and \nOpportunities [Internet]. arXiv [cs.AI]. 2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2303.04129 \n15 \n \n \n \n \n \n \n \nautonomous agents lies in the interface between the LLM as an agent and the environment with \nwhich it interacts. Li et al. employ the \"Internet Explorer\" approach, which enables LLMs to \ndynamically use the internet as a continuously updating, open-ended dataset. In this approach, \nsmaller models explore the web through self-supervision, locating relevant data to quickly enhance \ntask performance.44 Carta et al. examine a method to improve the alignment between the LLM's \nknowledge and its environment, while augmenting functional competence; the LLM is grounded in \nan interactive text world with online reinforcement learning, incrementally updating its knowledge \nbased on observations.45 Agents need to plan, and there is substantial interest in LLMs' ability to \nact as planners. Valmeekam et al. investigate the planning capabilities of LLMs, which exhibit poor \nperformance in fully autonomous mode during common-sense planning tasks. However, when \n\"heuristic guidance\" and \"human-in-the-loop\" modes are employed, performance improves, albeit \nmarginally.46 As an example of a direction toward autonomous planning, Wang et al. developed a \n\"Describe, Explain, Plan, and Select\" approach, which explores the use of LLMs as planning agents \nin open-ended planning scenarios with long-term, multi-step tasks. This approach significantly \nimproved performance in over 70 Minecraft tasks.47 Other research examines LLMs as a component \nin building AI agents. For example, Li et al. explore the use of LLMs as probabilistic priors for \ngeneralised decision-making, applicable to non-linguistic perception and control, as well as tasks \nsuch as semantic segmentation, household navigation, and activity recognition.48 The explosion of \nresearch interest at the intersection of autonomous agents and LLMs is relevant to our work, since \nagents that better understand the law are more likely to be aligned with society. By benchmarking \nlegal understanding of LLMs, we can contribute to assessing the safety of agentic LLM \ndeployments. \n \nAs LLMs demonstrate significant potential in tackling diverse tasks, research has focused on \nmethods of evaluating their performance. Increasingly specific benchmarks are being developed. \nExamples include G-Eval, a framework using LLMs to evaluate natural language generation output \nvia a CoT paradigm,49 and AmbiEnt, where even advanced models like GPT-4 struggle with correctly \ndisentangling ambiguous meanings.50  \n \n \n44 Li AC, Brown E, Efros AA, Pathak D. Internet Explorer: Targeted Representation Learning on the Open Web [Internet]. arXiv [cs.LG]. \n2023 [cited 2023 Mar 31]. Available from: https://arxiv.org/abs/2302.14051 \n45 Carta T, Romac C, Wolf T, Lamprier S, Sigaud O, Oudeyer PY. Grounding Large Language Models in Interactive Environments with \nOnline Reinforcement Learning. arXiv preprint arXiv:2302.02662. 2023 Feb \n46 Valmeekam K, Sreedharan S, Marquez M, Olmo A, Kambhampati S. On the Planning Abilities of Large Language Models (A Critical \nInvestigation with a Proposed Benchmark) [Internet]. arXiv [cs.AI]. 2023 [cited 2023 Mar 31]. Available from: \nhttps://arxiv.org/abs/2302.06706 \n47 Wang Z, Cai S, Liu A, Ma X, Liang Y. Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables \nOpen-World Multi-Task Agents. arXiv preprint arXiv:2302.01560. 2023 Feb 3 \n48 Li BZ, Chen W, Sharma P, Andreas J. LaMPP: Language Models as Probabilistic Priors for Perception and Action. arXiv preprint \narXiv:2302.02801. 2023 Feb 7. \n49 Liu Y, Iter D, Xu Y, Wang S, Xu R, Zhu C. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv preprint \narXiv:2303.16634. 2023 \n50 Liu A, Wu Z, Michael J, Suhr A, West P, Koller A, Swayamdipta S, Smith NA, Choi Y. We're Afraid Language Models Aren't Modeling \nAmbiguity. arXiv preprint arXiv:2304.14399. 2023 \n16 \n \n \n \n \n \n \nProviding LLMs with domain-specific knowledge, updated data, and specialised reasoning and \ncomputation abilities can improve their performance on some tasks. Mialon et al. review the current \nadvancements in augmentation, where LLMs are enhanced through reasoning capabilities, external \nmodules, and tools. The authors argue that augmentation could potentially ameliorate \ninterpretability, consistency, and scalability issues in LLMs. 51 Researchers have devised several \nmethods for LLMs to employ external resources. For instance, Peng et al. introduce a system that \nemploys plug-and-play external modules to refine grounded responses using external knowledge \nand iterative revision based on utility function feedback, substantially reducing LLM \nhallucinations.52 Zhou et al. develop Doc-Prompting, a natural-language-to-code technique that \nutilises library documentation retrieval for code generation.53 External documentation can also \nfacilitate LLM self-assessment: Wu et al. establish a \"Read and Reward\" framework to enable an \nLLM to self-evaluate through manual learning. This framework employs a Question and Answer \n(QA) extraction module that condenses manual information and a reasoning module to assess \ninteractions based on this information.54  \n \nQA has served as the testing ground for most of the LLM augmentation research thus far. Chen et \nal. review open-domain QA research.55 Sil et al. introduced PRIMEQA, an open-source repository \nto democratise cutting-edge QA methodologies. This end-to-end QA toolkit allows for custom app \ncreation with trainable retrievers and readers for deployment.56  Sun et al. propose recitation-\naugmented language models, enabling LLMs to retrieve pertinent information from their own \nmemory through sampling to answer questions.57 Khattab et al. present Demonstrate-Search-\nPredict (DSP) for retrieval-augmented in-context learning that decomposes problems into more \nmanageable components for both the language and retrieval models. 58  Ye et al. develop \nCompositional Exemplars for In-context Learning to assist in selecting the most diverse yet useful \nexamples for LLMs to learn from for in-context learning.59 Ram et al. present a simpler alternative \nto Retrieval-Augmented Language Modelling (RALM): in-context RALM, where grounding \ndocuments are incorporated into the LLM's input without modifying its architecture.60 In our paper, \n \n51 Mialon G, Dess\u00ec R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R, Rozi\u00e8re B, Schick T, Dwivedi-Yu J, Celikyilmaz A, Grave E, LeCun \nY, Scialom T. Augmented Language Models: a Survey. arXiv. 2023 Feb 21. arXiv:2302.07842 \n52  Peng B, Galley M, He P, Cheng H, Xie Y, Hu Y, et al. Check Your Facts and Try Again: Improving Large Language Models with \nExternal Knowledge and Automated Feedback. arXiv preprint. 2023 Feb 28:2302.12813 \n53 Zhou S, Alon U, Xu FF, Wang Z, Jiang Z, Neubig G. DocPrompting: Generating Code by Retrieving the Docs. arXiv preprint \narXiv:2207.05987. 2023 \n54 Wu Y, Fan Y, Liang PP, Azaria A, Li Y, Mitchell TM. Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction \nManuals. arXiv preprint arXiv:2302.04449. 2023 Feb 1 \n55 Chen D, Yih W. Open-Domain Question Answering. In: Proceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics: Tutorial Abstracts. Online: Association for Computational Linguistics; 2020. p. 34-37. Available from: \nhttps://aclanthology.org/2020.acl-tutorials.8. doi: 10.18653/v1/2020.acl-tutorials.8 \n56 Sil A, Sen J, Iyer B, Franz M, Fadnis K, Bornea M, et al. PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question \nAnswering Research and Development. arXiv preprint arXiv:2301.09715. 2023 Jan 26 \n57 Sun Z, Wang X, Tay Y, Yang Y, Zhou D. Recitation-Augmented Language Models. arXiv preprint arXiv:2210.01296. 2023 \n58 Khattab O, Santhanam K, Li XL, Hall D, Liang P, Potts C, Zaharia M. Demonstrate-Search-Predict: Composing retrieval and language \nmodels for knowledge-intensive NLP. arXiv preprint arXiv:2212.14024. 2023 \n59 Ye J, Wu Z, Feng J, Yu T, Kong L. Compositional Exemplars for In-context Learning. arXiv preprint arXiv:2302.05698. 2023 Feb 15 \n60 Ram O, Levine Y, Dalmedigos I, Muhlgay D, Shashua A, Leyton-Brown K, Shoham Y. In-Context Retrieval-Augmented Language \nModels. arXiv preprint arXiv:2302.00083. 2023 Feb \n17 \n \n \n \n \n \n \n \nwe focus on simple forms of augmentation, and leave testing these more sophisticated methods \nfor future work. \n \nThere are many tasks that larger LLMs can complete that smaller models cannot.61 Larger models \nhave more inherent resources (for example, GPT-2 has 1.5 billion parameters while GPT-3 has 175 \nbillion), and for some tasks that require various complex types of reasoning, LLMs\u2019 capability to do \nsuch tasks \u201cemerges\u201d in a nonlinear fashion after reaching a certain model size. Jason Wei has \ncompiled a list of 137 emergent abilities of LLMs that have been uncovered by research, which \nincludes things like \u201ccausal judgement\u201d and \u201cgeometric shapes.\u201d62 Our experiments suggest that \nlegal understanding could be one such emergent ability. \n \nNext Steps \n \nWith clear evidence showing increases in capabilities from older to newer LLMs, attention can be \nshifted towards validating and improving the abilities of the newest, most powerful models \navailable. \n \nRegarding prompting, further analysis of our results could investigate the relationship between \nprompt length and accuracy. One possibility is that the LLMs do not perform as well as they could \nbecause their performance degrades as the length of the input increases; just because newer \nmodels like GPT-4 have a wider context window may not necessarily mean filling it to the max is \noptimal.  \n \nMany of the more advanced prompting techniques discussed in the Related Work section are prime \ncandidates for increasing performance; in particular, the self-reflection and self-refinement \ntechniques. For example, the LLM can be prompted with its own answers, and the relevant context, \nand asked, \u201cAre there any ambiguities in this question that make it difficult to answer or for you to \ndoubt your current answer? If so, conduct additional legal research by generating a topic that we \nneed to search legal sources for.\u201d The response can then be used to conduct further retrieval \naugmented generation. \n \nRegarding document retrieval, we seek to close the gap between the \u201csimilarity search\u201d and \u201cgold \ntruth\u201d retrieval methods through better retrieval. Especially for GPT-4, we saw a clear performance \nboost when feeding in the \u201cgold truth\u201d legal documents, rather than performing similarity search \nto extract the relevant documents from a vector database. This result indicates that our similarity \nsearch technique, and the various hyperparameter defaults we used, did not provide the most \nrelevant \u201cgold truth\u201d sources into the LLM a significant portion of the time. Ultimately, as LLMs are \n \n61 Ornes S. The Unpredictable Abilities Emerging From Large AI Models [Internet]. Quanta Magazine. 2023 Mar 16 - [cited 2023 May \n21]. Available from: https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316 \n62 Wei J. 137 Emergent Abilities of Large Language Models [Internet]. Jason Wei Blog. [Date unknown] - [cited 2023 May 21]. Available \nfrom: https://www.jasonwei.net/blog/emergence \n18 \n \n \n \n \n \n \ndeployed in real-world settings where humans won\u2019t be providing the exact legal documents \nnecessary, the ability to retrieve the relevant documents will be important. We need to experiment \nwith factors such as the choice of model embeddings, retrieval technique, and the token length of \nvector database subsections and number of subsections retrieved and placed into the prompt. \n \nFinally, future work could compare performance between generally pre-trained LLMs, such as the \nOpenAI models in our experiments, and language models specifically pre-trained and fine-tuned \nfor legal reasoning. Developing best practices for fine-tuning models for legal reasoning tasks is \nan important step towards sufficiently boosting AI legal capabilities in real-world settings. \n \nAcknowledgments \n \nThe Mercatus Center at George Mason University funded Meghana Bhat\u2019s work related to this \nresearch, and some of the computing costs for running the experiments. Pierre-Loic Doulcet, of \nStanford University\u2019s CodeX Center for Legal Informatics, contributed to the U.S. Code vector \nembeddings work. \n \nAppendix \n \nCFR Exam Question Types: \n1. \nThe first CFR question type, \u201cCheck the Box,\u201d involves entity status. The issue is whether a \ngiven business entity is eligible to select its tax status (e.g., partnership or disregarded entity), or \nwhether it is required to be classified as a corporation. If it is eligible to select its tax status, the \nissue becomes what its default status would be and what status it would be able to elect into. \nSelecting a tax status is known colloquially as \u201cchecking the box.\u201d  \n2. \nThe second CFR question type, \u201cRestricted Property as Compensation/Employee,\u201d involves \ntransfers of property as compensation. These problems ask how much of a deduction an employee \nis permitted to take when the employee receives property as compensation. The terms of the \ncompensation require the employee to forfeit the property if certain conditions are not met.  \n3. \nThe final CFR question type, \u201cPart Gift/Part Sale,\u201d involves calculations related to the transfer \nof property that is in part a gift and in part a sale. These questions require determining how much \ngain a donor/seller recognizes on a transfer that is part gift/part sale, or the recipient\u2019s basis in the \nproperty that is received in a part gift/part sale transfer.  \n \nU.S. Code Exam Question Types: \n1. \nThe first U.S. Code question type, \u201cRestricted Property as Compensation/Employer,\u201d is like \nthe aforementioned CFR question type, \u201cRestricted Property as Compensation/Employee,\u201d but \ninstead pertains to how much of a deduction the employer is permitted to take due to transfer of \nrestricted property as compensation. \n2. \nThe second U.S. Code question type, \u201cUnrestricted Property as Compensation,\u201d asks about \nthe treatment of employees who receive unrestricted property as compensation, in particular the \n19 \n \n \n \n \n \n \n \namount of gain that these employees have when they sell the property they have received as \ncompensation. \n3. \nThe third U.S. Code question type is composed of three tax law areas: \u201cBasis of Property \nTransferred as Gift,\u201d \u201cBasis of Property Transferred at Death,\u201d and \"Basis from Part Donation/Part \nSale.\u201d These questions require determining the basis of property transferred as a gift, the basis of \nproperty that was acquired from a decedent, and how much gain a donor/seller recognizes on a \ntransfer to a tax-exempt entity that is part donation/part sale. \n4. \nThe final U.S. Code question type, \u201cQualified Business Income Deduction,\u201d requires \ndetermining the amount of deduction available under the qualified business income deduction \nprovision, Section 199A. \n \n \n \n20 \n \n \n \n \n \n \n \n \nGPT-4 \u201cmega_run\u201d Prompt Template: \n  \nSystem: You are an AI assistant for answering questions about Tax Law. \nUse the given pieces of context and multiple choice question-answer examples to answer the user's \nquestion. \nIf you don't know the answer, just say \"I don't know the answer.\" Don't make up an answer. \n \nHuman: Use the provided context extracted from relevant documents and the following three multiple \nchoice question-answer examples about Tax Laws to help you answer the fourth question at the end. \n \nIf you don't know the answer, just say \"I don't know the answer.\" Don't make up an answer. \n---------------- \nCONTEXT: \n{insert \u201cgold_truth\u201d sources here} \n---------------- \nQUESTION: De Dromedaris, vof, is organized in Netherlands as a Vennootschap onder firma. De \nDromedaris has more than one member. At least one member of the entity has unlimited liability. Is De \nDromedaris eligible to check the box? If so, what type of entity will it be if there is an election under the \ncheck-the-box rules (that is, if the entity does not take default status under the check-the-box rules)? (1) \nNo, because a Vennootschap onder firma organized in Netherlands is a per se corporation. (2) Yes, and its \nelective status is a disregarded entity. (3) Yes, and its elective status is a partnership. (4) Yes, and its elective \nstatus is a corporation. \nANSWER: (4) Yes, and its elective status is a corporation. \n \nQUESTION: Turkey, inc., is organized in Rhode Island as a corporation. Turkey has more than one member. \nAll members of Turkey have limited liability. Is Turkey eligible to check the box? If so, what is its default \nstatus if there is no election under the check-the-box rules? (1) No, because a corporation organized in \nRhode Island is a per se corporation. (2) Yes, and its default status is a disregarded entity. (3) Yes, and its \ndefault status is a partnership. (4) Yes, and its default status is a corporation. \nANSWER: (1) No, because a corporation organized in Rhode Island is a per se corporation. \n \nQUESTION: El Pollo, sa, is organized in Colombia as a Sociedad Anonima. El Pollo has more than one \nmember. All members of El Pollo have limited liability. Is El Pollo eligible to check the box? If so, what is its \ndefault status if there is no election under the check-the-box rules? (1) No, because a Sociedad Anonima \norganized in Colombia is a per se corporation. (2) Yes, and its default status is a disregarded entity. (3) Yes, \nand its default status is a partnership. (4) Yes, and its default status is a corporation. \nANSWER: (1) No, because a Sociedad Anonima organized in Colombia is a per se corporation. \n---------------- \nQUESTION: {insert actual question here} \nANSWER: \nLet's think step by step. \n \n"
  },
  {
    "title": "Retrieval-Enhanced Contrastive Vision-Text Models",
    "link": "https://arxiv.org/pdf/2306.07196.pdf",
    "upvote": "7",
    "text": "Published as a conference paper at ICLR 2024\nRETRIEVAL-ENHANCED CONTRASTIVE VISION-TEXT\nMODELS\nAhmet Iscen\nMathilde Caron\nAlireza Fathi\nCordelia Schmid\nGoogle Research\nABSTRACT\nContrastive image-text models such as CLIP form the building blocks of many\nstate-of-the-art systems. While they excel at recognizing common generic concepts,\nthey still struggle on fine-grained entities which are rare, or even absent from the\npre-training dataset. Hence, a key ingredient to their success has been the use\nof large-scale curated pre-training data aiming at expanding the set of concepts\nthat they can memorize during the pre-training stage. In this work, we explore an\nalternative to encoding fine-grained knowledge directly into the model\u2019s parameters:\nwe instead train the model to retrieve this knowledge from an external memory.\nSpecifically, we propose to equip existing vision-text models with the ability to\nrefine their embedding with cross-modal retrieved information from a memory at\ninference time, which greatly improves their zero-shot predictions. Remarkably,\nwe show that this can be done with a light-weight, single-layer, fusion transformer\non top of a frozen CLIP. Our experiments validate that our retrieval-enhanced\ncontrastive (RECO) training improves CLIP performance substantially on several\nchallenging fine-grained tasks: for example +10.9 on Stanford Cars, +10.2 on\nCUB-2011 and +7.3 on the recent OVEN benchmark, where we even outperform\nthe fine-tuned models on unseen classes.\n1\nINTRODUCTION\nIn the recent years, we have witnessed a surge in the development of vision-language models highly\nadaptable to a broad spectrum of downstream tasks (Jia et al., 2021; Radford et al., 2021; Yu et al.,\n2022; Chen et al., 2023; Singh et al., 2022). These models work by pre-training two parallel encoders\nusing contrastive learning (van den Oord et al., 2018) on large-scale, carefully curated, image-text\ndata (Radford et al., 2021). These two-tower models learn to encode images and texts into an aligned\nlatent space which enables appealing capabilities such as zero-shot transfer to different downstream\napplications, e.g. image classification (Radford et al., 2021), image-text retrieval (Plummer et al.,\n2015) or open-world recognition (Minderer et al., 2022; Liang et al., 2022). Although these models\nhave achieved state-of-the-art results across various generic vision-language benchmarks, we observe\nthat they tend to struggle on tasks requiring a more fine-grained understanding of visual or textual\nentities. Our hypothesis is that this disparity stems from the fact that it is hard to align the image\nand text modalities. While every image is metaphorically valued at a thousand words, it is often\npaired with a short, sometimes noisy, text that neither exclusively nor comprehensively describes it.\nFor example, current vision-language models are good at associating images of cars with generic\nconcepts such as \u201ccar\u201d, \u201cmechanics\u201d or \u201croad trip\u201d, because these are common words paired with car\nimages, but less at finegrained, instance-level, associations such as the specific brand, series or year\nof that car. This might therefore produce poor accuracy for zero-shot fine-grained car classification.\nThe current path taken by the research community has been to ever scale and curate the pre-training\ndataset in the hope of covering more and more image-text associations (Radford et al., 2021; Schuh-\nmann et al., 2021; Alayrac et al., 2022; Chen et al., 2023). An orthogonal effort has focused instead on\nmemory or knowledge-based approaches (Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard\net al., 2022; Guu et al., 2020b; Liu et al., 2023; Shen et al., 2022). These methods, instead of statically\ningesting and memorizing all the world knowledge into model parameters, propose to rely on the\naccess to an external source of knowledge. For example, K-Lite (Shen et al., 2022) explores how to\nimprove vision-text models by enhancing the text captions with more comprehensive text definitions\n1\narXiv:2306.07196v2  [cs.CV]  21 Feb 2024\nPublished as a conference paper at ICLR 2024\nretrieved from an external dictionary, i.e. WordNet (Meyer & Gurevych, 2012) or Wiktionary (Miller,\n1998). One caveat that we identify in this approach is that initial captions are augmented within their\nmodality only, hence limiting the potential added-value brought by the retrieved items.\nTo mitigate this issue, we put forth a retrieval-augmented approach that enhances the alignment\nbetween image and text representation. A critical observation of ours is that matching representations\nwithin the same modality is a significantly simpler task than matching representations across different\nmodalities. To clarify, we observe that the image representation can be effectively utilized to identify\nimages closely resembling the query image, or the text representation can be used to identify texts\nclosely resembling the query text. However, when crossing modalities, these representations are less\nsuccessful in identifying suitable matches, such as finding the text with the closest representation to a\nquery image representation. We utilize the inherent strength of learned image and text representations\nwithin their respective modalities to aid the alignment across modalities. To improve their compatibil-\nity, we convert these unimodal representations into a multi-modal format, as conceptually illustrated\nin Fig. 1. Utilizing a web-scale corpus of image-text pairs for retrieval, we use image representation\nas a query to identify the top-k most similar images and incorporate the associated text to create a\nmulti-modal representation. In a parallel manner, given a text representation as a query, we find the\ntop-k most similar texts and integrate the associated images to create a multi-modal representation.\nThrough this process, we successfully transform the image and text representations into multi-modal\nversions, which significantly simplifies their alignment. Our approach does not presuppose any\ndownstream knowledge and produces a single generic model that can be used effectively across\ndifferent tasks. We show that our method improves over original CLIP (Radford et al., 2021) or\nLiT (Zhai et al., 2022) models on 11 challenging fine-grained downstream tasks.\n2\nRELATED WORK\nVision-text pre-training. While early works have shown the promise of representation learning from\nimage-text paired data (Zhang et al., 2022; Gomez et al., 2017; Joulin et al., 2016; Desai & Johnson,\n2021), recent popular papers such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have\ntruly unleashed the potential of contrastive image-text pre-training. This paradigm simply works with\ntwo parallel uni-modal encoders that learn to distinguish between aligned and non-aligned image-text\npairs through a cross-modal contrastive objective (van den Oord et al., 2018; Miech et al., 2020).\nAppealing properties of these models are simplicity, scalability and great zero-shot performance (Xian\net al., 2018). As a result, vision-text contrastive models now form the basic building blocks of more\npowerful foundational models, such as CoCa (Yu et al., 2022), Flamingo (Alayrac et al., 2022),\nFLAVA (Singh et al., 2022), and PaLI (Chen et al., 2023) for example. In our work, we enhance\nthe capabilities of the CLIP model (Radford et al., 2021), by adding a light-weight retrieval module.\nNevertheless, our method is not specific to CLIP and can be applied to any vision-text model.\nKnowledge-based vision-text models. Several works have focused on ways of improving upon\ndifferent aspects of the contrastive vision-text models, such as their training objectives (Gao et al.,\n2022; Zhai et al., 2022; Dong et al., 2022) or through scaling (Cherti et al., 2022; Pham et al., 2021).\nYet, only little exploration has been done on their combination with memory or knowledge-based\ntechniques (Dwibedi et al., 2021; Banani et al., 2023; Liu et al., 2023; Shen et al., 2022; Fan et al.,\n2023). REACT (Liu et al., 2023) retrieves image-text pairs from an external memory in order to\nbuild a training dataset specialized for a specific downstream task. Unlike REACT (Liu et al., 2023),\nour work does not require any pre-knowledge about the nature of the downstream task, and is hence\napplicable in a full zero-shot transfer. Another key difference is that our model can leverage items\nfrom the memory at inference time, while REACT uses retrieved items to automatically generate a\ntraining set to finetune their model. Closer to our work, K-LITE (Shen et al., 2022) learns vision-text\nmodels by leveraging external sources of knowledge (i.e. WordNet (Meyer & Gurevych, 2012) or\nWiktionary (Miller, 1998)) to complete captions with more descriptive content. Unlike our approach,\nthe retrieved knowledge is uni-modal (e.g. they complement text with more text) and the external\nmemory is not used for the image tower. Also using a knowledge-based approach but for image-only\nrepresentation learning, NNCLR (Dwibedi et al., 2021) finds the visual nearest-neighbor of each\ntraining image from a memory for contrastive learning. LGSimCLR (Banani et al., 2023) uses the\nlanguage guidance to find most similar visual nearest-neighbor. Unlike our work, NNCLR and\nLGSimCLR only learn visual representations and use retrieval to enhance their supervision during\ntraining but not at inference.\n2\nPublished as a conference paper at ICLR 2024\nFigure 1: RECO works by complementing the frozen representations of pre-trained image-text\nencoders (such as CLIP) with knowledge retrieved from an external memory. We use an image\nrepresentation as a query to identify the k most similar images and integrate their associated text\nembeddings to create a multi-modal representation. Likewise, given a text representation as a query,\nwe find the top-k most similar texts and incorporate their associated images. The fusion of original\nand retrieved embeddings is done by learning a shallow fusion model to produce improved, multi-\nmodal and knowledge-enhanced versions of the original embeddings. We train for alignment between\nthe refined embeddings, as well as between the refined and original embeddings.\nRetrieval-based methods. The main argument of the retrieval-based methods is that not all the world\nknowledge can be compiled into a model\u2019s parameters. Thus, the model should also learn to rely on\nitems retrieved from an external memory at inference. Retrieval-based methods have shown their\npromise in various NLP tasks (Khandelwal et al., 2020; Guu et al., 2020a; Lewis et al., 2020; Wang\net al., 2022; Wu et al., 2022; Borgeaud et al., 2022). More recently, there is an increasing interest in\nthe computer vision for retrieval-based methods as well (Blattmann et al., 2022; F\u00fcrst et al., 2022;\nChen et al., 2022; Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard et al., 2022; Guu et al.,\n2020b; Liu et al., 2023; Shen et al., 2022; Iscen et al., 2023). Of particular interest, SuS-X (Udandarao\net al., 2023) shows that by either retrieving similar samples to the query sample from a large data-bank\nlike LAION can improve zero-shot classification performance of CLIP. Conceptually, SuS-X falls\nunder the \u201cCross-modal search and cross-modal fusion\u201d variant explored in this paper (see second\nscenario in Fig. 2). RA-CLIP (Xie et al., 2023) enriches the CLIP visual representation by retrieved\nimage and text. However, their attempt to enrich the text representation degrades the performance,\nwhereas we show that the retrieved data can also help produce better text representations.\n3\nMETHOD\nOur goal is to equip powerful pre-trained vision-language models (such as CLIP) with the ability to\ncomplement their representations with cross-modal knowledge retrieved from an external memory.\nWe aim to do this without requiring such models to be retrained from scratch, but by simply learning\na light-weight retrieval fusion module on top of them. We emphasize that this work does not propose\na new model or loss but rather a new way of adapting pre-trained models to use relevant retrieved\nknowledge at inference time. An overview of our approach, RECO, is shown in Fig. 1.\nPreliminaries.\nWe are given a pre-trained frozen dual-encoder vision-text model f, where v =\nfimage(I) is the embedding of image I, and t = ftext(T) is the embedding of text T. We say that\nthese embeddings are uni-modal since they are obtained purely from a single modality, either image\nor text. We assume that image and text embedding spaces are already aligned, meaning that they\nhave been trained to produce similar representations for matching image-text pairs and dissimilar\nrepresentations for non-matching pairs (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2022; van den\nOord et al., 2018). This alignment is usually obtained by minimizing the InfoNCE loss (or contrastive\nloss) (van den Oord et al., 2018) between embeddings of different modalities:\nLNCE(V, T) = \u2212\nn\nX\ni=1\n\uf8ee\n\uf8f0log\nev\u22a4\ni ti/\u03c4\nP\nj ev\u22a4\ni tj/\u03c4 + log\nev\u22a4\ni ti/\u03c4\nP\nj ev\u22a4\nj ti/\u03c4\n\uf8f9\n\uf8fb ,\n(1)\nwhere V (resp. T) is the matrix composed of the n visual (resp. text) embeddings in the minibatch\nand \u03c4 is the temperature parameter. We propose to augment the text and visual embeddings, i.e. t\nand v, with external cross-modal knowledge in order to enhance both their expressiveness and their\ncross-modality alignment. In the following of this section, we first detail how we retrieve relevant\n3\nPublished as a conference paper at ICLR 2024\nUni-modal search\nCross-modal search\nUni-modal search\nCross-modal search\n& cross-modal fusion\n& cross-modal fusion\n& uni-modal fusion\n& uni-modal fusion\n(Ours)\nFigure 2: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion. We illustrate\nthe different scenarios for an input image I while the scenarios for text input T are shown in Appendix.\ncross-modal knowledge based on within-modality search. Second, we present how we learn to fuse\nthe retrieved information into the original embeddings.\n3.1\nRETRIEVING CROSS-MODAL EXTERNAL KNOWLEDGE\nMemory. We define the external source of knowledge by a memory M = {(Ii, Ti)}M\ni=1 of M\nimage-text pairs. We assume that M is very large and covers a broad coverage of concepts. In\npractice, only a small-subset of M is relevant for a given input query. Thus, we only consider the k\nmost relevant items from M for each input obtained by the nearest neighbour search. We denote by\nKNN(v, M) and KNN(t, M) the sets formed by the embeddings of the k most relevant items to the\nqueries v and t from the memory, where KNN refers to the nearest-neighbour retrieval module.\nCross-modal fusion. Our goal is to augment the text and visual original embeddings with cross-\nmodal knowledge, not necessarily learned during the pre-training stage. For example, given the\nclass name Yellow bellied flycatcher in a fine-grained bird classification problem such as CUB (Wah\net al., 2011), we first look for captions in the memory that are semantically similar to the given class\nname. We then augment the class name representation with the visual representations of the retrieved\nsimilar captions, i.e. with what an Yellow bellied flycatcher looks like. Likewise, given a visual\nrepresentation of a bird, we look for similar images in M and use their corresponding captions in the\nhope that some of them might contain useful information for our problem such as the species of that\nbird. Specifically, for a given text or image input, the retrieval module KNN(., M) returns items with\nthe opposite modality than that of the input. We use the subscripts v or t to specify the modality of\nthe retrieved embeddings. That is, KNNt(v, M) returns text embeddings from an image input and\nKNNv(t, M) returns image embeddings for text input.\nNote that we also evaluate uni-modal fusion in our experiments, i.e. complementing visual repre-\nsentations with the retrieved visual knowledge and text representation with the retrieved captions.\nHowever, we find in practice that this variant leads to poorer performance than cross-modal fusion, as\nshown in Tab. 3. Intuitively, we hypothesize that this is because the signal brought by cross-modal\nfusion is richer due to the complementarity of the different modalities (Iscen et al., 2023).\nUni-modal search. We choose to search relevant items in the memory M based on within-modality\nsimilarities, which we refer to as \u201cuni-modal search\u201d as opposed to \u201ccross-modal search\u201d. Specifically,\nwe use text-to-text similarity (t \u2192 t) to identify suitable content from a text embedding t and image-\nto-image similarity (v \u2192 v) to retrieve relevant matches from a visual embedding v. Formally, let us\ndenote by VM and TM all the image and text embeddings from M given by our pretrained vision-\ntext model f, i.e. we have VM = [fimage(I1), . . . , fimage(IM)] and TM = [ftext(T1), . . . , ftext(TM)].\nThe retrieval module is hence finally denoted as KNNv\u2192v\nt\n(v, M) = TM\nNN(v;VM), i.e. for an input\nimage embedding v, the k-NN search is done between v and VM, but the corresponding k-NN\nindices from the text embeddings TM are selected. Similarly, we denote the retrieval process as\nKNNt\u2192t\nv\n(t, M) = VM\nNN(t;TM) for an input text embedding t.\n4\nPublished as a conference paper at ICLR 2024\nWe also evaluate cross-modal search but find that this leads to much poorer performance, especially\nin fine-grained problems, as shown in Tab. 3. An explanation is that the uni-modal search is an easier\ntask, hence the retrieved elements are more relevant (because more similar) to the input. On the other\nhand, cross-modal search suffers from the pre-trained CLIP model\u2019s lack of fine-grained alignment\nbetween the different modalities, resulting in noisier retrieval. Note that another advantage of uni-\nversus cross- modal search is that the latter requires the pre-trained image and text encoders to be\nalready aligned while we can potentially let go of this hypothesis with uni-modal search.\n3.2\nLEARNING HOW TO FUSE THE RETRIEVED KNOWLEDGE\nOur goal is to refine the original image and text embeddings v and t with the cross-modal knowledge\ngathered from M. We denote these refined image and text embeddings by v and t, defined as\nv = \u03d5image(v, KNNv\u2192v\nt\n(v, M)) and t = \u03d5text(t, KNNt\u2192t\nv\n(t, M)), where \u03d5 is the fusion model.\nTransformer fusion. We model \u03d5image and \u03d5text as one-layer multi-head self-attention transformer\nencoders (Vaswani et al., 2017; Dosovitskiy et al., 2021). Intuitively, this choice allows the original\nembedding to attend to all the retrieved elements in the fusion process. Note that while the fusion\nmodels for text and image encoders have identical architectures, they do not share parameters. In\npractice, the fusion module has a total of 3.16M parameters, which corresponds to only 2% of the\ntotal parameter count when using CLIP-B/32 as the backbone f. We have experimented with bigger\nfusion modules (see Appendix) but find that this light-weight solution works well in practice. We\nhave also tried mean fusion of retrieved and original elements by simply averaging their embeddings\nbut have found in practice that it performs poorly (see Tab. 3). Intuitively, the model needs to learn\nhow to incorporate this new information, by, for example, learning how to omit or enhance some of\nthe retrieved elements.\nLearning. We train the fusion model \u03d5 on a dataset D = {(Ii, Ti)}N\ni=1 by performing retrieval\nat training time from the memory M. The pre-trained encoder f is kept frozen. We minimize\nthe alignment loss between the refined embeddings which formally amounts to minimizing the\nInfoNCE loss of Eq. (1) with the refined embeddings instead of original embeddings, i.e. minimizing\nLNCE(V, T). We find that it is also sometimes beneficial to perform retrieval for only one of the\nbranches (text or image) at inference time depending on the nature of the downstream task (see\nTab. 4). Therefore, we also align the original and refined embeddings by minimizing the following\n\u201ccross\u201d loss terms: LNCE(V, T) and LNCE(V, T). This allows to disable one of branches at inference\ntime, since refined and original embeddings are now also aligned. Overall, we minimize:\nL = LNCE(V, T) + LNCE(V, T) + LNCE(V, T).\n(2)\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nTraining details. We train the fusion model on top of a frozen CLIP (-B/32 or -L/14 version)\nmodel (Radford et al., 2021). We also present a variant of RECO on top of a frozen LiT-L16L (Zhai\net al., 2022) model. We train on Conceptual Captions 12M (\u201cCC12M\u201d) (Changpinyo et al., 2021),\nan image-text dataset containing about 10M pairs. We use a batch size of 4096, learning rate of\n1e\u22123 decayed with a cosine schedule and weight decay of 1e\u22125. The temperature parameter is\nlearned (Radford et al., 2021). Training is done for 10 epochs, which lasts about 10 hours on a\n4x4 TPUv2 pod. For the memory, we use the subset of WebLI (Chen et al., 2023) containing 1B\nimage-text pairs. We remove the near-duplicates of the test images from the memory. We have also\nexplored using smaller but publicly available memory such as LAION-400M dataset (Schuhmann\net al., 2021) and show the results in Appendix.\nEvaluation datasets. We consider the following six image classification datasets: Stanford Cars\n(\u201cCars\u201d) (Krause et al., 2013), CUB-200-2011 (\u201cCUB\u201d) (Wah et al., 2011), Oxford Flowers (\u201cFlow-\ners\u201d) (Nilsback & Zisserman, 2008), ImageNet-1k (\u201cIm1k\u201d) (Russakovsky et al., 2015), Places365\n(\u201cPl365\u201d) (Zhou et al., 2017) and Stanford Dogs (\u201cDogs\u201d) (Khosla et al., 2011). We also consider\nthe recent Open-domain visual entity recognition (OVEN) benchmark (Hu et al., 2023), containing\n729K test images possibly belonging to 6M entity candidates. Finally, we also report performance on\ntext-to-image (\u201cT\u2192I\u201d) and image-to-text (\u201cI\u2192T\u201d) retrieval on Flickr30k (\u201cFlickr\u201d) (Plummer et al.,\n2015) and MS COCO (\u201cCOCO\u201d) (Lin et al., 2014) in Appendix. More details about these datasets\ncan be found in Appendix or in their corresponding publication.\n5\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-shot transfer to image classification. We report top-1 accuracy for classification.\nWe show the improvements obtained with RECO on top of CLIP-R50, CLIP-B/32, CLIP-L/14 and\nLiT-L16L: absolute performance gains are between brackets. For reference, we also include the\nperformance of K-Lite (Shen et al., 2022) and RA-CLIP (Xie et al., 2023) (other retrieval-augmented\nmethods) and other image-text models (Align-base (Jia et al., 2021) and PaLI-17B (Chen et al.,\n2023)). We also report the total parameter count (\u201c# par.\u201d) of the different models (in Million).\nMethod\n# par. Cars\nCUB\nFlowers Im1k\nPl365\nDogs\nCLIP-R-50\n102\n38.6\n52.0\n47.2\n59.2\n53.1\n60.6\n+ RECO\n114\n39.8(+1.2) 62.8(+10.8) 56.2(+9.0) 59.4(+0.2) 54.0(+0.9) 64.4(+3.8)\nCLIP-B/32\n151\n57.2\n52.8\n62.1\n63.5\n40.6\n58.6\n+ RECO\n154\n68.1(+10.9) 63.0(+10.2) 67.9(+5.8) 64.6(+1.1) 42.2(+1.6) 59.7(+1.1)\nCLIP-L/14\n428\n75.6\n61.7\n75.6\n75.5\n42.0\n72.7\n+ RECO\n435\n82.8(+7.2) 73.4(+11.7) 79.5(+3.9) 76.1(+0.6) 43.6(+1.6) 73.9(+1.2)\nLiT-L16L\n638\n90.5\n54.5\n77.4\n80.2\n45.2\n75.7\n+ RECO\n652\n90.8(+0.3) 74.8(+20.3) 84.1(+6.7) 80.9(+0.7) 45.4(+0.2) 81.3(+5.8)\nOther approaches\nK-Lite\n151\n10.0\n\u2013\n78.6\n52.3\n\u2013\n\u2013\nRA-CLIP\n151\n\u2013\n\u2013\n\u2013\n53.5\n\u2013\n26.1\nAlign\n247\n78.7\n38.2\n64.9\n67.6\n44.0\n56.3\nPaLI\n17,000 \u2013\n\u2013\n\u2013\n72.1\n\u2013\n\u2013\nTable 2: Zero-shot performance on OVEN. We report top-1 accuracy on seen and unseen categories\nand their harmonic mean. We also indicate the total number of parameters of each model (\u201c# params\u201d).\nMethod\n# params (M)\nSeen\nUnseen\nHarmonic mean\nZero-shot\nPaLI-17B (Chen et al., 2023)\n17,000\n4.4\n1.2\n1.9\nCLIP-L/14 (Radford et al., 2021)\n428\n5.6\n4.9\n5.3\nCLIP-L/14 (Radford et al., 2021) + RECO (Ours)\n435\n11.5 (+5.9)\n13.3 (+8.4)\n12.3 (+7.0)\nFine-tuning on the OVEN Seen categories\nCLIP-L/14 Fusion (Hu et al., 2023)\n880\n33.6\n4.8\n8.4\nPaLI-3B (Chen et al., 2023)\n3,000\n19.1\n6.0\n9.3\nCLIP-L/14 CLIP2CLIP (Hu et al., 2023)\n860\n12.6\n10.5\n11.5\nPaLI-17B (Chen et al., 2023)\n17,000\n28.3\n11.2\n16.1\nEvaluation protocol. We evaluate in the zero-shot setting for all the considered benchmarks, meaning\nthat no adaptation is done to the downstream task. As common in the literature (Radford et al., 2021;\nJia et al., 2021; Singh et al., 2022; Zhai et al., 2022), we add prompts to the text of the downstream\ntasks, following (Zhai et al., 2022). All evaluation protocols are in Appendix.\n4.2\nZERO-SHOT TRANSFER\nImage classification. In Tab. 1, we observe that RECO boosts the zero-shot performance of CLIP\nand LiT on zero-shot image classification with large improvements especially on the fine-grained\ndatasets. For example, we improve the original CLIP-B/32 accuracy by +10.9 on Cars, +10.2 on\nCUB and +5.8 on Flowers. The performance is also improved on less fine-grained benchmarks\nsuch as ImageNet or Places, though by more moderate margins (i.e. respectively +1.1 and +1.6).\nSecondly, we see in Tab. 1 that the performance gains are consistent across all vision-text backbones\n(CLIP-R-50, CLIP-B/32, CLIP-L/14, and LiT-L16L). Note that LiT-L16L is pre-trained on Webli,\nwhich is our memory bank, and we still observe the benefits of RECO. For reference, we also report\nin Tab. 1 the numbers from other popular vision-text approaches (Jia et al., 2021; Chen et al., 2023).\nOverall, the experiment in Tab. 1 confirms our initial motivation that retrieval from an external\nmemory improves zero-shot recognition tasks, especially in fine-grained settings.\nOpen-domain visual entity recognition (OVEN). In Tab. 2, we show the zero-shot performance of\nRECO on the OVEN benchmark. We see that our method improves greatly over CLIP-L/14 on this\nchallenging task, with an impressive relative improvement of +132%. Note that we do not train or\nfine-tune our model on the OVEN training set. Remarkably, we observe in Tab. 2 that RECO also\n6\nPublished as a conference paper at ICLR 2024\nTable 3: Uni-modal search for cross-modal fusion. We report top-1 accuracy for zero-shot image\nclassification. We evaluate the impact of uni-modal versus cross-modal search and uni-modal versus\ncross-modal fusion. These different mechanisms are conceptually illustrated in Fig. 2. We report\nabsolute improvement between brackets and the average relative improvement over not using retrieval\n(i.e. CLIP performance) in the last row (\u201cAvg. rel. \u2206\u201d).\nSearch\nFusion\nCars\nCUB\nFlowers\nIm1k\nPl365\nAvg. rel. \u2206\n\u2013\n\u2013\n57.2\n52.8\n62.1\n63.5\n40.6\n\u2013\n\u03d5 = Transformer fusion\n1 Uni-modal\nCross-modal\n68.1 (+10.9)\n63.0 (+10.2)\n67.9 (+5.8)\n64.6 (+1.1)\n42.5 (+1.9)\n+ 9.0 %\n2 Cross-modal Cross-modal\n56.6 (-0.6)\n53.8 (+1.0)\n64.3 (+2.2)\n64.3 (+0.8)\n42.4 (+1.8)\n+ 1.7 %\n3 Uni-modal\nUni-modal\n57.3 (+0.1)\n51.2 (-1.6)\n62.2 (+0.1)\n62.1 (-1.4)\n41.7 (+1.1)\n\u2212 0.4 %\n4 Cross-modal Uni-modal\n54.0 (-3.2)\n50.7 (-2.1)\n61.4 (-0.7)\n62.3 (-1.2)\n41.2 (+0.6)\n\u2212 1.9 %\n\u03d5 = Mean fusion\n5 Uni-modal\nCross-modal\n46.9 (-10.3)\n44.9 (-7.9)\n50.5 (-11.6)\n40.1 (-23.4)\n23.7 (-16.9)\n\u2212 21.7 %\n6 Cross-modal Cross-modal\n43.7 (-13.5)\n45.3 (-7.5)\n58.7 (-3.4)\n55.2 (-8.3)\n32.7 (-7.9)\n\u2212 11.0 %\n7 Uni-modal\nUni-modal\n44.0 (-13.2)\n47.2 (-5.6)\n61.3 (-0.8)\n55.1 (-8.4)\n36.2 (-4.4)\n\u2212 9.8 %\n8 Cross-modal Uni-modal\n33.4 (-23.8)\n30.2 (-22.6)\n38.9 (-23.2)\n40.0 (-23.5)\n24.7 (-15.9)\n\u2212 33.0 %\nsignificantly outperforms much bigger models which are directly fine-tuned for this task, for example\nCLIP2CLIP (Hu et al., 2023) or PaLI-3B (Chen et al., 2023) while using respectively 2 \u00d7 and 7 \u00d7\nless parameters. It even comes close to the performance of PaLI-17B while being 39 \u00d7 smaller and\nnot using any fine-tuning.\n4.3\nDESIGN CHOICE ANALYSES\nIn this section, we validate several components of our model, namely the uni-modal search and\ncross-modal fusion, training of the fusion module and the number of retrieved elements from the\nmemory. We also propose some qualitative examples to help understanding why RECO improves\nover CLIP performance. We use ViT-CLIP-B/32 throughout this section.\nUni-modal search and cross-modal fusion. In Tab. 3, we evaluate different alternatives for our\nmethod, namely (i) performing cross-modal search in the memory instead of uni-modal search and\n(ii) fusing uni-modal items (i.e. combining text with text and image with image) instead of cross-\nmodal fusion. These different scenarios (uni- versus cross- modal search and fusion) are detailed\nin Section 3.1 and conceptually illustrated in Fig. 2. Firstly, we observe in Tab. 3 that uni-modal\nsearch (row 1) leads to a better performance compared to cross-modal search (row 2), with +9.0\nversus +1.7 average relative improvement over CLIP. We remark that the gap is especially important\nfor fine-grained datasets such as Cars, CUB and Flowers. This agrees with our hypothesis that\ncross-modal search suffers from the pre-trained CLIP model\u2019s lack of fine-grained alignment between\ndifferent modalities. By contrast, using the inherent strength of image and text representations within\ntheir respective modalities allows to retrieve relevant matches, as qualitatively observed in Fig. 4.\nSecondly, we observe in Tab. 3 that uni-modal fusion (rows 3 and 4) works substantially worse than\ncross-modal fusion (rows 1 and 2). Indeed, we see that augmenting text embeddings with other text\nembeddings and image embeddings with other image embeddings does not bring any significant\nimprovement over the baseline, and even tends to hurt the performance. Intuitively, a possible\nexplanation is that cross-modal fusion allows us to inject complementary signal into the original\nembeddings (Iscen et al., 2023). By contrast, uni-modal provides signal that is already similar to\nthe input, hence not as much additional information. Finally, we see in Tab. 3 that all the variants\n(rows 5, 6, 7 and 8) fail when simply averaging retrieved and original embeddings instead of learning\nthe fusion with a transformer. This highlights the importance of learning to incorporate the retrieved\nitems to the original embeddings before deploying the model at inference.\nImage and text retrieval fusion modules. In Tab. 4, we compare models trained to fuse only text\noriginal embeddings (row 1), only image original embeddings (row 2) or both (row 3). We observe\nthat while models trained to fuse only image or text perform reasonably well on some benchmarks,\nthey typically lag behind on other benchmarks. For example, the model trained for only image fusion\n(row 2) is strong on zero-shot Dogs benchmark but behind on CUB and COCO. Secondly, as shown\nin Tab. 4, unlike the vision-only or text-only variants, our model can be used in different modes at\ninference time in a flexible manner. Indeed, because we have trained it to align the refined embeddings\n7\nPublished as a conference paper at ICLR 2024\nTable 4: Image and text retrieval fusion modules. We report zero-shot top-1 accuracy for image\nclassification and recall@1 for image retrieval. We compare models trained only for text fusion\n(row 1), image fusion (row 2) or both (row 3). Our model can be used in different modes at inference:\nretrieval only for image (v), retrieval only for text (t) or retrieval for both image and text (v&t).\nCUB\nDogs\nCOCO T\u2192I\n\u03d5image \u03d5text\nfusion training loss\nv\nt\nv&t Best\nv\nt\nv&t Best\nv\nt\nv&t Best\n1\n\u2713\nLNCE(V, T)\n\u2717 59.3\n\u2717\n59.3\n\u2717 59.6\n\u2717\n59.6\n\u2717 33.3\n\u2717\n33.3\n2 \u2713\nLNCE(V, T)\n59.7 \u2717\n\u2717\n59.7 59.2 \u2717\n\u2717\n59.2 31.3 \u2717\n\u2717\n31.3\n3 \u2713\n\u2713\nLNCE(V, T)+LNCE(V, T)+LNCE(V, T) 60.0 58.4 63.0 63.0 59.7 59.7 59.4 59.7 31.9 33.6 31.7 33.6\nMethod\nTrain data\nCUB\nCLIP-B/32\n\u2013\n52.8\n+ CLIP-style CC12M\n44.8\n+ CLIP-style CC12M + RWebli 46.8\n+ RECO\nCC12M + RWebli 63.0\n110 30 50 70\n100\n% of memory (training)\n52\n56\n60\n64\nCUB\ntrain memory\nfull memory\n1\n5\n10\n20\nk retrieved items (training)\n52\n56\n60\n64\nk'=20\nk'=10\nk'=5\nk'=2\nk'=1\nFigure 3: (left) Disantangling the effect of additional training and RECO. (middle) Effect of\nupdating the memory after training. (right) Effect of the number k of retrieved elements. We\nreport zero-shot top-1 accuracy on CUB. The CLIP baseline is shown with symbol 8.\nwith the original ones (see the cross terms in the loss 2), we can choose to disable the retrieval for one\nof the branches at inference time, depending on the task. Therefore, we compare in Tab. 4 different\noptions at inference time: using retrieval only for the image input, only for the text input or for\nboth of them, denoted respectively by v, t and v&t. We observe in Tab. 4 that depending on the\nnature of the task, one of these options might be preferable over the others. For example, for image\nclassification we see that augmenting the image embeddings with retrieved text has more positive\nimpact than augmenting the text embeddings, though the best of performance is obtained with both.\nOn the other hand, text and image retrievals seem to benefit more from augmenting the text rather\nthan the image side. This intuitively can be explained by the fact that text descriptions in retrieval\nbenchmarks are typically highly specific compared to the class names in image classification and so\naugmenting with visual examples of what they refer to greatly helps the alignment. We demonstrate\nqualitative examples of this hypothesis in Appendix. Overall, at inference time, one can choose the\nbest inference mode for a particular downstream task by validation on a held-out set.\nIs the performance boost merely due to additional training? We replace RECO with an MLP layer\nof the same capacity initialized from scratch. We train it in a CLIP-style manner on the subset of Webli\nthat we use when training RECO. We denote this subset by RWebli: it contains the k = 10 nearest-\nneighbors for each CC12M datapoint retrieved from the Webli dataset, and contains 61M examples.\nWe observe in Fig. 3 (left) that training an extra layer on top of CLIP does not bring any gains and\neven deteriorates its performance. Indeed, CLIP was extensively trained on a large dataset (Radford\net al., 2021) and additional training on a relatively small dataset deteriorates the general-purpose\nproperty of its representations. Overall, this experiment validates that the performance gains are due\nto our method and not to training an additional layer on top of CLIP.\nUpdating the memory after training. A clear advantage of the retrieval-based models is that the\nexternal memory can be updated with additional, and more contemporary information. We evaluate\nthe effectiveness of RECO when using a larger memory that is not observed during the training. We\nfirst create various random subsets of Webli by randomly removing a percentage of data. Then, we\ntrain separate RECO models with each Webli subset as its memory. At inference, we evaluate each\nRECO model either with the subset of memory that it was trained with, or the full Webli memory.\nResults are shown in Fig. 3 (center). We observe that training and evaluating RECO with only 1%\nof Webli as the memory does not show improvements compared to the CLIP baseline. However,\nwe observe a significant improvement when evaluating the same model with full Webli memory at\ninference. This confirms that RECO is capable of utilizing an updated memory without re-training.\nEffect of the number of retrieved elements. In Fig. 3 (right), we study the effect of the number of\nretrieved elements in the memory. We evaluate different numbers of k-NN during the training and\ninference time, i.e. we train our model with k items from the memory but use k\u2032 at inference. We\n8\nPublished as a conference paper at ICLR 2024\nQuery\nUni-modal search (Ours)\nCross-modal search\nFigure 4: Qualitative examples on CUB and Cars datasets. We compare uni- versus cross- modal\nsearch for two image queries (top) and two text queries (bottom). Uni-modal search allows to find\nmore suitable matches to the query, which improves the relevancy of the fused elements. We frame in\nred (resp. green) the unrelevant (resp. relevant) retrieved items to be fused with the query.\nsee in Fig. 3 (right) that RECO generally obtains a higher performance when k\u2032 > k at inference.\nInterestingly, the performance saturates after k = 10. An explanation is that increasing the number of\nretrieved elements goes with a reduction of the relevancy of the retrieved items.\nQualitative study. In Fig. 4, we provide illustrative examples of why RECO can be useful for\nfine-grained image classification on CUB or Cars datasets. We compare our method with a variant\nusing cross-modal search instead of uni-modal search to illustrate the importance of using the inherent\nstrength of image-only and text-only representations. We observe in Fig. 4 that uni-modal search\nallows to retrieve better matches for the query. This is because image-to-image or text-to-text search\nretrieves more similar items to the query than crossing modalities. As a result, retrieved items are\nmore accurate, which leads to a higher accuracy for fine-grained tasks.\nLimitations. A limitation of this work is that it assumes to have access to a large and rich source of\nimage-text pairs knowledge. While we show in Appendix that public datasets , e.g. LAION (Schuh-\nmann et al., 2021), can serve this purpose, the best of performance is obtained with a large private\nmemory. Alternatively, one could use search engine APIs as the memory. Another limitation is that\nthe performance gains of RECO come at the cost of increased inference time. In practice, we use\na highly-optimized approximate k-NN algorithm (Guo et al., 2020). It takes about 14ms to query\nWebli (956M examples) with a single 512-d ViT-B/32 CLIP embedding. Using retrieval at inference\ntime incurs an overhead of 25% compared to not using any retrieval at inference time (e.g. baseline\nCLIP model), but improves the accuracy by up to 10.9.\n5\nCONCLUSION\nIn this paper, we introduce RECO, a method that enhances the fine-grained recognition capabilities of\npre-trained vision-text models. Our approach shows the importance of uni-modal retrieval, yet cross-\nmodal fusion for image and text inputs. We show that RECO consistently improves the performance\non 11 zero-shot tasks and that the gains are especially important in challenging fine-grained tasks.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nMohamed El Banani, Karan Desai, and Justin Johnson. Learning visual representations via language-\nguided sampling. arXiv preprint arXiv:2302.12248, 2023.\nAndreas Blattmann, Robin Rombach, Kaan Oktay, Jonas M\u00fcller, and Bj\u00f6rn Ommer. Semi-parametric\nneural image synthesis. arXiv preprint arXiv:2204.11824, 2022.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Im-\nproving language models by retrieving from trillions of tokens. In Proceedings of the International\nConference on Machine Learning (ICML), 2022.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative com-\nponents with random forests. In Proceedings of the European Conference on Computer Vision\n(ECCV), 2014.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nConference on Computer Vision and Pattern Recognition (CVPR), 2021.\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented\ntext-to-image generator. arXiv preprint arXiv:2209.14491, 2022.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual\nlanguage-image model. International Conference on Learning Representations (ICLR), 2023.\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\nGordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\nTerrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object\nrecognition work for everyone? In Proceedings of the Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\nKaran Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In\nProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nXiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng,\nWeiming Zhang, Lu Yuan, Dong Chen, et al.\nMaskclip: Masked self-distillation advances\ncontrastive language-image pretraining. arXiv preprint arXiv:2208.12262, 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn International Conference on Learning Representations (ICLR), 2021.\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In\nProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nLijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training\nwith language rewrites. arXiv preprint arXiv:2305.20088, 2023.\nAndreas F\u00fcrst, Elisabeth Rumetshofer, Johannes Lehner, Viet T Tran, Fei Tang, Hubert Ramsauer,\nDavid Kreil, Michael Kopp, G\u00fcnter Klambauer, Angela Bitto, et al. Cloob: Modern hopfield\nnetworks with infoloob outperform clip. Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\n10\nPublished as a conference paper at ICLR 2024\nYuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramid-\nclip: Hierarchical feature alignment for vision-language model pretraining. Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\nGerry. Sports100: 100 sports image classification. https://www.kaggle.com/datasets/\ngpiosenka/sports-classification/metadata, 2021. Accessed: 2023-05-23.\nLluis Gomez, Yash Patel, Mar\u00e7al Rusinol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised\nlearning of visual features through embedding images into text topic spaces. In Proceedings of the\nConference on Computer Vision and Pattern Recognition (CVPR), 2017.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of\nthe Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. Kat:\nA knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614,\n2021.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar.\nAccelerating large-scale inference with anisotropic vector quantization. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2020.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. REALM: Retrieval\naugmented language model pre-training. In Proceedings of the International Conference on\nMachine Learning (ICML), 2020a.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In Proceedings of the International Conference on Machine Learning\n(ICML), 2020b.\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina\nToutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing\nmillions of wikipedia entities. Proceedings of the International Conference on Computer Vision\n(ICCV), 2023.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid,\nDavid A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with\nmulti-source multimodal knowledge memory. arXiv preprint arXiv:2212.05221, 2022.\nAhmet Iscen, Alireza Fathi, and Cordelia Schmid. Improving image recognition by retrieving from\nweb-scale image-text data. arXiv preprint arXiv:2304.05173, 2023.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In Proceedings of the International Conference on Machine Learning\n(ICML), 2021.\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features\nfrom large weakly supervised data. In Proceedings of the European Conference on Computer\nVision (ECCV), 2016.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\nIn Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. International Conference on Learning\nRepresentations (ICLR), 2020.\n11\nPublished as a conference paper at ICLR 2024\nAditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for\nfine-grained image categorization. In First Workshop on Fine-Grained Visual Categorization,\nCVPR, 2011.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In Proceedings of the International Conference on Computer Vision (ICCV), 2013.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. International Journal of Computer Vision,\n123, 2017.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\nFeng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter\nVajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip.\narXiv preprint arXiv:2210.04150, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of\nthe European Conference on Computer Vision (ECCV), 2014.\nHaotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan\nLi. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint\narXiv:2301.07094, 2023.\nAlexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan\nBlair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail\nvisual recognition. In Proceedings of the Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Proceedings of the Conference\non Computer Vision and Pattern Recognition (CVPR), 2019.\nChristian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring\nthe possibilities of collaborative lexicography. na, 2012.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis-\nserman. End-to-end learning of visual representations from uncurated instructional videos. In\nProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\nGeorge A Miller. WordNet: An electronic lexical database. MIT press, 1998.\nMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey\nDosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple\nopen-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230,\n2022.\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,\n2008.\nHieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu,\nYi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary\nimage classification. arXiv preprint arXiv:2111.10050, 2021.\n12\nPublished as a conference paper at ICLR 2024\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the International Conference on Computer Vision\n(ICCV), 2015.\nVinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer vision?\narXiv preprint arXiv:2006.16923, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In Proceedings of the International Conference on\nMachine Learning (ICML), 2021.\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for\nthe masses. arXiv preprint arXiv:2104.10972, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 2015.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan\nWang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge.\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In\nProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\nVishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of\nvision-language models. ICCV, 2023.\nA\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,\nPietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In\nProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing\nSystems (NeurIPS), 2017.\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\nShuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and\nMichael Zeng. Training data is more valuable than you think: A simple and effective method by\nretrieving from training data. arXiv preprint arXiv:2203.08773, 2022.\nTobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale\nbenchmark for instance-level recognition and retrieval. In Proceedings of the Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.\n13\nPublished as a conference paper at ICLR 2024\nYongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning\u2014a\ncomprehensive evaluation of the good, the bad and the ugly. Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 2018.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In Proceedings of the Conference on Computer\nVision and Pattern Recognition (CVPR), 2010.\nChen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, and Jingren Zhou. Ra-clip:\nRetrieval augmented contrastive language-image pre-training. In CVPR, 2023.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\nCoca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917,\n2022.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov,\nand Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the\nConference on Computer Vision and Pattern Recognition (CVPR), 2022.\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Con-\ntrastive learning of medical visual representations from paired images and text. In Machine\nLearning for Healthcare Conference, 2022.\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10\nmillion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2017.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering\nin images. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR),\n2016.\n14\nPublished as a conference paper at ICLR 2024\nTable 5: Zero-shot transfer to retrieval. We report recall@1 for retrieval. We show the improve-\nments obtained with RECO on top of CLIP-B/32 and CLIP-L/14: absolute performance gains are\nbetween brackets. For reference, we also include the performance of other standard image-text\nfoundation models (Flava (Singh et al., 2022) and Align-base (Jia et al., 2021). We also report the\ntotal parameter count (\u201c# par.\u201d) of the different models (in Million).\nT\u2192I\nI\u2192T\nMethod\n# par. COCO\nFlickr\nCOCO\nFlickr\nCLIP-B/32 151 30.2\n61.1\n51.2\n80.9\n+ RECO\n154 33.6(+3.4) 65.7(+4.6) 52.2(+1.1) 81.8(+0.9)\nCLIP-L/14 428 35.2\n68.6\n57.2\n87.5\n+ RECO\n435 38.7(+3.5) 72.6(+4.0) 58.0(+0.8) 88.5(+1.0)\nOther approaches\nFlava\n172 38.4\n65.2\n42.7\n67.7\nAlign\n247 40.2\n72.6\n55.1\n86.7\nA\nAPPENDIX\nA.1\nIMAGE/TEXT RETRIEVAL\nTab. 5 shows that RECO allows to boost the zero-shot performance of CLIP for image and text\nretrieval even further. We observe that we can improve the recall@1 up to 4.6 points for text-to-image\nretrieval. We also see smaller, but consistent gains for image-to-text retrieval.\nA.2\nUSING LAION-400M AS THE MEMORY\nIn Table 6, we show that our method also works when using a public dataset as the memory bank\ninstead of our private source of knowledge. Indeed, we observe that using LAION-400M (Schuhmann\net al., 2021) as the memory bank for RECO gives substantial gains of performance compared to the\nCLIP baseline across our different zero-shot tasks: for example +6.9 on Cars and +6.1 on CUB. This\nvalidates that our method is generic and can work with different choices of external knowledge.\nTable 6: Choice of memory bank. We report zero-shot top-1 accuracy on different image clas-\nsification tasks. We evaluate RECO when using two different sources of knowledge: the non\npublicly available WebLI (Chen et al., 2023) dataset (our default) and the publicly available LAION-\n400M (Schuhmann et al., 2021) dataset.\nMemory bank\nPublic\nCars\nCUB\nFlowers\nIm1k\nPl365\nNone\n\u2013\n57.2\n52.8\n62.1\n63.5\n40.6\nWebLI (Chen et al., 2023) (default)\n\u2717\n68.1 (+10.9) 63.0 (+10.2) 67.9 (+5.8) 64.6 (+1.1) 42.2 (+1.6)\nLAION-400M (Schuhmann et al., 2021)\n\u2713\n64.1 (+6.9)\n58.9 (+6.1)\n63.7 (+1.6) 63.4 (-0.1)\n42.3 (+1.7)\nA.3\nMORE COMPLEX FUSION MODULE\nIn Table 7, we experiment with fusion modules of varying sizes. We observe that using a fusion\nmodule of one or two layers works comparatively well. However, using larger fusion modules with\nmore layers, e.g. four, six or eight, deteriorates the performance. We hypothesize that this is because\nincreasing the capacity of the fusion creates overfitting. Overall, using a single-layer fusion module\nbrings large gains of performance on top of CLIP while being very light-weight to train.\nA.4\nEND-TO-END FINETUNING VERSUS FROZEN BACKBONE\nIn Table 8, we evaluate the behavior of RECO when finetuning the original encoders at the same\ntime as the fusion module. We observe in Table 8 that the performance is comparable to keeping the\nencoders frozen as in our default setting. Freezing the encoders has the advantage of requiring less\ncompute resources. In our implementation and using the same hardware, finetuning CLIP-B/32 along\n15\nPublished as a conference paper at ICLR 2024\nTable 7: Size of the fusion module. For each variant, we report the total number of parameters\n(\u201c# params\u201d) in millions and the percentage of the total parameter count which is part of the fusion\nmodules (\u201c% fusion params\u201d). We report zero-shot top-1 accuracy on three image classification tasks.\n# fusion layer\n# params (M)\n% fusion params\nCars\nFlowers\nIm1k\n0\n151.3\n0%\n57.2\n62.1\n63.5\n1 (default)\n154.4\n2.0%\n68.1\n67.9\n64.6\n2\n157.6\n4.0%\n67.8\n69.1\n64.3\n4\n163.9\n7.7%\n61.9\n62.8\n59.8\n6\n170.2\n11.1%\n60.6\n64.5\n59.6\n8\n176.5\n14.3%\n61.1\n64.4\n59.6\nTable 8: Full finetuning versus frozen encoders. Both mechanisms produce good performance but\nfreezing the backbone allows for 1.6\u00d7 faster training. We report zero-shot top-1 accuracy.\nCLIP encoders f\nCars\nFlowers\nIm1k\nFrozen (default)\n68.1\n67.9\n64.6\nFinetuned\n68.5\n67.1\n64.2\nwith the fusion module has a training step 1.6\u00d7 longer than working with frozen CLIP-B/32 and\ntraining only the fusion module.\nA.5\nQUALITATIVE EXAMPLES\nZero-shot retrieval. In Figure 5, we show some qualitative examples when applying RECO to\nzero-shot retrieval tasks on COCO dataset. Specifically, we aim to gain an understanding about why\nthe model benefits more from augmenting the text rather than the image side when applied to retrieval\ndownstream tasks (see Table 4 of the main paper). In Figure 5, we look at three different image-text\ninput pairs from the validation set of COCO and display the retrieved captions fused with the query\nimage as well as the retrieved images fused with the query text.\nWe observe in Figure 5 that the text captions of an image in COCO retrieval task usually focus on one\nspecific aspect of the image (for example the towel in the image of the bathroom or the British flag\non the train). We observe that the retrieved images from the input text are likely to also contain this\nparticular aspect of interest and hence match well with the original caption. For example, the retrieved\nimages from the train with a British flag caption (bottom of Figure 5) all contain representations of\nvehicles with painted British flags, which is more likely to help the alignment with the original input.\nOn the contrary, the captions retrieved from the original image may focus on another particularity of\nthe image, not mentioned in the original caption. For example, the captions retrieved from the train\nimage contain information about train numbers, train station locations or operators. This information\nis not useful for this task, because the ground-truth caption focuses on the fact that the train carriage\nhas a British flag on its side. This brings distracting signal instead of helping the alignment. Overall,\nwe think these qualitative examples help us understand why disabling retrieval for the image input\nand enabling it for the text side results in a better performance in this task.\nZero-shot image classification. In Figure 6, we display some qualitative examples of RECO for zero-\nshot image classification downstream tasks. Specifically, we consider several query images and their\nclass names and show the corresponding elements (captions for query image and images for query\nclass name) retrieved by our model. We observe in Figure 6 that in majority of cases, given a visual\ninput, searching for similar images and retrieving their corresponding captions effectively returns\ndescriptions containing useful information for fine-grained classification problem. For example, the\ncaptions retrieved from the cat image at the top of Figure 6 contain the breed of that cat (siamese).\nLikewise, given the class name \u201csiamese cat\u201d, RECO look for similar captions, for example \u201cpicture\nof a siamese cat\u201d, and returns their corresponding images. These all contain visual examples of what\na siamese cat looks like. Figure 6 show several successful examples of this mechanism and helps\ngiving intuition about why RECO helps for zero-shot image classification.\n16\nPublished as a conference paper at ICLR 2024\nFigure 5: Qualitative examples of RECO for image and text retrieval. We display image and\ntext queries on the left panel and retrieved captions and images on the right panel. We observe that\nretrieved images tend to match better with the input original image than retrieved captions with the\ninput original text. For example, the retrieved captions from the aerial view do not mention a lot\n\u201cmountains\u201d while this is present in the original text. Instead, they mention many specific locations,\nfor example lima, cuzco, arizona or afghanistan, which are not relevant to the original text description.\nOn the contrary, the retrieved images from the text query are semantically similar to the original\nimage. This qualitatively explains why the best of performance of RECO for zero-shot retrieval is\nachieved by disabling retrieval on the query image and enabling it on the query text (see Table 4 of\nthe main paper).\nInterestingly, we observe some failure cases when retrieving from a query class name which is\nambiguous in the sense that it can refer to several things. For example, the retrieved images from the\nclass name \u201cprince of wales feathers\u201d in Flowers dataset returns non useful information such as the\nemblem of prince of wales or a picture of a feather. This is because \u201cprince of wales feathers\u201d can\nrefer to many things other than a flower species. We observe this behavior for several class names of\nthe Flowers classification benchmark which have a meaning outside of the flower species they refer\nto; for example \u201cbird of paradise\u201d, or \u201cbishop of llandaff\u201d where one of the retrieved image is from\nthe actual person who used to be the Bishop of Llandaff, a community in Wales.\nB\nEVALUATION DETAILS\nB.1\nEVALUATION DATASETS\nWe report the details of each image classification dataset that we use to evaluate our model. Note that\nwe only use test or validation splits of each of these datasets, training sets are disregarded. Stanford\nCars (\u201cCars\u201d) (Krause et al., 2013) contains 8, 041 test images of 196 fine-grained car classes. Each\n17\nPublished as a conference paper at ICLR 2024\nFigure 6: Qualitative examples of RECO for image classification. We consider several query\nimages and their class names (\u201cquery text\u201d) and show the retrieved items to be fused with them. For a\ngiven a class name, looking for similar captions and using their corresponding images usually returns\nrelevant visual examples. For example, the images retrieved from the class name \u201cdungeness crab\u201d\nshow examples of what dungeness crabs visually look like. However, when the class name can refer\nto several things, for example \u201cbird of paradise\u201d which is both a flower and a bird species, then the\nvisual retrieved examples are not always relevant to the finegrained classification problem at hand.\nclass name consists of make, model and year of a car, e.g. 2012 Tesla Model S or 2012 BMW\nM3 coupe. CUB-200-2011 (\u201cCUB\u201d) (Wah et al., 2011) consists of 5, 794 test images of 200 bird\nspecies. The dataset contains additional annotations, such as part locations, binary attibutes, and\nbounding boxes, but we do not use any of them. Oxford Flowers (\u201cFlowers\u201d) (Nilsback & Zisserman,\n2008) has 6, 149 test images of 102 flower categories commonly found in the United Kingdom. The\ndataset images contain strong visual variations, such as scale, pose, and light changes. ImageNet-1k\n(\u201cIm1k\u201d) (Russakovsky et al., 2015), or also referred to as ILSVRC 2012, contains 50, 000 validation\nimages from 1, 000 classes. Class names are obtained from the synsets in WordNet (Miller, 1998),\nand come from a large variety of generic concepts. Places365 (\u201cPl365\u201d) (Zhou et al., 2017) has\n18\nPublished as a conference paper at ICLR 2024\nTable 9:\nStandard deviation of RECO results. We run five RECO training, each one with a\ndifferent random seed. We show zero-shot image classification and retrieval results, and their standard\ndeviation across 5 runs.\nImage classification\nT\u2192I\nI\u2192T\nMethod\nCars\nCUB\nFlowers Im1k\nPl365\nDogs\nCOCO\nFlickr\nCOCO\nFlickr\nCLIP-B/32 57.2\n52.8\n62.1\n63.5\n40.6\n58.6\n30.2\n61.1\n51.2\n80.9\nRECO\n68.1 \u00b10.3 63.0 \u00b10.3 67.9 \u00b10.4 64.6 \u00b10.1 42.2 \u00b10.1 59.7 \u00b10.2 33.6 \u00b10.1 65.7 \u00b10.3 52.2 \u00b10.3 81.8 \u00b10.3\n36, 500 validation images from 365 generic scene categories. Some examples of class names include\nliving room, cottage, lecture room, pier etc. Finally, Stanford Dogs (\u201cDogs\u201d) (Khosla et al., 2011)\nconsists of 8, 580 test images from 120 dog breeds.\nWe use two datasets for our retrieval experiments. Flickr30k (\u201cFlickr\u201d) (Plummer et al., 2015)\ncontains 1000 image-text pairs. Each image contains 5 sentence-level descriptions, or captions.\nSimilarly, MS COCO (\u201cCOCO\u201d) (Lin et al., 2014) test set, as defined by Karpathy and Li (Karpathy\n& Fei-Fei, 2015), contains 5, 000 image-text pairs, where each image contains 5 captions. We report\nthe performance for text-to-image (\u201cT\u2192I\u201d) and image-to-text (\u201cI\u2192T\u201d) retrieval on both datasets.\nB.2\nEVALUATION PROTOCOLS\nZero-shot image classification. We follow the standard setup (Radford et al., 2021) of embedding\neach class name with the text encoder. We classify an image to the class which has the highest cosine\nsimilarity between the image embedding and the corresping class name embedding. We report top-1\naccuracy in all the image classification benchmarks. There is no variance at zero-shot evaluation time\nsince the inference for both text and vision encoders are deterministic.\nImage and text retrieval. For image-to-text retrieval, given an input image, we rank all the text\nembeddings according to their similarity to this image embedding. We report the proportion of\nimages that ranks the correct text within the first R positions as the recall@R. The process is the\nsymetric for text-to-image retrieval by switching the role of text and image. We report recall@1 in all\nthe retrieval tasks. There is no variance at zero-shot evaluation time since the inference for both text\nand vision encoders are deterministic.\nVariance and error bars. We report the performance variance on our small CLIP-B/32 setting to\nmake sure that observed gains are significant. We train RECO with CLIP-B/32 backbone 5 times\nwith different random seeds. We perform the evaluation for each model separately and report the\naccuracy, averaged over 5 run, with the variance in Table 9. We observe that the standard deviation is\nsmall across 5 runs, always below 0.4 across all benchmarks.\nOVEN benchmark. OVEN benchmark (Hu et al., 2023) is created by combining 14 existing\ndatasets (ImageNet21k-P (Ridnik et al., 2021; Russakovsky et al., 2015), iNaturalist2017 (Van Horn\net al., 2018), Cars196 (Krause et al., 2013), SUN397 (Xiao et al., 2010), Food101 (Bossard et al.,\n2014), Sports100 (Gerry, 2021), Aircraft (Maji et al., 2013), Oxford Flowers (Nilsback & Zisserman,\n2008), Google Landmarks v2 (Weyand et al., 2020), and various VQA (visual question answering)\ndatasets (Goyal et al., 2017; Zhu et al., 2016; Krishna et al., 2017; Marino et al., 2019; Singh et al.,\n2019; Gerry, 2021)) and grounding their categories to Wikipedia entities. The benchmark consists of\ntwo splits. Entity Split measures the image recognition or retrieval capabilities of a model, whereas\nthe Query Split is designed as a VQA task. We focus on the Entity Split in this paper.\nThe Entity Splits contains training, validation, and test splits. However, since we focus on zero-shot\nimage classification in this paper, we ignore the training and validation splits, and evaluate our model\n(trained on CC12M as discussed in Sec. 4) directly on the test set. The test set contains 729, 259\nexamples from 20, 549 entities. Each example belongs to a single entity. Nevertheless, total number\nof candidate entities during inference is 6, 084, 494, i.e. there are more than 6M distractor entities at\ninference.\nNote that unlike other image classification datasets, each example is an image-text pair in OVEN.\nThe so-called intent text accompanies each image, and clarifies the question at hand, e.g. what is the\nmodel of this vehicle? Similarly, each entity is also an image-text pair, containing the entity name\n19\nPublished as a conference paper at ICLR 2024\nand entity image. We simply follow the same protocol as other image classification datasets in this\npaper, and only consider the example image and entity name.\nC\nILLUSTRATIVE COMPARISON OF UNI-/CROSS- MODAL SEARCH AND UNI-/CROSS- FUSION\nWe give a conceptual comparison of uni-/cross- modal search and uni-/cross- fusion for an image\ninput I in the paper. We now show in Figure 7 this comparison for a text input T.\nUni-modal search\nCross-modal search\nUni-modal search\nCross-modal search\n& cross-modal fusion\n& cross-modal fusion\n& uni-modal fusion\n& uni-modal fusion\n(Ours)\nFigure 7: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion. We illustrate\nthe different scenarios for a text input T while the scenarios for image input I are shown in the main\npaper.\nD\nNEAR-DUPLICATE FILTERING\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nCosine Similarity\n0\n2000\n4000\n6000\n8000\nnearest neighbor\nquery image\n0.95 similarity\nnearest neighbor\nquery image\n0.95 similarity\nnearest neighbor\nquery image\n0.95 similarity\nFigure 8: Left: Cosine similarity distribution between the test image queries of CUB2011 and their 10\nkNN. Right: Some of the queries and their nearest neighbors from the memory with 0.95 similarity.\nFigure 8 (Left) of the PDF shows the cosine similarity distribution between the queries and memory\nimages on CUB2011 dataset. It is shown that most of the retrieved items have a similarity around 0.9,\nwhich indicates that most of the retrieved elements are similar but not identical. Note that there is no\ncosine similarity above 0.95. This is because we remove any memory image (and their corresponding\ncaption) if they have a similarity of 0.95 or above with any of the test images. Figure 8 (Right)\nshows some of the kNNs with 0.95 cosine similarity. We see that the retrieved examples are not\nnear-duplicates, but they are conceptually similar as they should be.\nE\nBROADER IMPACT\nWe propose a retrieval-based recognition approach, where we search for similar images and text in\na large-scale memory. Data retrieved from such uncurated sources may be biased against certain\npopulations across the world (Prabhu & Birhane, 2020; De Vries et al., 2019). Furthermore, it is\nimportant that the privileged user data does not exist in such data collections, in order to avoid using\nthe data without the consent of its owner. We acknowledge these potential misuses, and encourage\nthe community to utilize more fair and responsible data collections.\n20\nPublished as a conference paper at ICLR 2024\nretrieved captions 1-5\nhoary \nredpoll\nhoary \nredpoll\nimmature \nfemale \nnorthern \ncardinals\nrose \nbreasted \ngrosbeak\npurple finch\nquery image\nimage-to-image \nsearch\nmale house \nfinch\nbirding not \nsure what's \ngoing on with \nthis house \nfinch any \nthoughts \nmilwaukee wi\npurple \nfinch \ndsc_2669\npinson\nann elias's\nsignature \nteal \nstacked \nbird feeder\nretrieved captions 6-10\nLabel: purple \nfinch\nretrieved captions 1-5\nsaltator \ngros bec\nlazuli \nbunting \npair by \nbrooke\nmiller\nlazuli \nbunting \npair by \nbrooke\nmiller\nwhiskered \ntreeswift\nwhiskered \ntreeswift \noriental bird \nclub image \ndatabase\nquery image\nimage-to-image \nsearch\n\u5317 \u5370\u65db \u6cbc\n\u30eb\u30ea\u30d3\u30bf\u30ad \u306e\n\u753b\u50cf\nlesser \nstriped \nswallow\n\u30c4\u30d0\u30e1\nyellow \nbellied \nflycatcher \nempidonax\nflaviventris\n\u30eb\u30ea\u30d3\u30bf\u30ad\niso1600\nretrieved captions 6-10\nLabel: lazuli \nbunting\nretrieved captions 1-5\nhouse finch\nbeautiful \npink colored\nbird brown \ncapped rosy \nfinch\nchechetka\n\u0447\u0435\u0447\u0451\u0442\u043a\u0430 \nacanthis\nflammea \nredpoll\ngray crowned \nrosy finch \ntaylor\nabbott;grey\ncrowned rosy \nfinch\nhouse finch \nemily gorda\nquery image\nimage-to-image \nsearch\ntallbit\nall about \nbirds \npurple \nfinch\nred \ncrossbill\nhouse finch\n\u0437\u044f\u0431\u043b\u0438\u043a \n\u0441\u0430\u043c\u0435\u0446;\u043e\u0431\u044b\u043a\u043d\n\u043e\u0432\u0435\u043d\u043d\u044b\u0439 \n\u0437\u044f\u0431\u043b\u0438\u043a\nretrieved captions 6-10\nLabel: gray\ncrowned rosy \nfinch\nFigure 9: Noisy examples. We display image queries and their labels on the left panel and 10\nretrieved captions and images on the right panel. Retrieved captions are in green if they contain useful\ninformation, red otherwise. RECO learns\nF\nNOISY EXAMPLES\nWe show the 10 retrieved captions for some of the test queries on the CUB2011 dataset on Figure 9.\nWe observe that RECO predicts the correct class even if the retrieved captions contain irrelevant\nexamples. This is because the Transformer module learns to aggregate the retrieved captions,\nimplicitly keeping the relevant ones and disregarding the irrelevant ones. Note that Webli contains\ncaptions in languages other than English, which are considered noisy for CLIP, because it is trained\nin an English corpus.\n21\n"
  },
  {
    "title": "Weakly supervised information extraction from inscrutable handwritten document images",
    "link": "https://arxiv.org/pdf/2306.06823.pdf",
    "upvote": "4",
    "text": "Weakly supervised information extraction from\ninscrutable handwritten document images\nSujoy Paul, Gagan Madan, Akankshya Mishra, Narayan Hegde, Pradeep Kumar,\nand Gaurav Aggarwal\nGoogle Research\nAbstract. State-of-the-art information extraction methods are limited\nby OCR errors. They work well for printed text in form-like documents,\nbut unstructured, handwritten documents still remain a challenge. Adapt-\ning existing models to domain-specific training data is quite expensive,\nbecause of two factors, 1) limited availability of the domain-specific\ndocuments (such as handwritten prescriptions, lab notes, etc.), and 2)\nannotations become even more challenging as one needs domain-specific\nknowledge to decode inscrutable handwritten document images. In this\nwork, we focus on the complex problem of extracting medicine names\nfrom handwritten prescriptions using only weakly labeled data. The data\nconsists of images along with the list of medicine names in it, but not their\nlocation in the image. We solve the problem by first identifying the regions\nof interest, i.e., medicine lines from just weak labels and then injecting a\ndomain-specific medicine language model learned using only synthetically\ngenerated data. Compared to off-the-shelf state-of-the-art methods, our\napproach performs > 2.5\u00d7 better in medicine names extraction from\nprescriptions.\nKeywords: handwriting \u00b7 language model \u00b7 prescription \u00b7 weakly-supervised\n1\nIntroduction\nOptical character recognition (OCR) enables the translation of any image con-\ntaining text into analyzable, editable and searchable format. Over the last decade,\nmany large scale models [10,18,26] and sophisticated techniques [4,5,29] have\nbeen developed with neural network based architectures for OCR. These systems\nare not only limited to printed text but also work quite well on handwritten text,\nas they are trained on large amount of labeled as well as synthetic handwritten\ndata. In the past, there have also been works around developing domain specific\nOCR models [6,21,41]. Most of these works develop these models for generic text\nlines [20,31], and require meticulously labeled data for learning. In this work, we\nprimarily focus on how we can improve the quality of existing OCR models on\nvery hard to read, unstructured documents for specific entities of interest, with\nan application in handwritten medical prescriptions.\nIn many countries, prescriptions are primarily delivered to patients in hand-\nwritten formats by doctors. A few billion prescriptions are generated every year\narXiv:2306.06823v1  [cs.CV]  12 Jun 2023\n2\nS. Paul et al.\nMedicines: Gluconorm, Forenza, \nMedrol, Formonide, Baro\nMedicines: Litec, Deswell, Calamine, \nAlbendazole, Flutec\nMedicines: Monocef, Efcorlin, Foracort, \nDuolin, Corex, Doxy, Ivermectol\nFig. 1: Samples representative images from the prescription dataset used in this\nwork. As we can see the handwriting is often inscrutable and does not follow\nany specific structure or format. The task we focus in this paper is to extract\nmedicine names from such images.\nworld-wide [19]. Digitizing them would unlock numerous applications for many\nstakeholders and use cases in the healthcare ecosystem like e-pharmacies, in-\nsurance companies, creating electronic health records necessary for preventive\nhealthcare, better diagnosis, analysis at a local and global level for policy making\nand so on. However, most of such documents, as shown in Figure 1 are often hard\nto read for non-pharmacists [33]. Even pharmacists go through months/years of\ntraining to decipher such prescriptions. Existing state-of-the-art OCR models\nthough trained on large amount of data, do not perform well on such inscrutable\ndocuments. Procuring large domain-specific datasets is not a cost-effective or\nscalable solution, as it involves annotation that too from domain experts which\ncan become quite expensive. Although there have been some works [1, 15, 34]\nin extracting information from handwritten prescriptions, the algorithms are\nnot generalizable, heavily hand-tuned and lack rigorous evaluations. With these\nproblems in mind, we propose an approach that can significantly enhance the\nperformance of existing state-of-the-art OCR systems by selectively infusing\ndomain knowledge using only weak supervision.\nMedical prescriptions consist of various information like data from lab reports,\nordered tests, health vitals, observations along with medicine names. Our work\nfocuses on the medicine section which is considered the most important from a\nconsumer standpoint, but the techniques can be similarly applied to other sections\nor other types of documents beyond prescriptions, such as printed forms filled\nwith handwriting. The medicine section of a prescription has a rough semantics\nconsisting of medicine name, category, frequency of intakes and quantity (see\nFigure 1). As these are non-form type of documents and quite unstructured, it is\na challenge to extract medicine name entities from such documents.\nMost OCR approaches [18,26] take a two step approach - first localize the text\nregions by detecting bounding boxes around them, and then recognizing the text\nusing line recognition models. The recognition model often consists of an optical\nrecognizer and a language model (LM) to correct the optical model errors. The\nWeakly supervised information extraction\n3\nLM gives us the flexibility to infuse domain-specific knowledge. But, injecting\nsuch knowledge to all lines in the document may not be optimal, as different\nparts of the document can correspond to different entities, or even domains. For\nexample, the pattern in which a medicine name is written is very different from\nthe pattern in which normal text such as observations are written in the same\nprescription. Thus, in order to enhance the recognition of medicine names and\nextract them from the prescription, we first detect lines where medicine names\nare written. Then in the recognition model, we inject a LM which is specific to\nmedicine names. For the rest of the image, we inject the vanilla LM.\nNote that to learn the model which detects medicine lines, we do not use\nstrong bounding polygon labels, but rather only weak labels, i.e., the medicine\nnames present in the image. Such weak labels are much easier to obtain, as the\nannotators do not need to draw a bounding polygon and often labeling comes\nfor free, for example, when a medicine bill is paired with a prescription. Apart\nfrom that, to learn the medicine LM, we do not use any annotated text lines, but\nrather generate synthetic text lines using a probabilistic programming approach.\nOur weakly supervised medicine line detector obtains 78% pixel mIoU with just\nweak labels, and helps to selectively infuse medicine LM, which in turn improves\nthe overall performance from 19% to 48% jaccard index. The main contributions\nof this work are:\n\u2013 Develop a weakly supervised segmentation method to detect specific text\nentities, such as medicine names in handwritten prescriptions.\n\u2013 Learning a domain-specific medicine LM using synthetic medicine name lines\ngenerated by probabilistic programs and using it to enhance the performance\nof state-of-the-art OCR models.\n\u2013 A model dependent unique way of enhancing the performance of matching\nwith words from the vocabulary.\n2\nRelated Works\nOptical Character Recognition OCR literature has seen tremendous im-\nprovements in the past decade. The successes [10, 18, 26] can be attributed\nto sophisticated models, synthetic data generation, various augmentation tech-\nniques, among others. An OCR system is made of multiple models, starting from\ntext detection [29,30,43], script identification [12,17], and finally line recogni-\ntion [3,10,26,27]. Even with all these advancements, recognition of handwritten\nlines still remains a challenging task as writing style can be a unique signature\nof the person, allowing room for huge variations. In our experiments, we found\nthat off-the-shelf line recognition models, even though perform quite well for a\nlot of printed and handwritten datasets, they fail to perform equally well on\nhandwritten images. In this work, we show how we can improve their accuracy\nby more than 2 times the baseline by first detecting specific entities of interest\n(rather than detecting all text) and then improving the line recognition model by\ninjecting domain-specific LMs. We next discuss the existing literature around\nthese topics.\n4\nS. Paul et al.\nWeakly-supervised Detection Detecting specific entities of interest in an\nimage can be posed as detection or segmentation task. However, to learn these\ntasks, traditional methods would need strong labels, i.e., either pixel-wise [9,30]\nor bounding box labels [30,37,38]. In the recent past, there has been a lot of work\nin developing methods which can learn from only weak labels, such as weakly-\nsupervised object detection [25,47], segmentation [22,44], action detection [32,46],\netc. These methods do not need access to strong labels such as bounding boxes,\nbut can learn from just weak labels, i.e., image-level labels of the object categories\npresent in the individual training images. Such a formulation reduces the manual\nlabor needed to acquire strong labels, thus making it scalable to large datasets.\nMotivated by these, we aim to learn a segmentation model to detect entities\nof interest in an image, such as medicine names from just weak labels, i.e., list of\nmedicine names given an image. In this use case, the individual entities do not\ncorrespond to any underlying category unlike segmentation or detection of objects\nin natural scenes. Recently, it has been shown [23] that using weakly labeled data\nalong with strong labels improves the performance of scene text recognition. In\nour task, we only have weakly labeled data without any strong labels (synthetic\nor real) and the text is primarily handwritten which is often inscrutable even\nif text detection is perfectly done. Moreover in our use case, we need to detect\nspecific entities among other cluttered text, and not any generic text. There are\nalso works on defining rules to derive weak labels from the data [36]. While that\nis quite challenging and not generalizable in our use case, we use the intuition to\nconvert the weak labels to strong labels via labeling functions.\nDomain-specific Language Models There has been a lot of work [24,35,45]\nwhich shows that injecting domain-specific knowledge in LMs helps to perform\nmuch better on those domains than models developed on generic text. Specifically\nfor OCR, there have been some works [11, 14] showing that having access to\ndomain related text data helps to adapt existing LMs and thus improving final\nOCR performance. However, in our use case of decoding medicine names, it is\nnon-trivial to acquire lines of medicine names written by doctors, as they are\nhardly available in normal text corpus. To solve that, we use domain knowledge to\ndefine a probabilistic program which can take in the medicine name and generates\npatterns of medicine lines as would be written by doctors in prescriptions. We\nshow that using such a LM in the OCR decoder improves the performance\nsignificantly.\n3\nMethodology\n3.1\nProblem Statement\nIn this work, we focus on the problem statement of extracting textual entities\nfrom non-form type handwritten document images, which are often hard to read.\nWe specifically focus on the challenging problem of extracting medicine names\nfrom handwritten prescriptions as shown in Figure 1. Formally, given an image x,\nWeakly supervised information extraction\n5\nTraining\nInference\nWeakly-\nlabeled data\nOCR Labeling \nFunction\nStrongly\nauto-labeled \n24% of dataset\nSegmentation \nLabeling Function\n90% of dataset\nStrongly\nauto-labeled \nPseudo-Labeling\nSegmentation \nNetwork\nOCR Pipeline\nSynthetic Data\nMedicineLM\nSegmentation Model\nLanguage Model\nFinal Predicted \nMedicines\nSynthetic Data Generation\nMedicineLM\nFig. 2: Training and inference pipelines for medicine name extraction from prescriptions.\nThe top-left block shows the weakly supervised medicine line segmentation pipeline.\nThe top-right block shows the process of generating synthetic medicine lines using\nprobabilistic programs and then using it to train a medicine LM. The bottom row shows\nthe inference pipeline, that first localizes the medicine names using the segmentation\nnetwork, and then injects the medicine LM while decoding the OCR outputs.\nthe output of the framework should be the medicine names {mj}n\nj=1 that appear\nin the image, where mj \u2208 V, the vocabulary of medicines. n denotes the number\nof medicines in the prescription that varies from prescription to prescription. The\ntraining data that we use to solve this problem is only weakly labeled, i.e. for\nevery image, we have a list of medicine names that appear in the image, and not\ntheir bounding box locations. Thus, our training data contains tuples of image\nand unordered set of medicine names as follows, D = {(xi, Gi = {mj}ni\nj=1)}N\ni=1,\nwhere ni denotes the number of medicines in that image, N denotes the number\nof images in the training data and Gi is the ground truth list of medicines.\n3.2\nOCR Line Recognition Model\nMost line recognition models have two parts - the encoder, often called the optical\npart of the model, which encodes the visual information, and the decoder, which\nis either trained end-to-end with the encoder, or CTC type decoder [13] where\nthe encoder outputs are combined with LM scores to obtain the final text. We\nuse the second option and train our network with CTC loss [13]. This allows us\nto decouple the optical and the LM, and replace it with domain specific LMs.\nEncoder: The encoder or the optical part of the line recognizer consists of first\n7 layers of inverted bottleneck conv layers [39] with 64 filters and stride of 1,\nfollowed by 12 layers of transformer encoder [42] with hidden size of 256 and 4\n6\nS. Paul et al.\nattention heads, and finally a fully connected symbol classification head. We use\nthis backbone from [10], as it achieves state-of-the-art performance on various\ndatasets. Our pre-trained model is also the same as [10]. It is interesting to note\nthat our method is agnostic to the encoder used as it can be used to boost the\nperformance of any OCR backbone.\nDecoder: We use a CTC decoder [13] following [10], which combines scores from\nthe encoder logits and a character n-gram LM. We set n = 9 unless otherwise\nmentioned. We will discuss how we train and use a medicine LM subsequently.\n3.3\nWeakly Supervised Line Segmentation\nWe next discuss our algorithm to detect medicine lines by just using weak\nlabels while training, i.e., only the medicine names for every image, and not\ntheir bounding polygons. Note that while we use this method for medicine line\ndetection, it can be also used for detecting other entities in other document types.\nLabeling Functions At the core of our algorithm is the idea of using labeling\nfunctions to automatically convert a weakly labeled dataset to strongly labeled.\nThere have been some works [36] in literature where rules are defined as labeling\nfunctions. The labeling functions may not be as perfect as a human oracle and the\nstrong labels they generate may have errors in them. There are often thresholds\nor rules used to reduce errors. Thus, while defining a labeling function we need to\noptimize coverage, which is the number of data points that can be labeled using\nsuch labeling functions and their error rate. Although there can be some noise\nin such labeling, this significantly reduces the annotation cost. We sequentially\napply two labeling functions, as discussed next to convert a list of medicine\nnames to bounding boxes. In our use case of assigning a bounding box to each\nmedicine name, we can consider it as an assignment problem between the detected\nbounding boxes (p) by a generic text detector and the number of medicines in\nit n. Considering p = 50 and n = 5, the number of possible assignments turns\nout to be pCnpPp \u2248 2.5e8. We solve this problem via two techniques - using the\ncontent of the boxes (via OCR Labeling Function), and using the visual features\n(via Segmentation Labeling Function).\nOCR Labeling Function: As for every image, we have the list of medicines\nthat appear on it, for each detected word in the image, we can naively find\nthe closest medicine name (by edit distance) from the ground truth list, albeit\napplying a threshold. However, directly using the edit distance may not respect\nthe model\u2019s predictions. For example, according to the OCR line recognition\nmodel, modifying an i to l may have lower cost than i to z, but it would be\nthe same edit distance for both the cases. Thus, in order to utilize the model\u2019s\npredictions, we decode up to the top-k predictions, and stop when we find an\nexact match with a medicine name from the list of ground truth medicines, i.e.,\nWeakly supervised information extraction\n7\nthe weak labels. The bounding box associated with these matched words then can\nbe used as the ground-truth bounding boxes of medicine names. We can define the\nlabeling function as F(x) = {(tj, lj, hj, wj, rj)}q\nj=1, where the bounding boxes of\nm medicines are in the rotated box format and tj, lj, hj, wj, rj representing top,\nleft, height, width, and rotation angle of each matched bounding box. Then, we\ncan construct a training dataset as follows: Dtr = {(xi, F(xi))}N\ni=1.\nThe number of matching bounding boxes qi \u2264 ni, as in most cases the\nhandwriting is so illegible that to decipher that even a higher number of top-k\nlines may not allow a match with the ground truth medicine names. This can\nhappen for a sizable number of images, which in turn can introduce a significant\nnoise in the data, leading to problems in learning the segmentation network.\nThus, we only use those images to train our network where we find that at least\n90% of the ground truth medicines have been matched. The reason behind setting\nsuch a high threshold is this set becomes the guiding signal for the rest of the\nalgorithm. Thus our modified strongly-labeled training dataset can be represented\nas: Dtr = {\n\u0000xi, F(xi)\n\u0001\f\f\f |F(xi)|\n|Gi|\n\u2265 0.9}N\ni=1. While increasing the number of top-k\npaths helps more images to pass this threshold, we find that it saturates after a\npoint, specially for documents which are hard to read, such as prescriptions used\nin this work. While the 0.9 threshold allows us to reduce missing bounding boxes\nin the training set, it also reduces the number of images in the training set, as\n|Dtr| \u2264 |D|. We next discuss a second labeling function to alleviate this problem.\nSegmentation Labeling Function It may happen that even after decoding\na high number of paths (k), we still are not able to match all the ground truth\nmedicine names. This can happen when the handwriting is quite challenging\nfor the model to predict accurately. In such a scenario, we leverage the visual\nappearance features via the segmentation model itself, rather than just labeling\nvia OCR. Motivated by the success of self-training in domain adaptation [2,28]\nand semi-supervised [7,40], we use the segmentation model to pseudo-label the\nimages in the rest of the dataset, i.e., D - Dtr.\nFirst, we train a segmentation network M using the relatively small training\ndata Dtr obtained from the OCR Labeling Function outlined above. Then, we\nuse it to predict the medicine lines on the images in D - Dtr. We can consider\nthe output of the model to be M(x) = {(tj, lj, hj, wj, rj)}l\nj=1. Following our\nprevious threshold, we add those images to the training dataset, where the union\nof the number of predicted medicine lines by the segmentation network and the\nOCR labeling function above, is at least 90% of total number of medicines in that\nimage. We can represent the new training set as follows: Dtr = {\n\u0000xi, F(xi) \u222a\nM(xi)\n\u0001\f\f\f |F(xi)\u222aM(xi)|\n|Gi|\n\u2265 0.9}N\ni=1.\nIdeally, we can repeat this process, i.e. repeat pseudo-labeling the training\nimages using a trained segmentation model and training a new model with the\npseudo-labeled training set. The training set would grow over iterations. The two\nlabeling functions can be generalized as: DT\ntr = {\n\u0000xi, \u222aT\nt=1Mt(xi)\n\u0001\f\f\f \u222aT\nt=1Mt(xi)\n|Gi|\n\u2265\n8\nS. Paul et al.\n(a) Iter 1\n(b) Iter 2\n(c) Iter 3\nFig. 3: Evolution of labels from the labeling functions. Iter 1 represent the OCR Labeling\nFunction and the subsequent ones represent the Segmentation Labeling Function for\ndifferent iterations. The green highlighted regions denote the detected medicine names.\n0.9}N\ni=1, where Mt = F for t = 1, and the tth medicine line segmentation model\nfor t \u2265 1, and T represents the total number of iterations.\nFigure 3 shows how segmentation improves over iterations. Using only the\nOCR Labeling Function misses out some of the medicine names, as it is dependent\non the ability of the underlying OCR model we use to decipher the medicine\nnames. However, applying the Segmentation Labeling Function on top of it helps\nto predict the medicine patches which were missed, as it does not depend on OCR\nor the content, but rather on the visual features, such as strokes, indentation,\netc. which we will discuss later in Section 4.\nSegmentation Model Given the bounding boxes obtained using the labeling\nfunctions, we can train a medicine line segmentation model. Our segmentation\nmodel is DeepLab [9] with a ResNet50 backbone [16]. Although we use this\narchitecture, it can be replaced by any other state-of-the-art segmentation model.\nWe convert the bounding boxes to label masks, and use them as supervision to\ntrain the segmentation network. The label mask has either 0 or 1 at each pixel\nlocation, denoting whether a pixel belongs to a medicine line. The segmentation\nmodel is trained with the above data using a semantic head with two output\nchannels. The predicted medicine label masks obtained from this model may not\nalways respect text boundaries, and hence we use a generic text detector in the\nOCR pipeline to detect text and refine the boundaries. Then, we crop out the\ndetected bounding box from the original image x and send only those lines to\nthe line recognizer. As these lines correspond to a special domain of medicine\nnames, we can inject that knowledge to the OCR using a LM.\n3.4\nDomain-specific Language Model\nIn OCR decoder, we can incorporate a LM to correct some of the OCR errors.\nSpecifically, the decoded string Y \u2217 can be obtained as follows:\nY \u2217 = arg max\nY\nP(Y |X)P(Y )\u03b1\n(1)\nWeakly supervised information extraction\n9\nwhere P(Y ) is obtained from the LM denoting the probability of occurrence of a\ncertain string Y in the dataset, \u03b1 is the weight applied on the LM, and X is the\ninput. In a generic OCR model, P(Y ) is trained on a large corpus of text such\nthat it represents a diverse set of documents. Particular to our use case, once we\nhave detected the medicine lines as discussed in the previous section, we need\nonly medicine line specific knowledge while decoding the OCR output. However,\nmedicine line patterns occurring in handwritten prescriptions often do not appear\nin normal text. It is also difficult and expensive to acquire and annotate such\nlarge corpora of handwritten prescriptions from which we can learn medicine line\nspecific LMs. We inject domain knowledge to solve this problem.\nIn order to gather medicine line specific text data, we defined a probabilistic\nprogram from which we can sample data and learn a character based LM. Medicine\nlines written by doctors often have a few elements - a enumeration token (-, .,\nnumbers, etc), followed by the type of medicines (injection, tablet, etc.), the root\nname of the medicine, and then the suffix. These altogether comprise a single\nmedicine name line. Note that some of these entities other than the root word\nmay not appear in all prescriptions. With this domain knowledge, we can define\na probabilistic program as shown in top-right portion of Figure 2. The program\nstarts from the START node and ends at the END node, and concatenates the\noutput of each node with spaces in between. To sample a medicine name line, the\nprogram takes as input the medicine name and the type of the medicine, both of\nwhich appears in the vocabulary of medicines. We can create an exhaustive set\nof all possible medicine name lines, and then train a character based n-gram LM\non that text corpus. Note that as we do not have the exact probabilities of the\ndifferent transitions, we use equi-probable transitions between nodes, as well as\nfor any choices in the nodes.\nIn OCR, as decoding is done at a character level, we need character LMs,\nunlike recent advanced large LMs which operate on word or sub-word tokens.\nThere are also character LM using transformers, but those are generally useful for\nlonger context. But, in our case, medicine names on average are only 7 characters\nlong. Moreover, using such a large model takes a lot more inference time. Hence\nwe stick to an n-gram model.\n3.5\nIn-Vocabulary Prediction\nIn many entity extraction tasks, such as medicine name prediction studied in this\npaper, the entities often belong to either from a fixed vocabulary, or are defined\nby a regular expression. However, the OCR predictions will not be constrained to\nour medicine vocabulary. To constrain that, we can make a nearest neighbor edit\ndistance search for each medicine line text and the medicine vocabulary. However,\nas we discussed before, it would not respect the model\u2019s confidence. Thus, we use\nthe top-k path decoding as a robust method. Specifically, for each line, we decode\nthe top-k predictions, and then find all the text which have an exact match with\none of the medicine names from the vocabulary. Then, we take a majority voting\nof all these matched names, and that becomes the prediction for every line. It is\npossible that for some of the detected medicine lines, we do not find any match\n10\nS. Paul et al.\nTable 1: (a) Statistics of the prescription dataset. (b) Coverage of different sections in\nprescriptions.\n(a)\n# Images\n9645\n# Doctors\n117\nAvg. medicines / image\n4.5\nAvg. images / doctor\n82.4\n(b)\nLab/Scan\n70.4%\nMedicine\n100%\nObservation\n99.9%\nVital\n40.5%\nfor any of the top-k prediction. These detected medicine lines would not have\nany output prediction. We find this method to be more effective compared to\nedit distance based matching with the top-1 prediction, or predicting only the\nfirst match from the top-k predictions, as shown in Section 4.\n4\nExperiments\nWe first introduce the dataset and implementation details, before sharing the\nexperimental results and rigorous ablations to understand the efficacy of the\nframework.\nPrescription Image Dataset: We use a dataset of handwritten prescriptions\nto validate the methodology outlined and evaluate the performance of the models.\nA few example images from the dataset are shown in Figure 1. The dataset\ncontains 9645 images written by 117 doctors. Table 1a outlines some of the\ndetails of the dataset, and Figure 4a shows the distribution of prescription\nimages per doctor. We use 80% of the dataset to train our models, and 20% for\nevaluation. There is no overlap between the doctors between the training and the\ntest set at each iteration, ensuring that our results capture understanding across\ndifferent handwriting styles. Each image in the dataset has a list of medicine\nnames appearing in them, which we call weak labels, without any positional\ninformation. However, just for evaluation, we strongly annotate 500 images\nfrom the evaluation set to evaluate the segmentation performance. Prescriptions\ngenerally have multiple other sections as well (although unstructured in free-\nform), and Table 1b shows the percentage of images which have other sections\nsuch as lab/scans reported, observations and vitals. Also, note that any and all\npersonally identifiable information was removed from the data prior to it being\nprovided to the authors for this study.\nMedicine Vocabulary: We also use a medicine name vocabulary consisting of\nmore than 90,000 medicine names. We use this to generate synthetic medicine\nname lines and train the character based medicine LM. This vocabulary is also\nused to make the in-vocabulary predictions.\nWeakly supervised information extraction\n11\n(a)\n(b)\nFig. 4: (a) This plot shows the number of prescriptions per doctor in the dataset, (b)\nThis plot shows the number of doctors per specialty.\nEvaluation Protocol: We evaluate all models on test set of the dataset men-\ntioned above. To evaluate the performance of the segmentation model, we use\nmean intersection over union (mIoU) as used in the segmentation literature [8].\nTo evaluate the performance of the end-to-end medicine name prediction model,\nwe use the mean jaccard index, over all the images. We also use two other metrics\nnamely mean precision and mean recall, and the mean jaccard index can be\nconsidered as a combination of both these metrics. These are defined as follows -\nMean Jaccard Index (mJI) = 1\nM\nM\nX\ni=1\n|Pi \u2229 Gi|\n|Pi \u222a Gi|\n(2)\nMean Precision (mP) = 1\nM\nM\nX\ni=1\n|Pi \u2229 Gi|\n|Pi|\n,\n(3)\nMean Recall (mR) = 1\nM\nM\nX\ni=1\n|Pi \u2229 Gi|\n|Gi|\n(4)\nwhere Pi, Gi are the predicted and ground truth list of medicines for the ith image.\nM is the number of evaluation images. The comparison between the prediction\nand ground-truths are not case-sensitive, as they are medicine names.\n4.1\nResults and Ablation Studies\nIterative Training Performance: As discussed in Section 3, our algorithm\nfor converting weak labels (only medicine names) to strong labels (bounding box\nannotations for each medicine name) involves two labeling functions - OCR and\nSegmentation Labeling Function, where the latter can be applied iteratively. The\nnumber of images auto-labeled by the labeling functions increases with iterations,\nand hence the performance of both the medicine line segmentation model as well\nas the medicine name prediction model increases with subsequent iterations. We\nhighlight this in Table 2. Iteration 1 shows the performance on only OCR Labeling\nFunction, and Iteration \u2265 2 shows the performance on multiple iterations of\n12\nS. Paul et al.\nTable 2: Performance over iterations of the proposed framework. Iter 1 represents\nlearning from only the OCR Labeling Function and iter \u2265 2 shows the performance\nafter iteratively including the Segmentation Labeling Function. The medicine name per-\nformances are only for topk=1. GT bbox shows the performance when the groundtruth\nbounding boxes are provided for medicine names only during evaluation.\nIteration\n1\n2\n3\nGT bbox\nTrain data (%)\n24.4\n66.3\n90.2\n-\nSegmentation (mIoU)\n72.6\n77.9\n77.2\n100%\nMedicine Name (mJI)\n44.8\n45.9\n45.9\n49.8%\nSegmentation Labeling Function. For a significant number of prescriptions, it is\ndifficult to decipher some of the medicine names, even when we use a high value\nof top-k (k=20,000 in our experiments) decoded outputs per line. For Iteration 1,\nthe number of auto-labeled prescriptions is < 25% of the training set. This shows\nthe difficulty level of the problem at hand. Note that the train sets are used to\ntrain only the medicine line segmentation model and not the lines recognizer of\nthe OCR, thus it can be with any off-the-shelf OCR model.\nThe segmentation performance as well as the medicine name performance\nimprove over iterations but saturates from Iteration 3. Note that mIoU computes\nthe performance for every pixel, but normally a small change in the final bounding\nbox do not have a lot of impact on the medicine name prediction, as long as they\nencapsulate the text within it. We also show the upper bound performance of\nmedicine line recognition by using ground-truth medicine bounding boxes only\nwhile evaluating. As we can see, our algorithm with just using weak labels can\nreach within a few points of the strongest upper-bound with strong labels.\nCues for medicine name segmentation: Unlike a generic text detector,\nspecifically detecting medicine lines can be challenging, as handwritten prescrip-\ntions do not have any specific structure or location in the page. However, the\nsegmentation model is still able to predict the location of the medicine lines\nwith high performance as shown in Table 2. In order to understand the cues the\nsegmentation model uses to segment the medicine names, we do the following\nexperiment. Given a test image x, using a sliding window, we remove square\npatches from the image to remove potential cues, one at a time. Consider xi,j as\nthe image when patch at location (i, j) is removed. We can run the segmentation\nmodel on this image, M(xi,j) and obtain the mIoU. For every location (i, j), on\nthe image, we can obtain the model\u2019s performance drop when a patch around that\nis removed, and then display that as a heatmap. A drop in performance in certain\nregions of this image depicts the regions necessary for the segmentation model\nto segment the medicine names correctly. As we can see in Fig. 5, the model\nis clearly utilizing cues from visual features surrounding medicine lines such as\nstarting of a line like Tab, Cap, hyphens, etc. These observations are aligned\nWeakly supervised information extraction\n13\n(a)\n(b)\n(c)\nFig. 5: Cues needed by the segmentation network. Deeper color denotes lower perfor-\nmance when a patch around that is removed. A few parts of the image other than the\nmedicine names, such as hyphens, Tab, Cap, etc., also appear to be darker, which are\nsome of the cues that the model looks at to determine whether it is a medicine line.\nwith what a pharmacist or even non-domain experts look to determine medicine\nlines, as in most cases the handwriting is illegible. These key demarcations serve\nas strong signals to recognize medicine lines, after which we can condition our\nknowledge to medicine names to enhance line recognition.\nContribution of medicine LM and segmentation model: Here we show\nhow selectively injecting medicine LMs can offer a significant improvment in\nperformance. The vanilla LM is trained on a generic corpus of text from the\nLatin script. However, the medicine name LM is trained as discussed in Section\n3.4. The performance improves with path length for both the models but for the\nmedicine LM, the top-1 path itself performs much better than top-1000 path\nfor the vanilla LM (Fig. 6). This also reduces the compute time in decoding the\ntop-k paths from the logits, which is linear in the number of paths.\nMoreover, segmenting and selectively injecting the LM plays a critical role on\nthe performance, and MedLM + Segmented Lines perform the best. Applying the\nMedLM on the full image actually reduces the precision significantly, but improves\nthe recall slightly as expected, but reducing the overall metric, i.e., jaccard index.\nThis shows that selectively injecting the LM is important, otherwise it can mess\nup the rest of the prescription, and hallucinate medicine names from them.\nPerformance with varying weight on LM: The weight \u03b1 in Eqn 1 on the\nLM scores can have an impact on the final performance. A low weight may lead\nto no improvement beyond the optical model\u2019s prediction, and a high weight\nmay not ground the output to the actual text on the image. Figure 7a shows an\nablation of the medicine name prediction performance on the LM weight. Note\nthat the changes in performance is much lower for top-10k paths than for top-1\npath, as only the first path in the top-10k path is affected by the LM because for\npaths > 1, the predictions come from the top-k decoded paths which is based on\nonly the logits without any LM scoring. Nonetheless, we see that the performance\nof both the models are very close after a certain value of \u03b1.\n14\nS. Paul et al.\n(a) Jaccard Index\n(b) Precision\n(c) Recall\nFig. 6: Jaccard index, precision and recall comparison using different language models\nand inputs (medicine line segmented and full page). The medicine LM on segmented\nmedicine lines works the best, the top-1 of which is better than the top-1000 of the\nvanilla LM. Applying the medicine LM on the entire image decreases the precision of\nthe predictions, as it hallucinates medicine names in the rest of the prescription.\n(a)\n(b)\nFig. 7: (a) Ablation of performance with weight on the language model \u03b1. \u03b1 = 0 denote\nthe performance of only the optical model. (b) Ablation of fraction of medicine names\nused to train the medicine language model. We present the performance when top-1\nand top-10k paths are used to predict after vocabulary matching.\nVarying the vocabulary of the LM: The medicine names used in generating\nthe synthetic lines can have an impact on the quality of the medicine name LM.\nHere we also show how the performance varies as we increase the number of\nmedicine names used to train the medicine LM. Figure 7b presents the results for\ntop-1 and top-10k with different size of medicine name dataset. The performance\nimproves as we add more medicines, but starts saturating after a certain point.\nPerformance with different n-gram models: The n-gram LM involves a\nparameter n, which is the number of history characters the model looks to obtain\nthe score of the next character. We created multiple n-gram models on the\nsynthetically generated medicine line text data, and show the results in Table 3.\nMore context definitely helps in performance, but it saturates after n = 7. This\nis also intuitive as the length of the medicine names is around 7.9 on average.\nPredicting In-Vocabulary Words: In the final step of our algorithm to predict\nmedicine names, we only predict those words where we find a direct match with\nWeakly supervised information extraction\n15\nTable 3: Ablation of different n-gram models trained on medicine line data.\nn=3\nn=5\nn=7\nn=9\nTop-1 (mJI) (%)\n27.2\n41.5\n45.9\n45.9\nTop-10k (mJI) (%)\n47.4\n48.1\n48.7\n48.7\nTable 4: Ablation of different algorithms to predict medicine names. We use k = 1e4.\ntop-1\ntop-1-edit\ntop-k\ntop-k+majority\nJaccard Index\n45.9\n45.8\n46.9\n48.7\nPrecision\n76.9\n68.6\n64.8\n66.8\nRecall\n51.0\n54.4\n58.5\n59.5\none of the elements of the medicine vocabulary. As discussed before, finding\na match for only the top-1 prediction may not be the best. Thus, we decode\nuntil top-k and find matches for all the text. As the top-k decoding is directly\ndependent on the output of the model, such a matching respects the model\u2019s\npredictions. We then take a majority voting of all the matches and that becomes\nthe final predicted medicine for a line. Note that some lines may not have any\nprediction at all. In this section, we compare multiple strategies of predicting\nin-vocabulary words in Table 4. Top-1 represents an exact match with the first\npath, top-1 edit distance finds the nearest prediction from the vocabulary by edit\ndistance, top-k denotes we decode the top-k outputs but stop when we find the\nfirst exact match, and finally top-k+majority is the algorithm we use, where we\ndecode all the top-k lines and take a majority voting of all the exact matches.\nNote that top-1-edit has the same jaccard index as top1, but the former has\nlower precision with higher recall than top-1, as expected, because it predicts\nbeyond exact matches. We tried with multiple thresholds for edit distance, and\nfound that 85% normalized distance performs the best. Increasing the threshold,\ni.e., allowing more matches significantly reduces the precision, at the gain of the\nrecall, but hurting the overall performance. This is because of the intuition we\ndiscussed earlier that topk decodings respect the model\u2019s confidence, but edit\ndistance treats every replacement with the same cost.\n4.2\nError-mode analysis\nThe two types of errors possible are - medicine names predicted but not in the\nground-truth (type I) and medicine names in the ground-truth but not predicted\n(type II). In our framework, there are two reasons behind the errors - segmentation\nnetwork and OCR. If a medicine name is not segmented, then it leads to a type-II\nerror. OCR errors contributes to the rest (type I and type II), a majority of\nwhich is contributed by misinterpreting very similar looking medicines such as\n16\nS. Paul et al.\nemtel vs entel, eenosol vs eenasof, folvite vs folite, paro vs baro, zincovit vs\nzincort, aloliv vs alcoliv. Also we observe that the doctor can commit spelling\nmistakes, or vaguely write a medicine name, where only the first few characters\nare recognizable. To correct such errors, pharmacists generally use other contexts\nsuch as observation. Learning such contexts would need a lot more data, and\ninjecting higher-level domain knowledge.\n5\nConclusion\nIn this paper, we looked into the problem of extracting medicine names from\ninscrutable handwritten prescriptions. Our algorithm can selectively infuse do-\nmain knowledge to specific portions of a document to significantly improve the\nperformance. We developed a framework that can learn to detect regions of\ninterest from just weak labels, and also learn a medicine language model us-\ning synthetically generated text lines using probabilistic programs. The idea is\ngeneric enough to be applied to a variety of other types of documents, such as\nhandwritten forms.\nAcknowledgement: We thank Srujana Merugu, Ansh Khurana, Manish Gupta,\nHarsh Dhand and Shruti Garg for all the support and discussions during the\ncourse of this project. Without their effort, this project would not have been\npossible.\nReferences\n1. Achkar, R., Ghayad, K., Haidar, R., Saleh, S., Al Hajj, R.: Medical handwritten\nprescription recognition using crnn. In: CITS. IEEE (2019)\n2. Araslanov, N., Roth, S.: Self-supervised augmentation consistency for adapting\nsemantic segmentation. In: CVPR (2021)\n3. Bhunia, A.K., Sain, A., Chowdhury, P.N., Song, Y.Z.: Text is text, no matter what:\nUnifying text recognition using knowledge distillation. In: ICCV (2021)\n4. Bissacco, A., Cummins, M., Netzer, Y., Neven, H.: Photoocr: Reading text in\nuncontrolled conditions. In: ICCV. pp. 785\u2013792 (2013)\n5. Breuel, T.M., Ul-Hasan, A., Al-Azawi, M.A., Shafait, F.: High-performance ocr for\nprinted english and fraktur using lstm networks. In: ICDAR. IEEE (2013)\n6. Bukhari, S.S., Kadi, A., Jouneh, M.A., Mir, F.M., Dengel, A.: anyocr: An open-\nsource ocr system for historical archives. In: ICDAR (2017)\n7. Cascante-Bonilla, P., Tan, F., Qi, Y., Ordonez, V.: Curriculum labeling: Revisiting\npseudo-labeling for semi-supervised learning. In: AAAI (2021)\n8. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. PAMI (2017)\n9. Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:\nPanoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic\nsegmentation. In: CVPR. pp. 12475\u201312485 (2020)\nWeakly supervised information extraction\n17\n10. Diaz, D.H., Qin, S., Ingle, R., Fujii, Y., Bissacco, A.: Rethinking text line recognition\nmodels. arXiv preprint arXiv:2104.07787 (2021)\n11. D\u2019hondt, E., Grouin, C., Grau, B.: Generating a training corpus for ocr post-\ncorrection using encoder-decoder model. In: IJCNLP (2017)\n12. Fujii, Y., Driesen, K., Baccash, J., Hurst, A., Popat, A.C.: Sequence-to-label script\nidentification for multilingual ocr. In: ICDAR. IEEE (2017)\n13. Graves, A., Fern\u00b4andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal\nclassification: labelling unsegmented sequence data with recurrent neural networks.\nIn: Proceedings of the 23rd international conference on Machine learning. pp.\n369\u2013376 (2006)\n14. Gupta, H., Del Corro, L., Broscheit, S., Hoffart, J., Brenner, E.: Unsupervised multi-\nview post-ocr error correction with language models. In: EMNLP. pp. 8647\u20138652\n(2021)\n15. Gupta, M., Soeny, K.: Algorithms for rapid digitalization of prescriptions. Visual\nInformatics (2021)\n16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPr. pp. 770\u2013778 (2016)\n17. Huang, J., Pang, G., Kovvuri, R., Toh, M., Liang, K.J., Krishnan, P., Yin, X.,\nHassner, T.: A multiplexed network for end-to-end, multilingual ocr. In: CVPR\n(2021)\n18. Ingle, R.R., Fujii, Y., Deselaers, T., Baccash, J., Popat, A.C.: A scalable handwritten\ntext recognition system. In: ICDAR (2019)\n19. Jayakumar,\nP.:\nOnline\ndoctor\nconsultation\nmarket\nto\ngrow\n(2021),\nhttps://www.businesstoday.in/lifestyle/health/story/\nonline-doctor-consultation-market-to-grow-72-to-836-million-by-march-2024-study-304689-2021-08-1\n20. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura,\nM., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015\ncompetition on robust reading. In: ICDAR. IEEE (2015)\n21. Karthikeyan, S., de Herrera, A.G.S., Doctor, F., Mirza, A.: An ocr post-correction\napproach using deep learning for processing medical reports. TCSVT (2021)\n22. Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B.: Simple does it: Weakly\nsupervised instance and semantic segmentation. In: CVPR (2017)\n23. Kittenplon, Y., Lavi, I., Fogel, S., Bar, Y., Manmatha, R., Perona, P.: Towards\nweakly-supervised text spotting using a multi-task transformer. In: CVPR (2022)\n24. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: Biobert: a\npre-trained biomedical language representation model for biomedical text mining.\nbioinformatics btz682 (2019)\n25. Li, D., Huang, J.B., Li, Y., Wang, S., Yang, M.H.: Weakly supervised object\nlocalization with progressive domain adaptation. In: CVPR (2016)\n26. Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.:\nTrocr: Transformer-based optical character recognition with pre-trained models.\narXiv preprint arXiv:2109.10282 (2021)\n27. Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter:\nselective context attentional scene text recognizer. In: CVPR (2020)\n28. Liu, H., Wang, J., Long, M.: Cycle self-training for domain adaptation. NeurIPS\n(2021)\n29. Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep learning\nera. IJCV (2021)\n30. Long, S., Qin, S., Panteleev, D., Bissacco, A., Fujii, Y., Raptis, M.: Towards\nend-to-end unified scene text detection and layout analysis. In: CVPR (2022)\n18\nS. Paul et al.\n31. Marti, U.V., Bunke, H.: The iam-database: an english sentence database for offline\nhandwriting recognition. IJDAR (2002)\n32. Paul, S., Roy, S., Roy-Chowdhury, A.K.: W-talc: Weakly-supervised temporal\nactivity localization and classification. In: ECCV (2018)\n33. Pragnadyuti, M., Rabindranath, D., Suhrita, P., Kumar, S.A., Kumar, J.S.: Legi-\nbility assessment of handwritten opd prescriptions of a tertiary care medical college\nand hospital in eastern india. SJMPS (2017)\n34. Rani, S., Rehman, A.U., Yousaf, B., Rauf, H.T., Nasr, E.A., Kadry, S.: Recognition\nof handwritten medical prescription using signature verification techniques. CMMM\n(2022)\n35. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized\nembeddings on large-scale structured electronic health records for disease prediction.\nNature (2021)\n36. Ratner, A., Bach, S.H., Ehrenberg, H., Fries, J., Wu, S., R\u00b4e, C.: Snorkel: Rapid\ntraining data creation with weak supervision. In: VLDB. NIH Public Access (2017)\n37. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,\nreal-time object detection. In: CVPR (2016)\n38. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object\ndetection with region proposal networks. NeurIPS (2015)\n39. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In: CVPR (2018)\n40. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk,\nE.D., Kurakin, A., Li, C.L.: Fixmatch: Simplifying semi-supervised learning with\nconsistency and confidence. NeurIPS (2020)\n41. Thompson, P., McNaught, J., Ananiadou, S.: Customised ocr correction for historical\nmedical text. In: 2015 digital heritage. IEEE (2015)\n42. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. NeurIPS (2017)\n43. Wang, P., Li, H., Shen, C.: Towards end-to-end text spotting in natural scenes.\nPAMI (2021)\n44. Wei, Y., Liang, X., Chen, Y., Shen, X., Cheng, M.M., Feng, J., Zhao, Y., Yan, S.:\nStc: A simple to complex framework for weakly-supervised semantic segmentation.\nPAMI (2016)\n45. Yang, X., PourNejatian, N., Shin, H.C., Smith, K.E., Parisien, C., Compas, C.,\nMartin, C., Flores, M.G., Zhang, Y., Magoc, T., et al.: Gatortron: A large clinical\nlanguage model to unlock patient information from unstructured electronic health\nrecords. arXiv preprint arXiv:2203.03540 (2022)\n46. Zhang, C., Cao, M., Yang, D., Chen, J., Zou, Y.: Cola: Weakly-supervised temporal\naction localization with snippet contrastive learning. In: CVPR (2021)\n47. Zhang, D., Han, J., Cheng, G., Yang, M.H.: Weakly supervised object localization\nand detection: A survey. PAMI (2021)\n"
  }
]