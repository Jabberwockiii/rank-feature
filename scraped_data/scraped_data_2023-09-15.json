[
  {
    "title": "Generative Image Dynamics",
    "link": "https://arxiv.org/pdf/2309.07906.pdf",
    "upvote": "50",
    "text": "Generative Image Dynamics\nZhengqi Li\nRichard Tucker\nNoah Snavely\nAleksander Holynski\nGoogle Research\n\u2026\nInput Picture\nX coefficients\nSpectral Volume\n(Image-Space Modal Basis)\n0.2Hz\n0.4Hz\n3.0Hz\nLooping \nvideo\nInteractive\ndynamics\n\u2026\nY coefficients\nFigure 1. We model a generative image-space prior on scene motion: from a single RGB image, our method generates a spectral volume [23],\na motion representation that models dense, long-term pixel trajectories in the Fourier domain. Our learned motion priors enable applications\nsuch as turning a single picture into a seamlessly looping video, or\u2014by interpreting a predicted spectral volume as an image-space modal\nbasis [22]\u2014taking an image and creating an interactive dynamic simulation that responds to inputs like dragging and releasing points. On\nthe right, we visualize output videos using space-time X-t slices through 10 seconds of video (along the scanline shown in the input picture).\nAbstract\nWe present an approach to modeling an image-space\nprior on scene motion. Our prior is learned from a collection\nof motion trajectories extracted from real video sequences\ndepicting natural, oscillatory dynamics such as trees, flow-\ners, candles, and clothes swaying in the wind. We model\nthis dense, long-term motion prior in the Fourier domain:\ngiven a single image, our trained model uses a frequency-\ncoordinated diffusion sampling process to predict a spectral\nvolume, which can be converted into a motion texture that\nspans an entire video. Along with an image-based render-\ning module, these trajectories can be used for a number\nof downstream applications, such as turning still images\ninto seamlessly looping videos, or allowing users to realis-\ntically interact with objects in real pictures by interpreting\nthe spectral volumes as image-space modal bases, which\napproximate object dynamics. See our project page for more\nresults: generative-dynamics.github.io.\n1. Introduction\nThe natural world is always in motion, with even seemingly\nstatic scenes containing subtle oscillations as a result of\nfactors such as wind, water currents, respiration, or other\nnatural rhythms. Motion is one of the most salient visual\nsignals, and humans are particularly sensitive to it: captured\nimagery without motion (or even with slightly unrealistic\nmotion) can often seem uncanny or unreal.\nWhile it is easy for humans to interpret or imagine motion\nin scenes, training a model to learn realistic scene motion is\nfar from trivial. The motion we observe in the world is the\nresult of a scene\u2019s underlying physical dynamics, i.e., forces\napplied to objects that respond according to their unique\nphysical properties \u2014 their mass, elasticity, etc. These prop-\nerties and forces are hard to measure and capture at scale, but\nfortunately, in many cases measuring them is unnecessary:\nthe necessary signals for producing plausible motion can\noften be extracted from observed 2D motion [23]. While\nreal-world observed motion is multi-modal and grounded in\narXiv:2309.07906v2  [cs.CV]  6 Nov 2023\ncomplex physical effects, it is nevertheless often predictable:\ncandles will flicker in certain ways, trees will sway, and their\nleaves will rustle. This predictability is ingrained in our hu-\nman perception of real scenes: by viewing a still image, we\ncan imagine plausible motions that might have been ongoing\nas the picture was captured \u2014 or, since there might have\nbeen many possible such motions, a distribution of natural\nmotions conditioned on that image. Given the facility with\nwhich humans are able to imagine these possible motions, a\nnatural research problem is to model this same distribution\ncomputationally.\nRecent advances in generative models, in particular con-\nditional diffusion models [43, 83, 85], have enabled us to\nmodel rich distributions, including distributions of real im-\nages conditioned on text [71\u201373]. This capability has en-\nabled a number of previously impossible applications, such\nas text-conditioned generation of diverse and realistic image\ncontent. Following the success of these image models, recent\nwork has extended these models to other domains, such as\nvideos [7, 42] and 3D geometry [75, 98, 99, 101].\nIn this paper, we explore modeling a generative prior for\nimage-space scene motion, i.e., the motion of all pixels in a\nsingle image. This model is trained on motion trajectories au-\ntomatically extracted from a large collection of real video se-\nquences. In particular, from each training video we compute\nmotion in the form of a spectral volume [22, 23], a frequency-\ndomain representation of dense, long-range pixel trajectories.\nThis motion representation is well-suited to scenes that ex-\nhibit oscillatory dynamics such as trees and flowers moving\nin the wind. We find that this representation is also highly\nefficient and effective as an output of a diffusion model for\nmodeling scene motions. We train a generative model that,\nconditioned on a single image, can sample spectral volumes\nfrom its learned distribution. A predicted spectral volume\ncan then be directly transformed into a motion texture\u2014a set\nof per-pixel, long-range pixel motion trajectories\u2014that can\nbe used to animate the image. Further, the spectral volume\ncan be interpreted as an image-space modal basis that can\nbe used to simulate interactive dynamics, using the modal\nanalysis technique of Davis et al. [22]. In this paper, we refer\nto the underlying frequency-space representation as either a\nspectral volume or an image-space modal basis, depending\non whether it is used to encode a specific motion texture, or\nused to simulate dynamics.\nWe predict spectral volumes from input images using a\ndiffusion model that generates coefficients one frequency at\na time, but coordinates these predictions across frequency\nbands through a shared attention module. The predicted\nmotions can be used to synthesize future frames (via an\nimage-based rendering model)\u2014turning still images into\nrealistic animations, as illustrated in Fig. 1.\nCompared with priors over raw RGB pixels, priors over\nmotion capture more fundamental, lower-dimensional under-\nlying structure that efficiently explains long-range variations\nin pixel values. Hence, generating intermediate motion leads\nto more coherent long-term generation and more fine-grained\ncontrol over animations when compared with methods that\nperform image animation via synthesis of raw video frames.\nWe demonstrate the use of our trained model in several\ndownstream applications, such as creating seamless looping\nvideos, editing the generated motions, and enabling inter-\nactive dynamic images via image-space modal bases, i.e.,\nsimulating the response of object dynamics to user-applied\nforces [22].\n2. Related Work\nGenerative synthesis.\nRecent advances in generative mod-\nels have enabled photorealistic synthesis of images condi-\ntioned on text prompts [16, 17, 24, 71\u201373]. These generative\ntext-to-image models can be augmented to synthesize video\nsequences by extending the generated image tensors along a\ntime dimension [7, 9, 42, 61, 82, 104, 104, 109]. While these\nmethods are effective at producing plausible video sequences\nthat capture the spatiotemporal statistics of real footage, the\nresulting videos often suffer from artifacts such as incoher-\nent motion, unrealistic temporal variation in textures, and\nviolations of physical constraints like preservation of mass.\nAnimating images.\nInstead of generating videos entirely\nfrom text, other techniques take as input a still picture and\nanimate it. Many recent deep learning methods adopt a 3D-\nUnet architecture to produce video volumes directly from\nan input image [27, 35, 39, 46, 52, 91]. Because these mod-\nels are effectively the same video generation models (but\nconditioned on image information instead of text), they\nexhibit similar artifacts to those mentioned above. One\nway to overcome these limitations is to not directly gen-\nerate the video content itself, but instead animate an input\nsource image through explicit or implicit image-based ren-\ndering, i.e., moving the image content around according\nto motion derived from external sources such as a driv-\ning video [50, 78\u201380, 97], motion or 3D geometry pri-\nors [8, 29, 45, 62, 63, 65, 88, 95, 99, 100, 102, 107], or\nuser annotations [6, 18, 20, 32, 37, 96, 103, 106]. Animating\nimages according to motion fields yields greater temporal\ncoherence and realism, but these prior methods require ad-\nditional guidance signals or user input, or otherwise utilize\nlimited motion representations (e.g., optical flow fields, as\nopposed to full-video dense motion trajectories).\nMotion models and motion priors.\nIn computer graphics,\nnatural, oscillatory 3D motion (e.g., water rippling or trees\nwaving in the wind) has long been modeled with noise that\nis shaped in the Fourier domain and then converted via an\ninverse Fourier transform to time-domain motion fields [77,\n86]. Some of these methods rely on a modal analysis of the\nunderlying dynamics of the systems being simulated [22,\n25, 87]. These spectral techniques were adapted to animate\nplants, water, and clouds from single 2D pictures by Chuang\net al. [20] with additional user annotations. Our work is\nparticularly inspired by that of Davis [23], who showed\nhow to connect modal analysis of a scene with the motions\nobserved in a video of that scene, and how to use this analysis\nto simulate interactive dynamics from a video. We adopt\nthe frequency-space motion representation of the spectral\nvolume from Davis et al., extract this representation from a\nlarge set of training videos, and show that this representation\nis suitable for predicting motion from single images with\ndiffusion models.\nOther methods have also used various motion representa-\ntions in prediction tasks \u2014 where an image or video is used\nto inform a deterministic future motion estimate [33, 69],\nor a more rich distribution of possible motions (which\ncan be modeled explicitly or by predicting the pixel val-\nues that would be induced by some implicit motion es-\ntimate) [92, 94, 102]. However, many of these methods\npredict an optical flow motion estimate (i.e., the instanta-\nneous motion of each pixel), not full per-pixel motion tra-\njectories. In addition, much of this prior work is focused\non tasks like activity recognition, not on synthesis tasks.\nMore recent work has demonstrated the advantages of mod-\neling and predicting motion using generative models in a\nnumber of closed-domain settings such as humans and ani-\nmals [2, 19, 28, 70, 89, 105].\nVideos as textures.\nCertain moving scenes can be thought\nof as a kind of texture\u2014termed dynamic textures by Doretto\net al. [26]\u2014that model videos as space-time samples of a\nstochastic process. Dynamic textures can represent smooth,\nnatural motions such as waves, flames, or moving trees, and\nhave been widely used for video classification, segmentation\nor encoding [12\u201315, 74]. A related kind of texture, called\na video texture, represents a moving scene as a set of input\nvideo frames along with transition probabilities between any\npair of frames [64, 76]. A large body of work exists for\nestimating and producing dynamic or video textures through\nanalysis of scene motion and pixel statistics, with the aim of\ngenerating seamlessly looping or infinitely varying output\nvideos [1, 21, 31, 57, 58, 76]. In contrast to much of this\nprevious work, our method learns priors in advance that can\nthen be applied to single images.\n3. Overview\nGiven a single picture I0, our goal is to generate a video\n{\u02c6I1, \u02c6I2., ..., \u02c6IT } of length T featuring oscillation dynamics\nsuch as those of trees, flowers, or candle flames moving in\nthe breeze. Our system consists of two modules, a motion\nprediction module and an image-based rendering module.\nOur pipeline begins by using a latent diffusion model (LDM)\nto predict a spectral volume S =\n\u0000Sf0, Sf1, ..., SfK\u22121\n\u0001\nfor\nthe input image I0. The predicted spectral volume is then\ntransformed to a sequence of motion displacement fields (a\nmotion texture) F = (F1, F2, ..., FT ) using an inverse dis-\ncrete Fourier transform. This motion determines the position\nof each input pixel at each future time step.\nGiven a predicted motion texture, our rendering module\nanimates the input RGB image using an image-based ren-\ndering technique that splats encoded features from the input\nimage and decodes these splatted features into an output\nframe with an image synthesis network (Sec. 5). We explore\napplications of this method, including producing seamless\nlooping animations and simulating interactive dynamics, in\nSec. 6.\n4. Predicting motion\n4.1. Motion representation\nFormally, a motion texture is a sequence of time-varying 2D\ndisplacement maps F = {Ft|t = 1, ..., T}, where the 2D\ndisplacement vector Ft(p) at each pixel coordinate p from\ninput image I0 defines the position of that pixel at a future\ntime t [20]. To generate a future frame at time t, one can\nsplat pixels from I0 using the corresponding displacement\nmap Dt, resulting in a forward-warped image I\u2032\nt:\nI\u2032\nt(p + Ft(p)) = I0(p).\n(1)\nIf our goal is to produce a video via a motion texture, then\none choice would be to predict a time-domain motion texture\ndirectly from an input image. However, the size of the motion\ntexture would need to scale with the length of the video: gen-\nerating T output frames implies predicting T displacement\nfields. To avoid predicting such a large output representation\nfor long output videos, many prior animation methods either\ngenerate video frames autoregressively [7, 29, 56, 59, 91],\nor predict each future output frame independently via an\nextra time embedding [4]. However, neither strategy ensures\nlong-term temporal consistency of generated video frames.\nFortunately, many natural motions, can be described as\na superposition of a small number of harmonic oscilla-\ntors represented with different frequencies, amplitude and\nphases [20, 23, 25, 49, 67]. Because the underlying motions\nare quasi-periodic, it is natural to model them in the fre-\nquency domain, from which it is convenient to generate a\nvideo of arbitrary length.\nHence, we adopt an efficient frequency space represen-\ntation of motion in a video from Davis et al. [23] called a\nspectral volume, visualized in Figure 1. A spectral volume is\nthe temporal Fourier transform of pixel trajectories extracted\nfrom a video, organized into images called modal images.\nDavis et al. further shows that, under certain assumptions,\nthe spectral volume, evaluated at certain frequencies, forms\nan image-space modal basis that is a projection of the vi-\nbration modes of the underlying scene (or, more generally,\ncaptures spatial correlations in motion) [22]. We use the term\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nFrequency (Hz)\n0\n20\n40\n60\n80\nAmplitude\nX-axis\nY-axis\n0.0\n0.5\n1.0\n1.5\nAmplitude of Fourier coefficent at 3.0 Hz\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\nScaling w/ resolution\nAdaptive normalization\nFigure 2. Left: We visualize the average motion power spectrum\nfor the X and Y motion components extracted from a dataset of\nreal videos, shown as the blue and green curves. Natural oscillation\nmotions are composed primarily of low-frequency components,\nand so we use the first K = 16 terms as marked by red dots.\nRight: we show a histogram of the amplitude of Fourier terms at\n3Hz (K = 16) after (1) scaling amplitude by image width and\nheight (blue), or (2) frequency adaptive normalization (red). Our\nadaptive normalization prevents the coefficients from concentrating\nat extreme values.\nspectral volume to refer to a frequency-space encoding of\na specific motion texture (with high frequencies removed).\nLater, we also refer to a spectral volume as an \u201cimage-space\nmodal basis\u201d when it is used for simulation.\nGiven this motion representation, we formulate the mo-\ntion prediction problem as a multi-modal image-to-image\ntranslation task: from an input image to an output spectral\nmotion volume. We adopt latent diffusion models (LDMs)\nto generate spectral volumes comprised of a 4K-channel 2D\nmotion spectrum map, where K << T is the number of\nfrequencies modeled, and where at each frequency we need\nfour scalars to represent the complex Fourier coefficients for\nthe x and y dimensions. Note that the motion trajectory of a\npixel at future time steps F(p) = {Ft(p)|t = 1, 2, ...T} and\nits representation as a spectral volume S(p) = {Sfk(p)|k =\n0, 1, .. T\n2 \u22121} are related by the Fast Fourier transform (FFT):\nS(p) = FFT(F(p)).\n(2)\nHow should we select the K output frequencies? Prior\nwork in real-time animation has observed that most natural\noscillation motions are composed primarily of low-frequency\ncomponents [25, 67]. To validate this observation, we com-\nputed the average power spectrum of the motion extracted\nfrom 1,000 randomly sampled 5-second real video clips. As\nshown in the left plot of Fig. 2, the power spectrum of the mo-\ntion decreases exponentially with increasing frequency. This\nsuggests that most natural oscillation motions can indeed\nbe well represented by low-frequency terms. In practice, we\nfound that the first K = 16 Fourier coefficients are sufficient\nto realistically reproduce the original natural motion in a\nrange of real videos and scenes.\n4.2. Predicting motion with a diffusion model\nWe choose a latent diffusion model (LDM) [72] as the back-\nbone for our motion prediction module, as LDMs are more\ncomputationally efficient than pixel-space diffusion models,\nwhile preserving generation quality. A standard LDM con-\nsists of two main modules: (1) a Variational Autoencoder\n(VAE) that compresses the input image to a latent space\nthrough an encoder z = E(I), then reconstructs the input\nfrom the latent features via a decoder I = D(z), and (2)\na U-Net based diffusion model that learns to iteratively de-\nnoise latent features starting from Gaussian random noise.\nOur training applies this not to input images but to motion\nspectra from real video sequences, which are encoded and\nthen diffused for n steps with a pre-defined variance sched-\nule to produce noisy latents zn. The 2D U-Nets are trained to\ndenoise the noisy latents by iteratively estimating the noise\n\u03f5\u03b8(zn; n, c) used to update the latent feature at each step\nn \u2208 (1, 2, ..., N). The training loss for the LDM is written\nas\nLLDM = En\u2208U[1,N],\u03f5n\u2208N (0,1)\n\u0002\n||\u03f5n \u2212 \u03f5\u03b8(zn; n, c)||2\u0003\n(3)\nwhere c is the embedding of any conditional signal, such\nas text, semantic labels, or, in our case, the first frame of\nthe training video sequence, I0. The clean latent features z0\nare then passed through the decoder to recover the spectral\nvolume.\nFrequency adaptive normalization.\nOne issue we ob-\nserved is that motion textures have particular distribution\ncharacteristics across frequencies. As visualized in the left\nplot of Fig. 2, the amplitude of our motion textures spans a\nrange of 0 to 100 and decays approximately exponentially\nwith increasing frequency. As diffusion models require that\noutput values lie between 0 and 1 for stable training and\ndenoising, we must normalize the coefficients of S extracted\nfrom real videos before using them for training. If we scale\nthe magnitudes of S coefficients to [0,1] based on image\nwidth and height as in prior work [29, 75], almost all the\ncoefficients at higher frequencies will end up close to zero,\nas shown in Fig. 2 (right-hand side). Models trained on such\ndata can produce inaccurate motions, since during inference,\neven small prediction errors can lead to large relative errors\nafter denormalization when the magnitude of the normalized\nS coefficients are very close to zero.\nTo address this issue, we employ a simple but effective\nfrequency adaptive normalization technique. In particular,\nwe first independently normalize Fourier coefficients at each\nfrequency based on statistics computed from the training set.\nNamely, at each individual frequency fj, we compute the\n97th percentile of the Fourier coefficient magnitudes over all\ninput samples and use that value as a per-frequency scaling\nfactor sfj. Furthermore, we apply a power transformation to\neach scaled Fourier coefficient to pull it away from extremely\nsmall or large values. In practice, we found that a square root\ntransform performs better than other transformations, such\nas log or reciprocal. In summary, the final coefficient values\nof spectral volume S(p) at frequency fj (used for training\n\u2026\n\u2026\nIterative denoising \nReshape\nSpatial layer\nFrequency Attention\nReshape\nTrain\nInference\nNoisy latent\nFigure 3. Motion prediction module. We predict a spectral volume S through a frequency-coordinated denoising model. Each block of the\ndiffusion network \u03f5\u03b8 interleaves 2D spatial layers with frequency cross-attention layers (red box, right), and iteratively denoises latent\nfeatures zn. The denoised features are fed to a VAE decoder D to produce S. During training, we concatenate the downsampled input I0\nwith noisy latent features encoded from a real motion texture via a VAE encoder E, and replace the noisy features with Gaussian noise zN\nduring inference (left).\nour LDM) are computed as\nS\u2032\nfj(p) = sign(Sfj)\ns\f\f\f\f\nSfj(p)\nsfj\n\f\f\f\f.\n(4)\nAs shown on the right plot of Fig. 2, after applying frequency\nadaptive normalization the spectral volume coefficients no\nlonger concentrate in a range of extremely small values.\nFrequency-coordinated denoising.\nThe straightforward\nway to to predict a spectral volume S with K frequency\nbands is to output a tensor of 4K channels from a standard\ndiffusion U-Net. However, as in prior work [7], we observe\nthat training a model to produce such a large number of\nchannels tends to produce over-smoothed and inaccurate\noutput. An alternative would be to independently predict\na motion spectrum map at each individual frequency by\ninjecting an extra frequency embedding to the LDM, but this\nresults in uncorrelated predictions in the frequency domain,\nleading to unrealistic motion.\nTherefore, we propose a frequency-coordinated denois-\ning strategy as shown in Fig. 3. In particular, given an input\nimage I0, we first train an LDM \u03f5\u03b8 to predict a spectral vol-\nume texture map Sfj with four channels to represent each\nindividual frequency fj, where we inject extra frequency\nembedding along with time-step embedding to the LDM\nnetwork. We then freeze the parameters of this LDM model\n\u03f5\u03b8 and introduce attention layers and interleave them with\n2D spatial layers of \u03f5\u03b8 across K frequency bands. Specif-\nically, for a batch size B of input images, the 2D spatial\nlayers of \u03f5\u03b8 treat the corresponding B \u00b7 K noisy latent fea-\ntures of channel size C as independent samples with shape\nR(B\u00b7K)\u00d7C\u00d7H\u00d7W . The cross-attention layer then interprets\nthese as consecutive features spanning the frequency axis,\nand we reshape the latent features from previous 2D spatial\nlayers to RB\u00d7K\u00d7C\u00d7H\u00d7W before feeding them to the atten-\ntion layers. In other words, the frequency attention layers\nare used to coordinate the pre-trained motion latent features\nacross all frequency channels in order to produce coher-\nent spectral volumes. In our experiments, we observed that\nthe average VAE reconstruction error improves from 0.024\nto 0.018 when we switch from a standard 2D U-Net to a\nfrequency-coordinated denoising module, suggesting an im-\nproved upper bound on LDM prediction accuracy; in our\nablation study in Sec. 7.6, we also demonstrate that this\ndesign choice improves video generation quality compared\nwith simpler configurations mentioned above.\n5. Image-based rendering\nWe now describe how we take a spectral volume S predicted\nfor a given input image I0 and render a future frame \u02c6It at\ntime t. We first derive motion trajectory fields in the time\ndomain using the inverse temporal FFT applied at each pixel\nF(p) = FFT\u22121(S(p)). The motion trajectory fields deter-\nmine the position of every input pixel at every future time\nstep. To produce a future frame \u02c6It, we adopt a deep image-\nbased rendering technique and perform splatting with the\npredicted motion field Ft to forward warp the encoded I0,\nas shown in Fig. 4. Since forward warping can lead to holes,\nand multiple source pixels can map to the same output 2D\nlocation, we adopt the feature pyramid softmax splatting\nstrategy proposed in prior work on frame interpolation [66].\nSpecifically, we encode I0 through a feature extractor net-\nwork to produce a multi-scale feature map M = {Mj|j =\n0, ..., J}. For each individual feature map Mj at scale j, we\nresize and scale the predicted 2D motion field Ft according\nto the resolution of Mj. As in Davis et al. [22], we use pre-\ndicted flow magnitude, as a proxy for depth, to determine\nFeature extractor\nSoftmax splatting\n(Subject to W)\nSynthesis network\nFigure 4. Rendering module. We fill in missing content and refine\nthe warped input image using a motion-aware deep image-based\nrendering module, where multi-scale features are extracted from the\ninput image I0. Softmax splatting is then applied over the features\nwith a motion field Ft from time 0 to t (subject to the weights W\nderived from motion). The warped features are fed to an image\nsynthesis network to produce the refined image \u02c6It.\nthe contributing weight of each source pixel mapped to its\ndestination location. In particular, we compute a per-pixel\nweight, W(p) = 1\nT\nP\nt ||Ft(p)||2 as the average magnitude\nof the predicted motion trajectory fields. In other words, we\nassume large motions correspond to moving foreground ob-\njects, and small or zero motions correspond to background\nobjects. We use motion-derived weights instead of learnable\nones because we observe that in the single-view case, learn-\nable weights are not effective for addressing disocclusion\nambiguities, as shown in the second column of Fig. 5.\nWith the motion field Ft and weights W, we apply soft-\nmax splatting to warp feature map at each scale to pro-\nduce a warped feature M \u2032\nj,t = Wsoftmax(Mj, Ft, W), where\nWsoftmax is the softmax splatting operation. The warped fea-\ntures M \u2032\nj,t are then injected into intermediate blocks of an\nimage synthesis decoder network to produce a final rendered\nimage \u02c6It.\nWe jointly train the feature extractor and synthesis net-\nworks with start and target frames (I0, It) randomly sampled\nfrom real videos, using the estimated flow field from I0 to\nIt to warp encoded features from I0, and supervising pre-\ndictions \u02c6It against It with a VGG perceptual loss [48]. As\nshown in Fig. 5, compared to direct average splatting and a\nbaseline deep warping method [45], our motion-aware fea-\nture splatting produces a frame without holes or artifacts\naround disocclusions.\n6. Applications\nWe demonstrate applications that add dynamics to single\nstill images using our proposed motion representations and\nanimation pipeline.\n(a) Average-splat\n(b) Baseline-splat\n(c) Ours\nFigure 5. From left to right, we show a rendered future frame with\n(a) average splatting in RGB pixel space, (b) softmax splatting with\nlearnable weights [45], and (c) motion-aware feature splatting.\n6.1. Image-to-video\nOur system enables the animation of a single still picture by\nfirst predicting a spectral volume from the input image and\ngenerating an animation by applying our image-based render-\ning module to the motion displacement fields derived from\nthe spectral volume. Since we explicitly model scene mo-\ntion, this allows us to produce slow-motion videos by linear\ninterpolating the motion displacement fields and to magnify\n(or minify) animated motions by adjusting the amplitude of\npredicted spectral volume coefficients.\n6.2. Seamless looping\nIt is sometimes useful to generate videos with motion that\nloops seamlessly, meaning that there is no discontinuity\nin appearance or motion between the start and end of the\nvideo. Unfortunately, it is hard to find a large collection of\nseamlessly looping videos for training diffusion models. In-\nstead, we devise a method to use our motion diffusion model,\ntrained on regular non-looping video clips, to produce seam-\nless looping video. Inspired by recent work on guidance for\nimage editing [3, 30], our method is a motion self-guidance\ntechnique that guides the motion denoising sampling pro-\ncessing using explicit looping constraints. In particular, at\neach iterative denoising step during the inference stage, we\nincorporate an additional motion guidance signal alongside\nstandard classifier-free guidance [44], where we enforce each\npixel\u2019s position and velocity at the start and end frames to\nbe as similar as possible:\n\u02c6\u03f5n = (1 + w)\u03f5\u03b8(zn; n, c) \u2212 w\u03f5\u03b8(zn; n, \u2205) + u\u03c3n\u2207znLn\ng\nLn\ng = ||F n\nT \u2212 F n\n1 ||1 + ||\u2207F n\nT \u2212 \u2207F n\n1 ||1\n(5)\nwhere F n\nt is the predicted 2D motion displacement field at\ntime t and denoising step n. w is the classifier-free guidance\nweight, and u is the motion self-guidance weight.\n6.3. Interactive dynamics from a single image\nAs shown in Davis et al. [22], the image-space motion spec-\ntrum from an observed video of an oscillating object, under\ncertain assumptions, is proportional to the projections of\nvibration mode shapes of that object, and thus a spectral\nvolume can be interpreted as an image-space modal basis.\nThe modal shapes capture underlying oscillation dynamics\nImage Synthesis\nVideo Synthesis\nMethod\nFID\nKID\nFVD FVD32 DTFVD DTFVD32\nTATS [34]\n65.8\n1.67\n265.6 419.6\n22.6\n40.7\nStochastic I2V [27] 68.3\n3.12\n253.5 320.9\n16.7\n41.7\nMCVD [91]\n63.4\n2.97\n208.6 270.4\n19.5\n53.9\nLFDM [65]\n47.6\n1.70\n187.5 254.3\n13.0\n45.6\nDMVFN [47]\n37.9\n1.09\n206.5 316.3\n11.2\n54.5\nEndo et al. [29]\n10.4\n0.19\n166.0 231.6\n5.35\n65.1\nHolynski et al. [45] 11.2\n0.20\n179.0 253.7\n7.23\n46.8\nOurs\n4.03\n0.08\n47.1\n62.9\n2.53\n6.75\nTable 1. Quantitative comparisons on the test set. We report both\nimage synthesis and video synthesis quality. Here, KID is scaled\nby 100. Lower is better for all error. See Sec. 7.4 for descriptions\nof baselines and error metrics.\nof the object at different frequencies, and hence can be used\nto simulate the object\u2019s response to a user-defined force such\nas poking or pulling. Therefore, we adopt the modal analysis\ntechnique from prior work [22, 68], which assumes that the\nmotion of an object can be explained by the superposition\nof a set of harmonic oscillators. This allows us to write the\nimage-space 2D motion displacement field for the object\u2019s\nphysical response as a weighted sum of Fourier spectrum\ncoefficients Sfj modulated by the state of complex modal\ncoordinates qfj,t at each simulated time step t:\nFt(p) =\nX\nfj\nSfj(p)qfj,t\n(6)\nWe simulate the state of the modal coordinates qfj,t via\na forward Euler method applied to the equations of mo-\ntion for a decoupled mass-spring-damper system (in modal\nspace) [22, 23, 68]. We refer readers to the original work for\na full derivation. Note that our method produces an interac-\ntive scene from a single picture, whereas these prior methods\nrequired a video as input.\n7. Experiments\n7.1. Implementation details\nWe use an LDM [72] as the backbone for predicting spectral\nvolumes, for which we use a variational auto-encoder (VAE)\nwith a continuous latent space of dimension 4. We train the\nVAE with an L1 reconstruction loss, a multi-scale gradient\nconsistency loss [53\u201355], and a KL-divergence loss with re-\nspective weights of 1, 0.2, 10\u22126. We train the same 2D U-Net\nused in the original LDM work to perform iterative denosing\nwith a simple MSE loss [43], and adopt the attention layers\nfrom [40] for frequency-coordinated denoising. For quan-\ntitative evaluation, we train the VAE and LDM on images\nof size 256 \u00d7 160, which takes around 6 days to converge\nusing 16 Nvidia A100 GPUs. For our main quantitative and\nqualitative results, we run the motion diffusion model with\nDDIM [84] for 250 steps. We also show generated videos of\n0\n20\n40\n60\n80\n100\n120\nFrame index\n0\n20\n40\n60\n80\n100\n120\nFID\nSling Window FID\nDMVFN\nMCVD\nStochastic I2V\nLFDM\nEndo et al.\nHolynski et al.\nOurs\n20\n40\n60\n80\n100\n120\nFrame index\n5\n10\n15\n20\n25\nDT-FVD\nSling Window DT-FVD\nFigure 6. Sliding Window FID and DT-FVD. We show sliding\nwindow FID of window size 30 frames, and DT-FVD of size 16\nframes, for videos generated by different methods.\nup to a resolution of 512 \u00d7 288, created by fine-tuning our\nmodels on corresponding higher resolution data.\nWe adopt ResNet-34 [38] as feature extractor in our IBR\nmodule. Our image synthesis network is based on a co-\nmodulation StyleGAN architecture, which is a prior condi-\ntional image generation and inpainting model [56, 108]. Our\nrendering module runs in real-time at 25FPS on a Nvidia\nV100 GPU during inference. Additionally, we adopt the\nuniversal guidance [3] to produce seamless looping videos,\nwhere we set weights w = 1.75, u = 200, and 500 DDIM\nsteps with 2 self-recurrence iterations.\n7.2. Data and baselines\nData.\nSince our focus is on natural scenes exhibiting oscil-\nlatory motion such as trees, flowers, and candles moving in\nthe wind, we collect and process a set of 3,015 videos of such\nphenomena from online sources as well as from our own\ncaptures, where we withhold 10% of the videos for testing\nand use the remainder for training. To generate ground truth\nspectral volumes for training our motion prediction module,\nwe found the choice of optical flow method to be crucial. In\nparticular, we observed that deep-learning based flow esti-\nmators tend to produce over-smoothed flow fields. Instead,\nwe apply a coarse-to-fine image pyramid-based optical flow\nalgorithm [10, 60] between selected starting image and every\nfuture frame within a video sequence. We treat every 10th\nframe from each training video as a starting image and gen-\nerate corresponding ground truth spectral volumes using the\nfollowing 149 frames. We filter out samples with incorrect\nmotion estimates or significant camera motion by removing\nexamples with an average flow motion magnitude >8 pixels,\nor where all pixels have an average motion magnitude larger\nthan one pixel. In total, our data consists of more than 150K\nsamples of image-motion pairs.\n7.3. Metrics\nBaselines.\nWe compare our approach to several recent\nsingle-image animation and video prediction methods. Both\nEndo et al. [29] and DMVFN [47] predict instantaneous 2D\nmotion fields and future frames in an auto-regressive manner.\nWe also compare with Holynsky et al. [45] which animate\nInput image\nReference\nStochastic I2V [27]\nMCVD [91]\nEndo et al. [29]\nOurs\nFigure 7. X-t slices of videos generated by different approaches. From left to right: input image and corresponding X-t video slices from\nthe ground truth video, from videos generated by three baselines [27, 29, 91], and finally videos generated by our approach.\na picture through predicted Eulerian Motion Fields. Other\nrecent work such as Stochastic Image-to-Video (Stochastic-\nI2V) [27], TATS [34], and MCVD [91] adopt either VAEs,\ntemporal transformers, or diffusion models to directly predict\nvideo frames. LFDM [65] predicts flow fields in latent space\nwith a diffusion model, then uses those flow fields to warp\nthe encoded input image, generating future frames via a\ndecoder. For methods that predict videos of short length,\nwe apply them autoregressively to generate longer videos\nby taking the last output frame and using it as the input to\nanother round of generation until the video reaches a length\nof 150 frames. We train all the above methods on our data\nusing their respective open-source implementations1.\nWe evaluate the quality of the videos generated by our\napproach and by prior baselines in two main ways. First, we\nevaluate the quality of individual synthesized frames using\nmetrics designed for image synthesis tasks. We adopt the\nFr\u00b4echet Inception Distance (FID) [41] and Kernel Inception\nDistance (KID) [5] to measure the average distance between\nthe distribution of generated frames and the distribution of\nground truth frames.\nSecond, to evaluate the quality and temporal coherence\nof synthesized videos, we adopt the Fr\u00b4echet Video Dis-\ntance [90] with window size 16 (FVD) and 32 (FVD32),\nbased on an I3D model [11] trained on the Human Kinetics\ndatasets [51]. To more faithfully reflect synthesis quality\nfor the natural oscillation motions we seek to generate, we\nalso adopt the Dynamic Texture Frechet Video Distance\n1We use open-source reimplementation from Fan et al. [81] for the\nmethod of Holynsky et al. [45].\nImage Synthesis\nVideo Synthesis\nMethod\nFID\nKID\nFVD FVD32 DTFVD DTFVD32\nK = 4\n3.97\n0.08\n60.3\n78.4\n3.12\n8.59\nK = 8\n3.95\n0.07\n52.1\n68.7\n2.71\n7.37\nK = 24\n4.09\n0.08\n48.2\n65.1\n2.50\n6.94\nScale w/ resolution 4.53\n0.09\n62.7\n80.1\n3.16\n8.19\nIndependent pred.\n4.00\n0.08\n52.5\n71.3\n2.70\n7.40\nVolume pred.\n4.74\n0.09\n53.7\n71.1\n2.83\n7.79\nAverage splat\n4.52\n0.10\n51.4\n68.9\n2.83\n7.44\nBaseline splat [45] 4.25\n0.09\n49.5\n66.8\n2.83\n7.27\nFull (K = 16)\n4.03\n0.08\n47.1\n62.9\n2.53\n6.75\nTable 2. Ablation study. Please see Sec. 7.6 for the details of the\ndifferent configurations.\nproposed by Dorkenwald et al. [27], which measures the\ndistance from videos of window size 16 (DTFVD) and size\n32 (DTFVD32), using a I3D model trained on the Dynamic\nTextures Database [36], a dataset consisting primarily of\nnatural motion textures.\nWe further use a sliding window FID of a window size\nof 30 frames, and a sliding window DTFVD with window\nsize 16 frames, as proposed by [56, 59], to measure how\ngenerated video quality degrades over time.\nFor all the methods, we evaluate each error metric on\nvideos generated without performing temporal interpolation,\nat 256 \u00d7 128 resolution.\nInput image\nReference\nEndo et al. [29]\nDMVFN [47]\nLFDM [65]\nOurs\nFigure 8. Visual comparisons of generated future frames and corresponding motion fields. By inspecting differences with a reference\nimage from the ground truth video, we observe that our approach produces more realistic textures and motions compared with baselines.\n7.4. Quantitative results\nTable 1 shows quantitative comparisons between our ap-\nproach and baselines on our test set of unseen video clips.\nOur approach significantly outperforms prior single-image\nanimation baselines in terms of both image and video synthe-\nsis quality. Specifically, our much lower FVD and DT-FVD\ndistances suggest that the videos generated by our approach\nare more realistic and more temporally coherent. Further,\nFig. 6 shows the sliding window FID and sliding window\nDT-FVD distances of generated videos from different meth-\nods. Thanks to the global spectral volume representation,\nvideos generated by our approach are more temporally con-\nsistent and do not suffer from drift or degradation over time.\n7.5. Qualitative results\nWe visualize qualitative comparisons between videos gen-\nerated by our approach and by baselines in two ways. First,\nwe show spatio-temporal X-t slices of the generated videos,\na standard way of visualizing small or subtle motions in a\nvideo [93]. As shown in Fig. 7, our generated video dynam-\nics more strongly resemble the motion patterns observed in\nthe corresponding real reference videos (second column),\ncompared to other methods. Baselines such as Stochastic\nI2V [27] and MCVD [91] fail to model both appearance\nand motion realistically over time. Endo et al. [29] produces\nvideo frames with fewer artifacts but exhibits over-smooth\nor non-oscillation motions.\nWe also qualitatively compare the quality of individual\ngenerated frames and motions across different methods by\nvisualizing the predicted image \u02c6It and its corresponding mo-\ntion displacement field at time t = 128. Fig. 8 shows that the\nframes generated by our approach exhibit fewer artifacts and\ndistortions compared to other methods, and our correspond-\ning 2D motion fields most resemble the reference displace-\nment fields estimated from the corresponding real videos. In\ncontrast, the background content generated by other methods\ntend to drift, as shown in the flow visualizations in the even-\nnumbered rows. Moreover, the video frames generated by\nother methods exhibit significant color distortion or ghosting\nartifacts, suggesting that the baselines are less stable when\ngenerating videos with long time duration.\n7.6. Ablation study\nWe conduct an ablation study to validate the major design\nchoices in our motion prediction and rendering modules,\ncomparing our full configuration with different variants.\nSpecifically, we evaluate results using different numbers\nof frequency bands K = 4, 8, 16, and 24. We observe that\nincreasing the number of frequency bands improves video\nprediction quality, but the improvement is marginal when\nusing more than 16 frequencies. Next, we remove adaptive\nfrequency normalization from the ground truth spectral vol-\numes, and instead just scale them based on input image\nwidth and height (Scale w/ resolution). Additionally, we\nremove the frequency coordinated-denoising module (Inde-\npendent pred.), or replace it with a simpler module where a\ntensor volume of 4K channel spectral volumes are predicted\njointly via a standard 2D U-net diffusion model (Volume\npred.). Finally, we compare results where we render video\nframes using average splatting (Average splat), or use a base-\nline rendering method that applies softmax splatting over\nsingle-scale features subject to learnable weights used in\nHolynski et al. [45] (Baseline splat). From Table 2, we ob-\nserve that all simpler or alternative configurations lead to\nworse performance compared with our full model.\n8. Discussion and conclusion\nLimitations.\nSince our approach only predicts spectral\nvolumes at lower frequencies, it might fail to model general\nnon-oscillating motions or high-frequency vibrations such\nas those of musical instruments. Furthermore, the quality\nof our generated videos relies on the quality of the motion\ntrajectories estimated from the real video sequences. Thus,\nwe observed that animation quality can degrade if the motion\nin the training videos consists of large displacements. More-\nover, since our approach is based on image-based rendering\nfrom input pixels, the animation quality can also degrade if\nthe generated videos require the creation of large amounts\nof content unseen in the input frame.\nConclusion.\nWe present a new approach for modeling nat-\nural oscillation dynamics from a single still picture. Our\nimage-space motion prior is represented with spectral vol-\numes [23], a frequency representation of per-pixel motion\ntrajectories, which we find to be highly suitable for pre-\ndiction with diffusion models, and which we learn from\ncollections of real world videos. The spectral volumes are\npredicted using our frequency-coordinated latent diffusion\nmodel and are used to animate future video frames using\na neural image-based rendering module. We show that our\napproach produces photo-realistic animations from a single\npicture and significantly outperforms prior baseline methods,\nand that it can enable other downstream applications such as\ncreating interactive animations.\nAcknowledgements.\nWe thank Abe Davis, Rick Szeliski,\nAndrew Liu, Boyang Deng, Qianqian Wang, Xuan Luo, and\nLucy Chai for fruitful discussions and helpful comments.\nReferences\n[1] Aseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh\nAgrawala, Michael Cohen, Brian Curless, David Salesin,\nand Richard Szeliski. Panoramic video textures. In ACM\nSIGGRAPH 2005 Papers, pages 821\u2013827. 2005.\n[2] Hyemin Ahn, Esteve Valls Mascaro, and Dongheui Lee.\nCan we use diffusion probabilistic models for 3d motion\nprediction? arXiv preprint arXiv:2302.14503, 2023.\n[3] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta, Micah Goldblum, Jonas Geiping, and\nTom Goldstein. Universal guidance for diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 843\u2013852, 2023.\n[4] Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, Chun-\nHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio\nEscalera, and Duygu Ceylan. Blowing in the wind: Cyclenet\nfor human cinemagraphs from still images. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 459\u2013468, 2023.\n[5] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel,\nand Arthur Gretton. Demystifying MMD GANs. arXiv\npreprint arXiv:1801.01401, 2018.\n[6] Andreas Blattmann, Timo Milbich, Michael Dorkenwald,\nand Bj\u00a8orn Ommer. ipoke: Poking a still image for controlled\nstochastic video synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14707\u2013\n14717, 2021.\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 22563\u201322575, 2023.\n[8] Richard Strong Bowen, Richard Tucker, Ramin Zabih, and\nNoah Snavely. Dimensions of motion: Monocular prediction\nthrough flow subspaces. In 2022 International Conference\non 3D Vision (3DV), pages 454\u2013464. IEEE, 2022.\n[9] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun\nWang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei\nEfros, and Tero Karras. Generating long videos of dynamic\nscenes. Advances in Neural Information Processing Systems,\n35:31769\u201331781, 2022.\n[10] Thomas Brox, Andr\u00b4es Bruhn, Nils Papenberg, and Joachim\nWeickert. High accuracy optical flow estimation based on a\ntheory for warping. In Proc. European Conf. on Computer\nVision (ECCV), pages 25\u201336. Springer, 2004.\n[11] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299\u20136308, 2017.\n[12] Dan Casas, Marco Volino, John Collomosse, and Adrian\nHilton. 4d video textures for interactive character appear-\nance.\nIn Computer Graphics Forum, volume 33, pages\n371\u2013380. Wiley Online Library, 2014.\n[13] Antoni B Chan and Nuno Vasconcelos. Mixtures of dynamic\ntextures. In Tenth IEEE International Conference on Com-\nputer Vision (ICCV\u201905) Volume 1, volume 1, pages 641\u2013647.\nIEEE, 2005.\n[14] Antoni B Chan and Nuno Vasconcelos. Classifying video\nwith kernel dynamic textures. In 2007 IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1\u20136. IEEE,\n2007.\n[15] Antoni B Chan and Nuno Vasconcelos. Modeling, clustering,\nand segmenting video with mixtures of dynamic textures.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 30(5):909\u2013926, 2008.\n[16] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T Freeman, Michael Rubinstein, et al. Muse: Text-\nto-image generation via masked generative transformers.\narXiv preprint arXiv:2301.00704, 2023.\n[17] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315\u201311325, 2022.\n[18] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffu-\nsion model for controllable video synthesis. arXiv preprint\narXiv:2304.14404, 2023.\n[19] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000\u201318010, 2023.\n[20] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian\nCurless, David H Salesin, and Richard Szeliski. Animat-\ning pictures with stochastic motion textures. In ACM SIG-\nGRAPH 2005 Papers, pages 853\u2013860. 2005.\n[21] Vincent C Couture, Michael S Langer, and Sebastien Roy.\nOmnistereo video textures without ghosting. In 2013 Inter-\nnational Conference on 3D Vision-3DV 2013, pages 64\u201370.\nIEEE, 2013.\n[22] Abe Davis, Justin G Chen, and Fr\u00b4edo Durand. Image-space\nmodal bases for plausible manipulation of objects in video.\nACM Transactions on Graphics (TOG), 34(6):1\u20137, 2015.\n[23] Myers Abraham Davis. Visual vibration analysis. PhD\nthesis, Massachusetts Institute of Technology, 2016.\n[24] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021.\n[25] Julien Diener, Mathieu Rodriguez, Lionel Baboud, and Li-\nonel Reveret. Wind projection basis for real-time animation\nof trees. In Computer graphics forum, volume 28, pages\n533\u2013540. Wiley Online Library, 2009.\n[26] Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and\nStefano Soatto. Dynamic textures. International journal of\ncomputer vision, 51:91\u2013109, 2003.\n[27] Michael Dorkenwald, Timo Milbich, Andreas Blattmann,\nRobin Rombach, Konstantinos G. Derpanis, and Bjorn Om-\nmer. Stochastic image-to-video synthesis using cinns. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 3742\u20133753,\nJune 2021.\n[28] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,\nAli Thabet, and Artsiom Sanakoyeu. Avatars grow legs:\nGenerating smooth human motion from sparse tracking in-\nputs with diffusion model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 481\u2013490, 2023.\n[29] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama.\nAnimating landscape: Self-supervised learning of decoupled\nmotion and appearance for single-image video synthesis.\nACM Transactions on Graphics (Proceedings of ACM SIG-\nGRAPH Asia 2019), 38(6):175:1\u2013175:19, 2019.\n[30] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023.\n[31] Matthew Flagg, Atsushi Nakazawa, Qiushuang Zhang,\nSing Bing Kang, Young Kee Ryu, Irfan Essa, and James M\nRehg. Human video textures. In Proceedings of the 2009\nsymposium on Interactive 3D graphics and games, pages\n199\u2013206, 2009.\n[32] Jean-Yves Franceschi, Edouard Delasalles, Micka\u00a8el Chen,\nSylvain Lamprier, and Patrick Gallinari. Stochastic latent\nresidual video prediction. In International Conference on\nMachine Learning, pages 3233\u20133246. PMLR, 2020.\n[33] Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2Flow:\nMotion hallucination from static images for action recog-\nnition. In Proc. Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[34] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer.\narXiv preprint arXiv:2204.03638,\n2022.\n[35] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023.\n[36] Isma Hadji and Richard P Wildes. A new large scale dy-\nnamic texture dataset with application to convnet under-\nstanding. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 320\u2013335, 2018.\n[37] Zekun Hao, Xun Huang, and Serge Belongie. Controllable\nvideo generation with sparse trajectories. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7854\u20137863, 2018.\n[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016.\n[39] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story: Storytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023.\n[40] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022.\n[41] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017.\n[42] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022.\n[43] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020.\n[44] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\n[45] Aleksander Holynski, Brian L Curless, Steven M Seitz, and\nRichard Szeliski. Animating pictures with Eulerian motion\nfields. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5810\u20135819,\n2021.\n[46] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling. Trans. Mach. Learn. Res., 2022, 2022.\n[47] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and\nShuchang Zhou. A dynamic multi-scale voxel flow network\nfor video prediction. ArXiv, abs/2303.09875, 2023.\n[48] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-\ntual losses for real-time style transfer and super-resolution.\nIn Computer Vision\u2013ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part II 14, pages 694\u2013711. Springer, 2016.\n[49] Hitoshi Kanda and Jun Ohya. Efficient, realistic method\nfor animating dynamic behaviors of 3d botanical trees. In\n2003 International Conference on Multimedia and Expo.\nICME\u201903. Proceedings (Cat. No. 03TH8698), volume 2,\npages II\u201389. IEEE, 2003.\n[50] Johanna Karras, Aleksander Holynski, Ting-Chun Wang,\nand Ira Kemelmacher-Shlizerman. Dreampose: Fashion\nimage-to-video synthesis via stable diffusion. arXiv preprint\narXiv:2304.06025, 2023.\n[51] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017.\n[52] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,\nChelsea Finn, and Sergey Levine. Stochastic adversarial\nvideo prediction. arXiv preprint arXiv:1804.01523, 2018.\n[53] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,\nNoah Snavely, Ce Liu, and William T Freeman. Learn-\ning the depths of moving people by watching frozen people.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 4521\u20134530, 2019.\n[54] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pages 2041\u20132050, 2018.\n[55] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4273\u20134284,\n2023.\n[56] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo\nKanazawa. Infinitenature-zero: Learning perpetual view gen-\neration of natural scenes from single images. In European\nConference on Computer Vision, pages 515\u2013534. Springer,\n2022.\n[57] Jing Liao, Mark Finch, and Hugues Hoppe. Fast computation\nof seamless video loops. ACM Transactions on Graphics\n(TOG), 34(6):1\u201310, 2015.\n[58] Zicheng Liao, Neel Joshi, and Hugues Hoppe. Automated\nvideo looping with progressive dynamism. ACM Transac-\ntions on Graphics (TOG), 32(4):1\u201310, 2013.\n[59] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Maka-\ndia, Noah Snavely, and Angjoo Kanazawa. Infinite nature:\nPerpetual view generation of natural scenes from a single\nimage. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14458\u201314467, 2021.\n[60] Ce Liu. Beyond pixels: exploring new representations and\napplications for motion analysis. PhD thesis, Massachusetts\nInstitute of Technology, 2009.\n[61] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan. Videofusion: Decomposed diffusion models\nfor high-quality video generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023.\n[62] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable\nanimation of fluid elements in still images. In Proc. Com-\nputer Vision and Pattern Recognition (CVPR), 2022.\n[63] Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit\nwarping for animation with image sets. Advances in Neural\nInformation Processing Systems, 35:22438\u201322450, 2022.\n[64] Medhini Narasimhan, Shiry Ginosar, Andrew Owens,\nAlexei A Efros, and Trevor Darrell. Strumming to the beat:\nAudio-conditioned contrastive video textures. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 3761\u20133770, 2022.\n[65] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18444\u201318455, 2023.\n[66] Simon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5437\u20135446, 2020.\n[67] Shin Ota, Machiko Tamura, Kunihiko Fujita, T Fujimoto,\nK Muraoka, and Norishige Chiba. 1/f/sup/spl beta//noise-\nbased real-time animation of trees swaying in wind fields. In\nProceedings Computer Graphics International 2003, pages\n52\u201359. IEEE, 2003.\n[68] Automne Petitjean, Yohan Poirier-Ginter, Ayush Tewari,\nGuillaume Cordonnier, and George Drettakis. Modalnerf:\nNeural modal analysis and synthesis for free-viewpoint navi-\ngation in dynamically vibrating scenes. In Computer Graph-\nics Forum, volume 42, 2023.\n[69] Silvia L. Pintea, Jan C. van Gemert, and Arnold W. M.\nSmeulders. D\u00b4ej`a vu: Motion prediction in static images. In\nProc. European Conf. on Computer Vision (ECCV), 2014.\n[70] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H\nBermano, and Daniel Cohen-Or. Single motion diffusion.\narXiv preprint arXiv:2302.05905, 2023.\n[71] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022.\n[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022.\n[73] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with\ndeep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\n[74] Payam Saisan, Gianfranco Doretto, Ying Nian Wu, and Ste-\nfano Soatto. Dynamic texture recognition. In Proceedings\nof the 2001 IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition. CVPR 2001, volume 2,\npages II\u2013II. IEEE, 2001.\n[75] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek\nKar, Mohammad Norouzi, Deqing Sun, and David J. Fleet.\nThe surprising effectiveness of diffusion models for optical\nflow and monocular depth estimation, 2023.\n[76] Arno Sch\u00a8odl, Richard Szeliski, David H Salesin, and Irfan\nEssa. Video textures. In Proceedings of the 27th annual con-\nference on Computer graphics and interactive techniques,\npages 489\u2013498, 2000.\n[77] Mikio Shinya and Alain Fournier.\nStochastic motion\u2014\nmotion under the influence of wind. Computer Graphics\nForum, 11(3), 1992.\n[78] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. Animating arbitrary objects\nvia deep motion transfer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2377\u20132386, 2019.\n[79] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for im-\nage animation. Advances in neural information processing\nsystems, 32, 2019.\n[80] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov.\nMotion representations for\narticulated animation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13653\u201313662, 2021.\n[81] Chen Qian Kwan-Yee Lin Hongsheng Li Siming Fan, Jing-\ntan Piao. Simulating fluids in real-world still images. arXiv\npreprint, arXiv:2204.11335, 2022.\n[82] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3626\u20133636, 2022.\n[83] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n[84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. arXiv:2010.02502, October\n2020.\n[85] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020.\n[86] Jos Stam. Multi-scale stochastic modelling of complex natu-\nral phenomena. PhD thesis, 1995.\n[87] Jos Stam. Stochastic dynamics: Simulating the effects of\nturbulence on flexible structures. Computer Graphics Forum,\n16(3), 1997.\n[88] Ryusuke Sugimoto, Mingming He, Jing Liao, and Pedro V\nSander. Water simulation and rendering from a still photo-\ngraph. In SIGGRAPH Asia 2022 Conference Papers, pages\n1\u20139, 2022.\n[89] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion\ndiffusion model. arXiv preprint arXiv:2209.14916, 2022.\n[90] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly.\nTowards accurate generative models of video: A new metric\n& challenges. arXiv preprint arXiv:1812.01717, 2018.\n[91] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher\nPal. Mcvd: Masked conditional video diffusion for predic-\ntion, generation, and interpolation. In (NeurIPS) Advances\nin Neural Information Processing Systems, 2022.\n[92] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. In Neural Informa-\ntion Processing Systems, 2016.\n[93] Neal Wadhwa, Michael Rubinstein, Fr\u00b4edo Durand, and\nWilliam T Freeman. Phase-based video motion process-\ning. ACM Transactions on Graphics (ToG), 32(4):1\u201310,\n2013.\n[94] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial\nHebert. An uncertain future: Forecasting from static images\nusing variational autoencoders. In Proc. European Conf. on\nComputer Vision (ECCV), 2016.\n[95] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense\noptical flow prediction from a static image. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 2443\u20132451, 2015.\n[96] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Videocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023.\n[97] Yaohui Wang, Di Yang, Francois Bremond, and Antitza\nDantcheva.\nLatent image animator: Learning to ani-\nmate images via latent space navigation. arXiv preprint\narXiv:2203.09043, 2022.\n[98] Frederik Warburg, Ethan Weber, Matthew Tancik, Alek-\nsander Holynski, and Angjoo Kanazawa. Nerfbusters: Re-\nmoving ghostly artifacts from casually captured nerfs. arXiv\npreprint arXiv:2304.10532, 2023.\n[99] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models. arXiv preprint\narXiv:2210.04628, 2022.\n[100] Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-\nShlizerman. Photo wake-up: 3d character animation from a\nsingle photo. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5908\u20135917,\n2019.\n[101] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF:\nRegularizing Neural Radiance Fields with Denoising Diffu-\nsion Models. In CVPR, 2023.\n[102] Tianfan Xue, Jiajun Wu, Katherine L Bouman, and\nWilliam T Freeman. Visual dynamics: Stochastic future gen-\neration via layered cross convolutional networks. Trans. Pat-\ntern Analysis and Machine Intelligence, 41(9):2236\u20132250,\n2019.\n[103] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023.\n[104] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18456\u201318466,\n2023.\n[105] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-\nmodiffuse: Retrieval-augmented motion diffusion model.\narXiv preprint arXiv:2304.01116, 2023.\n[106] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation. arXiv\npreprint arXiv:2305.13077, 2023.\n[107] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3657\u20133666, 2022.\n[108] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image com-\npletion via co-modulated generative adversarial networks.\nIn International Conference on Learning Representations\n(ICLR), 2021.\n[109] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022.\n"
  },
  {
    "title": "Agents: An Open-source Framework for Autonomous Language Agents",
    "link": "https://arxiv.org/pdf/2309.07870.pdf",
    "upvote": "39",
    "text": "Agents: An Open-source Framework\nfor Autonomous Language Agents\nWangchunshu Zhou1\u2217 Yuchen Eleanor Jiang1*\nLong Li1*\nJialong Wu1*\nTiannan Wang1\nShi Qiu1\nJintian Zhang1\nJing Chen1\nRuipu Wu1\nShuai Wang1\nShiding Zhu1\nJiyu Chen1\nWentao Zhang1\nXiangru Tang\nNingyu Zhang2\nHuajun Chen2\nPeng Cui3\nMrinmaya Sachan3\n1AIWaves Inc.\n2Zhejiang University\n3ETH Z\u00fcrich\nAbstract\nRecent advances on large language models (LLMs) enable researchers and de-\nvelopers to build autonomous language agents that can automatically solve var-\nious tasks and interact with environments, humans, and other agents using nat-\nural language interfaces. We consider language agents as a promising direc-\ntion towards artificial general intelligence and release AGENTS, an open-source\nlibrary with the goal of opening up these advances to a wider non-specialist\naudience.\nAGENTS is carefully engineered to support important features in-\ncluding planning, memory, tool usage, multi-agent communication, and fine-\ngrained symbolic control. AGENTS is user-friendly as it enables non-specialists\nto build, customize, test, tune, and deploy state-of-the-art autonomous language\nagents without much coding. The library is also research-friendly as its modu-\nlarized design makes it easily extensible for researchers. AGENTS is available at\nhttps://github.com/aiwaves-cn/agents.\n1\nIntroduction\n\u201cAn autonomous agent is a system situated within and a part of an environment that senses the environment\nand acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.\u201d\nIs it an Agent, or just a Program?: A Taxonomy for Autonomous Agents [Franklin and Graesser, 1996]\nLarge Language Models (LLMs) [Brown et al., 2020, Ouyang et al., 2022, OpenAI, 2023] such as\nChatGPT make it possible to build autonomous agents that can automatically solve complicated\ntasks and interact with the environment, humans, or other agents by perceiving, reasoning, planning,\nand acting in the world [Weng, 2023]. Language agents are a promising step towards artificial\ngeneral intelligence (AGI) and can help reduce human effort in certain roles such as customer service,\nconsulting, programming, writing, teaching, etc. Some recent demos such as AutoGPT [Richards and\net al., 2023] and BabyAGI [Nakajima, 2023] have demonstrated the potential of language agents and\nhave gained massive interest from developers, researchers, as well as more non-technical audiences.\nWhile intriguing, most of these demos or repositories are not friendly for customizing, tuning, and\ndeploying new agents even for experienced developers or researchers. This limitation comes from\nthe fact that these demos typically proof-of-concepts showcasing the possibility of language agents,\ninstead of being larger frameworks that can be used to build and customize language agents over time.\nMoreover, most of these open-source repositories only cover a small portion of the core abilities of\nlanguage agents including task decomposition [Nye et al., 2022], long-short term memory [Zhou\n\u2217Equal Contribution. Correspondence to: chunshu@aiwaves.cn\nPreprint. Work in progress.\narXiv:2309.07870v3  [cs.CL]  12 Dec 2023\nSOP\nPlanning\nPlanning\nWeb\nNavigation\nWeb\nNavigation\nTool  Use\nHuman-Agent\n   Interaction\nTool  Use\nMulti-Agent\nAgent 1\nAgent 2\n\uff08e.g  Editor\uff09\n\uff08e.g  Writer\uff09\nCommunication\nFigure 1: Illustration of the AGENTS framework.\net al., 2023a], web navigation [Nakano et al., 2021], tool usage [Schick et al., 2023], and multi-\nagent communication [Foerster et al., 2016]. In addition, most (if not all) existing language agent\nframeworks solely depend on a short task description and rely completely on the abilities of LLMs to\nplan and act. This results in significant randomness and inconsistency across different runs, delivering\nan unsatisfactory user experience and making it hard to customize and tune language agents.\nWe believe the aforementioned limitations are important barriers for recent advances in language\nagents to reach a broader non-specialist audience and impact our society in a positive way. To this\nend, we release AGENTS, an open-source library and framework for language agents dedicated to\nsupporting LLM-powered language agents. AGENTS\u2019s philosophy is to make customizing, tuning,\nand deploying language agents as simple as possible even for non-specialists while also remaining\neasily extensible for developers and researchers. In addition, the library also provides the following\nkey features that make it a versatile framework for language agents:\nLong-short term memory\nAccording to Franklin and Graesser [1996], a key difference between\nautonomous agents and computer programs (or machine learning models) is that machine learning\nmodels only need to respond to a single input/query, while autonomous agents need to interact with\nenvironments or other agents over time. Therefore, the ability to maintain long-short term memory is\nvery important for autonomous agents. AGENTS integrates the memory components in [Zhou et al.,\n2023a] and enables language agents to store and retrieve long-term memory with VectorDB and\nsemantic search, and regularly update a short-term working memory with a scratchpad. Users can\nchoose to equip an agent with long-term memory, short-term memory, or both of them by simply\nfilling in a field in the config file.\nTool usage & Web navigation\nAnother important feature for autonomous agents is the ability to\nuse external tools and surf the internet. This is especially important for language agents because\nthey rely on the language interface and thus need to use external tools to interact with environments\nbeyond language communication and navigate the web to gather useful information. Following [Patil\net al., 2023], AGENTS supports a few commonly used external APIs and provides an abstract class\nthat enables developers to integrate other tools with ease. We also enable agents to navigate the\ninternet and gather information by defining web search and web navigation as specialized APIs.\nMulti-agent communication\nIn addition to single-agent abilities, AGENTS also supports customiz-\ning multi-agent systems, which can be helpful for certain applications such as games [Park et al.,\n2023], social experiments [Li et al., 2023], software development [Qian et al., 2023], etc. One new\nfeature for multi-agent communication in AGENTS is the \u201cdynamic scheduling\u201d feature. Instead of\n2\nscheduling the order for the agents to act with hard-coded rules, dynamic scheduling provides an\noption to define a controller agent that acts as a \u201cmoderator\u201d and decides which agent to perform\nthe next action considering their roles and the current history. Dynamic scheduling has the potential\nto make communication between multiple agents more natural and flexible. Developers can easily\ncustomize the controller by specifying its rule in the config file using natural language.\nHuman-agent interaction\nOne limitation in existing agent frameworks is that while they enable\nagents, or multi-agents, to automatically solve tasks, it\u2019s not easy or even possible for human users\nto interact with the agents, especially in the multi-agent scenario. AGENTS seamlessly supports\nhuman-agent interaction in both single-agent and multi-agent scenarios, making it possible for one or\nmore humans to communicate and interact with language agents.\nControllabilty\nExisting agent frameworks generally define and control the agents\u2019 behavior only\nusing a system prompt and then let the agent plan and act on its own. In contrast, AGENTS provides a\nnovel paradigm to build controllable agents via a symbolic plan, also referred to as standard operating\nprocedures (SOPs). An SOP is a graph of multiple states that defines different situations an agent may\nencounter while accomplishing a task, and the transition rules between the states. Similar to SOPs\nin the real world, an SOP in AGENTS is a meticulously documented set of step-by-step instructions\nthat outlines how a particular task or process should be performed by an agent or a group of agents.\nSOPs can be generated by an LLM and edited by the user when customizing and tuning the agent.\nAfter deployment, an agent will behave following specified instructions and guidelines for each state\nand dynamically adjust its current state according to its interaction with the environment, humans, or\nother agents. The introduction of the symbolic plan offers the opportunity to provide fine-grained\ncontrol of an agent\u2019s behavior, making agents\u2019 behavior more stable/predictable and facilitating\ntuning/optimizing agents at the same time.\nIn addition, we propose an automated SOP generation pipeline to reduce human labor on writing\ndetailed SOP and config files when customizing (multi-) agent systems. The automated SOP\ngeneration pipeline is a \u201cmeta agent\u201d that can generate config files for language agents with retrieval-\naugmented generation given a short description of the task.\nAGENTS is an ongoing effort maintained by researchers and engineers from AIWaves2. We look for-\nward to support from community contributors on the project. The library and detailed documentation\nand tutorials are available on GitHub3.\n2\nRelated Work\n2.1\nAutonomous Language Agents\nThe concept of language agents has become very popular recently and a variety of language agents\ntargeting different tasks have been proposed. For example, Generative Agents [Park et al., 2023]\ndeveloped language agents to mimic human social behavior, WebAgent [Gur et al., 2023] demon-\nstrated the possibility to build language agents that can complete the tasks on real websites following\nnatural language instructions, Qian et al. [2023] and MetaGPT [Hong et al., 2023] experimented with\nsoftware development in multi-agent communication settings, and Zhou et al. [2023a] built language\nagents that act as interactive writing assistants.\nIn addition to language agents that target specific tasks, recent open-source projects such as Auto-\nGPT [Richards and et al., 2023], BabyAGI [Nakajima, 2023], and SuperAGI [SuperAGI, 2023] are\naimed at the goal of building autonomous agents that do whatever users want and attracted massive\ninterest from both developers and non-specialist audiences.\n2.2\nLanguage Agents Frameworks\nMore recently, a few open-source frameworks for language agents have been proposed. For example,\nTransformers Agents [Wolf et al., 2020] builds language agents that can automatically use tools\nto solve tasks described in natural language; LangChain [LangChain, 2022] supports end-to-end\n2https://www.aiwaves.org/\n3https://github.com/aiwaves-cn/agents\n3\nTable 1: Comparison of Language Agent Frameworks\nFramework\nTool Usage\nLong-short Term Memory\nMulti-Agent\nHuman-Agent Interaction\nSymbolic Control\nTransformers Agents\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nLangChain\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nAuto-GPT\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nGentopia\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nXLang\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nMeta-GPT\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\nCamel\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\nAgentVerse\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nAGENTS\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nlanguage agents that can automatically solve tasks specified in natural language; Camel [Li et al.,\n2023] and AgentVerse [Chen et al., 2023] are platforms tailored for building multi-agent systems;\nGentopia [Xu et al., 2023] and XLang4 are libraries for building tool-augmented agents. We illustrate\nthe key features supported by these platforms and AGENTS in Table 1. We can see that AGENTS is the\nonly framework that supports tool usage, long-short term memory, and multi-agent communication at\nthe same time. AGENTS also offers human-agent interaction and controllability through symbolic\nplans (SOPs) for the first time.\n3\nLibrary Design\nCode 1: Exemplar code for initializing and running a (multi) agent system with AGENTS\n1\ndef main ()\n2\n# agents is a dict of one or multiple\nagents.\n3\nagents = Agent.from_config(\"./ config.json\")\n4\nsop = SOP.from_config(\"./ config.json\")\n5\nenvironment = Environment.from_config(\"./ config.json\")\n6\nrun(agents ,sop ,environment)\nAGENTS is designed following the philosophy in Franklin and Graesser [1996]: \u201can autonomous\nagent is situated in an environment\u201d. Therefore, agent and environment are two major classes in\nthe AGENTS framework. In addition to these two classes, we also include a class for symbolic plans,\nnamed SOP (short for Standard Operating Procedure), to make language agents more controllable.\nThese main classes are all initialized from a config file which can be filled in plain text. In sum, a\ntypical script for initializing and running a (multi) agent system with AGENTS is illustrated in Code\n1. The config file not only defines these core objects but also factorizes complicated prompts into\nmodularized prompt components. The factorization of prompts significantly reduces the expertise\nrequirements and efforts for users to build (multi) agent systems. Using a single config file to define\nthe agents, plan, and basic environment also facilitates the sharing of language agents (which will be\ndiscussed in the Agent Hub section). Each of these three core classes consist of standardized APIs\nthat can be overwritten by experienced developers and researchers. We describe these classes in\ndetail:\nCode 2: Exemplar code for the running loop of a (multi) agent system in AGENTS\n1\ndef run(agents ,sop ,environment):\n2\nwhile not sop.finished:\n3\nagent ,state=sop.step(agents , environment)\n4\naction=agent.step(state ,environment)\n5\nenvironment.update(agent ,action)\n6\n# optional , in case of dynamic\nplanning\n7\n# new_states = get_new_states(action)\n8\n# sop.add_states(new_states)\n4https://github.com/xlang-ai/xlang\n4\n3.1\nAgent\nThe Agent class abstracts a language agent. Its UML is illustrated in Figure 1. We can see\nthat an agent maintains its long-short term memory and has methods to observe the environment\n(agent._observe(environment)), act according to its current state (agent._act()) and update\nits memory (agent._update_memory()). All these methods are wrapped in the agent.step()\nmethod. This factorization enables developers to customize agents with new functionalities easily.\nUnlike existing language agent frameworks that assume an agent must be based on an LLM, we\ninclude a \u201c_is_human\u201d property to an agent. If it is set to \u201cTrue\u201d, the (agent._act()) will opt to\nprovide observations and memory information to a human user and wait for the human user to input\nthe action. This design allows flexible human-agent interaction in both single-agent and multi-agent\nsystems by allowing human users to take the role of one or more language agents. It facilitates\ndevelopers to build various interesting applications such as allowing human users to act as members\nof a team in debate and collaborate with (agent or human-based) teammates to beat another team, or\nact as CTO/engineers in a software company and collaborate with others for software development.\n3.2\nSOP\nThe SOP class contains a graph of the states of agents. Each state specifies a certain sub-task or\nsub-goal of all agents when accomplishing the task described by the SOP. States are abstracted into\na State class. A State object contains modularized prompts for the agent to leverage an LLM\nand various tools or APIs that an agent can use in the state. We abstract everything an agent may\nuse for action in a state into a \u201cComponent\u201d class. The Component class consists of two subclasses\ncorresponding to different parts of the prompt and tools or external APIs, named \u201cPromptComponent\u201d\nand \u201cToolComponent\u201d, respectively. PromptComponent includes modularized prompts that specify\nthe task/goal, rules/constraints, (step-by-step) demonstrations for in-context learning, and the output\nformat. ToolComponent supports more complex usage beyond modularized prompts, including\nexternal tools and APIs such as web search, knowledge bases, etc. The results of the tools are either\nincluded in the prompt or directly returned and processed afterward, according to the config file.\nAn SOP object also includes an LLM-based control function that decides the transition between\ndifferent states and the next agent to act. The state transit function is named sop._transit()\nand the agent routing function is named sop._route(). Both of the functions are wrapped in an\nsop.next() function which is used in the main loop.\nFigure 2: (a) Customer service agent\nFigure 3: (b) Sales agent\n3.3\nEnvironment\nThe Environment class abstracts the environment in which the agents are situated. An environment\nconsists of two main functions:\nenvironment._observed() and environment.update().\nenvironment._observed() defines how the environment influences the agent\u2019s action (i.e., what\ninformation should be transferred to the agent upon observation, and environment.update()\ndefines how the agent\u2019s action impacts the environment.\n5\nFigure 4: Multi-Agent System: Fiction Studio.\nThe execution logic of a (multi) agent system based on AGENTS is very intuitive. As illustrated in\nCode 2, in each iteration, the SOP first decides the state transition and selects the next agent to act\nbased on the agents and the environment. The agent then takes an action based on its state and the\nenvironment. Then the environment updates itself based on the new action. Finally, if a workflow\nrequires dynamically adjusting the plan based on the intermediate execution results, one can parse\nthe output from an action, define a new state and add it into the current SOP.\n3.4\nImplementation Details of Core Features\nLong-short Term Memory\n: AGENTS implements long-short term memories for language agents\nfollowing Zhou et al. [2023a]. Specifically, long-term memories are action histories and are embedded\nby sentence-transformers [Reimers and Gurevych, 2019], stored in a VectorDB, and queried via\nsemantic search. Short-term memories, or working memories, are in natural language form and\nupdated by an LLM via a carefully tuned prompt.\nTool Usage & Web Navigation\n:\nAGENTS supports tool usage and web navigation via\nToolComponents.\nFor each external tool or API, developer can wrap the API call in\ntheToolComponent.func() method. For complicated tools of which the API call is context-\ndependent, AGENTS integrates the the \u201cFunction-calling\u201d feature of OpenAI\u2019s GPT APIs to let\nLLMs decide how to use the tools. Web navigation is achieved by implementing web search as a\nspecialized tool.\nMulti-Agent Communication\n: Different from most existing frameworks for multi-agent systems\nthat use pre-defined rules (e.g., let each agent act in a sequential order) to control the order for agents\u2019\naction, AGENTS includes a controller function that dynamically decides which agent will perform the\nnext action using an LLM by considering the previous actions, the environment, and the target of the\ncurrent states. This makes multi-agent communication more flexible.\nHuman-Agent Interaction\n: AGENTS supports human-agent interaction in multi-agent systems by\nallowing human users to change the \u201cis_human\u201d field for a certain agent in the config file to \u201cTrue\u201d.\nIn this case, the user can play the role of the agent by himself/herself and input his/her own actions\nand interact with other language agents in the environment.\n6\nFigure 5: Human-Agent Interaction in a debate.\n3.5\nDeployment\nExisting open-source frameworks for language agents focus on building proof-of-concept language\nagents that run either in the terminal or on Gradio [Abid et al., 2019]. In contrast, AGENTS supports\ndeploying language agents as APIs with FastAPI5. This greatly facilitates developers to integrate\nlanguage agents in real-world applications.\n3.6\nThe Agent Hub\nAGENTS aims to not only facilitate the development, testing, and tuning of a language agents system\nbut also makes the distribution and sharing of language agents easier. To this end, we introduce\nAGENT HUB, a platform that allows users to share their fine-tuned language agents as well as\nsearch/download useful language agents that others share on the platform. In this way, one can easily\ncustomize language agents by starting from community agents and slightly modifying them. This\ngreatly reduces the effort of designing, testing, and tuning language agents from scratch.\n3.7\nAutomatic Creation of Agent Systems\nWhile using an SOP to provide fine-grained control to language agents, it can sometimes be laborsome\nfor users to manually specify the SOP from scratch since it requires to set different states, their\nconnections, and the prompts and tools for each Component for all states. Therefore, we carefully\nimplement a pipeline for automatic SOP generation. Our SOP generation framework is based on\nretrieval-augmented generation (RAG) [Lewis et al., 2020]. The SOP generation pipeline itself is\nalso based on the AGENTS framework and has an SOP of first specifying the agents required, then\nplanning the states and their connections, and finally generating the Components. Therefore, this\npipeline can be regarded as a \u201cmeta agent\u201d that can create other agents and multi-agent systems.\nDetailed description of the automatic agent creation framework is decribed in [Zhou et al., 2023b].\n5https://fastapi.tiangolo.com/\n7\n4\nCase Studies\nWe then present a few case studies on different language agents built with the library, including\nsingle-agent systems, multi-agent systems, and systems that require human-agent interaction. All\ndemos are available at http://www.aiwaves-agents.com/.\n4.1\nSingle-agent systems\nWe implement a few single-agent systems with AGENTS including a chit-chat bot, two customer\nservice agents based on knowledge bases and web search engines, a shopping assistant agent, and a\nsales agent. The agents demonstrate different features of the library and the possibility of building\nlanguage agents of different use cases using AGENTS. We present a screenshot of the customer\nservice agent and the sales agent in Figure 2 and 3, respectively.\n4.2\nMulti-agent systems\nWe also demonstrate how one can build a multi-agent system consisting of multiple agents interacting\nwith each other in an environment. We select three scenarios including a fiction studio, a debate, and\na software company. These scenarios include both cooperative and competitive scenarios, which\nare two main categories of multi-agent systems. All of the scenarios include multiple subtasks that\nare controlled through symbolic plans, i.e., SOPs. One can easily observe the language agents\u2019\nbehavior in each subtask and engineer the corresponding prompts to customize and improve the\nsystem. We present a system screenshot of the fiction studio system in Figure 4. We also showcase\nthe human-agent interaction feature of the framework in a case study where a human user participate\nin a debate with language agents in Figure 5.\n5\nConclusion\nLLMs and language agents powered by them are playing increasingly important roles in both the\nNLP/AI community and our society in general. AGENTS, is a unified framework and open-source\nlibrary for language agents. AGENTS aims to facilitate developers to build applications with language\nagents, researchers to conduct language agents research, and general non-technical audiences to build\nand customize personalized language agents.\nReferences\nStan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous\nagents. In International workshop on agent theories, architectures, and languages, pages 21\u201335.\nSpringer, 1996.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh,\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information\nProcessing Systems, 2022. URL https://openreview.net/forum?id=TG8KACxEON.\nOpenAI. GPT-4 technical report, 2023.\n8\nLilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https:\n//lilianweng.github.io/posts/2023-06-23-agent/.\nToran Bruce Richards and et al. Auto-gpt: An autonomous gpt-4 experiment, 2023. URL https:\n//github.com/Significant-Gravitas/Auto-GPT. [Software].\nYohei Nakajima. Babyagi, 2023. URL https://github.com/yoheinakajima/babyagi. [Soft-\nware].\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\nmodels, 2022. URL https://openreview.net/forum?id=iedYJm92o0a.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou,\nRyan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long\ntext, 2023a.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\nBrowser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. CoRR, abs/2302.04761, 2023.\nJakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to commu-\nnicate with deep multi-agent reinforcement learning. In NIPS, pages 2137\u20132145, 2016.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for \"mind\" exploration of large scale language model society, 2023.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development, 2023.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\nAleksandra Faust. A real-world webagent with planning, long context understanding, and program\nsynthesis, 2023.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu.\nMetagpt: Meta programming for multi-agent collaborative framework, 2023.\nSuperAGI. Superagi, 2023. URL https://github.com/TransformerOptimus/SuperAGI. [Soft-\nware].\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttps://aclanthology.org/2020.emnlp-demos.6.\nLangChain. Langchain repository. https://github.com/langchain-ai/langchain, 2022.\n9\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia\nQin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating\nmulti-agent collaboration and exploring emergent behaviors in agents, 2023.\nBinfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu,\nZiyu Yao, and Dongkuan Xu. Gentopia: A collaborative platform for tool-augmented llms, 2023.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERT-\nNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\nabs/1908.10084.\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou.\nGradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569,\n2019.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS, 2020.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian\nZhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang,\nHuajun Chen, Peng Cui, and Mrinmaya Sachan. Towards language agents uniting connectionism\nand symbolism. 2023b. To be published.\n10\n"
  },
  {
    "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts",
    "link": "https://arxiv.org/pdf/2309.07430.pdf",
    "upvote": "24",
    "text": "Adapted Large Language Models Can Outperform\nMedical Experts in Clinical Text Summarization\nDave Van Veen\u2020, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali\u2021,\nChristian Bluethgen, Anuj Pareek, Malgorzata Polacin, William Collins, Neera Ahuja,\nCurtis P. Langlotz, Jason Hom, Sergios Gatidis, John Pauly, Akshay S. Chaudhari\nStanford University\nAnalyzing vast textual data and summarizing key information from electronic health records imposes\na substantial burden on how clinicians allocate their time. Although large language models (LLMs)\nhave shown promise in natural language processing (NLP), their effectiveness on a diverse range of\nclinical summarization tasks remains unproven. In this study, we apply adaptation methods to eight\nLLMs, spanning four distinct clinical summarization tasks: radiology reports, patient questions, progress\nnotes, and doctor-patient dialogue. Quantitative assessments with syntactic, semantic, and conceptual\nNLP metrics reveal trade-offs between models and adaptation methods. A clinical reader study with\nten physicians evaluates summary completeness, correctness, and conciseness; in a majority of cases,\nsummaries from our best adapted LLMs are either equivalent (45%) or superior (36%) compared to\nsummaries from medical experts. The ensuing safety analysis highlights challenges faced by both LLMs\nand medical experts, as we connect errors to potential medical harm and categorize types of fabricated\ninformation. Our research provides evidence of LLMs outperforming medical experts in clinical text\nsummarization across multiple tasks. This suggests that integrating LLMs into clinical workflows could\nalleviate documentation burden, allowing clinicians to focus more on patient care.\n1 Introduction\nDocumentation plays an indispensable role in healthcare practice. Currently, clinicians spend significant time\nsummarizing vast amounts of textual information\u2014whether it be compiling diagnostic reports, writing progress\nnotes, or synthesizing a patient\u2019s treatment history across different specialists [1\u20133]. Even for experienced\nphysicians with a high level of expertise, this intricate task naturally introduces the possibility for errors,\nwhich can be detrimental in healthcare where precision is paramount [4\u20136].\nThe widespread adoption of electronic health records has expanded clinical documentation workload, directly\ncontributing to increasing stress and clinician burnout [7\u20139]. Recent data indicates that physicians can expend\nup to two hours on documentation for each hour of patient interaction [10]. Meanwhile, documentation\nresponsibilities for nurses can consume up to 60% of their time and account for significant work stress [11\u201313].\nThese tasks divert attention from direct patient care, leading to worse outcomes for patients and decreased\njob satisfaction for clinicians [2, 14\u201316].\nIn recent years, large language models (LLMs) have gained remarkable traction, leading to widespread\nadoption of models such as ChatGPT [17], which excel at information retrieval, nuanced understanding, and\ntext generation [18, 19]. Although LLM benchmarks for general natural language processing (NLP) tasks\nexist [20, 21], they do not evaluate performance on relevant clinical tasks. Addressing this limitation presents\nan opportunity to accelerate the process of clinical text summarization, hence alleviating documentation\nburden and improving patient care.\nCrucially, machine-generated summaries must be non-inferior to that of seasoned clinicians\u2014especially when\nused to support sensitive clinical decision-making. Previous work has demonstrated potential across clinical\nNLP tasks [22, 23], adapting to the medical domain by either training a new model [24, 25], fine-tuning an\n\u2020 Corresponding author: vanveen@stanford.edu\n\u2021 Affiliation: The University of Texas at Austin\narXiv:2309.07430v4  [cs.CL]  15 Feb 2024\n \n \nMethods\nTasks\npatient health questions\nradiology reports\nprogress notes\npatient/doctor dialogue\nused for adaptation\nQLoRA\nin-context learning\n\u00d7 \n\u00d7 \nbest\nmodel/method\n \nModels\nproprietary\nopen-source\nFLAN-T5\nMed-Alpaca\nAlpaca\nFLAN-UL2\nLlama-2\nVicuna\nGPT-3.5\nGPT-4\nClinical\nreader study \nto evaluate correctness,\ncompleteness, and conciseness \nvia ten physicians \nfive hospitalists \nfive radiologists \nQuantitative \nevaluation \nto measure syntactic, semantic,  \nand conceptual similarity \nvia NLP metrics \nbest\nmodel/method\nSafety \nanalysis \nto quantify potential medical\nharm and categorize fabricated \ninformation \nFigure 1 | Framework overview. First, we quantitatively evaluate each valid combination (\u00d7) of LLM and adaptation method\nacross four distinct summarization tasks comprising six datasets. We then conduct a clinical reader study in which ten physicians\ncompare summaries of the best model/method against those of a medical expert. Lastly, we perform a safety analysis to quantify\npotential medical harm and to categorize types of fabricated information.\nexisting model [26, 27], or supplying domain-specific examples in the model prompt [27, 28]. However, adapting\nLLMs to summarize a diverse set of clinical tasks has not been thoroughly explored, nor has non-inferiority to\nmedical experts been achieved. With the overarching objective of bringing LLMs closer to clinical readiness,\nwe make the following contributions:\n\u2022 We implement adaptation methods across eight open-source and proprietary LLMs for four distinct\nsummarization tasks comprising six datasets. The subsequent evaluation via NLP metrics provides a\ncomprehensive assessment of contemporary LLMs for clinical text summarization.\n\u2022 Our exploration delves into a myriad of trade-offs concerning different models and adaptation methods,\nshedding light on scenarios where advancements in model size, novelty, or domain specificity do not\nnecessarily translate to superior performance.\n\u2022 Through a clinical reader study with ten physicians, we demonstrate that LLM summaries can surpass\nmedical expert summaries in terms of completeness, correctness, and conciseness.\n\u2022 Our safety analysis of examples, potential medical harm, and fabricated information reveals insights\ninto the challenges faced by both models and medical experts.\n\u2022 We identify which NLP metrics most correlate with reader preferences.\nOur study demonstrates that adapting LLMs can outperform medical experts for clinical text summarization\nacross the diverse range of documents we evaluate. This suggests that incorporating LLM-generated candidate\nsummaries could reduce documentation load, potentially leading to decreased clinician strain and improved\npatient care.\n2 Related Work\nLarge language models (LLMs) have demonstrated astounding performance, propelled by both the transformer\narchitecture [29] and increasing scales of data and compute, resulting in widespread adoption of models\nsuch as ChatGPT [17]. Although several of the more expansive models, such as GPT-4 [30] and PaLM [31],\nremain proprietary and provide access via \u201cblack-box\u201d interfaces, there has been a pronounced shift towards\nopen-sourced alternatives such as Llama-2 [32]. These open-source models grant researchers direct access to\nmodel weights for customization.\nPopular transformer models such as BERT [33] and GPT-2 [34] established the paradigm of self-supervised\npretraining on large amounts of general data and then adapting to a particular domain or task by tuning on\nspecific data. One approach is customizing model weights via instruction tuning, a process where language\nmodels are trained to generate human-aligned responses given specific instructions [35]. Examples of clinical\ninstruction-tuned models include Med-PALM [24] for medical question-answering or Radiology-GPT [36] for\nradiology tasks. To enable domain adaptation with limited computational resources, prefix tuning [37] and\nlow-rank adaptation (LoRA) [38] have emerged as effective methods that require tuning less than 1% of total\n|2\nparameters over a small training set. LoRA has been shown to work well for medical question-answering [26]\nand summarizing radiology reports [27]. Another adaptation method, requiring no parameter tuning, is\nin-context learning: supplying the LLM with task-specific examples in the prompt [39]. Because in-context\nlearning does not alter model weights, it can be performed with black-box model access using only a few\ntraining examples [39].\nRecent work has adapted LLMs for various medical tasks, demonstrating great potential for medical language\nunderstanding and generation [22, 23, 25, 40]. Specifically, a broad spectrum of methodologies has been\napplied to clinical text for specific summarization tasks. One such task is the summarization of radiology\nreports, which aims to consolidate detailed findings from radiological studies into significant observations and\nconclusions drawn by the radiologist [41]. LLMs have shown promise on this task [27] and other tasks such as\nsummarizing daily progress notes into a concise \u201cproblem list\u201d of medical diagnoses [42]. Lastly, there has\nbeen significant work on summarizing extended conversations between a doctor and patient into patient visit\nsummaries [28, 43, 44].\nWhile the aforementioned contributions incorporate methods to adapt language models, they often include\nonly a small subset of potential approaches and models, and/or they predominantly rely on evaluation via\nstandard NLP metrics. Given the critical nature of medical tasks, demonstrating clinical readiness requires\nincluding human experts in the evaluation process. To address this, there have been recent releases of expert\nevaluations for instruction following [3] and radiology report generation [45]. Other work employs human\nexperts to evaluate synthesized Cochrane review abstracts, demonstrating that NLP metrics are not sufficient\nto measure summary quality [46]. With this in mind, we extend our comprehensive evaluation of methods and\nLLMs beyond NLP metrics to incorporate a clinical reader study across multiple summarization tasks. Our\nresults demonstrate across many tasks that LLM summaries are comparable to\u2013\u2013\u2013and often surpass\u2013\u2013\u2013those\ncreated by human experts.\n3 Approach\n3.1 Large language models\nWe investigate a diverse collection of transformer-based LLMs for clinical summarization tasks. This includes\ntwo broad approaches to language generation: sequence-to-sequence (seq2seq) models and autoregressive\nmodels. Seq2seq models use an encoder-decoder architecture to map the input text to a generated output,\noften requiring paired datasets for training.\nThese models have shown strong performance in machine\ntranslation [47] and summarization [48]. In contrast, the autoregressive models typically only use a decoder.\nThey generate tokens sequentially\u2014where each new token is conditioned on previous tokens\u2014thus efficiently\ncapturing context and long-range dependencies. Autoregressive models are typically trained with unpaired\ndata, and they are particularly useful for various NLP tasks such as text generation, question-answering, and\ndialogue interactions [17, 49].\nTable 1 | We quantitatively evaluate eight models, including state-of-the-art sequence-to-sequence and autoregressive models.\nUnless specified, models are open-source (vs. proprietary).\nModel\nContext\nParameters\nProprietary?\nSeq2seq\nAutoreg.\nFLAN-T5\n512\n2.7B\n-\n\u2714\n-\nFLAN-UL2\n2,048\n20B\n-\n\u2714\n-\nAlpaca\n2,048\n7B\n-\n-\n\u2714\nMed-Alpaca\n2,048\n7B\n-\n-\n\u2714\nVicuna\n2,048\n7B\n-\n-\n\u2714\nLlama-2\n4,096\n7B, 13B\n-\n-\n\u2714\nGPT-3.5\n16,384\n175B\n\u2714\n-\n\u2714\nGPT-4\n32,768*\nunknown\n\u2714\n-\n\u2714\n*The context length of GPT-4 has since been increased to 128,000.\n|3\nWe include prominent seq2seq models due to their strong summarization performance [48] and autoregressive\nmodels due to their state-of-the-art performance across general NLP tasks [21]. As shown in Table 1, our\nchoice of models varies widely with respect to number of parameters (2.7 billion to 175 billion) and context\nlength (512 to 32,768), i.e. the maximum number of input tokens a model can process. We organize our\nmodels into three categories:\nOpen-source seq2seq models. The original T5 \u201ctext-to-text transfer transformer\u201d model [50] demonstrated\nexcellent performance in transfer learning using the seq2seq architecture. A derivative model, FLAN-T5 [51,\n52], improved performance via instruction prompt tuning. This T5 model family has proven effective for\nvarious clinical NLP tasks [27, 53]. The FLAN-UL2 model [54, 55] was introduced recently, which featured an\nincreased context length (four-fold that of FLAN-T5) and a modified pre-training procedure called unified\nlanguage learning (UL2).\nOpen-source autoregressive models. The Llama family of LLMs [32] has enabled the proliferation of\nopen-source instruction-tuned models that deliver comparable performance to GPT-3 [17] on many benchmarks\ndespite their smaller sizes. Descendants of this original model have taken additional fine-tuning approaches,\nsuch as fine-tuning via instruction following (Alpaca [56]), medical Q&A data (Med-Alpaca [57]), user-shared\nconversations (Vicuna [49]), and reinforcement learning from human feedback (Llama-2 [32]). Llama-2 allows\nfor two-fold longer context lengths (4,096) relative to the aforementioned open-source autoregressive models.\nProprietary autoregressive models. We include GPT-3.5 [58] and GPT-4 [30], the latter of which has\nbeen regarded as state-of-the-art on general NLP tasks [21] and has demonstrated strong performance on\nbiomedical NLP tasks such as medical exams [59\u201361]. Both models offer significantly higher context length\n(16,384 and 32,768) than open-source models. We note that since sharing our work, GPT-4\u2019s context length\nhas been increased to 128,000.\n3.2 Adaptation methods\nWe consider two proven techniques for adapting pre-trained general-purpose LLMs to domain-specific clinical\nsummarization tasks. To demonstrate the benefit of adaptation methods, we also include the baseline zero-shot\nprompting, i.e. m = 0 in-context examples.\nIn-context learning (ICL). ICL is a lightweight adaptation method that requires no altering of model\nweights; instead, one includes a handful of in-context examples directly within the model prompt [39]. This\nsimple approach provides the model with context, enhancing LLM performance for a particular task or\ndomain [27, 28]. We implement this by choosing, for each sample in our test set, the m nearest neighbors\ntraining samples in the embedding space of the PubMedBERT model [62]. Note that choosing \u201crelevant\u201d\nin-context examples has been shown to outperform choosing examples at random [63]. For a given model\nand dataset, we use m = 2x examples, where x \u2208 {0, 1, 2, 3, ..., M} for M such that no more than 1% of the\ns = 250 samples are excluded due to prompts exceeding the model\u2019s context length. Hence each model\u2019s\ncontext length limits the allowable number of in-context examples.\nQuantized low-rank adaptation (QLoRA). Low-rank adaptation (LoRA) [38] has emerged as an effective,\nlightweight approach for fine-tuning LLMs by altering a small subset of model weights\u2014often < 0.1% [27].\nLoRA inserts trainable matrices into the attention layers; then, using a training set of samples, this method\nperforms gradient descent on the inserted matrices while keeping the original model weights frozen. Compared\nto training model weights from scratch, LoRA is much more efficient with respect to both computational\nrequirements and the volume of training data required. Recently, QLoRA [64] has been introduced as a more\nmemory-efficient variant of LoRA, employing 4-bit quantization to enable the fine-tuning of larger LLMs\ngiven the same hardware constraints. This quantization negligibly impacts performance [64]; as such, we use\nQLoRA for all model training. Note that QLoRA cannot be used to fine-tune proprietary models on our\nconsumer hardware, as their model weights are not publicly available. Fine-tuning of GPT-3.5 via API was\nmade available after our internal model cutoff date of July 31st, 2023 [65].\n3.3 Data\nTo robustly evaluate LLM performance on clinical text summarization, we choose four distinct summarization\ntasks, comprising six open-source datasets. As depicted in Table 2, each dataset contains a varying number of\n|4\nsamples, token lengths, and lexical variance. Lexical variance is calculated as number of unique words\nnumber of total words across the\nentire dataset; hence a higher ratio indicates less repetition and more lexical diversity. We describe each task\nand dataset below. For task examples, please see Figures 8, A4, A5, and A6.\nRadiology reports Radiology report summarization takes as input the findings section of a radiology study\ncontaining detailed exam analysis and results. The goal is to summarize these findings into an impression\nsection, which concisely captures the most salient, actionable information from the study. We consider three\ndatasets for this task, where both reports and findings were created by attending physicians as part of routine\nclinical care. Open-i [66] contains de-identified narrative chest x-ray reports from the Indiana Network for\nPatient Care 10 database. From the initial set of 4K studies, Demner-Fushman et al. [66] selected a final set of\n3.4K reports based on the quality of imaging views and diagnostic content. MIMIC-CXR [67] contains chest\nx-ray studies accompanied by free-text radiology reports acquired at the Beth Israel Deaconess Medical Center\nbetween 2011 and 2016. For this study, we use a dataset of 128K reports [68] preprocessed by RadSum23\nat BioNLP 2023 [69, 70]. MIMIC-III [71] contains 67K radiology reports spanning seven anatomies (head,\nabdomen, chest, spine, neck, sinus, and pelvis) and two modalities: magnetic resonance imaging (MRI) and\ncomputed tomography (CT). This dataset originated from patient stays in critical care units of the Beth\nIsrael Deaconess Medical Center between 2001 and 2012. For this study, we utilize a preprocessed version via\nRadSum23 [69, 70]. Compared to x-rays, MRIs and CT scans capture more information at a higher resolution.\nThis usually leads to longer reports (Table 2), rendering MIMIC-III a more challenging summarization dataset\nthan Open-i or MIMIC-CXR.\nPatient questions Question summarization consists of generating a condensed question expressing the\nminimum information required to find correct answers to the original question [72]. For this task, we employ\nthe MeQSum dataset [72]. MeQSum contains (1) patient health questions of varying verbosity and coherence\nselected from messages sent to the U.S. National Library of Medicine (2) corresponding condensed questions\ncreated by three medical experts such that the summary allows retrieving complete, correct answers to\nthe original question without the potential for further condensation. These condensed questions were then\nTable 2 | Top:\nDescription of six open-source datasets with a wide range of token length and lexical variance,\ni.e. number of unique words\nnumber of total words . Bottom: Instructions for each of the four summarization tasks. See Figure 2 for the full prompt.\nDataset descriptions\nNumber\nAvg. number of tokens\nLexical\nDataset\nTask\nof samples\nInput\nTarget\nvariance\nOpen-i\nRadiology reports\n3.4K\n52 \u00b1 22\n14 \u00b1 12\n0.11\nMIMIC-CXR\nRadiology reports\n128K\n75 \u00b1 31\n22 \u00b1 17\n0.08\nMIMIC-III\nRadiology reports\n67K\n160 \u00b1 83\n61 \u00b1 45\n0.09\nMeQSum\nPatient questions\n1.2K\n83 \u00b1 67\n14 \u00b1 6\n0.21\nProbSum\nProgress notes\n755\n1,013 \u00b1 299\n23 \u00b1 16\n0.15\nACI-Bench\nDialogue\n126\n1,512 \u00b1 467\n211 \u00b1 98\n0.04\nTask Instructions\nTask\nInstruction\nRadiology reports\n\u201cSummarize the radiology report findings\ninto an impression with minimal text.\u201d\nPatient questions\n\u201cSummarize the patient health query\ninto one question of 15 words or less.\u201d\nProgress notes\n\u201cBased on the progress note, generate a list of 3-7 prob-\nlems (a few words each) ranked in order of importance.\u201d\nDialogue\n\u201cSummarize the patient/doctor dialogue\ninto an assessment and plan.\u201d\n|5\nvalidated by a medical doctor and verified to have high inter-annotator agreement. Due to the wide variety of\nthese questions, MeQSum exhibits the highest lexical variance of our datasets (Table 2).\nProgress notes The goal of this task is to generate a \u201cproblem list,\u201d or condensed list of diagnoses and medical\nproblems using the provider\u2019s progress notes during hospitalization. For this task, we employ the ProbSum\ndataset [42]. This dataset, generated by attending internal medicine physicians during the course of routine\nclinical practice, was extracted from the MIMIC-III database of de-identified hospital intensive care unit\n(ICU) admissions. ProbSum contains (1) progress notes averaging > 1, 000 tokens and substantial presence of\nunlabeled numerical data, e.g. dates and test results, and (2) corresponding problem lists created by attending\nmedical experts in the ICU. We utilize a version shared by the BioNLP Problem List Summarization Shared\nTask [42, 70, 73] and PhysioNet [74].\nDialogue The goal of this task is to summarize a doctor-patient conversation into an \u201cassessment and plan\u201d\nparagraph. For this task, we employ the ACI-Bench dataset [43, 44, 75], which contains (1) 207 doctor-patient\nconversations and (2) corresponding patient visit notes, which were first generated by a seq2seq model and\nsubsequently corrected and validated by expert medical scribes and physicians. Since ACI-Bench\u2019s visit notes\ninclude a heterogeneous collection of section headers, we choose 126 samples containing an \u201cassessment and\nplan\u201d section for our analysis. Per Table 2, this task entailed the largest token count across our six datasets\nfor both the input (dialogue) and target (assessment).\nAs we are not the first to employ these datasets, Table A2 contains quantitative metric scores from other\nworks [25, 27, 44, 76\u201378] who developed methods specific to each individual summarization task.\n4 Experiments\nThis section contains experimental details and study design for our evaluation framework, as depicted in\nFigure 1.\n4.1 Quantitative Evaluation\nBuilding upon the descriptions of models, methods, and tasks in Section 3, we now specify experimental details\nsuch as model prompts, data preparation, software implementation, and NLP metrics used for quantitative\nevaluation.\n4.1.1 Model prompts and temperature\nAs shown in Figure 2, we structure prompts by following best practices [79, 80] and evaluating 1-2 options\nfor model expertise and task-specific instructions (Table 2). We note the importance of specifying desired\nlength in the instruction, e.g. \u201cone question of 15 words or less\u201d for summarizing patient questions. Without\nthis specification, the model might generate lengthy outputs\u2014occasionally even longer than the input text.\nWhile in some instances this detail may be preferred, we steer the model toward conciseness given our task of\nsummarization.\nPrompt phrasing and model temperature can have a considerable effect on LLM output, as demonstrated\nin the literature [81, 82] and in Figure 2. For example, we achieve better performance by nudging the\nmodel towards expertise in medicine compared to expertise in wizardry or no specific expertise at all. This\nillustrates the value of relevant context in achieving better outcomes for the target task. We also explore\nthe temperature hyperparameter, which adjusts the LLM\u2019s conditional probability distributions during\nsampling, hence affecting how often the model will output less likely tokens. Higher temperatures lead to\nmore randomness and \u201ccreativity,\u201d while lower temperatures produce more deterministic outputs. Figure 2\ndemonstrates that the lowest value, 0.1, performed best. We thus set temperature to this value for all models.\nIntuitively, a lower value seems appropriate given our goal of factually summarizing text with a high aversion\nto factually incorrect text.\n|6\nExpertise\n(task-speci\ufb01c)\nInstruction \nExamples \ni = 1, ..., m \n#: delimiters \nfor ICL only, \nelse m = 0 \nInput \nYou are an expert medical professional.\nSummarize the [radiology report \ufb01ndings] \ninto an  \n \n\u00a0[impression with minimal text]. \nUse the examples to guide word choice. \n... input i:\u00a0 \u00a0 \u00a0 [example input] \nsummary i:\u00a0 \u00a0[example summary] \n## ... \n... input m+1:\u00a0 \u00a0 \u00a0 [input text] \nsummary m+1:\u00a0 \u00a0[summarized text] \nTemperature\nValue\n0.1 \n0.5 \n0.9 \nNone \nMedicine1 \nWizardry2\nParameter\nExpertise\nBLEU\nROUGE-L BERTScore MEDCON\n4.9\n4.9\n4.3\n10.4\n11.1\n4.3\n28.1\n27.1\n25.4\n34.3\n35.5\n27.8\n89.6\n89.7\n89.3\n90.2\n90.5\n89.7\n28.2\n27.5\n25.3\n30.7\n35.5\n28.5\n1: \"You are an expert medical professional.\"\u00a0 \u00a0 \u00a02: \"You are a mystical wizard in Middle Earth.\"\nFigure 2 | Left: Prompt anatomy. Each summarization task uses a slightly different instruction (Table 2). Right: Effect of\nmodel temperature and expertise. We generally find better performance when (1) using lower temperature, i.e. generating\nless random output, as summarization tasks benefit more from truthfulness than creativity (2) assigning the model clinical\nexpertise in the prompt. Output generated via GPT-3.5 on the Open-i radiology report dataset.\n4.1.2 Experimental Setup\nFor each dataset, we construct test sets by randomly drawing the same s samples, where s = 250 for all\ndatasets except dialogue (s = 100), which includes only 126 samples in total. After selecting these s samples,\nwe choose another s as a validation set for datasets which incorporated fine-tuning. We then use the remaining\nsamples as a training set for ICL examples or QLoRA fine-tuning.\nWe leverage PyTorch for our all our experiments, including the parameter-efficient fine-tuning [83] and the\ngenerative pre-trained transformers quantization [84] libraries for implementing QLoRA. We fine-tune models\nwith QLoRA for five epochs using the Adam optimizer with weight decay fix [85]. Our initial learning rate of\n1e\u22123 decays linearly to 1e\u22124 after a 100-step warm-up; we determine this configuration after experimenting\nwith different learning rates and schedulers. To achieve an effective batch size of 24 on each experiment, we\nadjust both individual batch size and number of gradient accumulation steps to fit on a single consumer GPU,\na NVIDIA Quadro RTX 8000. All open-source models are available on HuggingFace [86].\n4.1.3 Quantitative metrics\nWe use well-known summarization metrics to assess the quality of generated summaries. BLEU [87], the\nsimplest metric, calculates the degree of overlap between the reference and generated texts by considering 1- to\n4-gram sequences. ROUGE-L [88] evaluates similarity based on the longest common subsequence; it considers\nboth precision and recall, hence being more comprehensive than BLEU. In addition to these syntactic metrics,\nwe employ BERTScore, which leverages contextual BERT embeddings to evaluate the semantic similarity\nof the generated and reference texts [89]. Lastly, we include MEDCON [44] to gauge the consistency of\nmedical concepts. This employs QuickUMLS [90], a tool that extracts biomedical concepts via string matching\nalgorithms [91]. We restrict MEDCON to specific UMLS semantic groups (Anatomy, Chemicals & Drugs,\nDevice, Disorders, Genes & Molecular Sequences, Phenomena and Physiology) relevant for our work. All four\nmetrics range from [0, 100] with higher scores indicating higher similarity between the generated and reference\nsummaries.\n4.2 Clinical reader study\nAfter identifying the best model and method via NLP quantitative metrics, we perform a clinical reader study\nacross three summarization tasks: radiology reports, patient questions, and progress notes. The dialogue\ntask is excluded due to the unwieldiness of a reader parsing many lengthy transcribed conversations and\nparagraphs; see Figure A6 for an example and Table 2 for the token count.\nOur readers include two sets of physicians: (1) five board-certified radiologists to evaluate summaries of\nradiology reports (2) five board-certified hospitalists (internal medicine physicians) to evaluate summaries of\npatient questions and progress notes. For each task, each physician views the same 100 randomly selected\ninputs and their A/B comparisons (medical expert vs. the best model summaries), which are presented\n|7\nin a blinded and randomized order. An ideal summary would contain all clinically significant information\n(completeness) without any errors (correctness) or superfluous information (conciseness). Hence we pose the\nfollowing three questions for readers to evaluate using a five-point Likert scale.\n\u2022 Completeness: \u201cWhich summary more completely captures important information?\u201d This compares\nthe summaries\u2019 recall, i.e. the amount of clinically significant detail retained from the input text.\n\u2022 Correctness: \u201cWhich summary includes less false information?\u201d This compares the summaries\u2019 precision,\ni.e. instances of fabricated information.\n\u2022 Conciseness: \u201cWhich summary contains less non-important information?\u201d\nThis compares which\nsummary is more condensed, as the value of a summary decreases with superfluous information.\nFigure 7e demonstrates the user interface for this study, which we create and deploy via Qualtrics. To\nobfuscate any formatting differences between the model and medical expert summaries, we apply simple\npost-processing to standardize capitalization, punctuation, newline characters, etc.\nGiven this non-parametric, categorical data, we assess the statistical significance of responses using a Wilcoxon\nsigned-rank test with Type 1 error rate = 0.05 and adjust for multiple comparisons using the Bonferroni\ncorrection. We estimate intra-reader correlation based on a mean-rating, fixed agreement, two-may mixed\neffects model [92] using the Pingouin package [93]. Additionally, readers are provided comment space to make\nobservations for qualitative analysis.\n4.3 Safety analysis\nWe conduct a safety analysis connecting summarization errors to medical harm, inspired by the Agency for\nHealthcare Research and Quality (AHRQ)\u2019s harm scale [94]. This includes radiology reports (nr = 27) and\nprogress notes (nn = 44) samples which contain disparities in completeness and/or correctness between the\nbest model and medical expert summaries. Here, disparities occur if at least one physician significantly prefers\nor at least two physicians slightly prefer one summary to the other. These summary pairs are randomized\nand blinded. For each sample, we ask the following multiple-choice questions: \u201cSummary A is more complete\nand/or correct than Summary B. Now, suppose Summary B (worse) is used in the standard clinical workflow.\nCompared to using Summary A (better), what would be the...\u201d (1) \u201c... extent of possible harm?\u201d options:\n{none, mild or moderate harm, severe harm or death} (2) \u201c... likelihood of possible harm?\u201d options: {low,\nmedium, high}.\nSafety analysis of fabricated information is discussed in Section 5.2.2.\n4.4 Connecting quantitative and clinical evaluations\nWe now provide intuition connecting NLP metrics and clinical reader scores. Note that in our work, these\ntools measure different quantities; NLP metrics measure the similarity between two summaries, while reader\nscores measure which summary is better. Consider an example where two summaries are exactly the same:\nNLP metrics would yield the highest possible score (100), while clinical readers would provide a score of 0\nto denote equivalence. As the magnitude of a reader score increases, the two summaries are increasingly\ndissimilar, hence yielding a lower quantitative metric score. Given this intuition, we compute the Spearman\ncorrelation coefficient between NLP metric scores and the magnitude of the reader scores. Since these features\nare inversely correlated, for clarity we display the negative correlation coefficient values.\n5 Results and Discussion\n5.1 Quantitative evaluation\n5.1.1 Impact of domain-specific fine-tuning\nWhen considering which open-source models to evaluate, we first assess the benefit of fine-tuning open-source\nmodels on medical text. For example, Med-Alpaca [57] is a version of Alpaca [56] which was further instruction-\n|8\n0\n30\nAlpaca\n0\n30\nMed-Alpaca\nBLEU\n10\n55\nAlpaca\n10\n55\nMed-Alpaca\nROUGE-L\n82\n94\nAlpaca\n82\n94\nMed-Alpaca\nBERTScore\n10\n55\nAlpaca\n10\n55\nMed-Alpaca\nMEDCON\nFigure 3 | Alpaca vs. Med-Alpaca. Given that most data points are below the dashed lines denoting equivalence, we conclude\nthat Med-Alpaca\u2019s fine-tuning with medical Q&A data results in worse performance for our clinical summarization tasks. See\nSection 5.1 for further discussion. Note that each data point corresponds to the average score of s = 250 samples for a given\nexperimental configuration, i.e. {dataset \u00d7 m in-context examples}.\ntuned with medical Q&A text, consequently improving performance for the task of medical question-answering.\nFigure 3 compares these models for our task of summarization, showing that most data points are below\nthe dashed lines denoting equivalence. Hence despite Med-Alpaca\u2019s adaptation for the medical domain, it\nperforms worse than Alpaca for our tasks of clinical text summarization\u2014highlighting a distinction between\ndomain adaptation and task adaptation. With this in mind, and considering that Alpaca is commonly known\nto perform worse than our other open-source autoregressive models Vicuna and Llama-2 [21, 49], for simplicity\nwe exclude Alpaca and Med-Alpaca from further analysis.\n5.1.2 Comparison of adaptation strategies\nNext, we compare ICL (in-context learning) vs. QLoRA (quantized low-rank adaptation) across the remaining\nopen-source models using the Open-i radiology report dataset in Figure 4 and the patient health questions\nin Figure A2. We choose these datasets because their shorter context lengths allow for training with lower\ncomputational cost.\nFLAN-T5 emerged as the best-performing model with QLoRA. QLoRA typically\noutperformed ICL (one example) with the better models FLAN-T5 and Llama-2; given a sufficient number\nof in-context examples, however, most models surpass even the best QLoRA fine-tuned model, FLAN-T5\n(Figure A1). FLAN-T5 (2.7B) eclipsed its fellow seq2seq model FLAN-UL2 (20B), despite being an older\nmodel with almost 8\u00d7 fewer parameters.\nFLAN-T5\nFLAN-UL2\nLlama-2\nVicuna\nFigure 4 | One in-context example (ICL) vs. QLoRA across open-source models on Open-i radiology reports. FLAN-T5\nachieves best performance on both methods for this dataset. While QLoRA typically outperforms ICL with the better models\n(FLAN-T5, Llama-2), this relationship reverses given sufficient in-context examples (Figure A1). Figure A2 contains similar\nresults with patient health questions.\n|9\nGPT-4\nGPT-3.5\nFLAN-T5\nFLAN-UL2\nLlama-2\nVicuna\nFLAN-T5\n+ QLoRA\nFigure 5 | MEDCON scores vs. number of in-context examples across models and datasets. We also include the best\nmodel fine-tuned with QLoRA (FLAN-T5) as a horizontal dashed line for valid datasets. Note the allowable number of in-context\nexamples varies significantly by model and dataset. See Figure A1 for results across all four metrics.\nWhen considering trade-offs between adaptation strategies, availability of these models (open-source vs.\nproprietary) raises an interesting consideration for healthcare, where data and model governance are important\u2014\nespecially if summarization tools are cleared for clinical use by the Food and Drug Administration. This\ncould motivate the use of fine-tuning methods on open-source models. Governance aside, ICL provides many\nbenefits: (1) model weights are fixed, hence enabling queries of pre-existing LLMs (2) adaptation is feasible\nwith even a few examples, while fine-tuning methods such as QLoRA typically require hundreds or thousands\nof examples.\n5.1.3 Effect of context length for in-context learning\nFigure 5 displays MEDCON [44] scores for all models against number of in-context examples, up to the\nmaximum number of allowable examples for each model and dataset. This graph also includes the best\nperforming model (FLAN-T5) with QLoRA as a reference, depicted by a horizontal dashed line. Compared\nto zero-shot prompting (m = 0 examples), adapting with even m = 1 example considerably improves\nperformance in almost all cases, underscoring the importance of adaptation methods. While ICL and QLoRA\nare competitive for open-source models, proprietary models GPT-3.5 and GPT-4 far outperform other models\nand methods given sufficient in-context examples. For a similar graph across all metrics, see Figure A1.\n5.1.4 Head-to-head model comparison\nFigure 6 compares models using win rates, i.e. the head-to-head winning percentage of each model combination\nacross the same set of samples. In other words, for what percentage of samples do model A\u2019s summaries have\na higher score than model B\u2019s summaries? This presents trade-offs of different model types. Seq2seq models\n(FLAN-T5, FLAN-UL2) perform well on syntactical metrics such as BLEU [87] but worse on others, suggesting\nthat these models excel more at matching word choice than matching semantic or conceptual meaning. Note\nseq2seq models are often constrained to much shorter context length than autoregressive models (Table 1),\nbecause seq2seq models require the memory-intensive step of encoding the input sequence into a fixed-size\ncontext vector. Among open-source models, seq2seq models perform better than autoregressive (Llama-2,\nVicuna) models on radiology reports but worse on patient questions and progress notes (Figure A1). Given\nthat these latter datasets have higher lexical variance (Table 2) and more heterogeneous formatting compared\nto radiology reports, we hypothesize that autoregressive models may perform better with increasing data\nheterogeneity and complexity.\nBest model/method. We deemed the best model and method to be GPT-4 (context length 32,768) with a\nmaximum allowable number of in-context examples, hereon identified as the best-performing model.\n|10\nFigure 6 | Model win rate: a head-to-head winning percentage of each model combination, where red/blue intensities highlight\nthe degree to which models on the vertical axis outperform models on the horizontal axis. GPT-4 generally achieves the best\nperformance. While FLAN-T5 is more competitive for syntactic metrics such as BLEU, we note this model is constrained\nto shorter context lengths (Table 1). When aggregated across datasets, seq2seq models (FLAN-T5, FLAN-UL2) outperform\nopen-source autoregressive models (Llama-2, Vicuna) on all metrics.\n5.2 Clinical reader study\nGiven our clinical reader study design (Figure 7a), pooled results across ten physicians (Figure 7b) demonstrate\nthat summaries from the best adapted model (GPT-4 using ICL) are more complete and contain fewer errors\ncompared to medical expert summaries\u2014which were created either by medical doctors during clinical care or\nby a committee of medical doctors and experts.\nThe distributions of reader responses in Figure 7c show that medical expert summaries are preferred in\nonly a minority of cases (19%), while in a majority, the best model is either non-inferior (45%) or preferred\n(36%). Table A1 contains scores separated by individual readers and affirms the reliability of scores across\nreaders by displaying positive intra-reader correlation values. Based on physician feedback, we undertake a\nqualitative analysis to illustrate strengths and weaknesses of summaries by the model and medical experts;\nsee Figures 8, A4, and A5. Now, we discuss results with respect to each individual attribute.\n5.2.1 Completeness\nThe best model summaries are more complete on average than medical expert summaries, achieving statistical\nsignificance across all three summarization tasks with p < 0.001 (Figure 7b). Lengths of summaries were\ncomparable between the model and medical experts for all three datasets: 47 \u00b1 24 vs. 44 \u00b1 22 tokens for\nradiology reports, 15 \u00b1 5 vs. 14 \u00b1 4 tokens for patient questions, and 29 \u00b1 7 vs. 27 \u00b1 13 tokens for progress\nnotes (all p > 0.12). Hence the model\u2019s advantage in completeness is not simply a result of generating longer\nsummaries. We provide intuition for completeness by investigating a specific example in progress notes\nsummarization. In Figure A5, the model correctly identifies conditions that were missed by the medical expert,\nsuch as hypotension and anemia. Although the model was more complete in generating its progress notes\nsummary, it also missed historical context (a history of HTN, or hypertension).\n5.2.2 Correctness\nWith regards to correctness, the best model generated significantly fewer errors (p < 0.001) compared to\nmedical expert summaries overall and on two of three summarization tasks (Figure 7b). As an example of the\nmodel\u2019s superior correctness performance on radiology reports, we observe that it avoided common medical\nexpert errors related to lateral distinctions (right vs. left, Figure 8). For progress notes, Figure A5 reveals an\nintriguing case: during the blinded study, the physician reader erroneously assumed that a hallucination\u2014the\nincorrect inclusion of a urinary tract infection\u2014was made by the model. In this case, the medical expert was\nresponsible for the hallucination. This instance underscores the point that even medical experts, not just\nLLMs, can hallucinate. Despite this promising performance, the model was not perfect across all tasks. We\nsee a clear example in Figure A5 in which the model mistakenly generated several conditions in the problem\nlist that were incorrect, such as eosinophilia.\n|11\nc\ne\nWhich summary...\n[Completeness] \n... more completely captures important information?\n[Correctness]\u00a0 \u00a0  \n... includes less false information?\n[Conciseness]\u00a0 \n... contains less non-important information?\n10\n5\n0\n-5\n-10\n \nneither\nsigni\ufb01cantly\nExtent of harm \nSevere or death\nMild or moderate\nNone\nMedical expert\nBest model\nExtent\nLikelihood\nHigh\nMedium\nLow\nLikelihood of\u00a0harm\n21.7%\n15.5%\n11.3%\n13.7%\nd\na\nb\nMedical expert\nBest model\nMedical expert\nslightly\nslightly\nBest model\nsigni\ufb01cantly\nInput: there is focal high attenuation overlying a superior left frontal\ngyrus, probably a dural calcification. subsequent mri shows no evidence\nof hemorrhage in this region. the brain parenchyma is normal. the\nventricles and sulci are slightly prominent.\nSummary A: there is no evidence of a hemorrhage and no area of low\nattenuation or mass effect is seen to suggest an acute infarct.\nSummary B: no acute intracranial abnormality. probable dural\ncalcification overlying a superior left frontal gyrus.\nWhich summary...\n... more completely\ncaptures important\ninformation?\n... includes less false\ninformation?\n... contains less non-\nimportant information?\nA:\nsigni\ufb01cantly\nA:\nslightly\nneither\nB:\nslightly\nB:\nsigni\ufb01cantly\nFigure 7 | Clinical reader study. (a) Study design comparing the summaries from the best model versus that of medical\nexperts on three attributes: completeness, correctness, and conciseness. (b) Results. Model summaries are rated higher on all\nattributes. Highlight colors correspond to a value\u2019s location on the color spectrum. Asterisks (*) denote statistical significance by\nWilcoxon signed-rank test, p < 0.001. (c) Distribution of reader scores. Horizontal axes denote reader preference as measured by\na five-point Likert scale. Vertical axes denote frequency count, with 1,500 total reports for each plot. (d) Extent and likelihood\nof potential medical harm caused by choosing summaries from the medical expert (pink) or best model (purple) over the other.\nModel summaries are preferred in both categories. (e) Reader study user interface.\nBoth the model and medical experts faced challenges interpreting ambiguity, such as user queries in patient\nhealth questions. Consider Figure A4\u2019s first example, in which the input question mentioned \u201cdiabetes and\nneuropathy.\u201d The model mirrored this phrasing verbatim, while the medical expert interpreted it as \u201cdiabetic\nneuropathy.\u201d In Figure A4\u2019s second example, the model simply reformulated the input question about tests\nand their locations, while the medical expert inferred a broader query about tests and treatments. In both\ncases, the model\u2019s summaries leaned toward literalness, a trait that readers sometimes favored and sometimes\ndid not. In future work, a systematic exploration of model temperature could further illuminate this trade-off.\nFurther, the critical need for accuracy in a clinical setting motivates a more nuanced understanding of\ncorrectness. As such, we define three types of fabricated information: (1) misinterpretations of ambiguity, (2)\nfactual inaccuracies: modifying existing facts to be incorrect, and (3) hallucinations: inventing new information\nthat cannot be inferred from the input text. We found that the model committed these errors on 6%, 2%, and\n5% of samples, respectively, compared to 9%, 4%, and 12% by medical experts. Given the model\u2019s lower error\nrate in each category, this suggests that incorporating LLMs could actually reduce fabricated information in\nclinical practice.\nBeyond the scope of our work, there\u2019s further potential to reduce fabricated information through incorporating\nchecks by a human, checks by another LLM, or using a model ensemble to create a \u201ccommittee of experts\u201d [95,\n96].\n|12\nInput:\nThe patient is s/p left frontal craniotomy. A small amount of intracranial gas \nis seen posterior to the surgical intervention, which could represent \npostoperative changes. Extensive edema is seen in the left frontal lobe at \nthe site of presumed surgery. Additionally multiple foci of hemorrhage are \nseen in the region of the left frontal lobe. Midline shift to the right is seen in \nthe frontal region. The ventricles, cisterns, and sulci are unremarkable, \nwithout effacement. Comparison with prior studies from outside institution \nwould be helpful in further evaluation of these findings.\nSummary (medical expert):\nPostoperative changes in the right frontoparietal region identified, with \ndecreasing pneumocephalus. No new hemorrhage. Persistent mild leftward \nshift, not significantly changed from prior.\nSummary (best model):\nStable post-craniotomy changes with decreasing pneumocephalus and \npersistent mild leftward shift. No new hemorrhage. Increasing opacification \nof ethmoid and sphenoid sinuses. Large right subgaleal hematoma \nunchanged.\nInput:\nRight frontoparietal craniotomy again identified, with decreasing \npneumocephalus and small amount of subdural fluid. There is persistent \nmild leftward shift of approximately 2-3 mm, not significantly changed from \nprior study. Size and configuration of the ventricles is unchanged from prior \nstudy. There is no new hemorrhage. Multiple facial fractures, better \nevaluated on prior dedicated maxillofacial CT. Increasing opacification is \nseen in the ethmoid and sphenoid sinuses. Large right subgaleal \nhematoma again identified.\nExample 2: The model performed worse because it was less concise, i.e. included non-important information (green).\nExample 1: The model performed better because the medical expert made a laterality mistake (red).\nSummary (medical expert):\n1. Left frontal craniotomy. 2. Frontal midline shift to the right. 3. Extensive \nleft frontal lobe edema. 4. Multiple foci of hemorrhage in the right frontal \nlobe.\nSummary (best model):\nPostoperative changes following left frontal craniotomy with intracranial \ngas, extensive edema, and multiple foci of hemorrhage in the left frontal \nlobe. Midline shift to the right is noted.\nAttribute\nAverage\nExample 1\nExample 2\nCompleteness\n2.8\n5\n-1\nCorrectness\n1.7\n8\n0\nConciseness\n0\n0\n-4\nRadiology reports\nBlue: \ncorrect; exists in input + expert + model\nPurple: correct; exists in input + expert only\nGreen: correct; exists in input + model only\nOrange: incoherent or filler\nRed: \nincorrect\nColor key: \nReader scores: \nFigure 8 | Annotation: radiology reports. The table (lower left) contains reader scores for these two examples and the task\naverage across all samples. Top: the model performs better due to a laterality mistake by the human expert. Bottom: the model\nexhibits a lack of conciseness.\n5.2.3 Conciseness\nWith regards to conciseness, the best model performed significantly better (p < 0.001) overall and on two\ntasks (Figure 7b). We note the model\u2019s summaries are more concise while concurrently being more complete.\nRadiology reports were the only task in which physicians did not prefer the best model\u2019s summaries to medical\nexperts. See Figure 8 for an example. We suggest that conciseness could be improved with better prompt\nengineering, or modifying the prompt to improve performance. Of the task-specific instructions in Table 2,\nthe other two tasks (patient questions, progress notes) explicitly specify summary length, e.g. \u201c15 words or\nless.\u201d These phrases are included so that model summaries are generated with similar lengths to the human\nsummaries, enabling a clean comparison. Length specification in the radiology reports prompt instruction was\nmore vague, i.e. \u201c...with minimal text,\u201d perhaps imposing a softer constraint on the model. We leave further\nstudy of prompt instructions to future work.\n5.3 Safety Analysis\nThe results of this harm study (Figure 7d) indicate that the medical expert summaries would have both\na higher likelihood (14%) and higher extent (22%) of possible harm compared to the summaries from the\nbest model (12% and 16%, respectively). These percentages are computed with respect to all samples, such\nthat the subset of samples with similar A/B summaries (in completeness and correctness) are assumed to\ncontribute no harm. For the safety analysis of fabricated information, please see Section 5.2.2. Ultimately we\nargue that, beyond clinical reader studies, conducting downstream analyses is crucial to affirm the safety of\nLLM-generated summaries in clinical environments.\n5.4 Connecting quantitative and clinical evaluations\nFigure 9 captures the correlation between NLP metrics and physicians\u2019 preference. Compared to other metrics,\nBLEU correlates most with completeness and least with conciseness. Given that BLEU measures sequence\n|13\nCompleteness\nCorrectness\nConciseness\nCharacteristics\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nCorrelation\nBLEU\nROUGE-L\nBERTScore\nMEDCON\nFigure 9 | Correlation between NLP metrics and reader scores. The semantic metric (BERTScore) and conceptual\nmetric (MEDCON) correlate most highly with correctness. Meanwhile, syntactic metrics BLEU and ROUGE-L correlate most\nwith completeness. See Section 5.4 for further discussion.\noverlap, this result seems reasonable, as more text provides more \u201csurface area\u201d for overlap; more text also\nreduces the brevity penalty that BLEU applies on generated sequences which are shorter than the reference [87].\nThe metrics BERTScore (measuring semantics) and MEDCON (measuring medical concepts) correlate most\nstrongly with reader preference for correctness. Overall, however, the low magnitude of correlation values\n(approximately 0.2) underscores the need to go beyond NLP metrics with a reader study when assessing\nclinical readiness.\nAside from the low correlation values in Figure 9, our reader study results (Figure 7b) highlight another\nlimitation of NLP metrics, especially as model-generated summaries become increasingly viable. These metrics\nrely on a reference\u2014in our case, medical expert summaries\u2014which we have demonstrated may contain errors.\nHence we suggest that human evaluation is essential when assessing the clinical feasibility of new methods. If\nhuman evaluation is not feasible, Figure 9 suggests that syntactic metrics are better at measuring completeness,\nwhile semantic and conceptual metrics are better at measuring correctness.\n5.5 Limitations\nThis study has several limitations which motivate future research.\nModel temperature and prompt phrasing can be important for LLM performance (Figure 2), [81, 82]. However,\nwe only search over three possible temperature values. Further, we do not thoroughly engineer our prompt\ninstructions (Table 2); each was chosen after trying only 1-2 options over a small dataset. While this highlights\nthe potential for improvement, we\u2019re also encouraged that achieving convincing results does not require a\nthorough temperature search or prompt engineering.\nIn our quantitative analysis, we select state-of-the-art and highly regarded LLMs with a diverse range of\nattributes. This includes the 7B-parameter tier of open-source autoregressive models, despite some models\nsuch as Llama-2 having larger versions. We consider the benefit of larger models in Figure A3, finding\nthis improvement marginal for Llama-2 (13B) compared to Llama-2 (7B). While there may exist open-\nsource models which perform slightly better than our selections, we do not believe this would meaningfully\nalter our analysis\u2014especially considering the clinical reader study employs GPT-4, which is an established\nstate-of-the-art [21].\nOur study does not encompass all clinical document types, and extrapolating our results is tentative. For\ninstance, our progress notes task employs ICU notes from a single medical center. These notes may be\nstructured differently from non-ICU notes or from ICU notes of a different center. Additionally, more\nchallenging tasks may require summarizing longer documents or multiple documents of different types.\n|14\nAddressing these cases demands two key advancements: (1) extending model context length, potentially\nthrough multi-query aggregation or other methods [97, 98] (2) introducing open-source datasets that include\nbroader tasks and lengthier documents. We thus advocate for expanding evaluation to other summarization\ntasks.\nWe do not consider the inherently context-specific nature of summarization. For example, a gastroenterologist,\nradiologist, and oncologist may have different preferences for summaries of a cancer patient with liver\nmetastasis. Or perhaps an abdominal radiologist will want a different summary than a neuroradiologist.\nFurther, individual clinicians may prefer different styles or amounts of information. While we do not explore\nsuch a granular level of adaptation, this may not require much further development: since the best model\nand method uses a handful of examples via ICL, one could plausibly adapt using examples curated for a\nparticular specialty or clinician. Another limitation is that radiology report summaries from medical experts\noccasionally recommend further studies or refer to prior studies, e.g. \u201c... not significantly changed from prior\u201d\nin Figure 8. These instances are out of scope for our tasks, which do not include context from prior studies;\nhence in the clinical reader study, physicians were told to disregard these phrases. Future work can explore\nproviding the LLM with additional context and longitudinal information.\nAn additional consideration for ours and other LLM studies, especially with proprietary models, is that it is\nnot possible to verify whether a particular open-source dataset was included in model training. While three of\nour datasets (MIMIC-CXR, MIMIC-III, ProbSum) require PhysioNet [74] access to ensure safe data usage by\nthird parties, this is no guarantee against data leakage. This complication highlights the need for validating\nresults on internal data when possible.\nWe note the potential for LLMs to be biased [99, 100]. While our datasets do not contain demographic\ninformation, we advocate for future work to consider whether summary qualities have any dependence upon\ngroup membership.\n6 Conclusion\nIn this research, we evaluate methods for adapting LLMs to summarize clinical text, analyzing eight models\nacross a diverse set of summarization tasks. Our quantitative results underscore the advantages of adapting\nmodels to specific tasks and domains. The ensuing clinical reader study demonstrates that LLM summaries\nare often preferred over medical expert summaries due to higher scores for completeness, correctness, and\nconciseness. The subsequent safety analysis explores qualitative examples, potential medical harm, and\nfabricated information to demonstrate the limitations of both LLMs and medical experts. Evidence from this\nstudy suggests that incorporating LLM-generated candidate summaries into the clinical workflow could reduce\ndocumentation load, potentially leading to decreased clinician strain and improved patient care. Testing this\nhypothesis motivates future prospective studies in clinical environments.\n7 Acknowledgements\nMicrosoft provided Azure OpenAI credits for this project via both the Accelerate Foundation Models Academic\nResearch (AFMAR) program and also a cloud services grant to Stanford Data Science. Further compute\nsupport was provided by One Medical, which Asad Aali used as part of his summer internship. Curtis Langlotz\nis supported by NIH grants R01 HL155410, R01 HL157235, by AHRQ grant R18HS026886, by the Gordon and\nBetty Moore Foundation, and by the National Institute of Biomedical Imaging and Bioengineering (NIBIB)\nunder contract 75N92020C00021. Akshay Chaudhari receives support from NIH grants R01 HL167974, R01\nAR077604, R01 EB002524, R01 AR079431, and P41 EB027060; from NIH contracts 75N92020C00008 and\n75N92020C00021; and from GE Healthcare, Philips, and Amazon.\n8 Data and Code Availability\nWhile all datasets are publicly available, our GitHub repository github.com/StanfordMIMI/clin-summ includes\npreprocessed versions for those which do not require PhysioNet access: Open-i [66] (radiology reports),\nMeQSum [72] (patient questions), and ACI-Bench [44] (dialogue). Researchers can also access the original\n|15\ndatasets via the provided references. Any further distribution of datasets is subject to the terms of use and\ndata sharing agreements stipulated by the original creators. Our repository also contains experiment code\nand links to open-source models hosted by HuggingFace [86].\n9 Author contributions\nDVV collected data, developed code, ran experiments, designed studies, analyzed results, created figures, and\nwrote the manuscript. All authors reviewed the manuscript, providing meaningful revisions and feedback.\nCVU, LB, JBD provided technical advice in addition to conducting qualitative analysis (CVU), building\ninfrastructure for the Azure API (LB), and implementing the MEDCON metric (JB). AA assisted in model\nfine-tuning. CB, AP, MP, EPR, AS participated in the reader study as radiologists. NR, PH, WC, NA, JH\nparticipated in the reader study as hospitalists. CPL, JP, ASC provided student funding. SG advised on\nstudy design for which JH and JP provided additional feedback. JP, ASC guided the project, with ASC\nserving as principal investigator and advising on technical details and overall direction. No funders or third\nparties were involved in study design, analysis, or writing.\n|16\nReferences\n1.\nGolob Jr, J. F., Como, J. J. & Claridge, J. A. The painful truth: The documentation burden of a trauma surgeon. Journal\nof Trauma and Acute Care Surgery 80, 742\u2013747 (2016).\n2.\nArndt, B. G., Beasley, J. W., Watkinson, M. D., Temte, J. L., Tuan, W.-J., Sinsky, C. A. & Gilchrist, V. J. Tethered\nto the EHR: primary care physician workload assessment using EHR event log data and time-motion observations. The\nAnnals of Family Medicine 15, 419\u2013426 (2017).\n3.\nFleming, S. L., Lozano, A., Haberkorn, W. J., Jindal, J. A., Reis, E. P., Thapa, R., Blankemeier, L., Genkins, J. Z.,\nSteinberg, E., Nayak, A., et al. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical\nRecords. arXiv preprint arXiv:2308.14089 (2023).\n4.\nYackel, T. R. & Embi, P. J. Unintended errors with EHR-based result management: a case series. Journal of the American\nMedical Informatics Association 17, 104\u2013107 (2010).\n5.\nBowman, S. Impact of electronic health record systems on information integrity: quality and safety implications. Perspectives\nin health information management 10 (2013).\n6.\nGershanik, E. F., Lacson, R. & Khorasani, R. Critical finding capture in the impression section of radiology reports in\nAMIA Annual Symposium Proceedings 2011 (2011), 465.\n7.\nGesner, E., Gazarian, P. & Dykes, P. The burden and burnout in documenting patient care: an integrative literature\nreview. MEDINFO 2019: Health and Wellbeing e-Networks for All, 1194\u20131198 (2019).\n8.\nRatwani, R. M., Savage, E., Will, A., Arnold, R., Khairat, S., Miller, K., Fairbanks, R. J., Hodgkins, M. & Hettinger,\nA. Z. A usability and safety analysis of electronic health records: a multi-center study. Journal of the American Medical\nInformatics Association 25, 1197\u20131201 (2018).\n9.\nEhrenfeld, J. M. & Wanderer, J. P. Technology as friend or foe? Do electronic health records increase burnout? Current\nOpinion in Anesthesiology 31, 357\u2013360 (2018).\n10.\nSinsky, C., Colligan, L., Li, L., Prgomet, M., Reynolds, S., Goeders, L., Westbrook, J., Tutty, M. & Blike, G. Allocation of\nphysician time in ambulatory practice: a time and motion study in 4 specialties. Annals of internal medicine 165, 753\u2013760\n(2016).\n11.\nKhamisa, N., Peltzer, K. & Oldenburg, B. Burnout in relation to specific contributing factors and health outcomes among\nnurses: a systematic review. International journal of environmental research and public health 10, 2214\u20132240 (2013).\n12.\nDuffy, W. J., Kharasch, M. S. & Du, H. Point of care documentation impact on the nurse-patient interaction. Nursing\nAdministration Quarterly 34, E1\u2013E10 (2010).\n13.\nChang, C.-P., Lee, T.-T., Liu, C.-H. & Mills, M. E. Nurses\u2019 experiences of an initial and reimplemented electronic health\nrecord use. CIN: Computers, Informatics, Nursing 34, 183\u2013190 (2016).\n14.\nShanafelt, T. D., Dyrbye, L. N., Sinsky, C., Hasan, O., Satele, D., Sloan, J. & West, C. P. Relationship between clerical\nburden and characteristics of the electronic environment with physician burnout and professional satisfaction in Mayo\nClinic Proceedings 91 (2016), 836\u2013848.\n15.\nRobinson, K. E. & Kersey, J. A. Novel electronic health record (EHR) education intervention in large healthcare organization\nimproves quality, efficiency, time, and impact on burnout. Medicine 97 (2018).\n16.\nToussaint, W., Van Veen, D., Irwin, C., Nachmany, Y., Barreiro-Perez, M., D\u00edaz-Pel\u00e1ez, E., de Sousa, S. G., Mill\u00e1n, L.,\nS\u00e1nchez, P. L., S\u00e1nchez-Puente, A., et al. Design considerations for high impact, automated echocardiogram analysis.\narXiv preprint arXiv:2006.06292 (2020).\n17.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners. Advances in neural information processing systems 33, 1877\u20131901\n(2020).\n18.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223 (2023).\n19.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S.,\net al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).\n20.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A.,\net al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).\n21.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685 (2023).\n22.\nWornow, M., Xu, Y., Thapa, R., Patel, B., Steinberg, E., Fleming, S., Pfeffer, M. A., Fries, J. & Shah, N. H. The shaky\nfoundations of large language models and foundation models for electronic health records. npj Digital Medicine 6, 135\n(2023).\n23.\nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F. & Ting, D. S. W. Large language models\nin medicine. Nature Medicine, 1\u201311 (2023).\n24.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al. Large Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138 (2022).\n25.\nTu, T., Azizi, S., Driess, D., Schaekermann, M., Amin, M., Chang, P.-C., Carroll, A., Lau, C., Tanno, R., Ktena, I., et al.\nTowards generalist biomedical ai. arXiv preprint arXiv:2307.14334 (2023).\n26.\nToma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B. & Wang, B. Clinical Camel: An Open-Source Expert-Level\nMedical Language Model with Dialogue-Based Knowledge Encoding. arXiv preprint arXiv:2305.12031 (2023).\n27.\nVan Veen, D., Van Uden, C., Attias, M., Pareek, A., Bluethgen, C., Polacin, M., Chiu, W., Delbrouck, J.-B., Chaves,\nJ. M. Z., Langlotz, C. P., et al. RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large\nLanguage Models. arXiv preprint arXiv:2305.01146 (2023).\n28.\nMathur, Y., Rangreji, S., Kapoor, R., Palavalli, M., Bertsch, A. & Gormley, M. R. SummQA at MEDIQA-Chat 2023:\nIn-Context Learning with GPT-4 for Medical Summarization. arXiv preprint arXiv:2306.17384 (2023).\n29.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141. & Polosukhin, I. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n30.\nOpenAI. GPT-4 Technical Report 2023. arXiv: 2303.08774 [cs.CL].\n31.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n|17\n32.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale,\nS., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n33.\nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n34.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask\nlearners. OpenAI blog 1, 9 (2019).\n35.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M. & Le, Q. V. Finetuned language\nmodels are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n36.\nLiu, Z., Li, Y., Shu, P., Zhong, A., Yang, L., Ju, C., Wu, Z., Ma, C., Luo, J., Chen, C., et al. Radiology-Llama2:\nBest-in-Class Large Language Model for Radiology. arXiv preprint arXiv:2309.06419 (2023).\n37.\nLi, X. L. & Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190\n(2021).\n38.\nHu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, L. & Chen, W. LoRA: Low-Rank Adaptation of Large Language\nModels 2021. arXiv: 2106.09685 [cs.CL].\n39.\nLampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson, K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang, J. X.\n& Hill, F. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329 (2022).\n40.\nLiu, S., McCoy, A. B., Wright, A. P., Carew, B., Genkins, J. Z., Huang, S. S., Peterson, J. F., Steitz, B. & Wright, A.\nLeveraging Large Language Models for Generating Responses to Patient Messages. medRxiv, 2023\u201307 (2023).\n41.\nKahn Jr, C. E., Langlotz, C. P., Burnside, E. S., Carrino, J. A., Channin, D. S., Hovsepian, D. M. & Rubin, D. L. Toward\nbest practices in radiology reporting. Radiology 252, 852\u2013856 (2009).\n42.\nGao, Y., Dligach, D., Miller, T., Churpek, M. M. & Afshar, M. Overview of the Problem List Summarization (ProbSum)\n2023 Shared Task on Summarizing Patients\u2019 Active Diagnoses and Problems from Electronic Health Record Progress\nNotes. arXiv preprint arXiv:2306.05270 (2023).\n43.\nAbacha, A. B., Yim, W.-w., Adams, G., Snider, N. & Yetisgen-Yildiz, M. Overview of the MEDIQA-Chat 2023 Shared\nTasks on the Summarization & Generation of Doctor-Patient Conversations in Proceedings of the 5th Clinical Natural\nLanguage Processing Workshop (2023), 503\u2013513.\n44.\nYim, W.-w., Fu, Y., Abacha, A. B., Snider, N., Lin, T. & Yetisgen, M. ACI-BENCH: a Novel Ambient Clinical Intelligence\nDataset for Benchmarking Automatic Visit Note Generation. arXiv preprint arXiv:2306.02022 (2023).\n45.\nYu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E. P., Fonseca, E., Lee, H., Shakeri, Z., Ng, A., et al. Radiology\nReport Expert Evaluation (ReXVal) Dataset 2023.\n46.\nTang, L., Sun, Z., Idnay, B., Nestor, J. G., Soroush, A., Elias, P. A., Xu, Z., Ding, Y., Durrett, G., Rousseau, J. F., et al.\nEvaluating large language models on medical evidence summarization. npj Digital Medicine 6, 158 (2023).\n47.\nChen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey, W., Foster, G., Jones, L., Parmar, N., Schuster, M., Chen, Z.,\net al. The best of both worlds: Combining recent advances in neural machine translation. arXiv preprint arXiv:1804.09849\n(2018).\n48.\nShi, T., Keneshloo, Y., Ramakrishnan, N. & Reddy, C. K. Neural abstractive text summarization with sequence-to-sequence\nmodels. ACM Transactions on Data Science 2, 1\u201337 (2021).\n49.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I. & Xing, E. P. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality Mar. 2023.\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\n50.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. & Liu, P. J. Exploring the limits\nof transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 5485\u20135551\n(2020).\n51.\nChung, H., Hou, L., Longpre, S., et al. Scaling Instruction-Finetuned Language Models. https://doi.org/10.48550/arXiv.2210.11416\n(2022).\n52.\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J. & Roberts, A.\nThe Flan Collection: Designing Data and Methods for Effective Instruction Tuning 2023. arXiv: 2301.13688 [cs.AI].\n53.\nLehman, E., Hernandez, E., Mahajan, D., Wulff, J., Smith, M. J., Ziegler, Z., Nadler, D., Szolovits, P., Johnson, A. &\nAlsentzer, E. Do We Still Need Clinical Language Models? arXiv preprint arXiv:2302.08091 (2023).\n54.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster, T., Zheng, S., et al.\nUl2: Unifying language learning paradigms in The Eleventh International Conference on Learning Representations (2022).\n55.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al.\nScaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n56.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P. & Hashimoto, T. B. Stanford Alpaca: An\nInstruction-following LLaMA model https://github.com/tatsu-lab/stanford_alpaca. 2023.\n57.\nHan, T., Adams, L. C., Papaioannou, J.-M., Grundmann, P., Oberhauser, T., L\u00f6ser, A., Truhn, D. & Bressem, K. K.\nMedAlpaca\u2013An Open-Source Collection of Medical Conversational AI Models and Training Data. arXiv preprint\narXiv:2304.08247 (2023).\n58.\nOpenAI. ChatGPT Accessed: 2023-09-04. 2022. https://openai.com/blog/chatgpt.\n59.\nLim, Z. W., Pushpanathan, K., Yew, S. M. E., Lai, Y., Sun, C.-H., Lam, J. S. H., Chen, D. Z., Goh, J. H. L., Tan,\nM. C. J., Sheng, B., et al. Benchmarking large language models\u2019 performances for myopia care: a comparative analysis of\nChatGPT-3.5, ChatGPT-4.0, and Google Bard. EBioMedicine 95 (2023).\n60.\nRoso\u0142, M., G\u0105sior, J. S., \u0141aba, J., Korzeniewski, K. & M\u0142y\u0144czak, M. Evaluation of the performance of GPT-3.5 and GPT-4\non the Medical Final Examination. medRxiv, 2023\u201306 (2023).\n61.\nBrin, D., Sorin, V., Vaid, A., Soroush, A., Glicksberg, B. S., Charney, A. W., Nadkarni, G. & Klang, E. Comparing\nChatGPT and GPT-4 performance in USMLE soft skill assessments. Scientific Reports 13, 16492 (2023).\n62.\nDeka, P., Jurek-Loughrey, A., et al. Evidence Extraction to Validate Medical Claims in Fake News Detection in International\nConference on Health Information Science (2022), 3\u201315.\n63.\nNie, F., Chen, M., Zhang, Z. & Cheng, X. Improving few-shot performance of language models via nearest neighbor\ncalibration. arXiv preprint arXiv:2212.02216 (2022).\n64.\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. arXiv preprint\narXiv:2305.14314 (2023).\n|18\n65.\nPeng, A., Wu, M., Allard, J., Kilpatrick, L. & Heidel, S. GPT-3.5: Turbo, Fine-Tuning, and API Updates https:\n//openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates. Accessed: August 22, 2023. 2023.\n66.\nDemner-Fushman, D., Kohli, M. D., Rosenman, M. B., Shooshan, S. E., Rodriguez, L., Antani, S., Thoma, G. R. &\nMcDonald, C. J. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American\nMedical Informatics Association 23, 304\u2013310 (2016).\n67.\nJohnson, A. et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports.\nhttps://www.nature.com/articles/s41597-019-0322-0 (2019).\n68.\nChen, Z., Varma, M., Wan, X., Langlotz, C. & Delbrouck, J.-B. Toward Expanding the Scope of Radiology Report\nSummarization to Multiple Anatomies and Modalities in Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers) (Association for Computational Linguistics, Toronto, Canada, July\n2023), 469\u2013484. https://aclanthology.org/2023.acl-short.41.\n69.\nDelbrouck, J.-B., Varma, M., Chambon, P. & Langlotz, C. Overview of the RadSum23 Shared Task on Multi-modal\nand Multi-anatomical Radiology Report Summarization in Proceedings of the 22st Workshop on Biomedical Language\nProcessing (Association for Computational Linguistics, Toronto, Canada, July 2023).\n70.\nDemner-Fushman, D., Ananiadou, S. & Cohen, K. B. The 22nd Workshop on Biomedical Natural Language Processing\nand BioNLP Shared Tasks in The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\n(2023).\n71.\nJohnson, A., Bulgarelli, L., Pollard, T., Horng, S., Celi, L. A. & Mark, R. Mimic-iv. PhysioNet. Available online at:\nhttps://physionet. org/content/mimiciv/1.0/(accessed August 23, 2021) (2020).\n72.\nBen Abacha, A. & Demner-Fushman, D. On the Summarization of Consumer Health Questions in Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28th - August 2\n(2019).\n73.\nGao, Y., Miller, T., Afshar, M. & Dligach, D. BioNLP Workshop 2023 Shared Task 1A: Problem List Summarization in\nProceedings of the 22nd Workshop on Biomedical Language Processing (2023).\n74.\nGoldberger, A. L., Amaral, L. A. N., Glass, L., Hausdorff, J. M., Ivanov, P. C., Mark, R. G., Mietus, J. E., Moody, G. B., Peng,\nC.-K. & Stanley, H. E. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex\nPhysiologic Signals. Circulation 101. Circulation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full\nPMID:1085218; doi: 10.1161/01.CIR.101.23.e215, e215\u2013e220 (2000 (June 13)).\n75.\nYim, W., Ben Abacha, A., Snider, N., Adams, G. & Yetisgen, M. Overview of the MEDIQA-Sum Task at ImageCLEF\n2023: Summarization and Classification of Doctor-Patient Conversations in CLEF 2023 Working Notes (CEUR-WS.org,\nThessaloniki, Greece, Sept. 2023).\n76.\nMa, C., Wu, Z., Wang, J., Xu, S., Wei, Y., Liu, Z., Guo, L., Cai, X., Zhang, S., Zhang, T., et al. ImpressionGPT: an\niterative optimizing framework for radiology report summarization with chatGPT. arXiv preprint arXiv:2304.08448\n(2023).\n77.\nWei, S., Lu, W., Peng, X., Wang, S., Wang, Y.-F. & Zhang, W. Medical Question Summarization with Entity-driven\nContrastive Learning. arXiv preprint arXiv:2304.07437 (2023).\n78.\nManakul, P., Fathullah, Y., Liusie, A., Raina, V., Raina, V. & Gales, M. CUED at ProbSum 2023: Hierarchical Ensemble\nof Summarization Models. arXiv preprint arXiv:2306.05317 (2023).\n79.\nSaravia, E. Prompt Engineering Guide. https://github.com/dair-ai/Prompt-Engineering-Guide (Dec. 2022).\n80.\nBest Practices for Prompt Engineering with OpenAI API https://help.openai.com/en/articles/6654000-best-practices-for-\nprompt-engineering-with-openai-api. Accessed: 2023-09-08. OpenAI, 2023.\n81.\nStrobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H. & Rush, A. M. Interactive and visual prompt\nengineering for ad-hoc task adaptation with large language models. IEEE transactions on visualization and computer\ngraphics 29, 1146\u20131156 (2022).\n82.\nWang, J., Shi, E., Yu, S., Wu, Z., Ma, C., Dai, H., Yang, Q., Kang, Y., Wu, J., Hu, H., et al. Prompt engineering for\nhealthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670 (2023).\n83.\nMangrulkar, S., Gugger, S., Debut, L., Belkada, Y. & Paul, S. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning\nmethods https://github.com/huggingface/peft. 2022.\n84.\nFrantar, E., Ashkboos, S., Hoefler, T. & Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained\ntransformers. arXiv preprint arXiv:2210.17323 (2022).\n85.\nLoshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017).\n86.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.\nTransformers: State-of-the-art natural language processing in Proceedings of the 2020 conference on empirical methods in\nnatural language processing: system demonstrations (2020), 38\u201345.\n87.\nPapineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation in\nProceedings of the 40th annual meeting of the Association for Computational Linguistics (2002), 311\u2013318.\n88.\nLin, C.-Y. Rouge: A package for automatic evaluation of summaries in Text summarization branches out (2004), 74\u201381.\n89.\nZhang*, T., Kishore*, V., Wu*, F., Weinberger, K. Q. & Artzi, Y. BERTScore: Evaluating Text Generation with BERT\nin International Conference on Learning Representations (2020). https://openreview.net/forum?id=SkeHuCVFDr.\n90.\nSoldaini, L. & Goharian, N. Quickumls: a fast, unsupervised approach for medical concept extraction in MedIR workshop,\nsigir (2016), 1\u20134.\n91.\nOkazaki, N. & Tsujii, J. Simple and efficient algorithm for approximate dictionary matching in Proceedings of the 23rd\nInternational Conference on Computational Linguistics (Coling 2010) (2010), 851\u2013859.\n92.\nKoo, T. K. & Li, M. Y. A guideline of selecting and reporting intraclass correlation coefficients for reliability research.\nJournal of chiropractic medicine 15, 155\u2013163 (2016).\n93.\nVallat, R. Pingouin: statistics in Python. J. Open Source Softw. 3, 1026 (2018).\n94.\nWalsh, K. E., Harik, P., Mazor, K. M., Perfetto, D., Anatchkova, M., Biggins, C., Wagner, J., Schoettker, P. J., Firneno, C.,\nKlugman, R., et al. Measuring harm in healthcare: optimizing adverse event review. Medical care 55, 436 (2017).\n95.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N. & Wu, Y. Exploring the limits of language modeling. arXiv preprint\narXiv:1602.02410 (2016).\n|19\n96.\nChang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al. A survey on\nevaluation of large language models. arXiv preprint arXiv:2307.03109 (2023).\n97.\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S. & R\u00e9, C. Hyena hierarchy:\nTowards larger convolutional language models. arXiv preprint arXiv:2302.10866 (2023).\n98.\nDing, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N. & Wei, F. LongNet: Scaling Transformers to\n1,000,000,000 Tokens 2023. arXiv: 2307.02486 [cs.CL].\n99.\nOmiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R. Large language models propagate race-based\nmedicine. NPJ Digital Medicine 6, 195 (2023).\n100.\nZack, T., Lehman, E., Suzgun, M., Rodriguez, J. A., Celi, L. A., Gichoya, J., Jurafsky, D., Szolovits, P., Bates, D. W.,\nAbdulnour, R.-E. E., et al. Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a\nmodel evaluation study. The Lancet Digital Health 6, e12\u2013e22 (2024).\n|20\nA Appendix\nDatasets\nMetrics\nBLEU\nROUGE-L\nBERTScore\nMEDCON\nMIMIC-III\nKey\nGPT-4\nGPT-3.5\nFLAN-T5\nFLAN-UL2\nLlama-2\nVicuna\nFLAN-T5\n+ QLoRA\nPatient\nquestions\nRadiology reports\nMIMIC-CXR\nOpen-i\nDialogue\nProgress\nnotes\nFigure A1 | Metric scores vs. number of in-context examples across models and datasets. We also include the best model\nfine-tuned with QLoRA (FLAN-T5) as a horizontal dashed line. Note the allowable number of in-context examples varies\nsignificantly by model and dataset.\n|21\nFLAN-T5\nFLAN-UL2\nLlama-2\nVicuna\nFigure A2 | One in-context example (ICL) vs. QLoRA across open-source models on patient health questions. While QLoRA\ntypically outperforms ICL with the better models (FLAN-T5, Llama-2), this relationship reverses given sufficient in-context\nexamples (Figure A1). Figure 4 contains similar results with the Open-i radiology report dataset.\n0\n15\nLlama-2 (7B)\n0\n15\nLlama-2 (13B)\nBLEU\n10\n40\nLlama-2 (7B)\n10\n40\nLlama-2 (13B)\nROUGE-L\n82\n90\nLlama-2 (7B)\n82\n90\nLlama-2 (13B)\nBERTScore\n10\n55\nLlama-2 (7B)\n10\n55\nLlama-2 (13B)\nMEDCON\nFigure A3 | Comparing Llama-2 (7B) vs. Llama-2 (13B). As most data points are near or slightly above the dashed lines\ndenoting equivalence, we conclude that the larger Llama-2 model (13B parameters) delivers marginal improvement for clinical\nsummarization tasks compared to the 7B model. Note that each data point corresponds to the average score of s = 250 samples\nfor a given experimental configuration, i.e. {dataset \u00d7 m in-context examples}.\n|22\nTable A1 | Reader study results evaluating completeness, correctness, conciseness (columns) across individual readers and\npooled across readers. Scores are on the range [-10, 10], where positive scores denote the best model is preferred to the medical\nexpert. Intensity of highlight colors blue (model wins) or red (expert wins) correspond to the score. Asterisks (*) on pooled rows\ndenote statistical significance by a one-sided Wilcoxon signed-rank test, p < 0.001. Intra-class correlation (ICC) values across\nreaders are on a range of [\u22121, 1] where \u22121, 0, and +1 correspond to negative, no, and positive correlations, respectively. See\nFigure 7a for study overview.\nTask\nReader\nCompleteness\nCorrectness\nConciseness\n1\n3.5 \u00b1 5.6\n1.7 \u00b1 3.6\n1.2 \u00b1 4.8\n2\n3.6 \u00b1 6.6\n2.5 \u00b1 4.7\n-0.3 \u00b1 5.4\nRadiology\n3\n0.8 \u00b1 2.9\n0.6 \u00b1 3.2\n-1.7 \u00b1 3.0\nreports\n4\n4.7 \u00b1 4.7\n2.9 \u00b1 3.9\n1.2 \u00b1 3.8\n5\n1.4 \u00b1 4.0\n0.6 \u00b1 2.2\n-0.6 \u00b1 3.4\nPooled\n2.8 \u00b1 5.1 *\n1.7 \u00b1 3.7 *\n0.0 \u00b1 4.3\nICC\n0.45\n0.58\n0.48\n1\n1.7 \u00b1 7.2\n0.6 \u00b1 3.4\n0.3 \u00b1 3.4\n2\n1.0 \u00b1 5.6\n-0.1 \u00b1 3.6\n0.1 \u00b1 3.6\nPatient\n3\n2.3 \u00b1 7.2\n2.0 \u00b1 5.3\n2.2 \u00b1 5.9\nquestions\n4\n1.9 \u00b1 6.7\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n5\n0.9 \u00b1 5.7\n0.4 \u00b1 3.6\n0.4 \u00b1 3.6\nPooled\n1.6 \u00b1 6.5 *\n0.6 \u00b1 3.7 *\n0.6 \u00b1 3.9 *\nICC\n0.67\n0.31\n0.21\n1\n3.4 \u00b1 7.5\n0.5 \u00b1 2.5\n0.1 \u00b1 4.5\n2\n2.3 \u00b1 6.5\n0.6 \u00b1 4.4\n0.4 \u00b1 4.2\nProgress\n3\n2.7 \u00b1 6.3\n1.0 \u00b1 4.4\n0.9 \u00b1 3.7\nnotes\n4\n2.5 \u00b1 7.2\n0.5 \u00b1 6.8\n1.7 \u00b1 6.9\n5\n2.0 \u00b1 6.8\n-0.8 \u00b1 4.5\n-0.1 \u00b1 1.2\nPooled\n2.6 \u00b1 6.9 *\n0.4 \u00b1 4.8\n0.6 \u00b1 4.5 *\nICC\n0.77\n0.74\n0.42\nOverall\nPooled\n2.3 \u00b1 5.8 *\n0.8 \u00b1 3.7 *\n0.4 \u00b1 4.0 *\nICC\n0.63\n0.56\n0.38\n|23\nInput:\nWhere on site does it tell how diabetes and neuropathy causes other things \nlike neck and knee pain and other ailments\nSummary (medical expert):\nHow can I get tested and treated for trimethylaminuria?\nSummary (best model):\nWhat tests are available for trimethylaminuria and where can I get tested?\nInput:\nHello, I have been dealing with trimethylaminuria since I was a child. I have \ndone some of my own research and it looks like not much can be done for \nthis condition. I do not have it all over my body it\u2019s only in my armpits. In the \npast I\u2019ve gone to doctors and dermatologist they gave me no answers until \nI looked online today and finally found out what I have. I don\u2019t know maybe \nI\u2019m wrong. But this disease isn\u2019t even consider common because no one \nhas done anything about it. I\u2019m sure they\u2019re thousands of women with it... \nCan I be tested for it and help in some kind of way to finding a cure or \nsomething?  What testing is done for this?  And where? Thank you\nExample 2: The model performed worse because it did not interpret the patient's implied intention (purple).\nExample 1: The model performed better because it summarized the question more specifically (green).\nSummary (medical expert):\nWhat can diabetic neuropathy lead to?\nSummary (best model):\nHow does diabetes and neuropathy cause neck and knee pain?\nAttribute\nAverage\nExample 1\nExample 2\nCompleteness\n1.6\n3\n-4\nCorrectness\n0.6\n1\n-2\nConciseness\n0.6\n1\n-1\nPatient questions\nBlue: \ncorrect; exists in input + expert + model\nPurple: correct; exists in input + expert only\nGreen: correct; exists in input + model only\nOrange: incoherent or filler\nRed: \nincorrect\nColor key: \nReader scores: \nFigure A4 | Annotation: patient health questions. The table (lower left) contains reader scores for these two examples\nand the task average across all samples.\n|24\nFigure A5 | Annotation: progress notes. The tables (lower right) contain reader scores for this example and the task\naverage across all samples.\n|25\nInput:\n[DOCTOR] hi , [PATIENT] , how are you ? [PATIENT] hi . good to see you . [DOCTOR] it's good to see you as well . so i know that the nurse \ntold you about dax . i'd like to tell dax a little bit about you . [PATIENT] sure . [DOCTOR] okay ? so , [PATIENT] is a 62-year-old male with a \npast medical history significant for a kidney transplant , hypothyroidism , and arthritis , who presents today with complaints of joint pain . \n[PATIENT] , what's going on with your joint ? what happened ? [PATIENT] uh , so , over the the weekend , we've been moving boxes up and \ndown our basements stairs , and by the end of the day my knees were just killing me . [DOCTOR] okay . is , is one knee worse than the \nother ? [PATIENT] equally painful . [DOCTOR] okay . [PATIENT] both of them . [DOCTOR] and did you , did you injure one of them ? \n[PATIENT] um , uh , i've had some knee problems in the past but i think it was just the repetition and the weight of the boxes . [DOCTOR] \nokay . all right . and , and what have you taken for the pain ? [PATIENT] a little tylenol . i iced them for a bit . nothing really seemed to help , \nthough . [DOCTOR] okay . all right . um , and does it prevent you from doing , like , your activities of daily living , like walking and exercising \nand things like that ? [PATIENT] uh , saturday night it actually kept me up for a bit . they were pretty sore . [DOCTOR] mm-hmm . okay . and \nany other symptoms like fever or chills ? [PATIENT] no . [DOCTOR] joint pain ... i mean , like muscle aches ? [PATIENT] no . [DOCTOR] \nnausea , vomiting , diarrhea ? [PATIENT] no . [DOCTOR] anything like that ? [PATIENT] no . [DOCTOR] okay . all right . now , i know that \nyou've had the kidney transplant a few years ago for some polycystic kidneys . [PATIENT] mm-hmm . [DOCTOR] um , how are you doing \nwith that ? i know that you told dr. gutierrez- [PATIENT] mm . [DOCTOR] . a couple of weeks ago . [PATIENT] yes . [DOCTOR] everything's \nokay ? [PATIENT] so far , so good . [DOCTOR] all right . and you're taking your immunosuppressive medications ? [PATIENT] yes , i am . \n[DOCTOR] okay . all right . um , and did they have anything to say ? i have n't gotten any reports from them , so ... [PATIENT] no , n- \nnothing out of the ordinary , from what they reported . [DOCTOR] okay . all right . um , and in terms of your hyperthyroidism , how are you \ndoing with the synthroid ? are you doing okay ? [PATIENT] uh , yes , i am . [DOCTOR] you're taking it regularly ? [PATIENT] on the clock , \nyes . [DOCTOR] yes . okay . and any fatigue ? weight gain ? anything like that that you've noticed ? [PATIENT] no , nothing out of the \nordinary . [DOCTOR] okay . and just in general , you know , i know that we've kind of battled with your arthritis . [PATIENT] mm-hmm . \n[DOCTOR] you know , it's hard because you ca n't take certain medications 'cause of your kidney transplant . [PATIENT] sure . [DOCTOR] \nso other than your knees , any other joint pain or anything like that ? [PATIENT] every once in a while , my elbow , but nothing , nothing out \nof the ordinary . [DOCTOR] okay . all right . now i know the nurse did a review of systems sheet when you checked in . any other symptoms \ni might have missed ? [PATIENT] no . [DOCTOR] no headaches ? [PATIENT] no headaches . [DOCTOR] anything like that w- ... okay . all \nright . well , i wan na go ahead and do a quick physical exam , all right ? hey , dragon , show me the vital signs . so here in the office , your \nvital signs look good . you do n't have a fever , which is good . [PATIENT] mm-hmm . [DOCTOR] your heart rate and your , uh , blood \npressure look fine . i'm just gon na check some things out , and i'll let you know what i find , okay ? [PATIENT] perfect . [DOCTOR] all right . \ndoes that hurt ? [PATIENT] a little bit . that's tender . [DOCTOR] okay , so on physical examination , on your heart exam , i do appreciate a \nlittle two out of six systolic ejection murmur- [PATIENT] mm-hmm . [DOCTOR] . which we've heard in the past . okay , so that seems stable . \non your knee exam , there is some edema and some erythema of your right knee , but your left knee looks fine , okay ? um , you do have \nsome pain to palpation of the right knee and some decreased range of motion , um , on exam , okay ? so what does that mean ? so we'll go \nahead and we'll see if we can take a look at some of these things . i know that they did an x-ray before you came in , okay ? [PATIENT] \nmm-hmm . [DOCTOR] so let's take a look at that . [PATIENT] sure . [DOCTOR] hey , dragon , show me the right knee x-ray . so here's the r- \nhere's your right knee x-ray . this basically shows that there's good bony alignment . there's no acute fracture , which is not surprising , \nbased on the history . [PATIENT] mm-hmm . [DOCTOR] okay ? hey , dragon , show me the labs . and here , looking at your lab results , you \nknow , your white blood cell count is not elevated , which is good . you know , we get concerned about that in somebody who's \nimmunocompromised . [PATIENT] mm-hmm . [DOCTOR] and it looks like your kidney function is also very good . so i'm , i'm very happy \nabout that . [PATIENT] yeah . [DOCTOR] okay ? so i just wan na go over a little bit about my assessment and my plan for you . [PATIENT] \nmm-hmm . [DOCTOR] so for your knee pain , i think that this is an acute exacerbation of your arthritis , okay ? so i wan na go ahead and if \n... and prescribe some ultram 50 milligrams every six hours as needed . [PATIENT] okay . [DOCTOR] okay ? i also wan na go ahead and \njust order an autoimmune panel , okay ? hey , dragon , order an autoimmune panel . and you know , i , i want , i want you to just take it easy \nfor right now , and if your symptoms continue , we'll talk about further imaging and possibly referral to physical therapy , okay ? [PATIENT] \nyou got it . [DOCTOR] for your second problem , your hypothyroidism , i wan na go ahead and continue you on this ... on the synthroid , and \ni wan na go ahead and order some thyroid labs , okay ? [PATIENT] sure . [DOCTOR] hey , dragon , order a thyroid panel . and then for your \nlast problem , the arthritis , you know , we just kinda talked about that . you know , it's gon na be a struggle for you because again , you ca \nn't take some of those anti-inflammatory medications because of your kidney transplant , so ... [PATIENT] mm-hmm . [DOCTOR] you know , \nlet's see how we do over the next couple weeks , and again , we'll refer you to physical therapy if we need to , okay ? [PATIENT] you got it . \n[DOCTOR] you have any questions ? [PATIENT] not at this point . [DOCTOR] okay . hey , dragon , finalize the note.\nSummary (medical expert):\nASSESSMENT AND PLAN\n[PATIENT] is a 62-year-old male with a past medical history significant for a kidney transplant, hypothyroidism, \nand arthritis. He presents today with complaints of joint pain.\n \n Arthritis.\n \u2022 Medical Reasoning: The patient reports increased joint pain in his bilateral knees over the past weekend. Given \nthat his right knee x-ray was unremarkable, I believe this is an acute exacerbation of his arthritis.\n \u2022 Additional Testing: We will order an autoimmune panel for further evaluation.\n \u2022 Medical Treatment: Initiate Ultram 50 mg every 6 hours as needed.\n \u2022 Patient Education and Counseling: I advised the patient to rest his knees. If his symptoms persist, we can \nconsider further imaging and possibly a referral to physical therapy.\n \n Hypothyroidism.\n \u2022 Medical Reasoning: The patient is doing well on Synthroid and is asymptomatic at this time.\n \u2022 Additional Testing: We will order a thyroid panel.\n \u2022 Medical Treatment: Continue Synthroid.\n \n Status post renal transplant.\n \u2022 Medical Reasoning: He is doing well and has been compliant with his immunosuppressive medications. On \nrecent labs, his white blood cell count was within a normal limits and his kidney function is stable.\n \u2022 Medical Treatment: Continue current regimen.\nSummary (best model):\nASSESSMENT AND PLAN  \n[PATIENT] is a 62-year-old male with a past medical history significant for a kidney transplant, hypothyroidism, \nand arthritis. He presents today with complaints of joint pain.    \nAcute exacerbation of arthritis.  \n\u2022 Medical Reasoning: His right knee is swollen and erythematous, and he has pain with palpation and decreased \nrange of motion. His recent history of physical exertion and the findings on his physical exam are consistent with \nan acute exacerbation of his arthritis.  \n\u2022 Additional Testing: I will order an autoimmune panel to rule out any autoimmune causes of his arthritis.  \n\u2022 Medical Treatment: I will prescribe Ultram 50 mg every six hours as needed for pain.    \nHypothyroidism.  \n\u2022 Medical Reasoning: He has been stable on Synthroid.  \n\u2022 Additional Testing: I will order a thyroid panel to ensure his thyroid levels are within normal limits.  \n\u2022 Medical Treatment: Continue Synthroid at current dosage.    \nKidney transplant.  \n\u2022 Medical Reasoning: His kidney function is good, and he has no signs of infection.  \n\u2022 Medical Treatment: Continue current immunosuppressive medications.  \n\u2022 Patient Education and Counseling: I advised him to take it easy and avoid physical exertion that could \nexacerbate his arthritis. If his symptoms continue, we will consider further imaging and possibly referral to \nphysical therapy.\nDialogue\nFigure A6 | Example results: doctor-patient dialogue. Note this task is discluded from the reader study due to the\nunwieldiness of a reader parsing many transcribed conversations and lengthy text.\n|26\nTable A2 | Comparison of our general approach (GPT-4 using ICL) against baselines specific to each individual dataset. We\nnote the focal point of our study is not to achieve state-of-the-art quantitative results, especially given the discordance between\nNLP metrics and reader study scores. A - indicates the metric was not reported; a \u25e6 indicates the dataset was preprocessed\ndifferently.\nDataset\nBaseline\nBLEU\nROUGE-L\nBERTScore\nMEDCON\nOpen-i\nOurs\n46.0\n68.2\n94.7\n64.9\nImpressionGPT [76]\n-\n65.4\n-\n-\nMIMIC-CXR\nOurs\n29.6\n53.8\n91.5\n55.6\nRadAdapt [27]\n18.9\n44.5\n90.0\n-\nImpressionGPT [76]\n-\n47.9\n-\n-\nMIMIC-III\nOurs\n11.5\n34.5\n89.0\n36.5\nRadAdapt [27]\n16.2\n38.7\n90.2\n-\nMed-PaLM M [25]\n15.2\n32.0\n-\n-\nPatient questions\nOurs\n10.7\n37.3\n92.5\n59.8\nECL\u25e6 [77]\n-\n50.5\n-\n-\nProgress notes\nOurs\n3.4\n27.2\n86.1\n31.5\nCUED [78]\n-\n30.1\n-\n-\nDialogue\nOurs\n26.9\n42.9\n90.2\n59.9\nACI-Bench\u25e6 [44]\n-\n45.6\n-\n57.8\n|27\n"
  },
  {
    "title": "AudioSR: Versatile Audio Super-resolution at Scale",
    "link": "https://arxiv.org/pdf/2309.07314.pdf",
    "upvote": "20",
    "text": "AUDIOSR: VERSATILE AUDIO SUPER-RESOLUTION AT SCALE\nHaohe Liu1, Ke Chen2, Qiao Tian3, Wenwu Wang1, Mark D. Plumbley1\n1Centre for Vision Speech and Signal Processing, University of Surrey,\n2University of California San Diego,\n3Speech, Audio & Music Intelligence (SAMI), ByteDance\nABSTRACT\nAudio super-resolution is a fundamental task that pre-\ndicts high-frequency components for low-resolution audio,\nenhancing audio quality in digital applications.\nPrevious\nmethods have limitations such as the limited scope of audio\ntypes (e.g., music, speech) and specific bandwidth settings\nthey can handle (e.g., 4 kHz to 8 kHz). In this paper, we\nintroduce a diffusion-based generative model, AudioSR, that\nis capable of performing robust audio super-resolution on\nversatile audio types, including sound effects, music, and\nspeech. Specifically, AudioSR can upsample any input audio\nsignal within the bandwidth range of 2 kHz to 16 kHz to\na high-resolution audio signal at 24 kHz bandwidth with a\nsampling rate of 48 kHz. Extensive objective evaluation on\nvarious audio super-resolution benchmarks demonstrates the\nstrong result achieved by the proposed model. In addition, our\nsubjective evaluation shows that AudioSR can acts as a plug-\nand-play module to enhance the generation quality of a wide\nrange of audio generative models, including AudioLDM,\nFastspeech2, and MusicGen. Our code and demo are avail-\nable at https://audioldm.github.io/audiosr.\nIndex Terms\u2014 audio super-resolution, diffusion model\n1. INTRODUCTION\nAudio super-resolution (SR) aims to estimate the higher-\nfrequency information of a low-resolution audio signal, which\nyields a high-resolution audio signal with an expanded fre-\nquency range.\nHigh-resolution audio signals usually offer\na better listening experience, which is often referred to as\nhigh fidelity. Due to the ability to enhance audio signal qual-\nity, audio super-resolution plays a significant role in various\napplications, such as historical recording restoration [1].\nPrevious studies on audio SR have primarily focused on\nspecific domains, with a particular emphasis on speech SR.\nEarly research decompose the speech SR task into spectral\nenvelope estimation and excitation generation [2].\nRecent\nworks employing deep learning techniques, such as AECNN\n[3], NuWave [4], and NVSR [5], have shown superior per-\nformance compared to traditional methods.\nIn addition to\nFig. 1. Subjective evaluation shows that applying AudioSR for\naudio super-resolution on the output of audio generation mod-\nels can significantly enhance the perceptual quality.\nspeech, there have been efforts to address music SR, includ-\ning studies on general music [6] and specific instruments [7].\nApart from the limited scope of audio, existing research\non audio SR also has primarily been conducted in controlled\nexperimental settings, limiting its applicability in real-world\nscenarios. An important challenge in audio super-resolution,\nas highlighted in [5], is the issue of bandwidth mismatch. This\noccurs when the bandwidth of the test data differs from that of\nthe training data, leading to model failure. However, this issue\nhas not received significant attention in the literature, as pre-\nvious works typically assume consistent bandwidth settings\nfor both training and testing data. In practice, the input band-\nwidth of test audio can vary due to factors such as limitations\nin recording devices, sound characteristics, or applied com-\npression processes. Only a few studies have explored flexi-\nble input bandwidth, including NVSR [5] and NuWave2 [8].\nHowever, these methods still primarily focus on speech SR\nwithout generalizing to a broader domain.\nIn this paper, we propose a novel method that addresses\nthe limitations of previous work on limited audio types and\ncontrolled sampling rate settings.\nWe introduce a method\ncalled AudioSR, which extends audio SR to a general domain,\nincluding all audible sounds such as music, speech, and sound\neffects. Moreover, AudioSR is capable of handling a flexi-\nble input sampling rate between 4kHz and 32kHz, covering\nmost of the use cases in real-world scenarios. It has been\nfound that the prior knowledge learned by the neural vocoder\nis helpful for reconstructing higher frequency components\nin audio SR tasks [5].\nTherefore, AudioSR follows [5] to\nperform audio SR on the mel-spectrogram and utilizes a neu-\narXiv:2309.07314v1  [cs.SD]  13 Sep 2023\nral vocoder to synthesize the audio signal. To estimate the\nhigh-resolution mel-spectrogram, we follow AudioLDM [9]\nto train a latent diffusion model on learning the conditional\ngeneration of high-resolution mel-spectrogram from low-\nresolution mel-spectrogram.\nOur experiment demonstrates\nthat AudioSR has achieved promising SR results on speech,\nmusic, and sound effects with different input sampling rate\nsettings. Our subjective evaluation on enhancing the output\nof text-to-audio model AudioLDM [9], text-to-music model\nMusicGen [10], and text-to-speech model Fastspeech2 [11]\nshow that AudioSR can be a plug-and-play module for most\naudio generation models to enhance listening quality. Our\ncontributions are summarized as follows:\n\u2022 Our proposed AudioSR is the first system to achieve audio\nSR in the general audible audio domain, covering various\ntypes of audio such as music, speech, and sound effects.\n\u2022 AudioSR can handle a flexible audio bandwidth ranging\nfrom 2kHz to 16kHz, and extend it to 24kHz bandwith with\n48kHz sampling rate.\n\u2022 Besides the promising results on audio SR benchmarks,\nAudioSR is also verified to be a plug-and-play module for\nenhancing the audio quality of various audio generation\nmodels such as AudioLDM, MusicGen, and FastSpeech2.\nThe paper is organized as follows. Section 2 provides a\ngeneral formulation of the audio super resolution task. Sec-\ntion 3 provides a detailed explanation of the design of Au-\ndioSR. The detailed experimental setup is discussed in Sec-\ntion 4. Our experimental results are presented in Section 5,\nand we conclude the paper in Section 6.\n2. PROBLEM FORMULATION\nGiven an analog signal that has been discretely sampled at a\nrate of l samples per second, resulting in a low-resolution se-\nquence of values xl = [xi]i=1,2,...T \u00b7l, the goal of audio super-\nresolution (SR) is to estimate a higher resolution signal yh =\n[yi]i=1,2,...T \u00b7h sampled at a rate of h samples per second,\nwhere h > l and T is the total duration in seconds. Accord-\ning to Nyquist\u2019s theory, xl and yh have maximum frequency\nbandwidths of l/2 Hz and h/2 Hz respectively. Therefore, the\ninformation contained between frequencies of h/2 \u2212 l/2 Hz\nis missing from xl, and estimating this \u201cmissing\u201d frequency\ndata is the core objective of the SR task.\nIn this paper, we follow the method proposed in NVSR [5]\nto decompose the original audio SR task into two steps, in-\ncluding (i) High-resolution Mel spectrogram Estimation, and\n(ii) Mel Spectrogram to Waveform Reconstruction with a Neu-\nral Vocoder. Specifically, we first resampling xl to xh using\ncubic interpolation, where xh has a higher sampling rate h\nbut with limited maximum bandwidth of l/2 Hz. we follow\nthe steps in [5] to calculate the mel spectrogram of both xh\nand yh, resulting Xm\u00d7n and Ym\u00d7n, respectively, where m is\nLatent Diffusion Model\nNeural Vocoder\nLow-res Mel-spectrogram Input\nHigh-res Mel-spectrogram Estimation\nHigh-res Audio Estimation\n\ud835\udc7f\ud835\udc89\n\"\ud835\udc80\ud835\udc89\n$\ud835\udc9a\ud835\udc89\nReplace\nReplace\nSTFT Spectrogram\nLow-res Audio Input\n(Resampled to higher sampling rate)\nSTFT Spectrogram\n\ud835\udc99\ud835\udc89\nFig. 2. The AudioSR architecture. The replacement-based\npost-processing aims to preserve the original lower-frequency\ninformation in the model output.\nthe number of time frames and n is the number of mel fre-\nquency bins. Then we utilize a generative model to learning\nthe process of estimating Y based on X, which is denoted as\nG\u03b8 : X 7\u2192 \u02c6Y , where \u03b8 are the parameters of model G. Finally,\na neural vocoder is employed to reconstruct the high sampling\nrate audio signal based on the estimation of Y , which can be\nformulated as V\u03d5 : \u02c6Y 7\u2192 \u02c6yh, where V is the neural vocoder\nand \u03d5 are the learnable parameters.\n3. METHOD\nThe architecture of the proposed AudioSR is demonstrated in\nFigure 2. After resampling the low-resolution audio xl to xh,\nthe system first calculates both the STFT spectrogram and the\nmel spectrogram of xh. Note that the higher frequency bins\nin Xh are empty because xh does not have high-frequency in-\nformation. Xh is then used as a conditioning signal to guide\nthe pre-trained latent diffusion model to estimate the high-\nresolution mel spectrogram \u02c6Yh. To ensure consistency in the\nlow-frequency information between Xh and \u02c6Yh, we replace\nthe lower frequency part of \u02c6Yh with that of Xh. The mel-\nspectrogram after low-frequency replacement serves as the\ninput to the neural vocoder, which output applies a similar\ntechnique to replace the low-frequency information with that\nof the input low-resolution audio. We introduce the training of\nthe latent diffusion model and neural vocoder in Section 3.1.\nThe post-processing algorithm is elaborated in Section 3.2.\n3.1. High-resolution Waveform Estimation\nLatent diffusion model (LDM) has demonstrated promis-\ning results in various domains, including image synthesis [12]\nand audio generation [9]. In this study, we employ the LDM\nto estimate high-resolution mel-spectrograms. The training\nof our LDM is conducted within a latent space learned by\na pre-trained variational autoencoder (VAE) F(\u00b7). The VAE\nis trained to perform autoencoding with a small compressed\nlatent space in the middle, denoted as F : X 7\u2192 z0 7\u2192\n\u02c6X. By leveraging the lower-dimensional representation z0,\nthe LDM can learn the generation of z0 instead of X, re-\nsulting in a substantial reduction in computational cost. We\nadopt the methodology proposed in AudioLDM to optimize\nthe VAE model, including the use of reconstruction loss, Kull-\nback\u2013Leibler divergence loss, and discriminative loss.\nWe follow the formulation introduced in AudioLDM [9]\nto implement the LDM, with improvements on the training\nobjective, noise schedule, and conditioning mechanism. It\nhas been found that the common noise schedule used in the\ndiffusion model is flawed [13], particularly because the noise\nschedule in the final diffusion step of LDM does not corre-\nspond to a Gaussian distribution. To address this issue, we\nfollow [13] to update the noise schedule to a cosine sched-\nule. This adjustment ensures that a standard Gaussian distri-\nbution can be achieved at the final diffusion step during train-\ning. Additionally, we incorporate the velocity prediction ob-\njective [14] on reflection of using the new noise schedule. The\nfinal training objective of our LDM is\nargminG\u03b8||vk \u2212 G(zk, k, Fenc(Xl); \u03b8)||2\n2,\n(1)\nwhere zk represents the data of z0 at diffusion step k \u2208\n[1, ..., K], || \u00b7 ||2 denotes the Euclidean distance, Fenc denote\nthe VAE encoder, and as described in [13], vk is calculated\nbased on z0, representing the prediction target of G at time\nstep k.\nWe adopt the Transformer-UNet architecture pro-\nposed in [15] as G. The input to G is obtained by concatenat-\ning zk with the Fenc(Xl), which is the VAE latent extracted\nfrom the low-resolution mel-spectrogram Xl.\nTo incorpo-\nrate classifier-free guidance, following the formulation in [9],\nwe replace Fenc(Xl) with an empty tensor at a random rate\n(e.g., 10%) during training. After training the latent diffusion\nmodel, we perform sampling using the DDIM sampler [16].\nNeural Vocoder. The LDM is capable of estimating high-\nresolution mel spectrograms. However, since mel-spectrograms\nare not directly audible, we employ a neural vocoder based\non HiFiGAN [17] to convert the mel-spectrograms into wave-\nforms. To address the issue of spectral leakage when imple-\nmenting the original HiFiGAN, we adopt the multi-resolution\ndiscriminator [18] into the HiFiGAN vocoder. We optimize\nthe vocoder using diverse audio data, as discussed in Sec-\ntion 4, resulting in a vocoder that operates at a sampling rate\nof 48kHz and can work on diverse types of audio.\n3.2. Post-processing and Pre-processing\nPost-processing. The input low-resolution audio features Xh\nand xh are identical to the lower frequency bands in the esti-\nmation target, Yh and yh. As a result, we can reuse the avail-\nable information from Xh and xh to enhance both the LDM\noutput \u02c6Yh and neural vocoder output \u02c6yh. To accomplish this,\nwe first determine the 0.99 roll-off frequency c of the entire\ninput audio based on an open-source method1 applied to both\nXh and the STFT spectrogram of yh. Subsequently, we re-\nplace the spectrogram components below the cutoff frequency\nin the LDM output \u02c6Yh and vocoder output \u02c6yh, with the cor-\nresponding information in the Xh and xh, respectively. This\npost-processing method can ensure the final output does not\nsignificantly alter the lower-frequency information.\nPre-processing. To minimize the mismatch between model\ntraining and evaluation, we perform preprocessing to the in-\nput audio during evaluation with a lowpass-filtering opera-\ntion. We use the same method in post-processing to calculate\nthe 0.99 roll-off frequency and perform lowpass filtering with\nan order 8 Chebyshev filter.\n4. EXPERIMENT\nTraining Datasets.\nThe datasets used in this paper in-\nclude MUSDB18-HQ [19], MoisesDB [20], MedleyDB [21],\nFreeSound [22]2, and the speech dataset from OpenSLR3,\nwhich are downloaded by following the link provided by\nVoiceFixer [1].\nAll the audio data used are resampled at\n48kHz sampling rate. The total duration of the training data\nis approximately 7000 hours. We utilize all these datasets to\noptimize VAE, LDM, and HiFi-GAN.\nTraining Data Simulation.\nWe follow the method intro-\nduced in NVSR [5] to simulate low-high resolution audio data\npairs. Given a high-resolution audio data yh, we first per-\nform lowpass filtering to the audio with a cutoff frequency\nuniformly sampled between 2kHz and 16kHz. To address the\nfilter generalization problem [3], the type of the lowpass filter\nis randomly sampled within Chebyshev, Elliptic, Butterworth,\nand Boxcar, and the order of the lowpass filter is randomly se-\nlected between 2 and 10.\nEvaluation Datasets. We performed both subjective and ob-\njective evaluations. For subjective evaluations, we adopt the\noutput of MusicGen (caption from MusicCaps [23]), Audi-\noLDM (caption from AudioCaps [24]), and Fastspeech2 (tran-\nscription from LJSpeech [25]) to study if the AudioSR can\nenhance the quality of the generation.\nFor MusicGen we\nuse audio tagging 4 to filter out the non-musical generation\noutput. Finally, we collected 50 samples from MusicGen, 50\nsamples from AudioLDM, and 20 samples from FastSpeech2,\nand processed them with AudioSR for subjective evaluations\non listener preference. Besides, we curate three benchmarks\nfor objective evaluation, including ESC50 (sound effect) [26],\nAudioStock (music)5, and VCTK (speech) [5]. The AudioS-\ntock dataset is built by hand-picking 100 high-quality music\nwith 10 different genres. We only use the fold-5 in the ESC50\n1https://librosa.org/doc/main/generated/librosa.\nfeature.spectral_rolloff.html\n2https://labs.freesound.org/\n3https://openslr.org/\n4https://github.com/kkoutini/PaSST\n5https://audiostock.net/\nObjective Evaluation\nSubjective Evaluation\nVCTK (Speech)\nAudioStock (Music)\nESC-50 (Sound Effect)\nESC-50 (4kHz Cutoff Freq)\nCutoff-frequency\n4kHz\n8kHz\n12kHz\nGT-Mel\n0.64\n0.64\n0.64\nCutoff-frequency\n4kHz\n8kHz\n16kHz\n4kHz\n8kHz\n16kHz\nSystem\nOverall Quality\nUnprocessed\n5.15\n4.85\n3.84\nGT-Mel\n0.61\n0.61\n0.61\n0.84\n0.84\n0.84\nGT-Mel\n4.35\nNuWave [4]\n1.42\n1.36\n1.22\nUnprocessed\n4.25\n3.48\n1.99\n3.90\n3.07\n2.25\nUnprocessed\n3.01\nNVSR [5]\n0.91\n0.81\n0.70\nNVSR-DNN\n1.67\n1.49\n1.13\n1.64\n1.59\n1.76\nNVSR-DNN\n2.84\nAudioSR\n1.30\n1.11\n0.94\nNVSR-ResUNet\n1.70\n1.34\n0.95\n1.80\n1.69\n1.67\nNVSR-ResUNet\n3.16\nAudioSR-Speech\n1.03\n0.82\n0.69\nAudioSR\n0.99\n0.74\n0.73\n1.74\n1.57\n1.35\nAudioSR\n4.01\nTable 1. Objective and subjective evaluation results for 48kHz audio SR of speech, music, and sound effect data with varying\ncutoff frequencies in the input audio. The objective metric used for evaluation is the LSD, where lower values indicate superior\nperformance. The subjective metric measures the overall listening quality, with higher values indicating better performance.\nUnprocessed\nNVSR-DNN\nNVSR-ResUNet\nAudioSR\nGround Truth\nFig. 3. Comparison of different systems. AudioSR performs significantly better than the baseline NVSR models.\ndataset as the evaluation set.\nEvaluation Metrics For objective evaluation, we adopt the\nLSD metric, as used in prior studies [3, 5]. Following the\nsetup of [15], we conduct two types of subjective evaluation\non Amazon Mturk6: Overall quality rating and preference\ncomparison. In the overall quality rating, raters assign a score\nbetween 1 and 5 to reflect the audio quality. In the preference\ncomparison, raters compare two audio files and select the one\nthat sounds better.\n5. RESULT\nWe trained two versions of AudioSR for evaluation: the ba-\nsic AudioSR that works on arbitrary audio types and input\nsampling rates, and a speech data fine-tuned variant called\nAudioSR-Speech.\nOur primary baseline for comparison is\nNVSR [5], which employs a similar mel-spectrogram and\nvocoder-based pipeline for audio SR tasks. The main distinc-\ntion between AudioSR and NVSR lies in the mel-spectrogram\nestimation approach:\nAudioSR utilizes a latent diffusion\nmodel, while NVSR employs either a multilayer perceptron\n(NVSR-DNN) or a residual UNet (NVSR-ResUNet).\nFor\nspeech SR, we also compare with NuWave [4] as a baseline\nmodel, which also employs a diffusion model for audio SR.\nAs shown in Table 1, AudioSR has achieved promising re-\nsults on both objective and subjective evaluation. For music\nSR, AudioSR achieves state-of-the-art performance across all\ncutoff frequency settings, outperforming the baseline NVSR\nmodel by a large margin. For speech SR, AudioSR-Speech\nachieves the best performance on the 24kHz to 48kHz upsam-\npling task. Also, the comparison between AudioSR and Au-\ndioSR-Speech indicates that finetuning on a small domain of\ndata can significantly improve the LSD.\nThe LSD metric does not always align with perceptual\nquality. In the 8kHz (i.e., 4kHz cutoff frequency) to 48kHz\nupsampling task on the ESC-50 dataset, we observed that\nNVSR-DNN achieved the best performance with an LSD\nscore of 1.64.\nHowever, subjective evaluations indicated\n6https://www.mturk.com/\nthat the perceptual quality of NVSR-DNN the worst with a\nscore of 2.84, significantly lower than AudioSR\u2019s score of\n4.01. These findings suggest that LSD may not be a suitable\nevaluation metric for audio SR tasks on sound effect data,\nwarranting further investigation in future research.\nAs depicted in Figure 1, our subjective preference test\ndemonstrates that the utilization of AudioSR significantly en-\nhances the perceptual quality of the AudioLDM, MusicGen,\nand FastSpeech2 output. It is worth noting that the output of\nMusicGen is already in a high sampling rate of 32kHz, which\nmay contribute to the relatively high rate of \u201cNo Clear Differ-\nence\u201d responses. However, MusicGen still exhibits a signifi-\ncantly improved perceptual quality after applying AudioSR.\n6. CONCLUSION\nThis paper presents AudioSR, a 48kHz audio super-resolution\nmodel that is capable of working with diverse audio types and\narbitrary sampling rate settings. Through evaluation of mul-\ntiple audio super-resolution benchmarks, AudioSR demon-\nstrates superior and robust performance on various types of\naudio and sampling rates. Additionally, our subjective eval-\nuation highlights the effectiveness of AudioSR in enabling\nplug-and-play quality improvement for the audio generation\nmodels, including AudioLDM, MusicGen, and Fastspeech2.\nFuture work includes extending AudioSR for real-time ap-\nplications and exploring appropriate evaluation protocols for\naudio super-resolution in the general audio domain.\n7. ACKNOWLEDGMENTS\nThis research was partly supported by the British Broad-\ncasting Corporation Research and Development, Engineering\nand Physical Sciences Research Council (EPSRC) Grant\nEP/T019751/1 \u201cAI for Sound\u201d, and a PhD scholarship from\nthe Centre for Vision, Speech and Signal Processing (CVSSP),\nFaculty of Engineering and Physical Science (FEPS), Uni-\nversity of Surrey. For the purpose of open access, the authors\nhave applied a Creative Commons Attribution (CC BY) li-\ncense to any Author Accepted Manuscript version arising.\n8. REFERENCES\n[1] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang,\nC. Huang, and Y. Wang, \u201cVoiceFixer: Toward gen-\neral speech restoration with neural vocoder,\u201d arXiv\npreprint:2109.13731, 2021.\n[2] J. Kontio, L. Laaksonen, and P. Alku, \u201cNeural network-\nbased artificial bandwidth expansion of speech,\u201d Trans-\nactions on Audio, Speech, and Language Processing,\nvol. 15, no. 3, pp. 873\u2013881, 2007.\n[3] H. Wang and D. Wang, \u201cTowards robust speech super-\nresolution,\u201d Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 29, pp. 2058\u20132066, 2021.\n[4] J. Lee and S. Han, \u201cNuWave:\nA diffusion prob-\nabilistic model for neural audio upsampling,\u201d arXiv\npreprint:2104.02321, 2021.\n[5] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian, and D. Wang,\n\u201cNeural vocoder is all you need for speech super-\nresolution,\u201d INTERSPEECH, pp. 4227\u20134231, 2022.\n[6] S. Hu, B. Zhang, B. Liang, E. Zhao, and S. Lui, \u201cPhase-\naware music super-resolution using generative adversar-\nial networks,\u201d INTERSPEECH, pp. 4074\u20134078, 2020.\n[7] N. C. Rakotonirina, \u201cSelf-attention for audio super-\nresolution,\u201d in International Workshop on Machine\nLearning for Signal Processing.\nIEEE, 2021.\n[8] S. Han and J. Lee, \u201cNUWave 2: A general neural au-\ndio upsampling model for various sampling rates,\u201d arXiv\npreprint:2206.08545, 2022.\n[9] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic,\nW. Wang, and M. D. Plumbley, \u201cAudioLDM: Text-to-\naudio generation with latent diffusion models,\u201d Interna-\ntional Conference on Machine Learning, 2023.\n[10] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Syn-\nnaeve, Y. Adi, and A. D\u00b4efossez, \u201cSimple and con-\ntrollable music generation,\u201d arXiv preprint:2306.05284,\n2023.\n[11] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and\nT. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end\ntext to speech,\u201d in International Conference on Learning\nRepresentations, 2021.\n[12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, \u201cHigh-resolution image synthesis with latent\ndiffusion models,\u201d in Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 10 684\u201310 695.\n[13] S. Lin, B. Liu, J. Li, and X. Yang, \u201cCommon diffu-\nsion noise schedules and sample steps are flawed,\u201d arXiv\npreprint:2305.08891, 2023.\n[14] T. Salimans and J. Ho, \u201cProgressive distillation for fast\nsampling of diffusion models,\u201d International Confer-\nence on Learning Representations, 2022.\n[15] H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong,\nY. Wang, W. Wang, Y. Wang, and M. D. Plumb-\nley, \u201cAudioLDM 2: Learning holistic audio genera-\ntion with self-supervised pretraining,\u201d arXiv preprint\narXiv:2308.05734, 2023.\n[16] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion\nimplicit models,\u201d in International Conference on Learn-\ning Representations, 2020.\n[17] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative ad-\nversarial networks for efficient and high fidelity speech\nsynthesis,\u201d Advances in Neural Information Processing\nSystems, vol. 33, pp. 17 022\u201317 033, 2020.\n[18] J. You, D. Kim, G. Nam, G. Hwang, and G. Chae,\n\u201cGAN Vocoder: Multi-resolution discriminator is all\nyou need,\u201d arXiv preprint:2103.05236, 2021.\n[19] Z. Rafii, A. Liutkus, F.-R. St\u00a8oter, S. I. Mimilakis, and\nR. Bittner, \u201cMUSDB18-HQ - an uncompressed version\nof MUSDB18,\u201d Aug 2019.\n[20] I. Pereira, F. Ara\u00b4ujo, F. Korzeniowski, and R. Vogl,\n\u201cMoisesDB: A dataset for source separation beyond 4-\nstems,\u201d arXiv preprint:2307.15913, 2023.\n[21] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J. P. Bello, \u201cMedleyDB: A multitrack\ndataset for annotation-intensive mir research.\u201d in ISMIR,\nvol. 14, 2014, pp. 155\u2013160.\n[22] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao,\nM. D. Plumbley, Y. Zou, and W. Wang, \u201cWavCaps:\nA ChatGPT-assisted weakly-labelled audio captioning\ndataset for audio-language multimodal research,\u201d arXiv\npreprint:2303.17395, 2023.\n[23] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti,\nA. Caillon,\nQ. Huang,\nA. Jansen,\nA. Roberts, M. Tagliasacchi et al., \u201cMusicLM: Generat-\ning music from text,\u201d arXiv preprint:2301.11325, 2023.\n[24] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudioCaps:\nGenerating captions for audios in the wild,\u201d in NAACL-\nHLT, 2019, pp. 119\u2013132.\n[25] K. Ito and L. Johnson, \u201cThe LJSpeech dataset,\u201d https:\n//keithito.com/LJ-Speech-Dataset/, 2017.\n[26] K. J. Piczak, \u201cESC: Dataset for environmental sound\nclassification,\u201d in International Conference on Multime-\ndia, 2015, pp. 1015\u20131018.\n"
  },
  {
    "title": "OmnimatteRF: Robust Omnimatte with 3D Background Modeling",
    "link": "https://arxiv.org/pdf/2309.07749.pdf",
    "upvote": "6",
    "text": "OmnimatteRF: Robust Omnimatte with 3D Background Modeling\nGeng Lin1\nChen Gao2\nJia-Bin Huang1,2\nChangil Kim2\nYipeng Wang2\nMatthias Zwicker1\nAyush Saraf2\n1University of Maryland, College Park\n2Meta\nhttps://omnimatte-rf.github.io\nAbstract\nVideo matting has broad applications, from adding in-\nteresting effects to casually captured movies to assisting\nvideo production professionals.\nMatting with associated\neffects such as shadows and reflections has also attracted\nincreasing research activity, and methods like Omnimatte\nhave been proposed to separate dynamic foreground ob-\njects of interest into their own layers. However, prior works\nrepresent video backgrounds as 2D image layers, limiting\ntheir capacity to express more complicated scenes, thus hin-\ndering application to real-world videos. In this paper, we\npropose a novel video matting method, OmnimatteRF, that\ncombines dynamic 2D foreground layers and a 3D back-\nground model. The 2D layers preserve the details of the sub-\njects, while the 3D background robustly reconstructs scenes\nin real-world videos. Extensive experiments demonstrate\nthat our method reconstructs scenes with better quality on\nvarious videos.\n1. Introduction\nVideo matting is the problem of separating a video into\nmultiple layers with associated alpha mattes such that the\nlayers are composited back to the original video. It has a\nwide variety of applications in video editing as it allows\nfor substituting layers or processing them individually be-\nfore compositing back, and thus has been studied well over\ndecades. In typical applications like rotoscoping in video\nproduction and background blurring in online meetings, the\ngoal is to obtain the masks containing only the object of\ninterest. In many cases, however, it is often preferred to\nbe able to create video mattes that include not only the ob-\nject of interest but also its associated effects, like shadow\nand reflections. This could reduce the often-required, ad-\nditional manual segmentation of secondary effects and help\nincrease realism in the resulting edited video. Being able\nto factor out the related effects of foreground objects also\nhelps reconstruct a clean background, which is preferred in\n(a) Omnimatte BG\n(b) Omnimatte FG\n(c) Our BG\n(d) Our FG\nFigure 1. Video with parallax effects. Limited by their 2D im-\nage representation (a), previous works such as Omnimatte fail to\nhandle videos with parallax effects in the background. Their fore-\nground layer (b) has to capture (dis)occlusion effects to minimize\nthe reconstruction loss. In contrast, our method employs a 3D\nbackground (c), enabling us to obtain clean foreground layers (d).\napplications like object removal. Despite these benefits, this\nproblem is much more ill-posed and has been much less ex-\nplored than the conventional matting problem.\nThe most promising attempt to tackle this problem is\nOmnimatte [21]. Omnimattes are RGBA layers that cap-\nture dynamic foreground objects and their associated ef-\nfects. Given a video and one or more coarse mask videos,\neach corresponding to a foreground object of interest, the\nmethod reconstructs an omnimatte for each object, in addi-\ntion to a static background that is free from all of the ob-\njects of interest and their associated effects. While Om-\nnimatte [21] works well for many videos, it is limited by\nits use of homography to model backgrounds, which re-\nquires the background be planar or the video contains only\nrotational motion. This is not the case as long as there ex-\nists parallax caused by camera motions and objects occlude\neach other. This limitation hinders its application in many\nreal-world videos, as shown in Fig. 1.\nD2NeRF [36] attempts to address this issue using two\narXiv:2309.07749v1  [cs.CV]  14 Sep 2023\nradiance fields, which model the dynamic and static part of\nthe scene. The method works entirely in 3D and can han-\ndle complicated scenes with significant camera motion. It\nis also self-supervised in the sense that no mask input is\nnecessary. However, it separates all moving objects from a\nstatic background and it is not clear how to incorporate 2D\nguidance defined on video such as rough masks. Further, it\ncannot independently model multiple foreground objects. A\nsimple solution of modeling each foreground object with a\nseparate radiance field could lead to excessive training time,\nyet it is not clear how motions could be separated meaning-\nfully in each radiance field.\nWe propose a method that has the benefit of both by com-\nbining 2D foreground layers with a 3D background model.\nThe lightweight 2D foreground layers can represent multi-\nple object layers, including complicated objects, motions,\nand effects that may be challenging to be modeled in 3D.\nAt the same time, modeling background in 3D enables han-\ndling background of complex geometry and non-rotational\ncamera motions, allowing for processing a broader set of\nvideos than 2D methods. We call this method Omnimat-\nteRF and show in experiments that it works robustly on\nvarious videos without per-video parameter tuning.\nTo\nquantitatively evaluate the background separation of a 3D\nscene, D2NeRF released a dataset of 5 videos rendered with\nKubrics, which are simple indoor scenes with few pieces of\nfurniture and some moving objects that cast solid shadows.\nWe also render five videos from open-source Blender\nmovies [6] with sophisticated motions and lighting con-\nditions for more realistic and challenging settings.\nOur\nmethod outperforms prior works in both datasets, and we\nrelease the videos to facilitate future research.\nIn summary, our contributions include the following:\n1. We propose a novel method to make Omnimatte [21]\nmore robust by better modeling the static background\nin 3D using radiance fields [22].\n2. Utilizing the omnimatte masks, we propose a simple\nyet effective re-training step to obtain a clean static 3D\nreconstruction from videos with moving subjects.\n3. We release a new dataset of 5 challenging video se-\nquences rendered from open-source blender movies\n[6] with ground truths to better facilitate the develop-\nment and evaluation of the video matting with associ-\nated effects (aka omnimatting [21]) problem.\n2. Related Work\nVideo Matting.\nThere is a long line of work exploring\nvideo matting due to its importance in video editing. Green\nscreening and rotoscoping are critical first steps in any vi-\nsual effects pipeline. The matting problem aims to extract\nthe foreground subjects into their own RGBA layers and\nseparate them from the background RGB layer, which is a\nhighly under-constrained problem. Many approaches have\nutilized motion and depth cues in addition to integrating\nuser interactions [7, 3, 32, 16, 9]. Background Video Mat-\nting [18] specifically addresses real-time video matting of\npeople and preserving strand-level hair details.\nMatting with Associated Effects.\nVideo matting is often\ninsufficient, as foreground subjects might have associated\neffects like shadows or reflections that need to be extracted\ninto the foreground RGBA layers. This problem has not\nbeen explored as extensively and, in practice, is often dealt\nwith manually using advanced interactive rotoscoping tools\n[15]. Omnimatte [21] was the first to propose a generic\nframework capable of learning any associated effect. Pre-\nvious works often specifically addressed associated effects\nlike shadows [34, 33]. The ability to obtain matte layers\nwith associated effects has many exciting applications, such\nas re-timing motions of different people [20], consistent\nbackground editing [13, 14], background subtraction, green\nscreening, and many other video effects [21].\nRecently,\nFactorMatte [12] has been proposed to improve the qual-\nity with data augmentation and conditional priors. These\nworks have in common that they take predefined masks that\nhint at the foreground objects and decompose each video\ninto several layers, with one object in each layer with its\nassociated effects. Then, there is a background layer, a 2D\nstatic image, or a deformable atlas shared by all the frames.\nThe background is warped and cropped via a homography\nto render each frame. While the foreground layers have\nshown great potential in capturing dynamics, their single\nimage background limits the application of these methods\nto videos with planar environments without parallax effects\ncaused by camera motion.\nRadiance Fields.\nRadiance fields (RF) emerged as 3D\nrepresentations capable of capturing geometric details and\nphotorealistic appearances [22]. Radiance fields model the\n3D scene as a continuous function that maps the position\nand the viewing direction of any point in world space to\nits color and opacity. Novel views can be synthesized via\nvolume rendering along rays cast. This continuous func-\ntion is learned by optimizing with a reconstruction loss\non the rendered images. This view-dependent volumetric\nrepresentation can model various challenging scenes that\nprevious surface-based methods struggled to handle: e.g.,\nshiny surfaces like metals or fuzzy surfaces like hair or\nfur. Since then, it has been extended along multiple axes:\nbetter appearance modeling (e.g., reflection and refraction\n[31, 5, 2, 1], faster optimization [8, 27, 23] and model-\ning dynamic scenes [38, 17, 10, 19].\nSince the MLP-\nbased implicit RF representations are slow to train, we use\nvoxel-based explicit radiance field representations [8] [27].\nSpecifically, we use the factorized voxel grid representation\nfrom [8].\nSelf-Supervised Video Dynamics Factoring.\nAnother\nrelated work is video dynamics factoring without needing\na predefined mask. One recent work is deformable sprites\n[39] that rely only on motion cues. Similar to other video\nmatting works, it has a 2D foreground and background lay-\ners and the same limitations as Omnimatte. For modeling\nin 3D, D2NeRF[36] proposes to decouple the scene with\ntwo radiance fields, one for the dynamic content and the\nother for the statics. D2NeRF[36] handles a special case of\nmatting with only one foreground object, and, compared to\nthe other methods, it is not limited to planar backgrounds.\nHowever, the self-supervised method relies on the heuristics\nthat require per-video hyper-parameter tuning and does not\nrobustly generalize to new videos. The quality of the fore-\nground reconstruction can also be limited for objects that\nhave large nonrigid motions.\nWe, therefore, propose a method for video matting with\nassociated effects that has the advantages of supervised 2D\nmattes that support multiple individual objects with great\ndetails, as well as 3D background decoupling that works\nwith non-planar videos.\n3. Method\nThe concept of omnimattes is proposed by Lu et al. [21],\nextending RGBA video mattes to capture associated effects\nof the objects of interest like shadows and reflections. To\navoid any confusion, in the following text, we refer to their\nwork as capital Omnimatte, and the resulting RGBA layers\nas italic omnimatte. In the matting setup, the user prepares\na video of T frames {It}T\nt=1, and N ordered mask layers\n{M i\nt}N\ni=1, each containing a coarse mask video of an object\nof interest. The video\u2019s camera parameters are also precom-\nputed as {Pt}.\nThe goal is to predict RGBA foreground layers Ci\nt and\n\u03b1i\nt that contain the objects together with their associated ef-\nfects, and a background layer Bt which is clean and free\nfrom the effects cast by the foreground objects. An input\nframe It should be reconstructed by alpha compositing the\nforeground layers above the background.\nIn Omnimatte, the background is represented by a static\n2D image and a homography transform Pt. To compose\na frame, part of the static background is extracted accord-\ning to the estimated homography Pt. The key idea of our\nwork is to represent the static background in 3D using a\nradiance field, while keeping the foreground in 2D to bet-\nter capture the dynamics of objects. We employ an explicit\nfactorized voxel-based radiance field [8] to model the back-\nground. In this case, Pt represents a camera pose, and a\nbackground frame is rendered with volume rendering. Note\nthat the foreground layers are still 2D videos. We refer to\nthis combination as the OmnimatteRF model.\n3.1. The OmnimatteRF Model\nAn outline of our model is depicted in Figure 2. The\nmodel has two independent branches: foreground and back-\nground. For any given frame, the foreground branch pre-\ndicts an RGBA image (omnimatte) for each object, and the\nbackground branch renders a single RGB image.\nPreprocessing. Following similar works, we use an off-\nthe-shelf model RAFT [29] to predict optical flow between\nneighboring frames. The flow is used as an auxiliary input\nand ground truth for supervision, denoted by {Ft}. We also\nuse an off-the-shelf depth estimator MiDaS [26] to predict\nmonocular depth maps {Dt} for each frame and use them\nas ground truth for the monocular depth loss.\nBackground. The background branch consists of a static\nneural radiance field, fbg, encoding the 3D representation\nof the scene. To render a pixel in a frame It, a ray is traced\naccording to the estimated camera pose Pt, and the final\nRGB color is produced via volumetric rendering. The result\nof rendering the entire frame is (Bt, \u02c6Dt) = fbg(Pt), where\nBt is an RGB image and \u02c6Dt is a depth map.\nForeground.\nThe foreground branch is a UNet-style\nconvolutional neural network, ffg, similar to that of Omn-\nimatte. The input of the network is a concatenation of three\nmaps:\n1. The coarse mask M i\nt. The mask is provided by the\nuser, outlining the object of interest. Mask values are\nones if the pixels are inside the object.\n2. The optical flow Ft. It provides the network with mo-\ntion hints. Note that the network also predicts an opti-\ncal flow as an auxiliary task (detailed in Sec. 3.2.2).\n3. The feature map Et. Each pixel (x, y) in the feature\nmap is the positional encoding of the 3-tuple (x, y, t).\nMultiple foreground layers are processed individually.\nFor the i-th layer, the network predicts the omnimatte layer\n(Ci\nt, \u03b1i\nt) and the flow \u02c6F i\nt .\nDetail Transfer. For a tradeoff between image quality\nand training time, the foreground network typically pro-\nduces a color layer with missing details when the alpha\nlayers have captured sufficient associated effects. To boost\nthe output quality, Omnimatte transfers details from input\nframes. We include the same process in our pipeline. Note\nthat this is a post-processing step to produce final results,\nand does not apply to model optimization.\n3.2. Optimizing the Model\nWe optimize an OmnimatteRF model for every video\nsince both branches of our model are video-specific. To\nsupervise learning, we employ an image reconstruction loss\nand several regularization losses.\nM i\nt\nFt\nEt\n\u2295\nPt\nffg\nfbg\nCi\nt\n\u03b1i\nt\nBt\n\u02c6Dt\n\u02c6It\nIt\n\u02c6F i\nt\n\u02c6Ft\nAlpha\nComposite\nLrecons\nDt\nLdepth\nL\ufb02ow\nF bg\nt\nVolumetric\nRender\nFigure 2. Method overview. We propose a video matting method, named OmnimatteRF, which combines 2D foreground layers with a 3D\nbackground layer. The foreground branch (ffg, in green box) predicts an RGBA layer (Ci\nt, \u03b1i\nt) for each object, and an auxiliary flow output\n( \u02c6F i\nt ). The background branch (fbg, in yellow box) produces a background layer with depths (Bt, \u02c6Dt). Optimization. During training,\npredicted colors (\u02c6It) and flow ( \u02c6Ft) are alpha-composited, whose inputs have red and green borders respectively. The right most column\nillustrates the data terms in the loss function, and we omit the regularization terms in this illustration.\n3.2.1\nReconstruction Loss\nWe compute the reconstruction loss with the composed im-\nage \u02c6It by alpha composition of foreground and background\nlayers:\n\u02c6It =\nN\nX\ni=1\n\uf8eb\n\uf8ed\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j\nt)\u03b1i\ntCi\nt\n\uf8f6\n\uf8f8 +\nN\nY\ni=1\n(1 \u2212 \u03b1i\nt)Bt\n(1)\nAnd the reconstruction loss is the mean-squared-error\nbetween the predicted and input frame,\nLrecons = ||\u02c6It \u2212 It||2\n(2)\nThe reconstruction loss supervises both branches of our\npipeline simultaneously. Limited by the computational cost\nof volumetric rendering, the background layer is rendered\nonly at sparse random locations at each step, where Lrecons\nis computed for the composed pixel values.\n3.2.2\nForeground Losses\nWe follow Omnimatte and include the alpha regularization\nloss L\u03b1-reg, alpha warp loss L\u03b1-warp, and flow reconstruction\nloss Lflow. We also bootstrap the initial alpha prediction to\nmatch the input mask with the mask loss Lmask, which is\ngradually decayed and disabled once its value drops below\nthe threshold.\nWhile most regularization terms in Omnimatte can be\napplied directly to our pipeline, the flow reconstruction loss\nis an exception. The formulation of the loss remains iden-\ntical: given the per-layer flow prediction \u02c6F i\nt and a back-\nground layer flows F bg\nt , the complete flow \u02c6Ft is composed\nvia alpha composition (Eq. 1). Then, the loss is defined as:\nLflow = ||( \u02c6Ft \u2212 Ft) \u2297 M fg\nt ||2\n(3)\nHere, M fg\nt is the union of all foreground masks ({M i\nt})\nfor the frame It, and the loss is only evaluated at the loca-\ntion of input coarse masks. The authors of Omnimatte have\nshown the effectiveness of this loss in their case, and we\nalso demonstrate its importance in an ablation study.\nHowever, it remains unclear how F bg\nt\ncan be obtained.\nIn Omnimatte, the background flow can be derived from\nimage homography, which serves both as an input to the\nnetwork and a background for composition. On the other\nhand, since our 3D background has only known camera\nposes but not depths, we cannot obtain background flows\ndirectly. Instead, we use the ground truth flow Ft as net-\nwork input to provide motion cues and a masked version of\nFt as background flow for composition. The masked flow\nis F m\nt\n= Ft \u2297 (1 \u2212 M fg\nt ), which is the ground truth op-\ntical flow with the regions marked in the coarse masks set\nto zeros. \u2297 denotes elementwise multiplication. We find\nit crucial to use F m\nt rather than Ft for composition, as the\nlatter case encourages the network to produce empty layers\nwith \u03b1i\nt equal to zero everywhere.\n(a)\n(b)\n(c)\nFigure 3. Background Layer Training Signals. We illustrate how\nthe training signal to the background layer changes over time. It\nexplains why the background captures some of the associated ef-\nfects (in this example, shadows). We use the pixel circled in red\nas an example. (a) At the beginning of training, the foreground\nalpha value (in light green) does not include the shadow. There-\nfore, \u03b1 is small and at this pixel, \u02c6It(x, y) \u2248 Bt(x, y). The re-\nconstruction loss Lrecons encourages the background network fbg\nto produce dark prediction at this location from this viewing an-\ngle. (b) As training progresses, \u03b1 gets larger in the shadow region,\nand \u02c6It(x, y) \u2248 Ci\nt(x, y). This means that fbg receives little to no\nsupervision signals from this pixel. If it has modeled the shadow\nin some ways (in this case, a hole), it has little incentive to remove\nit, leaving the artifact in (c).\n3.2.3\nBackground Losses\nApart from the reconstruction loss, the background net-\nwork is supervised by the total variation regularization loss,\nLbg-reg, as in TensoRF [8]. In addition, monocular depth su-\npervision is used to improve scene reconstruction when the\ncamera motions consist of rotation only:\nLdepth = metric(Dt, \u02c6Dt),\n(4)\nwhere \u02c6Dt is the estimated depth from volume rendering\n[22], and the metric function is the scale-invariant loss from\nMiDaS [26]. Also, we empirically find that Ldepth can in-\ntroduce floaters, and employ the distortion loss Ldistort pro-\nposed in Mip-NeRF 360 [4] to reduce artifacts in the back-\nground.\n3.2.4\nSummary\nThe combined loss for joint optimization is:\nL =Lrecons + L\u03b1-reg + L\u03b1-warp + Lflow + Lmask\n|\n{z\n}\nForeground\n+\nLbg-reg + Ldepth + Ldistort\n|\n{z\n}\nBackground\n(5)\nAt every optimization step, Lrecons and background\nlosses are evaluated at sparse random locations. Foreground\nlosses are computed for the full image.\n3.3. Clean Background via Masked Retraining\nWhen the pipeline is trained jointly as described above,\nit is sometimes observed that the background radiance field\nmodels some of the foreground contents like shadows (see\nFig. 3(c)). Compared to 2D images, 3D radiance fields\nare so much more capable that they can exploit distorted\ngeometry constructs, such as holes and floaters, to capture\nsome temporal effects, although the models are given no\ntime information. For example, as the camera moves over\ntime, there may be a correlation between whether a surface\nis covered by shadow and the direction the surface is viewed\nfrom.\nWe illustrate this problem in Fig. 3 and explain the cause\nat an intuitive level. The foreground branch is bootstrapped\nto produce alpha values that match the coarse mask inputs,\nwhich include only the object without the associated effects.\nIn other words, \u03b1t values are close to one at the object, but\nzero in the shadows (for simplicity, we consider one fore-\nground layer in which the object casts a shadow, like in\nFig. 3). At a pixel (x, y) covered by shadow, Eq. 1 sim-\nply collapses to \u02c6It(x, y) \u2248 Bt(x, y). The reconstruction\nloss will therefore encourage Bt(x, y) to match the color of\nthe shadow for a ray shot toward this location.\nAs training proceeds, ffg will then gradually increase\nthe predicted alpha values at the shadowed regions. If the\nshadow is hard and \u03b1 gets close to one, Eq. 1 evaluates to\n\u02c6It(x, y) \u2248 Ci\nt(x, y), and the reconstruction loss gives little\nto no constraint to the background color at the pixel. As a\nresult, fbg is unable to learn to remove the shadow color that\nit produces for the ray towards frame It at (x, y).\nThere are also cases where the shadow is soft and \u03b1 is in\nbetween. In these cases, the problem remains ambiguous.\nTherefore, we propose to obtain clean background re-\nconstruction via an optional optimization step.\nIn joint\ntraining, the foreground omnimatte layers can capture most\nassociated effects, including the parts with leaked content\nin the background layer. The alpha layers \u03b1t can then be\nused to train a radiance field model from scratch, with no\nsamples from the foreground region where alpha values are\nhigh. We show in the ablation study (see Fig. 7) that this\nstep produces cleaner background reconstruction for in-the-\nwild videos. As only the background is optimized, the pro-\ncess is fast and takes less than an hour to complete.\n4. Evaluation\nWe compare our quantitative and qualitative methods\nwith Omnimatte and D2NeRF [21, 36], which are state-\nof-the-art methods in 2D video matting and 3D video seg-\nmentation, respectively. In addition, we compare with Lay-\nered Neural Atlas (LNA) [13], which uses a deformable 2D\nbackground in contrast to Omnimatte\u2019s static image.\n4.1. The Movies Dataset\nQuantitative evaluation of background segmentation re-\nquires a dataset with both input videos and ground-truth\nbackground imagery.\nPrior works primarily use datasets\nlike CDW-2014 [35], which are limited to mostly static\nbackgrounds and are not applicable to our settings. Re-\ncently, Kubrics is proposed in D2NeRF, which enables\nthe evaluation of 3D background synthesis. However, these\nvideos have relatively simple scenes and lighting. To facil-\nitate the evaluation of video matting and background seg-\nmentation in challenging scenarios, we select six clips from\nthree Blender movies in Blender Studio [6]. Compared to\nKubrics, they feature more complicated scenes and light-\ning conditions, large nonrigid motion of the characters, and\nhigher resolution. To ensure usability, we manually edit\nthe camera trajectories so that there are sufficient camera\nmotions and the actors have reasonable sizes. We render\nthe clips with and without the actors to obtain input and\nground truth for background reconstruction evaluation pur-\nposes. The camera poses are also exported.\n4.2. Experiment Setup\nWe evaluate the performance of our proposed method on\nfour datasets.\n1. Movies: our novel challenging dataset.\n2. Kubrics: the dataset generated and used in D2NeRF,\nwhich consists of five scenes of moving objects from\n3D Warehouse [30] rendered with Kubric [11].\n3. DAVIS [24, 25]: short clips with moving foreground\nsubjects, like humans, cars, and animals. This dataset\nis widely used to evaluate 2D-background matting\nmethods [21, 13, 39].\n4. Wild: in-the-wild sequences collected from the inter-\nnet that are closer to casually captured videos, with\nnatural and noisier camera motions, including trans-\nlations and rotations, as well as objects at different dis-\ntances from the camera. Naturally, these videos have\nbackgrounds that are challenging for pure 2D methods.\nKubrics and Movies are synthetic datasets with clean\nbackground layer renderings available.\nNote that novel\nview synthesis is not the focus of our method, so we eval-\nuate the background with input views. Both datasets have\nknown camera poses and object masks which are used for\ntraining and evaluation.\nDAVIS and Wild are real-world videos without clean\nbackground. Therefore, we only perform a qualitative eval-\nuation to demonstrate the robustness of our method. For\nvideos in Wild we recover camera poses with COLMAP.\nFor videos that COLMAP cannot process reliably, including\nDAVIS videos, we use poses from RoDynRF [19].\nD2NeRF\nOmnimatte\nLNA\nOurs\nku-bag\nku-chair\ndog\ndodge\nFigure 4. Background Reconstruction. We show examples of\nresults presented in quantitative evaluations. For videos with par-\nallax effects, 3D methods like D2NeRF and ours reconstruct less\ndistorted background than Omnimatte and LNA.\nTo obtain coarse object masks, we attempt to extract\nthem with pre-trained object segmentation models from De-\ntectron 2 [37]. In case it does not work, we use the Roto\nBrush tool in Adobe After Effects. Detailed procedures are\ndescribed in the supplementary material. It takes about 10\nminutes of manual effort to produce a 200-frame mask.\nFor all videos, we also estimate homographies with\nLoFTR [28] and OpenCV to enable Omnimatte processing.\nAs mentioned in D2NeRF [36], the method is sensitive\nto hyperparameters. The authors released five sets of con-\nfigurations for different videos. We experiment with every\nvideo using all provided configurations and report the best-\nperforming ones.\n4.3. Implementation Details\nOur network is built upon the publicly available offi-\ncial implementation of Omnimatte [21], and TensoRF [8].\nThe videos in Kubrics have resolution 512 \u00d7 512, and\nall methods run at the resolution 256 \u00d7 256. For videos in\nother datasets with a higher resolution of 1920 \u00d7 1080, we\ndownsample them by a factor of 4.\nWe optimize the networks for up to 15,000 steps. The\nlearning rate of ffg is set to 0.001 and is exponentially de-\ncayed after 10,000 steps. For fbg we use the learning rate\nscheduling scheme of TensoRF. Training takes up to 6 hours\non a single RTX3090 GPU. Detailed network architecture,\nhyper-parameters and timing data are presented in the sup-\nplementary. Our code and datasets will also be made pub-\nlicly available.\n4.4. Quantitative Evaluation\nWe quantitatively evaluate the background reconstruc-\ntion quality of our method on two synthetic datasets. We\nKubrics\nCar\nCars\nBag\nChair\nPillow\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nD2NeRF\n0.135\n0.854\n34.10\n0.105\n0.859\n34.77\n0.131\n0.880\n33.98\n0.090\n0.916\n33.29\n0.105\n0.926\n38.80\nOmnimatte\n0.162\n0.819\n31.14\n0.157\n0.834\n31.20\n0.271\n0.796\n23.64\n0.175\n0.865\n26.91\n0.270\n0.841\n21.17\nLNA\n-\n-\n-\n-\n-\n-\n0.138\n0.835\n27.08\n0.105\n0.881\n21.21\n0.080\n0.923\n31.66\nOurs\n0.033\n0.958\n39.09\n0.032\n0.961\n39.78\n0.029\n0.972\n39.58\n0.023\n0.977\n42.46\n0.022\n0.982\n43.62\nMovies\nDonkey\nDog\nChicken\nRooster\nDodge\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nLPIPS\u2193\nSSIM\u2191\nPSNR\u2191\nD2NeRF\n-\n-\n-\n0.370\n0.694\n22.73\n-\n-\n-\n0.340\n0.708\n25.13\n0.408\n0.729\n20.95\nOmnimatte\n0.315\n0.653\n19.11\n0.279\n0.706\n21.74\n0.312\n0.704\n20.95\n0.220\n0.741\n23.14\n0.067\n0.879\n23.88\nLNA\n0.104\n0.849\n18.79\n0.154\n0.828\n26.08\n0.190\n0.818\n19.22\n0.131\n0.804\n26.46\n0.068\n0.937\n24.94\nOurs\n0.005\n0.990\n38.24\n0.030\n0.976\n31.44\n0.021\n0.978\n32.86\n0.024\n0.969\n27.65\n0.006\n0.991\n39.11\nTable 1. Quantitative evaluations. We present the background reconstruction comparison of our method and baselines on the Kubrics\nand Movies datasets. Best results are in bold and second place are underlined. Results marked - are the ones the method failed to give\ngood separations (visuals in supplementary).\nreport PSNR, SSIM and LPIPS for all videos in Table 1, and\nsome visualizations in Fig. 4. For D2NeRF, we tried every\nprovided pre-set configuration for every video in Movies,\nand it only gave good results for the Dog, Rooster, and\nDodge videos.\nOmnimatte and LNA with the 2D back-\nground layers struggles in both datasets. Our method can\nhandle these videos well.\n4.5. Qualitative Evaluation\nWe present a qualitative comparison of the methods in\nFig. 5. Due to space limitations, we present at least one\nvideo from every dataset but show a frame from every se-\nlected video in the figure. The original videos are avail-\nable in supplementary and we highly recommend watching\nthem. D2NeRF works well for the fine-tuned videos but not\nfor new inputs without further hyper-parameter tuning. Om-\nnimatte background has significant distortion around ob-\njects, and its foreground layer has to compensate for the\nlimitation by capturing all residuals. Our method is versa-\ntile enough to perform well for a variety of videos with our\n3D background model.\n4.6. Ablation Studies\n4.6.1\nLoss Terms\nWe present background reconstruction results without\nLdepth in Fig. 6. For video sequences with rotational camera\nposes, the model struggles to extract 3D information from\nthe input videos because of a lack of 3D clues. This loss\nis critical to extending our method to a broader range of\nvideos. The effects of Lflow are also demonstrated in Fig. 6.\nThe auxiliary task improves foreground quality and reduces\nunrelated content.\n4.6.2\nClean Background Retraining\nWe employ an additional step for real-world sequences to\noptimize a clean background from scratch. In Fig. 7, we\ncompare the background layer from the initial joint opti-\nmization and the final result. This is a simple yet robust\nway to obtain a better background.\n4.7. Limitations\nWe list some limitations that future works can explore.\n1. If a background region is covered by shadows nearly\nall of the time, the background model cannot recover\nits color correctly. An example from a Movies video\nis shown in Fig.\n8.\nIn theory, an omnimatte layer\nhas an alpha channel and can capture only the addi-\ntive shadow that allows the background to have the\noriginal color. However, this problem is largely under-\nconstrained in the current setting, making it ambiguous\nand leading the background to unsatisfying solutions.\n2. The foreground layer captures irrelevant content. In\nreal-world videos, unrelated motions often exist in\nthe background, like swaying trees and moving cars.\nThese effects cannot be modeled by the static radiance\nfield and will be captured by the foreground layer re-\ngardless of their association with the object. Possible\ndirections include i) using a dummy 2D layer to catch\nsuch content or ii) a deformable 3D background model\nwith additional regularization to address the ambiguity\nas both background and foreground can model motion.\n3. Foreground objects may have missing parts in the omn-\nimatte layers if they\u2019re occluded. Since our foreground\nnetwork predicts pixel values for alpha composition, it\ndoes not always hallucinate the occluded parts.\n4. The video resolution is limited. This is primarily due\nto the U-Net architecture of the foreground model in-\nherited from Omnimatte. Higher resolutions can po-\ntentially be supported with the use of other lightweight\nimage encoders.\n5. The foreground layer may capture different content\nwhen the weights are randomly initialized differently.\nWe include visual results in the supplementary materi-\nals.\n5. Conclusion\nWe propose a method to obtain omnimattes, RGBA lay-\ners that include objects and their associated effects by com-\nBackground\nForeground\nInput\nD2NeRF\nOmnimatte\nOurs\nD2NeRF\nOmnimatte\nOurs\nku-pillow\nrooster\nbouldering car\ndog\nFigure 5. Qualitative comparison. We compare results of our and baseline methods on videos from each dataset. Readers are strongly\nencouraged to view videos files of more sequences available in the supplementary. The first two videos are synthetic from Kubrics and\nMovies, followed by three Wild videos. Omnimatte fails to handle objects in 3D and produces distorted background. D2NeRF works\nfor videos with appropriate hyper-parameters, but does not generalize to new videos easily. Our method handles videos in many different\nsettings. Due to space constraint we defer LNA results to the supplementary.\nInput\nwithout Ldepth\nwith Ldepth\nInput\nwithout Lflow\nwith Lflow\nFigure 6. Loss Term Ablations. Background of real-world videos\nwithout Ldepth and foreground without Lflow can de degraded for\nreal-world videos.\nbining 2D foreground layers and a 3D background model.\nExtensive experiments demonstrate that our approach is ap-\n(a) Coarse mask\n(b) Background\n(c) Depths\n(d) Learned mask\n(e) Background\n(f) Depths\nFigure 7. Clean Background Retraining.\nBackground layers\njointly trained can capture the shadows as a hole on the ground\n(a-c). After the joint training, the foreground omnimatte provides\na better mask that can be used to train a clean background (d-f).\nplicable to a wide variety of videos, expanding beyond the\ncapabilities of previous methods.\n(a)\n(b)\n(c)\nFigure 8. Limitations. (a), (b): Background can have baked-in\nshadows when the region is covered by shadows for most frames\nof the video. (c): The foreground layer captures irrelevant object\nmotions (middle left) in the background. Best viewed in videos.\nReferences\n[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2023. 2\n[2] Benjamin Attal, Jia-Bin Huang, Michael Zollh\u00a8ofer, Johannes\nKopf, and Changil Kim. Learning neural light fields with\nray-space embedding. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022.\n2\n[3] Xue Bai, Jue Wang, David Simons, and Guillermo Sapiro.\nVideo snapcut: robust video object cutout using localized\nclassifiers. ACM Trans. Graph., 28(3):70, 2009. 2\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 5\n[5] Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad,\nHans-Peter Seidel, and Tobias Ritschel. Eikonal fields for\nrefractive novel-view synthesis. In Munkhtsetseg Nandigjav,\nNiloy J. Mitra, and Aaron Hertzmann, editors, SIGGRAPH\n\u201922: Special Interest Group on Computer Graphics and In-\nteractive Techniques Conference, Vancouver, BC, Canada,\nAugust 7 - 11, 2022, pages 39:1\u201339:9. ACM, 2022. 2\n[6] Institute Blender, Oct 2022. 2, 6\n[7] Gabriel J. Brostow and Irfan A. Essa.\nMotion based de-\ncompositing of video. In Proceedings of the International\nConference on Computer Vision, Kerkyra, Corfu, Greece,\nSeptember 20-25, 1999, pages 8\u201313. IEEE Computer Soci-\nety, 1999. 2\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 2, 3, 5, 6, 11\n[9] Yung-Yu Chuang, Aseem Agarwala, Brian Curless, David\nSalesin, and Richard Szeliski.\nVideo matting of complex\nscenes. ACM Trans. Graph., 21(3):243\u2013248, 2002. 2\n[10] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021. 2\n[11] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-\nable dataset generator. 2022. 6\n[12] Zeqi Gu, Wenqi Xian, Noah Snavely, and Abe Davis. Fac-\ntormatte: Redefining video matting for re-composition tasks,\n2022. 2\n[13] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 2, 5, 6, 11\n[14] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Eliza-\nbeth Qiu, and Jia-Bin Huang. Shape-aware text-driven lay-\nered video editing. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2023.\n2\n[15] Wenbin Li, Fabio Viola, Jonathan Starck, Gabriel J. Bros-\ntow, and Neill D.F. Campbell. Roto++: Accelerating pro-\nfessional rotoscoping using shape manifolds. ACM Transac-\ntions on Graphics (In proceeding of ACM SIGGRAPH\u201916),\n35(4), 2016. 2\n[16] Yin Li, Jian Sun, and Heung-Yeung Shum. Video object cut\nand paste. ACM Trans. Graph., 24(3):595\u2013600, 2005. 2\n[17] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of\ndynamic scenes.\nIn IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2021, virtual, June 19-\n25, 2021, pages 6498\u20136508. Computer Vision Foundation /\nIEEE, 2021. 2\n[18] Shanchuan\nLin,\nAndrey\nRyabtsev,\nSoumyadip\nSen-\ngupta, Brian Curless, Steve Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\narXiv, pages arXiv\u20132012, 2020. 2\n[19] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\nhannes Kopf, and Jia-Bin Huang. Robust dynamic radiance\nfields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023. 2, 6\n[20] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew\nZisserman, David Salesin, William T Freeman, and Michael\nRubinstein. Layered neural rendering for retiming people in\nvideo. In SIGGRAPH Asia, 2020. 2\n[21] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman,\nWilliam T Freeman, and Michael Rubinstein. Omnimatte:\nAssociating objects and their effects in video.\nIn CVPR,\n2021. 1, 2, 3, 5, 6, 11\n[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 5, 11\n[23] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2\n[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016. 6\n[25] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 6\n[26] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(3), 2022. 3, 5\n[27] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 2\n[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-free local feature matching\nwith transformers. CVPR, 2021. 6\n[29] Zachary Teed and Jia Deng.\nRAFT: recurrent all-pairs\nfield transforms for optical flow. In Andrea Vedaldi, Horst\nBischof, Thomas Brox, and Jan-Michael Frahm, editors,\nComputer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part II, vol-\nume 12347 of Lecture Notes in Computer Science, pages\n402\u2013419. Springer, 2020. 3\n[30] Inc Trimble, Oct 2022. 6\n[31] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd E. Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan. Ref-nerf: Struc-\ntured view-dependent appearance for neural radiance fields.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-\n24, 2022, pages 5481\u20135490. IEEE, 2022. 2\n[32] Jue Wang, Pravin Bhat, Alex Colburn, Maneesh Agrawala,\nand Michael F. Cohen. Interactive video cutout. ACM Trans.\nGraph., 24(3):585\u2013594, 2005. 2\n[33] Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann\nHeng. Single-stage instance shadow detection with bidirec-\ntional relation learning.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 1\u201311, June 2021. 2\n[34] Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng,\nand Chi-Wing Fu. Instance shadow detection. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2020. 2\n[35] Yi Wang, Pierre-Marc Jodoin, Fatih Porikli, Janusz Kon-\nrad, Yannick Benezeth, and Prakash Ishwar. Cdnet 2014:\nAn expanded change detection benchmark dataset. In 2014\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops, pages 393\u2013400, 2014. 6\n[36] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-\nrester Cole, and Cengiz Oztireli. D2nerf: Self-supervised\ndecoupling of dynamic and static objects from a monocular\nvideo. In Advances in Neural Information Processing Sys-\ntems, 2022. 1, 3, 5, 6, 11\n[37] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 6, 11\n[38] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021, pages\n9421\u20139431. Computer Vision Foundation / IEEE, 2021. 2\n[39] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa,\nand Noah Snavely.\nDeformable sprites for unsupervised\nvideo decomposition. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), June 2022. 3, 6\nA. Additional Qualitative Results\nVideo files of results presented in the main paper (all\nvideos from the Movies, Kubrics, Wild, and DAVIS\ndatasets) are available on our project website as part of the\nsupplementary material. We highly recommend watching\nthem on: https://omnimatte-rf.github.io\n1. For our method (OmnimatteRF), we include results\n(inputs with masks, foreground layers, background\nlayer, background depth map) for every video.\n2. For D2NeRF [36], we use the best result among all\nconfigurations provided by the authors for every video.\nIf none of the configurations successfully reconstruct\nnon-empty static and dynamic layers, we drop the\nvideo files and only show a frame in Fig. A2.\n3. For Omnimatte [21] and Layered Neural Atlas (LNA)\n[13], we include videos from Wild, Movies, and\nKubrics. Results of DAVIS can be found in prior\nworks.\nB. Random Initialization\nAs is also discussed in Omnimatte [21], different random\ninitializations can lead to varying results of the foreground\nlayers. We show two examples in Fig. A1.\nIn all our experiments, the random seed is set to 3.\nSeed = 5\nSeed = 1\nSeed = 0\nSeed = 3\nSeed = 1\nSeed = 0\nSeed = 3\nSeed = 1\nSeed = 0\nFigure A1. Effect of random initialization.\nTop:\nfor the\nWild/dogwalk video, different seeds lead to different amount\nof hallucinated shadow of the person.\nBottom:\nfor the\nKubrics/cars video, seeds influence how shadows are asso-\nciated to the objects.\nC. Additional Implementation Details\nC.1. Mask Generation\nOur method and Omnimatte relies on coarse mask\nvideos that outlines every object of interest. The synthetic\nKubrics and Movies videos have ground truth object\nmasks and we use them directly. To obtan mask for an in-\nthe-wild video, we use one of the two workflows:\n1. We first process the video a the pretrained Mask R-\nCNN model (X101-FPN) from Detectron 2 [37].\nThen, we manually select a mask in every frame that\nbest capture the object.\n2. We use the Roto Brush tool in Adobe After Effects to\ntrack the object. This method is useful when Mask\nR-CNN fails produce good masks for a video. In par-\nticular, we processed Wild/dance and Wild/solo\nmanually.\nIt takes about 10 minutes of manual work to generate a\nmask sequence for a 200-frame video.\nC.2. Network Architecture\nOur foreground network is based on the U-Net architec-\nture of Omnimatte, which is detailed in their supplemen-\ntary [21]. To adopt their network to OmnimatteRF, we re-\nplace the background noise input by the 2D feature map Et.\nEach pixel in Et is the positional encoding of the 3D vec-\ntor (x, y, t) where (x, y) is the pixel location and t is the\nframe number. The positional encoding scheme is the same\nas proposed in NeRF [22], with L = 10 frequencies.\nFor background, we use the Vector-Matrix decomposi-\ntion model in TensoRF [8] with the MLP feature decoder.\nOur initial grid has the same resolution N0 = 128, and the\nfinal grid is limited to N = 640. The vectors and matrices\nare upsampled at steps 2000, 3000, 4000, 5500.\nC.3. Hyper-parameters\nFor all videos, we use a learning rate of 0.001 for the\nforeground network, which is exponentially decayed from\nthe 10,000 step at a rate of 0.1\u00d7 per 10,000 steps. We find\nthe decay crucial in preventing the foreground training from\ndiverging. The mask bootstrapping loss Lmask has an initial\nweight of 50, which is first reduced to 5 when the loss value\n(before weighting) drops to below 0.02, and then turned off\nafter the same number of steps. We document weights of\nother loss terms in Table A1.\nBackground network learning rate scheduling and Lbg-reg\nweight are identical as the original TensoRF [8].\nIn general, we use the same set of hyper-parameters for\nmost videos, and only add additional terms when artifacts\nare observed.\nVideo\nSteps\nLrecons\nL\u03b1-reg\nL\u03b1-warp\nLflow\nLdepth\nLdistort\nAll\n15,000\n1\n0.01 (L1) / 0.005 (L0)\n0.01\n1\n0\n0\nWild/bouldering\n-\n-\n-\n-\n-\n0.1\n0.01\nDAVIS\n10,000\n-\n-\n-\n-\n1\n0\nTable A1. Hyper-parameters. We document the hyper-parameters (number of steps and weights of loss terms) in our experiments. The\nfirst row is the configuration shared by most videos. Remaining rows are videos with different configurations, and - means unchanged from\nthe shared number.\nFG\nBG\nFG\nBG\nFG\nBG\nbouldering\ncar\nsolo\nwalk\ndog\nchicken\ndonkey\nFigure A2. D2NeRF results for failed scenes.\nMethod\nSteps\nTraining (hours)\nRendering (s/image)\nOmnimatte\n12,000\n2.7\n2.5\nD2NeRF\n100,000\n4.5\n4.8\nLNA\n400,000\n8.5\n0.40\nOurs\n15,000\n3.8\n3.5\nOmnimatte\n12,000\n1.2\n0.95\nD2NeRF\n100,000\n3.2\n2.2\nLNA\n400,000\n8.5\n0.21\nOurs\n15,000\n2.5\n1.2\nTable A2. Running Time Measurement. We measure and com-\npare the time it takes to train OmnimatteRF and baseline methods.\nTop: Movies, Wild (480\u00d7270, DAVIS has a similar resolution\nof 428 \u00d7 240). Bottom: Kubrics (256 \u00d7 256).\nC.4. Running Time Measurement\nWe measure and report the time it takes to train Om-\nnimatteRF and baseline methods in Table A2. All mea-\nsurements are conducted on a workstation with an eight-\ncore AMD R7-2700X CPU and a single NVIDIA RTX3090\nGPU.\nOur method takes longer to train than Omnimatte due to\nthe addition of the 3D background radiance field.\nOptimizing the background model only, as in the clean\nbackground retraining process, takes about 30 minutes per\nvideo.\n"
  },
  {
    "title": "Ambiguity-Aware In-Context Learning with Large Language Models",
    "link": "https://arxiv.org/pdf/2309.07900.pdf",
    "upvote": "3",
    "text": "Ambiguity-Aware In-Context Learning with Large Language Models\nLingyu Gao\u2217\nToyota Technological Institute at Chicago\nlygao@ttic.edu\nAditi Chaudhary\nGoogle Research\naditichaud@google.com\nKrishna Srinivasan\nGoogle Research\nkrishnaps@google.com\nKazuma Hashimoto\nGoogle Research\nkazumah@google.com\nKarthik Raman\nGoogle Research\nkarthikraman@google.com\nMichael Bendersky\nGoogle Research\nbemike@google.com\nAbstract\nIn-context learning (ICL), i.e., showing large\nlanguage models (LLMs) only a few task-\nspecific demonstrations, has led to downstream\ngains without task-specific fine-tuning. How-\never, LLMs are sensitive to the choice of\nprompts, and therefore a crucial research ques-\ntion is how to select good demonstrations for\nICL. One effective strategy is leveraging se-\nmantic similarity between the ICL demonstra-\ntions and test inputs by using a text retriever,\nwhich however is sub-optimal as that does\nnot consider the LLM\u2019s existing knowledge\nabout that task. From prior work (Lyu et al.,\n2023), we already know that labels paired with\nthe demonstrations bias the model predictions.\nThis leads us to our hypothesis whether con-\nsidering LLM\u2019s existing knowledge about the\ntask, especially with respect to the output la-\nbel space can help in a better demonstration\nselection strategy. Through extensive experi-\nmentation on three text classification tasks, we\nfind that it is beneficial to not only choose se-\nmantically similar ICL demonstrations but also\nto choose those demonstrations that help re-\nsolve the inherent label ambiguity surrounding\nthe test example. Interestingly, we find that\nincluding demonstrations that the LLM previ-\nously mis-classified and also fall on the test\nexample\u2019s decision boundary, brings the most\nperformance gain.\n1\nIntroduction\nLeveraging LLMs (Brown et al., 2020; Chowdhery\net al., 2022; Thoppilan et al., 2022) via in-context\nlearning (ICL) is now a popular strategy for im-\nproving downstream task performance, wherein the\nmodel is able to perform a task by simply being\nconditioned on the task definition and/or few task\ndemonstrations (input-output examples) (Brown\net al., 2020; Xie et al., 2021).\nAs ICL gets increasingly adopted, it has brought\nto light (Lester et al., 2021; Liu et al., 2022; Zhang\n\u2217Work done as an intern at Google Research.\net al., 2022; Lu et al., 2022) that LLMs are sen-\nsitive to the choice of prompts, making \u201cprompt\nengineering\u201d for different tasks challenging and\ntime-consuming. However, prompt engineering\ndoes not have to be a complete guessing game;\nrather it can be governed by some data-derived\nsignals. For example, selecting demonstrations\nthat are semantically similar to a new input has\nshown to be more effective over randomly sampled\ndemonstrations (Das et al., 2021; Liu et al., 2022;\nMargatina et al., 2023), wherein a text retriever is\nused to select the top-k training examples for each\ntest example based on the input text. The motiva-\ntion is that using information from existing similar\nsituations will help solve a new problem (Aamodt\nand Plaza, 1994).\nHowever, the solely input-based selection does\nnot explicitly capture the LLM\u2019s existing knowl-\nedge about the task-specific label space of both the\nICL demonstration as well as the test input. For ex-\nample, on a five-way sentiment classification task\n(SST (Socher et al., 2013)), we have observed that\nthe Flan-PaLM 2 model (size L) (Anil et al., 2023)\nis confused between two specific labels, \u2018Very Neg-\native\u2019 and \u2018Negative,\u2019 a lot more than say between\n\u2018Neutral\u2019 and \u2018Very Negative\u2019, as shown in Fig-\nure 2. This motivates us to investigate whether the\nmodel\u2019s existing knowledge can also be leveraged\nto select even more effective demonstrations.\nSpecifically, we derive signals from the underly-\ning LLM about the output label space of both the\nnew test example and the training data from which\nwe select the demonstrations. As motivated above,\nthe model\u2019s ambiguity around the new test exam-\nple\u2019s output label will help us know what the model\nis most confused about, which in turn can be used\nto select those demonstrations that help reduce this\nconfusion. For selecting such demonstrations from\nthe training data, we propose to consider not only\nthe ground truth labels paired with these demon-\nstrations, but also the usefulness by looking at their\narXiv:2309.07900v2  [cs.CL]  30 Jan 2024\nzero-shot\nTraining Data\nInput Text\n\u201cOk! I like making \nfriends\u201d\nRetriever\nLLM\nAmbiguous Label Set\n{Love, Joy}\nTop Semantic Matches\n1. I want to make friends too :( \nbut I feel like I have nothing \ngood to offer\n2. I ... I like you\n3. I, too, am a lot of fun at \nparties. We can stand \ntogether in the corner!\n4. FaceTime with wifey!! \nHappy anniversary!\n\u2026\nExamples & Labels\n1. Gold: Disappointment\nPredicted: Sadness\n2. Gold: Love\nPredicted: Love\n3. Gold: Joy\nPredicted: Amusement\n4. Gold: Joy\nPredicted: Love\n\u2026\nTop Retrieved Examples \nwith Gold label \u2208 {Love, Joy}\nExamples : [2, 3, 4, \u2026]\nGold label \u2260 Predicted Label\nExamples : [3, 4, ...]\nPredicted Label \u2208 {Love, Joy}\nExamples : [4, ...]\nAdd constraint\nLLM Model prediction\nAMBIG-ICL\nPrompt Construction\nLLM\nzero-shot\nAdd constraint\nFigure 1: Overview of our proposed method for selecting ICL demonstrations: For each test example, we first use a\nretriever to rank training data by semantic similarity. At the same time, we identify the ambiguous label set for each\ntest example and also obtain the output predictions on the retrieved training data. Next, we apply three constraints\non the top-ranked demonstrations which are: 1) select those demonstrations whose gold label is in the ambiguous\nlabel set, 2) select those which are also mis-classified by the model, and 3) select those mis-classified examples\nwhose predicted label is in the ambiguous label set. Finally, we construct prompts with selected ICL demonstrations\nto get the final model predictions.\nFigure 2: Confusion Matrix of zero-shot experiments\non SST with Flan-PaLM 2 (L). Labels: VPos (Very\nPositive), Pos (Positive), Neu (Neutral), Neg (Negative),\nVNeg (Very Negative).\nmodel prediction. First, given a test example and\npool of training data, for each test example we use\nan off-the-shelf retriever to retrieve top-k examples\nthat have similar input text. For each test example,\nwe identify an ambiguous label set of two output\nlabels that the model is most confused about. Next,\nwe select top-ranked demonstrations such that their\nground truth labels lie in the above label set. To fur-\nther find useful demonstrations, we identify those\nwhich are mis-classified by the model; the intu-\nition is that showing the model a previously mis-\nclassified demonstration could force it to correct\nit (Tan, 2006; Wang et al., 2020). Finally, on top\nof the mis-classified demonstrations we add a con-\nstraint to select only those demonstrations whose\nmodel prediction falls within the ambiguous label\nset, i.e., on the test example\u2019s decision boundary.\nTo test our hypothesis, we focus on multi-class\ntext classification tasks that have fine-grained nu-\nance in the label space. We conduct extensive\nexperimentation across three tasks, namely SST\n(Socher et al., 2013), GoEmotions (Demszky et al.,\n2020), and EDOS (Task-B) (Kirk et al., 2023), all\nof which have fine-grained label space, making the\nmodel more likely to be confused across labels.\nOur key observations are:\n1. Incrementally adding constraints, i.e., 1) con-\nsidering label ambiguity of test example, 2)\nlimiting ICL demonstrations to mis-classified\ndemonstrations, and 3) considering label am-\nbiguity of training examples leads to +1.5%,\n+2.2%, +2.6% improvement in F1 macro\nscores over the retriever-based ICL, averaged\nacross all datasets (Table 3).\n2. We find that adding such label-based con-\nstraints helps more on a smaller model, i.e.,\non Flan-PaLM 2 (M) (+3.9% gain) compared\nto +1.4% gain on Flan-PaLM 2 (L).\n3. We also attribute this success of our proposed\nmethods to the observation that the ambigu-\nous label set acts as a good proxy to the\ngold test label, and as noted by Min et al.\n(2022), labels in the ICL demonstrations bias\nthe model predictions the most. Therefore,\nshowing the models the \u2018likely\u2019 gold label\nguides the model to make the correct predic-\ntion (Table 5).\n2\nProposed Method\nTypically, in an ICL regime, we assume access to\ntraining data Dtrain = {(x0, y0), \u00b7 \u00b7 \u00b7 , (xT , yT )}\nfrom which the goal is to select d demonstrations\nto be used as prompt. As motivated in the introduc-\ntion, we follow a three-step approach for selecting\ndemonstrations, for each test example, we need\nto 1) extract semantically similar examples from\nDtrain, 2) identify the ambiguous label-set and 3)\nextract model predictions for Dtrain to identify mis-\nclassified examples. Below, we describe each step\nin more detail and how they are used together to\nselect the \u201cbest\u201d demonstrations.\nExtract Semantically Similar Demonstrations\nTypically, in this approach, demonstrations are se-\nlected for each test example xt by finding those\nexamples from the Dtrain that are semantically\nsimilar to the test input. The motivation being\nthat observing demonstrations that are similar to\nthe new input text will act as a hint for the model\n(Margatina et al., 2023). This requires the use of a\nretriever R, either an off-the-shelf one such as (Liu\net al., 2022; Agrawal et al., 2023; Margatina et al.,\n2023; Luo et al., 2023) or a retriever trained specif-\nically for that task (Das et al., 2021; Rubin et al.,\n2022). For each test example xt, the retriever R is\nused to rank examples from Dtrain based on seman-\ntic similarity of the text inputs. Top-k input-output\npairs are then selected from the ranked Dtrain to\nbe used as ICL demonstrations.\nIdentify Ambiguous Label-Set\nAs we can ob-\nserve from the confusion matrix in Figure 2, the\nmodel is often confused between two labels. We\nhypothesize that in addition to semantic similarity,\nproviding demonstrations that help the model re-\nsolve this ambiguity will help the model correct\nitself. Thus, as a next step, we construct a prompt\n\u03b8 for the test example xt, and use the model log-\nlikelihood to score each output label l \u2208 L given\nthe prompt. Using this we identify top-2 labels\nthat have the highest scores, which we refer to\nas the \u201cambiguous label set\u201d of xt, denoted as\nLambig,t = {\u02c6y(1)\nt , \u02c6y(2)\nt }, where \u02c6y(1)\nt\nand \u02c6y(2)\nt\nare\nthe first and second most likely labels, respectively.\nExtract Mis-classified Demonstrations\nThe fi-\nnal component in our recipe is to consider the\nmodel prediction of the training data. While prior\nwork Min et al. (2022); Yoo et al. (2022); Margatina\net al. (2023) has looked at training data label-space\nfrom the lens of ground-truth labels, i.e., whether\nto retain them in the ICL or not, we aim to look\nat label-space from the perspective of model pre-\ndictions. Specifically, we are interested in iden-\ntifying \u201chard\u201d demonstrations, i.e., examples on\nwhich the model makes mistakes. We hope that\nby showing the model such examples with their\nground truth labels will force the model to correct\nitself. Prior work has underscored the potential\nvalue of leveraging mis-classified examples from\nthe training set to enhance model performance (Tan,\n2006; Wang et al., 2020), but they haven\u2019t tested\nit for ICL demonstration selection on text classifi-\ncation. In addition to the mis-classified examples,\nwe further constrain the model prediction of these\nmis-classified examples to be one of the ambigu-\nous labels, identified in the above step. Given that\nwe already know which output labels the model is\nconfused between for the test examples, showing\nthe model those demonstrations (with their ground\ntruth labels) which fall on the decision boundary\nwill likely guide the model to choose the correct\nlabel for the test input.\n3\nExperimental Setup\n3.1\nModel\nWe experiment with the Flan-PaLM 2 model, an\ninstruction-tuned model which is finetuned on the\nFlan dataset (Chung et al., 2022; Longpre et al.,\n2023) based on PaLM-2 (Anil et al., 2023), a multi-\nlingual large language model pretrained on web\ndocuments, books, code, mathematics and con-\nversational data. We chose these models as Luo\net al., 2023 find that retrieved demonstration for\nICL works better with instruction-tuned models\nover general LLMs (e.g., GPT). In particular, we\nexperiment with two variants of the model, namely\nFlan-PaLM-2 (M) and Flan-PaLM-2 (L), where the\nlatter is a larger parameter model.1 The ICL demon-\nstrations are selected using an off-the-shelf retriever\nwhich is finetuned on mT5-base (Xue et al., 2021)\nusing the unsupervised objective proposed by Izac-\nard et al. (2021). Since the order of demonstrations\nmay impact the model performance (Kumar and\nTalukdar, 2021; Lu et al., 2022), we randomly shuf-\nfle the order of demonstrations for three random\nseeds and report the average results.\n3.2\nData\nAs mentioned above, the Flan-PaLM 2 models are\nfinetuned on the Flan dataset which is a mixture of\nmany supervised datasets. Specifically, we choose\nthree text classification datasets that satisfy the fol-\nlowing desiderata, 1) the output label space shows\nfine-grained nuance that spans multiple labels, and\n2) these datasets are not part of the Flan mixture to\navoid any inherent bias from the underlying model.\nWe describe them below, with dataset statistics\nshown in Table 1. All datasets are in English.\nEDOS (Task-B):\nThe Task B of Explainable De-\ntection of Online Sexism (Kirk et al., 2023), is a\ntopic classification task where the sexist content is\nclassified into four categories, i.e., 1) Threats, plans\nto harm & incitement, 2) Derogation, 3) Animosity,\nand 4) Prejudiced Discussion.\nSST:\nThe Stanford Sentiment Treebank (SST,\nSocher et al., 2013) is a 5-way sentiment classi-\nfication dataset for movie reviews with labels: Very\nNegative, Negative, Neutral, Positive, and Very\nPositive.\nGoEmotions:\nThe GoEmotions (Demszky et al.,\n2020) is a multi-class sentiment classification\ndataset with \u201cneutral\u201d and 27 emotional classes,\ne.g., \u201cadmiration\u201d and \u201cfear\u201d, collected from Red-\ndit comments. As the label space is very large and\ngiven that we have limited sequence length, it be-\ncomes even more crucial to select a concise but\neffective prompt. 2\n3.3\nBaselines\nWe compare our proposed method against the fol-\nlowing baselines:\n1Please refer to Anil et al. (2023) for more details on the\nmodels.\n2We exclude 24,848 examples (19,925 from training set,\n2,474 and 2,449 from dev and test set, respectively) that have\nmultiple labels annotated for a single input, for a simpler\nexperimental setting. We refer the reader to Demszky et al.\n(2020) for more information on the single-label setting.\ntrain\ndev\ntest\nEDOS\n3,398\n486\n970\nSST\n8,544\n1,101\n2,210\nGoEmotions\n23,485\n2,952\n2,978\nTable 1: Number of examples in each dataset split.\nFrequent Label (FREQ).\nSelect the most fre-\nquent label as the model prediction for all test ex-\namples.\nZero-shot ICL (ZERO).\nFor each test example\nxt, we prepend the task definition to each test in-\nput and prompt the models.3 To obtain the model\nprediction, we use the model log-likelihood to\nscore each output label l \u2208 L, given the prompt.\nThen, we select the label with the highest score.\nyt = arg maxL score(l, \u03b8) where \u03b8 refers to the\nprompt specifically used for this setting, and score\nrefers to the model\u2019s log-likelihood.\nStatic N-shot ICL (STATIC-N).\nWe manually\nselect N demonstrations from Dtrain, one for each\nof the N output labels (N = |L|).\nNote that\nthese demonstrations are static for all test exam-\nples. Thus, we concatenate the task definition, N\ndemonstrations and test example xt as the prompt\nfor ICL and use the log-likelihood scores, as de-\nscribed above, to get the model prediction.\nRetriever-based ICL (RETR).\nUnlike above,\nwhere we used the same prompt for all test inputs,\nin this baseline, we retrieve demonstrations for\neach test input xt. We use an off-the-shelf retriever\nR (subsection 3.1) to retrieve k nearest neighbors\n{x1,t, \u00b7 \u00b7 \u00b7 , xk,t} from Dtrain, similar to Das et al.\n(2021). We encode the input text of training set and\nthe test example, rank the training data by the inner\nproduct of the vectors. Of these k examples, we\nselect n = 4, 8 as ICL demonstrations.4\n3.4\nProposed Method: AMBIG-ICL\nAs described in section 2, our proposed method\nconsiders both semantic similarity and the label\nambiguity for selecting demonstrations. Below,\n3Please refer to Appendix A.1 for the exact prompt and\nprompt template used in this setting, as well as for few shot\nsettings such as the subsequent STATIC-N and RETR.\n4We chose k = 4, 8 for two reasons: a) to limit the se-\nquence length to 1024 tokens for faster inference, and b) in\nsome settings we found k = 4 often outperforming k = 8\n(Table 2), which led us to believe that adding more examples\nwill not benefit much.\nEDOS\nSST\nGoEmotions\nAvg.\nM\nL\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n15.9\n15.9\n7.5\n7.5\n0.8\n0.8\n8.1\n8.1\nZERO\n50.7\n60.5\n49.2\n54.1\n40.5\n43.4\n46.8\n52.7\nSTATIC-N\n51.1\u00b10.3\n58.5\u00b10.4\n50.3\u00b10.4\n56.5\u00b10.3\n34.3\u00b10.5\n44.4\u00b10.3\n45.2\n53.1\nRETR-4\n48.5\u00b10.3\n62.3\u00b10.4\n49.9\u00b10.3\n55.4\u00b10.3\n38.3\u00b10.3\n46.2\u00b10.4\n45.6\n54.6\nRETR-8\n47.1\u00b10.2\n61.8\u00b10.1\n51.5\u00b10.1\n55.2\u00b10.4\n37.5\u00b10.2\n46.7\u00b10.1\n45.4\n54.6\nOurs\nAMBIG-4\n+GOLD\n49.3\u00b10.6\n62.6\u00b10.2\n51.5\u00b10.4\n56.1\u00b10.0\n40.7\u00b10.3\n48.2\u00b10.2\n47.2\n55.6\n+GOLD+MIS\n52.2\u00b10.5\n61.7\u00b10.9\n52.3\u00b10.1\n57.4\u00b10.1\n40.1\u00b10.2\n47.6\u00b10.1\n48.2\n55.6\n+GOLD+MIS+PRED\n53.9\u00b10.5\n62.9\u00b10.4\n53.3\u00b10.4\n58.0\u00b10.0\n42.3\u00b10.5\n47.7\u00b10.2\n49.8\n56.2\nAMBIG-8\n+GOLD\n47.5\u00b10.1\n63.2\u00b10.2\n52.9\u00b10.1\n56.5\u00b10.6\n42.0\u00b11.2\n47.7\u00b10.1\n47.5\n55.8\n+GOLD+MIS\n50.4\u00b10.4\n62.0\u00b10.4\n53.4\u00b10.1\n57.7\u00b10.1\n43.9\u00b10.2\n47.6\u00b10.4\n49.2\n55.8\n+GOLD+MIS+PRED\n50.9\u00b10.6\n62.7\u00b10.2\n54.3\u00b10.2\n57.2\u00b10.3\n41.3\u00b10.3\n47.4\u00b10.3\n48.8\n55.8\nTable 2: F1 macro (%) comparison between our baselines (top) and our proposed methods (bottom) with Flan-PaLM\n2 (M/L). 4 or 8 refers to the number of ICL demonstrations. The best performance across all method is highlighted\nand the best performing baseline is underlined. The \u201cAvg.\u201d column shows the average scores across all datasets.\nThe standard deviations are computed over three random seeds, with the order of demonstrations shuffled.\nZERO\nSTATIC-N\nAMBIG-ICL\n+GOLD\n+MIS\n+PRED\nM\n1.3\n-0.2\n1.9\n3.3\n3.9\nL\n-1.9\n-1.5\n1.1\n1.1\n1.4\nall\n-0.3\n-0.9\n1.5\n2.2\n2.6\nWe omitted RETR in the table, which are inherently zero as we compare\nagainst RETR.\nFor both RETR and AMBIG-ICL, we average results on both 4 and 8 shots\nbefore computing differences.\nTable 3: F1 macro (%) differences compared to RETR,\naveraged across all datasets as detailed in Table 2. M\nand L refers to Flan-PaLM 2 sizes, and \u201call\u201d is averaged\non results of size M and L. \u201c+MIS\u201d and \u201c+PRED\u201d refer to\n\u201c+GOLD+MIS\u201d and \u201c+GOLD+MIS+PRED\u201d, respectively.\nwe summarize our proposed model variants. For\neach setting, we first retrieve the top-k most sim-\nilar examples from the training data Dtrain for\neach test example xt. We denote these candidates\nby R(xt) = {(x0,t, y0,t), \u00b7 \u00b7 \u00b7 , (xk,t, yk,t)}. At the\nsame time, for each xt, we also identify the ambigu-\nous label-set Lambig,t = {li, lj|l \u2208 L}. This set\ncontains the top-2 labels, li and lj, that the model\nis most confused about, where both labels belong\nto the set L of all output labels.\n+GOLD\nSelect those examples from R(xt) as\ndemonstrations where the ground truth label of\neach demonstration belongs to the ambiguous label\nset of xt denoted by:\nICL(xt) =\n\u001a (xi, yi) if yi \u2208 Lambig,t\nfor (xi, yi) \u2208 R(xt)\n\u001b\n+GOLD+MIS\nSelect those examples from R(xt)\nas demonstrations where the ground truth labels\nfall in Lambig,t and they are mis-classified, denoted\nby:\nICL(xt) =\n\u001a (xi, yi) if yi \u2208 Lambig,t, \u02c6yi \u0338= yi\nfor (xi, yi) \u2208 R(xt)\n\u001b\nNote that the model predictions (\u02c6y) on the R(xt)\nare obtained from the ZERO model.\n+GOLD+MIS+PRED\nSelect those examples from\nR(xt) as demonstrations where the ground truth la-\nbels fall in Lambig,t. Ensure they are mis-classified\nand with an additional constraint, that their model\npredictions also fall within Lambig,t, denoted by:\nICL(xt) =\n\u001a (xi, yi) if yi \u2208 Lambig,t, \u02c6yi \u0338= yi,\n\u02c6yi \u2208 Lambig,t for (xi, yi) \u2208 R(xt)\n\u001b\nSame as above, the model predictions on the train-\ning data are obtained from ZERO.\nFor all our proposed model variants, we select n\ndemonstrations where n = 4 and n = 8.\n4\nResults and Discussion\nWe report all our results in Table 2. Specifically,\nwe use the F1 macro scores to compare the model\nperformance, as all our tasks have unbalanced\ndatasets.5 First, we note across all three tasks, our\nproposed methods outperform the baselines.\nWe also note that the zero-shot model (ZERO)\nwhich only uses a task definition but no task demon-\nstrations, already is a strong baseline for both the\nFlan-PaLM 2 models (M/L). In particular, compar-\ning the average scores of the few-shot baselines\n5We report the accuracy, precision and recall in A.2.\nTest Example: Ok! I like making friends\nLambig,t: Love, Joy\nGold label: Love\nRETR\n1. Disappointment: I want to make friends too :( but I feel like I have nothing good\nto offer\n2. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n3. Gratitude: Thanks. I am. I make some new friends.\n4. Disapproval: Not really. My group of friends are awesome in every way possible\nexcept they are homophobic\nPredicted:\nJoy\nAMBIG-ICL\n+GOLD\n1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n2. Love: I ... I like you\n3. Love: Married to the love of my life. LOL\n4. Love: I do. but some people love it\nPredicted:\nLove\n+GOLD+MIS\n1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!\n2. Love: Too cute for me. Why cant i have a boyfriend *[NAME]*\n3. Joy: FaceTime with wifey!! Happy anniversary!\n4. Love: Stick around! Would love your input POV!\nPredicted:\nLove\n+GOLD+MIS+PRED\n1. Joy: FaceTime with wifey!! Happy anniversary!\n2. Joy: She want to take it slow, I can see that... I deal with those girls all the time,\nthey my favorite\n3. Love: Ha! I like that one.\n4. Love: Ooh I like that one :)\nPredicted:\nLove\nTable 4: Example demonstrations selected by the RETR and our proposed method AMBIG-ICL for the GoEmotions\ntask, for n = 4. Each demonstration comprises of the input text and the ground truth label, as selected from the\ntraining data. On Flan-PaLM 2 (L), where RETR mis-classified it as \u201cJoy\u201d, AMBIG-ICL predicted correctly under\nall three settings.\nand ZERO, we find that ZERO outperforms few-shot\nbaselines by 1.4% on Flan-PaLM 2 (M), but the\nlarger model Flan-PaLM 2 (L) benefits from the\naddition of ICL demonstrations (+1.4% gain). This\nis because larger-parameter models make better use\nof in-context learning (Chan et al., 2022; Aky\u00fcrek\net al., 2023; Wei et al., 2023). Interestingly, we also\nobserve that for SST and GoEmotions, the Flan-\nPaLM 2 (L) model achieves higher performance\nwith n = 4 over n = 8, which highlights that quan-\ntity does not necessarily lead to better performance.\nConsidering output label space is more impor-\ntant than semantic similarity.\nWithin the few-\nshot methods, where we use ICL demonstrations\nalong with the task definition, we compute from Ta-\nble 3 that our proposed methods AMBIG-* outper-\nforms retriever-based models (RETR-*) by +3.0%\n(avg.) for Flan-PaLM 2 (M), and by +1.2% (avg.)\nfor Flan-PaLM 2 (L), suggesting that considering\noutput label space for selecting demonstrations is\nas important as considering the input similarity. In\nparticular, we find that considering mis-classified\ndemonstrations that fall on the test example\u2019s deci-\nsion boundary leads to the overall best performance.\nIn Table 4, we show the demonstrations selected\nfor the n = 4 setting for one example of the GoE-\nmotions task. We see that for the test input \u201cOk! I\nlike making friends\u201d, the RETR method retrieved\nsimilar examples from Dtrain (all examples refer to\nfriends). Now from the ZERO model, we calculated\nthe model prediction scores and found that Love\nand Joy are the two labels the model is most con-\nfused about. However, because we do not consider\nany test example ambiguity in RETR, only one of\nthe retrieved examples represent the labels Love or\nJoy, which are the two labels the model is most con-\nfused about for this test example. Whereas, in the\nAMBIG-ICL setting, because of our constraints, all\nthe examples chosen for ICL belong to the ambigu-\nous label set. This allows all our proposed methods\nto better understand this fine-grained nuance across\nlabel space and make the correct model prediction\nof Love. Below, we conduct some analysis to fur-\nther explain the way our proposed methods work.\nConsidering output label space compensates for\nthe sacrifice in semantic similarity.\nAs we in-\ntroduce more constraints (i.e., +GOLD, +MIS, and\n+PRED), we find that we need to sacrifice the seman-\ntic similarity to the test input. For example, con-\nsider the 4-shot AMBIG-ICL experiment on EDOS\n(Task-B), to satisfy the constraints for the +GOLD\nsetting we need to select up to top-16 retrieved\nexamples in order to obtain the 4 ICL demonstra-\ntions; for +GOLD+MIS we need top-55 retrieved\nexamples and more than top-250 retrieved exam-\nples for +GOLD+MIS+PRED.6 Clearly, by select-\ning lower ranked examples from the retrieved set\nR(xt) we are sacrificing the semantic similarity to\nthe test input. While previous studies, such as (Das\net al., 2021; Liu et al., 2022; Margatina et al., 2023),\nhave indicated that greater semantic similarity can\nenhance model performance, we can see that our\nmethods can still outperform the retriever-based\nbaselines which prioritize it.\nThe ambiguous label set is a good proxy for the\ntest gold label.\nWhile Min et al. (2022) find that\nusing pseudo-demonstrations i.e. demonstrations\nwith random labels instead of the ground truth la-\nbels, does not affect the downstream performance\nmuch, Lyu et al. (2023) find that for demonstrations\nthat are similar to the test input, such as those from\na retriever, pseudo-demonstrations hurt the perfor-\nmance. They refer to this as the copying-effect\nhypothesis which says that the \u201cmodel prediction is\nbiased towards the labels paired with the inputs in\nthe demonstrations, especially when the inputs are\nsimilar to the test inputs\u201d. This, in turn, suggests\nthat the best performance could be achieved if the\nlabels paired with the inputs are same as the gold\nlabel of the test example. Given that we do not\nknow the gold label of the test example apriori, the\nquestion then becomes how do we approximate the\ngold label?. We find that our ambiguous label set\nacts as a close proxy. In Table 5, we compute how\nmany times is the label paired with ICL demonstra-\ntions the same as the test example gold label. We\nfind that 44.2% of our proposed methods\u2019 (AMBIG)\ndemonstrations have the same gold label as the test\nexample on average, compared to 30.9% from the\nRETR method. This is why including the ambigu-\nous label set in the demonstration selection process\nleads to a higher performance. This analysis also\nsheds light on the effectiveness of retriever-based\nICL. From Table 5 we can see that the demonstra-\ntions selected solely based on input text similarity\nis only 13.3% points (avg.) behind our proposed\nmethods. This confirms that finding demonstra-\ntions similar to the input text also leads to selecting\ndemonstrations that have the \u2018likely\u2019 gold label.\nAMBIG-ICL helps reduce the model confusion.\nTo understand whether including test label ambi-\n6We set a strict constraint on our selection (top-250 re-\ntrieved example for +GOLD, and top-250 misclassified re-\ntrieved examples for the other two). If there aren\u2019t sufficient\nexamples for +GOLD+MIS+PRED within the top-250 misclas-\nsified retrieved example, we fall-back on the previous setting\n(+GOLD+MIS).\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\n4-shot\n42.6\n29.6\n21.6\n8-shot\n42.5\n28.6\n20.5\nAMBIG-4\n+GOLD\n49.5 50.3\n46.5 47.1\n41.3\n41.9\n+GOLD+MIS\n46.4 44.3\n46.1 44.3\n38.7\n38.8\n+GOLD+MIS+PRED\n48.3 42.3\n46.1 44.6\n37.8\n40.7\nAMBIG-8\n+GOLD\n50.3 50.3\n46.0 46.8\n41.2\n41.7\n+GOLD+MIS\n46.9 43.8\n46.4 44.7\n38.7\n38.6\n+GOLD+MIS+PRED\n48.8 42.9\n46.5 44.9\n37.5\n40.3\nTable 5: Average percentage (%) of examples in the top\n4, 8 retrieved demonstrations that share the same gold\nlabels with test example.\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nuniform\n2.00\n2.32\n4.75\nZERO\n0.98 1.08\n1.58 1.19\n2.44 1.92\nSTATIC-N\n0.87 1.07\n1.41 1.11\n1.76\n1.77\nRETR-4\n0.78 0.97\n1.40 1.06\n1.89\n1.70\nRETR-8\n0.82 0.96\n1.38 1.04\n1.79\n1.69\nAMBIG-4\n+GOLD\n0.77 0.93\n1.39 1.02\n1.86\n1.43\n+GOLD+MIS\n0.85 0.98\n1.41 1.06\n1.92\n1.48\n+GOLD+MIS+PRED\n0.86 1.00\n1.42 1.07\n1.92\n1.46\nAMBIG-8\n+GOLD\n0.81 0.91\n1.36 0.98\n1.68\n1.33\n+GOLD+MIS\n0.89 0.97\n1.39 1.03\n1.74\n1.39\n+GOLD+MIS+PRED\n0.90 1.00\n1.40 1.04\n1.76\n1.37\nTable 6: Average entropy of predicted probability distri-\nbution. \u201cuniform\u201d refers to the entropy computed for an\nuniform probability distribution over the labels. Lower\nentropy is better.\nguity indeed helps decrease the model confusion,\nwe calculate the model entropy over the predicted\nprobability distribution of the output labels in Ta-\nble 6.7 Overall, we observe that our AMBIG-*\nmethods achieve the lowest entropy across all three\ndatasets and models. This suggests that by explic-\nitly identifying the point of model confusion (in this\ncase the confusion across fine-grained labels) and\nselecting demonstrations that help resolve this con-\nfusion is indeed effective in reducing the confusion\nacross labels, and thereby resulting in higher down-\nstream performance (Table 2). In particular, we\nfind that for the Flan-PaLM 2 (L), the gap between\nthe few-shot baselines and the AMBIG-* methods\nis larger, perhaps because larger models are better\nable to use the ICL demonstrations (Chan et al.,\n2022; Aky\u00fcrek et al., 2023; Wei et al., 2023).\n7We compute entropy with a base of 2.\nWe also compute the Pearson correlation coeffi-\ncient between F1 macro scores and average entropy\nof predicted probability distribution (shown in Ta-\nble 2 and Table 6, respectively), for all the three\ndatasets. We find that for the Flan-PaLM 2 (L)\nmodel, there is a negative correlation for all three\ndatasets, i.e., r=\u22120.78 for EDOS, \u22120.48 for SST\nand \u22120.92 for GoEmotions, which suggests that\nlower entropy translates to higher task performance.\nHowever, for the Flan-PaLM 2 (M), we have mixed\nresults, as r is positive for EDOS (0.47), negative\nfor SST (\u22120.55), and close to zero for GoEmotions\n(0.03).\n5\nRelated Work\nThe performance of large language models (LLMs)\nis significantly influenced by the quality of ICL\ndemonstrations, as demonstrated in multiple stud-\nies (Zhao et al., 2021; Liu et al., 2022; Zhang et al.,\n2022). Consequently, the focus on retrieving su-\nperior demonstrations has increased. One promi-\nnent strategy is to finetune a retriever for specific\ntasks by similarity metrics (Das et al., 2021; Hu\net al., 2022; Poesia et al., 2022) or by scores de-\nrived from language models (Rubin et al., 2022;\nShi et al., 2022). While some works introduce an\nunified retriever trained across various tasks (Li\net al., 2023; Cheng et al., 2023) for generalizabilty,\nanother direction is to leverage off-the-shelf re-\ntrievers. Liu et al., 2022 propose a KNN-based\nmethod to select ICL demonstrations based on se-\nmantic similarities; Margatina et al., 2023 select\nICL demonstrations with active learning algorithms\nbased on uncertainty, diversity, and similarity, and\nshow that selecting based on input text similar-\nity consistently outperforms other methods; and\nAgrawal et al., 2023 focus on selecting diverse\ndemonstrations as well as promoting n-gram over-\nlap between demonstrations and test examples. In\nour work, we adopt the off-the-shelf retriever ap-\nproach as our focus is to show the generalizability\nof our approach across different classification tasks.\nHowever, we expect that our method will also ben-\nefit from a task-specific retriever. Additionally, to\nthe best of our knowledge, we are the first ones to\nleverage the LLM\u2019s existing knowledge surround-\ning the test example for selecting demonstrations.\nPrior works have typically explored the LLM\u2019s ex-\nisting knowledge, considering the model prediction\nfor the training data.\nLuo et al., 2023 use the LLM prediction score\non the training data to train a task-specific retriever,\nand also use Chain-of-Thought prompting (Wei\net al., 2022) to improve model performance. Some\nworks (Kumar and Talukdar, 2021; Lu et al., 2022)\nhave found that ordering of the ICL demonstrations\nalso affects the downstream performance, that is\nwhy in Table 2 we report the results across three\nshuffle orders. These works are orthogonal to our\nwork but can be used in combination with our pro-\nposed methods.\n6\nConclusion and Next Steps\nIn this work, we find that using LLM\u2019s existing\nknowledge (e.g., the model prediction) regarding\nthe output label space of both the test example\nand the ICL demonstration pool is as important\nas considering the semantic similarity of the in-\nput text alone. We find that our proposed method\nconsistently outperform the baselines for all three\ntasks. Although, we only consider the top-2 most\nambiguous labels in selecting the ICL demonstra-\ntions, it would be interesting to expand the am-\nbiguous label set to more than two labels. This\nwould especially be more important for datasets\nlike GoEmotions where the label space is large and\nmuch more fine-grained. We leave this effort for\nfuture work. Furthermore, in this work, we focus\non sentence classification tasks, thus paving the\nway for others to use our proven techniques to also\nexplore label ambiguity for other token/span-level\ntasks such as Named Entity Recognition (NER),\nand Part-Of-Speech (POS) tagging.\n7\nLimitations\nWe focus on reducing LLM\u2019s label ambiguity by\nincorporating demonstrations that are misclassified\nby the LLM and reside on the test example\u2019s deci-\nsion boundary. While we show this methodology\u2019s\neffectiveness across datasets, even those with a\ngranular label structure, potential pitfalls remain.\nIf the actual gold label of test example often de-\nviates from the LLM\u2019s top two label choices in a\nparticular dataset or model, this can be indicative of\nsubpar zero-shot performance or flawed ambiguous\nlabel set selection. In these scenarios, our method\nmay lead to unsatisfying performance, necessitat-\ning further enhancements.\n8\nEthics Statement\nWe use pretrained large language models (LLMs)\nfor text classification. Notably, LLMs are shown\nto exhibit biases, which is a well-recognized chal-\nlenge and the broader community is currently work-\ning to address. Since our main goal is to improve\nthe downstream task performance, an improved per-\nformance on an offensive content classification task\ncould be misused. In particular, the EDOS dataset\nused in our work, contains offensive content. We\nselected this dataset for its fine-grained label nu-\nances and to ensure our research isn\u2019t biased by\nmodels inherently familiar with the data.\nReferences\nAgnar Aamodt and Enric Plaza. 1994. Case-based rea-\nsoning: Foundational issues, methodological varia-\ntions, and system approaches. AI Commun., 7(1):39\u2013\n59.\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2023. In-\ncontext examples selection for machine translation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 8857\u20138873, Toronto,\nCanada. Association for Computational Linguistics.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2023. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. In The Eleventh International\nConference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023. OpenReview.net.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hern\u00e1ndez\n\u00c1brego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan A. Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier\nGarcia, Sebastian Gehrmann, Lucas Gonzalez, and\net al. 2023.\nPalm 2 technical report.\nCoRR,\nabs/2305.10403.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian\nPedregosa, Andreas Mueller, Olivier Grisel, Vlad\nNiculae, Peter Prettenhofer, Alexandre Gramfort,\nJaques Grobler, Robert Layton, Jake VanderPlas, Ar-\nnaud Joly, Brian Holt, and Ga\u00ebl Varoquaux. 2013.\nAPI design for machine learning software: experi-\nences from the scikit-learn project. In ECML PKDD\nWorkshop: Languages for Data Mining and Machine\nLearning, pages 108\u2013122.\nStephanie C. Y. Chan, Ishita Dasgupta, Junkyung Kim,\nDharshan Kumaran, Andrew K. Lampinen, and Felix\nHill. 2022. Transformers generalize differently from\ninformation stored in context vs in weights. CoRR,\nabs/2210.05675.\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu\nWei, Denvy Deng, and Qi Zhang. 2023. UPRISE:\nuniversal prompt retrieval for improving zero-shot\nevaluation. CoRR, abs/2303.08518.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya God-\nbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros\nPolymenakos, and Andrew McCallum. 2021. Case-\nbased reasoning for natural language queries over\nknowledge bases. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 9594\u20139611, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4040\u20134054, Online. Association for Computational\nLinguistics.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022.\nIn-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2627\u20132643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning.\nCoRR, abs/2112.09118.\nHannah Kirk, Wenjie Yin, Bertie Vidgen, and Paul\nR\u00f6ttger. 2023. SemEval-2023 task 10: Explainable\ndetection of online sexism. In Proceedings of the\n17th International Workshop on Semantic Evaluation\n(SemEval-2023), pages 2193\u20132210, Toronto, Canada.\nAssociation for Computational Linguistics.\nSawan Kumar and Partha Talukdar. 2021.\nReorder-\ning examples helps during priming-based few-shot\nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages\n4507\u20134518, Online. Association for Computational\nLinguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,\nYuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023. Unified demonstration retriever for in-\ncontext learning. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4644\u20134668,\nToronto, Canada. Association for Computational Lin-\nguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022.\nWhat\nmakes good in-context examples for GPT-3?\nIn\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100\u2013114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le,\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\nThe flan collection: Designing data and methods for\neffective instruction tuning. In International Con-\nference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pages\n22631\u201322648. PMLR.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086\u20138098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasu-\npat, Seyed Mehran Kazemi, Chitta Baral, Vaiva\nImbrasaite, and Vincent Y. Zhao. 2023.\nDr.icl:\nDemonstration-retrieved in-context learning. CoRR,\nabs/2305.14128.\nXinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer,\nand Hannaneh Hajishirzi. 2023. Z-ICL: Zero-shot\nin-context learning with pseudo-demonstrations. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2304\u20132317, Toronto, Canada.\nAssociation for Computational Linguistics.\nKaterina Margatina, Timo Schick, Nikolaos Aletras, and\nJane Dwivedi-Yu. 2023. Active learning principles\nfor in-context learning with large language models.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048\u201311064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\nGustavo Soares, Christopher Meek, and Sumit Gul-\nwani. 2022. Synchromesh: Reliable code generation\nfrom pre-trained language models. In The Tenth In-\nternational Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655\u20132671, Seattle, United States.\nAssociation for Computational Linguistics.\nPeng Shi, Rui Zhang, He Bai, and Jimmy Lin.\n2022. XRICL: Cross-lingual retrieval-augmented in-\ncontext learning for cross-lingual text-to-SQL seman-\ntic parsing. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pages 5248\u2013\n5259, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631\u20131642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nSongbo Tan. 2006. An effective refinement strategy for\nKNN text classifier. Expert Syst. Appl., 30(2):290\u2013\n298.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Ag\u00fcera\ny Arcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland, Tyler Reddy, David Cournapeau, Ev-\ngeni Burovski, Pearu Peterson, Warren Weckesser,\nJonathan Bright, St\u00e9fan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay\nMayorov, Andrew R. J. Nelson, Eric Jones, Robert\nKern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng,\nEric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A.\nQuintero, Charles R. Harris, Anne M. Archibald, An-\nt\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mul-\nbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in\nPython. Nature Methods, 17:261\u2013272.\nXiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu,\nHailin Shi, and Tao Mei. 2020. Mis-classified vec-\ntor guided softmax loss for face recognition. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 12241\u2013\n12248. AAAI Press.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-\nbert Webson, Yifeng Lu, Xinyun Chen, Hanxiao\nLiu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.\nLarger language models do in-context learning dif-\nferently. CoRR, abs/2303.03846.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-\nsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\nand Taeuk Kim. 2022. Ground-truth labels matter: A\ndeeper look into input-label demonstrations. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2422\u2013\n2437, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9134\u2013\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697\u201312706. PMLR.\nA\nAppendix\nA.1\nPrompt Construction\nWe show our templates in Table 7 (we use 4-shot\nas an example for few-shot). Task definitions are\nlisted below, denoted by xdefn:\n\u2022 EDOS: Given a text input, the task is to classify the input\nas being a Threat, Prejudiced, Animosity, or Derogation\ncategory of sexism. Threat refers to language where an\nindividual expresses intent andor encourages others to take\naction against women which inflicts or incites serious harm\nand violence against them. It includes threats of physical,\nsexual or privacy harm. Prejudiced refers to language which\ndenies the existence of discrimination, and justifies sexist\ntreatment. It includes denial and justification of gender in-\nequality, excusing women\u2019s mistreatment, and the ideology\nof male victimhood. Animosity refers to language which ex-\npresses implicit or subtle sexism, stereotypes or descriptive\nstatements. It includes benevolent sexism, i.e., framed as a\ncompliment. Derogation refers to language which explicitly\nderogates, dehumanises, demeans or insults women. It in-\ncludes negative descriptions and stereotypes about women,\nobjectification of their bodies, strong negative emotive state-\nments, and dehumanising comparisons. It covers negative\nstatements directed at a specific woman and women in gen-\neral.\n\u2022 SST: Given sentences from movie reviews, the task is to\nclassify the sentences as being a Great, Good, Okay, Bad,\nor Terrible category of sentiment. Great refers to language\nthat expresses extremely positive sentiment. Good refers\nto language that expresses positive sentiment, but not to\nthe extreme. Okay refers to language that is neutral, i.e.,\nneither expresses clear positive nor negative sentiments.\nBad refers to language that expresses negative sentiment,\nbut not to the extreme. Terrible refers to language that\nexpresses extremely negative sentiment.\n\u2022 GoEmotions: Given sentences from Reddit comments, the\ntask is to classify the sentences as being an Admiration,\nApproval, Annoyance, Gratitude, Disapproval, Amusement,\nCuriosity, Love, Optimism, Disappointment, Joy, Realiza-\ntion, Anger, Sadness, Confusion, Caring, Excitement, Sur-\nprise, Disgust, Desire, Fear, Remorse, Embarrassment, Ner-\nvousness, Pride, Relief, or Grief category of emotions.\nA.2\nAccuracy, Precision, Recall\nPlease refer to Table 8, 9 and 10.\nA.3\nLabel-wise Percentage Analysis of Gold\nLabel Inclusion in Lambig,t\nWe compute the percentage of times that the test\nexample\u2019s gold label is in Lambig,t (as obtained\nwith ZERO) in Table 11, and we present label-wise\nresults in Table 12.\nA.4\nSorting Order with Predicted Probability\nDistribution Entropy\nSince we have the predicted probability distribu-\ntion of ICL demonstrations, we tried to sort the\nICL demonstrations by increasing entropy order.\nHowever, it doesn\u2019t consistently improve model\nperformance, which is shown in Table 13 and 14.\nxdefn\nThus given the following input:\ninput: xt\nanswer:\nxdefn\nSome examples are:\ninput: x1,t\nanswer: y1,t\ninput: x2,t\nanswer: y2,t\ninput: x3,t\nanswer: y3,t\ninput: x4,t\nanswer: y4,t\nThus given the following input:\ninput: xt\nanswer:\nTable 7: Prompt templates for zero-shot and few-shot\nICL. xt refers to the test example, and xi,t, yi,t refers to\nthe text inputs and gold labels of ICL demonstrations\nselected for xt, respectively.\nA.5\nExample on SST for Comparison between\nRETR and AMBIG-ICL\nWe list example demonstrations on SST where the\nmodel correctly predict the test example in our\nproposed method, but wrongly classify it in RETR\nin Table 15.\nA.6\nResponsible AI Checklist\nPackages for Evaluations.\nWe use SciPy (Vir-\ntanen et al., 2020) and scikit-learn (Buitinck et al.,\n2013) for evaluations.\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n11.7\n11.7\n4.6\n4.6\n0.4\n0.4\nZERO\n65\n60.7\n54\n56.2\n42.6\n46.3\nSTATIC-N\n65.2\u00b10.6\n58.1\u00b10.4\n54.5\u00b10.6\n58.2\u00b10.3\n42.6\u00b11.2\n46.2\u00b10.3\nRETR-4\n67.1\u00b11.1\n63.6\u00b10.5\n53.4\u00b10.3\n57.4\u00b10.4\n43.7\u00b10.4\n47.6\u00b10.4\nRETR-8\n65.0\u00b10.2\n63.9\u00b10.3\n54.4\u00b10.1\n57.6\u00b10.5\n43.7\u00b10.4\n48.3\u00b10.1\nOurs\nAMBIG-4\n+GOLD\n65.9\u00b10.8\n63.6\u00b10.4\n54.1\u00b10.3\n57.7\u00b10.1\n45.7\u00b10.3\n50.5\u00b10.2\n+GOLD+MIS\n66.6\u00b11.1\n63.6\u00b11.0\n54.1\u00b10.2\n58.8\u00b10.1\n44.8\u00b10.4\n49.2\u00b10.1\n+GOLD+MIS+PRED\n67.4\u00b10.4\n65.0\u00b10.5\n54.8\u00b10.5\n59.4\u00b10.0\n46.9\u00b11.3\n47.9\u00b10.2\nAMBIG-8\n+GOLD\n66.4\u00b11.1\n64.8\u00b10.1\n54.7\u00b10.2\n58.5\u00b10.7\n48.0\u00b11.8\n49.9\u00b10.1\n+GOLD+MIS\n68.4\u00b10.8\n64.4\u00b10.6\n54.5\u00b10.1\n59.6\u00b10.1\n48.7\u00b10.5\n48.8\u00b10.5\n+GOLD+MIS+PRED\n66.6\u00b11.2\n66.4\u00b10.3\n54.9\u00b10.2\n59.1\u00b10.4\n43.7\u00b10.5\n47.4\u00b10.3\nTable 8: Precision (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n25\n25\n20\n20\n3.7\n3.7\nZERO\n46\n62.8\n53.8\n55.2\n42.4\n47.2\nSTATIC-N\n46.2\u00b10.3\n63.0\u00b10.3\n54.0\u00b10.4\n56.5\u00b10.2\n34.8\u00b10.5\n49.5\u00b10.4\nRETR-4\n44.8\u00b10.3\n63.4\u00b10.2\n53.4\u00b10.3\n55.7\u00b10.3\n38.5\u00b10.2\n49.7\u00b10.3\nRETR-8\n44.0\u00b10.1\n62.1\u00b10.2\n54.2\u00b10.1\n55.3\u00b10.4\n37.8\u00b10.3\n50.1\u00b10.3\nOurs\nAMBIG-4\n+GOLD\n45.1\u00b10.6\n64.1\u00b10.2\n54.6\u00b10.4\n56.4\u00b10.1\n41.4\u00b10.3\n51.3\u00b10.2\n+GOLD+MIS\n48.0\u00b10.4\n62.1\u00b10.9\n54.9\u00b10.1\n57.3\u00b10.1\n40.9\u00b10.1\n51.0\u00b10.4\n+GOLD+MIS+PRED\n49.5\u00b10.4\n63.1\u00b10.3\n55.6\u00b10.4\n57.7\u00b10.0\n42.7\u00b10.2\n51.7\u00b10.4\nAMBIG-8\n+GOLD\n43.6\u00b10.1\n64.0\u00b10.2\n55.0\u00b10.1\n56.5\u00b10.6\n41.9\u00b10.9\n50.8\u00b10.5\n+GOLD+MIS\n47.3\u00b10.4\n61.8\u00b10.3\n54.9\u00b10.1\n57.3\u00b10.1\n44.4\u00b10.2\n51.4\u00b10.3\n+GOLD+MIS+PRED\n48.0\u00b10.4\n61.7\u00b10.2\n55.6\u00b10.2\n56.7\u00b10.2\n43.2\u00b10.3\n51.3\u00b10.1\nTable 9: Recall (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).\nEDOS\nSST\nGoEmotions\nM\nL\nM\nL\nM\nL\nBaselines\nFREQ\n46.8\n46.8\n23.1\n23.1\n11.7\n11.7\nZERO\n55.4\n59.2\n49.9\n57.1\n47.1\n46.2\nSTATIC-N\n54.3\u00b10.3\n57.6\u00b10.1\n50.5\u00b10.4\n59.0\u00b10.2\n39.8\u00b10.2\n46.7\u00b10.2\nRETR-4\n53.6\u00b10.3\n61.0\u00b10.5\n50.0\u00b10.3\n58.5\u00b10.3\n45.9\u00b10.0\n50.1\u00b10.2\nRETR-8\n53.8\u00b10.3\n61.1\u00b10.2\n51.8\u00b10.1\n58.6\u00b10.4\n45.3\u00b10.0\n51.0\u00b10.2\nOurs\nAMBIG-4\n+GOLD\n54.3\u00b10.4\n61.3\u00b10.4\n51.5\u00b10.4\n58.9\u00b10.1\n46.6\u00b10.2\n50.3\u00b10.1\n+GOLD+MIS\n56.1\u00b10.2\n60.9\u00b10.6\n52.1\u00b10.1\n59.7\u00b10.1\n45.6\u00b10.1\n49.5\u00b10.1\n+GOLD+MIS+PRED\n56.5\u00b10.1\n61.4\u00b10.4\n53.0\u00b10.4\n60.1\u00b10.1\n45.6\u00b10.1\n50.0\u00b10.2\nAMBIG-8\n+GOLD\n53.6\u00b10.2\n61.8\u00b10.0\n52.9\u00b10.1\n59.5\u00b10.6\n46.8\u00b10.1\n50.4\u00b10.2\n+GOLD+MIS\n55.4\u00b10.6\n61.1\u00b10.3\n53.2\u00b10.1\n60.2\u00b10.1\n45.8\u00b10.2\n50.0\u00b10.3\n+GOLD+MIS+PRED\n55.1\u00b10.6\n61.5\u00b10.3\n54.2\u00b10.2\n59.6\u00b10.3\n44.9\u00b10.2\n50.2\u00b10.2\nTable 10: Accuracy (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).\nEDOS\nSST\nGoEmotions\nFlan-PaLM 2 (M)\n91.2\n85.8\n61.2\nFlan-PaLM 2 (L)\n88.2\n87.6\n61.6\nTable 11: Percentage of times the test example\u2019s gold\nlabel is in Lambig,t (as obtained from ZERO model).\nEDOS\nM\nAnimosity 99.1, Derogation 97.4, Prejudiced 52.1, Threat 71.9\nL\nAnimosity 90.1, Derogation 90.1, Prejudiced 68.1, Threat 93.3\nSST\nM\nBad 78.4, Good 98.0, Great 88.0, Okay 73.8, Terrible 93.9\nL\nBad 89.3, Good 99.2, Great 99.2, Okay 59.1, Terrible 85.3\nGoEmotions\nM\nAdmiration 72.7, Amusement 90.3, Anger 58.8, Annoyance 44.3, Approval 24.6, Caring 52.9, Confu-\nsion 73.2, Curiosity 64.2, Desire 35.7, Disappointment 58.0, Disapproval 52.3, Disgust 55.3, Embarrass-\nment 30.4, Excitement 56.4, Fear 75.4, Gratitude 75.7, Grief 100.0, Joy 80.4, Love 86.8, Nervousness\n83.3, Optimism 51.4, Pride 42.9, Realization 27.0, Relief 28.6, Remorse 38.6, Sadness 71.6, Surprise\n62.1\nL\nAdmiration 40.2, Amusement 84.9, Anger 52.7, Annoyance 40.2, Approval 36.0, Caring 36.5, Confu-\nsion 74.2, Curiosity 64.8, Desire 58.9, Disappointment 59.1, Disapproval 81.0, Disgust 38.2, Embarrass-\nment 30.4, Excitement 61.8, Fear 73.8, Gratitude 88.4, Grief 100.0, Joy 84.8, Love 86.2, Nervousness\n83.3, Optimism 65.4, Pride 71.4, Realization 41.6, Relief 42.9, Remorse 70.5, Sadness 67.6, Surprise\n64.4\nTable 12: Label-wise percentage of times the test example\u2019s gold label is in Lambig,t (as obtained from ZERO),\nwhere M and L refers to Flan-PaLM 2 sizes.\nEDOS\nSST\nM\nL\nM\nL\nAMBIG-4\n+GOLD\n50.2\n62.9\n51.6\n55.8\n+GOLD+MIS\n51.2\n62.7\n53.0\n57.0\n+GOLD+MIS+PRED\n53.4\n63.8\n52.7\n57.7\nAMBIG-8\n+GOLD\n48.1\n63.3\n53.2\n56.5\n+GOLD+MIS\n50.4\n62.9\n53.6\n57.4\n+GOLD+MIS+PRED\n50.3\n62.9\n54.3\n57.1\nTable 13: F1 macro scores (%) of our method. M and L\nrefers to size of Flan-PaLM 2. The ICL demonstrations\nare sorted by increased entropy order.\nEDOS\nSST\nM\nL\nM\nL\nAMBIG-4\n+GOLD\n0.9\n0.3\n0.1\n-0.3\n+GOLD+MIS\n-1\n1\n0.7\n-0.4\n+GOLD+MIS+PRED\n-0.5\n0.9\n-0.6\n-0.3\nAMBIG-8\n+GOLD\n0.6\n0.1\n0.3\n0\n+GOLD+MIS\n0\n0.9\n0.2\n-0.3\n+GOLD+MIS+PRED\n-0.6\n0.2\n0\n-0.1\nTable 14: The difference of F1 macro scores (%) be-\ntween the \u201cincreased entropy order\u201d and the \u201caveraged\nover 3 random seeds\u201d.\nTest Example: A hip ride into hyper-time, Clockstoppers is a lively and enjoyable adventure for all ages at any time.\nLambig,t: Great, Good\nGold label: Great\nRETR\n1. Bad: See Clockstoppers if you have nothing better to do with 94 minutes.\n2. Bad: Time stands still in more ways that one in Clockstoppers, a sci-fi thriller as\nlazy as it is interminable.\n3. Bad: Clockstoppers is one of those crazy, mixed-up films that doesn\u2019t know what\nit wants to be when it grows up.\n4. Good: Even with all its botches, Enigma offers all the pleasure of a handsome\nand well-made entertainment.\nPredicted:\nGood\nAMBIG-ICL\n+GOLD\n1. Good: Even with all its botches, Enigma offers all the pleasure of a handsome\nand well-made entertainment.\n2. Great: A breathtaking adventure for all ages, Spirit tells its poignant and uplifting\nstory in a stunning fusion of music and images.\n3. Great: A rollicking ride, with jaw-dropping action sequences, striking villains, a\ngorgeous color palette, astounding technology, stirring music and a boffo last hour\nthat leads up to a strangely sinister happy ending.\n4. Great: This gorgeous epic is guaranteed to lift the spirits of the whole family.\nPredicted:\nGreat\n+GOLD+MIS\n1. Good: As action-adventure, this space-based homage to Robert Louis Stevenson\u2019s\nTreasure Island fires on all plasma conduits.\n2. Good: Horns and Halos benefits from serendipity but also reminds us of our own\nresponsibility to question what is told as the truth.\n3. Great: Return to Never Land is reliable, standard Disney animated fare, with\nenough creative energy and wit to entertain all ages.\n4. Great: It\u2019s a smart, solid, kinetically-charged spy flick worthy of a couple hours\nof summertime and a bucket of popcorn.\nPredicted:\nGreat\n+GOLD+MIS+PRED\n1. Good: As action-adventure, this space-based homage to Robert Louis Stevenson\u2019s\nTreasure Island fires on all plasma conduits.\n2. Good: Horns and Halos benefits from serendipity but also reminds us of our own\nresponsibility to question what is told as the truth.\n3. Great: Return to Never Land is reliable, standard Disney animated fare, with\nenough creative energy and wit to entertain all ages.\n4. Great: It\u2019s a smart, solid, kinetically-charged spy flick worthy of a couple hours\nof summertime and a bucket of popcorn.\nPredicted:\nGreat\nTable 15: Example demonstrations selected by the RETR and our proposed method AMBIG-ICL for the SST task,\nfor n = 4. The model used here for prediction is Flan-PaLM 2 (L).\n"
  },
  {
    "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
    "link": "https://arxiv.org/pdf/2309.07462.pdf",
    "upvote": "3",
    "text": "Are Large Language Model-based Evaluators the Solution to Scaling Up\nMultilingual Evaluation?\nRishav Hada\u2660\nVarun Gumma\u2660\nAdrian de Wynter\u2660\nHarshita Diddee\u2661\u2217\nMohamed Ahmed\u2660\nMonojit Choudhury\u2662\u2217\nKalika Bali\u2660\nSunayana Sitaram\u2660\n\u2660Microsoft Corporation\n\u2661Carnegie Mellon University\n\u2662MBZUAI\nrishavhada@gmail.com, sunayana.sitaram@microsoft.com\nAbstract\nLarge Language Models (LLMs) excel in vari-\nous Natural Language Processing (NLP) tasks,\nyet their evaluation, particularly in languages\nbeyond the top 20, remains inadequate due to\nexisting benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score\nother models\u2019 outputs emerges as a viable so-\nlution, addressing the constraints tied to hu-\nman annotators and established benchmarks.\nIn this study, we explore the potential of LLM-\nbased evaluators, specifically GPT-4 in enhanc-\ning multilingual evaluation by calibrating them\nagainst 20K human judgments across three text-\ngeneration tasks, five metrics, and eight lan-\nguages. Our analysis reveals a bias in GPT4-\nbased evaluators towards higher scores, under-\nscoring the necessity of calibration with native\nspeaker judgments, especially in low-resource\nand non-Latin script languages, to ensure ac-\ncurate evaluation of LLM performance across\ndiverse languages.\n1\nIntroduction\nLarge Language Models (LLMs) can achieve re-\nmarkable results on a variety of tasks, sometimes\neven outperforming humans on certain tasks and\ndomains (OpenAI, 2023; Chen and Ding, 2023;\nVeen et al., 2023; Chiang and Lee, 2023). However,\nmeasuring the performance of LLMs is challeng-\ning, as standard NLP benchmarks may not reflect\nreal-world applications. Other hurdles for LLM\nevaluation include the scarcity of benchmarks for\ndiverse and complex tasks, benchmark saturation,\ncontamination of benchmark data in LLM train-\ning data, and the weak correlation between auto-\nmated metrics and human judgment (Jacovi et al.,\n2023; Chang et al., 2023; Reiter, 2018; Liu and Liu,\n2008). Therefore, researchers have proposed alter-\nnative evaluation methods that go beyond bench-\nmarking to assess the abilities and limitations of\nLLMs (Chang et al., 2023).\n\u2217Work done when the author was at Microsoft\nEvaluation\nInstructions\nGeneration\nInstructions\nCalibration\nData\nPoints\nLLM\nOutputs\nLLM\nGenerator\nHuman\n     Annotators\n          GPT-4\n          GPT-4\n           Scores\nHuman\nScores\nFigure 1: Pipeline of our experiments involving genera-\ntion, evaluation, and calibration.\nWhile LLMs excel at various tasks in English,\ntheir capabilities in other languages are more lim-\nited. This disparity may increase the digital divide,\npreventing a significant portion of the global popu-\nlation from benefiting from LLMs and potentially\nharming them. Ahuja et al. (2023a,b) conduct a\ncomprehensive benchmarking of LLMs across the\navailable multilingual benchmarks covering sev-\neral tasks and languages, and show that the per-\nformance of LLMs degrades significantly on lan-\nguages that are transcribed in non-Latin scripts and\nunder-resourced languages.\nMultilingual evaluation is challenging to scale.\nCertain language families, such as Indo-European,\nare over-represented in multilingual benchmarks\nwith other language families having very little pres-\nence. There is a scarcity of multilingual bench-\nmarks designed to assess tasks that simulate actual\nLLM usage in real-world scenarios. The metrics\nused in these benchmarks may be unsuitable for\nlanguages with rich morphology or complex writ-\narXiv:2309.07462v2  [cs.CL]  13 Feb 2024\ning systems, as well as phenomena arising from\nlanguage contact such as borrowing, code-mixing,\nand transliteration. Evaluation by native speakers\nis the gold standard for building an accurate picture\nof model performance, especially in complex tasks\nwithout well-defined automated metrics. However,\nbudget constraints, turnaround time, and the lack of\neasy access to native speakers in some languages all\npose challenges in scaling evaluation. This leads\nto a situation in which LLM performance is un-\nknown for most languages of the world (Ahuja\net al., 2022).\nThe success of LLMs in complex tasks such\nas sentiment analysis, reasoning, problem-solving\n(Mao et al., 2023; Arora et al., 2023), and provid-\ning feedback for reducing LLM harms (Bai et al.,\n2022) has led to the question of whether LLMs can\nreplace human annotators, or help augment human\nevaluation (Gilardi et al., 2023). Utilizing LLMs as\nmultilingual evaluators is, therefore, an attractive\noption to decrease costs and circumvent the chal-\nlenges of scaling assessments by native speakers.\nHowever, LLMs have been demonstrated to have\ninferior performance even in some high-resource\nlanguages and have not been evaluated extensively\nacross many languages on dimensions such as tox-\nicity, fairness, and robustness (due to the absence\nof such benchmarks) (Ahuja et al., 2023a), it is\nprudent to proceed with caution. Failing to do so\ncan lead to misleading results which may further\nwiden the digital divide.\nIn this work, we study whether LLM-based eval-\nuation can be the answer to scaling up multilingual\nevaluation. In other words, can LLMs serve as sub-\nstitutes or supplements for human native speakers\nin delivering useful and accurate insights regard-\ning LLM outputs in non-English languages, while\nconsidering diverse aspects of interest like linguis-\ntic acceptability, task accomplishment, and safety?\nOur main contributions are as follows:\n1. We present the first evaluation of LLMs,\nspecifically GPT-4 as multilingual evaluators\nto examine whether LLMs can be used to scale\nup multilingual evaluation.\n2. We calibrate LLM judgments on an in-house\ndataset across three tasks, eight languages,\nand five dimensions by comparing them to\nover 20K human judgments on the same tasks,\nlanguages, and dimensions.\n3. We evaluate a variety of prompting strategies\nfor LLM-based evaluation in the multilingual\nsetting.\n4. We provide a framework for evaluating LLM-\nevaluators in the multilingual setting that\ncan generalize across tasks, metrics, and lan-\nguages1.\n5. We suggest best practices and provide recom-\nmendations for future work.\n2\nRelated Work\nBroadly, there are two main uses of LLMs as evalu-\nators: LLMs can be used as alternatives to metrics\nthat compare human and machine-generated text,\nsuch as BLEU (Papineni et al., 2002) and ROUGE\n(Lin, 2004). Word overlap-based metrics are lim-\nited, and LLM-based scorers have been shown to\noutperform them. GPTScore (Fu et al., 2023) is a\npopular LLM-based framework that can be used\nto score model outputs based on human-created\nreferences along various dimensions. However,\nthese scores still rely on having examples of human-\ncreated reference data.\nThe second use case of LLMs as evaluators is\nwhen the LLM is presented with the output of a sys-\ntem (usually an LLM, sometimes the same model)\nand asked to judge its quality or safety without\nany human output to compare against (Zheng et al.,\n2023). The LLM is instructed on how to perform\nthis evaluation with the help of the task description,\nevaluation rubric, and sometimes, one or more ex-\namples in the prompt. This is the use case we focus\non in this work.\nGilardi et al. (2023) prompt ChatGPT to anno-\ntate Tweets across various dimensions such as topic\nand stance and find that it outperforms crowdwork-\ners. Shen et al. (2023) explore the use of GPT3.5\nas an evaluator for abstractive summarization and\nfind that although GPT is a useful evaluator, as\nthe quality of summarization improves, the qual-\nity of evaluation degrades. Along similar lines,\nWang et al. (2023a) evaluate ChatGPT on various\nNLG tasks and find that it has a high correlation\nwith human judgments. Kocmi and Federmann\n(2023) evaluate the effectiveness of LLMs on eval-\nuation of translation quality and find that LLMs\nstarting from GPT3.5 and above achieve SOTA\nperformance on translation evaluation benchmarks.\nFernandes et al. (2023) leverage LLMs for fine-\ngrained annotation of errors in Machine Transla-\ntion outputs. LLM-based evaluators have also been\nused to score and refine outputs they produce, as\ndescribed in Madaan et al. (2023), ultimately pro-\n1Code available at: https://aka.ms/LLM-Eval\nducing outputs that are scored higher on human\nand automated metrics than the original outputs.\nNaismith et al. (2023) explore the use of LLM-\nbased evaluators on scoring written discourse for\ncoherence and find a strong correlation with human\njudgments. The success of LLM-based evaluators\nhas led many to question whether LLM-based eval-\nuation can replace or augment human evaluation\n(Chiang and Lee, 2023).\nHowever, there have been studies showing that\nLLM-based evaluators can have some biases. Wu\nand Aji (2023) demonstrate that LLMs tend to pre-\nfer answers with factual errors when they are too\nshort or contain grammatical errors. Pangakis et al.\n(2023) highlight the need for validating LLM-based\nevaluators on a task-by-task basis. Liu et al. (2023)\nperform NLG evaluation using GPT-4 and find that\nalthough it correlates well with human judgments,\nit may potentially be biased towards preferring\nLLM-generated texts. Koo et al. (2023) show that\nLLMs have egocentric bias where they prefer to\nrank their own outputs highly in evaluation. Wang\net al. (2023b) point out that GPT4-based evaluators\nhave positional bias and scores can be easily altered\nby changing the order of appearance. There are also\nseveral ethical issues with the use of LLMs as eval-\nuators described in Chiang and Lee (2023). Zhang\net al. (2023) suggest that wider and deeper LLMs\nare fairer evaluators, while Chan et al. (2023) in-\ntroduce a framework for multiple evaluator agents\nto reach a consensus, mimicking the situation of\nhaving multiple annotators.\nAlthough there has been some work measuring\nthe calibration of LLM-based evaluators to human\njudgments (Koo et al., 2023), previous studies have\nfocused on English, and ours is the first work (to the\nbest of our knowledge) that addresses this problem\nin the multilingual context.\n3\nExperimental Setup\nWe perform experiments on a text generation appli-\ncation that is powered by GPT-4, and evaluate the\nfollowing sub-tasks:\nOpen Prompt: This task processes a concise\nprompt to generate a document adhering to the\nprovided guidelines, producing up to 2, 048 tokens,\napproximately equivalent to one page in English or\nSpanish, and marginally less in other languages.\nContinue Writing: This task takes two textual\ninputs, termed \u201cleft\u201d and \u201cright\u201d to generate a co-\nherent continuation between them, accommodating\nup to 1, 000 tokens. Notably, one of the inputs may\nbe omitted.\nSummarize: Engages in standard summarization\nby condensing a document of at least 500 words\ninto a succinct summary. It allows for an optional\nuser-defined prompt to tailor the summary format,\nsuch as highlighting key points.\nWe cover the following languages: English (En),\nFrench (Fr), German (De), Spanish (Es), Chinese\n(Zh), Japanese (Ja), Italian (It), Brazilian Por-\ntuguese (Pt-Br), and Czech (Cs). Of these, the first\nsix are classified as very high resource languages\n(Class 5, or \u201cthe winners\u201d), while the last three are\nclassified as Class 4 (\u201cthe underdogs\u201d) according to\nJoshi et al. (2020). We plan to extend our study to\nlower-resource languages in the future. We study\nthe following dimensions of interest:\nLinguistic Acceptability (LA): This measures\nwhether the text sounds right to a native speaker.\nThe values of this metric are {0, 1, 2}, with 0 cor-\nresponding to not acceptable, 1 corresponding to\nsome errors, but acceptable and 2 to perfectly ac-\nceptable. We chose LA as opposed to grammati-\ncality to ensure a comparable, native-speaker-led\nevaluation that did not require formal training in\nthe language.\nOutput Content Quality (OCQ): Whether the\ngeneral quality of the content is good or not, with\nvalues {0, 1, 2}. A score of 0 could indicate that\nthe output is in the wrong language, is repetitive,\nor sounds like it has been scraped from the web, or\ntranslated. A score of 1 indicates that the output\nis okay in terms of grammar and word choice but\nstill sounds awkward in the language. A score of 2\nindicates that the text is of high quality.\nTask Quality (TQ): This measures the ability of\nthe model to follow the given instructions in the\nprompt. The values of this metric are {0, 1, 2},\nwith 0 indicating that the model did not follow the\ninstructions at all. Likewise, a score of 1 indicates\nthat the model followed the instructions approxi-\nmately well and 2 that it followed perfectly well.\nThe difference between TQ and OCQ is that the\nlatter focuses on whether the content is appealing\nto a user, while TQ emphasizes the ability of the\nmodel to follow the given instructions.\nProblematic Content (PC): Whether there was\nany offensive or problematic content in the output.\nThis is a binary metric, with 0 indicating that the\noutput contains this type of content.\nHallucinations (H): This measures how well-\ngrounded the model\u2019s output was to the input con-\ntent, and/or whether the model output counterfac-\ntual information conflicted with the input content.\nIt is a binary metric, with 0 indicating the presence\nof hallucinations.\n3.1\nHuman Evaluation Setup\nFor creating this in-house dataset, we asked hu-\nman judges to evaluate the output of LLM-based\nsystems configured to perform the three tasks de-\nscribed earlier. Each entry was annotated by three\nannotators. They were contracted through an ex-\nternal annotator services company at a starting rate\ndepending on locale ranging from $14 USD/hr and\nup to $30 USD/hr. The pay was adjusted based on\nlocale and experience level. Each annotator was\ngiven 250 texts to judge. We used a subset of the\nannotated data for our experiments.\n3.1.1\nAnnotation Guidelines\nWe provided annotators with the following infor-\nmation: General instructions about the task (in-\ncluding specific instructions from the prompt) and\nhigh-level descriptions of the metrics that we are\nseeking to evaluate, a description of the file that\ncontained data to be evaluated, and the output for-\nmat expected. Then we provided detailed descrip-\ntions of each metric including the range of values\nfor each metric and examples in English. These\nexamples were provided in the context of different\ntasks, as each metric could have slightly different\ninterpretations for different tasks.\n3.1.2\nData Statistics\nTable 1 contains the statistics of the human evalua-\ntion dataset for the three tasks across the languages\nwe consider. We create a subset of this data for\nexperimenting with prompting variations and its\nstatistics are available in the small column of the\naforementioned table. Our full dataset contains\nover 7, 300 data points, while the smaller subset\ncontains over 2, 700 data points. Each of the data\npoints in our dataset was annotated by 3 annotators.\n3.2\nLLM-based Evaluators\nWe use the GPT4-32K model as our LLM-based\nevaluator with a temperature of 0, except in our\nablation experiments. The model was accessed\nthrough Azure.\nLang.\nOpen\nPrompt\nSummarize\nContinue\nWriting\nAgg.\nFull\nSmall\nFull\nSmall\nFull\nSmall\nFull\nSmall\nCa\n255\n100\n158\n100\n325\n-\n738\n200\nDe\n246\n94\n251\n100\n320\n96\n817\n290\nEn\n200\n200\n200\n200\n200\n200\n600\n600\nEs\n247\n93\n257\n100\n593\n102\n1097\n295\nFr\n221\n88\n256\n99\n409\n97\n886\n284\nIt\n256\n99\n260\n100\n321\n100\n837\n299\nJa\n257\n100\n259\n100\n316\n102\n832\n302\nPt-Br\n246\n94\n258\n100\n327\n95\n831\n289\nZh\n255\n100\n160\n99\n320\n-\n735\n199\nAgg.\n2183\n968\n2059\n998\n3131\n792\n7373\n2758\nTable 1: Dataset statistics across tasks and languages.\n3.2.1\nPrompts\nOur evaluation prompts are constructed using the\n{{guidance}} toolkit2. guidance is a DSL that\nuses handlebar templating to enable the specifica-\ntion of prompts that interleave instructions and gen-\neration with data and logic. This makes it simpler\nto construct and validate complex prompts.\nEvaluation prompts were written to be clear,\nsimple, and not tuned for the data or task. All\nprompts for evaluation were specified in English,\nas past work has shown that instructions in native\nlanguages can lead to worse performance (Ahuja\net al., 2023a).\nIn writing the evaluation prompts, we started\nwith simple unstructured specifications (Natural\nlanguage sentences with no formatting or styling)\nand found that it often led to errors in formatting the\noutputs correctly or even returning all the expected\noutputs. We found adding styling and formatting,\nfor example, outputting JSON by providing the\nprompt with a JSON schema for the expected at-\ntributes improved the reliability of the LLM out-\nputs.\nWe tried to keep the task and metric descrip-\ntion as close as possible to the text that was shown\nto human annotators for evaluations in the default\nprompting variation. Each prompt consists of SYS-\nTEM, USER, and ASSISTANT components as shown\nin Figure 2 in a generic prompt schema. The metric\ndescription for Hallucinations is shown in Figure\n33.\n2https://github.com/guidance-ai/guidance/tree/\nmain\n3Prompts for task description and other metrics are in\nAppendix A.1.\n\u27e8system\u27e9\n# [system](#instructions)\n# Role\nYou are a helpful assistant.\n## Task\nDescription of the task\n### Outputs\nDescription and JSON format of expected outputs\n\u27e8/system\u27e9\n\u27e8user\u27e9\nInputs\n\u27e8/user\u27e9\n\u27e8system\u27e9\n# [system](#instructions)\nInstruction related to evaluation and metrics\n### Metrics\nDescription of the metrics in JSON format\n\u27e8/system\u27e9\n\u27e8assistant\u27e9\nGeneration space for GPT-4\n\u27e8/assistant\u27e9\nFigure 2: General Prompting Schema.\n\u201cname\": \u201challucinations\",\n\u201cdescription\": \u201cHallucination refers to the generation of text that\nis untrue, fabricated, inconsistent with the given input, deviates\nfrom generally accepted knowledge, or makes unverifiable claims.\",\n\u201cscoring\":\n\u201c1:\nNo\nhallucinations\nin\nthe\ntext;\n0:\ntext\nhas\nhallucinations\"\nFigure 3: Metric description for simple instructions\n(Hallucinations).\n3.3\nPrompting Variations\nFirst, we experiment with variations based on the\nnumber of metrics evaluated and instructions pro-\nvided4.\nSingle Call: In this variation, we call GPT-4 once\nper metric, without any in-context examples.\nCompound Call: In this variation, we call GPT-4\nonce for all the metrics in a single prompt.\nSingle Call - Detailed: In this variation, we call\nGPT-4 once for all the metrics in a single prompt,\nwith a very detailed metrics description.\nOne of the challenges with LLM evaluation is sensi-\ntivity to prompting instructions, which can greatly\naffect the performance of the LLM on tasks, in-\ncluding evaluation. We experiment with providing\ndetailed instructions for each metric in the prompt.\nDetailed instruction for Hallucination is shown in\nFigure 45. We queried GPT-4 to produce these\n4All experiments reported in this study are conducted zero-\nshot unless specified.\n5The detailed instructions for all metrics can be found in\nFigures 15 - 18 in Appendix A.2\ninstructions by providing it with the instructions\ngiven to annotators and manually modifying them.\n3.4\nCalibration with Human Judgments\nInter-annotator Agreement Analysis: We as-\nsessed inter-annotator agreement (IAA) among\nthree annotators Annot1,Annot2,Annot3 using\nPercentage Agreement (PA) to determine the pro-\nportion of data points with consistent annotations\nacross annotators. Weighted F1 scores are docu-\nmented in Table 2. Additionally, Fleiss\u2019 Kappa (\u03ba)\nvalues, which offer insights into agreement beyond\nchance, are provided in Table 3 (Appendix A.3).\nSince our dataset is skewed towards one or more\nclasses for each of the metrics, \u03ba values can be\nmisleading due to known issues with computing\nexpected agreement in such cases (Eugenio and\nGlass, 2004).\nIAA (3 annotators) and GPT: We measure IAA\nbetween the majority score of the three annota-\ntors and the LLM-evaluator. We refer to this as\nAnnotAgg,GPT4 and use PA to measure it.\nClass distribution: We analyze the class distribu-\ntion of scores across tasks, metrics, and languages\nto check for potential biases in the dataset and\nLLM-evaluator.\nWe perform experiments contrasting compound\nand single-call prompting on the full dataset and\nzero-shot vs. few-shot prompting on the smaller\ndataset. We analyze how well-calibrated our LLM-\nbased evaluators are with respect to human judg-\nments by examining PA, and class distribution of\nscores.\n3.5\nAblation Experiments\nIn addition, we perform some ablation experiments\nto check for consistency, the effect of hyperparam-\neters, and few-shot examples. We perform these\nablations on the smaller dataset.\nConsistency check: We prompt GPT-4 with the\nsame prompt five times to check its consistency.\nSingle Call \u2013 Few-Shot: In this variation, we call\nGPT-4 once per metric, with a few in-context exam-\nples. We provide examples in the prompt of human\njudgments for the same task and metric from a\nheld-out dev set. We take the majority vote from\nthe three human annotations per sample as the ag-\ngregate class for that sample to choose our few-shot\nexamples. For each task, language, and metric we\nchoose up to two samples per possible class for that\nmetric. Therefore, we have a minimum of two and\na maximum of six exemplars as few-shot examples.\n\u201cname\": \u201challucinations\",\n\u201cdescription\": \u201cHallucinations assess the extent to which a model\u2019s output remains anchored to, and consistent with, the input content provided.\nText with hallucinations while linguistically fluent, are factually baseless or counterfactual in relation to the input. These hallucinations can\nmanifest as additions, omissions, or distortions, and might lead to outputs that are misleading or factually incorrect. This metric serves as\na check against unwarranted deviations from the ground truth provided in the input. The scoring rubric is described below, with a few possible\nreasons (which might not be exhaustive) for a given score.\",\n\"scoring\": {\n\"1\": {\n\"(a)\": \"The model's output is strictly aligned with and grounded in the information provided in the input.\",\n\"(b)\": \"No evidence of added, omitted, or distorted facts that weren't part of the original content.\",\n\"(c)\": \"Maintains the integrity of the original information without any unwarranted extrapolations.\"\n},\n\"0\": {\n\"(a)\": \"The output introduces statements, claims, or details that weren't present or implied in the input.\",\n\"(b)\": \"Contains counterfactual information that directly conflicts with the input content.\",\n\"(c)\": \"Demonstrates unexplained deviations, extrapolations, or interpretations not grounded in the provided data.\"\n}\n}\nFigure 4: Metric description for complex instructions (Hallucinations).\nName\nAnnot1\nAnnot2\nAnnot3\nAnnotAgg\nGPT4_joint\nAnnotAgg\nGPT4_single\nAnnotAgg\nGPT4_SD\nCs\n0.89 \u00b1 0.09\n0.81 \u00b1 0.17\n0.82 \u00b1 0.16\n0.81 \u00b1 0.17\nDe\n0.93 \u00b1 0.07\n0.92 \u00b1 0.10\n0.93 \u00b1 0.09\n0.92 \u00b1 0.09\nEn\n0.98 \u00b1 0.02\n0.97 \u00b1 0.03\n0.97 \u00b1 0.03\n0.96 \u00b1 0.04\nEs\n0.91 \u00b1 0.08\n0.88 \u00b1 0.11\n0.89 \u00b1 0.11\n0.88 \u00b1 0.11\nLang.\nFr\n0.94 \u00b1 0.05\n0.90 \u00b1 0.10\n0.90 \u00b1 0.10\n0.90 \u00b1 0.10\nIt\n0.94 \u00b1 0.07\n0.91 \u00b1 0.11\n0.92 \u00b1 0.10\n0.91 \u00b1 0.11\nJa\n0.91 \u00b1 0.08\n0.78 \u00b1 0.22\n0.78 \u00b1 0.21\n0.78 \u00b1 0.22\nPt-Br\n0.96 \u00b1 0.04\n0.91 \u00b1 0.10\n0.91 \u00b1 0.10\n0.90 \u00b1 0.10\nZh\n0.89 \u00b1 0.10\n0.83 \u00b1 0.16\n0.83 \u00b1 0.16\n0.83 \u00b1 0.16\nH\n0.98 \u00b1 0.03\n0.96 \u00b1 0.04\n0.96 \u00b1 0.04\n0.96 \u00b1 0.04\nLA\n0.92 \u00b1 0.06\n0.88 \u00b1 0.13\n0.89 \u00b1 0.12\n0.88 \u00b1 0.12\nMetric\nOCQ\n0.86 \u00b1 0.08\n0.80 \u00b1 0.12\n0.80 \u00b1 0.12\n0.80 \u00b1 0.12\nPC\n1.00 \u00b1 0.01\n1.00 \u00b1 0.01\n1.00 \u00b1 0.01\n1.00 \u00b1 0.01\nTQ\n0.88 \u00b1 0.06\n0.76 \u00b1 0.15\n0.76 \u00b1 0.16\n0.75 \u00b1 0.16\nContinue\nWriting\n0.94 \u00b1 0.07\n0.88 \u00b1 0.14\n0.88 \u00b1 0.14\n0.88 \u00b1 0.15\nTask\nOpen\nPrompt\n0.91 \u00b1 0.08\n0.83 \u00b1 0.16\n0.84 \u00b1 0.16\n0.83 \u00b1 0.16\nSummarize\n0.94 \u00b1 0.07\n0.93 \u00b1 0.09\n0.93 \u00b1 0.09\n0.93 \u00b1 0.09\nTable 2: Weighted F1 values for different cases and annotator combinations on the full dataset. GPT4_SD means\nGPT4_single_detailed\n.\nFor all evaluations, the few-shot examples used are\nfixed.\nSensitivity analysis: We check the sensitivity of\nthe Linguistic Acceptability metric evaluation by\nrandomly shuffling 10% of the words in the whole\ntext for all instances and checking if the LA score\nprovided by the model changes.\nTemperature variation: We vary the temperature\nparameter to check its effect on LLM evaluation.\n4\nResults\n4.1\nPercentage Agreement\nIn this set of graphs, we look at the percentage\nagreement between LLM-evaluator and the annota-\ntors, and between the annotators. We aggregate the\nresults by task, metric, and language.\nFigure 5a shows the percentage agreement be-\ntween the aggregate of the human annotator scores\nand LLM-evaluator for the full dataset. The figures\nshow both joint (compound), single, and single\nwith detailed instructions prompting techniques for\nthe full dataset. We see that the PA between the\nannotators and GPT is lowest compared to the PA\nbetween the human annotators for Japanese and\nCzech, with the PA between annotators also being\nlower for Chinese.\nNext, we look at PA grouped by metric in Fig-\nures 5c for the full dataset with the same prompting\nvariations as before. We find that the PA of the\nLLM-evaluator with the annotators is lower for the\nOCQ metric. We also find that the PA between an-\nnotators is relatively low for the TQ metric, while\nall the PA values are very high for the problematic\ncontent metrics.\nFinally, we look at PA aggregated by task in Fig-\nure 5b. We find that PA is lower for the \u201cContinue\nWriting\u201d task, while the PA between GPT and the\nannotators is lower than the agreement between\nannotators for the \u201cOpen Prompt\u201d and \u201cContinue\nWriting\u201d tasks. Overall, we find that the LLM-\nevaluator prompted using the compound prompt\nhas a lower agreement with human annotators than\nthe single prompt variation.\nFigures 5a, 5b and 5c compare the PA of the\nLLM-evaluators with detailed instructions vs. the\nsimpler instructions described earlier. We find that\nPA drops slightly for all metrics with detailed in-\nstructions.\n4.2\nClass Distribution\nNext, we examine the distributions of the scores\nfrom native speakers and the LLM-evaluator. There\nare three cases to consider for metrics that have\nthree values: Full agreement (all three annotators\ngive the same score), partial agreement (two of\nthe three give the same score), and no agreement\n(all three give different scores). In metrics that\nhave binary values, we only have full or partial\nagreement. We group annotations into these classes\nand analyze responses across these classes.\nWe present results for metrics that have three\nvalues (LA, OCQ, and TQ), with 0 correspond-\ning to the lowest score and 2 corresponding to the\nhighest score. In Figures 6a and 6b, we find that\nthe LLM-evaluator provides a score of 2 in most\ncases, particularly in cases where human annota-\ntors disagree. This is even more evident in the case\nof non-English languages where there is partial\nagreement or no agreement between the annotators\n(around 15% of the time on average).\nNext, we look at languages that are either lower-\nresourced or not written in the Latin script. In\nFigures 7a and 7b we find that the LLM-evaluator\nalmost never provides scores of 0 and 1 in the 26%\nof cases that annotators disagree and find similar\nresults for Japanese and Czech shown in Figures\n22e, 22f, 22g and 22h in the Appendix A.4. Overall,\nwe find that LLM-based evaluators give a score\nof 2 in most cases. While this is consistent with\nhuman evaluations in a large part of the dataset, the\nLLM-based evaluator continues to assign a score\nof 2 even when humans disagree or provide lower\nscores6.\nInterestingly, even though PA drops slightly for\nall metrics with the detailed instructions, we find\nthat the LLM-based evaluator may be slightly less\nbiased towards producing high scores with these in-\nstructions as shown in Figures 8a and 8b. However,\nmore investigation is needed to determine whether\ndetailed instructions or a different prompting strat-\negy can eliminate the bias toward high scores.\n4.2.1\nConsistency Check\nWe use a temperature of 0 and receive the same\nscore and justification in each of the five tries, show-\ning that the LLM-evaluator exhibits high consis-\ntency.\n4.2.2\nFew-shot Prompting\nFigure 24 in Appendix A.7 shows the PA values\nwhen few-shot in-context examples are provided.\nWe observe no significant changes in PA values,\nsuggesting that in-context examples might not sig-\nnificantly aid LLM-based evaluators. This also\naligns with the findings of Min et al. (2022).\n4.3\nSensitivity Analysis\nAs described earlier, we perturb the word order of\nsentences and check the sensitivity of the Linguistic\nAcceptability metric on the small dataset. Figure\n9 shows the distribution of cases per language per\ntask where the LLM-based evaluator changes its\nevaluation from a higher score to a lower score. The\nevaluator shows the most sensitivity to inputs for\nthe Summarization task for all languages except\nJapanese. For \u201cContinue Writing\u201d, Chinese and\nJapanese show very little sensitivity. For \u201cOpen\nPrompt\", Chinese and Japanese show no sensitivity\nto the perturbations. One possible explanation for\nthis could be that the evaluator is genuinely less\nsensitive to these languages. Alternatively, it might\nbe attributed to the flexible word order characteris-\ntics of Chinese and Japanese. The examination of\ntokenizer efficiency in logographic languages, and\nthe exploration of sensitivity across other metrics\ncan be an interesting future exploration.\n4.4\nTemperature Variation\nFigure 23 in Appendix A.6 show the PA values for\ntemperatures of 0, 0.3, 0.7 and 1.0. PA reduces as\nwe increase temperature, indicating that a tempera-\nture of 0 should be used for LLM-based evaluators.\n6Figures for other languages included in Appendix A.4\nand A.5.\nEn\nDe\nIt\nPt-Br\nFr\nEs\nZh\nCs\nJa\nLanguage\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 joint (PA)\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single detailed (PA)\nAnnot1,Annot2,Annot3 (PA)\n(a) PA by language: Full dataset\nSummarize\nContinue Writing\nOpen Prompt\nTask\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 joint (PA)\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single detailed (PA)\nAnnot1,Annot2,Annot3 (PA)\n(b) PA by task: Full dataset\nPC\nH\nLA\nOCQ\nTQ\nMetric\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 joint (PA)\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single detailed (PA)\nAnnot1,Annot2,Annot3 (PA)\n(c) PA by metric: Full dataset\nFigure 5: Percentage Agreement (PA) for different cases and annotator combinations.\nFull\n\u224896% samples\nPartial\n\u22483% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEn\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(a) Single Call - English\nFull\n\u224880% samples\nPartial\n\u224818% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEs\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(b) Single Call - Spanish\nFigure 6: Class distribution for En and Es. Results are aggregated over all tasks and metrics with 3 classes (LA,\nOCQ, TQ).\nFull\n\u224890% samples\nPartial\n\u22489% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(a) Single Call - Portuguese (Br)\nFull\n\u224873% samples\nPartial\n\u224825% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nZh\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(b) Single Call - Chinese\nFigure 7: Class distribution for Pt-Br and Zh. Results are aggregated over all tasks and metrics with 3 classes (LA,\nOCQ, TQ).\nWe also observe that increasing the temperature\nmakes the model more susceptible to any noise in\nthe data, making the evaluations highly stochastic\nand not reproducible.\n5\nDiscussion\nOverall, our results indicate that GPT-based eval-\nuators have relatively high consistency for non-\nEnglish languages when set to a temperature of 0.\nFull\n\u224890% samples\nPartial\n\u22489% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(a) Single call detailed - Portuguese (Br)\nFull\n\u224890% samples\nPartial\n\u22489% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(b) Single Call (simple) - Portuguese (Br)\nFigure 8: Class distribution for Pt-Br detailed and simple. Results are aggregated for all metrics with 3 classes (LA,\nOCQ, TQ).\nEn\nFr\nDe\nEs\nIt\nPt-Br\nCs\nJa\nZh\nLanguages\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentage of Sensitive Samples\nSensitivity Analysis\nOpen Prompt\nContinue Writing\nSummarize\nFigure 9: Percentage of samples where GPT evaluation\nchanged from a higher score to a lower score after per-\nturbation. Note: We do not have Chinese and Czech for\nthe Summarize task in the small dataset.\nThey also display a fair sensitivity to input varia-\ntions along the dimension of linguistic acceptability.\nWhile LLM-based evaluators show a high Percent-\nage Agreement, there is a noticeable bias towards\npositive scores, particularly when human opinions\ndiffer. It remains uncertain what score an LLM-\nbased evaluator should provide when humans can-\nnot reach a consensus, but consistently high scores\nin such situations might create a misleading im-\npression of good performance in more challeng-\ning evaluations. We find that PA and bias towards\nhigher scores are particularly evident in non-Latin\nscript languages such as Chinese and Japanese, and\nlower-resource languages such as Czech, which is\nconsistent with prior work on the performance of\nLLMs on various tasks (Ahuja et al., 2023a).\nWe experiment with several prompting strategies\nfor LLM-based evaluators and find that evaluating\na single metric at a time produces better results than\nevaluating all metrics in one go, which comes at the\ncost of having to make multiple calls to the LLM.\nWe also find that providing few-shot examples does\nnot help improve performance. We also provide\nmore detailed instructions to the LLM-evaluator\nbut find that it does not eliminate the problem of\nbias toward higher scores. In this work, we only\nuse evaluators based on GPT-4. An interesting\nfuture direction is the use of smaller models for\nevaluation or models trained with better coverage\nof non-English data. We also do not do exten-\nsive prompt tuning - future work in this direction\nincludes exploring better prompting approaches in-\ncluding automatically tuning prompts to a held-out\nset.\nOur results show that LLM-based evaluators\nmay perform worse on low-resource and non-Latin\nscript languages. Certain metrics corresponding to\noutput quality and task completion may be chal-\nlenging for LLM-based evaluators. Hence, we ad-\nvocate for a cautious approach in using LLM-based\nevaluators for non-English languages and suggest\nthat all LLM-based multilingual evaluations should\nbe calibrated with a set of human-labeled judg-\nments in each language before deployment.\n6\nLimitations\nIn this work, we utilize a dataset comprising human\nassessments of a text generation system executing\nvarious tasks in eight languages. As we do not\nregulate the quality of the system\u2019s output, most\nof the generated texts receive positive ratings from\nhuman evaluators. Consequently, the high Percent-\nage Agreement\u2019s origin remains unclear \u2013 whether\nit stems from the inclination of the LLM-evaluator\nto assign high scores or not. In future work, we\naim to replicate this study using a dataset with a\nmore balanced distribution of human judgments,\nachieved by controlling the output quality.\nIn this work, we utilize an in-house annotated\ndataset that, due to restrictions, cannot be released,\nlimiting the reproducibility of our research. How-\never, we intend to make a dataset available to the\nresearch community for calibrating LLM-based\nevaluators in the future. An important research\ndirection is the creation of datasets with good lan-\nguage coverage, multiple annotators per data point,\nand clear annotation instructions, covering a variety\nof dimensions to calibrate LLM-based evaluators.\nExploring the development of various evaluator\npersonas to represent diverse perspectives of hu-\nman evaluators and achieve consensus is another\nresearch direction that needs further investigation.\n7\nEthical Considerations\nWe use the framework by Bender and Friedman\n(2018) to discuss the ethical considerations for our\nwork.\n\u2022 Institutional Review: We used an in-house\ndataset annotated by an external company that\nhas long-standing contracts with the organi-\nzation and was employed by the organization\nregularly to do this work.\n\u2022 Data: The LLM evaluator scores were gen-\nerated using API calls to GPT-4. The dataset\nused for calibration is an in-house dataset that\nwill not be released publicly. The dataset was\nnot created with the intent of studying hu-\nman and LLM calibration; hence, it is not\na balanced dataset. Specific instructions were\nprovided to LLMs to avoid generating prob-\nlematic content, and our ratings of the Prob-\nlematic Content metrics show no such data;\nhowever, the possibility still exists.\n\u2022 Annotator Demographics: Annotators were\nrecruited through an external annotator ser-\nvices company. The pay was adjusted after\ndeliberation with the company, based on the\nannotator\u2019s location and expertise. No demo-\ngraphic information is available about the an-\nnotators. The annotators are governed by their\ncompany\u2019s and our organization\u2019s privacy pol-\nicy.\n\u2022 Annotation Guidelines: We draw inspiration\nfrom the community standards set for simi-\nlar tasks. Annotators were given general in-\nstructions about the task, detailed instructions\nabout the metrics to be evaluated, and exam-\nples in English.\n\u2022 Methods: In this study, we explore several\nmethods of calibrating human judgments with\nLLM judgments on various tasks and lan-\nguages. While these methods can be misused\nto replace human judgments with LLM judg-\nments, our intent with this study is to highlight\nthe gap between the two and urge the commu-\nnity to proceed with caution.\nReferences\nKabir Ahuja, Sandipan Dandapat, Sunayana Sitaram,\nand Monojit Choudhury. 2022. Beyond static models\nand test sets: Benchmarking the potential of pre-\ntrained models across tasks and languages. NLP-\nPower 2022, 10(12):64.\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed\nAhmed, Kalika Bali, and Sunayana Sitaram. 2023a.\nMEGA: Multilingual evaluation of generative AI.\nIn Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, pages\n4232\u20134267, Singapore. Association for Computa-\ntional Linguistics.\nSanchit Ahuja, Divyanshu Aggarwal, Varun Gumma,\nIshaan Watts, Ashutosh Sathe, Millicent Ochieng,\nRishav Hada, Prachi Jain, Maxamed Axmed, Ka-\nlika Bali, and Sunayana Sitaram. 2023b.\nMega-\nverse: Benchmarking large language models across\nlanguages, modalities, models and tasks.\nDaman Arora, Himanshu Singh, and Mausam. 2023.\nHave LLMs advanced enough? a challenging prob-\nlem solving benchmark for large language models.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7527\u20137543, Singapore. Association for Computa-\ntional Linguistics.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587\u2013604.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan\nLiu. 2023. Chateval: Towards better llm-based eval-\nuators through multi-agent debate. arXiv preprint\narXiv:2308.07201.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2023. A sur-\nvey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109.\nHonghua Chen and Nai Ding. 2023. Probing the \u201ccre-\nativity\u201d of large language models: Can models pro-\nduce divergent semantic association? In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12881\u201312888, Singapore. Asso-\nciation for Computational Linguistics.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607\u201315631, Toronto,\nCanada. Association for Computational Linguistics.\nBarbara Di Eugenio and Michael Glass. 2004. The\nkappa statistic: A second look. Computational lin-\nguistics, 30(1):95\u2013101.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein,\nParker Riley, Andr\u00e9 FT Martins, Graham Neubig,\nAnkush Garg, Jonathan H Clark, Markus Freitag,\nand Orhan Firat. 2023. The devil is in the errors:\nLeveraging large language models for fine-grained\nmachine translation evaluation.\narXiv preprint\narXiv:2308.07286.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav\nGoldberg. 2023. Stop uploading test data in plain\ntext: Practical strategies for mitigating data contam-\nination by evaluation benchmarks. arXiv preprint\narXiv:2305.10160.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282\u20136293, Online. Association for Computational\nLinguistics.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\nZae Myung Kim, and Dongyeop Kang. 2023. Bench-\nmarking cognitive biases in large language models as\nevaluators.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nFeifan Liu and Yang Liu. 2008. Correlation between\nROUGE and human evaluation of extractive meeting\nsummaries. In Proceedings of ACL-08: HLT, Short\nPapers, pages 201\u2013204, Columbus, Ohio. Associa-\ntion for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nRui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin,\nand Erik Cambria. 2023.\nGpteval: A survey on\nassessments of chatgpt and gpt-4. arXiv preprint\narXiv:2308.12488.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048\u201311064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nBen Naismith, Phoebe Mulcaire, and Jill Burstein. 2023.\nAutomated evaluation of written discourse coherence\nusing GPT-4. In Proceedings of the 18th Workshop\non Innovative Use of NLP for Building Educational\nApplications (BEA 2023), pages 394\u2013403, Toronto,\nCanada. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nNicholas Pangakis, Samuel Wolken, and Neil Fasching.\n2023. Automated annotation with generative ai re-\nquires validation. arXiv preprint arXiv:2306.00176.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311\u2013318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nEhud Reiter. 2018. A structured review of the validity of\nBLEU. Computational Linguistics, 44(3):393\u2013401.\nChenhui Shen, Liying Cheng, Yang You, and Lidong\nBing. 2023. Are large language models good evalua-\ntors for abstractive summarization? arXiv preprint\narXiv:2305.13091.\nDave Van Veen, Cara Van Uden, Louis Blankemeier,\nJean-Benoit Delbrouck, Asad Aali, Christian Bl\u00fcth-\ngen, A. Pareek, Malgorzata Polacin, William Collins,\nNeera Ahuja, C. Langlotz, Jason Hom, S. Gatidis,\nJohn Pauly, and Akshay S Chaudhari. 2023. Clinical\ntext summarization: Adapting large language models\ncan outperform human experts. Research Square.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023a. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023b. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nMinghao Wu and Alham Fikri Aji. 2023. Style over sub-\nstance: Evaluation biases for large language models.\narXiv preprint arXiv: 2307.03025.\nXinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv,\nTingwen Liu, Fei Huang, Hongbo Xu, and Yongbin\nLi. 2023. Wider and deeper llm networks are fairer\nllm evaluators. arXiv preprint arXiv:2308.01862.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-bench and chatbot arena.\nIn Thirty-seventh Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track.\nA\nAppendix\nA.1\nPrompts for Simple Instructions\nFigure 10 shows task description. Figures 11 - 14\nshow simple instructions for various metrics.\n\u201cOpen Prompt\": \u201cGiven a short user provided starting prompt and\nits concise completion (which is roughly a page long), your task\nis to evaluate the completion with respect to the starting prompt\nand listed set of metrics. For each metric listed, you must always\nreturn a score and a justification of the score. Note that, both\nthe starting prompt and its completion are given in {{language}}.\",\n\u201cContinue Writing\": \u201cGiven two passages (passage_a and passage_b),\none of which may be empty, and third passage (passage_c), which\naims\nto\nprovide\na\nseamless\ntransitions\nbetween\npassage_a\nand\npassage_b. Your task is to evaluate the passage_c with respect to\nthe listed set of metrics. For each metric listed, you must always\nreturn a score and a justification of the score.\nNote that, all\nthree passages are given in {{language}}.\",\n\u201cSummarize\": \u201cGiven a passage and a brief summary of that passage\nwhich attempts to capture the essence of it, your task is to evaluate\nthe summary with respect to the given passage and listed set of\nmetrics. For each metric listed, you must always return a score and\na justification of the score. Note that, both the passage and its\nsummary are given in {{language}}.\"\nFigure 10: Task description.\n\u201cname\": \u201clinguistic_acceptability\",\n\u201cdescription\":\n\u201cLinguistic acceptability means does this sound\nright to a native speaker?, not does this stick to the rules of\nthe grammar.\",\n\u201cscoring\": \"0: not acceptable; 1: some weird things but ok; 2: no\nerrors found/acceptable.\"\nFigure 11: Metric description for simple instructions\n(Linguistic Acceptability).\n\u201cname\": \u201cproblematic_content\",\n\u201cdescription\":\n\u201cProblematic\ncontent\nrefers\nto\nthe\npresence\nof\nharmful\n(bias,\noffensive,\ntoxic,\nfor\nexample)\ncontent\nin\nthe\noutput.\",\n\u201cscoring\": \u201c1: no problematic content in the text; 0: if there is\nproblematic content in the text\"\nFigure 12: Metric description for simple instructions\n(Problematic Content).\n\u201cname\": \u201ctask quality\",\n\u201cdescription\":\n\u201cThe\nquality\nof\nthe\noutput\nis\nrelated\nto\nthe\ntask. We are evaluating whether the model did what the task asked.\",\n\u201cscoring\": \u201c0: the model did not do what the task asked; 1: mostly\ndid what the task asked, with some errors; 2: did what the task\nasked.\"\nFigure 13: Metric description for simple instructions\n(Task Quality).\n\u201cname\": \u201coutput content quality\",\n\u201cdescription\":\n\u201cLow-Quality Content means whether the discourse\n(text) is any good.\",\n\u201cscoring\": \u201c0: bad content \u2013 If the text sounds repetitive (or is\nnon-factual/ inconsistent or it\u2019s not in the given language, or\nseems to have been web-scrapped); 1: OK content, but some flaws\nfound \u2013 If it\u2019s ok (grammatical, lexically, vocab is good) but kind\nof goes around in circles; 2; good or above content.\"\nFigure 14: Metric description for simple instructions\n(Output Quality Content).\nA.2\nPrompts for Detailed Instructions\nFigures 15 - 18 show complex instructions for vari-\nous metrics.\nA.3\nFleiss\u2019 Kappa\nTable 3 shows the Fleiss\u2019 Kappa (\u03ba) on the full\ndataset for various annotator combinations, aggre-\ngated by language, task, and metrics.\nA.4\nClass distribution for Metrics with 3\nclasses\nFigures 19 and 20 show class distribution for var-\nious languages, aggregated over metrics with 3\nclasses - LA, OCQ, TQ.\nA.5\nClass distribution for Metrics with 2\nclasses\nFigures 21 and 22 show class distribution for var-\nious languages, aggregated over metrics with 2\nclasses - H, PC.\nA.6\nTemperature Variations\nFigure 23 shows PA values for different tempera-\nture values, results are aggregated over language,\ntask, and metrics.\nA.7\nfew-shot Results\nFigure 24 shows PA values for few-shot prompt-\ning, results are aggregated over language, task, and\nmetrics.\n\u201cname\": \u201clinguistic_acceptability\",\n\u201cdescription\": \u201cLinguistic acceptability pertains to the degree to which a given language structure (e.g., phrase, sentence, discourse) aligns\nwith the implicit norms and rules of a native speaker\u2019s linguistic intuition. In the study of language, it\u2019s distinct from \u2019grammaticality\u2019, which\nis a stricter and narrower concept based on the prescriptive rules of a language. Linguistic acceptability, on the other hand, captures broader\nnative-speaker intuitions and encompasses factors like fluency, idiomacy, and appropriateness in context.\nIn the context of language models,\nevaluating linguistic acceptability involves assessing the output of the model not just for its adherence to grammar rules, but for its overall\nfit within the natural, expected, and intuitive contours of fluent human language. The scoring rubric is described below, with a few possible\nreasons (which might not be exhaustive) for a given score.\",\n\"scoring\": {\n\"0\": {\n\"(a)\": \"Sentences that lack clear syntactic structure.\",\n\"(b)\": \"Usage of non-existent or incorrect words.\",\n\"(c)\": \"Grossly inappropriate word choices for a given context.\"\n},\n\"1\": {\n\"(a)\": \"Overly verbose or stilted phrasing.\",\n\"(b)\": \"Minor grammatical errors that do not impede understanding.\",\n\"(c)\": \"Use of a word that's technically correct but not the most appropriate for context.\"\n},\n\"2\": {\n\"(a)\": \"Seamless integration of contextually relevant vocabulary\",\n\"(b)\": \"Effective use of idiomatic expressions without sounding forced.\",\n\"(c)\": \"Sentences that reflect natural rhythm, emphasis, and intonation of spoken language.\"\n}\n}\nFigure 15: Metric description for complex instructions (Linguistic Acceptability).\n\u201cname\": \u201cproblematic_content\",\n\u201cdescription\": \u201cProblematic Content assesses the presence of any content that may be deemed offensive, inappropriate, or harmful in the model\u2019s\noutput. This metric serves as a filter against outputs that might perpetuate harmful stereotypes, misinformation, or any form of bigotry. This\nmetric highlights the importance of ethical considerations in outputs and underscores the need for vigilance against unintended harmful content.\nThe scoring rubric is described below, with a few possible reasons (which might not be exhaustive) for a given score.\",\n\"scoring\": {\n\"1\": {\n\"(a)\": \"The output is free from any content that might be perceived as offensive, harmful, or inappropriate.\",\n\"(b)\": \"No propagation of stereotypes, misinformation, or inappropriate assertions.\",\n\"(c)\": \"The response respects cultural, social, and individual sensitivities.\"\n},\n\"0\": {\n\"(a)\": \"The output contains statements, suggestions, or themes that can be deemed problematic or offensive.\",\n\"(b)\": \"There is evidence of perpetuation of harmful stereotypes or biases.\",\n\"(c)\": \"Contains misinformation or promotes inappropriate or harmful narratives.\"\n}\n}\nFigure 16: Metric description for complex instructions (Problematic content).\n\u201cname\": \u201ctask quality\",\n\u201cdescription\": \u201cTask Quality gauges the degree to which a model adheres to and executes the specific directives given in the prompt. This metric\nzeroes in exclusively on the fidelity of the model\u2019s response to the prompt\u2019s instructions. An ideal response not only recognizes the overt commands\nof the prompt but also respects its nuance and subtleties. The scoring rubric is described below, with a few possible reasons (which might not be\nexhaustive) for a given score.\"\n\"scoring\": {\n\"0\": {\n\"(a)\": \"The model disregards the instructions entirely.\",\n\"(b)\": \"The output is entirely irrelevant to the prompt.\",\n\"(c)\": \"There is a clear disconnect between the user's request and the model's response.\"\n},\n\"1\": {\n\"(a)\": \"The model grasps and addresses the main theme or element of the instruction but may miss out on finer details or nuances.\",\n\"(b)\": \"There is partial alignment with the prompt, indicating some elements of relevance, but not a complete match.\",\n\"(c)\": \"The response might include extraneous details not asked for, or it might omit some requested specifics.\"\n},\n\"2\": {\n\"(a)\": \"The model demonstrates a precise understanding and adherence to the prompt's instructions.\",\n\"(b)\": \"The output holistically satisfies all aspects of the given directive without any deviation.\",\n\"(c)\": \"There's a clear and direct correlation between the user's instruction and the model's response, with no aspect of the\ninstruction left unaddressed.\"\n}\n}\nFigure 17: Metric description for complex instructions (task quality).\n\u201cname\": \u201coutput content quality\",\n\u201cdescription\": \u201cOutput Content Quality measures the overall caliber of the content generated, factoring in its relevance, clarity, originality,\nand linguistic fluency. High-quality output should not only be grammatically sound but should also convey information in an articulate, coherent,\nand engaging manner without any evidence of plagiarism, redundancy, or artificiality. This metric ensures that the produced content meets the\nexpectations of originality, clarity, and contextual relevance in addition to linguistic fluency. The scoring rubric is described below, with a\nfew possible reasons (which might not be exhaustive) for a given score.\",\n\"scoring\": {\n\"0\": {\n\"(a)\": \"The output is in a language different from the intended/requested one.\",\n\"(b)\": \"Content appears scraped from the web, giving a plagiarized feel.\",\n\"(c)\": \"The output is repetitive or overly redundant.\",\n\"(d)\": \"Displays artifacts of poor machine translation.\"\n},\n\"1\": {\n\"(a)\": \"The content is generally accurate in terms of grammar and word choice.\",\n\"(b)\": \"Sounds unnatural or awkward in the language, lacking smoothness.\",\n\"(c)\": \"May have minor discrepancies in content clarity or relevance.\",\n\"(d)\": \"Shows traces of generative patterns or repetitiveness, albeit less pronounced than level 0.\"\n},\n\"2\": {\n\"(a)\": \"The text shows a high level of originality and authenticity.\",\n\"(b)\": \"Demonstrates clear, coherent, and contextually appropriate content.\",\n\"(c)\": \"Engages the reader with natural linguistic flow and rhythm.\",\n\"(d)\": \"Absence of any noticeable generative artifacts or awkward.\"\n}\n}\nFigure 18: Metric description for complex instructions (Output content quality).\nName\nAnnot1\nAnnot2\nAnnot3\nAnnotAgg\nGPT4_joint\nAnnotAgg\nGPT4_single\nAnnotAgg\nGPT4_SD\nCs\n0.46 \u00b1 0.29\n0.05 \u00b1 0.12\n0.08 \u00b1 0.17\n0.07 \u00b1 0.15\nDe\n0.29 \u00b1 0.29\n0.07 \u00b1 0.11\n0.13 \u00b1 0.16\n0.13 \u00b1 0.15\nEn\n0.47 \u00b1 0.42\n0.15 \u00b1 0.22\n0.18 \u00b1 0.24\n0.11 \u00b1 0.17\nEs\n0.32 \u00b1 0.22\n0.04 \u00b1 0.11\n0.04 \u00b1 0.12\n0.04 \u00b1 0.11\nLang.\nFr\n0.44 \u00b1 0.31\n0.12 \u00b1 0.21\n0.20 \u00b1 0.23\n0.22 \u00b1 0.22\nIt\n0.41 \u00b1 0.33\n0.06 \u00b1 0.11\n0.08 \u00b1 0.16\n0.08 \u00b1 0.14\nJa\n0.44 \u00b1 0.33\n0.01 \u00b1 0.13\n0.02 \u00b1 0.14\n0.04 \u00b1 0.15\nPt-Br\n0.52 \u00b1 0.37\n0.11 \u00b1 0.19\n0.09 \u00b1 0.17\n0.12 \u00b1 0.20\nZh\n0.35 \u00b1 0.32\n0.00 \u00b1 0.08\n0.01 \u00b1 0.07\n0.02 \u00b1 0.07\nH\n0.40 \u00b1 0.39\n0.04 \u00b1 0.15\n0.05 \u00b1 0.15\n0.08 \u00b1 0.18\nLA\n0.41 \u00b1 0.24\n-0.02 \u00b1 0.06\n0.05 \u00b1 0.15\n0.09 \u00b1 0.16\nMetric\nOCQ\n0.54 \u00b1 0.19\n0.13 \u00b1 0.17\n0.16 \u00b1 0.19\n0.14 \u00b1 0.17\nPC\n0.11 \u00b1 0.32\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\nTQ\n0.60 \u00b1 0.20\n0.18 \u00b1 0.19\n0.20 \u00b1 0.21\n0.16 \u00b1 0.18\nContinue\nWriting\n0.45 \u00b1 0.33\n0.06 \u00b1 0.15\n0.07 \u00b1 0.17\n0.08 \u00b1 0.16\nTask\nOpen\nPrompt\n0.49 \u00b1 0.32\n0.12 \u00b1 0.19\n0.16 \u00b1 0.19\n0.15 \u00b1 0.18\nSummarize\n0.29 \u00b1 0.29\n0.02 \u00b1 0.09\n0.06 \u00b1 0.15\n0.05 \u00b1 0.13\nTable 3: Fleiss\u2019 Kappa (\u03ba) values for different cases and annotator combinations on the full dataset. GPT4_SD\nmeans GPT4_single_detailed\n.\nFull\n\u224896% samples\nPartial\n\u22483% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEn\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(a) Compound call - English\nFull\n\u224896% samples\nPartial\n\u22483% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEn\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(b) Single Call - English\nFull\n\u224880% samples\nPartial\n\u224818% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nEs\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(c) Compound call - Spanish\nFull\n\u224880% samples\nPartial\n\u224818% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEs\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(d) Single Call - Spanish\nFull\n\u224887% samples\nPartial\n\u224812% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nFr\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(e) Compound call - French\nFull\n\u224887% samples\nPartial\n\u224812% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nFr\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(f) Single Call - French\nFull\n\u224884% samples\nPartial\n\u224814% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nDe\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(g) Compound call - German\nFull\n\u224884% samples\nPartial\n\u224814% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nDe\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(h) Single Call - German\nFull\n\u224885% samples\nPartial\n\u224813% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nIt\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(i) Compound call - Italian\nFull\n\u224885% samples\nPartial\n\u224813% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nIt\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(j) Single Call - Italian\nFigure 19: Class distribution per language (En, Es, Fr, De, It). Results are aggregated over all tasks and metrics with\n3 classes (LA, OCQ, TQ).\nFull\n\u224890% samples\nPartial\n\u22489% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(a) Compound call - Portuguese (Br)\nFull\n\u224890% samples\nPartial\n\u22489% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(b) Single Call - Portuguese (Br)\nFull\n\u224873% samples\nPartial\n\u224825% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nZh\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(c) Compound call - Chinese\nFull\n\u224873% samples\nPartial\n\u224825% samples\nNone\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nZh\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(d) Single Call - Chinese\nFull\n\u224879% samples\nPartial\n\u224820% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nJa\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(e) Compound call - Japanese\nFull\n\u224879% samples\nPartial\n\u224820% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nJa\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(f) Single Call - Japanese\nFull\n\u224876% samples\nPartial\n\u224822% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\nPercentage of Samples in the Given Case\nCs\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(g) Compound call - Czech\nFull\n\u224876% samples\nPartial\n\u224822% samples\nNone\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nCs\n0 Human\n0 GPT\n1 Human\n1 GPT\n2 Human\n2 GPT\n(h) Single Call - Czech\nFigure 20: Class distribution per language (Pt-Br, Zh, Ja, Cz). Results are aggregated over all tasks and metrics with\n3 classes (LA, OCQ, TQ).\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEn\n0 Human\n0 GPT\n1 Human\n1 GPT\n(a) Compound call - English\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEn\n0 Human\n0 GPT\n1 Human\n1 GPT\n(b) Single Call - English\nFull\n\u224897% samples\nPartial\n\u22482% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEs\n0 Human\n0 GPT\n1 Human\n1 GPT\n(c) Compound call - Spanish\nFull\n\u224897% samples\nPartial\n\u22482% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nEs\n0 Human\n0 GPT\n1 Human\n1 GPT\n(d) Single Call - Spanish\nFull\n\u224896% samples\nPartial\n\u22483% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nFr\n0 Human\n0 GPT\n1 Human\n1 GPT\n(e) Compound call - French\nFull\n\u224896% samples\nPartial\n\u22483% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nFr\n0 Human\n0 GPT\n1 Human\n1 GPT\n(f) Single Call - French\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nDe\n0 Human\n0 GPT\n1 Human\n1 GPT\n(g) Compound call - German\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nDe\n0 Human\n0 GPT\n1 Human\n1 GPT\n(h) Single Call - German\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nIt\n0 Human\n0 GPT\n1 Human\n1 GPT\n(i) Compound call - Italian\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nIt\n0 Human\n0 GPT\n1 Human\n1 GPT\n(j) Single Call - Italian\nFigure 21: Class distribution per language (En, Es, Fr, De, It). Results are aggregated over all tasks and metrics with\n2 classes (hallucinations and problematic content).\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n(a) Compound call - Portuguese (Br)\nFull\n\u224899% samples\nPartial\n\u22480% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nPt-Br\n0 Human\n0 GPT\n1 Human\n1 GPT\n(b) Single Call - Portuguese (Br)\nFull\n\u224897% samples\nPartial\n\u22482% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nZh\n0 Human\n0 GPT\n1 Human\n1 GPT\n(c) Compound call - Chinese\nFull\n\u224897% samples\nPartial\n\u22482% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nZh\n0 Human\n0 GPT\n1 Human\n1 GPT\n(d) Single Call - Chinese\nFull\n\u224898% samples\nPartial\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nJa\n0 Human\n0 GPT\n1 Human\n1 GPT\n(e) Compound call - Japanese\nFull\n\u224898% samples\nPartial\n\u22481% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nJa\n0 Human\n0 GPT\n1 Human\n1 GPT\n(f) Single Call - Japanese\nFull\n\u224896% samples\nPartial\n\u22483% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nCs\n0 Human\n0 GPT\n1 Human\n1 GPT\n(g) Compound call - Czech\nFull\n\u224896% samples\nPartial\n\u22483% samples\nCases of Agreement\n0\n20\n40\n60\n80\n100\nPercentage of Samples in the Given Case\nCs\n0 Human\n0 GPT\n1 Human\n1 GPT\n(h) Single Call - Czech\nFigure 22: Class distribution per language (Pt-Br, Zh, Ja, Cz). Results are aggregated over all tasks and metrics with\n2 classes (hallucinations and problematic content).\nEn\nDe\nIt\nPt-Br\nFr\nEs\nZh\nCs\nJa\nLanguage\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nPercent Agreement (PA) between AnnotAgg and GPT4 single\nTemp=0.0\nTemp=0.3\nTemp=0.7\nTemp=1.0\n(a) PA by language with temperature variation\nSummarize\nContinue Writing\nOpen Prompt\nTask\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nPercent Agreement (PA) between AnnotAgg and GPT4 single\nTemp=0.0\nTemp=0.3\nTemp=0.7\nTemp=1.0\n(b) PA by task with temperature variation\nPC\nH\nLA\nOCQ\nTQ\nMetric\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nPercent Agreement (PA) between AnnotAgg and GPT4 single\nTemp=0.0\nTemp=0.3\nTemp=0.7\nTemp=1.0\n(c) PA by metric with temperature variation\nFigure 23: Percentage Agreement (PA) for different cases\nand temperature variations. Values reported are on the small\ndataset.\nEn\nDe\nIt\nPt-Br\nFr\nEs\nZh\nCs\nJa\nLanguage\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single fewshot (PA)\n(a) PA by language with few-shot examples\nSummarize\nContinue Writing\nOpen Prompt\nTask\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single fewshot (PA)\n(b) PA by task with few-shot examples\nPC\nH\nLA\nOCQ\nTQ\nMetric\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPercentage Agreement\nAnnotAgg,GPT4 single (PA)\nAnnotAgg,GPT4 single fewshot (PA)\n(c) PA by metric with few-shot examples\nFigure 24: Percentage Agreement (PA) for different cases\nwith few-shot examples. Values reported are on the small\ndataset.\n"
  }
]