[
  {
    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
    "link": "https://arxiv.org/pdf/2310.11511.pdf",
    "upvote": "60",
    "text": "Preprint.\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\nCRITIQUE THROUGH SELF-REFLECTION\nAkari Asai\u2020, Zeqiu Wu\u2020, Yizhong Wang\u2020\u00a7, Avirup Sil\u2021, Hannaneh Hajishirzi\u2020\u00a7\n\u2020University of Washington\n\u00a7Allen Institute for AI\n\u2021IBM Research AI\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\nABSTRACT\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration (SELF-RAG) that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.1\n1\nINTRODUCTION\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n1Our code and trained models are available at https://selfrag.github.io/.\n1\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023\nPreprint.\nStep 1: Retrieve K documents\nCalifornia was named after a \ufb01ctional \nisland in a Spanish book. \nPrompt How did US states get their names? \nUS states got their names from a variety of \nsources. Eleven states are named after an \nindividual person (e.g, California was named \nafter Christopher Columbus). Some states \nincluding Texas and Utah, are named after \nNative American tribe.\nRetrieval-Augmented Generation (RAG)\nOurs: Self-re\ufb02ective Retrieval-Augmented Generation (Self-RAG) \nPopular names by states. In Texas, \nEmma is a popular baby name. \nOf the \ufb01fty states, eleven are named \nafter an individual person. \nPrompt How did US states get their names? + \nStep 2: Prompt LM with K docs and generate\nRetriever\nLM\nPrompt How did US states get their names? \nUS states got their names from a variety of sources. \nRetrieve\nStep 1: Retrieve on demand  \nPrompt +  \n11 of 50 state names\nRelevant\nStep 2: Generate segment in parallel \ncome from persons.\nSupported\nIrrelevant\nTexas is named\nafter a Native American tribe. \nStep 3: Critique outputs and select best segment\norigins in a 16th-century novel \nLas Sergas de Esplandi\u00e1n. \nCalifornia's name has its\nRelevant\nPartially\nUS states got their names from a variety of sources. 11 of 50 \nstates names are come from persons.    26 states are named \nafter Native Americans, including Utah. \nPrompt: Write an essay of your best summer vacation\nPrompt: Write an essay of your best summer vacation\nNo Retrieval\nMy best summer vacation is when my family and I embarked on a road trip along \u2026\nMy best\u2026 \n>\nRepeat.\u2026\nNo information in passages\nContradictory\n>\nPrompt +  \nPrompt +  \nRetrieve\nFigure 1: Overview of SELF-RAG. SELF-RAG learns to retrieve, critique, and generate text passages\nto enhance overall generation quality, factuality, and verifiability.\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\ngeneration quality. Moreover, SELF-RAG provides citations for each segment with its self-assessment\nof whether the output is supported by the passage, leading to easier fact verification.\nSELF-RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\nassess its own predictions after each generated segment as an integral part of the generation output.\nSELF-RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\nmodels\u2019 behaviors to user preferences by leveraging reflection tokens through segment-level beam\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAG outperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n2\nRELATED WORK\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n2\nPreprint.\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b).\nWhile prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nConcurrent RAG work.\nA few concurrent works2 on RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\non retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n3\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n3.1\nPROBLEM FORMALIZATION AND OVERVIEW\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n3\nPreprint.\nType\nInput\nOutput\nDefinitions\nRetrieve\nx / x, y\n{yes, no, continue}\nDecides when to retrieve with R\nISREL\nx, d\n{relevant, irrelevant}\nd provides useful information to solve x.\nISSUP\nx, d, y\n{fully supported, partially\nsupported, no support}\nAll of the verification-worthy statement in y\nis supported by d.\nISUSE\nx, y\n{5, 4, 3, 2, 1}\ny is a useful response to x.\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\nAlgorithm 1 SELF-RAG Inference\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN}\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\n2: M predicts Retrieve given (x, y<t)\n3: if Retrieve == Yes then\n4:\nRetrieve relevant text passages D using R given (x, yt\u22121)\n\u25b7 Retrieve\n5:\nM predicts\nISREL given x, d and yt given x, d, y<t for each d \u2208 D\n\u25b7 Generate\n6:\nM predicts\nISSUP and\nISUSE given x, yt, d for each d \u2208 D\n\u25b7 Critique\n7:\nRank yt based on\nISREL ,\nISSUP ,\nISUSE\n\u25b7 Detailed in Section 3.3\n8: else if Retrieve == No then\n9:\nMgen predicts yt given x\n\u25b7 Generate\n10:\nMgen predicts\nISUSE given x, yt\n\u25b7 Critique\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF-RAG at inference. For\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\nutility of the response.4 To generate each segment, SELF-RAG processes multiple passages in parallel\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\nd1 is selected at the first time step since d2 does not provide direct evidence (\nISREL is Irrelevant)\nand d3 output is only partially supported while d1 are fully supported.\nTraining overview. SELF-RAG enables an arbitrary LM to generate text with reflection tokens\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\nM to generate reflection tokens by itself without relying on the critic at inference time.\n3.2\nSELF-RAG TRAINING\nHere, we describe the supervised data collection and training of two models, the critic C (Section 3.2.1)\nand the generator M (Section 3.2.2).\n3.2.1\nTRAINING THE CRITIC MODEL\nData collection for critic model. Manual annotation of reflection tokens for each segment is\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n4We follow Liu et al. (2023a) in using a \u201cperceived\u201d utility value that is independent of retrieved passages.\n4\nPreprint.\nInput: How did US states get their names? \nInput: Write an essay of your best summer vacation\nOutput: My best summer vacation was a magical escape \nto the coastal town of Santorini. The azure waters, \ncharming white-washed building are unforgettable. \nCritic LM\nOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor \nof King Louis XIV of France and Georgia was named after King George II. \nRetrieve\nPartially\nAugmented Output:                \nRetrieve\n<p>LOUISIANA: Named in\n<p>Of the \ufb01fty states, eleven are named after an individual person</p>.  \n             11 of 50 states\u2019 names come from person. \nRelevant\nSupported\nhonor of Louis XIV of France.</p>.  Relevant\nFor instance, Louisiana was named after King Louis XIV, and\nUtil: 5\nGeorgia was named after King George II. \nUtil: 5\nAugmented Output:                     My best summer \nvacation was a magical escape to the coastal town of \nSantorini.                     The azure waters, charming white-\nwashed building are unforgettable experience.\nNo Retrieval\nNo Retrieval\nRetriever\nFigure 2: SELF-RAG training examples. The left example does not require retrieval while the right\none requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\nused to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\ncan raise API costs and diminish reproducibility (Chen et al., 2023). We create supervised data by\nprompting GPT-4 to generate reflection tokens and then distill their knowledge into an in-house C.\nFor each group of reflection tokens, we randomly sample instances from the original training data:\n{Xsample, Y sample} \u223c {X, Y }. As different reflection token groups have their own definitions and\ninput, as shown in Table 1, we use different instruction prompts for them. Here, we use Retrieve as\nan example. We prompt GPT-4 with a type-specific instruction (\u201cGiven an instruction, make a\njudgment on whether finding some external documents from the web helps to generate a better\nresponse.\u201d) followed by few-shot demonstrations I the original task input x and output y to predict\nan appropriate reflection token as text: p(r|I, x, y). Manual assessment reveals that GPT-4 reflection\ntoken predictions show high agreement with human evaluations. We collect 4k-20k supervised\ntraining data for each type and combine them to form training data for C. Appendix Section D shows\nthe full list of instructions, and A.1 contains more details and our analysis.\nCritic learning.\nAfter we collect training data Dcritic, we initialize C with a pre-trained LM and\ntrain it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\nmax\nC\nE((x,y),r)\u223cDcritic log pC(r|x, y), r for reflection tokens.\n(1)\nThough the initial model can be any pre-trained LM, we use the same one as the generator LM\n(i.e., Llama 2-7B; Touvron et al. 2023) for C initialization. The critic achieves a higher than 90%\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 5).\n3.2.2\nTRAINING THE GENERATOR MODEL\nData collection for generator.\nGiven an input-output pair (x, y), we augment the original output\ny using the retrieval and critic models to create supervised data that precisely mimics the SELF-\nRAG inference-time process (Section 3.1). For each segment yt \u2208 y, we run C to assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken Retrieve =Yes is added, and R retrieves the top K passages, D. For each passage, C further\nevaluates whether the passage is relevant and predicts\nISREL . If a passage is relevant, C further\nevaluates whether the passage supports the model generation and predicts\nISSUP . Critique tokens\nISREL and\nISSUP are appended after the retrieved passage or generations. At the end of the output, y\n(or yT ), C predicts the overall utility token\nISUSE , and an augmented output with reflection tokens\nand the original input pair is added to Dgen. See the example training data in Figure 2.\nGenerator learning. We train the generator model M by training on the curated corpus augmented\nwith reflection tokens Dgen using the standard next token objective:\nmax\nM E(x,y,r)\u223cDgen log pM(y, r|x).\n(2)\nUnlike C training (Eq. 1), M learns to predict the target output as well as the reflection tokens. During\ntraining, we mask out the retrieved text chunks (surrounded by <p> and </p> in Figure 2) for loss\ncalculation and expand the original vocabulary V with a set of reflection tokens { Critique , Retrieve }.\nConnections to prior work on learning with critique.\nRecent work incorporates additional\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\n5\nPreprint.\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3\nSELF-RAG INFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold. SELF-RAG dynamically decides when to retrieve text passages by\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G (e.g.,\nISREL ), we denote its score at timestamp\nt as sG\nt , and we compute a segment score as follows:\nf(yt, d, Critique ) = p(yt|x, d, y<t)) + S( Critique ), where\n(3)\nS( Critique ) =\nX\nG\u2208G\nwGsG\nt for G = { ISREL , ISSUP , ISUSE },\n(4)\nwhere sG\nt =\npt(\u02c6r)\nPNG\ni=1 pt(ri) stands for the generation probability of the most desirable reflection token\n\u02c6r (e.g.,\nISREL =Relevant) for the critique token type G with N G distinct tokens (that represent\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\ny is mostly supported by evidence, we can set a weight term for the\nISSUP score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable\nCritique token (e.g.,\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF-RAG tailors an LM with no additional training.\n4\nEXPERIMENTS\n4.1\nTASKS AND DATASETS\nWe conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a fact verification dataset about public health (PubHealth;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC-\n6\nPreprint.\nChallenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\nShort-form generations tasks include two open-domain question answering (QA) datasets,\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work\u2019s validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. 5\n4.2\nBASELINES\nBaselines without retrievals.\nWe evaluate strong publicly available pre-trained LLMs,\nLlama27B,13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B,13B (Dubois et al., 2023)\n(our replication based on Llama2); and models trained and reinforced using private data, Chat-\nGPT (Ouyang et al., 2022) and Llama2-chat13B. For instruction-tuned LMs, we use the official\nsystem prompt or instruction format used during training if publicly available. We also compare our\nmethod to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt\nengineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\nsystem.\nThe second category includes concurrent methods that are trained with retrieved text\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with API calls (e.g., Wikipedia APIs).6\n4.3\nEXPERIMENTAL SETTINGS\nTraining data and settings. Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input. More training details are in the Appendix Section B.1.\nInference settings. As a default configuration, we assign the weight terms\nISREL ,\nISSUP ,\nISUSE\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n5https://github.com/princeton-nlp/ALCE\n6We report numbers using the results reported in the paper as the implementations are not available.\n7\nPreprint.\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models. \u2217 indicates concurrent or recent results reported by\nconcurrent work. \u2013 indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\nShort-form\nClosed-set\nLong-form generations (with citations)\nPopQA\nTQA\nPub\nARC\nBio\nASQA\nLM\n(acc)\n(acc)\n(acc)\n(acc)\n(FS)\n(em)\n(rg)\n(mau)\n(pre)\n(rec)\nLMs with proprietary data\nLlama2-c13B\n20.0\n59.3\n49.4\n38.4\n55.9\n22.4\n29.6\n28.6\n\u2013\n\u2013\nRet-Llama2-c13B\n51.8\n59.8\n52.1\n37.9\n79.9\n32.8\n34.8\n43.8\n19.8\n36.1\nChatGPT\n29.3\n74.3\n70.1\n75.3\n71.8\n35.3\n36.2\n68.8\n\u2013\n\u2013\nRet-ChatGPT\n50.8\n65.7\n54.7\n75.3\n\u2013\n40.7\n39.9\n79.7\n65.1\n76.6\nPerplexity.ai\n\u2013\n\u2013\n\u2013\n\u2013\n71.2\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nBaselines without retrieval\nLlama27B\n14.7\n30.5\n34.2\n21.8\n44.5\n7.9\n15.3\n19.0\n\u2013\n\u2013\nAlpaca7B\n23.6\n54.5\n49.8\n45.0\n45.8\n18.8\n29.4\n61.7\n\u2013\n\u2013\nLlama213B\n14.7\n38.5\n29.4\n29.4\n53.4\n7.2\n12.4\n16.0\n\u2013\n\u2013\nAlpaca13B\n24.4\n61.3\n55.5\n54.9\n50.2\n22.9\n32.0\n70.6\n\u2013\n\u2013\nCoVE65B *\n\u2013\n\u2013\n\u2013\n\u2013\n71.2\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nBaselines with retrieval\nToolformer*6B\n\u2013\n48.8\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nLlama27B\n38.2\n42.5\n30.0\n48.0\n78.0\n15.2\n22.1\n32.0\n2.9\n4.0\nAlpaca7B\n46.7\n64.1\n40.2\n48.0\n76.6\n30.9\n33.3\n57.9\n5.5\n7.2\nLlama2-FT7B\n48.7\n57.3\n64.3\n65.8\n78.2\n31.0\n35.8\n51.2\n5.0\n7.5\nSAIL*7B\n\u2013\n\u2013\n69.2\n48.4\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nLlama213B\n45.7\n47.0\n30.2\n26.0\n77.5\n16.3\n20.5\n24.7\n2.3\n3.6\nAlpaca13B\n46.1\n66.9\n51.1\n57.6\n77.7\n34.8\n36.7\n56.6\n2.0\n3.8\nOur SELF-RAG 7B\n54.9\n66.4\n72.4\n67.3\n81.2\n30.0\n35.7\n74.3\n66.9\n67.8\nOur SELF-RAG 13B\n55.8\n69.3\n74.5\n73.1\n80.2\n31.7\n37.0\n71.6\n70.3\n71.3\n5\nRESULTS AND ANALYSIS\n5.1\nMAIN RESULTS\nComparison against baselines without retrieval.\nTable 2 (top) presents the baselines without\nretrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama265B to refine output.\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAG also\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\nin this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate\n8\nPreprint.\nPQA\nMed\nAS\n(acc)\n(acc)\n(em)\nSELF-RAG (50k)\n45.5\n73.5\n32.1\nTraining\nNo Retriever R\n43.6\n67.8\n31.0\nNo Critic C\n42.6\n72.0\n18.1\nTest\nNo retrieval\n24.7\n73.0\n\u2013\nHard constraints\n28.3\n72.6\n\u2013\nRetrieve top1\n41.8\n73.1\n28.6\nRemove\nISSUP\n44.1\n73.2\n30.6\n(a) Ablation\n1\n2\n70.0\n70.5\nPrecision\n1\n2\nWeight for IsSupport\n90\n95\nMauve\n(b) Customization\n0.0\n0.2\n0.4\n0.6\n0.98\n0.99\n0.99\n1.00\nAccuracy\nPubHealth\n0.0\n0.2\n0.4\n0.6\nRetrieval Threshold\n0.6\n0.8\n1.0\nAccuracy\nPopQA\n0.0\n0.5\n1.0\nFrequency\n0.25\n0.50\n0.75\n1.00\nFrequency\n(c) Retrieval\nFigure 3: Analysis on SELF-RAG: (a) Ablation studies for key components of SELF-RAG training\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\nprecisely grounded yet shorter outputs. Llama2-FT7B, which is the baseline LM trained on the same\ninstruction-output pairs as SELF-RAG without retrieval or self-reflection and is retrieval-augmented\nat test time only, lags behind SELF-RAG. This result indicates SELF-RAG gains are not solely from\ntraining data and demonstrate the effectiveness of SELF-RAG framework.\n5.2\nANALYSIS\nAblation studies.\nWe conduct a set of ablations of our framework to identify which factors play\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\ntop one document only, similar to standard RAG approaches; Remove\nISSUP indicates the model\nperformance that removes\nISSUP score only during critique-guided beam search in Eq. 4. In this\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF-RAG and No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing\nISSUP during the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG\u2019s\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\nEffects of inference-time customization. One key benefit of our proposed framework is that it\nenables us to control how much each critique type affects the final generation sampling. We analyze\nthe effects of different parameter weights on the top of our 7B model during inference time on\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\nthe weighting term for\nISSUP , which criticizes how supported the output is by the text passage. As\nthe figure shows, increasing the weight leads to positive effects on the models\u2019 citation precision\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\n9\nPreprint.\n0\n50\n100\n150\nNum of training (k)\n35\n40\n45\n50\n55\nPerfomance\n(a) PopQA\n0\n100\nNum of training (k)\n71\n72\n73\n(b) PubHealth\n0\n100\nNum of training (k)\n40\n60\n(c) ASQA (prec)\nPop\nBio.\nS & P\n92.5\n70.0\nISREL\n95.0\n90.0\nISSUP\n90.0\n85.0\n(d) Human evaluation on PopQA\nand Bio generation.\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\nHuman analysis on SELF-RAG outputs as well as reflection tokens.\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\u2019 behaviors at\ntest time by adjusting such parameters without requiring additional training.\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\nof threshold \u03b4 (larger \u03b4 results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\nthe model\u2019s retrieval frequencies dramatically change on both datasets. as \u03b4 varies. On one hand,\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\nEffects of training data size. We conduct an analysis of how the data scale affects the model\u2019s\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n150k training instances, and fine-tune four SELF-RAG 7B variants on those subsets. Then, we compare\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\u2019\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\n50k to 150k. These results also indicate that further expanding the training data of SELF-RAG may\nlead to further improvements, although in this work we limit our training data size to 150k.\nHuman evaluations. We conduct small human evaluations on SELF-RAG outputs, as well as the\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\nverify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\nreflection tokens about\nISREL and\nISSUP match their inspections (e.g., whether the fully supported\noutput is supported by the cited evidence). Human annotators find SELF-RAG answers are often\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\nconsistent with Menick et al. (2022). Human annotators also find\nISREL and\nISSUP reflection token\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\nexamples and explanations on assessments.\n6\nCONCLUSION\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate,\nand critique text passages and its own generation by predicting the next tokens from its original\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\nsix tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with\nmore parameters or with conventional retrieval-augmented generation approaches.\n10\nPreprint.\nETHICAL CONCERNS\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\nadvice). While our method shows significant improvements in terms of performance, factuality, and\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\nACKNOWLEDGMENTS\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\nREFERENCES\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-\ning to retrieve reasoning paths over wikipedia graph for question answering. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=SJgVHkrYDH.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and appli-\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Associ-\nation for Computational Linguistics, 2023b. URL https://aclanthology.org/2023.\nfindings-acl.225.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037,\n2022. URL https://arxiv.org/abs/2212.08037.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time? arXiv\npreprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\n11\nPreprint.\nlearn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.\norg/abs/2305.14387.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/\n2305.14627.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International Conference on Machine Learning, 2020. URL\nhttps://dl.acm.org/doi/pdf/10.5555/3524938.3525306.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research, 2022a. URL https://openreview.net/\nforum?id=jKN1pXi7b0.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b. URL https:\n//arxiv.org/abs/2208.03299.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig.\nActive retrieval augmented generation.\narXiv preprint\narXiv:2305.06983, 2023. URL https://arxiv.org/abs/2305.06983.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. URL\nhttps://aclanthology.org/P17-1147.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019. URL https://arxiv.org/abs/1909.05858.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\nIn International Conference on Machine Learning, 2023. URL https://openreview.net/\nforum?id=AT8Iw8KOeC.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\nQ19-1026.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/\n2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\naugmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352.\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848, 2023a. URL https://arxiv.org/abs/2304.09848.\n12\nPreprint.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.\nURL https://arxiv.org/abs/2303.16634.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\nIn Advances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=5HaIds3ux5O.\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint\narXiv:2305.15225, 2023. URL https://arxiv.org/abs/2305.15225.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-\nrefine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. URL\nhttps://arxiv.org/abs/2303.17651.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2023. URL https://aclanthology.org/2023.\nacl-long.546.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nURL https://arxiv.org/abs/2203.11147.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.\norg/D18-1260.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\nfor weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.org/\nD19-1284.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:\n//arxiv.org/abs/2305.14251.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:\n//arxiv.org/abs/2112.09332.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.\norg/abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\n13\nPreprint.\nRyan Lowe. Training language models to follow instructions with human feedback. In Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?\nid=TG8KACxEON.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\nand Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint\narXiv:2304.01904, 2023. URL https://arxiv.org/abs/2304.01904.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00a8aschel,\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021. URL https://aclanthology.org/\n2021.naacl-main.200.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In Advances in Neural Information Processing Systems, 2021. URL https:\n//openreview.net/forum?id=Tqx7nJp7PR.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, 2020. URL https://dl.acm.\norg/doi/10.5555/3433701.3433727.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics, 2023. URL https://arxiv.org/abs/2302.00083.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv.org/\nabs/1707.06347.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nSch\u00a8arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\n//proceedings.mlr.press/v202/shi23a.html.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.566.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\nscale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), 2018. URL https://aclanthology.org/N18-1074.\n14\nPreprint.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.\norg/abs/2307.09288.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.\nURL https://arxiv.org/abs/2306.04751.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\nid=gEZrGCozdqR.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https:\n//arxiv.org/abs/2306.01693.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decom-\nposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633,\n2023. URL https://arxiv.org/abs/2305.00633.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\ncompression and selective augmentation, 2023. URL https://arxiv.org/abs/2310.\n04408.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language\nmodels robust to irrelevant context, 2023. URL https://arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https:\n//arxiv.org/abs/2305.06311.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\narXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\nagent tree search unifies reasoning acting and planning in language models, 2023. URL https:\n//arxiv.org/abs/2310.04406.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n15\nPreprint.\nAPPENDIX\nA\nSELF-RAG Details\n17\nA.1\nReflection Tokens.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.2\nSELF-RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.3\nSELF-RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB\nExperimental Details\n19\nB.1\nMore Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.2\nMore Details of Evaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC Results\n20\nC.1\nAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.2\nHuman Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.3\nQualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nD Full List of Instructions and Demonstrations for GPT-4\n21\n16\nPreprint.\nA\nSELF-RAG DETAILS\nA.1\nREFLECTION TOKENS.\nDefinitions of reflection tokens.\nBelow, we provide a detailed definition of reflection type and\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\nonly given at each output level.\n\u2022 Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\nto use evidence, which indicates that a model can continue to use the evidence retrieved\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\ngenerates multiple segments based on the passage.\n\u2022 Relevant (\nISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n\u2022 Supported (\nISSUP ): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\nsupported, Partially supported, and No support / Contradictory, follow-\ning Yue et al. (2023); Nakano et al. (2021).\n\u2022 Useful (\nISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\nas whether the response is a helpful and informative answer to the query, independently from\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\nDetails of GPT-4-based data collections.\nWe use the instruction and demonstration pairs to prompt\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\noutputs with \u201c##\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\nfor\nISSUP , 19,317 for relevance, 3,831 for utility.\nManual analysis of the GPT-4 predictions.\nThe authors of this paper manually assess randomly\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\nthe same instruction, demonstrations, and test instances. We found our assessments show high\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\ndisagreement between 1 and 2 or 4 and 5.\nA.2\nSELF-RAG TRAINING\nOverview of training.\nAlgorithm 2 provides a high-level overview of our training.\nFull list of seed datasets.\nTo sample diverse input-output pairs, we sample instances of the Open-\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\nPerformance of the Critic C.\nWe evaluate the accuracy of reward predictions by splitting GPT-4\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\n17\nPreprint.\nAlgorithm 2 SELF-RAG Training\n1: Input input-output data D = {X, Y }, generator M, C \u03b8\n2: Initialize C with a pre-trained LM\n3: Sample data {Xsample, Y sample} \u223c {X, Y }\n\u25b7 Training Critic LM (Section 3.2.1)\n4: for (x, y) \u2208 (Xsample, Y sample) do\n\u25b7 Data collections for C\n5:\nPrompt GPT-4 to collect a reflection token r for (x, y)\n6:\nAdd {(x, y, r)} to Dcritic\n7: Update C with next token prediction loss\n\u25b7 Critic learning; Eq. 1\n8: Initialize M with a pre-trained LM\n\u25b7 Training Generator LM (Section 3.2.2)\n9: for (x, y) \u2208 (X, Y ) do\n\u25b7 Data collection for M with Dcritic\n10:\nRun C to predict r given (x, y)\n11:\nAdd (x, y, r) to Dgen\n12: Update M on Dgen with next token prediction loss\n\u25b7 Generator LM learning; Eq. 2\nDataset name\ncategory\nData source\nthe number of instances\nGPT-4 Alpaca\nInstruction-following\nOpen-Instruct\n26,168\nStanford Alpaca\nInstruction-following\nOpen-Instruct\n25,153\nFLAN-V2\nInstruction-following\nOpen-Instruct\n17,817\nShareGPT\nInstruction-following\nOpen-Instruct\n13,406\nOpen Assistant 1\nInstruction-following\nOpen-Instruct\n9,464\nWizard of Wikipedia\nKnowledge-intensive\nKILT\n17,367\nNatural Questions\nKnowledge-intensive\nKILT\n15,535\nFEVER\nKnowledge-intensive\nKILT\n9,966\nOpenBoookQA\nKnowledge-intensive\nHF Dataset\n4,699\nArc-Easy\nKnowledge-intensive\nHF Dataset\n2,147\nASQA\nKnowledge-intensive\nASQA\n3,897\nTable 3: The generator LM M training data statistics.\nbase LM\nRetrieve\nISSUP\nISREL\nISUSE\nLlama2-7B\n93.8\n93.5\n80.2\n73.5\nFLAN-3B\n85.6\n73.1\n82.0\n72.1\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\nlower performance on\nISUSE , this is because both models often confuse between the two highest\ncases (5 and 4), where human annotators can also disagree.\nDetails of M data creation.\nHere, we provide detailed data creation procedures. Algorithm 3\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\nnot. For the instances where the critic predicts Retrieve =No, we only predict the\nISUSE given input\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\nthen split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. If C predicts\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\nt-th segment. For each retrieved passage, we predict\nISREL and\nISSUP . If there is any passage and\ncontinuation with\nISREL =Relevant and\nISSUP =Fully Supported /\nISSUP =Partially\n7https://spacy.io/\n18\nPreprint.\nSupported, then we sample it as the continuation. If there is more than one passage satisfying this\ncriterion, we use the one with the highest retrieval score. If there are only\nISREL =Irrelevant or\nISSUP =No Support passages, we randomly sample one passage.\nAlgorithm 3 Mgen Data creation\n1: Input Input-output data D = X, Y\n2: for (x, y) \u2208 {X, Y } do\n3:\nGiven (x, y) C predicts Retrieve\n4:\nif Retrieve is predicted then\n5:\nRetrieve relevant passages D using R given (x, y)\n\u25b7 Retrieve passages\n6:\nfor d \u2208 D do\n7:\nC predicts\nISREL for each d\n\u25b7 Predict relevance of passages\n8:\nC predicts\nISSUP for each (y, d)\n\u25b7 Predict supports of outputs\n9:\nC predicts\nISUSE for each d\n\u25b7 Predict overall utility (t = T only)\n10:\nSample d\n11:\nelse if Retrieve is not predicted then\n12:\nC predicts\nISUSE given x, y\nAdd augmented (x, y, d, r) to Dgen\nTraining examples.\nTable 4 show several training examples used for M training.\nA.3\nSELF-RAG INFERENCE\nDetails of beam-search score calculations.\nWe first compute scores for each critique type by\ntaking the normalized probabilities of desirable tokens. For\nISREL , we compute the score as follows:\ns( ISREL ) =\np( ISREL = RELEVANT)\np( ISREL = RELEVANT) + p( ISREL = IRRELEVANT).\nFor\nISSUP , we compute the score as follows:\ns( ISREL ) = p( ISSUP = FULLY)\nS\n+ 0.5 \u00d7 p( ISSUP = PARTIALLY)\nS\n,\nwhere S = P\nt\u2208{FULLY,PARTIALLY,NO} p( ISSUP = t). For\nISUSE where we have a five-scale score, we\ncompute the weighted sum of the scores. We assigns weighted scores of w = {\u22121, \u22120.5, 0, 0.5, 1}\nto the tokens\nISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:\ns( ISUSE ) =\n5\nX\ni\nwi\np( ISUSE = i)\nS\n,\nwhere S = P\nt\u2208{1,2,3,4,5} p( ISUSE = t).\nDetails of adaptive retrieval.\nFor retrieval based on soft constraints, we trigger retrieval if the\nfollowing condition is satisfied:\np( Retrieve = YES)\np( Retrieve = YES) + p(p( Retrieve = NO) > \u03b4.\nB\nEXPERIMENTAL DETAILS\nB.1\nMORE DETAILS OF TRAINING\nMore details of training and computations.\nWe use 4 Nvidia A100 with 80GB memory to train\nour models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\nmemory.\n19\nPreprint.\nB.2\nMORE DETAILS OF EVALUATIONS\nRetrieval setup details.\nBy default, we use Contriever-MS MARCO to retrieve the top five\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings.8 The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search9 and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\nDetailed experimental settings for individual datasets.\nFor OpenQA datasets, we set the max-\nimum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF-RAG inference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\none of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF-RAG at\neach segment level.\nTask-specific instructions.\nTable 5 shows the list of the instructions used during evaluations. For\nOpen-domain QA, we do not provide explicit instructions.\nC\nRESULTS\nC.1\nANALYSIS\nReliance on parametric- and non-parametric memories.\nWe conduct analysis on how frequently\nmodel answers come from retrieved passages (non-parametric memories) or their own parametric\nmemories. On two open-domain QA datasets, TriviaQA and PopQA, we conduct the following\nanalysis: 1) sample query models successfully answer correctly, 2) for each query in this group,\ncheck whether the matched ground-truth answer is a sub-string of the retrieved passage or not. We\nevaluate SELF-RAG 7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF-RAG\nsignificantly less frequently generates answers that are not included in the provided evidence; in\nparticular, in Alpaca 30B, 20% of the correct predictions are not included in the provided passages,\nfollowed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF-RAG. When\nretrieved passages are not relevant, SELF-RAG generates\nISREL =Irrelevant, indicating that the\nfollowing answers may not be factually grounded, while those instruction-tuned models continue to\ngenerate plausible answers.\n8https://github.com/facebookresearch/atlas\n9https://programmablesearchengine.google.com/about/\n20\nPreprint.\nC.2\nHUMAN EVALUATION EXAMPLES\nTable 6 shows examples with human evaluations on S&P and correctness of\nISREL and\nISSUP\nreflection tokens.\nC.3\nQUALITATIVE EXAMPLES\nTable 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model\noutput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\nday of rest from labor, and further the second citation supports the fact that the official adoption\nof Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\nContradictory to the first output as the output says the person has served as the CEO since 2010,\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\neasily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\nD\nFULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\nthe instruction and demonstrations used to collect the three-way output tokens for\nRetrieve given\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\nused to collect the three-way output tokens for\nISREL . Table 11 shows an instruction and demon-\nstrations used to collect the three-way output tokens for\nISREL . Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for\nISUSE .\n21\nPreprint.\nInput Identify the two types of computer memory. (from gpt4 alpaca)\nOutput [ Retrieve =Yes]<p>Computer memory Computer memory \u2013 In computing, memory refers to the\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\nwith the term \u201cprimary storage\u201d. Computer memory operates at a high speed, for example random-access memory\n(RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If\nneeded, contents of the computer memory can be transferred to secondary storage; a very common way of doing\nthis is through a memory management technique called \u201cvirtual memory\u201d. An archaic synonym for memory is\nstore. The term \u201cmemory\u201d, meaning \u201cprimary storage\u201d or \u201cmain memory\u201d, is<p>[\nISREL =Relevant]Primary\nmemory, also known as main memory, is the memory that the computer can directly access and manipulate.\n[\nISSUP =Fully Supported] [ Retrieve =Yes]<p>Computer memory\u2013often associated with addressable\nsemiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as\nprimary storage but also other purposes in computers and other digital electronic devices. There are two main\nkinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory\n(used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware\nsuch as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access\nmemory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM)\nthat is fast but energy-consuming, offering lower memory<p>[\nISREL =Relevant]Primary memory can be\naccessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[\nISSUP =Partially\nSupported]...[\nISUSE :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt4 alpaca)\nOutput [ Retrieve =No]The manager realizes that there is an extensive list of tasks that need to be completed in\norder to make the event a success.[ Retrieve =Yes]<p>Delegation\u2013Delegation is the assignment of any responsi-\nbility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\nis one of the core concepts of management leadership. However, the person who delegated the work remains\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\nmuch input,<p>[\nISREL =Relevant]This includes tasks such as creating a guest list, designing and sending in-\nvitations, booking a venue, coordinating with vendors, planning the event\u00b4s program, and monitoring the budget.[\nISSUP =Partially Supported][ Retrieve =No]Since it is not possible for the manager to handle everything\nalone, they must delegate tasks to their team members.[ Retrieve =No]The manager would then assign each\nteam member specific responsibilities based on their individual skillsets and strengths.[ Retrieve =Yes]<p>Joint\napplication design \u2013 Subject Matter Experts: These are the business users, the IS professionals, and the outside\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting<p>[\nISREL\n=Irrelevant]For instance, the team\u00b4s graphic designer might be tasked with designing the invitations, while\nthe event coordinator would be in charge of planning the program and working with vendors...[ Retrieve =No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of the event.[\nISUSE\n=5]\nInput Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford alpaca)\nOutput [ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\npeaceful moment in its trance. [ Retrieve =No] A chorus of crickets sing a lullaby, To every creature that roams\nthe sky [\nISUSE =5]\nInput when did walking dead season 7 come out (from nq)\nOutput [ Retrieve =Yes]<p>The Walking Dead (season 7)\u2013The Walking Dead (season 7) The seventh season of\n\u201cThe Walking Dead\u201d, an American post-apocalyptic horror television series on AMC, premiered on October 23,\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\n...<p>[\nISREL =Relevant]October 23 , 2016[\nISSUP =Fully Supported][\nISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\nresponse: Hello, I love German shepherd dogs and you? \u2013 My favorite dog, they are officially known as German\nShepherd Dogs? \u2013 Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent.[ Retrieve =Yes]<p>German Shepherd \u2013 a simple\ndifference of opinion, it is the fundamental issue of the breed\u00b4s essential conformation and movement. ... They\nare known for being easy to train and good for performing tasks and following instructions. They are especially\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\nholding suspects. <p>[\nISREL =Relevant]Because of this, they are often used for disability assistance and\nfor police work. [\nISSUP =Partially Supported][\nISUSE =2]\nTable 4: List of the training examples.\n22\nPreprint.\nDataset\nInstruction\nARC-C\nGiven four answer candidates, A, B, C and D, choose the best answer choice. Please answer\nwith the capitalized alphabet only, without adding any extra phrase or period.\nPubHealth\nIs the following statement correct or not? Say true if it\u2019s correct; otherwise, say false. Don\u2019t\ncapitalize or add periods, just say \u201ctrue\u201d or \u201cfalse\u201d.\nBio Generation\nTell me a bio about [Person Name]\nASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\nthe documents.\nASQA (ours)\nAnswer the following question. The question may be ambiguous and have multiple correct\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don\u2019t\nuse any task specific instruction and simply use the original questions as input query.\n23\nPreprint.\nInput What genre is A Question and Answer Guide to Astronomy? (PopQA)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL =Relevant] A Question and Answer Guide to As-\ntronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [\nISSUP =Fully\nSupported][\nISUSE =5]\nReference [1] A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\nof science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\nS&P Supported and Plausible.\nCorrectness of\nISREL and\nISSUP : Both\nISREL and\nISSUP are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (PopQA)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL =Relevant] The author of The Lie is Sam Harris.[\nISSUP =Fully Supported][\nISUSE =5]\nReference [1] Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\nwhere others often lie.\nS&P Plausible but not supported.\nCorrectness of\nISREL and\nISSUP\nBoth\nISREL and\nISSUP are incorrect, as the Reference [1] is about a\ndifferent book whose title is similar to The Lie.\nInput Tell me a bio about Ronaldinho (Bio)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL =Relevant] Ronaldinho is a retired professional\nfootballer from Brazil.[\nISSUP =Fully Supported][ Retrieve =Continue]He played as an attacking\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\nkicks[ Retrieve =Continue]He won two FIFA World Player of the Year awards and a Ballon d\u2019Or.[\nISUSE =5]\nReference [1] Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Ga\u00b4ucho or\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\none of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d\u2019Or. A\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\nS&P Plausible and fully supported.\nCorrectness of\nISREL and\nISSUP Both\nISREL and\nISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL\n=Relevant] Sergei Vladimirovich Bodrov\n(born June 28,\n1948) is a Russian film director,\nscreenwriter,\nand producer.[\nISSUP\n=Fully\nSupported][ Retrieve =Continue]He was born in Moscow, Russia.[ Retrieve =Continue]He is the presi-\ndent of the Russian Guild of Film Producers.[\nISUSE =5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\nS&P Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\nin Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of\nISREL and\nISSUP\nISREL and\nISSUP are correct as the reference document is highly relevant\nand the first sentence is fully supported.\nTable 6: Examples of human evaluation results.\n24\nPreprint.\nInput When did sunday became the seventh day of the week (asqa)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL =Relevant] In 321 AD, the Roman Emperor Con-\nstantine the Great made Sunday a day of rest from labor, stating: \u201cOn the venerable Day of the Sun let the\nmagistrates and people residing in cities rest, and let all workshops be closed\u201d.[\nISSUP =Fully Supported]\n[ Retrieve =Yes] <p>Reference [2]<p>[\nISREL =Relevant] The official adoption of Sunday as a day of\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week.[2][\nISSUP =Fully\nSupported][\nISUSE =5]\nReference [1] Emperor Constantine then enacted the first Sunday Laws, for \u201cthe venerable Day of the Sun\u201d\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lord\u00b4s Day assembly is evidenced in\nIgnatius\u00b4s letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\nfact that the canon had to be issued at all is an indication that adoption of Constantine\u2019s decree of 321 was still\nnot universal ...\nInput Tell me a bio about Rory Tapner. (bio generation)\nOutput <p>Reference [1]<p>[\nISREL =Relevant] Rory Tapner is a British businessman who has served as\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[\nISSUP\n=Contradictory], Reference [2]<p>[\nISREL =Relevant] Rory Tapner was born on 30 September 1959\nin London, England.[\nISSUP =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\nas CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [ Retrieve =Yes] <p>Reference [1]<p>[\nISREL =Relevant] G. Venugopal is a popu-\nlar playback singer in the Malayalam film industry. [ Retrieve =Continue] He has sung over 300 songs in\nMalayalam, Tamil, and Hindi movies.[\nISSUP =Partially Supported] [ Retrieve =Yes] <p>Reference\n[2]<p>G. Venugopal was born on December 10, 1960 in India.[\nISSUP =Fully Supported]\nReference [1] \u2019G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\nwork in Malayalam films. He started his singing career in the film \u201cOdaruthammaava Aalariyaam\u201d (1984). Since\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\nthird time for the song \u201daadedee..aadaadedee\u201d from the film \u201dUllam\u201d.Venugopal\u2019\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Profes-\nsional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\nTable 7: Examples of outputs.\n25\nPreprint.\nInstructions\nGiven an instruction, please make a judgment on whether finding some external documents\nfrom the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or\n[No] and write an explanation.\nDemonstrations\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or\nsome reliable sources to explain the effects of different behaviors on health. So retrieving\ndocuments is helpful to improve the response to this query.\nInstruction Describe a time when you had to make a difficult decision.\nNeed retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not\nrequire one to find some external documents.\nInstruction Write a short story in third person narration about a protagonist who\nhas to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external\nevidence to verify.\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a\nwidely known fact, retrieving web documents for this question can still help.\nInstruction Find the area of a circle given its radius. Radius = 4\nNeed retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents\ndescribing a formula, it is unlikely to find a document exactly mentioning the answer.\nInstruction Arrange the words in the given sentence to form a grammatically cor-\nrect sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn\u2019t require any external evidence, as it is a simple grammatical\nquestion.\nInstruction Explain the process of cellular respiration in plants.\nNeed retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is\nhighly likely that we can find a reliable and useful document to support the response.\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.\n26\nPreprint.\nInstructions\nYou will be provided with an instruction, evidence, output sentence, and preceding sentences\n(optional). If the preceding sentence is given, the output should be the sentence that follows\nthose preceding sentences. Your task is to determine whether the information in the output\nsentence can be fully verified by the evidence or if it requires further external verification.\nThere are three cases:\n- If the output sentence can be verified solely with the evidence, then respond with [Continue\nto Use Evidence].\n- If the sentence doesn\u2019t require any factual verification (e.g., a subjective sentence or a\nsentence about common sense), then respond with [No Retrieval].\n- If additional information is needed to verify the output sentence, respond with [Retrieval].\nPlease provide explanations for your judgments.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nEvidence: Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\nclassification, predicting the next word in a sequence, and understanding synonyms and\nanalogies.\nRating [Retrieval]\nExplanation The output discusses the applications of word embeddings, while the evidence\nonly discusses the definitions of word embeddings and how they work. Therefore, we need to\nretrieve other evidence to verify whether the output is correct or not.\nTable 9: Instructions and demonstrations for Retrieve aspect given the input, preceding generations,\nand retrieved passages.\n27\nPreprint.\nInstructions\nYou\u2019ll be provided with an instruction, along with evidence and possibly some preceding\nsentences. When there are preceding sentences, your focus should be on the sentence that\ncomes after them. Your job is to determine if the evidence is relevant to the initial instruction\nand the preceding context, and provides useful information to complete the task described in\nthe instruction. If the evidence meets this requirement, respond with [Relevant]; otherwise,\ngenerate [Irrelevant].\nInstruction Given four answer options, A, B, C, and D, choose the best answer.\nInput Earth\u2019s rotating causes\nA: the cycling of AM and PM\nB: the creation of volcanic eruptions\nC: the cycling of the tides\nD: the creation of gravity\nEvidence Rotation causes the day-night cycle which also creates a corresponding cycle of\ntemperature and humidity creates a corresponding cycle of temperature and humidity. Sea\nlevel rises and falls twice a day as the earth rotates.\nRating [Relevant]\nExplanation The evidence explicitly mentions that the rotation causes a day-night cycle, as\ndescribed in the answer option A.\nInstruction age to run for US House of Representatives\nEvidence The Constitution sets three qualifications for service in the U.S. Senate: age (at\nleast thirty years of age); U.S. citizenship (at least nine years); and residency in the state a\nsenator represents at the time of election.\nRating [Irrelevant]\nExplanation The evidence only discusses the ages to run for the US Senate, not for the\nHouse of Representatives.\nTable 10: Instructions and demonstrations for\nISREL aspect given the input only.\n28\nPreprint.\nInstructions\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the\npreceding sentence is given, the output should be the sentence that follows those preceding\nsentences. Your task is to evaluate if the output is fully supported by the information provided\nin the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions\nfrom the evidence. This is only applicable when the output and part of the evidence are\nalmost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there\nis major information in the output that is not discussed in the evidence. For example, if an\ninstruction asks about two concepts and the evidence only discusses either of them, it should\nbe considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the\nevidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the\ninstruction.\nMake sure to not use any external information/knowledge to judge whether the out-\nput is true or not. Only check whether the output is supported by the evidence, and not\nwhether the output follows the instructions or not.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification,\npredicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Methods to generate this mapping include neural networks, dimensionality\nreduction on the word co-occurrence matrix, probabilistic models, explainable knowledge\nbase method, and explicit representation in terms of the context in which words appear. Word\nand phrase embeddings, when used as the underlying input representation, have been shown\nto boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next\ntoken predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the\nevidence mentions all of the applications syntactic parsing, sentiment analysis, next token\npredictions as well as analogy detection as the applications. Therefore, the score should be\n[Fully supported].\nTable 11: Instructions and demonstrations for\nISSUP tokens.\n29\nPreprint.\nInstructions\nGiven an instruction and an output, rate whether the response appears to be a helpful and\ninformative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived\nutility. The detailed criterion is as follows: 5: The response provides a complete, highly\ndetailed, and informative response to the query, fully satisfying the information needs. 4: The\nresponse mostly fulfills the need in the query, while there can be some minor improvements\nsuch as discussing more detailed information, having better structure of the response, or im-\nproving coherence. 3: The response is acceptable, but some major additions or improvements\nare needed to satisfy users\u2019 needs. 2: The response still addresses the main request, but it is\nnot complete or not relevant to the query. 1: The response is barely on-topic or completely\nirrelevant.\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime\nminister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it\ndoesn\u2019t answer the instruction. Therefore, the utility is 2.\nInstruction Given a description of a travel destination, recommend 10 tourist attrac-\ntions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights\ninclude the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish\nMarket, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn\u2019t provide descriptions of each attraction and the number of\nthe attractions is also less than 10. While this output partially answers the instructions, it\ndoesn\u2019t match the instructions strictly.\nTable 12: Instructions and demonstrations for\nISUSE tokens.\n30\n"
  },
  {
    "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
    "link": "https://arxiv.org/pdf/2310.11954.pdf",
    "upvote": "22",
    "text": "MusicAgent: An AI Agent for Music Understanding and Generation with\nLarge Language Models\nDingyao Yu1,2, Kaitao Song2, Peiling Lu2, Tianyu He2\nXu Tan2, Wei Ye1\u2217, Shikun Zhang1\u2217, Jiang Bian2\nPeking University1, Microsoft Research Asia2\n{yudingyao, wye, zhangsk}@pku.edu.cn,\n{kaitaosong, peil, tianyuhe, xuta, jiabia}@microsoft.com\nhttps://github.com/microsoft/muzic\nAbstract\nAI-empowered music processing is a diverse\nfield that encompasses dozens of tasks, ranging\nfrom generation tasks (e.g., timbre synthesis)\nto comprehension tasks (e.g., music classifica-\ntion). For developers and amateurs, it is very\ndifficult to grasp all of these task to satisfy their\nrequirements in music processing, especially\nconsidering the huge differences in the repre-\nsentations of music data and the model appli-\ncability across platforms among various tasks.\nConsequently, it is necessary to build a system\nto organize and integrate these tasks, and thus\nhelp practitioners to automatically analyze their\ndemand and call suitable tools as solutions to\nfulfill their requirements. Inspired by the recent\nsuccess of large language models (LLMs) in\ntask automation, we develop a system, named\nMusicAgent, which integrates numerous music-\nrelated tools and an autonomous workflow to\naddress user requirements. More specifically,\nwe build 1) toolset that collects tools from di-\nverse sources, including Hugging Face, GitHub,\nand Web API, etc. 2) an autonomous workflow\nempowered by LLMs (e.g., ChatGPT) to orga-\nnize these tools and automatically decompose\nuser requests into multiple sub-tasks and in-\nvoke corresponding music tools. The primary\ngoal of this system is to free users from the\nintricacies of AI-music tools, enabling them to\nconcentrate on the creative aspect. By grant-\ning users the freedom to effortlessly combine\ntools, the system offers a seamless and enrich-\ning music experience. The code is available on\nGitHub1 along with a brief instructional video2.\n1\nIntroduction\nAI-empowered music processing is a multifaceted\nand intricate domain, encompassing a diverse range\n*Corresponding Author:\nWei Ye, wye@pku.edu.cn;\nShikun Zhang, zhangsk@pku.edu.cn\n1https://github.com/microsoft/muzic/tree/main/\nmusicagent\n2https://youtu.be/tpNynjdcBqA\nRequest\nLLM\nTask List\nName\nInput\nOutput\n+\nTool List\nName\nDescrip\n-tion\nName\nDescrip\n-tion\nText\nAudio\n\u2026\nResult\nResponse\nTask Planning\nTool Selection\nTask Execution\nResponse Generation\nFigure 1: MusicAgent has gathered a rich collection\nof music-related tasks and diverse sources of tools, ef-\nfectively integrating them with LLMs to achieve profi-\nciency in handling complex music tasks.\nof tasks. Mastering this field presents a challenging\nendeavor due to the wide array of tasks it involves.\nGenerally, the realm of music includes various gen-\neration and comprehension tasks, such as songwrit-\ning (Sheng et al., 2021; Ju et al., 2021), music gen-\neration (Agostinelli et al., 2023; Dai et al., 2021;\nLu et al., 2023; Lv et al., 2023), audio transcrip-\ntion (Benetos et al., 2018; Foscarin et al., 2020),\nmusic retrieval (Wu et al., 2023b), etc. Specifi-\ncally, music is a complex art form that weaves to-\ngether a variety of distinct elements, such as chords\nand rhythm, to create vibrant and intricate content.\nPrevious works have frequently encountered chal-\nlenges in collaborating to complete complex music\ntasks, primarily due to differences in music feature\ndesigns and variations across platforms. Therefore,\narXiv:2310.11954v2  [cs.CL]  25 Oct 2023\nhow to build a system to automatically accomplish\nmusic-related tasks from the requests of users with\nvarying levels of expertise is still an enticing direc-\ntion worth exploring.\nRecently, large language models (LLMs) have at-\ntracted considerable attention due to their outstand-\ning performance in solving natural language pro-\ncessing (NLP) tasks (Brown et al., 2020; Ouyang\net al., 2022; Zhang et al., 2022b; Chowdhery et al.,\n2022; Zeng et al., 2022; Touvron et al., 2023).\nThe huge potentials of LLMs also inspire and di-\nrectly facilitate many emerging techniques (e.g.,\nin-context learning (Xie et al., 2021; Min et al.,\n2022), instruct tuning (Longpre et al., 2023; Wang\net al., 2022), and chain-of-thought prompting (Wei\net al., 2022; Kojima et al., 2022)), which also fur-\nther elevate the capability of LLMs. On the ba-\nsis of these LLM capabilities, many researchers\nhave extended the scope of LLMs to various top-\nics. They borrow the idea of acting LLMs as the\ncontrollers to orchestrate various domain-specific\nexpert models for solving complex AI tasks, such\nas HuggingGPT (Shen et al., 2023), AutoGPT and\nother modality-specifics ones (Chen et al., 2022;\nWu et al., 2023a; Huang et al., 2023). These suc-\ncesses also motivate us to explore the possibility to\ndevelop a system capable of assisting with various\nmusic-related tasks.\nDistinguishing from other modalities, incorpo-\nrating LLMs with music presents the following\nfeatures and challenges:\n1. Tool Diversity: On one hand, music-related\ntasks exhibit a wide range of diversity, and\non the other hand, the corresponding tools for\nthese tasks might not always reside on the\nsame platform. These tools could be parame-\nterized models available in open-source com-\nmunities like GitHub, presented as software\nand applications, or even hosted through Web\nAPIs for certain retrieval tasks. Considering\nall these factors is crucial when undertaking a\ncomprehensive music workflow.\n2. Cooperation: The collaboration between mu-\nsic tools is also constrained by two factors.\nFirst, the diversity of music domain tasks\nleads to the absence of explicit input-output\nmodality standards. Second, even when the\nmodalities are identical, the music formats\nmay differ, for instance, between symbolic\nmusic and audio music.\nTo address these issues, we introduce MusicA-\ngent, a specialized system designed to tackle the\nchallenges. Inspired by recent work like Hugging-\nGPT (Shen et al., 2023), MusicAgent is a frame-\nwork that utilizes the power of LLMs as the con-\ntroller and massive expert tools to accomplish user\ninstructions, just as illustrated in Figure 1. For\nthe toolset, in addition to utilizing the models pro-\nvided by Hugging Face, we further integrate vari-\nous methods from different sources, including code\nfrom GitHub and Web APIs. To make collaboration\nbetween diverse tools, MusicAgent enforces stan-\ndardized input-output formats across various tasks\nto promote seamless cooperation between tools. As\na music-related system, all samples are trimmed\nto fit within a single audio segment, facilitating\nfundamental music operations among samples. For\nmore system details and guidance on integrating\nadditional tools, please refer to Section 3.\nOverall, the MusicAgent presents several signifi-\ncant contributions:\n\u2022 Accessibility: MusicAgent eliminates the\nneed to master complex AI music tools. By\nutilizing LLMs as the task planner, the system\ndynamically selects the most suitable meth-\nods for each music-related task, making music\nprocessing accessible to a broader audience.\n\u2022 Unity: MusicAgent bridges the gap between\ntools from diverse sources by unifying the\ndata format (e.g., text, MIDI, ABC notation,\naudio). The system enables seamless coopera-\ntion among tools on different platforms.\n\u2022 Modularity: MusicAgent is highly extensi-\nble, allowing users to easily expand its func-\ntionality by implementing new functions, in-\ntegrating GitHub projects, and incorporating\nHugging Face models.\n2\nRelated Works\n2.1\nAI-Empowered Music Processing\nMusic generation and understanding are multi-\nfaceted tasks that encompass various sub-tasks. In\nthe realm of music generation, these tasks involve\nmelody generation (Yu et al., 2020; Zhang et al.,\n2022a; Yu et al., 2022), audio generation (Don-\nahue et al., 2018), singing voice synthesis (Ren\net al., 2020; Lu et al., 2020), and sound mixing. In\ncontrast, music understanding encompasses track\nseparation (D\u00e9fossez et al., 2019), audio recogni-\ntion, score transcription (Bittner et al., 2022), audio\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nFigure 2: MusicAgent consists of four core components: the task planner, tool selector, task executor, and response\ngenerator. Among these, the task planner, tool selector, and response generator are built upon language language\nmodels (LLMs). When users make requests, MusicAgent decomposes and organizes the requests into subtasks.\nThe system then selects the most suitable tool for each task. The chosen tool processes the input and populates\nthe anticipated output. The LLM subsequently organizes the output, culminating in a comprehensive and efficient\nmusic processing system.\nclassification (Choi et al., 2017; Zeng et al., 2021),\nand music retrieval (Wu et al., 2023b). In addition\nto these diverse and complex music-related tasks,\nanother significant challenge in traditional music\nprocessing is substantial differences in input and\noutput formats across each task. These diversities\nin tasks and data formats also hinder the unification\nin music processing, which makes it difficult for us\nto develop a copilot for solving different musical\ntasks. Therefore, in this paper, we will discuss how\nto design a copilot to unified musical data format\nand combine these tools to automatically accom-\nplish tasks by utilizing large language model.\n2.2\nLarge Language Models\nThe field of natural language processing (NLP) is\nundergoing a revolutionary shift due to the emer-\ngence of large language models (LLMs). These\nmodels (Brown et al., 2020; Touvron et al., 2023)\nhave exhibited powerful performance in various\nlanguage tasks, such as translation, dialogue mod-\neling, and code completion, making them a focal\npoint in NLP.\nBased on these advantages, LLMs have been ap-\nplied to many applications. Recently, a new trend\nis to use LLMs to build autonomous agents for\ntask automation, just like AutoGPT 3 and Hugging-\nGPT (Shen et al., 2023). In these works, they will\nleverage an LLM as the controller to automatically\nanalyze user requests and then invoke the appropri-\nate tool for solving tasks. Although there are some\nsuccessful trials in vision (Chen et al., 2022) or\nspeech (Huang et al., 2023), it is still challenging\nto build an autonomous agent for music process-\ning, due to its diversity and complexity in tasks\nand data. Therefore, we present a system called\nMusicAgent, which integrates various functions to\nhandle multiple music-related tasks, to accomplish\nrequests from different users, including novices\nand professionals.\n3https://github.com/Significant-Gravitas/\nAuto-GPT\nTable 1: Overview of tasks and the associated example tools in MusicAgent.\nTask\nInput\nOutput\nTask Type\nExample Tool\ntext-to-symbolic-music\ntext\nsymbolic music\nGeneration\nMuseCoco4\nlyric-to-melody\ntext\nsymbolic music\nGeneration\nROC5\nsinging-voice-synthesis\ntext\naudio\nGeneration\nHiFiSinger6\ntext-to-audio\ntext\naudio\nGeneration\nAudioLDM\ntimbre-transfer\naudio\naudio\nGeneration\nDDSP7\naccompaniment\nsymbolic music\nsymbolic music\nGeneration\nGetMusic8\nmusic-classification\naudio\ntext\nUnderstanding\nWav2vec2\nmusic-separation\naudio\naudio\nUnderstanding\nDemucs\nlyric-recognition\naudio\ntext\nUnderstanding\nWhisper-large-zh9\nscore-transcription\naudio\ntext\nUnderstanding\nBasic-pitch\nartist/track-search\ntext\naudio\nAuxiliary\nSpotify API10\nlyric-generation\ntext\ntext\nAuxiliary\nChatGPT\nweb-search\ntext\ntext\nAuxiliary\nGoogle API\n3\nMusicAgent\nMusicAgent is a comprehensive system that en-\nhances the capabilities of large language models\n(LLMs) and tailors them to the music domain\nby integrating additional data sources, dependent\ntools, and task specialization. As illustrated in Fig-\nure 2, MusicAgent designs an LLM-empowered\nautonomous workflow, which includes three key\nskills: Task Planner, Tool Selector, and Response\nGenerator.\nThese skills, along with the music-\nrelated tools forming the Task Executor, are in-\ntegrated, resulting in a versatile system capable of\nexecuting various applications. In this section, we\nwill delve into different aspects of this system, ex-\nploring its functionalities and contributions to the\nfield of music processing.\n3.1\nTasks and Tools Collection\nTable 1 provides a comprehensive overview of the\nmusic-related tasks and representative tools gath-\nered in the current MusicAgent. We have organized\nthe task sets based on the music processing flow\nillustrated in Figure 3. Aside from generation and\nunderstanding tasks, the collected tasks are primar-\nily categorized into three groups:\n4https://github.com/microsoft/muzic/tree/main/\nmusecoco\n5https://github.com/microsoft/muzic\n6https://github.com/CODEJIN/HiFiSinger\n7https://github.com/magenta/ddsp\n8https://github.com/microsoft/muzic/tree/main/\nmusecoco/getmusic\n9https://huggingface.co/jonatasgrosman/\nwhisper-large-zh-cv11\n10https://spotify.com\nTextual \nDescription\n-\nEmotion\n-\nGenre\n-\nTheme\nSymbolic \nMusic\n-\nMelody\n-\nAccompaniment\nAudio\n-\nSinging Voice\n-\nInstrument Sound \nMixing\nSeparation\nGeneration\nUnderstanding\nFigure 3: MusicAgent collects tasks and tools within\nthe framework of music generation and understanding.\nIt encompasses various tasks, including single-modal\ntasks and modality transfer tasks, such as converting\nsheet music to audio through singing voice synthesis.\nGeneration tasks: This category includes text-\nto-music, lyric-to-melody, singing-voice-synthesis,\ntimbre-transfer, accompaniment, and etc. These\ntasks enable the collaborative music generation\nstarting from simple descriptions.\nUnderstanding tasks:\nThe tasks of music-\nclassification, music-separation, lyric recognition,\nmelody generation \u2026\n\u2026\nrelationships from music theory\u2026\n\u2026\nlyric: \u2026\n\u2026\ngenerate the lyrics \u2026 Finally, I used the \nand 0d63.wav \u2026\nreason: \u2026\nFigure 4: The LLM backend is responsible for the following steps: The Task Planner takes user requests and\nproduces parsed task queue, the Tool Selector chooses suitable tools, and the Response Generator collects tool\noutputs and organizes the responses.\nand music-transcription are under this category.\nCombining these tasks enables the conversion of\nmusic into symbolic representation and the analysis\nof various music features.\nAuxiliary tasks: This category encompasses web\nsearch and various audio processing toolkits. Web\nsearch includes text search using the Google API,\nas well as music search through the Spotify API.\nThese tasks primarily provide rich data sources and\nperform basic operations on audio/MIDI/text data,\nserving as auxiliary functions.\nFurthermore, Figure 3 illustrates the utilization\nof three main data formats in the system: i) text,\nwhich includes lyric, genre or any other attributes\nrelated to the music. ii) sheet music, represented\nas MIDI files, describes the score of the music. iii)\naudio, containing the sound of the music.\n3.2\nAutonomous Workflow\nThe MusicAgent system consists of two parts: the\nautonomous workflow and the plugins. The au-\ntonomous workflow serves as the core LLM in-\nteraction component, as shown in Figure 2, and\nit comprises three skills: Task Planner, Tool Se-\nlector, and Response Generator, all supported by\nthe LLM. Figure 4 further demonstrates how these\ncomponents work together harmoniously.\nTask Planner: The Task Planner plays a critical\nrole in converting user instructions into structured\ninformation, as most existing music tools only ac-\ncept specialized inputs. The user input processed\nby the Task Planner will form the backbone of the\nentire workflow, encompassing the determination\nof each subtask and its corresponding input-output\nformat, as well as the dependencies between the\nsubtasks, creating a dependency graph. Leverag-\ning in-context learning, MusicAgent demonstrates\nexcellent task decomposition performance. We pro-\nvide task planner descriptions, supported tasks, and\ninformation structure in the prompt, along with\nseveral examples of music task-related decompo-\nsitions. The user\u2019s interaction history and current\ninput will replace the content at the corresponding\nposition in the prompt. By utilizing the Semantic\nKernel (Microsoft, 2023), users can insert the re-\nquired task flow in text format, thereby enhancing\ntask planning effectiveness.\nTool Selector: The Tool Selector chooses the most\nappropriate tool from the open-source tools rele-\nvant to a specific subtask. Each tool is associated\nwith its unique attributes, such as textual descrip-\ntions, download count, star ratings, and more. By\nincorporating these tool attributes with the user\ninput, LLM presents the tool\u2019s ID and correspond-\ning reasoning for what it deems the most suitable\nselection. Users have the flexibility to adjust the\ntool attributes and determine how LLM interprets\nthese attributes. For instance, users can emphasize\ndownload count to meet diverse requirements.\nResponse Generator: The Response Generator\ngathers all intermediate results from the execution\nof subtasks and ultimately compiles them into a\ncoherent response. Examples in Figure 5 demon-\nstrate how LLM organizes the tasks and results to\ngenerate answers.\n3.3\nPlugins\nWhen all the dependent tasks of a subtask have\nbeen completed, and all inputs have been instan-\ntiated, the LLM backend passes the task to the\nTask Executor, where the tool selects the necessary\nparameters from the inputs. Additionally, the tool\nneeds to identify the task type, as a tool may handle\nmultiple tasks.\nMusicAgent stores model parameters on the\nCPU and only loads them into the GPU when ac-\ntively in use. This approach is especially advan-\ntageous for users with limited GPU memory, as it\noptimizes resource utilization and ensures smooth\ntask execution without overburdening the GPU\nmemory.\n4\nSystem Usage\nIn this section, we will provide comprehensive\nguidelines on how to effectively use the MusicA-\ngent toolkit.\n4.1\nCode Usage\nUsers have the flexibility to run this system either\nby following the instructions on GitHub or by in-\ntegrating it as a module in their code or using it\nthrough the command line for more advanced us-\nage, enabling the incorporation of custom tools. As\ndepicted in Listing 1, users can add custom task\ntypes, update tool attributes, and design prompts for\neach subtask, enhancing support for specific tasks.\nIt is important to note that embedding the prompt\nin the history is a temporary operation, and there is\na possibility of overlap if the context exceeds the\nlimit. For permanent storage, it is recommended to\ndirectly include the prompt in the code.\n## 1. Initialze the agent\nfrom agent import MusicAgent\nmusic_agent = MusicAgent(CONFIG_PATH)\n## 2. Add custom tasks and tools\nmusic_agent.task_map[MY_TASK ]. append(\nMY_TOOL)\nmusic_agent.pipelines.append(\nMY_TOOL_CLASS)\n# Update prompts\nmusic_agent._init_task_context ()\nmusic_agent._init_tool_context ()\n## 3. Update tool's information\nmusic_agent.update_tool_attributes(\nMY_TOOL , {\"stars\":..,\"likes\":..})\nmusic_agent._init_tool_context ()\n## 4. Update the prompt\n# Take task planner as an example\n# There is a risk of being overwritten\nmusic_agent.task_context[\"history\"] +=\n\"MY\u2423CUSTOM\u2423PROMPT\"\n## 5. Chat with the agent\nmusic_agent.chat(\"Generate\u2423a\u2423song ...\")\nListing 1: Code usage of MusicAgent\n4.2\nDemo Usage\nApart from command-line usage, we have also pro-\nvided a Gradio demo for users, where an OpenAI\ntoken is required. In the Gradio demo, users can\ndirectly upload audio and visually observe all the\nintermediate results generated by the system, as\ndepicted in Figure 6. Additionally, although Mu-\nsicAgent includes built-in context truncation, users\ncan still clear all LLM interaction history in the\ninterface to refresh the agent.\n5\nConclusion\nIn this paper, we introduce MusicAgent, an LLM-\npowered autonomous agent in the music domain.\nOur system can be considered as an auxiliary tool\nto help developers or audiences to automatically\nanalyze user requests and select appropriate tools\nas solutions. Moreover, our framework directly\nintegrates numerous music-related tools from var-\nious sources (e.g., Hugging Face, GitHub, Web\nsearch and etc). We also adapt the autonomous\nworkflow to enable better compatibility in musical\ntasks and allow users to extend its toolset. In the\nfuture, we also further envision integrating more\nmusic-related functions into MusicAgent.\nAcknowledgements\nWe extend our gratitude to all anonymous reviewers\nand members of the Machine Learning group at Mi-\ncrosoft Research Asia for their valuable contribu-\ntions and insightful suggestions in the development\nof this system.\nReferences\nAndrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos,\nJesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco\nTagliasacchi, et al. 2023. Musiclm: Generating mu-\nsic from text. arXiv preprint arXiv:2301.11325.\nEmmanouil Benetos, Simon Dixon, Zhiyao Duan, and\nSebastian Ewert. 2018. Automatic music transcrip-\ntion: An overview. IEEE Signal Processing Maga-\nzine, 36(1):20\u201330.\nRachel M. Bittner, Juan Jos\u00e9 Bosch, David Rubinstein,\nGabriel Meseguer-Brocal, and Sebastian Ewert. 2022.\nA lightweight instrument-agnostic model for poly-\nphonic note transcription and multipitch estimation.\nIn Proceedings of the IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing\n(ICASSP), Singapore.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nJun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed\nElhoseiny. 2022. Visualgpt: Data-efficient adapta-\ntion of pretrained language models for image caption-\ning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n18030\u201318040.\nKeunwoo Choi, Gy\u00f6rgy Fazekas, Mark Sandler, and\nKyunghyun Cho. 2017.\nConvolutional recurrent\nneural networks for music classification. In 2017\nIEEE International conference on acoustics, speech\nand signal processing (ICASSP), pages 2392\u20132396.\nIEEE.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nShuqi Dai, Zeyu Jin, Celso Gomes, and Roger B Dan-\nnenberg. 2021. Controllable deep melody generation\nvia hierarchical music structure representation. arXiv\npreprint arXiv:2109.00663.\nAlexandre D\u00e9fossez, Nicolas Usunier, L\u00e9on Bottou, and\nFrancis Bach. 2019. Demucs: Deep extractor for mu-\nsic sources with extra unlabeled data remixed. arXiv\npreprint arXiv:1909.01174.\nChris Donahue, Julian McAuley, and Miller Puckette.\n2018. Adversarial audio synthesis. arXiv preprint\narXiv:1802.04208.\nFrancesco Foscarin, Andrew Mcleod, Philippe Rigaux,\nFlorent Jacquemard, and Masahiko Sakai. 2020.\nAsap: a dataset of aligned scores and performances\nfor piano transcription.\nIn International Society\nfor Music Information Retrieval Conference, CONF,\npages 534\u2013541.\nRongjie Huang, Mingze Li, Dongchao Yang, Jia-\ntong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2023.\nAudiogpt: Understanding and generating speech,\nmusic, sound, and talking head.\narXiv preprint\narXiv:2304.12995.\nZeqian Ju, Peiling Lu, Xu Tan, Rui Wang, Chen Zhang,\nSongruoyao Wu, Kejun Zhang, Xiangyang Li, Tao\nQin, and Tie-Yan Liu. 2021. Telemelody: Lyric-to-\nmelody generation with a template-based two-stage\nmethod. arXiv preprint arXiv:2109.09617.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nPeiling Lu, Jie Wu, Jian Luan, Xu Tan, and Li Zhou.\n2020. Xiaoicesing: A high-quality and integrated\nsinging voice synthesis system.\narXiv preprint\narXiv:2006.06261.\nPeiling Lu, Xin Xu, Chenfei Kang, Botao Yu, Chengyi\nXing, Xu Tan, and Jiang Bian. 2023. Musecoco:\nGenerating symbolic music from text. arXiv preprint\narXiv:2306.00110.\nAng Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang,\nJiang Bian, and Rui Yan. 2023. Getmusic: Gen-\nerating any music tracks with a unified represen-\ntation and diffusion framework.\narXiv preprint\narXiv:2305.10841.\nMicrosoft. 2023. Semantic kernel. https://github.\ncom/microsoft/semantic-kernel.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022.\nRethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nYi Ren, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, and\nTie-Yan Liu. 2020. Deepsinger: Singing voice syn-\nthesis with data mined from the web. In Proceed-\nings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, pages\n1979\u20131989.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nZhonghao Sheng, Kaitao Song, Xu Tan, Yi Ren, Wei Ye,\nShikun Zhang, and Tao Qin. 2021. Songmass: Auto-\nmatic song writing with pre-training and alignment\nconstraint. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 13798\u2013\n13805.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi,\nYeganeh Kordi,\nAmirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nSuper-naturalinstructions: Generalization via declar-\native instructions on 1600+ nlp tasks. arXiv preprint\narXiv:2204.07705.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023a.\nVisual chatgpt:\nTalking, drawing and editing\nwith visual foundation models.\narXiv preprint\narXiv:2303.04671.\nShangda Wu, Dingyao Yu, Xu Tan, and Maosong Sun.\n2023b.\nClamp: Contrastive language-music pre-\ntraining for cross-modal symbolic music information\nretrieval. arXiv preprint arXiv:2304.11029.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nBotao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan,\nWei Ye, Shikun Zhang, Tao Qin, and Tie-Yan Liu.\n2022.\nMuseformer:\nTransformer with fine-and\ncoarse-grained attention for music generation. Ad-\nvances in Neural Information Processing Systems,\n35:1376\u20131388.\nYi Yu, Florian Harsco\u00ebt, Simon Canales, Gurunath\nReddy M, Suhua Tang, and Junjun Jiang. 2020.\nLyrics-conditioned neural melody generation.\nIn\nMultiMedia Modeling: 26th International Confer-\nence, MMM 2020, Daejeon, South Korea, January\n5\u20138, 2020, Proceedings, Part II 26, pages 709\u2013714.\nSpringer.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nMingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin,\nand Tie-Yan Liu. 2021. Musicbert: Symbolic music\nunderstanding with large-scale pre-training. arXiv\npreprint arXiv:2106.05630.\nChen Zhang, Luchin Chang, Songruoyao Wu, Xu Tan,\nTao Qin, Tie-Yan Liu, and Kejun Zhang. 2022a. Re-\nlyme: Improving lyric-to-melody generation by in-\ncorporating lyric-melody relationships. In Proceed-\nings of the 30th ACM International Conference on\nMultimedia, pages 1047\u20131056.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022b. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nA\nAppendix\nTo generate a Chinese poetry song for \nyou, I first used the ChatGPT model to \ngenerate the lyrics: \u2026. Then, I used the \nmuzic/roc model to convert the lyrics to \nsheet music, and generated the sheet \nmusic file e3a1.mid, and the \ncorresponding audio file e3a1.wav. Next, \nI used the DiffSinger model to convert \nthe sheet music to audio, and generated \nthe audio file 486f.wav. Finally, I used the \nbasic-merge model to convert the \ne3a1.wav and the 486f.wav. 486f.wav to \ngenerate the final audio file 726f.wav. \nYou can find these files in the \npublic/audios/ directory.\nGenerate a song in the \nstyle of Chinese poetry\nDownload some songs by Jay Chou and \nanalyze their styles\nTo download a Jay Chou song for you and \nseparate the vocals in it, I first searched \nJay Chou's songs from Spotify using the \ntrack-search model, and the search results \nincluded three versions of \"Tracks\", which \nare public/audios/8440.mp3, \npublic/audios/f68e.mp3, and public/ \naudios/840e.mp3. Then, I analyze these \nthree songs with wav2vec2 modeling and \nfinally get that all of them are in pop style.\nConverting vocals to violin in 8440.mp3\nTo convert the vocals in 8440.mp3 to the \nsound of a violin, I first used the demucs\nmodel to separate the 8440.mp3 into the \nhuman vocal file 05aa.wav, and then I used \nthe ddsp model to convert the 05aa.wav \ninto the sound of a violin, generating \n9561.wav. you can find these files in the \npublic/audios/ directory.\nFigure 5: Sample Q&A for users and agents.\nFigure 6: Gradio Demomstration.\n"
  },
  {
    "title": "Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts",
    "link": "https://arxiv.org/pdf/2310.11784.pdf",
    "upvote": "10",
    "text": "Preprint\nPROGRESSIVE3D: PROGRESSIVELY LOCAL EDITING\nFOR TEXT-TO-3D CONTENT CREATION WITH COM-\nPLEX SEMANTIC PROMPTS\nXinhua Cheng\u22171, Tianyu Yang\u2020 3, Jianan Wang3, Yu Li3, Lei Zhang3, Jian Zhang1, Li Yuan\u2020 1,2\n1Peking University\n2Peng Cheng Laboratory\n3International Digital Economy Academy (IDEA)\nABSTRACT\nRecent text-to-3D generation methods achieve impressive 3D content creation ca-\npacity thanks to the advances in image diffusion models and optimizing strate-\ngies. However, current methods struggle to generate correct 3D content for a\ncomplex prompt in semantics, i.e., a prompt describing multiple interacted ob-\njects binding with different attributes. In this work, we propose a general frame-\nwork named Progressive3D, which decomposes the entire generation into a se-\nries of locally progressive editing steps to create precise 3D content for complex\nprompts, and we constrain the content change to only occur in regions determined\nby user-defined region prompts in each editing step. Furthermore, we propose\nan overlapped semantic component suppression technique to encourage the op-\ntimization process to focus more on the semantic differences between prompts.\nExperiments demonstrate that the proposed Progressive3D framework is effec-\ntive in local editing and is general for different 3D representations, leading to\nprecise 3D content production for prompts with complex semantics for various\ntext-to-3D methods. Our project page is https://cxh0519.github.io/\nprojects/Progressive3D/\n1\nINTRODUCTION\nHigh-quality 3D digital content that conforms to the requirements of users is desired due to its var-\nious applications in the entertainment industry, mixed reality, and robotic simulation. Compared\nto the traditional 3D generating process which requests manual design in professional modeling\nsoftware, automatically creating 3D content with given text prompts is more friendly for both begin-\nners and experienced artists. Driven by the recent progress of neural 3D representations (Mildenhall\net al., 2020; Wang et al., 2021; Yariv et al., 2021; Shen et al., 2021) and text-to-image (T2I) diffusion\nmodels (Nichol et al., 2021; Rombach et al., 2022; Mou et al., 2023; Zhang et al., 2023), Dreamfu-\nsion (Poole et al., 2022) demonstrates impressive 3D content creation capacity conditioned on given\nprompts by distilling the prior knowledge from T2I diffusion models into a Neural Radiance Field\n(NeRF), which attracts board interests and emerging attempts in text-to-3D creation.\nAlthough text-to-3D methods have tried to use various 3D neural representations (Lin et al., 2023;\nChen et al., 2023; Tsalicoglou et al., 2023) and optimization strategies (Wang et al., 2023a; Huang\net al., 2023b; Wang et al., 2023b) for improving the quality of created 3D content and achieving\nremark accomplishments, they rarely pay attention to enhancing the semantic consistency between\ngenerated 3D content and given prompts. As a result, most text-to-3D methods struggle to produce\ncorrect results when the text prompt describes a complex scene involving multiple objects binding\nwith different attributes. As shown in Fig. 1(a), existing text-to-3D methods suffer from challenges\nwith complex prompts, leading to significant object missing, attribute mismatching, and quality\nreduction. While recent investigations (Feng et al., 2022; Huang et al., 2023a; Lu et al., 2023)\nhave demonstrated that current T2I diffusion models tend to generate inaccurate results when facing\n\u2217Work done during the internship at IDEA.\n\u2020Corresponding Authors.\n1\narXiv:2310.11784v2  [cs.CV]  16 Mar 2024\nPreprint\n\u201cAn astronaut holding a red \nrifle and riding a green \norigami motorcycle and \nwearing a cyan chef\u2019s hat.\u201d\n\u201cAn astronaut.\u201d\n\u201cAn astronaut \nholding a red rifle.\u201d\n\u201cAn astronaut \nholding a red rifle \nand riding a green \norigami motorcycle .\u201d\n\u201cAn astronaut holding a red \nrifle and riding a green \norigami motorcycle and \nwearing a cyan chef\u2019s hat.\u201d\n(a) Generating Directly\n(b) Generating with Progressive3D\nDreamTime\nTextMesh\nFantasia3D\n\u201cA yellow tulip and a blue \npeony and a red rose in a \ngolden vase.\u201d\n\u201cA golden vase.\u201d\n\u201cA red rose in a \ngolden vase.\u201d\n\u201cA blue peony and a \nred rose in a golden \nvase.\u201d\n\u201cA yellow tulip and a blue \npeony and a red rose in a \ngolden vase.\u201d\n\u201cThe Ironman with \nGundam\u2019s arm and Hulk\u2019s \narm and Spiderman\u2019s body.\u201d\n\u201cThe Ironman.\u201d\n\u201cThe Ironman with \nGundam\u2019s arm.\u201d\n\u201cThe Ironman with \nGundam\u2019s arm and \nHulk\u2019s arm.\u201d\n\u201cThe Ironman with \nGundam\u2019s arm and Hulk\u2019s \narm and Spiderman\u2019s body.\u201d\nMVDream\n\u201cA medieval soldier with \nmetal armor wearing an \nwhite astronaut helmet and \nholding a golden axe and \nriding a terracotta wolf.\u201d\n\u201cA medieval soldier \nwith metal armor.\u201d\n\u201cA medieval soldier \nwith metal armor \nholding a golden axe.\u201d\n\u201cA medieval soldier with \nmetal armor holding a \ngolden axe and riding a \nterracotta wolf.\u201d\n\u201cA medieval soldier with \nmetal armor wearing an \nwhite astronaut helmet and \nholding a golden axe and \nriding a terracotta wolf.\u201d\nFigure 1: Conception. Current text-to-3D methods suffer from challenges when given prompts de-\nscribing multiple objects binding with different attributes. Compared to (a) generating with existing\nmethods, (b) generating with Progressive3D produces 3D content consistent with given prompts.\nprompts with complex semantics and existing text-to-3D methods inherit the same issues from T2I\ndiffusion models, works on evaluating or improving the performance of text-to-3D methods in com-\nplex semantic scenarios are still limited. Therefore, how to generate correct 3D content consistent\nwith complex prompts is critical for many real applications of text-to-3D methods.\nTo address the challenges of generation precise 3D content from complex prompts, we propose\na general framework named Progressive3D, which decomposes the difficult creation of complex\nprompts into a series of local editing steps, and progressively generates the 3D content as is shown\nin Fig. 1(b). For a specific editing step, our framework edits the pre-trained source representation\nin the 3D space determined by the user-defined region prompt according to the semantic differ-\nence between the source prompt and the target prompt. Concretely, we propose two content-related\nconstraints, including a consistency constraint and an initialized constraint for keeping content be-\nyond selected regions unchanged and promoting the separate target geometry generated from empty\nspace. Furthermore, a technique dubbed Overlapped Semantic Component Suppression (OSCS) is\ncarefully designed to automatically explore the semantic difference between the source prompt and\nthe target one for guiding the optimization process of the target representations.\nTo evaluate Progressive3D, we construct a complex semantic prompt set dubbed CSP-100 consisting\nof 100 various prompts. Prompts in CSP-100 are divided into four categories including color, shape,\n2\nPreprint\nmaterial and composition according to appeared attributes. Experiments conducted on existing text-\nto-3D methods driven by different 3D representations including NeRF-based DreamTime (Huang\net al., 2023b) and MVDream(Shi et al., 2023), SDF-based TextMesh (Tsalicoglou et al., 2023), and\nDMTet-based Fantasia3D (Chen et al., 2023) demonstrate that our framework produces precise 3D\nmodels through multi-step local editing achieve better alignment with text prompts both in metrics\nand user studies than current text-to-3D creation methods when prompts are complex in semantics.\nOur contribution can be summarized as follows: (1) We propose a framework named Progressive3D\nfor creating precise 3D content prompted with complex semantics by decomposing a difficult gener-\nation process into a series of local editing steps. (2) We propose the Overlapped Semantic Compo-\nnent Suppression to sufficiently explore the semantic difference between source and target prompts\nfor overcoming the issues caused by complex prompts. (3) Experiments demonstrate that Progres-\nsive3D is effective in local editing and is able to generate precise 3D content consistent with complex\nprompts with various text-to-3D methods driven by different 3D neural representations.\n2\nRELATED WORKS\nText-to-3D Content Creation. Creating high-fidelity 3D content from only text prompts has at-\ntracted broad interest in recent years and there are many earlier attempts (Jain et al., 2022; Michel\net al., 2022; Mohammad Khalid et al., 2022). Driven by the emerging text-to-image diffusion mod-\nels, Dreamfusion (Poole et al., 2022) firstly introduces the large-scale prior from diffusion models\nfor 3D content creation by proposing the score distillation sampling and achieves impressive re-\nsults. The following works can be roughly classified into two categories, many attempts such as\nSJC (Wang et al., 2023a), Latent-NeRF (Metzer et al., 2022), Score Debiasing (Hong et al., 2023)\nDreamTime (Huang et al., 2023b), ProlificDreamer (Wang et al., 2023b) and MVDream(Shi et al.,\n2023) modify optimizing strategies to create higher quality content, and other methods including\nMagic3D (Lin et al., 2023), Fantasia3D (Chen et al., 2023), and TextMesh (Tsalicoglou et al., 2023)\nemploy different 3D representations for better content rendering and mesh extraction. However,\nmost existing text-to-3D methods focus on promoting the quality of generated 3D content, thus their\nmethods struggle to generate correct content for complex prompts since no specific techniques are\ndesigned for complex semantics. Therefore, we propose a general framework named Progressive3D\nfor various neural 3D representations to tackle prompts with complex semantics by decomposing the\ndifficult generation into a series of local editing processes, and our framework successfully produces\nprecise 3D content consistent with the complex descriptions.\nText-Guided Editing on 3D Content. Compared to the rapid development of text-to-3D creation\nmethods, the explorations of editing the generated 3D content by text prompts are still limited.\nAlthough Dreamfusion (Poole et al., 2022) and Magic3D (Lin et al., 2023) demonstrate that content\nediting can be achieved by fine-tuning existing 3D content with new prompts, such editing is unable\nto maintain 3D content beyond editable regions untouched since the fine-tuning is global to the\nentire space. Similar global editing methods also include Instruct NeRF2NeRF (Haque et al., 2023)\nand Instruct 3D-to-3D (Kamata et al., 2023), which extend a powerful 2D editing diffusion model\nnamed Instruct Pix2Pix (Brooks et al., 2023) into 3D content. Furthermore, several local editing\nmethods including Vox-E (Sella et al., 2023) and DreamEditor (Zhuang et al., 2023) are proposed\nto edit the content in regions specified by the attention mechanism, and FocalDreamer (Li et al.,\n2023) only generates the incremental content in editable regions with new prompts to make sure\nthe input content is unchanged. However, their works seldom consider the significant issues in\n3D creations including object missing, attribute mismatching, and quality reduction caused by the\nprompts with complex semantics. Differing from their attempts, our Progressive3D emphasizes the\nsemantic difference between source and target prompts, leading to more precise 3D content.\n3\nMETHOD\nOur Progressive3D framework is proposed for current text-to-3D methods to tackle prompts with\ncomplex semantics. Concretely, Progressive3D decomposes the 3D content creation process into a\nseries of progressively local editing steps. For each local editing step, assuming we already have\na source 3D representation \u03d5s supervised by the source prompt ys, we aim to obtain a target 3D\nrepresentation \u03d5t which is initialized by \u03d5s to satisfy the description of the target prompt yt and\n3\nPreprint\nthe 3D region constraint of user-defined region prompts yb. We first convert user-defined region\nprompts to 2D masks for each view separately to constrain the undesired contents in \u03d5t untouched\n(Sec. 3.1), which is critical for local editing. Furthermore, we propose the Overlapped Semantic\nComponent Suppression (OSCS) technique to optimize the target 3D representation \u03d5t with the\nguidance of the semantic difference between the source prompt ys and the target prompt yt (Sec. 3.2)\nfor emphasizing the editing object and corresponding attributes. The overview illustration of our\nframework is shown in Fig .2.\n3.1\nEDITABLE REGION DEFINITION AND RELATED CONSTRAINTS\nIn this section, we give the details of the editable region definition with a region prompt yb and\ndesigned region-related constraints. Instead of directly imposing constraints on neural 3D represen-\ntations to maintain 3D content beyond selected regions unchanged, we adopt 2D masks rendered\nfrom 3D definitions as the bridge to connect various neural 3D representations (e.g., NeRF, SDF,\nand DMTet) and region definition forms (e.g., 3D bounding boxes, custom meshes, and 2D/3D seg-\nmentation results (Liu et al., 2023; Cheng et al., 2023; Cen et al., 2023)), which enhances the gener-\nalization of our Progressive3D. We here adopt NeRF as the neural 3D representation and define the\neditable region with 3D bounding box prompts for brevity.\nGiven a 3D bounding box prompt yb = [cx, cy, cz; sx, sy, sz] which is user-defined for specifying\nthe editable region in 3D space, where [cx, cy, cz] is the coordinate position of the box center, and\n[sx, sy, sz] is the box size on the {x, y, z}-axis respectively. We aim to obtain the corresponding 2D\nmask Mt converted from the prompt yb and pre-trained source representation \u03d5s that describes the\neditable region for a specific view v. Concretely, we first calculate the projected opacity map \u02c6O and\nthe projected depth map \u02c6D of \u03d5s similar to the Eq. 10. Then we render the given bounding box\nto obtain its depth Db = render(yb, v, R), where v is the current view and R is the rotate matrix\nof the bounding box. Before calculating the 2D editable mask Mt at a specific v, we modify the\nprojected depth map \u02c6D according to \u02c6O to ignore the floating artifacts mistakenly generated in \u03d5s:\n\u02dcD(r) =\n(\n\u221e, if \u02c6O(r) < \u03c4o;\n\u02c6D(r), otherwise;\n(1)\nwhere r \u2208 R is the ray set of sampled pixels in the image rendered at view v, and \u03c4o is the filter\nthreshold. Therefore, the 2D mask Mt of the editable region, as well as the 2D opacity mask Mo,\ncan be calculated for the following region-related constraints:\nMt(r) =\n\u001a1, if Db(r) < \u02dcD(r);\n0, otherwise.\nMo(r) =\n(\n1, if \u02c6O(r) > \u03c4o;\n0, otherwise.\n(2)\nContent Consistency Constraint. We emphasize that maintaining 3D content beyond user-defined\neditable regions unchanged during the training of the target representation \u03d5t is critical for 3D\nediting. We thus propose a content consistency constraint to impose the content between the target\nrepresentation \u03d5t and the source representation \u03d5s to be consistent in undesired regions, which\nconditioned by our obtained 2D mask Mt which represents the editable regions:\nLconsist =\nX\nr\u2208R\n\u0012\n\u00af\nMt(r)Mo(r)\n\f\f\f\n\f\f\f \u02c6Ct(r) \u2212 \u02c6Cs(r)\n\f\f\f\n\f\f\f\n2\n2 + \u00af\nMt(r) \u00af\nMo(r)\n\f\f\f\n\f\f\f \u02c6Ot(r)\n\f\f\f\n\f\f\f\n2\n2\n\u0013\n,\n(3)\nwhere \u00af\nMt = 1 \u2212 Mt is the inverse editable mask, \u00af\nMo = 1 \u2212 Mo is the inverse opacity mask, and\n\u02c6Cs, \u02c6Ct are projected colors of \u03d5s, \u03d5t respectively.\nInstead of constraining the entire unchanged regions by color similarity, we divide such regions into\na content region and an empty region according to the modified opacity mask Mo, and an additional\nterm is proposed to impose the empty region remains blank during training. We separately constrain\ncontent and empty regions to avoid locking the backgrounds during the training, since trainable\nbackgrounds are proved (Guo et al., 2023) beneficial for the quality of foreground generation.\nContent Initialization Constraint. In our progressive editing steps, a usual situation is the corre-\nsponding 3D space defined by region prompts is empty. However, creating the target object from\n4\nPreprint\n2D Diffusion\nModel\nOverlapped Semantic Component Suppression\nNoise\nSource Prompt\n\u201cAn astronaut\u201d\nSource Representation\nTarget Representation\nInitialize\nPre-training\nTarget Prompt\n\u201cAn astronaut holding a red rifle\u201d\nUser-Defined Region Prompt\nFigure 2: Overview of a local editing step of our proposed Progressive3D. Given a source repre-\nsentation \u03d5s supervised by source prompt ys, our framework aims to generate a target representation\n\u03d5t conforming to the input target prompt yt in 3d space defined by the region prompt yb. Condi-\ntioned on the 2D mask Mt(r), we constrain the 3D content with Lconsist and Linital. We further\npropose an Overlapped Semantic Component Suppression technique to impose the optimization fo-\ncusing more on the semantic difference for precise progressive creation.\n(a) Source Content and \nRegion Prompt\n(b) 2D Mask  \n(c) w/o\n(d) w/o\n(e) w/o Overlapped Semantic\nComponent Suppression\n(f) Ours\nFigure 3: Qualitative ablations. The source prompt ys=\u201cA medieval soldier with metal armor\nholding a golden axe.\u201d and the target prompt yt=\u201cA medieval soldier with metal armor holding\na golden axe and riding a terracotta wolf.\u201d, where green denotes the overlapped prompt and red\ndenotes the different prompt.\nscratch often leads to rapid geometry variation and causes difficulty in generation. We thus provide\na content initialization constraint to encourage the user-defined 3D space filled with content, which\nis implemented by promoting \u02c6Ot increase in editable regions during the early training phase:\nLinital = \u03ba(k)\nX\nr\u2208R\nMt(r)\n\f\f\f\n\f\f\f \u02c6Ot(r) \u2212 1\n\f\f\f\n\f\f\f\n2\n2 ; \u03ba(k) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u03bb(1 \u2212 k\nK ), if 0 \u2264 k < K;\n0, otherwise,\n(4)\nwhere \u03ba(k) is a weighting function of the current training iteration k, \u03bb is the scale factor of the\nmaximum strength, and K is the maximum iterations that apply this constraint to avoid impacting\nthe detail generation in the later phase.\n3.2\nOVERLAPPED SEMANTIC COMPONENT SUPPRESSION\nAlthough we ensure the content edits only occur in user-defined regions through region-related\nconstraints, obtaining desired representation \u03d5t which matches the description in the target prompt\nyt is still challenging. An intuitive approach to create \u03d5t is fine-tuning the source representation\n\u03d5s with the target prompt yt directly (Poole et al., 2022; Lin et al., 2023). However, we point out\nthat merely leveraging the target prompt yt for fine-grained editing will cause attribute mismatching\nissues, especially when yt describes multiple objects binding with different attributes.\nFor instance in Fig. 3, we have obtained a source representation \u03d5s matching the source prompt ys\nand a target prompt yt for the following local editing step. If we adjust \u03d5s guided by yt directly,\nas shown in Fig. 3(e), the additional content \u201cwolf\u201d could be both impacted by additional attribute\n\u201cterracotta\u201d and overlapped attribute \u201cmetal, golden\u201d during the generation even if the overlapped\nattribute has been considered in \u03d5s, which leads to an undesired result with attribute confusing.\n5\nPreprint\nDreamTime\nDreamTime +\nProgressive3D\n(c) Quality Reduction\n(b) Attribute Mismatching\n(a) Object Missing\n\u201cAn orange cat wearing a \nyellow suit and green sneakers \nand a red chef\u2019s hat.\u201d\n\u201cA green spoon on a red \ncake in a yellow tray.\u201d\n\u201cA lego man wearing a silver \ncrown and riding a golden \nmotorcycle.\u201d\n\u201cA green cactus in a \nhexagonal cup \non a star-shaped tray.\u201d\n\u201cA round cake on a square \ntray on a hexagonal table.\u201d\n\u201cA blue peony in a pink vase.\u201d\nFigure 4: Current text-to-3D methods often fail to produce precise results when the given prompt\ndescribes multiple interacted objects binding with different attributes, leading to significant issues\nincluding object missing, attribute mismatching, and quality reduction.\nFurthermore, the repeated attention to overlapped prompts causes the editing process less consider\nthe objects described in additional prompts, leading to entire or partial object ignoring (e.g., \u201cwolf\u201d\nis mistakenly created without its head and integrated with the soldier). Hence, guiding the optimiza-\ntion in local editing steps to focus more on the semantic difference between ys and yt instead of yt\nitself is critical for alleviating attribute mismatching and obtaining desired 3D content.\nTherefore, we proposed a technique named Overlapped Semantic Component Suppression (OSCS)\ninspired by (Armandpour et al., 2023) to automatically discover the overlapped semantic component\nbetween ys and yt with vector projection, and OSCS then suppresses the overlapped component\nto enhance the influence of the different semantic during the training of \u03d5t for precise content\ncreation. Concretely, both prompts ys and yt firstly produce separate denoising components with\nthe unconditional prediction \u03f5\u03b8(xt, t):\n\u2206\u03f5s\n\u03b8 = \u03f5\u03b8(xt, ys, t) \u2212 \u03f5\u03b8(xt, t); \u2206\u03f5t\n\u03b8 = \u03f5\u03b8(xt, yt, t) \u2212 \u03f5\u03b8(xt, t).\n(5)\nAs shown in Fig. 2, we then decompose \u2206\u03f5t\n\u03b8 into the projection component \u2206\u03f5proj\n\u03b8\nand the perpen-\ndicular component \u2206\u03f5prep\n\u03b8\nby projecting \u2206\u03f5t\n\u03b8 on \u2206\u03f5s\n\u03b8:\n\u2206\u03f5t\n\u03b8 = \u27e8\u2206\u03f5s\n\u03b8, \u2206\u03f5t\n\u03b8\u27e9\n||\u2206\u03f5s\n\u03b8||2\n\u2206\u03f5s\n\u03b8\n|\n{z\n}\nProjection Component\n+\n \n\u2206\u03f5t\n\u03b8 \u2212 \u27e8\u2206\u03f5s\n\u03b8, \u2206\u03f5t\n\u03b8\u27e9\n||\u2206\u03f5s\n\u03b8||2\n\u2206\u03f5s\n\u03b8\n!\n|\n{z\n}\nPerpendicular Component\n= \u2206\u03f5proj\n\u03b8\n+ \u2206\u03f5prep\n\u03b8\n,\n(6)\nwhere \u27e8\u00b7, \u00b7\u27e9 denotes the inner product. We define \u2206\u03f5proj\n\u03b8\nas the overlapped semantic component\nsince it is the most correlated component from \u2206\u03f5t\n\u03b8 to \u2206\u03f5s\n\u03b8, and regard \u2206\u03f5prep\n\u03b8\nas the different\nsemantic component which represents the most significant difference in semantic direction. Fur-\nthermore, we suppress the overlapped semantic component \u2206\u03f5proj\n\u03b8\nduring training for reducing the\ninfluence of appeared attributes, and the noise sampler with OSCS is formulated as:\n\u02c6\u03f5\u03b8(xt, ys, yt, t) = \u03f5\u03b8(xt, t) + \u03c9\nW \u2206\u03f5proj\n\u03b8\n+ \u03c9\u2206\u03f5prep\n\u03b8\n; W > 1,\n(7)\nwhere \u03c9 is the original guidance scale in CFG described in Eq. 14, and W is the weight to control\nthe suppression strength for the overlapped semantics. We highlight that W > 1 is important for\nthe suppression, since \u02c6\u03f5\u03b8(xt, ys, yt, t) is degenerated to \u02c6\u03f5\u03b8(xt, yt, t) when W = 1. Therefore, the\nmodified Score Distillation Sampling (SDS) with OSCS is formulated as follows:\n\u2207\u03d5 \u02dcLSDS(\u03b8, x) = Et,\u03f5\n\u0014\nw(t)(\u02c6\u03f5\u03b8(xt, ys, yt, t) \u2212 \u03f5)\u2202x\n\u2202\u03d5\n\u0015\n.\n(8)\nCompared to Fig. 3(e), leveraging OSCS effectively reduces the distraction of appeared attributes\nand assists Progressive3D in producing desired 3D content, as is shown in Fig. 3(f).\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETTINGS\nWe only provide important experimental settings including dataset, metrics, and baselines here due\nto the page limitation, more detailed experimental settings can be found at Appendix B.\n6\nPreprint\n\u201cA golden table.\u201d\n\u201cA ceramic tea pot\non a golden table.\u201d\n\u201cA ceramic tea pot \nand an origami box\non a golden table.\u201d\n\u201cA round cabinet.\u201d\n\u201cA hexagonal cup on\na round cabinet.\u201d\n\u201cA yellow pineapple in \na hexagonal cup on a \nround cabinet.\u201d\n\u201cA toy robot.\u201d\n\u201cA toy robot wearing \na golden shirt.\u201d\n\u201cA toy robot wearing \na golden shirt and a \nwooden crown.\u201d\n\u201cA model of a round \nbuilding.\u201d\n\u201cA model of a round \nbuilding with square roof.\u201d\n\u201cA model of a round \nbuilding with square roof\non a hexagonal park.\u201d\n\u201cA standing black \nShiba Inu.\u201d\n\u201cA standing black \nShiba Inu wearing a \ngolden sweater.\u201d\n\u201cA standing black \nShiba Inu wearing a \ngolden sweater and \nsilver boots.\u201d\n\u201cA head of \nterracotta army.\u201d\n\u201cA head of terracotta army \nwearing a red sunglass.\u201d\n\u201cA head of terracotta \narmy wearing a red \nsunglass and gray hat.\u201d\n\u201cA metal vase.\u201d\n\u201cA red rose in a \nmetal vase.\u201d\n\u201cA red rose and \nyellow tulip in a \nmetal vase.\u201d\n\u201ca golden biplane.\u201d\n\u201ca ceramic lion driving a \ngolden biplane.\u201d\n\u201ca ceramic lion wearing a \nbronze chef's hat and \ndriving a golden biplane.\u201d\nDreamTime\nDreamTime\n+Progressive3D\nTextMesh\nTextMesh\n+Progressive3D\nFantasia3D\nFantasia3D \n+Progressive3D\nMVDream\nMVDream\n+Progressive3D\nFigure 5: Progressive editing processes driven by various text-to-3D methods equipped with our\nProgressive3D. Compared to original methods, Progressive3D assists current methods in tackling\nprompts with complex semantics well. 3D Cyan boxes denote the user-defined region prompts.\nDataset Construction. We construct a Complex Semantic Prompt set named CSP-100 which in-\nvolves 100 complex prompts to verify that current text-to-3D methods suffer issues when prompts\nare complex in semantics and proposed Progressive3D efficiently alleviates these issues. CSP-100\nintroduces four sub-categories of prompts including color, shape, material, and composition accord-\ning to the appeared attribute and more details are in Appendix B.\nEvaluation Metrics. Existing text-to-3D methods (Poole et al., 2022; Tsalicoglou et al., 2023; Li\net al., 2023) leverage CLIP-based metrics to evaluate the semantic consistency between generated\n3D creations and corresponding text prompts. However, CLIP-based metrics are verified (Huang\net al., 2023a; Lu et al., 2023) that fail to measure the fine-grained correspondences between de-\nscribed objects and binding attributes. We thus adopt two recently proposed metrics fine-grained\n7\nPreprint\nDreamTime\nDreamTime\n+CEBM\nDreamTime\n+A&E\nDreamTime\n+Progressive3D\n\u201cA ceramic tea pot and a lego car on a golden table.\u201d\n\u201cA triangle sandwich on a round cabinet.\u201d\n\u201cA yellow tulip and a white tulip in a pink vase.\u201d\nFigure 6: Visual comparison with DreamTime-based\ncompositional generation baselines.\nTable 1: Quantitative comparison on met-\nrics and user studies over CSP-100.\nMethod\nMetrics\nHuman\nB-VQA \u2191 mGPT-CoT \u2191 Preference \u2191\nDreamTime\n0.227\n0.522\n16.8%\n+ CEBM\n0.186\n0.491\n-\n+ A&E\n0.243\n0.528\n-\n+ Progressive3D\n0.474\n0.609\n83.2%\nOurs\n\u201cAn astronaut.\u201d\n\u201cAn astronaut \nholding a red rifle.\u201d\n\u201cAn astronaut holding a \nred rifle and riding a \ngolden motorcycle.\u201d\nFne-tuning\nFigure 7:\nQualitative ablations between\nfine-tuning with target prompts and editing\nwith Progressive3D on MVDream.\nincluding BLIP-VQA and mGPT-CoT (Huang et al., 2023a), evaluate the generation capacity of\ncurrent methods and our Progressive3D when handling prompts with complex semantics.\nBaselines. We incorporate our Progressive3D with 4 text-to-3D methods driven by different 3D\nrepresentations: (1) DreamTime (Huang et al., 2023b) is a NeRF-based method which enhances\nDreamFusion (Poole et al., 2022) in time sampling strategy and produce better results. We adopt\nDreamTime as the main baseline for quantitative comparisons and ablations due to its stability and\ntraining efficiency. (2) TextMesh (Tsalicoglou et al., 2023) leverages SDF as the 3D representation\nto improve the 3D mesh extraction capacity. (3) Fantasia3D (Tsalicoglou et al., 2023) is driven\nby DMTet which produces impressive 3D content with a disentangled modeling process. (4) MV-\nDream (Shi et al., 2023) is a NeRF-based method which leverages a pre-trained multi-view consis-\ntent text-to-image model for text-to-3D generation and achieves high-quality 3D content generation\nperformance. To further demonstrate the effectiveness of Prgressive3D, we re-implement two com-\nposing text-to-image methods including Composing Energy-Based Model (CEBM) (Liu et al., 2022)\nand Attend-and-Excite (A&E) (Chefer et al., 2023) on DreamTime for quantitative comparison.\n4.2\nPROGRESSIVE3D FOR TEXT-TO-3D CREATION AND EDITING\nComparison with current methods. We demonstrate the superior performance of our Progres-\nsive3D compared to current text-to-3D methods in both qualitative and quantitative aspects in this\nsection. We first present visualization results in Fig. 4 to verify that DreamTime faces significant\nchallenges including (a) object missing, (b) attribute mismatching, and (c) quality reduction when\ngiven prompts describe multiple interacted objects binding with different attributes. Thanks to our\ncareful designs, Progressive3D effectively promotes the creation performance of DreamTime when\ndealing with complex prompts. In addition, more progressive editing processes based on various\ntext-to-3D methods driven by different neural 3D representations are shown in Fig. 5, which further\ndemonstrate that Progressive3D stably increases the generation capacity of based methods when\ngiven prompts are complex, and our framework is general for various current text-to-3D methods.\nWe also provide quantitative comparisons on fine-grained semantic consistency metrics including\nBLIP-VQA and mGPT-CoT, and the results are shown in Tab. 1, which verify that our Progres-\nsive3D achieves remarkable improvements for 3D content creation with complex semantics com-\npared to DreamTime-based baselines. As shown in Fig. 6, baselines that combine 2D composing\nT2I methods including CEBM (Liu et al., 2022) and A&E (Chefer et al., 2023) with DreamTime\n8\nPreprint\nTable 2: Quantitative ablation studies for\nproposed constraints and the OSCS tech-\nnique based on DreamTime over CSP-100.\nComponents\nMetrics\nIndex Lconsist Linitial OSCS B-VQA \u2191 mGPT-CoT \u2191\n1\n\u2713\n0.255\n0.567\n2\n\u2713\n\u2713\n0.370\n0.577\n3\n\u2713\n\u2713\n0.347\n0.581\n4\n\u2713\n\u2713\n\u2713\n0.474\n0.609\n\u201cA triangular sandwich \non a round cabinet.\u201d\n\u201cAn astronaut holding a \ngreen pistol and riding \na red motorcycle with \na yellow trunk.\u201d\nSource Content and \nRegion Prompt\nTarget Prompt\nDreamTime\nTextMesh\nFigure 8: Qualitative ablations for suppression weight\nW in proposed OSCS.\nstill achieve limited performance for complex 3D content generation, leading to significant issues\nincluding object missing, attribute mismatching, and quality reduction. Furthermore, we collected\n20 feedbacks from humans to investigate the performance of our framework. The human prefer-\nence shows that users prefer our Progressive3D in most scenarios (16.8% vs. 83.2%), demonstrating\nthat our framework effectively promotes the precise creation capacity of DreamTime when facing\ncomplex prompts.\n4.3\nABLATION STUDIES\nIn this section, we conduct ablation studies on DreamTime and TextMesh to demonstrate the effec-\ntiveness of proposed components including content consistency constraint Lconsist, content initial-\nization constraint Linitial and Overlapped Semantic Component Suppression (OSCS) technique, we\nhighlight that a brief qualitative ablation is given in Fig. 3.\nWe first present ablation results between fine-tuning directly and editing with Progressive3D based\non TextMesh in Fig. 7 to demonstrate that fine-tuning with new prompts cannot maintain source ob-\njects prompted by overlapped semantics untouched and is unusable for progressive editing. Another\nvisual result in Fig. 8 shows the parameter analysis of the suppression weight w in OSCS. With the\nincrease of W (i.e.,\n\u03c9\nW decreases), the different semantics between source and target prompts play\nmore important roles in optimizations and result in more desirable 3D content. On the contrary, the\nprogressive step edits failed results with object missing or attribute mismatching issues when we\nincrease the influence of overlapped semantics by setting W = 0.5, which further proves that our\nexplanation of perpendicular and projection components is reasonable.\nWe then show the quantitative comparison in Tab. 2 to demonstrate the effectiveness of each pro-\nposed component, where content consistency constraint is not involved in quantitative ablations\nsince consistency is the foundation of 3D content local editing which guarantees content beyond\nuser-defined regions untouched. We underline that Linitial is proposed to simplify the geometry\ngeneration from empty space and OSCS is designed to alleviate the distraction of overlapped at-\ntributes, thus both components can benefit the creation performance with no conflict theoretically.\nThis has been proofed by the quantitative ablations in Tab. 2: index 2 and 3 show that applying\nLinitial and OSCS alone both promote the metrics compared to the baseline in index 1, and in-\ndex 4 shows that leveraging both Linitial and OSCS together can further contribute to the creation\nperformance over CSP-100.\n5\nCONCLUSION\nIn this work, we propose a general framework named Progressive3D for correctly generating 3D\ncontent when the given prompt is complex in semantics. Progressive3D decomposes the difficult\ncreation process into a series of local editing steps and progressively generates the aiming object\nwith binding attributes with the assistance of proposed region-related constraints and the over-\nlapped semantic suppression technique in each step. Experiments conducted on complex prompts\nin CSP-100 demonstrate that current text-to-3D methods suffer issues including object missing, at-\ntribute mismatching, and quality reduction when given prompts are complex in semantics, and the\nproposed Progressive3D effectively creates precise 3D content consistent with complex prompts\nthrough multi-step local editing. More discussions on the limitations and potential directions for\nfuture works are provided in Appendix A.\n9\nPreprint\nREFERENCES\nMohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, and Mingyuan\nZhou. Re-imagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus\nproblem and beyond. arXiv preprint arXiv:2304.04968, 2023. 6\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18392\u201318402, 2023. 3\nJiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\nSegment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. 4\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.\nAttend-and-excite:\nAttention-based semantic guidance for text-to-image diffusion models. ACM Transactions on\nGraphics (TOG), 42(4):1\u201310, 2023. 8\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.\n1, 3, 17\nXinhua Cheng, Yanmin Wu, Mengxi Jia, Qian Wang, and Jian Zhang. Panoptic compositional\nfeature field for editable scene rendering with network-inferred labels via metric learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4947\u2013\n4957, 2023. 4\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. URL https://\nlmsys.org/blog/2023-03-30-vicuna/. 16\nDeepFloyd-Team.\nDeepfloyd-if, 2023.\nURL https://huggingface.co/DeepFloyd/\nIF-I-XL-v1.0. 17\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for\ncompositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 1\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo,\nChia-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestu-\ndio: A unified framework for 3d content generation, 2023. URL https://github.com/\nthreestudio-project/threestudio. 4, 17\nAyaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa.\nInstruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789, 2023.\n3\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022. 18\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn Pro-\nceedings of the Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp.\n6840\u20136851, 2020. 18\nSusung Hong, Donghoon Ahn, and Seung Wook Kim. Debiasing scores and prompts of 2d diffusion\nfor robust text-to-3d generation. ArXiv, abs/2303.15413, 2023. 3\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu.\nT2i-compbench: A com-\nprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint\narXiv:2307.06350, 2023a. 1, 7, 8, 16\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dream-\ntime:\nAn improved optimization strategy for text-to-3d content creation.\narXiv preprint\narXiv:2306.12422, 2023b. 1, 3, 8, 17\n10\nPreprint\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 867\u2013876, 2022. 3\nHiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, and Takuya Narihira. Instruct\n3d-to-3d: Text instruction guided 3d-to-3d conversion. arXiv preprint arXiv:2303.15780, 2023.\n3\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023. 14\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference\non Machine Learning, pp. 12888\u201312900, 2022. 16\nYuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bing-\nbing Ni.\nFocaldreamer: Text-driven 3d editing via focal-fusion assembly.\narXiv preprint\narXiv:2308.10608, 2023. 3, 7, 14, 16\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d con-\ntent creation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 300\u2013309, 2023. 1, 3, 5\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. In European Conference on Computer Vision, pp.\n423\u2013439. Springer, 2022. 8\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023. 4, 14\nYujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveil-\ning the power of large language models in text-to-image synthesis evaluation. arXiv preprint\narXiv:2305.11116, 2023. 1, 7, 16\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. 3\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven\nneural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 13492\u201313502, June 2022. 3\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings\nof the European Conference on Computer Vision (ECCV), pp. 405\u2013421, 2020. 1, 18\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Gener-\nating textured meshes from text using pretrained image-text models. In SIGGRAPH Asia 2022\nconference papers, pp. 1\u20138, 2022. 3\nChong Mou, Xintao Wang, Liangbin Xie, Jing Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. ArXiv, abs/2302.08453, 2023. 1\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. In International Conference on Machine Learning, 2021. 1\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022. 1, 3, 5, 7, 8, 16, 18\n11\nPreprint\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In Interna-\ntional Conference on Machine Learning, 2021. 16\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022. 1, 17\nEtai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel\nediting of 3d objects. arXiv preprint arXiv:2303.12048, 2023. 3\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:\na hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information\nProcessing Systems, 34:6087\u20136101, 2021. 1\nYichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3, 8\nJascha Sohl-Dickstein, EricL. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. arXiv: Learning,arXiv: Learning, 2015.\n18\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint arXiv:2304.12439,\n2023. 1, 3, 7, 8, 16, 17\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jaco-\nbian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12619\u201312629, 2023a.\n1, 3\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction.\narXiv\npreprint arXiv:2106.10689, 2021. 1\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b. 1, 3\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces.\nAdvances in Neural Information Processing Systems, 34:4805\u20134815, 2021. 1\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image\ndiffusion models. In IEEE International Conference on Computer Vision (ICCV), 2023. 1\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023. 16\nJingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. Dreameditor: Text-driven 3d\nscene editing with neural fields. arXiv preprint arXiv:2306.13455, 2023. 3, 14\n12\nPreprint\nA\nDISCUSSIONS\nA.1\nREALISTIC USAGE OF PROGRESSIVE3D\nWe note that Progressive3D contains multiple local editing steps for creating complex 3D content,\nwhich accords with user usage pipeline, i.e., creating a primary object first, then adjusting its at-\ntribute or adding more related objects. However, Progressive3D is flexible in realistic usage since\nthe generation capacity of basic text-to-3D method and user goals are variant. For instance in Fig. 9,\nwe desire to create the 3D content consistent with the prompt \u201cAn astronaut wearing a green top hat\nand riding a red horse\u201d. We find that MVDream fails to create the precise result while generating\n\u201cAn astronaut riding a red horse\u201d correctly. Thus the desired content can be achieved by editing \u201cAn\nastronaut riding a red horse\u201d within one-step editing, instead of starting from \u201can astronaut\u201d.\n\u201cAn astronaut wearing \na green top hat and \nriding a red horse.\u201d\n\u201cAn astronaut riding a \nred horse.\u201d\n\u201cAn astronaut wearing \na green top hat and \nriding a red horse.\u201d\nMVDream\nProgressive3D\nFigure 9: MVDream successfully creates \u201cAn astronaut riding a red horse\u201d while failing to create\n\u201cAn astronaut wearing a green top hat and riding a red horse\u201d By leveraging one-step Progressive3D\nediting, correct 3D content is obtained.\nA.2\nOBJECT GENERATING ORDER\nDifferent object generating orders in Progressive3D typically result in correct 3D content consistent\nwith the complex prompts. However, the content details of the final content are impacted by created\nobjects since Progressive3D is a local editing chain started from the source content, and we give\nan instance in Fig. 10. With different generating orders, Progressive3D creates 3D content with\ndifferent details while they are both consistent with the prompt \u201cAn astronaut sitting on a wooden\nchair\u201d. In our experiments, we first generate the primary object which is desired to occupy most of\nthe space and interact with other additional objects.\n\u201cAn astronaut\u201d\n\u201cAn astronaut sitting \non a wooden chair\u201d\n\u201cA wooden chair\u201d\n\u201cAn astronaut sitting \non a wooden chair\u201d\nFigure 10: Creating \u201cAn astronaut sitting on a wooden chair\u201d from different generating orders.\n13\nPreprint\nA.3\nVARIOUS REGION DEFINITIONS\nWe highlight that Progressive3D is a general framework for various region definition forms since the\ncorresponding 2D mask of each view can be achieved. As shown in Fig. 11, Progressive3D performs\ncorrectly on various definition forms including multiple 3D bounding boxes, custom mesh, and the\nfine-grained 2D segmentation prompted by the keyword \u201chelmet\u201d through Grounded-SAM (Liu\net al., 2023; Kirillov et al., 2023).\nSegmentaion\nCustom Mesh\n\u201cAn astronaut holding a red rifle and riding an brown wolf.\u201d\nMultiple boxes\n\u201cAn astronaut wearing box gloves.\u201d\nSource Content\nTarget Content\n2D Mask\nRegion Prompt\n\u201cAn astronaut wearing a golden helmet.\u201d\n\u201chelmet\u201d\nFigure 11: Progressive3D supports various definition forms of regions since the corresponding 2D\nmasks of each view can be obtained.\nA.4\nATTRIBUTE EDITING\nWe emphasize that Progressive3D supports both modifying attributes of existing objects and creat-\ning additional objects with attributes not mentioned in source prompts from scratch in user-selected\nregions, and we provide attribute editing results in Fig. 12. Noticing that creating additional objects\nwith attributes not mentioned in source prompts from scratch is more difficult than editing the at-\ntributes of existing objects. Therefore, attribute editing costs significantly less time than additional\nobject generation.\nA.5\nWHY ADOPTING 2D CONSTRAINTS\nCompared to directly maintaining 3D content beyond editable regions unchanged on 3D represen-\ntations, we adopt 2D constraints for achieving such a goal from the perspective of generalization.\nWe notice that current text-to-3D methods are developed based on various neural 3D representa-\ntions, thus most 3D editing methods propose careful designs for a specific representation and are\nunusable on other representations. For instance, DreamEditor (Zhuang et al., 2023) distills the orig-\ninal NeRF into a mesh-based radiance field to localize the editable regions, and FocalDreamer (Li\n14\nPreprint\n\u201cAn orange cat \nwearing a cyan suit \nand green snearkers\nand red chef\u2019s hat.\u201d\n\u201cAn orange cat \nwearing a yellow suit \nand blue snearkers\nand red chef\u2019s hat.\u201d\n\u201cAn orange cat \nwearing a yellow suit \nand green snearkers\nand red chef\u2019s hat.\u201d\n\u201cA silver tea pot and \nan origami box on a \nwooden table.\u201d\n\u201cA ceramic tea pot \nand an origami box \non a golden table.\u201d\n\u201cA silver tea pot and \nan origami box on a \ngolden table.\u201d\nFigure 12: Prgressive3D supports attribute editing on existing generated objects.\net al., 2023) proposes multiple regularized losses specifically designed for DMTet to maintain the\nundesired regions unchanged. In addition, their specific designs also limit the available definition\nforms of editable regions.\nHowever, we underline that the optimization core of most text-to-3D is the SDS loss which is super-\nvised on rendered views in a 2D way, and different neural 3D representations can be rendered as 2D\nprojected color, opacity, and depth maps through volume rendering or rasterization. Therefore, our\nproposed 2D region-related constraints can effectively bridge different representations and various\nuser-provided definition forms, and the strength weights of our region-related constraints are easy to\nadjust since our constraints and SDS loss are all imposed on 2D views.\nA.6\nLIMITATIONS\nOur Progressive3D efficiently promotes the generation capacity for current text-to-3d methods when\nfacing complex prompts. However, Progressive3D still faces several challenges.\nFirstly, Progressive3D decomposes a difficult generation into a series of editing processes, which\nleads to multiplying time costs and more human intervention. A potential future direction is further\nintroducing layout generation into the text-to-3d area, e.g., creating 3D content with complex se-\nmantics in one generation by inputting a global prompt, a string of pre-defined 3D regions, and their\ncorresponding local prompts. Whereas 3D layout generation intuitive suffers more difficulties and\nrequires further exploration.\nAnother problem is that the creation quality of Progressive3D is highly determined by the generative\ncapacity of the base method. We believe our framework can achieve better results when stronger 2D\ntext-to-image diffusion models and neural 3D representations are available, and we leave the adap-\ntion between possible improved text-to-3D creation methods and Progressive3D in future works.\nB\nEXPERIMENTS SETTINGS\nB.1\nCSP-100\nCSP-100 can be divided into four sub-categories of prompts including color (e.g. red, green, blue),\nshape (e.g. round, square, hexagonal), material (e.g. golden, wooden, origami) and composition\naccording to the attribute types appeared in prompts, as shown in Fig. 13. Each prompt in the\ncolor/shape/material categories describes two objects binding with color/shape/material attributes,\nand each prompt in the composition category describes at least three interacted objects with corre-\nsponding different attributes. We provide the detailed prompt list in both Tab. 4 and Tab. 5.\n15\nPreprint\nDreamTime\nDreamTime +\nProgressive3D\n\u201cA wooden dog driving an \norigami sport car.\u201d\n(c) Material\n(a) Color\n\u201cAn orange cat wearing a \ngreen shirt.\u201d\n\u201cA round gift box on a \nhexagonal table.\u201d\n(b) Shape\n(d) Composition\n\u201cAn astronaut holding a red \nrifle and riding a green \norigami motorcycle.\u201d\n\u201cA red rose in a hexagonal \ncup on a star-shaped tray.\u201d\n\u201cA lego tank with a golden \ngun and a red flying flag.\u201d\nFigure 13: Prompts in CSP-100 can be divided into four categories including Color, Shape, Material,\nand Composition according to appeared attributes.\nB.2\nMETRICS\nThe CLIP-based metrics utilized by current text-to-3D methods (Poole et al., 2022; Tsalicoglou\net al., 2023; Li et al., 2023) calculates the cosine similarity between text and image features extracted\nby CLIP (Radford et al., 2021) However, recent works (Huang et al., 2023a; Lu et al., 2023) demon-\nstrate that CLIP-based metrics can only measure the coarse text-image similarity but fail to measure\nthe fine-grained correspondences among multiple objects and their binding attributes. Therefore, we\nadopt two fine-grained text-to-image evaluation metrics including BLIP-VQA and mGPT-CoT pro-\nposed by (Huang et al., 2023a) to show the effectiveness of Progressive3D. We provide comparisons\nin Fig. 14 to demonstrate that the CLIP metric fails to measure the fine-grained correspondences\nwhile BLIP-VQA performs well, and we report the quantitative comparison of DreamTime-based\nmethods on CLIP metric over CSP-100 in Tab. 3.\n\u201cAn astronaut holding a red rifle and riding a green \norigami motorcycle and wearing a cyan chef\u2019s hat.\u201d\n\u201cA medieval soldier with metal armor wearing an white astronaut \nhelmet and holding a golden axe and riding a terracotta wolf.\u201d\nDreamTime\nMVDream\nCLIP\nBLIP-VQA\n0.340\n0.312\n0.062\n0.556\n0.341\n0.347\n0.074\n0.626\nFigure 14: Quantitative comparison for metrics including\nCLIP and BLIP-VQA on DreamTime and MVDream.\nTable 3: Quantitative comparison on\nCLIP over CSP-100.\nMethod\nCLIP \u2191\nDreamTime\n0.289\n+ CEBM\n0.275\n+ A&E\n0.281\n+ Progressive3D 0.292\nBLIP-VQA is proposed based on the visual question answering (VQA) ability of BLIP (Li et al.,\n2022). Concretely, BLIP-VQA decomposes a complex prompt into several separate questions and\ntakes the probability of answering \u201cyes\u201d as the score for a question. The final score of a specific\nprompt is the product of the probability of answering \u201cyes\u201d for corresponding questions. For in-\nstance, the complex prompt is \u201cAn astronaut holding a red rifle.\u201d, the final score of BLIP-VQA is\nthe product of the probability for questions including \u201cAn astronaut?\u201d and \u201cA red rifle?\u201d\nSince multimodal large language models such as MiniGPT-4 (Zhu et al., 2023) show impressive\ntext-image understanding capacity, MiniGPT4 with Chain-of-Thought (mGPT-CoT) is proposed to\nleverage such cross-modal understanding performance to evaluate the fine-grained semantic sim-\nilarity between query text and image. Specifically, we ask two questions in sequence including\n\u201cDescribe the image.\u201d and \u201cPredict the image-text alignment score.\u201d, and the multimodal LLM\nis required to output the evaluation mGPT-CoT score with detailed Chain-of-Thought prompts. In\npractice, we adopt MiniGPT4 fine-tuned from Vicuna 7B (Chiang et al., 2023) as the LLM.\nWhat\u2019s more, we define the preference criterion of human feedback as follows: Users are requested\nto judge whether the 3D creations generated by DreamTime or Progressive3D are consistent with\nthe given prompts. If one 3D content is acceptable in the semantic aspect while the other is not, the\ncorresponding acceptable method is considered superior. On the contrary, if both 3D creations are\n16\nPreprint\nconsidered to satisfy the description in the given prompt, users are asked to prefer the 3D content\nwith higher quality.\nB.3\nIMPLEMENT DETAILS\nOur Progressive3D is implemented based on the Threestudio (Guo et al., 2023) project since Dream-\nTime (Huang et al., 2023b) and TextMesh (Tsalicoglou et al., 2023) have not yet released their\nsource code and the official Fantasia3D (Chen et al., 2023) code project, and all experiments are\nconducted on NVIDIA A100 GPUs. We underline that the implementation in ThreeStudio (Guo\net al., 2023) and details could be different from their papers, especially ThreeStudio utilizes Deep-\nFloyd IF (DeepFloyd-Team, 2023) as the text-to-image diffusion model for more stable and higher\nquality in the generation, while Fantasia3D adopts Stable Diffusion (Rombach et al., 2022) as the\n2D prior. The number of iterations N and batch size for DreamTime and TextMesh are set to 10000\nand 1 respectively. The training settings of Fantasia3D are consistent with officially provided train-\ning configurations, e.g., N is 3000 and 2000 for geometry and appearance modeling stages, and\nbatch size is set to 12. We leverage Adam optimizer for progressive optimization and the learning\nrate is consistent with base methods. Therefore, one local editing step costs a similar time to one\ngeneration of base methods from scratch. In most scenarios, the filter threshold \u03c4o is set to 0.5, the\nstrength factor \u03bb, iteration threshold K in Lconsist are set to 0.5 and N\n4 , and the suppression weight\nW in overlapped semantic component suppression technique is set to 4.\nC\nQUALITATIVE RESULTS\nC.1\nCONTENT CONSTRAINT WITH BACKGROUND\nWe divide the Lconsist into a content term and an empty term to avoid mistakenly treating back-\ngrounds as a part of foreground objects. We give a visual comparison of restricting the backgrounds\nand foregrounds as an entirety in Fig. 15, and the \u02c6Lconsist can be formulated as:\n\u02c6Lconsist =\nX\nr\u2208R\n\u0012\n\u00af\nMt(r)\n\f\f\f\n\f\f\f \u02c6Ct(r) \u2212 \u02c6Cs(r)\n\f\f\f\n\f\f\f\n2\n2\n\u0013\n.\n(9)\nThe visual results demonstrate that mistakenly treating backgrounds as a part of foreground objects\nleads to significant floating.\nSource Content\nWith\nWith\nFigure 15: Mistakenly restricting background content as foreground leads to significant floating in\nediting result.\nC.2\nMORE PROGRESSIVE EDITING PROCESS\nWe here provide more progressive editing results for correctly creating 3D content for prompts\nwith complex semantics in Fig. 16. More qualitative results with various complex prompts further\ndemonstrate the creation precision and diversity of our Progressive3D.\n17\nPreprint\nD\nPRELIMINARY\nNeural Radiance Field (NeRF) (Mildenhall et al., 2020) uses a multi-layer perception (MLP) to\nimplicitly represent the 3D scene as a continuous volumetric radiance field. Specifically, MLP \u03b8\nmaps a spatial coordinate and a view direction to a view-independent density \u03c3 and view-dependent\ncolor c. Given the camera ray r(k) = o + kd with camera position o, view direction d and depth\nk \u2208 [kn, kf], the projected color of r(k) is rendered by sampling N points along the ray:\n\u02c6C(r) =\nN\nX\ni=1\n\u2126i(1 \u2212 exp(\u2212\u03c1i\u03b4i))ci,\n(10)\nwhere \u03c1i and ci denote the density and color of i-th sampled point, \u2126i = exp(\u2212 Pi\u22121\nj=1 \u03c1j\u03b4j) indi-\ncates the accumulated transmittance along the ray, and \u03b4i is the distance between adjacent points.\nDiffusion Model (Sohl-Dickstein et al., 2015; Ho et al., 2020) is a generative model which defines\na forward process to slowly add random noises to clean data x0 \u223c p(x) and a reverse process to\ngenerate desired results from random noises \u03f5 \u223c N(0, I) within T time-steps:\nq(xt|xt\u22121) = N(xt; \u221a\u03b1txt\u22121, (1 \u2212 \u03b1t)I),\n(11)\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03c32\nt I),\n(12)\nwhere \u03b1t and \u03c3t are calculated by a pre-defined scale factor \u03b2t, and \u00b5\u03b8(xt, t) is calculated by xt\nand the noise \u03f5\u03b8(xt, t) predicted by a neural network, which is optimized with prediction loss:\nL = Ext,\u03f5,t\n\u0002\nw(t)||\u03f5\u03b8(xt, t) \u2212 \u03f5||2\n2\n\u0003\n,\n(13)\nwhere w(t) is a weighting function that depends on the time-step t. Recently, text-to-image dif-\nfusion models achieve impressive success in text-guided image generation by learning \u03f5\u03b8(xt, y, t)\nconditioned by the text prompt y. Furthermore, classifier-free guidance (CFG) (Ho & Salimans,\n2022) is widely leveraged to improve the quality of results via a guidance scale parameter \u03c9:\n\u02c6\u03f5\u03b8(xt, y, t) = (1 + \u03c9)\u03f5\u03b8(xt, y, t) \u2212 \u03c9\u03f5\u03b8(xt, t),\n(14)\nScore Distillation Sampling (SDS) is proposed by (Poole et al., 2022) to create 3D contents from\ngiven text prompts by distilling 2D images prior from a pre-trained diffusion model to a differen-\ntiable 3D representation. Concretely, the image x = g(\u03d5) is rendered by a differentiable generator\ng and a representation parameterized by \u03d5 , and the gradient is calculated as:\n\u2207\u03d5LSDS(\u03b8, x) = Et,\u03f5\n\u0014\nw(t)(\u02c6\u03f5\u03b8(xt, y, t) \u2212 \u03f5)\u2202x\n\u2202\u03d5\n\u0015\n.\n(15)\n18\nPreprint\n\u201cAn orange cat \nwearing a green \nshirt.\u201d\n\u201cAn orange cat \nwearing a yellow suit \nand cyan boots.\u201d\n\u201cAn orange cat \nwearing a red \nfireman suit.\u201d\n\u201cAn orange cat \nwearing a yellow suit \nand red pumps.\u201d\n\u201cAn orange cat.\u201d\n\u201cAn orange cat \nwearing a yellow suit.\u201d\n\u201cAn orange cat \nwearing a yellow suit \nand green snearkers.\u201d\n\u201cA blue peony in \na pink vase.\u201d\n\u201cA white tulip and \na yellow tulip in a \npink vase.\u201d\n\u201cA red rose in \na pink vase.\u201d\n\u201cA blue peony and \na yellow tulip in a \npink vase.\u201d\n\u201cA pink vase.\u201d\n\u201cA yellow tulip in \na pink vase.\u201d\n\u201cA red rose and \na yellow tulip in a \npink vase.\u201d\n\u201cA triangular cake on \na star-shaped tray.\u201d\n\u201cA white lily in a \nhexagonal cap on a \nstar-shaped tray.\u201d\n\u201cA square pepper on \na star-shaped tray.\u201d\n\u201cA red rose in a \nhexagonal cap on a \nstar-shaped tray.\u201d\n\u201cA star-shaped \ntray.\u201d\n\u201cA hexagonal cap on \na star-shaped tray.\u201d\n\u201cA green cactus in a \nhexagonal cap on a \nstar-shaped tray.\u201d\n\u201cAn origami box on a \ngolden table.\u201d\n\u201cA ceramic tea pot \nand wooden shoes on \na golden table.\u201d\n\u201cA silver vase on a \ngolden table.\u201d\n\u201cA ceramic tea pot \nand a cardboard box \non a golden table.\u201d\n\u201cA golden table.\u201d\n\u201cA ceramic tea pot \non a golden table.\u201d\n\u201cA ceramic tea pot \nand a lego car on a \ngolden table.\u201d\n\u201cA wooden dog \ndriving an origami \nsport car.\u201d\n\u201cA lego man driving \nan origami sport car.\u201d\n\u201cAn origami \nsport car.\u201d\n\u201cA metal monkey \ndriving an origami \nsport car.\u201d\n\u201cA wooden dog riding \na golden motorcycle.\u201d\n\u201cA silver bunny riding \na golden motorcycle.\u201d\n\u201cA golden \nmotorcycle.\u201d\n\u201cA lego man riding a \ngolden motorcycle.\u201d\n\u201cA metal monkey wearing a \nwooden top hat and driving \nan origami sport car.\u201d\n\u201cA metal monkey wearing \na golden crown and driving \nan origami sport car.\u201d\n\u201cA metal monkey wearing \na chef\u2019s hat and driving \nan origami sport car.\u201d\n\u201cA lego man wearing a \nchef\u2019s hat and riding a \ngolden motorcycle.\u201d\n\u201cA lego man wearing a \nwooden top hat and riding \na golden motorcycle.\u201d\n\u201cA lego man wearing a \nsilver crown and riding a \ngolden motorcycle.\u201d\nFigure 16: More progressive editing results created with our Progressive3D based on DreamTime.\n19\nPreprint\nTable 4: Detailed prompt list of CSP-100. (Part 1)\nIndex\nPrompt\nColor (i.e., two interacted objects binding with different colors)\n1\nan orange cat wearing a yellow suit\n2\nan orange cat wearing a green shirt\n3\nan orange cat wearing a red fireman uniform\n4\na red rose in a pink vase\n5\na blue peony in a pink vase\n6\na yellow tulip in a pink vase\n7\na pair of red sneakers on a blue chair\n8\na stack of green books on a blue chair\n9\na purple gift box on a blue chair\n10\na red cake in a yellow tray\n11\na blue spoon in a yellow tray\n12\na pair of green chopsticks in a yellow tray\n13\na green vase on a red desk\n14\na pair of blue sneakers on a red desk\n15\na yellow tray on a red desk\nShape (i.e., two interacted objects binding with different shapes)\n16\na round gift box on a hexagonal table\n17\na triangular cake on a hexagonal table\n18\na square tray on a hexagonal table\n19\na hexagonal cup on a round cabinet\n20\na triangular sandwich on a round cabinet\n21\na square bowl on a round cabinet\n22\na hexagonal cup on a star-shaped tray\n23\na triangular cake on a star-shaped tray\n24\na square pepper on a star-shaped tray\n25\na model of a round house with a hexagonal roof\n26\na model of a round house with a square roof\n27\na model of a round house with a spherical roof\nMaterial (i.e., two interacted objects binding with different materials)\n28\na lego man riding a golden motorcycle\n29\na silver bunny riding a golden motorcycle\n30\na wooden dog riding a golden motorcycle\n31\na lego man driving an origami sport car\n32\na metal monkey driving an origami sport car\n33\na wooden dog driving an origami sport car\n34\na ceramic tea pot on a golden table\n35\na silver vase on a golden table\n36\nan origami box on a golden table\n37\na model of a silver house with a golden roof\n38\na model of a silver house with a wooden roof\n39\na model of a silver house with a bronze roof\n40\na lego tank with a golden gun\n41\na lego tank with a silver gun\n42\na lego tank with a wooden gun\nComposition (i.e., more than two interacted objects binding with different attributes)\n43\nan orange cat wearing a yellow suit and green sneakers\n44\nan orange cat wearing a yellow suit and red pumps\n45\nan orange cat wearing a yellow suit and cyan boots\n46\nan orange cat wearing a yellow suit and green sneakers and cyan top hat\n47\nan orange cat wearing a yellow suit and green sneakers and pink cap\n48\nan orange cat wearing a yellow suit and green sneakers and red chef\u2019s hat\n49\na blue peony and a yellow tulip in a pink vase\n50\na red rose and a yellow tulip in a pink vase\n20\nPreprint\nTable 5: Detailed prompt list of CSP-100. (Part 2)\nIndex\nPrompt\n51\na white tulip and a yellow tulip in a pink vase\n52\na purple rose and a red rose and a yellow tulip in a pink vase\n53\na blue peony and a red rose and a yellow tulip in a pink vase\n54\na white tulip and a red rose and a yellow tulip in a pink vase\n55\na golden cat on a stack of green books on a blue chair\n56\na wooden bird on a stack of green books on a blue chair\n57\na lego car on a stack of green books on a blue chair\n58\na blue candle on a red cake in a yellow tray\n59\na green spoon on a red cake in a yellow tray\n60\na pair of cyan chopsticks on a red cake in a yellow tray\n61\na cyan cake on a yellow tray on a red desk\n62\na ceramic tea pot on a yellow tray on a red desk\n63\na green apple on a yellow tray on a red desk\n64\na square cake on a square tray on a hexagonal table\n65\na round cake on a square tray on a hexagonal table\n66\na triangular sandwich on a square tray on a hexagonal table\n67\na green apple in a hexagonal cup on a round cabinet\n68\na pink peach in a hexagonal cup on a round cabinet\n69\na yellow pineapple in a hexagonal cup on a round cabinet\n70\na red rose in a hexagonal cup on a star-shaped tray\n71\na white lily in a hexagonal cup on a star-shaped tray\n72\na green cactus in a hexagonal cup on a star-shaped tray\n73\na lego man wearing a chef\u2019s hat and riding a golden motorcycle\n74\na lego man wearing a wooden top hat and riding a golden motorcycle\n75\na lego man wearing a silver crown and riding a golden motorcycle\n76\na metal monkey wearing a golden crown and driving an origami sport car\n77\na metal monkey wearing a chef\u2019s hat and driving an origami sport car\n78\na metal monkey wearing a wooden top hat and driving an origami sport car\n79\na ceramic tea pot and a lego car on a golden table\n80\na ceramic tea pot and an origami box on a golden table\n81\na ceramic tea pot and a cardboard box on a golden table\n82\na ceramic tea pot and a pair of wooden shoes on a golden table\n83\na ceramic tea pot and an origami box and a green apple on a golden table\n84\na ceramic tea pot and an origami box and a yellow tray on a golden table\n85\na ceramic tea pot and an origami box and a stack of blue books on a golden table\n86\na model of a silver house with a golden roof beside an origami coconut tree\n87\na model of a silver house with a golden roof beside a wooden car\n88\na model of a silver house with a golden roof beside a lego man\n89\na lego tank with a golden gun and a red flying flag\n90\na lego tank with a golden gun and a blue flying flag\n91\na lego tank with a golden gun and a yellow flying flag\n92\na model of a round house with a spherical roof on a hexagonal park\n93\na model of a round house with a spherical roof on a square park\n94\na model of a round house with a spherical roof on an oval park\n95\nan astronaut holding a red rifle and riding a green origami motorcycle\n96\nan astronaut holding a red rifle and riding a cyan scooter\n97\nan astronaut holding a red rifle and riding a golden motorcycle\n98\nan astronaut holding a red rifle and riding a green origami motorcycle and wearing a blue top hat\n99\nan astronaut holding a red rifle and riding a green origami motorcycle and wearing a cyan chef\u2019s hat\n100\nan astronaut holding a red rifle and riding a green origami motorcycle and wearing a pink cowboy hat\n21\n"
  }
]