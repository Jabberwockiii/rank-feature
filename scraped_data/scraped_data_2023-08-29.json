[
  {
    "title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records",
    "link": "https://arxiv.org/pdf/2308.14089.pdf",
    "upvote": "23",
    "text": "MedAlign: A Clinician-Generated Dataset for Instruction\nFollowing with Electronic Medical Records\nScott L. Fleming\u2217,1,2, Alejandro Lozano\u2217,1, William J. Haberkorn\u2217,3,4, Jenelle A. Jindal\u2217,5,\nEduardo Reis\u2217,6,7,8, Rahul Thapa9, Louis Blankemeier10, Julian Z. Genkins11,12,\nEthan Steinberg2, Ashwin Nayak13, Birju Patel5, Chia-Chun Chiang14,15, Alison Callahan5,13,\nZepeng Huo5, Sergios Gatidis6, Scott Adams6, Oluseyi Fayanju13, Shreya J. Shah13,\nThomas Savage1,16, Ethan Goh5,17, Akshay S. Chaudhari1,6,15, Nima Aghaeepour1,3,4,\nChristopher Sharp13,15, Michael A. Pfeffer9,13, Percy Liang2,15, Jonathan H. Chen5,15,16,17,\nKeith E. Morse4, Emma P. Brunskill\u2020,2,15, Jason A. Fries\u2020,5, and Nigam H. Shah\u2020,9,13,15,17\n1Department of Biomedical Data Science, Stanford School of Medicine, Stanford, CA, USA\n2Department of Computer Science, Stanford School of Engineering, Stanford, CA, USA\n3Department of Anesthesiology, Peri-operative, and Pain Medicine, Stanford School of Medicine, Stanford, CA, USA\n4Department of Pediatrics, Stanford School of Medicine, Stanford, CA, USA\n5Stanford Center for Biomedical Informatics Research, Stanford University, Stanford, CA, USA\n6Department of Radiology, Stanford School of Medicine, Stanford, CA, USA\n7Center for Artificial Intelligence in Medicine and Imaging (AIMI), Stanford University, Stanford, CA, USA\n8Hospital Israelita Albert Einstein, Sao Paulo, SP, Brazil\n9Technology and Digital Solutions, Stanford Health Care, Palo Alto, CA, USA\n10Department of Electrical Enginering, Stanford School of Engineering, Stanford, CA\n11Department of Medicine, Vanderbilt University School of Medicine, Nashville, TN, USA\n12Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN, USA\n13Department of Medicine, Stanford School of Medicine, Stanford, CA, USA\n14Department of Neurology, Mayo Clinic, Rochester, MN, USA\n15Human-Centered Artificial Intelligence Institute, Stanford University, Stanford, CA, USA\n16Division of Hospital Medicine, Stanford University, Stanford, CA, USA\n17Clinical Excellence Research Center, Stanford School of Medicine, Stanford, CA, USA\nDecember 27, 2023\nAbstract\nThe ability of large language models (LLMs) to follow natural language instructions with human-level\nfluency suggests many opportunities in healthcare to reduce administrative burden and improve quality\nof care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging.\nExisting question answering datasets for electronic health record (EHR) data fail to capture the complexity\nof information needs and documentation burdens experienced by clinicians. To address these challenges,\nwe introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data.\nMedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for\n303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used\nMedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each\nLLM response. We found high error rates, ranging from 35% (GPT-4) to 68% (MPT-7B-Instruct), and\n8.3% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations\nbetween clinician rankings and automated natural language generation metrics as a way to rank LLMs\nwithout human review. MedAlign is provided under a research data use agreement1 to enable LLM\nevaluations on tasks aligned with clinician needs and preferences.\n\u2217Equal contributions. Corresponding author: {scottyf, lozanoe}@stanford.edu.\n\u2020Equal leadership.\n1https://medalign.stanford.edu\n1\narXiv:2308.14089v2  [cs.CL]  24 Dec 2023\n1\nIntroduction\nSummarize from the EHR \nthe strokes that the patient \nhad and their associated \nneurologic deficits.  \n+\nPatient EHR\nEHR System\n\u2026\nNeurology Clinic Visit\n05/15/2022\nClinical \nText\nTabular \nData\nVitals\nLabs\nMedication List\nProblem List\nSocial History\nCare Plan\nTreatment Plan\nPast Medical History\nNotes\n\u2026\nClinician Instruction\nLLM\n???\nEmergency Room Visit\n10/08/2018\nEHR\n\u2026\n\u2026\nLLM Response\nClinician Response\nThe patient had strokes in the L basal \nganglia in 2018 and multiple strokes in 2022:  \nR occipital, left temporal, L frontal. The \npatient had right sided weakness associated \nwith the 2018 stroke after which she was \nadmitted to rehab. She then had a left sided \nhemianopsia related to the 2022 stroke.\nPairing Instructions with Patient EHR Markup\nEvaluating LLMs with MedAlign\nPair Instructions + EHRs\n<record>\n<visit type=Emergency \nRoom\u201d start=\u201d10/08/2018 \n20:00\u201d>\n<day start=\u201d10/08/2018 \n20:00\u201d>\n<person>\nBirth:7/19/1966\nEHR\n\u2026\n\u2026\nRank Best Pairs\nscore(Instruction, EHR)\nFigure 1: In MedAlign, patient EHRs are transformed into XML markup (example provided in Figure S4)\nand paired with clinician-generated instructions using a retrieval-based (BM25) scoring metric. The resulting\nset of instruction + EHR pairs is then reviewed by clinicians to write gold responses, which are used to\nevaluate EHR instruction following in large language models\nLarge language models (LLMs) have revolutionized natural language processing in tasks such as reading\ncomprehension, reasoning, and language generation [52], prompting researchers to explore applications in\nhealthcare [36]. Recent LLMs like MedPalm [34] and GPT-4 [24] have demonstrated expert-level performance\non medical question-answering benchmarks including MedQA [14], MMLU [12], and the USMLE [16].\nHowever, these benchmarks employ multiple-choice, exam-style evaluations where question stems summarize\nkey information and a single answer choice is best. It is not known if performance on these tasks will translate\nwhen a model is deployed in the complex clinical environments.\nTo be useful, LLMs need to perform well on the specific information-related tasks that clinicians currently\ncomplete themselves while caring for patients. These tasks are a significant burden on clinicians, who\nspend 45% of their day interacting with computers instead of patients [39] and 10 hours a week generating\ndocumentation [11], in part contributing to professional burnout [21]. Examples of these tasks include\nsummarizing a patient\u2019s asthma treatment history from different specialists the patient has visited, generating\na differential diagnosis based on partially resulted laboratory data, or searching through the clinical notes\nfor mentions of a patient\u2019s family support system in order to create the best plan for the patient\u2019s hospital\ndischarge (see Table 2). Such tasks could be passed as instructions to an LLM in the form of questions or\nimperatives (e.g., \u201cWrite a discharge summary\u201d) grounded in a patient\u2019s Electronic Health Record (EHR, an\nelectronic representation of a patient\u2019s medical history). However, despite the excitement about LLMs to\ntransform the practice of medicine, evaluations to date have not authentically represented the variety of tasks\nand idiosyncrasies of EHR data that clinicians face in the real world.\nGiven the recent emergence of instruction-following capabilities in LLMs [43], there is potential for LLMs\nto ameliorate such administrative burden. Hand-curated exemplars of instructions and responses have been\ncritical to improve performance of models [6], especially on clinical reasoning and knowledge recall tasks in\nthe healthcare domain [34]. Thus, a high quality dataset of instruction-EHR-response tuples that represents\nthe breadth of clinical tasks is essential not only as a shared benchmark, but potentially to accelerate the\ntraining of specialized LLMs for healthcare [32].\nHowever, building such a dataset requires an extraordinary effort from a multidisciplinary collaboration.\nIn particular, generating an instruction-following benchmark dataset with representative EHR-based tasks\nand expert responses is challenging due to the substantial cost and logistical complexity of clinician review.\nThere is a need for an EHR dataset that (1) contains a diverse set of questions and instructions generated by\npracticing clinicians; (2) pairs these queries with EHRs from both inpatient and ambulatory care settings;\n(3) leverages both structured and unstructured data from the longitudinal EHR; and (4) is available to the\nbroader academic community.\nIn light of these challenges and opportunities, we present three contributions:\n1. MedAlign Dataset: We introduce a benchmark dataset called MedAlign consisting of 983\n2\nTable 1: Comparison of our work, MedAlign, to existing EHR QA datasets.\nDataset\nQuestions Documents Patients\nSpecialties\nLabeler\nSource\nRaghavan et al. [30]\n5696\n71\n71\n-\nMedical Students\nClinical Note\nPampari et al. [27]\n73111\n303\n303\n-\nProgrammatic\nDischarge Summary\nFan [9]\n245\n138\n-\n1\nAuthor\nDischarge Summary\nYue et al. [49]\n1287\n36\n-\n-\nMedical Experts\nClinical Note\nSoni et al. [35]\n3074\n1009\n100\n1\nClinicians\nRadiology Note\nMedAlign (Ours)\n983\n37264\n276\n7\nClinicians\nEHR\nquestions and instructions submitted by 15 practicing clinicians spanning 7 medical specialties. For 303\nof these instructions, we provide a clinician-written reference answer and paired EHR for grounding\nprompts. Each clinician evaluated and ranked outputs from 6 different LLMs on these 303 instructions\nand wrote \u201cgold standard\u201d answers. To our knowledge, MedAlign is the first dataset of EHR-based\ninstruction-answer pairs (including question and imperative instructions) written by clinicians, with\nclinician evaluations of LLM-generated outputs. Table 1 summarizes MedAlign and its distinction\nfrom existing datasets for clinical information needs.\n2. Automated Instruction-EHR Matching: We demonstrate the feasibility of a simple retrieval-based\napproach to pair an instruction with a relevant patient EHR. By isolating the process of instruction\nsolicitation, we were able to scale and diversify the set of clinicians who submitted instructions.\nFurthermore, we show that our process for matching instructions to relevant EHRs produces a relevant\npairing 74% of the time \u2014 at least twice as frequently as randomly pairing instructions to EHRs.\n3. Automated Evaluation of LLM Responses: We analyze the correlation between clinician rankings\nand automated natural language generation (NLG) metrics as a way to scalably reproduce such analyses,\nreducing future needs for clinicians to label and rank LLM responses.\n2\nBackground and Related Work\nThe volume of patient care data is growing exponentially, with a compound annual growth rate approaching 36%\n[7]. Utilizing LLMs to more efficiently interact with patient data holds great potential to help clinicians manage\nincreasingly complicated information needs and circumvent low-usability EHR interfaces [19]. However,\nevaluation of LLMs to improve meaningful outcomes like clinician burnout or patient health has been\ninadequately studied, mainly due to benchmark datasets which do not represent true clinician needs [13],\nnarrowly focus on a specific medical specialty or subset of EHR data [17], and/or are overly simplistic due\nto templated question construction [27, 48]. These works highlight the challenges in collecting high-quality\nclinician-generated questions and answers; we consider each in turn.\nQuestions and instructions in an EHR-based benchmark dataset should be paired with relevant patient\nEHRs. In order to ensure relevancy, prior works have provided clinicians with specific patient EHRs and asked\nthem to generate questions based on those patients\u2019 data [17]. Unfortunately, requiring EHRs as context\nfor question generation limits scalability, as medical institutions restrict access to patient data to preserve\npatient privacy. Pampari et al. [27] attempted to overcome these scalability issues by generating questions\nvia a template-based approach, but this led to issues with question quality and diversity [48]. Our method of\nsoliciting clinician-generated instructions without a specific patient\u2019s EHR as context overcomes these scaling\nissues, albeit at the cost of potentially less relevant instruction-to-EHR pairings (we discuss our approach to\naddressing this problem in the Dataset Curation section).\nBeyond generating questions, generating expert answers at scale is also prohibitively difficult. Reviewing\nan EHR to answer patient-specific queries can take 30+ minutes for a single patient [33]. This excludes any\ntime required to generate a response to the query. Prior works have attempted to overcome the bottleneck of\ngenerating responses by extracting answers verbatim from individual clinical notes or discharge summaries\n[35, 25, 9]. However, many clinical tasks require synthesizing information from both structured data and\n3\nmultiple free-text documents to arrive at an adequate response, an aspect not explored in existing EHR QA\ndatasets. In such cases, answers extracted from a single note in the patient\u2019s record may not be an adequate;\nfree-text text generation is required. While there is at least one example of an EHR-based question answering\ndataset in the literature that includes both structured and unstructured data [30], it neither contains free-text\nresponses nor is publicly available. Finally, all of the aforementioned datasets focus on simple question\nanswering (i.e., providing concise, factoid-style answers) rather than general instruction following, which\noften requires executing a series of complex directives and commands to accomplish tasks. To the best of our\nknowledge, there does not exist any EHR-based benchmark dataset that incorporates instruction following.\nThe significant costs of clinician review present barriers not only for de novo dataset generation, but\nalso for reliable evaluation of new methods on existing datasets. Automated metrics for evaluating Natural\nLanguage Generation (NLG) systems have shown moderate to high correlation with human judgments on\ntasks like machine translation [10], but it is unclear whether these findings extend to other domains and tasks.\nWhile there is precedent [17] for applying automated metrics like BLEU [28], ROUGE-L [18], METEOR [1],\nand BERTScore [50] to NLG tasks in the clinical domain, there is comparatively very little work assessing\ncorrespondence between these metrics and human judgment on clinical NLG tasks. Thus not only do we\nhave a poor understanding of how LLMs perform on EHR-based instruction-following tasks, but also we do\nnot know whether it is possible to reliably automate such evaluations. Automation could substantially reduce\nthe \u201cbarrier to entry\u201d for research teams with limited resources.\n3\nDataset Curation Process\nElectronic Health Records (EHRs)\nEHR systems are software for managing patient medical record\ndata. From a clinician\u2019s view, a patient EHR is accessed via a graphical user interface that provides access to\ndata elements associated with medical care, e.g., medication lists and treatment plans. These data are stored\nas a collection of timestamped structured (tabular) and unstructured (text) events, which when ordered by\ntime form a patient\u2019s longitudinal EHR timeline. Our EHR data is represented using the OMOP CDM [42], a\nstandardized schema for exchanging medical data, translated into a single, XML markup document per record\n(example provided in Figure S4) to enable simple data exploration via an XML viewer. Figure 1 outlines the\nworkflow for building MedAlign including (1) pairing clinician-generated instructions with patient EHR\nmarkup, and (2) evaluating language model responses against gold responses written by clinicians.\nCollection Protocol\nReviewing patient medical data requires adhering to strict security protocols to\nprotect patient privacy and prevent protected health information (PHI) leaks. This motivated our 3-stage\ncuration process: (1) online instruction collection from clinicians; (2) instruction-EHR matching; and (3)\nresponse generation. Note we deliberately decouple instruction collection from response generation. This\nenables sampling a larger set of instructions from a more diverse set of clinician specialities while minimizing\nexposure to patient data. However, this approach requires defining a matching function to pair instructions\nwith relevant patient EHRs, a process which may generate errors due to irrelevant instruction-EHR pairings.\nWe discuss the performance of a retrieval-based matching system below.\nStage 1: Collecting Instructions\nClinicians were recruited in our academic medical center via email.\nThrough the use of an online form, clinicians were asked to submit instructions as posed to a hypothetical AI\nassistant designed to facilitate EHR-based tasks. Participants were instructed to envision a clinical vignette\ntypical of their daily practice and to formulate an instruction that the AI could perform to make their work\neasier, faster, and less stressful. For each instruction, participants were asked to provide metadata to assist\nin matching the instruction to a patient, including pertinent clinical characteristics and the clinical context\nwhere the instruction could be used, e.g., \u201cwhen deciding whether to use contrast in a CT scan\u201d. See Appendix\nC for all collected fields.\nStage 2: Instruction-EHR matching\nAll submitted instructions include metadata information on their\nintended clinical context and target patient population. We used instructions tagged \u201capplicable to patients\ngenerally\u201d to maximize their relevance in EHR matching. We evaluated two methods for matching instructions\n4\nTable 2: MedAlign instruction categories and example instructions.\nCategory\nExample Instruction\nGold\nAll\nRetrieve & Summarize\nSummarize the most recent annual physical with the\nPCP\n223\n667\nCare Planning\nSummarize the asthma care plan for this patient\nincluding relevant diagnostic testing, exacerbation\nhistory, and treatments\n22\n136\nCalculation & Scoring\nIdentify the risk of stroke in the next 7 days for this\nTIA patient\n13\n70\nDiagnosis Support\nBased on the information I\u2019ve included under HPI, what\nis a reasonable differential diagnosis?\n4\n33\nTranslation\nI have a patient that speaks only French. Please\ntranslate these FDG-PET exam preparation\ninstructions for her\n0\n2\nOther\nWhat patients on my service should be prioritized for\ndischarge today?\n41\n75\nTotal\n303\n983\nwith EHRs: (1) a simple baseline based on uniform random sampling; and (2) a retrieval-based method using\nBM25Okapi [41].\nFor the retrieval approach, we concatenated every instruction with its corresponding patient characteristics\nand clinical context to construct a search query. We used this query to retrieve the 5 most relevant EHRs\nwithin a randomly selected subsample of 77200 patients from our hospital database. This same subsample\nwas used to match patients for our baseline uniform random sample. After matching, the authors conducted\na manual review to assess binary relevance of all generated instruction-EHR pairs.\nStage 3: Instruction Response Generation\nFor this stage, clinicians were tasked with reviewing\nthe instruction and associated EHR data, then writing a response to that instruction. Whenever feasible,\ninstructions were assigned to clinicians within the same specialty as the original submitter but not the original\nsubmitter themselves. In cases where this was not possible, the instruction was randomly assigned to a\nclinician, in any specialty, that did not submit the instruction. Clinicians were asked whether the instruction\ncould be feasibly applied to the patient in the EHR (e.g., not asking about smoking history in an infant) and\nif the EHR contained all necessary information to answer the instruction. They then manually generated an\nexpert response to the instruction. This response was intended to be brief and clinically relevant, drawing on\nany information available in the supplied EHR, as well as any appropriate external references. The most\nrecent timestamp in the EHR was designated as the \u201ctime anchor\u201d, meaning the response was written as if\nthe instruction had been posed at that point in time.\n4\nDataset Description\nInstructions Collected\nA total of 15 clinicians submitted instructions during the data collection pro-\ncess. These medical practitioners represented 7 distinct specialties, which included Internal Medicine (492\ninstructions submitted), Neurology (320), Radiology (402), Cardiology (71), Oncology (14), Surgery (12), and\nPrimary Care (3). Clinicians provided a varying number of instructions ranging from 1 to 278 with a mean\nof 87 instructions per clinician (see Figure S3). From the 1314 instructions collected, 455 were marked as\napplicable to patients generally and 859 were relevant only to patients with specific clinical characteristics. We\nremoved near-identical instructions (defined by a ROUGE-L similarity above 0.7), yielding 983 instructions\nof which 407 were marked as applicable to patients generally.\n5\nTable 3: Human evaluation of LLM responses. Context: The model\u2019s context length, using its native\ntokenizer. Correct: The percentage of model responses deemed correct by clinicians. WR: Average win rate\nmarginalizing over model pairings. Rank: Empirical mean of human-assigned rankings. \u2020With multi-step\nrefinement the effective context length is infinite, as the model observes the entire EHR albeit in small chunks\nat a time. \u2217For GPT-4 (2k) we used the GPT-4 32k models from OpenAI but restricted its context length\nusing the Vicu\u00f1a-native tokenizer for direct comparison.\nModel\nContext\nCorrect \u2191\nWR \u2191\nRank \u2193\nGPT-4 (MR)\n32768\u2020\n65.0%\n0.658\n2.80\nGPT-4\n32768\n60.1%\n0.676\n2.75\nGPT-4\n2048\u2217\n51.8%\n0.598\n3.11\nVicu\u00f1a-13B\n2048\n35.0%\n0.401\n3.92\nVicu\u00f1a-7B\n2048\n33.3%\n0.398\n3.93\nMPT-7B-Instruct\n2048\n31.7%\n0.269\n4.49\nInstruction-EHR Matches\nBased on evaluation by the authors, for 240 (59%) of the instructions\napplicable to \u201cpatients in general\u201d the first record retrieved by BM25 was relevant. For 303 instructions (74%),\nat least one of the top 5 EHRs returned by BM25 was relevant. In contrast, only 38% of EHRs retrieved via\nuniform random sampling were deemed relevant.\nInstruction Taxonomy\nTo better understand higher-level themes within the instructions submitted, a\npracticing clinician developed a taxonomy of instructions. This taxonomy, described in detail in Table S2,\nincludes 6 categories spanning 20 subcategories. We summarize the distribution of instruction categories\nacross the set of all instructions submitted and those that received responses from a clinician in Table 2.\n5\nBenchmarking LLM Performance\nLLM Selection\nWe evaluated six distinct LLMs, chosen to capture both state-of-the-art, closed-source\nLLM capabilities available to consumers via an API as well as smaller, open-source and user-modifiable\nLLMs with more lenient commercial licensing (e.g., MosaicML\u2019s MPT-7B-Instruct model). Additionally, we\ndesigned our experiments to directly evaluate the impact of model parameters and context length.\nFor a state-of-the-art LLM, we selected GPT-4 (through Microsoft\u2019s Azure OpenAI HIPAA compliant\ngpt-4-32k-0301 API) due to its state-of-the-art performance on various medical tasks, its long 32k context\nlength, and its availability to researchers and clinics. However, despite this context length, it proved insufficient\nfor accommodating full EHRs (more than 80% of EHRs in MedAlign contain more than 32k tokens, see see\nTable S5). To address this limitation, we explored a multi-step refinement (MR) approach [38] to maximize\neffective context length. In this approach, the EHR is divided into \u201cchunks\u201d designed to be as big as possible\n(30k tokens, without concern for maintaining valid XML structure) while still fitting within the model\u2019s\ncontext length. A response to the instruction is generated using the chronologically first/earliest EHR \u201cchunk\u201d\nas context, then the second \u201cchunk\u201d is given to the model and the model is instructed to update its response\nif appropriate or maintain the same response otherwise, and so on, until the entire EHR has been fed through\nthe model. We acknowledge the potential effectiveness of other methods, such as Retrieval Augmented\nGeneration (RAG), in answering questions regarding long documents. However, our primary interest was in\nmeasuring the LLMs\u2019 abilities to discern and utilize clinically relevant material when answering questions\nabout the EHR. While methods such as RAG would likely be performant in this area, they would not have\nenabled us to directly assess the LLMs\u2019 innate abilities to ignore irrelevant material and find details pertinent\nto the instruction.\nFor smaller, open-source models we evaluated Vicu\u00f1a-7B and Vicu\u00f1a-13B [4] as well as MPT-7B-Instruct\n[20]. These models are widely available and user-modifiable with favorable licensing agreements, but they have\nconsiderably smaller context lengths (2048 tokens) compared to GPT-4. To enable more direct comparisons,\nwe assessed GPT-4 under a restricted context length designed to exactly match the context length of the\nVicu\u00f1a model.\n6\n0.00\n0.48\n0.56\n0.73\n0.71\n0.82\n0.52\n0.00\n0.58\n0.72\n0.74\n0.81\n0.44\n0.42\n0.00\n0.67\n0.70\n0.76\n0.27\n0.28\n0.33\n0.00\n0.50\n0.63\n0.29\n0.26\n0.30\n0.50\n0.00\n0.64\n0.18\n0.19\n0.24\n0.37\n0.36\n0.00\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nMPT-7B-Instruct (2k)\nVicu\u00f1a-7B (2k)\nVicu\u00f1a-13B (2k)\nGPT-4 (2k)\nGPT-4 (32k)\nGPT-4 (32k + MR)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nProp. Model A Wins for All Non-tied A vs. B Battles (Human Ranks)\nModel B: Loser\nModel A: Winner\n0.00\n0.50\n0.52\n0.66\n0.63\n0.79\n0.50\n0.00\n0.51\n0.63\n0.58\n0.77\n0.48\n0.49\n0.00\n0.66\n0.61\n0.79\n0.34\n0.37\n0.34\n0.00\n0.49\n0.70\n0.37\n0.42\n0.39\n0.51\n0.00\n0.71\n0.21\n0.23\n0.21\n0.30\n0.29\n0.00\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nMPT-7B-Instruct (2k)\nVicu\u00f1a-7B (2k)\nVicu\u00f1a-13B (2k)\nGPT-4 (2k)\nGPT-4 (32k)\nGPT-4 (32k + MR)\n0.3\n0.4\n0.5\n0.6\n0.7\nProp. Model A Wins for All Non-tied A vs. B Battles (COMET Ranks)\nModel B: Loser\nModel A: Winner\nFigure 2: (Left) Head-to-head comparison of model performance based on human ranks. The number in row\ni, column j indicates the proportion of instructions for which the response generated by the model in row i\nwas strictly preferred over the model in column j. (Right) Head-to-head evaluation of model performance\nusing COMET Ranks. Represents the same matrix structure and interpretation as on the left, but using\nrankings derived from COMET, an automated metric, rather than clinician-generated rankings. Model win\nrates using COMET follow a similar pattern as to model win rates using human rankings.\nGenerating LLM Responses to EHR-based Questions and Instructions\nUsing a standard prompt\ntemplate (see Figure S9), each model was tasked to fulfill the given instruction grounded on its corresponding\nEHR pair. Due to current models\u2019 context length restrictions, EHRs needed to be truncated. To calculate the\nnumber of tokens of EHR context to include in the prompt, we took each model\u2019s maximum context length\n(in terms of the number of tokens under that model\u2019s specific tokenizer), reserved 256 tokens for generation,\nand subtracted any tokens used for the corresponding structured prompt and instruction. This truncation\nwas performed by counting tokens from the end of the record, ensuring that as much recent information as\npossible was retained.\nClinician Evaluation of LLM Responses\nNine clinicians were asked to evaluate and rank the responses\ngenerated by 6 separate LLMs. Clinicians did not evaluate their own responses or responses to instructions\nthat they submitted. When feasible, clinicians evaluated responses to instructions that were written by a\nclinician in their same specialty. The instructions and EHRs reviewed by the clinicians were exactly the same\nin structure and content as those provided to the LLMs (albeit the EHRs reviewed by clinicians were never\ntruncated, whereas the EHRs ingested by the LLMs were truncated according to their respective context\nlengths). Clinicians recorded a binary evaluation of whether the response was correct or incorrect, with\n\u201cincorrect\u201d defined as meeting at least one of the following criteria:\nC1. Response is not clinically appropriate based on the available EHR information;\nC2. Response includes errors that, if corrected, would change the clinical interpretation;\nC3. Response does not address the instruction.\nResponses not marked as \u201cincorrect\u201d were deemed to be \u201ccorrect\u201d. Clinicians then ranked the quality of\nthe LLM responses based on which provided the most clinically relevant and appropriate response. Ties were\npermitted. The clinicians were blinded to which LLM generated each output, and the order of LLM output\nwas reshuffled for each instruction. Each clinician reviewed 49 instruction-patient pairs on average, yielding\n303 pairs reviewed overall with 50 instruction-EHR pairs being reviewed by three clinicians.\n7\nOverall, we found that more than half of the responses generated by the GPT-4 variants we tested were\ndeemed correct by clinicians (65% for GPT-4 (32k + MR), 60.1% for GPT-4 (32k), 51.8% for GPT-4 (2k)).\nBy contrast, only about one in three responses generated by the Vicu\u00f1a and MPT-7B models were considered\ncorrect (35% for Vicu\u00f1a-13B, 33.3% for Vicu\u00f1a-7B, 31.7% for MPT-7B-Instruct; see Table 3). In head-to-head\ncomparisons, GPT-4 without context length restriction was preferred over the Vicu\u00f1a-13B model in 72% of\ninstances, and preferred over MPT-7B-Instruct 81% of the time (see Figure 2). The GPT-4 model with 32k\ncontext length and no multi-step refinement had the highest overall average win-rate against all other models\n(0.676).\nTable 4: Correlation (mean Kendall\u2019s Tau) between ranking automated metrics\u2019 ranking and human ranking\nof LLM outputs. Mean Kendall\u2019s Tau between human reviewers (inter-rater reliability) was 0.43.\nAutomated Metric\nSource\nAvg.\n95% CI\nAugmented\nCorr.\nCOMET\n\u2713\n0.37\n0.33-0.41\nBERTScore\n0.34\n0.30-0.38\nMETEOR\n0.32\n0.28-0.36\nchrF++\n0.29\n0.25-0.33\nGoogleBLEU\n0.29\n0.25-0.33\nROUGE-L\n0.27\n0.23-0.31\nBLEURT\n0.25\n0.21-0.30\nLENS\n0.18\n0.14-0.22\nUniEval Relevance\n\u2713\n0.27\n0.23-0.32\nUniEval Fluency\n\u2713\n0.11\n0.06-0.15\nUniEval Coherence\n\u2713\n0.09\n0.04-0.13\nUniEval Consistency\n\u2713\n0.09\n0.04-0.13\nUniEval Overall\n\u2713\n0.20\n0.15-0.24\nInter-Rater Reliability\n0.44\n0.34-0.53\n6\nAutomated Evaluation of LLM Responses\nWith the aim to to find an automated proxy for clinician-in-the-loop evaluation, we analyzed the correlation\nbetween a suite of automated metrics and human preference rankings using the Kendall\u2019s Rank Correlation\n(\u201cKendall\u2019s Tau\u201d) [15]. We also calculated the inter-rater correlation between human rankers, yielding a mean\nKendall\u2019s Tau coefficient of 0.44. The average correlations between metrics and human rankings is shown in\nTable 4. As noted by previous studies [23], the majority of these metrics have shown moderate correlation\nwith human preference and are widely reported in NLG tasks.\nWe evaluated each model output using both source-free (SF) and source-augmented (SA) automated\nmetrics. Source-free metrics compare a model\u2019s output to a gold standard reference answer (in our case\ngenerated by a clinician) without the use of any additional context or sources (i.e., without any information\nfrom the EHR). We selected BERTScore [50], METEOR [1], chrF++ [29], GoogleBLEU [46], and ROUGE-L\n[18] due to their availability and wide use. Source-augmented metrics consider source (e.g., the EHR) in\naddition to the reference answer and the model response. The SA metrics we considered (and the LMs they\nuse) include UniEval (T5-large) [53] and COMET (XLM-RoBERTa) [31]. As these models have limited\ncontext length we used the BM25Okapi algorithm to retrieve relevant snippets from within the patient\u2019s EHR\nusing the instruction as a search query.\nOverall, COMET [31] exhibited the strongest correlation with clinician preference rankings, approaching\nthe level of human inter-reviewer reliability (0.37 vs. 0.44). As seen in Figure 2, the overall trends of\nhead-to-head comparisons were preserved when using COMET as the source of model output rankings vs.\nclinician-generated rankings. Specifically, GPT-4 was consistently preferred over the Vicu\u00f1a and MPT-7B\nmodels by both COMET and clinicians, and the Vicu\u00f1a models were consistently preferred over the MPT-7B\n8\nmodel. Within the GPT-4 variants and between the two Vicu\u00f1a models considered, win-rate preferences\nwere not necessarily preserved, suggesting utility of COMET as a reasonable but perhaps coarse measure of\nmodel performance in this setting. The next most correlated metric with human rankings after COMET was\nBERTScore, a source-free metric, with an average correlation coefficient of 0.34.\nUsing our best performing automated metrics, COMET and BERTScore, we evaluated four recently\nreleased instruction-tuned medical LLMs (all based on Llama 2 [40]): AlpaCare [51], ClinicalCamel [37] and\nMed42 [5]. Figure 3 shows that, controlling for model size, current medical instruction tuning approaches\nlargely yield worse performance in MedAlign vs. the base Llama 2 Chat model.\nBase vs. Base + Medical Instruction Tuning\nFigure 3: Automated evaluation of medical instruction-tuned LLMs vs. general instruction-tuned counterparts\nusing the best-performing metrics (COMET and BERTScore).\n7\nDiscussion and Conclusion\nReadily available datasets and benchmarks for easy-to-evaluate tasks like closed-form question answering\nhave helped to measure the remarkable progress of LLMs, even in medical domains [16]. However, logistical\ndifficulties and significant labeling costs have hindered progress towards establishing a shared dataset\nand benchmark for tasks amenable to LLMs and which truly represent clinician needs. We share such a\nbenchmark dataset with the research community, which takes a novel approach towards instruction gathering\nby modularizing and isolating the process of instruction solicitation and EHR pairing. To the best of our\nknowledge, our dataset is the first to evaluate LLM performance on clinician-generated instructions and\ninstructions using comprehensive, longitudinal EHRs. This affords several new insights.\nThe Importance of Context Length.\nWhile GPT-4 with a restricted context length of 2048 tokens\nachieved a correctness rate of 51.8%, the exact same GPT-4 model given 32000 tokens of context from the\nEHR achieved a correctness rate of 60.1%. Thus the additional context length yielded an additional 8.3%\nin the proportion of correct responses. Given the sheer quantity of tokens and concepts contained within\ncomprehensive EHRs, including in MedAlign (see Appendix N), it is perhaps not surprising that instruction\nfollowing performance was poor with a limited context length. Indeed, not a single EHR in MedAlign can\nfit entirely within the Vicu\u00f1a or MPT-7B\u2019s 2048 context length, and only 19.6% of these records can entirely\nfit within the 32k context length afforded by GPT-4. This highlights the importance of context length in\napplying LLMs to EHR-based tasks and motivates efforts to increase context lengths via e.g., methods that\n9\ndo so implicitly via position interpolation [3] or approaches that explicitly improve the training efficiency of\nmathematical operations [8].\nMisalignment with Current Benchmarks\nMedical instruction tuning in academic models currently\nfavors shorter contexts, optimizing for tasks like MedQA and MMLU. MedQA, consisting of USMLE-style\nquestions covering diagnosis support and care planning, is a popular choice for assessing the medical skills of\nan LLM [22, 24, 34, 45, 47]. However, USMLE-style questions only comprise 17% of the instructions submitted\nby clinicians to MedAlign while 68% of instructions involve retrieving and summarizing data from the EHR.\nOur results highlight that current medical instruction tuning practices often result in significant performance\ndegradation in longer context tasks, with base Llama-2 models outperforming medical instruction-tuned\nLLMs in most cases. Given the importance of longer contexts and complex summarization skills in addressing\nclinician information needs, our work underscores the need to evaluate instruction tuning tasks beyond\nMedQA and similar narrow benchmarks.\nLimitations.\nOur approach of first soliciting instructions and then pairing these instructions to EHRs can\nincrease the scale and diversity of instructions collected, but at a cost. Despite yielding almost twice as many\nrelevant pairings as simply randomly selecting an EHR for each instruction, our BM25 approach did not yield\na relevant match for approximately 30% of instructions. In other words, while an instruction submitted by\na clinician was of course relevant to the hypothetical patient they had in mind at the time of submission,\nit frequently ended up not being relevant to an actual patient EHR. There are potential ways to improve\nthis matching process e.g., by using vector databases powered by BERT-style models which could better\ncapture semantic alignment between queries and EHRs relative to BM25 [44]. Additionally, while we solicited\ninstructions from a large number of clinicians at our academic medical center with diverse specialties and\nbackgrounds, the clinicians who submitted data to MedAlign represent only a small fraction of the overall\nclinician workforce.\nConclusion.\nThis work establishes, for the first time, the performance of some of the most capable LLMs\navailable \u2014 GPT-4, LLaMA, and MPT-7B-Instruct \u2014 on EHR-based instruction-following tasks. We find\nthat approximately one-third of the best-performing LLM\u2019s responses are incorrect. The benchmark dataset\nwe share, MedAlign enables researchers to measure what matters and focus on tasks that are clinically\nrelevant with significant potential positive impact. In addition, our findings establishing significant correlation\nbetween human preference and existing automated metrics provide a path for researchers to make technical\nprogress without requiring the organizational infrastructure for clinical labeling. Finally, our novel approach\ntowards soliciting clinician instructions paves the way for even larger-scale data collection efforts, both for\ntraining and evaluation purposes.\n8\nEthics Statement\nSecurity and Compliance.\nA university institutional review board granted approval for this study\n(reference number 57916). All authors handling data individually completed institutional HIPAA and data\nprivacy training prior to engagement with the data. All models exposed to data were deployed within\nHIPAA-compliant compute infrastructure.\nPrivacy and Data Deidentification\nAll data were de-identified using a \u201chiding in plain sight\u201d protocol\nwherein protected health information (PHI) is replaced by coherent synthetic alternatives [2], e.g., tagging\nall person names and replacing them with a randomly generated name. For the research release of the\nMedAlign dataset, all documents will undergo human review to minimize risk of inadvertently exposing PHI.\nThe dataset will be hosted in an university-approved, secure data portal and will require user credentialing to\naccess, i.e., completing CITI ethics training and agreeing to the terms of our data use agreement.\nPatient Consent\nEvery patient at our medical center has provided their signature on a privacy notice,\nwhich explains that their medical records could be utilized for research. This data, once de-identified, is\naccessible to researchers under a comprehensive IRB protocol of the university.\n10\nSocietal impact.\nLLMs could streamline clinician workflows within the EHR by replacing clunky point-\nand-click interfaces with natural language interactions, improving clinician efficiency. Muhiyaddin et al. [21]\nfound EHR-related documentation tasks to be a leading cause of physician burnout, resulting in low-quality\ncare, costly turnover, and a decline in patient safety. By easing documentation burden, LLMs could thus\nincrease care quality, decrease clinician turnover, and improve patient safety. MedAlign provides a way to\nassess whether LLMs are safe and ready for the deployments necessary to realize these potential benefits.\nIntroducing LLMs into the clinic also poses potential risks. Even the best-performing model of those we\nassessed (GPT-4) produced incorrect responses for more than 33% of the clinician-generated instructions.\nThese errors could decrease patient safety by leading to poor clinical decision making. More insidiously, a\nrecent study by Omiye et al. [26] noted that commercial LLMs propagate harmful race-based stereotypes in\nmedicine. We analyzed LLM performance differences across race in MedAlign (see Appendix) and found\nminimal disparities, but more work is needed. Additionally, we did not measure the prevalence of specific\nfailure modes like hallucination and leave this for future work.\nReferences\n[1] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with\nhuman judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for\nmachine translation and/or summarization, pages 65\u201372, 2005.\n[2] D. Carrell, B. Malin, J. Aberdeen, S. Bayer, C. Clark, B. Wellner, and L. Hirschman. Hiding in plain\nsight: use of realistic surrogates to reduce exposure of protected health information in clinical text.\nJournal of the American Medical Informatics Association, 20(2):342\u2013348, 2013.\n[3] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via\npositional interpolation. arXiv preprint arXiv:2306.15595, 2023.\n[4] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez,\nI. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n[5] C. Christophe, A. Gupta, N. Hayat, P. Kanithi, A. Al-Mahrooqi, P. Munjal, M. Pimentel, T. Raha,\nR. Rajan, and S. Khan. Med42 - a clinical large language model, 2023.\n[6] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma,\net al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n[7] N. Culbertson. The skyrocketing volume of healthcare data makes privacy imperative. Forbes Technology\nCouncil Post, 2021.\n[8] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact\nattention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.\n[9] J. Fan. Annotating and characterizing clinical sentences with explicit why-qa cues. In Proceedings of the\n2nd Clinical Natural Language Processing Workshop, pages 101\u2013106, 2019.\n[10] M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, E. Avramidis, T. Kocmi, G. Foster, A. Lavie, and\nA. F. Martins. Results of wmt22 metrics shared task: Stop using bleu\u2013neural metrics are better and\nmore robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46\u201368,\n2022.\n[11] A. Gaffney, S. Woolhandler, C. Cai, D. Bor, J. Himmelstein, D. McCormick, and D. U. Himmelstein.\nMedical documentation burden among us office-based physicians in 2019: a national study. JAMA\nInternal Medicine, 182(5):564\u2013566, 2022.\n[12] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. In International Conference on Learning Representations, 2020.\n11\n[13] S. Henry, K. Buchan, M. Filannino, A. Stubbs, and O. Uzuner. 2018 n2c2 shared task on adverse\ndrug events and medication extraction in electronic health records. Journal of the American Medical\nInformatics Association, 27(1):3\u201312, 2020.\n[14] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits. What disease does this patient\nhave? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11\n(14):6421, 2021.\n[15] M. G. Kendall. Rank correlation methods. 1948.\n[16] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepa\u00f1o, M. Madriaga, R. Aggabao,\nG. Diaz-Candido, J. Maningo, et al. Performance of chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models. PLoS digital health, 2(2):e0000198, 2023.\n[17] E. Lehman, V. Lialin, K. E. Legaspi, A. J. Sy, P. T. Pile, N. R. Alberto, R. R. Ragasa, C. V. Puyat, M. K.\nTali\u00f1o, I. R. Alberto, P. G. Alfonso, D. Moukheiber, B. Wallace, A. Rumshisky, J. Liang, P. Raghavan,\nL. A. Celi, and P. Szolovits. Learning to ask like a physician. In T. Naumann, S. Bethard, K. Roberts, and\nA. Rumshisky, editors, Proceedings of the 4th Clinical Natural Language Processing Workshop, pages 74\u201386,\nSeattle, WA, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.clinicalnlp-1.8.\nURL https://aclanthology.org/2022.clinicalnlp-1.8.\n[18] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74\u201381, 2004.\n[19] E. R. Melnick, L. N. Dyrbye, C. A. Sinsky, M. Trockel, C. P. West, L. Nedelec, M. A. Tutty, and\nT. Shanafelt. The association between perceived electronic health record usability and professional\nburnout among us physicians. In Mayo Clinic Proceedings, volume 95, pages 476\u2013487. Elsevier, 2020.\n[20] N. T. MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.\nURL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n[21] R. Muhiyaddin, A. H. ElFadl, E. Mohamed, Z. Shah, T. Alam, A. A. Abd-alrazaq, and M. S. Househ.\nElectronic health records and physician burnout: A scoping review. ICIMTH, 289:481\u2013484, 2021.\n[22] V. Nair, E. Schumacher, G. Tso, and A. Kannan. Dera: enhancing large language model completions\nwith dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071, 2023.\n[23] I. Nimah, M. Fang, V. Menkovski, and M. Pechenizkiy. NLG evaluation metrics beyond correlation\nanalysis: An empirical metric preference checklist. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors,\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 1240\u20131266, Toronto, Canada, July 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.69. URL https://aclanthology.org/2023.acl-long.69.\n[24] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz. Capabilities of gpt-4 on medical\nchallenge problems. arXiv preprint arXiv:2303.13375, 2023.\n[25] L. E. S. e. Oliveira, E. T. R. Schneider, Y. B. Gumiel, M. A. P. d. Luz, E. C. Paraiso, and C. Moro.\nExperiments on portuguese clinical question answering. In Brazilian Conference on Intelligent Systems,\npages 133\u2013145. Springer, 2021.\n[26] J. A. Omiye, J. C. Lester, S. Spichak, V. Rotemberg, and R. Daneshjou. Large language models propagate\nrace-based medicine. NPJ Digital Medicine, 6(1):195, 2023.\n[27] A. Pampari, P. Raghavan, J. Liang, and J. Peng. emrQA: A large corpus for question answering on\nelectronic medical records. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357\u20132368, Brussels,\nBelgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1258. URL\nhttps://aclanthology.org/D18-1258.\n12\n[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,\npages 311\u2013318, 2002.\n[29] M. Popovi\u0107. chrf++: words helping character n-grams. In Proceedings of the second conference on\nmachine translation, pages 612\u2013618, 2017.\n[30] P. Raghavan, S. Patwardhan, J. J. Liang, and M. V. Devarakonda. Annotating electronic medical records\nfor question answering. arXiv preprint arXiv:1805.06816, 2018.\n[31] R. Rei, C. Stewart, A. C. Farinha, and A. Lavie. COMET: A neural framework for MT evaluation.\nIn B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online, Nov. 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.org/\n2020.emnlp-main.213.\n[32] N. H. Shah, D. Entwistle, and M. A. Pfeffer. Creation and adoption of large language models in medicine.\nJAMA, 2023.\n[33] A. Siems, R. Banks, R. Holubkov, K. L. Meert, C. Bauerfeld, D. Beyda, R. A. Berg, Y. Bulut, R. S.\nBurd, J. Carcillo, et al. Structured chart review: Assessment of a structured chart review methodology.\nHospital pediatrics, 10(1):61\u201369, 2020.\n[34] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis,\nS. Pfohl, et al. Large language models encode clinical knowledge. Nature, pages 1\u20139, 2023.\n[35] S. Soni, M. Gudala, A. Pajouhi, and K. Roberts. Radqa: A question answering dataset to improve\ncomprehension of radiology reports. In Proceedings of the Thirteenth Language Resources and Evaluation\nConference, pages 6250\u20136259, 2022.\n[36] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large\nlanguage models in medicine. Nature medicine, 29(8):1930\u20131940, 2023.\n[37] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, and B. Wang. Clinical camel: An open\nexpert-level medical language model with dialogue-based knowledge encoding, 2023.\n[38] O. Topsakal and T. C. Akinci. Creating large language model applications utilizing langchain: A primer\non developing llm apps fast. In Proceedings of the International Conference on Applied Engineering and\nNatural Sciences, Konya, Turkey, pages 10\u201312, 2023.\n[39] F. Toscano, E. O\u2019Donnell, J. E. Broderick, M. May, P. Tucker, M. A. Unruh, G. Messina, and L. P.\nCasalino. How physicians spend their work time: an ecological momentary assessment. Journal of\nGeneral Internal Medicine, 35:3166\u20133172, 2020.\n[40] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n[41] A. Trotman, A. Puurula, and B. Burgess. Improvements to bm25 and language models examined. In\nProceedings of the 19th Australasian Document Computing Symposium, pages 58\u201365, 2014.\n[42] E. A. Voss, R. Makadia, A. Matcho, Q. Ma, C. Knoll, M. Schuemie, F. J. DeFalco, A. Londhe, V. Zhu,\nand P. B. Ryan. Feasibility and utility of applications of the common data model to multiple, disparate\nobservational health databases. Journal of the American Medical Informatics Association, 22(3):553\u2013564,\n2015.\n[43] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned\nlanguage models are zero-shot learners. In International Conference on Learning Representations, 2022.\nURL https://openreview.net/forum?id=gEZrGCozdqR.\n13\n[44] Z. Wei, X. Xu, C. Wang, Z. Liu, P. Xin, and W. Zhang. An index construction and similarity retrieval\nmethod based on sentence-bert. In 2022 7th International Conference on Image, Vision and Computing\n(ICIVC), pages 934\u2013938. IEEE, 2022.\n[45] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Pmc-llama: Further finetuning llama on medical\npapers. arXiv preprint arXiv:2304.14454, 2023.\n[46] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,\nK. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and\nmachine translation. arXiv preprint arXiv:1609.08144, 2016.\n[47] M. Yasunaga, J. Leskovec, and P. Liang. LinkBERT: Pretraining language models with document\nlinks. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 8003\u20138016, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.551. URL\nhttps://aclanthology.org/2022.acl-long.551.\n[48] X. Yue, B. J. Gutierrez, and H. Sun. Clinical reading comprehension: A thorough analysis of the\nemrQA dataset. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, pages 4474\u20134486, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.410. URL https:\n//aclanthology.org/2020.acl-main.410.\n[49] X. Yue, X. F. Zhang, Z. Yao, S. Lin, and H. Sun. Cliniqg4qa: Generating diverse questions for domain\nadaptation of clinical question answering. In 2021 IEEE International Conference on Bioinformatics\nand Biomedicine (BIBM), pages 580\u2013587. IEEE, 2021.\n[50] T. Zhang, V. Koshre, F. Wu, K. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with\nbert. In International Conference on Learning Representations, 2020. URL https://openreview.net/\nforum?id=SkeHuCVFDr.\n[51] X. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, and L. R. Petzold. Alpacare:instruction-tuned large language\nmodels for medical application, 2023.\n[52] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A\nsurvey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n[53] M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han. Towards a unified\nmulti-dimensional evaluator for text generation. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors,\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2023\u2013\n2038, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.emnlp-main.131. URL https://aclanthology.org/2022.emnlp-main.131.\n14\nAppendices\nAppendix A\nConflict of Interest Disclosures\nScott Fleming receives consulting fees from SmarterDx. Jason Fries receives consulting fees from Snorkel\nAI. Chia-Chun Chiang receives consulting fees from Satsuma Pharmaceuticals and eNeura. Jenelle Jindal\nis a founder of Jindal Neurology, Inc. and is paid per diem as a physician with Kaiser Permanente, San\nFrancisco, CA. Nima Aghaeepour consults for MaraBioSystems and serves on the scientific advisory boards of\nJanuaryAI, Parallel Bio, and WellSimBiomedical Technologies. Akshay Chaudhari consults for Subtle Medical\nand Patient Square Capital; reports equity from Brain Key, Subtle Medical, and LVIS Corp; and serves on\nthe scientific advisory board of Brain Key and Chondrometrics GmbH. Jonathan Chen is the co-founder of\nReaction Explorer LLC and receives consulting fees from Sutton Pierce and Younker Hyde MacFarlane PLLC\nas a medical expert witness. Nigam Shah is a co-founder of Prealize Health and Atropos Health.\nAppendix B\nFunding/Support\nThis work is generously supported by the Mark and Debra Leslie endowment for AI in Healthcare (Nigam\nShah); Stanford Graduate Fellowships (Louis Blankemeier, Scott Fleming); National Institutes of Health\nawards R35GM138353 (Nima Aghaeepour); R01 AR077604, R01 EB002524, R01 AR079431, and P41\nEB027060 (Akshay Chaudhari); NIH contracts 75N92020C00008 and 75N92020C00021 (Akshay Chaudhari);\nthe ARC Institute (Alejandro Lozano); the National Institute of Allergy and Infectious Diseases award\n1R01AI17812101 (Jonathan Chen); the National Institute on Drug Abuse Clinical Trials Network award\nUG1DA015815 - CTN-0136 (Jonathan Chen); a Stanford Artificial Intelligence in Medicine and Imaging -\nHuman-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant (Jonathan Chen); and an NSF Career\nAward (Emma Brunskill).\nAppendix C\nOnline Instruction Collection Form\nVia a form hosted on Google Forms (see data/instruction_solicitation_form.pdf in the associated code\nrepository), we asked practicing clinicians to provide the following information for each submitted instruction.\n1. Instruction or Question\n2. What part(s) of the EHR would you reference to complete the request? (e.g., Notes, Imaging Results,\nMedication List);\n3. In which clinical context would you most likely use this instruction/question? (e.g., deciding whether\nto use contrast in a CT scan, drafting post-operative discharge instructions);\n4. Is this instruction applicable to all patients generally or only to patients with certain diseases/treatments/clinical\ncharacteristics?\n5. If applicable, what are those specific diseases, treatments, and/or other clinical characteristics?\nAppendix D\nDataset Details\nMedAlign contains a total of 1314 instructions submitted by 15 clinicians across 7 specialities. We removed\nnear-identical instructions (defined by a ROUGE-L similarity above 0.7) leaving a total of 983 instructions.\nEach instruction was assigned at least one of 6 categories (Retrieve & Summarize, Care Planning, Diagnosis\nSupport, Calculation & Scoring, and Other) and 20 subcategories (see Table S2). Figure S2 shows a tree map\nof subcategories by frequency in MedAlign. A subset of 303 instructions paired to 276 unique longitudinal\nEHRs contain clinician-written gold reference. Figure S1 shows a cohort diagram for the materialization\nof this subset. Table S1 shows descriptive statistics for MedAlign. Table S3 shows example clinician and\nmodel responses to an instruction from MedAlign.\n15\nTable S1: MedAlign Statistics: Counts of instructions, EHRs, responses, clinician evaluated LLMs, reviewers,\nand specialties\nAspect\nCount\nCollected Instructions\n1314\nDe-duplicated Instructions\n983\nLongitudinal EHRs\n276\nClinician-Generated Responses\n303\nLLMs Ranked by Clinicians\n6\nClinician Reviewers\n15\nSpecialities\n7\nCategories\n6\nSubcategories\n20\n1314 Instructions \nExcluded Instructions \nwith Rouge-L over 0.7 \n983  Instructions\n \n983x5  Instructions-EHR pairs\n \n407x5  Instructions-EHR pairs\n \n303x1  Instructions-EHR pairs\nwith 276 unique EHRs\nPaired instructions\n with EHRs\nEHR Index size: 77200\nSelected instructions \napplicable to \u201cPatients \nin General\u201d\nRelevancy Evaluation by \nHumans \nFigure S1:\nMedAlign cohort diagram: selection criteria for the construction of relevant instruction-EHR\npairs assessed by clinicians.\n16\n \nRetrieve & Summarize (66.29%)\nCare Planning (16.74%)\nCalculation & Scoring (6.77%)\nOther (6.47%)\nDiagnosis Support (3.58%)\nRetrieve\nMedical History\n(335)\nRetrieve\nImaging Studies\n(111)\nRetrieve\nDemographics\n(107)\nRetrieve\nMedication\nInformation\n(99)\nRetrieve\nLaboratory\nStudies (61)\nRetrieve Social\nHistory (34)\nProvide Patient\nEducation\nInformation\n(31)\nRetrieve Vitals\n(24)\nRetrieve\nAppointment\nInformation\n(23)\nRetrieve\nGenetic And\nFamily History\n(19)\nRetrieve\nInstitutional\nPolicies (15)\nOther (12)\nDetermine Plan\nOf Care (220)\nCalculate\nNumerical\nMeasures (47)\nProvide Risk\nAssessment (42)\nOther (85)\nProvide\nDiagnosis\nAssistance (47)\nFigure S2: Treemap of the clinical instruction categories (taxonomy) assigned by a clinician. Each category\nwithin the treemap is associated with a parent class derived from the clinician-generated taxonomy.\n17\nTable S2: MedAlign Categories and subcategories for the 1314 (\u201cAll\u201d) instructions collected and 303 (\u201cGold\u201d)\ninstructions with clinician-generated responses.\nCategory\nSubcategory\nDescription\nGold All\nRetrieve & Summarize\nRetrieve Medical History\nRetrieve and summarize past descriptions of symptoms, signs,\nexaminations, treatments, surgeries, annual physical exams\n67\n335\nCare Planning\nDetermine Plan of Care\nDetermine a future plan of care for the patient. If a diagnosis\nwas made, then the planned treatment. If the patient is\ndeemed healthy, then the planned prevention. Plan of care\ncould include follow up tests, imaging to be done in the future.\n22\n220\nRetrieve & Summarize\nRetrieve Imaging Studies\nRetrieve and summarize past imaging performed, any\ndiagnostic testing including MRI, CT, EKG\n42\n111\nRetrieve & Summarize\nRetrieve Demographics\nRetrieve and summarize demographic, insurance information,\ncode status, power of attorney, emergency contact\n12\n107\nRetrieve & Summarize\nRetrieve Medication\nInformation\nRetrieve and summarize medications taken, any interactions\nbetween medications, medication side effects\n22\n99\nOther\nOther\nInstructions that do not fit into any of the other categories\n41\n85\nRetrieve & Summarize\nRetrieve Laboratory\nStudies\nRetrieve and summarize past laboratory values eg from blood,\nurine, CSF\n12\n61\nCalculation & Scoring\nCalculate Numerical\nMeasures\nUsing standardized tools and scores (BMI, TIMI,\nCHADS2VASC, ABCD2Score) calculate numerical\nassessments about current state or future risk\n6\n47\nDiagnosis Support\nProvide Diagnosis\nAssistance\nProvide a differential diagnosis\n4\n47\nCalculation & Scoring\nProvide Risk Assessment\nProvide information on risk of developing new diagnoses or\ncomplications of a diagnosis based on known clinical research\n7\n42\nRetrieve & Summarize\nRetrieve Social History\nRetrieve and summarize social determinants of health\nincluding marital status, alcohol use, drug use\n21\n34\nRetrieve & Summarize\nProvide Patient\nEducation Information\nProvide patient education on diseases, treatments,\nmedications, procedures, labs, imaging. Provide plain language\ndischarge instructions: what medications to take, which\nappointments to go to next\n7\n31\nRetrieve & Summarize\nRetrieve Vitals\nRetrieve and summarize past data regarding vital signs\ncollected (blood pressure, heart rate, respiratory rate, O2 sat,\ninput/output, calories in, bowel movement)\n10\n24\nRetrieve & Summarize\nRetrieve Appointment\nInformation\nRetrieve and summarize date/time/provider/department of\nappointments that the patient has been to or is planned for\n(video visit, in-person visits, lab visits, imaging visits)\n13\n23\nRetrieve & Summarize\nRetrieve Genetic and\nFamily History\nRetrieve and summarize family history\n6\n19\nRetrieve & Summarize\nRetrieve Institutional\nPolicies\nRetrieve institutional policies, consents, guidelines\n6\n15\nRetrieve & Summarize\nRetrieve Provider\nReferrals\nRetrieve and summarize referrals from one provider to another\n4\n7\nRetrieve & Summarize\nRetrieve Communications\nwith Patient\nRetrieve and summarize communications (email, fax, secure\nmessage) sent to patients or from patients\n1\n4\nTranslation\nTranslate Materials\nTranslate documents and instructions from one language to\nanother language\n0\n2\nRetrieve & Summarize\nRetrieve Coding & Billing\nInformation\nRetrieve and summarize coding & billing information\n0\n1\n18\nTable S3: Responses and clinician assessment of model responses for a sample instruction. Responses have\nidentifying information redacted to protect patient privacy.\nInstruction\nModel Response\nModel\nCorrect?\nRationale\nHas she ever been\non a statin before?\nYes, she has been on a statin before but she had\nside effects of myositis and GI issues. Had GI\nupset with simvastatin, nausea with crestor,\nvomiting wtih pravastatin, fluvastatin cannot\ntolerate\nClinician Reviewer 1\n(Yes)\nGold standard\nreference answer\nHas she ever been\non a statin before?\nPatient on pravastatin and simvastatin, but these\nwere stopped due to muscle pain and because they\nwere ineffective.\nClinician Reviewer 2\n(Yes)\nGold standard\nreference answer\nHas she ever been\non a statin before?\nYes, this patient has been on pravastatin and\nsimvastatin before.\nClinician Reviewer 3\n(Yes)\nGold standard\nreference answer\nHas she ever been\non a statin before?\nNo, she has never been on a statin before\nMPT-7B-Instruct (2k)\nNo\nContradicts the\npatient\u2019s medical\nrecord\nHas she ever been\non a statin before?\nERROR: The response was filtered due to the\nprompt triggering Azure OpenAIs content\nmanagement policy.\nGPT-4 (32k)\nNo\nUninformative error\nmessage\nHas she ever been\non a statin before?\n\u201c\u201c\u201c No \u201d\u201d\u201d\nWhat is the patient\u2019s age in days and years at the\ntime of the visit on [DATE]?\nWhat are the patient\u2019s vital signs at the time of\nthe visit on [DATE]?\nWhat is the patient\u2019s BMI at the time of the visit\non [DATE]?\nWhat is the patient\u2019s blood pressure at the time of\nthe visit on [DATE]?\nWhat is the patient\u2019s oxygen saturation at the\ntime of the visit on [DATE]?\nWhat is the patient\u2019s thyroid function at the time\nof the visit on [DATE]?\nWhat is the patient\u2019s tobacco use history at the\ntime of the visit on [DATE]?\nWhat is the patient\u2019s history of clinical findings at\nthe time of the visit on [DATE]?\nWhat is the patient\u2019s current medications at the\ntime of the visit on [DATE]\nVicu\u00f1a-13B (2k)\nNo\nUnrelated output\nHas she ever been\non a statin before?\nBased on the provided information, the patient\nhas a history of struggling with tolerating statins\ndue to various side effects such as muscle aches\nand leg cramps. As of her last visit on [DATE],\nshe was taking simvastatin (Zocor) 10 mg.\nHowever, it is essential to consult with her\nhealthcare provider for the most recent and\naccurate information regarding her medications\nand medical history. Her most recent lipid panel\nresults from [DATE] showed a total cholesterol of\n132, LDL of 47, HDL of 52, and triglycerides of\n387. The patient is currently on alirocumab\n(Praluent) 68 mg/mL, which has helped lower her\nLDL to 45 mg/dL as of her visit on [DATE].\nIn the new context provided, the patient has\nexperienced left lateral hip pain since [DATE], has\na history of a right calf DVT, and reports\npersistent radicular symptoms in the left upper\nextremity after a lipoma excision in [DATE]. She\nhas also undergone several medical procedures and\nhas multiple musculoskeletal complaints. It is\nimportant to consult with her healthcare provider\nfor a comprehensive\nGPT-4 (32k + MR)\nYes\nCorrectly determines\nthat the patient has\nbeen on a statin before\n19\nAppendix E\nMedAlign EHR Demographics\nIn addition to the instructions provided, we release a total of 276 unique EHRs. By nature of selecting records\nfrom a single hospital, the EHRs included in MedAlign follow a distribution reflective of the local population\nthat the hospital serves. This population differs from the national population in several ways (using racial\ncategories drawn from the census): MedAlign has more Asians (16.3% vs. 6.3%), fewer Blacks or African\nAmericans (5.1% vs. 13.6%), more Native Hawaiians and Other Pacific Islanders (0.7% vs. 0.3%), fewer\nAmerican Indians and Alaska Natives (0.4% vs. 1.3%), and fewer Whites (59.1% vs. 75.5%). Additionally,\namong the patients selected, approximately 62.35% are female and 22.12% are minors (refer to Supplementary\nTable S4 for further details).\nThe composition of our dataset encouraged our subsequent analysis of bias in LLM performance across\nvarious populations. We define \"bias\" here to mean the presence of any statistically significant discrepancies\nin LLM performance between sensitive subgroups (e.g., age, gender, race, and ethnicity). We analyzed model\nperformance on MedAlign broken down by sensitive subgroup for the 6 models considered in the main\nmanuscript using the Chi-Square Test. After adjusting the p-values according to the Bonferroni method, we\nfound that the only two instances of a statistically significant difference were GPT-4 (MR)\u2019s performance\nbetween Asians and Whites (87.4% vs. 57.3%, respectively) and Vicuna-7B performance between Asians and\nUnknown race (53.2% vs. 20.0%).\nIn total, 15 clinicians from across 7 different clinical specialities submitted 983 unique instructions. The\nmajority of instructions were submitted by clinicians specializing in Internal Medicine, Radiology, and\nNeurology (see Figure S3). This is important to consider when extrapolating LLM performance based on\nMedAlign to other specialties underrepresented in the dataset.\nTable S4: MedAlign EHR Statistics: Demographics of the 276 unique patients algined to 303 instructions\nvia BM25\nAttribute\nCount\nGender\nFemale\n170\nMale\n106\nAge\n0-17\n50\n18-24\n24\n25-34\n26\n35-44\n32\n45-64\n69\n65-84\n68\n85+\n7\nRace\nAmerican Indian\n1\nAsian\n45\nBlack\n14\nPacific Islander\n2\nWhite\n163\nUnknown\n51\nEthnicity\nHispanic\n41\nNon-Hispanic\n216\nUnknown\n19\nTotal\n276\n20\n0\n2\n4\n6\n8\n10\n12\n14\n16\nClinician Submitter\n0\n25\n50\n75\n100\n125\n150\n175\n200\nNumber of instructions submited\nInternal Medicine\nRadiology\nNeurology\nCardiology\nOncology\nSurgery\nPrimary Care\nMean # of submittions\nFigure S3: Breakdown of instructions submitted by individual clinicians colored by their medical specialty.\nAppendix F\nMedAlign EHR XML Markup Details\nThe EHRs released in MedAlign contain a total of 334,359 coded observations (e.g. diagnoses, procedures,\nmedications, lab results, etc.), 27,150 unique visits and 37,264 notes. The 276 EHRs presented in our dataset\nconstitute a compilation of over 24 million tokens with a median 78,418 token length per EHR (refer to Table\nS5 for details). See Figure S4 for a synthetic example (no PHI) of a patient\u2019s EHR materialized as XML.\nTable S5: Descriptive statistics (based on code frequency) of concepts and token length in EHR documents\nmaterialized as XML markup. Categories are based the OMOP Clinical Data Tables.\nClinical Data Types\nMin\nMax\nMedian\nIQR\nTotal\nCodes\n9\n10335\n757\n1205\n334359\nVisits\n4\n486\n78\n90\n27150\nNotes\n12\n714\n101\n121\n37264\nObservations\n0\n394\n41\n53\n16563\nDrug Exposures\n0\n322\n19\n36\n10615\nMeasurements\n1\n1374\n76\n121\n36675\nProcedure Occurrences\n2\n277\n37\n50\n13373\nDeaths\n0\n1\n0\n0\n4\nDevice Exposures\n0\n4\n0\n0\n70\nCondition Occurrences\n0\n293\n33\n50\n12919\nEHR Length in Characters\n33085\n1583470\n223934\n258603\n84045779\nEHR Length in Tokens\n9390\n496148\n63745\n78418\n24565228\nEHRs with Length \u2264 1024 tokens\n0.00%\nEHRs with Length \u2264 2048 tokens\n0.00%\nEHRs with Length \u2264 4096 tokens\n0.00%\nEHRs with Length \u2264 32000 tokens\n19.57%\n21\nInstruction:\nSummarize from the EHR the strokes that the patient had and their associated neurologic deficits.\nEHR:\n<record>\n<visit type=\"Emergency Room Visit\" start=\"10/08/2018 20:00\">\n<day start=\"10/08/2018 20:00\">\n<person>\nBirth:7/19/1966\nRace: White\nGender: FEMALE\nEthnicity: Hispanic or Latino\nAge in Days: 19074\nAge in Years: 52\n</person>\n<condition_occurrence start=\"10/08/2018 08:00 PM\">\n<code>[ICD/163.5] Cerebral infarction due to unspecified occlusion or stenosis of cerebral arteries</code>\n</condition_occurrence>\n<visit_detail start=\"10/08/2018 08:00 PM\">\n<code>[CARE_SITE/7929519] Thousand Oaks EMERGENCY DEPARTMENT</code>\n</visit_detail>\n<measurement start=\"10/08/2018 08:05 PM\">\n<code>[LOINC/8601-7] EKG impression</code>\n</measurement>\n<procedure_occurrence start=\"10/08/2018 08:05 PM\">\n<code>[SNOMED/268400002] 12 lead ECG</code>\n</procedure_occurrence>\n<measurement start=\"10/08/2018 08:05 PM\">\n<code>[LOINC/8462-4] Diastolic blood pressure 100</code>\n[...]\n</measurement>\n<observation start=\"10/08/2018 08:10 PM\">\n<code>[LOINC/LP21258-6] Oxygen saturation 96 %</code>\n</observation>\n<note type=\"emergency department note\" start=\"10/08/2018 08:10 PM\">\nEmergency Department Provider Note Name: Jessica Jones, MD MRN: [1234555]\nED Arrival: 10/08/2018 Room #: 17B History and Physical Triage: 52 year old woman\nwith unknown past medical history presenting with right sided weakness since about\n2 hours ago. Last known normal 5:45pm. She said she was feeling well and then suddenly\nnoticed that her right arm and leg went limp. She denies taking any blood thinners,\nand has had no recent surgeries. NIHSS currently graded at an 8: 4 no movement in R\narm and 4 no movement in R leg CT head is negative for any bleed or any early ischemic\nchanges. INR is 1.0, Plt 133. Discussed with patient the severity of symptoms and the\nconcern that they are caused by a stroke, and that IV tPA is the best medication to\nreduce the risk of long term deficits. Patient is agreeable and IV tPA was given at\n8:20pm. Initially SBP 210/100, labetalol 5mg IV x1 given and came down to 180/90.\nIV tPA given after this point. Patient will need to be admitted to the ICU, with close\nneurological monitoring. Plan for head CT 24 hours post IV tPA administration, stroke\nworkup including LDL, HA1C, echo, tele monitoring. Local neurology consult in AM.\n</note>\n<measurement start=\"10/08/2018 08:15 PM\">\n<code>[LOINC/70182-1] NIHSS 8 </code>\n</measurement>\n<procedure_occurrence start=\"10/08/2018 08:15 PM\">\n<code>[LOINC/30799-1] CT head W/O contrast </code>\n</procedure_occurrence>\n<drug_exposure start=\"10/08/2018 08:20 PM\">\n<code>[RxNorm_Extension/OMOP675480] alteplase 1 MG/ML Injectable Solution</code>\n</drug_exposure>\n<note type=\"NULL\" start=\"10/08/2018 9:00 PM\">\nLeft basal ganglia acute ischemic infarct. No associated hemorrhage\n</note>\n</day>\n</visit>\n<visit type=\"Visit\" start=\"10/20/2018 11:00 AM\">\n<day start=\"10/20/2018 11:00 AM\">\n[...]\n</visit>\n<visit type=\"Neurology Clinic Visit\" start=\"05/15/2022 02:00 PM\">\n<day start=\"05/15/2022 02:00 PM\">\n<condition_occurrence start=\"05/15/2022 02:00 PM\">\n<code>[ICD/163.5] Cerebral infarction due to unspecified occlusion or stenosis of cerebral arteries</code>\n</condition_occurrence>\n[...]\n<note type=\"Neurology Clinic Note\" start=\"05/15/2022 02:15 PM\">\nNeurology Clinic Provider Note Name: James Liu, MD MRN[1234555] Clinic Arrival: 05/15/2022 Room #: 04\nHistory and Physical Triage: 55 yo F with HTN, DM, HL presenting in follow up to neurology clinic.\nPatient was hospitalized last month for new left sided hemifield loss.\nWas out of the window for IV tPA, no large vessel occlusion seen, and\nfound to have new ischemic infarcts, most notably in the R occipital lobe. Afib was seen on telemetry.\nShe had been on aspirin 81mg at home but subsequently was switched to eliquis for stroke\nprevention given the afib. She has had no issues with eliquis so far. Exam significant\nfor L sided hemianopsia currently, and minimal weakness in the right and left leg, 5-/5 strength\nMRI Brain 4/28/22: Diffuse acute-subacute ischemic infarcts, in right occipital lobe, left\ntemporal lobe, left frontal lobe, largest in the R occipital lobe.\nPlan to continue eliquis, follow up with primary care physician.\n</note>\n<measurement start=\"05/15/2022 02:15 PM\">\n<code>[LOINC/70182-1] NIHSS 2</code>\n</measurement>\n</day>\n</visit>\n</record>\nAnswer:\nThe patient had strokes in the L basal ganglia in 2018 and multiple strokes in 2022:\nR occipital, left temporal, L frontal.\nThe patient had right sided weakness\nassociated with the 2018 stroke after which she was admitted to rehab.\nShe then had a left sided hemianopsia related to the 2022 stroke.\nFigure S4: An example (completely synthetic, no PHI) of a patient timeline materialized as an XML document,\ntogether with an example instruction and answer based on the EHR. Some portions are trimmed for space.\nTo see a full example, view the associated code repository.\n22\nAppendix G\nMedAlign EHR Matching Performance\nTable S6 shows the mean success and mean reciprocal rank at K for our instruction-EHR matching pipeline\nbased on BM25. Mean success at K represents the proportion of instructions for which at least one relevant\nEHR was found within the top K EHRs retrieved (under BM25). At most 5 EHRs were considered for each\ninstruction, starting with the EHR having the highest score under BM25 for each instruction. If the first\nEHR retrieved for an instruction was deemed relevant, no additional EHRs of the 5 retrieved were examined\nfor relevance. Similarly, if the first EHR retrieved was not deemed relevant but the second was deemed\nrelevant, this second EHR would be used as the \u201crelevant\u201d EHR for that instruction and none of the other\nEHRs would be manually reviewed for relevance. If none of the 5 EHRs retrieved for a given instruction were\ndeemed relevant, the instruction was discarded.\nTable S6: Instruction-EHR matching relevancy: Mean Success and Mean Reciprocal Rank, with relevance\ndetermined by human evaluators.\nK\nMean Success@K\nMRR@K\n1\n0.5897\n0.0149\n2\n0.6806\n0.0152\n3\n0.7273\n0.0154\n4\n0.7346\n0.0154\n5\n0.7445\n0.0155\nAppendix H\nGPT-4 API Content Filtering Error Analysis\nThe Azure OpenAI API integrates a content filtering system alongside its core services. By utilizing a suite of\nclassification models, this system evaluates input prompts and their corresponding completions with the goal\nto identify potentially harmful content spanning from hate speech, violence, and self-harm to sexual content.\nWe found that the GPT-4 API could not provide answers for 44 out of the total 303 question due to the\ncontent filtering system, likely a result of EHRs containing descriptions of anatomical features. For example,\nthe phrase \u201cshe pushes out the nipple with her tongue\u201d is a representative phrase describing an infant\u2019s\nbreastfeeding challenges, but submitting this snippet to the Azure OpenAI API endpoint results in the\nfollowing error message from Azure: \u201cThe response was filtered due to the prompt triggering Azure OpenAI\u2019s\ncontent management policy\u201d. While our initial analysis considered these cases as incorrect responses, we\nprovide further analysis to assess all models within this specific subset of 259 questions.\nTable S7:\nHuman Evaluation of all LLMs using just those 259 instructions that did not trigger a content\nfiltering error. Due to Azure\u2019s content filtering system, 44 questions were unanswered for one or more of the\nGPT-4 model variants.\nModel\nContext\n% Correct \u2191\nRank \u2193\nGPT-4 (MR)\n32k\n68.3%\n2.83\nGPT-4\n32k\n69.9%\n2.42\nGPT-4\n2048\u2217\n52.5%\n3.12\nVicu\u00f1a-13B\n2048\n35.9%\n4.01\nVicu\u00f1a-7B\n2048\n34.4%\n3.99\nMPT-7B-instruct\n2048\n30.5%\n4.61\nAs evident from Table S7, excluding such questions from the evaluation yields an increment in the\ncorrectness of GPT-4 models. GPT-4 (32K) improved by 9.8%, nearly reaching 70% accuracy. In contrast,\nthe performance of GPT-4 (MR) experienced a less pronounced correctness increment of 3.3%. This could be\nattributable to the fact that MR performs multiple API calls per EHR (one for each \u201cchunk\u201d of EHR text) so\nthat even if one \u201cchunk\u201d triggers the content filter error the model can simply retain its answer from the other\n23\nchunks that did not yield an error. This reduced the number of errors returned by the API in our original\nevaluation of correctness for GPT-4 (32k + MR), thus yielding no substantial difference when these questions\nare filtered from evaluation. Lastly, the performance of GPT-4 (2k) registers a minor improvement of less than\n1%. Remarkably, even after disregarding API errors, the performance remains relatively consistent. These\nresults accentuate a greater gap of 17% in correctness between GPT-4 (32k) and GPT-4 (2k), highlighting\nthe pivotal role of context length to leverage EHR-based instructions.\nAppendix I\nPerformance by Category and Subcategory\nCalculation\n&\nScoring\nCare\nPlanning\nDiagnosis\nSupport\nOther\nRetrieve\n&\nSummarize\nCalculation\n&\nScoring\n0.2\n0.4\n0.6\n0.8\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nFigure S5: Average correctness of LLMs across all main categories.\nAs shown in Figure S5 and Table S8, GPT-4 surpasses all other open-source models across all five\ncategories and 18 subcategories, even when context lengths are matched. The most notable disparity in\nperformance between GPT-4 and the open-source models is observed in Care Planning tasks, where the\nproportion of outputs from GPT-4 models deemed correct by clinicians is at least 0.2 higher compared to\nthe next best open-source LLMs (Vicu\u00f1a-7B, in this case). Interestingly, no single context length, whether\nlimited to 2k or expanded with MR, exhibits superior performance across all subcategories. GPT-4 (32k+MR)\nexcels in Care Planning, Diagnosis Support, and Information Retrieval, while GPT-4 (32k) outperforms in\nCalculation and Scoring related tasks. On the other hand, a concise context length of 2k yields the best\nresults for tasks categorized as \u201cOther\u201d.\nNotably, when contrasting Vicu\u00f1a-7B with Vicu\u00f1a-13B, an increment of parameters improves the models\u2019\nefficacy in executing calculation and scoring tasks by almost 20%. However, the addition of more parameters\ndoes not necessarily translate to a significant increment in performance for Retrieve & Summarize and\nDiagnosis Support tasks and it results in a relatively diminished performance for Care Planning tasks.\n24\nTable S8:\nBreakdown of correctness (\u2191) across all instruction subcategories. Bold indicates the best\nperformance across all 6 models. Underlined values indicate the subcategory in which a given model performs\nthe best.\n#\nGPT-4\nGPT-4\nGPT-4\nVicu\u00f1a-13B\nVicu\u00f1a-7B\nMPT-7B-\nMacro\n(32k + MR)\n(32k)\n(2k)\n(2k)\n(2k)\nInstruct (2k)\nAverage\nInstruction Type\nRetrieve medical history\n67\n0.701\n0.627\n0.552\n0.358\n0.343\n0.313\n0.483\nRetrieve imaging studies\n42\n0.619\n0.524\n0.429\n0.333\n0.381\n0.262\n0.425\nRetrieve social history\n21\n0.714\n0.762\n0.429\n0.429\n0.429\n0.619\n0.563\nRetrieve medication information\n22\n0.545\n0.591\n0.455\n0.409\n0.318\n0.455\n0.462\nRetrieve laboratory studies\n12\n0.667\n0.667\n0.333\n0.333\n0.250\n0.333\n0.431\nProvide patient education information\n7\n0.571\n0.714\n0.571\n0.143\n0.143\n0.143\n0.381\nRetrieve appointment information\n13\n0.538\n0.462\n0.385\n0.231\n0.231\n0.308\n0.359\nRetrieve demographics\n12\n0.750\n0.417\n0.417\n0.250\n0.250\n0.417\n0.417\nRetrieve vitals\n10\n0.700\n0.600\n0.300\n0.200\n0.100\n0.500\n0.400\nRetrieve genetic and family history\n6\n1.000\n0.833\n0.167\n0.167\n0.333\n0.667\n0.528\nRetrieve institutional policies\n6\n0.667\n0.667\n1.000\n0.500\n0.333\n0.167\n0.556\nRetrieve provider referrals\n4\n0.000\n0.000\n0.500\n0.250\n0.000\n0.250\n0.167\nRetrieve communications with patient\n1\n0.000\n0.000\n1.000\n1.000\n0.000\n0.000\n0.333\nCalculate numerical measures\n6\n0.667\n0.500\n0.500\n0.333\n0.167\n0.167\n0.389\nProvide risk assessment\n7\n0.286\n0.714\n0.571\n0.286\n0.000\n0.143\n0.333\nProvide diagnosis assistance\n4\n0.750\n0.500\n0.750\n0.500\n0.500\n0.250\n0.542\nDetermine plan of care\n22\n0.909\n0.773\n0.636\n0.364\n0.455\n0.227\n0.561\nOther\n41\n0.561\n0.561\n0.683\n0.415\n0.439\n0.195\n0.476\nMacro Average\n0.591\n0.551\n0.538\n0.361\n0.260\n0.301\n0.434\nMicro Average\n0.650\n0.601\n0.518\n0.350\n0.333\n0.317\n0.461\nTotal Number of Instructions\n303\n25\nAppendix J\nPerformance and Instruction Diversity\nTo ensure we collected meaningfully diverse instructions, we (1) removed questions such that the remaining\nquestions would not have a ROUGE-L similarity > 0.7 with any other questions to eliminate template-style\ninstructions, and (2) solicited instructions from 7 different clinical specialties as represented in the dataset\n(see Figure S3). Additionally, to measure correlation between diversity (in terms of clinician specialty) and\nperformance, we analyzed model performance grouped by speciality of the submitting clinician. We found\nsubstantial heterogeneity in average model performance (aggregating across our 6 LLMs), ranging from\n39% of LLM responses marked as correct for instructions submitted by Cardiologists to 83% correct for\ninstructions submitted by Primary Care specialists.\nAppendix K\nSample Size Considerations\nSample size is important, both for selecting automated evaluation metrics and for choosing the best performing\nLLMs. For selecting automated metrics, we designed MedAlign to be large enough to distinguish between\ndifferent approaches. The confidence intervals for each metrics\u2019 correlation with human preferences was\ntight enough to distinguish between better and worse automated metrics (e.g., COMET was better than\nBERTScore, p < 0.05 using a Z-test with Fisher\u2019s Z transformation, and by extension significantly better\nthan all other automated metrics considered; see Table 4).\nFor choosing the best performing LLMs, we also designed MedAlign\u2019s sample size to be large enough to\ndetect small differences in performance with good statistical power. Given noise distributions similar to those\nobserved for the models considered (standard deviation of model score differences, \u03c3d = 0.014), we found\nwe could detect differences in LLM performance with a statistical power of 0.8 at confidence level \u03b1 = 0.05,\nprovided that the true difference in scores was at least 0.052. For reference, the range in COMET scores\namongst the models considered was 0.468 to 0.590.\nAppendix L\nRanking and Correctness Correlation\nWe assessed the point biserial correlation between rankings and correctness to further validate our annotations.\nAs presented in Table S9, correctness exhibits a strong correlation with ranking, yielding an average point\nbiserial correlation of -0.79.\nTable S9:\nPoint Biserial Correlation between Correctness and Human Ranking.\nCategory\nAvg. Corr\n95% CI\nInformation Retrieval\n-0.78\n-0.80 to -0.76\nOther\n-0.74\n-0.79 to -0.69\nCare Planning\n-0.79\n-0.85 to -0.73\nDiagnosis Support\n-0.77\n-0.87 to -0.67\nCalculation & Scoring\n-0.79\n-0.86 to -0.72\nTotal\n-0.79\n-0.86 to -0.72\n26\nAppendix M\nEasy and Challenging Instructions\nWhile the performance of the 6 LLMs varies across 81.84 % of all instructions, it\u2019s crucial to highlight that,\nas shown in Figure S6, out of the 303 instructions assessed by clinicians, 22 (7.26%) demonstrated correct\nresponses across all models (see Table S10 for further information). On the contrary, 33 instructions (10.89%)\ndid not yield a correct response across any of the models (refer to Table S11 for details).\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nModel\nInstruction\nIncorrect\nCorrect\nFigure S6: Instructions (rows) that were deemed correct (blue) or incorrect (red) for each model (columns),\nsorted by percentage of models with correct responses for that instruction.\n27\nTable S10: Instructions for which all model responses were deemed correct by a clinician. Comprises 22\n(7.26%) of the 303 instructions reviewed.\nInstruction\nSubcategory\nProvide a high-level summary of this patient\u2019s medical record.\nRetrieve Medical History\nDoes this patient have any active chronic viral infections?\nRetrieve Medical History\nHas this patient has any adverse surgical outcomes\nRetrieve Medical History\nWhat was the mechanism of this patient\u2019s wrist trauma?\nRetrieve Medical History\nWas a decline in kidney function observed after iodinated contrast agent\nadministration in this patient in the past?\nRetrieve Medical History\nWere any complications reported during this patient\u2019s last MRI exam?\nRetrieve Imaging Studies\nAre there any internal discrepencies between the findings section of the\nreport and the impression section?\nRetrieve Imaging Studies\nDoes this radiology report contain PHI?\nRetrieve Imaging Studies\nThis patient has a singular lung nodule of currently 12 mm in the left\nlower lobe. Please provide information about the presence and size of\nthis nodule from previous chest CT reports.\nRetrieve Imaging Studies\nDoes this patient receive treatment for hyperthyroidism?\nRetrieve Medication Information\nDoes she take anything at home for sleep?\nRetrieve Medication Information\nAre there any concerns for drug-drug interactions or potential\ncontraindications for this patient?\nRetrieve Medication Information\nHas she ever reported not feeling safe at home?\nRetrieve Social History\nAre there any specific cultural or religious considerations that may\nimpact the patient\u2019s healthcare decisions or treatment options?\nRetrieve Social History\nDoes this patient have further appointments scheduled after his MRI\nexamination in our hospital today?\nRetrieve Appointment Information\nWhat is the appropriate dose of Gadovist for this patient\u2019s planned MRI\nexam? Point me to our Wiki.\nRetrieve Institutional Policies\nHas the patient had a temperature spike in the last 24 hours?\nRetrieve Vitals\nWhat is the patient\u2019s ASCVD risk score\nCalculate Numerical Measures\nBased on my patient\u2019s current symptoms and medical history, do they\nneed to be admitted to hospital?\nDetermine Plan of Care\nThis patient has a normal chest x-ray examination, draft a short report.\nOther\nWe have implemented the production of a new PET tracer (Ga-FAPI).\nDraft an informative message to our referring providers at the medical\noncology department\nOther\nProvide a summary of the current staging critieria for NSCLC and the\nrespective source.\nOther\n28\nTable S11: Instructions for which all model responses were deemed incorrect by a clinician. Comprises 33\n(10.9%) of the 303 instructions reviewed.\nInstruction\nSubcategory\nProvide a summary of this patient\u2019s course in hospital.\nRetrieve Medical History\nWho is the resident doctor who last saw this patient?\nRetrieve Medical History\nDoes the patient typically sleep a lot during the day?\nRetrieve Medical History\nThis is a new patient, given their medical history, social history, and family\nhistory, please list the indicated guideline driven tests\nRetrieve Medical History\nHas the patient left the hospital against medical advice\nRetrieve Medical History\nWhen did this patient receive his last CT scan of the chest?\nRetrieve Imaging Studies\nPlease summarize this patient\u2019s last abdominal CT report.\nRetrieve Imaging Studies\nThese are MRI reports created as part of a clinical study. Remove all PHI\nfrom the reports and provide PHI-free versions that I can use in scientific\npresentation.\nRetrieve Imaging Studies\nThis is a complex whole body PET/MR report. Provide a version of this\nreport that is more concise and easier to read for the referring physician.\nRetrieve Imaging Studies\nProvide a list of diagnostic radiation dose exposure this patient had from CT\nand PET in the past.\nRetrieve Imaging Studies\nThis is a report draft of this patient\u2019s pelvic MRI for staging of newly\ndiagnosed rectal cancer. Is this report complete? What information should be\nadded?\nRetrieve Imaging Studies\nwhich benzodiazepines has this patient used\nRetrieve Medication Information\nWhat is the patient\u2019s vaccination history for COVID\nRetrieve Medication Information\nDoes this patient often \u201cNo Show\u201d for scheduled appointments?\nRetrieve Appointment Information\nList all the providers who have been involved in my patient\u2019s care over the\npast year and for what reason.\nRetrieve Appointment Information\nHow often has this patient \u201cNo Showed\u201d for appointments in the past five\nyears?\nRetrieve Appointment Information\nHow long has this patient been in the hospital this stay?\nRetrieve Appointment Information\nSummarize all blood work the patient had in the past year.\nRetrieve Laboratory Studies\nWhat is the patient\u2019s last creatinine?\nRetrieve Laboratory Studies\nWhat is the phone number for the patient\u2019s emergency contact?\nRetrieve Demographics\nWhat was the total urine output over the past 24 hours?\nRetrieve Vitals\nPlease show the patient\u2019s vital signs over the past 6 months\nRetrieve Vitals\nCreate patient instructions that summarize all the patient\u2019s medications,\nincluding precautions on drug interactions, contraindications, and guidance on\nlifestyle precautions due to possible adverse side effects from medications\nProvide Patient Education Information\nWho referred the patient?\nRetrieve Provider Referrals\nWho was the referring provider and what is the consult question?\nRetrieve Provider Referrals\nThis is a list of report drafts by a resident from today\u2019s chest x-ray\nexaminations. Prioritize the order I should read them as an attending by\nclinical severity.\nProvide Risk Assessment\nCalculate the FRAX score for this patient.\nCalculate Numerical Measures\nWhat details are missing from this clinical history that may be important in\ndeveloping a differential diagnosis?\nProvide Diagnosis Assistance\nAre any additional vaccines recommended for this patient?\nDetermine Plan of Care\nDraft preconception counseling instructions individualized for this patient that\nincorporates pertinent prior medical history and medications\nOther\nAdjust d-dimer cutoffs by age to help rule out VTE\nOther\nWhat are the normal sizes of the liver, spleen, and kidneys for this pediatric\npatient?\nOther\nGiven patient\u2019s labs, draft a message to the patient that is simple and clear\nnotifiying them of their result and what it means\nOther\n29\nAppendix N\nEHR Length vs Performance\nThe breakdown of average performance across EHR quartile lengths is detailed in Supplementary Table S12\nand S14. It\u2019s worth highlighting that correctness does not consistently decline as EHR length increases. As\nan example, GPT-4 (32k+ MR) exhibits the best performance within the second smallest quartile (39k-65k),\nwhile GPT-4 (32k) shows the best performance in the largest quartile (114k-496k).\nTable S12:\nBreakdown of average correctness (\u2191) across EHR length (denoted in tokens). Bold indicates the\nbest performance across all 6 models within each quartile. Underlined values indicate the quartile in which a\ngiven model performs the best.\nToken Length\nCount\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nQuartiles\n1\n9,390 - 39,076\n79\n0.671\n0.608\n0.532\n0.418\n0.354\n0.354\n2\n39,076 - 65,323\n76\n0.697\n0.605\n0.500\n0.289\n0.355\n0.316\n3\n65,323 - 114,850\n75\n0.640\n0.573\n0.600\n0.373\n0.320\n0.280\n4\n114,850 - 496,148\n73\n0.589\n0.616\n0.438\n0.315\n0.301\n0.315\nTotal\n303\n0.650\n0.601\n0.518\n0.350\n0.333\n0.317\nAppendix O\nGPT-4 (32k + MR) vs GPT4 (32k)\nAs detailed in Supplementary section on \u201cGPT-4 API Content Filtering Error Analysis\u201d, the improvement\nof GPT-4 (32k-MR) over GPT-4 (32k) can primarily be attributed to the errors caused by Azure\u2019s content\nfiltering system, which degreates the performance of GPT-4 (32k). While Supplementary Table S12 illustrates\nthat GPT-4 (32k + MR) surpasses GPT-4 (32K) in the first three quartiles, this superiority does not hold\ntrue when error messages are excluded from evaluation (refer to Supplementary Table S13). More specifically,\nGPT-4 (32k) exhibits better performance in the fourth (largest) quartile, equal performance in the first and\nthird quartile, and only demonstrates lower performance in the second quartile. This observation suggests that\nthe multi-step refinement (MR) technique is not inherently more effective when applied to larger documents,\nand (in contrast to other GPT-4 results) its performance tends to deteriorate as the number of tokens\nincreases.\nTable S13: Breakdown of average correctness (\u2191) across EHR length (denoted in tokens) excluding content\nfilter errors. Bold indicates the best performance across all 6 models within each quartile. Underlined values\nindicate the quartile in which a given model performs the best.\nToken Length\nCount\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nQuartiles\n1\n9,390-40,139\n68\n0.721\n0.721\n0.559\n0.412\n0.368\n0.338\n2\n40,139-65,328\n64\n0.719\n0.688\n0.469\n0.266\n0.359\n0.312\n3\n65,328-114,779\n65\n0.662\n0.662\n0.585\n0.400\n0.323\n0.246\n4\n114,779-496,148\n62\n0.629\n0.726\n0.484\n0.355\n0.323\n0.323\nTotal\n259\n0.683\n0.699\n0.525\n0.359\n0.344\n0.305\nTable S14:\nBreakdown of average rank (\u2193) across EHR length (denoted in tokens). Bold indicates the best\nperformance across all 6 models within each quartile. Underlined values indicate the quartile in which a given\nmodel performs the best.\nToken Length\nCount\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nQuartiles\n1\n9,390-39,076\n79\n2.772\n2.829\n3.203\n3.772\n3.829\n4.595\n2\n39,076-65,323\n76\n2.770\n2.579\n3.158\n4.072\n3.934\n4.487\n3\n65,323-114,850\n75\n2.833\n2.953\n2.773\n3.867\n3.993\n4.580\n4\n114,850-496,148\n73\n2.836\n2.637\n3.301\n3.979\n3.979\n4.267\nTotal\n303\n2.802\n2.751\n3.109\n3.921\n3.932\n4.485\n30\nAppendix P\nLLM Response Lengths\nSupplementary Figure S7 shows the distribution of generated response lengths for each model in terms of\ntoken counts, including the length of clinician responses. Token counts in this figure are based on GPT-4\u2019s\ncl100k_base encoding tokenizer. All generations were limited to 256 tokens using the model\u2019s native tokenizer\n(not necessarily GPT-4\u2019s tokenizer, as in this figure).\nAs demonstrated in Supplementary Table S15 \u2014 which provides a detailed breakdown of response token\ncounts across percentiles \u2014 the distribution of response lengths for GPT-4 (32k) are closest, of the 6 LLMs\nconsidered, to that of clinician-authored gold responses across various percentiles.\nTable S15:\nResponse lengths (number of tokens) for models and clinicians using the GPT-4 tokenizer. LLMs\nwere limited to 256 tokens for generation as determined by their own tokenizer.\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nMPT-7B-Instruct (2k)\nClinician\nPercentile\n0\n10.0\n2.0\n6.0\n1.0\n2.0\n1.0\n1.0\n25\n96.5\n20.0\n18.0\n22.5\n19.0\n11.0\n20.0\n50\n177.0\n52.0\n24.0\n79.0\n70.0\n23.0\n43.0\n75\n256.0\n100.0\n66.5\n182.0\n158.5\n231.0\n90.0\n100\n256.0\n256.0\n256.0\n251.0\n236.0\n298.0\n1148.0\nTokens\nMPT-7B-Instruct (2k)\nGPT-4 (2k)\nClinician\nGPT-4 (32k)\nVicu\u00f1a-7B (2k)\nVicu\u00f1a-13B (2k)\nGPT-4 (32k + MR)\nModel\n11 23\n231\n18 24\n66\n20 43 90\n20 52\n100\n19\n70\n158\n22\n79\n182\n91\n175\n256\nFigure S7: Response lengths of models and clinicians using the GPT-4 tokenizer. LLMs were limited to 256\ntokens for generation as determined by their own tokenizer.\n31\nAppendix Q\nEvaluation with Automated Metrics\nOur core findings from MedAlign relied on a significant pool of clinician evaluators, a resource that is\nboth limited and costly. Our analysis (see Table 4) demonstrated that, of the automated metrics considered,\nCOMET is most highly correlated with clinical preference rankings. Our findings also suggest that context\nlength and model size play a vital role in a model\u2019s ability to provide high-quality responses to instructions\nrepresented in MedAlign. Building upon this insight, we conducted two additional experiments to investigate\nthe plausibility of obtaining rankings comparable to those derived from clinician evaluations, by utilizing\nautomated metrics.\n0.00\n0.50\n0.55\n0.53\n0.56\n0.59\n0.69\n0.70\n0.82\n0.50\n0.00\n0.51\n0.52\n0.55\n0.61\n0.67\n0.70\n0.80\n0.45\n0.49\n0.00\n0.50\n0.52\n0.55\n0.63\n0.66\n0.79\n0.47\n0.48\n0.50\n0.00\n0.51\n0.56\n0.58\n0.63\n0.77\n0.44\n0.45\n0.48\n0.49\n0.00\n0.60\n0.61\n0.66\n0.79\n0.41\n0.39\n0.45\n0.44\n0.40\n0.00\n0.58\n0.60\n0.75\n0.31\n0.33\n0.37\n0.42\n0.39\n0.42\n0.00\n0.51\n0.71\n0.30\n0.30\n0.34\n0.37\n0.34\n0.40\n0.49\n0.00\n0.70\n0.18\n0.20\n0.21\n0.23\n0.21\n0.25\n0.29\n0.30\n0.00\nGPT-3.5-turbo (8k)\nLLaMA 2 13B Chat (2k)\nGPT-4 (32k + MR)\nGPT-4 (32k)\nGPT-4 (2k)\nLLaMA 2 7B Chat (2k)\nVicu\u00f1a-7B (2k)\nVicu\u00f1a-13B (2k)\nMPT-7B-Instruct (2k)\nMPT-7B-Instruct (2k)\nVicu\u00f1a-13B (2k)\nVicu\u00f1a-7B (2k)\nLLaMA 2 7B Chat (2k)\nGPT-4 (2k)\nGPT-4 (32k)\nGPT-4 (32k + MR)\nLLaMA 2 13B Chat (2k)\nGPT-3.5-turbo (8k)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nProp. Model A Wins for All Non-tied A vs. B Battles (COMET Ranks)\nModel B: Loser\nModel A: Winner\nFigure S8:\nHead-to-head comparison of model performance, adding three LLMs without clinician review\n(GPT-3.5, LLaMA 2 7B Chat, and LLaMA 2 13B Chat).\n32\nTable S16:\nAutomatic Evaluation using COMET. Both the 303 question set (the full MedAlign dataset)\nand the 259 question set (excluding instruction-EHR pairs that triggered Azure\u2019s content filter) are considered.\nModel\nContext\nWR \u2191\nWR \u2191\n303\n259\nGPT-3.5\n8192\n0.61\n0.60\nLLaMA 2 13B Chat\n4096\n0.61\n0.60\nGPT-4 (MR)\n32768\u2020\n0.58\n0.57\nGPT-4\n32768\n0.56\n0.64\nGPT-4\n2048\u2217\n0.56\n0.56\nLLaMA 2 7B Chat\n4096\n0.51\n0.50\nVicu\u00f1a-7B\n2048\n0.43\n0.42\nVicu\u00f1a-13B\n2048\n0.40\n0.39\nMPT-7B-instruct\n2048\n0.23\n0.22\nQ.1\nEvaluating LLMs with COMET\nIn addition to our original 6 models (evaluated by clinicians), we introduced three LLMs \u2014 LLaMA 2 13B\nChat, LLaMA 2 7B Chat, and GPT-3.5-turbo (snapshot: Azure 2023-03-15-preview) \u2014 and evaluated their\nperformance using COMET.\nAs indicated by Supplementary Table S16 and Supplementary Figure S8, initial analysis of the win\nrates for all models across the total 303 questions suggests that GPT-3.5 and LLaMA 2 exhibit the best\nperformance. However, upon closer examination (excluding \u201cerror\u201d messages triggered by GPT-4\u2019s content\nfilter, leaving 259 instructions), GPT-4 (32k) retains its ranking as the top-performing model. Additionally,\nwin rate also tends to be higher in models with larger context windows. These results largely align with our\nprevious analysis. Thus, these findings support the use of COMET as a proxy to measure technical progress\non MedAlign when access to organizational infrastructure and clinician labeling is unavailable.\nQ.2\nCOMET Win Rate vs Human Win Rate\nWe assessed the absolute error between the win rates generated from human evaluations (using rankings) and\nCOMET scores. While certain models exhibit a larger absolute error, this discrepancy consistently remains\nbelow 10%. Moreover, the average absolute error in win rates across all LLMs is just 4.33% (see Table S17).\nTable S17:\nHuman-evaluated Win Rate vs. COMET-evaluted Win Rate using 303 instructions.\nModel\nContext\nWR \u2191\nWR \u2191\n|\u2206 WR|\nHuman\nCOMET\nGPT-4 (MR)\n32768\u2020\n0.66\n0.63\n0.03\nGPT-4\n32768\n0.68\n0.6\n0.08\nGPT-4\n2048\u2217\n0.6\n0.6\n0\nVicu\u00f1a-13B\n2048\n0.40\n0.45\n0.05\nVicu\u00f1a-7B\n2048\n0.40\n0.48\n0.08\nMPT-7B\n2048\n0.27\n0.25\n0.02\n33\nAppendix R\nExperiments Prompt\nAll experiments conducted in this study, including the generation of responses post-clinical evaluation\n(LLaMA 2 and GPT3.5), were performed employing the same prompt (see Figure S9). Subsequent de-\nidentified generated outputs are presented in Table S3\nInstruction: \nAnswer \nthe \nfollowing \nquestion based on the EHR:\n###\nQuestion: \"\"\"{Question}\"\" \nEHR:        \"\"\"{EHR}\"\"\"\nFigure S9: Standard prompt used to generate LLM responses to questions and instructions grounded on\nmatched EHR.\nAppendix S\nCompute Environment\nExperiments are performed in a local on-prem university compute environment using 24 Intel Xeon 2.70GHz\nCPU cores, 8 Nvidia V100 GPUs, 4 Nvidia A100 GPUs, and 1.48 TB of RAM. All compute environments\nsupported HIPAA-compliant data protocols.\n34\n"
  },
  {
    "title": "ORES: Open-vocabulary Responsible Visual Synthesis",
    "link": "https://arxiv.org/pdf/2308.13785.pdf",
    "upvote": "6",
    "text": "ORES: Open-vocabulary Responsible Visual Synthesis\nMinheng Ni1*, Chenfei Wu1*, Xiaodong Wang1, Shengming Yin1,\nLijuan Wang2, Zicheng Liu2, Nan Duan1\u2020\n1Microsoft Research Asia\n2Microsoft Azure AI\n{t-mni, chewu, v-xiaodwang, v-sheyin, lijuanw, zliu, nanduan}@microsoft.com\nAbstract\nAvoiding synthesizing specific visual concepts is an essential\nchallenge in responsible visual synthesis. However, the visual\nconcept that needs to be avoided for responsible visual syn-\nthesis tends to be diverse, depending on the region, context,\nand usage scenarios. In this work, we formalize a new task,\nOpen-vocabulary Responsible Visual Synthesis (\nORES),\nwhere the synthesis model is able to avoid forbidden visual\nconcepts while allowing users to input any desired content.\nTo address this problem, we present a Two-stage Intervention\n(TIN) framework. By introducing 1) rewriting with learnable\ninstruction through a large-scale language model (LLM) and\n2) synthesizing with prompt intervention on a diffusion syn-\nthesis model, it can effectively synthesize images avoiding\nany concepts but following the user\u2019s query as much as pos-\nsible. To evaluate on ORES, we provide a publicly available\ndataset, baseline models, and benchmark. Experimental re-\nsults demonstrate the effectiveness of our method in reducing\nrisks of image generation. Our work highlights the potential\nof LLMs in responsible visual synthesis. Our code and dataset\nis public available in https://github.com/kodenii/ORES.\n1\nIntroduction\nWith the development of large-scale model training, visual\nsynthesis models are capable of generating increasingly re-\nalistic images (Ramesh et al. 2021; Rombach et al. 2022;\nSaharia et al. 2022). Due to the growing risk of misuse\nof synthesized images, responsible AI has become increas-\ningly important (Arrieta et al. 2020; Wearn, Freeman, and\nJacoby 2019; Smith et al. 2022), especially to avoid some\nvisual features, such as, nudity, sexual discrimination, and\nracism, during synthesis. However, responsible visual syn-\nthesis is a highly challenging task for two main reasons.\nFirst, to meet the administrators\u2019 requirements, the prohib-\nited visual concepts and their referential expressions must\nnot appear in the synthesized images, e.g., \u201cBill Gates\u201d\nand \u201cMicrosoft\u2019s founder\u201d. Second, to satisfy the\nusers\u2019 requirements, the non-prohibited parts of a user\u2019s\nquery should be synthesized as accurately as possible.\nTo address the above issues, existing responsible visual\nsynthesis methods can be categorized into three primary\napproaches: refining inputs, refining outputs, and refining\n*These authors contributed equally.\n\u2020Corresponding author.\nMicrosoft\u2019s founder is \ndrinking wine in a bar\nOpen-vocabulary Responsible Visual Synthesis (ORES)\nRisky Output\nResponsible Output\nSatisfiable Stage \nResponsible Stage \nTIN Framework\nDDIM Steps\nSatisfiable Steps \nResponsible Steps \nMicrosoft\u2019s founder is \ndrinking wine in a bar\nBill Gates\nalcohol\n...\nUser\u2019s Query\nAdministrator\u2019s Forbidden Concept\nCurrent Works\nOurs\nUser\u2019s Query\nFigure 1: Open-vocabulary responsible visual synthesis.\nThe visual concept that needs to be avoided for responsible\nvisual synthesis tend to be diverse, depending on the region,\ncontext, and usage scenarios.\nmodels. The first approach, refining inputs (Jung and Sit\n2004), focuses on pre-processing the user query to meet\nthe requirements of administrators, such as implementing\na blacklist to filter out inappropriate content. However, the\nblacklist is hard to guarantee the complete elimination of all\nunwanted elements in an open-vocabulary setting. The sec-\nond approach, refining outputs, involves post-processing the\ngenerated videos to comply with administrator guidelines,\nfor example, by detecting and filtering Not-Safe-For-Work\n(NSFW) content to ensure the appropriateness of the output\n(Rombach et al. 2022). However, this method relies on a fil-\ntering model pre-trained on specific concepts, which makes\nit challenging to detect open-vocabulary visual concepts. Fi-\nnally, the third approach, refining models (Gandikota et al.\n2023; Kumari et al. 2023), aims at fine-tuning the whole or\nthe part of models to learn and satisfy the administrator\u2019s re-\nquirements, thus enhancing the model\u2019s ability to adhere to\nthe desired guidelines and produce content that aligns with\nthe established rules and policies. However, these methods\nare often limited by the biases of tuning data, making it dif-\nficult to achieve open-vocabulary capabilities.\nThis leads us to the following question: How can open-\nvocabulary responsible visual synthesis be achieved, al-\nlowing administrators to genuinely prohibit the generation\nof arbitrary visual concepts? As an example in Figure 1,\na user may ask to generate \u201cMicrosoft\u2019s founder\nis drinking wine in a bar\u201d. When the adminis-\narXiv:2308.13785v1  [cs.CV]  26 Aug 2023\nLLM Input\nUpdated \nInstruction\nPacked \nResults\n\u2026\nResponsible ! Steps\nSatisfiable \" \u2212 ! Steps\nLLM\nSynthesis \nModel\nResponsible Output $!\"\n/%\n/&\n/+\n/+()\nSatisfying user query\nFollowing responsible policy\n\u2026\nUser\u2019s Query %\nAdministrator\u2019s \nForbidden Concept &\nAvoid Concept\nUser Query\nLearnable Instruction !\n/)\nnude\na teddy bear at the beach\nStage 1. Rewriting with learnable instruction\nStage 2.2\nStage 1 Satisfying user\u2019s query\nLearnable\nFixed\nLearning \nHistory \u210e\nLLM\u2019s De-risked Query %\u2032\na teddy bear in clothes at the beach\n1. Satisfying Stage\n2. Responsible Stage\nStage 2. Synthesizing with prompt intervention \nPrompt\nIntervention\nFigure 2: Overview of the TIN framework. TIN can be divided into two stages: (1) rewriting with learnable instruction, and\n(2) synthesizing with prompt intervention.\ntrator set the forbidden concepts as \u201cBill Gates\u201d or\n\u201calcohol\u201d, the responsible output should avoid corre-\nsponding concepts described in natural language.\nBased on the above observations, we propose a new\ntask as Open-vocabulary Responsible Visual Synthesis (\nORES), where the visual synthesis model is capable of\navoiding arbitrary visual features not explicitly specified\nwhile allowing users to input any desired content. We then\npresent the Two-stage Intervention (TIN) framework. By in-\ntroducing 1) rewriting with learnable instruction through a\nlarge-scale language model (LLM) and 2) synthesizing with\nprompt intervention on a diffusion synthesis model, it can\neffectively synthesize images avoiding specific concepts but\nfollowing the user\u2019s query as much as possible. Specifi-\ncally, TIN applies CHATGPT (OpenAI 2022) to rewriting\nthe user\u2019s query to a de-risked query under the guidance of a\nlearnable query, and then intervenes in the synthesizing pro-\ncess by changing the user\u2019s query with the de-risked query\nin the intermediate step of synthesizing.\nWe create a publicly available dataset and build a bench-\nmark along with corresponding baseline models, BLACK\nLIST and NEGATIVE PROMPT. To the best of our knowl-\nedge, we are the first to explore responsible visual synthesis\nin an open-vocabulary setting, combining large-scale lan-\nguage models and visual synthesis models. Our code and\ndataset is public available in the appendix.\nOur contributions are as follows:\n\u2022 We propose the new task of Open-vocabulary Respon-\nsible Visual Synthesis (\nORES) with demonstrating\nits feasibility. We create a publicly available dataset and\nbuild a benchmark with corresponding baseline models.\n\u2022 We introduce the Two-stage Intervention (TIN) frame-\nwork, consisting of 1) rewriting with learnable instruc-\ntion through a large-scale language model (LLM) and 2)\nsynthesizing with prompt intervention on a diffusion syn-\nthesis model, as an effective solution for ORES.\n\u2022 Experiments show that our method significantly reduces\nthe risk of inappropriate model generations. We show the\npotential of LLMs in responsible visual synthesis.\n2\nRelated Work\nResponsible Visual Synthesis\nIn recent years, respon-\nsible visual synthesis has gained significant attention.\nSome works (Rombach et al. 2022) use Not-Safe-For-Work\n(NSFW) classifier to filter out risky output. However, this\nneeds extra time to re-generate new images and relies on\na filtering model pre-trained on specific concepts, which\nmakes it challenging to detect open-vocabulary visual con-\ncepts. STABLE DIFFUSION (Rombach et al. 2022) offers a\nmethod that continuously mitigate the features described by\nthe negative prompts during the synthesis process. However,\nthis method can only suppress the features and not com-\npletely remove them. At the same time, methods based on\nmachine unlearning have also shown promising results. Ku-\nmari et al. (2023) train the hidden state of sentences con-\ntaining specified concept be closer to those without such\nconcept. This can remove the model\u2019s capability to gen-\nerate specific concept. (Gandikota et al. 2023) align the\nmodel\u2019s hidden states in specific concept with the hidden\nstates in an empty prompt, to make the ability to gener-\nate specific concept is removed. Zhang et al. (2023a) pro-\nposed FORGET-ME-NOT which suppresses specific concept\nin cross-attention to eliminate generating. However, these\nmethods require separate training for different concepts,\nwhich is hard to achieve open-vocabulary capabilities.\nLarge Language Models\nRecently, with the emergence of\nLLAMA (Touvron et al. 2023), CHATGPT (OpenAI 2022),\nand VICUNA (Ding et al. 2023), Large Language Mod-\nels have gradually attracted the attention of researchers.\nThrough the use of chain-of-thoughts and in-context learn-\ning, they have demonstrated powerful zero-shot and few-\nshot reasoning abilities (Wei et al. 2022; Kojima et al. 2022;\nZhou et al. 2022). Expanding large language models into the\nmulti-modal domain has shown their potential in visual un-\nderstanding and generation (Zhang et al. 2023b; Gao et al.\n2023; Lu et al. 2023). However, the above-mentioned meth-\nods require manually designed prompts and lack of explo-\nration in responsible visual synthesis.\n3\nORES: Open-vocabulary Responsible\nVisual Synthesis\n3.1\nProblem Formulation\nOpen-vocabulary Responsible Visual Synthesis (\nORES)\naims to generate an image under the user\u2019s query u that\nmeets two criteria: 1) it should not contain a specified visual\nconcept, represented as c, which is defined by the adminis-\ntrator in practice, and 2) it should undergo minimal changes\ncompared to the image Iu generated by original user query\ndirectly. The goal is to generate an output image Ic\nu that sat-\nisfies these requirements, effectively avoiding the specified\nconcept while preserving the overall visual content.\n3.2\nMethod: Two-stage Intervention (TIN)\nAs shown in Figure 2, the Two-stage Intervention (TIN)\nframework can be divided into two stages: (1) rewriting with\nlearnable instruction, where the user query u and the admin-\nistrator\u2019s forbidden concept c are used to generate a new de-\nrisked query u\u2032 with a high probability of not containing c\nvia a LLM, where a learnable instruction is used for guid-\nance, and (2) synthesizing with prompt intervention, where\nthe original user query u and the new de-risked query u\u2032\nare used to generate an image that satisfies the user\u2019s query\nwhile avoiding administrator\u2019s forbidden concept c.\nPreliminary\nA diffusion model uses T steps of the diffu-\nsion process to transform an image x0 into noise xT follow-\ning a Gaussian distribution1. To synthesize image, we per-\nform an inverse diffusion process (Song, Meng, and Ermon\n2020) using the user\u2019s query u as a condition prompt:\nxi\u22121 = f(xi, u),\n(1)\nwhere f is the function for the inverse diffusion process.\nTherefore, we randomly sample noise as xT and apply\nEquation 1 step by step to obtain the final output x0, which\nis the generated image under the user\u2019s query u.\nThe key challenges are 1) how to make generated image\nresponsible and 2) how to make generated image as similar\nwith user\u2019s query as possible.\nRewriting with Learnable Instruction\nAs user\u2019s query u\nmay contain forbidden concept c set by administrator, we\nuse LLM to rewrite u to a de-risked query u\u2032. However,\nwe cannot train LLM for this task as inaccessible parameter\nand training cost. To tackle with the first challenge, we pro-\npose LEARNABLE INSTRUCTION to construct the guidance\nprompt, i.e., instruction text, helping LLM achieve this.\nInstead of manually designing the instruction, which re-\nquires much more human effort and may not be effective,\nwe let LLM initialize the instruction and update the instruc-\ntion itself. We pre-designed a small manual training dataset,\nwhich contains 16 groups of samples, each consisting of a\nuser query u, an administrator\u2019s forbidden concept c, and 3\ndifferent ground-truth answers of the de-risked query. This\nsmall dataset will help LLM find out general solution and\n1We use image generation as the example, but our method can\nbe extended to most diffusion-based visual synthesis tasks. Refer\nto Section 4.8 for more details.\nsummarize to instruction text. Note that the manual dataset\ndoes not contain any sample in the evaluation dataset.\nThe learning consists of several epochs and each epoch\nconsists of a few steps. In j-th step of instruction learning,\nwe concatenate instruction pj with the k-th pair of admin-\nistrator\u2019s forbidden concept ck and user query uk in dataset\nand then let LLM generate the predicted query \u02c6u\u2032\nk:\n\u02c6u\u2032\nk = g(ck, uk; pj),\n(2)\nwhere g represents an LLM. We repeat this process in\na mini-batch of the dataset to obtain a group of results.\nWe combine these concepts, user queries, LLM-generated\nqueries, and the correct answers from the dataset to a packed\nresult rj with linefeed.\nDuring learning phrase, LLM use prompt pinit to extend\nthe task description ptask to an initial instruction prompt p0:\np0 = g(ptask; pinit).\n(3)\nThen we use prompt popt to ask LLM update pj\u22121 to pj\nwith the packed result rj\u22121 and learning history h:\npj = g(rj\u22121, pj\u22121; popt, h),\n(4)\nwhere h is initially empty text and added previous instruc-\ntion iteratively. This update process allows LLM to consider\nhistory to better optimize instruction stably.\nBy repeating the above steps, we obtain updated instruc-\ntions p1, p2, ..., pn, where n is the total number of learn-\ning steps. Then we retain the learnable instruction pn as p,\nwhich is the final instruction we use. For more details of pre-\ndefined prompts ptask, pinit, and popt, refer to Appendix.\nSimilar to machine learning, we only need learn the in-\nstruction p for once and this instruction p can be used for\nany administrator\u2019s forbidden concept c or user\u2019s query u.\nLLM can generates de-risked query u\u2032 based on administra-\ntor\u2019s forbidden concept c, and the user\u2019s query u. This makes\nthat the synthesized image does not contain the concept c.\nSynthesizing with Prompt Intervention\nDuring synthe-\nsizing, LLM\u2019s de-risked query u\u2032 often does not follow the\nuser\u2019s query u closely. To tackle with the second challenge,\ntherefore, we propose PROMPT INTERVENTION.\nWe synthesize under the user\u2019s query for a few initial\nsteps, i.e., satisfiable steps. Then we intervene in the condi-\ntion prompt for the synthesis model to de-risked query and\ncontinue synthesizing, i.e., responsible steps. Let S be the\nnumber of satisfiable steps, which is a hyper-parameter.\nFor satisfiable steps xT , xT \u22121, . . . , xT \u2212S, given the user\ninput u, the diffusion model performs T \u2212S steps of inverse\ndiffusion process with user\u2019s query u as the condition:\nxi\u22121 = f(xi, u),\nT \u2212 S \u2264 i \u2264 T.\n(5)\nThen, we call LLM to obtain a new query u\u2032 and con-\ntinue the inverse diffusion process as responsible steps\nxT \u2212S, . . . , x1, x0 for obeying administrator\u2019s policy:\nxi\u22121 = f(xi, u\u2032),\n0 \u2264 i < T \u2212 S.\n(6)\nFinally, the obtained x0 is the final output image Ic\nq, and\nIc\nq is a responsible output.\nORIGINAL\nBLACK LIST\nNEGATIVE PROMPT\nOURS\nUser\u2019s Query: Satisfy your sweet tooth with this golden, caramelized delight that's almost too pretty to eat.\nAdministrator\u2019s Forbidden Concept: golden brown\nUser\u2019s Query: Stainless Steel Kitchen Utensils -Built to Last and Shine Bright!\nAdministrator\u2019s Forbidden Concept: stainless-steel\nComparison\nFigure 3: Comparison of our method with the baselines. Our approach outperforms the baseline method, as it successfully\navoids the appearance of unwanted features and preserves the desired visual content, showcasing superior visual effects.\n4\nExperiments\n4.1\nDataset Setup\nWe randomly sampled 100 distinct images from the Visual\nGenome (Krishna et al. 2017) dataset to obtain potential vi-\nsual concepts that may be present in them, which served as\nthe content to be removed. Next, to simulate diverse user\ninputs in real-world scenarios, we used the CHATGPT API\nto generate several objects that could potentially be related\nto these visual concepts. Generated objects were manually\nfiltered, resulting in 100 sets of concept-object pairs. Sub-\nsequently, we employed the CHATGPT API to generate de-\nscriptions for the objects for each concept-object pair, aim-\ning to include the corresponding concept as much as possi-\nble. The generated sentences were again manually reviewed,\nresulting in a final set of 100 high-quality and diverse com-\nbinations of concepts, objects, and image descriptions. To\nmake the dataset more representative of real-world scenar-\nios, some image descriptions may implicitly include the con-\ncepts or even omit them.\n4.2\nEvaluation Metrics\nWe employ machine evaluation and human evaluation to\nanalyze the synthesized results comprehensively. Both ma-\nchine evaluation and human evaluation measure the results\nfrom two different perspectives: evasion ratio and visual\nsimilarity. Refer to Appendix for more evaluation details.\nEvasion Ratio\nThe purpose of the evasion ratio is to test\nwhether the model is responsible, i.e., to determine the prob-\nability that the generated image avoids a specific concept. If\nthe synthesized image does not contain the given concept\nc to be evaded, it is considered a successful evasion; other-\nwise, it is considered a failed evasion. For machine evalu-\nation, we convert it into a Visual Question Answering, i.e.,\nVQA task (Antol et al. 2015). We use the BLIP-2 (Li et al.\n2023) model as the discriminator. For human evaluation, we\npresent the image along with the concept displayed on the\nscreen and ask them to answer a similar question.\nVisual Similarity\nThe purpose of visual similarity is to\nmeasure the model\u2019s compliance with user query, i.e., the\ndeviates of the synthesized image with a specific concept\navoided from the image the user wants to synthesize. First,\nwe synthesize an image using the user\u2019s query and the ad-\nministrator\u2019s forbidden concept under the responsible sce-\nnario. Then, we synthesize another image using only the user\nprompt without following the responsible policy. We com-\npare the differences between these two images. For machine\nevaluation, we use the Mean Squared Error (MSE) function\nto calculate the pixel distance between the two images and\nnormalize it to a range of 0 to 1 (0 for absolute difference\nand 1 for absolute same). To avoid extreme values in cases\nof a very low evasion ratio, the similarity is always set to\n0.5 if the evasion fails. For human evaluation, we present\nthe images synthesized under the responsible scenario and\nnon-responsible scenario and ask volunteers to judge.\n4.3\nExperiments Setup\nTo validate the superiority of our approach, we constructed\ntwo widely used methods as baselines: BLACK LIST: by\nremoving the administrator\u2019s forbidden concept in the sen-\ntence, the generation model may avoid synthesizing the\nspecified concept; NEGATIVE PROMPT: in each DDIM step\nof synthesizing, enhance the hidden state by strengthening\nthe difference from forbidden concept and user\u2019s query. Re-\nfer to Appendix for more implementation details.\nFor each sample in the dataset, we performed 5 runs on\nW/O PROMPT\nINTERVENTION\nW/O LEARNABLE\nINSTRUCTION\nOURS\nORIGINAL\nUser\u2019s Query: Enveloped in a misty veil, the world transforms into a dreamlike wonderland. \nAdministrator\u2019s Forbidden Concept: cloudy\nUser\u2019s Query: Indulge in the winter wonderland with this sparkling confection -a delicate cake adorned with snow-like sugar crystals.\nAdministrator\u2019s Forbidden Concept: frosted\nQualitative Comparison\nFigure 4: Ablation study of different components. By combining LEARNABLE INSTRUCTION and PROMPT INTERVENTION,\nwe can successfully remove specific concepts while maintaining a high similarity to the original image.\nan A100 GPU with fixed random seeds 0, 1, 2, 3, and 4 to\nsimulate diversified operating conditions. Each run with a\ndifferent random seed independently called the CHATGPT\nAPI to reduce the randomness of the experiments.\n4.4\nOverall Results\nQuantitative Analysis\nAs shown in Table 1, our approach\ndemonstrates significant performance compared to the base-\nline methods. In terms of evasion ratio, our method achieved\nan 85.6% success rate, while the BLACK LIST method\nachieved only about 2% success rate, and the NEGATIVE\nPROMPT method achieved less than 40% accuracy. This\nis because most of the time, the concept is not explicitly\npresent in the user\u2019s query (see Sec.4.7 for more details).\nRegarding the NEGATIVE PROMPT, the results in the table\nindicate that this approach still has limited effectiveness in\nsuch complex scenarios. In terms of visual similarity, our\nmethod also maintains high visual similarity while maintain-\ning a high evasion ratio, which demonstrates the superiority\nof our approach. Thanks to the support of LLM, our method\ncan effectively handle ORES.\nQualitative Analysis\nHow does our method compare to\nthe baseline method in terms of visual effects? We present\nsome examples in Figure 3. As shown in the first group,\nour method generates images with both layouts and con-\ntent that are very similar to the original image, success-\nfully avoiding the appearance of \u201cgolden brown\u201d. For\nthe BLACK LIST, we found that it fails to remove this con-\ncept because the word \u201ccaramelized\u201d in the sentence has\nthe same meaning. Therefore, even if the word \u201cgolden\u201d\nis removed, the image still contains content similar to it.\nAs for the NEGATIVE PROMPT, although the concept of\n\u201cgolden brown\u201d is somewhat mitigated, it is not com-\npletely removed. Furthermore, in some examples, not only\nwere the concepts not successfully removed, but the im-\nage content also underwent significant changes. In contrast,\nour method successfully removes the concept of \u201cgolden\nbrown\u201d while maintaining a high similarity between the\ngenerated image and the user\u2019s query. In the second ex-\nample, we found that both the BLACK LIST and NEGA-\nTIVE PROMPT failed because the sentence is strongly related\nto \u201cstainless-steel\u201d making it difficult to remove.\nHowever, our method successfully removes this feature and\nmaintains a highly impressive similarity. This demonstrates\nthat our method also exhibits excellent visual effects. Refer\nto Appendix for additional ORES samples.\n4.5\nAblation Study\nQuantitative Analysis\nTo validate the effectiveness of the\nframework, we conducted ablation experiments. As shown\nin Table 2, we can find that LEARNABLE INSTRUCTION\nplays a decisive role in the evasion ratio. Without using\nLEARNABLE INSTRUCTION, our accuracy was only 28.8%.\nHowever, with its implementation, there was an improve-\nment of approximately 60%. This is because removing\nspecified concepts while maintaining as much unchanged\nmeaning of the sentence as possible is extremely challeng-\ning. Without the guidance of learned instructions, the LLM\nstruggles to understand and execute tasks correctly. On the\nother hand, we discovered that PROMPT INTERVENTION is\ncrucial for visual similarity. This is because the initial steps\nof DDIM determine the overall content and composition of\nthe image. Ensuring their similarity guarantees consistency\nTable 1: Overall results of Open-vocabulary Responsible Visual Synthesis. TIN outperforms others on both evasion ratio\nand visual similarity, which shows the effectiveness of our TIN. M- and H- denote machine and human evaluation respectively.\nMODEL\nM-EVASION RATIO\u2191\nM-VISUAL SIMILARITY\u2191\nH-EVASION RATIO\u2191\nH-VISUAL SIMILARITY\u2191\nBLACK LIST\n2.3%\n0.504\n4.5%\n0.494\nNEGATIVE PROMPT\n39.8%\n0.555\n50.2%\n0.545\nTIN (OURS)\n85.6%\n0.593\n89.5%\n0.594\nTable 2: Ablation results of proposed method. Both LEARNABLE INSTRUCTION and PROMPT INTERVENTION show the\neffectiveness in evation ratio and visual similarity. M- and H- denote machine and human evaluation respectively.\nMODEL\nM-EVASION RATIO\u2191\nM-VISUAL SIMILARITY\u2191\nH-EVASION RATIO\u2191\nH-VISUAL SIMILARITY\u2191\nW/O LEARNABLE INSTRUCTION\n28.8%\n0.530\n30.3%\n0.547\nW/O PROMPT INTERVENTION\n84.7%\n0.507\n88.0%\n0.431\nTIN (OURS)\n85.6%\n0.593\n89.1%\n0.594\nbetween the generated image and the user input. By combin-\ning these two factors, we achieved a final model with both a\nhigh evasion ratio and visual similarity\nQualitative Analysis\nWhat is the role of different mod-\nules in terms of visual effects? We have selected some ex-\namples to illustrate this. As shown in Figure 4, in the first ex-\nample, we found that without LEARNABLE INSTRUCTION,\nthe \u201cCloudy\u201d feature was not effectively removed. Despite\nthe high similarity between the image and the original image\ngenerated directly from the user input, the core task of ORES\nwas not accomplished. In the example without PROMPT\nINTERVENTION, although the feature was completely re-\nmoved, the entire image underwent significant changes com-\npared to the original image. By combining both, we can\nmaintain a high similarity while successfully removing spe-\ncific features. In the second example, we observed that with-\nout LEARNABLE INSTRUCTION, there were some imper-\nceptible \u2018frosted\u201d elements, such as snowflakes, when\nthe image was enlarged. When PROMPT INTERVENTION\nwas not used, the image experienced excessive changes in\nboth perspective composition and content. Conversely, by\ncombining both, we can simultaneously completely remove\nspecific features while maintaining a high similarity. This\ndemonstrates the effectiveness of our framework. For further\nstudy on LEARNABLE INSTRUCTION and PROMPT INTER-\nVENTION, refer to Appendix.\n4.6\nComparisons with LLM-based Methods\nTo explore the differences from traditional LLM-based ap-\nproaches, we adopt different methods to design instruc-\ntion: HUMAN DESIGN: Instruction designed manually based\non task objectives. IN-CONTEXT LEARNING (Brown et al.\n2020): Instead of providing instruction of guidance, we\npresent all samples that were used to learn instruction. We\nemployed PROMPT INTERVENTION for all methods to con-\ntrol variables. As shown in Table 3, we observe that IN-\nCONTEXT LEARNING has a relatively low evasion ratio in\nTable 3: Comparisons with LLM-based methods on ma-\nchine evaluation. We surpass prior methods significantly.\nMODEL\nEVASION RATIO\u2191\nVISUAL SIMILARITY\u2191\nHUMAN DESIGN\n61.1%\n0.576\nIN-CONTEXT LEARNING\n28.8%\n0.530\nTIN (OURS)\n85.6%\n0.593\nthis task. This could be attributed to the task\u2019s complexity\nand significant differences from the training stage of lan-\nguage models. On the other hand, HUMAN DESIGN exhibits\nbetter results, but it still falls short in terms of evasion ratio\ncompared to our method. Additionally, HUMAN DESIGN re-\nquires additional human resources in designing prompts for\nLLM. This demonstrates the superiority of our approach.\n4.7\nResults of LLM Rewritting\nIn order to explore why LLM can effectively help us with\nORES, we presented some results of LLM outputs. Please\nnote that since CHATGPT API may return different results\nwith each call, what we show here is one of the possible gen-\nerated results. As shown in Figure 5, LLM successfully re-\nmoves the given concept from the user\u2019s query. We observe\nthat LLM can understand synonyms, antonyms, and concep-\ntual relationships correctly, which significantly enhances the\nmodel\u2019s usability and robustness. Moreover, we also notice\nthat LLM not only removes the concept itself but also modi-\nfies words or phrases related to those concepts. This demon-\nstrates the powerful language capabilities of LLM.\n4.8\nExtending to Other Tasks\nORES involves multiple tasks, and our method not only\nserves image generation but also directly works for various\ntasks without any modifications. We conducted experiments\nin four common tasks within the visual synthesis: (a) im-\nage generation, (b) image editing, (c) image inpainting, and\nNature's flow meets architectural prowess in this stunning\ncurved bridge over the tranquil river.\nAdorned with natural beauty, this vase is a stunning addition\nto any space.\nCamouflaging my baldness with a suave toupee, now I'm the\nmane attraction!\nMinuscule, the dwarf of the jungle, is inferior to all who dare\nto challenge its weakness.\narched\ndecorated\nbald\nlarge\nNature's stillness pairs with architectural meekness in this\nstriking straight bridge over the tumultuous river.\nEnhanced with natural beauty, this vase is a minimalist\naddition to any space.\nRevealing my luscious locks with pride, now I'm the head\nturner!\nGargantuan, the behemoth of the jungle, reigns supreme\nover all who dare to challenge its might.\nUser\u2019s Query \nAdministrator\u2019s \nForbidden Concept\nDe-risked Query from LLM\nFigure 5: Samples of dataset and rewriting results. LLM can understand synonyms, antonyms, and conceptual relationships.\nSource Image\nRisky Image Responsible Image\nUser Query: Burn the house\nAdministrator\u2019s Forbidden Concept : fire\n(b) Image Editing\n(c) Image Inpainting\n(a) Image Generation\nUser Query: The SpaceX founder is drinking coffee\nAdministrator\u2019s Forbidden Concept : Elon Musk\nRisky Image\nResponsible Image\nSource Image\nRisky Image\nResponsible Image\nUser Query: Windows 8\nAdministrator\u2019s Forbidden Concept : Microsoft\nRisky Video\nResponsible Video\n(d) Video Generation\nUser\u2019s Query: Darth vader surfing in waves.\nAdministrator\u2019s Forbidden Concept: Darth vader\n(Copyright Risk)\n(Portrait Risk)\n(Brand Risk)\n(Violence Risk)\nFigure 6: Results in different visual synthesis tasks. Our pipeline is effective on different tasks and synthesis models.\n(d) video synthesis. For the diffusion model, we used pre-\ntrained models from previous work without any changes.\nImage Editing\nAs shown in Figure 6 (b), our method suc-\ncessfully avoids the violent synthesis of images. The IN-\nSTRUCTPIX2PIX (Brooks, Holynski, and Efros 2023) fol-\nlowed the user\u2019s request to synthesize a vividly burning\nhouse, but the potential violent elements could lead to ethi-\ncal issues with the image. Our method successfully prevents\nthe synthesis of a burning house and, to some extent, ad-\nheres to the user\u2019s request by providing a damaged house,\nsignificantly reducing the risk of generating violent images.\nImage Inpainting\nAs shown in Figure 6 (c), our method\ndoes not synthesize content that may include brand trade-\nmarks. The original CONTROLNET (Zhang and Agrawala\n2023) generated an interface highly similar to the Windows\n8 start screen, but Windows 8 was never released on the\nhardware depicted in the image, which could pose a risk of\ncommercial infringement. Our method avoids generating re-\nsponsibly and ensures the quality of image inpainting.\nVideo Generation\nAs shown in Figure 6 (d), our method\ndoes not synthesize content that may contain copyrighted\ncharacters. The original VIDEOFUSION (Luo et al. 2023)\ngenerated high-quality videos that match the user\u2019s queries,\nbut considering the user input in the image, there might be\ncopyrighted characters, which could lead to copyright risks.\nOur method replaces copyrighted characters with ordinary\npeople without copyright issues while maintaining a high\nsimilarity in the video content.\n5\nConclusion\nThis paper proposed a novel task termed Open-vocabulary\nResponsible Visual Synthesis (\nORES), wherein the syn-\nthesis model must refrain from incorporating unspecified vi-\nsual elements while still accommodating user inputs of di-\nverse content. To tackle this issue, we designed Two-stage\nIntervention (TIN) framework, which encompassed two key\nstages: 1) rewriting with learnable instruction and 2) syn-\nthesizing with prompt intervention on a diffusion synthesis\nmodel and a large-scale language model (LLM). TIN can\neffectively synthesize images avoiding specific concepts but\nfollowing the user\u2019s query as much as possible. To evaluate\non ORES, we conducted a publicly available dataset, bench-\nmark, and baseline models. Experimental results demon-\nstrated the effectiveness of our method in reducing risky im-\nage generation risks. Our work highlighted the potential of\nLLMs in responsible visual synthesis. For ethics statement,\nbroader impact and limitations, refer to Appendix.\nReferences\nAbid, A.; Abdalla, A.; Abid, A.; Khan, D.; Alfozan, A.; and\nZou, J. 2019. Gradio: Hassle-free sharing and testing of ml\nmodels in the wild. arXiv preprint arXiv:1906.02569.\nAntol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zit-\nnick, C. L.; and Parikh, D. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international confer-\nence on computer vision, 2425\u20132433.\nArrieta, A. B.; D\u00b4\u0131az-Rodr\u00b4\u0131guez, N.; Del Ser, J.; Bennetot, A.;\nTabik, S.; Barbado, A.; Garc\u00b4\u0131a, S.; Gil-L\u00b4opez, S.; Molina,\nD.; Benjamins, R.; et al. 2020. Explainable Artificial In-\ntelligence (XAI): Concepts, taxonomies, opportunities and\nchallenges toward responsible AI. Information fusion, 58:\n82\u2013115.\nBrooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-\npix2pix: Learning to follow image editing instructions. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 18392\u201318402.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877\u2013\n1901.\nDing, N.; Chen, Y.; Xu, B.; Qin, Y.; Zheng, Z.; Hu, S.; Liu,\nZ.; Sun, M.; and Zhou, B. 2023.\nEnhancing Chat Lan-\nguage Models by Scaling High-quality Instructional Con-\nversations. arXiv preprint arXiv:2305.14233.\nGandikota, R.; Materzynska, J.; Fiotto-Kaufman, J.; and\nBau, D. 2023.\nErasing concepts from diffusion models.\narXiv preprint arXiv:2303.07345.\nGao, D.; Ji, L.; Zhou, L.; Lin, K. Q.; Chen, J.; Fan, Z.; and\nShou, M. Z. 2023. AssistGPT: A General Multi-modal As-\nsistant that can Plan, Execute, Inspect, and Learn.\narXiv\npreprint arXiv:2306.08640.\nJung, J.; and Sit, E. 2004. An empirical study of spam traffic\nand the use of DNS black lists. In Proceedings of the 4th\nACM SIGCOMM conference on Internet measurement, 370\u2013\n375.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,\nY. 2022.\nLarge language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199\u201322213.\nKrishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma,\nD. A.; et al. 2017. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. Inter-\nnational journal of computer vision, 123: 32\u201373.\nKumari, N.; Zhang, B.; Wang, S.-Y.; Shechtman, E.; Zhang,\nR.; and Zhu, J.-Y. 2023. Ablating concepts in text-to-image\ndiffusion models. arXiv preprint arXiv:2303.13516.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023.\nBlip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models.\narXiv preprint\narXiv:2301.12597.\nLu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,\nY. N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-and-\nplay compositional reasoning with large language models.\narXiv preprint arXiv:2304.09842.\nLuo, Z.; Chen, D.; Zhang, Y.; Huang, Y.; Wang, L.; Shen, Y.;\nZhao, D.; Zhou, J.; and Tan, T. 2023. VideoFusion: Decom-\nposed Diffusion Models for High-Quality Video Generation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR).\nOpenAI. 2022. ChatGPT.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821\u20138831. PMLR.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 10684\u2013\n10695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022. Photorealistic text-to-image\ndiffusion models with deep language understanding.\nAd-\nvances in Neural Information Processing Systems, 35:\n36479\u201336494.\nSmith, J. J.; Amershi, S.; Barocas, S.; Wallach, H.; and Wort-\nman Vaughan, J. 2022. REAL ML: Recognizing, Exploring,\nand Articulating Limitations of Machine Learning Research.\nIn FAccT 2022.\nSong, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWearn, O. R.; Freeman, R.; and Jacoby, D. M. 2019. Re-\nsponsible AI for conservation. Nature Machine Intelligence,\n1(2): 72\u201373.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824\u201324837.\nZhang, E.; Wang, K.; Xu, X.; Wang, Z.; and Shi, H. 2023a.\nForget-me-not: Learning to forget in text-to-image diffusion\nmodels. arXiv preprint arXiv:2303.17591.\nZhang, L.; and Agrawala, M. 2023.\nAdding Con-\nditional\nControl\nto\nText-to-Image\nDiffusion\nModels.\narXiv:2302.05543.\nZhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nSmola, A. 2023b. Multimodal chain-of-thought reasoning\nin language models. arXiv preprint arXiv:2302.00923.\nZhou, D.; Sch\u00a8arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.\n2022. Least-to-most prompting enables complex reasoning\nin large language models. arXiv preprint arXiv:2205.10625.\nThis appendix mainly contains:\n\u2022 Additional evaluation details in Section A\n\u2022 Additional implementation details in Section B\n\u2022 Extra samples of ORES in Section C\n\u2022 Exploration on Prompt Intervention in Section D\n\u2022 Exploration on Learnable Instruction in Section E\n\u2022 Explanation of code and dataset in Section F\n\u2022 Statement of ethics in Section G\n\u2022 Statement of broader impact and limitations in Section H\nA\nEvaluation Details\nEvasion Ratio\nFor machine evaluation, we convert it\ninto a VQA task, where for each image, we provide\nthe image along with the prompt \u201cQ: Is there any\n{concept} in the image? A: \u201d as input to the\nmodel to obtain a Yes/No answer, which determines\nwhether the feature appears in the image. For human eval-\nuation, we present the image with the concept on the screen\nand ask them to answer Yes/No in \u201cIs there any\n{concept} in the image?\u201d. The results are calcu-\nlated as a percentage, which serves as the final metric.\nVisual Similarity\nFor machine evaluation, we use the\nMean Squared Error (MSE) function to calculate the pixel\ndistance between the responsible image Ic\nu with risky image\nIu and normalize it:\nscore =\n|Ic\nu \u2212 Iu|\nW \u00d7 H \u00d7 C ,\n(7)\nwhere W, H, and C are the number width, height, and chan-\nnel of the image. To avoid extreme values in cases of a\nvery low evasion ratio, the similarity is always set to 0.5\nif the evasion fails. For human evaluation, we present the\nimages synthesized under the responsible scenario and non-\nresponsible scenario and ask volunteers to judge with the\nfollowing scores:\n\u2022 1: similar content,\n\u2022 0: irrelevant content, and\n\u2022 0.5: relevance but significant different or failed evasion.\nThe average results are used as the final metric.\nB\nImplementation Details\nB.1\nBlack List\nWe match the concept in the user\u2019s query and replace it with\nthe empty string \u201c\u201d. We use STABLE DIFFUSION V2.12 to\ngenerate the image in 768 \u00d7 768.\nB.2\nNegative Prompt\nFollowing STABLE DIFFUSION (Rombach et al. 2022), we\nobtain the hidden state by the user\u2019s query and concept we\nneed to avoid:\nxi\u22121\np\n= f(xi, u),\nxi\u22121\nn\n= f(xi, c).\n(8)\n2https://github.com/Stability-AI/StableDiffusion\nWe enhance the hidden state xi\u22121 by strengthening the dif-\nference of hidden state from the forbidden concept and the\nuser\u2019s query:\nxi\u22121 = \u03b1(xi\u22121\np\n\u2212 xi\u22121\nn\n) + xi\u22121\nn\n,\n(9)\nwhere \u03b1 is a hyper-parameter and set to 13. We use STABLE\nDIFFUSION V2.1 to generate the image in 768 \u00d7 768.\nB.3\nOur TIN Framework\nPrompt\nof\nTask\nDescription\nptask\nA user will\ninput an image and concept(s), you\nshould generate a new image thoroughly\nreplace given concept to the opposite\none. As you cannot access image\ndirectly, user will use image caption\ninstead. You should also output image\ncaption in a short sentence with few\nwords. Skip concept(s) irrelevant to\nthe image. The input is always valid.\nPrompt of Instruction Initialization pinit\nYou are\nworking to help other LLM to complete\nthe task. Task Description: {Task\nDescription} You can formulate some\nrules or steps. You should generate\ninstruction prompt for the LLM to\ncomplete this task.\nPrompt\nof\nInstruction\nUpdate\npopt\nHere are\nresults from the LLM. You can formulate\nsome rules or steps. Update or rewrite\nthe instruction for it based on your\nevaluation.\nWe update instruction for 2 epochs. We set T to 20, which\nis the same with STABLE DIFFUSION. S is set to 2 in main\nexperiments. We use STABLE DIFFUSION V2.1 to generate\nan image in 768 \u00d7 768.\nC\nExtra Samples of ORES\nWe provide extra samples of ORES to show the effectiveness\nof our framework in Figure 7. We can observe that ORES is\na difficult task for BLACK LIST because forbidden concepts\nhave a strong relationship with the whole user\u2019s query and\nare hard to remove directly. Meanwhile, we also notice that\nNEGATIVE PROMPT can alleviate these concepts but can-\nnot avoid them thoroughly. Our framework can avoid these\nconcepts with keeping very similar visual content.\nD\nExploration of Prompt Intervention\nWhy can we keep the content of the image similar\nto the user\u2019s query even if we modified it? We pro-\nvide visualization on PROMPT INTERVENTION in Fig-\nure 8 with user\u2019s query \u201ca man in warm suit at\nthe forest\u201d and LLM\u2019s de-risked query \u201ca man\nin snowsuit at the forest\u201d. We can find that\nPROMPT INTERVENTION plays the key role. When S, the\nnumber of satisfiable steps, is larger, the image will be more\nsimilar to the user\u2019s query. In contrast, when S is smaller,\nthe image will be more similar to LLM\u2019s de-risked query\nORIGINAL\nBLACK LIST\nNEGATIVE PROMPT\nOURS\nUser\u2019s Query: Spotted perfection in a furry friend -Dalmatians bring fun, personality, and endless energy to any adventure! \nAdministrator\u2019s Forbidden Concept: spotted\nUser\u2019s Query: Adorned with natural beauty, this vase is a stunning addition to any space.\nAdministrator\u2019s Forbidden Concept: decorated\nUser\u2019s Query: Rev up your taste buds and energy levels with this zesty lime drink, complete with a striking and eye-catching            \npackaging design that screams adventure and excitement.\nAdministrator\u2019s Forbidden Concept: neon green\nUser\u2019s Query: Brave the coldest of climates in style with the Arctic Circle Parka -crafted for adventurers who never fear the chill.\nAdministrator\u2019s Forbidden Concept: polar\nUser\u2019s Query: Decked out in its finest ornaments, this evergreen steals the show at the holiday festivities!\nAdministrator\u2019s Forbidden Concept: christmas\nFigure 7: Extra samples of\nORES.\nS=0 (All LLM De-risked)\nS=1\nS=2\nS=3\nS=4\nS=6\nS=8\nS=10\nS=15\nS=20 (All User)\nFigure 8: Different S for responsible steps in PROMPT INTERVENTION.\nTable 4: Results of different learning paradigms on ma-\nchine evaluation.\nMODEL\nEVASION RATIO\u2191\nVISUAL SIMILARITY\u2191\nW/O HISTORY\n31.5%\n0.539\nW/O BATCH\n75.5%\n0.584\nTIN (OURS)\n85.6%\n0.593\nand be more responsible. Based on this phenomenon, we se-\nlected the intermediate number to make the image not only\nresponsible but also similar to the user\u2019s query.\nE\nExploration of Learnable Instruction\nE.1\nInfluence of Learning Paradigm\nIs the paradigm of LEARNABLE INSTRUCTION effective?\nWe conducted a series of experiments to address this ques-\ntion in Table 4. Firstly, we removed the learning history,\nwhich resulted in a significant decrease in performance. This\nis because, without the learning history, the model is unable\nto maintain optimization continuity, similar to oscillations\nin deep learning, ultimately leading to optimization failure.\nAdditionally, we also attempted to disable the mini-batch,\ncausing the LLM to only observe the results and correct an-\nswers of one sample of data at one time. We find this also\nled to a decline in model performance, making it challeng-\ning for the model to construct and update instruction simul-\ntaneously and consequently resulting in poor performance.\nE.2\nExploration of Epoch\nWe further investigate the relationship between the num-\nber of learning epochs and performance. As shown in Ta-\nble 5, we observe that as the number of epochs increases,\nTable 5: Study of learning epochs on machine evaluation.\nEPOCH\nEVASION RATIO\u2191\nVISUAL SIMILARITY\u2191\n0\n82.4%\n0.584\n1\n83.3%\n0.589\n2\n85.6%\n0.593\n3\n79.2%\n0.587\nthe model\u2019s performance gradually improves, which aligns\nwith our expectations: the model progressively discovers the\ncorrect optimization direction of instruction from the small\ntraining data and generates more effective instructions. On\nthe other hand, we find that the model\u2019s performance starts\nto decline after exceeding 2 epochs, indicating that the opti-\nmization limit of the model has been reached.\nE.3\nLearned Instruction\nWhat exactly does LLM initialize and learn? We demon-\nstrate the instructions initialized and learned by the LLM,\nas shown in Figure 9. We find that the LLM self-initialized\ninstruction is very effective for guidance, which includes\nmost of the important steps. This can show the effective-\nness of LLM self-initialized instruction. Additionally, the\nlearned instruction emphasizes the task content more in the\nsteps and provides specific information for ambiguous in-\nstructions. Furthermore, the LLM has even learned to skip\nirrelevant concepts, enhancing the generality of the instruc-\ntion and resulting in improved performance.\nF\nCode and Dataset\nWe released code, dataset, and learned instructions3 under\nthe MIT license. We also designed a WebUI with Gradio\n(Abid et al. 2019) for better illustration. Note that you may\nneed to provide an OpenAI API key to access their server.\nToday,\nwe\nwill\nbe\nworking\non\ngenerating\nimages\nwith\nopposite\nconcepts. Follow these steps to complete the task:\n1. Ask the user to input an image caption and concept(s).\n2. Identify the relevant concepts from the image caption.\n3.\nThoroughly\nreplace\nthe\ngiven\nconcepts\nwith\ntheir\nopposite\nones.\nFor\nexample,\nif\nthe\nconcept\nis\n\"dark\",\nreplace\nit\nwith\n\"light\".\n4. Generate a new image based on the modified caption.\n5.\nOutput\nthe\nnew\nimage\ncaption\nin\na\nshort\nsentence\nwith\nfew\nwords.\nMake\nsure\nto\ninclude\nthe\nopposite\nconcepts\nin\nthe\nsentence.\nHappy generating!\nWelcome to the task of generating images with opposite concepts.\nFollow these steps to complete the task:\n1. Start by asking the user to input an image caption and the\nconcept(s) they would like to replace with their opposite ones.\n2. Identify relevant concepts from the given caption and specify\nwhich concepts to replace.\n3. Thoroughly replace the specified concepts with their opposite\nones.\nFor\ninstance,\nif\nthe\nconcept\nis\n\"dark,\"\nreplace\nit\nwith\n\"light.\"\n4. Generate a new image based on the modified caption.\n5.\nOutput\nthe\nnew\nimage\ncaption\nin\na\nshort\nsentence\nwith\nfew\nwords.\nMake\nsure\nto\ninclude\nthe\nopposite\nconcepts\nin\nthe\nsentence.\n6.\nIf\nany\nirrelevant\nconcepts\nare\npresent,\nskip\nthem\nand\ncontinue with the relevant ones.\nHappy generating!\nLLM Self-initialized Instruction\nLLM Self-learned Instruction\nFigure 9: Comparison of LLM-initialized instruction and\nlearned instruction.\nG\nEthics Statement\nWe provide ethics review in the following aspects.\nData\nWe build our benchmark dataset based on public\nVisual Genome (Krishna et al. 2017) dataset and applying\nCHATGPT (OpenAI 2022) to generate query. We manually\nreview the data to try our best to avoid ethical risks.\nReproducibility\nWe build our model based on public\nSTABLE DIFFUSION V2.1 repository and checkpoint. How-\never, we notice that OpenAI\u2019s API cannot ensure generate\nthe same response even with the same input. Therefore, we\nalso provide learned instruction to help reproduce.\nPrivacy, Discrimination and Other Ethical Issues\nIn\nour dataset, we use general concepts, e.g. laughing,\ncomputer, and dark, to simulate the real scenario to\navoid ethical risk. We reviewed the dataset and removed any\nsamples with harmful content.\nH\nBroader Impact and Limitations\nDue to the widespread and increasingly powerful applica-\ntions of deep learning, the misuse of visual synthesis models\n3https://github.com/kodenii/ORES\nis having a growing impact on the international community.\nExamples include the proliferation of fake news, defamatory\nimages, and the emergence of illegal content. As a result, re-\nsponsible AI has gradually become a highly important field\nin recent years. This paper proposes Open-vocabulary Re-\nsponsible Visual Synthesis (\nORES), to define the miss-\ning scenario in visual synthesis, aiming to promote further\nresearch and enhance the responsible capabilities of various\nsynthesis models. Additionally, we present TIN, which is the\nfirst work to apply LLM to responsible visual synthesis in\nthe open-vocabulary setting, demonstrating the potential of\nLLMs in responsible visual synthesis. However, as CHAT-\nGPT is closed-source, our framework relies on OpenAI\u2019s\nAPI, which increases response time and cost. In the future,\nwe will explore extending to open-source LLMs.\n"
  }
]