[
  {
    "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",
    "link": "https://arxiv.org/pdf/2308.01320.pdf",
    "upvote": "42",
    "text": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF\nTraining of ChatGPT-like Models at All Scales\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari,\nXiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li,\nConnor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko,\nHeyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He\nDeepspeed of Microsoft\nAbstract\nChatGPT-like models have revolutionized various applications in artificial intelligence,\nfrom summarization and coding to translation, matching or even surpassing human per-\nformance. However, the current landscape lacks an accessible, efficient, and cost-effective\nend-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for\nthese powerful models, particularly when training at the scale of billions of parameters.\nThis paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF train-\ning, making it accessible to the AI community.\nDeepSpeed-Chat offers three key ca-\npabilities: an easy-to-use training and inference experience for ChatGPT-like models, a\nDeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and\na robust DeepSpeed-RLHF system that combines various optimizations for training and\ninference in a unified way. The system delivers unparalleled efficiency and scalability, en-\nabling training of models with hundreds of billions of parameters in record time and at a\nfraction of the cost. With this development, DeepSpeed-Chat paves the way for broader\naccess to advanced RLHF training, even for data scientists with limited resources, thereby\nfostering innovation and further development in the field of AI.\n1\nOverview\nChatGPT [1] like models have taken the AI world by storm, and it would not be an overstate-\nment to say that it\u2019s impact on the digital world has been revolutionary. These models are\nincredibly versatile, capable of performing tasks like summarization, coding, and translation\nwith results that are on-par or even exceeding the capabilities of human experts. Given the\nsheer power of these models, multiple efforts are underway in the AI open-source community\nto make ChatGPT-style models more accessible (e.g. ChatLLaMa [2], Alpaca [3], Vicuna [4],\nDatabricks-Dolly [5], etc.).\nDespite these incredible efforts, there is still a lack of an end-to-end RLHF pipeline capable\nof training powerful ChatGPT like model that is easily accessible to the AI community. For\ninstance, training a modest 6.7B ChatGPT model with existing systems typically requires\nexpensive multi-GPU setup that is beyond the reach of many data scientists. Even with access\nto such computing resources, training efficiency is often less than 5% of what these machines\nare capable of (as illustrated in Figure 6). And finally, existing solutions simply cannot support\n1\narXiv:2308.01320v1  [cs.LG]  2 Aug 2023\nTable 1: Single-Node 8x A100: Training Time and Corresponding Approximate Cost on Azure.\nGPUs\nOPT-6.7B\nOPT-13B\nOPT-30B\nOPT-66B\n8x A100-40GB\n5.7 hours\n10.8 hours\n1.85 days\nNA\n8x A100-80GB\n4.1 hours ($132)\n9 hours ($290)\n18 hours ($580)\n2.1 days ($1620)\neasy, fast and affordable training state-of-the-art ChatGPT models with hundreds of billions of\nparameters, even given access to multi-GPU clusters.\nThese limitations stem from a lack of a robust system design that is capable of effectively\nsupporting the complex InstructGPT\u2019s RLHF training pipeline that is quite different from\nthe standard pre-training and fine-tuning pipelines that existing DL systems are designed for.\nTherefore, in the spirit of democratizing ChatGPT-like models, and making RLHF training\ntruly accessible to the AI community, today we are releasing DeepSpeed-Chat with the following\nthree capabilities:\n\u2022 Easy-to-use Training and Inference Experience for ChatGPT Like Models: A\nsingle script capable of taking a pre-trained Huggingface [6] model, running it through\nall three steps of InstructGPT [7] training using DeepSpeed-RLHF system and producing\nyour very own ChatGPT like model. In addition, we provide an inference API for testing\nconversation-style interactions after the model is trained.\n\u2022 DeepSpeed-RLHF Pipeline: DeepSpeed-RLHF pipeline primarily replicates the train-\ning pipeline from the InstructGPT [7] paper with careful attention to ensure completeness\nand one-to-one correspondence with the three-steps that includes a) Supervised Fine-\ntuning (SFT), b) Reward Model Fine-tuning and c) Reinforcement Learning with Human\nFeedback (RLHF) [8]. Additionally, we offer data abstraction and blending capabilities\nto enable training with multiple data sources.\n\u2022 DeepSpeed-RLHF System: A robust and sophisticated RLHF system that combines the\ntraining and inference prowess of DeepSpeed into single unified Hybrid Engine (DeepSpeed-\nHE) for RLHF. The Hybrid-Engine is capable of seamlessly transitioning between infer-\nence and training modes within RLHF, allowing it to leverage various optimizations from\nDeepSpeed-Inference such as tensor-parallelism and high-performance transformer kernels\nfor generation, while also benefiting from the multitude of ZeRO- and LoRA [9]-based\nmemory optimization strategies for RL training. DeepSpeed-HE is also aware of the full\nRLHF pipeline, allowing it to make optimal decisions in terms of memory management\nand data movement across different phases of RLHF.\nDeepSpeed-RLHF system is capable of unparalleled efficiency at scale, making complex RLHF\ntraining fast, affordable, and easily accessible to the AI community:\nEfficiency and Affordability: In terms of efficiency, DeepSpeed-HE is over 15x faster than\nexisting systems, making RLHF training both fast and affordable. For instance, DeepSpeed-HE\ncan train an OPT-13B [10] in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under\n$300 and $600, respectively, as shown in Table 1.\nExcellent Scalability: DeepSpeed-HE supports models with hundreds of billions of pa-\nrameters and can achieve excellent scalability on multi-node multi-GPU systems. As a result,\neven a 13B model can be trained in 1.25 hours and a massive 175B model can be trained with\nDeepSpeed-HE in under a day as shown in Table 2.1\n1Very Important Details:\nThe numbers in both tables (1, 2) above are for Step 3 of the training\n2\nTable 2:\nMulti-Node 64x A100-80GB: Training Time and Corresponding Approximate Cost\non Azure.\nGPUs\nOPT-13B\nOPT-30B\nOPT-66B\nOPT-175B\n64x A100-80G\n1.25 hours ($320)\n4 hours ($1024)\n7.5 hours ($1920)\n20 hours ($5120)\nTable 3:\nMax Model Size Supported by DeepSpeed-HE on a Single GPU.\nV100 32G\nA6000 48G\nA100 40G\nA100 80G\nModel Size\nOPT-2.7B\nOPT-6.7B\nOPT-6.7B\nOPT-13B\nDemocratizing RLHF Training: With just a single GPU, DeepSpeed-HE supports train-\ning models with over 13 billion parameters as shown in Table 3, enabling data scientists without\naccess to multi-GPU systems to create not just toy RLHF models but large and powerful ones\nthat can be used in real-world scenarios.\nNext, we dive deeper into the three capabilities of DeepSpeed-Chat introduced above.\n2\nEasy-to-use ChatGPT Training and Inference Experi-\nence\nWe start with the easy-to-use experience by showing how you can train OPT-13B and then\nOPT-66B models with DeepSpeed-RLHF system. If you are short on time, you can even train\nan OPT-1.3B model on a single consumer-grade GPU in just two hours. We also demonstrate\nhow you can use our DeepSpeed-chat RLHF API to develop your own custom pipelines.\n2.1\nTraining your first ChatGPT-Style model is so easy with DeepSpeed-\nChat\u2019s RLHF examples\nOne single script completes all three stages of RLHF training and generates your\nfirst ChatGPT model!\nWe use an example of pretrained OPT-13B as the actor model\nand OPT-350M as the reward model in the following single script to generate a final 13B\nChatGPT-style model:\npip\ni n s t a l l\ndeepspeed >=0.9.0\ng i t\nclone\nhttps :// github . com/ microsoft /DeepSpeedExamples . g i t\ncd DeepSpeedExamples/ a p p l i c a t i o n s /DeepSpeed\u2212Chat/\npip\ni n s t a l l \u2212r\nrequirements . txt\nand based on actual measured training throughput on DeepSpeed-RLHF curated dataset and training recipe\nwhich trains for one epoch on a total of 135M tokens. We have in total 67.5M query tokens (131.9k queries\nwith sequence length 256) and 67.5M generated tokens (131.9k answers with sequence length 256), and a\nmaximum global batch size per step of 0.5M tokens (1024 query-answer pairs).\nWe urge readers to pay\nattention to these specifications before making any cost and e2e time comparisons with DeepSpeed-RLHF.\nSee our benchmark settings (https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/\nDeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.md) page for more details.\n3\nTable 4:\nE2E time breakdown for training a 13 billion parameter ChatGPT model via\nDeepSpeed-Chat on a single DGX node with 8 NVIDIA A100-40G GPUs.\nModel Sizes\nStep 1\nStep 2\nStep 3\nTotal\nActor: OPT-13B, Reward: OPT-350M\n2.5hr\n0.25hr\n10.8hr\n13.6hr\nTable 5:\nE2E time breakdown for training a 66 billion parameter ChatGPT model via\nDeepSpeed-Chat on 8 DGX nodes with 8 NVIDIA A100-80G GPUs/node.\nModel Sizes\nStep 1\nStep 2\nStep 3\nTotal\nActor: OPT-66B, Reward: OPT-350M\n82 mins\n5 mins\n7.5hr\n9hr\npython\ntrain . py \u2212\u2212actor\u2212model facebook /opt\u221213b \u2212\u2212reward\u2212model \\\nfacebook /opt \u2212350m \u2212\u2212deployment\u2212type\nsingle node\nIn about half a day, your 13B model would be fully trained with its checkpoints ready.\nTable 4 demonstrates a breakdown of the training time for each of the three steps.\nTest your final model with DeepSpeed-Chat\u2019s Inference API\nNow it is time to plug-\nin and test your final model trained by DeepSpeed-Chat! You can play with not only Q&A\nsessions but also conversation-style interactions.\nHuman:\nDo you know Microsoft ?\nAssistant :\nYes ,\nI \u2019m a big\nfan\nof\nMicrosoft . What about them?\nHuman:\nCan you explain\ni t\nto a 6\u2212year\nold\nchild ? I wonder how\nI\nshould\ndescribe\ni t\nAssistant :\nSure ,\nI can try .\nMicrosoft\ni s\na company that makes\ncomputers ,\nand they make a program\nc a l l e d\n\u2018 \u2018Windows \u2019 \u2019\nwhich\ni s\nthe\noperating\nsystem\nthat\nruns on the computer .\nIt \u2019 s\nl i k e\nthe\n\u2018 \u2018 brain \u2019 \u2019\nof\nthe computer .\nIt \u2019 s where\na l l\nthe programs and\nf i l e s\nare\nstored .\nIt \u2019 s\nalso\nwhere\nyou \u2019 l l\nfind\nthe\n\u2018 \u2018 start \u2019 \u2019\nbutton ,\nwhich\ni s\nwhere\nyou \u2019 l l\ns t a r t\nto\nuse\nthe computer .\n2.2\nWant to try different model sizes and configurations? You got it!\nWe understand users often like to play with different model sizes and configurations to meet\ntheir training time, resources, and quality requirements.\nWith DeepSpeed-Chat, users can\neasily do that. For example, if you want to train a larger and higher-quality model on your\nGPU cluster for your research or business, you can simply use the same script with your desired\nmodel size e.g., 66B and GPU counts e.g., 64 GPUs:\npython\ntrain . py \u2212\u2212actor\u2212model facebook /opt\u221266b \\\n\u2212\u2212reward\u2212model facebook /opt \u2212350m \u2212\u2212deployment\u2212type\nmulti node\nWithin 9 hours, you can have your 66 billion parameters ChatGPT model ready to be served\nin your favorite front-end GUI as shown in Table 5.\n4\nTable 6:\nE2E time breakdown for training a 1.3 billion parameter ChatGPT model via\nDeepSpeed-Chat on a single commodity NVIDIA A6000 GPU with 48GB memory.\nModel Sizes\nStep 1\nStep 2\nStep 3\nTotal\nActor: OPT-1.3B, Reward: OPT-350M\n2900 secs\n670 secs\n1.2hr\n2.2hr\nIf you only have around 1-2 hours for coffee or lunch break, you can also try to train a\nsmall/toy model with DeepSpeed-Chat. For example, we prepared a training example for a\n1.3B model with a single dataset to test our framework on your consumer-grade GPUs. The\nbest part is that you will have your model checkpoint ready to play with when you are back\nfrom your lunch break (see Table 6)!\npython\ntrain . py \u2212\u2212actor\u2212model facebook /opt \u22121.3b \\\n\u2212\u2212reward\u2212model facebook /opt \u2212350m \u2212\u2212deployment\u2212type\nsingle gpu\n2.3\nCustomizing your own RLHF training pipeline using DeepSpeed-\nChat\u2019s RLHF APIs\nDeepSpeed-Chat allows users to build their very own RLHF training pipeline using our flexible\nAPIs shown below, which users can use to reconstruct their own RLHF training strategy. This\nenables a general interface and backend for creating a wide range of RLHF algorithms for\nresearch exploration.\nengine = DeepSpeedRLHFEngine(\nactor model name or path=args . actor model name or path ,\ncritic model name or path=args . critic model name or path ,\ntokenizer=tokenizer ,\nn um total iters=num total iters ,\nargs=args )\nt r a i n e r = DeepSpeedPPOTrainer ( engine=engine ,\nargs=args )\nf o r\nprompt batch\nin\nprompt train dataloader :\nout = t r a i n e r . generate experience ( prompt batch )\na c t o r l o s s ,\nc r i t i c l o s s = t r a i n e r . t r a i n r l h f ( out )\n3\nFull-fledged RLHF Training Pipeline\nTo provide a seamless training experience, we follow InstructGPT and include a full-fledged\nend-to-end training pipeline in DeepSpeed-Chat as shown in Figure 1.\nOur pipeline includes three main steps:\n\u2022 Step 1: Supervised finetuning (SFT), where human responses to various queries are\ncarefully selected to finetune the pretrained language models.\n\u2022 Step 2: Reward model finetuning, where a separate (usually smaller than the SFT)\nmodel (RW) is trained with a dataset that has human-provided rankings of multiple\nanswers to the same query.\n5\nFigure 1: The illustration of DeepSpeed-Chat\u2019s RLHF training pipeline with optional features.\n\u2022 Step 3: RLHF training, where the SFT model is further finetuned with the reward\nfeedback from the RW model using the Proximal Policy Optimization (PPO) [11] algo-\nrithm.\nWe provide two additional features in Step 3 to help improve model quality:\n\u2022 Exponential Moving Average (EMA) collection, where an EMA based checkpoint\ncan be chosen for the final evaluation.\n\u2022 Mixture Training, which mixes the pretraining objective (i.e., the next word prediction)\nwith the PPO objective to prevent regression performance on public benchmarks like\nSQuAD2.0 [12].\nThe two training features, EMA and Mixed Training, are often omitted by other recent\nefforts since they can be optional.\nHowever, according to InstructGPT, EMA checkpoints\ngenerally provide better response quality than conventional final trained model and Mixture\nTraining can help the model retain the pre-training benchmark solving ability. As such, we\nprovide them for users to fully get the training experience as described in InstructGPT and\nstrike for higher model quality.\nIn addition to being highly consistent with InstructGPT paper [7], we also provide conve-\nnient features to support researchers and practitioners to train their own RLHF model with\nmultiple data resources:\n\u2022 Data Abstraction and Blending Capabilities: DeepSpeed-Chat is able to train the\nmodel with multiple datasets for better model quality. It is equipped with (1) an abstract\ndataset layer to unify the format of different datasets; and (2) data splitting/blending\ncapabilities so that the multiple datasets are properly blended then split across the 3\ntraining stages.\nTo illustrate the effectiveness of our training pipeline, we demonstrate the model quality\nwith multi-round conversation as shown in the experience section.\n6\n4\nDeepSpeed Hybrid Engine \u2013 Unified Infrastructure to\nPower and Optimize RLHF Training\nStep 1 and Step 2 of the instruct-guided RLHF pipeline resemble regular fine-tuning of large\nmodels, and they are powered by ZeRO-based optimizations and flexible combination of par-\nallelism strategies in DeepSpeed training to achieve scale and speed. Step 3 of the pipeline,\non the other hand, is the most complex part to handle in terms of performance implications.\nEach iteration requires efficient processing of two phases a) inference phase for token/experience\ngeneration, producing inputs for the training and b) training phase to update the weights of\nactor and reward models, as well as the interaction and scheduling between them. It introduces\ntwo major costs: (1) the memory cost, as several copies of the SFT and RW models need to\nbe served throughout stage 3; and (2) the predominant generation phase, which if not acceler-\nated properly, will significantly slow down the entire stage 3. Additionally, the two important\nfeatures we added in Stage 3, including Exponential Moving Average (EMA) collection and\nMixture Training, will incur additional memory and training costs.\nTo tackle these challenges, we composed the full system capability of DeepSpeed Train-\ning and Inference into a unified infrastructure that we call Hybrid Engine. It leverages the\noriginal DeepSpeed engines for fast training mode while effortlessly applying DeepSpeed in-\nference engine for generation/evaluation mode, providing a significantly faster training system\nfor RLHF training at Stage 3. As Figure 2 shows, the transition between DeepSpeed training\nand inference engine is seamless: by having the typical eval and train modes enabled for the\nactor model, when running for inference and training pipeline, DeepSpeed selects its different\noptimizations to run the model faster and improve the overall system throughput.\nDuring its inference execution for experience generation phase of RLHF training, Deep-\nSpeed Hybrid Engine uses a light-weight memory management system to handle the KV-cache\nand intermediate results, together with highly optimized inference-adapted kernels and ten-\nsor parallelism implementation, to achieve significant boost in throughput (tokens-per-second)\ncompared to the existing solutions.\nDuring the training execution, Hybrid Engine enables memory optimization techniques such\nas DeepSpeed\u2019s ZeRO family of technologies and Low Rank Adaption (LoRA). We designed\nand implemented these system optimizations in a way that they are compatible with each other\nand can be composed together to deliver the highest training efficiency under the unified Hybrid\nEngine.\nHybrid Engine can seamlessly change model partitioning across training and inference to\nsupport tensor-parallelism based inferencing and ZeRO-based sharding mechanism for training.\nIt can also reconfigure the memory system to maximize memory availability during each of these\nmodes. This allows for improved performance by avoiding memory allocation bottlenecks and\nsupporting large batch sizes. Packed with a spectrum of system technologies from DeepSpeed\ntraining and inference, Hybrid Engine pushes the boundary of modern RLHF training and\ndelivers unparalleled scale and system efficiency for RLHF workloads.\n5\nDeepSpeed RLHF: Unparalleled Scale and Efficiency\nvia Hybrid Engine\n5.1\nCapability Recap\nAs discussed, DeepSpeed-HE is an amalgamation of powerful system technologies for inference\nand training, architected to achieve excellent scale and efficiency for DeepSpeed-RLHF pipeline\n7\nFigure 2:\nDeepSpeed Hybrid Engine design for accelerating the most time-consuming portion\nof a RLHF pipeline.\nacross a wide range of hardware, making RLHF training fast, affordable, and easily accessible\nto AI community.\nIn terms of efficiency and affordability, as shown in Table 1, DeepSpeed-HE can train OPT-\n13B in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600,\nrespectively. In terms of speed and scalability, as shown in Table 2, even a 13B model can be\ntrained in 1.25 hours and a massive 175B model can be trained in under a day using a 64 GPU\ncluster. And in terms of accessibility and democratization of RLHF, DeepSpeed-HE supports\ntraining models with over 13 billion parameters on a single GPU as shown in Table 3.\n5.2\nThroughput and Model Size Scalability Comparisons with Exist-\ning RLHF Systems\nCompared to other RLHF systems like Colossal-AI [13] or HuggingFace [6] powered by native\nPyTorch [14], DeepSpeed-RLHF excels in system performance and model scalability:\n\u2022 With respect to throughput, DeepSpeed enables over 10x improvement for RLHF training\non a single GPU (Figure 3).\nOn multi-GPU setup, it enables 6 \u2013 19x speedup over\nColossal-AI and 1.4 \u2013 10.5x over HuggingFace DDP (Figure 4).\n\u2022 With respect to model scalability, Colossal-AI can run a max model size of 1.3B on a\nsingle GPU and 6.7B on a single A100 40G node, DeepSpeed-HE can run 6.5B and 50B\nmodels respectively on the same hardware, up to 7.5x larger.\nTherefore, with over an order of magnitude higher throughput, DeepSpeed-HE unlocks the\nability to train significantly larger actor models under the same latency budget or train models\n8\nFigure 3: Step 3 throughput comparison against two other system frameworks for accelerating\nRLHF training on a single NVIDIA A100-40G commodity GPU. No icons represent OOM\nscenarios.\nof similar size at over 10x lower cost, compared to existing RLHF systems like Colossal-AI or\nHuggingFace DDP.\nThis improvement in efficiency stems from DeepSpeed-HE\u2019s ability to accelerate RLHF gen-\neration phase of the RLHF processing leveraging DeepSpeed inference optimizations. Figure 5\nshows the time breakdown for a 1.3B parameter model at an RLHF training iteration: major-\nity of the time goes to the generation phase. By leveraging high performance inference kernels\nfrom DeepSpeed, DeepSpeed-HE can achieve up to 9x throughput improvement during this\nphase over HuggingFace and 15x over Colossal-AI allowing it to achieve unparallel end-to-end\nefficiency.\n5.3\nEffective Throughput and Scalability Analysis\n(I) Effective Throughput Analysis.\nThe effective throughput of DeepSpeed-HE dur-\ning Stage 3 of the RLHF training depends on the throughput that it achieves during the\ngeneration and RL training phases. In our RLHF pipeline, the generation phase comprises\napproximately 20% of the total computation while the RL training phase comprises of re-\nmaining 80% (see benchmark settings https://github.com/microsoft/DeepSpeedExamples/\ntree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.\nmd page for details). However, despite having a small proportion, the former can take a large\nportion of the e2e time as it requires running the actor model once for each of the 256 generated\ntokens with initial prompt of 256 tokens, making it memory bandwidth bound and difficult to\nachieve high throughput. In contrast, the RL training phase is compute bound running the\nreference actor model with just a couple of forward and backward passes with full 512 tokens\nfrom both prompt and generation per sample and can achieve good throughput.\nTo maximize the effective throughput, DeepSpeed-HE optimizes both phases.\nFirst, it\nuses the largest batch size possible to get higher efficiency on both phases. Second, during\nthe generation phase, it leverages high-performance transformer kernels to maximize GPU\nmemory bandwidth utilization when the model fits in single GPU memory, and leverage tensor-\nparallelism (TP) when it does not. Using TP in the generation phase instead of ZeRO to fit\nthe model reduces the inter-GPU communication and maintains high GPU memory bandwidth\nutilization.\nFigure 6 shows the best achievable effective throughput for DeepSpeed-HE in terms of\nTFlops/GPU for model sizes ranging from 1.3B to 175B. It also shows the throughput achieved\n9\nFigure 4: End-to-end training throughput comparison for step 3 of the training pipeline (the\nmost time consuming portion) with different model sizes on a single DGX node equipped with\n8 NVIDIA A100-40G GPUs. No icons represent OOM scenarios.\nFigure 5: Superior generation phase acceleration from DeepSpeed Chat\u2019s Hybrid Engine: A\ntime/sequence breakdown for training OPT-1.3B actor model + OPT-350M reward model on\na single DGX node with 8 A100-40G GPUs.\n10\nFigure 6: RLHF Generation, training, and effective throughput with DeepSpeed-HE for differ-\nent model sizes, at the GPU count that maximizes efficiency.\nby each of the generation and training phases. DeepSpeed-HE is the most efficient for models in\nthe range 6.7B-66B. Going beyond this range to 175B, the throughput drops due to the limited\nmemory to support larger batch sizes, while still achieving 1.2x better efficiency than the small\n1.3B model. The per-GPU throughput of these gigantic models could improve further when we\nscale them to more GPUs with more memory available for larger batch sizes.\nFurthermore, we would like to point out that our effective performance is 19x higher than\nexisting systems, as shown in Figure 4, which suggests that they are operating at lower than\n5% of the peak. This demonstrates the challenge of optimizing RLHF workloads as well as the\neffectiveness of our system despite the challenge.\n(II) Scalability Analysis\nThe best effective throughput for different model sizes is achieved\nat different GPU count. This is in part because some of the larger model sizes require more\nmemory to run. However, a large part of this behavior stems from DeepSpeed-HE\u2019s scalability\nproperties that we discuss next.\nFigure 7 shows that DeepSeed-RLHF has achieved good scaling overall on up to 64 GPUs.\nHowever, if we look more closely, it shows that DeepSpeed-RLHF training achieves super-linear\nscaling at small scale, followed by near linear or sub-linear scaling at larger scales. This is due\nto interaction between memory availability and max global batch size.\nAs DeepSpeed-HE is powered by ZeRO-based technology [15] for training, it allows model\nstates to be partitioned across the available GPUs. As a result, the memory consumption per\nGPU reduces with the increase in the number of GPUs, allowing DeepSpeed-HE to support\na larger batch per GPU resulting in super-linear scaling. However, at large scale, while the\navailable memory continues to increase, the maximum global batch size (1024, in our case,\n11\nFigure 7: Scalability for training 13B (left) and 66B (right) actor model+350M reward model\non an increasing number of DGX nodes with 8 A100-40/80G GPUs\nwith a sequence length of 512) limits the batch size per GPU, resulting in near-linear or sub-\nlinear scaling. As a result, for a given max global batch size, DeepSpeed-HE achieves the best\nthroughput and cost efficiency at the boundary of super-linear and sub-linear scalability, and\nthe exact point is mostly determined by the largest batch size that can be run per GPU as the\nfunction of available memory and global batch size.\n6\nRelease: Try DeepSpeed Chat Today!\nWe are very excited to share that DeepSpeed-Chat is now open-sourced and available to the\nAI community.\n\u2022 To get started, please visit our github page for DeepSpeed-Chat: GitHub Landing Page\n(https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-\nChat)\n\u2022 We will continue to improve DeepSpeed-Chat with your feedback and support.\nOur\nroadmap (https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/\nDeepSpeed-Chat/README.md#-deepspeed-chats-roadmap-) shows currently supported\nfeatures as well as ones that are planned for future.\nDeepSpeed-Chat is part of the bigger DeepSpeed ecosystem comprising of a multitude of\nDeep Learning systems and modeling technologies. To learn more,\n\u2022 Please visit our website (https://www.deepspeed.ai/) for detailed blog posts, tutorials,\nand helpful documentation.\n12\n\u2022 You can also follow us on our English Twitter (https://twitter.com/MSFTDeepSpeed),\nJapanese Twitter (https://twitter.com/MSFTDeepSpeedJP), and Chinese Zhihu (https:\n//www.zhihu.com/people/deepspeed) for latest news on DeepSpeed.\nDeepSpeed welcomes your contributions! We encourage you to report issues, contribute PRs,\nand join discussions on the DeepSpeed GitHub (https://github.com/microsoft/DeepSpeed/)\npage. Please see our contributing guide (https://github.com/microsoft/DeepSpeed/blob/\nmaster/CONTRIBUTING.md) for more details. We are open to collaborations with universities,\nresearch labs, companies, such as those working together on deep learning research, applying\nDeepSpeed to empower real-world AI models and applications, and so on.\nContributions\nZY: Full engagement. RYA: Hybrid engine. OR: DeepSpeed ZeRO feature adaptation. SR:\nSystem support and blog contribution. XW: Training pipeline and bench-marking support.\nAAA: Software support and post-release debugging. JR: Software support and post-release\ndebugging.\nMZ: Training pipeline and system support, post-release debugging.\nCL: Data\nsupport and post-release debugging. CH: System support. ZZ: Benchmarking. MW: Software\nsupport and post-release debugging. MS: Post-release debugging. LK: Post-release debugging.\nHQ: System support. MT: System support. SC: Software support. SLS: System support,\nblog and tutorial contribution. YH: Team lead.\nAcknowledgment\nWe thank the entire DeepSpeed team for their contributions on developing, debugging, testing,\nand releasing the DeepSpeed-Chat software.\nReferences\n[1] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\n[2] ChatLLaMa Authors. Chatllama. https://github.com/juncongmoo/chatllama, 2023.\n[3] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n[4] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,\nand Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[5] Databricks.\nDatabricks-dolly.\nhttps://www.databricks.com/blog/2023/\n03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html?\nscid=7018Y000001Fi1CQAS&utm_medium=paid+search&utm_source=google&\nutm_campaign=17107065832&utm_adgroup=150868748114&utm_content=\nblog&utm_offer=hello-dolly-democratizing-magic-chatgpt-open-\nmodels.html&utm_ad=661606835947&utm_term=databricks%20dolly&gclid=\nCj0KCQjwiIOmBhDjARIsAP6YhSV89V2agFl3zFuWiZiV1N3IVNhZWr8pGtXVxXrlkuPHlW3cXbGfiHsaAmIDEALw_\nwcB, 2023.\n13\n[6] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, An-\nthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al.\nHug-\ngingface\u2019s transformers:\nState-of-the-art natural language processing.\narXiv preprint\narXiv:1910.03771, 2019.\n[7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language mod-\nels to follow instructions with human feedback. Advances in Neural Information Processing\nSystems, 35:27730\u201327744, 2022.\n[8] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano.\nLearning to summarize with human\nfeedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\n[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[10] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[11] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[12] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable\nquestions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[13] Colossal AI Authors. Colossal ai. https://github.com/hpcaitech/ColossalAI, 2022.\n[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library. Advances in neural informa-\ntion processing systems, 32, 2019.\n[15] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory op-\ntimizations toward training trillion parameter models. In SC20: International Conference\nfor High Performance Computing, Networking, Storage and Analysis, pages 1\u201316. IEEE,\n2020.\n14\n"
  },
  {
    "title": "Learning to Model the World with Language",
    "link": "https://arxiv.org/pdf/2308.01399.pdf",
    "upvote": "33",
    "text": "Learning to Model the World with Language\nJessy Lin\nYuqing Du\nOlivia Watkins\nDanijar Hafner\nPieter Abbeel\nDan Klein\nAnca Dragan\nUC Berkeley\nAbstract\nTo interact with humans and act in the world, agents need to understand the\nrange of language that people use and relate it to the visual world. While current\nagents learn to execute simple language instructions from task rewards, we aim\nto build agents that leverage diverse language that conveys general knowledge,\ndescribes the state of the world, provides interactive feedback, and more. Our key\nidea is that language helps agents predict the future: what will be observed, how\nthe world will behave, and which situations will be rewarded. This perspective\nunifies language understanding with future prediction as a powerful self-supervised\nlearning objective. We present Dynalang, an agent that learns a multimodal world\nmodel to predict future text and image representations and learns to act from\nimagined model rollouts. Unlike traditional agents that use language only to predict\nactions, Dynalang acquires rich language understanding by using past language\nalso to predict future language, video, and rewards. In addition to learning from\nonline interaction in an environment, Dynalang can be pretrained on datasets of\ntext, video, or both without actions or rewards. From using language hints in grid\nworlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types\nof language to improve task performance, including environment descriptions,\ngame rules, and instructions.\n1\nIntroduction\nA long-standing goal of artificial intelligence is to develop agents that can use language to interact\nnaturally with people in the physical world [68]. Current embodied agents can follow simple, low-\nlevel instructions like \u201cget the blue block\u201d [48] or \u201cgo past the elevator and turn right\u201d [5]. However,\nto communicate freely interactive agents should understand the full range of ways people use language\nbeyond the \u201chere and now\u201d [30]: transmitting knowledge such as \u201cthe top left button turns off the\nTV,\u201d providing situational information such as \u201cwe\u2019re out of milk,\u201d and coordinating by saying \u201cI\nalready vacuumed the living room.\u201d Much of what we read in text or hear from others communicates\nknowledge about the world, either about how the world works or about the current state of the world.\nHow could we enable agents to use diverse types of language? One way to train language-conditioned\nagents to solve tasks is reinforcement learning (RL). However, current language-conditioned RL\nmethods primarily learn to generate actions from task-specific instructions, e.g. taking a goal\ndescription like \u201cpick up the blue block\u201d as an input and outputting a sequence of motor controls.\nWhen we consider the diversity of functions that natural language serves in the real world, directly\nmapping language to optimal actions presents a challenging learning problem. Consider the example\n\u201cI put the bowls away\u201d: if the task at hand is cleaning up, the agent should respond by moving on\nto the next cleaning step, whereas if it is serving dinner, the agent should retrieve the bowls. When\nlanguage does not talk about the task, it is only weakly correlated with optimal actions the agent\nshould take. Mapping language to actions, particularly using task reward alone, is therefore a weak\nlearning signal for learning to use diverse language inputs to accomplish tasks.\nWebsite: dynalang.github.io\nCorrespondence to: jessy_lin@berkeley.edu\narXiv:2308.01399v1  [cs.CL]  31 Jul 2023\nt=61\nt=0\nthe bottle is in the \nliving room\nthe plates are in the\nget the bottle\nkitchen\nContext\nDynalang Model Rollouts\nText prediction\nReward prediction\nVideo prediction\nVideo and text inputs\nt=65\nt=60\nt=30\nr=0\nr=0\nr=0\nr=0\nr=1\nFigure 1: Dynalang learns to use language to make predictions about future (text + image) observa-\ntions and rewards, which helps it solve tasks. Here, we show real model predictions in the HomeGrid\nenvironment. The agent has explored various rooms while receiving video and language observations\nfrom the environment. From the past text \u201cthe bottle is in the living room\u201d, the agent predicts at\ntimesteps 61-65 that it will see the bottle in the final corner of the living room. From the text \u2018get the\nbottle\u201d describing the task, the agent predicts that it will be rewarded for picking up the bottle. The\nagent can also predict future text observations: given the prefix \u201cthe plates are in the\u201d and the plates it\nobserved on the counter at timestep 30, the model predicts the most likely next token is \u201ckitchen.\u201d\nInstead, we propose that a unifying way for agents to use language is to help them predict the\nfuture. The utterance \u201cI put the bowls away\u201d helps agents make better predictions about future\nobservations (i.e., that if it takes actions to open the cabinet, it will observe the bowls there). Much of\nthe language we encounter can be grounded in visual experience in this way. Prior knowledge such\nas \u201cwrenches can be used to tighten nuts\u201d helps agents predict environment transitions. Statements\nsuch as \u201cthe package is outside\u201d help agents predict future observations. This framework also unifies\nstandard instruction following under predictive terms: instructions help agents predict how they\nwill be rewarded. Similar to how next-token prediction allows language models to form internal\nrepresentations of world knowledge [52], we hypothesize that predicting future representations\nprovides a rich learning signal for agents to understand language and how it relates to the world.\nWe present Dynalang, an agent that learns a world model of language and images from online\nexperience and uses the model to learn how to act. Dynalang decouples learning to model the world\nwith language (supervised learning with prediction objectives) from learning to act given that model\n(reinforcement learning with task rewards). The world model receives both visual and textual inputs\nas observation modalities and compresses them to a latent space. We train the world model to predict\nfuture latent representations with experience collected online as the agent acts in the environment.\nWe train the policy to take actions that maximize task reward, taking the latent representation of the\nworld model as input. Because world modeling is separated from action, Dynalang can be pretrained\non single modalities (text-only or video-only data) without actions or task reward. Additionally,\nlanguage generation can also be unified in our framework: the agent\u2019s perception can inform an\nagent\u2019s language model (i.e., its predictions about future tokens), enabling it to speak about the\nenvironment by outputting language in the action space.\nWe evaluate Dynalang on a broad range of domains with different types of language context. In a multi-\ntask home cleanup environment, Dynalang learns to use language hints about future observations,\nenvironment dynamics, and corrections to accomplish tasks more efficiently. On the Messenger\nbenchmark [29], Dynalang can read game manuals to fit the most challenging stage of the game,\noutperforming task-specific architectures. In vision-language navigation [38], we demonstrate that\nDynalang can learn to follow instructions in visually and linguistically complex domains.\nOur contributions are as follows:\n\u2022 We propose Dynalang, an agent that grounds language to visual experience via future prediction.\n\u2022 We demonstrate that Dynalang learns to understand diverse kinds of language to solve a broad\nrange of tasks, often outperforming state-of-the-art RL algorithms and task-specific architectures.\n\u2022 We show that the Dynalang formulation enables additional capabilities: language generation can\nbe unified in the same model, as well as text-only pretraining without actions or task rewards.\n2\nLangRoom\nHomeGrid\nMessenger\nHabitat\nEnvironment\nInputs\nActions\nPixels\nInstructions\nWalk into the \nliving room and \nturn right. Stop \nby the end table.\nMotor\nStop\nPositions\nRules\n1. Your enemy is \ninside of a \nplane\u2026\n2. The top secret \npaperwork is\u2026\n0\n0\n0\n2\n3\n0\n0\n0\n0\n2\n0\n1\n0\n0\n0\n0\nPixels\nQuestions\nPixels\nWhat color is the \nvase?\nTasks & Hints\nclean up the \npapers\npedal to open the \nrecycling bin\nMotor\nMotor\nMotor\nAnswers\nInteraction\npick up\npedal\ndrop\ngrasp\nget\nlift\nIt is green.\nFigure 2: We consider a wide range of environments that feature visual inputs and diverse types of\nlanguage. HomeGrid is a challenging visual grid world with instructions and diverse hints. Messenger\nis a benchmark with symbolic inputs and hundreds of human-written game manuals that require\nmulti-hop reasoning. Habitat simulates photorealistic 3D homes for vision-language navigation,\nwhere the agent has to locate objects in hundreds of scenes. LangRoom is a simple visual grid world\nwith partial observability, where the agent needs to produce both motor actions and language.\n2\nRelated Work\nMuch work has focused on teaching reinforcement learning agents to utilize language to solve\ntasks by directly conditioning policies on language [5, 61, 47] or by augmenting agents with large\nlanguage models (LLMs) [44, 1, 31]. While most of these agents focus on learning from high-level\nspecifications of goals or step-by-step guidance, relatively few works have addressed learning to\nuse broader types of language such as descriptions of how the world works [10, 69, 29]. Instead of\ndirectly learning a language-conditioned policy, we learn a language-conditioned world model and\ndemonstrate its ability to understand diverse kinds of language about the world in a single model.\nAdditionally, in contrast to LLM-based policies, Dynalang is multimodal, extending the next-token\nprediction paradigm to observations of both language and images rather than relying on translating\nobservations to text. Dynalang can also be updated online, allowing the agent to continually learn\nlanguage and how it relates to the world. We refer to Appendix C for detailed related work.\n3\nDynalang\nAlgorithm 1: Dynalang\nwhile acting do\nStep environment rt, ct, xt, lt \u2190 env(at\u22121).\nEncode observations zt \u223c enc(xt, lt, ht).\nExecute action at \u223c \u03c0(at | ht, zt).\nAdd transition (rt, ct, xt, lt, at) to replay buffer.\nwhile training do\nDraw batch {(rt, ct, xt, lt, at)} from replay buffer.\nUse world model to compute multimodal\nrepresentations zt, future predictions \u02c6zt+1,\nand decode \u02c6xt, \u02c6lt, \u02c6rt, \u02c6ct.\nUpdate world model to minimize Lpred + Lrepr.\nImagine rollouts from all zt using \u03c0.\nUpdate actor to minimize L\u03c0.\nUpdate critic to minimize LV .\nwhile text pretraining do\nSample text batch {lt} from dataset.\nCreate zero images xt and actions at.\nUse world model to compute representations\nzt, future predictions \u02c6zt+1, and decode \u02c6lt.\nUpdate world model to minimize Lpred + Ll.\nDynalang learns to utilize diverse types of\nlanguage in visual environments by encoding\nmultiple modalities into compressed repre-\nsentations and then predicting the sequence\nof future representations given actions. For\nour algorithm, we build on the model-based\nreinforcement learning algorithm Dream-\nerV3 [28] and extend it to process, and op-\ntionally produce, language. The world model\nis continuously trained from a replay buffer\nof past experience while the agent is inter-\nacting with the environment. It can addition-\nally be pretrained from text-only data. To se-\nlect actions, we train an actor-critic algorithm\nfrom sequences of representations imagined\nby the world model. The algorithm is sum-\nmarized in Algorithm 1.\nProblem setting\nTo perform interactive\ntasks, an agent chooses actions at to inter-\nact with an environment that responds with\nrewards rt, a flag for whether the episode\n3\na2\nz2\na1\nz1\na3\nz3\n\u1e912\n\u1e913\n\u1e914\nr\u0302 2\nr\u0302 3\nr\u0302 4\nWorld Model\nx2\nl2\nx1\nl1\nx3\nl3\n(a) World Model Learning\n     World Model\nr\u0302 2\nv2\na2\na1\nz1\n\u1e912\nr\u0302 3\nv3\na3\n\u1e913\nr\u0302 4\nv4\na4\n\u1e914\nx1\nl1\n(b) Learning to Act by Latent Imagination\nFigure 3: During world model learning, the model compresses observations of image frames and\ntext to a latent representation. The model is trained to predict the next representation and reconstruct\nobservations from the representation. During policy learning, imagined rollouts are sampled from the\nworld model and the policy is trained to maximize imagined rewards.\ncontinues ct, and observations ot. In this paper, we consider multimodal environments where\not = (xt, lt) consist of an image xt and a language token lt at each time step. The agent\u2019s goal is to\nchoose actions that maximize the expected discounted sum of rewards E\n\u0002 PT\nt=1 \u03b3trt\n\u0003\n, where \u03b3 < 1\nis a discount factor, T is the episode length, and cT = 0 signals the episode end. In most of our\nexperiments, the actions at are integers in a categorical action space. However, we also consider\nfactorized action spaces where the agent outputs both a discrete movement command and a language\ntoken at each time step.\nMultimodal alignment\nWe consider a diverse range of environments, summarized in Figure 2,\nwhere agents receive a continuous stream of video and text observations. While previous settings\nspecify that language such as instructions arrive at the beginning of an episode, we are interested in\nenabling agents to act in more flexible settings where they face a continuous stream of video and text,\nas in the real world. For humans, reading, listening, and speaking extends over time, during which\nwe receive new visual inputs and can perform motor actions. Analogously, we provide our agent with\none video frame and one language token at each time step and the agent produces one motor action,\nand in applicable environments one language token, per time step. An additional benefit of providing\none language token per time step is that the algorithm does not have to decide on an arbitrary way to\nsegment language temporally, enabling Dynalang to be applied across a range of tasks with diverse\nkinds of language. We show in Section 4.6 that token-level representations substantially outperform\nsentence-level representations.\n3.1\nWorld Model Learning\nThe world model learns representations of all sensory modalities that the agent receives and then\npredicts the sequence of these latent representations given actions. Predicting future representations\nnot only provides a rich learning signal to ground language in visual experience but also allows\nplanning and policy optimization from imagined sequences. The world model is shown in Figure 3a.\nAt each time step, it receives an image xt, a language token lt, and an action at. The image and\nlanguage observations are compressed into a discrete representation zt and fed together with the\naction into the sequence model to predict the next representation \u02c6zt+1. The multimodal world model\nconsists of the following components, where ht is a recurrent state:\nSequence model:\n\u02c6zt, ht = seq(zt\u22121, ht\u22121, at\u22121)\nMultimodal encoder:\nzt \u223c enc(xt, lt, ht)\nMultimodal decoder:\n\u02c6xt, \u02c6lt, \u02c6rt, \u02c6ct = dec(zt, ht)\n(1)\nWe implement the world model as a Recurrent State Space Model [RSSM 26], where the sequence\nmodel is implemented as GRU [15] with recurrent state ht. Using a recurrent model has the benefit\nthat the policy does not have to integrate information over time anymore, but other sequence models\nsuch as Transformers can also be used [13, 58]. At each timestep, the encoder conditions on the\nobservations and model state ht, effectively learning to compress observations to codes zt relative\n4\nto the history. The sequence model then conditions on the encoded observations zt to integrate new\nobservations into the next model state. The decoder is trained to reconstruct observations and other\ninformation, thus shaping the model representations.\nThe world model is trained jointly to minimize a representation learning loss Lrepr and a future\nprediction loss Lpred, which we describe below.\nMultimodal representations\nThe world model learns to compress inputs images xt and language\ntokens lt into stochastic latent representations zt through a variational autoencoding objective [36, 57].\nThe representations are shaped by reconstructing the input observations, providing a rich learning\nsignal for grounding. We also predict the reward, \u02c6rt, and whether the episode continues, \u02c6ct, so\nthat the policy can be learned directly on top of the latent representations, as discussed in the next\nsection. Finally, the representations are regularized towards a prior distribution over codes. We\nuse the predicted distribution over \u02c6zt as this prior, essentially regularizing the representations to be\npredictable. The representation learning loss Lrepr is the sum of terms:\nImage loss:\nLx = \u2225\u02c6xt \u2212 xt\u22252\n2\nLanguage loss:\nLl = catxent(\u02c6lt, lt)\nReward loss:\nLr = catxent(\u02c6rt, twohot(rt))\nContinue loss:\nLc = binxent(\u02c6ct, ct)\nRegularizer:\nLreg = \u03b2reg max(1, KL\n\u0002\nzt\n\r\r sg(\u02c6zt)\n\u0003\n)\n(2)\nHere, we denote the categorical cross entropy loss as catxent, the binary cross entropy loss as\nbinxent, the stop gradient operator as sg, and \u03b2reg = 0.1 is a hyperparameter. As the network\narchitecture we choose a strided CNN image encoder, a strided CNN as image decoder, and MLPs\nfor all other model components. We evaluate our method both with one-hot token observations\n(i.e., learning the embeddings from scratch) and pretrained embeddings from T5 [54]. One-hot\nrepresentations are reconstructed with the cross entropy loss above and pretrained embeddings are\nreconstructed with a squared error. For more details on world model learning, refer to Appendix A.\nFuture prediction\nThe world model learns to predict the sequence of multimodal representations,\nwhich enables it to plan and ground language. The sequence model produces \u02c6zt from the current\nmodel state (zt\u22121, ht\u22121) and the current action at\u22121, whih is trained to match the actual representation\nat the next timestep zt. Concretely, the future prediction objective is:\nPrediction loss:\nLpred = \u03b2pred max(1, KL\n\u0002\nsg(zt)\n\r\r \u02c6zt\n\u0003\n)\n(3)\nwhere the gradient around the target distribution for zt is stopped since it is also a learned representa-\ntion and \u03b2pred = 0.5 is a hyperparameter. Intuitively, the codes zt contain information from current\nobservation, but also additional information that may be required to predict the reward and episode\ncontinuation. By training the world model to make predictions \u02c6zt of its future representations, it\neffectively learns to predict future images, language, and rewards from its inputs, encouraging the\nagent to extract information from language and learn the correlations between its multiple modalities.\nFor example, when the language input describes that \"the book is in the living room\" and the agent\nlater on visually observes the book, the agent will learn this multimodal association even if the reward\nsignal does not directly relate the two. This objective provides a rich learning signal for grounding.\nThe world model is trained to optimize the overall loss Lrepr +Lpred with respect to all its parameters.\nSingle-Modality Pretraining\nOne potential benefit of separating world modeling from policy\nlearning is that the world model can be trained offline, benefitting from large-scale text-only and\nvideo-only datasets without actions. To pretrain the world model with text-only data as in Section 4.5,\nwe zero out the image and action inputs and set the image, reward, and continuation decoder loss\ncoefficients to 0 so the pretraining focuses on learning to represent text and text dynamics (i.e.\nlanguage modeling). Dynalang can then be finetuned on experience with all modalities (language,\nimages, and actions) by initializing the actor and critic from scratch, while continuing to train the\nworld model. Note that unlike the typical language modeling objective, the model is not explicitly\ntrained to predict the next token from the prefix, except through the prediction of the representation\nat the next timestep.\n5\n0.0\n1.5\n3.0\n4.5\n6.0\nScore\nTask Only\n0.0\n1.5\n3.0\n4.5\n6.0\nTask Only\nWith Future\n0.0\n1.5\n3.0\n4.5\n6.0\nTask Only\nWith Corrections\n0.0\n1.5\n3.0\n4.5\n6.0\nTask Only\nWith Dynamics\nIMPALA\nR2D2\nDynalang\nFigure 4: HomeGrid performance after 50M steps (2 seeds). Dynalang learns to use all types\nof language hints to achieve higher scores than when just provided with the task information,\noutperforming language-conditioned IMPALA and R2D2, where we see performance decrease when\nwe include language hints.\n3.2\nPolicy Learning\nTo select actions, we train an actor critic algorithm [67] purely from imagined sequences of multi-\nmodal representations predicted by the world model [64]. Unlike many other language-conditioned\nRL agents, our policy network is not directly conditioned on language. Instead, it leverages the rich\nmultimodal representations learned by the world model, which contain compressed information about\nboth visual inputs and language. The purpose of the actor network is to predict a distribution over\nactions, which could be a categorical over possible movement directions or language tokens to speak,\nor a factorized action space of both. The purpose of the critic is to estimate the discounted sum of\nfuture rewards for each state to guide the actor learning. Both networks are MLPs:\nActor network:\n\u03c0(at|ht, zt)\nCritic network:\nV(ht, zt)\n(4)\nWe do not modify the policy learning algorithm of DreamerV3 and refer to Appendix B for details.\nIn short, during training time, we generate imagined rollouts of length T = 15 to train the policy.\nStarting from states sampled from the replay buffer, we sample actions from the actor network and\nobservations from the world model. The world model also predicts rewards and continuation flags,\nfrom which we compute \u03bb-returns. The critic network is trained to regress these return estimates,\nwhereas the actor network is trained to maximize them by REINFORCE [67]. During environment\ninteraction, we sample actions from the actor without planning.\n4\nExperiments\nOur experiments aim at investigating the following hypotheses:\nH1) Dynalang enables agents to leverage language beyond instructions to improve task performance,\nwithout having to learn about the world via trial and error. To test this, we investigate whether\nadding different kinds of language hints in HomeGrid improves task performance (Section 4.1),\nand whether Dynalang can learn from game manuals in Messenger (Section 4.2).\nH2) It is more useful to ground diverse language with the future prediction objective than to predict\nactions directly. To test this, we compare our method against model-free RL baselines.\nH3) Interpreting instructions as future reward prediction is no worse than learning to predict actions\ndirectly from instructions, as is typically done. To test this, we compare performance to baselines\nwith task-only language in HomeGrid and on vision-language navigation (Section 4.3).\nH4) The Dynalang formulation additionally enables the agent to generate language (Section 4.4).\nLanguage encodings\nWe tokenize all text with the T5 tokenizer [54], with a vocabulary size of\n32,100. In HomeGrid we use one-hot token encodings. In Messenger and VLN-CE, where agents\nmust generalize to synonyms and linguistic variations, we embed each sentence with T5-small (60M\nparameters) and use the last hidden layer representation for each token.\nBaselines\nWe compare against two model-free RL baselines: IMPALA [22], an off-policy actor\ncritic algorithm, and R2D2, an off-policy DQN-like algorithm [35]. The architecture for both\nalgorithms consists of an LSTM that takes in input embeddings from a CNN image encoder and\nan MLP language encoder. We use the implementations from the SeedRL repository [23]. We pass\nthe same language observations to the baselines as to our method (token-by-token embeddings or\none-hot encodings). We also try providing the baselines with sentence embeddings from a pretrained\nall-distilroberta-v1 model from the Sentence Transformers library [56] and did not find a\nconsistent improvement across our tasks. Both baseline models are \u223c10M parameters, and we did\nnot find that these models benefit from scaling parameter count.\n6\n4.1\nHomeGrid: Language Hints\nFuture Observations\nCorrections\nDynamics\nthe papers are in \nthe living room\nget the papers\nget the fruit\nno, turn around\npedal to open the \ncompost bin\nopen the compost \nbin\nt=25\nt=35\nFigure 5: In HomeGrid, the agent is pro-\nvided with language hints in addition to\ntask specifications. We show real trajecto-\nries from a trained agent using language to\nmaximize reward.\nAs most standard RL benchmarks do not provide lan-\nguage beyond instructions, we introduce a new environ-\nment, HomeGrid, that evaluates how well agents can\nground diverse types of language to solve tasks. Home-\nGrid is a multitask gridworld where agents receive task\nspecifications in language but also language hints, as\ndepicted in Figure 5. Hints provide prior knowledge\nabout world dynamics, information about world state, or\ncorrections that assist the agent. The agent can otherwise\nacquire the same information through autonomous inter-\naction with the environment, as in standard RL. Agents\ncan achieve higher performance if they learn to ground\nlanguage to the environment.\nThere are five task types involving objects and trash bins\n(find, get, clean up, rearrange, open), for a total of 38\ntasks. Agents get pixel observations with a partially\nobserved view over the environment and can move and\ninteract with objects and trash bins. Object locations, bin\nlocations, and bin dynamics (i.e., which action correctly\nopens the bin) are randomized on each episode. Ob-\njects are also randomly moved throughout the episode.\nAgents receive task specifications in language. When a\ntask is completed, the agent gets a reward of 1 and a new\ntask is sampled. To achieve a high score, agents must\ncomplete as many tasks as possible before the episode\nterminates in 100 steps. In addition to task specifica-\ntions, hints are sampled at random points throughout the\nepisode and are provided token-by-token while the agent\ncontinues to act. We script the following language hints:\n\u2022 Future observations\nDescriptions of where objects are in the world or where they have been\nmoved. Without language, the agent must explore the environment to find objects.\n\u2022 Dynamics\nDescriptions of the correct action to open each trash bin. Without language, the agent\ncan try all the different actions, although taking the wrong action can disable the trash can for a\nvariable number of timesteps or potentially the rest of the episode (irreversible dynamics).\n\u2022 Corrections\nTell the agent \u201cno, turn around\u201d when its distance to the current goal object is\nincreasing. Without language, the agent must explore on its own.\nFigure 4 shows that Dynalang benefits from all kinds of language, achieving higher scores with hints\nrelative to just using instructions. Notably, agents never receive direct supervision about what the\nhints mean in HomeGrid, and hints are often far removed from the objects or observations they refer\nto. Dynalang learns to ground language to the environment purely via the future prediction objective.\nLanguage-conditioned IMPALA struggles to learn the task at all, while R2D2 learns to use the types\nof language that are correlated with reward (tasks and corrections). Interestingly, we find that while\nR2D2\u2019s performance drops as it gets overwhelmed with more diverse language, while Dynalang\nimproves across the board, supporting H1 and H2. We hypothesize that additional language input\nmakes it more difficult for the model-free methods to learn to process observations to solve the task.\n4.2\nMessenger: Game Manuals\nNext, we evaluate Dynalang on the Messenger game environment [29], which tests whether agents\ncan read text manuals describing game dynamics to achieve high scores. In Messenger, the agent must\nretrieve a message from one of the entities in the environment and deliver it to another entity, while\navoiding enemies. In each episode, the agent receives a manual describing the randomized entity\nroles and movement dynamics. The challenge is grounding the text references to the environment,\nwhich requires multi-hop reasoning over both visual and text inputs (e.g. combining the manual\ninformation that the goal entity is a \u201cfleeing wizard\u201d with observations of entity identities and\nmovement dynamics). Messenger has three stages of increasing length and difficulty (S1, S2, S3).\n7\n0\n500K\n1M\nEnv Steps\n0.5\n0.0\n0.5\n1.0\nScore\nStage 1\n0\n12.5M\n25M\nEnv Steps\n0.5\n0.5\n1.5\nStage 2\n0\n25M\n50M\nEnv Steps\n0.5\n0.5\n1.5\nStage 3\nDynalang (ours)\nEMMA\nR2D2\nIMPALA\nFigure 6:\nMessenger training\nperformance (2 seeds).\nDy-\nnalang outperforms language-\nconditioned IMPALA and R2D2,\nas\nwell\nas\nthe\ntask-specific\nEMMA architecture, fitting the\nmost complex stage of the game\nwhere other methods fail to\nachieve non-trivial performance.\nThe language in Messenger is generated from human-written templates, resulting in diverse sentences\nwith multiple ways of referring to each entity and a total vocabulary size of 1,125. Observations are\npresented as a symbolic grid of entity IDs, and the agent takes discrete actions to move. We input the\nmanual into Dynalang by inputting the manual into the world model before the episode begins.\nIn addition to the baselines above, we compare the performance of Dynalang to EMMA, the original\nbaseline for the benchmark that uses a specialized grid-based architecture for the task and learns a\nlanguage-conditioned policy with PPO [59]. The architecture provides a gridworld-specific inductive\nbias that each text token should map to some region in the current observation, and assumes that\nthe model has access to the spatial locations of entities in the scene. As in the original benchmark,\nwe initialize all models from the converged model trained on the previous game stage. Since we\nare not focused on studying generalization and distribution shift on this particular task, we compare\nmodels on the Messenger train environments rather than the benchmark held-out variations. As\nseen in Figure 6, Dynalang achieves higher performance and learns more efficiently than EMMA,\nIMPALA and R2D2. While other methods fail to fit S3 at all, our method learns to interpret the\nmanuals to achieve non-trivial performance on the most challenging stage, further supporting H2.\n4.3\nVision-Language Navigation: Instruction Following\nTo evaluate how Dynalang performs in more complex environments, we apply it to the popular\nVision-Language Navigation (VLN) [5] benchmark. Agents must navigate Matterport3D panoramas\ncaptured in real homes [12], following crowd-sourced natural language instructions that indicate\nwhere the agent should navigate to, such as \u201cGo past the end of the bed to the door on the left.\nEnter the hallway,...\" We focus on the more challenging variant, Vision-and-Language Navigation\nin Continuous Environments (VLN-CE) [38]. Rather than providing a waypoint navigation graph\nas in the original VLN task (which generally are not available when navigating in real homes), in\nVLN-CE agents move freely in a continuous environment. The best-performing methods on this task\nuse expert demonstrations [4] or train navigation-specialized hierarchical agents [39]. In this task, our\ngoal is to demonstrate that Dynalang can learn policies in this challenging instruction-conditioned\nRL setting while interpreting instructions as predicting future rewards.\nEach episode randomly samples a language instruction and corresponding scene from the training\ndataset, which is comprised of 10,819 unique instructions total. The agent is trained with a dense\nreward based on relative positions to the current goal, a success reward when taking the stop action at\nthe correct location, and a penalty otherwise. As shown in the example trajectory in Figure 7, the agent\nlearns to follow naturally phrased navigation instructions in visually realistic home environments.\nCompared to the model-free R2D2 baseline, Dynalang succeeds at a significantly higher portion of\nthe training instructions, supporting H3. While Dynalang successfully learns to ground instructions\nfrom scratch, performance is not yet competitive with state-of-the-art VLN methods (many of which\nuse expert demonstrations or specialized architectures), and further work is needed to close the gap.\nExit the bedroom, go straight down the hallway, make a right \ninto the doorway of the bathroom and stop.\n0\n15M\n30M\nEnv Steps\n0\n25\n50\nSuccess (%)\nVLN-CE\nDynalang\nR2D2\nFigure 7: VLN-CE results. (left) A portion of a trained agent trajectory, given the instruction \u201cExit\nthe bedroom, go straight down the hallway, make a right into the doorway of the bathroom and stop\".\n(right) Success rate during RL training, averaged across 3 seeds for Dynalang and 2 seeds for R2D2.\n8\nwhat color is the ball?\nAgent View\nGlobal View\nit is blue\n0\n15M\n30M\n45M\nEnv Steps\n0.0\n1.5\n3.0\n4.5\n6.0\nScore\nLangRoom\nDynalang\nFigure 8: LangRoom results. (left) A real trajectory from a trained agent. The agent learns to take\ninformation-gathering actions from reward. When asked \u201cwhat color is the ball?\u201d the agent walks to\nthe corner with the ball and generates the tokens \u201cit is blue.\u201d (right) Training curve. The agent learns\nto answer more questions accurately.\n4.4\nLangRoom: Embodied Question Answering\nFinally, we show how Dynalang can also generate language in the same framework. On the other\nbenchmarks, language is used to inform agents\u2019 future predictions about the world, but perception can\nalso inform future predictions about what might be said. For example, agents could predict that they\nwill hear descriptive utterances such as \u201cthe stove is on\u201d that are consistent with its own observations\nof the burner producing flames. In contrast to language models, Dynalang is multimodal, enabling\nthe agent to ground its language generation to the real world. We introduce the LangRoom embodied\nquestion answering environment to demonstrate a proof-of-concept of this capability. We expand the\naction space to include language by allowing the agent to output one language token per timestep as\nan action. The environment contains a room with objects with fixed positions but randomized colors.\nThe language observations from the environment are questions \"what color is the <object>?.\" The\nagent only has a partial view over the environment, so it must move to the object. When prompted by\nthe environment, the agent is rewarded for emitting a language action saying the correct color. See\nAppendix D.3 for details on the task.\nAs shown in Figure 8, the agent learns to answer more questions correctly with task reward, supporting\nH4. We show an example trajectory demonstrating that the agent has learned to take information\ngathering actions to observe the color of the object and generate text consistent with the world state.\n4.5\nText-only Pretraining\nDynalang can be pretrained on single-modality data by zeroing out the other modality and action\ninputs. This provides a way for RL agents to benefit from large-scale offline data in a single\narchitecture. To evaluate this capability, we pretrain Dynalang from scratch on (1) in-domain text\nwith manuals from Messenger S2 games (2) domain-general text with TinyStories [21], a dataset of\n2M short stories generated by GPT-3.5 and GPT-4. We evaluate on Messenger S2, where models that\nlearn to embed one-hot token observations from scratch struggle to learn the complex language in S2\nwithout pretraining on S1. We use the T5 vocabulary and compare S2 task performance with learned\nembeddings to using pretrained T5 embeddings, training all methods from scratch on S2 without\ninitializing from S1. As shown in Figure 9, Dynalang is able to benefit from offline pretraining on\ntext-only data. Even a small amount of in-domain text closes much of the gap between training text\nembeddings from scratch and using T5 embeddings. Furthermore, pretraining on TinyStories exceeds\nthe final performance of using T5 embeddings, likely because pretraining allows the model to learn\ntext dynamics offline rather than during environment interaction.\nAlthough the model is not trained explicitly to do language modeling except through next-\nrepresentation prediction, we can generate language from the world model by doing rollouts in\nlatent space and reconstructing the token from the latent representation. One consequence of this\napproach is that the model can potentially do planning in latent space rather than token space. In\nAppendix E we show the model\u2019s preliminary language generation capabilities after pretraining on\nTinyStories, which suggest that Dynalang could potentially be trained and used as a language model.\n4.6\nFurther Analysis\nLanguage updates model predictions in interpretable ways.\nFigure 11 shows that we can\ninterpret what the model has learned by rolling out the world model state into the future and\nreconstructing observations from the latent state, conditioned on some history. We can see that the\nmodel represents the information and correctly grounds it to observations: given the information\n9\n0\n10M\n20M\nEnv Steps\n0.5\n0.5\n1.5\nScore\nText Pretraining\nDynalang\nw/ T5 embed\nw/ one-hot\nw/ one-hot +\nin-domain pretraining\nw/ one-hot +\ngeneral pretraining\nFigure 9: One-hot token encodings underperform\npretrained embeddings on S2, but pretraining Dy-\nnalang with a small amount of text-only data closes\nmuch of the gap.\n0\n1M\n2M\nEnv Steps\n1\n0\n1\nScore\nLanguage Encoding\nDynalang\nw/ token embed\nw/ token\nw/ sentence embed\nFigure 10: Sentence embeddings lead to much\nslower learning, even on S1 where both one-\nhot and pretrained token encodings quickly\nreach ceiling performance.\nthat the papers and bottle are in the living room, different samples from the world model represent\ndifferent possible futures, both of which are consistent with the information described in text. The\nmodel also correctly predicts that in the future where the papers are on the table, it will receive a\nreward of +1 for doing a pickup action, and that it will not be rewarded if it picks up the bottle.\nToken representations outperform sentence representations.\nFigure 10 shows that consuming\none sentence of the manual per timestep causes the agent to learn much more slowly, compared to\nour model which reads one token per timestep. We use embeddings from the Sentence Transformers\nall-distilroberta-v1 model [56]. We hypothesize that the sentence encoder output could be a\nlossy bottleneck, making it difficult for Dynalang to extract information from the text particularly\nwhen the sentences contain a lot of information.\n5\nDiscussion\nLimitations\nOur recurrent architecture may make optimization challenging in extremely long\nhorizon environments. Our design decision to interleave vision and language tokens one-to-one\nallows the agent to act while communicating but may cause sequence length to be the bottleneck for\nlearning in some tasks. While Dynalang can generate text, the generation quality is not competitive\nwith pure language models and further work will be needed to close that gap.\nConclusion\nWe present Dynalang, an agent that grounds language to visual experiences, actions,\nand rewards through future prediction as a rich self-supervised objective. Dynalang learns to act\nbased on various types of language across a diverse range of tasks, often outperforming model-free\nmethods that struggle with increased language complexity. The ability to pretrain on video and text\nwithout actions or rewards suggests that Dynalang could be scaled to large web datasets, paving the\nway towards a self-improving multimodal agent that interacts with humans in the world.\nContext\nGround Truth\nSampled Dynalang Model Rollouts\nthe papers are in the \nliving room\nthe bottle is in the \nliving room\nget the papers\nt=29\nt=0\nt=29\nt=36\nt=36\nr=1\nr=1\nr=0\nFigure 11: Imagined rollouts from the world model. Conditioned on a language description, the\ntask, and the same action sequence, we sample rollouts of the world model\u2019s imagined trajectories.\nSince the papers and bottle can be in any of multiple possible locations in the living room, the model\nsamples exhibit uncertainty over the possible futures. In one rollout (top), the agent predicts the\npapers are on the table and correctly predicts it will get rewarded for picking it up. In the second\nrollout (bottom), it predicts that the bottle is on the table and that it will not get rewarded.\n10\nAcknowledgments\nWe thank Aditi Raghunathan, Michael Chang, Sanjay Subramanian, and Nicholas Tomlin for helpful\ndiscussions, and Kuba Grudzien, Jacob Steinhardt, Meena Jagadeesan, and Alex Wei for draft\nfeedback. We thank the TPU Research Cloud (TRC) program and Danat Pomeranets from Google\nfor access to compute resources. This work was funded in part by The Berkeley Center for Human\nCompatible AI (CHAI) and the Office of Naval Research (ONR-YIP).\nReferences\n[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano,\nK. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine,\nY. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet,\nN. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan.\nDo as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint\narXiv:2204.01691, 2022.\n[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022.\n[3] P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep\nreinforcement learning. arXiv preprint arXiv:1812.01628, 2018.\n[4] D. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang. Etpnav: Evolving\ntopological planning for vision-language navigation in continuous environments. arXiv preprint\narXiv:2304.03047, 2023.\n[5] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. D. Reid,\nS. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-\ngrounded navigation instructions in real environments.\nIn 2018 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018, pages 3674\u20133683. IEEE Computer Society, 2018.\ndoi: 10.1109/CVPR.2018.\n00387. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_\nVision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html.\n[6] J. Andreas and D. Klein. Alignment-based compositional semantics for instruction following.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 1165\u20131174, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi:\n10.18653/v1/D15-1138. URL https://www.aclweb.org/anthology/D15-1138.\n[7] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy\nsketches. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference\non Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70\nof Proceedings of Machine Learning Research, pages 166\u2013175. PMLR, 2017. URL http:\n//proceedings.mlr.press/v70/andreas17a.html.\n[8] Y. Bengio, N. L\u00e9onard, and A. Courville. Estimating or propagating gradients through stochastic\nneurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[9] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou,\nJ. May, A. Nisnevich, N. Pinto, and J. Turian. Experience grounds language. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n8718\u20138735, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.emnlp-main.703. URL https://aclanthology.org/2020.emnlp-main.703.\n[10] S. Branavan, L. Zettlemoyer, and R. Barzilay. Reading between the lines: Learning to map high-\nlevel instructions to commands. In Proceedings of the 48th Annual Meeting of the Association\nfor Computational Linguistics, pages 1268\u20131277, Uppsala, Sweden, 2010. Association for\nComputational Linguistics. URL https://www.aclweb.org/anthology/P10-1129.\n11\n[11] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P. Oudeyer. Grounding large language\nmodels in interactive environments with online reinforcement learning. CoRR, abs/2302.02662,\n2023. doi: 10.48550/arXiv.2302.02662. URL https://doi.org/10.48550/arXiv.2302.\n02662.\n[12] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and\nY. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint\narXiv:1709.06158, 2017.\n[13] C. Chen, Y.-F. Wu, J. Yoon, and S. Ahn. Transdreamer: Reinforcement learning with transformer\nworld models. arXiv preprint arXiv:2202.09481, 2022.\n[14] J. Chen, H. Guo, K. Yi, B. Li, and M. Elhoseiny. Visualgpt: Data-efficient adaptation of\npretrained language models for image captioning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 18030\u201318040, 2022.\n[15] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\n[16] G. Dagan, F. Keller, and A. Lascarides. Learning the effects of physical actions in a multi-modal\nenvironment. In Findings of the Association for Computational Linguistics: EACL 2023, pages\n133\u2013148, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2023.findings-eacl.10.\n[17] I. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus.\nCollaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763,\n2023.\n[18] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\n[19] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas. Guiding\npretraining in reinforcement learning with large language models, 2023.\n[20] J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. Reading to learn: Constructing features\nfrom semantic abstracts. In Proceedings of the 2009 Conference on Empirical Methods in\nNatural Language Processing, pages 958\u2013967, Singapore, 2009. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/D09-1100.\n[21] R. Eldan and Y. Li. Tinystories: How small can language models be and still speak coherent\nenglish?, 2023.\n[22] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,\nI. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner\narchitectures. In International conference on machine learning, pages 1407\u20131416. PMLR,\n2018.\n[23] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, and M. Michalski. Seed rl: Scalable and\nefficient deep-rl with accelerated central inference. arXiv preprint arXiv:1910.06591, 2019.\n[24] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale\nknowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022. URL https://openreview.net/forum?id=rc8o_j8I8PX.\n[25] J. Guo, J. Li, D. Li, A. M. H. Tiong, B. Li, D. Tao, and S. Hoi. From images to textual prompts:\nZero-shot visual question answering with frozen large language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10867\u201310877,\n2023.\n[26] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent\ndynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n12\n[27] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models.\narXiv preprint arXiv:2010.02193, 2020.\n[28] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world\nmodels. arXiv preprint arXiv:2301.04104, 2023.\n[29] A. W. Hanjie, V. Zhong, and K. Narasimhan. Grounding language to entities and dynamics for\ngeneralization in reinforcement learning. In M. Meila and T. Zhang, editors, Proceedings of\nthe 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning Research, pages 4051\u20134062. PMLR,\n2021. URL http://proceedings.mlr.press/v139/hanjie21a.html.\n[30] C. F. Hockett and C. D. Hockett. The origin of speech. Sci. Am., 203(3):88\u201397, 1960.\n[31] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In International Conference on Machine\nLearning. PMLR, 2022.\n[32] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter.\nInner monologue: Embodied reasoning through planning with language models. In arXiv\npreprint arXiv:2207.05608, 2022.\n[33] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn. Language as an abstraction for hierarchical deep\nreinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.\n[34] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu,\nand L. Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint\narXiv:2210.03094, 2022.\n[35] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay\nin distributed reinforcement learning. In International conference on learning representations,\n2019.\n[36] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[37] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved\nvariational inference with inverse autoregressive flow.\nAdvances in neural information\nprocessing systems, 29, 2016.\n[38] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-\nand-language navigation in continuous environments. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVIII 16, pages\n104\u2013120. Springer, 2020.\n[39] J. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets. Waypoint models for instruction-\nguided navigation in continuous environments. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 15162\u201315171, 2021.\n[40] B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language models.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 1813\u20131827, Online, Aug. 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.143. URL https://aclanthology.org/2021.acl-long.143.\n[41] B. Z. Li, W. Chen, P. Sharma, and J. Andreas. Lampp: Language models as probabilistic priors\nfor perception and action. arXiv e-prints, 2023.\n[42] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n13\n[43] K. Li, A. K. Hopkins, D. Bau, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. Emergent world\nrepresentations: Exploring a sequence model trained on a synthetic task. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=DeG07_TcZvT.\n[44] S. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pre-\ntrained language models for interactive decision-making. arXiv preprint arXiv:2202.01771,\n2022.\n[45] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. Unified-io: A unified model for\nvision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\n[46] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson,\nand T. Rockt\u00e4schel. A survey of reinforcement learning informed by natural language. In\nS. Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artificial\nIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 6309\u20136317. ijcai.org, 2019.\ndoi: 10.24963/ijcai.2019/880. URL https://doi.org/10.24963/ijcai.2019/880.\n[47] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data.\nRobotics: Science and Systems, 2021. URL https://arxiv.org/abs/2005.07648.\n[48] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence.\nInteractive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.\n[49] S. Mirchandani, S. Karamcheti, and D. Sadigh. Ella: Exploration through learned language\nabstraction. Advances in Neural Information Processing Systems, 34:29529\u201329540, 2021.\n[50] J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. D. Goodman, T. Rockt\u00e4schel, and\nE. Grefenstette.\nImproving intrinsic exploration with language abstractions.\nIn\nNeurIPS, 2022.\nURL http://papers.nips.cc/paper_files/paper/2022/hash/\ndb8cf88ced2536017980998929ee0fdf-Abstract-Conference.html.\n[51] K. Narasimhan, R. Barzilay, and T. Jaakkola.\nGrounding language for transfer in deep\nreinforcement learning. Journal of Artificial Intelligence Research, 63:849\u2013874, 2018.\n[52] F. Petroni, T. Rockt\u00e4schel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller. Language\nmodels as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China, Nov. 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.\norg/D19-1250.\n[53] S. T. Piantadosi and F. Hill. Meaning without reference in large language models. ArXiv,\nabs/2208.02957, 2022.\n[54] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal\nof Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/\n20-074.html.\n[55] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,\n2022.\n[56] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908.\n10084.\n[57] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate\ninference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.\n[58] J. Robine, M. H\u00f6ftmann, T. Uelwer, and S. Harmeling. Transformer-based world models are\nhappy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023.\n14\n[59] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[60] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. arXiv\npreprint arXiv:2110.01517, 2021.\n[61] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and\nD. Fox. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020, pages 10737\u201310746. IEEE, 2020. doi: 10.1109/CVPR42600.2020.\n01075. URL https://doi.org/10.1109/CVPR42600.2020.01075.\n[62] M. Shridhar, X. Yuan, M.-A. C\u00f4t\u00e9, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning\ntext and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768,\n2020.\n[63] I. Singh, G. Singh, and A. Modi. Pre-trained language models as prior knowledge for playing\ntext-based games. arXiv preprint arXiv:2107.08408, 2021.\n[64] R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSIGART Bulletin, 2(4):160\u2013163, 1991.\n[65] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[66] A. C. Tam, N. C. Rabinowitz, A. K. Lampinen, N. A. Roy, S. C. Y. Chan, D. Strouse, J. Wang,\nA. Banino, and F. Hill.\nSemantic exploration from language abstractions and pretrained\nrepresentations. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/\n2022/hash/a28e024ccd623ed113fb19683fa0910d-Abstract-Conference.html.\n[67] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229\u2013256, 1992.\n[68] T. Winograd. Understanding natural language. Cognitive Psychology, 3(1):1\u2013191, 1972.\nISSN 0010-0285. doi: https://doi.org/10.1016/0010-0285(72)90002-3. URL https://www.\nsciencedirect.com/science/article/pii/0010028572900023.\n[69] V. Zhong, T. Rockt\u00e4schel, and E. Grefenstette. RTFM: generalising to new environment\ndynamics via reading. In 8th International Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://\nopenreview.net/forum?id=SJgob6NKvH.\n15\nA\nWorld Model Learning\nRepresentation Learning\nThe discrete codes zt are vectors of one-hot categoricals that are sampled\nduring the forward pass and optimized using straight-through gradients on the backward pass [8, 27].\nTwo-hot Reward Prediction\nWe follow DreamerV3 in predicting rewards using a softmax classifier\nwith exponentially spaced bins that regresses the twohot encoding of the real-valued rewards and\nin clipping the regularizer at 1 free nat [37]. The two-hot regression decouples the gradient scale\nfrom the arbitrary scale of the rewards and free nats prevent over-regularization, known as posterior\ncollapse.\nB\nActor Critic Learning\nBecause we optimize the policy from imagined rollouts, all involved quantities are predictions rather\nthan environment observations. For simplicity, we omit the hats from the notation now and e.g.\nwrite zt instead of \u02c6zt. To train the actor and critic networks, we predict a sequence of T = 15\nrepresentations zt by sampling from the world model and the actor network. The sequences start at\nall representations computed from the world model training step. From a sequence of representations\nzt and recurrent states ht, we fill in the rewards rt and episode continuation flags ct by applying their\ntwo MLPs, without invoking the image or language decoders. Given the quantities, we compute a\n\u03bb-return [65] that estimates the discounted sum of future rewards:\nRt = rt + \u03b3ct\n\u0010\n(1 \u2212 \u03bb)V (zt+1, ht+1) + \u03bbRt+1\n\u0011\nRT .= V (zT , hT )\n(5)\nThe return estimate Rt serves as a prediction target for the critic network, which uses discrete\nregression using a categorical cross entropy loss towards the twohot encoded targets. The actor\nnetwork is trained to maximize the return estimates subject to an entropy regularizer on the action\ndistribution:\nLV = catxent(Vt(ht, zt), sg(twohot(Rt)))\nL\u03c0 = \u2212 sg(Rt \u2212 V (zt, ht))/ max(1, S) log \u03c0(at | ht, zt) \u2212 \u03b7 H\n\u0002\n\u03c0(at\n\f\f ht, zt)\n\u0003\n(6)\nTo trade off the two actor loss terms without having to tune hyperparameters, the actor loss normalized\nreturns that exceed a magnitude of 1 are normalized by an exponential moving average of the 5th\nto 95th percentile range of returns, S = ema(per(Rt, 95) \u2212 per(Rt, 5)). When interacting with\nthe environment, we choose actions by incorporating the new observation into the world model\nrepresentation and then sampling from the actor network.\nC\nDetailed Related Work\nLanguage and Embodied Agents\nLanguage can be used in embodied settings in a variety of\nways [46]. In instruction following, agents must interpret language specifications of high-level\ngoals or step-by-step guidance [10, 6, 5, 61, 47]. Language can also be used as an abstraction to\nassist learning or decision-making, e.g. for planning by decomposing high-level tasks into low-\nlevel subgoals [7, 33, 1, 31, 44, 60]. Instead of planning in language, our model treats language as\nanother modality in observation space and plans in latent space. Finally, language can be used to\ndescribe the world, e.g. to enable semantic exploration [49, 66, 50, 19], to communicate domain\nknowledge [20, 10, 51, 69, 24], or as feedback from the environment [32]. Our work investigates how\nto unify these settings so that agents can learn from all kinds of language they might encounter in the\nworld, including instructions and descriptions. While most of these works directly condition policies\non language to generate actions (model-free), our algorithm uses language for future prediction,\nlearning a world model that is then used for planning and acting.\nMultimodal Models\nDeveloping agents that can leverage both vision and text observations requires\ntraining multimodal models. Previous works develop vision-language models (VLMs) by augmenting\nLLMs with visual encoders [2, 42, 14, 25] or training models jointly over all modalities [45]\nHowever, because VLMs are prohibitively expensive to query and finetune, recent work on using\nVLMs as policies has focused on supervised learning from demonstrations [18, 34], rather than using\n16\nthem in embodied agents that can learn online. More similar to our work, [55] trains a multimodal\nembodied agent across various tasks, modalities, and embodiments by additionally learning to\ngenerate actions. Unlike prior approaches, our algorithm uses a future prediction objective to ground\ndifferent modalities together, and we show that this enables our model to learn from rich language\nbeyond instructions. Furthermore, our scheme is amenable to both online training and pretraining.\nDecision-making with Large Language Models\nLarge language models (LLMs) learn about the\nworld via next-token prediction on web-text, implicitly modeling world state [40, 43] and relations\nbetween concepts [53]. When acting in purely text-based or symbolic environments, language models\ncan be used as complete world models [3, 63]. In visual environments, LLMs are not grounded to real\nenvironment observations and cannot directly take actions, unless observations are translated to text\n[62, 32, 17]. However, representing visual inputs as text is inherently low bandwidth. Additionally,\nwhile LLMs can be used as a prior over actions or observations [41], they are difficult to update\nwith feedback from the environment except in limited cases [11, 16]. In contrast, we learn a single\nmultimodal world model from experience with autoregressive prediction on both text and images\n(predicting both modalities in the future from both modalities as input), thus grounding language to\nexperience [9]. Our model can also be trained on text-only data as a language model or video-only\ndata as a video prediction model.\nD\nEnvironment Details\nD.1\nHomeGrid\nThe HomeGrid environment is a grid with different objects, receptacles, and rooms. Agents receive\npixel observations of 3x3 grid cells centered on the current agent position. The action space is: move-\nment (left, right, up, down), object interaction (pick up, drop), and trash bin interaction\n(get, pedal, grasp, lift). The agent can carry one object in its inventory by executing the\npick up action in front of an object or the get action in front of a trash bin with an object inside.\nThere are three rooms (living room, dining room, kitchen) indicated by different flooring textures,\nthree possible trash bin types with different colors (blue recycling, black trash, green compost) and\nfour possible trash object types (bottle, fruit, papers, plates). Trash bins can be open, closed, or\nknocked over (represented visually as toppled over sideways). Each trash bin can be opened with a\nspecific action that is randomly selected from {pedal, grasp, lift} in each episode. If agents\napply the wrong action on a bin, it becomes broken and cannot be interacted with further until reset\nby the environment. When a trash bin is open, one object can be dropped into the bin with the drop\naction and the current object in the bin (if any) can be retrieved into the agent\u2019s inventory with get.\nFor each episode, the environment is randomly initialized with two objects and two trash bins in\nrandom positions. Trash bins are initialized in the open state with probability 0.5. One bin is\nirreversibly broken if the wrong action is applied and the other bin is reset after 5 timesteps if broken.\nAt each timestep, each object is moved to a new position with probability 0.05 and new objects are\nspawned with probability 0.1\u2217num_remaining_unique_objects at a random position.\nIn our experiments, agents are evaluated on setups with different language inputs: task instructions,\ntask instructions + dynamics, task instructions + future observations, and task instructions + correc-\ntions. Language for each type is generated with templates from the underlying environment state,\nwith the following semantics:\nTasks\n\u2022 find the [object/bin]: the agent will receive a reward of 1 if it is facing the correct object /\nbin\n\u2022 get the [object]: the agent will receive a reward of 1 if it has the correct object in inventory\n\u2022 put the [object] in the [bin]: the agent will receive a reward of 1 if the bin contains the\nobject\n\u2022 move the [object] to the [room]: the agent will receive a reward of 1 if the object is in\nthe room\n\u2022 open the [bin]: the agent will receive a reward of 1 if the bin is in the open state\n17\nFuture Observations: descriptions of environment state the agent may observe in the future\n\u2022 [object/bin] is in the [room]: the object or bin is in the indicated room\n\u2022 i moved the [object] to the [room]: the object has been moved to the room\n\u2022 there will be [object] in the [room] later: the object will spawn in the room in five\ntimesteps\nDynamics: descriptions of environment transitions\n\u2022 [action] to open the [bin]: the indicated action is the correct action to open the bin\nCorrections: task-specific feedback about the agent\u2019s current trajectory\n\u2022 no, turn around: the agent\u2019s distance to the current goal object or bin (given the task) has\nincreased compared to the last timestep\nLanguage is provided to the agent one token per timestep. All language are provided while the agent\nacts and the environment state is changing, except for dynamics descriptions (which apply to the\nwhole episode). For dynamics descriptions, we randomly shuffle all possible descriptions and input\nthem to the agent in sequence up to a maximum of 28 tokens while the agent is fixed in place. For\nlanguage provided during the episode, on each timestep, if there is not currently an utterance being\nprovided to the agent, either (1) the task instruction is repeated, every 20 timesteps (2) an utterance\ndescribing one of the events that occurred at this timestep is provided (i.e. objects moved or spawned)\n(3) a description of future observations or dynamics is provided (4) a correction is provided, with\nprobability 0.1. If there is a new task instruction (i.e. the agent just completed the last task), any\ncurrently streaming sentence will be interrupted and the agent will immediately receive the tokens\nof the new instruction. All evaluation setups share the same underlying environment dynamics and\nparameters (e.g. each trash bin must be operated with the correct action even if the agent does not\nreceive hints about dynamics).\nD.2\nVLN-CE\nThe VLN-CE training set consists of 10,819 unique natural instructions total, spread across 61 scenes.\nThe instruction and corresponding scene are randomly sampled per episode. In addition to language,\nthe agent observes an egocentric RGB and depth image at each timestep. Agents have access to\ndiscrete low-level actions (moving forward 0.25m, turning left or right 15 degrees), as well as a stop\naction. Crucially, the agent must learn to take the stop action when it thinks it has reached the goal\nto indicate that it recognizes the goal position. This makes the task more challenging, as the agent\nmust learn to only terminate the episode at the appropriate goal locations. The agent receives a dense\nreward at every timestep based on the delta in position from the goal. Following [39] we provide an\nadditional success reward of 1000 when the agent takes the stop action at the correct location, and a\npenalty of -10 when the agent takes the stop action elsewhere.\nD.3\nLangRoom\nIn LangRoom, the environment contains four objects in the corners of a room. The positions of the\nobjects are fixed but the colors are randomized. The action space for the agent includes the four\ncardinal movement actions, stay, and 15 tokens that the agent can say. The language observations\nfrom the environment are questions \"what color is the <object>?,\" followed by a random silence\nduration (allowing the agent to find out the answer), followed by the answer \"it is <color>.\" After\neach question and answer, the colors are randomized and the environment asks a new question, up to a\nfixed episode length of 200 timesteps. Agents are rewarded +1 for saying the correct \u201c<color>\u201d token\nat the same timestep that the environment produces the \u201c<color>\u201d token, -0.1 for saying the wrong\ncolor at that timestep, -0.01 for speaking at other timesteps, and 0 for saying nothing. The agent only\nhas a partial view over the environment, so it must move to the object before the environment starts\nprompting it for the answer.\n18\nE\nText Generation Samples\nDynalang is not explicitly trained on the language modeling objective, but we can still generate text\nfrom the model by sampling rollouts from the world model and decoding the token from the latent\nrepresentation at each timestep. Here, we show sampled 10-token generations conditioned on a prefix\nof 50 tokens for validation examples in TinyStories.\nPrompt: Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She\nclimbed trees, rocks, and hills. One day, Roxy found an icy hill.\nTrue: She had never seen anything like it before. It\nSamples:\nShe wanted to climb down the hill.</s> friends and\nIt was a steep tree, but she was not\nShe wanted to own it, but it was too hot\nShe thought it would be fun banana scary, andffy\nShe wanted to skip through the. But Once upon\nPrompt: Once upon a time, there was a thoughtful girl named Sue. Sue loved to help her mom around the house.\nOne day, her mom asked her to wipe the table after they ate their lunch. Sue was happy to help. As\nTrue: Sue was wiping the table, she saw\nSamples:\nthey her big room. playly remembered her\nshe was placing\u201e, she saw a\nshe got being, she saw hera all she on\nshe was organizing, she saw the pin case in the\nshe was their best delicate turkey on, she saw\nPrompt: Once upon a time, there was a little girl named Lucy. She had a pet cat named Tom. They loved to play\ntogether in the big green park near their house. One sunny day, they went to the park to play.\nTrue: While playing, Tom saw a big s\nSamples:\n</s> Once upon a time, there was scarf\n</s> \" Jenny, you are my sweet. You must\n</s> Once heard a kind girl and asked Mom to\n</s> When taking a small, thin thing he\n</s> The. lesson its if can improve and\nPrompt: Once upon a time, there was a little boy named Tom. He loved to play with his red ball. One sunny\nday, Tom went outside to play with his ball in the land near his home. Tom kicked the ball high in\nTrue: the sky. The ball went far, far away\nSamples:\nthe sky and ity it.\"</s> Once day,\nthe air and loved then it rain outside. We can\nthe sky, but was enormous diary to with baby\nthe sky.</s> red ball went and all game,\nthe air and ran after to catchMoo. His was\nPrompt: Once upon a time, there was a girl named Mia. Mia loved her jewelry. She had a big box full of pretty\nthings. She liked to wear them all day. But at night, she had to sleep. One\nTrue: day, Mia met a talking cat named\nSamples:\nday, shea was mad. She did not want\nnight, shea socks out wanted to hurt up.\nday, shea could not find her skirt dress She\nday, hera\u2019s mom came to her.\nday, Miaa fell her hair could. It\n19\nPrompt: Once upon a time, there was a little boy named Tom. Tom had a special belt that he loved to wear. One\nday, he could not find his belt and felt very sad. Tom\u2019s mom saw him and\nTrue: asked, \"Why are you sad, Tom?\"\nSamples:\nfrustrated and asked him what was rude.</s> Once upon\nasked, \"Why are you sad, Tom?\"</s>\nasked, \"Howeny, I did, get\nsaid, \"Don\u2019t worry, Tom. We\nsaid, \"To tree, you look be in\nPrompt: Once upon a time, in a small house, there lived a kind and honest girl named Lily. She loved to bake\ncakes for her friends and family. One day, she made a big, yummy cake for her best friend\nTrue: \u2019s birthday. Lily carefully put the cake\nSamples:\n, Ben.</s> Tom went Mike opened the and,\n, Tom.</s> Oneo decided the biggest ow\n, Tim.</s> Once upon a time, there\n, Tim.</s> lady.</s> </s> and Lily\n, Tom.</s> Once upon a time, there\nPrompt: One day, a young boy named Tim found a dull, round rock. He picked it up and looked at it. He\nthought it was not very fun, but he took it with him to the park. At the park, Tim\nTrue: saw a girl named Sue. She had\nSamples:\nhe met. favorite friend He put it his to\nmet a girl named Sue. Sue saw the ball\nsaw a stick top Sam. He kept playing with\nplayed with his friends and but they friends!\"</s> Li\nmet a girl named Lily.</s> ly saw\nPrompt: Once upon a time, there was a little boy named Tim. Tim loved candy more than anything else. One\nday, Tim saw a big candy store. He was very happy and ran to the store. Inside the store, Tim met\nTrue: a strange man. The man said, \"\nSamples:\na nice lady named Sue.</s> The thing the\na tall named Max.</s> that the clever\na girl friend named Sue. They said, \"\na big dog named Theffy.</s> said\na new prize car two cars. things.</s>\nPrompt: Once upon a time, there was a big, heavy alligator. He lived near a small pond. He was very hungry\nand wanted to eat something. One day, a little bunny came close to the\nTrue: pond. The alligator saw the bun\nSamples:\nflower. The bunny said, \"Hello\nkitchen. He thisly and said, \"This\nbunny and askede \"Do, smell you\nsunflower. The bun said, \"Stop, sunset\nbunny. The bunny said, \"\n20\nF\nHomeGrid Training Curves\n0\n25M\n50M\nEnv Steps\n0\n3\n6\nScore\nTask Only\n0\n25M\n50M\nEnv Steps\n0\n3\n6 With Future\n0\n25M\n50M\nEnv Steps\n0\n3\n6With Corrections\n0\n25M\n50M\nEnv Steps\n0\n3\n6With Dynamics\nDynalang\nR2D2\nIMPALA\nFigure F.1: HomeGrid training curves.\nG\nAdditional Baseline Experiments\nG.1\nToken vs. Sentence Embeddings for Baselines\n0\n25M\n50M\nEnv Steps\n0\n3\n6\nScore\nTask Only\n0\n25M\n50M\nEnv Steps\n0\n3\n6\nWith Future\n0\n25M\n50M\nEnv Steps\n0\n3\n6\nWith Corrections\n0\n25M\n50M\nEnv Steps\n0\n3\n6\nWith Dynamics\nR2D2 (token embed)\nIMPALA (token embed)\nR2D2 (sentence embed)\nIMPALA (sentence embed)\n(a) HomeGrid.\n0\n500K 1M 1.5M\nEnv Steps\n0.5\n0.0\n0.5\n1.0\nScore\nStage 1\n0\n10M 20M 30M\nEnv Steps\n0.5\n0.5\n1.5\nStage 2\n0\n25M\n50M\nEnv Steps\n0.5\n0.5\n1.5\nStage 3\nR2D2 (token embed)\nIMPALA (token embed)\nR2D2 (sentence embed)\nIMPALA (sentence embed)\n(b) Messenger.\nFigure G.1: Token vs. sentence embedding performance for IMPALA and R2D2 on all tasks, averaged\nacross 3 seeds. Sentence embeddings help R2D2 perform better on Messenger S1 and S2 but does\nnot help consistently across tasks and methods.\n21\nG.2\nModel Scaling for Baselines\n0\n500K 1M 1.5M\nEnv Steps\n1.0\n0.5\n0.0\n0.5\n1.0\nScore\nStage 1\n0\n10M\n20M\nEnv Steps\n1.0\n0.5\n0.0\n0.5\n1.0\nStage 2\nR2D2 (1.7M)\nR2D2 (10M)\nR2D2 (37M)\n(a) R2D2.\n0\n500K 1M 1.5M\nEnv Steps\n1.0\n0.5\n0.0\n0.5\n1.0\nScore\nStage 1\n0\n10M\n20M\nEnv Steps\n1.0\n0.5\n0.0\n0.5\n1.0\nStage 2\nIMPALA (1.5M)\nIMPALA (8.8M)\nIMPALA (34M)\n(b) IMPALA.\nFigure G.2: Model scaling curves for R2D2 and IMPALA. Performance does not increase with larger\nmodel sizes. Stage 2 runs were initialized from scratch.\nG.3\nAuxiliary Reconstruction Loss for Baselines\nWe tried adding an auxiliary loss for reconstructing the visual and language observations at the current\ntimestep. The loss was implemented by adding a linear layer that predicts each auxiliary target from\nthe LSTM hidden state. The loss used is MSE (for continuous values) or cross-entropy (for discrete\nlanguage vocab tokens). The auxiliary loss was added to the RL loss with a loss scale of 1. This did\nnot meaningfully change performance.\n0.0\n1.5\n3.0\n4.5\nEnv Steps 1e7\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\nScore\nHomeGrid + Dynamics\n0.0\n0.8\n1.6\n2.4\nEnv Steps 1e7\n0.8\n0.4\n0.0\n0.4\n0.8\nMessenger Stage 2\nR2D2\nR2D2 + recon loss\nFigure G.3: Model-free R2D2 performance with an auxiliary reconstruction loss.\n22\nG.4\nBaseline Hyperparameters\nHomeGrid\nMsgr S1\nMsgr S2\nMsgr S3\nVLN\nTotal model parameters\n27M\n10M\n10M\n10M\n10M\nLanguage inputs\nOne-hot\nT5 Embed\nT5 Embed\nT5 Embed\nT5 Embed\nVocabulary size\n32100\nn/a\nn/a\nn/a\nn/a\nLanguage MLP layers\n1\n1\n1\n1\n1\nLanguage MLP units\n512\n512\n512\n512\n512\nImage input\nPixel\nSymbol\nSymbol\nSymbol\nPixel\nImage size\n(64, 64, 3)\n(16, 16, 17)\n(16, 16, 17)\n(16, 16, 17)\n(64, 64, 3)\nReplay ratio\n7\n7\n7\n7\n7\nBatch size\n32\n64\n16\n16\n8\nUnroll length\n100\n100\n100\n100\n100\nLSTM recurrent units\n1024\n1024\n1024\n1024\n1024\nLearning rate\n4.8e-4\n4.8e-4\n4.8e-4\n4.8e-4\n4.8e-4\nBuffer Size\n1000\n1000\n1000\n1000\n1000\nEnv steps\n50M\n1M\n25M\n50M\n30M\nNumber of envs\n80\n80\n80\n80\n5\nTable G.1: Model hyperparameters and training information for the R2D2 baseline.\nHomeGrid\nMsgr S1\nMsgr S2\nMsgr S3\nTotal model parameters\n10M\n9M\n9M\n9M\nLanguage inputs\nOne-hot\nT5 Embed\nT5 Embed\nT5 Embed\nVocabulary size\n32100\nn/a\nn/a\nn/a\nLanguage MLP layers\n1\n1\n1\n1\nLanguage MLP units\n512\n512\n512\n512\nImage input\nPixel\nSymbol\nSymbol\nSymbol\nImage size\n(64, 64, 3)\n(16, 16, 17)\n(16, 16, 17)\n(16, 16, 17)\nBatch size\n16\n64\n64\n64\nLSTM recurrent units\n1024\n1024\n1024\n1024\nLearning rate\n3e-4\n3e-4\n3e-4\n3e-4\nEnv steps\n50M\n1M\n25M\n50M\nNumber of envs\n80\n80\n80\n80\nTable G.2: Model hyperparameters and training information for the IMPALA baseline.\n23\nG.5\nDynalang Hyperparameters\nWe use the default model hyperparameters for the XL DreamerV3 model unless otherwise specified\nbelow. For VLN, we use a larger GRU deterministic state and a bottleneck layer of size 1024 between\ntimesteps. To process both one-hot and embedding language inputs, we use a 5-layer MLP with 1024\nMLP units in each layer. All models were trained on NVIDIA A100 GPUs.\nHomeGrid\nMsgr S1\nMsgr S2\nMsgr S3\nVLN\nLangRoom\nTotal model parameters\n281M\n148M\n148M\n148M\n268M\n243M\nLanguage inputs\nOne-hot\nT5 Embed\nT5 Embed\nT5 Embed\nT5 Embed\nOne-hot\nVocabulary size\n32100\nn/a\nn/a\nn/a\nn/a\n15\nLanguage MLP layers\n5\n5\n5\n5\n5\n5\nLanguage MLP units\n1024\n1024\n1024\n1024\n1024\n1024\nImage input\nPixel\nSymbol\nSymbol\nSymbol\nPixel\nPixel\nImage size\n(64, 64, 3)\n(16, 16, 17)\n(16, 16, 17)\n(16, 16, 17)\n(64, 64, 3)\n(64, 64, 3)\nTrain ratio\n32\n64\n64\n32\n32\n16\nBatch size\n16\n16\n24\n24\n8\n16\nBatch length\n256\n256\n512\n512\n256\n64\nGRU recurrent units\n4096\n4096\n4096\n4096\n8192\n6144\nBottleneck units\nn/a\nn/a\nn/a\nn/a\n1024\n2048\nEnv steps\n50M\n1M\n25M\n50M\n30M\n45M\nNumber of envs\n66\n16\n16\n66\n8\n4\nTraining time (GPU days)\n3.75\n2.5\n16\n24\n16\n2\nTable G.3: Dynalang hyperparameters and training information for each environment.\n24\n"
  },
  {
    "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
    "link": "https://arxiv.org/pdf/2308.01390.pdf",
    "upvote": "30",
    "text": "OpenFlamingo: An Open-Source Framework for Training\nLarge Autoregressive Vision-Language Models\nAnas Awadalla\u22171\nIrena Gao\u22172\nJosh Gardner1\nJack Hessel3\nYusuf Hanafy1\nWanrong Zhu5\nKalyani Marathe1\nYonatan Bitton6\nSamir Gadre7\nShiori Sagawa2\nJenia Jitsev4,9\nSimon Kornblith8\nPang Wei Koh1,8\nGabriel Ilharco1\nMitchell Wortsman1\nLudwig Schmidt1,3,4\nAbstract\nWe introduce OpenFlamingo, a family of au-\ntoregressive vision-language models ranging from\n3B to 9B parameters. OpenFlamingo is an on-\ngoing effort to produce an open-source replica-\ntion of DeepMind\u2019s Flamingo models [3].\nOn\nseven vision-language datasets, OpenFlamingo\nmodels average between 80 - 89% of correspond-\ning Flamingo performance.\nThis technical re-\nport describes our models, training data, hy-\nperparameters, and evaluation suite. We share\nour models and code at https://github.com/\nmlfoundations/open_flamingo.\n1\nIntroduction\nA popular format for vision and language mod-\nels is (image, text) \u2192 text, i.e., models take as\ninput an image and some text, and produce text\nas output, e.g., BLIP-2 [22].The flexible format\ndirectly supports tasks like image classification\nand visual question answering (VQA).\nHowever, assuming a single image as input is\nlimiting: autoregressive vision-language models\nenable new capabilities by instead mapping an\narbitrarily interleaved sequence of images and\n*Equal\ncontribution.\n1University\nof\nWashington\n2Stanford\nUniversity\n3Allen\nInstitute\nfor\nAI\n4LAION\n5University of California Santa Barbara\n6Hebrew Univer-\nsity 7Columbia University 8Google DeepMind 9Juelich Su-\npercomputing Center, Research Center Juelich.\nCorrespon-\ndence to <anasa2@cs.washington.edu, irena@cs.stanford.edu,\nschmidt@cs.washington.edu>.\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nOpenFlamingo model\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n110%\nPercentage of Flamingo performance\nOpenFlamingo average performance compared to Flamingo\nFigure 1: OpenFlamingo performance as a fraction of\ncorresponding Flamingo performance, averaged across\nevaluation settings (7 datasets \u00d7 5 options for number\nof in-context examples). Demonstrations are chosen\nusing RICES (Retrieval-based In-Context Example\nSelection). More details regarding selecting demon-\nstrations can be found in Section 3.4. We compare\nOpenFlamingo-3B and -4B models to Flamingo-3B,\nand OpenFlamingo-9B to Flamingo-9B. Error bars\nare standard deviations over settings. \u201cOF-3B (I)\u201d\nrefers to OpenFlamingo-3B (Instruct), the 3B model\ntrained with a language-instruction-tuned backbone.\ntext to textual outputs. This interface provides\nimportant flexibility: the input sequence can in-\nclude demonstrations for a new task, enabling few-\nshot, in-context learning [3] or multi-round multi-\nmodal chatbot interactions. Evaluations suggest\nthat autoregressive vision-language models can\nbe performant foundation models [5]: models like\nFlamingo [3], CM3 [1], Kosmos-1 [12], PALM-\nE [8], and multimodal GPT-4 [28] generalize well\nacross diverse vision-language tasks.\narXiv:2308.01390v2  [cs.CV]  7 Aug 2023\nOutput: Two \ncats are \nsleeping next \nto each other \non a sofa.\nOutput: A \nracoon \nwearing a \nspacesuit.\nAn apple with the \nword \"iPod\" \nwritten on it.\n\ud83d\udc64 Input Prompt\n\ud83e\udda9Completion\nOutput: \n\u201cUnderground\u201d\nOutput: \n\u201cPike Pl\u201d\n\u201cRed Brick Pizza\u201d\nTesla Model 3.\nOutput: \nOutput: \nQuestion: \nWhich video \ngame is \nrepresented in \nthe image? \nAnswer: \nAmong Us.\nQuestion: \nWhat latte art \nis presented in \nthe image? \nAnswer: A \nswan.\nQuestion: \nWhat car is \nfeatured in the \nimage? \nAnswer:\nFigure 2: OpenFlamingo-9B (pictured) can process interleaved image-and-text sequences. This interface\nallows OpenFlamingo to learn many vision-language tasks through in-context demonstrations.\nUnfortunately,\nthese\nautoregressive\nvision-\nlanguage models are closed-source, and their\nweights, training data, code, and hyperparam-\neters are proprietary. This limits the academic\ncommunity\u2019s ability to conduct research on au-\ntoregressive vision-language models, e.g., to un-\nderstand how web-scraped image-text data affects\nmodels\u2019 performance and safety. Open-source al-\nternatives, such as LLaVA [25], LLaMA-Adapter\n[41], BLIP-2 [23], and mPLUG-Owl [39], only\ntake in single images, and they often directly\ntrain on curated datasets like COCO [24] rather\nthan web data.\nIn this technical report, we document our expe-\nriences building an open-source reproduction of\nthe Flamingo models [3]. Following Flamingo, we\naugment the layers of pretrained, frozen language\nmodels so that they cross attend to the outputs\nof a frozen vision encoder while predicting the\nnext token. The cross-modal module is trained\non web-scraped image-text sequences, in our case,\ntwo open source datasets: LAION-2B [32] and\nMultimodal C4 [45].\nOur stack is built using\npublicly available components, including CLIP as\na vision encoder [30] and open-source language\nmodels as decoders [27, 35].\nWe call the resulting family of five models\nOpenFlamingo.\nThese models range from 3B\nto 9B parameters, with both standard and\ninstruction-tuned [37] language model backbones.\nWhen averaging performance across 7 evalua-\ntion datasets, OpenFlamingo-3B and -9B mod-\nels attain 85% and 89% of their corresponding\nFlamingo models respectively (Figure 1). Models\nand code are open-sourced at https://github.\ncom/mlfoundations/open_flamingo.\n2\nRelated work\nGenerative vision-language models output\ntext conditioned on an image-text sequence.\nWhile many such architectures, such as BLIP-\n2\nTable 1: Architecture details of the OpenFlamingo models. All five models use a CLIP ViT-L/14 vision\nencoder [30]. A cross-attention interval of 4 means that a cross-attention module is inserted every 4th\nlanguage model layer. Note that OpenFlamingo models labeled (Instruct) use language models that were\nfinetuned on language-only tasks; we have not instruction-tuned OpenFlamingo models on vision-language\ntasks.\nModel\nLanguage model\nCross-attention\ninterval\n<image>\nand\n<|endofchunk|>\nOpenFlamingo-3B\nMPT-1B [27]\n1\nTrainable\nOpenFlamingo-3B (Instruct)\nMPT-1B (Instruct) [27]\n1\nTrainable\nOpenFlamingo-4B\nRedPajama-3B [35]\n2\nFrozen\nOpenFlamingo-4B (Instruct)\nRedPajama-3B (Instruct) [35]\n2\nFrozen\nOpenFlamingo-9B\nMPT-7B [27]\n4\nTrainable\n2 and LLaVa, can incorporate only one image in\ntheir context [6, 16, 22, 25, 39, 41], autoregressive\nvision-language models accept interleaved image-\ntext sequences, enabling in-context learning.\nWe chose to replicate Flamingo because of its\nstrong in-context learning abilities. Aggregated\nacross evaluation sets, Flamingo models see\nsteady performance improvements up to 32 in-\ncontext examples [3]. This is in contrast with\nother autoregressive vision-language models, for\nexample Kosmos-1 [12]; on captioning tasks\nCOCO [24] and Flickr-30K [29], Kosmos-1 shows\nperformance improvements up to 4 in-context ex-\namples, but performance degrades when using 8\nin-context examples.\nOpen-source image-text datasets.\nPropri-\netary autoregressive vision-language models are\ntypically trained on closed-source datasets [1, 3,\n8, 12]. For example, Flamingo relies on image-\ntext pairs from the ALIGN dataset [14] and in-\nterleaved image-text sequences from the M3W\ndataset [3]; both are unavailable to the pub-\nlic. Recent efforts to replicate these web-scraped\ndatasets include LAION-2B, a dataset of image-\ntext pairs, and Multimodal C4 [45] and OBELISC\n[18], datasets of image-text sequences. We use\nLAION-2B and Multimodal C4 for training Open-\nFlamingo models.\nLauren\u00b8con et al. [18] also\ntrain 9B and 80B Flamingo-style models; their\nmodels differ in the choice of pretraining dataset\n(OBELISC instead of Multimodal C4) and lan-\nguage model (LLaMA-9B [41] instead of the MPT\nand RedPajama-3B models [27, 35]).\n3\nApproach\n3.1\nArchitecture\nWe match the Flamingo architecture [3]. Given\nan interleaved sequence of images with text to-\nkens, OpenFlamingo models predict the next text\ntoken conditioned on all previous text tokens and\nthe last preceding image. Text tokens attend\nto their corresponding images via dense cross-\nattention modules, which we attach to the layers\nof a frozen, autoregressive language model. To\nembed images, we extract patch features from a\nfrozen vision encoder and pass these through a\ntrainable Perceiver resampler [13].\nAs a preprocessing step, we first mark the loca-\ntions of images in the text sequence with <image>\ntokens. We also insert <|endofchunk|> tokens af-\nter the text tokens following an image; e.g. the\nsequence x Hello world, where x is an image,\nwould be preprocessed into <image> Hello world\n<|endofchunk|> .\nUnlike Flamingo, we do not support video inputs\nat this time. We leave this for future work.\nTable 1 describes the five OpenFlamingo mod-\nels based on their language model and density\n3\n(A) LAION-2B\n(B) Multimodal C4\n(C) ChatGPT-generated data\nGolden Week (\u30b4\u30fc\u30eb\u30c7\u30f3\u30a6\u30a3\u30fc\u30af ) is one of the busiest holidays in Japan.\nthat occur within seven days every spring. Combined with weekends, the holidays \nallow for almost the entire nation to take time o\ufb00 work and travel, making it the \nlongest vacation period of the year for most Japanese employees. Transportation \nprices soar, hotels book up in advance, and whole towns\u2019 populations seem to \ntravel around and even outside of Japan. Sh\u014dwa Day is a Japanese [...] \nMilk comes from cows.\nEggs come from chickens.\nGolden Week refers to a collection of four national Japanese  holidays\nGolden Week is the most popular time for Japanese people to \nManuelina Culinary Pasta Program \nempty out.\nFigure 3: Samples from (A) LAION-2B [32], (B) Multimodal C4 [45], and (C) ChatGPT-generated data.\nTable 2: Statistics for training datasets. \u201cChatGPT\u201d\nstands for the ChatGPT-generated sequences. The\nmedian numbers of images and tokens per sequence\nwere calculated using a random sample of 1,000 se-\nquences.\nDataset\nMedian images\nper sequence\nMedian tokens\nper sequence\nLAION-2B\n1\n17\nMMC4\n2\n256\nChatGPT\n3\n56\nof cross-attention layers; all models use CLIP\nViT-L/14 [30] as a vision encoder.\nIn most\ncases, the <image> and <|endofchunk|> embed-\ndings are trainable, while other text embeddings\nare frozen. For the OpenFlamingo-4B models, all\nembeddings are frozen, including the randomly\ninitialized <image> and <|endofchunk|> embed-\ndings. This was due to complications with gra-\ndient masking when using Fully Sharded Data\nParallel (\u00a73.3).\n3.2\nTraining data\nWe train our models on a mixture of image-text\npairs and interleaved image-text sequences. Dur-\ning training, we sample dataset shards with re-\nplacement using the WebDataset format [34].\n0\n50\n100\n150\n200\n250\n0\n20\n40\n% of sequences\nTokens per sequence\n1\n2\n3\n4\n5\n6\n0\n10\n20\n30\n% of sequences\nImages per sequence\nFigure 4: Histograms of the number of text tokens\nand images per MMC4 sequence, based on a sample of\n1,000 sequences. Sequences are long with few images.\nLAION-2B [32].\nWhen training Flamingo,\nAlayrac et al. [3] use ALIGN [14], a closed-source\ndataset of over 1B single images paired with short\nalt-text captions. To train OpenFlamingo, we\nreplace ALIGN with LAION-2B, an open-source\nweb-scraped dataset consisting of 2B image-text\npairs (Figure 3A). We use part of the English sub-\nset and truncate captions to 32 tokens. All image-\ntext pairs in LAION-2B have a cosine similarity\nof at least 0.28 according to CLIP ViT-B/32.\n4\nMultimodal C4 [45].\nIn addition to image-\ntext pairs, Alayrac et al. [3] train Flamingo using\nM3W, an internal web-scraped dataset of 43M in-\nterleaved image-text sequences. We replace M3W\nwith Multimodal C4 (MMC4), an open-source\ndataset of 101M interleaved samples (Figure 3B).\nUnlike M3W or OBELISC [18], which directly\nparse HTML documents to extract multimodal\nsequences, MMC4 uses CLIP to soft align images\nwith sentences in a document. To ensure data\nquality, we exclude images if their cosine simi-\nlarity with the subsequent text falls below 0.24,\naccording to CLIP ViT-L/14. Sequences contain\nbetween 1 and 6 images (median 2). To encour-\nage learning from sequences with multiple images,\nwe reject single-image sequences with probability\n0.5. The resulting distribution is shown in Figure\n4. Additional notes on MMC4 filtering are in\nAppendix B.\nSynthetic data.\nFor the OpenFlamingo-4B\nmodels, we also experimented with training on\nChatGPT-generated synthetic data (Figure 3C)\nThese 417K image-text sequences were generated\nby prompting ChatGPT to generate a sequence\nof interleaved text and image alt-texts (in place of\nimages). The alt-texts are used to retrieve a cor-\nresponding images from LAION-5B. Additional\ndetails of the prompting and data construction\nprocess are described in Appendix C. The median\nnumber of images per sequence is higher than in\nMMC4, while the median number of text tokens\nis lower (Table 2). We release these sequences\nthrough the OpenFlamingo repository.\n3.3\nTraining details\nOpenFlamingo models were trained for 60M inter-\nleaved (MMC4) examples1 and 120M LAION-2B\nexamples. All models are trained using the next-\ntoken prediction objective and optimized with\n1OpenFlamingo-4B models use both MMC4 and\nChatGPT-generated data as interleaved sequences; 60M\ninterleaved examples translates to approximately 240K\nChatGPT-generated sequences and 59.8M MMC4 se-\nquences. Other models train on 60M MMC4 examples.\nTable 3: Training used either DistributedDataParallel\n(DDP) or FullyShardedDataParallel (FSDP) [43].\nModel\nGPU type\nSharding\nstrategy\nPrecision\nOF-3B\nA100-80GB\nDDP\nfp32\nOF-3B (I)\nA100-40GB\nDDP\nfp32\nOF-4B\nA100-40GB\nFSDP\nfp32\nOF-4B (I)\nA100-40GB\nFSDP\nfp32\nOF-9B\nA100-80GB\nDDP\namp bf16\nAdamW. The learning rate is linearly increased at\nthe beginning of training, and then held constant\nat 1e-4 throughout training. We apply weight\ndecay of 0.1 on the dense cross attention layers.\nThe batch size for LAION-2B is twice the batch\nsize of the interleaved dataset (MMC4, optionally\nwith ChatGPT-generated sequences), and the\nloss weights are set to Flamingo defaults of 1 and\n0.2 for MMC4 and LAION-2B respectively. We\naccumulate gradients over both datasets between\noptimizer steps.\nDistributed training.\nWe train all models\nusing 64 GPUs distributed across 8 nodes on\nStabilty AI\u2019s cluster (Table 3). OpenFlamingo-4B\nmodels were trained using model sharding with\nFully Sharded Data Parallel [43]; other models\nwere trained using only data parallel.\nLoss curves.\nFigure 5 tracks LAION-2B and\nMMC4 loss over the course of training. After an\ninitial improvement, MMC4 loss decreases very\nslowly. We speculate that, since MMC4 sequences\ntend to include long paragraphs between images\n(Figure 2), most text tokens can be generated\nwithout referencing the image. Thus, the loss may\nbe dominated by whether the frozen language\nmodel can fit unrelated paragraphs of text.\n3.4\nEvaluation method\nWe evaluate OpenFlamingo on seven vision-\nlanguage datasets including captioning (COCO\n[7], Flickr-30K [40]), visual question answer-\n5\n0\n10M\n20M\n30M\n40M\n50M\n60M\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nMMC4 Loss\n0\n20M\n40M\n60M\n80M\n100M 120M\nNumber of samples\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nLAION Loss\nOpenFlamingo-9B\nOpenFlamingo-4B (Instruct)\nOpenFlamingo-3B\nFigure 5: MMC4 and LAION-2B language modeling\nloss throughout training. Curves shown with Gaussian\nsmoothing with window size 100.\ning (VQAv2 [2], OK-VQA [26], TextVQA [33],\nVizWiz [11]), and rank classification (Hateful-\nMemes [15]). For each dataset, we measure per-\nformance at 0, 4, 8, 16, and 32 in-context exam-\nples. Evaluation was done in automatic mixed\nprecision, with linear layers computed in bfloat16.\nSelecting in-context examples.\nFor each\nevaluation example, we sample in-context exam-\nples from the training split uniformly at random.\nAdditionally, in Appendix A.2, we include eval-\nuations of OpenFlamingo using Retrieval-based\nIn-Context Example Selection (RICES) [38].\nEvaluation\nsubsets.\nWe evaluate on the\ndataset splits used by Alayrac et al. [3].\nWe\nrun each evaluation across three seeds, where the\nrandomness is over selected in-context demon-\nstrations, and average the results to obtain our\nfinal scores.\nPrompts.\nFor captioning tasks, we format\ndemonstrations as <image> Output: [caption], re-\nplacing [caption] with the ground-truth caption.\nFor VQA, we format examples as <image> Ques-\ntion: [question] Short answer: [answer].\nFor\nHatefulMemes,\nwe prompt the model with\n<image> is an image with: \u2018[text]\u2019 written on\nit. Is it hateful? Answer: [answer].\nFollowing Alayrac et al. [3], we prompt the model\nwith two in-context examples during zero-shot\nevaluations, removing their images, and for classi-\nfication tasks, we implement prompt ensembling\nby averaging logits across 6 permutations of the\nin-context examples.\nDecoding parameters.\nWe evaluate caption-\ning and VQA using beam search with 3 beams,\nstopping generation at 20 tokens for captioning, 5\ntokens for VQA, or whenever the model produces\nan <|endofchunk|> token. For HatefulMemes, we\ncompute the log-likelihood of completions \u201cyes\u201d\nand \u201cno\u201d and answer with the most likely com-\npletion.\nMetrics.\nFor\ncaptioning,\nwe\nuse\nCIDEr\nscore [36]. For VQA, we report VQA accuracy,\ni.e., exact match accuracy over a set of ground\ntruth answers [2]. For HatefulMemes, we com-\npute AUC ROC.\n4\nResults\nIn Table 4, we compare OpenFlamingo and\nFlamingo models across 0, 4, and 32 in-context\nexamples. On average, OpenFlamingo-3B, -3B\n(Instruct), -4B (Instruct), and -9B attain more\nthan 86% of the performance of their correspond-\ning Flamingo models (Figure 1).\nIn the 0- and 4-shot regimes, OpenFlamingo mod-\nels approach or match Flamingo performances on\n6\n0 4 8\n16\n32\n80\n90\n100\nCIDEr\nCOCO\n0 4 8\n16\n32\n50\n55\n60\n65\n70\nCIDEr\nFlickr30K\n0 4 8\n16\n32\n45\n50\n55\n60\n65\nROC AUC\nHatefulMemes\n0 4 8\n16\n32\n30\n40\n50\nVQA Accuracy\nOK-VQA\n0 4 8\n16\n32\n15\n20\n25\n30\nVQA Accuracy\nTextVQA\n0 4 8\n16\n32\n45\n50\n55\n60\nVQA Accuracy\nVQAv2\n0 4 8\n16\n32\n20\n30\n40\nVQA Accuracy\nVizWiz\n0 4 8\n16\n32\n45\n50\n55\n60\nAveraged scores\n(Average)\nNumber of in-context examples\nEvaluations with random demonstrations\nFlamingo-3B\nFlamingo-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nFigure 6: Evaluation results per dataset across 0, 4, 8, 16, and 32 in-context examples. Each point is the\naverage across three evaluation runs, where the randomness is over choice of in-context demonstrations. Error\nbars are standard deviations over random seeds. Results are reported in tabular form in Table 11.\nseveral datasets. For example, OpenFlamingo-\n9B improves upon Flamingo-9B\u2019s 0-shot perfor-\nmance on VQAv2 (51.8% \u2192 52.7% VQA ac-\ncuracy) and COCO (79.4 \u2192 79.5 CIDEr), and\nOpenFlamingo-9B approaches Flamingo-9B\u2019s 0-\nshot performance on Flickr-30K and VizWiz.\nMoreover, OpenFlamingo-9B approaches the 4-\nshot performance of Flamingo-9B on COCO,\nVQAv2, and VizWiz.\nHowever, on OK-VQA and TextVQA, Open-\nFlamingo models are notably weaker than their\nFlamingo counterparts: OpenFlamingo-9B un-\nderperforms Flamingo-9B in 0-shot evaluations\nby 6.9 percentage points on OK-VQA and 7.8 per-\ncentage points on TextVQA. OpenFlamingo-3B\nalso underperforms Flamingo-3B by 4.6 percent-\nage points in 0-shot VQAv2 accuracy. The reason\nfor generally low VQA performance is unclear,\nalthough discussions in \u00a75.2 may be related.\nExtrapolating to more in-context examples.\nIn Figure 6, we plot performance as a function\nof the number of in-context examples. We ob-\nserve that the OpenFlamingo-3B and -9B models\ngenerally improve with the number of in-context\nexamples.\nHowever, the rate of improvement\nis lower than the Flamingo models: in the bot-\ntom right corner of Figure 6, we observe that\ngaps between OpenFlamingo-9B and Flamingo-\n9B widen with the number of in-context examples.\nWe speculate that this behavior may stem from\nthe quality of our pre-training data, which mostly\nconsists of sequences with few images (Table 2).\nIn contrast with the -3B and -9B models, which\ngenerally improve with more in-context examples,\nthe OpenFlamingo-4B models unexpectedly de-\ngrade in performance after 4 or 8 shots. The\n4B models use RedPajama language models [35]\ninstead of MPT backbones [27]; they also use\nfrozen <image> and <|endofchunk|> embeddings.\nWe investigate the effect of the latter in \u00a75.1.\nTrends by model size.\nOpenFlamingo-9B\ngenerally outperforms smaller models, except on\n7\nBenchmark\nShots\nFl-3B\nFl-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nCOCO [7]\n0\n73.0\n79.4\n74.9 (0.2)\n74.4 (0.6)\n76.7 (0.2)\n81.2 (0.3)\n79.5 (0.2)\n4\n85.0\n93.1\n77.3 (0.3)\n82.7 (0.7)\n81.8 (0.4)\n85.8 (0.5)\n89.0 (0.3)\n32\n99.0\n106.3\n93.0 (0.6)\n94.8 (0.3)\n95.1 (0.3)\n99.2 (0.3)\n99.5 (0.1)\nFlickr-30K [40]\n0\n60.6\n61.5\n52.3 (1.0)\n51.2 (0.2)\n53.6 (0.9)\n55.6 (1.3)\n59.5 (1.0)\n4\n72.0\n72.6\n57.2 (0.4)\n59.1 (0.3)\n60.7 (1.2)\n61.2 (0.5)\n65.8 (0.6)\n32\n71.2\n72.8\n61.1 (1.3)\n64.5 (1.3)\n56.9 (0.7)\n53.0 (0.5)\n61.3 (0.7)\nVQAv2 [2]\n0\n49.2\n51.8\n44.6 (0.0)\n44.1 (0.1)\n45.1 (0.1)\n46.9 (0.0)\n52.7 (0.2)\n4\n53.2\n56.3\n45.8 (0.0)\n45.7 (0.1)\n49.0 (0.0)\n49.0 (0.0)\n54.8 (0.0)\n32\n57.1\n60.4\n47.0 (0.1)\n44.8 (0.1)\n43.0 (0.2)\n47.3 (0.0)\n53.3 (0.1)\nOK-VQA [26]\n0\n41.2\n44.7\n28.2 (0.2)\n28.7 (0.1)\n30.7 (0.1)\n31.7 (0.1)\n37.8 (0.2)\n4\n43.3\n49.3\n30.3 (0.5)\n30.6 (0.2)\n35.1 (0.0)\n34.6 (0.0)\n40.1 (0.1)\n32\n45.9\n51.0\n31.0 (0.1)\n30.6 (0.1)\n26.4 (0.2)\n34.7 (0.3)\n42.4 (0.0)\nTextVQA [33]\n0\n30.1\n31.8\n24.2 (0.2)\n23.1 (0.2)\n21.0 (0.3)\n21.1 (0.4)\n24.2 (0.5)\n4\n32.7\n33.6\n27.0 (0.3)\n28.1 (0.4)\n25.9 (0.0)\n27.2 (0.3)\n28.2 (0.4)\n32\n30.6\n32.6\n28.3 (0.2)\n28.5 (0.1)\n14.1 (0.2)\n23.2 (0.2)\n23.8 (0.2)\nVizWiz [11]\n0\n28.9\n28.8\n23.7 (0.5)\n23.4 (0.3)\n18.8 (0.1)\n21.5 (0.2)\n27.5 (0.2)\n4\n34.0\n34.9\n27.0 (0.3)\n27.7 (0.1)\n26.6 (0.5)\n26.5 (0.4)\n34.1 (0.7)\n32\n45.5\n44.0\n39.8 (0.1)\n39.3 (0.4)\n23.1 (1.1)\n31.3 (0.2)\n44.0 (0.5)\nHatefulMemes [15]\n0\n53.7\n57.0\n51.2 (2.5)\n50.1 (2.2)\n52.3 (2.3)\n53.1 (2.2)\n51.6 (1.8)\n4\n53.6\n62.7\n50.6 (0.8)\n49.5 (0.6)\n51.5 (1.4)\n54.9 (1.1)\n54.0 (2.0)\n32\n56.3\n63.5\n50.2 (1.8)\n47.8 (2.2)\n52.2 (1.2)\n54.9 (1.1)\n53.8 (2.1)\nTable 4: Evaluation results across seven vision-language datasets using 0, 4, and 32 in-context examples.\n\u201cOF-3B (I)\u201d refers to OpenFlamingo-3B (Instruct), the 3B model trained with a language-instruction-tuned\nbackbone, while \u201cFl-3B\u201d refers to Flamingo-3B. Flamingo results taken from Alayrac et al. [3]. The highest\nnumber in each row is bolded. Full results (including 8- and 16-shot performance) are in Table 11.\nHatefulMemes and for large numbers of in-context\nexamples on Flickr-30K and TextVQA. However,\nOpenFlamingo-4B models often underperform\nthe smaller 3B models, including on Flickr-30K,\nHatefulMemes, TextVQA, and VizWiz.\nEffect of language instruction-tuning.\nWe\ntrain two OpenFlamingo models at each of the\n3B and 4B scales: one model using a base lan-\nguage model, and one with an instruction-tuned\nvariant of the same language model.\nIn the\nlower right corner of Figure 6, we observe that\nthe instruction-tuned variants of MPT-1B and\nRedPajama-3B on average outperform the base\nmodels. The difference is starkest for RedPajama-\n3B. Transfer of language instruction tuning to\nvision-language tasks was previously reported\nin Huang et al. [12], Li et al. [23].\nComparison to fine-tuned state-of-the-art.\nFigure 7 plots each model\u2019s performance rela-\nCOCO\nFlickr30K\nVQAv2\nOK-VQA\nTextVQA\nVizWiz HatefulMemes\nEvaluation dataset\n0%\n20%\n40%\n60%\n80%\n100%\n% of fine-tuned SoTA\nFlamingo-9B\nOpenFlamingo-9B\nFigure 7: OpenFlamingo-9B and Flamingo-9B perfor-\nmance relative to fine-tuned SoTA performance.\ntive to fine-tuned state-of-the-art performance,\nas listed on Papers With Code on June 19, 2023.\nOpenFlamingo-9B averages more than 62% of\nfine-tuned state-of-the-art performance with 32\nRICES-selected in-context examples, compared\nto 72% achieved by Flamingo-9B. For more de-\ntails on the fine-tuned SoTAs, see Appendix A.1.\n8\n0-shot\n4-shot\n8-shot\nCOCO\ntrainable\n46.5\n58.6\n61.2\nfrozen\n41.9 (\u22124.6)\n54.5 (\u22124.1)\n57.4 (\u22123.8)\nVQAv2\ntrainable\n17.6\n23.2\n28.7\nfrozen\n5.5 (\u221212.1)\n8.4 (\u221214.8)\n18.8 (\u22129.9)\nTable 5: COCO and VQAv2 validation performance\nwhen using trainable <image> and <|endofchunk|>\nembeddings compared to frozen, randomly initialized\nembeddings. The model used in this experiment is\nbased on CLIP ViT-L/14 and OPT 125M, with cross-\nattention every layer, and trained on 20M interleaved\nsamples, including ChatGPT-sequences.\n5\nDiscussion\n5.1\nFrozen embeddings\nIn \u00a74, we observed that OpenFlamingo-4B mod-\nels underperform their 3B counterparts on most\ndatasets. One notable way the OpenFlamingo-\n4B models differ from the 3B and 9B models is\nthat their <image> and <|endofchunk|> embed-\ndings are randomly initialized and frozen, rather\nthan trained.\nIn Table 5, we investigate the effect of this differ-\nence. We train small models using OPT-125M as\na language model [42] to 20M interleaved samples\n(one-third of full training). Freezing the <image>\nand <|endofchunk|> embeddings results in a drop\nof 4.6 CIDEr for 0-shot COCO, and 12.1% ac-\ncuracy for 0-shot VQAv2. This suggests that\nfrozen <image> and <|endofchunk|> embeddings\nmay impact downstream trends.\n5.2\nVQAv2 validation trends\nDuring development, we used the VQAv2 valida-\ntion set as a temperature check for visual question\nanswering capabilities. In this section, we discuss\ntrends observed during development.\nTraining dynamics.\nTo understand how eval-\nuation performance evolves over the course of\ntraining, Figure 8 plots validation performance\nof OpenFlamingo-9B on COCO and VQAv2\nthroughout training. While COCO performance\nsteadily improves, VQAv2 progress is flatter.\n5K\n10K\n15K\n20K\nSteps\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nCIDEr score\nCOCO\n0-shot\n4-shot\n5K\n10K\n15K\n20K\nSteps\n0\n10\n20\n30\n40\n50\n60\nVQA Accuracy\nVQAv2\n0-shot\n4-shot\nFigure\n8:\nValidation\nsplit\nperformance\nfor\nOpenFlamingo-9B across training:\nwhile COCO\nCIDEr improves throughout training, VQAv2 per-\nformance is more stagnant.\nThis matches trends reported by Li et al. [23].\nEffect of language model.\nAlthough addi-\ntional training did not dramatically affect VQAv2\nperformance, changing language model backbones\ndid. Table 7 illustrates this effect on the VQAv2\nvalidation split; notably, switching from OPT-\n1.3B to MPT-1B (Instruct) added nearly 10 per-\ncentage points in 0-shot performance. We hy-\npothesize that the language model has similarly\nlarge effects for other VQA tasks.\nCommon VQA failure modes (Table 6).\nOpenFlamingo models struggle with counting; on\nthe VQAv2 validation split, OpenFlamingo-9B\nscores 30.5% on questions with numerical an-\nswers, compared to 70.6% on yes / no questions.\nAdditionally, because VQA accuracy uses an ex-\nact match criterion for generations, models must\nanswer concisely to score well; OpenFlamingo\nmodels are often too verbose. Finally, VQA ques-\ntions can ask about objects other than the central\nobject in the image; models sometimes answer\nabout the central item instead.\n5.3\nApplications of OpenFlamingo\nMultiple models have already developed on top\nof OpenFlamingo. Li et al. [20] fine-tuned Open-\nFlamingo on MIMIC-IT [19], a multi-image/video\ninstruction following dataset, creating Otter, a\n9\nCounting\nVerbosity\nNon-central object\nQ: How many people are on the sidewalk?\nQ: What is this sheep trying to do?\nQ: What color are the curtains?\nOF-9B: \u201cone\u201d\nOF-9B: \u201cit is trying to get\u201d\nOF-9B: \u201cgreen\u201d\nGround truth:\n{\u201c4\u201d, \u201c5\u201d}\nGround truth: {\u201cget out\u201d, \u201cescape\u201d}\nGround truth: {\u201cyellow\u201d, \u201cgold\u201d}\nTable 6: OpenFlamingo-9B errors from the VQAv2 validation split. Common failure modes for OpenFlamingo\nincluding counting, giving answers that are too verbose (and thus truncated), and answering about the central\nobject in the image rather than the non-central object in the question.\nVQAv2 validation\nShots\nLanguage model\n0\n4\nOPT-125M\n17.6\n23.2\nOPT-1.3B\n32.8\n27.2\nMPT-1B (Instruct)\n41.9\n43.7\nMPT-7B\n47.4\n49.4\nTable 7: VQAv2 validation performance at 20M in-\nterleaved samples across different language models.\nPerformance largely differs between language models.\nmultimodal assistant. Gong et al. [10] released\nMultimodal-GPT, an OpenFlamingo model in-\nstruction fine-tuned on both vision and language\ninstruction datasets. We hope the community\ncontinues to use OpenFlamingo models.\n5.4\nLimitations\nOpenFlamingo models carry the same risks as\ntheir foundational language models. In particular,\nthese models train on web-scraped data, and they\nhave not undergone safety-focused fine-tuning.\nModels thus may produce unexpected, inappro-\npriate, or inaccurate outputs. We hope to further\ninvestigate the safety properties of autoregressive\nvision-language models like OpenFlamingo.\n6\nConclusion\nIn this technical report, we described Open-\nFlamingo, a family of five autoregressive vision-\nlanguage models across the 3B, 4B, and 9B scales.\nOpenFlamingo remains an active research project,\nand we continue to work on training and releas-\ning high-quality autoregressive vision-language\nmodels. We hope our contribution enables more\nresearchers to train and study such models.\nAcknowledgements\nWe would like to thank Jean-Baptiste Alayrac and An-\ntoine Miech for their advice on reproducing Flamingo.\nWe also thank Rohan Taori, Nicholas Schiefer, Deep\nGanguli, Thomas Liao, Tatsunori Hashimoto, and\nNicholas Carlini for their help with assessing the safety\nrisks of our first release of OpenFlamingo. Thanks to\nStability AI for compute resources.\nReferences\n[1] Armen Aghajanyan, Po-Yao (Bernie) Huang,\nCandace Ross, Vladimir Karpukhin, Hu Xu, Na-\nman Goyal, Dmytro Okhonko, Mandar Joshi,\nGargi Ghosh, Mike Lewis, and Luke Zettlemoyer.\nCm3: A causal masked multimodal model of the\ninternet. arXiv preprint arXiv:2201.07520, 2022.\n[2] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol,\nMargaret Mitchell, C. Lawrence Zitnick, Devi\nParikh, and Dhruv Batra. Vqa: Visual question\n10\nanswering. International Journal of Computer\nVision, 123:4\u201331, 2015.\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline\nLuc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican,\nMalcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances\nin Neural Information Processing Systems, 35:\n23716\u201323736, 2022.\n[4] Romain Beaumont.\nClip retrieval:\nEasily\ncompute clip embeddings and build a clip re-\ntrieval system with them. https://github.com/\nrom1504/clip-retrieval, 2022.\n[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. On the oppor-\ntunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021.\n[6] Xi Chen, Xiao Wang, Soravit Changpinyo,\nAJ Piergiovanni, Piotr Padlewski, Daniel Salz, Se-\nbastian Goodman, Adam Grycner, Basil Mustafa,\nLucas Beyer, et al. Pali: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794, 2022.\n[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll\u00b4ar,\nand C. Lawrence Zitnick. Microsoft coco captions:\nData collection and evaluation server.\narXiv\npreprint arXiv:1504.00325, 2015.\n[8] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Ho\nVuong, Tianhe Yu, Wenlong Huang, Yevgen\nChebotar, Pierre Sermanet, Daniel Duckworth,\nSergey Levine, Vincent Vanhoucke, Karol Haus-\nman, Marc Toussaint, Klaus Greff, Andy Zeng,\nIgor Mordatch, and Peter R. Florence. Palm-e:\nAn embodied multimodal language model. arXiv\npreprint arXiv:2303.03378, 2023.\n[9] Samir Yitzhak Gadre, Gabriel Ilharco, Alex\nFang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman,\nDhruba Ghosh, Jieyu Zhang, et al. Datacomp:\nIn search of the next generation of multimodal\ndatasets. arXiv preprint arXiv:2304.14108, 2023.\n[10] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong\nWang, Miao Zheng, Qianmengke Zhao, Kuikun\nLiu, Wenwei Zhang, Ping Luo, and Kai Chen.\nMultimodal-gpt: A vision and language model\nfor dialogue with humans.\narXiv preprint\narXiv:2305.04790, 2023.\n[11] Danna Gurari, Qing Li, Abigale Stangl, Anhong\nGuo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P. Bigham. Vizwiz grand challenge: An-\nswering visual questions from blind people. In\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3608\u20133617, 2018.\n[12] Shaohan Huang, Li Dong, Wenhui Wang, Yaru\nHao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu,\net al. Language is not all you need: Aligning\nperception with language models. arXiv preprint\narXiv:2302.14045, 2023.\n[13] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol\nVinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative at-\ntention. In International conference on machine\nlearning, pages 4651\u20134664. PMLR, 2021.\n[14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen,\nZarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up vi-\nsual and vision-language representation learning\nwith noisy text supervision. In International Con-\nference on Machine Learning, pages 4904\u20134916.\nPMLR, 2021.\n[15] Douwe Kiela, Hamed Firooz, Aravind Mohan,\nVedanuj Goswami, Amanpreet Singh, Pratik\nRingshia, and Davide Testuggine. The hateful\nmemes challenge: Detecting hate speech in multi-\nmodal memes. arXiv preprint arXiv:2005.04790,\n2020.\n[16] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel\nFried. Grounding language models to images\nfor multimodal generation.\narXiv preprint\narXiv:2301.13823, 2023.\n[17] Gokul Karthik Kumar and Karthik Nandakumar.\nHate-clipper: Multimodal hateful meme classifi-\ncation based on cross-modal interaction of clip\nfeatures. arXiv preprint arXiv:2210.05916, 2022.\n[18] Hugo Lauren\u00b8con, Lucile Saulnier, L\u00b4eo Tronchon,\nStas Bekman, Amanpreet Singh, Anton Lozhkov,\n11\nThomas Wang, Siddharth Karamcheti, Alexan-\nder M. Rush, Douwe Kiela, Matthieu Cord, and\nVictor Sanh.\nObelisc: An open web-scale fil-\ntered dataset of interleaved image-text docu-\nments. arXiv preprint arXiv:2306.16527, 2023.\n[19] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao\nWang, Fanyi Pu, Jingkang Yang, C. Li, and Ziwei\nLiu. Mimic-it: Multi-modal in-context instruc-\ntion tuning. arXiv preprint arXiv:2306.05425,\n2023.\n[20] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao\nWang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction\ntuning. arXiv preprint arXiv:2305.03726, 2023.\n[21] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei\nWang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen,\nGuohai Xu, Zheng da Cao, Ji Zhang, Songfang\nHuang, Feiran Huang, Jingren Zhou, and Luo\nSi. mplug: Effective and efficient vision-language\nlearning by cross-modal skip-connections. arXiv\npreprint arXiv:2205.12005, 2022.\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and\nSteven Hoi. Blip: Bootstrapping language-image\npre-training for unified vision-language under-\nstanding and generation. In International Con-\nference on Machine Learning, pages 12888\u201312900.\nPMLR, 2022.\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and\nSteven Hoi.\nBlip-2: Bootstrapping language-\nimage pre-training with frozen image encoders\nand large language models.\narXiv preprint\narXiv:2301.12597, 2023.\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Pi-\notr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft\ncoco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part V 13, pages 740\u2013755. Springer,\n2014.\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and\nYong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[26] Kenneth Marino, Mohammad Rastegari, Ali\nFarhadi, and Roozbeh Mottaghi.\nOk-vqa: A\nvisual question answering benchmark requiring\nexternal knowledge. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition,\npages 3190\u20133199, 2019.\n[27] MosaicML. Introducing mpt-7b: A new standard\nfor open-source, commercially usable llms, 2023.\n[28] R OpenAI. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[29] Bryan A Plummer, Liwei Wang, Chris M Cer-\nvantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collect-\ning region-to-phrase correspondences for richer\nimage-to-sentence models. In IEEE international\nconference on computer vision, pages 2641\u20132649,\n2015.\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy,\nAditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In\nInternational conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[31] Shibani Santurkar, Yann Dubois, Rohan Taori,\nPercy Liang, and Tatsunori Hashimoto.\nIs a\ncaption worth a thousand images? a controlled\nstudy for representation learning. arXiv preprint\narXiv:2207.07635, 2022.\n[32] Christoph\nSchuhmann,\nRomain\nBeaumont,\nRichard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta,\nClayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next\ngeneration image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[33] Amanpreet Singh, Vivek Natarajan, Meet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach.\nTowards vqa models\nthat can read.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n8309\u20138318, 2019.\n[34] Thomas\nBreuel.\nWebDataset:\nA\nhigh-\nperformance Python-based I/O system for large\n(and small) deep learning problems, with strong\nsupport for PyTorch.\nAvailable at:\nhttps:\n//github.com/webdataset/webdataset, 2020.\n12\n[35] Together.xyz. Releasing 3b and 7b redpajama-\nincite\nfamily\nof\nmodels\nincluding\nbase,\ninstruction-tuned & chat models. https://www.\ntogether.xyz/blog/redpajama-models-v1,\n2023.\n[36] Ramakrishna Vedantam, C. Lawrence Zitnick,\nand Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In IEEE Conference on\nComputer Vision and Pattern Recognition, pages\n4566\u20134575, 2014.\n[37] Jason Wei, Maarten Bosma, Vincent Y Zhao,\nKelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned\nlanguage models are zero-shot learners. arXiv\npreprint arXiv:2109.01652, 2021.\n[38] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xi-\naowei Hu, Yumao Lu, Zicheng Liu, and Lijuan\nWang. An empirical study of gpt-3 for few-shot\nknowledge-based vqa. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36,\npages 3081\u20133089, 2022.\n[39] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo\nYe, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al.\nmplug-owl: Modularization empowers large lan-\nguage models with multimodality. arXiv preprint\narXiv:2304.14178, 2023.\n[40] Peter Young, Alice Lai, Micah Hodosh, and\nJ. Hockenmaier.\nFrom image descriptions to\nvisual denotations: New similarity metrics for se-\nmantic inference over event descriptions. Trans-\nactions of the Association for Computational Lin-\nguistics, 2:67\u201378, 2014.\n[41] Renrui Zhang, Jiaming Han, Aojun Zhou, Xi-\nangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient\nfine-tuning of language models with zero-init at-\ntention. arXiv preprint arXiv:2303.16199, 2023.\n[42] Susan Zhang, Stephen Roller, Naman Goyal,\nMikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Vic-\ntoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh\nKoura,\nAnjali Sridhar,\nTianlu Wang,\nand\nLuke Zettlemoyer.\nOpt:\nOpen pre-trained\ntransformer language models.\narXiv preprint\narXiv:2205.01068, 2022.\n[43] Yanli Zhao, Andrew Gu, Rohan Varma, Liang\nLuo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer,\net al.\nPytorch fsdp:\nexperiences on scaling\nfully sharded data parallel.\narXiv preprint\narXiv:2304.11277, 2023.\n[44] Luowei\nZhou,\nHamid\nPalangi,\nLei\nZhang,\nHoudong Hu, Jason J. Corso, and Jianfeng\nGao.\nUnified vision-language pre-training for\nimage captioning and vqa.\narXiv preprint\narXiv:1909.11059, 2019.\n[45] Wanrong Zhu, Jack Hessel, Anas Awadalla,\nSamir Yitzhak Gadre, Jesse Dodge, Alex Fang,\nYoungjae Yu, Ludwig Schmidt, William Yang\nWang, and Yejin Choi. Multimodal c4: An open,\nbillion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023.\n13\nTable 8: Fine-tuned state-of-the-art numbers used in\nthis report.\nMethod\nDataset\nScore\nmPLUG [21]\nCOCO\n155.1\nUnified VLP [44]\nFlickr-30K\n67.4\nPali-17B [6]\nVQAv2\n84.3\nPali-17B [6]\nOK-VQA\n64.5\nPali-17B [6]\nTextVQA\n73.1\nPali-17B [6]\nVizWiz\n73.3\nHate-CLIPper [17]\nHatefulMemes\n85.8\nA\nExtended results\nTable 11 provides full evaluation results for 0, 4, 8, 16,\nand 32 in-context examples. For ease of comparison to\nFlamingo, we calculate each OpenFlamingo model\u2019s\nperformance as a fraction of corresponding Flamingo\nperformance in Figure 11.\nA.1\nComparison to fine-tuned SoTAs\nIn Figure 9, we compare OpenFlamingo models to\nfine-tuned SoTA performances for different numbers\nof in-context examples. The fine-tuned methods used\nwere pulled from PapersWithCode on 06/19/23 (Table\n8).\n0\n4\n8\n16\n32\nNumber of in-context examples\n50%\n55%\n60%\n65%\n70%\nAggregated % of fine-tuned SoTA\nFlamingo-3B\nFlamingo-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nFigure 9: We plot each model\u2019s performance relative\nto fine-tuned state-of-the-art performance, averaged\nacross datasets.\nBenchmark\nShots\nRandom\nRICES\nCOCO\n4\n89.0\n93.1 (+4.1)\n32\n99.5\n109.0 (+9.5)\nFlickr-30K\n0\n59.5\n39.2 (\u221220.3)\n4\n65.8\n52.2 (\u221213.6)\n8\n62.9\n58.7 (\u22124.2)\n32\n61.3\n63.0 (+1.7)\nVQAv2\n4\n54.8\n55.1 (+0.3)\n32\n53.3\n56.8 (+3.5)\nOK-VQA\n4\n40.1\n38.3 (\u22121.8)\n32\n42.4\n46.3 (+3.9)\nTextVQA\n4\n28.2\n34.2 (+6)\n32\n23.8\n31.1 (+7.3)\nVizWiz\n4\n27.5\n41.0 (+13.5)\n32\n44.0\n46.4 (+2.4)\nHatefulMemes\n4\n54.0\n70.1 (+16.1)\n32\n53.8\n73.6 (+19.8)\nTable 9: Using RICES [38] to select in-context exam-\nples often outperforms using random demonstrations.\nScores in table are for OpenFlamingo-9B.\nA.2\nEvaluations using RICES\nIn the main text, we evaluate OpenFlamingo by se-\nlecting in-context examples uniformly at random. In\nthis appendix, we include additional evaluation results\nusing Retrieval-based In-Context Example Selection\n(RICES) [38]. For a given test example, RICES selects\nthe top-k most similar training examples as demonstra-\ntions, where similarity is measured by cosine similarity\nof the images according to the frozen vision encoder\n(CLIP ViT-L/14). Full results with RICES are listed\nin Table 12 and illustrated in Figure 10.\nIn Table 9, we compare OpenFlamingo-9B perfor-\nmance using RICES to performance using randomly\nselected in-context examples. We observe that RICES\nsignificantly boosts performance in most evaluation\nsettings, including by 19.2 ROC AUC using 32 shots\non HatefulMemes. However, on Flickr-30K, we ob-\nserve significant degradations from using RICES:\nCIDEr degrades by 20.4 in 0-shot evaluations2 and\n13.1 in 4-shot evaluations. We hypothesize that the\ndemonstrations RICES selects in Flickr-30K are more\nsimilar to the test example than in other datasets.\nThis leads OpenFlamingo-9B to parrot captions from\nthe in-context examples, including incorrect details.\nFor an example, see Table 10 in Appendix A.\n2In 0-shot evaluations, RICES is still used to select the\ntwo text-only examples used for the prompt (\u00a73.4).\n14\n0 4 8\n16\n32\n80\n90\n100\n110\nCIDEr\nCOCO\n0 4 8\n16\n32\n40\n50\n60\n70\nCIDEr\nFlickr30K\n0 4 8\n16\n32\n55\n60\n65\n70\nROC AUC\nHatefulMemes\n0 4 8\n16\n32\n30\n35\n40\n45\n50\nVQA Accuracy\nOK-VQA\n0 4 8\n16\n32\n20\n25\n30\n35\nVQA Accuracy\nTextVQA\n0 4 8\n16\n32\n45\n50\n55\n60\nVQA Accuracy\nVQAv2\n0 4 8\n16\n32\n30\n35\n40\n45\nVQA Accuracy\nVizWiz\n0 4 8\n16\n32\n45\n50\n55\n60\nAveraged scores\n(Average)\nNumber of in-context examples\nEvaluations with RICES\nFlamingo-3B\nFlamingo-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nFigure 10: Evaluation results per dataset across 0, 4, 8, 16, and 32 in-context examples. Results are reported\nin tabular form in Table 12.\nB\nAdditional notes on filtering\nMMC4\nWhen training contrastive vision-language models,\nfiltering image-text pairs by CLIP cosine similarity\nhas proven particularly helpful for improving data\nquality [31, 9]. We use a similar notion for filtering\ninterleaved sequences in MMC4: if an image and\nits matched sentence had cosine similarities that fell\nbelow a fixed threshold (0.24), according to CLIP\nViT-L/14 embeddings, we omitted the image from the\nsequence, keeping the text. If all images in a sequence\nare omitted, we discard the sequence entirely. This\naims to ensure that images are relevant to the text\nfollowing it.\nHowever, increasing the image-text similarity thresh-\nold has a side effect: it reduces the typical number\nof images per interleaved sequence. When using simi-\nlarity 0.32, nearly 58% of a sample of 1,000 MMC4\nsequences contain only 1 image per sequence, com-\npared to 38% in Figure 4, which uses a threshold of\n0.24. Training with long sequences may be important\nfor producing models that can handle a large amount\nof in-context examples. Further, we estimate that\n88.7% of MMC4 sequences are discarded completely\nwhen filtering with threshold 0.32, compared to 42.7%\nwith threshold 0.24.\nAs future work, we are interested in understanding\nhow to balance length, quality, and dataset size ob-\njectives to improve OpenFlamingo models.\nC\nSynthetic data prompt\nWe provide the prompt used to generate the ChatGPT-\ngenerated data (see \u00a73.2) in Table 12. After generating\ncandidate sequences, we query LAION-5B using [4] to\ninfill images. For each unique caption we generate, we\nattempt to retrieve 10 candidate images from the in-\ndex using index=laion5B-L-14, aesthetic score=9, and\naesthetic weight=0.5. After this search, we re-rank\nthe retrieved images using CLIP ViT-L/16@336px\nand select the image with the highest similarity to\ninterleave.\nD\nImage credits\nWe include the links to the images we used in Figure 2\nin Table 13.\n15\nRandom demonstrations\nRICES\nDemos\nA person hanging from a telephone pole near\nthe mountains.\nThe brown dog is running through the grass\nwith a yellow toy in its mouth.\nA trio of male musicians are performing with\none playing a guitar and singing into a micro-\nphone, another holding a harmonica, and the\nthird playing a bass guitar.\nA white dog rushes down a dirt path sur-\nrounded by grass and trees.\nTwo men, both in strange hats, working over\nrocks in a busy urban street.\nThe tan dog is carrying a green squeak toy in\nits mouth.\nSeveral people are in a group where a man in\na blue shirt is smiling.\nA yellow dog running through a yard covered in\nleaves while holding a yellow toy in his mouth.\nTest example\nOF-9B generations:\nA yellow labrador retriever running with a\nball.\nA yellow dog running through a yard cov-\nered in leaves while holding a green squeak\ntoy in his mouth\nGround truth: A white dog fetching a yellow toy.\nTable 10: Comparison of OpenFlamingo-9B outputs for a Flickr-30K 4-shot evaluation using RICES vs.\nrandom demonstrations. With RICES, OpenFlamingo-9B patches together these demonstration captions to\nanswer for the test image, including incorrect details.\n16\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\n0\n4\n8\n16\n32\nCOCO\n1.03\n1.02\n1.05\n1.11\n1.00\n0.91\n0.97\n0.96\n1.01\n0.96\n0.95\n0.97\n1.00\n1.05\n0.97\n0.94\n0.96\n0.98\n1.03\n0.97\n0.94\n0.96\n0.96\n1.00\n0.94\n0\n4\n8\n16\n32\nFlickr30K\n0.86\n0.84\n0.88\n0.92\n0.97\n0.79\n0.82\n0.84\n0.85\n0.91\n0.82\n0.85\n0.78\n0.82\n0.86\n0.81\n0.86\n0.77\n0.75\n0.86\n0.86\n0.91\n0.80\n0.74\n0.84\n0\n4\n8\n16\n32\nHatefulMemes\n0.95\n0.93\n0.97\n0.99\n0.91\n0.94\n0.92\n0.96\n1.02\n0.86\n0.95\n0.93\n1.01\n1.07\n0.86\n0.88\n0.88\n0.99\n1.03\n0.84\n0.89\n0.85\n0.93\n0.98\n0.85\n0\n4\n8\n16\n32\nOK-VQA\n0.68\n0.70\n0.75\n0.77\n0.85\n0.70\n0.71\n0.81\n0.80\n0.81\n0.70\n0.71\n0.76\n0.76\n0.82\n0.68\n0.67\n0.62\n0.69\n0.84\n0.68\n0.67\n0.58\n0.76\n0.83\n0\n4\n8\n16\n32\nTextVQA\n0.80\n0.77\n0.70\n0.70\n0.76\n0.83\n0.86\n0.79\n0.83\n0.84\n0.85\n0.90\n0.66\n0.77\n0.87\n0.88\n0.92\n0.57\n0.73\n0.81\n0.92\n0.93\n0.46\n0.76\n0.73\n0\n4\n8\n16\n32\nVQAv2\n0.91\n0.90\n0.92\n0.95\n1.02\n0.86\n0.86\n0.92\n0.92\n0.97\n0.83\n0.83\n0.87\n0.86\n0.94\n0.82\n0.81\n0.80\n0.80\n0.91\n0.82\n0.78\n0.75\n0.83\n0.88\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\n0\n4\n8\n16\n32\nVizWiz\n0.82\n0.81\n0.65\n0.74\n0.95\n0.79\n0.81\n0.78\n0.78\n0.98\n0.84\n0.84\n0.75\n0.76\n0.98\n0.83\n0.82\n0.57\n0.72\n0.99\n0.87\n0.86\n0.51\n0.69\n1.00\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nOpenFlamingo performance as fraction of Flamingo performance\nFigure 11: OpenFlamingo performance as a fraction of corresponding Flamingo performance for each\nevaluation setting. We compare OpenFlamingo-3B and -4B models to Flamingo-3B, and OpenFlamingo-9B\nto Flamingo-9B. 0, 4, 8, 16, and 32 refer to the number of in-context examples used.\n17\nBenchmark\nShots\nFl-3B\nFl-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nCOCO [7]\n0\n73.0\n79.4\n74.9 (0.2)\n74.4 (0.6)\n76.7 (0.2)\n81.2 (0.3)\n79.5 (0.2)\n4\n85.0\n93.1\n77.3 (0.3)\n82.7 (0.7)\n81.8 (0.4)\n85.8 (0.5)\n89.0 (0.3)\n8\n90.6\n99.0\n85.9 (0.6)\n87.8 (0.5)\n90.7 (0.3)\n94.8 (0.2)\n96.3 (0.1)\n16\n95.4\n102.2\n89.8 (0.2)\n91.9 (0.3)\n93.9 (0.4)\n98.0 (0.3)\n98.8 (0.7)\n32\n99.0\n106.3\n93.0 (0.6)\n94.8 (0.3)\n95.1 (0.3)\n99.2 (0.3)\n99.5 (0.1)\nFlickr-30K [40]\n0\n60.6\n61.5\n52.3 (1.0)\n51.2 (0.2)\n53.6 (0.9)\n55.6 (1.3)\n59.5 (1.0)\n4\n72.0\n72.6\n57.2 (0.4)\n59.1 (0.3)\n60.7 (1.2)\n61.2 (0.5)\n65.8 (0.6)\n8\n71.7\n73.4\n58.6 (1.1)\n60.7 (0.6)\n55.9 (1.3)\n59.0 (1.0)\n62.9 (1.0)\n16\n73.4\n72.7\n59.2 (0.5)\n63.0 (0.4)\n56.8 (0.5)\n54.8 (1.0)\n62.8 (1.0)\n32\n71.2\n72.8\n61.1 (1.3)\n64.5 (1.3)\n56.9 (0.7)\n53.0 (0.5)\n61.3 (0.7)\nVQAv2 [2]\n0\n49.2\n51.8\n44.6 (0.0)\n44.1 (0.1)\n45.1 (0.1)\n46.9 (0.0)\n52.7 (0.2)\n4\n53.2\n56.3\n45.8 (0.0)\n45.7 (0.1)\n49.0 (0.0)\n49.0 (0.0)\n54.8 (0.0)\n8\n55.4\n58.0\n46.2 (0.0)\n45.9 (0.1)\n48.3 (0.0)\n47.4 (0.0)\n54.8 (0.0)\n16\n56.7\n59.4\n46.6 (0.0)\n45.8 (0.0)\n45.5 (0.1)\n45.1 (0.1)\n54.3 (0.0)\n32\n57.1\n60.4\n47.0 (0.1)\n44.8 (0.1)\n43.0 (0.2)\n47.3 (0.0)\n53.3 (0.1)\nOK-VQA [26]\n0\n41.2\n44.7\n28.2 (0.2)\n28.7 (0.1)\n30.7 (0.1)\n31.7 (0.1)\n37.8 (0.2)\n4\n43.3\n49.3\n30.3 (0.5)\n30.6 (0.2)\n35.1 (0.0)\n34.6 (0.0)\n40.1 (0.1)\n8\n44.6\n50.0\n31.1 (0.3)\n31.5 (0.3)\n33.9 (0.1)\n33.7 (0.2)\n41.1 (0.2)\n16\n45.6\n50.8\n30.9 (0.3)\n30.7 (0.3)\n28.5 (0.2)\n31.3 (0.1)\n42.7 (0.2)\n32\n45.9\n51.0\n31.0 (0.1)\n30.6 (0.1)\n26.4 (0.2)\n34.7 (0.3)\n42.4 (0.0)\nTextVQA [33]\n0\n30.1\n31.8\n24.2 (0.2)\n23.1 (0.2)\n21.0 (0.3)\n21.1 (0.4)\n24.2 (0.5)\n4\n32.7\n33.6\n27.0 (0.3)\n28.1 (0.4)\n25.9 (0.0)\n27.2 (0.3)\n28.2 (0.4)\n8\n32.4\n33.6\n27.7 (0.1)\n29.1 (0.1)\n21.3 (0.2)\n25.1 (0.2)\n29.1 (0.1)\n16\n31.8\n33.5\n28.0 (0.2)\n29.1 (0.1)\n18.2 (0.4)\n23.2 (0.1)\n27.3 (0.1)\n32\n30.6\n32.6\n28.3 (0.2)\n28.5 (0.1)\n14.1 (0.2)\n23.2 (0.2)\n23.8 (0.2)\nVizWiz [11]\n0\n28.9\n28.8\n23.7 (0.5)\n23.4 (0.3)\n18.8 (0.1)\n21.5 (0.2)\n27.5 (0.2)\n4\n34.0\n34.9\n27.0 (0.3)\n27.7 (0.1)\n26.6 (0.5)\n26.5 (0.4)\n34.1 (0.7)\n8\n38.4\n39.4\n32.1 (0.7)\n32.1 (0.6)\n28.8 (0.4)\n29.1 (0.2)\n38.5 (0.1)\n16\n43.3\n43.0\n36.1 (0.3)\n35.3 (0.1)\n24.6 (0.2)\n31.0 (0.6)\n42.5 (0.4)\n32\n45.5\n44.0\n39.8 (0.1)\n39.3 (0.4)\n23.1 (1.1)\n31.3 (0.2)\n44.0 (0.5)\nHatefulMemes [15]\n0\n53.7\n57.0\n51.2 (2.5)\n50.1 (2.2)\n52.3 (2.3)\n53.1 (2.2)\n51.6 (1.8)\n4\n53.6\n62.7\n50.6 (0.8)\n49.5 (0.6)\n51.5 (1.4)\n54.9 (1.1)\n54.0 (2.0)\n8\n54.7\n63.9\n52.0 (1.1)\n50.7 (1.8)\n55.2 (0.8)\n58.5 (0.3)\n54.7 (2.8)\n16\n55.3\n64.5\n48.5 (0.7)\n48.7 (1.0)\n54.5 (1.3)\n56.9 (1.5)\n53.9 (3.1)\n32\n56.3\n63.5\n50.2 (1.8)\n47.8 (2.2)\n52.2 (1.2)\n54.9 (1.1)\n53.8 (2.1)\nTable 11: Full evaluation results using demonstrations sampled uniformly at random across seven\nvision-language datasets using 0, 4, 8, 16, and 32 in-context examples. Results are averaged across 3 evaluation\nseeds and reported with standard deviations.\n18\nBenchmark\nShots\nFl-3B\nFl-9B\nOF-3B\nOF-3B (I)\nOF-4B\nOF-4B (I)\nOF-9B\nCOCO [7]\n0\n73.0\n79.4\n81.6\n79.0\n75.9\n79.5\n76.8\n4\n85.0\n93.1\n91.3\n90.5\n89.0\n92.7\n93.1\n8\n90.6\n99.0\n97.4\n96.8\n96.6\n101.6\n99.8\n16\n95.4\n102.2\n102.2\n101.1\n102.4\n106.0\n105.6\n32\n99.0\n106.3\n105.5\n104.1\n105.1\n107.8\n109.0\nFlickr-30K [40]\n0\n60.6\n61.5\n46.0\n45.0\n36.3\n43.0\n39.2\n4\n72.0\n72.6\n54.0\n53.9\n48.9\n53.6\n52.2\n8\n71.7\n73.4\n58.6\n58.6\n51.3\n53.4\n58.7\n16\n73.4\n72.7\n62.0\n61.5\n56.5\n56.4\n60.6\n32\n71.2\n72.8\n65.5\n64.6\n56.2\n56.6\n63.0\nVQAv2 [2]\n0\n49.2\n51.8\n44.5\n44.0\n44.9\n46.6\n53.0\n4\n53.2\n56.3\n45.9\n45.3\n49.0\n49.6\n55.1\n8\n55.4\n58.0\n47.0\n46.1\n48.5\n47.7\n56.0\n16\n56.7\n59.4\n48.0\n46.6\n46.6\n45.4\n56.6\n32\n57.1\n60.4\n49.1\n46.7\n44.5\n48.9\n56.8\nOK-VQA [26]\n0\n41.2\n44.7\n28.6\n29.2\n31.0\n32.0\n38.3\n4\n43.3\n49.3\n31.0\n31.4\n34.8\n35.1\n42.0\n8\n44.6\n50.0\n32.6\n32.8\n33.3\n34.6\n44.1\n16\n45.6\n50.8\n33.8\n33.9\n29.2\n32.7\n45.1\n32\n45.9\n51.0\n35.1\n34.6\n28.7\n36.5\n46.3\nTextVQA [33]\n0\n30.1\n31.8\n24.5\n26.0\n22.8\n23.5\n28.3\n4\n32.7\n33.6\n27.2\n28.9\n26.7\n28.9\n34.2\n8\n32.4\n33.6\n28.0\n28.8\n22.7\n25.6\n34.8\n16\n31.8\n33.5\n27.5\n28.8\n19.5\n22.0\n33.4\n32\n30.6\n32.6\n29.2\n29.0\n17.93\n23.0\n31.1\nVizWiz [11]\n0\n28.9\n28.8\n32.1\n32.6\n28.0\n30.1\n34.6\n4\n34.0\n34.9\n37.3\n39.0\n37.4\n36.6\n41.0\n8\n38.4\n39.4\n41.7\n42.5\n35.7\n37.0\n45.0\n16\n43.3\n43.0\n44.8\n44.9\n31.5\n36.6\n46.2\n32\n45.5\n44.0\n45.8\n45.9\n27.3\n36.4\n46.4\nHatefulMemes [15]\n0\n53.7\n57.0\n63.9\n65.2\n64.8\n66.2\n67.1\n4\n53.6\n62.7\n66.1\n65.7\n65.4\n66.8\n70.1\n8\n54.7\n63.9\n67.2\n66.0\n67.8\n69.3\n71.2\n16\n55.3\n64.5\n68.1\n65.0\n67.5\n67.1\n73.2\n32\n56.3\n63.5\n68.2\n67.7\n64.4\n62.2\n73.6\nTable 12: Full evaluation results using RICES across seven vision-language datasets using 0, 4, 8, 16, and 32\nin-context examples.\nRow 1\nLink\nLink\nLink\nRow 2\nLink\nLink\nLink\nRow 3\nLink\nLink\nLink\nTable 13: Source of images for Figure 2 for each row the links are from left to right in the order of how the\nimages are presented.\n19\nsystem prompt (human authored)\nYou are a helpful assistant that knows a lot about image descriptions and how they are used with text in broader contexts.\nYour job is to generate an interleaved sequence of images (referenced by their image descriptions) and sentences.\nImage descriptions should only describe the single image. They should be specific, 1-sentence, self-contained descriptions that\ncapture the salient elements of the image.\nThe sentences can refer to concepts in any of the images and to each-other. The interleaved sequences are interesting, clever, and\nrepresent compelling patterns. To understand them requires knowledge of real-world entities/events, comparison between the\nimages, and commonsense knowledge. Real-world references/facts must be correct.\nRemember to:\n1) be creative. tell stories, reference real-world entities/events, etc. The images/sentence can play off each-other in fun ways.\n2) be interesting. generate sequences that are cool, fun, compelling and require interesting commonsense reasoning across and\nbetween images/sentences.\n3) make sure the image descriptions are self-contained, and the output format follows the requested format.\nuser (human authored)\nGenerate a creative, interesting sequence of sentences/images with the following format: (image A, sentence 1, image B, sentence\n2, image C, sentence 3)\nassistant (human authored)\nSure!\nSequence format: (image A, sentence 1, image B, sentence 2, image C, sentence 3). This interleaved sequence of images/sentences\ncontains images of animals from various parts of the world. The sentences first name the depicted animal, and then describe\nwhere the animal is native to.\nImage A: A photo of a person holding a chinchilla in their hand\nSentence 1: This is a chinchilla. They are mainly found in Chile.\nImage B: A photo of a shiba-enu in the fall sitting in the forest\nSentence 2: This is a shiba. They are very popular in japan.\nImage C: A photo of a flamingo with a lake in the background\nSentence 3: This is a flamingo. They are found in the Caribbean and South America.\n(...\n4x more hand designed/interleaved examples...)\nuser (human authored)\nGenerate a creative, interesting sequence of sentences/images with the following format: (image A, image B, sentence 1, image C,\nimage D, sentence 2, image E, image F, sentence 3).\nassistant (machine generated)\nSure!\nThis interleaved sequence of images/sentences contains images of nature. The sentences that follow each image are statements\nthat lead to conclusions. The final sentence explains the broader significance of the pattern.\nImage A: A photo of a seed being planted in the soil.\nImage B: A photo of a small plant sprouting from the soil.\nSentence 1: The plant is growing.\nImage C: A photo of trees in a forest.\nImage D: A photo of an orchard with rows of apple trees.\nSentence 2: Trees are used to produce fruits.\nImage E: A photo of a river running through a valley.\nImage F: A photo of a waterfall cascading down a mountain.\nSentence 3: Water is a critical component of many natural processes, from plant growth to the formation of landscapes.\nFigure 12: We prompt ChatGPT using a system prompt, 5 interleaved human-authored sequences, and a\nquery pattern, in this case \u201c(Image A, Image B, Sentence 1, Image C, Image D, Sentence 2, Image E, Image\nF, Sentence 3)\u201d. Based on these examples, ChatGPT first explains what sequence it is attempting to generate\n(here: \u201c... images of nature. The sentences that follow each image are statements that lead to conclusions\u201d).\nLater, images are retrieved based on the generated \u201cPhoto X\u201d descriptions to form interleaved sequences.\n20\n"
  },
  {
    "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
    "link": "https://arxiv.org/pdf/2308.01825.pdf",
    "upvote": "19",
    "text": "Preprint\nSCALING RELATIONSHIP ON LEARNING MATHEMATI-\nCAL REASONING WITH LARGE LANGUAGE MODELS\nZheng Yuan\u2217, Hongyi Yuan\u2217\u2020, Chengpeng Li\u2020, Guanting Dong\u2020, Keming Lu\nChuanqi Tan, Chang Zhou, Jingren Zhou\nAlibaba DAMO Academy\n{yuanzheng.yuanzhen,yuanhongyi.yhy}@alibaba-inc.com\n{lichengpeng.lcp,dongguanting.dgt,lukeming.lkm}@alibaba-inc.com\n{chuanqi.tcq,ericzhou.zc,jingren.zhou}@alibaba-inc.com\nABSTRACT\nMathematical reasoning is a challenging task for large language models (LLMs),\nwhile the scaling relationship of it with respect to LLM capacity is under-explored.\nIn this paper, we investigate how the pre-training loss, supervised data amount,\nand augmented data amount influence the reasoning performances of a supervised\nLLM. We find that pre-training loss is a better indicator of the model\u2019s perfor-\nmance than the model\u2019s parameter count. We apply supervised fine-tuning (SFT)\nwith different amounts of supervised data and empirically find a log-linear re-\nlation between data amount and model performance, and we find better models\nimprove less with enlarged supervised datasets. To augment more data samples\nfor improving model performances without any human effort, we propose to ap-\nply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to gen-\nerate and collect correct reasoning paths as augmented fine-tuning datasets. We\nfind with augmented samples containing more distinct reasoning paths, RFT im-\nproves mathematical reasoning performance more for LLMs. We also find RFT\nbrings more improvement for less performant LLMs. Furthermore, we combine\nrejection samples from multiple models which push LLaMA-7B to an accuracy of\n49.3% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy\nof 35.9% significantly. We release our codes and rejection sampling augmented\ndata in https://github.com/OFA-Sys/gsm8k-ScRel.\n1\nINTRODUCTION\nLarge language models (LLMs) (Anil et al., 2023; Touvron et al., 2023b; OpenAI, 2023) have shown\nconsiderable abilities in various math reasoning tasks (Saxton et al., 2019; Cobbe et al., 2021; Light-\nman et al., 2023). It is of interest to understand, predict, and improve an LLM\u2019s math reasoning\nability based on different pre-trained LLMs and supervised datasets. With this knowledge, we can\nbetter decide the effort we put into improving the LLM or augmenting the dataset. Many recent\nworks are focusing on using different prompts (Wei et al., 2022b; Yao et al., 2023) or ensembling /\nreranking multiple times of inferences (Cobbe et al., 2021; Uesato et al., 2022; Wang et al., 2023;\nLightman et al., 2023) to improve models\u2019 reasoning performances. While in-context learning (ICL)\nand performing multiple inferences can improve performance, it is computationally expensive and\nnot suitable for online deployment scenarios. Therefore, we focus on the performance of the super-\nvised LLMs with inference only once which is a setting closer to online deployment.\nTo this end, we empirically investigate the scaling relationship of factors that influence the math\nreasoning abilities of a supervised LLM, including pre-training losses, the amount of supervised\ndata, and the amount of augmented data. Firstly, we analyze the supervised fine-tuning (SFT) and\nICL performance of LLMs. We observe that the pre-training loss is approximately negatively linear\ncorrelated to the SFT and ICL accuracy in a given interval which is a better performance indicator\nthan pre-trained model sizes or pre-trained token counts. Secondly, we analyze the relationship\n\u2217Contributed Equally.\n\u2020Work done during internships at Alibaba DAMO Academy.\n1\narXiv:2308.01825v2  [cs.CL]  13 Sep 2023\nPreprint\nFigure 1: The key findings of scaling relationship on learning math reasoning ability with LLMs.\nbetween SFT and different amounts of supervised data. We observe that the model performance\nhas a log-linear relation versus the supervised data amount while the increase diminishes with the\nbetter pre-trained model. Thirdly, we want to leverage the model itself to generate more supervised\ndata to reinforce its reasoning ability and analyze the scaling relationship of the augmented data\namount. We apply rejection sampling on SFT models to sample and select correct reasoning paths\nas augmented dataset (Uesato et al., 2022; Zhu et al., 2023). We use these augmented datasets to\nfine-tune base LLMs which would achieve better performances compared to SFT and we denote it\nas rejection sampling fine-tuning (RFT). We find the key factor influencing RFT performance is the\ndistinct reasoning path amount which can be increased by sampling more times or combing samples\nfrom multiple models. We apply RFT on several pre-trained LLMs and show larger improvement\non less performant models. We discuss the reason RFT works is it provides multiple reasoning paths\nwhich makes LLMs have better reasoning generalization. We also discuss that RFT is much cheaper\nthan pre-training in computational resources while training an LLM with lower pre-training loss is\nthe fundamental solution.\nThe key findings of this paper are shown in Figure 1 and are summarized here:\n\u2022 When the pre-training loss gets smaller (i.e. the pre-trained model gets better), the model\nreasoning performances of SFT and ICL increase linearly within a range. The SFT perfor-\nmance improves slower than ICL.\n\u2022 SFT improves in a log-linear manner with the increase of supervised data amount. The\nbenefits of increasing data amount diminish as the pre-trained model gets better.\n\u2022 The model performance for RFT improves as the distinct reasoning path amount increases.\nThe RFT performance improves slower than SFT.\n\u2022 The combination of rejection sampling samples from multiple models further enhances the\nRFT performance, resulting in an accuracy of 49.3 for LLaMA-7B (+13.4 compared to\nSFT), 50.3 for LLaMA2-7B (+8.7 compared to SFT), 52.1 for LLaMA-13B (+9.1 com-\npared to SFT), and 55.4 for LLaMA2-13B (+5.4 compared to SFT).\n2\nRELATED WORKS\nLearning Math Reasoning with LLMs\nRecent research on LLMs has discovered the emergent\nability to solve reasoning tasks beyond a certain model scale (Wei et al., 2022a). Such reasoning\nabilities in LLMs can be elicited by fine-tuning, few-shot prompting, or zero-shot prompting (Cobbe\net al., 2021; Wei et al., 2021; Nye et al., 2021; Wei et al., 2022b; Kojima et al., 2022). A large\n2\nPreprint\namount of research focuses on the reasoning tasks of math word problems (MWP), and methods are\nevaluated on the benchmarks spanning different levels of MWPs (Koncel-Kedziorski et al. (2016);\nPatel et al. (2021); Lan et al. (2021); Cobbe et al. (2021); Jie et al. (2022); Yuan et al. (2023a); Fu\net al. (2023a), inter alia). The core idea of improving the mathematical reasoning ability of LLMs\nis to aggregate various sampled reasoning paths during either fine-tuning or inference. Cobbe et al.\n(2021) trained and devised a reasoning path verifier to select the correct results during inference.\nWang et al. (2023) proposed to sample various reasoning paths during inference and then derive the\nfinal result by majority voting on the answers or through verifiers (Li et al., 2023). Several works\napplied the idea of rejection sampling along with other techniques to filter the diverse sampled\nreasoning paths for fine-tuning data augmentation (Huang et al., 2022; Zelikman et al., 2022; Ni\net al., 2023; Zhu et al., 2023). Rejection sampling is a simple-yet-effective fine-tuning augmentation\ntechnique and is also used for LLM alignment with human preference (Bai et al., 2022; Yuan et al.,\n2023b; Dong et al., 2023; Touvron et al., 2023b; Song et al., 2023). Uesato et al. (2022) explored\nto use of reinforcement learning methods for improving the mathematical reasoning abilities of\nLLMs and they further discussed the difference between outcome-based and process-based reward\nmodeling. Followed by Lightman et al. (2023), they collected large-scale process-based supervision\nsignals through human annotation and verified that LLMs can benefit more from process-based\nreward modeling with human-annotated supervision than outcome-based reward modeling. There is\nalso prior research that distilled the emergent reasoning ability of LLMs to small language models\n(Fu et al., 2023b; Shridhar et al., 2023). Compared to previous works (Zelikman et al., 2022; Uesato\net al., 2022; Zhu et al., 2023; Ni et al., 2023), we are using a simpler way of generating augmented\nsamples without any trained process-level reward models and we are focusing on researching the\nscaling relationship between LLMs and math reasoning ability.\nScaling Laws of Large Language Models\nIt is important to understand and predict the perfor-\nmance gain as the language model scales up. Kaplan et al. (2020) first investigated and derived a\npredictable relationship on how the number of model parameters and data sizes contribute to the\nloss over many orders of magnitudes. Hoffmann et al. (2022) refined the scaling laws in (Kaplan\net al., 2020) and found the scaling laws for computation-optimal training. Muennighoff et al. (2023)\nexplored and extended the scaling laws under a data-constrained scenario. Besides investigating the\nscaling performance for pre-training, Gao et al. (2022) discussed the scaling laws for overparame-\nterized reward models for alignment with human preference, and Hernandez et al. (2021) developed\nscaling laws for transferring performance from pre-trained models to downstream tasks. Henighan\net al. (2020); Caballero et al. (2022) investigated scaling laws of math problems. In this paper, we\nare investigating the scaling relationships of large language models on learning math word problems\nwith pre-training losses, supervised data amount, and augmented data amount.\n3\nTHE FACTORS OF MATH REASONING ABILITY IN SUPERVISED LLM\nThe target of this paper is to try to understand the performances of supervised LLMs in math reason-\ning. We expect a pre-trained LLM \u03c1 to learn reasoning ability from a supervised reasoning dataset\nD. The dataset is defined by D = {qi, ri, ai}i, where q is a question, r is a chain-of-thought reason-\ning path, and a is a numerical answer. We perform supervised fine-tuning on dataset D to obtain an\nSFT model \u03c0. We use \u03c0 to generate reasoning paths and answers in the test set by greedy decoding\nand report the accuracy (i.e. acc or maj1@1) as our metric here.\n3.1\nMODEL ACCURACY VS. PRE-TRAINING LOSS\nPrevious works state that the larger LLM shows better reasoning ability across the same series of\nmodels (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a;b), and we find LLaMA\noutperforms GPT-3 which shows the model parameter counts should not be the only indicator of\nreasoning ability. While LLMs have different architectures, model parameters, and pre-training\ntoken numbers, we find the pre-training loss is a stable performance indicator of the math reasoning\nability and we use it to represent the model instead of using their model parameters and pre-training\ntoken numbers.\nWe analyze the SFT and ICL (8-shot) performance of GPT-3 (Brown et al., 2020), LLaMA (Touvron\net al., 2023a), LLaMA2 (Touvron et al., 2023b), and GPT-4 (OpenAI, 2023). The pre-training losses\n3\nPreprint\nFigure 2: The performance of SFT (blue lines) and ICL (red lines) settings on GSM8K. GPT-4 states\nthey use some part of the GSM8K data in pre-training, and suggest others consider its performance\nbetween SFT and ICL.\nof these models are observed in their paper, we should notice that pre-training losses correspond to\ndifferent pre-training datasets and different tokenizers which means they could not be compared\nstrictly (and we cannot use it to do any sort of regression directly) while the tendency among these\nlosses is still enlightening. We use the results of GPT-3 fine-tuning from (Cobbe et al., 2021) and\nwe fine-tune LLaMA and LLaMA2 on the GSM8K training set (detailed in Appendix A.1). For\nin-context learning, we use the results from LLaMA (Touvron et al., 2023a) and LLaMA2 (Touvron\net al., 2023b) paper.\nIn Figure 2, we can find that:\n\u2022 The pre-training losses are approximately negatively linear correlated to the SFT and ICL\naccuracy during the given pre-training loss interval.\n\u2022 SFT outperforms ICL consistently, while the improvements diminish when the pre-training\nloss is lower.\nThe linear relation of SFT and ICL accuracy may only work in the given interval. The reasons are\n(1) the slope of ICL is steeper than SFT, while the SFT performance should be greater than ICL\nperformance; (2) the accuracy can not bigger than 1 or smaller than 0. It should be using \u2212 log(acc)\ninstead of acc as the dependent variable theoretically while we find an apparent linear relationship\namong pre-training loss and acc and use acc as the dependent variable. LLaMA-2 7B(13B) can be\nviewed as an approximation of continue-training of LLaMA 7B(13B). As it trains longer, its ICL\nand SFT performance both improve without changing the parameter count. From the observations,\none effective way to improve reasoning ability is to train a better base model with lower pre-training\nloss (Pre-training is all you need!). The models with lower pre-training loss improve less from the\nfine-tuning which may be due to the models having already obtained more reasoning abilities during\npre-training and the supervised data can provide less signal to supervise them.\n4\nPreprint\nFigure 3: The performance of SFT with different amounts of supervised data on GSM8K.\n3.2\nMODEL ACCURACY VS. SUPERVISED DATA COUNT\nSupervised fine-tuning does improve LLMs\u2019 reasoning ability, we want to know how the super-\nvised data amount influences the model\u2019s improvement. We fine-tune LLaMA and LLaMA2 with\n{1, 1/2, 1/4, 1/8, 1/16, 1/32} amount of the training set from GSM8K (detailed in Appendix A.2).\nWe want to use this experiment to extrapolate the model performances if we have more supervised\ndata. In Figure 3, we plot the results of training with different amounts of supervised data. From\nthis figure, we can observe that:\n\u2022 The model performance has a log-linear relation versus data amount. When the data amount\ndoubles, the performance increases by a unit.\n\u2022 Better model needs more amount of data to outperform its ICL performance.\n\u2022 Better model benefits less when supervised data amount doubles.\nThe log-linear relation is stable during {1, 1/2, 1/4, 1/8} amount of the training data. From the ob-\nservation, it is straightforward to enlarge the training dataset to improve the performance, especially\nfor worse models. For better models, it benefits less which echoes that better models have learned\nmore reasoning ability during pre-training.\n3.3\nMODEL ACCURACY VS. AUGMENTED DATA COUNT\nIncreasing the amount of math reasoning labeled data is difficult, especially proposing a new ques-\ntion. It is easy for a well-educated student to solve hundreds of math word problems per day, but\nit is very hard to come up with diverse and educational math problems. So our direction changes\nto augment new data using existing resources. We have tried augmenting new queries (detailed in\nAppendix D.1) and augmenting revisions (detailed in Appendix D.2). These approaches have none\nto marginal improvements compared to SFT. We find a simplified version of rejection sampling (Zhu\net al., 2023) is a naive and effective way to augment new reasoning paths and can improve the model\nperformance. And we find the key factor influences fine-tuning on rejection sampling (RFT) aug-\nmented data is distinct reasoning path amount. Combining rejection sampling samples from multiple\n5\nPreprint\nSetting\n7B\n7B-2\n13B\n13B-2\n33B\nPretrain loss\n1.8\n1.75\n1.73\n1.68\n1.62\nICL\n11.0/18.1\n14.6/-\n17.8/29.3\n28.7/-\n35.6/53.1\nSFT\n35.9/48.7\n41.6/55.4\n43.0/55.2\n50.0/61.7\n54.6/-\nRFT k = 100\n41.7/52.7\n47.5/58.7\n49.1/59.9\n54.8/65.4\n54.5/-\nCorrect paths per question\n53.3\n60.8\n62.5\n71.6\n88.7\nDistinct paths per question\n5.25\n5.19\n5.26\n5.29\n2.78\nTable 1: The performance of RFT with k = 100 on GSM8K compared with SFT and ICL. Distinct\npath amount means distinct equation list amount here.\nmodels, we can further fine-tune a LLaMA-7B model to an accuracy of 49.3 (compared with SFT\n35.9) and a LLaMA-13B model to an accuracy of 52.1 (compared with SFT 43.0).\nRejection Sampling Fine-tuning\nThe SFT model \u03c0 obtains the ability to perform zero-shot chain-\nof-thought reasoning, and we use \u03c0 to generate more correct reasoning paths rij to supply the\ntraining dataset. For each qi, we generate k candidate reasoning paths and answers r, a with a\ntemperature of 0.7 following (Cobbe et al., 2021). We first filter out reasoning paths with wrong\nanswers a \u0338= ai or wrong calculations based on Python evaluation. Each reasoning path contains\na list of equations ej, and we select one reasoning path rij for each distinct equation list as the\naugmented data and remove other reasoning paths with the same list of equations to deduplicate\nsimilar reasoning paths. Different order of elements (e.g. 3 + 4 = 7 and 4 + 3 = 7) or different\norder of equations (e.g. 1 + 2 = 3, 3 + 4 = 7 and 1 + 4 = 5, 2 + 5 = 7) are considered different.\nIt is helpful for models to know these orders can be exchanged and is hard for models to learn this\nwith only one reasoning path each problem. We define D\u2032\n\u03c0 = D \u222a {qi, rij, ai}i,j as the augmented\ndataset. We fine-tune D\u2032 on pre-trained LLM \u03c1 to \u03c0RFT as RFT, and we detail how we apply RFT\nin Appendix A.3. We list the results of RFT with sampling k = 100 candidate reasoning paths\non LLaMA and LLaMA-2 in Table 1. For ICL, SFT, and RFT, we list the maj1@1 (accuracy) and\nmaj1@100 (sample 100 times and calculate accuracy based on majority voting) as metrics.\nIn the case of 7B and 13B models, RFT yields an approximate increase of 5 to 6 points in maj1@1\nand about 4 points increase in maj1@100. For 33B models, RFT does not improve performance\ncompared to SFT. The main reason comes from the augmented samples from rejection sampling.\nWe can find that better models generate more correct reasoning paths per question. For LLaMA-\n33B-SFT, it can generate an average of 88.7 correct paths per question. However, it overfits the\ntraining set and has difficulty generating more diverse paths on the training set questions. Rejection\nsampling with 33B is very time-consuming and we do not conduct a temperate grid search, we have\ntried using a larger temperate 1.0 for decoding LLaMA-33B-SFT models, it generates 82.4 correct\npaths and 4.77 distinct paths per question which is more diverse than using temperate 0.7 but still\nless diverse than 7B and 13B models. We admit there should be a temperate (or generation config)\nthat can produce more distinct paths and generate good results for RFT in 33B and even larger\nmodels while it does need more computation resources for inference compared to sampling using\n7B and 13B models. We will show we can use 7B and 13B models only for rejection sampling to\nimprove the 33B model.\nModel Accuracy vs Rejection Sampling Data Count\nTo understand the performance of RFT,\nwe vary k among 1, 3, 6, 12, 25, 50, 100 and apply RFT. We also have another setting of k = 100\nwhile not removing any reasoning paths denoted as no dedup. We list the RFT results with different\nk on Figure 4. Comparing using RFT with k = 100 and no dedup, the performance is similar and\nshows that it is better to estimate RFT performance based on distinct reasoning path amount instead\nof RFT augmented sample counts. Furthermore, using deduplication has better performances for 3\nof 4 models and needs much less training time.\nWhen using k = 3, RFT outperforms SFT by 2 points stably. For most data points, using larger\nk leads to better performances. However, the merits of RFT are decreasing when doubling k. We\ncalculate different paths per question for different k in Table 2. We can see that the amount of\ndifferent reasoning paths is not growing quickly along k growing. In Figure 3, we know doubling\ntraining samples can have a linear performance improvement. Doubling reasoning paths should\n6\nPreprint\nFigure 4: The performance of RFT with different amounts of sampling count k on GSM8K.\nk\n7B\n7B-2\n13B\n13B-2\n33B\n1\n1.17\n1.19\n1.15\n1.18\n1.06\n3\n1.44\n1.47\n1.41\n1.45\n1.16\n6\n1.74\n1.78\n1.69\n1.76\n1.28\n12\n2.20\n2.23\n2.11\n2.21\n1.46\n25\n2.93\n2.93\n2.88\n2.94\n1.77\n50\n3.94\n3.91\n3.90\n3.94\n2.19\n100\n5.25\n5.19\n5.26\n5.29\n2.78\n400 (U13B)\n12.84\n500 (U33B)\n13.65\nTable 2: Different reasoning paths per question generated by different SFT models with different k.\nimprove less than doubling training samples since obtaining different reasoning paths does not obtain\nany new questions. Therefore, doubling k leads to diminished performance improvements.\nCombining rejection sampling samples from multiple models\nThe experiment results above\ndemonstrate performance boosts in mathematical reasoning, benefitting from rejection sampling.\nThrough case studies in 4.1, we show that rejection sampling can augment training data with rea-\nsoning paths of diverse calculation processes. However, the reasoning paths sampled from one single\nSFT model can be logically non-diverse. Therefore, we expect to further improve the mathematical\nreasoning performance by leveraging rejection sampled reasoning paths aggregated from different\nmodels. We denote two final datasets as D\u2032\nU13B and D\u2032\nU33B, which are aggregated from rejection sam-\npling different models D\u2032\nU13B = D\u2032\n7B \u2295 D\u2032\n7B2 \u2295 D\u2032\n13B \u2295 D\u2032\n13B2 and D\u2032\nU33B = D\u2032\nU13B \u2295 D\u2032\n33B, where U\nmeans models under a certain size, 7B/13B/33B means LLaMA-7B/13B/33B and 7B2/13B2 means\nLLaMA2-7B/13B. \u2295 means an aggregation process in which all the reasoning paths from different\nsets are first combined and then Algorithm 1 is applied to deduplicate the reasoning paths with the\nsame calculation process regarding the equation forms and orders.\nWe can see, through the results visualized in Figure 5, that using the aggregated dataset D\u2032\nU13B and\nD\u2032\nU33B can lead to uniformly better performance than fine-tuning with datasets from a single model\nacross different model sizes. RFT on these two augmented datasets D\u2032\nU13B and D\u2032\nU33B decreases the\nperformance gaps among the same size models in SFT and RFT k = 100 which mean the combined\naugmented datasets provide enough reasoning supervision to fulfill the pre-training gap. We can\nassume with sufficient supervised data amounts, the performance indicator should be the model size\nbut not the pre-training losses.\n7\nPreprint\nFigure 5: The performance of RFT with rejection sampling samples from multiple models.\nWe have stated that it is expensive to apply RFT k = 100 on 33B models and it needs a temperate\ngrid search to achieve an improvement compared to SFT. However fine-tuning on D\u2032\nU13B has similar\nrejection sampling computational cost compared with sampling 100 times on 33B and achieve better\nperformance.\nAnother phenomenon is including D\u2032\n33B in aggregation barely influences the performance. To give\na more comprehensive analysis of the results, we calculate the average reasoning path number per\nquestion in Table 2 and depict a Venn diagram to visualize the source of different reasoning paths\nshown in Figure 6. In Table 2, the average reasoning path numbers of D\u2032\nU13B and D\u2032\nU33B surpass\nthose of a single model by large amounts, while D\u2032\nU33B only have slightly more reasoning paths than\nD\u2032\nU13B by 0.81. In the meanwhile, as shown in Figure 6, the models under and including the size of\n13B can contribute unique reasoning paths of similar proportion in D\u2032\nU33B around 15%. However,\nonly 6.5% of the reasoning paths can be exclusively acquired from LLaMA-33B-SFT model. This\nshows that the SFT model of 33B can provide limited reasoning diversity when sampling the training\nquestions. This finding is consistent with the results above in Table 1, indicating the 33B model (and\npossibly 65B and 70B models) can well memorize the human-annotated reasoning paths.\nFor 65B models, we find using D\u2032\nU13B does not improve the performance compared to SFT. The\nreason can be better models benefit less from the supervised sample amounts while it has learnt\nmore reasoning ability during pre-training.\nOverall, we can come to the conclusion that (1) RFT improves the mathematical reasoning per-\nformance of (worse) LLMs through diverse reasoning paths from rejection sampling of the SFT\nmodels, and aggregating more diverse reasoning paths can improve the performance further. (2)\nDifferent SFT models can contribute reasoning paths with different calculation processes from re-\njection sampling, leading to more diverse training data for RFT, and LLMs of larger parameter sizes\nmay degrade in generating diversified reasoning paths as a result of overfitting the training ques-\ntions. There may be a generation config or training config for large enough LMs not to overfit on\nthe training dataset while it is not trivial to find them.\nComparing to other baselines\nWe compare our RFT results of training on D\u2032\nU13B to several base-\nlines and the results are detailed in Table 3. Although LLaMA and LLaMA2 are top-tier open-\nsourced LLMs 1, their mathematical reasoning performances still lag behind the current proprietary\nLLMs which are of larger parameter scales, such as GPT-4 and PaLM2. Compared to results on\n1https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n8\nPreprint\nFigure 6: The Venn diagram of the proportions of the reasoning calculation paths that each model\nprovide to D\u2032\nU33B. For example, 15.5% (in the yellow part) of the reasoning calculation paths in\nD\u2032\nU33B can only be exclusively found in the rejection sampling results from LLaMA2-13B-SFT.\nopen-resourced models, our results on LLaMA present better performance than two recent state-\nof-the-art reasoning augmentation methods. Our RFT method is simpler compared to CoRE, since\nRFT does not require training verifier models and decoding with Monte Carlo Tree Search (MCTS).\nCompared to other open-sourced aligned language models, we can find that 7B models struggle at\na level of 35 scores which are very similar to SFT performances of LLaMA-7B. We guess they use\nGSM8K during their pre-training phase following (OpenAI, 2023) or human alignment fine-tuning\nphase following (Qingyi et al., 2023). Using our augmented dataset D\u2032\nU13B to replace the original\nGSM8K can significantly boost their 7B models\u2019 performances.\n4\nDISCUSSION\n4.1\nDIFFERENT DISTRIBUTION OF REASONING PATHS\nIn the aforementioned analysis of RFT training data, we observe that rejection sampling can augment\nthe training question with diverse reasoning calculation paths. In this section, we investigate whether\nRFT models can learn to generate different reasoning paths to reach the correct answers. We fine-\ntune LLaMA and LLaMA2 of 7B and 13B on D\u2032\nU13B. During inference, we sample 100 different\nreasoning paths from each trained model for each test set question with a temperature of 0.7. For\neach question, we compute the number of different calculation processes presented in 100 sampled\nreasoning paths that lead to the correct answer and draw histograms with respect to test set questions.\nSFT and RFT models on self-sampled datasets (RFT k=100) are included for comparison.\nAs shown in Figure 7, the models trained by RFT on D\u2032\nU13B exhibit more question counts than the\nmodels trained by RFT k=100 and SFT on the larger numbers of unique calculation processes. There\nare more question counts for SFT models where all the sampled reasoning paths only correspond to\none single calculation process and SFT models can barely generate more than 8 different calculation\n9\nPreprint\nBase Model\nTraining\nmaj1@1\nmaj1@K*\nProprietary LLMs\nGPT-4 (OpenAI, 2023)\n5-shot ICL\n92.0\n-\nGPT-3-175B (Brown et al., 2020)\nSFT\n34.0\n-\nPaLM2 (Anil et al., 2023)\n8-shot ICL\n80.7\n91.0@K=40\nPaLM-540B (Chowdhery et al., 2022)\n8-shot ICL\n56.5\n74.4@K=40\nChinchilla-70B (Uesato et al., 2022)\n5-shot ICL\n43.7\n58.6@K=96\nChinchilla-70B\nSFT\n58.9\n77.7@K=96\nOpen-sourced LLMs\nGPT-Neo-2.7B (Black et al., 2021)\nFCS + PCS (Ni et al., 2023)\n19.5\n41.4\nGPT-J-6B (Wang & Komatsuzaki, 2021)\nCoRE (Zhu et al., 2023)\n34.9\n63.2@K=40\nChatGLM2-6B (Zeng et al., 2022)\n8-shot ICL\n32.4\n-\nChatGLM2-6B\nHuman Alignment\n28.1\n-\nChatGLM2-12B\n8-shot ICL\n40.9\n-\nChatGLM2-12B\nHuman Alignment\n38.1\n-\nInternLM-7B (Team, 2023)\n4-shot ICL\n31.2\n-\nInternLM-7B\nHuman Alignment\n34.5\nLLaMA-7B\nSFT\n35.9\n48.7\nOur RFT on open-sourced LLMs\nLLaMA-7B\nRFT-U13B\n49.3\n61.8\nLLaMA2-7B\nRFT-U13B\n50.3\n65.6\nLLaMA-13B\nRFT-U13B\n52.1\n66.2\nLLaMA2-13B\nRFT-U13B\n55.4\n69.1\nTable 3: Compare GSM8K results with other baselines. RFT-U13B means models fine-tuned on\nD\u2032\nU13B. FCS and PCS represent fully-correct solutions and partially-correct solutions respectively.\n*K=100 if not specified.\nFigure 7: The histograms of question numbers solved with different numbers of unique reasoning\ncalculation paths. We show the difference in question counts between SFT and RFT U13B in two\ncases where the numbers of unique reasoning calculation paths are 1 or more than 10.\nprocesses for a question. This analysis demonstrates that diverse reasoning calculation paths in\ntraining data can equip the LLMs with finding diverse reasoning logic for solving math problems.\n10\nPreprint\nModel size\n7B\n7B-2\n13B\n13B-2\n33B\n65B\n70B\nPre-train FLOPs\n4.2 \u00d7 1022\n8.4 \u00d7 1022\n7.8 \u00d7 1022\n1.6 \u00d7 1023\n2.7 \u00d7 1023\n5.5 \u00d7 1023\n8.4 \u00d7 1023\nSFT FLOPs\n1.7 \u00d7 1017\n3.3 \u00d7 1017\n7.7 \u00d7 1017\n1.3 \u00d7 1018\n1.7 \u00d7 1018\nRFT Inference FLOPs\n1.4 \u00d7 1018\n2.6 \u00d7 1018\n6.9 \u00d7 1018\n1.4 \u00d7 1019\n1.8 \u00d7 1019\nRFT-U33B FLOPs\n3.0 \u00d7 1018\n5.7 \u00d7 1018\n1.3 \u00d7 1019\n2.2 \u00d7 1019\n3.0 \u00d7 1019\nPre-train GPU hrs\n82k\n184k\n135k\n368k\n530k\n1022k\n1720k\nSFT GPU hrs\n0.6\n4\n40\n74\n80\nRFT Inference GPU hrs\n10\n0.1k\n0.1k\n4.3k\n4.5k\nRFT-U33B GPU hrs\n9\n62\n0.6k\n1k\n1.2k\nICL Accuracy\n11.0\n14.6\n17.8\n28.7\n35.6\n50.9\n56.8\nSFT Accuracy\n35.9\n41.6\n43.0\n50.0\n54.6\n59.3\n63.2\nRFT-U33B Accuracy\n49.1\n51.2\n51.4\n55.3\n57.9\n-\n-\nTable 4: The statistics of FLOPs and GPU hours required for pre-training, SFT, RFT inference, and\nRFT. We take the pre-training GPU hours from Touvron et al. (2023a;b). The GPU hours for RFT\ninference are calculated for 7,473 train set questions and 100 samples per question. To make the best\nof GPUs and properly fit models into the GPU memory, we tune the inference batch size. For 33B,\n65B, and 70B models, we use DeepSpeed ZeRO3 (Rasley et al., 2020) for distributed training. All\nthe GPU hours are based on NVIDIA A100 80GB GPU. Note we use non-embedding parameters to\ncompute FLOPs in our experiments.\n4.2\nTOWARDS EXCELSIOR MATHEMATICAL REASONING\nFrom our findings, there are two main factors that can improve mathematical reasoning abilities\ngiven a preset amount of human-annotated samples, including: (1) Pre-training the LLMs to lower\nlosses; (2) Augmenting fine-tuning with rejection sampling. Through extensive experiments, we em-\npirically verify the scaling relationships between the mathematical reasoning performance of LLM\nwith both factors respectively. Out of the consideration of sustainable NLP, in this section, we inves-\ntigate the possible computational resources required to extrapolate the mathematical performance of\nLLMs by both factors and discuss how to improve the performance more efficiently.\nWe estimate the pre-training, SFT, RFT inference, and RFT FLOPs following Kaplan et al. (2020)\nand GPU times in Table 4 which is detailed in Appendix E. We can find that the cost times of SFT\n(\u223c 1 \u00d7 10\u22125) and RFT (\u223c 1 \u00d7 10\u22124) are negligible compared to pre-training. One can always use\nSFT and RFT to improve models\u2019 performance. However, it could be hard to use RFT to further\nboost performance. Since we need much more sampling counts (at an exponential level) to increase\ndistinct reasoning paths and there exists an upper bound of distinct reasoning path amount for a\ngiven math reasoning question.\nWe assume that performance follows RFT>SFT>ICL, from the findings in this paper we know\nthe improvement speed follows RFT<SFT<ICL. And if we have an omnipotent language model\nwhich has a pre-training loss that is the same as the corpus randomness, it could have RFT = SFT\n= ICL = 100. Thus when you pre-train a better language model (i.e. smaller pre-training loss),\nyour model\u2019s performance still follows RFT>SFT>ICL but their performance gaps are diminishing.\nSince you can obtain an RFT model without too much effort (compared to pre-training), then the\nmost important thing we should do is to decrease the model\u2019s pre-training loss. From LLaMA-7B to\nLLaMA2-7B, it needs to add 4.2\u00d71022 FLOPs to obtain a 2.1 improvement in the RFT-U33B setting\nwith a 0.05 pre-training loss decrease. From LLaMA-7B to LLaMA-13B, it adds 3.6 \u00d7 1022 FLOPs\nto obtain a 2.3 improvement in the RFT-U33B setting with a 0.07 pre-training loss decrease. While\nminimizing pre-training loss is expensive compared to SFT and RFT, we believe other abilities may\nfollow a similar pattern and better pre-training can benefit all other tasks.\n5\nCONCLUSIONS\nIn this paper, we are investigating the scaling relationship in supervising math reasoning abilities\nwith large language models. We find the relationship between math performance and pre-training\n11\nPreprint\nlosses, supervised data amount, and distinct reasoning paths. We find that better language models\nbenefit less with SFT and RFT, and the most important thing is to pre-train a better language model\ntowards excellent math reasoning abilities.\n6\nACKNOWLEDGEMENT\nWe would like to express our sincere appreciation to Tianhang Zhu, Runji Lin, Kai Dang, Keming\nLu, Wei Wang, and Junyang Lin for their valuable insights and contributions to this paper.\n7\nLIMITATIONS\nIn this paper, we miss the following parts which are very important for building math reasoning\nabilities for LLMs and should be discussed in the revised version of this paper or future works.\n\u2022 RFT for 65B and 70B LLaMA models.\n\u2022 Pre-training on the math-related corpus. This is obviously useful shown in Lewkowycz\net al. (2022). While the pre-training loss obtained here cannot align with general domain\npre-trained models\u2019 losses.\n\u2022 We do not regress any scaling laws in this paper since many numbers are estimated and\npre-training losses, ICL prompts and SFT settings of various models may not be aligned.\nREFERENCES\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experi-\nence replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur-\nran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/\npaper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-\nson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\nJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,\nKamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mer-\ncado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna\nKravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario\nAmodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai:\nHarmlessness from ai feedback, 2022.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au-\ntoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.\norg/10.5281/zenodo.5297715.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot\nlearners. ArXiv, abs/2005.14165, 2020. URL https://api.semanticscholar.org/\nCorpusID:218971783.\n12\nPreprint\nEthan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv\npreprint arXiv:2210.14891, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\n2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,\nand Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment,\n2023.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A\ncontinuous effort to measure large language models\u2019 reasoning performance, 2023a.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023b.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701, 2020.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer,\n2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-\nnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language models, 2022.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\nHan. Large language models can self-improve, 2022.\nZhanming Jie, Jierui Li, and Wei Lu. Learning to reason deductively: Math word problem solving\nas complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 5944\u20135955, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.410. URL\nhttps://aclanthology.org/2022.acl-long.410.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=e2TBb5y0yFf.\n13\nPreprint\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.\nMAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, pp. 1152\u20131157, San Diego, California, June 2016. Association for Computational Linguis-\ntics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,\nand Ee-Peng Lim. Mwptoolkit: An open-source framework for deep learning-based math word\nproblem solvers, 2021.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with lan-\nguage models, 2022.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making\nlanguage models better reasoners with step-aware verifier. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013\n5333, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https:\n//aclanthology.org/2023.acl-long.291.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nNiklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-\nmane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language\nmodels, 2023.\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir\nRadev, and Jianfeng Gao.\nLearning math reasoning from self-sampled correct and partially-\ncorrect solutions. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=4D4TSJE6-K.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Au-\ngustus Odena. Show your work: Scratchpads for intermediate computation with language models,\n2021.\nOpenAI. Gpt-4 technical report, 2023.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pp. 2080\u2013\n2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nnaacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.\nSi Qingyi, Wang Tong, Gu Naibin, Liu Rui, and Lin Zheng. Alpaca-cot: An instruction-tuning\nplatform with unified interface of instruction collection, parameter-efficient methods, and large\nlanguage models. https://github.com/PhoebusSi/alpaca-CoT, 2023.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\nKDD \u201920, pp. 3505\u20133506, New York, NY, USA, 2020. Association for Computing Machin-\nery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.\n1145/3394486.3406703.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-\nsoning abilities of neural models, 2019.\n14\nPreprint\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into\nsmaller language models. In Findings of the Association for Computational Linguistics: ACL\n2023, pp. 7059\u20137073, Toronto, Canada, July 2023. Association for Computational Linguistics.\nURL https://aclanthology.org/2023.findings-acl.441.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\nInternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.\nhttps://github.com/InternLM/InternLM, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and\noutcome-based feedback, 2022.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le.\nFinetuned language models are zero-shot learn-\ners.\nArXiv, abs/2109.01652, 2021.\nURL https://api.semanticscholar.org/\nCorpusID:237416585.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. Trans. Mach. Learn. Res., 2022, 2022a. URL https://api.semanticscholar.\norg/CorpusID:249674500.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc\nLe, and Denny Zhou.\nChain of thought prompting elicits reasoning in large language mod-\nels.\nArXiv, abs/2201.11903, 2022b.\nURL https://api.semanticscholar.org/\nCorpusID:246411621.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large\nlanguage models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023a.\n15\nPreprint\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank\nresponses to align language models with human feedback without tears, 2023b.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with\nreasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Ad-\nvances in Neural Information Processing Systems, 2022. URL https://openreview.net/\nforum?id=_3ELRdg2sgI.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\nTianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E. Gonzalez. The wisdom of\nhindsight makes language models better instruction followers, 2023.\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang,\nand Yujiu Yang. Solving math word problems via cooperative reasoning induced language mod-\nels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 4471\u20134485, Toronto, Canada, July 2023. Association for Compu-\ntational Linguistics. URL https://aclanthology.org/2023.acl-long.245.\nA\nDETAILED EXPERIMENT SETTING\nA.1\nSFT ON GSM8K\nWe fine-tune GSM8K with 3 epochs and a batch size of 128 on NVIDIA A100 GPUs. We use 8\nGPUs for 7B and 13B models, 16 GPUs for 33B models, and 32 GPUs for 65B and 70B models\nduring fine-tuning. We use a peak learning rate of 2e-5 with a 3% learning rate warmup. We\nevaluate the results on the final epoch. We use greedy decode to calculate maj1@1 and decode with\ntemperature 0.7 to calculate maj1@100.\nA.2\nSFT ON DOWNSAMPLED GSM8K\nWe random downsample GSM8K dataset for fine-tuning. We find that using 3 epochs for little\ndata will result in very poor results which are listed in Table 5. We search training epoch among\n{3,\n3\ndata fraction} and evaluate the latest epoch. We report better test results among these two different\nepoch settings.\nA.3\nREJECTION SAMPLING FINE-TUNING ON GSM8K\nWe use an SFT model \u03c0 to sample on training dataset for k = 100 times with a temperature of\n0.7. We extract the equation list in generated reasoning paths by finding <<equation>> first,\nremoving all white spaces, and joining the equation string list by a special symbol to a string (called\n16\nPreprint\nit get equation in our algorithm) for deduplication. We select the reasoning paths by this algorithm:\nAlgorithm 1: Reasoning Path Selection\nData: Reasoning paths for question q, Rq\nResult: Selected reasoning paths for question q, Rs\nq\n1 Initialize selected reasoning paths, Rs\nq = list()\n2 Initialize appeared equation set, Es\nq = set()\n3 for r in Rq do\n4\nif get equation(r) /\u2208 Es\nq then\n5\nRs\nq.append(r);\n6\nEs\nq .update([get equation(r)])\n7\nend\n8\nelse\n9\nfind rs \u2208 Rs\nq s.t. get equation(rs) = get equation(r);\n10\nif P\ni:rs\ni \u2208Esq ,rs\ni \u0338=rs Levenstein dist(r, rs\ni ) > P\ni:rs\ni \u2208Esq ,rs\ni \u0338=rs Levenstein dist(rs, rs\ni ) then\n11\nrs = r;\n12\nend\n13\nend\n14 end\nWe are trying to find the most dissimilar reasoning paths based on Levenstein distances. The idea\ncomes from we want diverse reasoning paths for better generalization.\nB\nDETAILED RESULTS OF SFT AND RFT\nWe list detailed results of SFT and RFT in Table 5 and 6.\nModel\nData\nEpoch\n7B\n7B-2\n13B\n13B-2\n33B\n65B\n70B-2\nICL-8shot\n0\n0\n11.0\n14.6\n17.8\n28.7\n35.6\n50.9\n56.8\nSFT\n1/32\n96\n9.5\n10.1\n8.6\n17.1\n18.6\n25.2\n27.4\nSFT\n1/16\n48\n14.3\n15.5\n14.2\n23.9\n25.9\n28.9\n33.6\nSFT\n1/8\n24\n17.9\n20.8\n18.4\n28.5\n31.6\n35.8\n38.9\nSFT\n1/4\n12\n21.6\n27.7\n26.7\n36.3\n38.4\n45.6\n46.9\nSFT\n1/2\n6\n29.0\n33.1\n35.2\n43.7\n48.6\n50.5\n57.5\nSFT\n1/32\n3\n7.8\n14.2\n0.0\n5.9\n25.3\n28.9\n15.8\nSFT\n1/16\n3\n12.7\n16.2\n7.4\n27.7\n29.2\n39.5\n52.8\nSFT\n1/8\n3\n16.5\n21.8\n19.5\n33.4\n39.3\n46.0\n57.8\nSFT\n1/4\n3\n22.7\n28.1\n27.4\n37.5\n44.6\n50.4\n57.8\nSFT\n1/2\n3\n30.9\n34.6\n36.1\n45.3\n50.8\n55.6\n61.0\nSFT\n7.4K\n3\n35.9\n41.6\n43.0\n50.0\n54.6\n59.3\n63.2\nRFT no dedup\n1/32\n3\n37.5\n-\n-\n-\n-\n-\n-\nRFT no dedup\n1/16\n3\n38.3\n-\n-\n-\n-\n-\n-\nRFT no dedup\n1/8\n3\n41.1\n-\n-\n-\n-\n-\n-\nRFT no dedup\n1/4\n3\n41.2\n-\n-\n-\n-\n-\n-\nRFT no dedup\n1/2\n3\n43.9\n-\n-\n-\n-\n-\n-\nRFT no dedup\n400K\n3\n43.6\n46.7\n46.9\n53.7\n-\n-\n-\nRFT k=1\n\u223c12K\n3\n37.6\n43.4\n42.7\n52.1\n-\n-\n-\nRFT k=3\n\u223c15K\n3\n39.0\n45.3\n45.2\n51.9\n-\n-\n-\nRFT k=6\n\u223c18K\n3\n39.5\n45.6\n46.8\n52.2\n-\n-\n-\nRFT k=12\n\u223c22K\n3\n41.6\n45.3\n48.0\n53.1\n-\n-\n-\nRFT k=25\n\u223c28K\n3\n40.9\n46.5\n46.0\n52.6\n-\n-\n-\nRFT k=50\n\u223c35K\n3\n40.7\n47.0\n49.4\n54.5\n-\n-\n-\nRFT k=100\n\u223c47K\n3\n41.7\n47.5\n49.1\n54.8\n54.5\n-\n-\nRFT-U13B\n104K\n3\n49.3\n50.3\n52.1\n55.4\n56.5\n59.0\n62.3\nRFT-U33B\n110K\n3\n49.1\n51.2\n51.4\n55.3\n57.9\n59.7\n64.8\nTable 5: Detailed numerical results in this paper, some experiments are still under running. We\nreport maj1@1 (accuracy) in this table.\n17\nPreprint\nSetting\n7B\n7B-2\n13B\n13B-2\n33B\n65B\n70B-2\nICL-8shot\n11.0/18.1\n14.6/-\n17.8/29.3\n28.7/-\n35.6/53.1\n50.9/69.7\n56.8/-\nSFT\n35.9/48.7\n41.6/55.4\n43.0/55.2\n50.0/61.7\n54.6/72.6\n59.3/69.7\n63.2/73.5\nRFT k=100\n41.7/52.7\n47.5/58.7\n49.1/59.9\n54.8/65.4\n54.5/-\n-\n-\nRFT-U13B\n49.3/61.8\n50.3/65.6\n52.1/66.2\n55.4/69.1\n56.5/-\n59.0/-\n62.3/-\nRFT-U33B\n49.1/61.6\n51.2/64.1\n51.4/66.3\n55.3/69.1\n57.9/71.3\n59.7/-\n64.8/-\nTable 6: Detailed numerical results in this paper, some experiments are still under running. We\nreport maj1@100 in this table.\nQuery A\nWeng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting.\nHow much did she earn?\nPath 1\nWeng earned 12/60 = $\u2329\u232912/60=0.2\u232a\u232a0.2 an hour for the 50 minutes that she worked yes-\nterday. Therefore, she earned $0.2 x 50 = $\u2329\u23290.2*50=10\u232a\u232a10 for the 50 minutes that she\nworked yesterday.#### 10\nPath 2\nWeng earns 12 / 60 = $\u2329\u232912/60=0.2\u232a\u232a0.2 per minute.\nSo, Weng earned 50 * 0.2 =\n$\u2329\u232950*0.2=10\u232a\u232a10 for the 50 minutes of babysitting. #### 10\nPath 3\nSince there are 60 minutes in an hour, then Weng did 50/60 = \u2329\u232950/60=0.8333\u232a\u232a0.8333 of\nan hour of babysitting. So, she earned $12 x 0.8333 = $\u2329\u232912*0.8333=10\u232a\u232a10. #### 10\nPath 4\nShe earns 12 * 50 / 60 = $\u2329\u232912*50/60=10\u232a\u232a10. #### 10\nPath 5\nWeng earns 12 * (50 / 60) = $\u2329\u232912*(50/60)=10\u232a\u232a10. #### 10\nQuery B\nA local college is offering German lessons and currently has 8 students enrolled. Through\nadvertising, 8 more became interested but a fourth of these dropped out within a day. 2\nmore got frustrated and left. The class then rallied to show how simple the course actually\nis and increased enrollment by 5 times the amount of students already enrolled in the\nclass, but 2 had to drop it because of scheduling conflicts. After one last rally, 6 more\npeople enrolled. As the days passed, half of the class eventually dropped, and half of the\nremaining students graduated. How many are still enrolled?\nPath 1\n8+8=\u2329\u23298+8=16\u232a\u232a16 students are enrolled in the German class.\n8*.25=\u2329\u23298*.25=2\u232a\u232a2\ndropped out from the new recruits. 2 more left, so the class had 16-2-2=\u2329\u232916-2-2=12\u232a\u232a12\nstudents left. There were 12*5+12=\u2329\u232912*5+12=72\u232a\u232a72 total students in the class after\nthe rally.\nTwo people had to drop the class, so there were 72-2=\u2329\u232972-2=70\u232a\u232a70 left.\n70+6=\u2329\u232970+6=76\u232a\u232a76 were enrolled after the final rally. 76/2=\u2329\u232976/2=38\u232a\u232a38 are left after\nhalf the class dropped. 38/2=\u2329\u232938/2=19\u232a\u232a19 are left after half the class graduated. ####\n19\nPath 2\n8+8=\u2329\u23298+8=16\u232a\u232a16 students are enrolled after the first rally. 16/4=\u2329\u232916/4=4\u232a\u232a4 dropped\nout. 16-4=\u2329\u232916-4=12\u232a\u232a12 remain. 12*5+12=\u2329\u232912*5+12=72\u232a\u232a72 after the second rally.\n72-2=\u2329\u232972-2=70\u232a\u232a70 after 2 more dropped. 70+6=\u2329\u232970+6=76\u232a\u232a76 after 6 more enrolled.\n76/2=\u2329\u232976/2=38\u232a\u232a38 remain after half the class dropped. 38/2=\u2329\u232938/2=19\u232a\u232a19 remain after\nhalf the class graduated. #### 19\nPath 3\nAdvertising brought in 8+8=\u2329\u23298+8=16\u232a\u232a16 students. Of these, 16/4=\u2329\u232916/4=4\u232a\u232a4 dropped\nout.\n16-4-2=\u2329\u232916-4-2=10\u232a\u232a10 remained.\nThen 10*5=\u2329\u232910*5=50\u232a\u232a50 more enrolled.\nThis brought the class to 50+10=\u2329\u232950+10=60\u232a\u232a60 students. Then 60+6=\u2329\u232960+6=66\u232a\u232a66\nenrolled. So there were 66+10=\u2329\u232966+10=76\u232a\u232a76 students. Then 76/2=\u2329\u232976/2=38\u232a\u232a38\ndropped. So 76-38=\u2329\u232976-38=38\u232a\u232a38 remained. Then 38/2=\u2329\u232938/2=19\u232a\u232a19 graduated. So\n38-19=\u2329\u232938-19=19\u232a\u232a19 were left. #### 19\nTable 7: Cases of generated reasoning paths with different reasoning complexity from rejection\nsampling for RFT. The calculations are highlighted in red.\nC\nCASE STUDY OF RFT\nIn this section, we present the cases of the training samples from rejection sampling. The case studies\nwould shed light on how RFT potentially improves the mathematical reasoning performance of\nLLMs. The cases are shown in Table 7. As aforementioned, RFT considers the reasoning paths with\ndifferent calculation processes regarding equation forms or orders, leading to the correct answers.\nIn the cases from Table 7, all the reasoning paths from RFT result in the correct answer of 10, while\nthe calculation processes of reasoning are diverse. Path 1 and 2, as well as Path 4 and 5, are different\nin the equation forms as highlighted in red. Path 1 and 2 present a two-step calculation reasoning\nprocess while Path 4 and 5 alter to a one-step calculation reasoning process. The case demonstrates\n18\nPreprint\nthat rejection sampling can potentially provide more supervision signals that improve mathematical\nreasoning performance. The filtered reasoning paths sampled from LLMs themselves are of similar\nquality to the reasoning demonstrations from human annotations.\nD\nPRELIMINARY EXPERIMENTS\nD.1\nSELF QUERY AUGMENTATION\nThrough our preliminary experiments and case studies, the errors made by the fine-tuned LLMs are\npartly attributed to the incorrect reasoning chains where LLMs mistakenly understand the context\ninformation or fail to consider all the information in the queries. Although such incorrect reasoning\nchains lead to wrong answers to the original queries, the reasoning chains themselves represent\nreasonable logic. For example, for the query Josh decides to try flipping a house. He buys a house\nfor $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How\nmuch profit did he make?, a fine-tuned LLaMA model predicts The value of the house increased\nby 80,000*.15=$12,000. So the house was worth 80,000+12,000=$92,000. So he made a profit\nof 92,000-80,000-50,000=$42,000 where the model erroneously interprets 150% as 15%, but the\nreasoning chain is reasonable if we ignore the error.\nTherefore, such wrong predictions made by the LLMs may be correct under other queries (if we\nchange 150% to 15% in the above example). We conduct experiments to generate queries for the\npredicted reasoning chains. This is a similar idea to the hindsight experience replay (Andrychowicz\net al., 2017) in reinforcement learning where the method is designed to deal with the sparse reward\nproblems by changing the original objectives for the failed samples to form samples with positive\nrewards. Such an idea was recently adopted by HIR (Zhang et al., 2023) to better align LLMs with\ninstructions.\nConcretely, we reformat GSM8K reversely by predicting the query given the corresponding ground-\ntrue reasoning result and then we fine-tune a LLaMA model on the reversed task. We use this model\nto generate queries on the predicted reasoning chains by a normally fine-tuned LLaMA model on\nthe training set of GSM8K, formalizing a training sample for augmentation. We experiment on the\nLLaMA 7B model and fine-tune models on the data mixing original and generated samples or solely\non generated samples.\nThe results are shown in the left subfigure in Figure 8. We can see that fine-tuning with self query\naugmentation data leads to the worst results, and the performance of mixing the original data with\nself query augmented data still falls short of that of the original data. The fine-tuned performance for\nmathematical reasoning does not benefit from the naive idea of self query augmentation. Through\nseveral case studies of generated data, we find that there are two major defects in the generated\ndata. The first one is some reasoning chains themselves are not logically reasonable, for example,\nthere may be some calculation errors in the reasoning chains. The second one is that the generated\nquery may not be suitable for a reasoning chain. The query generation model may still erroneously\ninterpret the information in the reasoning chains. Both defects attribute to a mediocre augmented\ndata quality, hence can be possible reasons for the failure of this data augmentation procedure.\nD.2\nSELF REVISING AUGMENTATION\nWe also explore improving the mathematical reasoning abilities of LLMs through revising augmen-\ntation. To equip LLaMA with revising abilities, we generate a revising dataset by first sampling\nK reasoning paths from a fine-tuned LLaMA model, then concatenating the query with one of the\nsampled reasoning paths using a template, and finally pairing with the ground-true reasoning path\nto form a training sample. We use a sampling temperature of 0.7 for generating reasoning paths.\nDuring inference, we use the fine-tuned revising model to revise the prediction from the normally\nfine-tuned model.\nThe results are shown in the middle subfigure of Figure 8. We can see that with K = 1 the revising\nmodel improves the final accuracy marginally comparing 36.09% to 35.90%. Surprisingly, as we\nincrease K, the performances degrade. The possible defect of the revising model is that generated\nsamples on the training set for revising training suffer from a distribution discrepancy with generated\nsamples on the test set for revising inference. The sampled reasoning paths on the training set may\n19\nPreprint\nFigure 8: Results for different methods of self data augmentation. GSM. and H. represent GSM8K\nand Hindsight respectively. The red dotted lines in the middle and right figures represent the results\nof vanilla fine-tuning on GSM8K.\nhave a larger lexical similarity to the ground true reasoning paths compared to those on the test set.\nTherefore we try two different procedures to alleviate such an issue.\n1. We use the sampled reasoning path with the largest Levenstein distance out of K sampled paths\nwith respect to the ground true path to form a training sample.\n2. We split the train set to N folds, and fine-tune a model on each N \u2212 1 folds and sampling\nreasoning path on the left fold.\nThe results are shown in the middle and right subfigures in Figure 8, we can see that when leveraging\nLevenstein distance for reasoning path selection, the fine-tuned revising model enjoys a performance\nboost, harvesting uniformly better performance than the fine-tuning baseline across different K\u2019s.\nThe results demonstrate that for the revising performance, the lexical diversity of reasoning paths\nmatters when constructing training samples. However, the revising performance does not benefit\nfrom the N-fold procedure.\nE\nESTIMATING FLOPS OF SFT AND RFT\nWe mainly follow the notations of (Kaplan et al., 2020) here.\nTraining FLOPs\nFor each input sample of length nctx in GSM8K dataset, we can split it into two\nparts:\nnctx = nQ + nR\n(1)\nwhere nQ, nR denotes the length of question and generated reasoning path and answers respectively.\nCtrain \u2248 6NnctxNs\n(2)\nwhere Ns denotes the numbers of samples.\nInference FLOPs\nWe roughly computed the FLOPs of each token during the forward pass:\nCforward(nctx) = 2N + 2nlayernctxdmodel\n(3)\nTo ensure the results were more accurate and reliable, we also took into account the Key-Value (KV)\ncache during the decoding procedure.\nKVcache \u2248 4nlayerd2\nmodel\n(4)\n20\nPreprint\nTherefore, we obtain the FLOPs per token during the forward pass considering the KV cache.\nC\n\u2032\nforward(nctx) = 2N + 2nlayernctxdmodel \u2212 KVcache\n(5)\n= 24nlayerd2\nmodel + 2nlayernctxdmodel \u2212 4nlayerd2\nmodel\n(6)\n= 20nlayerd2\nmodel + 2nlayernctxdmodel\n(7)\n\u2248 1.66N + 2nlayernctxdmodel\n(8)\nThe total inference FLOPs are computed as follows:\nCtotal = Ns \u00b7 [nqCforward(nq) +\nnq+nr\nX\ni=nq\ni \u00b7 C\n\u2032\nforward(i)]\n(9)\nwhere Ns denotes the numbers of samples. nq, nr denotes the average length (tokens) of the user\nquery and generated response respectively. In GSM8K dataset, nq \u2248 66 and nr \u2248 130.\n21\n"
  },
  {
    "title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies",
    "link": "https://arxiv.org/pdf/2308.01546.pdf",
    "upvote": "15",
    "text": "MusicLDM: Enhancing Novelty in Text-to-Music\nGeneration Using Beat-Synchronous Mixup Strategies\nKe Chen1\u2217\nYusong Wu2\u2217\nHaohe Liu3\u2217\nMarianna Nezhurina4\nTaylor Berg-Kirkpatrick1\nShlomo Dubnov1\n1University of California San Diego\n2Mila, Quebec Artificial Intelligence Institute, Universit\u00e9 de Montr\u00e9al\n3Centre for Vision Speech and Signal Processing, University of Surrey\n4LAION\nAbstract\nDiffusion models have shown promising results in cross-modal generation tasks,\nincluding text-to-image and text-to-audio generation.. However, generating music,\nas a special type of audio, presents unique challenges due to limited availability of\nmusic data and sensitive issues related to copyright and plagiarism. In this paper,\nto tackle these challenges, we first construct a state-of-the-art text-to-music model,\nMusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music\ndomain. We achieve this by retraining the contrastive language-audio pretraining\nmodel (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a\ncollection of music data samples. Then, to address the limitations of training\ndata and to avoid plagiarism, we leverage a beat tracking model and propose two\ndifferent mixup strategies for data augmentation: beat-synchronous audio mixup\nand beat-synchronous latent mixup, which recombine training audio directly or\nvia a latent embeddings space, respectively. Such mixup strategies encourage the\nmodel to interpolate between musical training samples and generate new music\nwithin the convex hull of the training data, making the generated music more\ndiverse while still staying faithful to the corresponding style. In addition to popular\nevaluation metrics, we design several new evaluation metrics based on CLAP\nscore to demonstrate that our proposed MusicLDM and beat-synchronous mixup\nstrategies improve both the quality and novelty of generated music, as well as the\ncorrespondence between input text and generated music.\n1\nIntroduction\nText-guided generation tasks have gained increasing attention in recent years and have been applied\nto various modailties, including text-to-image, text-to-video, and text-to-audio generation. Text-\nto-image generation has been used to create both realistic and stylized images based on textual\ndescriptions, which can be useful in various scenarios including graphic design. Text-to-audio\ngeneration is a relatively new, but rapidly growing area, where the goal is to generate audio pieces,\nsuch as sound events, sound effects, and music, based on textual descriptions. Diffusion models have\nshown superior performance in these types of cross-modal generation tasks, including systems like\nDALLE-2 [29] and Stable Diffusion [31] for text-to-image; and AudioGen [22], AudioLDM [24],\nand Make-an-Audio [16] for text-to-audio.\n*The first three authors have equal contribution. Contact: Ke Chen (knutchen@ucsd.edu), Yusong Wu\n(wu.yusong@mila.quebec), and Haohe Liu (haohe.liu@surrey.ac.uk)\nPreprint. Under review.\narXiv:2308.01546v1  [cs.SD]  3 Aug 2023\nAs a special type of audio generation, text-to-music generation has many practical applications [2, 7].\nFor instance, musicians could use text-to-music generation to quickly build samples based on specific\nthemes or moods and speed up their creative process. Amateur music lovers could leverage generated\npieces to learn and practice for the purpose of musical education. However, text-to-music generation\npresents several specific challenges. One of the main concerns is the limited availability of text-music\nparallel training data [1]. Compared to other modalities such as text-to-image, there are relatively\nfew text-music pairs available, making it difficult to train a high-quality conditional model. Large\nand diverse training sets may be particularly imperative for music generation, which involves many\nnuanced musical concepts, including melody, harmony, rhythm, and timbre. Further, the effectiveness\nof diffusion models trained on more modest training sets has not been fully explored. Finally, a\nrelated concern in text-to-music generation is the risk of plagiarism or lack of novelty in generated\noutputs [1]. Music is often protected by copyright laws, and generating new music that sounds too\nsimilar to existing music can lead to legal issues. Therefore, it is important to develop text-to-music\nmodels that can generate novel and diverse music while avoiding plagiarism, even when trained on\nrelatively small training datasets.\nIn this paper, we focus on both these challenges: we develop a new model and training strategy\nfor learning to generate novel text-conditioned musical audio from limited parallel training data.\nCurrently, since there is no open-source model for text-to-music generation, we first construct a\nstate-of-the-art text-to-music generation model, MusicLDM, which adapts the Stable Diffusion [31]\nand AudioLDM [24] architectures to the music domain. Next, to address the limited availability of\ntraining data and to encourage novel generations, we adapt an idea from past work in other modalities:\nmixup [41], which has been applied to computer vision and audio retrieval tasks, augments training\ndata by recombining existing training points through linear interpolation. This type of augmentation\nencourages models to interpolate between training data rather than simply memorizing individual\ntraining examples, and thus may be useful in addressing data limitations and plagiarism in music\ngeneration. However, for music generation, the naive application of mixup is problematic. Simply\ncombining waveforms from two distinct musical pieces leads unnatural and ill-formed music: tempos\nand beats (as well as other musical elements) are unlikely to match. Thus, we propose two novel\nmixup strategies, specifically designed for music generation: beat-synchronous audio mixup (BAM)\nand beat-synchronous latent mixup (BLM), which first analyze and beat-align training samples before\ninterpolating between audio samples directly or encoding and then interpolating in a latent space,\nrespectively.\nWe design new metrics that leverage a pretrained text and audio encoder (CLAP) to test for plagiarism\nand novelty in text-to-music generation. In experiments, we find that our new beat-synchronous\nmixup augmentation strategies, by encouraging the model to generate new music within the convex\nhull of the training data, substantially reduce the amount of copying in generated outputs. Further, our\nnew model, MusicLDM, in combination with mixup, achieves better overall musical audio quality as\nwell as better correspondence between output audio and input text. In both automatic evaluations and\nhuman listening tests, MusicLDM outperforms state-of-the-art models at the task of text-to-music\ngeneration while only being trained on 9K text-music sample pairs.\nMusic samples and qualitative results are available at musicldm.github.io. Code and models are\navailable at https://github.com/RetroCirce/MusicLDM/.\n2\nRelated Work\n2.1\nText-to-Audio Generation\nText-to-audio generation (TTA) [24, 22, 40] is a type of generative task that involves creating\naudio content from textual input. In years past, text-to-speech (TTS) [30, 35] achieved far better\nperformance than other types of audio generation. However, with the introduction of diffusion models,\nsuperior performance in various generation tasks became more feasible. Recent work has focused\non text-guided generation in general audio, with models such as Diffsound [40], AudioGen [22],\nAudioLDM [24], and Make-an-Audio [16] showing impressive results. In the domain of music,\ntext-to-music models include the retrieval-based MuBERT [26], language-model-based MusicLM\n[1], diffusion-based Riffusion [8] and Noise2Music [15]. However, a common issue with most recent\ntext-to-audio/music models is the lack of open-source training code. Additionally, music models\n2\naudio waveform \nmel-spectrogram\nSTFT+MelFB\nA spectacular\ndramatic trailer\ncorresponding text\nCLAP\nAudio Encoder\nCLAP\nText Encoder\nU-Net Latent Diffusion Model\n......\nFiLM Cocatenation\nVAE\nDecoder\nVAE\nEncoder\nHifi-GAN\nOR\nFigure 1: The architecture of MusicLDM, which contains a contrastive language-audio pretraining\n(CLAP) model, an audio latent diffusion model with VAE, and a Hifi-GAN nerual vocoder.\noften requires large amounts of privately-owned music data that are inaccessible to the public, which\nmakes it difficult for researchers to reproduce and build upon their work.\nAmong all these models, AudioLDM is based on open-source Stable Diffusion [31], CLAP [39],\nand HiFi-GAN [19] architectures. Therefore, we base our text-to-music generation model on the\nAudioLDM architecture, to create MusicLDM for our experiments.\n2.2\nPlagiarism on Diffusion Models\nDiffusion models have been shown to be highly effective at generating high-quality and diverse\nsamples for text-to-image tasks. However, a potential issue with these models is the risk of plagiarism\n[33, 3], or the generation novelty. As stated by [33], diffusion models are capable of memorizing and\ncombining different image objects from training images to create replicas, which can lead to highly\nsimilar or even identical samples to the training data. [3] explores different methods that could extract\nthe training data with a generate-and-filter pipeline, showing that new advances in privacy-preserving\ntraining of diffusion models are required. Such issues are especially concerning in the domain\nof music, where copyright laws are heavily enforced and violations can result in significant legal\nand financial consequences. Therefore, there is a need to develop strategies to mitigate the risk of\nplagiarism in text-to-music generation using diffusion models.\n2.3\nMixup on Data Augmentation\nMixup [41] is a widely used data augmentation technique that has shown remarkable success in\nimproving model generalization and mitigating overfitting. The basic principle of mixup is to linearly\ncombine pairs of training samples to effectively construct new samples that lie on the line connecting\nthe original samples in the feature space, encouraging the model to learn a more continuous and robust\ndecision boundary. In this paper, we explore the mixup technique in the context of text-to-music\ngeneration based on latent diffusion models. Different from the mixup in other modalities, music\nmixup involves a delicate balance of musical elements to prevent the mixed music from being chaotic\nnoise. Moreover, in diffusion models, mixup also can refer to the combination of latent features,\nrather than music signals. We propose two mixup strategies tailored for music latent diffusion models\nand explore their potential benefits for data augmentation and generation performance.\n3\nMethodology\n3.1\nMusicLDM\nAs illustrated in Figure 1, MusicLDM has similar architecture as AudioLDM: a contrastive language-\naudio pretraining (CLAP) model [39], an audio latent diffusion model [24] with a pretrained varia-\ntional auto-encoder (VAE) [18], and a Hifi-GAN neural vocoder [19].\nFormally, given an audio waveform x \u2208 RT its corresponding text, where T is the length of samples,\nwe feed the data into three modules:\n3\n1. We pass x through the audio encoder [5] of CLAP faudio(\u00b7), to obtain the semantic audio\nembedding Ea\nx \u2208 RD, where D is the embedding dimension.\n2. We pass the text of x through the text encoder [25] of CLAP ftext(\u00b7), to to obtain the semantic\ntext embedding Et\nx \u2208 RD.\n3. We transform x into in the mel-spectrogram xmel \u2208 RT \u00d7F . Then we pass xmel into the VAE\nencoder, to obtain an audio latent representation y \u2208 RC\u00d7 T\nP \u00d7 F\nP , where T is the mel-spectrogram\nframe size, F is the number of mel bins, C is the latent channel size of VAE, and P is the\ndownsampling rate of VAE. The VAE is pretrained to learn to encoder and decode the mel-\nspectrogram of music data.\nIn MusicLDM, the latent diffusion model has a UNet architecture where each encoder or decoder\nblock is composed of a ResNet layer [11] and a spatial transformer layer [31]. During the training,\nthe semantic embedding of the input audio Ex is concatenated with the latent feature of each UNet\nencoder and decoder block by the FiLM mechanism [27]. The output of the diffusion model is the\nestimated noise \u03f5\u03b8(zn, n, Ex) from n to (n \u2212 1) time step in the reverse process, where \u03b8 is the\nparameter group of the diffusion model, and zn is the n-step feature generated by the forward process.\nWe adopt the training objective [13, 21] as the mean square error (MSE) loss function:\nLn(\u03b8) = Ez0,\u03f5,n||\u03f5 \u2212 \u03f5\u03b8(zn, n, Ex)||2\n2\n(1)\nwhere z0 = y is the audio latent representation from VAE (i.e., the groundtruth), and \u03f5 is the target\nnoise for training. More details regarding the training and the architecture of the latent diffusion\nmodel can be referred in Appendix A.\nFor MusicLDM, we make two changes from the original AudioLDM to enhance its performance on\ntext-to-music generation. First, since the original contrastive language-audio pretraining (CLAP)\nmodel is pretrained on text-audio pair datasets dominated by sound events, sound effects and natural\nsounds, we retrained the CLAP on text-music pair datasets (details in Appendix B) to improve its\nunderstanding of music data and corresponding texts. We also retrained the Hifi-GAN vocoder on\nmusic data to ensure high-quality transforms from mel-spectrograms to music waveforms. Second,\nin the original AudioLDM, the model is only fed with audio embeddings as the condition during\nthe training process, i.e., Ex = Ea\nx; and it is fed with text embeddings to perform the text-to-audio\ngeneration, i.e., Ex = Et\nx. This approach leverages the alignment of text and audio embeddings\ninside CLAP to train the latent diffusion model with more audio data without texts. However, this\naudio-to-audio training \u03f5\u03b8(zn, n, Ea\nx) is essentially an approximation of the text-to-audio generation\n\u03f5\u03b8(zn, n, Et\nx). Although CLAP is trained to learn joint embeddings for text and audio, it does not\nexplicitly enforce the embeddings to be distributed similarly in the latent space, which can make\nit challenging for the model to generate coherent text-to-audio outputs solely with audio-to-audio\ntraining. This problem becomes more severe when the available text-music pair data is limited.\nMoreover, relying solely on audio embeddings ignores the available text data, which means that we\nare not leveraging the full potential of our dataset. Consequently, generating accurate and realistic\ntext-to-audio generations may not be effective.\nTo further investigate this task, we introduce two additional training approaches for comparison:\n1. Train the MusicLDM directly using the text embedding as the condition, i.e., \u03f5\u03b8(zn, n, Et\nx)\n2. Train the MusicLDM using the audio embedding as the condition, then finetune it with text\nembedding, i.e.,\u03f5\u03b8(zn, n, Ea\nx) \u2192 \u03f5\u03b8(zn, n, Et\nx)\nThe first approach follows the original target of text-to-audio, serving as a comparison with audio-to-\naudio training. The second approach is proposed as an improvement on audio-to-audio generation.\nas we shift the condition distribution from the audio embedding back to the text embedding during\nthe training of the diffusion model. In section 4.2, we compared the above two approaches with the\noriginal audio-to-audio training approaches to determine the best approach for generating high-quality\nand highly correlated text-to-music outputs.\n3.2\nBeat-Synchronous Mixup\nAs shown in Figure 2, we propose two mixup strategies to augment the data during the MusicLDM\ntraining: Beat-Synchronous Audio Mixup (BAM) and Beat-Synchronous Latent Mixup (BLM).\n4\nBeat Transformer\nMusic Dataset\ntempo \ndownbeat\nalignment\nBeat-Synchronous Audio Mix-Up\nCLAP\nVAE\nDiffusion\nModel\nCLAP\nVAE Dec.\nBeat-Synchronous  Latent Mix-Up\nVAE\nEnc.\nHifi-GAN\nDiffusion\nModel\nAudio Space\nLatent Space\naudio mix-up\nlatent mix-up\nMusic\nOthers\nFigure 2: Mixup strategies. Left: tempo grouping and downbeat alignment via Beat Transformer.\nMiddle: BAM and BLM mixup strategies. Right: How BAM and BLM are applied in the feature\nspace of audio signals and audio latent variables.\nBeat-tracking via Beat Transformer\nMusical compositions are made up of several elements, such\nas chord progressions, timbre, and beats. Of these, beats play a crucial role in determining the musical\nstructure and alignment. In most audio retrieval tasks, mixup is a popular technique that involves\nrandomly mixing pairs of audio data to augment the training data. However, when mixing two music\nsamples that have different tempos (beats per minute), the mixture can be chaotic and unappealing. To\navoid this, we use a state-of-the-art beat tracking model, Beat Transformer [42], to extract the tempo\nand downbeat map of each music track, as shown in the left of Figure 2. We categorize each music\ntrack into different tempo groups and during training, we only mixed tracks within the same tempo\ngroup to ensure the tracks were in similar tempos. Furthermore, we align the tracks by comparing\ntheir downbeat maps and selecting a certain downbeat to serve as the starting position for the mixup\ntrack. This preprocessing approach allows us to better select the music data available for mixup,\nresulting in mixup tracks that are neatly ordered in terms of tempo and downbeats.\nBeat-Synchronous Audio Mixup\nAs depicted in the upper part of Figure 2, once we select two\naligned music tracks x1 and x2, we mix them by randomly selecting a mixing ratio from the beta\ndistribution \u03bb \u223c B(5, 5), as:\nx = \u03bbx1 + (1 \u2212 \u03bb)x2\n(2)\nWe then use the mixed data x to obtain the CLAP embedding Ex and the audio latent variable y.\nWe train the latent diffusion model using the standard pipeline. This beat-synchronous audio mixup\nstrategy is referred to as BAM.\nBeat-Synchronous Latent Mixup\nAs depicted in the lower part of Figure 2, in the latent diffusion\nmodel, the mixup process can also be applied on the latent variables, referred as beat-synchronous\nlatent mixup (BLM). After selecting two aligned music tracks x1 and x2, we feed them into the VAE\nencoder to obtain the latent variables y1 and y2. We then apply the mixup operation to the latent\nvariables:\ny = \u03bby1 + (1 \u2212 \u03bb)y2\n(3)\nIn contrast to BAM, BLM applies the mixup operation to the latent space of audio, where we cannot\nensure that the mixture of the latent variables corresponds to the actual mixture of the music features\nin the appearance. Therefore, we first generate a mixed mel-spectrogram xmel by feeding the mixed\nlatent variable y into the VAE decoder. Then, we feed xmel to the Hifi-GAN vocoder to obtain the\nmixed audio x as the input music. With x and y, we follow the pipeline to train the MusicLDM.\nWhat are BAM and BLM doing?\nAs shown in the right of Figure 2, we demonstrate the interpola-\ntion between the feature space of audio when using BAM and BLM. In the feature space of audio\nsignals, the \"\u2022\" represents the feature point of music data, while the \"\u25b3\" denotes the feature point of\nother audio signals, such as natural sound, audio activity, and noise. During the pretraining process\nof VAE, a latent space is constructed for encoding and decoding the music data. The VAE aims to\nlearn the distribution of the latent variables that can best represent the original data and transform the\noriginal feature space into a lower-dimensional manifold. This manifold is designed to capture the\n5\nunderlying structure of the music data. Therefore, any feature point within this manifold is considered\nto be a valid representation of music.\nBAM and BLM are concerned with augmenting the data at different levels of feature space. As shown\nin right of Figure 2, BAM linearly combines two points in audio space to form a new point on the red\nline. BLM, represented by the blue line, performs a similar operation, but result in a new point in the\nVAE-transformed latent space, which will be decoded back onto the music manifold of audio space.\nBoth BAM and BLM offer unique advantages and disadvantages. BAM applies mixup in the original\nfeature space, resulting in a smooth interpolation between feature points. However, BAM cannot\nensure a reasonable music sample that lies within the music manifold. This issue is more problematic\nusing the simple audio mixup strategy without tempo and downbeat alignments. BLM, conversely,\naugments within the music manifold, fostering robust and diverse latent representations. However,\nBLM is computationally more expensive as it requires computing the latent feature back to audio via\nVAE decoder and Hifi-GAN. Furthurmore, when the ill-defined or collapsed latent exists in VAE,\nBLM may be out of effectiveness.\nBoth BAM and BLM are effective data augmentation techniques that encourage the model to learn\na more continuous and robust decision boundary on the audio feature space, or implicitly from the\nlatent space to the audio space, which can improve the model\u2019s generalization performance and\nmitigate overfitting. In the context of text-to-music generation, these mixup strategies can have a\npotential to mitigate the limitations of data size and help avoid plagiarism issues. By introducing\nsmall variations through mixup, the model can touch a more rich space of music data and generate\nmusic samples that are correlated to the texts but show differences to the original training data. In\nSection 4.2, we evaluated whether these strategies mitigate the data limitation and plagiarism issues.\n4\nExperiments\nIn this section, we conducted four experiments on our proposed methods. First, we retrained a new\nCLAP model to provide the condition embedding for MusicLDM. Second, we trained MusicLDM\nwith different mixup strategies and compared them with available baselines. Third, we evaluated\nMusicLDM in terms of text-music relevance, novelty and plagiarism risk via metrics based on CLAP\nscores. Finally, we conducted a subjective listening test to give an additional evaluation.\n4.1\nExperimental Setup\nDataset\nThe original CLAP model trained on mostly acoustic events and sound effect datasets.\nIn this work, we trained a CLAP model on music datasets in addition to its original training data,\nallowing it to better understand the relation between music and textual descriptions. The new CLAP\nmodel is trained on dataset of 2.8 Million text-audio pairs, in an approximate total duration of\n20 000 hours. Compared to previous CLAP model, the newly trained CLAP model performs well in\nzero-shot classification for both acoustic event and music. Please refer further details on training and\nperformance of the new CLAP in Appendix B. For MusicLDM, we used the Audiostock dataset for\ntraining, along with VAE and Hifi-GAN. Specifically, the Audiostock dataset contains 9000 music\ntracks for training and 1000 tracks for testing. The total duration is 455.6 hours. It provides a correct\ntextual description of each music track. Although CLAP is trained on more text-music data pairs,\nthe large number of them are pseudo-captions composed primarily of non-specific metadata, such\nas author, song title, and album information (e.g., [a song by author A from the album B]).\nThese captions do not align with our specific objective of text-to-music generation.\nHyperparameters and Training Details\nWe trained all MusicLDM modules with music clips\nof 10.24 seconds at 16 kHz sampling rate. In both the VAE and diffusion model, music clips are\nrepresented as mel-spectrograms with T = 1024 frames and F = 128 mel-bins. Unlike AudioLDM,\nMusicLDM\u2019s VAE utilizes a downsampling rate of P = 8 and a latent dimension of C = 16. The\narchitecture of MusicLDM\u2019s latent diffusion model follows that of AudioLDM. The training process\nof MusicLDM aligns with AudioLDM\u2019s approach. For additional hyperparameters and training\ndetails, refer to Appendix A.\n6\nTable 1: The evaluation of generation quality among MusicLDMs and baselines. AA-Train. and\nTA-Train. refer to the audio-audio training scheme and the text-audio traning scheme.\nModel\nAA-Train.\nTA-Train.\nFDpann \u2193\nFDvgg \u2193\nInception Score \u2191\nKL Div. \u2193\nRiffusion [8]\n\u2717\n\u2713\n68.95\n10.77\n1.34\n5.00\nMuBERT [26]\n\u2014\n\u2014\n31.70\n19.04\n1.51\n4.69\nAudioLDM\n\u2713\n\u2717\n38.92\n3.08\n1.67\n3.65\nMusicLDM\n\u2713\n\u2717\n26.67\n2.40\n1.81\n3.80\nMusicLDM (Only TA-Training)\n\u2717\n\u2713\n32.40\n2.51\n1.49\n3.96\nMusicLDM w/. mixup\n\u2713\n\u2717\n30.15\n2.84\n1.51\n3.74\nMusicLDM w/. BAM\n\u2713\n\u2717\n28.54\n2.26\n1.56\n3.50\nMusicLDM w/. BLM\n\u2713\n\u2717\n24.95\n2.31\n1.79\n3.40\nMusicLDM w/. Text-Finetune\n\u2713\n\u2713\n27.81\n1.75\n1.76\n3.60\nMusicLDM w/. BAM & Text-Finetune\n\u2713\n\u2713\n28.22\n1.81\n1.61\n3.61\nMusicLDM w/. BLM & Text-Finetune\n\u2713\n\u2713\n26.34\n1.68\n1.82\n3.47\n4.2\nMusicLDM Performance\n4.2.1\nGeneration Quality\nFollowing evaluation techniques used in past work on audio generation [24], we use frechet distance\n(FD), inception score (IS), and kullback-leibler (KL) divergence to evaluate the quality of generated\nmusical audio outputs. Frechet distance evaluates the audio quality by using an audio embedding\nmodel to measure the similarity between the embedding space of generations and that of targets. In\nthis paper, we use two standard audio embedding models: VGGish [12] and PANN [20]. The resulting\ndistances we denote as FDvgg and FDpann, respectively. Inception score measures the diversity\nand the quality of the full set of audio outputs, while KL divergence is measured on individual pairs\nof generated and groundtruth audio samples and averaged. We use the audioldm_eval library1 to\nevaluate all the metrics mentioned above, comparing the groundtruth audio from the Audiostock\n1000-track test set with the 1000 tracks of music generated by each system based on the corresponding\ntextual descriptions.\nTable 1 presents the FD, IS, and KL results for our models in comparison with baseline models.\nIn the first section, we utilized textual descriptions from the test set, sending them to the offical\nAPIs of Riffusion and MuBERT to generate corresponding results. Both Riffusion and MuBERT\nwere unable to achieve results comparable to the remaining models. Upon reviewing the generated\nmusic from the two systems, we found that the sub-optimal performance of Riffusion resulted from\npoor music generation quality, with many samples either inaudible or outside the desired musical\nrange. MuBERT, while generating high-quality pieces from real music sample libraries, fell short\nin replicating the distribution of Audiostock dataset. Due to the unavailability of their detailed\narchitectures, training scripts, and data, Riffusion and MuBERT\u2019s evaluations offered only partial\ncomparisons.\nWe also retrained the original AudioLDM model on the Audiostock dataset, comparing it to Musi-\ncLDM variants. The distinction between AudioLDM and MusicLDM lies in the different CLAP\nmodels used for condition embeddings. Our comparison revealed that MusicLDM outperforms\nAudioLDM in terms of FDpann, FDvgg, and IS. This underscores the efficacy of the novel CLAP\nmodel pretrained for music, providing more suitable music embeddings as conditioning information.\nComparing MusicLDM\u2019s performance with audio-to-audio training (\u03f5\u03b8(zn, n, Exa)) and text-to-\naudio training (\u03f5\u03b8(zn, n, Ext)), denoted by \u201cMusicLDM (Only TA-Training)\u201d, we note inferior\nresults in the latter approach. This may be suggests that a gap exists between distribution of text\nembedding and audio embedding, making it challenging for the diffusion model to generate high-\nquality audio solely from text embedding. In contrast, CLAP\u2019s audio embedding may leak low-level\naudio information to the diffusion model during initial training stages, hurting the model\u2019s ability to\ngeneralize to text embedding inputs. This hypothesis is further supported by the results of MusicLDM\nwith combined audio-to-audio training and text-to-audio fine-tuning. We observe a significant\ndecrease in FDvgg with small changes in FDpann and IS, indicating a substantial improvement\nin music generation quality, driven by leveraging both audio and text embeddings during training.\nThe former facilitates good audio reconstruction during early training, while the latter shifts the\ndistribution from audio to text to align with the eventual test-time task of text-to-music generation.\n1https://github.com/haoheliu/audioldm_eval\n7\nTable 2: The objective metrics to measure the relevance and novelty (plagiarism). And the subjective\nlistening test to evaluate the quality, relevance, and musicality.\nModel\nObjective Metrics\nSubjective Listening Test\nRelevance\nNovelty and Plagiarism Risk\nText-Audio Similarity\u2191\nSIMAA@90 \u2193\nSIMAA@95 \u2193\nQuality\u2191\nRelevance\u2191\nMusicality\u2191\nTest Set (Ref.)\n0.325\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nRetrieval Max (Ref.)\n0.423\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nMuBERT\n0.131\n0.107\n0\n2.02\n1.50\n2.33\nMusicLDM (original)\n0.281\n0.430\n0.047\n1.98\n2.17\n2.19\nMusicLDM w/. mixup\n0.234\n0.391\n0.028\n\u2014\n\u2014\n\u2014\nMusicLDM w/. BAM\n0.266\n0.402\n0.027\n2.04\n2.21\n2.01\nMusicLDM w/. BLM\n0.268\n0.401\n0.020\n2.13\n2.31\n2.07\n1.0\n0.5\n0.0\nMusicLDM (mix-up)\nMusicLDM (original)\nMusicLDM (BLM)\nMusicLDM (BAM)\nMuBERT\nText-Audio Similarity\nAudio-Audio Similarity\nFigure 3: The violin plot of the audio-audio similarity, and the text-to-audio similarity.\nLast, we compared MusicLDM with different mixup strategies, namely simple mixup [41], BAM,\nand BLM. The comparison reveals the negative impact of the simple mixup on all metrics. This\ndegradation in generated sample quality, characterized by instrumental interference and noise, is\nattributed to the simple mixup\u2019s inability to guarantee musicality in the mix. Similar observations\nare evident in the BAM results, indicated by a drop in FDpann and IS. However, BAM\u2019s tempo\nand downbeat alignment, along with the original mixup benefits, counterbalance this defect to a\ncertain extent, enhancing the model\u2019s generalization ability and improving certain metrics. BLM, as\na latent space mixing method, aligns with our hypothesis in Section 3.2 that latent space mixup yield\naudio closely resembling music. This technique allows us to largely bypass the potential confusion\nissues tied to audio mixing, thus capitalizing on mixup\u2019s ability to drive generalization and prevent\ncopying via data augmentation. Furthermore, incorporating text-finetuning results in a comprehensive\nimprovement of music generation quality, solidifying BLM as the most effective strategy.\n4.2.2\nText-Audio Relevance, Novelty and Plagiarism\nWe proposed two metric groups, text-audio similarity and nearest-neighbor audio similarity ratio\nto assess text-audio relevance, novelty, and plagiarism risk in various models.\nFirst, text-audio similarity measures the relevance between the text and the audio. It is defined as the\ndot product between the groundtruth text embedding Et\ngd from the test set and the audio embedding\nEa from music generated by models, i.e., Et\ngd \u00b7 Ea. The embeddings from both text and audio are\nnormalized in CLAP model, thus the dot product computes the cosine similarity between text and\naudio embeddings.\nSecond, we would also like to measure the extent to which models are directly copying samples from\nthe training set. We verify this by first computing the dot products between the audio embedding of\neach generated music output to all audio embeddings from the Audiostock training set and returning\nthe maximum \u2013 i.e., the similarity of the nearest-neighbor in the training set. Then, we compute the\nfraction of generated outputs whose nearest-neighbors are above a threshold similarity. We refer this\nas the nearest-neighbor audio similarity ratio, providing SIMAA@90 where the threshold is 0.9, and\nSIMAA@95 with 0.95. The lower this fraction, the lower the risk of plagiarism \u2013 i.e., fewer samples\nhave very close training neighbors. In the Appendix D, we show pairs of examples with both high\nand low similarity scores to give further intuition for this metric.\n8\nAs shown in the left and middle column of Table 2, we present the average text-audio similarity and\nnearest-neighbor audio similarity ratios for two thresholds on the 1000 tracks in the Audiostock test\nset and the generated music from MuBERT and different variants of MusicLDM. We also provide\ntwo reference points for text-audio similarity: \u201cTest Set\u201d and \u201cRetrieval Max\u201d. Specifically, \u201cTest Set\u201d\nrefers to computing the cosine similarity between the groudtruth text embedding and the groudtruth\naudio embedding. And \u201cRetrieval Max\u201d refers to first computing the cosine similarities between\neach text embedding from the test set to all audio embeddings from the training set, then picking the\nhighest score as the score of this text, and taking the average over all text scores.\nWe can observe that the original MusicLDM without mixup achieves the highest text-audio relevance\nwith an average score of 0.281, but also the highest (worst) nearest-neighbor audio similarity ratio.\nMusicLDM with the simple mixup strategy achieves the lowest SIMAA@90 ratio while sacrificing a\nlot in the relevance of the generation. The MusicLDM with BAM and BLM achieve a balance between\nthe audio similarity ratios and the text-to-audio similarity. In combination with the quality evaluation\nresults in Table 1, we can conclude that all mixup strategies are effective as a data augmentation\ntechniques to improve generalization of the model to generate more novel music. However simple\nmixup degrades the generation quality, which affects the relevance score between audio and text, and\nalso thus makes it less similar to the tracks in the training set. BAM and BLM apply the tempo and\ndownbeat filtering on the music pairs to mix, allowing the model to maintain superior generation\nquality (Table 1) and text-audio relevance (Table 2), while still utilizing the benefit brought by the\nmixup technique to make the generation more novel (less plagiarism). Among the objective metrics,\nBLM is the best mixup strategy in terms of quality, relevance and novelty of the generated audio.\nThis indicates mixing in the latent space is more efficient than mixing directly in audio space, perhaps\nbecause the latent embedding approach implicitly projects the mixture to the learned manifold of\nwell-formed music. We show the detailed distribution of these metrics over 1000 generated tracks in\nFigure 3, where, for example, audio-audio similarity denotes the individual scores used to calculate\nthe average SIMAA. We find that the original MusicLDM without mixup has more samples with\nhigh training similarity than other models, which further reflects that it is more prone to copying.\n4.3\nSubjective Listening Test\nAs shown in the right of Table 2, we conduct the subjective listening test on four models, namely\nMuBERT, the original MusicLDM, and that with BAM or BLM strategy, to further evaluate the actual\nhearing experience of the generated music. We do not include the simple mixup MusicLDM because\nits generation is at a low quality while we avoid confusing subjects with too many models in the same\ntime. We invite 15 subjects to listen to 6 groups of the generations randomly selected from the test\nset. Each of group contains four generations from four models and the corresponding text description.\nThe subjects are required to rate the music in terms of quality, relevance, and musicality (detailed in\nAppendix E). We observe that the samples of MusicLDM with BAM or BLM mixup strategy achieve\na better relevance and quality than those of MuBERT and the original MusicLDM, this strengths\nour above analysis. The MuBERT samples achieve the best Musicality, because its generation is\ncombined from the real music samples. Combined with the objective metrics, beat-synchronous\nlatent mixup stands to be the most effectiveness method for enhancing the text-to-music generation\nin terms of quality, text-music relevance and novelty (i.e., reducing the risk of plagiarism).\n5\nLimitations\nIn this section we outline the recognized limitations of our study, serving as a roadmap for future\nimprovements. Firstly, MusicLDM is trained on the music data in a sampling rate of 16 kHz, while\nmost standard music productions are 44.1 kHz. This constraint, tied to the Hifi-GAN vocoder\u2019s subpar\nperformance at high sampling rates, impedes practical text-to-music application and necessitates\nfurther improvements. Secondly, resource constraints such as limited real text-music data and GPU\nprocessing power prevent us from scaling up MusicLDM\u2019s training. We are unable to determine\nif mix-up strategies could yield similar trends as observed with the Audiostock dataset. This issue\nexists in the image generation task as well. Lastly, while we recognize beat information as crucial for\nmusic alignment, there is scope for exploring other synchronization techniques like key signature and\ninstrument alignment. We also intend to investigate the application of different audio space filters to\nselect suitable music pairs for mixing.\n9\n6\nConclusion\nIn this paper, we introduce MusicLDM, a text-to-music generation model that incorporates CLAP,\nVAE, Hifi-GAN, and latent diffusion models. We enhance MusicLDM by proposing two efficient\nmixup strategies: beat-synchronous audio mixup (BAM) and beat-synchronous latent mixup (BLM),\nintegrated into its training process. We conduct comprehensive evaluations on different variants\nof MusicLDM using objective and subjective metrics, assessing quality, text-music relevance, and\nnovelty. The experimental results demonstrate the effectiveness of BLM as a standout mixup strategy\nfor text-to-music generation.\n7\nAcknowledgments\nWe would like to thank the Institute for Research and Coordination in Acoustics and Music (IRCAM)\nand Project REACH: Raising Co-creativity in Cyber-Human Musicianship for supporting this project.\nThis project has received funding from the European Research Council (ERC REACH) under the\nEuropean Union\u2019s Horizon 2020 research and innovation programme (Grant Agreement #883313).\nWe would like to thank the support of computation infrastructure from LAION.\nReferences\n[1] Andrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating\nmusic from text. arXiv preprint:2301.11325, 2023.\n[2] Jean-Pierre Briot, Ga\u00ebtan Hadjeres, and Fran\u00e7ois-David Pachet. Deep learning techniques for\nmusic generation, volume 1. Springer, 2020.\n[3] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer,\nBorja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models.\narXiv preprint:2301.13188, 2023.\n[4] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale\naudio-visual dataset. In Proc. ICASSP, 2020.\n[5] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov.\nHts-at: A hierarchical token-semantic audio transformer for sound classification and detection.\nIn Proc. ICASSP, pages 646\u2013650. IEEE, 2022.\n[6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning\naudio concepts from natural language supervision. In Proc. ICASSP, pages 1\u20135. IEEE, 2023.\n[7] Rebecca Fiebrink and Baptiste Caramiaux. The machine learning algorithm as creative musical\ntool. arXiv preprint:1611.00379, 2016.\n[8] Seth Forsgren and Hayk Martiros. Riffusion - Stable diffusion for real-time music generation.\n2022.\n[9] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training\nimagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n[10] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. Audioclip: Extending clip to\nimage, text and audio. In Proc. ICASSP, pages 976\u2013980. IEEE, 2022.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proc. CVPR, pages 770\u2013778, 2016.\n[12] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Chan-\nning Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn architectures\nfor large-scale audio classification. In Proc. ICASSP, pages 131\u2013135. IEEE, 2017.\n10\n[13] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Proc. NeurIPS,\n2020.\n[14] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n[15] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong\nZhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Text-conditioned music\ngeneration with diffusion models. arXiv preprint:2302.03917, 2023.\n[16] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui\nYe, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with\nprompt-enhanced diffusion models. arXiv preprint:2301.12661, 2023.\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc.\nICLR, 2014.\n[18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Proc. ICLR, 2013.\n[19] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HifiGAN: Generative adversarial networks\nfor efficient and high fidelity speech synthesis. Proc. NeurIPS, 33:17022\u201317033, 2020.\n[20] Qiuqiang Kong, Yin Cao, and Turab Iqbal et al. Panns: Large-scale pretrained audio neural\nnetworks for audio pattern recognition. IEEE Trans. Audio, Speech, Lang. Process., 2020.\n[21] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. In Proc. ICLR, 2021.\n[22] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet,\nDevi Parikh, Yaniv Taigman, and Yossi Adi. AudioGen: Textually guided audio generation.\nProc. ICLR, 2022.\n[23] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proc. IEEE, 1998.\n[24] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. Proc.\nICML, 2023.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint:1907.11692, 2019.\n[26] MubertAI. Mubert: A simple notebook demonstrating prompt-based music generation.\n[27] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:\nVisual reasoning with a general conditioning layer. In Proc. AAAI, volume 32, 2018.\n[28] Karol J. Piczak. ESC: dataset for environmental sound classification. In Proc. ACM Multimed.,\n2015.\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint:2204.06125, 2022.\n[30] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech 2:\nFast and high-quality end-to-end text to speech. arXiv preprint:2006.04558, 2020.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proc. CVPR, pages 10684\u201310695,\n2022.\n[32] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban\nsound research. In Proc. ACM Multimed., 2014.\n11\n[33] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\nDiffusion art or digital forgery? investigating data replication in diffusion models. arXiv\npreprint:2212.03860, 2022.\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Proc.\nICLR, 2020.\n[35] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong\nLeng, Yuanhao Yi, Lei He, et al. NaturalSpeech: End-to-end text to speech synthesis with\nhuman-level quality. arXiv preprint:2205.04421, 2022.\n[36] George Tzanetakis and Perry Cook. Musical genre classification of audio signals. IEEE\nTransactions on speech and audio processing, 10(5):293\u2013302, 2002.\n[37] Ashish Vaswani, Noam Shazeer, and Niki Parmar et al. Attention is all you need. In Proc.\nNeurIPS, 2017.\n[38] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning\nrobust audio representations from clip. In Proc. ICASSP, pages 4563\u20134567. IEEE, 2022.\n[39] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo\nDubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-\nto-caption augmentation. In Proc. ICASSP, pages 1\u20135. IEEE, 2023.\n[40] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.\nDiffSound: Discrete diffusion model for text-to-sound generation. IEEE Trans. Audio, Speech,\nLang. Process., 2023.\n[41] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond\nempirical risk minimization. Proc. ICLR, 2017.\n[42] Jingwei Zhao, Gus Xia, and Ye Wang. Beat transformer: Demixed beat and downbeat tracking\nwith dilated self-attention. Proc. ISMIR, 2022.\n12\nTable 3: Comparison of zero-shot classification performance of the CLAP used in this work with\nprevious audio-language contrastive learning models.\nESC-50\nUS8K\nVGGSound\nGTZAN\nWav2CLIP [38]\n41.4\n40.4\n10.0\n-\naudioCLIP [10]\n68.6\n68.8\n-\n-\nCLAP (Elizalde et al. [6])\n82.6\n73.2\n-\n25.2\nCLAP (Wu et al. [39])\n91.0\n77.0\n46.2\n71.0\nCLAP (ours on music data)\n90.1\n80.6\n46.6\n71.0\nA\nMusicLDM Details\nHyperparameters\nFor audio signal processing, we use the sampling rate of 16 kHz to convert all\nmusic samples for the training of MusicLDM. Each input data is a chunk of 10.24 seconds randomly\nselected from the dataset, i.e., L = 163840. We use the hop size 160, the window size 1024, the filter\nlength 1024, and the number of mel-bins 128 to compute the short-time Fourier transform (STFT)\nand mel-spectrograms. As the result, the input mel-spectrogram has the time frame T = 1024 and\nthe mel-bins F = 128.\nWe adopt a convolutional VAE as the latent audio representation model, consisting of a 4-block\ndownsampling encoder and a 4-block upsampling decoder. The downsampling and upsampling\nrate P = 8 and the latent dimension C = 16, i.e., the bottleneck latent variable y has a shape of\n(C \u00d7 T\nP \u00d7 F\nP ) = (16 \u00d7 128 \u00d7 16). For the latent diffusion model, we refer the UNet latent diffusion\nmodel2. It contains 4 encoder blocks, 1 bottleneck block, and 4 decoder blocks. Each block contains\n2 residual CNN layers [23] and 1 spatial transformer layer [37]. The channel dimensions of encoder\nblocks are 128, 256, 384, and 640 and reversed in decoder blocks. For Hifi-GAN, we adopt its\nofficial repository3 along with the configuration4. We change the number of mel-bins to 128 to fit the\nprocessing of MusicLDM.\nImplementation and Training Details\nFor the training of VAE, we use the Adam optimizer with a\nlearning rate of 4.5 \u00d7 10\u22126 with a batch size of 24. We apply the mel-spectrogram loss, adversarial\nloss, and a Gaussian constraint loss as the training object of VAE. For the training of Hifi-GAN, we\nuse the batch size of 96 and the AdamW optimizer with \u03b21 = 0.8, \u03b22 = 0.99 at the learning rate of\n2 \u00d7 10\u22124. For the training of MusicLDM, we use the batch size of 24 and the AdamW optimizer with\nthe basic learning of 3 \u00d7 10\u22125. In the forward process, we use 1000-step of a linear noise schedule\nfrom \u03b21 = 0.0015 to \u03b21000 = 0.0195. In the sampling process, we use the DDIM [34] sampler with\n200 steps. We adopt the classifier-free guidance [14] with a guidance scale w = 2.0. When applying\nthe mixup strategy, we use the mixup rate p = 0.5. The CLAP model is trained on 24 A100 GPUs.\nThe VAE and HifiGAN model are trained on 4 A60 GPUs. Last, the latent diffusion model is trained\non single NVIDIA A40. All models are converged at the end of the training.\nImplementation of Comparison Model\nFor generating from Riffusion and MuBERT, we use the\nofficial API of Riffusion5 and MuBERT6.\nB\nCLAP Details\nB.1\nHyperparameters\nFor model hyperparameters, we refer to the official repository7 to conduct the training process of\nCLAP. The audio encoder of CLAP is HTS-AT-base [5] and the text encoder is RoBERTa-base\n[25]. The HTS-AT-base model has an embedding dimension of 128, and a window size of 8. The\n2https://huggingface.co/spaces/multimodalart/latentdiffusion\n3https://github.com/jik876/hifi-gan\n4https://github.com/jik876/hifi-gan/blob/master/config_v1.json\n5https://huggingface.co/riffusion/riffusion-model-v1\n6https://github.com/MubertAI/Mubert-Text-to-Music\n7https://github.com/LAION-AI/CLAP\nMusic Tracks in the Training Set\nGenerated Samples from Models\nSimilarity=0.924\nSimilarity=0.935\nSimilarity=0.961\nFigure 4: The spectrograms of music pairs indicated by high cosine similiarity score of CLAP audio\nembeddings.\nHTS-AT-base model has 4 groups of swin-transformer blocks, each group has depth of [2, 2, 12, 2]\nand number of head in [4, 8, 16, 32]. The RoBERTa-base consists of a transformer model with 12\nlayers, 8 heads, and a inner width of 512. The audio embedding and the text embedding have the\ndimension size D = 512.\nB.2\nTraining Details\nFor the training of CLAP, we use the batch size of 2304 and the Adam [17] optimizer with \u03b21 = 0.99,\n\u03b22 = 0.9 with a warm-up [9] and cosine learning rate decay at a basic learning rate of 1 \u00d7 10\u22124.\nB.3\nZero-shot Classification Performance\nWe follow previous works on audio-language contrastive learning [9] to evaluate the performance of\nCLAP on the zero-shot audio classification tasks, namely on the benchmark datasets of ESC-50 [28],\nUrbanSound 8K [32], and VGGSound [4]. To demonstrate that the retrained CLAP involves more\nunderstandings of music data, we further add a music genre classification benchmark dataset GTZAN\n[36] into the evalution. As shown in Table 3, our retained CLAP achieves best performance acoustic\nevent classification in UrbanSound 8K and VGGSound dataset, while still maintaining comparable\nperformance in ESC-50 dataset and on par performance in GTZAN music classification dataset.\nAlthough the performance on GTZAN music dataset is not improved, the extra data used for training\nCLAP might result in a better representation space which is beneficial for text-to-music generation\nmodel.\nC\nNearest-Neighbor Audio Similarity Samples\nAs mentioned in section 4.2.2, we introduced the computation of the nearest-neighbor audio similarity\nratio by comparing the cosine similarity between generated music and music tracks in the training set\nof Audiostock.\nIn this section, we provide visualizations of the similarity between the generated music and the\ntraining music using spectrograms, showcasing how well the cosine similarity between CLAP audio\nembeddings captures this similarity.\nAs shown in Figure 4 and Figure 5, display both three examples of music pairs with high and\nlow similarity. To achieve this, we divided the training music tracks into 10-second segments and\ndetermined the most similar segment to the generated music track (i.e., the query track).\n14\nMusic Tracks in the Training Set\nGenerated Samples from Models\nSimilarity=0.803\nSimilarity=0.813\nSimilarity=0.820\nFigure 5: The spectrograms of music pairs indicated by low cosine similarity score of CLAP audio\nembeddings.\nFor instances with high similarity, the cosine similarity of CLAP audio embeddings reveals highly\nsimilar structural patterns, indicating a close resemblance in the music arrangements. Conversely, low\nCLAP cosine similarity indicates significant differences between the spectrograms of the generated\nmusic and the training music. This demonstrates the effectiveness of CLAP embeddings in assessing\nthe similarity between music tracks and serving as a means to detect novelty and potential instances\nof plagiarism in the generated samples.\nD\nSubjective Listening Test\nThe subjective listening test was conducted in an online survey format to gather feedback and insights\non the text-to-music generation using MusicLDMs and MuBERT. The generation of Riffusion was not\nincluded due to its lower quality and relevance compared to the standard. The test had an estimated\nduration of approximately 10 minutes.\nAt the beginning of the test, participants were asked to provide their age range and music background\nas metadata. Subsequently, participants were randomly assigned six groups of generated songs. Each\ngroup consisted of four songs generated from MusicLDM, MusicLDM with BAM, MusicLDM with\nBLM, and MuBERT, all based on the same textual description. The order of the songs within each\ngroup was shuffled to eliminate positional bias during rating. Participants were required to rate each\nsong based on three metrics:\n\u2022 Relevance: Determine how well the song matches the given music description. Rate the\nsong based on how closely it aligns with the provided description.\n\u2022 Quality: Assess the overall quality of the music. Consider factors such as clarity, absence of\nnoise, and general audio quality. Rate the song based on these criteria.\n\u2022 Musicality: Evaluate the musical attributes of the song, including rhythm, melodies, and\ntextures. Rate the song based on its overall musical appeal.\nEach song in the subjective listening test had a duration of approximately 10 seconds and included\na fade-in and fade-out to mitigate bias from the song\u2019s beginning and ending sections. The rating\nscale used for evaluating the songs was designed such that a higher score indicates better quality.\nParticipants were asked to rate each song based on the provided metrics, taking into account the\nsong\u2019s overall quality, relevance to the given text, and personal preference on its musicality.\n15\nE\nBroader Impact\nThe development and implementation of MusicLDM, or generally a text-to-music generation model\noffers potential benefits and also raises concerns that must be addressed responsibly.\nPositive Impacts\n\u2022 Promoting Creativity: This model can serve as a tool to augment human creativity. Artists,\ncomposers, and music amateurs can use it to transfer their textual ideas into music, broad-\nening the realm of artistic exploration and making music creation more accessible and\nconvenient.\n\u2022 Cultural Preservation and Evolution: The model provides a unique platform to archive,\ndigitize, and even evolve cultural musical expressions. Textual descriptions of traditional\nand folk music can be transformed into the actual music, thereby helping to preserve heritage\nwhile simultaneously allowing for creative adaptations. Literature, such as poetry, can be\ninterpreted as music to explore more relations between different types of cultural expression\nforms.\n\u2022 Education and Research: In academia, this model can be used as a pedagogical tool in\nmusic education. It can aid in understanding the complex relationship between music and\nlinguistic structures, enriching interdisciplinary research in musicology, linguistics, and\nartificial intelligence.\n\u2022 Entertainment Industry Innovation: The entertainment industry could use this model to\ngenerate soundtracks for movies, games, and other media based on scripts. This could\npotentially revolutionize the way music is produced for media, reducing time and costs.\nNegative Impacts\n\u2022 Artistic Job Displacement: While this model can augment human creativity, it may also\nlead to job losses in the music industry if widely adopted for composing and production.\nThe model could potentially replace human composers in certain contexts, particularly in\nindustries such as film and gaming that require a significant amount of background music.\n\u2022 Copyright Issues: In this paper, one of the targets is to mitigate the copyright issues and\nplagiarism. The generated music could unintentionally resemble existing works, raising\ncomplex copyright infringement issues. It is crucial to implement measures to ensure that\nthe model does not violate intellectual property rights.\n\u2022 Ethical Misuse: The model could be misused to create music promoting hate speech,\nmisinformation, or other harmful content if the input text has such characteristics. Thus, it\nis essential to develop safeguards to mitigate the risk of misuse.\n\u2022 Cultural Appropriation and Homogenization: While the model can help preserve music,\nthere is a risk of homogenizing unique cultural musical styles or misappropriating them\nwithout proper credit or context.\nThe design and application of this model should be carried out responsibly, considering the potential\nethical, social, and economic consequences. Balancing its many benefits with its potential downsides\nwill require the collective effort of developers, users, policy makers, and society at large.\n16\n"
  },
  {
    "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
    "link": "https://arxiv.org/pdf/2308.01544.pdf",
    "upvote": "14",
    "text": "Multimodal Neurons in Pretrained Text-Only Transformers\nSarah Schwettmann1*, Neil Chowdhury1*, Samuel Klein2, David Bau3, Antonio Torralba1\n1MIT CSAIL,\n2MIT KFG,\n3Northeastern University\n{schwett, nchow, sjklein, torralba}@mit.edu, d.bau@northeastern.edu\nAbstract\nLanguage models demonstrate remarkable capacity to\ngeneralize representations learned in one modality to down-\nstream tasks in other modalities. Can we trace this ability\nto individual neurons? We study the case where a frozen\ntext transformer is augmented with vision using a self-\nsupervised visual encoder and a single linear projection\nlearned on an image-to-text task. Outputs of the projection\nlayer are not immediately decodable into language describ-\ning image content; instead, we find that translation between\nmodalities occurs deeper within the transformer. We intro-\nduce a procedure for identifying \u201cmultimodal neurons\u201d that\nconvert visual representations into corresponding text, and\ndecoding the concepts they inject into the model\u2019s residual\nstream. In a series of experiments, we show that multimodal\nneurons operate on specific visual concepts across inputs,\nand have a systematic causal effect on image captioning.\nProject page: mmns.csail.mit.edu\n1. Introduction\nIn 1688, William Molyneux posed a philosophical riddle\nto John Locke that has remained relevant to vision science\nfor centuries: would a blind person, immediately upon gain-\ning sight, visually recognize objects previously known only\nthrough another modality, such as touch [24, 30]? A pos-\nitive answer to the Molyneux Problem would suggest the\nexistence a priori of \u2018amodal\u2019 representations of objects,\ncommon across modalities.\nIn 2011, vision neuroscien-\ntists first answered this question in human subjects\u2014no, im-\nmediate visual recognition is not possible\u2014but crossmodal\nrecognition capabilities are learned rapidly, within days af-\nter sight-restoring surgery [15]. More recently, language-\nonly artificial neural networks have shown impressive per-\nformance on crossmodal tasks when augmented with addi-\ntional modalities such as vision, using techniques that leave\npretrained transformer weights frozen [40, 7, 25, 28, 18].\nVision-language models commonly employ an image-\nconditioned variant of prefix-tuning [20, 22], where a sep-\n*Indicates equal contribution.\nFigure 1. Multimodal neurons in transformer MLPs activate on\nspecific image features and inject related text into the model\u2019s next\ntoken prediction. Unit 2019 in GPT-J layer 14 detects horses.\narate image encoder is aligned to a text decoder with a\nlearned adapter layer. While Frozen [40], MAGMA [7],\nand FROMAGe [18] all use image encoders such as CLIP\n[33] trained jointly with language, the recent LiMBeR [28]\nstudy includes a unique setting: one experiment uses the\nself-supervised BEIT [2] network, trained with no linguistic\nsupervision, and a linear projection layer between BEIT and\nGPT-J [43] supervised by an image-to-text task. This setting\nis the machine analogue of the Molyneux scenario: the ma-\njor text components have never seen an image, and the ma-\njor image components have never seen a piece of text, yet\nLiMBeR-BEIT demonstrates competitive image captioning\nperformance [28]. To account for the transfer of semantics\nbetween modalities, are visual inputs translated into related\ntext by the projection layer, or does alignment of vision\nand language representations happen inside the text trans-\nformer? In this work, we find:\n1. Image prompts cast into the transformer embedding\nspace do not encode interpretable semantics. Transla-\ntion between modalities occurs inside the transformer.\n2. Multimodal neurons can be found within the trans-\nformer, and they are active in response to particular\nimage semantics.\n3. Multimodal neurons causally affect output: modulat-\ning them can remove concepts from image captions.\narXiv:2308.01544v2  [cs.CV]  1 Oct 2023\nFigure 2. Top five multimodal neurons (layer L, unit u), for sample images from 6 COCO supercategories. Superimposed heatmaps (0.95\npercentile of activations) show mean activations of the top five neurons over the image. Gradient-based attribution scores are computed\nwith respect to the logit shown in bold in the GPT caption of each image. The two highest-probability tokens are shown for each neuron.\n2. Multimodal Neurons\nInvestigations of individual units inside deep networks\nhave revealed a range of human-interpretable functions: for\nexample, color-detectors and Gabor filters emerge in low-\nlevel convolutional units in image classifiers [8], and later\nunits that activate for object categories have been found\nacross vision architectures and tasks [44, 3, 31, 5, 16]. Mul-\ntimodal neurons selective for images and text with similar\nsemantics have previously been identified by Goh et al. [12]\nin the CLIP [33] visual encoder, a ResNet-50 model [14]\ntrained to align image-text pairs. In this work, we show that\nmultimodal neurons also emerge when vision and language\nare learned entirely separately, and convert visual represen-\ntations aligned to a frozen language model into text.\n2.1. Detecting multimodal neurons\nWe analyze text transformer neurons in the multimodal\nLiMBeR model [28], where a linear layer trained on\nCC3M [36] casts BEIT [2] image embeddings into the in-\nput space (eL = 4096) of GPT-J 6B [43]. GPT-J transforms\ninput sequence x = [x1, . . . , xP ] into a probability distri-\nbution y over next-token continuations of x [42], to create\nan image caption (where P = 196 image patches). At layer\n\u2113, the hidden state h\u2113\ni is given by h\u2113\u22121\ni\n+ ai\u2113 + mi\u2113, where\nai\u2113 and mi\u2113 are attention and MLP outputs. The output\nof the final layer L is decoded using Wd for unembedding:\ny = softmax(WdhL), which we refer to as decoder(hL).\nRecent work has found that transformer MLPs encode\ndiscrete and recoverable knowledge attributes [11, 6, 26,\n27]. Each MLP is a two-layer feedforward neural network\nthat, in GPT-J, operates on h\u2113\u22121\ni\nas follows:\nmi\n\u2113 = W \u2113\noutGELU(W \u2113\ninh\u2113\u22121\ni\n)\n(1)\nMotivated by past work uncovering interpretable roles of\nindividual MLP neurons in language-only settings [6], we\ninvestigate their function in a multimodal context.\nAttributing model outputs to neurons with image input.\nWe apply a procedure based on gradients to evaluate the\ncontribution of neuron uk to an image captioning task. This\napproach follows several related approaches in neuron attri-\nbution, such as Grad-CAM [35] and Integrated Gradients\n[39, 6]. We adapt to the recurrent nature of transformer to-\nken prediction by attributing generated tokens in the caption\nto neuron activations, which may be several transformer\npasses earlier. We assume the model is predicting c as the\nmost probable next token t, with logit yc. We define the\nattribution score g of uk on token c after a forward pass\nthrough image patches {1, . . . , p} and pre-activation output\nZ, using the following equation:\ngk,c = Zk\np\n\u2202yc\n\u2202Zkp\n(2)\nThis score is maximized when both the neuron\u2019s output\nand the effect of the neuron are large. It is a rough heuristic,\nloosely approximating to first-order the neuron\u2019s effect on\nthe output logit, compared to a baseline in which the neu-\nron is ablated. Importantly, this gradient can be computed\nefficiently for all neurons using a single backward pass.\n2.2. Decoding multimodal neurons\nWhat effect do neurons with high gk,c have on model\noutput? We consider uk \u2208 U \u2113, the set of first-layer MLP\nunits (|U \u2113| = 16384 in GPT-J). Following Equation 1 and\nthe formulation of transformer MLPs as key-value pairs\nfrom [11], we note that activation Ak\ni of uk contributes a\n\u201cvalue\u201d from Wout to hi. After the first layer operation:\nmi = WoutAi\n(3)\nAs Ak\ni grows relative to Aj\ntion of mi approaches W k i (where j \u0338= k), the direc-\noutAk\ni , where W k\nout is one row\nof weight matrix Wout. As this vector gets added to the\nresidual stream, it has the effect of boosting or demoting\nBERTScore\nCLIPScore\nrandom\n.3627\n21.74\nmultimodal neurons\n.3848\n23.43\nGPT captions\n.5251\n23.62\nTable 1. Language descriptions of multimodal neurons correspond\nwith image semantics and human annotations of images. Scores\nare reported for a random subset of 1000 COCO validation images.\nEach BERTScore (F1) is a mean across 5 human image annota-\ntions from COCO. For each image, we record the max CLIPScore\nand BERTScore per neuron, and report means across all images.\ncertain next-word predictions (see Figure 1). To decode the\nlanguage contribution of uk to model output, we can di-\nrectly compute decoder(W k\nout), following the simplifying\nassumption that representations at any layer can be trans-\nformed into a distribution over the token vocabulary using\nthe output embeddings [11, 10, 1, 34]. To evaluate whether\nuk translates an image representation into semantically re-\nlated text, we compare decoder(W k\nout) to image content.\nDo neurons translate image semantics into related text?\nWe evaluate the agreement between visual information in\nan image and the text multimodal neurons inject into the\nimage caption. For each image in the MSCOCO-2017 [23]\nvalidation set, where LiMBeR-BEIT produces captions on\npar with using CLIP as a visual encoder [28], we calculate\ngk,c for uk across all layers with respect to the first noun c\nin the generated caption. For the 100 uk with highest gk,c\nfor each image, we compute decoder(W k\nout) to produce a\nlist of the 10 most probable language tokens uk contributes\nto the image caption. Restricting analyses to interpretable\nneurons (where at least 7 of the top 10 tokens are words in\nthe English dictionary containing \u2265 3 letters) retains 50%\nof neurons with high gk,c (see examples and further imple-\nmentation details in the Supplement).\nWe measure how well decoded tokens (e.g. horses,\nracing, ponies, ridden, . . . in Figure 1) corre-\nspond with image semantics by computing CLIPScore [17]\nrelative to the input image and BERTScore [45] rela-\ntive to COCO image annotations (e.g. a cowboy riding a\nhorse). Table 1 shows that tokens decoded from multimodal\nneurons perform competitively with GPT image captions\non CLIPScore, and outperform a baseline on BERTScore\nwhere pairings between images and decoded multimodal\nneurons are randomized (we introduce this baseline as we\ndo not expect BERTScores for comma-separated token lists\nto be comparable to GPT captions, e.g. a horse and rider).\nFigure 2 shows example COCO images alongside top-\nscoring multimodal neurons per image, and image regions\nwhere the neurons are maximally active. Most top-scoring\nneurons are found between layers 5 and 10 of GPT-J (L =\n28; see Supplement), consistent with the finding from [26]\nthat MLP knowledge contributions occur in earlier layers.\nFigure 3. CLIPScores for text-image pairs show no significant dif-\nference between decoded image prompts and random embeddings.\nFor image prompts, we report the mean across all image patches\nas well as the distribution of max CLIPScores per image.\nRandom\nPrompts\nGPT\nCOCO\nCLIPScore\n19.22\n19.17\n23.62\n27.89\nBERTScore\n.3286\n.3291\n.5251\n.4470\nTable 2. Image prompts are insignificantly different from ran-\ndomly sampled prompts on CLIPScore and BERTScore. Scores\nfor GPT captions and COCO nouns are shown for comparison.\n3. Experiments\n3.1. Does the projection layer translate images into\nsemantically related tokens?\nWe decode image prompts aligned to the GPT-J embed-\nding space into language, and measure their agreement with\nthe input image and its human annotations for 1000 ran-\ndomly sampled COCO images. As image prompts corre-\nspond to vectors in the embedding space and not discrete\nlanguage tokens, we map them (and 1000 randomly sam-\npled vectors for comparison) onto the five nearest tokens\nfor analysis (see Figure 3 and Supplement). A Kolmogorov-\nSmirnov test [19, 37] shows no significant difference (D =\n.037, p > .5) between CLIPScore distributions comparing\nreal decoded prompts and random embeddings to images.\nWe compute CLIPScores for five COCO nouns per image\n(sampled from human annotations) which show significant\ndifference (D > .9, p < .001) from image prompts.\nWe\nmeasure\nagreement\nbetween\ndecoded\nimage\nprompts and ground-truth image descriptions by computing\nBERTScores relative to human COCO annotations. Table 2\nshows mean scores for real and random embeddings along-\nside COCO nouns and GPT captions. Real and random\nprompts are negligibly different, confirming that inputs to\nGPT-J do not readily encode interpretable semantics.\nFigure 4. Top-activating COCO images for two multimodal neu-\nrons. Heatmaps (0.95 percentile of activations) illustrate consis-\ntent selectivity for image regions translated into related text.\n3.2. Is visual specificity robust across inputs?\nA long line of interpretability research has shown that\nevaluating alignment between individual units and seman-\ntic concepts in images is useful for characterizing feature\nrepresentations in vision models [4, 5, 46, 16]. Approaches\nbased on visualization and manual inspection (see Figure 4)\ncan reveal interesting phenomena, but scale poorly.\nWe quantify the selectivity of multimodal neurons for\nspecific visual concepts by measuring the agreement of their\nreceptive fields with COCO instance segmentations, follow-\ning [3]. We simulate the receptive field of uk by comput-\ning Ak\ni on each image prompt xi \u2208 [x1, ..., xP ], reshap-\ning Ak\ni into a 14 \u00d7 14 heatmap, and scaling to 224 \u00d7 224\nusing bilinear interpolation. We then threshold activations\nabove the 0.95 percentile to produce a binary mask over the\nimage, and compare this mask to COCO instance segmen-\ntations using Intersection over Union (IoU). To test speci-\nficity for individual objects, we select 12 COCO categories\nFigure 5. Across 12 COCO categories, the receptive fields of mul-\ntimodal neurons better segment the concept in each image than\nrandomly sampled neurons in the same layers. The Supplement\nprovides additional examples.\nFigure 6. Ablating multimodal neurons degrades image caption\ncontent. We plot the effect of ablating multimodal neurons ordered\nby gk,c and randomly sampled units in the same layers (left), and\nshow an example (right) of the effect on a single image caption.\nwith single object annotations, and show that across all cat-\negories, the receptive fields of multimodal neurons better\nsegment the object in each image than randomly sampled\nneurons from the same layers (Figure 5). While this exper-\niment shows that multimodal neurons are reliable detectors\nof concepts, we also test whether they are selectively ac-\ntive for images containing those concepts, or broadly active\nacross images. Results in the Supplement show preferential\nactivation on particular categories of images.\n3.3. Do multimodal neurons causally affect output?\nTo investigate how strongly multimodal neurons causally\naffect model output, we successively ablate units sorted by\ngk,c and measure the resulting change in the probability\nof token c. Results for all COCO validation images are\nshown in Figure 6, for multimodal neurons (filtered and un-\nfiltered for interpretability), and randomly selected units in\nthe same layers. When up to 6400 random units are ablated,\nwe find that the probability of token c is largely unaffected,\nbut ablating the same number of top-scoring units decreases\ntoken probability by 80% on average. Ablating multimodal\nneurons also leads to significant changes in the semantics\nof GPT-generated captions. Figure 6 shows one example;\nadditional analysis is provided in the Supplement.\n4. Conclusion\nWe find multimodal neurons in text-only transformer\nMLPs and show that these neurons consistently translate\nimage semantics into language. Interestingly, soft-prompt\ninputs to the language model do not map onto interpretable\ntokens in the output vocabulary, suggesting translation be-\ntween modalities happens inside the transformer. The ca-\npacity to align representations across modalities could un-\nderlie the utility of language models as general-purpose in-\nterfaces for tasks involving sequential modeling [25, 13, 38,\n29], ranging from next-move prediction in games [21, 32]\nto protein design [41, 9]. Understanding the roles of indi-\nvidual computational units can serve as a starting point for\ninvestigating how transformers generalize across tasks.\n5. Limitations\nWe study a single multimodal model (LiMBeR-BEIT)\nof particular interest because the vision and language com-\nponents were learned separately. The discovery of multi-\nmodal neurons in this setting motivates investigation of this\nphenomenon in other vision-language architectures, and\neven models aligning other modalities. Do similar neurons\nemerge when the visual encoder is replaced with a raw pixel\nstream such as in [25], or with a pretrained speech autoen-\ncoder? Furthermore, although we found that the outputs\nof the LiMBeR-BEIT projection layer are not immediately\ndecodable into interpretable language, our knowledge of\nthe structure of the vector spaces that represent information\nfrom different modalities remains incomplete, and we have\nnot investigated how concepts encoded by individual units\nare assembled from upstream representations. Building a\nmore mechanistic understanding of information processing\nwithin transfomers may help explain their surprising ability\nto generalize to non-textual representations.\n6. Acknowledgements\nWe are grateful for the support of the MIT-IBM Watson\nAI Lab, and ARL grant W911NF-18-2-0218. We thank Ja-\ncob Andreas, Achyuta Rajaram, and Tazo Chowdhury for\ntheir useful input and insightful discussions.\nReferences\n[1] J Alammar. Ecco: An open source library for the explain-\nability of transformer language models. In Proceedings of\nthe 59th annual meeting of the association for computational\nlinguistics and the 11th international joint conference on\nnatural language processing: System demonstrations, pages\n249\u2013257, 2021. 3\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021. 1, 2\n[3] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and\nAntonio Torralba. Network dissection: Quantifying inter-\npretability of deep visual representations. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 6541\u20136549, 2017. 2, 4\n[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and\nAntonio Torralba. Network dissection: Quantifying inter-\npretability of deep visual representations. In Computer Vi-\nsion and Pattern Recognition, 2017. 4\n[5] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,\nBolei Zhou, and Antonio Torralba. Understanding the role of\nindividual units in a deep neural network. Proceedings of the\nNational Academy of Sciences, 2020. 2, 4\n[6] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang,\nand Furu Wei. Knowledge neurons in pretrained transform-\ners. arXiv preprint arxiv:2104.08696, 2022. 2\n[7] Constantin Eichenberg, Sidney Black, Samuel Weinbach,\nLetitia Parcalabescu, and Anette Frank. Magma\u2013multimodal\naugmentation of generative models through adapter-based\nfinetuning. arXiv preprint arXiv:2112.05253, 2021. 1\n[8] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal\nVincent. Visualizing higher-layer features of a deep network.\nUniversity of Montreal, 1341(3):1, 2009. 2\n[9] Noelia Ferruz and Birte H\u00a8ocker.\nControllable protein de-\nsign with language models.\nNature Machine Intelligence,\n4(6):521\u2013532, 2022. 4\n[10] Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval\nSadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. Lm-\ndebugger: An interactive tool for inspection and interven-\ntion in transformer-based language models. arXiv preprint\narXiv:2204.12130, 2022. 3\n[11] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.\nTransformer feed-forward layers are key-value memories.\narXiv preprint arXiv:2012.14913, 2020. 2, 3\n[12] Gabriel Goh, Nick Cammarata \u2020, Chelsea Voss \u2020, Shan\nCarter, Michael Petrov, Ludwig Schubert, Alec Radford, and\nChris Olah.\nMultimodal neurons in artificial neural net-\nworks.\nDistill, 2021.\nhttps://distill.pub/2021/multimodal-\nneurons. 2\n[13] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen\nChi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLan-\nguage models are general-purpose interfaces. arXiv preprint\narXiv:2206.06336, 2022. 4\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2\n[15] Richard Held, Yuri Ostrovsky, Beatrice de Gelder, Tapan\nGandhi, Suma Ganesh, Umang Mathur, and Pawan Sinha.\nThe newly sighted fail to match seen with felt. Nature neu-\nroscience, 14(5):551\u2013553, 2011. 1\n[16] Evan Hernandez, Sarah Schwettmann, David Bau, Teona\nBagashvili, Antonio Torralba, and Jacob Andreas. Natural\nlanguage descriptions of deep visual features. In Interna-\ntional Conference on Learning Representations, 2022. 2, 4\n[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. CLIPScore: a reference-free evaluation met-\nric for image captioning. In EMNLP, 2021. 3\n[18] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal gen-\neration. arXiv preprint arXiv:2301.13823, 2023. 1\n[19] Andrej N Kolmogorov. Sulla determinazione empirica di una\nlegge didistribuzione. Giorn Dell\u2019inst Ital Degli Att, 4:89\u2013\n91, 1933. 3\n[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-efficient prompt tuning. arXiv preprint\narXiv:2104.08691, 2021. 1\n[21] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00b4egas,\nHanspeter Pfister, and Martin Wattenberg. Emergent world\nrepresentations: Exploring a sequence model trained on a\nsynthetic task. arXiv preprint arXiv:2210.13382, 2022. 4\n[22] Xiang Lisa Li and Percy Liang.\nPrefix-tuning: Optimiz-\ning continuous prompts for generation.\narXiv preprint\narXiv:2101.00190, 2021. 1\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 3\n[24] John Locke. An Essay Concerning Human Understanding.\nLondon, England: Oxford University Press, 1689. 1\n[25] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.\nPretrained transformers as universal computation engines.\narXiv preprint arXiv:2103.05247, 1, 2021. 1, 4, 5\n[26] Kevin Meng, David Bau, Alex J Andonian, and Yonatan Be-\nlinkov. Locating and editing factual associations in gpt. In\nAdvances in Neural Information Processing Systems, 2022.\n2, 3\n[27] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan\nBelinkov, and David Bau. Mass editing memory in a trans-\nformer. arXiv preprint arxiv:2210.07229, 2022. 2\n[28] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly mapping from image to text space. arXiv\npreprint arXiv:2209.15162, 2022. 1, 2, 3\n[29] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,\nDanny Driess, Montserrat Gonzalez Arenas, Kanishka\nRao, Dorsa Sadigh, and Andy Zeng.\nLarge language\nmodels as general pattern machines.\nIn arXiv preprint\narXiv:2307.04721, 2023. 4\n[30] Michael J. Morgan. Molyneux\u2019s Question: Vision, Touch and\nthe Philosophy of Perception. Cambridge University Press,\n1977. 1\n[31] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.\nFeature visualization. Distill, 2(11):e7, 2017. 2\n[32] Vishal Pallagani, Bharath Muppasani, Keerthiram Muruge-\nsan, Francesca Rossi, Lior Horesh, Biplav Srivastava,\nFrancesco Fabiano, and Andrea Loreggia.\nPlansformer:\nGenerating symbolic plans using transformers.\narXiv\npreprint arXiv:2212.08681, 2022. 4\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2\n[34] Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov,\nJonathan Berant, and Amir Globerson. What are you token\nabout? dense retrieval as distributions over the vocabulary.\narXiv preprint arXiv:2212.10380, 2022. 3\n[35] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-CAM: Visual explanations from deep networks via\ngradient-based localization. International Journal of Com-\nputer Vision, 128(2):336\u2013359, oct 2019. 2\n[36] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, Melbourne, Australia, July 2018. Association\nfor Computational Linguistics. 2\n[37] Nickolay Smirnov. Table for estimating the goodness of fit\nof empirical distributions. The annals of mathematical statis-\ntics, 19(2):279\u2013281, 1948. 3\n[38] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu\nAwal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta, Adri`a Garriga-\nAlonso, et al.\nBeyond the imitation game: Quantifying\nand extrapolating the capabilities of language models. arXiv\npreprint arXiv:2206.04615, 2022. 4\n[39] Mukund Sundararajan, Ankur Taly, and Qiqi Yan.\nAx-\niomatic attribution for deep networks. In Doina Precup and\nYee Whye Teh, editors, Proceedings of the 34th Interna-\ntional Conference on Machine Learning, volume 70 of Pro-\nceedings of Machine Learning Research, pages 3319\u20133328.\nPMLR, 06\u201311 Aug 2017. 2\n[40] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021. 1\n[41] Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal\nTurhan, Aybar C Acar, and Tunca Do\u02d8gan. Learning func-\ntional properties of proteins with language models. Nature\nMachine Intelligence, 4(3):227\u2013245, 2022. 4\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[43] Ben\nWang\nand\nAran\nKomatsuzaki.\nGPT-J-6B:\nA\n6\nBillion\nParameter\nAutoregressive\nLanguage\nModel.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax, May 2021. 1, 2\n[44] Matthew D Zeiler and Rob Fergus.\nVisualizing and un-\nderstanding convolutional networks. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part I 13, pages\n818\u2013833. Springer, 2014. 2\n[45] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. Bertscore: Evaluating text genera-\ntion with bert. arXiv preprint arXiv:1904.09675, 2019. 3\n[46] Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba.\nInterpreting deep visual representations via network dissec-\ntion.\nIEEE transactions on pattern analysis and machine\nintelligence, 41(9):2131\u20132145, 2018. 4\nSupplemental Material for Multimodal Neurons in Pretrained Text-Only\nTransformers\nS.1. Implementation details\nWe follow the LiMBeR process for augmenting pre-\ntrained GPT-J with vision as described in Merullo et al.\n(2022). Each image is resized to (224, 224) and encoded\ninto a sequence [i1, ..., ik] by the image encoder E, where\nk = 196 and each i corresponds to an image patch of size\n(16, 16). We use self-supervised BEIT as E, trained with\nno linguistic supervision, which produces [i1, ..., ik] of di-\nmensionality 1024. To project image representations i into\nthe transformer-defined embedding space of GPT-J, we use\nlinear layer P from Merullo et al. (2022), trained on an\nimage-to-text task (CC3M image captioning). P transforms\n[i1, ..., ik] into soft prompts [x1, ..., xk] of dimensionality\n4096, which we refer to as the image prompt. Following\nconvention from SimVLM, MAGMA and LiMBeR, we ap-\npend the text prefix \u201cA picture of\u201d after every every image\nprompt. Thus for each image, GPT-J receives as input a\n(199, 4096) prompt and outputs a probability distribution y\nover next-token continuations of that prompt.\nTo calculate neuron attribution scores, we generate a cap-\ntion for each image by sampling from y using temperature\nT = 0, which selects the token with the highest probability\nat each step. The attribution score gk,c of neuron k is then\ncalculated with respect to token c, where c is the first noun\nin the generated caption (which directly follows the image\nprompt and is less influenced by earlier token predictions).\nIn the rare case where this noun is comprised of multiple\ntokens, we let c be the first of these tokens. This attribution\nscore lets us rank multimodal neurons by how much they\ncontribute to the crossmodal image captioning task.\nS.2. Example multimodal neurons\nTable S.1 shows additional examples of multimodal neu-\nrons detected and decoded for randomly sampled images\nfrom the COCO 2017 validation set. The table shows the\ntop 20 neurons across all MLP layers for each image. In\nanalyses where we filter for interpretable neurons that cor-\nrespond to objects or object features in images, we remove\nneurons that decode primarily to word fragments or punc-\ntuation. Interpretable units (units where at least 7 of the top\n10 tokens are words in the SCOWL English dictionary, for\nen-US or en-GB, with \u2265 3 letters) are highlighted in bold.\nS.3. Evaluating agreement with image captions\nWe use BERTScore (F1) as a metric for evaluating how\nwell a list of tokens corresponds to the semantic content\nof an image caption. Section 2.2 uses this metric to evalu-\nate multimodal neurons relative to ground-truth human an-\nnotations from COCO, and Section 3.1 uses the metric to\ndetermine whether projection layer P translates [i1, ..., ik]\ninto [x1, ..., xk] that already map visual features onto re-\nlated language before reaching transformer MLPs. Given\nthat [x1, ..., xk] do not correspond to discrete tokens, we\nmap each x onto the 5 token vectors with highest cosine\nsimilarity in the transformer embedding space for analysis.\nTable S.2 shows example decoded soft prompts for a ran-\ndomly sampled COCO image. For comparison, we sample\nrandom vectors of size 4096 and use the same procedure\nto map them onto their nearest neighbors in the GPT-J em-\nbedding space. BERTScores for the random soft prompts\nare shown alongside scores for the image soft prompts. The\nmeans of these BERTScores, as well as the maximum val-\nues, are indistinguishable for real and random soft prompts\n(see Table S.2 for a single image and Figure 3 in the main\npaper for the distribution across COCO images). Thus we\nconclude that P produces image prompts that fit within the\nGPT-J embedding space, but do not already map image fea-\ntures onto related language: this occurs deeper inside the\ntransformer.\nS.4. Selectivity of multimodal neurons\nFigure S.1 shows additional examples of activation\nmasks of individual multimodal neurons over COCO val-\nidation images, and IoU scores comparing each activation\nmask with COCO object annotations.\nWe conduct an additional experiment to test whether\nmultimodal neurons are selectively active for images con-\ntaining particular concepts. If unit k is selective for the\nimages it describes (and not, for instance, for many im-\nages), then we expect greater Ak\nxi on images where it rel-\nevant to the caption than on images where it is irrelevant. It\nis conceivable that our method merely extracts a set of high-\nactivating neurons, not a set of neurons that are selectively\nactive on the inputs we claim they are relevant to captioning.\nWe select 10 diverse ImageNet classes (see Figure S.2)\nand compute the top 100 scoring units per image on each\nof 200 randomly sampled images per class in the ImageNet\ntraining set, filtered for interpretable units. Then for each\nclass, we select the 20 units that appear in the most im-\nages for that class. We measure the mean activation of these\nunits across all patches in the ImageNet validation images\nfor each of the 10 classes. Figure S.2(a) shows the compari-\nson of activations across each of the categories. We find that\nneurons activate more frequently on images in their own\ncategory than for others. This implies that our pipeline does\nnot extract a set of general visually attentive units, but rather\nunits that are specifically tied to image semantics.\nImages\nLayer.unit\nPatch\nDecoding (top 5 tokens)\nAttr. score\nL7.u15772\n119\n\u2018 animals\u2019, \u2018 embryos\u2019, \u2018 kittens\u2019, \u2018 mammals\u2019, \u2018 eggs\u2019\n0.0214\nL5.u4923\n119\n\u2018 birds\u2019, \u2018 cages\u2019, \u2018 species\u2019, \u2018 breeding\u2019, \u2018 insects\u2019\n0.0145\nL7.u12134\n119\n\u2018 aircraft\u2019, \u2018 flight\u2019, \u2018 airplanes\u2019, \u2018 Flight\u2019, \u2018 Aircraft\u2019\n0.0113\nL5.u4888\n119\n\u2018 Boat\u2019, \u2018 sails\u2019, \u2018voy\u2019, \u2018 boats\u2019, \u2018 ships\u2019\n0.0085\nL7.u5875\n119\n\u2018 larvae\u2019, \u2018 insects\u2019, \u2018 mosquitoes\u2019, \u2018 flies\u2019, \u2018 species\u2019\n0.0083\nL8.u2012\n105\n\u2018 whales\u2019, \u2018 turtles\u2019, \u2018 whale\u2019, \u2018 birds\u2019, \u2018 fishes\u2019\n0.0081\nL7.u3030\n119\n\u2018 Island\u2019, \u2018 island\u2019, \u2018 Islands\u2019, \u2018 islands\u2019, \u2018 shore\u2019\n0.0078\nL7.u14308\n119\n\u2018uses\u2019, \u2018 dec\u2019, \u2018bill\u2019, \u2018oid\u2019, \u2018FS\u2019\n0.0078\nL9.u12771\n119\n\u2018 satellites\u2019, \u2018 Flight\u2019, \u2018 orbiting\u2019, \u2018 spacecraft\u2019, \u2018 ship\u2019\n0.0075\nL4.u12317\n119\n\u2018 embryos\u2019, \u2018 chicken\u2019, \u2018 meat\u2019, \u2018 fruits\u2019, \u2018 cows\u2019\n0.0071\nL8.u2012\n119\n\u2018 whales\u2019, \u2018 turtles\u2019, \u2018 whale\u2019, \u2018 birds\u2019, \u2018 fishes\u2019\n0.0062\nL5.u4530\n119\n\u2018 herds\u2019, \u2018 livestock\u2019, \u2018 cattle\u2019, \u2018 herd\u2019, \u2018 manure\u2019\n0.0056\nL5.u4923\n105\n\u2018 birds\u2019, \u2018 cages\u2019, \u2018 species\u2019, \u2018 breeding\u2019, \u2018 insects\u2019\n0.0055\nL6.u8956\n119\n\u2018 virus\u2019, \u2018 strains\u2019, \u2018 infect\u2019, \u2018 viruses\u2019, \u2018 parasites\u2019\n0.0052\nL7.u2159\n105\n\u2018 species\u2019, \u2018species\u2019, \u2018 bacteria\u2019, \u2018 genus\u2019, \u2018 Species\u2019\n0.0051\nL10.u4819\n119\n\u2018c\u00b8K\u00b8 \u00b0\u2019, \u2018\u00ac\u00bc\u2019, \u2018\u201d\u201d\u201d\u2019, \u2018 Marketable\u2019, \u2018\u02daa\u00a7\u2019\n0.0051\nL5.u4923\n118\n\u2018 birds\u2019, \u2018 cages\u2019, \u2018 species\u2019, \u2018 breeding\u2019, \u2018 insects\u2019\n0.0050\nL10.u927\n3\n\u2018onds\u2019, \u2018rog\u2019, \u2018lys\u2019, \u2018arrow\u2019, \u2018ond\u2019\n0.0050\nL11.u7635\n119\n\u2018 birds\u2019, \u2018birds\u2019, \u2018 butterflies\u2019, \u2018 kittens\u2019, \u2018 bird\u2019\n0.0049\nL9.u15445\n119\n\u2018 radar\u2019, \u2018 standby\u2019, \u2018 operational\u2019, \u2018 flight\u2019, \u2018 readiness\u2019\n0.0048\nL5.u15728\n119\n\u2018 playoff\u2019, \u2018 players\u2019, \u2018 teammate\u2019, \u2018 player\u2019, \u2018Players\u2019\n0.0039\nL12.u11268\n113\n\u2018elson\u2019, \u2018ISA\u2019, \u2018Me\u2019, \u2018PRES\u2019, \u2018SO\u2019\n0.0039\nL5.u9667\n119\n\u2018 workouts\u2019, \u2018 workout\u2019, \u2018 Training\u2019, \u2018 trainer\u2019, \u2018 exercises\u2019\n0.0034\nL9.u15864\n182\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0034\nL9.u9766\n119\n\u2018 soccer\u2019, \u2018 football\u2019, \u2018 player\u2019, \u2018 baseball\u2019, \u2018player\u2019\n0.0033\nL10.u4819\n182\n\u2018c\u00b8K\u00b8 \u00b0\u2019, \u2018\u00ac\u00bc\u2019, \u2018\u201d\u201d\u201d\u2019, \u2018 Marketable\u2019, \u2018\u02daa\u00a7\u2019\n0.0033\nL18.u15557\n150\n\u2018imer\u2019, \u2018ohan\u2019, \u2018ellow\u2019, \u2018ims\u2019, \u2018gue\u2019\n0.0032\nL12.u6426\n160\n\u2018\u02c6a\u00a2\u2019, \u2018 \u02c6A\u00ae\u2019, \u2018 syndrome\u2019, \u2018 Productions\u2019, \u2018 Ltd\u2019\n0.0032\nL8.u15435\n119\n\u2018 tennis\u2019, \u2018 tournaments\u2019, \u2018 tournament\u2019, \u2018 golf\u2019, \u2018 racing\u2019\n0.0032\nL11.u4236\n75\n\u2018 starring\u2019, \u2018 played\u2019, \u2018 playable\u2019, \u2018 Written\u2019, \u2018 its\u2019\n0.0031\nL8.u6207\n119\n\u2018 player\u2019, \u2018 players\u2019, \u2018 Player\u2019, \u2018 \u00a8A\u2019, \u2018 talent\u2019\n0.0031\nL6.u5975\n119\n\u2018 football\u2019, \u2018 soccer\u2019, \u2018 basketball\u2019, \u2018 Soccer\u2019, \u2018 Football\u2019\n0.0030\nL2.u10316\n75\n\u2018\u00a8\u0131\u2019, \u2018/**\u2019, \u2018Q\u2019, \u2018The\u2019, \u2018//\u2019\n0.0028\nL12.u8390\n89\n\u2018etheless\u2019, \u2018viously\u2019, \u2018theless\u2019, \u2018bsite\u2019, \u2018terday\u2019\n0.0028\nL5.u7958\n89\n\u2018 rugby\u2019, \u2018 football\u2019, \u2018 player\u2019, \u2018 soccer\u2019, \u2018 footballer\u2019\n0.0028\nL20.u9909\n89\n\u2018 Associates\u2019, \u2018 Alt\u2019, \u2018 para\u2019, \u2018 Lt\u2019, \u2018 similarly\u2019\n0.0026\nL5.u8219\n75\n\u2018 portion\u2019, \u2018 regime\u2019, \u2018 sector\u2019, \u2018 situation\u2019, \u2018 component\u2019\n0.0026\nL11.u7264\n75\n\u2018 portion\u2019, \u2018 finale\u2019, \u2018 environment\u2019, \u2018iest\u2019, \u2018 mantle\u2019\n0.0026\nL20.u452\n103\n\u2018 CLE\u2019, \u2018 plain\u2019, \u2018 clearly\u2019, \u2018 Nil\u2019, \u2018 Sullivan\u2019\n0.0026\nL7.u16050\n89\n\u2018pc\u2019, \u2018IER\u2019, \u2018 containing\u2019, \u2018 formatted\u2019, \u2018 supplemented\u2019\n0.0026\nL10.u927\n73\n\u2018onds\u2019, \u2018rog\u2019, \u2018lys\u2019, \u2018arrow\u2019, \u2018ond\u2019\n0.0087\nL5.u9667\n101\n\u2018 workouts\u2019, \u2018 workout\u2019, \u2018 Training\u2019, \u2018 trainer\u2019, \u2018 exercises\u2019\n0.0081\nL9.u3561\n73\n\u2018 mix\u2019, \u2018 CRC\u2019, \u2018 critically\u2019, \u2018 gulf\u2019, \u2018 mechanically\u2019\n0.0076\nL9.u5970\n73\n\u2018 construct\u2019, \u2018 performance\u2019, \u2018 global\u2019, \u2018 competing\u2019, \u2018 transact\u2019\n0.0054\nL10.u562\n73\n\u2018 prev\u2019, \u2018 struct\u2019, \u2018 stable\u2019, \u2018 marg\u2019, \u2018 imp\u2019\n0.0054\nL6.u14388\n87\n\u2018 march\u2019, \u2018 treadmill\u2019, \u2018 Championships\u2019, \u2018 racing\u2019, \u2018 marathon\u2019\n0.0052\nL14.u10320\n73\n\u2018 print\u2019, \u2018 handle\u2019, \u2018 thing\u2019, \u2018catch\u2019, \u2018error\u2019\n0.0051\nL9.u3053\n73\n\u2018essel\u2019, \u2018ked\u2019, \u2018 ELE\u2019, \u2018ument\u2019, \u2018ue\u2019\n0.0047\nL5.u4932\n73\n\u2018eman\u2019, \u2018rack\u2019, \u2018ago\u2019, \u2018anne\u2019, \u2018ison\u2019\n0.0046\nL9.u7777\n101\n\u2018dr\u2019, \u2018thur\u2019, \u2018tern\u2019, \u2018mas\u2019, \u2018mass\u2019\n0.0042\nL6.u16106\n73\n\u2018umble\u2019, \u2018archives\u2019, \u2018room\u2019, \u2018 decentral\u2019, \u2018Root\u2019\n0.0040\nL5.u14519\n73\n\u2018 abstract\u2019, \u2018 global\u2019, \u2018map\u2019, \u2018exec\u2019, \u2018kernel\u2019\n0.0039\nL11.u10405\n73\n\u2018amed\u2019, \u2018elect\u2019, \u20181\u2019, \u2018vol\u2019, \u2018vis\u2019\n0.0038\nL9.u325\n87\n\u2018 training\u2019, \u2018 tournaments\u2019, \u2018ango\u2019, \u2018 ballet\u2019, \u2018 gymn\u2019\n0.0038\nL6.u14388\n101\n\u2018 march\u2019, \u2018 treadmill\u2019, \u2018 Championships\u2019, \u2018 racing\u2019, \u2018 marathon\u2019\n0.0038\nL7.u3844\n101\n\u2018DERR\u2019, \u2018Charges\u2019, \u2018wana\u2019, \u2018\u00ac\u00bc\u2019, \u2018verages\u2019\n0.0036\nL9.u15864\n101\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0036\nL7.u3330\n101\n\u2018 Officers\u2019, \u2018 officers\u2019, \u2018 patrolling\u2019, \u2018 patrols\u2019, \u2018 troops\u2019\n0.0036\nL8.u8807\n73\n\u2018 program\u2019, \u2018 updates\u2019, \u2018 programs\u2019, \u2018 document\u2019, \u2018 format\u2019\n0.0034\nL6.u12536\n87\n\u2018 ankles\u2019, \u2018 joints\u2019, \u2018 biome\u2019, \u2018 injuries\u2019, \u2018 injury\u2019\n0.0034\nImages\nLayer.unit\nPatch\nDecoding (top 5 tokens)\nAttr. score\nL8.u14504\n13\n\u2018 upstairs\u2019, \u2018 homeowners\u2019, \u2018 apartments\u2019, \u2018 houses\u2019, \u2018 apartment\u2019\n0.0071\nL13.u15107\n93\n\u2018 meals\u2019, \u2018 meal\u2019, \u2018 dinner\u2019, \u2018 dishes\u2019, \u2018 cuisine\u2019\n0.0068\nL8.u14504\n93\n\u2018 upstairs\u2019, \u2018 homeowners\u2019, \u2018 apartments\u2019, \u2018 houses\u2019, \u2018 apartment\u2019\n0.0052\nL8.u14504\n150\n\u2018 upstairs\u2019, \u2018 homeowners\u2019, \u2018 apartments\u2019, \u2018 houses\u2019, \u2018 apartment\u2019\n0.0048\nL9.u4691\n13\n\u2018 houses\u2019, \u2018 buildings\u2019, \u2018 dwellings\u2019, \u2018 apartments\u2019, \u2018 homes\u2019\n0.0043\nL8.u13681\n93\n\u2018 sandwiches\u2019, \u2018 foods\u2019, \u2018 salad\u2019, \u2018 sauce\u2019, \u2018 pizza\u2019\n0.0041\nL12.u4638\n93\n\u2018 wash\u2019, \u2018 Darkness\u2019, \u2018 Caps\u2019, \u2018 blush\u2019, \u2018 Highest\u2019\n0.0040\nL9.u3561\n93\n\u2018 mix\u2019, \u2018 CRC\u2019, \u2018 critically\u2019, \u2018 gulf\u2019, \u2018 mechanically\u2019\n0.0040\nL7.u5533\n93\n\u2018bags\u2019, \u2018Items\u2019, \u2018 comprehens\u2019, \u2018 decor\u2019, \u2018bag\u2019\n0.0039\nL9.u8687\n93\n\u2018 eaten\u2019, \u2018 foods\u2019, \u2018 food\u2019, \u2018 diet\u2019, \u2018 eating\u2019\n0.0037\nL12.u4109\n93\n\u2018 Lakes\u2019, \u2018 Hof\u2019, \u2018 Kass\u2019, \u2018 Cotton\u2019, \u2018Council\u2019\n0.0036\nL8.u943\n93\n\u2018 Foods\u2019, \u2018Food\u2019, \u2018let\u2019, \u2018 lunch\u2019, \u2018commercial\u2019\n0.0036\nL5.u16106\n93\n\u2018ware\u2019, \u2018 halls\u2019, \u2018 salt\u2019, \u2018WARE\u2019, \u2018 mat\u2019\n0.0032\nL8.u14504\n143\n\u2018 upstairs\u2019, \u2018 homeowners\u2019, \u2018 apartments\u2019, \u2018 houses\u2019, \u2018 apartment\u2019\n0.0032\nL9.u11735\n93\n\u2018 hysterical\u2019, \u2018 Gould\u2019, \u2018 Louie\u2019, \u2018 Gamble\u2019, \u2018 Brown\u2019\n0.0031\nL8.u14504\n149\n\u2018 upstairs\u2019, \u2018 homeowners\u2019, \u2018 apartments\u2019, \u2018 houses\u2019, \u2018 apartment\u2019\n0.0031\nL5.u2771\n93\n\u2018 occupations\u2019, \u2018 industries\u2019, \u2018 operations\u2019, \u2018 occupational\u2019, \u2018 agriculture\u2019\n0.0029\nL9.u15864\n55\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0028\nL9.u4691\n149\n\u2018 houses\u2019, \u2018 buildings\u2019, \u2018 dwellings\u2019, \u2018 apartments\u2019, \u2018 homes\u2019\n0.0028\nL7.u10853\n13\n\u2018 boutique\u2019, \u2018 firm\u2019, \u2018 Associates\u2019, \u2018 restaurant\u2019, \u2018 Gifts\u2019\n0.0028\nL8.u15435\n160\n\u2018 tennis\u2019, \u2018 tournaments\u2019, \u2018 tournament\u2019, \u2018 golf\u2019, \u2018 racing\u2019\n0.0038\nL1.u15996\n132\n\u2018276\u2019, \u2018PS\u2019, \u2018ley\u2019, \u2018room\u2019, \u2018 Will\u2019\n0.0038\nL5.u6439\n160\n\u2018 ge\u2019, \u2018 fibers\u2019, \u2018 hair\u2019, \u2018 geometric\u2019, \u2018 ori\u2019\n0.0037\nL9.u15864\n160\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0034\nL12.u2955\n160\n\u2018Untitled\u2019, \u2018Welcome\u2019, \u2018========\u2019, \u2018Newsletter\u2019, \u2018====\u2019\n0.0033\nL12.u2955\n146\n\u2018Untitled\u2019, \u2018Welcome\u2019, \u2018========\u2019, \u2018Newsletter\u2019, \u2018====\u2019\n0.0032\nL7.u2688\n160\n\u2018rection\u2019, \u2018itud\u2019, \u2018 Ratio\u2019, \u2018lat\u2019, \u2018 ratio\u2019\n0.0031\nL8.u4372\n160\n\u2018 footage\u2019, \u2018 filmed\u2019, \u2018 filming\u2019, \u2018 videos\u2019, \u2018 clips\u2019\n0.0029\nL10.u4819\n146\n\u2018c\u00b8K\u00b8 \u00b0\u2019, \u2018\u00ac\u00bc\u2019, \u2018\u201d\u201d\u201d\u2019, \u2018 Marketable\u2019, \u2018\u02daa\u00a7\u2019\n0.0029\nL8.u15435\n93\n\u2018 tennis\u2019, \u2018 tournaments\u2019, \u2018 tournament\u2019, \u2018 golf\u2019, \u2018 racing\u2019\n0.0029\nL8.u15435\n146\n\u2018 tennis\u2019, \u2018 tournaments\u2019, \u2018 tournament\u2019, \u2018 golf\u2019, \u2018 racing\u2019\n0.0029\nL10.u927\n132\n\u2018onds\u2019, \u2018rog\u2019, \u2018lys\u2019, \u2018arrow\u2019, \u2018ond\u2019\n0.0027\nL9.u15864\n146\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0026\nL1.u8731\n132\n\u2018 \u02c6aG\u00b8 \u00a6\u2019, \u2018 [\u02c6aG\u00b8 \u00a6]\u2019, \u2018\u02c6aG\u00b8 \u00a6\u2019, \u2018 ...\u2019, \u2018 Will\u2019\n0.0025\nL8.u16330\n160\n\u2018 bouncing\u2019, \u2018 hitting\u2019, \u2018 bounce\u2019, \u2018 moving\u2019, \u2018 bounced\u2019\n0.0025\nL9.u1908\n146\n\u2018 members\u2019, \u2018 country\u2019, \u2018 VIII\u2019, \u2018 Spanish\u2019, \u2018 330\u2019\n0.0024\nL10.u4819\n160\n\u2018c\u00b8K\u00b8 \u00b0\u2019, \u2018\u00ac\u00bc\u2019, \u2018\u201d\u201d\u201d\u2019, \u2018 Marketable\u2019, \u2018\u02daa\u00a7\u2019\n0.0024\nL11.u14710\n160\n\u2018Search\u2019, \u2018Follow\u2019, \u2018Early\u2019, \u2018Compar\u2019, \u2018Category\u2019\n0.0024\nL6.u132\n160\n\u2018 manually\u2019, \u2018 replace\u2019, \u2018 concurrently\u2019, \u2018otropic\u2019, \u2018 foregoing\u2019\n0.0024\nL7.u5002\n160\n\u2018 painting\u2019, \u2018 paintings\u2019, \u2018 sculpture\u2019, \u2018 sculptures\u2019, \u2018 painted\u2019\n0.0024\nImages\nLayer.unit\nPatch\nDecoding (top 5 tokens)\nAttr. score\nL5.u13680\n132\n\u2018 driver\u2019, \u2018 drivers\u2019, \u2018 cars\u2019, \u2018heading\u2019, \u2018cars\u2019\n0.0091\nL11.u9566\n132\n\u2018 traffic\u2019, \u2018 network\u2019, \u2018 networks\u2019, \u2018 Traffic\u2019, \u2018network\u2019\n0.0090\nL12.u11606\n132\n\u2018 chassis\u2019, \u2018 automotive\u2019, \u2018 design\u2019, \u2018 electronics\u2019, \u2018 specs\u2019\n0.0078\nL7.u6109\n132\n\u2018 automobile\u2019, \u2018 automobiles\u2019, \u2018 engine\u2019, \u2018 Engine\u2019, \u2018 cars\u2019\n0.0078\nL6.u11916\n132\n\u2018 herd\u2019, \u2018loads\u2019, \u2018 racing\u2019, \u2018 herds\u2019, \u2018 horses\u2019\n0.0071\nL8.u562\n132\n\u2018 vehicles\u2019, \u2018 vehicle\u2019, \u2018 cars\u2019, \u2018veh\u2019, \u2018 Vehicles\u2019\n0.0063\nL7.u3273\n132\n\u2018ride\u2019, \u2018 riders\u2019, \u2018 rides\u2019, \u2018 ridden\u2019, \u2018 rider\u2019\n0.0062\nL13.u5734\n132\n\u2018 Chevrolet\u2019, \u2018 Motorsport\u2019, \u2018 cars\u2019, \u2018 automotive\u2019, \u2018 vehicle\u2019\n0.0062\nL8.u2952\n132\n\u2018 rigging\u2019, \u2018 valves\u2019, \u2018 nozzle\u2019, \u2018 pipes\u2019, \u2018 tubing\u2019\n0.0059\nL13.u8962\n132\n\u2018 cruising\u2019, \u2018 flying\u2019, \u2018 flight\u2019, \u2018 refuel\u2019, \u2018 Flying\u2019\n0.0052\nL9.u3561\n116\n\u2018 mix\u2019, \u2018 CRC\u2019, \u2018 critically\u2019, \u2018 gulf\u2019, \u2018 mechanically\u2019\n0.0051\nL13.u107\n132\n\u2018 trucks\u2019, \u2018 truck\u2019, \u2018 trailer\u2019, \u2018 parked\u2019, \u2018 driver\u2019\n0.0050\nL14.u10852\n132\n\u2018Veh\u2019, \u2018 driver\u2019, \u2018 automotive\u2019, \u2018 automakers\u2019, \u2018Driver\u2019\n0.0049\nL6.u1989\n132\n\u2018text\u2019, \u2018light\u2019, \u2018TL\u2019, \u2018X\u2019, \u2018background\u2019\n0.0049\nL2.u14243\n132\n\u2018ousel\u2019, \u2018 Warriors\u2019, \u2018riages\u2019, \u2018illion\u2019, \u2018Ord\u2019\n0.0048\nL5.u6589\n132\n\u2018 vehicles\u2019, \u2018 motorcycles\u2019, \u2018 aircraft\u2019, \u2018 tyres\u2019, \u2018 cars\u2019\n0.0046\nL7.u4574\n132\n\u2018 plants\u2019, \u2018 plant\u2019, \u2018 roof\u2019, \u2018 compost\u2019, \u2018 wastewater\u2019\n0.0045\nL7.u6543\n132\n\u2018 distance\u2019, \u2018 downhill\u2019, \u2018 biking\u2019, \u2018 riders\u2019, \u2018 journeys\u2019\n0.0045\nL16.u9154\n132\n\u2018 driver\u2019, \u2018 drivers\u2019, \u2018 vehicle\u2019, \u2018 vehicles\u2019, \u2018driver\u2019\n0.0045\nL12.u7344\n132\n\u2018 commemor\u2019, \u2018 streets\u2019, \u2018 celebrations\u2019, \u2018 Streets\u2019, \u2018 highways\u2019\n0.0044\nL12.u9058\n174\n\u2018 swimming\u2019, \u2018 Swim\u2019, \u2018 swim\u2019, \u2018 fishes\u2019, \u2018 water\u2019\n0.0062\nL17.u10507\n174\n\u2018 rivers\u2019, \u2018 river\u2019, \u2018 lake\u2019, \u2018 lakes\u2019, \u2018 River\u2019\n0.0049\nL7.u3138\n174\n\u2018 basin\u2019, \u2018 ocean\u2019, \u2018 islands\u2019, \u2018 valleys\u2019, \u2018 mountains\u2019\n0.0046\nL5.u6930\n149\n\u2018 rivers\u2019, \u2018 river\u2019, \u2018 River\u2019, \u2018 waters\u2019, \u2018 waterways\u2019\n0.0042\nL7.u14218\n174\n\u2018 docks\u2019, \u2018 Coast\u2019, \u2018 swimming\u2019, \u2018 swim\u2019, \u2018melon\u2019\n0.0040\nL9.u4379\n149\n\u2018 river\u2019, \u2018 stream\u2019, \u2018 River\u2019, \u2018 Valley\u2019, \u2018 flow\u2019\n0.0038\nL6.u5868\n149\n\u2018water\u2019, \u2018 water\u2019, \u2018 waters\u2019, \u2018 river\u2019, \u2018 River\u2019\n0.0036\nL9.u4379\n174\n\u2018 river\u2019, \u2018 stream\u2019, \u2018 River\u2019, \u2018 Valley\u2019, \u2018 flow\u2019\n0.0036\nL5.u6930\n174\n\u2018 rivers\u2019, \u2018 river\u2019, \u2018 River\u2019, \u2018 waters\u2019, \u2018 waterways\u2019\n0.0032\nL7.u3138\n149\n\u2018 basin\u2019, \u2018 ocean\u2019, \u2018 islands\u2019, \u2018 valleys\u2019, \u2018 mountains\u2019\n0.0029\nL6.u5868\n174\n\u2018water\u2019, \u2018 water\u2019, \u2018 waters\u2019, \u2018 river\u2019, \u2018 River\u2019\n0.0028\nL7.u416\n136\n\u2018 praise\u2019, \u2018 glimpse\u2019, \u2018 glimps\u2019, \u2018 palate\u2019, \u2018 flavours\u2019\n0.0027\nL10.u15235\n149\n\u2018 water\u2019, \u2018 waters\u2019, \u2018water\u2019, \u2018 lake\u2019, \u2018 lakes\u2019\n0.0026\nL4.u2665\n136\n\u2018 levels\u2019, \u2018 absorbed\u2019, \u2018 density\u2019, \u2018 absorption\u2019, \u2018 equilibrium\u2019\n0.0026\nL10.u14355\n149\n\u2018 roads\u2019, \u2018 paths\u2019, \u2018 flows\u2019, \u2018 routes\u2019, \u2018 streams\u2019\n0.0026\nL17.u10507\n149\n\u2018 rivers\u2019, \u2018 river\u2019, \u2018 lake\u2019, \u2018 lakes\u2019, \u2018 River\u2019\n0.0024\nL7.u7669\n174\n\u2018 weather\u2019, \u2018 season\u2019, \u2018 forecast\u2019, \u2018 rains\u2019, \u2018 winters\u2019\n0.0024\nL8.u9322\n136\n\u2018 combustion\u2019, \u2018 turbulence\u2019, \u2018 recoil\u2019, \u2018 vibration\u2019, \u2018 hydrogen\u2019\n0.0024\nL9.u15864\n182\n\u2018lihood\u2019, \u2018/**\u2019, \u2018Advertisements\u2019, \u2018.\u201d.\u2019, \u2018\u201d\u201d\u201d\u2019\n0.0022\nL7.u3138\n78\n\u2018 basin\u2019, \u2018 ocean\u2019, \u2018 islands\u2019, \u2018 valleys\u2019, \u2018 mountains\u2019\n0.0021\nTable S.1. Results of attribution analysis for randomly sampled images from the COCO validation set. Includes decoded tokens for the top\n20 units by attribution score. The first column shows the COCO image and superimposed heatmaps of the mean activations from the top\n20 units and the top interpretable units (shown in bold). Units can repeat if they attain a high attribution score on multiple image patches.\nImage\nCOCO Human Captions\nGPT Caption\nA man riding a snowboard down the side of a snow covered slope.\nA person jumping on the ice.\nA man snowboarding down the side of a snowy mountain.\nPerson snowboarding down a steep snow covered slope.\nA person snowboards on top of a snowy path.\nThe person holds both hands in the air while snowboarding.\nPatch\nImage soft prompt (nearest neighbor tokens)\nBSc.\nRandom soft prompt (nearest neighbor tokens)\nBSc.\n144\n[\u2018nav\u2019, \u2018GY\u2019, \u2018+++\u2019, \u2018done\u2019, \u2018Sets\u2019]\n.29\n[\u2018Movement\u2019, \u2018Ord\u2019, \u2018CLUD\u2019, \u2018levy\u2019, \u2018LI\u2019]\n.31\n80\n[\u2018heels\u2019, \u2018merits\u2019, \u2018flames\u2019, \u2018platform\u2019, \u2018fledged\u2019]\n.36\n[\u2018adic\u2019, \u2018Stub\u2019, \u2018imb\u2019, \u2018VER\u2019, \u2018stroke\u2019]\n.34\n169\n[\u2018ear\u2019, \u2018Nelson\u2019, \u2018Garden\u2019, \u2018Phill\u2019, \u2018Gun\u2019]\n.32\n[\u2018Thank\u2019, \u2018zilla\u2019, \u2018Develop\u2019, \u2018Invest\u2019, \u2018Fair\u2019]\n.31\n81\n[\u2018vanilla\u2019, \u2018Poc\u2019, \u2018Heritage\u2019, \u2018Tarant\u2019, \u2018bridge\u2019]\n.33\n[\u2018Greek\u2019, \u2018eph\u2019, \u2018jobs\u2019, \u2018phylogen\u2019, \u2018TM\u2019]\n.30\n89\n[\u2018oily\u2019, \u2018stant\u2019, \u2018cement\u2019, \u2018Caribbean\u2019, \u2018Nad\u2019]\n.37\n[\u2018Forestry\u2019, \u2018Mage\u2019, \u2018Hatch\u2019, \u2018Buddh\u2019, \u2018Beaut\u2019]\n.34\n124\n[\u2018ension\u2019, \u2018ideas\u2019, \u2018GY\u2019, \u2018uler\u2019, \u2018Nelson\u2019]\n.32\n[\u2018itone\u2019, \u2018gest\u2019, \u2018Af\u2019, \u2018iple\u2019, \u2018Dial\u2019]\n.30\n5\n[\u2018proves\u2019, \u2018Feed\u2019, \u2018meaning\u2019, \u2018zzle\u2019, \u2018stripe\u2019]\n.31\n[\u2018multitude\u2019, \u2018psychologically\u2019, \u2018Taliban\u2019, \u2018Elf\u2019, \u2018Pakistan\u2019]\n.36\n175\n[\u2018util\u2019, \u2018elson\u2019, \u2018asser\u2019, \u2018seek\u2019, \u2018////////////////////\u2019]\n.26\n[\u2018ags\u2019, \u2018Git\u2019, \u2018mm\u2019, \u2018Morning\u2019, \u2018Cit\u2019]\n.33\n55\n[\u2018Judicial\u2019, \u2018wasting\u2019, \u2018oen\u2019, \u2018oplan\u2019, \u2018trade\u2019]\n.34\n[\u2018odd\u2019, \u2018alo\u2019, \u2018rophic\u2019, \u2018perv\u2019, \u2018pei\u2019]\n.34\n61\n[\u2018+++\u2019, \u2018DEP\u2019, \u2018enum\u2019, \u2018vernight\u2019, \u2018posted\u2019]\n.33\n[\u2018Newspaper\u2019, \u2018iii\u2019, \u2018INK\u2019, \u2018Graph\u2019, \u2018UT\u2019]\n.35\n103\n[\u2018Doc\u2019, \u2018Barth\u2019, \u2018details\u2019, \u2018DEF\u2019, \u2018buckets\u2019]\n.34\n[\u2018pleas\u2019, \u2018Eclipse\u2019, \u2018plots\u2019, \u2018cb\u2019, \u2018Menu\u2019]\n.36\n99\n[\u2018+++\u2019, \u2018Condition\u2019, \u2018Daytona\u2019, \u2018oir\u2019, \u2018research\u2019]\n.35\n[\u2018Salary\u2019, \u2018card\u2019, \u2018mobile\u2019, \u2018Cour\u2019, \u2018Hawth\u2019]\n.35\n155\n[\u2018Named\u2019, \u2018910\u2019, \u2018collar\u2019, \u2018Lars\u2019, \u2018Cats\u2019]\n.33\n[\u2018Champ\u2019, \u2018falsely\u2019, \u2018atism\u2019, \u2018styles\u2019, \u2018Champ\u2019]\n.30\n145\n[\u2018cer\u2019, \u2018args\u2019, \u2018olis\u2019, \u2018te\u2019, \u2018atin\u2019]\n.30\n[\u2018Chuck\u2019, \u2018goose\u2019, \u2018anthem\u2019, \u2018wise\u2019, \u2018fare\u2019]\n.33\n189\n[\u2018MOD\u2019, \u2018Pres\u2019, \u2018News\u2019, \u2018Early\u2019, \u2018Herz\u2019]\n.33\n[\u2018Organ\u2019, \u2018CES\u2019, \u2018POL\u2019, \u2018201\u2019, \u2018Stan\u2019]\n.31\n49\n[\u2018Pir\u2019, \u2018Pir\u2019, \u2018uum\u2019, \u2018akable\u2019, \u2018Prairie\u2019]\n.30\n[\u2018flame\u2019, \u2018roc\u2019, \u2018module\u2019, \u2018swaps\u2019, \u2018Faction\u2019]\n.33\n20\n[\u2018ear\u2019, \u2018feed\u2019, \u2018attire\u2019, \u2018demise\u2019, \u2018peg\u2019]\n.33\n[\u2018Chart\u2019, \u2018iw\u2019, \u2018Kirst\u2019, \u2018PATH\u2019, \u2018rhy\u2019]\n.36\n110\n[\u2018+++\u2019, \u2018Bee\u2019, \u2018limits\u2019, \u2018Fore\u2019, \u2018seeking\u2019]\n.31\n[\u2018imped\u2019, \u2018iola\u2019, \u2018Prince\u2019, \u2018inel\u2019, \u2018law\u2019]\n.33\n6\n[\u2018SIGN\u2019, \u2018Kob\u2019, \u2018Ship\u2019, \u2018Near\u2019, \u2018buzz\u2019]\n.36\n[\u2018Tower\u2019, \u2018767\u2019, \u2018Kok\u2019, \u2018Tele\u2019, \u2018Arbit\u2019]\n.33\n46\n[\u2018childhood\u2019, \u2018death\u2019, \u2018ma\u2019, \u2018vision\u2019, \u2018Dire\u2019]\n.36\n[\u2018Fram\u2019, \u2018exper\u2019, \u2018Pain\u2019, \u2018ader\u2019, \u2018unprotected\u2019]\n.33\n113\n[\u2018Decl\u2019, \u2018Hide\u2019, \u2018Global\u2019, \u2018orig\u2019, \u2018meas\u2019]\n.32\n[\u2018usercontent\u2019, \u2018OTUS\u2019, \u2018Georgia\u2019, \u2018ech\u2019, \u2018GRE\u2019]\n.32\n32\n[\u2018ideas\u2019, \u2018GY\u2019, \u2018+++\u2019, \u2018Bake\u2019, \u2018Seed\u2019]\n.32\n[\u2018GGGGGGGG\u2019, \u2018dictators\u2019, \u2018david\u2019, \u2018ugh\u2019, \u2018BY\u2019]\n.31\n98\n[\u2018Near\u2019, \u2018Near\u2019, \u2018LIN\u2019, \u2018Bee\u2019, \u2018threat\u2019]\n.30\n[\u2018Lavrov\u2019, \u2018Debor\u2019, \u2018Hegel\u2019, \u2018Advertisement\u2019, \u2018iak\u2019]\n.34\n185\n[\u2018ceans\u2019, \u2018Stage\u2019, \u2018Dot\u2019, \u2018Price\u2019, \u2018Grid\u2019]\n.33\n[\u2018wholesale\u2019, \u2018Cellular\u2019, \u2018Magn\u2019, \u2018Ingredients\u2019, \u2018Magn\u2019]\n.32\n166\n[\u2018bys\u2019, \u2018767\u2019, \u2018+++\u2019, \u2018bottles\u2019, \u2018gif\u2019]\n.32\n[\u2018Bras\u2019, \u2018discipl\u2019, \u2018gp\u2019, \u2018AR\u2019, \u2018Toys\u2019]\n.33\n52\n[\u2018Kob\u2019, \u2018Site\u2019, \u2018reed\u2019, \u2018Wiley\u2019, \u2018\u02c6aL\u00b8\u2019]\n.29\n[\u2018THER\u2019, \u2018FAQ\u2019, \u2018ibility\u2019, \u2018ilities\u2019, \u2018twitter\u2019]\n.34\n90\n[\u2018cytok\u2019, \u2018attack\u2019, \u2018Plug\u2019, \u2018strategies\u2019, \u2018uddle\u2019]\n.32\n[\u2018Boots\u2019, \u2018Truman\u2019, \u2018CFR\u2019, \u2018\u02dca \u02c6H\u00a3\u2019, \u2018Shin\u2019]\n.33\n13\n[\u2018nard\u2019, \u2018Planetary\u2019, \u2018lawful\u2019, \u2018Court\u2019, \u2018eman\u2019]\n.33\n[\u2018Nebraska\u2019, \u2018tails\u2019, \u2018 \u02daA\u0141\u2019, \u2018DEC\u2019, \u2018Despair\u2019]\n.33\n47\n[\u2018pport\u2019, \u2018overnight\u2019, \u2018Doc\u2019, \u2018ierra\u2019, \u2018Unknown\u2019]\n.34\n[\u2018boiling\u2019, \u2018A\u2019, \u2018Ada\u2019, \u2018itude\u2019, \u2018flawed\u2019]\n.31\n19\n[\u2018mocking\u2019, \u2018chicks\u2019, \u2018GY\u2019, \u2018ear\u2019, \u2018done\u2019]\n.35\n[\u2018illet\u2019, \u2018severely\u2019, \u2018nton\u2019, \u2018arrest\u2019, \u2018Volunteers\u2019]\n.33\n112\n[\u2018avenue\u2019, \u2018gio\u2019, \u2018Parking\u2019, \u2018riages\u2019, \u2018Herald\u2019]\n.35\n[\u2018griev\u2019, \u2018Swanson\u2019, \u2018Guilty\u2019, \u2018Sent\u2019, \u2018Pac\u2019]\n.32\n133\n[\u2018\u02dca \u02c6H\u02d8I\u2019, \u2018itto\u2019, \u2018iation\u2019, \u2018asley\u2019, \u2018Included\u2019]\n.32\n[\u2018Purs\u2019, \u2018reproductive\u2019, \u2018sniper\u2019, \u2018instruct\u2019, \u2018Population\u2019]\n.33\n102\n[\u2018drawn\u2019, \u2018Super\u2019, \u2018gency\u2019, \u2018Type\u2019, \u2018blames\u2019]\n.33\n[\u2018metric\u2019, \u2018Young\u2019, \u2018princip\u2019, \u2018scal\u2019, \u2018Young\u2019]\n.31\n79\n[\u2018Vand\u2019, \u2018inement\u2019, \u2018straw\u2019, \u2018ridiculous\u2019, \u2018Chick\u2019]\n.34\n[\u2018Rez\u2019, \u2018song\u2019, \u2018LEGO\u2019, \u2018Login\u2019, \u2018pot\u2019]\n.37\n105\n[\u2018link\u2019, \u2018ede\u2019, \u2018Dunk\u2019, \u2018Pegasus\u2019, \u2018Mao\u2019]\n.32\n[\u2018visas\u2019, \u2018Mental\u2019, \u2018verbal\u2019, \u2018WOM\u2019, \u2018nda\u2019]\n.30\nAverage\n.33\n.33\nTable S.2. Image soft prompts are indistinguishable from random soft prompts via BERTScore. Each image is encoded as a sequence of\n196 soft prompts, corresponding to image patches, that serve as input to GPT-J. Here we randomly sample 35 patches for a single COCO\nimage and map them onto nearest-neighbor tokens in transformer embedding space. BERTScore is measured relative to COCO human\nannotations of the same image (we report the mean score over the 5 human captions). For comparison we sample random vectors in the\ntransformer embedding space and compute BERTScores using the same procedure.\nOriginal Image\nAverage Mask\nTop Individual Multimodal Neurons \nCOCO ID = 507042\nIoU=.17 L5.u4923\nIoU=.11 L8.u2210\nIoU = .10 L4.u2858\nIoU = .15 L5.u450             IoU=.18 L7.u5875           IoU=.13 L8.u2012\nIoU=.16\nCOCO ID = 438269\nIoU=.26 L7.u12134\nIoU=.26 L8.u11473\nIoU = .19 L7.u5875\nIoU = .24 L5.u4923           IoU=.23 L10.u6432         IoU=.10 L10.u6015\nIoU=.16\nCOCO ID = 181859\nIoU=.64 L12.u12884\nIoU=.60 L8.u2790\nIoU = .61 L7.u16297 \nIoU = .53 L7.u15772         IoU=.68 L7.u16297         IoU=.53 L5.u12136\nIoU=.50\nCOCO ID = 520910\nIoU=.16 L8.u16024\nIoU=.15 L5.u10004\nIoU = .09 L5.u12040\nIoU = .07 L7.u8417           IoU=.15 L7.u9458           IoU=.06 L7.u6918\nIoU=.14\nCOCO ID = 353518\nIoU=.13 L9.u6991\nIoU=.18 L7.u14218\nIoU = .09 L5.u15561\nIoU = .18 L7.u14218         IoU=.09 L7.u3030           IoU=.11 L7.u14008\nIoU=.09\nCOCO ID = 108864\nIoU=.68 L7.u3273\nIoU=.53\nL8.u6593\nIoU = .49\nL12.u12884\nIoU = .41 L14.u2019         IoU=.36 L6.u11916         IoU=.52 L9.u5970\nIoU=.32\nCOCO ID = 430973\nIoU=.51 L8.u13681\nIoU=.45 L6.u3132\nIoU = .56 L1.u6130\nIoU = .08 L6.u3132          IoU=.56 L11.u14710       IoU=.41 L9.u6991\nIoU=.14\nCOCO ID = 486479\nIoU=.40 L12.u12884\nIoU=.39 L10.u14325\nIoU = .19 L10.u11572\nIoU = .16\nL8.u4372           IoU=.13 L5.u4530           IoU=.34 L7.u16297\nIoU=.39\nFigure S.1. Multimodal neurons are selective for objects in images. For 8 example images sampled from the COCO categories described\nin Section 3.2 of the main paper, we show activation masks of individual multimodal neurons over the image, as well as mean activation\nmasks over all top multimodal neurons. We use IoU to compare these activation masks to COCO object annotations. IoU is calculated by\nupsampling each activation mask to the size of the original image (224) using bilinear interpolation, and thresholding activations in the\n0.95 percentile to produce a binary segmentation mask.\nFigure S.2. Multimodal neurons are selective for image categories. (a) For 10 ImageNet classes we construct the set of interpretable\nmultimodal neurons with the highest attribution scores on training images in that class, and calculate their activations on validation images.\nFor each class, we report the average activation value of top-scoring multimodal units relative to the maximum value of their average\nactivations on any class. Multimodal neurons are maximally active on classes where their attribution scores are highest. (b) Sample images\nand top-scoring units from two classes.\nS.5. Ablating Multimodal Neurons\nIn Section 3.3 of the main paper, we show that ablating\nmultimodal neurons causally effects the probability of out-\nputting the original token. To investigate the effect of re-\nmoving multimodal neurons on model output, we ablate the\ntop k units by attribution score for an image, where k \u2208\n{0, 50, 100, 200, 400, 800, 1600, 3200, 6400}, and compute\nthe BERTScore between the model\u2019s original caption and\nthe newly-generated zero-temperature caption. Whether we\nremove the top k units by attribution score, or only those\nthat are interpretable, we observe a strong decrease in cap-\ntion similarity. Table S.3 shows examples of the effect of\nablating top neurons on randomly sampled COCO valida-\ntion images, compared to the effect of ablating random neu-\nrons. Figure S.3 shows the average BERTScore after ablat-\ning k units across all COCO validation images.\nFigure S.3. BERTScores of generated captions decrease when\nmultimodal neurons are ablated, compared to the ablation of ran-\ndom neurons from the same layers.\nS.6. Distribution of Multimodal Neurons\nWe perform a simple analysis of the distribution of multi-\nmodal neurons by layer. Specifically, we extract the top 100\nscoring neurons for all COCO validation images. Most of\nthese neurons are found between layers 5 and 10 of GPT-J\n(L = 28), suggesting translation of semantic content be-\ntween modalities occurs in earlier transformer layers.\nFigure S.4. Unique multimodal neurons per layer chosen using the\ntop 100 attribution scores for each COCO validation image. Inter-\npretable units are those for which at least 7 of the top 10 logits are\nwords in the English dictionary containing \u2265 3 letters.\nCaptions after ablation\nImg. ID\n# Abl.\nAll multimodal\nBSc.\nInterpretable multimodal\nBSc.\nRandom neurons\nBSc.\n219578\n0\na dog with a cat\n1.0\na dog with a cat\n1.0\na dog with a cat\n1.0\n50\na dog and a cat\n.83\na dog and a cat\n.83\na dog with a cat\n1.0\n100\na lion and a zebra\n.71\na dog and cat\n.80\na dog with a cat\n1.0\n200\na dog and a cat\n.83\na dog and a cat\n.83\na dog with a cat\n1.0\n400\na lion and a lioness\n.64\na dog and a cat\n.83\na dog with a cat\n1.0\n800\na tiger and a tiger\n.63\na lion and a zebra\n.71\na dog with a cat\n1.0\n1600\na tiger and a tiger\n.63\na lion and a zebra\n.71\na dog with a cat\n1.0\n3200\na tiger\n.67\na tiger and a tiger\n.63\na dog with a cat\n1.0\n6400\na tiger\n.67\na tiger in the jungle\n.60\na dog with a cat\n1.0\n131431\n0\nthe facade of the cathedral\n1.0\nthe facade of the cathedral\n1.0\nthe facade of the cathedral\n1.0\n50\nthe facade of the church\n.93\nthe facade of the cathedral\n1.0\nthe facade of the cathedral\n1.0\n100\nthe facade of the church\n.93\nthe facade of the cathedral\n1.0\nthe facade of the cathedral\n1.0\n200\nthe facade\n.75\nthe facade\n.75\nthe facade of the cathedral\n1.0\n400\nthe exterior of the church\n.80\nthe facade\n.75\nthe facade of the cathedral\n1.0\n800\nthe exterior of the church\n.80\nthe dome\n.65\nthe facade of the cathedral\n1.0\n1600\nthe dome\n.65\nthe dome\n.65\nthe facade of the cathedral\n1.0\n3200\nthe dome\n.65\nthe dome\n.65\nthe facade of the cathedral\n1.0\n6400\nthe exterior\n.61\nthe dome\n.65\nthe facade\n.75\n180878\n0\na cake with a message\na cake with a message\na cake with a message\nwritten on it.\n1.0\nwritten on it.\n1.0\nwritten on it.\n1.0\n50\na cake with a message\na cake with a message\na cake with a message\nwritten on it.\n1.0\nwritten on it.\n1.0\nwritten on it.\n1.0\n100\na cake with a message\na cake for a friend\u2019s birthday.\n.59\na cake with a message\nwritten on it.\n1.0\nwritten on it.\n1.0\n200\na cake with a message\na cake for a friend\u2019s birthday.\n.59\na cake with a message\nwritten on it.\n1.0\nwritten on it.\n1.0\n400\na cake with a message\na cake for a friend\u2019s birthday.\n.59\na cake with a message\nwritten on it.\n1.0\nwritten on it.\n1.0\n800\na cake\n.59\na cake for a birthday party\n.56\na cake with a message\nwritten on it.\n1.0\n1600\na cake\n.59\na poster for the film.\n.49\na cake with a message\nwritten on it.\n1.0\n3200\na man who is a fan of\na typewriter\n.44\na cake with a message\nfootball\n.42\nwritten on it.\n1.0\n6400\nthe day\n.34\na typewriter\n.44\na cake with a message\nwritten on it.\n1.0\n128675\n0\na man surfing on a wave\n1.0\na man surfing on a wave\n1.0\na man surfing on a wave\n1.0\n50\na man in a kayak on a lake\n.74\na man surfing on a wave\n1.0\na man surfing on a wave\n1.0\n100\na man in a kayak on a lake\n.74\na man surfing on a wave\n1.0\na man surfing on a wave\n1.0\n200\na man in a kayak on a lake\n.74\na man surfing a wave\n.94\na man surfing on a wave\n1.0\n400\na man in a kayak on a lake\n.74\na man surfing a wave\n.94\na man surfing on a wave\n1.0\n800\na man in a kayak\n.64\na surfer riding a wave\n.84\na man surfing on a wave\n1.0\n1600\na girl in a red dress\na surfer riding a wave\n.84\na man surfing on a wave\n1.0\nwalking on the beach\n.66\n3200\na girl in a red dress\n.53\na girl in a red dress\n.53\na man surfing on a wave\n1.0\n6400\na girl in the water\n.62\na girl in a dress\n.59\na man surfing on a wave\n1.0\nImg. ID\n# Abl.\nAll multimodal\nBSc.\nInterpretable multimodal\nBSc.\nRandom neurons\nBSc.\n289960\n0\na man standing on a rock\na man standing on a rock\na man standing on a rock\nin the sea\n1.0\nin the sea\n1.0\nin the sea\n1.0\n50\na man standing on a rock\na man standing on a rock\na man standing on a rock\nin the sea\n1.0\nin the sea\n1.0\nin the sea\n1.0\n100\na man standing on a rock\na man standing on a rock\na man standing on a rock\nin the sea\n1.0\nin the sea.\n.94\nin the sea\n1.0\n200\na kite soaring above the waves\n.62\na man standing on a rock\na man standing on a rock\nin the sea\n1.0\nin the sea\n1.0\n400\na kite soaring above the waves\n.62\na kite surfer on the beach.\n.62\na man standing on a rock\nin the sea\n1.0\n800\na kite soaring above the waves\n.62\na bird on a wire\n.63\na man standing on a rock\nin the sea\n1.0\n1600\na kite soaring above the clouds\n.65\na kite surfer on the beach\n.63\na man standing on a rock\nin the sea\n1.0\n3200\na kite soaring above the sea\n.69\na bird on a wire\n.63\na man standing on a rock\nin the sea\n1.0\n6400\na helicopter flying over the sea\n.69\na bird on a wire\n.63\na man standing on a rock\nin the sea\n1.0\n131431\n0\nthe bridge at night\n1.0\nthe bridge at night\n1.0\nthe bridge at night\n1.0\n50\nthe bridge\n.70\nthe street at night\n.82\nthe bridge at night\n1.0\n100\nthe bridge\n.70\nthe street at night\n.82\nthe bridge at night\n1.0\n200\nthe bridge\n.70\nthe street at night\n.82\nthe bridge at night\n1.0\n400\nthe bridge\n.70\nthe street\n.55\nthe bridge at night\n1.0\n800\nthe bridge\n.70\nthe street\n.55\nthe bridge at night\n1.0\n1600\nthe bridge\n.70\nthe street\n.55\nthe bridge at night\n1.0\n3200\nthe night\n.61\nthe street\n.55\nthe bridge at night\n1.0\n6400\nthe night\n.61\nthe street\n.55\nthe bridge at night\n1.0\n559842\n0\nthe team during the match.\n1.0\nthe team during the match.\n1.0\nthe team during the match.\n1.0\n50\nthe team.\n.70\nthe team.\n.70\nthe team during the match.\n1.0\n100\nthe team.\n.70\nthe team.\n.70\nthe team during the match.\n1.0\n200\nthe team.\n.70\nthe team.\n.70\nthe team during the match.\n1.0\n400\nthe group of people\n.52\nthe team.\n.70\nthe team during the match.\n1.0\n800\nthe group\n.54\nthe team.\n.70\nthe team during the match.\n1.0\n1600\nthe group\n.54\nthe team.\n.70\nthe team during the match.\n1.0\n3200\nthe group\n.54\nthe team.\n.70\nthe team during the match\n1.0\n6400\nthe kids\n.46\nthe team.\n.70\nthe team during the match.\n1.0\n47819\n0\na man and his horse.\n1.0\na man and his horse.\n1.0\na man and his horse.\n1.0\n50\na man and his horse.\n1.0\na man and his horse.\n1.0\na man and his horse.\n1.0\n100\nthe soldiers on the road\n.47\na man and his horse.\n1.0\na man and his horse.\n1.0\n200\nthe soldiers on the road\n.47\nthe soldiers on the road\n.47\na man and his horse.\n1.0\n400\nthe soldiers\n.46\nthe soldiers\n.46\na man and his horse.\n1.0\n800\nthe soldiers\n.46\nthe soldiers\n.46\na man and his horse.\n1.0\n1600\nthe soldiers\n.46\nthe soldiers\n.46\na man and his horse.\n1.0\n3200\nthe soldiers\n.46\nthe soldiers\n.46\na man and his horse.\n1.0\n6400\nthe soldiers\n.46\nthe soldiers\n.46\na man and his horse.\n1.0\nTable S.3. Captions and BERTScores (relative to original GPT caption) after incremental ablation of multimodal MLP neurons. All multi-\nmodal neurons are detected, decoded, and filtered to produce a list of \u201cinterpretable\u201d multimodal neurons using the procedure described in\nSection 2 of the main paper. Random neurons are sampled from the same layers as multimodal neurons for ablation. Images are randomly\nsampled from the COCO validation set. Captions are generated with temperature = 0.\n"
  },
  {
    "title": "HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions",
    "link": "https://arxiv.org/pdf/2308.01477.pdf",
    "upvote": "11",
    "text": "HANDAL: A Dataset of Real-World Manipulable Object Categories\nwith Pose Annotations, Affordances, and Reconstructions\nAndrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, Stan Birchfield\nNVIDIA\nFig. 1: Gallery of our HANDAL dataset which covers a variety of object categories that are friendly for robotic manipulation and functional grasping; it\nincludes both static and dynamic scenes. Each subfigure shows precise ground-truth annotations of an image: 2D bounding box (orange), pixel-wise object\nsegmentation (purple) obtained by projecting the reconstructed 3D model, pixel-wise projection of 3D handle affordance segmentation (green), and 6-DoF\ncategory-level pose+scale. Image contrast is adjusted for visualization purposes.\nAbstract\u2014 We present the HANDAL dataset for category-\nlevel object pose estimation and affordance prediction. Unlike\nprevious datasets, ours is focused on robotics-ready manipulable\nobjects that are of the proper size and shape for functional\ngrasping by robot manipulators, such as pliers, utensils, and\nscrewdrivers. Our annotation process is streamlined, requiring\nonly a single off-the-shelf camera and semi-automated pro-\ncessing, allowing us to produce high-quality 3D annotations\nwithout crowd-sourcing. The dataset consists of 308k annotated\nimage frames from 2.2k videos of 212 real-world objects in\n17 categories. We focus on hardware and kitchen tool objects\nto facilitate research in practical scenarios in which a robot\nmanipulator needs to interact with the environment beyond\nsimple pushing or indiscriminate grasping. We outline the\nusefulness of our dataset for 6-DoF category-level pose+scale\nestimation and related tasks. We also provide 3D reconstructed\nmeshes of all objects, and we outline some of the bottlenecks to\nbe addressed for democratizing the collection of datasets like\nthis one. Project website: https://nvlabs.github.io/HANDAL/\nI. INTRODUCTION\nIf robots are to move beyond simple pushing and indis-\ncriminate top-down grasping, they must be equipped with\ndetailed awareness of their 3D surroundings [1]. To this\nend, high-quality 3D datasets tailored to robotics are needed\nfor training and testing networks. Compared to the many\nlarge-scale 2D computer vision datasets (e.g., ImageNet [2],\nCOCO [3], and OpenImages [4]), existing 3D robotics\ndatasets tend to be rather small in size and scope.\nThe primary challenge in 3D is the annotation of real\nimages, which requires either depth or multiple images,\nthus making it difficult to scale up annotation processes.\nWhile the datasets of the BOP challenge [5] have filled this\ngap for instance-level object pose estimation, the challenge\nstill remains for category-level object pose estimation, as\nwell as for learning functional affordances\u2014such as object\narXiv:2308.01477v1  [cs.RO]  2 Aug 2023\nhandles\u2014for task-oriented grasping and manipulation [6]\u2013\n[10].\nTo address these problems, we introduce the HANDAL\ndataset.1 Using an off-the-shelf camera to collect data, and\na semi-automated pipeline for annotating the data in 3D,\nwe have created a rather large labeled dataset without much\nhuman effort (once the pipeline was set up). Fig. 1 shows\nexamples of our image frames with precise 3D annotations\nincluding segmentation, affordances (handles), 6-DoF poses,\nand bounding boxes. We see this dataset as an important step\ntoward democratizing the creation of labeled 3D datasets.\nTo this end, we do not use any specialized hardware for\ncollection, nor any crowdsourcing for annotation. Our dataset\nhas the following properties:\n\u2022 Short video clips of object instances from 17 categories\nthat are well-suited for robotic functional grasping and\nmanipulation, with 6-DoF object pose+scale annotations\nfor all image frames. Multiple captures of each instance\nensure diversity of backgrounds and lighting.\n\u2022 3D reconstructions of all objects, along with task-inspired\naffordance annotations.\n\u2022 Additional videos of the objects being handled by a human\nto facilitate task-oriented analysis in dynamic scenarios.\nThese dynamic videos are also equipped with accurate\nground-truth annotations.\nAt the end of the paper we include a discussion of remaining\nbottlenecks for the process of creating a dataset like ours.\nII. PREVIOUS WORK\nLarge-object datasets. Precursors to the present work\ninclude datasets for autonomous driving, such as KITTI 3D\nobject detection [11] and nuScenes [12], where the objects to\nbe detected are vehicles, cyclists, and pedestrians. Similarly,\ndatasets like PASCAL3D+ [13] and SUN RGB-D [14] enable\n3D object detection of tables, chairs, couches, and other large\nindoor items. In both cases the objects are much too large for\nrobotic manipulation, and their upright orientation generally\nprecludes the need for full 6-DoF pose estimation.\nCategory-level object pose datasets. The first dataset\nof real images for small, manipulable objects was NOCS-\nREAL275 [15], which consists of a small number of anno-\ntated RGBD videos of six categories: bottle, bowl, camera,\ncan, laptop and mug. Building on this work, Wild6D [16]\nexpands the number of videos and images, but with similar\ncategories: bottle, bowl, camera, laptop, and mug.\nMore recent datasets [17], [18] address the same problem\nas ours, but on a smaller scale. PhoCaL [17] contains RGBD\nvideos of eight categories: bottle, box, can, cup, remote,\nteapot, cutlery, and glassware. HouseCat6D [18] contains\nRGBD videos of a similar set of ten categories: bottle,\nbox, can, cup, remote, teapot, cutlery, glass, shoe, and tube.\nSimilar to our work, the objects in these categories are\npotentially manipulable by a robot.\nObjectron [19] scales up the number of object instances\nand videos considerably, but most of the categories are\n1\u201cHousehold ANnotated Dataset for functionAL robotic grasping and\nmanipulation\u201d. The name also fits because all of our objects have handles.\nnot manipulable. This dataset is therefore more applicable\nto computer vision than to robotics. Similarly, CO3D [20]\ncontains a large number of videos of a large number of\ncategories, and most of these are not manipulable. CO3D is\ndifferent, however, in that its focus is on category-specific\n3D reconstruction and novel-view synthesis, rather than\ncategory-level object pose estimation. As a result, this dataset\nomits some key qualities that are required for the latter, such\nas absolute scale and object pose with respect to a canonical\ncoordinate system.\nIn contrast to these works, our focus is on manipulable\nobjects, particularly those well-suited for robotic functional\ngrasping. As such, we collect data of relatively small objects\nwith handles, and we annotate their affordances as well as\n6-DoF pose+scale. See Table I for a comparison between our\n3D dataset and related works.\nCategory-level object pose estimation. Category-level\nobject pose estimation is a relatively recent problem that has\nbeen gaining attention lately. A natural extension of instance-\nlevel object pose estimation [5], [21]\u2013[25], category-level\npose estimation [15], [26]\u2013[29] addresses broad classes of\nobjects instead of known sets of object instances, and is\ntherefore potentially more useful for robotics applications.\nThe category-level datasets mentioned above have enabled\nrecent research in this important area.\nIn their seminal work, Wang et al. [15] introduced the\nnormalized object coordinate system (NOCS), which was\nthen used to perform 3D detection and category-level object\npose estimation from a single RGBD image. This method\nwas evaluated on the NOCS-REAL275 dataset. In follow-up\nwork, CPS [30] also evaluates on REAL275, but this method\nonly requires an RGB image at inference time. The method\nwas trained in a self-supervised manner using unlabeled\nRGBD images, and it also infers the shape of the instance\nas a low-dimensional representation. The method of Dual-\nPoseNet [31] uses a pair of decoders, along with spherical fu-\nsion, to solve this problem using a single RGBD image. The\nmethod was also evaluated on NOCS-REAL275. More recent\napproaches of SSP-Pose [32], CenterSnap [33], iCaps [34],\nand ShAPO [35] all evaluate on NOCS-REAL275.\nMobilePose [36] aims to recover object pose from a single\nRGB image, evaluating on the Objectron dataset. Center-\nPose [37] and its tracking extension, CenterPoseTrack [38],\naddress the same problem, using a combination of keypoints\nand heatmaps, connected by a convGRU module. These two\nmethods were also evaluated on Objectron.\nTo our knowledge, previous methods have been evaluated\non only two datasets: REAL275 and Objectron. We aim\nto extend this line of research by providing the largest\ndataset yet focused on categories that are amenable to robotic\nmanipulation, along with annotations that facilitate functional\ngrasping.\nIII. DATASET OVERVIEW\nOur goal with this dataset is to support research in per-\nception that will enable robot manipulators to perform real-\nworld tasks beyond simple pushing and pick-and-place. As\nTABLE I: Comparison of category-level object datasets. See Section III-B for details. Only the annotated images of Wild6D and our dataset are considered\nhere. \u2020PhoCaL and HouseCat6D also use a polarization camera. \u2021CO3D has an additional 5 categories that can be indiscriminantly grasped but that do\nnot afford functional grasping. \u2217Our dataset also includes dynamic videos from an RGBD camera.\ndataset\nmodality\ncat.\nmanip.\nobj.\nvid.\nimg.\npose\nscale\n3D recon.\nstat.\ndyn.\n360\u25e6\nocc.\nafford.\nNOCS-REAL275 [15]\nRGBD\n6\n4\n42\n18\n8k\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\nPhoCaL [17]\nRGBD\u2020\n8\n8\n60\n24\n3k\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\nHouseCat6D [18]\nRGBD\u2020\n10\n10\n194\n41\n24k\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\nWild6D [16]\nRGBD\n5\n3\n162\n486\n10k\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nObjectron [19]\nRGB\n9\n4\n17k\n14k\n4M\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nCO3D [20]\nRGB\n50\n7\u2021\n19k\n19k\n1.5M\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\nHANDAL (Ours)\nRGB\u2217\n17\n17\n212\n2k\n308k\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\na result, we were motivated to select object categories with\nfunctional purposes. In particular, we focused on categories\nwith a handle to facilitate functional grasps.\nA. Object categories\nWe have selected 17 categories of graspable, functional\nobjects: claw hammer, fixed-joint pliers, slip-joint pliers,\nlocking pliers, power drill, ratchet, screwdriver, adjustable\nwrench, combination wrench, ladle, measuring cup, mug,\npot/pan, spatula, strainer, utensil, and whisk. At a high level,\nthese categories generally fall into one of two larger super-\ncategories: objects that might be found in a toolbox (e.g.,\nhammers) and objects that might be found in a kitchen (e.g.,\nspatulas).\nBecause all the objects were designed for human handling,\nthey are of an appropriate size for robot grasping and\nmanipulation, with two caveats. First, some objects may be\ntoo heavy for existing robot manipulators; we envision that\n3D printed replicas or plastic versions could be substituted\nfor actual manipulation experiments. Secondly, since all our\nobjects support functional grasping, that is, grasping for a\nparticular use\u2014as opposed to indiscriminate grasping\u2014some\nobjects may require anthropomorphic robotic hands rather\nthan parallel-jaw grippers. We expect that our dataset will\nopen avenues for further research in this area.\nThe objects are composed of a variety of materials and\ngeometries, including some with reflective surfaces and some\nwith perforated or thin surfaces. The many challenges intro-\nduced by such properties should make our dataset useful\nfor furthering perception research to deal with real-world\nscenarios.\nSome of the objects in our dataset allow for articulation,\nbut we treat the articulated objects as if they were rigid by\nchoosing a default state for each instance before capture.\nNevertheless, we believe that our annotations set the stage\nfor future research in this important area.\nB. Comparison with other datasets\nTo put this work in context, our dataset is compared with\nexisting category-level object datasets in Tab. I. From left\nto right, the table captures the input modality, number of\ncategories, number of manipulable categories, number of ob-\nject instances, number of videos, number of images/frames,\nwhether object pose is recorded, whether absolute scale is\navailable, whether 3D reconstructions are provided, whether\nthe videos include static (object not moving) and/or dynamic\n(object moving) scenes, whether videos capture 360\u25e6 views\nof the objects, whether objects are partially occluded in some\nviews/scenes, and whether object affordances are annotated.\nOur dataset is unique because it contains instance- as\nwell as category-level pose annotations, since we captured\nmultiple videos per instance. As a result, our data can also\nbe used to train an instance-level pose estimator that operates\nwith respect to a specific 3D object model. We also have the\nability to reconstruct 3D models of the objects and align\nthem across videos, which yields pose annotations that are\nmuch more precise than is possible with manual annotation.\nDetermining whether an object is manipulable is some-\nwhat subjective. For the table above, we do not include cam-\neras or laptops (from NOCS-REAL275 [15], Wild6D [16],\nand Objectron [19]) because they are fragile and costly to\nreplace, nor do we include bikes or chairs (Objectron [19])\nbecause they are too large. We also do not include books\n(from Objectron [19] and CO3D [20]) because their non-\nrigidity makes them nearly impossible to grasp with a single\nrobotic hand. Moreover, we do not include apples, carrots,\noranges, bananas, and balls (from CO3D [20]) because they\ndo not support functional grasping; although they could be\nincluded if only pick-and-place were considered.\nNote that NOCS-REAL275 [15], PhoCaL [17], and\nHouseCat6D [18] have a separate scanning process for 3D\nreconstruction, whereas our dataset and CO3D [20] obtain\nreconstructions from the captures. Although CO3D [20]\nincludes camera poses with respect to each object instance\nin a single video, there is no correspondence across videos.\nFor this reason, the poses in CO3D are insufficient to train\na category-level object pose estimator.\nIV. METHOD\nIn this section we describe the methodology we used for\ncollecting and annotating the dataset.\nA. Data collection\nFor each category, we acquired at least twelve different\nobject instances, i.e., different brands or sizes, for a total\nof 212 objects across 17 categories. For each object, ap-\nproximately 10 static scene scans were captured in different\nlighting conditions, backgrounds, and with different distrac-\ntor objects. Sample images are shown in Fig. 2.\nFor some objects, an additional reference scan (with the\nobject carefully placed to avoid occlusion and self-occlusion)\nwas captured in order to generate a canonical mesh for\nFig. 2: One hundred of the object instances collected in our dataset. The train/test split is shown on the top-left of each thumbnail.\nregistration in the data annotation step. For objects where a\nreference scan was not captured, we selected the scene mesh\nwith the best reconstructed geometry to be the canonical\nmesh. The final mesh of each object was reconstructed using\nall image frames from all scenes.\nVideos were captured at HD resolution using rear-facing\nmobile device cameras while simultaneously estimating\nrough camera poses by sensor fusion using the built-in\nARKit2 or ARCore3 libraries. Before any other processing,\nwe first selected the sharpest image from each set of k\n2https://developer.apple.com/documentation/arkit\n3https://developers.google.com/ar\nconsecutive frames of the video, with k = \u230ancaptured/ndesired\u230b,\nwhere ncaptured \u2248 1000, and ndesired = 120. This step used the\nvariance of the Laplacian of Gaussian to measure sharpness.\nWe also captured 51 dynamic scenes with the front-facing\nTrueDepth camera of the iPad Pro, recording at 640 \u00d7 480.\nOne video was captured per object, using the subset of\nobjects that we found to be amenable to depth sensing. These\ncaptures were divided into two parts. First, the object was\nrotated in front of the static camera so as to view it from as\nmany angles as possible to create a quality 3D reconstruction\nusing BundleSDF [39]. Second, the object was grasped by\na human in a functional manner (e.g., grabbing the handle)\nFig. 3: Reconstructed meshes with the ground truth affordance annotations\nshowing the handles (green).\nand moved along a functional path in slow motion (e.g.,\npretending to strike a nail with the hammer). By manipulat-\ning objects in a natural way like this, we provide somewhat\nrealistic scenarios for object pose estimation correlated with\nobject affordances. Note that these dynamic captures together\nwith accurate annotations are unique to our dataset, as all\nprevious datasets in this space are restricted to static scenes.\nB. Data annotation of static scenes\nAlthough ARKit/ARCore provide camera pose estimates\nfor every frame, we found these to be too noisy for 3D\nreconstruction and annotation.4 As a result, we instead used\nCOLMAP [40] to estimate unscaled camera poses from\nthe images alone. We ran COLMAP on the unmodified\nimages (without cropping or segmenting) to take advantage\nof background texture. Then we ran XMem [41] to segment\nthe foreground object from the background. Although this\nstep required a small amount of manual intervention (usually\none or two mouse clicks in the first frame), it was otherwise\nautomatic. We then ran Instant NGP [42] on the segmented\nimages to create a neural 3D representation of the object,\nfrom which we extracted a mesh using Marching Cubes. This\nprocess was repeated for every video/scene.\nBecause the camera poses generated by COLMAP are\nwithout absolute scale or absolute orientation, we used the\ncaptured ARKit/ARCore poses to rescale the COLMAP\nposes (multiplying by a single scalar) and to align the vertical\norientation with respect to gravity. The resulting transform\nwas also applied to the 3D reconstructed meshes.\nFor each category, a canonical coordinate frame conven-\ntion was established to ensure consistency across instances.\nAs a general rule, the axis of symmetry was set as the x axis\nfor categories with rotational symmetry (e.g., screwdrivers)\nand the plane of symmetry was set as the x-y plane for the\nremaining categories. We aligned all categories such that the\n4Specifically, the median difference between AR-provided camera poses\nand COLMAP-generated poses was 1.8 cm and 11.4 degrees after aligning\nthe transform clouds and accounting for scale. These errors were large\nenough to prevent our Instant-NGP process from converging.\n\u201cfront\u201d of the object is +x and the \u201ctop\u201d of the object is\n+y. For example, hammers were oriented so that the handle\nis along the x axis, with the head of the hammer at +x; the\nhead itself extends along the y axis, with the face being +y\nand the claw \u2212y.\nAfter establishing the coordinate frame of the canonical\nreference mesh for an object instance, we computed the\ntransform between this reference mesh and each of the\nmeshes for the same object from different scenes. Due to\nnoise in the captures that can obscure small details, we were\nnot able to find a tool capable of automatically, reliably, and\nefficiently calculating this transform. Instead, we found it\nmore practical to align the videos using a semi-automatic\ntool that used the extent of the oriented bounding box of the\nobject as an initial alignment, from which the final alignment\nwas obtained interactively. In our experience, it takes no\nmore than a minute to do this alignment, and the alignment\nis only required once per scene.\nUsing\nthe\ntransform\nfrom\nthe\nobject\u2019s\nCOLMAP-\ncoordinate pose to the canonical reference pose, together\nwith the camera poses generated by COLMAP, the pose\nof the object relative to the camera was computed in every\nframe. Occasionally (a few frames in some of the scenes),\nthe pose was incorrect due to errors in the camera poses.\nIn order to ensure pose quality across the entire dataset, we\nreprojected the mesh using the computed poses onto the input\nimages and manually removed any frames where the overlap\nof the reprojection was poor.\nC. Data annotation of dynamic scenes\nTo obtain the ground-truth object segmentation mask for\ndynamic scenes, we leveraged XMem [41] (as with static\nscenes), followed by manual verification and correction.\nUnlike the static scenes where ground-truth object pose can\nbe trivially inferred from camera pose localization, deter-\nmining object poses in dynamic scenes is extremely chal-\nlenging. To solve this problem, we applied BundleSDF [39]\nto these dynamic videos to simultaneously reconstruct the\ngeometry of the object, as well as to track the 6-DoF pose\nof the object throughout the image frames. Compared to\nBundleTrack [29], the additional 3D reconstruction from\nBundleSDF allows to register to the model created from\nthe static scene so as to unify the pose annotations. To\nassess the BundleSDF result, we randomly added noise to\nthe output pose in each frame, then applied ICP to align the\nmesh with the depth image. We then manually inspected all\nframes to ensure high quality. For most frames, the output of\nBundleSDF and ICP are nearly identical, and it is difficult\nto assess which of the two is more accurate. Therefore, we\nobtain ground truth by averaging them.\nBecause BundleSDF tracks throughout the entire video, it\nallows for the entire object to be reconstructed\u2014including\nthe underside\u2014which is typically an unsolved problem for\nstatic table-top scanning processes. Finally we repeated the\nsame global registration step (as in the static scenes) between\nthe obtained mesh to the canonical mesh of the same object\ninstance, in order to compute the transform to the canon-\nFig. 4: Challenging images with extreme lighting conditions, glare, shadows, occlusion, reflective surfaces, and perforated objects. Shown are the original\nimage (top) and object overlay (bottom).\nFig. 5: Qualitative results of 6-DoF object pose on dynamic pliers sequence (left to right), where a human operator first rotates the object for full\nreconstruction and then demonstrates the functional affordance. Red box visualizes the raw output by BundleSDF [39] and green box visualizes our refined\nground-truth.\nical reference frame. From this process, we obtained the\ncategory-level object pose w.r.t. the camera in every frame\nof the video.\nD. Annotating affordances\nFor affordances, we manually labeled handle regions in\nthe 3D mesh from each object reconstruction. Fig. 3 shows\nsome 3D object reconstructions with the handles labeled. Our\nannotations could be extended to include other affordance-\nbased segmentation or keypoint labels that capture the func-\ntional use of the objects.\nV. RESULTS\nA. Dataset statistics\nFor each object instance we have a 3D reference mesh,\nabout 10 static RGB captures both with and without clutter,\nobject segmentation masks, 6-DoF pose in all frames with\nrespect to a canonical coordinate frame for the category, and\naffordance annotations. Some objects also have a dynamic\nRGBD video. All annotations are stored in standard formats:\nCOCO [3] 2D bounding boxes/instance masks and BOP [5]\n6-DoF poses.\nOur capture protocol was designed to record 360\u25e6 around\neach object. Such surrounding captures are not common\namong datasets, but are important for quality 3D reconstruc-\ntions, as well as for view diversity.\nWe ensured that our object selection and captures included\nextreme lighting conditions, glare, moving shadows, shiny\nFig. 6: Category-level object detection and 6-DoF pose estimation results\nfrom CenterPose [37] (red) on test images containing unseen object in-\nstances, compared with ground-truth (green).\nobjects, and heavily perforated objects. See Fig. 4. Within\neach category, we also purposely chose a diverse range\nof individual objects. While this makes it challenging for\ncategorical pose estimators and object detectors to perform\nwell on our dataset, we expect our dataset to motivate further\nresearch in handling these real-world challenges. Tab. II\npresents the dataset statistics, along with quantitative results\nof 2D detection and 6-DoF pose estimation, described next.\nTABLE II: Our dataset has 17 object categories. Statistics (left-to-right): the number of object instances, videos, and annotated image frames. (The latter\nwould increase by about 10\u00d7 if non-annotated frames were included.) 2D metrics: average precision (AP) of bounding box detection, and AP of pixel-wise\nsegmentation. 6-DoF pose metrics: percentage of frames with 3D intersection over union (IoU) above 50%; area under the curve (AUC) of percentage of\ncuboid vertices whose average distance (ADD) is below a threshold ranging from 0 to 10 cm; symmetric version of ADD; and percentage of correct 2D\nkeypoints (PCK) within \u03b1\u221an pixels, where n is the number of pixels occupied by the object in the image, and \u03b1 = 0.2. For 3D IoU and AUC of ADD\n/ ADD-S, the first number is from RGB only, the second number is from shifting/scaling the final result based on the single ground truth depth value.\nDataset Statistics\n2D Detection Metrics\n6-DoF Pose Metrics\nCategory\nInstances\nVideos\nFrames\nBBox AP\nSeg. AP\n3D IoU\nADD\nADD-S\nPCK\nHammer\n19\n195\n24.0k\n0.757\n0.624\n0.552 / 0.812\n0.568 / 0.710\n0.581 / 0.721\n0.772\nPliers-Fixed Joint\n12\n125\n22.9k\n0.481\n0.375\n0.669 / 0.988\n0.824 / 0.915\n0.834 / 0.920\n0.973\nPliers-Slip Joint\n12\n128\n17.8k\n0.415\n0.271\n0.094 / 0.368\n0.311 / 0.470\n0.355 / 0.512\n0.715\nPliers-Locking\n12\n130\n18.0k\n0.767\n0.674\n0.146 / 0.430\n0.344 / 0.481\n0.385 / 0.541\n0.717\nPower Drill\n12\n131\n18.4k\n0.736\n0.754\n0.590 / 0.830\n0.391 / 0.613\n0.404 / 0.620\n0.732\nRatchet\n12\n123\n15.7k\n0.668\n0.439\n0.218 / 0.588\n0.370 / 0.577\n0.409 / 0.625\n0.555\nScrewdriver\n12\n140\n20.9k\n0.596\n0.557\n0.196 / 0.649\n0.378 / 0.626\n0.432 / 0.707\n0.270\nWrench-Adjust.\n13\n142\n18.1k\n0.628\n0.482\n0.205 / 0.607\n0.406 / 0.604\n0.457 / 0.655\n0.711\nWrench-Comb.\n12\n133\n17.6k\n0.790\n0.523\n0.256 / 0.599\n0.534 / 0.688\n0.568 / 0.730\n0.767\nHARDWARE TOOLS\nLadle\n12\n127\n18.0k\n0.805\n0.673\n0.419 / 0.781\n0.383 / 0.571\n0.410 / 0.609\n0.494\nMeasuring Cup\n12\n121\n15.3k\n0.532\n0.490\n0.201 / 0.486\n0.346 / 0.497\n0.386 / 0.619\n0.564\nMug\n12\n130\n18.3k\n0.532\n0.551\n0.668 / 0.905\n0.503 / 0.667\n0.507 / 0.722\n0.695\nPot / Pan\n12\n127\n16.1k\n0.796\n0.807\n0.629 / 0.795\n0.353 / 0.531\n0.373 / 0.552\n0.634\nSpatula\n12\n131\n20.2k\n0.456\n0.395\n0.155 / 0.532\n0.211 / 0.450\n0.251 / 0.505\n0.408\nStrainer\n12\n127\n16.0k\n0.656\n0.624\n0.235 / 0.681\n0.239 / 0.487\n0.260 / 0.536\n0.247\nUtensil\n12\n118\n14.8k\n0.740\n0.566\n0.160 / 0.477\n0.368 / 0.521\n0.405 / 0.589\n0.606\nWhisk\n12\n125\n15.9k\n0.791\n0.751\n0.427 / 0.786\n0.370 / 0.562\n0.443 / 0.641\n0.431\nKITCHEN ITEMS\nTotal\n212\n2253\n308.1k\n0.656\n0.562\n0.340 / 0.670\n0.407 / 0.588\n0.440 / 0.637\n0.605\nB. Object detection\nTo validate the dataset, we defined a test set by holding\nout 3 objects from each category (see Fig. 2 for examples),\nincluding all scenes in which they appear. We trained a single\nMask-RCNN [43] model to localize and segment instances\nfrom the 17 object categories. Training was performed with\nthe Detectron2 toolkit5 using model weights pretrained on the\nCOCO instance segmentation task. For 2D object detection,\nwe achieved AP50 = 0.774 and AP75 = 0.733, where APn\nis average precision at n% IoU. Combining thresholds from\n50% to 95%, AP = 0.656. For segmentation, we achieved\nAP50 =0.776, AP75 =0.657, and AP=0.562. These results\nsuggest that our data set is large and diverse enough to\nsupport interesting research in this area, though additional\nimages could help to yield even more robust results. Our\ntraining process could also be augmented with synthetic data,\nwhich could be facilitated by our 3D reconstructed models,\nwith some additional work, as discussed below.\nC. Category-level object pose estimation\nAs another proof-of-concept, we show the capability of\nlearning category-level 6-DoF pose estimation using our\ndataset. Specifically we evaluated the RGB-based category-\nlevel pose estimation method CenterPose [37] following the\nsame train/test split. Due to the wide diversity of poses, we\nrotate the image by the increment of 90\u25e6 that best aligns the\nvertical axis of the object with the vertical axis of the image,\nand we assume access to ground-truth object dimensions.\nExample qualitative results are demonstrated in Fig. 6.\n5https://github.com/facebookresearch/detectron2\nVI. DISCUSSION\nOne of our goals in this project has been to explore the\ndemocratization of the creation of high-quality 3D datasets\nfor robotic manipulation. This motivation has guided us\nto use off-the-shelf cameras and to select objects with\nproperties that have been difficult for traditional annotation\nmethods (e.g., reflective utensils, perforated strainers, and\nthin whisks). We aim to make the annotation pipeline as\nautomatic as possible so that it is realistic for researchers to\ngenerate their own datasets. As a result, we did not outsource\nany of our annotation or leverage a large team. In fact, once\nthe pipeline was set up, our entire dataset consisting of 308k\nannotated frames from 2.2k videos was captured and fully\nprocessed by a couple researchers over the span of just a\ncouple weeks.\nOverall, we estimate that it takes approximately 5\u20136\nminutes of manual interactive time, on average, to process a\nsingle static scene. This estimate includes both capture time\nand human intervention of the software process (rewatching\nthe video to verify XMem results, overseeing the coordinate\ntransformation process, and so forth). The rest of the process\nis automatic, requiring approximately 20 minutes or so of\ncompute. Of this time, COLMAP is by far the most compute-\nintensive part.\nGiven these time estimates, it is realistic for a single\nresearcher familiar with our pipeline to capture and pro-\ncess a single category, consisting of \u223c12 objects across\n\u223c100 scenes with \u223c15k annotated frames in diverse set-\ntings in about 12 hours\u2014excluding the computation time\nof COLMAP. Thus, we believe that our research has made\nsignificant advances toward this goal.\nWhile our pipeline has been shown to be scalable, there\nare still many bottlenecks requiring manual intervention. We\nhad initially created an automatic mesh alignment algorithm\nusing 3D point cloud matching with 3DSmoothNet [44] but\nfound that it was too slow, taking over 30 minutes for certain\nobjects, and sensitive to scale variations. This led us to\nadopt a much faster but more manual approach using the\noriented 3D bounding box of the mesh. Segmentation using\nXMem [41] is currently another step in our data annotation\npipeline requiring manual intervention. While this is typically\nminimal for most scenes, requiring no more than a few clicks\nat the beginning, it still requires 1\u20132 minutes of supervision.\nA fully automatic segmentation method would remove this\nbottleneck and further increase the scalability of our pipeline.\nLastly, meshes exported from Instant NGP currently have\npoor texture. While these renderings can be improved, baked-\nin lighting will make it difficult to render the objects realisti-\ncally in new environments. In addition, there is currently no\nway to edit the material properties for domain randomiza-\ntion. Overall, these factors preclude generating realistic and\nhigh-quality synthetic data for training. Fortunately, neural\nreconstruction and rendering is a rapidly developing field.\nFurther research in this area will add another dimension to\nour pipeline by allowing it to be used for synthetic data\ngeneration in addition to annotation of real data.\nVII. CONCLUSION\nWe have presented a large dataset of images annotated\nwith 6-DoF category-level pose and scale for robotics. This\ndataset is the largest non-outsourced of its kind. As such, it\nprovides lessons on how other researchers can collect such\ndata in the future. By capturing dynamic scenes, full 360\u25e6\nscans, and occlusions, and providing object affordances and\n3D reconstructions, our dataset provides unique character-\nistics for perceptual learning for robotic manipulation and\nfunctional grasping.\nREFERENCES\n[1] Z. Tang et al., \u201cRGB-only reconstruction of tabletop scenes for\ncollision-free manipulator control,\u201d in ICRA, 2023.\n[2] J. Deng et al., \u201cImageNet: A large-scale hierarchical image database,\u201d\nin CVPR, 2009.\n[3] T.-Y. Lin et al., \u201cMicrosoft COCO: Common objects in context,\u201d in\nECCV, 2014.\n[4] A. Kuznetsova et al., \u201cThe Open Images Dataset V4: Unified image\nclassification, object detection, and visual relationship detection at\nscale,\u201d IJCV, 2020.\n[5] T. Hoda\u02c7n et al., \u201cBOP: Benchmark for 6D object pose estimation,\u201d in\nECCV, 2018.\n[6] B. Wen et al., \u201cCatGrasp: Learning category-level task-relevant grasp-\ning in clutter from simulation,\u201d in ICRA, 2022.\n[7] \u2014\u2014, \u201cYou only demonstrate once: Category-level manipulation from\nsingle visual demonstration,\u201d RSS, 2022.\n[8] B. Huang et al., \u201cParallel Monte Carlo tree search with batched\nrigid-body simulations for speeding up long-horizon episodic robot\nplanning,\u201d in IROS, 2022.\n[9] K. Gao and J. Yu, \u201cToward efficient task planning for dual-arm tabletop\nobject rearrangement,\u201d in IROS, 2022.\n[10] K. Gao, S. W. Feng, and J. Yu, \u201cOn minimizing the number of running\nbuffers for tabletop rearrangement,\u201d RSS, 2021.\n[11] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\nThe KITTI dataset,\u201d IJRR, 2013.\n[12] H. Caesar et al., \u201cnuScenes: A multimodal dataset for autonomous\ndriving,\u201d in CVPR, 2020.\n[13] Y. Xiang, R. Mottaghi, and S. Savarese, \u201cBeyond PASCAL: A\nbenchmark for 3D object detection in the wild,\u201d in WACV, 2014.\n[14] S. Song, S. P. Lichtenberg, and J. Xiao, \u201cSUN RGB-D: A RGB-D\nscene understanding benchmark suite,\u201d in CVPR, 2015.\n[15] H. Wang et al., \u201cNormalized object coordinate space for category-level\n6D object pose and size estimation,\u201d in CVPR, 2019.\n[16] Y. Fu and X. Wang, \u201cCategory-level 6D object pose estimation in\nthe wild: A semi-supervised learning approach and a new dataset,\u201d\nNeurIPS, 2022.\n[17] P. Wang et al., \u201cPhoCaL: A multi-modal dataset for category-level\nobject pose estimation with photometrically challenging objects,\u201d in\nCVPR, 2022.\n[18] H. Jung et al., \u201cHouseCat6D \u2013 a large-scale multi-modal category level\n6D object pose dataset with household objects in realistic scenarios,\u201d\nin arXiv:2212.10428, 2022.\n[19] A. Ahmadyan et al., \u201cObjectron: A large scale dataset of object-centric\nvideos in the wild with pose annotations,\u201d in CVPR, 2021.\n[20] J. Reizenstein et al., \u201cCommon objects in 3D: Large-scale learning and\nevaluation of real-life 3D category reconstruction,\u201d in ICCV, 2021.\n[21] B. Calli et al., \u201cBenchmarking in manipulation research: Using the\nYale-CMU-Berkeley object and model set,\u201d IEEE Robotics and Au-\ntomation Magazine, 2015.\n[22] Y. Xiang et al., \u201cPoseCNN: A convolutional neural network for 6D\nobject pose estimation in cluttered scenes,\u201d in RSS, 2018.\n[23] S. Tyree et al., \u201c6-DoF pose estimation of household objects for robotic\nmanipulation: An accessible dataset and benchmark,\u201d in IROS, 2022.\n[24] B. Wen et al., \u201cRobust, occlusion-aware pose estimation for objects\ngrasped by adaptive hands,\u201d in ICRA, 2020.\n[25] \u2014\u2014, \u201cse(3)-TrackNet: Data-driven 6D pose tracking by calibrating\nimage residuals in synthetic domains,\u201d in IROS, 2020.\n[26] K. Chen and Q. Dou, \u201cSGPA: Structure-guided prior adaptation for\ncategory-level 6D object pose estimation,\u201d in ICCV, 2021.\n[27] W. Chen et al., \u201cFS-Net: Fast shape-based network for category-level\n6D object pose estimation with decoupled rotation mechanism,\u201d in\nCVPR, 2021.\n[28] T. Lee et al., \u201cTTA-COPE: Test-time adaptation for category-level\nobject pose estimation,\u201d in CVPR, 2023.\n[29] B. Wen and K. Bekris, \u201cBundleTrack: 6D pose tracking for novel\nobjects without instance or category-level 3D models,\u201d in IROS, 2021.\n[30] F. Manhardt et al., \u201cCPS++: Improving class-level 6D pose and shape\nestimation from monocular images with self-supervised learning,\u201d in\narXiv:2003.05848, 2020.\n[31] J. Lin et al., \u201cDualPoseNet: Category-level 6D object pose and size\nestimation using dual pose network with refined learning of pose\nconsistency,\u201d in ICCV, 2021.\n[32] R. Zhang et al., \u201cSSP-Pose: Symmetry-aware shape prior deformation\nfor direct category-level object pose estimation,\u201d in IROS, 2022.\n[33] M. Z. Irshad et al., \u201cCenterSnap: Single-shot multi-object 3D shape\nreconstruction and categorical 6D pose and size estimation,\u201d in ICRA,\n2022.\n[34] X. Deng, J. Geng, T. Bretl, Y. Xiang, and D. Fox, \u201ciCaps: Iterative\ncategory-level object pose and shape estimation,\u201d RAL, 2022.\n[35] M. Z. Irshad et al., \u201cShAPO: Implicit representations for multi-object\nshape, appearance, and pose optimization,\u201d in ECCV, 2022.\n[36] T. Hou et al., \u201cMobilePose: Real-time pose estimation for unseen\nobjects with weak shape supervision,\u201d in arXiv:2003.03522, 2020.\n[37] Y. Lin et al., \u201cSingle-stage keypoint-based category-level object pose\nestimation from an RGB image,\u201d in ICRA, 2022.\n[38] \u2014\u2014, \u201cKeypoint-based category-level object pose tracking from an\nRGB sequence with uncertainty estimation,\u201d in ICRA, 2022.\n[39] B. Wen et al., \u201cBundleSDF: Neural 6-DoF tracking and 3D recon-\nstruction of unknown objects,\u201d in CVPR, 2023.\n[40] J. L. Sch\u00a8onberger and J.-M. Frahm, \u201cStructure-from-motion revisited,\u201d\nin CVPR, 2016.\n[41] H. K. Cheng et al., \u201cXMem: Long-term video object segmentation\nwith an Atkinson-Shiffrin memory model,\u201d in ECCV, 2022.\n[42] T. M\u00a8uller et al., \u201cInstant neural graphics primitives with a multireso-\nlution hash encoding,\u201d ACM Trans. Graph., 2022.\n[43] K. He et al., \u201cMask R-CNN,\u201d in ICCV, 2017.\n[44] Z. Gojcic, C. Zhou, J. D. Wegner, and W. Andreas, \u201cThe perfect match:\n3D point cloud matching with smoothed densities,\u201d in CVPR, 2019.\n"
  },
  {
    "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
    "link": "https://arxiv.org/pdf/2308.01907.pdf",
    "upvote": "10",
    "text": "The All-Seeing Project: Towards Panoptic Visual\nRecognition and Understanding of the Open World\nWeiyun Wang\u22171,2, Min Shi\u22171,3, Qingyun Li\u22171,4, Wenhai Wang\u22171, Zhenhang Huang\u22171,\nLinjie Xing\u22171, Zhe Chen1,5, Hao Li1,6, Xizhou Zhu1,7, Zhiguo Cao3, Yushi Chen4,\nTong Lu5, Jifeng Dai\u20201,8, Yu Qiao1\n1OpenGVLab, Shanghai AI Laboratory\n2Fudan University\n3Huazhong University of Science and Technology\n4Harbin Institute of Technology\n5Nanjing University\n6The Chinese University of Hong Kong\n7SenseTime Research\n8Tsinghua University\nCode: https://github.com/OpenGVLab/all-seeing\nDemo: https://huggingface.co/spaces/OpenGVLab/all-seeing\nAbstract\nWe present the All-Seeing (AS)1 project: a large-scale data and model for recogniz-\ning and understanding everything in the open world. Using a scalable data engine\nthat incorporates human feedback and efficient models in the loop, we create a\nnew dataset (AS-1B) with over 1 billion regions annotated with semantic tags,\nquestion-answering pairs, and detailed captions. It covers a wide range of 3.5\nmillion common and rare concepts in the real world, and has 132.2 billion tokens\nthat describe the concepts and their attributes. Leveraging this new dataset, we\ndevelop the All-Seeing model (ASM), a unified framework for panoptic visual\nrecognition and understanding. The model is trained with open-ended language\nprompts and locations, which allows it to generalize to various vision and language\ntasks with remarkable zero-shot performance, including region-text retrieval, region\nrecognition, captioning, and question-answering. We hope that this project can\nserve as a foundation for vision-language artificial general intelligence research.\n1\nIntroduction\nCreating artificial general intelligence (AGI) systems that can match human intelligence and excel\nin any task across domains is the ultimate goal of artificial intelligence. Recent advancements in\nLarge Language Models (LLMs) have demonstrated impressive zero-shot capabilities in user-tailored\nnatural language processing (NLP) tasks, suggesting new avenues for achieving AGI. However, as\nshown in Fig. 1a, most popular LLMs [62, 64, 77, 78, 80, 19, 17] are limited to processing language\ninformation and lack the ability to perceive or understand the visual world.\nAlthough there have been some recent developments [61, 111, 54, 48, 47, 22, 96, 55] in open-world\nvisual understanding, they are primarily focused on understanding images as a whole, rather than\nrecognizing and comprehending individual instances within the scene (see Fig. 1b). This goes against\nthe nature of the human visual system, as described by the feature integration theory [81], which\n\u2217Equal contribution. This work is done when Weiyun Wang, Min Shi, and Qingyun Li are interns at\nShanghai AI Laboratory. \u2020Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\n1\u201cAll-Seeing\u201d is derived from \u201cThe All-Seeing Eye\u201d, which means having complete knowledge, awareness,\nor insight into all aspects of existence.\narXiv:2308.01907v1  [cs.CV]  3 Aug 2023\nLarge Language Model\nOutput\nWeb-Scale Language\nDatasets e.g., CC,\nWikipedia, WebText2,\nBookCorpus, etc.\nPrompt\n(a)\nLarge\nLanguage\nModels\n(LLMs) possess extensive world\nknowledge\nand\ndemonstrate\nimpressive reasoning capabilities,\nbut lack the ability to receive and\ncomprehend visual information.\nVisual Large\nLanguage Model\nImage\nPrompt\nOutput\nWeb-Scale Image-\nText Datasets e.g.,\nLAION-400M/5B,\nDataComp, etc.\n(b) Visual Large Language Models\n(VLLMs) can process both text and\nimages, but they can only capture\nthe holistic visual information of\nthe whole image and understand it\nbased on LLMs.\nThe All-Seeing\nModel\nOutput\nThe AS-1B Dataset:\n1B Region-Text Pairs\n3.5M Concepts\n132B Tokens\nImage w/ Loc.\nPrompt\n(c) Our All-Seeing Model (ASM)\ncan comprehensively recognize\nand understand the objects or con-\ncepts in regions of interest, while\nmaintaining the capabilities of\nVLLMs and LLMs.\nFigure 1: Overview and comparison of our All-Seeing project with other popular large foun-\ndation models. To address the limitations of LLMs in understanding visual inputs and VLLMs in\neffectively leveraging region-aware information, we propose (1) a large-scale dataset AS-1B which\nconsists of 2 billion region-text pairs, 3.5 million open-world concepts, and over 1 billion tokens\nof region-related question-answering and caption; and (2) the All-Seeing model (ASM), which is\ncapable of recognizing and understanding context in arbitrary regions.\nsuggests that we attentively gather visual features and contexts in certain regions to achieve high-level\nunderstanding and recognition, rather than analyzing all information simultaneously.\nTo achieve instance-level visual understanding like humans, there are two major challenges as follows:\n(1) The scarcity of open-world instance-text pair data. As listed in Table 1, existing datasets, such\nas Visual Genome [43], have limitations in terms of data scale and open-world annotations. Laion-\n5B [72] only contains web-crawled image-text pairs without location information, and SA-1B [42]\nlacks semantic information. (2) The lack of spatial information modeling in most existing models.\nThese models mainly focus on whole-image understanding as mentioned above.\nIn this work, we propose the All-Seeing (AS) project for open-world panoptic visual recognition\nand understanding, driven by the goal of creating a vision system that mimics human cognition. The\nterm \u201cpanoptic\u201d refers to including everything visible in one view [41]. The AS project addresses the\nchallenges from both the data and model perspectives.\nFrom the data aspect, we propose the All-Seeing 1B (AS-1B) dataset, consisting of over 1 billion\nregion annotations in various formats, such as semantic tags, locations, question-answering pairs, and\ncaptions (refer to Fig. 2). AS-1B dataset is made possible by a scalable semi-automatic data engine,\nwhich significantly lowers the previously unaffordable expense of manually annotating a massive\namount of open-world semantics. The data engine operates in a \u201cdata-human-model\u201d loop, iteratively\nrefining data quality. Initially, diverse models, including large language models (LLMs) [17],\ndetection[88, 25, 51], captioning [48], and visual question answering models (VQA) [54, 111, 55],\nare employed as \u201cannotators\u201d, which add semantic annotations to dense region proposals generated\nby state-of-the-art object detectors [42, 25, 51, 88]. Subsequently, human annotators verify the\ngenerated pseudo labels and provide feedback with high-quality data, which is then used to fine-tune\nthe models to improve their performance. The enhanced models are then used to re-annotate the data,\nstarting another iteration of the loop. As shown in Fig. 2, AS-1B contains a wide range of open-world\nconcepts, including over 3.5 million different semantic tags ranging from common categories (e.g.,\nhuman, backpack) to fine-grained or rare categories with attributes (e.g., old metal latches). AS-1B\nalso encompasses the 3.3 billion visual question-answering pairs and 1.2 billion region captions for\n1.2 billion regions.\n2\nIn terms of the model perspective, we propose the All-Seeing model (ASM), a unified location-\naware image-text foundation model. The model consists of two key components: a location-aware\nimage tokenizer and an LLM-based decoder. The location-aware image tokenizer uses location\ninformation such as box, mask, and point set as conditions (see Fig. 1c) to extract image features,\nwhich contribute to the location capability of ASM. The LLM-based decoder inherits the world\nknowledge and reasoning capability from LLMs such as LLaMA [80], providing a strong foundation\nfor visual recognition and understanding. In addition, to unify image-text aligning and generation\ntasks, we introduce a new decoding approach, where the aligning tasks are reformulated as a \u201cspecial\u201d\ngeneration task, enabling our model to generalize to various vision-language tasks with shared\nweights.\nCompared to previous methods [67, 2, 48, 54, 111], our work offers several advantages as follows: (1)\nOur model not only excels in image-level understanding but also demonstrates exceptional capability\nin recognizing and comprehending objects at the instance level, closely aligning with human cognitive\nprocesses. (2) Our model is a unified framework that supports a wide range of image-text tasks,\nincluding discriminative tasks like image-text retrieval, as well as generative tasks such as visual\ncaptioning and question-answering. (3) Our model comes with AS-1B the largest dataset with\nopen-world panoptic semantics. Data and models feed each other in the data engine, iteratively\nimproving the model performance, data scale and diversity.\nIn summary, our contributions are three folds:\n(1) We propose a new large-scale dataset (AS-1B) for open-world panoptic visual recognition and\nunderstanding, using an economical semi-automatic data engine that combines the power of off-the-\nshelf vision/language models and human feedback. As reported in Table 1, we have 159 times more\nsemantic tags and 33 times more regions compared with its counterparts.\n(2) Based on the dataset, we develop a unified vision-language foundation model (ASM) for open-\nworld panoptic visual recognition and understanding. Aligning with LLMs, our ASM supports\nversatile image-text retrieval and generation tasks, demonstrating impressive zero-shot capability.\n(3) We evaluate our model on a representative vision and vision-language tasks. Our ASM outperforms\nCLIP [67] by 10.4 and 14.3 (mAP) on COCO [53] and LVIS [31] in zero-shot region recognition\ntasks. When trained with AS-1B (region-level data) and LaionCOCO [71] (image-level data), our\nmodel achieves superior zero-shot and fine-tuned performance compared to recent image-level [47,\n22, 87, 98, 35] and region-level [99, 92, 65] VLLMs.\n2\nRelated Work\nThe Emergence of Large Language Models. In recent years, based on the large-scale text cor-\npora [28, 83, 69, 116, 82, 100], the field of Large Language Models (LLMs) has witnessed remark-\nable progress [69, 8, 56, 70, 106]. Prominent models such as ChatGPT [62] and GPT-4 [61] have\ndemonstrated excellent performance across various tasks, showcasing their potential for semantic\nunderstanding, dialogue generation, programming, mathematical problem-solving, and more. How-\never, there is a growing concern that these leading institutes are becoming increasingly conservative\nin sharing the technical details of their models and roadmaps. To catch up with the performance of\nChatGPT, the open-source community has devoted substantial efforts [80, 90, 77, 17, 102, 29, 104].\nFor instance, Self-Instruct [90] introduced an iterative bootstrapping algorithm that leverages off-\nthe-shelf LLMs and a seed set of manually-written instructions to expand the instruction collection.\nAlpaca [77] utilized the Self-Instruct technique to generate high-quality instruction-following data,\nwhich was then used to fine-tune the LLaMA [80] model. Vicuna [17] demonstrated that fine-tuning\non user-shared ChatGPT conversations can spark dialog and improve instruction-following capa-\nbilities. Furthermore, there has been a focus on improving multilingual capabilities, particularly in\nChinese, with LLMs like Chinese-Alpaca [21], GLM-130B [102], InternLM [78], MOSS [19], and\nothers. These LLMs have shown excellent proficiency in learning world knowledge, which lays the\ngroundwork for open-world understanding.\nDatasets for Visual Recognition and Understanding. The dataset plays a critical role in the\nadvancement of deep learning models, especially in the field of visual recognition and comprehension.\nPrior to the era of large-scale models, datasets are primarily closed-world or have limited data\nscale, including CIFAR-10/100 [44], ImageNet [23], and iNaturalist [84] for image classification,\n3\nFrequency of Semantic Tags (lg)\nSemantic Tag\nStone Sphinx\nQ1: What is the material of this sphinx?\nA1: The material of the sphinx is made of \nsandstone.\nQ2: What is the color of this sphinx?\nA2: The color of the sphinx is brown.\nQ3: What is the height of this sphinx?\nA3: The height of the Sphinx of Giza is \napproximately 65 feet (18.5 meters) tall.\nDetailed Caption: The Sphinx of Giza is a \nsandstone statue with a brown color and \na height of approximately 65 feet (18.5 \nmeters).\nDetailed Annotation\n3.5M\n0.0 \n1.0 \n2.0 \n3.0 \n4.0 \n5.0 \n6.0 \n7.0 \n8.0 \nperson\nbackpack\ncat\nbatting helmet\nStone Sphinx\nold metal latches\nstormy clouds\nblue and black eyes\nSematic Tags\nQ1: What is the name of this pyramid?\nA1: The name of the pyramid is the Great\nPyramid of Giza.\nQ2: What is the height of this pyramid?\nA2:\nThe\nheight\nof\nthe\npyramid\nis\napproximately 144 meters (479 feet).\nQ3: Who built this pyramid?\nA3: The Great Pyramid of Giza, also\nknown as the Pyramid of Khufu, was built\nby the Egyptians over 4,500 years ago.\nDetailed Caption: The Great Pyramid of\nGiza, also known as the Pyramid of Khufu,\nwas built by the Egyptians ...\nDetailed Annotation\nSemantic Tag\nPyramid\nFigure 2: Semantic concepts and annotations in the AS-1B dataset. The semantic tags in AS-1B\ndataset encompass a wide range of concepts, from common objects to rare and fine-grained categories\nwith attributes. Beyond brief semantic tags, detailed annotations, including visual-question-answering\npairs and region captions are also provided.\nDataset\n#Images\n#Regions\n#Concepts\n#Tokens\nLocation\nSemantic\nImage-Level\nImageNet-22K [23]\n15M\n\u2212\n22,000\n\u2212\n\u2212\nClosed-Set\nCOCO Caption [15]\n0.1M\n\u2212\n\u2212\n8.4M\n\u2212\nClosed-Set\nSBU [63]\n0.8M\n\u2212\n\u2212\n14.6M\n\u2212\nOpen-World\nCC12M [11]\n12.4M\n\u2212\n\u2212\n250.9M\n\u2212\nOpen-World\nYFCC15M [38]\n15M\n\u2212\n\u2212\n1.0B\n\u2212\nOpen-World\nCOYO700M [9]\n700M\n\u2212\n\u2212\n15.0B\n\u2212\nOpen-World\nLaion-5B [72]\n5B\n\u2212\n\u2212\n135.0B\n\u2212\nOpen-World\nClass-Agnostic\nSA-1B [42]\n11M\n1.1B\n\u2212\n\u2212\nOpen-World\n\u2212\nRegion-Level\nCOCO [53]\n0.1M\n0.9M\n80\n\u2212\nClosed-Set\nClosed-Set\nLVIS [31]\n0.1M\n1.5M\n1,203\n\u2212\nClosed-Set\nClosed-Set\nObjects365 [74]\n0.6M\n10.1M\n365\n\u2212\nClosed-Set\nClosed-Set\nOpen Images [45]\n1.5M\n14.8M\n600\n\u2212\nClosed-Set\nClosed-Set\nBigDetection [10]\n3.5M\n36.0M\n600\n\u2212\nClosed-Set\nClosed-Set\nV3Det [86]\n0.2M\n1.5M\n13,029\n\u2212\nClosed-Set\nClosed-Set\nVisual Genome [43]\n0.1M\n0.3M\n18,136\n51.2M\nOpen-World\nOpen-World\nAS-1B (ours)\n11M\n1.2B\n3.5M\n132.2B\nOpen-World\nOpen-World\nTable 1: Comparison with popular vision and vision-language datasets. \u201c#\u201d denotes the number\nof something. We see that the proposed AS-1B dataset has a significantly larger data scale and\ndiversity than prior region-level datasets.\nPascal VOC [24], COCO [53], LVIS [31], OpenImages [45], ADE20K [109], and Cityscape [20]\n4\nfor visual location, as well as SBU [63], CC3M [75], CC12M [11], YFCC15M [79], and VQA [4],\nVQA 2.0 [30], ICDAR 2015 [40], SCUT-CTW1500 [101] for visual understanding. Additionally,\ndatasets like Visual Genome [43] and Visual7W [115] integrate visual location and understanding,\noffering more comprehensive tasks to describe the visual world. However, these datasets have\nlimited semantics and fail to encompass diverse scenarios in the open world, which hinders the\ngeneralization ability of models. To achieve open-world capability, CLIP [67] and ALIGN [37]\npropose training models using web-scale image-text pairs collected from the internet. Subsequent\nworks, such as Laion-400M [73], Laion-5B [72], COYO-700M [9] and DataComp [27], have also\nbeen introduced for open-source research. However, these approaches only include descriptions\nor question-answering pairs corresponding to the entire image, resulting in models struggling to\naccurately recognize and understand specific objects at the instance level. Recently, Kirillov et\nal. introduced SA-1B [42], which provides open-world location information such as boxes and\nmasks but still lacks semantic details. So existing datasets cannot meet the requirements of data\nscale, open-world location and semantics necessary for achieving visual AGI models, thus posing\nchallenges in supporting human-like panoptic visual recognition and understanding.\nModels for Visual Recognition and Understanding. Significant advancements have been made in\nthe field of visual recognition and understanding in recent years. Previous methods [33, 39, 16, 113,\n14, 41, 93, 46] mainly concentrate on the close-set recognition while recent works begin to focus on\nthe open world understanding. Models trained with contrastive learning-based methods, including\nCLIP [67], ALIGN [37], EVA [26] and FLIP [52], are able to recognize and understand the open world\nsemantics under an image-text matching framework while the lack of generation ability limits their\napplicability. To address this limitation, subsequent works, such as SimVLM [91], UniPerceiver [114],\nVL-BERT [7], VLMo [6], BEiT-3 [89], ALBEF [49], CoCa [98], as well as Flamingo [2], have\nincorporated generative training tasks. However, these models are trained from scratch and do not\ncapitalize on the powerful perception capabilities of existing powerful vision foundation models\nfor image, and Large Language Models for text, increasing the cost of developing new models.\nThe recent progress of LLMs [61, 62, 68, 69, 8] initiates a new era, leading to the emergency\nof many LLM-based multimodal models [48, 47, 111, 54, 96, 104, 60, 87, 12] and interactive\nsystems [94, 55, 76, 110, 50, 112, 95]. However, these works are only capable of recognizing the\nentire image, lacking the ability to comprehend specific regions within the image. Some concurrent\nmethods, such as ChatSpot [107], Shikra [13], KOSMOS-2 [65], and GPT4RoI [105] begin to\nfocus on location-aware understanding. However, without the support of large-scale instance-level\nvisual understanding data, the generalization ability of these models is still limited. Besides, these\nmodels only support generative tasks, limiting their application to discriminative tasks, such as\nimage-text retrieval and zero-shot object recognition. In this work, we propose a unified location-\naware image-text foundation model, based on ViT-g [26] and Husky [55]. Our model supports both\nimage-text matching and generation tasks, expanding its range of applications and contributing to the\nadvancement of AGI models.\n3\nThe All-Seeing Dataset (AS-1B)\nIn this section, we introduce the All-Seeing-1B (AS-1B) dataset for open-world panoptic visual\nrecognition and understanding. The dataset consists of 1.2 billion regions in 11 million images2.\nEach region is annotated with comprehensive information, including categories, locations, attributes,\ncaptions, and question-answer pairs. Compared with the previous visual recognition datasets like\nImageNet [23] and COCO [53], visual understanding datasets like Visual Genome [43] and Laion-\n5B [72], the proposed AS-1B dataset stands out due to its rich and diverse instance-level location\nannotation and corresponding detailed object concepts and descriptions.\n3.1\nData Annotation Engine\nWe develop a semi-automatic data engine that efficiently uses a wide range of state-of-the-art\nfoundation models as annotators, reducing the enormous labeling cost to an acceptable level. As\ndepicted in Fig. 3, the process of the data engine begins by generating noisy pseudo data using\nwell-trained off-the-shelf foundation models from diverse fields. Subsequently, these pseudo data\nare iteratively refined through multiple loops with the aid of models fine-tuned on human feedback\n2Images source from SA-1B [42]\n5\n(c) Semantic-Location Matching\nSphinx\nStatue\n...\nBricks\nWall\n...\nEgyptian\nPyramid\n...\nCave\nHole\n...\nAligned Region-Tags\nAutomaEc GeneraEon\nHuman Feedback\n(d) Detailed Description Generation\nAKribuEons\nDetailed \nDescripEon\n(b) Open-World Semantic Tags Generation\n1. Sphinx\n2. Bricks\n3. Statue\n...\nSemantic Tags\n(a) Open-World LocaEon GeneraEon\nClass-Agnostic\nClosed-Set\nOpen-Vocabulary\nRegion Proposals\nSemantic\nTags\nRegion\nProposals\nCLIP, ASM\n(e) Human in the Loop\nCorrect or Not?\nSpotter\nImaginator\nSpliKer\nMagnifier\nQuestioner\nResponder\nWriter\nFigure 3: Data engine for AS-1B dataset. Our data engine consists of an automatic annotation\npipeline (i.e., (a), (b), (c), (d)) and human verification stage (i.e., (e)). We combine strong object\ndetectors, LLMs, and VLLMs to produce open-world locations and annotations for different regions.\nThe automatic annotations are sampled and verified by human experts. Automated annotations are\nused together with human validation results to train region-aware alignment and generation models,\nwhich are then used in the automated annotation pipeline to improve data quality.\ndata. By employing this \u201cdata-human-model\u201d cycle, we can generate a large number of region-level\nannotations with exceptional quality.\nAs the core component of the data engine, the pseudo data generation pipeline consists of five steps\nas follows: (1) Creating open-world location (e.g., bounding box, mask, point set) with an ensemble\nof state-of-the-art class-agnostic, visual grounding, and closed-set perception models [42, 51, 88, 25];\n(2) Generating open-world semantic tags using the combination of image captioning models [48, 111]\nand LLMs [17]; (3) Matching the semantic tags to proper regions with image-text aligning models\nsuch as CLIP [67]; (4) Using LLM [17] and VQA models [55] to generate the attributions of each\nregion based on the matched semantic tags; (5) Generating detailed captions based on the semantics\nand attributions of each region.\n3.2\nOpen-World Localization\nTo obtain comprehensive locations of all instances in an image, we combine the results of state-of-\nthe-art perception models from different fields, including (1) class-agnostic model: we adopt the\nSAM [42] to provide initial proposals of most objects in view. (2) closed-set detection model: we\nuse InternImage-H [88] and EVA-02 [25] trained on BigDetection [10] and LVIS [31], respectively,\nto detect the common-seen objects. (3) grounding model: we use GLIP [51] to ground open-world\nsemantics generated by LLMs [111] (see Sec. 3.3). All the bounding boxes are gathered together to\nensure that all possible objects in view are covered.\nDue to the incomparable score ranges of different models, directly using non-maximum suppression\n(NMS) to eliminate duplicated proposals from multiple resources is infeasible. Therefore, we develop\nan effective strategy that keeps all the semantics while removing highly-overlapped regions. As\nshown in Alg. 1, the merging strategy works as follows: (1) We start by initializing the result region\nproposal set R with the class-agnostic bounding boxes generated by SAM. (2) When a set of region\nproposals R\u2032 from a new source (e.g., closed-set/grounding detector) comes in, we calculate the\nIntersection over Union (IoU) between the regions in R\u2032 and R. (3) If the IoU between a new region\nr\u2032 \u2208 R\u2032 and an existing region r \u2208 R is greater than a threshold TIoU, the region r\u2032 is removed, and\nits closed-set/grounding tags are appended to the tag list of the matched region r. (3) Finally, the\nremaining low-IoU regions in R\u2032 along with their tags are added to R. By employing this strategy, we\nsequentially combine the results of SAM, InternImage, EVA-02 and GLIP to obtain comprehensive\nlocation information for an image.\n6\nAlgorithm 1 Region Proposal Merging\nInput:\nExisting region proposals R\nNew region proposals R\u2032\nIoU threshold TIoU\nOutput:\nMerged region proposals R\n1: for region r\u2032 \u2208 R\u2032 do\n2:\nCalculate IoU between r\u2032 and proposals in R\n3:\nif maximum IoU > TIoU then\n4:\nMerge semantic tags from r\u2032 into the semantic tag of corresponding regions in R\n5:\nDelete r\u2032\n6:\nelse\n7:\nAdd r\u2032 into R\n8:\nend if\n9: end for\n3.3\nOpen-World Semantic\nManually labeling billions of regions for an open-world semantic description is impractical due to\nthe enormous cost and time required. On the other hand, generating annotations with off-the-shelf\nmulti-modal models is also non-trivial, as it demands sufficient world knowledge and context-related\nreasoning capabilities to accurately label diverse objects in the wild. To remedy these challenges, we\ndraw inspiration from the recent advancements in Large Language Models (LLMs) [8, 80, 19, 77,\n17, 78, 102] and Visual Large Language Models (VLLMs) [61, 54, 48, 55, 87, 111, 96], we leverage\na series of LLMs and VLLMs as \u201csemantic generators\u201d and tap into their vast world knowledge\nand reasoning abilities for open-world semantic generation. These \u201csemantic generators\u201d can be\nspecialized for producing short semantic tags (such as category names and brief attributes) or detailed\nannotations (including attributes, question-answering pairs, and captions) based on specially-designed\nprompts.\n3.3.1\nSemantic Tags\nTo generate as many semantic tags as possible for a view, different instructions are employed to\nharness the diverse capabilities of LLMs and VLLMs, turning them into annotators with different\nfocuses and skills. Specifically, we have (1) a spotter, which identifies major instances and provides\nan overview of the scenes, (2) a imaginator that leverages world knowledge to imagine plausible\nobjects, (3) a splitter that divides complicated objects into parts, as well as (4) which zooms on\neach region to produce region-specific candidates. These models complement each other to create a\npowerful system that can generate comprehensive open-world semantic tags for each region and the\nentire image. Here are the details of each model:\nSpotter. This module aims to list the prominent and major objects present in the given image. To\nachieve this, we use MiniGPT4 [111] to provide an overall caption of the input image. From the\ngenerated captions, we extract noun phrases to serve as the semantic tags shared by all the regions in\nthe input image. In addition, we also add an OCR detector [18] to detect the texts as semantic tags in\nthe scenes. Note that the generated caption will also be passed to other annotators, which gives visual\nsignal for the LLMs, serving as their eyes.\nImaginator. Although the \u201cspotter\u201d can find out the major objects in the scenes, it fails to identify\nmany insignificant objects. To address this limitation, we develop an \u201cimaginator\u201d to further expand\nthe semantic tag list with plausible imagination. The \u201cimaginator\u201d emulates human-like thinking.\nWhen provided with descriptions of a particular scene, humans can effortlessly imagine the potential\nobjects present. For instance, if informed that an image depicts a group of children standing in a\nclassroom, one may envision objects like \u201cteacher\u201d, \u201cblackboard\u201d, and \u201cstationery\u201d. In our data\nengine, we utilize Vicuna [17] to imagine possible objects in scenes based on the captions generated\n7\nby the \u201cspotter\u201d, and then extend the set using web search engines [66]. The \u201cimaginator\u201d excels at\nsupplementing scene-specific object candidates, such as suggesting \u201cairport stuff\u201d instead of simply\n\u201cperson\u201d. This significantly enhances the concept diversity within this project.\nSplitter. This model is proposed to divide the generated concepts into more fine-grained parts. We\nfind that some region proposals only cover a part of the objects, such as the wing of a plane or the\nwindshield of a car. However, most of the existing perception or caption models are not capable\nof detecting parts. To this end, we further instruct the Vicuna [17] to divide the semantic tag into\nparts. For example, \u201cbuilding\" will be decomposed into \u201croof\u201d, \u201cdoor\u201d, \u201cwindows\u201d and \u201cwalls\u201d. We\ntailor the prompt for LLM so that the model only divides the semantic tag that represents a concrete\nobject into parts. LLM is instructed to ignore the semantic candidate that is non-physical or cannot\nbe further divided, such as \u201cwater\u201d, \u201csky\u201d, etc.\nMagnifier. Although hundreds of open-world semantic tags can be generated by the aforementioned\nannotators for each image, there still exists some regions whose semantics are absent from the\ngenerated tag lists. So we introduce a \u201cmagnifier\u201d to zoom in on each region and add semantic tags\nfor them. We simply crop the region and use a caption model to describe the cropped image, and then\nextract the noun phrases, which are used as the semantic candidates exclusive for the corresponding\nregions. In this model, we use BLIP [48] for efficiency.\n3.3.2\nDetailed Descriptions\nTo provide detailed descriptions that include attributes and statuses of each region, we develop a\npipeline that expands the region description using the open-world location and its matched semantic\ntags (see Sec. 3.4 for location-semantic matching). Similar to how we generate semantic tags, we\nutilize a series of skilled LLMs, including (1) a questioner that asks specific questions about the\nattributes or status of a given semantic tag; (2) a responder that provides the accurate answers for\nthese questions based on the region\u2019s content; and (3) a writer responsible for composing a detailed\ncaption for each region, according to the generated semantic tags, attributes, and status.\nQuestioner. Given semantic tag, to determine its commonly-used attributes, we use Vicuna [17] as a\nquestioner to generate three questions about the attributes or statuses. The prompt is shown below. In\nthis way, we leverage the world knowledge and reasoning capabilities of LLMs to identify the most\nrelevant attribute of an object.\nPrompt: I will give you some objects. Please list 3 questions about the given objects. These\nquestions must be answerable based on a photograph of the object and cannot rely on any\noutside knowledge. Some examples are listed as follows:\nHuman: Person\nAssistant: Q1: What is the sex of this person? Q2: What is the hairstyle of this person? Q3:\nWhat is this person doing?\nHuman: {Semantic Tag}\nAssistant:\nResponder. After obtaining the questions related to a semantic tag, we employ Husky [55], an\nLLM-based VQA model, to generate the responses to each question. The responses are generated\nin several sentences, taking into account the content of the region. An example prompt is shown\nbelow. This approach enables us to gather additional information about a region while preventing the\ninclusion of irrelevant content.\nHuman: What is the material of this sphinx? Assistant:\nWriter. Based on the question-answering pairs, we proceeded to rephrase them into a single sentence,\nresulting in a detailed description of the region. The prompt used during annotation is \u201cPlease\nparaphrase the following sentences into one sentence. {answer for question 1} {answer for question\n2} {answer for question 3}\u201d. It is notable that both the question-answering pairs from previous steps\nand the region captions from this step are valuable for visual recognition and understanding models.\n8\n3.4\nMatching Location and Semantic\nGiven the generated open-world location and semantic labels, we devise a matching pipeline to select\nand appropriate tags for each region. Semantic tags that are most related to the region will be picked.\nIn the matching process, we employ a region-text aligning model to measure the similarity between a\ncertain region and its semantic tag list. For each region, the semantic tag list is constructed by LLMs\n(i.e., \u201cspotter\u201d, \u201cimaginator\u201d, and \u201cdivider\u201d) and closed-set/grounding object detectors. Initially, in\nthe first iteration of the data engine, we use a CLIP model [67] for the region-text alignment, where\nthe input is the cropped region. Subsequently, we upgrade the model to our All-Seeing Model.\nIn addition, in the first round of data engine, we find that only using CLIP led to erroneous results as\nit cannot tell which candidate is the major object in the bounding boxes. For example, a bounding box\nthat perfectly frames a person can be classified as a \u201cbackpack\u201d if the person is carrying a backpack.\nTo remedy this, we use CLIPSeg [58] to generate the mask for each candidate, and the original CLIP\nconfidence is modulated with the corresponding mask area. In this way, the candidate belonging to\nthe main object in the region can be selected.\n3.5\nHuman Verification\nAlbeit efficient, annotations from the automated pipeline still contains some noise due to the cropping\nprocess, which might discard essential context information. For instance, a lampshade hanging on\nthe ceiling could be mistakenly described as a \u201ccup\u201d due to its similar shape and color. Therefore, to\nenhance the data quality, we find it crucial to include human verification.\nSemantic tags. We design a data sampling strategy and simplify the task for annotators by focusing\non picking the incorrect ones from the top-5 candidates in each region. In the real world, concepts\nexhibit long-tail distribution as shown in Fig. 2. Therefore, many rare concepts will be missed if\nthe region is randomly sampled for validation. To address this issue, we implement a concept-wise\nsampling strategy. Specifically, we collect a list of concepts in the first 1M images in the AS-1B\ndataset. From this list, we select most concepts for verification. We randomly sample 6 regions from\nthe least frequent concepts and 90 regions from the concepts with the highest number of regions.\nDuring the human verification process, the semantic tag list for the sampled regions is provided to\nthe annotators, who are then tasked with filtering out any incorrect tags.\nVisual Question-Answering Pairs. Although using LLMs/VLLMs greatly reduces the annotation\ncost of generating visual question-answer pairs, there are still some issues that may introduce noise\ninto the data. (1) The answer to the question is wrong since the VLLM is not perfect. (2) The generated\nquestion for the semantic tag may be unanswerable according to the given image content. (3) The\nsemantic tag assigned to a region may be incorrect, leading to meaningless generated questions. For\nexample, if a region containing a dog is wrongly labeled as a cat, asking about the color of the cat\nwould be nonsensical.\nTo address these issues, we perform a two-stage verification procedure. In the first stage, human\nannotators are provided with the image, location (bounding box), and corresponding question-answer\npairs. They are then asked to annotate the visual question-answer pair with one of four choices: correct\nanswer, wrong answer, unanswerable question, or wrong semantic tag. Samples annotated as \u201ccorrect\nanswer\u201d are retained, while those annotated as \u201cwrong answer\u201d are re-annotated with a correct answer\ngenerated by human annotators in the second stage. Samples annotated as \u201cunanswerable question\u201d or\n\u201cwrong semantic tag\u201d are annotated with a rejection answer, such as \u201cThis question is unanswerable\naccording to the image\u201d or \u201cThe object in this region is incorrectly labeled\u201d, respectively.\nVerification Review. We engaged 50 human annotators to perform verification on the annotations\ngenerated by our model. To guarantee the quality of this verification process, we additionally request\n10 experts to review the verified annotations. These experts are selected based on their domain\nknowledge and experience in annotation tasks. To streamline the process, we organize the regions\nrequiring review into groups of 100. Each group is assigned to one expert, who checks the accuracy\nand consistency of the annotations within the group. Any package with an accuracy rate below 95%\nwill be sent back for re-verification by another annotator. This review process double-checks the\nannotations, further ensuring their reliability and validity for our models.\n9\n3.6\nData Engine Iteration\nTo continuously improve the data quality, we implement a \u201cdata-human-model\u201d loop that maximizes\nthe utilization of both human-verified data and models. As depicted Alg. 2, the data engine iteration\ncomprises three steps as follows: (1) The images are processed with the annotation pipeline which\nproduces automatic annotations. (2) The ASM model is then trained using these coarse annotations,\nenabling it to perform both discriminative and generative tasks such as region-text matching and\nregion captioning. (3) The automatic annotations are sampled and reviewed and corrected by human\nannotators, yielding high-quality human annotations. This verified data is then used to fine-tune the\nASM model, thereby enhancing its performance. (4) The fine-tuned model is utilized to re-rank the\nsemantic tags and generate more accurate region captions and answers. Repeat the third and fourth\nsteps until the data quality meets the requirements. By following this data iteration process, we ensure\ncontinuous optimization of data quality, ultimately leading to superior results.\nAlgorithm 2 Data Engine\nInput:\nIteration Number n\nImages I\nModels M\nAnnotation Pipeline P(M, I)\nOutput:\nAnnotations: A\nImproved Models M\n1: Generate initial annotation A0 by off-the-shelf models;\n2: Train ASM with A0, yield M0;\n3: i \u2190 0\n4: while i < n do\n5:\nPerform Human verification on Ai, yield A\u2032\ni;\n6:\nFine-tune Mi with A\u2032\ni, obtain Mi+1;\n7:\nObtain Annotation Ai+1 by P(Mi+1, I);\n8:\ni \u2190 i + 1\n9: end while\n4\nThe All-Seeing Model (ASM)\n4.1\nOveral Architecture\nOur objective is to create a unified framework that supports contrastive and generative image-text\ntasks at both the image level and region levels. By leveraging pre-trained LLMs and powerful\nvision foundation models (VFMs), this model demonstrates promising performance in discriminative\ntasks like image-text retrieval and zero classification, as well as generative tasks such as visual\nquestion answering (VQA), visual reasoning, image captioning, region captioning/VQA, etc. Addi-\ntionally, our model shows potential in grounding tasks like phrase grounding and referring expression\ncomprehension, with the assistance of a class-agnostic detector.\nAs illustrated in Fig. 4, our All-Seeing Model (ASM) comprises three key designs: (1) a location-\naware image tokenizer extracting features from both the image and region levels based on the\ninput image and bounding box, respectively. (2) a trainable task prompt that is incorporated at the\nbeginning of the vision and text tokens to guide the model in distinguishing between discriminative\nand generative tasks. In the case of the discriminative task, a trainable align token is appended\nto the input sequence to gather the overall representation, and its embedding is then used in the\nmatching process. (3) an LLM-based decoder that is utilized to extract vision and text features for\ndiscriminative tasks, as well as to auto-regressively generate response tokens in generative tasks.\nThe training objective of ASM contains two objectives: next token prediction and region-text aligning,\nas formulated in Eqn. 1. The primary objective focuses on enhancing the model\u2019s generation capability,\n10\nBackbone\nLLM-based Decoder\nTask Instruction, Question ...\nDesired Output\nTransformer Decoder\n...\n...\n...\n...\nImage w/ Location\nRandom Queries\nRegion Queries\nLocation-Aware Image Tokenizer\nLLM-based Decoder\n\ud835\udcab!\n\ud835\udc4f\ud835\udc5c\ud835\udc60 Human: \ud835\udcb1 What is \nthis? Assistant:\nThis is the Sphinx\nGenerative\nLoss \u2112!\"#\nLLM-based Decoder\n\ud835\udcab\"\n\ud835\udc4f\ud835\udc5c\ud835\udc60 Human: \ud835\udcb1 What is\nthis? \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b\nLLM-based Decoder\n\ud835\udcab\"\n\ud835\udc4f\ud835\udc5c\ud835\udc60 Assistant: A photo \nof the Sphinx. \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b\nFor generative tasks:\n\u2131\n\u211b\n\ud835\udcb1\nDiscriminaDve  Loss \u2112$%&\nFor discriminative tasks:\nFigure 4: Architecture and task modeling of the All-Seeing Model (ASM). ASM incorporates a\nlocation-aware image tokenizer to perform region-text alignment tasks. Image-level and region-level\nfeatures are encoded as visual tokens V, and fed into the LLM-based decoder along with the users\u2019\ntext input. ASM employs a specific prompt design that allows the LLM decoder to handle both\ngenerative tasks and discriminative tasks using a unified architecture with shared parameters. We\nadd soft prompt tokens (i.e., Pg and Pd) to indicate the desired tasks and use an \u201c\u27e8align\u27e9\u201d token to\nperform image-text alignment at the LLM\u2019s output. \u27e8bos\u27e9 denotes the beginning token of a sentence.\nwhereas the secondary objective aims to improve its discriminative and retrieval capabilities.\nLtotal = Lgen + Ldis,\n(1)\nwhere the generation loss Lgen is for the next token prediction, and is the same as the loss of GPT\nseries [60, 69, 8, 61]. The discriminative loss Ldis is for tasks like region-text aligning/retrieval. The\ndiscriminative loss follows the contrastive loss of CLIP [67], where each region is treated as an image\nwhen calculating the loss.\n4.2\nLocation-Aware Image Tokenizer\nTo achieve location-aware image tokenizing, we introduce a query-based image tokenizer that\nconditions its queries on location information, such as bounding boxes, masks, or points. As depicted\nin Fig. 4, we first encode the input image using the ViT-g/14 [26] backbone, resulting in image\nfeatures F \u2208 RH\u00d7W \u00d7D, where H and W denote the spatial size of the feature maps and D denotes\nthe feature dimension. Next, we use the RoIAlign [33] to extract the region features R \u2208 RHr\u00d7Wr\u00d7D\nfrom the image features F, according to the given bounding box (or mask, point set). Here, Hr and\nWr denote the spatial size of the RoI features. We then flatten the region features R, use two fully-\nconnection (FC) layers to project them into Qr \u2208 RG\u00d7Dq, which has the same shape as randomly\ninitialized query tokens Q\u2032 \u2208 RG\u00d7Dq. Here, G denotes the number of tokens in a query group3, and\nDq denotes the dimension of a query token. Subsequently, the Qr of N bounding boxes and Q\u2032 are\nconcatenated to form location-aware query tokens Q \u2208 R(N+1)G\u00d7Dq. These location-aware query\ntokens Q are then passed through a transformer decoder with 12 blocks to extract output features.\nFinally, the output features are projected to match the feature dimension Dt of the LLM and are\nused as the soft prompt V \u2208 R(N+1)G\u00d7Dt for subsequent decoding processes. Particularly, when\nno location information is provided, the bounding box is assumed to cover the entire image. This\nmethod guarantees a consistent approach for both local region and whole image tokenization.\n4.3\nLLM-Based Decoder\nTo develop a unified LLM-based framework that can effectively handle both generation tasks and\ndiscriminative tasks, we utilize Husky-7B [55] as our foundation language model to handle various\n3A query group represents a randomly initialized query or a set of query tokens conditioned by a bounding\nbox.\n11\nvision-language tasks under the guidance of user instructions and learnable soft prompts that contain\nimage-level and region-level visual information.\nFor generative tasks, the input sequence comprises three types of tokens, including (1) learnable\ngenerative task prompt Pg \u2208 RM\u00d7Dt, which informs the model that it should perform a generative\ntask. (2) location-aware image tokens V that contain the extracted image-level and region-level\ninformation from the input image and (3) user prompt that expresses his/her requirements. Given\nsuch an input sequence, the LLM generates text tokens sequentially in an autoregressive manner until\nan end token \u27e8eos\u27e9 is reached. An example prompt is provided below:\nPrompt #1: \u201c{Pg} \u27e8bos\u27e9 Human: {V} What is this? Assistant:\u201d ,\nwhere the token number of task prompt M is set to 5. \u27e8bos\u27e9 represents the beginning of the sentence.\nFor discriminative tasks, different from the CLIP-based framework that directly aligns the output\nfeature from vision and language encoders, we introduce a trainable align token \u27e8align\u27e9 to extract the\nholistic representation of the current input sequence. An example prompt for encoding input image is\nshown as follows:\nPrompt #2: \u201c{Pd} \u27e8bos\u27e9 Human: {V} What is this? \u27e8align\u27e9\u201d ,\nwhere Pd \u2208 RM\u00d7Dt represents the learnable task prompt used for discriminative tasks.\nSimilarly, the input sequence of input text consists of soft prompt tokens that indicate task information,\nas well as text tokens that represent the corresponding region caption or object class name. We omit\nthe vision tokens to avoid information leakage. Here is an example prompt:\nPrompt #3: \u201c{Pd} \u27e8bos\u27e9 Assistant: A photo of the Sphinx. \u27e8align\u27e9\u201d .\nDuring the process of region-text matching, we can achieve image-text retrieval by simply computing\nthe similarity of the embedding of the \u27e8align\u27e9 token. It is notable that the learnable task prompt and\nalign tokens used in Prompt #2 and #3 are shared, while the task prompt differs between generative\ntasks (Prompt #1) and discriminative tasks (Prompt #2 and #3).\nCompared to the CLIP-based framework, our LLM-based decoder offers two advantages: (1) Our\napproach builds upon LLMs, allowing us to leverage the powerful world knowledge and reasoning\ncapability of LLMs. (2) Both the image and text aligning embedding of our method are generated\nby an LLM, which bridges the gap between the pre-training task for the language model and the\nlanguage-image pre-training task.\n5\nData Analysis\nWe conduct an in-depth analysis of our AS-1B dataset. We begin by showcasing the abundance of\ndata in terms of quantity. Next, we explore the data diversity and open-world semantics captured in\nAS-1B. Finally, we thoroughly analyze the data quality of the initial automatic annotation pipeline\nand explain how we have improved it through data engineering and human feedback.\n5.1\nData Scale\nStatistics. The AS-1B dataset consists of a vast collection of 1.2 billion region-text pairs extracted\nfrom 11 million images, encompassing 3.5 million distinct semantic tags. Regions in the dataset are\ncategorized into five different resolution scales: tiny, small, medium, large, xlarge, and huge. As\nindicated in Table 2, the distribution of region resolutions follows a roughly normal distribution. Over\nhalf of the regions are on the medium or large scale. In Sec. 3.2, we utilize several region proposal\ngenerators, including SAM [42], InternImage [88], EVA-02 [25], and GLIP [51], to generate region\nproposals for the AS-1B dataset. Table 3 presents the proportion of regions provided by each model in\nthe 1.2 billion regions. SAM generates 36.4% of the regions, while the other three models contribute\nto 63.6% of the regions. Therefore, although our dataset shares images with SA-1B [42] and has a\nsimilar number of regions, the actual regions are different due to the use of diverse region proposal\ngenerators.\n12\nRegion Type\nArea Range\nProportion\n(V)LLMs\nBLIP\nInternImage\nEVA-02\nGLIP\nTiny\n< 202\n4.2%\n33.8%\n16.5%\n24.6%\n25.1%\n0.0%\nSmall\n202 \u223c 402\n8.7%\n34.5%\n14.3%\n24.6%\n25.9%\n0.7%\nMedium\n402 \u223c 1002\n35.8%\n55.6%\n22.9%\n8.3%\n11.6%\n1.7%\nLarge\n1002 \u223c 2002\n23.7%\n58.5%\n26.2%\n5.0%\n7.9%\n2.3%\nXlarge\n2002 \u223c 5002\n18.3%\n62.6%\n27.1%\n3.0%\n4.3%\n3.0%\nHuge\n> 5002\n9.5%\n69.7%\n24.9%\n1.6%\n1.2%\n2.7%\nAll\n\u2212\n100%\n55.4%\n24.0%\n8.2%\n10.4%\n2.1%\nTable 2: Region statistics and semantic sources. The percentage of semantic tags generated by\ndifferent models at each resolution are reported. LLM/VLLMs [17, 111, 48] contribute significantly\nto the semantic diversity of our dataset.\n18.6 \n25.3 \n37.5 \n40.2 \n45.0 \n52.6 \n40.4 \n23.6 \n34.7 \n51.9 \n55.2 \n58.9 \n69.0 \n53.9 \n71.3 \n62.9 \n61.5 \n61.3 \n62.0 \n68.5 \n62.5 \n77.9 \n72.0 \n65.6 \n61.6 \n60.2 \n60.1 \n65.5 \n76.5 \n67.3 \n64.4 \n65.1 \n69.9 \n66.5 \n10\n20\n30\n40\n50\n60\n70\n80\nTiny\nSmall\nMedium\nLarge\nXlarge\nHuge\nAll\nSemantic Tag accuracy (%)\n(V)LLMs\nBLIP\nInternImage\nEVA-02\nGLIP\nFigure 5: The accuracy of semantic tags from different sources. LLM/VLLMs [17, 111, 48] show\nlower accuracy than other models, especially on low resolution regions.\nEach region is also annotated with detailed question-answer pairs and a caption, which yields a total\nof 3.3 billion visual question-answering pairs and 1.2 billion detailed region captions. As seen in\nTable 4, the average token number of the answers is 16.91, while the average token number of the\ncomposited caption is 34.84. The total number of tokens in our detailed region captions amounts to\napproximately 42.2 billion. This extensive collection of detailed captions provides valuable textual\ndescriptions of regions within the images.\nComparisons. When comparing the AS-1B dataset with popular datasets containing region-level\nannotations, AS-1B stands out with a significantly larger number of regions. It has about 33\ntimes more regions than the current largest detection dataset, BigDetection [10]. While AS-1B has\nfewer images compared to close-set classification datasets [23] or vision-language datasets [72], it\ncompensates with valuable region annotations. Additionally, AS-1B offers an abundant collection\nof detailed region annotations. Compared to the largest region-level dataset, Visual Genome [43],\nAS-1B\u2019s detailed region annotation is about 1941 times larger than Visual Genome\u2019s 1.7 million pairs\nof VQA annotations and 222 times larger than its 5.4 million region captions.\n5.2\nData Diversity\nStatistics. A distinctive feature of AS-1B is its vast inclusion of open-world concepts, demonstrated\nthrough two key aspects: 1) a large number of semantic tags and 2) long and informative detailed\ndescriptions. Fig. 6 visually demonstrates the wide range of open-world concepts present in AS-1B.\nThe dataset covers diverse categories, including fine-grained categories like \u201clynx\u201d, proper nouns\nsuch as \u201cThe Sphinxs\u201d, object parts like \u201ccharging cords\u201d, and attributes like \u201cpink and white baby\ncribs\u201d. In Fig. 2, we display the frequency distribution of semantic tags, revealing a clear long-tail\npattern. The most frequent semantic tags predominantly represent broad category names, while less\nfrequent tags correspond to fine-grained category names or instances with specific attributes.\nIn Table 2, we analyze the sources of each semantic tag to understand how open-world concepts\nare enriched. We report the proportion of sources for the top-1 semantics in the semantic tags at\ndifferent scales. The results reveal that 55% of the top-1 semantic candidates are from the LLM,\nwhile 24% originate from the BLIP (the \"magnifier\" in Sec. 3.3). Interestingly, only 19% of the top-1\ncandidates are generated from the closed-set detectors, InternImage, and EVA-02. This highlights\n13\nModel\nSAM InternImage EVA-02 GLIP\nProportion 36.4%\n20.5%\n22.5% 20.6%\nTable 3: The proportion of region proposals\ngenerated by different models. Only 40% re-\ngions are generated from SAM.\nType\nNumber #Tokens Average Tokens\nQuestion\n3.3B\n34.6B\n10.50\nAnswer\n3.3B\n55.4B\n16.91\nCaption\n1.2B\n42.2B\n34.84\nTable 4: The statistics of detailed description\nin AS-1B dataset. The overall number of tokens\nreaches 132.2 billion.\nType\nCorrect answer\nWrong answer\nInvalid question\nWrong semantic\nProportion\n47.1%\n18.6%\n19.0%\n15.3%\nTable 5: The statistics of attribute question-answering. The answers generated by the \u201cresponder\"\nhad an accuracy of 47.1%. Wrong semantic denotes that the semantic tags are incorrect.\nthat the majority of concepts in the AS-1B dataset are obtained from open-world sources, especially\nthe LLMs and VLLMs.\nAs for the detailed region caption, the VQA-based generation approach in AS-1B has proven\nadvantageous, resulting in longer and more informative region descriptions. A more straight-forward\nway is to directly ask the VLLM to generate region captions. However, without guidance from\nsemantic tags and questions, the model tends to output inaccurate information or hallucinations.\nComparisons. Instead of using fixed labels from a pre-defined set, the AS-1B dataset employs\nflexible and open-world semantic tags to label each region. Table 1 highlights that AS-1B contains a\nsignificantly larger number of semantic tags and concepts compared to close-set classification datasets\nor object detection datasets. For example, the number of semantic tags in AS-1B is approximately\n159 times greater than the widely-used classification dataset ImageNet-22k [23], and it is 268 times\nlarger than the category number in V3Det [86].\n5.3\nData Quality\nThe Accuracy of Automatic Annotations. We evaluated the data quality using two metrics: top-1\naccuracy and semantic tag accuracy. Top-1 accuracy refers to the probability that the top-1 candidates\nare correct, as selected by the human annotators. On the other hand, semantic tag accuracy denotes\nthe probability the generated semantic tags are selected by the annotators. In the verified annotations,\nwe obtained a top-1 accuracy of 54.8% and a candidate accuracy of 47.0%.\nAs shown in Figure 5, we find that different models in the annotation pipeline exhibit complementary\nbehavior. The LLM and BLIP models show lower accuracy for small regions as they are not robust\nfor the cropped low-resolution images. In contrast, close-set detectors perform better on these small\nregions, providing more accurate semantic candidates. For larger regions, LLMs and VLLMs become\nmore accurate. Hence, the inclusion of close-set detectors can provide a trade-off between data\nquality and open-world semantics. This interplay of models contributes to the overall improvement\nof data quality in AS-1B.\nAs discussed in Sec. 3.5, the detailed region descriptions are also verified by human experts using\na similar procedure. The human annotators are tasked with classifying the VQA pairs into four\nsituations: 1) the question is proper, and the answer is correct; 2) the answer is incorrect; 3) the\ngenerated question is unanswerable given the image (e.g., the production date of a car); 4) the\nsemantic tag is wrong. As shown in Table 5, the accuracy of question-answer pairs is 47.1%.\nConsumption Analysis. Here we focus on the consumption and efficiency of human verification in\nthe context of the semi-automatic data engine we constructed. This approach significantly reduces\nthe human labor required for data refinement compared with annotating all the data by humans. For\nverifying semantic tags, it takes approximately 10 seconds for one annotator to complete one region.\nVerifying every 1 million regions would take about 2,750 working hours. Considering a group of\n50 annotators in our case, the entire verification process takes approximately 15 days. If we were to\n14\nannotate all regions, the annotation consumption would become 1,000 times larger, approximately 42\nyears. Such a large-scale human annotation effort would be unaffordable.\nMoreover, for detailed captions with longer texts, the verification process would take even longer,\ne.g., 15 seconds for each VQA annotation. Therefore, for large-scale annotation involving billions of\nregions in our case, utilizing models to annotate data at scale and correcting the models\u2019 bias with\nlimited human annotation proves to be both feasible and efficient.\n6\nExperiments\nWe analyze and compare the proposed ASM with a CLIP-based baseline model and leading Multi-\nmodality Large Language models (VLLMs) on representative vision tasks including zero-shot region\nrecognition, image-level caption and region-level caption. Additionally, since using conventional\nimage captioning metrics to evaluate LLM-based models can be limiting [107], we also perform\nhuman subject evaluation to compare our model with existing powerful VLLMs [111, 54].\n6.1\nImplementation Details\nTraining Setting. The training of the All-Seeing Model (ASM) involves three types of labels obtained\nfrom the AS-1B dataset, including region-level semantic tags, question-answer pairs, and detailed\ncaptions. The semantic tags are used for aligning regions with corresponding text, while the other\nannotations are used to train the text generation task. In addition, we also include LaionCOCO [71]\nin our training process, since the image-level caption data from LaionCOCO is beneficial for ASM\u2019s\nability to comprehend the whole images.\nWe adopt a multi-task training approach that combines text generation and region-text alignment tasks\nto train our ASM. The batch size for text generation is set to 256, while for region text alignment\nit is set to 32,768. We employ the AdamW optimizer [57] with the \u03b21 of 0.9, the \u03b22 of 0.999, and\nthe weight decay of 0. During training, the learning rate is initialized as 5 \u00d7 10\u22124 and includes a\nlinear warmup that lasts until the first 10% of training steps. The warmup is followed by a cosine\ndecay strategy with a minimum learning rate of 0. Unless otherwise specified, the image resolution\nfor all experiments is set to 224 \u00d7 224. We initialize the model parameters using Husky [55] and\ntrain the model for one epoch. In addition, we also provide a second-stage fine-tuning setting to\nfurther improve the effectiveness of ASM. Specifically, we utilize high-quality multi-modal data\nMiniGPT-4 [111], LLaVA-150k [54], and COCO caption dataset [15] as image-level text generation\ndata, along with VG [43] and RefCOCOg [59] datasets as region-level text data. Human-verified\nregion annotations are also included. During fine-tuning, we set the learning rate to 5 \u00d7 10\u22125 and\napply a weight decay of 0. The other settings remain the same as during pre-training. The fine-tuned\nASM is denoted as ASM-FT.\nBaseline Model. To make comparison with recent popular multi-modality large language models\n(VLLMs) [111, 54, 47] that only focus on processing the entire image, we crop a region from the\nimage and input it to these model for region-level visual recognition and understanding. However,\nthis cropping may result in the loss of some contextual information from the entire image. For better\ncomparison, we implement a simple region-text contrastive model based on CLIP [67] as a baseline.\nThe baseline model, named Region-Aware CLIP (R-CLIP), is equipped with an RoIAlign layer [33]\non the feature maps obtained from the vision encoder in the CLIP model. To initialize the model\nweights, we leverage OpenCLIP [36] (ViT-L/14) and then train the CLIP model on our AS-1B dataset.\nThe model is trained for 10, 000 steps with a batch size of 32,768. Other training settings is the same\nas those of ASM.\n6.2\nText Generation\nEvaluation Setting. We evaluate the image-level caption ability of our model on Flickr30K [97]\nand NoCaps [1] dataset. We report the CIDEr [85] and SPICE [3] metric on these benchmarks.\nTo assess the region-level caption ability, we also evaluate ASM on the Visual Genome [43] and\nRefCOCOg [59]. On the region caption task, we adopt both the Meteor [5] and CIDEr [85] metric as\nour evaluation metrics. The Meteor, CIDEr, and SPICE metrics are computed by COCOEvalCap4.\n4https://github.com/salaniz/pycocoevalcap\n15\nBurj \nkhalifa\nair traffic \ncontrol tower\npink and white \nbaby cribs\na boat lift\nThe Sphinxs\ncandle \ndrippings\nCharging cords\nEagle beak\nTrump\nlynx\nFigure 6: Examples of the semantic tags. Benefitting from the world knowledge of LLMs/VLLMs,\nthe AS-1B dataset covers diversity semantic tags in the real world.\nResults. For region-level captioning, as shown in Table 6, our ASM model surpasses the concurrent\nregion-aware VLLMs, Kosmos-2 [65], by 1.4 points on the RefCOCOg dataset, under the zero-shot\nsetting. After the second-stage fine-tuning, our ASM model has achieved a new record for referring\nexpression generation on RefCOCOg. Besides, on the Visual Genome (VG) dataset, although the\n16\nQ1:  What color is the shirt this person is wearing?\nA1: The shirt the person is wearing is yellow.\nQ2: Is the shirt tucked in or out?\nA2: The shirt is tucked in.\nQ3: What type of material is the shirt made of?\nA3: The shirt is made of a lightweight material, such \nas cotton or linen, which allows for easy movement \nand breathability.\nDetailed Caption: The person is wearing a yellow \nshirt that is tucked in and made of a lightweight \nmaterial, such as cotton or linen, which allows for \neasy movement and breathability.\nDetailed Annotation\nSemantic Tag\na yellow shirt\nSemantic Tag\nbanner\nDetailed Annotation\nQ1: What is the size of this banner?\nA1: The size of the banner is not provided in the \nimage.\nQ2: What is the material used to make this banner?\nA2: The material used to make the banner is a \ncombination of fabric and plastic.\nQ3: : What is the purpose of this banner? \nA3: : The purpose of this banner is to advertise a \nproduct or service, likely a beverage, to the public.\nDetailed Caption: The banner in the image is made \nof a combination of fabric and plastic and is used to \nadvertise a beverage product or service to the public, \nbut the size of the banner is not specified.\nDetailed Annotation\nSemantic Tag\nsmall windows\nSemantic Tag\nstatue\nDetailed Annotation\nQ1: What is the material this statue is made of?\nA1: The statue is made of bronze.\nQ2: What is the height of this statue?\nA2: The height of the statue is not provided in the \nimage.\nQ3: What is the subject of this statue?\nA3: The subject of the statue is a woman, possibly a \ngoddess or a historical figure, standing in front of a \nbuilding.\nDetailed Caption: The statue, made of bronze, \ndepicts a woman, possibly a goddess or a historical \nfigure, standing in front of a building, with no \nprovided height in the image.\nQ1: Are these small windows operable? \nA1: Yes, the small windows in the image are \noperable.\nQ2: What is the purpose of these small windows?\nA2: The purpose of these small windows is to \nprovide natural light and ventilation to the building, \nallowing fresh air to circulate and brighten up the \nspace.\nQ3: Are these small windows located on the \nexterior or interior of the building?\nA3: The small windows are located on the exterior of \nthe building.\nDetailed Caption: The statue, made of bronze, \ndepicts a woman, possibly a goddess or a historical \nfigure, standing in front of a building, with no \nprovided height in the image.\nQ1: What is the length of this barge?\nA1: The length of the barge is not provided \nin the image.\nQ2: What is the color of this barge?\nA2: The color of the barge is red.\nQ3: What is the purpose of this barge?\nA3: The purpose of this barge is to transport \npeople and goods across the river.\nDetailed Caption: The length of the barge is \nnot specified in the image, but it is red and \nis used to transport people and goods \nacross the river.\nDetailed Annotation\nSemantic Tag\nbarge\nSemantic Tag\na building\nDetailed Annotation\nQ1: What is the name of this building?\nA1: The name of the building is not provided in the \nimage.\nQ2: What is the architectural style of this building?\nA2: The architectural style of the building in the \nimage is modern and sleek, featuring a combination \nof white and gray colors and a minimalist design.\nQ3: What is the purpose of this building?\nA3: The purpose of this building is to serve as a \nresidential or commercial space for people or \nbusinesses.\nDetailed Caption: The image shows a modern and \nsleek building with a white and gray color scheme \nand minimalist design, which serves as a residential \nor commercial space for people or businesses.\nFigure 7: Examples of the detailed region annotations. Visual question-answering pairs and\ncaptions are provided based on the semantic tags. Failure cases are marked in red.\nMeteor score of zero-shot ASM is inferior to GRiT [92], ASM-FT achieves significantly better results\nthan GRiT given relevant data.\nIn addition, our model also excels at image-level captioning, as presented in Table 7, our ASM\nmodel demonstrates promising zero-shot performance on Flickr30K [97] and NoCaps [1] dataset.\nSpecifically, under the zero-shot setting, our model achieves a CIDEr score of 77.9 without the\nsecond-stage fine-tuning and 87.7 after the second-stage fine-tuning, which outperforms all the\nconcurrent VLLMs, such as InstructBLIP [22], Shikra-13B [13] and Kosmos-2 [65]. Furthermore,\non the NoCaps dataset, ASM also achieves comparable performance compared to the baselines\nunder the zero-shot setting. These results indicate that our ASM model retains a strong image-level\ncomprehension ability while also being region-aware.\nIn summary, these results highlight the strong region-level text generation capabilities of our model,\nwhile also showcasing its ability to comprehend the entire image. The promising zero-shot perfor-\nmance of ASM further demonstrates the effectiveness of our proposed AS-1B dataset. Moreover, the\nunified model structure of ASM enables it to effectively utilize diverse data sources during training,\nenhancing its overall performance.\n17\nModel\nZero-shot\nVisual Genome\nRefCOCOg\nMeteor\nCIDEr\nMeteor\nCIDEr\nGRiT [92]\n%\n17.1\n142.0\n15.2\n71.6\nSLR [99]\n%\n-\n-\n15.4\n59.2\nSLR+Rerank [99]\n%\n-\n-\n15.9\n66.2\nKosmos-2 (Few-shot,k=2) [65]\n%\n-\n-\n13.8\n62.2\nKosmos-2 (Few-shot,k=4) [65]\n%\n-\n-\n14.1\n62.3\nKosmos-2 [65]\n!\n-\n-\n12.2\n60.3\nASM\n!\n12.6\n44.2\n13.6\n41.9\nASM-FT\n%\n18.0\n145.1\n20.8\n103.0\nTable 6: Performance on the region-level captioning task. \u201c-FT\u201d denotes ASM with second-stage\nfine-tuning.\nModel\nZero-shot\nFlickr30k\nNoCap\nCIDEr\nSPICE\nCIDEr\nSPICE\nMetaVLM [32]\n!\n43.4\n11.7\n-\n-\nVinVL [103]\n!\n-\n-\n95.5\n13.5\nLEMON [34]\n!\n-\n-\n106.8\n14.1\nFlamingo-3B [2]\n!\n60.6\n-\n-\n-\nFlamingo-9B [2]\n!\n61.5\n-\n-\n-\nSimVLM [91]\n!\n-\n-\n110.3\n14.5\nCoCa [98]\n!\n-\n-\n120.6\n15.5\nBLIP [48]\n!\n-\n-\n113.2\n14.7\nBLIP-2 [47]\n!\n-\n-\n121.6\n15.8\nInstructBLIP [22]\n!\n82.8\n-\n123.1\n-\nShikra-13B [13]\n!\n73.9\n-\n-\n-\nKosmos-1 [35]\n!\n67.1\n14.5\n-\n-\nKosmos-2 [65]\n!\n66.7\n-\n-\n-\nASM (ours)\n!\n77.9\n17.3\n104.8\n14.5\nASM-FT (ours)\n!\n87.7\n18.7\n117.2\n15.6\nTable 7: Zero-shot performance on the image-level captioning tasks. Our ASM shows comparable\nor even better performance than models dedicated to image-level captioning.\n6.3\nZero-shot Region Recognition\nEvaluation Setting. We use zero-shot region recognition to evaluate the region-text alignment ability\nof our model. We use COCO [53] and LVIS [31] detection dataset for evaluation. Since our current\nfocus is not on object localization, we use the ground-truth boxes and use the model to predict the\ncategories given the corresponding texts following RegionCLIP [108]. We report the mean Average\nPrecision (mAP) metrics for this evaluation.\nResults. As shown in Table 8, both our baseline model R-CLIP and the proposed ASM achieve\npromising zero-shot region recognition performance. On the COCO dataset, R-CLIP outperforms\nthe original CLIP by 9.7 mAP, and ASM further increases the performance by 10.4 mAP. On the\nmore challenging LVIS dataset with 1,203 categories, R-CLIP outperforms CLIP by 7.7 mAP, and\nASM achieves a more significant improvement of 14.3 mAP over CLIP. These results demonstrate\nthe effectiveness of region-text data in AS-1B dataset and the proposed ASM in region-text alignment\ntasks. Notably, our ASM simultaneously performs caption and region recognition tasks with the same\nweight, showcasing its versatility and efficiency.\nThese results demonstrate that, despite the semantic tags in AS-1B contain some noise, we can\nstill fine-tune a robust region-aware CLIP model with minor modifications. The result suggests\n18\nModel\nCOCO\nLVIS\nmAP\nAPS\nAPM\nAPL\nmAP\nAPS\nAPM\nAPL\nCLIP [67]\n58.9\n50.7\n70.4\n58.3\n47.1\n40.3\n59.2\n57.4\nOpenCLIP [36]\n63.3\n47.8\n75.6\n60.9\n49.1\n37.4\n62.8\n66.5\nR-CLIP (our baseline)\n68.6\n61.4\n75.4\n79.3\n54.8\n49.3\n60.6\n66.6\nASM (ours)\n69.3\n64.3\n78.0\n71.0\n61.4\n56.7\n67.9\n69.2\nTable 8: Zero-Shot object recognition performance. We report the zero-shot recognition accuracy\non COCO and LVIS dataset. The ground-truth boxes are used for inference.\nData Scale COCO LVIS\n1M\n67.8\n54.0\n2M\n67.5\n55.0\n5M\n68.6\n54.8\nTable 9: Zero-shot object\nrecognition performance\n(mAP) with different train-\ning data scale.\nData Cleaning COCO LVIS\n%\n61.8\n46.5\n!\n67.8\n54.0\nTable 10: Zero-shot object\nrecognition performance\n(mAP) with and without\ndata cleaning.\nHuman Data Input Scale COCO LVIS\n%\n224\n67.8\n54.8\n!\n224\n70.2\n55.0\n%\n896\n76.7\n65.7\n!\n896\n80.0\n68.4\nTable 11: Zero-shot object recogni-\ntion performance (mAP) with and\nwithout fine-tuning on human-verified\nannotations.\nthat region-text data in AS-1B dataset can be beneficial in enabling the model to learn region-text\nalignment by considering both the region itself and its context.\n6.4\nData Engineering\nHere, we use quantitative results to show the impact of data quantity and data engineering. Con-\nsidering the cost of the experiment, we use our baseline model R-CLIP. We use the Zero-shot\nobject recognition metrics as in Sec. 6.3 to inspect the impact of data engineering, i.e., we use the\nground-truth boxes and use R-CLIP to determine the categories following RegionCLIP [108]. Unless\notherwise specified, we train the model with semantic tags from 1M images in the AS-1B dataset.\nData Scale up. We find that scaling up the semantic tags can be helpful for zero-shot region\nrecognition. To verify this, we train our baseline R-CLIP with different amounts of semantic tags. As\nshown in Table 9, with more training data (from 1M to 5M images), the R-CLIP model attains higher\nZero-shot object recognition performance.\nData Cleaning. Data cleaning and post-processing are important. In practice, the original data\nannotation pipeline outputs a total of 2.14 billion regions. We devise a simple data cleaning strategy:\n(1) we sample the top 100 regions with the highest CLIP score at different scales from each image in\nthe AS-1B dataset and (2) we further re-rank the semantic candidates with CLIPSeg [58], as discussed\nin Sec. 3.4. This data cleaning process will compress the original 2.14B regions into 1.2B regions.\nAs shown in Table 10, adding data cleaning can significantly improve the mAP by 6.0% and 7.5% on\nCOCO and LVIS datasets.\nHow human verification improves the model? An important part of our data engine is to improve\nthe model with human feedback. In this way, the improved model can be used to refine the initial\ndata which is automatically generated. In this section, we investigate the effectiveness of human\nverification process. We fine-tune the trained R-CLIP model with human-verified region annotations,\nand find that a small number of human labels can significantly boost the model performance.\nSpecifically, to make the most of human labels, we utilized both the positive and negative candidates\nmarked by the human annotators. When calculating the contrastive loss, for each region, we randomly\nselected one positive candidate and use all the unselected candidates as negative samples. Compared\nwith the image-to-text part in the original CLIP-style contrastive loss, each region will be compared\nwith more negative text samples. The unselected candidates can be viewed as valuable hard samples,\nindicating when the model will make mistakes.\n19\nLabel 1: flippers\nLabel 2: waves\nLabel 3: waves\nLabel 1: tires\nLabel 2: life buoy\nLabel 3: tires\nLabel 1: silver spoons\nLabel 2: reflector \nLabel 3: reflector\nLabel 1: car\nLabel 2: white car\nLabel 3: mirror\nLabel 1: front grille\nLabel 2: front grille \nLabel 3: handle\nCLIP\nR-CLIP / ASM\nASM / R-CLIP\nHuman \nVerification\nLabel 2 \nLabel 1 \nLabel 3 \ngenerate\ntrain\ngenerate\ntrain\ngenerate\nFigure 8: Visualization of the data iteration process. The iteration process improves the label\naccuracy. We visualize three types of models: (1) Label 1: labels produced the original CLIP; (2)\nLabel 2: labels produced by R-CLIP or ASM, trained with Label 1 as input data; (3) Label 3: labels\nproduced by R-CLIP or ASM which is further tuned with human verification data.\nIn practice, we use a batch size of 1024 and a learning rate of 5e-4 to fine-tune the pre-trained model\non the human data for four epochs with only 40k human verified semantic tags. Table 11 shows\nthat fine-tuning the model with human data can yield significant performance gain: +2.4 and +3.3\nCOCO mAP on the resolution of 224 and 896. This demonstrates that a small amount of human\ndata can correct the model\u2019s bias and hard cases thus improving performance. The effectiveness of\nhuman verification lays the foundation for data quality improvement in the data engine iterations.\nTo intuitively show the data quality improvements, we show the coarse labeling results for CLIP\nas well as the output of R-CLIP output before and after the human data fine-tuning in Fig. 8. The\noriginal CLIP is unreliable at lower resolutions, e.g., the reflectors and handles on the white cars\nare categorized into wrong classes. R-CLIP pre-trained on AS-1B data performs better in these\nlow-resolution areas. However, it may fail to recognize some objects due to noisy labels, e.g., labeling\nthe tires hung by the boat as a \u201clife buoy\u201d. The human data fine-tuning process can correct the\npre-trained R-CLIP.\n6.5\nHuman Evaluation\nAs discussed in ChatCaptioner [110], using conventional image caption metrics such as Meteor [5]\nand CIDEr [85] may not reliably evaluate relatively lengthy texts generated from LLM-based models.\nTo better assess the text generation ability from a human perspective, we conducted a user study.\nEvaluation Setting. In our user study, we involve a total of 5 participants to evaluate the performance\nof the All-Seeing Model (ASM) along with two other powerful VLLMs: MiniGPT4 [111], and\nLLaVA [54]. We evaluate image and region-level captioning. For the evaluation, we randomly select\n20 samples from each of the Visual Genome, RefCOCOg, COCO, and Flickr30K datasets. Partici-\npants are asked to choose the most informative captions without any factual errors or hallucination.\nAside from model outputs, we also add the ground truth captions as options, which can be viewed as\nhuman outputs.\nResults. The human evaluation results in Table 12 indicate that captions generated by our ASM\nare preferred over those from MiniGPT4 and LLaVA. While LLaVA and MiniGPT4 may produce\nlonger captions for region-level tasks (VG and RefCOCOg), they often introduce over-association,\nhallucinations, and factual errors. In contrast, ASM generates captions with moderate length and\nmore accurate information. On RefCOCOg, Flickr30K, and NoCaps datasets, ASM even outperforms\nhuman annotations with longer and more detailed captions. This is because human annotators tend\nto write short captions, while users prefer longer, detailed captions generated by ASM, which also\ncontain fewer factual errors. For image-level generation tasks, ASM produces captions with similar\nlength to those from MiniGPT4 and LLaVA but is more frequently favored by users.\nThe results clearly demonstrate the effectiveness of ASM and the AS-2B dataset. The VQA-based\nannotation pipeline provides region-specific information with less irrelevant content, reducing the\noccurrence of hallucinations. Moreover, human verification further enhances the data quality, leading\nto significantly better performance on region-level tasks.\n20\nModel\nVisual Genome\nRefCOCOg\nFlickr30K\nNoCaps\nRate\nLength\nRate\nLength\nRate\nLength\nRate\nLength\nHuman\n47.8\n13.6\n10.3\n6.3\n30.0\n16.0\n27.3\n15.1\nLLaVA [54]\n4.3\n110.8\n15.4\n100.6\n17.5\n114.0\n9.1\n108.4\nMiniGPT4 [111]\n8.7\n110.9\n15.4\n113.5\n14.2\n114.6\n13.6\n101.0\nASM (ours)\n39.2\n37.5\n46.1\n33.6\n38.3\n112.4\n50.0\n102.1\nTable 12: Human evaluation results on caption tasks. We ask the users to select the caption that\ncontains the most information regarding the image/region while does not producing any factual errors.\n7\nConclusion\nIn this paper, we present the All-Seeing (AS) Project, which develops a comprehensive system\nfor panoptic visual recognition and understanding in the open world from both dataset and model\nperspectives. In terms of data, we elaborate a semi-automatic data engine consisting of an automatic\nannotation pipeline and a human verification step. Using this data engine, we annotated the AS-1B\ndataset comprising over 1 billion region-level comprehensive annotations, with controllable costs.\nFrom the model aspect, we propose a region-aware multi-modal large language model called the\nAll-Seeing Model (ASM). The ASM utilizes a unified LLM decoder to model both region-text\nalignment and image-conditioned text generative tasks. Leveraging the AS-1B dataset and other\nhigh-quality data, ASM achieves state-of-the-art results on image and region-level tasks. We also\nfine-tune a region-aware CLIP model exclusively on the AS-1B dataset, surpassing the original CLIP\nby significant margins in region recognition. We believe that the data engine, AS-1B dataset, and\nthe ASM model proposed in the All-Seeing Project will inspire further research and development\ntowards empowering artificial intelligence systems with an \u201call-seeing eye,\u201d enabling them to achieve\na deeper understanding of the world.\nCredit Attribution of Equal Contribution Authors\nWeiyun Wang is responsible for the implementation and experiments of ASM, constructing the\ndetailed annotation data pipeline, optimizing the reasoning efficiency of LLM/VLLM-related annota-\ntion modules, refactoring the code of R-CLIP and improving its efficiency, implementing the code of\nthe open-world semantic generation, and drafting the corresponding method and experiment sections.\nMin Shi is responsible for managing the construction of the data engine, joint debugging the data\nengine, constructing the semantic tag annotation pipeline, designing data cleaning and conducting\ndata-related analysis and experiments, implementing part of the R-CLIP\u2019s code and main experiments\nof R-CLIP, implementing the open-world semantic matching, participating in the human verification\nprocess, drafting partial manuscripts and revising the manuscript.\nQingyun Li is responsible for the main part of the open-world localization, optimizing partial\nlocalization models, implementing the main code of the R-CLIP, some refining experiments, setting\nup the human evaluation platform for ASM, and drafting partial manuscripts.\nWenhai Wang is the technical manager of the AS project, responsible for the task decomposition,\nprototyping, and optimization suggestions of each part of the project, and drafted and revised the\nentire manuscript.\nZhenghang Huang is responsible for the main part of the human verification process, setting up\nthe human verification platform, implementing part of the location annotators, communicating and\nguiding the manual annotation team, and drafting partial manuscripts.\nLinjie Xing is responsible for optimizing most of the location annotator and part of the semantic\ngeneration modules, implementing part of the location annotators, reviewing part of the human\nverification results, and drafting partial manuscripts.\nSpecial acknowledgment to Xizhou Zhu and Hao Li for the preliminary idea and verification of the\nAS project.\n21\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Int. Conf. Comput.\nVis., 2019. 15, 17\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot\nlearning. Adv. Neural Inform. Process. Syst., 2022. 3, 5, 18\n[3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional\nimage caption evaluation. In Eur. Conf. Comput. Vis., 2016. 15\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Int. Conf. Comput. Vis., 2015. 5\n[5] Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for MT evaluation with improved\ncorrelation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss,\neditors, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65\u201372.\nAssociation for Computational Linguistics, 2005. 15, 20\n[6] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit\nSom, Songhao Piao, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-\nexperts. Adv. Neural Inform. Process. Syst., 2022. 5\n[7] Hangbo Bao, Wenhui Wang, Li Dong, and Furu Wei. Vl-beit: Generative vision-language pretraining.\narXiv preprint arXiv:2206.01127, 2022. 5\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdv. Neural Inform. Process. Syst., 2020. 3, 5, 7, 11\n[9] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. 4, 5\n[10] Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, and Xiangyang Xue. Bigdetection: A large-scale\nbenchmark for improved object detector pre-training. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.\n4, 6, 13\n[11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2021. 4, 5\n[12] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang,\nYu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint\narXiv:2305.13292, 2023. 5\n[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 5, 17, 18\n[14] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection.\narXiv preprint arXiv:2211.09788, 2022. 5\n[15] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015. 4, 15\n[16] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. In Int. Conf. Learn. Represent., 2023. 5\n[17] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 1, 2, 3, 6, 7, 8,\n13\n[18] EasyOCR contributors. Easyocr. https://github.com/JaidedAI/EasyOCR, 2023. 7\n[19] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS, 2023. 1, 3, 7\n22\n[20] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Be-\nnenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene\nunderstanding. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 4\n[21] Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca.\narXiv preprint arXiv:2304.08177, 2023. 3\n[22] Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat, Junqi Zhao, Weisheng Wang, Boyang Li,\nPascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. arXiv preprint arXiv:2305.06500, 2023. 1, 3, 17, 18\n[23] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., 2009. 3, 4, 5, 13, 14\n[24] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. Int. J. Comput. Vis., 88:303\u2013338, 2010. 4\n[25] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331, 2023. 2, 6, 12\n[26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong\nWang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In IEEE\nConf. Comput. Vis. Pattern Recog., 2023. 5, 11\n[27] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,\nRyan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next\ngeneration of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 5\n[28] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language\nmodeling. arXiv preprint arXiv:2101.00027, 2020. 3\n[29] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. 3\n[30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In IEEE Conf. Comput.\nVis. Pattern Recog., 2017. 5\n[31] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2019. 3, 4, 6, 18\n[32] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu\nWei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022. 18\n[33] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Int. Conf. Comput. Vis.,\n2017. 5, 11, 15\n[34] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nScaling up vision-language pre-training for image captioning. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2022. 18\n[35] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023. 3, 18\n[36] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and\nLudwig Schmidt. Openclip. July 2021. 15, 19\n[37] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning., 2021. 5\n[38] Sebastian Kalkowski, Christian Schulze, Andreas Dengel, and Damian Borth. Real-time analysis and\nvisualization of the yfcc100m dataset. In Proceedings of the 2015 workshop on community-organized\nmultimodal mining: opportunities for novel solutions, pages 25\u201330, 2015. 4\n23\n[39] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-modulated detection for end-to-end multi-modal understanding. In Int. Conf. Comput. Vis., 2021. 5\n[40] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov,\nMasakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al.\nIcdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis\nand Recognition (ICDAR), pages 1156\u20131160. IEEE, 2015. 5\n[41] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2019. 2, 5\n[42] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023. 2, 4, 5, 6, 12\n[43] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. Int. J. Comput. Vis., 123:32\u201373, 2017. 2, 4, 5, 13, 15\n[44] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 3\n[45] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship detection at scale. Int. J. Comput. Vis.,\n128(7):1956\u20131981, 2020. 4\n[46] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M Ni, and Heung-Yeung Shum. Mask\ndino: Towards a unified transformer-based framework for object detection and segmentation. In IEEE\nConf. Comput. Vis. Pattern Recog., 2023. 5\n[47] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n1, 3, 5, 15, 18\n[48] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning., 2022. 1, 2, 3, 5, 6, 7, 8, 13, 18\n[49] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. Align before fuse: Vision and language representation learning with momentum distillation. Adv.\nNeural Inform. Process. Syst., 2021. 5\n[50] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 5\n[51] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In IEEE\nConf. Comput. Vis. Pattern Recog., 2022. 2, 6, 12\n[52] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-\nimage pre-training via masking. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 5\n[53] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eur. Conf. Comput. Vis., 2014.\n3, 4, 5, 18\n[54] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023. 1, 2, 3, 5, 7, 15, 20, 21\n[55] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang\nYang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatbots\nbeyond language. arXiv preprint arXiv:2305.05662, 2023. 1, 2, 5, 6, 7, 8, 11, 15\n[56] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,\nBarret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688, 2023. 3\n[57] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn. Represent.,\n2019. 15\n24\n[58] Timo L\u00fcddecke and Alexander Ecker. Image segmentation using text and image prompts. In IEEE Conf.\nComput. Vis. Pattern Recog., 2022. 9, 19\n[59] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2016. 15\n[60] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,\nYu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv\npreprint arXiv:2305.15021, 2023. 5, 11\n[61] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 3, 5, 7, 11\n[62] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 1, 3, 5\n[63] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Adv. Neural Inform. Process. Syst., 2011. 4, 5\n[64] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Adv. Neural Inform. Process. Syst., 2022. 1\n[65] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-\n2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3,\n5, 16, 17, 18\n[66] Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Visual semantic complex network for web images. In Int.\nConf. Comput. Vis., 2013. 8\n[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning., 2021. 3, 5, 6, 9, 11, 15,\n19\n[68] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understand-\ning by generative pre-training. OpenAI, 2018. 5\n[69] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI, 2019. 3, 5, 11\n[70] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 3\n[71] Christoph Schuhman, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion\ncoco: 600m synthetic captions from laion2b-en. 2022. 3, 15\n[72] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. Adv. Neural Inform. Process. Syst., 2022. 2, 4, 5,\n13\n[73] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered\n400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 5\n[74] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365: A large-scale, high-quality dataset for object detection. In Int. Conf. Comput. Vis., 2019. 4\n[75] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Ann. Meeting of the Assoc. for\nComput. Linguistics, 2018. 5\n[76] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 5\n[77] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center\nfor Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 2023. 1, 3, 7\n25\n[78] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.\n2023. 1, 3, 7\n[79] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM,\n59(2):64\u201373, 2016. 5\n[80] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 1, 3, 7\n[81] Anne M Treisman and Garry Gelade. A feature-integration theory of attention. Cognitive psychology,\n12(1):97\u2013136, 1980. 1\n[82] Trieu H Trinh and Quoc V Le.\nA simple method for commonsense reasoning.\narXiv preprint\narXiv:1806.02847, 2018. 3\n[83] Daniel A Updegrove, Sheldon B Smith, and Wendy Rickard Bollentin. Ccnews: An online forum for\nnewsletter editors. In Proceedings of the 16th annual ACM SIGUCCS conference on user services, 1988.\n3\n[84] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classification and detection dataset. In IEEE Conf.\nComput. Vis. Pattern Recog., 2018. 3\n[85] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In IEEE Conf. Comput. Vis. Pattern Recog., 2015. 15, 20\n[86] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua\nLin. V3det: Vast vocabulary visual detection dataset. arXiv preprint arXiv:2304.03752, 2023. 4, 14\n[87] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie\nZhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric\ntasks. arXiv preprint arXiv:2305.11175, 2023. 3, 5, 7\n[88] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu,\nLewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with\ndeformable convolutions. In Int. Conf. Comput. Vis., 2023. 2, 6, 12\n[89] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. 5\n[90] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. Ann.\nMeeting of the Assoc. for Comput. Linguistics, 2022. 3\n[91] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple\nvisual language model pretraining with weak supervision. Int. Conf. Learn. Represent., 2022. 5, 18\n[92] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.\nGrit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280,\n2022. 3, 17, 18\n[93] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping\nLuo. Polarmask: Single shot instance segmentation with polar representation. In IEEE Conf. Comput. Vis.\nPattern Recog., 2020. 5\n[94] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. arXiv preprint arXiv:2303.11381, 2023. 5\n[95] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models. Int. Conf. Learn. Represent., 2022. 5\n[96] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023. 1, 5, 7\n26\n[97] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67\u201378, 2014.\n15, 17\n[98] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. Trans. Mach. Learn. Res., 2022. 3, 5, 18\n[99] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speaker-listener-reinforcer model for\nreferring expressions. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 3, 18\n[100] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie\nTang. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open,\n2:65\u201368, 2021. 3\n[101] Liu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang Sheng. Detecting curve text in the wild: New\ndataset and new solution. arXiv preprint arXiv:1712.02170, 2017. 5\n[102] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. Int. Conf. Learn.\nRepresent., 2022. 3, 7\n[103] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. Vinvl: Making visual representations matter in vision-language models. arXiv preprint\narXiv:2101.00529, 2021. 18\n[104] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv\npreprint arXiv:2303.16199, 2023. 3, 5\n[105] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601,\n2023. 5\n[106] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022. 3\n[107] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng,\nRunpei Dong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring\ninstruction tuning. arXiv preprint arXiv:2307.09474, 2023. 5, 15\n[108] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei\nZhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image\npretraining. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 18, 19\n[109] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nSemantic understanding of scenes through the ade20k dataset. Int. J. Comput. Vis., 127:302\u2013321, 2019. 4\n[110] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny.\nChatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint\narXiv:2303.06594, 2023. 5, 20\n[111] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023. 1, 2, 3, 5, 6, 7, 13, 15, 20, 21\n[112] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei\nLu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments\nvia large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144,\n2023. 5\n[113] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In Int. Conf. Learn. Represent., 2021. 5\n[114] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-\nperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In\nIEEE Conf. Comput. Vis. Pattern Recog., 2022. 5\n27\n[115] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in\nimages. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 5\n[116] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies\nand reading books. In Int. Conf. Comput. Vis., 2015. 3\n28\n"
  },
  {
    "title": "DETR Doesn't Need Multi-Scale or Locality Design",
    "link": "https://arxiv.org/pdf/2308.01904.pdf",
    "upvote": "6",
    "text": "DETR Doesn\u2019t Need Multi-Scale or Locality Design\nYutong Lin1\u2020\nYuhui Yuan2\u2020\nZheng Zhang2\u2020\nChen Li1\nNanning Zheng1\nHan Hu2\u2020\n1Xi\u2019an Jiaotong University\n2Microsoft Research Asia\nAbstract\nThis paper presents an improved DETR detector that\nmaintains a \u201cplain\u201d nature: using a single-scale feature\nmap and global cross-attention calculations without spe-\ncific locality constraints, in contrast to previous leading\nDETR-based detectors that reintroduce architectural induc-\ntive biases of multi-scale and locality into the decoder. We\nshow that two simple technologies are surprisingly effective\nwithin a plain design to compensate for the lack of multi-\nscale feature maps and locality constraints.\nThe first is\na box-to-pixel relative position bias (BoxRPB) term added\nto the cross-attention formulation, which well guides each\nquery to attend to the corresponding object region while\nalso providing encoding flexibility. The second is masked\nimage modeling (MIM)-based backbone pre-training which\nhelps learn representation with fine-grained localization\nability and proves crucial for remedying dependencies on\nthe multi-scale feature maps. By incorporating these tech-\nnologies and recent advancements in training and problem\nformation, the improved \u201cplain\u201d DETR showed exceptional\nimprovements over the original DETR detector. By leverag-\ning the Object365 dataset for pre-training, it achieved 63.9\nmAP accuracy using a Swin-L backbone, which is highly\ncompetitive with state-of-the-art detectors which all heavily\nrely on multi-scale feature maps and region-based feature\nextraction. Code will be available at https://github.com/\nimpiga/Plain-DETR.\n1. Introduction\nThe recent revolutionary advancements in natural lan-\nguage processing highlight the importance of keeping\ntask-specific heads or decoders as general, simple, and\nlightweight as possible, and shifting main efforts towards\nbuilding more powerful large-scale foundation models [37,\n11, 2].\nHowever, the computer vision community often\ncontinues to focus heavily on the tuning and complexity\nof task-specific heads, resulting in designs that are increas-\ningly heavy and complex.\n\u2020Equal contribution. \u0000 {yuhui.yuan, hanhu}@microsoft.com\nSwin Small\nSwin Large\n35\n40\n45\n50\n55\n60\n50.3\n37.2\n50.9\n54.2\n42.7\n55.7\n+13.7\n+13.0\nDeformable DETR (local + m.s.)\nPlain DETR (global + s.s.)\nImproved Plain DETR (global + s.s.)\nPlain DETR (global + s.s.)\nImproved Plain DETR (global + s.s.)\nFigure 1: We improve the plain DETR detectors, which\nrely on global cross-attention calculation and single-scale\n(s.s.) feature maps, by huge margins, using both Swin-S\nand Swin-L backbones. It makes plain DETRs as compet-\nitive as the present leading DETR detectors based on local\ncross-attention and multi-scale (m.s.) feature maps.\nThe development of DETR-based object detection meth-\nods follows this trajectory. The original DETR approach [4]\nis impressive in that it discarded complex and domain-\nspecific designs such as multi-scale feature maps and\nregion-based feature extraction that require a dedicated un-\nderstanding of the specific object detection problem. Yet,\nsubsequent developments [55, 54] in the field have reintro-\nduced these designs, which do improve training speed and\naccuracy but also contravene the principle of \u201cfewer induc-\ntive biases\u201d [13].\nIn this work, we aim to improve upon the original DETR\ndetector, while preserving its \u201cplain\u201d nature: no multi-scale\nfeature maps, no locality design for cross-attention calcu-\nlation. This is challenging as object detectors need to han-\ndle objects of varying scales and locations. Despite the lat-\nest improvements in training and problem formulation, as\nshown in Table 1, the plain DETR method still lags greatly\nbehind state-of-the-art detectors that utilize multi-scale fea-\nture maps and regional feature extraction design.\nSo, how can we compensate for these architectural \u201cin-\nductive biases\u201d in addressing multi-scale and arbitrarily\nlocated objects?\nOur exploration found that two sim-\nple technologies, though not entirely new, were surpris-\ningly effective in this context: box-to-pixel relative posi-\n1\narXiv:2308.01904v1  [cs.CV]  3 Aug 2023\ntion bias (BoxRPB) and masked image modeling (MIM)\npre-training. BoxRPB is inspired by the relative position\nbias (RPB) term in vision Transformers [34, 33] which en-\ncodes the geometric relationship between pixels and en-\nhances translation invariance. BoxRPB extends RPB to en-\ncode the geometric relationship between 4d- boxes and 2d-\npixels. We also present an axial decomposition approach for\nefficient computation, with no loss of accuracy compared to\nusing the full term. Our experiments show that the BoxRPB\nterm can well guide the cross-attention computation to be\nwell dedicated to individual objects (see Figure 4, and it dra-\nmatically improves detection accuracy by +8.9 mAP over a\nplain DETR baseline of 37.2 mAP on the COCO benchmark\n(see Table 2).\nThe utilization of MIM pre-training is another crucial\ntechnology in enhancing the performance of plain DETR.\nOur results demonstrate also a significant improvement of\n+7.4 mAP over the plain DETR baseline (see Table 2),\nwhich may be attributed to its fine-grained localization ca-\npability [49].\nWhile MIM pre-training has been shown\nto moderately improve the performance of other detec-\ntors [20, 50], its impact in plain settings is profound. Fur-\nthermore, the technology has proven to be a key factor\nin eliminating the necessity of using multi-scale feature\nmaps from the backbones, thereby expanding the findings\nin [28, 15] to detectors that utilize hierarchical backbones\nor single-scale heads.\nBy incorporating these technologies and the latest im-\nprovements in both training and problem formulation, our\nimproved \u201cplain\u201d DETR has demonstrated exceptional im-\nprovements over the original DETR detector, as illustrated\nin Figure 1. Furthermore, our method achieved an accuracy\nof 63.9 mAP when utilizing the Object365 dataset for pre-\ntraining, making it highly competitive with state-of-the-art\nobject detectors that rely on multi-scale feature maps and\nregion-based feature extraction techniques, such as cascade\nR-CNN [33] and DINO [54], among others.\nBeyond these outcomes, our methodology exemplifies\nhow to minimize the architectural \u201cinductive bias\u201d when\ndesigning an effective task-specific head or decoder, as op-\nposed to relying on detection-specific multi-scale and local-\nized designs. Our study hopes to inspire future research on\nusing generic plain decoders, such as that of DETR, for a\nwider range of visual problems with minimal effort, thus\nallowing the field to shift more energy to developing large\nfoundation visual models, similar to what occurs in the field\nof natural language processing.\n2. A Modernized Plain DETR Baseline\n2.1. A Review of the Original DETR\nThe original DETR detector [4] is consist of 3 sub-\nnetworks:\n\u2022 A backbone network Fb to extract image features from\nan image. We denote the input image as I\u2208RH\u00d7W\u00d73.\nThe backbone network can provide multi-scale feature\nmaps C2, C3, C4, C5, if a convectional ConvNet is\nused, i.e., ResNet [22]. The spatial resolutions are typ-\nically 1/42, 1/82, 1/162, and 1/322 of the input im-\nage. The original DETR detector used the mainstream\nbackbone architecture at the time, ResNet, as its back-\nbone network, and either an original ResNet or a vari-\nant with a dilated stage 5 network is used. Now the\nmainstream backbone network has evolved to vision\nTransformers, which will be used in our experiments,\ne.g., Swin Transformer [34].\n\u2022 A Transformer encoder Fe to enhance the image fea-\ntures. It applies on P5 \u2208 R\nHW\n322 \u00d7C (C=256), obtained\nvia a linear projection on C5. The Transformer en-\ncoder usually consists of several stacking Transformer\nblocks, i.e., 6 in the original DETR.\n\u2022 A global Transformer decoder Fd to decode object\nbounding boxes from the image feature map using\na set of randomly initialized object queries Q =\n{q0, q1, \u00b7 \u00b7 \u00b7 , qn}. The Transformer decoder also usu-\nally consists of multiple layers, with each layer in-\ncluding a self-attention block, a cross-attention block,\nand a feed-forward block. Each of the decoder layers\nwill produce a set of objects with labels and bounding\nboxes, driven by a set matching loss.\nThe DETR framework possesses several merits, includ-\ning: 1) Conceptually straightforward and generic in applica-\nbility. It views object detection as a pixel-to-object \u201ctrans-\nlation\u201d task, with a generic notion of decoding image pixels\ninto problem targets. 2) Requiring minimal domain knowl-\nedge, such as custom label assignments and hand-designed\nnon-maximum suppression, due to the use of an end-to-end\nset matching loss. 3) Being plain, avoiding domain-specific\nmulti-scale feature maps and region-based feature extrac-\ntion.\nIn the following, we will first build an enhanced DETR-\nbased detector by incorporating recent advancements re-\ngarding both training and problem formulation, while main-\ntaining the above nice merits.\n2.2. An Enhanced Plain DETR Baseline\nBasic setup. Our basic setup mostly follows the original\nDETR framework, except for the following adaptations: 1)\nWe use a stronger Swin-T backbone, instead of the original\nResNet50 backbone; 2) We create a feature map of P4 from\nC5 by deconvolution, instead of adding dilation operations\nto the last stage of the backbone, for simplicity purpose. 3)\nWe set the number of queries as 300, and the dropout ratio\nof the Transformer decoder as 0. 4) We use 1\u00d7 scheduler\n2\nsettings (12 epochs) for efficient ablation study. As shown\nin Table 1, this basic setup produces a 22.5 mAP on COCO\nval.\nIn the following, we incorporate some recent advance-\nments in training and problem formulation into the basic\nsetup, and gradually improve the detection accuracy to 37.2\nmAP, as shown in Table 1.\nMerging Transformer encoder into the backbone. The\nbackbone network and Transformer encoder serve the same\npurpose of encoding image features. We discovered that\nby utilizing a Vision Transformer backbone, we are able\nto consolidate the computation budget of the Transformer\nencoder into the backbone, with slight improvement, prob-\nably because more parameters are pre-trained. Specifically,\nwe employed a Swin-S backbone and removed the Trans-\nformer encoder. This method resulted in similar computa-\ntion FLOPs compared to the original Swin-T plus 6-layer\nTransformer encoder. This approach simplifies the overall\nDETR framework to consist of only a backbone (encoder)\nand a decoder network.\nFocal loss for better classification. We follow [55] to uti-\nlize focal loss [30] to replace the default cross-entropy loss,\nwhich improves the detection accuracy significantly from\n23.1 mAP to 31.6 mAP.\nIterative refinement. We follow the iterative refinement\nscheme [43, 55, 3] to make each decoder layer predict the\nbox delta over the latest bounding box produced by the pre-\nvious decoder layer, unlike the original DETR that uses\nindependent predictions within each Transformer decoder\nlayer. This strategy improves the detection accuracy by +1.5\nmAP to reach 33.1 mAP.\nContent-related query. We follow [55] to generate ob-\nject queries based on image content.\nThe top 300 most\nconfident predictions are selected as queries for the subse-\nquent decoding process. A set matching loss is used for\nobject query generation, thereby maintaining the merit of\nno domain-specific label assignment strategy. This modi-\nfication resulted in a +0.9 mAP improvement in detection\naccuracy, reaching 34.0 mAP.\nLook forward twice.\nWe incorporate the look forward\ntwice scheme [54, 26] to take advantage of the refined\nbox information from previous Transformer decoder layers,\nthereby more effectively optimizing the parameters across\nadjacent Transformer decoder layers.\nThis modification\nyields +0.8 mAP improvements.\nMixed query selection. This method [54] combines the\nstatic content queries with image-adaptive position queries\nto form better query representations. It yields +0.4 mAP\nimprovements.\nHybrid matching. The original one-to-one set matching is\nless efficacy in training positive samples. There have been\nMTE\nFL\nIR\nTS\nLFT\nMQS\nHM\nAP\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n22.5\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n23.1\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n31.6\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n33.1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n34.0\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n34.8\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n35.2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n37.2\nTable 1: Preliminary ablation results on the effect of each fac-\ntor that is used to modernize plain DETR. MTE: merging the\nTransformer encoder. FL: classification loss as a focal loss. IR:\nIterative refinement. TS: two-stage. LFT: look forward twice.\nMQS: mixed query selection. HM: hybrid matching.\nseveral methods to improve the efficacy through an auxil-\niary one-to-many set matching loss [26, 6, 27]. We opted\nfor the hybrid matching approach [26], as it preserves the\nadvantage of not requiring additional manual labeling noise\nor assignment designs. This modification resulted in a +2.0\nmAP improvement in detection accuracy, achieving a final\n37.2 mAP.\n3. Box-to-Pixel Relative Position Bias\nIn this section, we introduce a simple technology, box-\nto-pixel relative position bias (BoxRPB), that proves critical\nto compensate for the lack of multi-scale features and the\nexplicit local cross-attention calculations.\nThe original DETR decoder adopts a standard cross-\nattention computation:\nO = Softmax(QKT)V + X,\n(1)\nwhere X and O are the input and output features of each\nobject query, respectively; Q, K and V are query, key, and\nvalue features, respectively.\nAs will be shown in Figure 4, the original cross-attention\nformulation often attends to irrelevant image areas within a\nplain DETR framework. We conjecture that this may be a\nreason for its much lower accuracy than that with multi-\nscale and explicit locality designs.\nInspired by the suc-\ncess of pixel-to-pixel relative position bias for vision Trans-\nformer architectures [34, 33], we explore the use of box-\nto-pixel relative position bias (BoxRPB) for cross-attention\ncalculation:\nO = Softmax(QKT + B)V + X,\n(2)\nwhere B is the relative position bias determined by the ge-\nometric relationship between boxes and pixels.\nDifferent from the original relative position bias (RPB)\nwhich is defined on 2d- relative positions, the BoxRPB\n3\nneeds to handle a larger geometric space of 4d. In the fol-\nlowing, we introduce two implementation variants.\nA Naive BoxRPB implementation.\nWe adapt the con-\ntinuous RPB method\n[33] to compute the 4d- box-to-\npixel relative position bias. The original continuous RPB\nmethod [33] produces the bias term for each relative posi-\ntion configuration by a meta-network applied on the cor-\nresponding 2d- relative coordinates. When extending this\nmethod for BoxRPB, we use the top-left and bottom-right\ncorners to represent a box and use the relative positions\nbetween these corner points and the image pixel point as\ninput to the meta-network.\nDenote the relative coordi-\nnates as (\u2206x1, \u2206y1) \u2208 RK\u00d7H\u00d7W\u00d72 and (\u2206x2, \u2206y2) \u2208\nRK\u00d7H\u00d7W\u00d72, the box-to-pixel relative position bias can be\ndefined as:\nB = MLP(\u2206x1, \u2206y1, \u2206x2, \u2206y2),\n(3)\nwhere B is in a shape of RK\u00d7WH\u00d7M, with M de-\nnoting the number of attention heads, K denoting the\nnumber of predicted bounding boxes, W, H denoting\nthe width and height of the output feature maps; the\nMLP network consists of two linear layers: Linear \u2192\nReLU\n\u2192\nLinear.\nThe input/output shapes of these\ntwo linear layers are: K\u00d7H\u00d7W\u00d74\u2192K\u00d7H\u00d7W\u00d7256 and\nK\u00d7H\u00d7W\u00d7256\u2192K\u00d7H\u00d7W\u00d7M, respectively.\nOur experiments show that this naive implementation al-\nready performs very effectively, as shown in Table 3a. How-\never, it will consume a lot of GPU computation and memory\nbudget and thus is not practical.\nA decomposed BoxRPB implementation.\nNow, we\npresent a more efficient implementation of BoxRPB. In-\nstead of directly computing the bias term for a 4d- input, we\nconsider decomposing the bias computation into two terms:\nB = unsqueeze(Bx, 1) + unsqueeze(By, 2),\n(4)\nwhere Bx \u2208 RK\u00d7W\u00d7M and By \u2208 RK\u00d7H\u00d7M are the biases\nregarding x- axis and y- axis, respectively. They are com-\nputed as:\nBx = MLP1(\u2206x1, \u2206x2),\nBy = MLP2(\u2206y1, \u2206y2),\n(5)\nThe overall process of the decomposed BoxRPB imple-\nmentation is also illustrated in Figure 2.\nThe input/out-\nput shapes of the two linear layers within MLP1 are:\nK\u00d7W\u00d72\u2192K\u00d7W\u00d7256 and K\u00d7W\u00d7256\u2192K\u00d7W\u00d7M, re-\nspectively. Similarly, the input/output shapes for the two\nlinear layers within MLP2 follow the same pattern.\nThrough decomposition, both the computation FLOPs\nand memory consumption are significantly reduced, while\nthe accuracy almost keeps, as shown in Table 3a.\nThis\ndecomposition-based implementation is used default in our\nexperiments.\nFigure 4 shows the effect of this additional BoxRPB term\nfor cross-attention computation. In general, the BoxRPB\nterm makes the attention focused more on the objects\nand box boundaries, while the cross-attention without the\nBoxRPB may attend to many irrelevant areas. This may\nexplain the significantly improved accuracy (+8.9 mAP) by\nthe BoxRPB term, as shown in Table 2.\n4. More Improvements\nIn this section, we introduce two other technologies that\ncan additionally improve the plain DETR framework.\nMIM pre-training. We leverage the recent advances of\nmasked image modeling pre-training[1, 20, 51, 28] which\nhave shown better locality[49]. Specifically, we initialize\nthe Swin transformer backbone with SimMIM pre-trained\nweights that are learned on ImageNet without labels as\nin[51].\nAs shown in Table 2, the MIM pre-trainig brings +7.4\nmAP improvements over our plain DETR baseline. The\nprofound gains of MIM pre-training on the plain DETR\nframework than on other detectors may highlight the im-\nportance of the learned localization ability for a plain DETR\nframework. On a higher baseline where BoxRPB has been\ninvolved, the MIM pre-training can still yield +2.6 mAP\ngains, reaching 48.7 mAP. Moreover, we note that MIM pre-\ntraining is also crucial for enabling us abandon the multi-\nscale backbone features with almost no loss of accuracy, as\nshown by Table 5b and 5c.\nBounding box regression with re-parameterization. An-\nother improvement we would like to highlight is the bound-\ning box re-parameterization when performing bounding box\nregression.\nThe original DETR framework [4] and most of its vari-\nants directly scale the box centers and sizes to [0,1]. It will\nface difficulty in detecting small objects due to the large\nobjects dominating the loss computation. Instead, we re-\nparameterize the box centers and sizes of l-th decoder layer\nas:\ntl\nx = (gx \u2212 pl\u22121\nx\n)/pl\u22121\nw ,\ntl\ny = (gy \u2212 pl\u22121\ny\n)/pl\u22121\nh\n,\ntl\nw = log(gw/pl\u22121\nw ),\ntl\nh = log(gh/pl\u22121\nh\n)\n(6)\nwhere pl\u22121\nx\n/pl\u22121\ny\n/pl\u22121\nw /pl\u22121\nh\nare the predicted unnormalized\nbox positions and sizes of (l\u22121)-th decoder layer.\nTable 2 shows that this modification can enhance the\noverall detection performance by +2.2 AP. Especially, it\nachieves a larger +2.9 AP improvements on small objects.\n4\nH\nW\nBox-RPB Attention\n-2\n-1\n1\n2\n3\n-4\n-3\n-2\n-1\n1\nMLP\n-4\n-3\n-2\n-1\n1\n2\n3\n4\n5\n6\n7\nMLP\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n1\n2\n3\n4\nAxial Decomposition\nBox Query \nFigure 2: Illustrating the details of the proposed BoxRPB scheme. (Left): The black grid represents an input image. The blue sketch\nregion represents a predicted bounding box. We mark the top-left and right-down corners of the box with red stars. (Middle): Our BoxRPB\ncalculates the offsets between all positions and the two corners along both x-axis and y-axis. Then, we concatenate the offset vectors along\neach axis to form (\u2206x1, \u2206x2) and (\u2206y1, \u2206y2) and apply an independent MLP to obtain the relative position bias terms Bx and By.\n(Right): We broadcast and add Bx to By to get the 2D relative bias term B. We color the positions with higher attention values with red\ncolor and blue color otherwise.\nBoxRPB\nMIM\nreparam.\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\n\u2717\n\u2717\n\u2717\n37.2\n63.7\n37.6\n17.8\n40.5\n55.6\n\u2713\n\u2717\n\u2717\n46.1\n67.6\n49.1\n27.2\n50.5\n64.9\n\u2717\n\u2713\n\u2717\n44.6\n67.0\n48.3\n26.9\n49.1\n59.1\n\u2717\n\u2713\n\u2713\n46.3\n68.2\n51.1\n30.7\n51.0\n58.4\n\u2713\n\u2713\n\u2717\n48.7\n67.7\n53.0\n31.3\n53.1\n63.0\n\u2713\n\u2713\n\u2713\n50.9\n69.3\n55.5\n34.2\n55.1\n65.5\nTable 2: Core ablation results of the proposed components.\nEquipped with these components, a plain DETR could achieve\ncompetitive performance.\n5. Ablation Study and Analysis\n5.1. The importance of box relative position bias\nIn Table 3, we study the effect of each factor within our\nBoxRPB scheme and report the detailed comparison results\nin the following discussion.\nEffect of axial decomposition. Modeling the 2D relative\nposition without any decomposition is a naive baseline com-\npared with our axial decomposition schema, and it can be\nparameterized as (\u2206x1, \u2206y1, \u2206x2, \u2206y2) \u2208 RK\u00d7H\u00d7W\u00d74.\nThis baseline requires a quadratic computation overhead\nand memory consumption while the decomposed one de-\ncreases the cost to linear complexity. In Table 3a, we com-\npared the two approaches and find that the axial decom-\nposition scheme achieves comparable performance (50.9\nvs. 50.8) while it requires a much lower memory footprint\n(9.5G vs. 26.8G) and smaller computation overhead (5.8G\nFLOPs vs. 265.4G FLOPs).\nEffect of box points. Table 3b shows the comparison of\nusing only the center points or the two corner points. We\nfind that applying only the center points improves the base-\nline (fourth row of Table 2) by +1.7 AP. However, its per-\nformance is worse than that of using two corner points. In\nparticular, while the two methods achieve comparable AP50\nresults, utilizing corner points could boost AP75 by +2.2.\nThis shows that not only the position (center) but also the\nscale (height and width) of the query box are important to\nprecisely model relative position bias.\nEffect of hidden dimension. We study the effect of the\nhidden dimension in Equation 5. As shown in Table 3c, a\nsmaller hidden dimension of 128 would lead to a perfor-\nmance drop of 0.5, indicating that the position relation is\nnon-trivial and requires a higher dimension space to model.\nComparison with other methods. We study the effect of\nchoosing other schemes to compute the modulation term B\nin Equation 2. We compared to several representative meth-\nods as follows: (i) Conditional cross-attention scheme [35],\nwhich computes the modulation term based on the inner\nproduct between the conditional spatial (position) query\nembedding and the spatial key embedding. (ii) DAB cross-\nattention scheme [31], which builds on conditional cross-\nattention and further modulates the positional attention map\nusing the box width and height information. (iii) Spatially\nmodulated cross-attention scheme (SMCA) [16], which de-\nsigns handcrafted query spatial priors, implemented with a\n2D Gaussian-like weight map, to constrain the attended fea-\ntures to be around the object queries\u2019 initial estimations.\nTable 3d reports the detailed comparison results. Our ap-\nproach achieves the best performance among all the meth-\nods.\nSpecifically, the conditional cross-attention module\nachieves similar performance with our center-only setting\n(first row of Table 3b). DAB cross-attention and SMCA are\nslightly better than the conditional cross-attention module,\nbut they still lag behind the BoxRPB by a gap of 2.5 AP and\n2.2 AP, respectively.\nWe also compare BoxRPB with DAB cross-attention\nbased on its official open-source code. Replacing DAB po-\nsitional module with BoxRPB achieves a +1.8 mAP perfor-\nmance gain.\n5.2. Comparison with local attention scheme\nIn this section, we compared our global attention schema\nwith other representative local cross-attention mechanisms,\n5\ndecomp. mem. GFLOPs AP AP50 AP75\n\u2717\n26.8G\n265.4\n50.8 69.3 55.4\n\u2713\n9.5G\n5.8\n50.9 69.3 55.5\n(a) axial decomposition.\nbox points\nAP AP50 AP75\ncenter\n48.0 69.0 53.3\n2\u00d7corners 50.9 69.3 55.5\n(b) box points.\nhidden dim. AP AP50 AP75\n128\n50.4 69.1 55.1\n256\n50.9 69.4 55.4\n512\n50.9 69.3 55.5\n(c) hidden dim.\nmethod\nAP AP50 AP75\nstandard cross attn.\n46.3 68.2 51.1\nconditional cross attn. 48.3 68.8 52.9\nDAB cross attn.\n48.4 68.9 53.4\nSMCA cross attn.\n48.7 69.2 53.6\nours\n50.9 69.3 55.5\n(d) cross-attention modulation.\nTable 3: Ablation of box relative position bias scheme. (a) Axial decomposition can significantly decrease the computation overhead\nand GPU memory footprint. (b) The corner points perform better than the center point. (c) The higher the hidden dimension, the better\nperformance. (d) Our approach performs much better than other related methods designed to modulate the cross-attention maps.\n1/8\n1/32\n1/16\n1/32\n1/4\n1/16\n1/8\nTransformer Decoder\n(a) (C3,C4,C5) \u2192 (P3, P4, P5)\n1/8\n1/32\n1/16\n1/32\n1/4\n1/16\n1/8\nTransformer Decoder\n(b) (C3,C4,C5) \u2192 P4\n1/8\n1/32\n1/16\n1/4\n1/16\nTransformer Decoder\n(c) C5 \u2192 P4\nFigure 3: We compare the architecture designs when using different feature maps output by the backbone and sent to the Transformer\ndecoder. From (a) to (b), we simplify the dependency on sending multi-scale feature maps to the Transformer decoder. From (b) to (c), we\nremove the dependency on fusing multi-scale feature output by the backbone. We adopt (c) as our default architecture setting.\nmethod\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\ndeformable cross attn.\n50.2\n68.5\n54.8\n34.1\n54.4\n63.3\nRoIAlign\n49.6\n68.3\n54.1\n31.9\n54.2\n63.5\nRoI Sampling\n49.3\n68.2\n53.8\n33.1\n53.2\n63.0\nBox Mask\n48.6\n68.7\n52.9\n31.8\n52.7\n63.0\nOurs\n50.9\n69.3\n55.5\n34.2\n55.1\n65.5\nTable 4:\nComparison with local cross-attention scheme.\nGlobal cross-attention with BoxRPB outperforms all the local\ncross-attention counterparts and have a significant gain on large\nobjects.\nincluding deformable cross-attention [55], RoIAlign [21],\nRoI Sampling (sampling fixed points inside the Region of\nInterest), and box mask inspired by [7]. We illustrate the\nkey differences between those methods in the supplemen-\ntary material.\nAs shown in Table 4, our method surpasses all the local\ncross-attention variants. In addition, we observed that large\nobjects have larger improvements for our method. A similar\nobservation is also reported in DETR [4], it may be due to\nmore effective long-range context modeling based on the\nglobal attention scheme.\n5.3. On MIM pre-training\nWe explore different ways of using the backbone and de-\ncoder feature maps with or without MIM pre-training. We\nevaluate the performance of three different architecture con-\nfigurations, which are illustrated in Figure 3. We discuss\nand analyze the results as follows.\nbackbone \u2192 decoder\nMIM AP AP50 AP75 APS APM APL\n(C3,C4,C5) \u2192 (P3, P4, P5)\n\u2717\n49.6 69.2 53.8 31.5 53.4 65.2\n(C3,C4,C5) \u2192 (P3, P4, P5)\n\u2713\n51.1 69.3 56.0 34.8 55.4 65.2\n(a)\nbackbone \u2192 decoder MIM\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\n(C3,C4,C5) \u2192 P5\n\u2717\n47.0\n68.2\n50.4\n28.0\n51.5\n64.2\n(C3,C4,C5) \u2192 P4\n\u2717\n49.6\n69.8\n53.4\n31.4\n53.7\n65.5\n(C3,C4,C5) \u2192 P3\n\u2717\n49.7\n69.8\n53.9\n32.7\n53.5\n65.2\n(C3,C4,C5) \u2192 P5\n\u2713\n50.3\n69.3\n54.9\n33.4\n54.7\n64.9\n(C3,C4,C5) \u2192 P4\n\u2713\n51.0\n69.4\n55.7\n34.5\n55.1\n65.2\n(C3,C4,C5) \u2192 P3\n\u2713\n50.9\n69.2\n55.4\n34.4\n55.0\n64.5\n(b)\nbackbone \u2192 decoder MIM\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nC5 \u2192 P5\n\u2717\n46.4\n67.7\n49.7\n26.9\n50.5\n64.4\nC5 \u2192 P4\n\u2717\n48.0\n68.7\n51.8\n30.4\n52.2\n64.4\nC5 \u2192 P3\n\u2717\n48.7\n69.1\n52.6\n30.7\n52.9\n64.9\nC5 \u2192 P5\n\u2713\n50.2\n69.1\n55.0\n33.5\n54.5\n64.6\nC5 \u2192 P4\n\u2713\n50.9\n69.3\n55.5\n34.2\n55.1\n65.5\nC5 \u2192 P3\n\u2713\n50.9\n69.2\n55.7\n34.6\n54.9\n65.0\n(c)\nTable 5: Ablation of MIM pre-training. (a) multi-scale feature\nmaps output by the backbone + multi-scale feature maps for the\nTransformer decoder. (b) multi-scale feature maps output by the\nbackbone + single-scale feature map for the Transformer decoder.\n(c) single-scale feature map output by the backbone + single-scale\nfeature map for the Transformer decoder.\nMIM pre-training brings consistent gains. By comparing\nthe experimental results under the same architecture config-\n6\nmethod\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nCascade Mask R-CNN[3]\n53.7\n71.9\n58.7\n36.9\n57.4\n69.1\nOurs\n53.8\n73.4\n58.9\n35.9\n57.0\n68.9\nTable 6: Comparison of the improved plain DETR and Cas-\ncade Mask R-CNN with a MIM pre-trained ViT-Base back-\nbone. Our plain DETR with global cross-attention is slightly bet-\nter than the region-based, multi-scaled Cascade Mask R-CNN.\nuration, we found that using MIM pre-training consistently\nachieves better performance. For example, as shown in Ta-\nble 5, using MIM pre-training outperforms using supervised\npre-training by 1.5 AP in the(C3,C4,C5) \u2192 (P3, P4, P5)\nconfiguration and 2.9 AP in the C5 \u2192 P4 configuration.\nMulti-scale feature maps for the decoder can be re-\nmoved. By comparing the results between Table 5a and\nTable 5b, we found that using high-resolution feature maps\ncan match or even surpass the performance of using multi-\nscale feature maps.\nFor example, (C3,C4,C5) \u2192 P3\nachieves comparable performance with (C3,C4,C5) \u2192\n(P3, P4, P5) with or without using MIM pre-training.\nThis observation is not trivial as most existing detection\nheads still require multi-scale features as input, and it makes\nbuilding a competitive single-scale plain DETR possible.\nWe hope this finding could ease the design of future de-\ntection frameworks.\nMulti-scale feature maps from the backbone are non-\nnecessary. We analyze the effect of removing the multi-\nscale feature maps from the backbone by comparing the\nresults of Table 5b and Table 5c.\nWhen using a super-\nvised pre-trained backbone, adopting only the last feature\nmap C5 from the backbone would hurt the performance.\nFor example, when using the supervised pre-trained back-\nbone, the C5 \u2192 P5 reaches 46.4 AP, which is worse than\n(C3,C4,C5) \u2192 P5 (47.0 AP) by 0.6 AP. However, when\nusing the MIM pre-trained backbone, C5 \u2192 P5 reaches\n50.2 mAP, which is comparable with the performance of\n(C3,C4,C5) \u2192 P5 (50.3 AP). These results show that MIM\npre-training can reduce the reliance on multi-scale feature\nmaps.\nSingle-scale feature map from the backbone and single-\nscale feature map for the decoder is enough. Based on\nthe above observations, we can reach a surprisingly simple\nbut important conclusion that we can completely eliminate\nthe need for multi-scale feature maps in both the backbone\nand Transformer decoder by using our proposed BoxRPB\nscheme and MIM pre-training.\n5.4. Application to a plain ViT\nIn this section, we build a simple and effective fully\nplain object detection system by applying our approach\nFigure 4: Visualizations of the cross-attention maps of models w.\nor w/o. BoxRPB. For each group, the first column shows the input\nimage and the object query. The first row presents the attention\nmaps of the model w. BoxRPB, while the second row displays at-\ntention maps of the model w/o. BoxRPB. BoxRPB helps to guide\nthe cross-attention to focus on the individual objects.\nto the plain ViT [13].\nOur system only uses a single-\nresolution feature map throughout a plain Transformer\nencoder-decoder architecture, without any multi-scale de-\nsign or processing.\nWe compare our approach with the\nstate-of-the-art Cascade Mask R-CNN [3, 28] on the COCO\ndataset. For the fair comparison, We use a MAE [20] pre-\ntrained ViT-Base as the backbone and train the object de-\ntector for \u223c50 epochs. As shown in Table 8, our method\nachieves comparable results with Cascade Mask R-CNN\nwhich relies on using multi-scale feature maps for better\nlocalization across different object scales. Remarkably, our\nmethod does not train with instance mask annotations that\nare usually considered to be beneficial for object detection.\n5.5. Visualization of cross-attention maps\nFigure 4 shows the cross-attention maps of models with\nor without BoxRPB. For the model with BoxRPB, the cross-\nattention concentrate on the individual object. In the con-\ntrary, the cross-attention of model without BoxRPB attend\nto multiple objects that have similar appearance.\n6. System-level Results\nWe compare our method with other state-of-the-art\nmethods in this section. Table 7 shows results, where all\nexperiments reported in this table utilize a Swin-Large as\nthe backbone. As other works usually apply an encoder to\nenhance the backbone features, we also stack 12 window-\nbased single-scale transformer layers (with a feature dimen-\n7\nmethod\nframework\nextra data\n#params\n#epoch\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nSwin [34]\nHTC\n284M\n72\n57.7\n76.2\n63.1\n33.4\n52.9\n64.0\nDETA [36]\nDETR\n218M\n24\n58.5\n76.5\n64.4\n38.5\n62.6\n73.8\nDINO-DETR [54]\nDETR\n218M\n36\n58.6\n76.9\n64.1\n39.4\n61.6\n73.2\nOurs\u2217\nDETR\n228M\n36\n60.0\n78.9\n66.4\n42.8\n62.7\n73.7\nDETA [36]\nDETR\nO365\n218M\n24 + 24\n63.5\n80.4\n70.2\n46.1\n66.9\n76.9\nDINO-DETR [54]\u2217\nDETR\nO365\n218M\n26 + 18\n63.3\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nOurs\u2217\nDETR\nO365\n228M\n24 + 24\n63.9\n82.1\n70.7\n48.2\n66.8\n76.7\nTable 7: System-level comparisons with the state-of-the-art results on COCO test-dev. All methods adopt the Swin-Large backbone. The\n\u2217 marks the results with test time augmentation.\nsion of 256) on top of the backbone for a fair comparison.\nWith the 36 training epochs, our model achieves 60.0 AP\non the COCO test-dev set, which outperforms DINO-DETR\nby 1.4 AP. Further introducing the Objects365 [40] as the\npre-training dataset, our method reaches 63.9 AP on the\ntest-dev set, which is better than DINO-DETR and DETA\nby a notable margin. These strong results verify that the\nplain DETR architecture does not have intrinsic drawbacks\nto prevent it from achieving high performance.\n7. Related work\nDETR-based object detection. DETR [4] has impressed\nthe field for its several merits, including the conceptually\nstraightforward and generic in applicability, requiring min-\nimal domain knowledge that avoids customized label as-\nsignments and non-maximum suppression, and being plain.\nWhile the original DETR maintains a plain design, it also\nsuffers from slow convergence rate and lower detection ac-\ncuracy.\nThere have been many follow-up works includ-\ning [35, 16, 9, 47, 55, 53, 52, 17, 54], and now many top\nobject detectors have been built upon this line of works,\nthanks to the reintroduction of multi-scale and locality de-\nsigns [54, 14, 46]. Unlike these leading works, we aim for\nan improved DETR framework that maintains a \u201cplain\u201d na-\nture without multi-scale features and local cross-attention\ncomputation.\nRegion-based object detection. Prior to the DETR frame-\nwork, the object detectors were usually built in a region-\nbased fashion: the algorithms analyze every region of the\nentire image locally, and the object detections are obtained\nby ranking and filtering the results of each region. Due to\nthe locality nature, it\u2019s hard for them to flexibly leverage\nglobal information for object detection. Moreover, while\nsome early attempts use single scale feature map on the\nhead [19, 38, 18, 39, 32], later, the leading methods are\nalmost all built by multi-scale features such as FPN [29],\nBiFPN [42], Cascade R-CNN [3], and HTC [5], etc. We\nexpect our strong plain DETR detector may also inspire\nresearch in exploring single-scale feature map for region-\nbased detection.\nPosition encoding.\nThis paper is also related to posi-\ntion encoding techniques. The original Transformer [45]\nuses absolute position encoding. Early vision Transform-\ners [4, 12, 44] inherit this absolute position encoding set-\nting. Swin Transformers [34, 33] highlight the importance\nof relative position bias for Transformer-based visual recog-\nnition, where some early variants can be found in both\nlanguage and vision domains [23, 41, 24, 10, 25, 8, 48].\nThis paper extends the relative position bias for box-to-pixel\npairs, instead of previous pixel-to-pixel pairs. It also reveals\nthat the RPB can effect even more critical in the context of\nplain DETR detectors.\nPre-training.\nThe pre-training methods [20, 51, 1] that\nfollow the path of masked image modeling have drawn in-\ncreasing attention due to their strong performance on vari-\nous core vision tasks such as object detection and seman-\ntic segmentation.\nAlthough some recent works [28, 49]\nhave revealed some possible reasons why MIM outperforms\nthe conventional supervised pre-training and confirmed that\nFPN can be simplified, few works attempt to build a fully\nplain object detection head based on MIM pre-trained back-\nbones. Our experiment results show that MIM pre-training\nis a key factor in fully plain object detection architecture\ndesign.\n8. Conclusion\nThis paper has present an improved plain DETR detec-\ntor which achieves exceptional improvements over the orig-\ninal plain model, and achieves a 63.9 mAP accuracy us-\ning a Swin-L backbone, which is highly competitive with\nstate-of-the-art detectors that have been heavily tuned using\nmulti-scale feature maps and region-based feature extrac-\ntion. We highlighted the importance of two technologies\n8\nof BoxRPB and MIM-based pre-training for this improved\nplain DETR framework. We hope the effective detector em-\npowered by minimal architectural \u201cinductive bias\u201d can en-\ncourage future research to explore generic plain decoders in\nother vision problems.\nReferences\n[1] H. Bao, L. Dong, S. Piao, and F. Wei. Beit: Bert pre-training\nof image transformers.\narXiv preprint arXiv:2106.08254,\n2021. 4, 8\n[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al.\nLanguage models are few-shot learners.\nAdvances\nin neural information processing systems, 33:1877\u20131901,\n2020. 1\n[3] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high\nquality object detection. In CVPR, pages 6154\u20136162, 2018.\n3, 7, 8\n[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-end object detection with trans-\nformers. In European conference on computer vision, pages\n213\u2013229. Springer, 2020. 1, 2, 4, 6, 8\n[5] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng,\nZ. Liu, J. Shi, W. Ouyang, et al. Hybrid task cascade for\ninstance segmentation. In CVPR, pages 4974\u20134983, 2019. 8\n[6] Q. Chen, J. Wang, C. Han, S. Zhang, Z. Li, X. Chen, J. Chen,\nX. Wang, S. Han, G. Zhang, et al. Group detr v2: Strong\nobject detector with encoder-decoder pretraining.\narXiv\npreprint arXiv:2211.03594, 2022. 3, 12\n[7] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Gird-\nhar. Masked-attention mask transformer for universal image\nsegmentation. arXiv preprint arXiv:2112.01527, 2021. 6, 11\n[8] X. Chu, B. Zhang, Z. Tian, X. Wei, and H. Xia. Do we re-\nally need explicit position encodings for vision transformers.\narXiv preprint arXiv:2102.10882, 3(8), 2021. 8\n[9] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang.\nDynamic detr: End-to-end object detection with dynamic at-\ntention. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 2988\u20132997, 2021. 8\n[10] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and\nR. Salakhutdinov.\nTransformer-xl:\nAttentive language\nmodels beyond a fixed-length context.\narXiv preprint\narXiv:1901.02860, 2019. 8\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805, 2018. 1\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 8\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. In ICLR, 2021. 1, 7\n[14] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang,\nT. Huang, X. Wang, and Y. Cao. Eva: Exploring the lim-\nits of masked visual representation learning at scale. arXiv\npreprint arXiv:2211.07636, 2022. 8\n[15] Y. Fang, S. Yang, S. Wang, Y. Ge, Y. Shan, and X. Wang. Un-\nleashing vanilla vision transformer with masked image mod-\neling for object detection. arXiv preprint arXiv:2204.02964,\n2022. 2\n[16] P. Gao, M. Zheng, X. Wang, J. Dai, and H. Li. Fast con-\nvergence of detr with spatially modulated co-attention. In\nICCV, pages 3621\u20133630, 2021. 5, 8\n[17] Z. Gao, L. Wang, B. Han, and S. Guo. Adamixer: A fast-\nconverging query-based object detector. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5364\u20135373, 2022. 8\n[18] R. Girshick. Fast r-cnn. In ICCV, pages 1440\u20131448, 2015. 8\n[19] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 580\u2013587,\n2014. 8\n[20] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00b4ar, and R. Girshick.\nMasked autoencoders are scalable vision learners. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16000\u201316009, 2022. 2, 4, 7,\n8, 10\n[21] K. He, G. Gkioxari, P. Doll\u00b4ar, and R. Girshick. Mask R-\nCNN. In ICCV, 2017. 6, 11\n[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning\nfor Image Recognition. In CVPR, 2016. 2\n[23] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation net-\nworks for object detection. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n3588\u20133597, 2018. 8\n[24] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 3464\u2013\n3473, 2019. 8\n[25] Z. Huang, D. Liang, P. Xu, and B. Xiang. Improve trans-\nformer models with better relative position embeddings.\narXiv preprint arXiv:2009.13658, 2020. 8\n[26] D. Jia, Y. Yuan, H. He, X. Wu, H. Yu, W. Lin, L. Sun,\nC. Zhang, and H. Hu. Detrs with hybrid matching. arXiv\npreprint arXiv:2207.13080, 2022. 3, 12\n[27] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang.\nDn-detr: Accelerate detr training by introducing query de-\nnoising. arXiv preprint arXiv:2203.01305, 2022. 3\n[28] Y. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision\ntransformer backbones for object detection. arXiv preprint\narXiv:2203.16527, 2022. 2, 4, 7, 8\n[29] T.-Y. Lin, P. Doll\u00b4ar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2117\u20132125, 2017. 8\n[30] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00b4ar. Focal\nloss for dense object detection. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2980\u2013\n2988, 2017. 3\n9\n[31] S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu,\nand L. Zhang. Dab-detr: Dynamic anchor boxes are bet-\nter queries for detr. arXiv preprint arXiv:2201.12329, 2022.\n5\n[32] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.\nFu, and A. C. Berg. Ssd: Single shot multibox detector. In\nECCV, pages 21\u201337. Springer, 2016. 8\n[33] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,\nZ. Zhang, L. Dong, et al.\nSwin transformer v2: Scaling\nup capacity and resolution. In CVPR, pages 12009\u201312019,\n2022. 2, 3, 4, 8\n[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\nB. Guo. Swin transformer: Hierarchical vision transformer\nusing shifted windows. In ICCV, pages 10012\u201310022, 2021.\n2, 3, 8, 12\n[35] D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun,\nand J. Wang. Conditional detr for fast training convergence.\nIn Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), 2021. 5, 8\n[36] J. Ouyang-Zhang, J. H. Cho, X. Zhou, and P. Kr\u00a8ahenb\u00a8uhl.\nNms strikes back. arXiv preprint arXiv:2212.06137, 2022. 8\n[37] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever,\net al. Improving language understanding by generative pre-\ntraining. 2018. 1\n[38] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You\nonly look once: Unified, real-time object detection. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2016. 8\n[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. Advances in neural information processing systems,\n28, 2015. 8\n[40] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li,\nand J. Sun. Objects365: A large-scale, high-quality dataset\nfor object detection. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 8430\u20138439,\n2019. 8, 11\n[41] P. Shaw, J. Uszkoreit, and A. Vaswani.\nSelf-attention\nwith relative position representations.\narXiv preprint\narXiv:1803.02155, 2018. 8\n[42] M. Tan, R. Pang, and Q. V. Le. Efficientdet: Scalable and\nefficient object detection. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 10781\u201310790, 2020. 8\n[43] Z. Teed and J. Deng. Raft: Recurrent all-pairs field trans-\nforms for optical flow.\nIn Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part II 16, pages 402\u2013419. Springer,\n2020. 3\n[44] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,\nand H. J\u00b4egou. Training data-efficient image transformers &\ndistillation through attention. In International conference on\nmachine learning, pages 10347\u201310357. PMLR, 2021. 8\n[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all\nyou need. Advances in neural information processing sys-\ntems, 30, 2017. 8\n[46] W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu,\nT. Lu, L. Lu, H. Li, et al. Internimage: Exploring large-\nscale vision foundation models with deformable convolu-\ntions. arXiv preprint arXiv:2211.05778, 2022. 8\n[47] Y. Wang, X. Zhang, T. Yang, and J. Sun. Anchor detr: Query\ndesign for transformer-based detector, 2021. 8\n[48] K. Wu, H. Peng, M. Chen, J. Fu, and H. Chao. Rethinking\nand improving relative position encoding for vision trans-\nformer. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 10033\u201310041, 2021. 8\n[49] Z. Xie, Z. Geng, J. Hu, Z. Zhang, H. Hu, and Y. Cao. Re-\nvealing the dark secrets of masked image modeling. arXiv\npreprint arXiv:2205.13543, 2022. 2, 4, 8\n[50] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and\nH. Hu. Simmim: A simple framework for masked image\nmodeling. arXiv preprint arXiv:2111.09886, 2021. 2\n[51] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and\nH. Hu. Simmim: A simple framework for masked image\nmodeling. In CVPR, pages 9653\u20139663, 2022. 4, 8\n[52] G. Zhang, Z. Luo, Y. Yu, J. Huang, K. Cui, S. Lu, and\nE. P. Xing. Semantic-aligned matching for enhanced detr\nconvergence and multi-scale feature fusion. arXiv preprint\narXiv:2207.14172, 2022. 8\n[53] G. Zhang, Z. Luo, Y. Yu, Z. Tian, J. Zhang, and S. Lu. To-\nwards efficient use of multi-scale features in transformer-\nbased object detectors.\narXiv preprint arXiv:2208.11356,\n2022. 8\n[54] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni,\nand H.-Y. Shum. Dino: Detr with improved denoising an-\nchor boxes for end-to-end object detection. arXiv preprint\narXiv:2203.03605, 2022. 1, 2, 3, 8, 12\n[55] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai.\nDe-\nformable detr: Deformable transformers for end-to-end ob-\nject detection. arXiv preprint arXiv:2010.04159, 2020. 1, 3,\n6, 8, 10, 11\n9. Supplementary\nA. More Plain ViT Results\nTable 8 reports more comparison results based on the\nplain ViT. We use the default setup, described in Section\n5.4 of the main text, to adopt a MAE [20] pre-trained ViT-\nBase as the backbone and train the model for \u223c50 epochs.\nAccording to the results, we observe that (i) our method\nboosts the plain DETR baseline from 46.5 AP to 53.8 AP\nwhen only using a global cross-attention scheme to process\nsingle-scale feature maps; (ii) our approach outperforms\nthe strong DETR-based object detector, e.g., Deformable\nDETR [55], which uses a local cross-attention scheme to\nexploit the benefits of multi-scale feature maps.\nB. Runtime Comparison with Other Methods\nWe further analyze the runtime cost of different cross-\nattetnion modulations in Table 9.\nBoxRPB slightly in-\n10\nmethod\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nPlain DETR\n46.5\n70.2\n50.0\n26.3\n50.2\n65.7\nDeformable DETR[55]\n52.1\n71.6\n56.9\n33.5\n55.2\n69.0\nOurs\n53.8\n73.4\n58.9\n35.9\n57.0\n68.9\nTable 8: Comparison of the plain DETR baseline, Deformable\nDETR, and the improved plain DETR with a MIM pre-trained\nViT-Base backbone. Our plain DETR with global cross-attention\nimproves the baseline by a huge margin and outperforms the De-\nformable DETR, which relies on multi-scale features and local\ncross attention.\nmethod\nTraining (min/epoch)\nInference (fps)\nstandard cross attn.\n69\n9.9\nconditional cross att.\n72\n9.5\nDAB cross attn.\n73\n9.3\nSMCA cross attn.\n79\n9.6\nOurs\n75\n9.5\nTable 9:\nRuntime comparison with local cross-attention\nscheme.\nGlobal cross-attention with BoxRPB has comparable\nspeed to other positional bias methods.\ncreases runtime compared to standard cross-attention, while\nhaving comparable speed to other positional bias methods.\nC. More Details of Local Attention Scheme\nFigure 5 shows how our method differs from local cross-\nattention methods like deformable cross-attention [55],\nRoIAlign [21], RoI Sampling (fixed points in the Region of\nInterest), and box mask from [7]. Most local cross-attention\nmethods need to construct a sparse key-value space with\nspecial sampling and interpolation mechanism. Our method\nuses all image positions as the key-value space and learns a\nbox-to-pixel relative position bias term (gradient pink circu-\nlar area in (e)) to adjust the attention weights. This makes\nour method more flexible and general than previous meth-\nods.\nD. System-level Comparison on COCO val\nTable 10 compares our method with previous state-of-\nthe-art methods when using Swin-Large as the backbone.\nWith 36 training epochs, our model achieves 59.8 AP\non COCO val, outperforming DINO-DETR by +1.3 AP.\nWith Objects365[40] pre-training, our method gets 63.8 AP,\nmuch higher than DINO-DETR. These results show that,\nwith our approach, the improved plain DETR can achieve\ncompetitive performance without intrinsic limitations.\n11\nmethod\nframework\nextra data\n#params\n#epoch\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nSwin [34]\nHTC\nN/A\n284M\n72\n57.1\n75.6\n62.5\n42.4\n60.7\n71.1\nGroup-DETR [6]\nDETR\nN/A\n\u2265218M\n36\n58.4\n\u2212\n\u2212\n41.0\n62.5\n73.9\nH-Deformable-DETR [26]\nDETR\nN/A\n218M\n36\n57.8\n76.5\n63.7\n42.3\n61.8\n73.1\nDINO-DETR [54]\nDETR\nN/A\n218M\n36\n58.5\n77.0\n64.1\n41.5\n62.3\n74.0\nOurs\u2217\nDETR\nN/A\n228M\n36\n59.8\n78.8\n66.0\n45.5\n63.4\n74.2\nDINO-DETR [54]\u2217\nDETR\nO365\n218M\n26 + 18\n63.2\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nOurs\u2217\nDETR\nO365\n228M\n24 + 24\n63.8\n81.9\n70.6\n50.9\n67.8\n77.1\nTable 10: System-level comparisons with the state-of-the-art methods on COCO val. All methods adopt the Swin-Large backbone. The\nsuperscript \u2217 marks the results with test time augmentation.\n(a) Deformable cross-attn.\n(b) RoIAlign\n(c) RoI Sampling\n(d) Box Mask\n(e) Ours\nFigure 5:\nIllustrating the comparisons between different local cross-attention mechanisms and our global cross-attention schema. We\nmark the sampling positions with pink color. The input image is represented by the black grid and the green box is the predicted bounding\nbox from the previous layer. The red star marks the bounding box center. (a) Deformable cross-attention: It learns to sample important\npositions around the predicted bounding box for the key-value space. (b) RoIAlign: It uses bi-linear interpolation to compute the value\nof each sampling position in each RoI bin for the key-value space. (c) RoI Sampling: It quantizes the sampling positions to discrete bins\nand uses them as the key-value space. (d) Box mask: It selects all the positions within the green bounding box as the key-value space. (e)\nOur method: It improves global cross-attention with BoxRPB, which uses all the positions in the input image as the key-value space. The\nattention values are indicated by color intensity.\n12\n"
  },
  {
    "title": "Ambient Adventures: Teaching ChatGPT on Developing Complex Stories",
    "link": "https://arxiv.org/pdf/2308.01734.pdf",
    "upvote": "6",
    "text": "Ambient Adventures: Teaching ChatGPT on Developing Complex Stories\nZexin Chen*, Eric Zhou*, Kenneth Eaton, Xiangyu Peng, Mark Riedl\nGeorgia Institute of Technology, Atlanta, GA, 30332, USA\nAbstract\nImaginative play is an area of creativity that could allow\nrobots to engage with the world around them in a much more\npersonified way. Imaginary play can be seen as taking real ob-\njects and locations and using them as imaginary objects and\nlocations in virtual scenarios. We adopted the story genera-\ntion capability of large language models (LLMs) to obtain the\nstories used for imaginary play with human-written prompts.\nThose generated stories will be simplified and mapped into\naction sequences that can guide the agent in imaginary play.\nTo evaluate whether the agent can successfully finish the\nimaginary play, we also designed a text adventure game to\nsimulate a house as the playground for the agent to interact.\n1\nIntroduction\nIn recent years, the domain of agents has experienced ex-\ntraordinary progress, driving the creation of intelligent ma-\nchines that connect the realms of science fiction and reality.\nAs researchers, engineers, and innovators collaborate, the\nevolution of agents keeps pushing the limits of technology.\nHowever, how do we ensure that agents have a persistent, yet\nnon-intrusive presence in the household? Considering kids:\nthey are never idle \u2014 they find ways to occupy their time\nthrough play and if that play is imaginative play, then the en-\ntire home becomes a playground. We propose to develop the\ncomputational capability for agents to engage in imaginative\nplay and link that play to navigation through the home. This\nwill increase the presence of the agent in the home without\ndirectly demanding attention from people, but also using cu-\nriosity to invite engagement.\nImaginative play is an exemplar of everyday human cre-\nativity in which real-world, mundane objects and locations\nact as substitutes for imaginary objects and locations as part\nof a pretend scenario(Zook, Magerko, and Riedl 2011). A\nterrarium can be a garden for growing magic seeds, a kitchen\ncan be a laboratory, or a broom handle can be a light saber.\nImaginative play is fundamental to human creativity. Com-\nputational systems that can engage in imaginative play can\ncreate a sense of presence and persona and provide opportu-\nnities for improvisational interactions.\n*These authors contributed equally.\nCopyright \u00a9 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nIn this paper, we are focusing on exploring how to guide\nan agent to execute imaginary play with large language mod-\nels such as ChatGPT (OpenAI 2022). Text adventure games\nserve as useful test beds because they have also been demon-\nstrated to transfer to visual and real-world domains (Wang\net al. 2022; Shridhar et al. 2021; Peng, Riedl, and Am-\nmanabrolu 2022).\n2\nRelated Work\next games are turn-based games where players read descrip-\ntions of the current scene for information and interact with\nshort descriptions of actions (C\u02c6ot\u00b4e et al. 2018; Wang et al.\n2022; Shridhar et al. 2021). During the designing of the text\nadventure game, we follow the text game structure (Peng\net al. 2023) to create a house consisting of rooms that con-\ntain objects as the realistic mappings of the imaginary play.\nThe difference is that our adventure games don\u2019t have NPCs\nsince we want to focus on whether the story of imaginary\nplay can guide the agent instead of elevating the difficulty of\ninteraction in the game.\nChatGPT is an LLM chatbot developed by OpenAI that\ncan be interacted with in multiple ways, including giving\nprompts to write stories (OpenAI 2022). Many pre-trained\nLLMs have had success in story generation; by decomposing\ntextual story data into a series of events, it has been found\nthat these models can generate stories from these events that\nare more coherent and logical (Martin et al. 2018; Peng et al.\n2022b,a). We aim to adopt this ability to train the model with\nprompting and generate stories that can guide the agent in\nimaginary play with ChatGPT.\n3\nStory Generation\nA natural language story is generated automatically as an\nexemplar of the behavior the agent is to enact in imaginary\nplay. Given the topic of the imaginary play, Large Language\nModels (LLMs) such as ChatGPT and GPT-4 are used to\ncreate the imaginary story, and the real-world objects are\ntransformed into similar objects in an imaginary world \u2014\nimaginary objects, to facilitate the imaginary play.\n3.1\nImaginary Story Generation\nFirstly, the agent scans the layout of each house to obtain the\nreal-world objects and rooms, as well as their respective lo-\narXiv:2308.01734v1  [cs.CL]  3 Aug 2023\nFigure 1: Pipeline Architecture for Text Game and ChatGPT. A sample iteration is demonstrated.\ncations in the house. In this process, LLMs (ChatGPT) gen-\nerate imaginary objects which match the setting of the imag-\ninary world (whether that be magical, horror, etc.) that have\nsimilar characteristics to the object it was mapped from. For\nexample, a \u201cbroom\u201d can be transformed into a \u201cwand\u201d in\nthe imaginary setting because they have similar shapes, are\nboth made of wood and can be held. See Fig 1, where the\nfirst prompt by ChatGPT is used to map each original object\nto an imaginary one.\nWith each real-world object, we also obtain the admissi-\nble actions for each one \u2013 Ao \u2013 an admissible action refers\nto one that can be performed with that object. Let there be a\nset of real-world objects, such as \u27e8\u201cbroom\u201d, \u201cdresser\u201d,\n\u201cmug\u201d\u27e9. The set Oo denotes the set of real-world objects. For\nexample, a \u201cbroom\u201d in the house may have the set of admis-\nsible actions \u27e8\u201csweep\u201d, \u201cpick up\u201d\u27e9. With this, LLMs are\nprompted to find the closest, most similar imaginary object\nfor each item in Oo, based on the setting of the story. By this,\nwe map all of our objects from the original Oo to the set of\nimaginary objects \u2014 On. Within any story, a topic, such as\n\u201dsaving a princess\u201d, is required for LLMs (ChatGPT) to aim\nto complete. A wide variety of topics were given based on\nthe setting. For example, a magical setting could entail sav-\ning a princess, or a horror story would entail finding a key to\nescape.\nTo ensure that the generated story by ChatGPT can be\nsimplified into phrases later, several training samples were\ngiven to it to ensure that it would be easy to do so. These\ntraining samples are short, 5-7 sentence imaginary stories\nthat are concise and contain several random imaginary ob-\njects and a topic (such as \u201ddefeating the dragon\u201d). Refer to\nFig 1 and the second ChatGPT prompt which uses these\ntraining samples and a list of the imaginary objects to gener-\nate new samples to use. A common limitation of ChatGPT\u2019s\nstory generation is that it will simply create a story where the\nagent immediately obtains every imaginary item in one sen-\ntence and follows by completing the topic. While this indeed\nworks logically, it is far from interesting. With the training\nsamples given, ChatGPT is prompted to at most add one new\nitem in each sentence. For example, if ChatGPT is given 5\nmagical items and a topic to save a princess, it will initially\nget all 5 items in the first sentence and then save the princess\nin the second. With this restriction, ChatGPT has to find a\nlogical way to use objects to get others and continue until it\nhas enough objects to save the princess.\nLLMs may sometimes be prompted to rewrite the story\n\u2013 instead of entirely rewriting the story ChatGPT will be\nprompted to continue it \u2013 refer to Table 1 for the second iter-\nation. A sample iteration would involve the last sentence of\nthe original imaginary story from Table 1 being removed and\nChatGPT prompted to generate new sentences starting from\nthis point to reach the intended topic. At this point, ChatGPT\nhas successfully generated a story, and now this story needs\nto be distilled and translated back into admissible actions in\nthe text game.\n3.2\nMapping and Filtering\nThe reason for distilling the story is so that these actions can\nbe given for the text game to easily understand. Each sen-\ntence in the imaginary story is distilled into a phrase by tak-\ning the one imaginary object in each sentence and the action\nverb that is associated with it. Refer to Figure 1 and the third\nChatGPT prompt, as well as the Simplified Story in Table 1.\nIf there is more than one object in a sentence, the newly ob-\ntained object is chosen. For example, if a sentence is \u201copen\nchest to reveal staff\u201d, the distilled phrase will be\n\u201creveal staff\u201d, not \u201copen chest\u201d. Once we have\nall of the phrases, we now want to map these phrases back\ninto admissible actions that can be performed in the real\nworld.\nRemember that Ao refers to the admissible actions for\noriginal objects, such as \u27e8\u201csweep\u201d, \u201cuse\u201d\u27e9 for a broom.\nChatGPT can then identify the most similar admissible ac-\ntion in Ao that best matches the action performed in the\nimaginary world. For example, a \u201cbroom\u201d in the real world\nis mapped to a \u201cwand\u201d \u2013 see Table 1 for an example\nTable 1: Magical Story Example\nTopic: Magical World - Saving a Princess\nImaginary Story (First Iteration): Whisperweaver\ndiscovers hidden passage. Uncover ancient chest in\nhidden passage. Open chest to reveal enchanted staff.\nAlso find Crescent Mirror in chest. Wield enchanted\nstaff for enhanced spellcasting. Use Crescent Mirror\nfor scrying and divination. Harness the power of the\nenchanted staff and mirror to defeat evil forces and\nsave princess.\nSimplified Story: 1. Discovers Whisperweaver 2. Un-\ncover Ancient Chest 3. Reveal Enchanted Staff 4. Find\nCrescent Mirror 5. Wield Enchanted Staff 6. Use Cres-\ncent Mirror 7. Harness Enchanted Staff.\nReal-World Translation: 1. Wear clothes 2. Open\nnightstand 3. Use broom. 4. Open dresser 5. Use\nbroom. 6. Open dresser 7. Use broom.\nImaginary Story (Second Iteration): Whisper-\nweaver discovers hidden passage. Uncover ancient\nchest in hidden passage. Open chest to reveal en-\nchanted staff. Also find Crescent Mirror in chest.\nWield enchanted staff for enhanced spellcasting. Use\nCrescent Mirror for scrying and divination. Discover\nrecipe for elixir with Crescent Mirror. Brew elixir in\nthe cauldron. Use enchanted staff to activate the elixir.\nUse transformed abilities from elixir to defeat the evil\nthreat.\nof an imaginary story. If the \u201cwand\u201d is used to cast a\nspell, ChatGPT would determine which admissible action\nwould be most similar to the action \u201ccast a wand\u201d. If\n\u201csweep\u201d is chosen, then \u201ccast a wand\u201d will be mapped\ninto \u201csweep broom\u201d \u2013 see Table 1 once again and the\nmapping from Simplified Story to Real-World Translation.\nThe agent can then use these mapped admissible actions to\ninteract with the real-world environment.\n4\nText Adventure Games\nText Adventure Game is the testbed to show how the agent\ndoes imaginary play in the real world. Text games show the\nevent happening within the current scene by depicting the\nexisting objects and happened actions in short sentences.\nObjects taken as entities in the game contain the states and\nactions used in presetting the interactions with the agent.\nThe story might ignore details when events happen, but the\ntext game can record the hidden state changes with words.\nFor this reason, we utilize text games as test playgrounds.\nWe evaluate the performance of an imaginary story by\nwhether the agent can perform all input actions sequentially,\nwhich indicates that the model-translated action sequences\ncan function as guidance to the agent in imaginary play. The\nreward of each round of the game equals the score of the last\naction. To tell the game result directly, we set the last action\nin the sequence as the win state. Thus, if the reward equals\nthe score of the preset win state, the agent successfully fin-\nishes all actions given in the sequence.\nWe developed our text game in TextWorld (C\u02c6ot\u00b4e et al.\n2018), an open-source, extensible engine that both gen-\nerates and simulates text games. In the game, we mimic\nthe physical environment by mapping out the house floor\nplan and including pre-scripted interactions with each ob-\nject in the room to give guidance under different use cases\n(Narasimhan, Kulkarni, and Barzilay 2015).\n4.1\nGame Design Details\nWe design a game by inserting a base map that records the\nlocation of each room and logic objects related to differ-\nent rooms (See Fig 2). Each room has its own furniture\nand appliances, some of which are required, like a light.\nEach object Oi has its respective action set Ai which can\nchange the states of itself and the game. For instance, the\nobject Clothes, Oclo, has two states: \u201dwashed\u201d and \u201dnot\nwashed\u201d. The action wash in an action set Aclo can convert\nthe state of Oclo from \u201dnot washed\u201d to \u201dwashed\u201d when the\nagent successfully finishes the action \u2014 \u201cwash cloth\u201d.\nTo execute this action, the agent starts in the parentBedroom\nto grab the dirty clothes. When it moves to the laundry and\nfinishes washing the clothes, the agent gains 2 points for\nwash clothes.\nFor the agent to distinguish similar verbs with similar\nmeanings and reactions, for instance, \u201cwash cloth\u201d and\n\u201cclean cloth\u201d should be taken as the same thing to do.\nIn that way, we ensure the agent will take the same reaction\nevery time with synonyms.\nFigure 2: Layout of game \u201cHousework\u201d.\n4.2\nReinforcement Learning Agent in Text Game\nThe agent has an action sequence that needs to be finished in\nthe text game and obtains rewards for successfully changing\nthe state of the objects or the game. The game process is as\nfollows: the agent always starts in a fixed room with a given\ninput action sequence. The agent will obtain a reward when\nit finishes the input action by interacting with the surround-\ning environment. From section 4.1, if the agent successfully\nfinishes the action \u201cwash cloth\u201d, it will obtain 2 points\nas a reward. The reward the agent will gain depends on the\ndifficulty of the action. We categorize the activities into three\nlevels: stand-alone, interactive, and win, corresponding to 2,\n3, and 5 points.\n1. We define stand-alone actions as actions that the agent\nis able to finish without using any other objects.For ex-\nample, when the agent takes the action \u201cturn on the\nlight\u201d, the Light can be turned on directly after locat-\ning the Light.\n2. The definition of interactive actions is the actions that\nthe agent is able to interact with other objects. For example,\nwhen the agent takes action \u201cwater plant\u201d and sees the\nPlant, it cannot directly take action if there is no water\nin its hands. The agent needs to get the kettle, then check\nwhether the kettle is full of water. If not, it will fill the kettle,\nthen carry it to water plant.\n3. Win action is the last action that the agent needs to fin-\nish. When the agent takes the last action in the given action\nsequence, \u201cclean the oven\u201d, it comes to the kitchen,\nlocates the Oven, and then clean it. The game ends when\nthe agent successfully goes through the whole action se-\nquence and finishes all actions in it.\n5\nFindings\nStory Generation of Large Language Model:\nWe used ChatGPT as the LLM to generate stories dur-\ning our experiment (See Section 3). Most stories required\nseveral iterations of revision (Refer to Table 1) until they in-\ncluded the win state in the action sequence. Two limitations\nof the current model are limited prompting formats and dif-\nficulty in understanding interactive actions in the text game.\nThe first relates to the drawback of the language model\nis that the generation is uncontrolled. Aside from an initial\nprompt, generative language models are guided by word co-\noccurrence, which can lead to repetition, as well as a ten-\ndency to focus on descriptive details that do not move a story\nforward. To solve the problem, we kept crafting prompts to\ndirect the model to create coherent and executable stories\nwith a clear goal and formed a fixed prompting format. The\nformat limits the adaptivity of the agent to varied types of\nimaginary play. If the setting in imaginary play is modified,\nthe model needs new prompts for the changes.\nThe other limitation is the difficulty forChatGPT to un-\nderstand connections between objects in the text game. The\ngenerated story cannot associate the objects picked from a\nprevious room with those in the current room if no detailed\nprompts. That may lead to generating actions not allowed\nwithin the text game in the action sequence. The solution to\nalleviate such problems is to introduce the missing connec-\ntions into the prompting and have more iterations of story\ngeneration to update the prompt with the generated output.\nGame Results with Promptings.\nResults from our sam-\nple stories indicate that the agent cannot determine the fi-\nnal win state by itself. To increase the possibility of win,\nwe record the result and feed it back to the language model\n(ChatGPT). The model knows whether the agent success-\nfully reaches the win state from the previous round\u2019s score.\nIf the agent doesn\u2019t win, the prompt will tell the model to\ngenerate more descriptions of directional information in new\nstories to guide the agent in the next round. Although new\ninstructions might not give the expected results every time,\nwe still are able to catch the pattern and re-prompting Chat-\nGPT to better train in zero-shot learning.\n6\nConclusions\nImaginary play is a creative direction for developing agent\nlearning abilities. With the help of story generation from\nLLMs (such as ChatGPT (OpenAI 2022)), we can tell the\nmodel to generate imaginary play stories that guide the\nagent\u2019s interactions through prompts. Story generation al-\nlows the agent to develop interesting imaginative stories\nwith the objects and topic given, allowing the agent to en-\ngage in imaginative play in the real world.\nWe use text games to model what happens within a given\nstory and the interactions the agent generates with the set-\nting, making the interaction controllable and explainable.\nThrough mapping imaginative play to real-world scenarios\nthrough text games, we figured out how to use rewards to\nbetter prompt the model and construct the stories that can\nguide the agent in imaginary play.\nReferences\nC\u02c6ot\u00b4e, M.-A.; K\u00b4ad\u00b4ar, A.; Yuan, X.; Kybartas, B.; Barnes, T.;\nFine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.;\net al. 2018.\nTextworld: A learning environment for text-\nbased games.\nIn Workshop on Computer Games, 41\u201375.\nSpringer.\nMartin, L.; Ammanabrolu, P.; Wang, X.; Hancock, W.;\nSingh, S.; Harrison, B.; and Riedl, M. 2018. Event Repre-\nsentations for Automated Story Generation with Deep Neu-\nral Nets. Proceedings of the AAAI Conference on Artificial\nIntelligence, 32(1).\nNarasimhan, K.; Kulkarni, T.; and Barzilay, R. 2015. Lan-\nguage Understanding for Text-based Games Using Deep Re-\ninforcement Learning. arXiv:1506.08941.\nOpenAI. 2022.\nChatGPT: A Large-Scale Open-Domain\nChatbot. https://openai.com/blog/chatgpt/.\nPeng, X.; Cui, C.; Zhou, W.; Jia, R.; and Riedl, M. 2023.\nStory Shaping: Teaching Agents Human-like Behavior with\nStories. arXiv:2301.10107.\nPeng, X.; Li, S.; Wiegreffe, S.; and Riedl, M. 2022a. Infer-\nring the Reader: Guiding Automated Story Generation with\nCommonsense Reasoning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, 7008\u20137029.\nPeng, X.; Riedl, M.; and Ammanabrolu, P. 2022.\nIn-\nherently explainable reinforcement learning in natural lan-\nguage. Advances in Neural Information Processing Systems,\n35: 16178\u201316190.\nPeng, X.; Xie, K.; Alabdulkarim, A.; Kayam, H.; Dani, S.;\nand Riedl, M. 2022b. Guiding Neural Story Generation with\nReader Models. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022, 7087\u20137111.\nShridhar, M.; Yuan, X.; C\u02c6ot\u00b4e, M.-A.; Bisk, Y.; Trischler, A.;\nand Hausknecht, M. 2021. ALFWorld: Aligning Text and\nEmbodied Environments for Interactive Learning. In Pro-\nceedings of the International Conference on Learning Rep-\nresentations (ICLR).\nWang, R.; Jansen, P.; C\u02c6ot\u00b4e, M.-A.; and Ammanabrolu, P.\n2022.\nScienceWorld: Is your Agent Smarter than a 5th\nGrader?\nZook, A.; Magerko, B.; and Riedl, M. 2011. Formally mod-\neling pretend object play. In Proceedings of the 8th ACM\nConference on Creativity and Cognition, 147\u2013156.\n"
  },
  {
    "title": "Computational Long Exposure Mobile Photography",
    "link": "https://arxiv.org/pdf/2308.01379.pdf",
    "upvote": "3",
    "text": "Computational Long Exposure Mobile Photography\nERIC TABELLION, NIKHIL KARNAD, NOA GLASER, BEN WEISS, DAVID E. JACOBS, and YAEL PRITCH,\nGoogle Research, USA\n(a) Foreground blur examples\n(b) Background blur examples\nFig. 1. At the tap of the shutter button, our smartphone camera system captures, processes and outputs both conventional and long exposure corresponding\nphotographs in a few seconds, as shown in the top row. Examples of foreground blur captured hand-held are shown in (a), and examples of background blur\ncaptured without precise tracking, are shown in (b). Our computational long exposure photography pipeline handles both use cases fully automatically.\nLong exposure photography produces stunning imagery, representing mov-\ning elements in a scene with motion-blur. It is generally employed in two\nmodalities, producing either a foreground or a background blur effect. Fore-\nground blur images are traditionally captured on a tripod-mounted camera\nand portray blurred moving foreground elements, such as silky water or\nlight trails, over a perfectly sharp background landscape. Background blur\nimages, also called panning photography, are captured while the camera\nis tracking a moving subject, to produce an image of a sharp subject over\na background blurred by relative motion. Both techniques are notoriously\nchallenging and require additional equipment and advanced skills. In this\npaper, we describe a computational burst photography system that operates\nin a hand-held smartphone camera app, and achieves these effects fully\nautomatically, at the tap of the shutter button. Our approach first detects\nand segments the salient subject. We track the scene motion over multiple\nframes and align the images in order to preserve desired sharpness and to\nproduce aesthetically pleasing motion streaks. We capture an under-exposed\nAuthors\u2019 address: Eric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David E. Jacobs,\nYael Pritch, Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA, 94043, USA.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author\u2019s version of the work. It is posted here for your personal use. Not for\nredistribution. The definitive Version of Record was published in ACM Transactions on\nGraphics, https://doi.org/10.1145/3592124.\nburst and select the subset of input frames that will produce blur trails of\ncontrolled length, regardless of scene or camera motion velocity. We predict\ninter-frame motion and synthesize motion-blur to fill the temporal gaps\nbetween the input frames. Finally, we composite the blurred image with\nthe sharp regular exposure to protect the sharpness of faces or areas of the\nscene that are barely moving, and produce a final high resolution and high\ndynamic range (HDR) photograph. Our system democratizes a capability\npreviously reserved to professionals, and makes this creative style accessible\nto most casual photographers.\nMore information can be found on our project webpage: https://motion-\nmode.github.io/.\nCCS Concepts: \u2022 Computing methodologies \u2192 Computational photog-\nraphy; Computer vision problems; Computer graphics.\nAdditional Key Words and Phrases: machine learning, mobile computing\nACM Reference Format:\nEric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David E. Jacobs,\nand Yael Pritch. 2023. Computational Long Exposure Mobile Photogra-\nphy. ACM Trans. Graph. 42, 4, Article 48 (August 2023), 15 pages. https:\n//doi.org/10.1145/3592124\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\narXiv:2308.01379v1  [cs.CV]  2 Aug 2023\n48:2\n\u2022\nTabellion et al.\n1\nINTRODUCTION\nMobile photography is ever present in consumers\u2019 daily lives and\nis often superseding traditional photography. Using image burst\ncapture and post-processing techniques, modern mobile phones\u2019\nimaging pipelines produce very high quality results, providing high\ndynamic range tone mapping, exceptional low light performance\nand simulating depth-of-field bokeh effects, which were previously\nachievable only with much bigger and heavier cameras and lenses.\nDespite these outstanding improvements, long exposure mobile\nphotography remains poorly treated to the best of our knowledge.\nExisting solutions don\u2019t help users produce results where moving\nand static scene elements appear blurry and sharp respectively.\nThis juxtaposition of sharp against blurry is a key property of a\ncompelling image, that cannot be achieved by simply exposing a\nhand-held camera sensor for a longer duration.\nTraditional long exposure photography is typically performed in\none of two ways, according to the scene and situation. One approach\nproduces a foreground blur effect (e.g. silky waterfall, light trails,\netc.) over a sharp background, using very long exposure times that\ncan last up to several seconds. This requires using a tripod, as even\na slight camera shake can cause undesired loss of background sharp-\nness. Additionally, a neutral density (ND) filter must be added to the\nlens, to avoid over-exposing the sensor. A second approach, called\npanning photography, produces a rendition with a sharp moving\nsubject over a background that is blurred with motion relative to\nthe subject. It is achieved by tracking the moving subject with the\ncamera, while keeping the shutter open with the exposure time\nincreased modestly, e.g. half a second, and the aperture slightly\nreduced to avoid over-exposing the image. The photographer must\ntrack the subject motion as precisely as possible to avoid undesired\nloss of subject sharpness, while also pressing the shutter button at\nthe right moment. Both approaches require advanced skills, prac-\ntice and choosing the camera shutter speed manually, taking into\naccount how fast the scene is moving to achieve the desired result.\nThe main contribution of this paper is a computational long ex-\nposure mobile photography system, implemented in two variants,\nwhich democratize the two aforementioned use cases. It is imple-\nmented in a new camera mode called \"Motion Mode\" on Google\nPixel 6 and 7 smartphones, which allows the user to easily capture\nthese effects, without the need for a tripod or lens filter, nor the\nneed to track the moving subject precisely or at all. Our method\nis fully automatic end-to-end within each variant: after the user\nchooses which of foreground or background blur result they wish to\nproduce, we generate long exposure 12 megapixel photographs at\nthe tap of the shutter button, while compensating for camera and/or\nsubject motion, thereby preserving desired background and subject\nsharpness. The main components of our system are:\n\u2022 Capture schedule and frame selection, producing normalized\nblur trail lengths independent of scene or camera velocity,\n\u2022 Subject detection that combines gaze saliency with people\nand pets face region predictions, and tracking of their motion,\n\u2022 Alignment of input images to cancel camera hand-shake, sta-\nbilize the background in the presence of moving foreground\nelements, or to annul subject motion while producing pleas-\ning background motion blur trails,\n\u2022 Dense motion prediction and blur synthesis, spanning mul-\ntiple high resolution input frames and producing smooth\ncurved motion blur trails with highlight preservation.\nFurthermore, our system architecture, which includes several\nneural networks, performs efficiently on a mobile device under\nconstrained compute and memory budgets, implementing an HDR\nimaging pipeline that produces both related conventional and long\nexposure results in just a few seconds.\n2\nRELATED WORK\n2.1\nMobile Phone Computational Photography\nMany computational photography advances in recent years define\ntoday\u2019s mobile photography capabilities. The work from Hasinoff\net al. [2016] describes a mobile camera pipeline that captures, aligns\nand merges bursts of under-exposed raw images. Combined with the\nwork of Wronski et al. [2019], they are able to strongly improve the\nSignal to Noise Ratio (SNR), dynamic range and image detail, over-\ncoming the limitations of small smartphone sensors and lenses. Our\nsystem is built on top of such a computational imaging foundation.\nTo handle very low light situations without using a flash, Liba\net al. [2019] employ a scene motion metering approach to adjust\nthe number of input burst frames and determine their exposure\ntime. Similarly, we adjust the frame capture schedule based on\nscene motion, estimated when the shutter button is pressed, for the\npurpose of normalizing the resulting amount of motion-blur.\nSince the small camera lenses used on smartphones cannot pro-\nduce shallow depth-of-field effects optically, Wadhwa et al. [2018]\ndesign a synthetic bokeh approach, that relies on a semantic person\nsegmentation neural network, with the intent to isolate a subject\nfrom a distracting background. Our system is analogous, as we strive\nto isolate a subject from the relative motion of the background, while\nattempting to emphasize the dynamic nature of the scene.\n2.2\nAuto-tracking a Subject (background blur)\nDetermining the subject of a background blur capture is a hard prob-\nlem. Many synthetic long exposure pipelines avoid it altogether by\nrequiring manually tagging the subject region, or using a heuristic\nsuch as the middle region of the image [Lancelle et al. 2019; Luo\net al. 2018; Mikamo et al. 2021]. In contrast, we present a pipeline\nwhich determines the subject region automatically by predicting\nvisual saliency and face regions.\nUsing saliency-driven image edits to highlight a main subject\nfrom a distracting background was introduced in [Aberman et al.\n2022]. Existing methods to detect and track subject motion over time\ninclude [Stengel et al. 2015], which use gaze saliency prediction to\ndetect the subject and optical flow to track its motion, and [Mikamo\net al. 2021], which require the user to specify the subject region\nusing a bounding box and similarly track its motion. In our work,\nwe detect the subject using a combination of gaze saliency and\nsemantic segmentation using a highly efficient mobile architecture\ninspired by [Bazarevsky et al. 2019]. We track its motion using\nfeature tracking, and introduce an alignment regularization term to\nresult in more visually pleasing motion-blur trails, which are more\nconsistent with overall subject motion.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:3\nOpenCL \nRenderer\nDensity-sampled, weighted \nFeature tracks\nLine and Weight Kernel Maps\nFlow Magnitude &\nFaces Mask\nAveraged Motion-Blur\nRAW Bayer burst\n4032 x 3024\nLong Exposure\nOutput\nConventional \nExposure Output\nSubject detection\nGaze Saliency\nPortrait Mask\nPeople & Pet Faces\nSubject Weight Map\nMotion tracking and \nalignment\nFeature \nTracker\nAligned Motion \nStatistics\nSufficient \nmotion?\nProcess Next Frame\nProceed to Compositing\nNo\nYes\nWarped Feature Tracks\nDeciding whether to add more frames\nInter-frame motion prediction\nRendering\nUpsampling, \nCompositing & \nTone-Mapping\nAlignment \nSolver\nSaliency, Portrait & \nFace Prediction\nMotion\nPrediction\nImage warping\nWarped Downsampled 2x\nWarped Downsampled 8x\nDownsampled 8x\n8-bit Tone-mapped sRgb\nDownsampled 2x\n16-bit Linear RGB\nFig. 2. Our system processes raw burst images incrementally, from left to right on this diagram, first at low resolution (8x downsampling) for initial subject\npredictions, feature tracking, motion analysis and motion prediction. Images are also processed at half resolution (2x downsampling) to synthesize motion-blur\nbefore being upsampled back to full 12 megapixel resolution for compositing and tone-mapping the final results. More detail is provided in Section 3.\n2.3\nStabilizing the Background (foreground blur)\nImages captured by handheld cameras are often shaky and may often\ncontain parallax. In the foreground-blur case, we need to stabilize the\nbackground to keep the resulting regions sharp. This can be solved\nfor using structure-from-motion (SFM) techniques [Liu et al. 2014] to\nconstruct a 3-d representation of the scene [Hartley and Zisserman\n2004], then a stabilized camera path can be solved for in 3-d, and the\nscene finally re-rendered using the new smooth path [Liu et al. 2009].\nHowever, these techniques rely on 3-d reconstruction, for which a\nfast and robust implementation is challenging [Liu et al. 2011]. At the\nother end of the spectrum are 2-d stabilization techniques that use\nmuch simpler motion models such as global homographies or affine\ntransformations [Morimoto and Chellappa 1998]. These techniques\nare fast and robust, but cannot model effects such as parallax, rolling-\nshutter, or lens distortion. There is a large body of work that extends\nthese 2-d methods, such as using gyroscopic sensors only [Karpenko\net al. 2011], gyroscopes with face detection [Shi et al. 2019], targeted\ncrops [Grundmann et al. 2011] and trajectory filtering [Liu et al.\n2011]. Our method is analogous to techniques that start with 2-d\nfeature trajectories to estimate per-frame global transformations\nand refine this estimate with spatially varying image warps [Liu\net al. 2013; Zaragoza et al. 2013] to achieve the desired trade-off\nbetween speed and robustness.\n2.4\nSynthesizing Motion Trails\nThere is a large body of prior work on synthesizing motion-blur,\nin the context of non-photorealistic rendering [Lee et al. 2009],\nstop-motion animation [Brostow and Essa 2001], or 3D computer\ngraphics rendering in real-time [R\u00f8nnow et al. 2021] or offline [Lehti-\nnen et al. 2011; Navarro et al. 2011]. There is work describing single\nphotograph post-processing interactive applications to create artis-\ntic motion effects [Luo et al. 2018, 2020; Teramoto et al. 2010] or\nthat process multiple previously stabilized images and can achieve\nnon-physical renditions [Salamon et al. 2019].\nOur work compares more directly to prior work on computational\nlong exposure from multiple photographs or video frames. Lancelle\net al. [2019] describe a pipeline that can handle both foreground and\nbackground blur effects, but requires substantial user interaction to\nhandle all the cases. Like other approaches, they require significant\ncompute time in an offline processing application, as they rely on\nexpensive optical-flow based image warping or frame interpolation,\nto synthesize smooth motion-blur spanning the input frames pair-\nwise. In contrast, our pipeline is fully automatic, is integrated in a\nsmartphone photo camera and produces the result in a few seconds\nat 12 megapixel resolution. To synthesize motion-blur, we use a line\nkernel prediction neural network, derived from [Brooks and Barron\n2019], combined with a GPU rendering algorithm that can handle the\ninput image alignment warping implicitly, while producing smooth\nand curved motion-blur trails spanning multiple input frames.\n3\nSYSTEM OVERVIEW\nA diagram of our computational long-exposure system is shown\nin Figure 2. The stream of captured raw images is processed in-\ncrementally at two different resolutions through four stages, each\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:4\n\u2022\nTabellion et al.\ncorresponding to a row of the diagram in Figure 2: initial subject\ndetection, motion analysis, motion prediction and rendering. The\ninitial saliency and face prediction stage (Section 4.2) computes\nthe main signals for our subject detection, producing a normalized\nweight map. The motion analysis stage is responsible for tracking\n(Section 4.3) and aligning (Section 4.4) a set of feature points cor-\nresponding to the detected subject or to the background, and for\nselecting frames based on motion statistics (Section 4.5). The mo-\ntion prediction stage (Section 4.6) predicts dense line kernel and\nweight maps, that are used in the rendering stage (Section 4.7) to\nproduce smooth motion-blur spanning a given input frame pair. The\nfinal compositing stage (Section 4.8) layers the final results while\npreserving the sharpness of important areas in the final image.\nThe first three stages use as their input, images that have been\ntone-mapped and converted to sRGB, downsampled by a factor of 8\nto a low resolution of 504 x 376. This resolution is chosen to achieve\nlow latency when processing frames, which is dominated by the\ndense motion prediction neural network. This also ensures that the\nreceptive field covers most practical motion disparities in the input\nfull resolution images. The last stage however, uses the intentionally\nunder-exposed burst raw images converted to 16-bit linear RGB. The\nhigh bit-depth is necessary to preserve the scene\u2019s high dynamic\nrange during rendering, i.e. to avoid saturating the highlights and\nbanding artifacts in the shadows. Images are downsampled by a\nfactor of 2 as a trade-off that preserves enough detail in the final\nresult while operating within a reduced memory footprint.\nThe incremental processing loop converts and downsamples an\nadditional burst frame at each iteration, feeding the three last stages\nof our pipeline and resulting in an accumulated averaged motion-\nblur image. The loop stops when the frame selection criteria is\nreached, using an estimate of motion-blur trails\u2019 length. We then\ncomposite the final results, while upsampling images back to full\nresolution. At the end of our pipeline, images are converted to a low\ndynamic range 8-bit representation, using tone-mapping to preserve\nthe high dynamic range visual appearance of the scene.\n4\nIMPLEMENTATION\n4.1\nBurst Capture\nOur camera system captures frames at a rate of 30 frames per second,\nusing fully automatic aperture, shutter speed, and focus settings.\nThese settings may adjust dynamically to scene changes across\nframes, and our system performs well with all but abrupt variations.\nIn the background blur case, we target scenes with fast moving\nnearby subjects. When tapping the shutter button, the most recently\ncaptured frame is used as the base frame to produce the conventional\nexposure [Hasinoff et al. 2016], as it depicts the peak of the action\nchosen by the user. Up to 8 additional frames in the past may then\nbe selected by our algorithm to produce the background blur effect.\nWhen producing a foreground blur effect, we target scenes with\na much larger range of scene motion velocity, including slow and\nfar away moving content. To produce a compelling effect, this re-\nquires extending the capture for a duration up to several seconds,\naccording to scene motion. When the shutter button is pressed, we\nquickly analyze the scene motion statistics using the last 5 frames\nseen through the camera viewfinder and automatically determine a\nsubsequent capture duration that aims to satisfy our frame selection\ncriteria. We use a lightweight variant of the motion tracking and\nimage alignment described in the next few sections, that operates\nin under 50ms, to compute an estimate of scene velocity. With this\nestimate, under a constant velocity assumption, we trivially derive\na capture duration that yields the desired blur trail length (see Sec-\ntion 4.5). Given the extended capture duration of up to 7 seconds, we\nalso derive a frame processing rate, to select an evenly distributed\nsubset of up to 12 captured frames for processing, balancing the\ncompute budget with a suitable temporal sampling rate. The cap-\ntured frames selected for processing are queued up for immediate\nconcurrent processing by the following stages, thereby hiding some\nof the processing latency during capture.\n4.2\nAutomatic Subject Detection\nIn the background blur case, we want the effect of a fixed subject\nwith the rest of the world blurred behind them. Therefore, we au-\ntomatically detect and track the main subject, and align the input\nframes to negate its motion. The subject is represented as a weight\nmap, and is used in solving for the inverse subject motion alignment.\nThe main subject is first predicted using the proxy task of atten-\ntion saliency. For this task, we use a mobile-friendly 3-level U-Net\nwith skip connections, with an encoder comprised of 15 BlazeBlock\nwith 10 base channels [Bazarevsky et al. 2019] and a corresponding\ndecoder made of separable convolutions and bi-linear upsampling\nlayers. It is distilled from a larger model trained on the SALICON\ndataset [Jiang et al. 2015]. To focus on the peak of saliency in our\nsignal, we re-normalize predicted values to the interval [0, 1] and\nzero out values below a threshold (we empirically chose 0.43).\nThe saliency signal tends to peak on the subject center, so we com-\nplement it with a face signal, which helps keep subject faces sharp,\nwhich is especially important in subjects with complex articulated\nmotion. We compute the face signal by first predicting human, cat,\nand dog face regions, then feathering the resulting regions using a\nsmootherstep falloff [Ebert et al. 2003], and lastly masking it by a\nwhole-subject segmentation similar to that of [Wadhwa et al. 2018].\nWe combine the saliency and face signals as follows, to produce\nthe subject weight map with per pixel weight \ud835\udc64 = \ud835\udc60 (1 + \ud835\udc53 ), where\n\ud835\udc60 \u2208 [0, 1] is the saliency signal value and \ud835\udc53 \u2208 [0, 1] is the face\nsignal value, followed by a re-normalization to the interval [0, 1].\nThe face signal is also used in the compositing step to preserve face\nsharpness, as described in Section 4.8.\n4.3\nMotion Tracking\nWe use a feature tracking library based on [Grundmann et al. 2011]\nfor extracting the motion tracks used in subsequent image alignment.\nMotion track statistics are also used to select frames, to determine\nwhen sufficient motion has been captured in the scene.\nSubject tracking in background blur requires a high concentra-\ntion of tracks on the subject for stable, high quality alignments. As\na latency optimization, we use rejection sampling over an image\ngrid with cells of 5 x 5 pixels each, to generate feature tracks with\ndensity proportional to the subject weight map (Section 4.2). We\nonly attempt to extract feature tracks in cells where a sampled uni-\nform random variable \ud835\udc63 \u2208 [0, 1] is smaller than the corresponding\naverage track-weight at that grid location.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:5\nFig. 3. City scene. Top: Traffic moving through a busy city intersection with\nforeground motion vectors (red) and background motion vectors (green).\nBottom: Our foreground blur (left) and background blur (right) results.\n4.4\nImage Alignment\nGiven the feature track correspondences from Section 4.3, we first\nestimate global transforms to align all the frames to our reference\nframe. This cancels out overall camera motion, including both hand-\nshake and sweeping motions used to track subjects. The remaining\nimage alignment stages are specific to the desired motion-blur effect:\nforeground or background blur. For the purpose of illustration, we\npick an example scene that could be rendered as either: a taxicab\npassing through a busy city intersection as shown in Figure 3.\n4.4.1\nForeground blur. To keep the background as sharp as possible,\nwe must account for spatial effects such as parallax, rolling shutter,\nand lens distortion. After applying global transforms, we compute\na residual vector as the position difference between a transformed\ntracked feature and its corresponding position in the base frame. We\nthen use the residuals to estimate local refinement transforms on\na grid of vertices across the image. The resulting spatially varying\nwarp cancels motion in the background while retaining motion\nin the foreground, producing sharp backgrounds as in Figure 3\n(bottom-left).\nIn [Zaragoza et al. 2013], the authors weight points by distance\nfrom each grid vertex to produce a spatially varying as-projective-as-\npossible warp. Our approach to placing a grid and estimating local\ntransforms is similar, but we weight our points uniformly and use\na hard cut-off for point inclusion during local similarity transform\nestimation for better performance. The estimation is controlled\nby the support radius of each vertex (shown as magenta circle in\nFigure 4), i.e. the maximum distance from a vertex that a feature\npoint needs to be for inclusion in the local refinement estimation.\nWe found that setting this radius to 1.5 times the cell size of the\nmesh grid and using a grid size of 8 x 6 cells, was large enough for\nthe local refinement transforms to vary smoothly across the entire\nfield-of-view, yet small enough that disparate scene objects from\ndifferent parts of the scene do not affect each other. The estimated\ntransforms are applied to each vertex to then generate a spatially\nFig. 4. Spatially varying warp. Top-left: The background flow vectors (green)\ninside one of the mesh vertex (orange) support regions (magenta) contribute\nto its local displacement (yellow). Top-right: The resulting mesh and spatially\nvarying warp that aligns the background to that of the reference frame.\nForeground blur results using only a single global homography transform\n(bottom-left) and using our spatially varying mesh warp (bottom-right).\nInsets are displayed at 5x magnification.\nvarying mesh that aligns the background of any frame to that of the\nreference frame. To optimize latency, the application of this mesh is\nfolded into the downstream rendering stage by passing a texture of\nlocal 2-d displacement vectors to the GPU.\n4.4.2\nBackground blur. In this case, as shown in Figure 3 (bottom-\nright), we want the foreground to be as sharp as possible. We use the\nsubject mask from Section 4.2 to select the subset of feature tracks\nthat correspond to the foreground subject. With this as a starting\npoint, we further use spectral clustering to select the most salient\nmotion cluster to help discern the right motion segment to track\nand to remove outliers [Porikli 2004]. This is especially useful for\narticulated subjects, such as a running person whose limbs move\ndifferently from their torso.\nThe goal of our objective function is to balance two goals: (1)\nsubject sharpness: minimize the overall reprojection error of the\nsalient point correspondences, and (2) temporal smoothness: keep\nthe transformed background motion vectors as parallel as possible\nto those from the previous time step, as shown in Figure 5 and\nFigure 16.\nGiven a pair of frames \ud835\udc56 and \ud835\udc57 with point correspondences x\ud835\udc56 and\nx\ud835\udc57, we define a similarity transform that scales uniformly (\ud835\udc60\ud835\udc57,\ud835\udc56 \u2208 R1),\nrotates (Rj,i \u2208 \ud835\udc46\ud835\udc42(2)) and translates (tj,i \u2208 R2) 2-dimensional points\nfrom frame \ud835\udc56 to frame \ud835\udc57 as follows.\n\u02c6xj = \ud835\udc60\ud835\udc57,\ud835\udc56Rj,ixi + tj,i\n(1)\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:6\n\u2022\nTabellion et al.\nFor simplicity, we omit the from-and-to indices from the transform\nparameters \ud835\udc60, R and t and define our objective function as follows.\nminimize\n\ud835\udc60,R,t\n\ud835\udf06\ud835\udc53 \ud835\udc38\ud835\udc53 (\ud835\udc60, R, t) + \ud835\udf06\ud835\udc4f\ud835\udc38\ud835\udc4f (\ud835\udc60, R, t)\n(2)\nThe scalars \ud835\udf06\ud835\udc53 and \ud835\udf06\ud835\udc4f are the relative weights of the objective func-\ntion terms. The subject sharpness term \ud835\udc38\ud835\udc53 is defined on the fore-\nground points x \u2208 X\ud835\udc53 as the L-2 norm of the reprojection error of\ntransformed point correspondences (using Eq. 1) as follows.\n\ud835\udc38\ud835\udc53 (\ud835\udc60, R, t) =\n\u2211\ufe01\nx\u2208X\ud835\udc53\n\u2225xj \u2212 \ud835\udc60Rxi \u2212 t\u22252\n(3)\nThe background term \ud835\udc38\ud835\udc4f is used as a temporal smoothness prior\nto penalize background flow vectors that are not parallel to their\ncounterparts from the previous frame pair. Given three distinct\nframe indices \ud835\udc56, \ud835\udc57 and \ud835\udc58, this is defined using vector dot product as\na measure of parallelism as follows:\n\ud835\udc38\ud835\udc4f (\ud835\udc60\ud835\udc56,\ud835\udc58, R\ud835\udc56,\ud835\udc58, t\ud835\udc56,\ud835\udc58) =\n\u2211\ufe01\nx\u2208X\ud835\udc4f\nsmooth\n\ud835\udc3f1\n\u0012 |v\ud835\udc56,\ud835\udc57 \u00b7 v\ud835\udc57,\ud835\udc58 |\n\u2225v\ud835\udc56,\ud835\udc57 \u2225\u2225v\ud835\udc57,\ud835\udc58 \u2225\n\u0013\n(4)\nwhere v\ud835\udc5d,\ud835\udc5e = \u02c6x\ud835\udc5e \u2212x\ud835\udc5e is the directional displacement vector between\na point in the reference frame \ud835\udc5e and its transformed point correspon-\ndence from frame \ud835\udc5d. The smooth-\ud835\udc3f1 loss function from [Huber 1964]\nis used to prevent vanishing gradients during the optimization.\nsmooth\n\ud835\udc3f1\n(\ud835\udc65) =\n(\n0.5\ud835\udc652,\nif |\ud835\udc65| < 1\n|\ud835\udc65| \u2212 0.5,\notherwise\n(5)\nWithout loss of generality, we chose \ud835\udc56 = 0 as the fixed reference\nframe index, and set \ud835\udc58 = \ud835\udc57 + 1 in Eq. 4 to estimate the transforms\nincrementally for each new frame pair (\ud835\udc57, \ud835\udc57 + 1) in a single forward\npass through the frame sequence. We solved this non-linear opti-\nmization problem using the Ceres library [Agarwal et al. 2022]. We\nuse values 1 and 10 for \ud835\udf06\ud835\udc53 and \ud835\udf06\ud835\udc4f respectively, to balance the effect\nof both error terms.\nThe temporal regularization term \ud835\udc38\ud835\udc4f is not defined for the first\nframe pair, which we handle as a special case using a constraint\non the scale and rotation parameters, \ud835\udc60 and R, from Eq. 2. Our key\nobservation comes from subjects of background blur shots that\nundergo rotational motion in the image plane. Simply inverting\nthis rotation produces undesirable multiple sharp regions in the\nresult, as shown in Figure 5. In traditional panning photography, it\nis uncommon to attempt the rotation blur effect and exceedingly\ndifficult to achieve subject sharpness in this manner (i.e. rotating the\ncamera around an axis centered away from the subject). Instead, it\nis typically done panning the camera, tracking the overall trajectory\nof the subject, and our method aims for these outcomes.\nTo estimate the initial scale and rotation, we use the integrated\nestimation technique from [Zin\u00dfer et al. 2005]. We then constrain\nthe estimated rotation, R in Eq. 2, to prevent any additional sharp\nregions from detracting away from the sharp subject. We empirically\nfound that constraining the roll angle to 25% of its estimated value\nhelps make the blur field more linear, as shown in Figure 5, with\nonly the subject kept sharp as desired. More examples of image\nalignment for background-blur scenes are provided in Section 5.2.\n(a) Unconstrained rotation\n(b) Constrained rotation\nFig. 5. Undesirable rotations. Fully inverting the subject\u2019s rotation (a) gives\nus an undesirable result with an additional sharp region below the subject.\nEven though the rotating blur can be a fun effect, the sharpness region at\nthe center of rotation attracts the viewer\u2019s attention away from the main\nsubject and degrades the subject separation from the background, both of\nwhich goes against photography composition rules. We alleviate this by\nconstraining the estimated rotation (b).\n4.5\nFrame Selection\nOur system uses a frame selection mechanism that computes an es-\ntimate of motion-blur trails\u2019 length, to decide when the incremental\nframe processing outer-loop should stop (see Section 3). First, we\nuse the transformations computed by the alignment solver to trans-\nform the motion feature tracks to the reference space of the base\nframe, where they align spatially with the corresponding tracked\nfeatures\u2019 motion-blur trails in the output image. The length of each\naligned track can then be computed, and we use a high percentile\nof the track length distribution as an estimate of overall blur trail\nlength. This estimate is finally compared to a constant target setting,\nto decide whether the frame selection criteria is satisfied.\nWe measure the track length in percentage of image diagonal, a\nmetric that is largely insensitive to image resolution or aspect-ratio.\nIn the case of foreground blur, our criteria is for the 98th percentile\nto reach a target of 30%, producing relatively long and smooth blur\ntrails for the fastest moving object. In the background blur case,\nwe use the 80th percentile and a target of 2.8%, producing short\nblur trails for a larger area of the background, aiming to preserve\nsubject sharpness and avoid losing the context of the surrounding\nscene. These settings were derived empirically, iterating over large\ncollections of input bursts.\n4.6\nMotion Prediction\nOnce the input low-resolution images are aligned, we feed them\nthrough a motion-blur kernel-prediction neural network, one input\nframe pair at a time, predicting a pair of line and weight kernel\nmaps at each iteration. The low-resolution kernel maps are used to\nsynthesize motion-blur segments at half resolution, spanning the\ncorresponding input frames, as described in Section 4.7.\nThe motion prediction model is responsible for predicting the\nparameters of two spatial integrals along line segments, which ap-\nproximate the temporal integral defining the averaging of colors\nseen through each motion-blurred output pixel, during the cor-\nresponding time interval. We use a model based on [Brooks and\nBarron 2019], with further modifications that improve the trade-off\nbetween performance and image quality, allowing us to fit within a\nreasonable memory and compute budget on mobile devices.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:7\nTheir mathematical formulation predicts weight maps \ud835\udc4a\ud835\udc56 per\ninput frame \ud835\udc56 in a given image pair \ud835\udc58, with \ud835\udc41 = 17 channels, which\nare used to weigh each corresponding texture sample along the\npredicted line segments. We simplify this model by predicting only\na single channel, used to weigh the result of the integral from each\ninput frame. An example gray-scale map can be seen in Figure 2,\nshowing that the network predicts approximately equal weights\neverywhere across input images, except in areas of dis-occlusion\nwhere the weights favor the result from one of the two inputs. This\nsimplification significantly reduces system complexity and memory\nuse, and allows for more of the network capacity to be devoted to\npredicting the line segments.\nIn addition, we eliminate artifacts due to the predicted line seg-\nments\u2019 endpoint error [Zhang et al. 2016], causing them to meet\nimperfectly at the end of the spanned time interval, and resulting\nin very noticeable artifacts in the middle of blur trails, as illustrated\nin Figure 6. To avoid this issue, we scale the input image texture\nsamples further by a normalized decreasing linear ramp function\ud835\udc64\ud835\udc5b,\nthat favors samples close to the output pixel and gradually down-\nweighs samples further away along each predicted line segment.\nThe intensity of the output pixel (\ud835\udc65,\ud835\udc66) for the input frame pair \ud835\udc58 is:\n\ud835\udc3c\ud835\udc58 (\ud835\udc65,\ud835\udc66) =\n\u2211\ufe01\n\ud835\udc56\u2208{\ud835\udc58,\ud835\udc58+1}\n\ud835\udc4a\ud835\udc56 (\ud835\udc65,\ud835\udc66)\n\u00cd\ud835\udc41 \u22121\n\ud835\udc5b=0 \ud835\udc64\ud835\udc5b\n\ud835\udc41 \u22121\n\u2211\ufe01\n\ud835\udc5b=0\n\ud835\udc64\ud835\udc5b \ud835\udc3c\ud835\udc56 (\ud835\udc65\ud835\udc56\ud835\udc5b,\ud835\udc66\ud835\udc56\ud835\udc5b)\n(6)\nwith \ud835\udc64\ud835\udc5b = 1 \u2212 \ud835\udc5b/\ud835\udc41, and with sampled positions:\n\ud835\udc65\ud835\udc56\ud835\udc5b = \ud835\udc65 + (\n\ud835\udc5b\n\ud835\udc41 \u2212 1) \u0394\ud835\udc65\n\ud835\udc56 (\ud835\udc65,\ud835\udc66)\nand \ud835\udc66\ud835\udc56\ud835\udc5b = \ud835\udc66 + (\n\ud835\udc5b\n\ud835\udc41 \u2212 1) \u0394\ud835\udc66\n\ud835\udc56 (\ud835\udc65,\ud835\udc66)\nwhere \u0394\ud835\udc56 are the predicted line segments.\nWe also modify the network architecture as follows. First, we\nreplace the leaky ReLU convolution activations throughout, with a\nparameterized ReLU [He et al. 2015], where the slope coefficient is\nlearned. Next, to avoid common checkerboard artifacts [Odena et al.\n2016], we replace the 2x resampling layers to use average pooling\nfor downsampling, and bi-linear upsampling followed by a 2x2\nconvolution. This results in a model labeled \"Ours-large\" analyzed in\nSection 5. Furthermore, to improve the balance between the number\nof floating operations, number of parameters and receptive field, we\nfurther reduce the U-Net model topology to only 3 levels, where\neach level is using a 1x1 convolution, followed by a ResNet block [He\net al. 2016] with four 3x3 convolution layers. This results in a model\nlabeled \"Ours\" with significantly fewer learned parameters.\nAs shown in Figure 6, the ramp function \ud835\udc64\ud835\udc5b brings a significant\nbenefit to our learned single weight model, as it causes the predicted\nline segments to span spatially in each input image, the equivalent\nof the full time interval being integrated. When our model is trained\nwith this term ablated, resulting in the model \"Ours-abl.\", the net-\nwork predicts line segments that span approximately half of the\ntime interval on each side, causing the noticeable discontinuity in\nthe middle of blur trails. More examples can be found in the model\ncomparison analysis provided in Section 5.\n4.7\nRendering\nThe line and weight kernel maps output by the motion prediction\nnetwork are used by a renderer that synthesizes the motion-blurred\nimage. The renderer is implemented in an OpenCL kernel, which\n(a) Blur from single image pair\n(b) Without weight ramp (Ours-abl.)\n(c) With weight ramp (Ours)\nFig. 6. Motion prediction model ramp function ablation. (a) Rendering of\na motion-blurred moving train synthesized from a single input image pair.\nBoth a motion-blurred closeup of the front of the train and corresponding\ninput image pair overlaid with a vector field visualization representing the\npredicted line segments, is shown in (b) and (c) using the models \"Ours-abl.\"\nand \"Ours\", i.e. without and with the ramp function \ud835\udc64\ud835\udc5b, respectively. In\nimage (b)-left, the blue arrows indicate the full span of motion blur trails\nand the red arrows showcase the gap discontinuities in the middle of blur\ntrails that are most noticeable.\nruns very efficiently on the mobile device\u2019s GPU, leveraging the\nhardware texturing units while adaptively sampling texture lookups\nin the half resolution input images (the number of texture samples \ud835\udc41\nis adjusted proportionally to the length of the predicted line vectors).\nMotion prediction and rendering iterations can be performed one\ninput frame-pair at a time, producing piecewise-linear motion-blur\ntrails. Kernel maps are up-sampled from low to half-resolution by\nusing bi-linear texture lookups.\n4.7.1\nSpline interpolation. Piecewise-linear motion interpolation\nmay introduce jagged visual artifacts in motion trails. To interpolate\nthe motion more smoothly, we interpolate the inferred instanta-\nneous flow \u0394\ud835\udc56 between frames using cubic Hermite splines.\nThe instantaneous flow \ud835\udeff\ud835\udc56 at each pixel is inferred by constructing\na vector \ud835\udc3b (\u0394+\n\ud835\udc56 , \u0394\u2212\n\ud835\udc56 ) parallel to (\u0394+\n\ud835\udc56 + \u0394\u2212\n\ud835\udc56 ), with magnitude equal to\nthe harmonic mean of |\u0394+\n\ud835\udc56 | and |\u0394\u2212\n\ud835\udc56 |. Superscripts + and \u2212 refer to\ntime directions. If \u0394+\n\ud835\udc56 and \u0394\u2212\n\ud835\udc56 deviate by an angle \ud835\udf03 from a straight-\nline path, the vector is further scaled by a factor of (\ud835\udf03/sin\ud835\udf03) for\nsmaller angular deviations (< 90\u00b0), tapering this adjustment back\ntowards zero for larger deviations (where the path doubles back\nacutely) to avoid singularities. These correctional factors reduce\novershoot, and keep the parametric spline speed more stable for\nregions of moderate curvature.\n\ud835\udeff\ud835\udc56 = \ud835\udc3b (\u0394+\n\ud835\udc56 , \u0394\u2212\n\ud835\udc56 ) (\ud835\udf03/sin\ud835\udf03) \u00d7\n(\n1,\n\ud835\udf03 \u2264 \ud835\udf0b/2\n1 \u2212 (2\ud835\udf03/\ud835\udf0b \u2212 1)4,\n\ud835\udf03 > \ud835\udf0b/2\n(7)\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:8\n\u2022\nTabellion et al.\nFig. 7. Our spline extrapolation\nstrategy. See Section 4.7.1.\nFig. 8. Comparison of traditional\ngamma\ud835\udefe vs. our soft gamma\ud835\udefe\ud835\udc60. See\nSection 4.7.3.\nFor the accumulated blur of \ud835\udc3c\ud835\udc58 on the interval [k .. k+1] for output\nposition (x, y), we solve for a parametric 2D cubic spline path \ud835\udf0c(x, y,\nt) satisfying four constraints:\n\u2022 \ud835\udf0c(x, y, 0) = (x, y)\n\u2022 \ud835\udf0c(x, y, 1) = (x, y) + \u0394+\n\ud835\udc56 (\ud835\udc65,\ud835\udc66)\n\u2022 \ud835\udf0c\u2032(\ud835\udc65,\ud835\udc66, 0) = \ud835\udeff\ud835\udc56 (\ud835\udf0c(\ud835\udc65,\ud835\udc66, 0))\n\u2022 \ud835\udf0c\u2032(\ud835\udc65,\ud835\udc66, 1) = \ud835\udeff\ud835\udc56+1(\ud835\udf0c(\ud835\udc65,\ud835\udc66, 1))\nWe then accumulate the blur along this path by sampling uni-\nformly in parameter space, normalizing the weight of each sample\nto compensate for the non-uniform spatial sampling in image space\nin order to ensure spatially uniform brightness along motion trails.\nAt the burst endpoints we extrapolate the flow beyond the first\nand last frames by attempting to preserve the curvature of the flow\nthrough those endpoints. As shown in Figure 7: if \u2019C\u2019 represents the\nfinal frame in a burst, a motion trail position at the \"next\" frame D\nis extrapolated by reflecting A in the line bisecting BC (constructing\nA\u2019), then clamping the magnitude of CA\u2019 to |BC| to form CD. The\nflow at C is then inferred from points {B,C,D}.\n4.7.2\nFrame accumulation. In practice, the blur is accumulated in\nseveral passes: two passes per frame pair, weighted to fall off linearly\nbetween one frame and the next. For an output pixel at position p at\nframe \ud835\udc3c\ud835\udc56, the blur between frame \ud835\udc3c\ud835\udc56 and \ud835\udc3c\ud835\udc56+1 is accumulated by using\nthe aforementioned flow splines to determine the projected position\np\u2019 in frame \ud835\udc3c\ud835\udc56 at relative time t. For K frame pairs in the burst, 2K\nsuch passes (K forward, K backward) are computed and summed to\nproduce the final blur result. For each temporal direction:\n\ud835\udc3c (\ud835\udc65,\ud835\udc66) =\n\ud835\udc3e\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udc41 \u22121\n\u2211\ufe01\n\ud835\udc5b=0\n\ud835\udc3c\ud835\udc56 (\ud835\udf0c\ud835\udc56 (\ud835\udc65,\ud835\udc66,\ud835\udc61\ud835\udc5b)) |\ud835\udf0c\u2032\n\ud835\udc56 (\ud835\udc65,\ud835\udc66,\ud835\udc61\ud835\udc5b)| \ud835\udc64\ud835\udc5b\n(8)\n4.7.3\nSoft Gamma Colorspace. Very bright highlights (e.g. car head-\nlights) tend to saturate the camera sensor, resulting in their blurred\nmotion trails becoming unrealistically dim even when processed\nin linear colorspace. The clipping is due to the finite range of the\ninput sensor, and the brightness loss becomes noticeable when\nthe clipped input highlight energy is distributed (i.e. synthetically\nmotion-blurred) over many output pixels.\nTo work around this limitation, we process the blur in an in-\ntentionally non-linear colorspace, using an invertible gamma-like\n\"soft gamma\" function \ud835\udefe\ud835\udc60, shown in Figure 8, on the interval [0..1].\nThis adjusts the brightness curve in the opposite direction from a\nlinear-to-sRGB color transformation, emphasizing highlights with-\nout crushing shadows, allowing the nonlinear frames to be stored\nwith usable fidelity in 16-bit buffers. The function is applied to the\nwarped downsampled 2x buffers on creation, using a value of 3.0\nfor \ud835\udc58, and is later inverted (by reapplying with \ud835\udc58 = 1.0/3.0) after\naccumulating the blur for all frames. (See ablation in Section 5).\n\ud835\udefe\ud835\udc60 (\ud835\udc63) =\n\ud835\udc63\n\ud835\udc63 + (1 \u2212 \ud835\udc63) \ud835\udc58\n\u2248\n\ud835\udc63\ud835\udc58\n(9)\nThis is homologous to the Bias family of functions in [Schlick 1994],\nbut our reparameterization in Eq. 9 makes clearer the connection\nto the corresponding gamma curve with exponent \ud835\udc58. The idea of\nprocessing the blur in the modified colorspace was inspired by the\nOrdinal Transform technique in [Weiss 2006]. Our goal is similar to\nthe clipped highlight recovery technique in [Lancelle et al. 2019],\nwhich in comparison uses a more abrupt discontinuous highlight\nboosting function, that may further clip the signal.\n4.8\nCompositing\nThe synthetically blurred image described in Section 4.7 is computed\nat half resolution to satisfy device memory and latency constraints.\nAccordingly, even perfectly aligned, zero-motion regions of the\nblurred image will lose detail due to the upsampling of the result\ncomputed at half resolution. To preserve details, we composite the\nblurred image with a maximally sharp regular exposure where we\nexpect things to be sharp. Two categories need this protection: 1)\nstationary scene content, and 2) semantically important subjects\nwith little movement, as shown in Figure 9.\n(a) Fully sharp\n(b) Flow mask\n(c) Flow+face mask\n(d) Fully blurred\n(e) Flow protected\n(f) Flow+face protected\nFig. 9. Compositing. The regular exposure (a) is composited with the syn-\nthetically blurred image (d) to produce our final output with details pre-\nserved. A flow-based mask (b) protects nearly motionless image regions to\nproduce (e), note the preserved texture detail in the subject\u2019s dress. Further\nincluding face signals in the mask (c) also preserves moving, but semanti-\ncally important image regions (f).\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:9\nFig. 10. Several foreground and background blur examples produced by our system. Several more examples can be found in the supplementary material,\nalong with the corresponding regular exposure and a tone-mapped input burst example. Input RAW burst examples are available on our project webpage:\nhttps://motion-mode.github.io/.\nFor category 1, we produce a mask of pixels with very little motion\nacross the entire set of frame pairs, \ud835\udc40flow:\n(1) Compute a per-pixel maximum motion magnitude |\ud835\udc39 | across\nall frame pairs.\n(2) Compute a reference motion magnitude |\ud835\udc39 |ref that\u2019s effec-\ntively a robust max over all pixels in |\ud835\udc39 | (i.e., 99th percentile).\n(3) Rescale and clamp the per-pixel motion magnitudes such that\nanything below \ud835\udefc|\ud835\udc39 |ref is mapped to 0 and anything above\n\ud835\udefd|\ud835\udc39 |ref is mapped to 1. We use values 0.16 and 0.32 for \ud835\udefc and\n\ud835\udefd respectively.\n\ud835\udc40flow =\n|\ud835\udc39 | \u2212 \ud835\udefc|\ud835\udc39 |ref\n\ud835\udefd|\ud835\udc39 |ref \u2212 \ud835\udefc|\ud835\udc39 |ref\n(4) Apply a bilateral blur using the sharp image as a guide [He\net al. 2013], to ensure that any edges in \ud835\udc40flow correspond\nto real edges and minimize artifacts where the flow field is\nunreliable (e.g., uniform or texture-less regions like skies).\nCategory 2 is more complicated and breaks from the physical\nbehavior of optical motion blur in favor of aesthetics. E.g., if a scene\nhas two subjects moving with different trajectories, it would be\nimpossible to sharply align on both simultaneously. Even a single\nsubject can be impossible to align due to movement within the\nsubject, e.g., changes in facial expression, etc. An image with a\nblurry subject face is a (bad) blurry image. Our solution is to reuse\nthe semantic face signal described in 4.2, modified to only include\nthe faces that have low average feature movement in the aligned\nreference frame.\nFinally, we combine the flow and clipped face masks with a simple\nmax operator. Figure 9 shows the cumulative effect of the two mask\ntypes on the final composite.\n5\nRESULTS\nFigure 10 shows several foreground and background blur typical use\ncases, captured and processed using our system. The bursts were all\ncaptured hand-held and the results were generated fully automati-\ncally, without the need to adjust any settings. In both cases, what\nmakes these long exposure photographs successful is the presence\nand contrast between sharp and blurry elements.\nThe on-device latency of our system varies according to the num-\nber of frames selected for processing. The latency for the main stages\n(see Figure 2), measured on a Google Pixel 7 device, are as follows.\nSubject detection, including 8x downsampling and tone-mapping of\nthe base frame: 330ms; motion tracking and alignment, including\n8x downsampling and tone-mapping: 55ms per frame; inter-frame\nmotion prediction, including concurrent 2x downsampling and RAW\nto linear conversion: 77ms per selected frame pair; rendering: 42ms\nper selected frame pair; final upsampling, compositing and tone-\nmapping of both image results: 790ms. In the background blur case,\na small number of frames are typically selected (e.g. 4), leading to a\nshort total latency (e.g. 1.7s). In the foreground blur case, a higher\nnumber of frames are typically selected (e.g. 12) but most of the pro-\ncessing is happening during the extended capture (see Section 4.1)\nand the latency is therefore largely hidden from the user.\n5.1\nTrack Weights Comparison\nIn the following ablation, we compare the effect of including face-\nregion upweighting in motion track weight maps for background-\nblur alignment quality. (Please refer to Section 4.2 for more details).\nWe find that including both gaze saliency and face detections in\nthe motion track weight map benefits image subjects with complex\narticulated motion (which can cause the wrong part of the subject\nto be tracked). A representative example is shown in Figure 11.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:10\n\u2022\nTabellion et al.\n(a) Without face semantic masking\n(b) With face semantic masking\nFig. 11. Up-weighting track weights in faces helps subjects with com-\nplex/articulated motion. Larger images are the long exposure results ren-\ndered with compositing masking disabled (as in Figure 9d) for clarity of\nthe comparison. To their right are the intermediate outputs: track weights\nare visualized as added green/red channel values, motion track clusters in\nthe top image, and the highest weight selected cluster in the bottom image.\n(a) Gaze saliency alone peaks the weights on the cat\u2019s center, assigning\nthe highest weight to the tracks cluster on the cat\u2019s body - resulting in\nundesirable alignment. (b) Our pipeline\u2019s results: up-weighting the face-\nregion causes its motion cluster to be selected, resulting in the desired long\nexposure tracking the cat\u2019s face.\n5.2\nImage Alignment Comparison\nIn Figure 16, we showcase additional examples of image alignment\non background-blur scenes, comparing the aesthetics of results\nwhen the regularization term \ud835\udc38\ud835\udc4f from Eq. 2 is excluded (left) and\nincluded (right). In the left-hand side column of Figure 16, we ob-\nserve that optimizing just for the subject\u2019s sharpness \ud835\udc38\ud835\udc4f doesn\u2019t\naccount for the background of the scene. Consequently, sudden\nchanges in transform parameters over time are allowed, resulting in\ndifferent parts of the field of view having motion blur in completely\ndifferent directions. By adding the temporal regularization term \ud835\udc38\ud835\udc4f,\nwe get the results on the right-hand side column of Figure 16 with\nconsistent blur trails. The second example showcases the effect of\ndampening the rotational parameter, avoiding the blur vortex (green\ninsets).\n5.3\nMotion Prediction Comparison\nWe compare models described in Section 4.6 with those from [Brooks\nand Barron 2019] that use uniform weights, labelled \"BB19-uni.\",\nand that learn \ud835\udc41 = 17 weights per input image, labelled \"BB19\".\nAll the compared models were trained with the same hyper-\nparameters described in [Brooks and Barron 2019]. To supervise the\ntraining, we generate a bracketed dataset of input image triplets\nfrom many videos, as described in [Reda et al. 2022], synthesizing\nthe pseudo ground-truth motion-blurred image using a previously\ntrained FILM frame interpolation model. To evaluate our model we\nused a test set with 2000 examples and report the PSNR and SSIM,\ni.e. comparing the synthesized motion-blur image to the pseudo\nground-truth, in Table 1.\nA visual comparison on 512 x 384 image crops is provided in\nFigure 17 and shows that our model performs visually similarly to\n\"BB19\" (e.g. blur smoothness, handling of dis-occlusions), despite the\nsignificant simplifications of our implementation to run on mobile\ndevices. It also reveals that both models \"Ours-abl.\" and \"BB19-uni.\"\nTable 1. Comparing motion prediction models evaluation performance\n(PSNR and SSIM) and properties: number of learned parameters in mil-\nlions (M-Par.), number of floating point operations at the evaluated image\ninput resolution in billions (B-Flop) and receptive field computed based\non [Dumoulin and Visin 2016] in pixels (Rec. Field). Our simplified model\nto run on mobile devices, shows comparable quality performance to models\nwith 1.7 times as many parameters, both quantitatively and qualitatively\n(shown in Figure 17).\nModel\nPSNR.\nSSIM\nM-Par.\nB-Flop\nRec. Field\nBB19\n41.78\n0.9862\n7.057\n107.28\n202\nBB19-uni.\n40.07\n0.9803\n7.056\n106.81\n202\nOurs-large.\n41.32\n0.9842\n7.301\n107.40\n202\nOurs\n40.78\n0.9823\n4.173\n114.67\n128\nOurs-abl.\n40.61\n0.9808\n4.173\n114.67\n128\nsuffer from the same discontinuity artifacts in the middle of blur\ntrails, which are described in Section 4.6.\nOur model runs in under 80ms on a Google Pixel 7 mobile device\u2019s\nTPU [Gupta 2021], when given an input image pair from our low\nresolution image pipeline.\n5.4\nRendering Comparison\n5.4.1\nMotion Interpolation. In Figure 12, we compare the effects\nof piecewise-linear flow interpolation vs cubic spline interpolation.\nParticularly when camera or object motion from frame to frame is\nirregular, spline interpolation can impart a much more natural and\nphotorealistic appearance to the motion trails.\nFig. 12. Comparison of interpolation methods described in Section 4.7.1 on\na scene with a car traversing a roundabout. Top: An intermediate frame\nfrom the input burst. Bottom, left to right: average of several successive\ninput frames, linear flow interpolation, spline flow interpolation. Piecewise-\nlinear motion trails look synthetically generated, revealing the number\nof input frame pairs used to render the image, whereas curved motion\ntrails look optically generated as in a single continuous long exposure. See\nsupplementary material for more examples.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:11\n(a) sRGB\n(b) Linear\n(c) Soft gamma\nFig. 13. Colorspace comparison of the blurring operation. (a) sRGB col-\norspace blur loses most of the motion-blurred highlights intensity. (b) Linear\ncolorspace is physically correct but produces dull blur trails due to clipping\noccurring in the sensor prior to blurring. (c) Soft gamma colorspace blur, de-\nscribed in Section 4.7.3, is able to preserve strong motion-blurred highlights\nand increases blur trails contrast and color saturation. See supplementary\nmaterial for more examples.\n5.4.2\nRendering Colorspace. In Figure 13, we compare the results\nof performing the blurring operation in a conventional sRGB col-\norspace, versus a linear physically correct colorspace, versus our\nnon-physical \"soft gamma\" colorspace, obtained by adjusting the\nlinear-space image in a direction opposite from a usual linear to\nsRGB color transformation. The figure illustrates how blurring in\nthe soft-gamma colorspace emphasizes and preserves the brightness\nof the motion trails in the highlights and generally increases their\ncontrast and color saturation.\n5.5\nComparison to Mobile Phone Camera Applications\nUnlike other works which realize a long exposure effect [Lancelle\net al. 2019; Lee et al. 2009; Luo et al. 2018, 2020; Mikamo et al.\n2021; Teramoto et al. 2010], our pipeline is a responsive mobile\nphone capture experience. Therefore, we also compare our results\nto released capture experiences for consumer phones.\nSeveral mobile applications allow a more manual control of the\ncapture schedule on mobile phones such as Even Longer, Moment,\nNeoshot, and Procam 8 (all available in the iOS App Store). These\napps do not seem to have frame alignment capability, and therefore\nrequire a tripod for capturing sharp long exposure images. Spectre,\nreleased on iOS, seems to have stabilization and auto-exposure\ncapabilities. Through a capture study of dozens of scenes, we found\nthe hand-held performance of Spectre to be inconsistent. Figure 14\nshows representative comparisons of the results of our pipeline with\nSpectre.\nTo our knowledge, our pipeline is the only mobile-phone capture\nexperience with all of the following features: background-blur align-\nment (automatically tracking and keeping a moving subject sharp),\nrobust foreground-blur alignment (keeping the background sharp),\nmotion interpolation for smooth motion trails (compared to results\nshowing temporal undersampling), and face-region sharpness pro-\ntection (keeping slightly moving subjects sharp).\n(a) Spectre\n(b) Ours\nFig. 14. Comparison of the app Spectre (a) [Spectre [n.d.]] vs. our method\n(b), on scenes captured hand-held. The light trail scene in the middle row\nwas captured in very windy conditions. Our pipeline shows better back-\nground alignment, subject preservation, and more noticeable motion trails.\nA more extensive comparison with a few additional apps can be found in\nthe supplement.\n5.6\nEvaluation on Existing Datasets\nIn Figure 15, we evaluate our method on the publicly available video\ndataset in [Liu et al. 2014]. The images are also available in the\nsupplement, and can be compared to their results (see their Figure\n9), as well as results in [Lancelle et al. 2019] (see their Figure 15).\nOur automatic subject detection aligns the result on people\u2019s faces\nwhen they are detected, and on visually salient features otherwise,\nwhich matches the selected subject from previous work in many\nscenes. When multiple faces are detected, our method chooses to\nalign on the largest face, which may lead to a different outcome\n(Figure 15a middle-right and lower-right examples). We also observe\na possible mismatch when no faces are present or are too small to be\ndetected, e.g. while our saliency signal reacts to the most colorful or\nbrightest areas of the image (Figure 15a lower-left and lower-middle\nexamples respectively).\nEven though we use a simple 2D image alignment approach (see\nSection 4.4.2), our method leads to comparable subject stabilization\nin most cases. Our similarity transform solver is able to preserve\nsubject sharpness and models a relative virtual camera motion that\nis similar to that of compared works and is sometimes more accurate\n(Figure 15a center and Figure 15b right examples).\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:12\n\u2022\nTabellion et al.\n(a) Our pipeline, without using the segmentation mask data.\n(b) Our pipeline, using a manual saliency override mask.\nFig. 15. Several examples showing our method evaluated on the input video\ndataset in [Liu et al. 2014]. We include a few examples in (a) middle-right\nand lower row, where our subject detection leads to a different outcome\nvs. their 3D aware segmentation-driven approach. We add corresponding\ncomparable results in (b), obtained by manually overriding our saliency\nsignal with a subject mask.\nOur rendering approach is most similar to [Lancelle et al. 2019]\nbut differs in the implementation to interpolate motion between\nframes. Our method scales to very high resolution images in an\nefficient manner on a mobile device, and we find the resulting\nmotion-blur quality to be comparable. Both works benefit from\nintegrating motion-blur from the input images spanning the whole\ntime-interval, unlike the approach in [Liu et al. 2014], which uses a\nspatially-varying blur of only the base frame. Our method correctly\nrenders the dynamic motion of the scene handling dis-occlusions,\nshowing other moving scene objects, people\u2019s moving limbs, and\nthe motion of the background, all relative to the subject and as seen\nthrough the virtual camera aligned over time. In contrast, blurring\nonly the base frame assumes the scene is static and only the aligned\ncamera transformation affects the blur. This is most noticeable when\ncomparing our turtle result in Figure 15b-left to theirs. Our system\nrenders the relative coral motion direction correctly, as can be seen\nin the input video, and shows the turtle\u2019s moving fin and the swirling\nindividual motion of surrounding fish.\nThe amount of blur is normalized by our frame selection algo-\nrithm described in Section 4.5. Our stylistic background blur length\ntarget is shorter than the results in [Lancelle et al. 2019], and is mo-\ntivated by the goal to preserve subject sharpness and scene context.\n6\nLIMITATIONS AND FUTURE WORK\nBackground blur scenes with very small subjects tend to signifi-\ncantly increase the occurrence of saliency and portrait mask mis-\npredictions and feature tracking errors, ultimately resulting in an\nundesirable alignment and preserving the sharpness of the incorrect\nimage region. Although our system can handle reasonably small\nsubjects, this problem can be improved further by refining these\npredictions using the appropriate sub-region of the input images\ndown-sampled at a higher resolution.\nOur motion prediction model with receptive field window of 128\npixels can handle approximately 64 pixels of motion disparity at the\nchosen input low resolution. In our system, this corresponds to 512\npixels of disparity at full resolution, which is a good practical upper\nbound when capturing 12 megapixel bursts at 30fps. Larger motion\ndisparities across frame pairs cause inaccurate predicted kernel\nmaps and result in significant artifacts in the synthesized motion-\nblur. When these rare cases are detected in the motion analysis stage\nof our pipeline, we decide to output only the sharp exposure.\nNone of the models we tested perfectly handle motion silhou-\nettes, when the foreground and background are moving between\nperpendicular and opposite directions or in the presence of large\nmotion, causing swirly looking or disocclusion artifacts. We also\nnotice similar artifacts in areas where a cast shadow moves in a\ndifferent direction than the object where the shadow is projected\nonto. Some examples can be found in the supplementary material in\nFigure 1. Resolving these challenging cases is left for future work.\n7\nCONCLUSION\nIn this paper, we described a long exposure computational photog-\nraphy system, that is able to produce high quality long exposure\nforeground or background blur effects. Our system operates in a\nsmartphone camera app and outputs both long and conventional\nexposures fully automatically, in just a few seconds after pressing\nthe shutter button. We described the key elements that make our\nsystem successful: adapting the burst capture schedule to scene mo-\ntion velocity, separating the main subject from the background and\ntracking their motion, creating custom aesthetic alignments of input\nburst images, synthesizing smooth curved motion-blur spanning\nmultiple underexposed HDR sharp input images, and compositing\nsharp and motion-blurred results to protect important scene loca-\ntions - exceeding what would be physically possible with traditional\nlong-exposure photography. The careful combination of these ele-\nments gives our system the ability to produce aesthetically pleasing\nblur trails and to preserve sharpness where it is most needed. The\nend-to-end integration of our system into a consumer mobile device\ncamera makes it possible for casual users to access a creative style\npreviously reserved to more advanced photographers.\nACKNOWLEDGMENTS\n\"Motion Mode\" and the work described in this paper is the result of\nmany individuals\u2019 effort in Google Research\u2019s Creative Camera team\nand the Google Camera App team, with help from many collaborat-\ning partners in the Gcam, reModel, Pixel and Silicon teams. We are\nespecially thankful to Janne Kontkanen for leading the research on\nFILM, to Gabriel Nava, Shamvi Punja, Qiurui He, Alex Schiffhauer,\nSteven Hickson, Po-Ya Hsu, Zhijie Deng, Yen-Hsiang Huang, Kiran\nMurthy and Sam Hasinoff for their contributions, and to Tim Brooks\nand Jon Barron for early research. Credit for photography and im-\nage quality evaluations goes to our staff photographers Michael\nMilne, Andy Radin, Nicholas Wilson, Brandon Ruffin, as well as\nChris Haney, Sam Robinson, Christopher Farro and Cort Muller. We\nare also very grateful to Orly Liba, Cassidy Curtis, Kfir Aberman\nand David Salesin for their help reviewing early drafts of this paper.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:13\nREFERENCES\nKfir Aberman, Ju He, Yossi Gandelsman, Inbar Mosseri, David E. Jacobs, Kai Kohlhoff,\nYael Pritch, and Michael Rubinstein. 2022. Deep Saliency Prior for Reducing Visual\nDistraction. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE Computer Society, Los Alamitos, CA, USA, 19819\u201319828.\nhttps:\n//doi.org/10.1109/CVPR52688.2022.01923\nSameer Agarwal, Keir Mierle, and The Ceres Solver Team. 2022. Ceres Solver. Google,\nInc. https://github.com/ceres-solver/ceres-solver\nValentin Bazarevsky, Yury Kartynnik, Andrey Vakunov, Karthik Raveendran, and\nMatthias Grundmann. 2019. BlazeFace: Sub-millisecond Neural Face Detection\non Mobile GPUs. ArXiv abs/1907.05047 (2019).\nTim Brooks and Jonathan T Barron. 2019. Learning to Synthesize Motion Blur. In\n2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE\nComputer Society, Los Alamitos, CA, USA, 6833\u20136841.\nhttps://doi.org/10.1109/\nCVPR.2019.00700\nGabriel J. Brostow and Irfan Essa. 2001. Image-Based Motion Blur for Stop Motion\nAnimation. In Proceedings of the 28th Annual Conference on Computer Graphics and\nInteractive Techniques (SIGGRAPH \u201901). Association for Computing Machinery, New\nYork, NY, USA, 561\u2013566. https://doi.org/10.1145/383259.383325\nVincent Dumoulin and Francesco Visin. 2016. A guide to convolution arithmetic for\ndeep learning. https://doi.org/10.48550/ARXIV.1603.07285\nDavid S Ebert, F Kenton Musgrave, Darwyn Peachey, Ken Perlin, and Steven Worley.\n2003. Texturing & modeling: a procedural approach. Morgan Kaufmann.\nMatthias Grundmann, Vivek Kwatra, and Irfan Essa. 2011. Auto-directed video stabi-\nlization with robust L1 optimal camera paths. In 2011 IEEE Conference on Computer\nVision and Pattern Recognition. IEEE Computer Society, Los Alamitos, CA, USA,\n225\u2013232. https://doi.org/10.1109/CVPR.2011.5995525\nMonika Gupta. 2021. Google Tensor is a milestone for machine learning.\nhttps:\n//blog.google/products/pixel/introducing-google-tensor\nR. I. Hartley and A. Zisserman. 2004. Multiple View Geometry in Computer Vision\n(second ed.). Cambridge University Press, ISBN: 0521540518.\nSamuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T. Barron,\nFlorian Kainz, Jiawen Chen, and Marc Levoy. 2016. Burst Photography for High\nDynamic Range and Low-Light Imaging on Mobile Cameras. ACM Trans. Graph. 35,\n6, Article 192 (dec 2016), 12 pages. https://doi.org/10.1145/2980179.2980254\nKaiming He, Jian Sun, and Xiaoou Tang. 2013. Guided Image Filtering. IEEE Transactions\non Pattern Analysis and Machine Intelligence 35, 6 (2013), 1397\u20131409. https://doi.\norg/10.1109/TPAMI.2012.213\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving Deep into\nRectifiers: Surpassing Human-Level Performance on ImageNet Classification. In 2015\nIEEE International Conference on Computer Vision (ICCV). IEEE Computer Society,\nLos Alamitos, CA, USA, 1026\u20131034. https://doi.org/10.1109/ICCV.2015.123\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning\nfor Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). IEEE Computer Society, Los Alamitos, CA, USA, 770\u2013778. https:\n//doi.org/10.1109/CVPR.2016.90\nPeter J. Huber. 1964. Robust Estimation of a Location Parameter. The Annals of Mathe-\nmatical Statistics 35, 1 (1964), 73 \u2013 101. https://doi.org/10.1214/aoms/1177703732\nMing Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao. 2015. SALICON: Saliency\nin Context. In 2015 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE Computer Society, Los Alamitos, CA, USA, 1072\u20131080. https://doi.\norg/10.1109/CVPR.2015.7298710\nAlexandre Karpenko, David Jacobs, Jongmin Baek, and Marc Levoy. 2011. Digital\nvideo stabilization and rolling shutter correction using gyroscopes. Technical Report.\nStanford University CS. 13 pages.\nM. Lancelle, P. Dogan, and M. Gross. 2019. Controlling Motion Blur in Synthetic\nLong Time Exposures. Computer Graphics Forum 38, 2 (2019), 393\u2013403.\nhttps:\n//doi.org/10.1111/cgf.13646\nH. Lee, C. H. Lee, and K. Yoon. 2009. Motion based Painterly Rendering. Computer\nGraphics Forum 28, 4 (2009), 1207\u20131215. https://doi.org/10.1111/j.1467-8659.2009.\n01498.x\nJaakko Lehtinen, Timo Aila, Jiawen Chen, Samuli Laine, and Fr\u00e9do Durand. 2011.\nTemporal Light Field Reconstruction for Rendering Distribution Effects. ACM Trans.\nGraph. 30, 4, Article 55 (jul 2011), 12 pages. https://doi.org/10.1145/2010324.1964950\nOrly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil Karnad, Qiurui\nHe, Jonathan T. Barron, Dillon Sharlet, Ryan Geiss, Samuel W. Hasinoff, Yael Pritch,\nand Marc Levoy. 2019. Handheld Mobile Photography in Very Low Light. ACM\nTrans. Graph. 38, 6, Article 164 (nov 2019), 16 pages. https://doi.org/10.1145/3355089.\n3356508\nFeng Liu, Michael Gleicher, Hailin Jin, and Aseem Agarwala. 2009. Content-Preserving\nWarps for 3D Video Stabilization. ACM Trans. Graph. 28, 3, Article 44 (jul 2009),\n9 pages. https://doi.org/10.1145/1531326.1531350\nFeng Liu, Michael Gleicher, Jue Wang, Hailin Jin, and Aseem Agarwala. 2011. Subspace\nVideo Stabilization. ACM Trans. Graph. 30, 1, Article 4 (feb 2011), 10 pages. https:\n//doi.org/10.1145/1899404.1899408\nShuaicheng Liu, Jue Wang, Sunghyun Cho, and Ping Tan. 2014. TrackCam: 3D-Aware\nTracking Shots from Consumer Video. ACM Trans. Graph. 33, 6, Article 198 (Nov\n2014), 11 pages. https://doi.org/10.1145/2661229.2661272\nShuaicheng Liu, Lu Yuan, Ping Tan, and Jian Sun. 2013. Bundled Camera Paths for\nVideo Stabilization. ACM Trans. Graph. 32, 4, Article 78 (jul 2013), 10 pages. https:\n//doi.org/10.1145/2461912.2461995\nXuejiao Luo, Nestor Z. Salamon, and Elmar Eisemann. 2018. Adding Motion Blur\nto Still Images. In Proceedings of the 44th Graphics Interface Conference (GI \u201918).\nCanadian Human-Computer Communications Society, Waterloo, CAN, 108\u2013114.\nhttps://doi.org/10.20380/GI2018.15\nXuejiao Luo, Nestor Z. Salamon, and Elmar Eisemann. 2020. Controllable Motion-Blur\nEffects in Still Images. IEEE Transactions on Visualization and Computer Graphics 26,\n7 (2020), 2362\u20132372. https://doi.org/10.1109/TVCG.2018.2889485\nMichihiro Mikamo, Ryo Furukawa, and Hiroshi Kawasaki. 2021. A Method for Adding\nMotion-Blur on Arbitrary Objects by Using Auto-Segmentation and Color Compen-\nsation Techniques. In 2021 IEEE International Conference on Image Processing (ICIP).\n1854\u20131858. https://doi.org/10.1109/ICIP42928.2021.9506443\nC. Morimoto and R. Chellappa. 1998. Evaluation of image stabilization algorithms.\nIn Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and\nSignal Processing, ICASSP \u201998 (Cat. No.98CH36181), Vol. 5. 2789\u20132792 vol.5. https:\n//doi.org/10.1109/ICASSP.1998.678102\nFernando Navarro, Francisco J. Ser\u00f3n, and Diego Gutierrez. 2011. Motion Blur Ren-\ndering: State of the Art. Computer Graphics Forum 30, 1 (2011), 3\u201326.\nhttps:\n//doi.org/10.1111/j.1467-8659.2010.01840.x\nAugustus Odena, Vincent Dumoulin, and Chris Olah. 2016. Deconvolution and Checker-\nboard Artifacts. Distill (2016). https://doi.org/10.23915/distill.00003\nFatih Porikli. 2004. Learning object trajectory patterns by spectral clustering. In 2004\nIEEE International Conference on Multimedia and Expo (ICME), Vol. 2. IEEE Computer\nSociety, Los Alamitos, USA, 1171\u20131174. https://doi.org/10.1109/ICME.2004.1394427\nFitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and\nBrian Curless. 2022. FILM: Frame Interpolation For Large Motion. In Computer\nVision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part VII. Springer-Verlag, Berlin, Heidelberg, 250\u2013266. https://doi.org/\n10.1007/978-3-031-20071-7_15\nMads J.L. R\u00f8nnow, Ulf Assarsson, and Marco Fratarcangeli. 2021. Fast analytical\nmotion blur with transparency. Computers & Graphics 95 (2021), 36\u201346.\nhttps:\n//doi.org/10.1016/j.cag.2021.01.006\nNestor Z. Salamon, Markus Billeter, and Elmar Eisemann. 2019. ShutterApp: Spatio-\ntemporal Exposure Control for Videos. Computer Graphics Forum 38, 7 (2019),\n675\u2013683. https://doi.org/10.1111/cgf.13870\nChristophe Schlick. 1994. Fast Alternatives to Perlin\u2019s Bias and Gain Functions. Academic\nPress Professional, Inc., USA, 401\u2013403.\nFuhao Shi, Sung-Fang Tsai, Youyou Wang, and Chia-Kai Liang. 2019. Steadiface: Real-\nTime Face-Centric Stabilization On Mobile Phones. In 2019 IEEE International Con-\nference on Image Processing (ICIP). 4599\u20134603. https://doi.org/10.1109/ICIP.2019.\n8803679\nSpectre. [n.d.]. Spectre app. https://spectre.cam. Accessed: 2023-01-17.\nMichael Stengel, Pablo Bauszat, Martin Eisemann, Elmar Eisemann, and Marcus Magnor.\n2015. Temporal Video Filtering and Exposure Control for Perceptual Motion Blur.\nIEEE Transactions on Visualization and Computer Graphics 21, 5 (2015), 663\u2013671.\nhttps://doi.org/10.1109/TVCG.2014.2377753\nOkihide Teramoto, In Kyu Park, and Takeo Igarashi. 2010. Interactive Motion Pho-\ntography from a Single Image. Vis. Comput. 26, 11 (nov 2010), 1339\u20131348. https:\n//doi.org/10.1007/s00371-009-0405-6\nNeal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori Kanazawa, Robert\nCarroll, Yair Movshovitz-Attias, Jonathan T. Barron, Yael Pritch, and Marc Levoy.\n2018. Synthetic Depth-of-Field with a Single-Camera Mobile Phone. ACM Trans.\nGraph. 37, 4, Article 64 (jul 2018), 13 pages. https://doi.org/10.1145/3197517.3201329\nBen Weiss. 2006. Fast Median and Bilateral Filtering. ACM Trans. Graph. 25, 3 (jul 2006),\n519\u2013526. https://doi.org/10.1145/1141911.1141918\nBartlomiej Wronski, Ignacio Garcia-Dorado, Manfred Ernst, Damien Kelly, Michael\nKrainin, Chia-Kai Liang, Marc Levoy, and Peyman Milanfar. 2019. Handheld Multi-\nFrame Super-Resolution. ACM Trans. Graph. 38, 4, Article 28 (jul 2019), 18 pages.\nhttps://doi.org/10.1145/3306346.3323024\nJulio Zaragoza, Tat-Jun Chin, Michael S. Brown, and David Suter. 2013. As-Projective-\nAs-Possible Image Stitching with Moving DLT. In 2013 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE Computer Society, Los Alamitos, CA,\nUSA, 2339\u20132346. https://doi.org/10.1109/CVPR.2013.303\nCongxuan Zhang, Ling ling Zhu, Zhen Chen, Ding ding Kong, and Xuan Shang. 2016.\nAn Improved Evaluation Method for Optical Flow of Endpoint Error. In Proceedings\nof the International Conference on Computer Networks and Communication Technology\n(CNCT 2016). Atlantis Press, 312\u2013317. https://doi.org/10.2991/cnct-16.2017.44\nTimo Zin\u00dfer, Jochen Schmidt, and Heinrich Niemann. 2005. Point set registration with\nintegrated scale estimation. In International conference on pattern recognition and\nimage processing. 116\u2013119.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n48:14\n\u2022\nTabellion et al.\nFig. 16. Background blur aesthetics visualized across different parts of the field of view (red and blue insets) comparing results without (left), and with (right)\nthe regularization term \ud835\udc38\ud835\udc4f from Eq. 2. By additionally imposing the rotational constraints from Section 4.4, we remove an undesirable sharp region surrounded\nby a blur vortex (green inset). Insets are displayed at 5x magnification.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\nComputational Long Exposure Mobile Photography\n\u2022\n48:15\n(a) Input average\n(b) FILM (ground truth)\n(c) BB19-uni.\n(d) BB19\n(e) Ours\n(f) Ours-abl.\nFig. 17. Model comparisons from single input image pairs shown overlaid in column (a). Column (b) is rendered using recursive frame interpolation with the\nFILM model from [Reda et al. 2022], and is used as ground truth when training our models. Columns (c) and (d) are rendered with the model from [Brooks\nand Barron 2019], with uniform and learned weights respectively. Columns (e) and (f) are rendered with our model, with and without the ramp function\n\ud835\udc64\ud835\udc5b respectively. Differences are subtle, showing that our mobile model simplifications do not affect image quality substantially. The examples on each row\nshowcase blur quality and disocclusions with various amount of motion disparity. The last row contains disocclusions with opposite motion. Several additional\nchallenging examples can be found in the supplementary material, we encourage the reader to compare the accompanying images, to see differences more\nclearly.\nACM Trans. Graph., Vol. 42, No. 4, Article 48. Publication date: August 2023.\n"
  },
  {
    "title": "TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations",
    "link": "https://arxiv.org/pdf/2308.01499.pdf",
    "upvote": "2",
    "text": "1\nTDMD: A Database for Dynamic Color Mesh\nSubjective and Objective Quality Explorations\nQi Yang, Joel Jung, Timon Deschamps, Xiaozhong Xu, Member, IEEE, and Shan Liu, Fellow, IEEE\nAbstract\u2014Dynamic colored meshes (DCM) are widely used in various applications; however, these meshes may undergo different\nprocesses, such as compression or transmission, which can distort them and degrade their quality. To facilitate the development of\nobjective metrics for DCMs and study the influence of typical distortions on their perception, we create the Tencent - dynamic colored\nmesh database (TDMD) containing eight reference DCM objects with six typical distortions. Using processed video sequences (PVS)\nderived from the DCM, we have conducted a large-scale subjective experiment that resulted in 303 distorted DCM samples with mean\nopinion scores, making the TDMD the largest available DCM database to our knowledge. This database enabled us to study the impact\nof different types of distortion on human perception and offer recommendations for DCM compression and related tasks. Additionally,\nwe have evaluated three types of state-of-the-art objective metrics on the TDMD, including image-based, point-based, and video-based\nmetrics, on the TDMD. Our experimental results highlight the strengths and weaknesses of each metric, and we provide suggestions\nabout the selection of metrics in practical DCM applications. The TDMD will be made publicly available at the following location:\nhttps://multimedia.tencent.com/resources/tdmd.\nIndex Terms\u2014Dynamic Mesh Quality Assessment, Subjective Experiment, Database, Objective Metric\n\u2726\n1\nINTRODUCTION\nAs a typical representation of three-dimensional (3D) graph-\nics, 3D mesh plays a crucial role in various applications [1],\nsuch as animated advertisement, digital entertainment, and\neducation. These applications generally require high quality\nmeshes to provide better quality of experience through\nimproved immersion. Thus, accurately evaluating the influ-\nence of distortions on mesh quality is an essential task.\nMost of the current work on mesh quality assessment\nfocuses on static meshes [2], and sometimes disregards\nthe color information. However, dynamic colored meshes\n(DCMs) have become increasingly popular, and dedicated\ncompression algorithms have attracted considerable atten-\ntion in the WG 7 - MPEG Coding of 3D Graphics and\nHaptics\n[3]. Conducting comprehensive DCM quality as-\nsessment research is important to control tasks such as lossy\ncompression [4], transmission [5], reconstruction [6], and\nenhancement [7].\nDCM can be classified into two different categories de-\npending on how color information is represented. In the\nfirst category, the color information is stored in a texture\nmap, and a group of UV coordinates is used to indicate\nthe texture of the mesh samples. In the second category, the\ncolor information is stored per vertex in a similar way to\ncolored point cloud. Since the first type is more commonly\nused, our focus in this paper is on studying DCM using\ntexture maps.\nStudies on quality assessment of DCMs typically involve\ntwo types of evaluations: subjective and objective. To elab-\norate on the rationale for our work, we begin by discussing\nthe issues associated with each of them.\nQ. Yang, J. Jung, T. Deschamps, X. Xu, and S. Liu are from Ten-\ncent Media Lab, (e-mail: {chinoyang, joeljung, timond, xiaozhongxu,\nshanl}@tencent.com)\nThe main purpose of DCM subjective assessment is to\nstudy the influence of mesh distortions on human percep-\ntion. Regarding the viewing environment, the DCM can\neither be rendered and presented on a two-dimensional\nscreen, or it can be rendered via a virtual reality (VR) head-\nset and presented in an immersive space. Currently, most\nsubjective assessment studies focus on static meshes [8], [9],\n[10]. To contribute to the development of objective metrics,\nthey investigate how humans perceive mesh degradations\nin different viewing environments. However, there is a lack\nof evidence on whether the conclusions obtained on static\nmeshes remain valid on DCM. Although [11] investigates\ndynamic meshes with a 2D monitor viewing environment,\nthe proposed samples are non-colored meshes, and color\ninformation tends to have a masking effect for artifacts\non subjective perception\n[12]. The lack of studies on the\nresponses of human perception to different DCM distortions\nis consequently the first motivation for providing in this\npaper a new DCM database.\nThe main purpose of DCM objective assessment is to\npropose effective metrics that have a high correlation with\nhuman perception and can replace the expensive and time-\nconsuming subjective assessment in downstream DCM pro-\ncessing and applications. Currently, there are few metrics\nspecifically designed for DCM, since most mesh-oriented\nmetrics are designed for uncolored static mesh (e.g.,\n[9],\n[13], [14]). Furthermore, these metrics require the mesh\nsamples to share the same connectivity, same vertex density,\nor the same level of details, otherwise, they cannot extract\nfeatures and output objective scores. These requirements\nlimit the utilization of these metrics, especially since the\npossible distortions of mesh samples can be complex and\ndiverse. For example, compression can lead to the overlap\nof mesh vertices, and reconstruction can generate mesh\nsamples with holes or cracks. The DCM samples used in WG\narXiv:2308.01499v1  [cs.CV]  3 Aug 2023\n2\n7 [15], which are representations of human models, do not\nmeet the requirements of the above metrics. This indicates\nthat above listed metrics cannot be used to evaluate the\nquality of DCM which are under study and application in\nstandardization organizations. Although WG 7\n[16] pro-\nposed two strategies, image-based and point-based metrics,\nas substitute to measure DCM quality, these metrics are\nnot initially designed for meshes. The lack of reliable and\napplicable objective quality metrics for DCM is another\nmotivation of this paper for providing in this paper a new\nDCM database.\nTo study the influence of mesh processing algorithms on\nDCM perception and validate whether objective evaluation\nstrategies are effective for DCMs, a major contribution of\nthis paper is to create and release a new DCM database,\ncalled Tencent - DCM database (TDMD). TDMD contains\neight reference meshes and six types of distortions, namely\ncolor noise (CN), texture map downsampling (DS), geomet-\nrical Gaussian noise (GN), mesh decimation (MD), MPEG\nlossy compression (MLC), and texture map compression\n(TC). Each distortion is applied with five to seven degrees\nof severity. We convert the DCM samples into processed\nvideo sequences (PVS) with a predefined camera path and\nconduct large-scale subjective experiments to obtain mean\nopinion scores (MOS). TDMD contains MOS for 303 dis-\ntorted DCM samples which makes it, as far as we know, the\nlargest publicly available DCM database. On the basis of this\nproposed database, the second contribution of this paper\nis to analyze the impact of different distortions on human\nperception. The third contribution consists in testing three\ntypes of objective metrics on the proposed database. Besides\nthe two kinds proposed by the WG 7, we additionally use\nPVSs to test extra image/video quality assessment metrics,\nsuch as SSIM [17] and VMAF [18]. We label this category\nas the video-based metric. Two correlation indicators are\ncalculated to reveal the performance of the objective met-\nrics: the Pearson linear correlation coefficient (PLCC) and\nthe Spearman rank-order correlation coefficient (SROCC).\nMoreover, point-based metrics rely on mesh sampling and\nlittle research has been conducted on the influence of sam-\npling. Therefore, the last contribution of this paper is to\nstudy the influence of sampling on the performance of\npoint-based metrics. Based on the experiment results, we\nanalyze the advantages and disadvantages of each metric\nand provide suggestions for metric application.\nThe remainder of this paper is laid out as follows. Section\n2 presents the related work about mesh quality assessment.\nSection 3 details the construction of the TDMD database\nand analyzes the characteristics of distortion in human\nperception. Section 4 studies the performance of different\ntypes of objective metrics on the proposed database. Finally,\nsection 5 concludes the paper and highlights future work.\n2\nRELATED WORK\nIn this section, we summarize the state of the art of mesh\nquality assessment.\n2.1\nSubjective Mesh Quality Assessment\nMesh subjective quality assessment has been studied for\nmany years. Initially, researchers focused on the artifacts of\nnon-colored meshes, including compression, watermarking,\nsurface noise, and simplification. Specifically, [8] collected\nfour 3D human reconstructions and four scanned objects as\nreferences, and selected three different codecs to generate\ndistorted samples. A VR application was used to realize\npairwise testings to evaluate the visual quality of com-\npressed meshes. In [9], a 2D monitor was used to present\nfour mesh objects with watermarking to a panel of eleven\nsubjects, who were asked to score the samples.\n[11] also\nused a 2D monitor to display meshes with compression and\nsurface noise distortions. Ten non-colored dynamic meshes\nwere used, and the authors proposed simulated distortion\n(e.g., surface noise) and real-world distortion (e.g., compres-\nsion, network transmission error).\nWith the increasing application of meshes in various\nfields, such as digital entertainment and computer games,\nthere have been an increasing number of studies focusing on\ncolored meshes. [10] introduced a database containing five\ncolored meshes with texture maps as references. Five types\nof distortions were proposed and a video-based subjective\nmethodology was adopted to collect scores. [19] used a VR\nenvironment to score five mesh samples with four types\nof distortions. The color information of these meshes was\nstored per vertex (as opposed to using a texture map). In\n[20], subjective experiments were conducted using crowd-\nsourcing and four types of distortions were used for meshes\nwith texture maps.\nWe list published mesh databases in Table 1. In summary,\n[8], [9], [10], [19], [20], [21] proposed static mesh databases,\n[11] created a dynamic non-colored mesh database. There-\nfore, no DCM database is currently available, and conse-\nquently, the influence of DCM typical distortions on per-\nceived quality is unclear. Therefore, there is a need to\nconstruct a new database, involving state-of-the-art (SOTA)\nmesh distortions (e.g., mesh decimation, lossy compression\nalgorithms) to facilitate the study of DCM subjective quality\nassessment.\nTABLE 1\nMesh database survey.\nName\nType\nColor\nScale\nDistortions\nMMM2019 [8]\nStatic\nNo\n88\n2\nTMM2006 [9]\nStatic\nNo\n44\n3\nLIRIS GPD [21]\nStatic\nNo\n88\n2\nLIRIS TDMD [10]\nStatic\nYes\n136\n5\nTVCG2021 [19]\nStatic\nYes\n80\n4\nTOG2023 [20]\nStatic\nYes\n343K\n(Pseudo MOS)\n5\nSPIC2015 [11]\nDynamic\nNo\n276\n4\nTDMD\nDynamic\nYes\n303\n6\n2.2\nObjective Mesh Quality Assessment\nIn a similar fashion to the development of mesh subjective\nquality assessment, early mesh objective metrics were de-\nveloped for non-colored meshes. The simplest metrics are\ndistance-based, such as the root mean square error (RMSE)\n[22] and the Hausdorff distance (HD) [23]. RMSE calculates\nthe distances between the vertices in the reference mesh\n3\nand the corresponding vertices in the distorted mesh, and\nthen averages the distances as the objective score. HD first\nmeasures the distance between the points from the reference\nsurface and the corresponding points from the distorted\nsurface, and then selects the maximum value as the objective\nscore. Similarly to the mean square error (MSE) and peak\nsignal-to-noise ratio (PSNR) in the case of image quality\nassessment (IQA), these distance-based metrics have a low\ncorrelation with human perception and generally fail to\nreflect the influence of distortion on perceived quality.\nInspired by the achievements of SSIM\n[17] in the\nIQA field, researchers proposed using structural features\nto quantify mesh quality.\n[13] suggested leveraging sur-\nface curvature to infer mesh geometrical distortion.\n[9]\nintroduced global roughness to measure mesh surface dis-\ntortion.\n[24] made use of Geometric Laplacian to more\naccurately capture subtle visual distortions resulting from\nlossy compression.\n[14] integrated dihedral angle and a\nmasking weight to predict mesh quality. These metrics have\na strong relationship with the topological characteristics of\nthe mesh and show better performance than distance-based\nmetrics [2]. However, they have strict constraints regarding\nthe meshes\u2019 geometrical topology. For meshes having dupli-\ncated vertices, surface holes, or isolated faces, these metrics\ncannot extract features and fail to predict objective quality.\nTo solve the above problem, two strategies have been\nproposed. The first converts meshes into images through\nprojection, then uses IQA metrics to predict their quality.\nSpecifically, [25] conducted a comprehensive study involv-\ning seven 2D metrics (among which MSE, SSIM, and FSIM\n[26]) and illustrated their benefits for various applications.\n[16] proposed to use Fibonacci sphere lattice\n[27] to gen-\nerate 16 viewpoints for projection. They then used depth\nand color information to predict quality. Besides, with the\ndevelopment of machine learning and deep neural network,\nmany no-reference image-based mesh quality metrics (i.e.,\nmetrics that only rely on the assessed distorted sample to be\ncomputed) have been proposed, such as [20], [28], [29], [30].\nAn advantage of this strategy is that it converts the challeng-\ning mesh quality evaluation problem into a well-researched\nimage quality assessment problem. Consequently, many ef-\nfective IQA metrics can be employed to predict the objective\nscore. This strategy also presents obvious drawbacks, such\nas the necessity for rendering before capturing images and\nthe influence of viewpoint selection on final results [31].\nThe second approach is to convert meshes into point\nclouds (via mesh surface sampling), then use point cloud\nobjective metrics to assess the quality. Several sampling\nmethods have been proposed, such as grid sampling, face\nsampling, and surface subdivision sampling\n[16]. Point\ncloud-based methods allow quality assessment for all kinds\nof meshes without restrictions and a number of effective col-\nored point cloud metrics [32], [33], [34], [35], [36], [37], [38],\n[39] that can be chosen from. However, different sampling\nmethods might generate different point clouds for the same\nmesh, which influences the results of the quality assessment.\nTherefore, there is a need to construct a new database,\ninvolving state-of-the-art mesh distortions to facilitate the\nstudy of DCM objective quality metrics.\nTABLE 2\nReference Dynamic Mesh Samples.\nClass\nName\nFrames Vertices Faces\nGP\nTCP\nTMS\nA\nLongdress\n300\n22K\n40K\n10 bits 12 bits 2K\u00d72K\nSoldier\n300\n22K\n40K\n10 bits 12 bits 2K\u00d72K\nB\nBasketball player\n300\n20K\n40K\n12 bits 12 bits 2K\u00d72K\nDancer\n300\n20K\n40K\n12 bits 12 bits 2K\u00d72K\nC\nMitch\n300\n16K\n30K\n12 bits 13 bits 4K\u00d74K\nThomas\n300\n16K\n30K\n12 bits 13 bits 4K\u00d74K\nFootball\n300\n25K\n40K\n12 bits 13 bits 4K\u00d74K\nLevi\n150\n20K\n40K\n12 bits 13 bits 4K\u00d74K\n3\nDATABASE CONSTRUCTION\nIn this part, we introduce the construction of the TDMD,\nincluding content description, distortion generation, video\ngeneration, and subjective testing methodology.\n3.1\nContent Description\nFor DCM, the most typical content is 3D digital human, cap-\ntured by the camera array. This explains why WG 7 uses hu-\nman sequences as test material to study the compression an-\nchor [3]. To effectively facilitate the optimization of compres-\nsion algorithms through this study, we also use the DCM\nsequences provided by WG 7 as reference samples. There are\neight human samples with different characteristics: \u201cLong-\ndress\u201d and \u201cSoldier\u201d from\n[40], [41], \u201cBasketball player\u201d\nand \u201cDancer\u201d from [42], \u201cMitch\u201d and \u201cThomas\u201d from [43],\n\u201cFootball\u201d from\n[43] and \u201cLevi\u201d from\n[44]. The eight\nsamples are divided into three different classes according\nto their encoding complexity. Class A corresponds to low-\nprecision mesh geometry and low-resolution texture map,\nclass B corresponds to high-precision mesh geometry and\nlow-resolution texture map, and class C corresponds to\nhigh-precision mesh geometry and high-resolution texture\nmap. We have summarized the detailed information of the\neight samples in Table 2. We denote geometry precision as\nGP, texture coordinate precision as TCP, and texture map\nsize as TMS. We illustrate each reference mesh by a snapshot\nin Fig. 1.\n3.2\nDistortion Generation\nTo explore the influence of typical mesh distortion on subjec-\ntive perception, we apply six common distortion types: CN,\nDS, GN, MD, MLC and TC. More specifically, we propose\nfive or seven different distortion levels for each type of\ndistortion, with details shown below:\n1)\nCN: Color noise is applied to the texture map of\nthe mesh sample. This noise simulates the distortion\ninjected into the image during channel transmission.\nWe use the Matlab function \u201cimnoise\u201d to add \u201csalt\n& pepper\u201d noise to the texture map with noise\ndensities set to 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, and 0.3.\n2)\nDS: Downsampling is a very simple and effective\nmethod to reduce the complexity of data processing.\nWe use the Matlab function \u201cimresize\u201d with cubic\ninterpolation to resize the resolution of the texture\n4\nLongdress\nSoldier\nBasketball_player\nDancer\nMitch\nThomas\nFootball\nLevi\nLongdress\nSoldier\nBasketball_player\nDancer\nMitch\nThomas\nFootball\nLevi\nFig. 1. Snapshots of reference mesh.\nmap with ratios set to 0.5, 0.3, 0.1, 0.08, 0.05, 0.03,\nand 0.01, applied in each direction.\n3)\nGN: Geometrical Gaussian noise is applied to the\nspatial coordinates of the vertices of the mesh sam-\nple, to simulate the distortion injected during a\nserial mesh processing (e.g., mesh scanning and cap-\nture). We first generate random noise in the interval\n[0, 1] for each mesh vertex, then scale the noise value\ncorresponding to the bounding box size, as shown\nin Eq. (1).\nNi = NRi \u2217 Ratio \u2217 MinBBlength,\n(1)\nThe noise of the i-th vertex, Ni, is represented in\nEq. (1), where NRi represents the random noise\nin the i-th vertex, MinBBlength represents the\nminimum size of the bounding box among three\ndimensions. Ratio controls the degree of distortion\nwith the following values: 0.001, 0.004, 0.007, 0.01,\n0.013, 0.016, and 0.02.\n4)\nMD: Mesh decimation is applied to the faces of the\nmesh sample. The principle of mesh decimation is to\nreduce the number of faces and to generate models\nwith different levels of detail for different needs.\nWG 7\n[15] proposes five MD degrees which are\nthe basis of the Draco compression\n[45] to real-\nize specific bitrates (i.e., current lossy compression\nanchors first conduct MD and then compress the\nmeshes with encoder). Therefore, we directly use\nthe MD samples proposed by WG 7 to better study\nthe independent influence of MD. The number of\nfaces after decimation are 25K, 20K, 15K, 10K, and\n5K. Quadric edge collapse decimation [with texture]\nis used, as implemented by MeshLab. For \u201cLevi\u201d,\nMeshLab cannot perform MD, as the mesh contains\nseveral small texture map islands and the algorithm\ncannot achieve the target face count [15].\n5)\nMLC: MPEG lossy compression is applied to both\nthe texture map and geometry information (e.g.,\nvertex spatial coordinates and UV coordinates) of\nthe mesh sample. It refers to the lossy compression\nanchor with the all-intra mode\n[15]. The texture\nmap is first converted to YUV420 video format\nusing HDRTools [46], and then encoded with HEVC\nHM 16.21 reference software with the Screen Con-\ntent Coding Extension [47]. For the geometry infor-\nmation, MD is applied first and the information is\ncompressed with Draco [45]. There are five different\ntarget bitrates (i.e., R1 to R5 with the increase of\nbitrate, the detailed compression parameters are\npresented in Appendix) for each of the eight sam-\nples proposed in\n[15]. For \u201cLevi\u201d, R1 cannot be\nreached due to the decimation problem explained\nabove. R1 is the lowest bitrate and it requires the\napplication of MD. Therefore, there are only four\navailable target bitrates for Levi.\n6)\nTC: Texture map compression is applied to study\nthe influence of SOTA image compression algo-\nrithms. We use FFMPEG 5.1.2 with libx265 [48] to\ncompress the texture map with quantization param-\neters (QP) set to 22, 27, 32, 37, 42, 48, and 50.\nFig. 2 illustrates the snapshots of the distorted meshes\nfor \u201cBasketall player\u201d. All distortions considered, there are\n8\u00d7(4\u00d77+2\u00d75)\u22121 = 303 distorted samples in the proposed\ndatabase (eight reference meshes, four types of distortions\nwith seven distortion degrees, two types of distortions with\nfive distortion degrees, and minus one for the unreachable\nR1 of MLC for \u201cLevi\u201d).\n3.3\nVideo Generation\nThere are two prevalent methods to render mesh samples\nfor subjective evaluation: 2D video-based and VR-based\nmethods. For video-based subjective methods, we can refer\nto ITU-R BT.500\n[49] and ITU-T P.910 Recommendations\n[50] to conduct the subjective experiment. For VR-based\nmethods, although there is a lot of academic research to\nexplore the principles of the subjective experiment, an au-\nthoritative protocol is yet to be standardized, which is part\nof ongoing activities in ITU-T SG12/Q7 P.IntVR [51]. In this\npaper, we use a video-based method to render our database.\nTo render the DCM samples into PVSs and fully display\ntheir geometrical and texture characteristics, we adopted\nspecific approach to design the camera path. First, we set\nthe views presented in Fig. 1 as the starting points. The\ncamera is fixed for one second to let the viewer observe the\n5\nFig. 2. Snapshots of distorted mesh in TDMD.\ninitial view. Then, the camera rotates anticlockwise around\nthe sample during four seconds before returning to its orig-\ninal position, where it remains fixed again for one second.\nFinally, the camera rotates clockwise around the sample to\nreturn to its original position within four seconds. Overall,\na DCM sample of 300 frames is converted to a smooth 10-\nsecond video at 30 frames per second. For Levi, which only\nhas 150 frames, we use the inverse order of the original 150\nframes, indexed as the 151st to 300th frames, to obtain the\n10-second videos with the camera path defined above.\nWe use Open3D [52] to render the mesh samples and\ncapture the images at a resolution of 1920 \u00d7 1080. After\nobtaining the images, we use FFMPEG [48] to group them\ninto videos with libx265. The constant rate factor (CRF) is set\nto 10, which guarantees smooth display and visually lossless\ndegradation, as suggested in [53].\n3.4\nSubjective Test Methodology\nIn this part, we introduce the design of our subjective\nexperiment, including stimulus and scale, the setting of the\ntraining and rating sessions, as well as the test environment.\n3.4.1\nStimulus and Scale\nWe use the double stimulus impairment scale (DSIS) where\nthe reference PVS is first displayed, followed by two-second\nbreaks with a white blank screen. The distorted PVS is then\ndisplayed, and followed by a white blank screen for eight\nseconds, allowing the viewer to rate the PVS. For rating\nsamples, we use the 11-grade voting method proposed by\nITU-T P.910 Recommendation.\n3.4.2\nTraining Session\nTo collect reliable subjective scores from viewers and reduce\noutliers, a clear and judicious training session is needed to\nhelp viewers foster a good understanding of the relationship\nbetween mesh quality and subjective scores.\nWe carefully select eight distorted PVSs for the training\nsession, corresponding to the predefined expected quality\nrange. The predefined expected quality scores are evenly\ndistributed between the lowest score and the highest score.\nThe training session is displayed twice, and the viewers are\nrequired to score the eight distorted PVSs in the second\nround. If the scores given by the viewer present a high corre-\nlation with the expected quality interval, it is considered that\nthe viewer has understood the principle of the subjective ex-\nperiment and is qualified for the rating sessions. Otherwise,\nwe repeat the training procedure until the viewer can give\nreasonable scores for the training PVSs, without ever telling\nthe viewer how to score a certain training PVS.\n3.4.3\nRating Session\nThe rating procedure is shown in Fig. 3: 30s are spent on\neach distorted sample. As mentioned in section 3.2, there\nare 303 distorted samples to rate. To avoid visual vertigo\nand fatigue caused by an overly long experiment time, we\nlimit the duration of a single rating session to 20-30 minutes\nby randomly splitting the 303 distorted samples into 12\nsubgroups, having 25 to 30 samples each.\nVote\nBreak\n10s\n10s\n2s\n8s\n...\n Rating Session\nReference PVS\nDistorted PVS\nVote\nBreak\n10s\n10s\n2s\n8s\nReference PVS\nDistorted PVS\nNth Rating Sample\nFirst Rating Sample\nFig. 3. Procedure of rating session. s represents second (s).\n3.4.4\nTest Environment\nPVSs are displayed in their original resolution on a 4K\nmonitor (PHILIPS 329P1, 32 inches with 3840\u00d72160 res-\nolution) in a low-light lab environment. We recruited 63\npeople for subjective experiments, 35 females and 28 males\naged from 18 to 50 years old. They are company employees\nor university students. Most of them have no experience\nwith subjective experiments, and are naive to the DCM\nprocessing activity. Each viewer is only presented some of\nthe subgroups since going through the entire database is a\nrelatively onerous task. When a viewer participates in the\nscoring of more than two subgroups, we ensure that there\nis a sufficient interval time between the first and the second\nsubjective experiments to avoid cross-influence. We collect\nat least 25 groups of raw scores for each subgroup to ensure\nthat we will have at least 16 valid scores after the outlier\nremoval.\n3.5\nOutlier Detection\nWe apply two consecutive steps to remove outliers from the\nraw subjective scores. In the first step, we insert \u201ctrapping\nsamples\u201d for each subgroup. Two types of trapping samples\nare considered. Firstly, we add a very low quality PVS,\nexpected to be scored 0, 1 or 2. If a viewer gives a higher\n6\nscore for this sample, the scores of this subgroup collected\nfrom this viewer are considered incorrect. Secondly, we add\na randomly selected duplicate PVS in each subgroup. These\nPVSs are not displayed consecutively. The score differences\nof the duplicated PVSs should be below 3, otherwise, the\nscores of this subgroup collected from this viewer are con-\nsidered wrong. In the second step, we calculate the average\nscore for all the samples in the database, then we calculate\nthe correlation between the average score and the raw scores\nof each viewer via Eq. (2),\n\u03c1x,y = Cov(x, y)\n\u03c3x \u00d7 \u03c3y\n,\n(2)\nwhere the correlation of the array x and y, \u03c1x,y, is calculated\nbased on the covariance Cov(x, y) and the variances \u03c3x, \u03c3y.\nWe remove the scores of viewers whose correlation is lower\nthan 0.8 and then update the average scores to get the MOS.\nWe do not adopt the outlier detecting method proposed\nin ITU-R BT.500 as our viewers have only rated parts of\nthe database, while the ITU-R BT.500 method requires each\nviewer to rate the entire database.\nSix groups of scores (scores from a rating session are\nlabeled as a group of scores) were detected as dubious scores\nin the first step, and the scores of one viewer were removed\nin the second step.\n3.6\nMOS Analysis\nTo illustrate the influence of different distortions on human\nperception, we plot in Fig. 4 the MOS distribution versus\nthe distortion types. We can observe that:\n1)\nFor CN, DS, MLC, and GN distortions, the MOSs\ncover wide value ranges. For instance, the MOS of\nCN for \u201cLongdress\u201d ranges from 3 to 8, and the\nMOS of DS for \u201cSoldier\u201d ranges from 1 to 9.\n2)\nFor MD and TC distortions, the MOS values are\ngenerally high, most of which are higher than 7. It\nindicates that the influence of MD and TC distor-\ntion degree considered in this paper on perceptual\nquality is limited.\nBased on the above experimental results, the MOS varia-\ntions of CN, DS, MLC, and GN, which present wide quality\nvariation, satisfy our expectations. We consequently focus\nour analysis on studying why the degree increase of MD\nand TC has a limited impact on human perception.\nBy carefully checking MD samples, we postulate that the\nmain reason for having limited perceptual impact is that\nthe MD degrees proposed by WG 7 are low, resulting in\na number of faces after MD that remain large enough to\npreserve most sample characteristics. For these MD samples,\ncontour flicker is the main manifestation of perception qual-\nity degradation, which is less distracting than other types of\ndistortion.\nBy carefully checking TC samples, we hypothesize that\nthe main reason having a limited impact is that the notice-\nable distortions caused by TC are relatively small, because\nthey are masked by the frame switching of the dynamic\nsequence. The detailed analysis of MD and TC is presented\nin the Appendix.\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nLongdress\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nSoldier\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nBasketball_player\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nDancer\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nMitch\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nThomas\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nFootball\n \nCN\nDS\nMLC\nGN\nMD\nTC\n \nDistortion\n0\n2\n4\n6\n8\n10\nMOS\nLevi\nFig. 4. MOS plots for the 8 test sequence.\n4\nOBJECTIVE DCM METRICS\nTo evaluate the quality of DCM, WG 7 [16] proposes two\nmethods: an evaluation via image-based metrics using con-\nventional mesh rendering techniques and an evaluation via\npoint-based metrics using sampling to convert the mesh into\na point cloud. Another method, labeled as the video-based\nmetrics, that infers mesh quality by applying IQA/VQA\nmetrics on the PVSs rendered in the subjective experiment.\nIn this section, we first give a brief introduction to the three\ntypes of metrics before analyzing their performance on the\nproposed database. Additionally, we explore the influence\nof sampling on SOTA point-based metrics.\n4.1\nDescription of the Metrics\n4.1.1\nImage-based Metrics\nBefore applying image-based metrics, we must first convert\nreference and distorted meshes into images. We do this by\nprojecting them through 16 viewpoints resulting from the\nFibonacci sphere lattice [27].\n7\nTABLE 3\nMetrics performance on TDMD.\nIndex\nAll\nSequence\nDistortion\nMetric\nPLCC SROCC Longdress Soldier Basketball player Dancer Mitch Thomas Football Levi CN\nDS\nGN MLC MD\nTC\nA\ngeopsnr\n0.48\n0.16\n0.48\n0.52\n0.14\n0.52\n0.54\n0.54\n0.21\n0.37\n-\n-\n0.82\n0.76\n0.85\n-\nrgbpsnr\n0.85\n0.83\n0.91\n0.89\n0.90\n0.92\n0.93\n0.92\n0.89\n0.84 0.96 0.91 0.83\n0.80\n0.81 0.56\nyuvpsnr\n0.86\n0.84\n0.91\n0.90\n0.92\n0.92\n0.93\n0.93\n0.89\n0.84 0.96 0.92 0.85\n0.80\n0.80 0.53\nB\nD1\n0.49\n0.16\n0.47\n0.51\n0.50\n0.50\n0.55\n0.53\n0.48\n0.57\n-\n-\n0.77\n0.89\n0.96\n-\nD2\n0.50\n0.16\n0.47\n0.51\n0.50\n0.50\n0.55\n0.54\n0.49\n0.58\n-\n-\n0.79\n0.90\n0.96\n-\nD1-h\n0.43\n0.13\n0.40\n0.46\n0.45\n0.48\n0.47\n0.43\n0.31\n0.55\n-\n-\n0.79\n0.92\n0.94\n-\nD2-h\n0.45\n0.14\n0.45\n0.49\n0.46\n0.46\n0.49\n0.48\n0.42\n0.53\n-\n-\n0.79\n0.88\n0.95\n-\nyuvp\n0.82\n0.80\n0.91\n0.85\n0.93\n0.93\n0.93\n0.93\n0.91\n0.94 0.96 0.82 0.78\n0.86\n0.52 0.41\nPCQMp\n0.91\n0.87\n0.93\n0.93\n0.95\n0.95\n0.95\n0.92\n0.93\n0.94 0.96 0.95 0.88\n0.83\n0.92 0.66\nC\nPSNR\n0.78\n0.78\n0.80\n0.71\n0.76\n0.79\n0.85\n0.73\n0.82\n0.88 0.96 0.91 0.86\n0.94\n0.58 0.67\nSSIM\n0.84\n0.81\n0.91\n0.90\n0.86\n0.87\n0.90\n0.88\n0.85\n0.94 0.93 0.90 0.92\n0.88\n0.93 0.48\nMS-SSIM\n0.90\n0.88\n0.94\n0.94\n0.91\n0.92\n0.93\n0.91\n0.91\n0.97 0.94 0.95 0.93\n0.93\n0.93 0.74\nVMAF\n0.80\n0.79\n0.82\n0.77\n0.80\n0.79\n0.87\n0.77\n0.85\n0.86 0.95 0.94 0.86\n0.94\n0.88 0.82\nVQM\n0.79\n0.77\n0.82\n0.78\n0.83\n0.83\n0.91\n0.84\n0.83\n0.75 0.95 0.91 0.83\n0.76\n0.80 0.69\n3SSIM\n0.83\n0.81\n0.88\n0.85\n0.87\n0.86\n0.86\n0.84\n0.88\n0.94 0.93 0.84 0.85\n0.94\n0.93 0.71\nBased on the 16 obtained images, we tested three met-\nrics based on the proposal in\n[16]: geopsnr, rgbpsnr and\nyuvpsnr. geopsnr calculates the depth information differences\nbetween the reference and the distorted meshes. The depth\ninformation is recorded when capturing the images as an\nimage itself, whose pixel values are normalized to 255, to\nget PSNR values comparable to the ones obtained using\nthe next two metrics. rgbpsnr and yuvpsnr calculate the dif-\nferences of (R, G, B) and (Y, U, V ) color channels between\nthe reference and the distorted images. When capturing the\nimages, the color information is recorded as an image. A\nmore detailed description of the metrics can be found in\n[16].\nThe frame rates of DCM sequences are usually 30Hz\n[15], and the videos we generated have a frame rate of 30Hz\nas well. To balance accuracy and computation complexity,\nwe propose to calculate the objective scores at a frequency of\nthree frames per second. Based on our testing, the results are\nclose between using all the frames and using three frames\nper second. Therefore, to obtain an overall objective score for\na DCM that contains hundreds of frames, we calculate the\nobjective scores every ten frames, which are then averaged\nto get the final results.\n4.1.2\nPoint-based Metrics\nThe point-based metrics directly use the raw data from\nthe reference and the distorted meshes to extract features\nand predict quality. Two steps are applied\n[16]. Firstly,\nmeshes are sampled to be converted into point clouds. Sec-\nondly, point cloud objective metrics are calculated to predict\nquality. Several methods for sampling have been proposed,\nincluding grid sampling, face sampling, and surface sub-\ndivision sampling. As\n[16] reported that grid sampling\nhas a stable behavior, it is used in this section with a grid\nresolution of 1024 to generate colored point clouds. This grid\nresolution controls the point density of the sampled point\ncloud: the larger the resolution, the denser the point cloud.\nMore results corresponding to other types of sampling and\nvalues of resolution are presented in section 4.3.\nAfter generating the point clouds, six SOTA point cloud\nmetrics are computed: p2point (D1), p2plane (D2), p2point-\nhausdorff (D1-h), p2plane-hausdorff (D2-h), yuvpsnr (yuvp)\nand PCQMpsnr (PCQMp). In the same manner as for\nimage-based metrics, we calculate the objective scores every\nten frames and average them to obtain the final results.\n4.1.3\nVideo-based Metrics\nTo apply IQA/VQA metrics, we use the PVSs displayed\nduring the subjective experiment. The MSU video quality\nmetric benchmark\n[54] is used to compute the following\nmetrics: PSNR, SSIM\n[17], MS-SSIM\n[55], VMAF\n[18],\nVQM [56] and 3SSIM [57].\n4.2\nPerformance of the Metrics\nTo ensure the consistency between the objective and sub-\njective evaluation scores for the various quality assessment\nmetrics, the Video Quality Experts Group recommends\nmapping the dynamic range of the scores from objective\nquality assessment metrics to a common scale using five-\nparameters logistic regression [58], e.g.,\nQi = k1\n\u00121\n2 \u2212\n1\n1 + ek2(si\u2212k3)\n\u0013\n+ k4si + k5,\n(3)\nwhere si is the objective score of the i-th distorted samples,\nQi is the corresponding mapped score. k1, k2, k3, k4 and\nk5 are the regression model parameters to be determined\nby minimizing the sum of squared differences between the\nobjective and subjective evaluation scores.\nBesides, two performance correlation indicators com-\nmonly used by the quality assessment community are em-\nployed to quantify the efficiency of the object metrics: PLCC\nand SROCC. PLCC can reflect prediction accuracy, and\nSROCC can reflect prediction monotonicity. The higher the\nvalues of the PLCC or SROCC, the better the performance\nof the metric [59].\n8\nTable 3 shows the results of the objective metrics applied\non TDMD. In addition to the PLCC and SROCC results on\nthe whole dataset, marked as \u201cAll\u201d, we report the PLCC\nresults for different sequences and different distortion types\nin \u201cSequence\u201d and \u201cDistortion\u201d columns. \u2019- means that\nthe results of the metric for the samples applied with this\nkind of distortion are meaningless, and the reasons will be\nexplained in the following analysis.\n4.2.1\nImage-based Metric\nTable 3-A presents the performance of image-based metrics.\nWe observe that the geopsnr results are significantly inferior\nthan the rgbpsnr and yuvpsnr results, with an overall PLCC\nand SROCC of only 0.48 and 0.16, respectively. rgbpsnr and\nyuvpsnr have a close performance, with average PLCC and\nSROCC around 0.85. rgbpsnr reports a slightly more stable\nresults than yuvpsnr, the standard deviations of rgbpsnr and\nyuvpsnr with respect to different sequences are 0.028 and\n0.030, respectively.\nFor the performance of image-based metrics on different\ndistortions, we see that geopsnr presents high PLCC for GN,\nTMC, and MD distortions. However, it cannot detect CN,\nDS and TC distortions, as it only considers depth informa-\ntion which belongs to geometrical feature, while CN, DS,\nand TC are lossless with regards to geometry information.\nTherefore, geopsnr reports the same objective scores for these\nsamples. rgbpsnr and yuvpsnr show close performance in\ndifferent types of distortions. They exhibit high PLCC for\nCN and DS distortions (above 0.9) and low correlations for\nTC distortions (below 0.6).\nIn summary, projecting DCM into colored images and\nthen applying metrics such as rgbpsnr and yuvpsnr is more\neffective than only capturing depth information to apply\ngeopsnr. One influencing factor of image-based metrics is\nthe number of viewpoints used to generate images. With the\nincrease in the number of projected images, the performance\nof image-based metrics becomes more stable [31], [60], [61],\nbut this is accompanied by a higher calculation complexity.\n4.2.2\nPoint-based Metric\nTable 3-B presents the performance of point-based metrics.\nWe see that D1 and D2 show close performance, with\naverage PLCC and SROCC values of around 0.5 and 0.15.\nD1-h and D2-h exhibit close results, with overall PLCC and\nSROCC around 0.45 and 0.13. yuvp demonstrates clearly\nhigher PLCC and SROCC than D1 (-h)/D2 (-h), with av-\nerage PLCC and SROCC of 0.82 and 0.80. PCQMp re-\nports the best performance among all point-based metrics,\nshowcasing overall PLCC and SROCC values of 0.91 and\n0.87. The results of PCQMp are more stable than yuvp: the\nstandard deviation of yuvp between different sequences is\n0.029, while that of PCQMp is 0.012.\nFor the performance of point-based metrics on different\ndistortions, we see that D1 (-h) and D2 (-h) reveal close\nresults on GN, MLC and MD distortions, but cannot handle\nCN, DS and TC distortions, the reason being that these four\nmetrics only consider geometry information when quanti-\nfying distortion, while CN, DS and TC are color-related\ndistortions. yuvp shows unstable performance among the\ndifferent types of distortions. It displays high correlations\nfor CN distortions (above 0.90), but low correlations for\nMD and TC distortions (below 0.60). Considering that the\nfeatures used in yuvp are point-to-point differences on the\nY, U, and V channels, it is more suitable for point-wise color\ndistortion than other metrics. PCQMp demonstrates nearly\nthe best and most stable results on different distortions.\nPCQMp pools local surface curvature and color features\ntogether and, therefore, showcases robust performance for\nall types of distortions.\nIn summary, partial point-based metrics (e.g., PCQMp)\ndemonstrate impressive performance on the proposed\ndatabase, and using both geometry and color information\ncan significantly improve their effectiveness and robustness.\n4.2.3\nVideo-based Metric\nTable 3-C presents the results of video-based metrics. We\nsee that among video-based metrics, MS-SSIM reports the\nstrongest overall results, with average PLCC and SROCC\nvalues of 0.90 and 0.88, similar to the performance of\nPCQMp. SSIM and 3SSIM show close results, averaging\naround 0.85 and 0.80 for PLCC and SROCC, respectively.\nPSNR, VMAF, and VQM illustrate close performance, with\noverall PLCC and SROCC scores equal to or slightly below\n0.8. To illustrate the reasons for the difference in perfor-\nmances among video-based metrics, we display the scatter\nplots of SSIM, MS-SSIM, VMAF, and VQM in Fig. 5, the red\nlines represent the best fitted curves. The objective scores\nare mapped to the 0-10 range using Eq. (3).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMOS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nObjective Score\nSSIM\nCN\nDS\nGN\nMLC\nMD\nTC\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMOS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nObjective Score\nMS-SSIM\nCN\nDS\nGN\nMLC\nMD\nTC\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMOS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nObjective Score\nVMAF\nCN\nDS\nGN\nMLC\nMD\nTC\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMOS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nObjective Score\nVQM\nCN\nDS\nGN\nMLC\nMD\nTC\nFig. 5. Scatter plots for the video-based metrics.\nAfter mapping the objective scores to the scale of MOS,\nan ideal scatter plot would have all its points distributed\nclose to the \u201cy=x\u201d function (y representing objective scores,\nand x representing the MOS), indicating that the predicted\nobjective scores are equal or very close to the subjective\nscores. Based on Fig. 5, we see that MS-SSIM shows the best\ndistribution, explaining why it illustrates the best overall\nPLCC and SROCC. SSIM, VMAF, and VQM have partial\nscatters far away from the best fitted curves. For example,\nSSIM leans towards giving low scores for CN and high\nscores for DS, VMAF tends to score higher for DS and\nlower for MD, and VQM favors TC and DS while giving\nlower scores to MD. As shown in the analysis in section 3.6,\nviewers tend not to be bothered by MD distortions and give\nthem higher scores, which explains the lower performances\nof VMAF and VQM when compared to SSIM.\n9\nFor the results of video-based metrics on different distor-\ntions, we see that all the video-based metrics present good\nperformance for CN, DS, and GN. PSNR shows low corre-\nlations for MD, with an average PLCC of 0.58. SSIM reports\npoor performance for TC, having PLCC values of 0.48. MS-\nSSIM presents robust performance on CN, DS, GN, MLC,\nand MD distortions, the PLCC are higher than 0.90. VMAF\nalso reveals good results on individual distortions, but as\nwe analyzed in the previous paragraph, the relative values\nof VMAF scores for different distortions have some biases,\ne.g., lower scores for MD. VQM shows the worst PLCC on\nMLC (0.76) compared to other metrics. 3SSIM is the only\nmetric that the PLCC of DS is lower than 0.90. However,\nit demonstrates good performance on MLC in which DS is\nintroduced during texture map compression. Therefore, we\nthink that 3SSIM can successfully detect other operations\nwhich can incur distortions such as Draco compression.\nIn summary, partial video-based metrics such as MS-\nSSIM show reliable performance. However, some canonical\nmetrics report opposite results to human perception for\nspecific distortion (e.g., VMAF and VQM on MD).\n4.2.4\nSummary of Objective Metrics\nPCQMp and MS-SSIM report the best overall performance,\nwith PLCCs and SROCCs of (0.91, 0.87) and (0.90, 0.88),\nrespectively. MS-SSIM is more stable than PCQM among\ndifferent types of distortion. For example, based on Table.\n3, for PLCC, MS-SSIM has five results higher than 0.90\nwhile PCQMp only has three, and the worst PLCC of MS-\nSSIM is 0.74 which is also higher than PCQMp\u2019s worst of\n0.66. If these two metrics have close overall results, they\nalso present advantages and drawbacks. The performance of\nPCQMp could be affected by the resolution of the sampling\nmethod used. For MS-SSIM, extra time is needed to render\nthe DCM as PVSs. Moreover, the camera path used to\nrender the PVSs is handcrafted and worth pondering. For\ndifferent types of DCM, viewers may have different regions\nof interest. It is assumed that different camera paths could\nlead to different prediction results. However, when meshes\nshare similar characteristics, as in the proposed test, applied\nto DCM, this phenomenon is less likely to happen, relying\non MS-SSIM is a good option. Otherwise, we believe that\nPCQMp is a more appropriate choice.\n4.3\nThe Influence of Sampling on Point-based Metric\nAs outlined in the experimental results of section 4.2.2,\npoint-based metrics (particularly PCQMp) showcase high\nperformance on DCM quality assessment. An advantage of\nthese metrics over image-based and video-based metrics is\nthat they do not need rendering: they extract features from\nraw mesh data, which generally have less computing time\noverhead. However, these metrics rely on mesh sampling,\nand little research has been conducted on the influence\nof sampling on DCM quality assessment [62]. Therefore,\nin this section, we first present the performance of point-\nbased metrics with various grid sampling resolutions, then\ncompare the differences in performances between sampling\nmethods.\n4.3.1\nGrid Sampling Resolution\nThe goal of this part is to study the correlation between\ngrid sampling resolution and metric accuracy. We propose\neight different grid resolutions: 1024, 512, 256, 128, 64, 32, 16,\nand 8. Table 4 illustrates the sampled point clouds of grid\nsampling with different resolutions, which will be used for\nthe calculation of point-based metrics. We use the first frame\nof \u201cLongdress\u201d and \u201cFootball\u201d as examples to reveal the\nvariation of the number of sampled points. Fig. 6 presents\nthe visual impact of using fewer points. We can observe that\nwith the decrease of sampling resolutions, the point clouds\nbecome sparser.\nTABLE 4\nExamples of sampled point clouds with different grid resolutions\nResolution\nPoints\nLongdress Football\n1024\n577,679\n548,633\n512\n144,218\n136,920\n256\n35,912\n34,126\n128\n8,974\n8,501\n64\n2,189\n2,045\n32\n528\n497\n16\n120\n113\n8\n22\n17\nResolution          1024                             256                                 64\nResolution          1024                             256                                 64\nFig. 6. Grid sampling results concerning different resolutions.\nWe plot the performance of yuvp and PCQMp with\nregards to several grid sampling resolutions on eight se-\nquences in Fig. 7. First, we see that both yuvp and PCQMp\n10\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nLongdress\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nSoldier\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nDancer\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nBasketball_player\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nMitch\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nThomas\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nFootball\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\n1024\n512\n256\n128\n64\n32\n16\n8\n \nResolution\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nLevi\nyuvp PLCC\nPCQM p PLCC\nyuvp SROCC\nPCQM p SROCC\nFig. 7. Performance of yuvp and PCQMp with different grid sampling\nresolutions on different sequences.\ntend to show worse performance with the decrease of\nsampling resolution. One exception is \u201cLevi\u201d, the PLCC of\nyuvp and the SROCC of yuvp are increased when resolution\ndecreases from 16 to 8. We think the reason is that: when\nsampling resolutions blow 32, the sampled point clouds are\ntoo sparse to calculate effective structure feature. For exam-\nple, PCQMp uses a radius search to construct a local patch\nto calculate the local curvature. If the patch only has a few or\none points, the calculated feature might not represent local\ncurvature anymore. Second, compared to other sequences,\ngrid sampling obviously showcases stable performance on\n\u201cLevi\u201d. We hypothesize that the main reason is that the\ntexture of \u201cLevi\u201d is relatively simple compared with other\nsequences, e.g., the black sweater, the black shoes, and the\ndark skin, making it an easier case for sampling.\nGenerally, the denser the sampled point clouds, the\ncloser the perception to the original DCM. In practical\napplications, to better balance accuracy and computational\ncomplexity for point-based metrics, we recommend using\ngrid sampling with a resolution between 1024 and 256\nfor DCM samples sharing similar characteristics as TDMD\nsamples.\n4.3.2\nDifferent Sampling Methods\nWe study the results of point-based metrics on four different\nsampling methods proposed in [16]: grid sampling, face\nsampling, surface subdivision sampling (SDIV), and edge\nsubdivision sampling (EDIV). Grid sampling generates the\npoints based on the intersections between faces and a 3D\ngrid, face sampling generates the points face by face using\na 2D grid following the direction of the triangle edges\n(the directions of the edges of 2D grid are oriented par-\nallel to the two edges of the triangular face, respectively).\nSDIV recursively subdivides faces until an area threshold is\nreached, while EDIV recursively subdivides faces until an\nedge length threshold is reached. The detailed introduction\nof these sampling methods can be found in [16].\nWe show an overview of the sampled point clouds cor-\nresponding to the four sampling methods in Fig. 8. To better\nillustrate the characteristics of these methods, we zoom in\non the contents of the orange-dotted rectangle box. We see\nthat grid sampling incurs horizontal and vertical cracks.\nThe reason for this phenomenon is that the generation of\na point relies on whether the face intersects the 3D grid,\nand thus the density of the points is affected by the grid\nresolution. If a face is close to or smaller than the smallest\nunit of the grid, few points are sampled, which causes strip\ncracks. Face sampling exhibits some triangular areas with\n\u201cuneven\u201d point density. Face sampling samples points face\nby face using the same resolution, which means that the\npoint density is identical for each triangle patch. The angle\nbetween the viewing direction and the normal vector direc-\ntion of each face is different, causing the perceived point\ndensity to be different. EDIV and SDIV present irregular\ncracks because they also sample points face by face but\nuse a threshold to indicate the triangle edge and the size\nsubdivision endpoint, respectively. Considering that the face\nedges and sizes are different, the sampling results illustrate\ndifferent and irregular point distributions.\nAfter previously reporting the performances of grid\nsampling, we will now study the results of face sampling,\nSDIV, and EDIV. These three sampling methods all sample\npoints face by face. For face sampling, the minimum number\nof sampled points is equal to 1/3 of the number of mesh\nvertices; for SDIV and EDIV, the minimum number is equal\nto the number of mesh vertices. We set seven sampling\ngrades for each sampling method: for face sampling and\nEDIV, we set sampling resolution to 1024, 512, 256, 128,\n64, 32 and 16; for SDIV, we use the 1/16, 1/8, 1/4, 1/2,\n1, twofold, and fourfold average face size of each sequence\nas the thresholds. For the same grade of different sampling\nmethods, the sampled point clouds might have a different\nnumber of points. For example, for the first grade, using the\nfirst frame of \u201cLongdress\u201d as an example, face sampling,\nEDIV, and SDIV have 1200K, 1260K, and 1460K points,\nrespectively.\nThe results are shown in Fig. 9. We see that with the de-\ncrease of the number of points (that is, the increase in grade),\nyuvp and PCQMp tend to show worse results regardless of\nthe types of sampling methods. The results of PCQMp with\nSDIV presents more unstable performance than face sam-\npling and EDIV for sequence \u201cDancer\u201d, \u201cBasketball player\u201d\nand \u201cThomas\u201d (cf., the green line). Three sampling methods\nmight present unchanged results for the last several grades\non some sequences, suggesting that the increase of sampling\ngrade will not change the sampling results anymore, i.e., the\nsampled point clouds have reached the minimum points.\n\u201cLevi\u201d again illustrates more stable results than other se-\n11\nTABLE 5\nMetrics performance (PLCC and SROCC ) for the proposed database with different sampling methods.\nGrade\nI\nII\nIII\nIndex\nPLCC\nSROCC\nPLCC\nSROCC\nPLCC\nSROCC\nMetric\nFace EDIV SDIV Face EDIV SDIV Face EDIV SDIV Face EDIV SDIV Face EDIV SDIV Face EDIV SDIV\nyuvp\n0.82\n0.82\n0.81\n0.80\n0.79\n0.79\n0.81\n0.80\n0.79\n0.79\n0.77\n0.77\n0.78\n0.71\n0.70\n0.76\n0.69\n0.68\nPCQMp\n0.91\n0.91\n0.91\n0.88\n0.87\n0.87\n0.90\n0.88\n0.88\n0.86\n0.83\n0.83\n0.86\n0.77\n0.77\n0.82\n0.74\n0.74\nquences with regards to different sampling methods, which\nshares the same reason as grid sampling.\nThe performance of point-based metrics might be influ-\nenced by the point density. To ensure a relatively fair per-\nformance comparison, the sampled point clouds between\ndifferent sampling methods should have a close number\nof points. We choose three sampling grades (I, II, and III)\nrepresenting a number of points of around 1200K, 400K, and\n100K, respectively. For face sampling, the corresponding\nsampling resolutions are 1024, 512, and 256. For EDIV, the\ncorresponding sampling resolutions are 1024, 512, and 128.\nFor SDIV, the thresholds are 1/16, 1/4, and fourfold average\nface size of each sequence.\nThe results are shown in Table 5. Three sampling meth-\nods reveal close performance for grade I and II. Therefore,\nif the sampled point clouds are dense enough, the types of\nsampling methods will have a reduced effect on the point-\nbased metrics. Face sampling presents better performances\nthan EDIV and SDIV in grade III, which is a relatively sparse\nsampling result. The sparser the point clouds, the less com-\nputationally complex the point-based metrics are. Taking\ninto account all the results from Table 5, we recommend\nusing face sampling for low-latency applications.\n5\nCONCLUSION\nIn this paper, we first propose a new DCM database which\ncontains 303 samples, called TDMD. It gathers eight ref-\nerence sequences and six types of distortions: color noise,\ntexture map downsampling, geometrical Gaussian noise,\nmesh decimation, MPEG lossy compression, and texture\nmap compression. We have used a predefined camera path\nto convert DCM to PVS and have conducted extensive\nsubjective experiments. We analyze the influence of different\ndistortions on DCM perception. The MOSs of CN, DS, MLC,\nand GN present wide variations, while MD and TC seem\nto have limited impact on visual perception. We think the\nlevels of MD suggested by WG7, and considered in this\npaper are too low to cause obvious perception degradation,\nand fine texture distortion caused by TC is masked due to\nthe frame switching of dynamic sequence.\nThen, we test three types of objective metrics on the\ndatabase, including image-based, point-based and video-\nbased metrics. We present the advantages and drawbacks\nof different metrics. PCQMp and MS-SSIM report the best\noverall performance. Image-based and video-based metrics\nneed rendering before calculation which is time-consuming,\nwhile point-based metrics rely on the results of sampling\nwhich might reduce the overall performance. Based on the\nabove results, we give suggestions about the selection of the\nmetric in practical applications.\nThere are many interesting tracks for future study. Be-\nsides viewing DCM via a 2D monitor, rendering DCM\nvia a VR headset in an immersive environment is another\noption that can illustrate the different characteristics of DCM\nas a 3D media. Whether the viewers will report different\nsubjective perceptions between the 2D monitor and the VR\nenvironment, and whether the objective metrics will show\nconsistent results for these two viewing conditions deserve\nfurther exploration.\nAPPENDIX\nMLC parameters\nTable 6 is the target bitrates of MPEG WG7 all-intra lossy\ncompression mode, and the Draco quantization parameters\nfor the position (Draco-QP) and texture coordinates (Draco-\nQT) vary from one sample to another.\nFor \u201cLevi\u201d, R1 cannot be reached due to the decimation\nproblem explained above. As seen in Table 6, R1 is the low-\nGrid Sampling                                Face Sampling                             EDIV Sampling                                 SDIV Sampling\nFig. 8. Snapshots of different sampling methods.\n12\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nLongdress\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nSoldier\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nDancer\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nBasketball_player\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nMitch\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nThomas\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nFootball\n1\n2\n3\n4\n5\n6\n7\n \nGrade\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nCorrelation\nLevi\nFig. 9. Performance of yuvp and PCQMp with different sampling meth-\nods on different sequences.\nTABLE 6\nTarget bitrates for all-intra lossy compression mode.\nTest Sequence\nR1\nR2\nR3\nR4\nR5\nLongdress\n5\n9\n12\n15\n22\nSoldier\n5\n9\n12\n15\n22\nBasketball player\n3\n5\n10\n14\n21\nDancer\n3\n5\n10\n14\n21\nMitch\n4\n6\n11\n16\n24\nThomas\n4\n6\n11\n16\n24\nFootball\n4\n8\n12\n17\n25\nLevi\n4\u2217\n8\n12\n17\n25\nest bitrate and it requires the application of MD. Therefore,\nthere are only four available target bitrates for Levi.\nAnalysis of MD on Human Perception\nWe postulate that the main reason for the increase of MD\ndegree has a limited impact on human perception are: the\nMD degrees proposed by WG7 are low, meaning that the\nnumber of faces after MD is still large enough to preserve\nmost sample characteristics; and for these MD samples, con-\ntour flicker is the main manifestation in perception quality\ndegradation, which is less distracting than other types of\ndistortion.\nWe plot the zoom-in snapshots of the 5th and 6th frames\nof the reference and the MD-5 distorted \u201cBasketball player\u201d\nmesh in Fig. 10. First, we can observe that the reference and\nthe MD-5 samples show close subjective perceptions. This\nillustrates 5K faces can still exhibit sample characteristics\nwith high fidelity.\nSecondly, a pink curve is plotted to\nmark the general contour of the head for the four examples.\nWe use orange and green ellipses to circle areas having\nobvious differences between the reference and the MD-5\nsamples. We find that for adjacent frames, the transition of\nthe reference contours is smoother than that of the MD-5.\nFor the rendered PVSs, MD can lead to contour flicker, and\nthe higher the degree of MD, the more obvious the flicker.\nreference                          5th   frame                                                 6th frame\n  MD-5                                     5th   frame                                                 6th frame\nreference                          5th   frame                                                 6th frame\n  MD-5                                     5th   frame                                                 6th frame\nFig. 10. Zoom-in snapshots of MD.\nAnalysis of TC on Human Perception\nWe hypothesize that the main reason that TC has a lim-\nited impact on human perception is that for the subjective\nexperiment setting in this paper, the noticeable distortions\ncaused by TC are relatively small. Moreover, the perceptual\ndistortion is masked due to the frame switching of dynamic\nsequence.\nWe illustrate the global and zoom-in snapshots of the\n\u201cLongdress\u201d reference, TC-1, TC-5 and TC-7 in Fig. 11.\nFor the global snapshots, we cannot perceive obvious dif-\nferences. For the zoom-in snapshots of the texture map,\nhowever, we can see that TC causes a slight blurring of\nthe texture (cf., the content in the orange ellipses), which\nis limited even for the distortion caused by the maximum\nQP value (TC-7: QP = 50). Besides, the frame switching\n13\nof dynamic sequences tends to mask the visibility of fine\ntextures, which makes it hard for the viewers to distinguish\nthe quality degradation, making the TC distortions more\nobvious for static colored meshes. Therefore, the viewers\nusually give a high score for TC samples.\nReference\nTC-1\nTC-5\nTC-7\nReference\nTC-1\nTC-5\nTC-7\nFig. 11. Snapshots of distortion TC.\nREFERENCES\n[1]\nM. Attene, M. Campen, and L. Kobbelt, \u201cPolygon mesh repairing:\nAn application perspective,\u201d ACM Computing Surveys, vol. 45,\nno. 2, pp. 1\u201333, 2013.\n[2]\nI. Abouelaziz, A. Chetouani, M. El Hassouni, and H. Cherifi,\n\u201cMesh visual quality assessment metrics: A comparison study,\u201d\nIEEE Int. Conf. Signal-Image Technology & Internet-Based Systems,\npp. 283\u2013288, 2017.\n[3]\nWG7, \u201cCfp for dynamic mesh coding,\u201d ISO/IEC JTC 1/SC 29/WG\n7, MPEG 3D Graphics Coding, WG7 N0231, 2021.\n[4]\nA. Maglo, G. Lavou\u00b4e, F. Dupont, and C. Hudelot, \u201c3d mesh\ncompression: survey, comparisons, and emerging trends,\u201d ACM\nComputing Surveys, vol. 47, no. 3, pp. 1\u201341, 2015.\n[5]\nS. Yang, C.-S. Kim, and C.-C. Kuo, \u201cA progressive view-dependent\ntechnique for interactive 3-d mesh transmission,\u201d IEEE Trans.\nCircuits and Systems for Video Technology, vol. 14, no. 11, pp. 1249\u2013\n1264, 2004.\n[6]\nL. Jiang, J. Zhang, B. Deng, H. Li, and L. Liu, \u201c3d face reconstruc-\ntion with geometry details from a single image,\u201d IEEE Trans. Image\nProcessing, vol. 27, no. 10, pp. 4756\u20134770, 2018.\n[7]\nG. A. Hansen, R. W. Douglass, and A. Zardecki, \u201cMesh enhance-\nment: selected elliptic methods, foundations and applications,\u201d\nWorld Scientific, 2005.\n[8]\nK. Christaki, E. Christakis, P. Drakoulis, A. Doumanoglou,\nN. Zioulis, D. Zarpalas, and P. Daras, \u201cSubjective visual quality\nassessment of immersive 3d media compressed by open-source\nstatic 3d mesh codecs,\u201d Int. Conf. MultiMedia Modeling, pp. 80\u201391,\n2019.\n[9]\nM. Corsini, E. D. Gelasca, T. Ebrahimi, and M. Barni, \u201cWater-\nmarked 3-d mesh quality assessment,\u201d IEEE Trans. Multimedia,\nvol. 9, no. 2, pp. 247\u2013256, 2007.\n[10] J. Guo, V. Vidal, I. Cheng, A. Basu, A. Baskurt, and G. Lavoue,\n\u201cSubjective and objective visual quality assessment of textured 3d\nmeshes,\u201d ACM Trans. Applied Perception, vol. 14, no. 2, pp. 1\u201320,\n2016.\n[11] F. Torkhani, K. Wang, and J.-M. Chassery, \u201cPerceptual quality as-\nsessment of 3d dynamic meshes: Subjective and objective studies,\u201d\nSignal Processing: Image Communication, vol. 31, pp. 185\u2013204, 2015.\n[12] A. Javaheri, C. Brites, F. Pereira, and J. Ascenso, \u201cPoint cloud ren-\ndering after coding: Impacts on subjective and objective quality,\u201d\nIEEE Trans. Multimedia, vol. 23, pp. 4049\u20134064, 2021.\n[13] G. Lavou\u00b4e, E. D. Gelasca, F. Dupont, A. Baskurt, and T. Ebrahimi,\n\u201cPerceptually driven 3d distance metrics with application to wa-\ntermarking,\u201d Applications of Digital Image Processing XXIX, vol.\n6312, pp. 150\u2013161, 2006.\n[14] L. Vasa and J. Rus, \u201cDihedral angle mesh error: a fast percep-\ntion correlated distortion measure for fixed connectivity triangle\nmeshes,\u201d Computer Graphics Forum, vol. 31, no. 5, pp. 1715\u20131724,\n2012.\n[15] WG7, \u201cAnchors for dynamic mesh coding evaluation,\u201d ISO/IEC\nJTC 1/SC 29/WG 7, MPEG 3D Graphics Coding, WG7 N0278, 2022.\n[16] \u2014\u2014, \u201cMetrics for dynamic mesh coding,\u201d ISO/IEC JTC 1/SC\n29/WG 7, MPEG 3D Graphics Coding, WG7 N0225, 2021.\n[17] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage\nquality assessment: from error visibility to structural similarity,\u201d\nIEEE Trans. Image Processing, vol. 13, no. 4, pp. 600\u2013612, 2004.\n[18] Z. Li, A. Aaron, I. Katsavounidis, A. Moorthy, and M. Manohara,\n\u201cToward a practical perceptual video quality metric,\u201d The Netflix\nTech Blog, vol. 6, no. 2, p. 2, 2016.\n[19] Y. Nehm\u00b4e, F. Dupont, J.-P. Farrugia, P. Le Callet, and G. Lavou\u00b4e,\n\u201cVisual quality of 3d meshes with diffuse colors in virtual reality:\nSubjective and objective evaluation,\u201d IEEE Trans. Visualization and\nComputer Graphics, vol. 27, no. 3, pp. 2202\u20132219, 2020.\n[20] Y. Nehm\u00b4e, J. Delanoy, F. Dupont, J.-P. Farrugia, P. Le Callet,\nand G. Lavou\u00b4e, \u201cTextured mesh quality assessment: Large-scale\ndataset and deep learning-based quality metric,\u201d ACM Trans.\nGraphics, vol. 42, no. 3, pp. 1\u201320, 2023.\n[21] G. Lavou\u00b4e, E. D. Gelasca, F. Dupont, A. Baskurt, and T. Ebrahimi,\n\u201cPerceptually driven 3d distance metrics with application to wa-\ntermarking,\u201d Applications of Digital Image Processing XXIX, vol.\n6312, pp. 150\u2013161, 2006.\n[22] P. Cignoni, C. Rocchini, and R. Scopigno, \u201cMetro: measuring error\non simplified surfaces,\u201d Computer Graphics Forum, vol. 17, no. 2,\npp. 167\u2013174, 1998.\n[23] N. Aspert, D. Santa-Cruz, and T. Ebrahimi, \u201cMesh: Measuring\nerrors between surfaces using the hausdorff distance,\u201d Proc. Int.\nConf. Multimedia and Expo, vol. 1, pp. 705\u2013708, 2002.\n[24] Z. Karni and C. Gotsman, \u201cSpectral compression of mesh geom-\netry,\u201d Proc. conf. Computer Graphics and Interactive Techniques, pp.\n279\u2013286, 2000.\n[25] G. Lavou\u00b4e, M. C. Larabi, and L. Vasa, \u201cOn the efficiency of image\nmetrics for evaluating the visual quality of 3d models,\u201d IEEE Trans.\nVisualization and Computer Graphics, vol. 22, no. 8, pp. 1987\u20131999,\n2015.\n[26] L. Zhang, L. Zhang, X. Mou, and D. Zhang, \u201cFsim: A feature\nsimilarity index for image quality assessment,\u201d IEEE Trans. Image\nProcessing, vol. 20, no. 8, pp. 2378\u20132386, 2011.\n[27] M.\nRoberts,\n\u201cHow\nto\nevenly\ndistribute\npoints\non\na\nsphere\nmore\neffectively\nthan\nthe\ncanonical\nfi-\nbonacci\nlattice,\u201d\nhttp://extremelearning.com.au/\nevenly-distributing-points-on-a-sphere/.\n[28] I. Abouelaziz, A. Chetouani, M. El Hassouni, L. J. Latecki, and\nH. Cherifi, \u201c3d visual saliency and convolutional neural network\nfor blind mesh quality assessment,\u201d Neural Computing and Applica-\ntions, vol. 32, pp. 16 589\u201316 603, 2020.\n[29] \u2014\u2014, \u201cCombination of handcrafted and deep learning-based fea-\ntures for 3d mesh quality assessment,\u201d IEEE Int. Conf. Image\nProcessing, pp. 171\u2013175, 2020.\n14\n[30] \u2014\u2014, \u201cConvolutional neural network for blind mesh visual quality\nassessment using 3d visual saliency,\u201d IEEE Int. Conf. Image Process-\ning, pp. 3533\u20133537, 2018.\n[31] Q. Yang, H. Chen, Z. Ma, Y. Xu, R. Tang, and J. Sun, \u201cPredicting\nthe perceptual quality of point cloud: A 3d-to-2d projection-based\nexploration,\u201d IEEE Trans. Multimedia, vol. 23, pp. 3877\u20133891, 2020.\n[32] G. Meynet, Y. Nehm\u00b4e, J. Digne, and G. Lavou\u00b4e, \u201cPcqm: A full-\nreference quality metric for colored 3d point clouds,\u201d in IEEE Int.\nConf. Quality of Multimedia Experience, 2020, pp. 1\u20136.\n[33] Q. Yang, Z. Ma, Y. Xu, Z. Li, and J. Sun, \u201cInferring point cloud\nquality via graph similarity,\u201d IEEE Trans. Pattern Analysis and\nMachine Intelligence, vol. 44, no. 6, pp. 3015\u20133029, 2022.\n[34] Q. Yang, Y. Liu, S. Chen, Y. Xu, and J. Sun, \u201cNo-reference point\ncloud quality assessment via domain adaptation,\u201d Proc. IEEE/CVF\nConf. Computer Vision and Pattern Recognition, pp. 21 179\u201321 188,\nJune 2022.\n[35] Q. Yang, Y. Zhang, S. Chen, Y. Xu, J. Sun, and Z. Ma, \u201cMped:\nQuantifying point cloud distortion based on multiscale potential\nenergy discrepancy,\u201d IEEE Trans. Pattern Analysis and Machine\nIntelligence, pp. 1\u201318, 2022.\n[36] Z. Shan, Q. Yang, R. Ye, Y. Zhang, Y. Xu, X. Xu, and S. Liu,\n\u201cGpa-net:no-reference point cloud quality assessment with multi-\ntask graph convolutional network,\u201d IEEE Trans. Visualization and\nComputer Graphics, pp. 1\u201313, 2023.\n[37] H. Su, Q. Liu, Y. Liu, H. Yuan, H. Yang, Z. Pan, and Z. Wang,\n\u201cBitstream-based perceptual quality assessment of compressed 3d\npoint clouds,\u201d IEEE Trans. Image Processing, vol. 32, pp. 1815\u20131828,\n2023.\n[38] Q. Liu, H. Yuan, R. Hamzaoui, H. Su, J. Hou, and H. Yang,\n\u201cReduced reference perceptual quality model with application to\nrate control for video-based point cloud compression,\u201d IEEE Trans.\nImage Processing, vol. 30, pp. 6623\u20136636, 2021.\n[39] Q. Liu, H. Su, Z. Duanmu, W. Liu, and Z. Wang, \u201cPerceptual\nquality assessment of colored 3d point clouds,\u201d IEEE Trans. Vi-\nsualization and Computer Graphics, vol. 29, no. 8, pp. 3642\u20133655,\n2023.\n[40] E. d\u2019Eon, B. Harrison, T. Myers, and P. A. Chou, \u201c8i voxelized full\nbodies \u2013 a voxelized point cloud dataset,\u201d ISO/IEC JTC1/SC29 Joint\nWG11/WG1 (MPEG/JPEG) input document M40059/M74006, 2017.\n[41] D. Graziosi, A. Zaghetto, and A. Tabatabai, \u201cV-pcc][ee2.6-related]\nmesh generation script update,\u201d ISO/IEC JTC1/SC29/WG7 M55366,\n2017.\n[42] Y. Xu, Y. Lu, and Z. Wen, \u201cOwlii dynamic human mesh sequence\ndataset,\u201d ISO/IEC JTC1/SC29/WG11 M41658, 2017.\n[43] R. Schaefer, P. Andrivon, J. Ricard, and C. Guede, \u201cVolucap and\nxd productions datasets,\u201d ISO/IEC JTC1/SC29/WG7 M56192, 2021.\n[44] R. Pag\u00b4es, E. Zerman, K. Amplianitis, J. Ondrej, and A. Smolic,\n\u201cVolograms\n&\nvsense\nvolumetric\nvideo\ndataset,\u201d\nISO/IEC\nJTC1/SC29/WG7 M56767, 2021.\n[45] Google, \u201cDraco library for 3d geometric meshes and point cloud\ncompression,\u201d https://google.github.io/draco/.\n[46] B. Williams, \u201cHDRTools software,\u201d https://gitlab.com/standards/\nHDRTools.git.\n[47] K.\nSuehring,\n\u201cHEVC\nreference\nsoftware,\u201d\nhttps://vcgit.hhi.\nfraunhofer.de/jvet/HM/-/tags/HM-16.21+SCM-8.8.\n[48] FFMPEG, \u201cA complete, cross-platform solution to record, convert\nand stream audio and video.\u201d https://ffmpeg.org/.\n[49] B. ITU-R RECOMMENDATION, \u201cMethodology for the subjective\nassessment of the quality of television pictures,\u201d International\nTelecommunication Union, 2002.\n[50] P. ITU-T RECOMMENDATION, \u201cSubjective video quality assess-\nment methods for multimedia applications,\u201d International Telecom-\nmunication Union, 1999.\n[51] ITU-T, \u201cSubjective test method for interactive virtual reality ap-\nplications,\u201d https://www.itu.int/ITU-T/workprog/wp item.aspx?isn=\n17817.\n[52] Q.-Y. Zhou, J. Park, and V. Koltun, \u201cOpen3D: A modern library for\n3D data processing,\u201d arXiv:1801.09847, 2018.\n[53] J. Jung, M. Wien, and V. Baronini, \u201cDraft guidelines for remote\nexperts viewing sessions (v2),\u201d ISO/IEC JTC 1/SC 29/AG 5 M57896,\n2021.\n[54] A. Antsiferova, S. Lavrushkin, M. Smirnov, A. Gushchin, D. S. Va-\ntolin, and D. Kulikov, \u201cVideo compression dataset and benchmark\nof learning-based video-quality metrics,\u201d Conf. Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2022.\n[55] Z. Wang, E. P. Simoncelli, and A. C. Bovik, \u201cMultiscale structural\nsimilarity for image quality assessment,\u201d IEEE Asilomar Conf.\nSignals, Systems & Computers, vol. 2, pp. 1398\u20131402, 2003.\n[56] M. Pinson and S. Wolf, \u201cA new standardized method for objec-\ntively measuring video quality,\u201d IEEE Trans. Broadcasting, vol. 50,\nno. 3, pp. 312\u2013322, 2004.\n[57] C. Li and A. C. Bovik, \u201cThree-component weighted structural\nsimilarity index,\u201d Image quality and system performance VI, vol. 7242,\npp. 252\u2013260, 2009.\n[58] VQEG, \u201cFinal report from the video quality experts group on\nthe validation of objective models of video quality assessment,\u201d\nhttp://www.its.bldrdoc.gov/vqeg/vqeg-home.aspx.\n[59] Q. Yang, Z. Ma, Y. Xu, L. Yang, W. Zhang, and J. Sun, \u201cModeling\nthe screen content image quality via multiscale edge attention\nsimilarity,\u201d IEEE Trans. Broadcasting, vol. 66, no. 2, pp. 310\u2013321,\n2020.\n[60] E. Alexiou and T. Ebrahimi, \u201cExploiting user interactivity in\nquality assessment of point cloud imaging,\u201d IEEE Int. Conf. Quality\nof Multimedia Experience, pp. 1\u20136, 2019.\n[61] K. Yang, Q. Yang, J. Jung, Y. Xu, X. Xu, and S. Liu, \u201cExploring the\ninfluence of view and camera path selection for dynamic mesh\nquality assessment,\u201d IEEE Int. Conf. Multimedia and Expo, 2023.\n[62] C. Fu, X. Zhang, T. Nguyen-Canh, X. Xu, G. Li, and S. Liu,\n\u201cSurface-sampling based objective quality assessment metrics for\nmeshes,\u201d in IEEE Int. Conf. Acoustics, Speech and Signal Processing,\n2023, pp. 1\u20135.\n"
  }
]