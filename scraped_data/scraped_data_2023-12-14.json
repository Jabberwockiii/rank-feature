[
  {
    "title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention",
    "link": "https://arxiv.org/pdf/2312.07987.pdf",
    "upvote": "39",
    "text": "SWITCHHEAD:\nACCELERATING\nTRANSFORMERS\nWITH MIXTURE-OF-EXPERTS ATTENTION\nR\u00f3bert Csord\u00e1s1\nPiotr Pi\u02dbekos2\nKazuki Irie3\u2020 J\u00fcrgen Schmidhuber1,2\n1The Swiss AI Lab IDSIA, USI & SUPSI, Lugano, Switzerland\n2AI Initiative, KAUST, Thuwal, Saudi Arabia\n3Center for Brain Science, Harvard University, Cambridge, MA, USA\nrobert@idsia.ch, piotr.piekos@kaust.edu.sa\nkirie@fas.harvard.edu, juergen@idsia.ch\nABSTRACT\nThe costly self-attention layers in modern Transformers require memory and com-\npute quadratic in sequence length. Existing approximation methods usually un-\nderperform and fail to obtain significant speedups in practice. Here we present\nSwitchHead\u2014a novel method that reduces both compute and memory require-\nments and achieves wall-clock speedup, while matching the language modeling\nperformance of baseline Transformers with the same parameter budget. Switch-\nHead uses Mixture-of-Experts (MoE) layers for the value and output projections\nand requires 4 to 8 times fewer attention matrices than standard Transformers.\nOur novel attention can also be combined with MoE MLP layers, resulting in an\nefficient fully-MoE \u201cSwitchAll\u201d Transformer model. Our code is public.1\n1\nINTRODUCTION\nLarge language models (LLMs) have shown remarkable capabilities (Radford et al., 2019; Brown\net al., 2020; OpenAI, 2022; 2023) and great versatility (Bubeck et al., 2023). However, training\nenormous Transformers (Vaswani et al., 2017; Schmidhuber, 1992) requires a considerable amount\nof computing power and memory, which is not accessible to most researchers, academic institutions,\nand even companies. Even running them in inference mode, which is much less resource-intensive,\nrequires significant engineering effort (Gerganov, 2023). Accelerating big Transformers remains an\nimportant open research question.\nIn the literature, Mixture of Experts (MoE)-based feedforward multi-layer perceptron (MLP) layers\n(Shazeer et al., 2017; Jacobs et al., 1991; Ivakhnenko & Lapa, 1965) have been popular methods to\nscale up Transformers to a large number of parameters (Lewis et al., 2021; Lepikhin et al., 2021;\nFedus et al., 2022; Clark et al., 2022; Chi et al., 2022). However, in these works, the parameter effi-\nciency of MoEs has not been studied; MoE models have been typically compared to dense baselines\nwith the same number of FLOPs but with much less parameters. In fact, it has remained a common\nbelief that they are not competitive against their dense counterparts with the same number of pa-\nrameters. More recently, this parameter-efficiency perspective of MoEs has been studied by Csord\u00e1s\net al. (2023). They have proposed a novel MoE model, called \u03c3-MoE, which not only speeds up\nTransformer training through reduced resource requirements, but also achieves performance com-\nparable to or even superior to its parameter-matched dense counterparts; effectively demonstrating\nthe promise of MoEs to scaling up neural networks.\nSuch MoE research often focuses on accelerating MLPs. However, attention layers (Schmidhu-\nber, 1991; Bahdanau et al., 2015) also account for a considerable amount of compute and memory\nusage in Transformers, especially for long context sizes. Existing methods, such as linear atten-\ntion (Schmidhuber, 1991; Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021;\nSchlag et al., 2021), typically fail to achieve significant speedups in practice (Dao et al., 2022) and\noften underperform its quadratic counterpart. As an alternative, MoE-based approaches for attention\nhave been also proposed (Zhang et al., 2022; Peng et al., 2020), but in practice, they only achieve a\n1https://github.com/robertcsordas/moe_attention \u2020Work done at IDSIA.\n1\narXiv:2312.07987v2  [cs.LG]  14 Dec 2023\nFigure 1: Schematic representation of SwitchHead. It consists of few independent heads, each with\nmultiple experts for value and output projections. Each head has a single attention matrix.\nmodest reduction in computing and memory requirements, and typically require a lot of engineering\ntricks for successful training. Generally, MoE-based attention remains underexplored.\nHere we present a novel MoE-based attention mechanism, SwitchHead, which reduces the number\nof attention matrices that need to be computed and stored. Our method is based on the \u03c3-MoE\nby Csord\u00e1s et al. (2023) and does not require regularization or extra tricks for stable training. We\nevaluate our method on a variety of language modeling datasets and two model sizes. We demon-\nstrate that SwitchHead can achieve performance comparable to parameter-matched baselines, with\na fraction of the compute and memory budget. Furthermore, we introduce the \u201cSwitchAll\u201d model,\na fully MoE-based Transformer model, that combines a \u03c3-MoE-based MLP layer with our Switch-\nHead attention, often outperforming dense baselines with the same parameter budgets. Lastly, we\nanalyze the attention maps of our SwitchHead, and find that the maximum of attention maps taken\nover all heads are qualitatively similar to the dense baselines, indicating a significant reduction in\nredundancy without a loss of expressivity. Additionally, expert selections are often interpretable.\n2\nMETHOD\n2.1\nBACKGROUND\nThe standard multi-head self-attention (MHA) layer (Vaswani et al., 2017) consists of four major\nsteps: (1) computing key (K), query (Q), and value (V) projections, (2) computing the attention\nmatrix, (3) using the attention matrix to project the values, and (4) mapping the projected values to\nthe output. Let h, T, dmodel, dhead denote positive integers. Let x \u2208 RT \u00d7dmodel denote an input to the\nMHA layer, T be the sequence length, and dmodel denote the size of the hidden representations of\nthe model. W h\n{K,V,Q} \u2208 Rdmodel\u00d7dhead are the projection matrices for head h. Then Kh = xW h\nK,\nQh = xW h\nQ, and V h = xW h\nV (thus Kh, Qh, V h \u2208 RT \u00d7dhead) are the keys, queries, and values,\nrespectively. The attention matrix for the head h, Ah \u2208 RT \u00d7T , and the output y \u2208 RT \u00d7dmodel are\ncalculated as follows:\nAh = softmax\n\u0012\n1\n\u221admodel\nQhKh\u22ba\n\u0013\n(1)\ny = WO(A0V 0|A1V 1|...|AHV H)\n(2)\nwhere | denotes concatenation in the last dimension, the softmax(\u00b7) is also over the last dimension,\nand WO \u2208 Rdmodel\u00d7Hdhead. However, an alternative formulation reflects the role of WO better. Let\nus divide WO along the second dimension into submatrices for each head, W h\nO \u2208 Rdmodel\u00d7dhead, such\nthat WO = W 0\nO|W 1\nO|...|W H\nO . In this case, the output can be equivalently written as:\ny =\nX\nh\nW h\nOAhV h\n(3)\nFrom this, it can be seen that all computations are local to the heads. Computing the attention ma-\ntrix Ah and the readout AhV h requires compute in order of O(HdheadT 2) MACs (multiplication-\naccumulation operation). During training, it requires the storage of O(HT 2) for the attention ma-\ntrices and O(HTdhead) numbers for storing the sub-results of the projections. Given a sufficiently\nlong sequence, computing the attention matrix and projecting the values will dominate the compute\nrequirements due to the quadratic dependence on the sequence length T.\n2\n2.2\nFROM DENSE TO SWITCHHEAD\nOur goal is to obtain resource reductions while maintaining the fundamental properties of attention\nand retaining a fully expressive attention matrix. In fact, there is still room for improvement: mod-\nern LLMs use tens of heads (Brown et al., 2020; Touvron et al., 2023). Are so many of them all\nnecessary? As we show later in Sec. 3, indeed, naively reducing the number of heads (while keep-\ning the same number of parameters by increasing the head dimension) results in performance loss.\nExplaining the reason for the need for many heads is beyond the scope of this paper. Nevertheless,\nhere are some hypotheses: (1) they provide multiple inputs for the operations that the network per-\nforms in each step, (2) they are specialized and provide inputs only for specific operations. In this\ncase, each operation would use a different subset of heads. (3) They may also provide alternatives\nwith different initializations, some being more successful than others, thus enabling better learning.\nAmong these, some (2) and (3) offer an opportunity for resource savings: if not all heads are needed\nat the same time, it might be possible to switch between them. The simplest method of doing so is to\nproduce a gating signal using a linear projection WS \u2208 Rdmodel\u00d7H, and use the ones with the highest\nactivation, by replacing Eq. 3 with Eq. 6:\ns = \u03c3 (xWS)\n(4)\nE = arg topk(s, K), E \u2282 {1, ..., H}\n(5)\ny[t, c] =\nX\nh\u2208E\ns[t, h](W h\nOAhV h)[t, c]\n(6)\nwhere y[t, c] denotes indexing the specific element of the matrix, specifically denoting timestep t and\nchannel c. Following Csord\u00e1s et al. (2023), we use a non-competitive selection function. Intuitively,\nthis corresponds to choosing a subset of attention heads for each output position. Our preliminary\nexperiments confirmed that this method is indeed feasible for language modeling on WikiText-103.\nHowever, it is difficult to achieve acceleration and memory savings with this method. To see why,\nnotice that the entries of the attention matrix Ah depend on pairs of inputs in different positions, but\nthe choice is made only based on the output position. Thus, in the worst case, all possible projections\nhave to be computed on the \"source side\" for the keys and values, which we would like to avoid.\nAn alternative approach, which we propose here, is to perform the conditional computation on the\nprojections, independently for the source side (K and V ) and the destination side (Q and output).\nThis avoids conditional computation that involves the attention matrix itself. The obvious way to\nmake the projections conditional is to use Mixture of Experts (MoEs). In this case, the concepts\nof \"heads\" are not well defined anymore. Therefore, we define a head to be a specific, computed\nattention matrix. For each head h, we define a list of E experts. Then, the projection matrices\nbecome W h,e\nK , W h,e\nQ , W h,e\nV\nand W h,e\nO , where h denotes the head index and e the specific expert.\nThen we compute the source-side expert selection as following:\nsh\nS = \u03c3(xW h\nS )\n(7)\nEh\nS = arg topk(sh\nS, K), Eh\nS \u2282 {1, ..., E}\n(8)\nWe compute the destination-side experts similarly: sh\nD = \u03c3(xW h\nD), Eh\nD = arg topk(sh\nD, K), Eh\nS \u2282\n{1, ..., E}. Then, the value projection V h is computed as a weighted sum of the selected experts:\nV h =\nX\ne\u2208Eh\nS\nsh\nS[e]xW h,e\nV\n(9)\nThe key and query projections are computed similarly: Kh = P\ne\u2208Eh\nS sh\nS[e]xW h,e\nK , and Qh =\nP\ne\u2208Eh\nD sh\nD[e]xW h,e\nQ . The output projection also becomes an MoE:\ny =\nH\u22121\nX\nh=0\nX\ne\u2208Eh\nD\nW h,e\nO AhV h\n(10)\nAs we\u2019ll show, it is not necessary to make all projections MoEs. In Section 3.1 we show that keeping\na single copy of the projections Q and K and reusing them for all experts is beneficial. We call this\nmethod SwitchHead. If this method can reduce the number of heads H by having more experts, E,\n3\nthen it provides an easy way to reduce the resource requirements of MHA. Note that our method\ndoes not depend on the specific implementation of the attention, allowing easy experimentation and\nresearch. A schematic representation is shown in Fig. 1.\nUnlike standard MoE methods, we found that no regularization is necessary to achieve good perfor-\nmance with our method.\n2.3\nRESOURCE USAGE OF DIFFERENT METHODS\nIn this section, we discuss the compute and memory usage of different attention variants. We will\ndefine the compute in terms of the number of multiply-accumulate operations (MACs, also used\nby Zhang et al. (2022)), which is arguably better defined than FLOPs (e.g., does one step of the\nmatrix multiplication count as 1 FLOP or 2? Do we include the softmax?). All calculations will be\npresented for a single attention layer for a single sequence, and they are presented this way in all our\ntables. Both the memory and compute requirements scale linearly with both the batch size and the\nnumber of layers.\nConsider a sequence of inputs of length T, with representation size dmodel. Let dhead be the width\nof the K, Q, and V projections used for the attention layer. For Transformer XL-style attention, let\nthe size of the context be CT, where C \u2212 1 is the number of past chunks included in the context\nof the current attention step. We can divide the computation into two major parts: calculating the\nprojections, which do not involve the attention map, and calculating the attention map and projecting\nthe sequence of values using it.\nFirst, consider the case of the standard Transformer XL (Dai et al., 2019). Here, from the input x \u2208\nRT \u00d7dmodel, we calculate the Kh, Qh, V h \u2208 RT \u00d7dhead using projection matrices of shape Rdmodel\u00d7dhead.\nThe output after the attention is projected in a similar manner (Eq. 3). Thus, the projections take a\ntotal of 4Tdmodeldhead MACs per head. For backpropagation, we have to store all the intermediate\nresults. This takes Tdhead numbers of Kh, Qh and V h. Also, the projected values should be stored.\nThey have an identical shape, therefore, the total memory used by projections is 4Tdhead numbers\nper head. Now consider the resource usage related to the attention matrix. It involves calculating the\nproduct of QhKh\u22ba, which takes dheadCT 2 MACs (multiplication by C is needed because the shape\nof Kh and V h for Transformer XL is CT \u00d7 dhead). The projection of the values with the attention\nmatrix AhV h is similar. For the memory usage, the attention needs CT 2 numbers, but it needs to\nbe stored both before and after the activation function. In addition, calculating the projection of the\nposition encodings is necessary. This depends on the implementation, but in our case, it involves a\nmatrix multiplication, and the total amount of computation is 2dheaddmodelTC, and it needs 2dheadTC\nnumbers of storage. Thus the resource requirements are:\nN XL\nMAC = H\n\u00004Tdheaddmodel + 2CT 2dhead + 2CTdheaddmodel\n\u0001\n(11)\nN XL\nmem = H\n\u00004Tdhead + 2CT 2 + 2CTdhead\n\u0001\n(12)\nThe resource usage of SwitchHead is different. First, the number of heads H is significantly re-\nduced, but dhead is typically larger. Additionally, there are K experts active at the same time. Here,\nwe only consider the case where the value and outputs are experts, but Qh and Kh are not (this ver-\nsion performs the best; see Sec. 3.1). Then, we have two projections that are identical with that of\nTransformer XL, and two MoE-based projections. These use TKdmodeldhead MACs to calculate the\nprojection and another TKdhead to calculate their weighted average. With a smart kernel implemen-\ntation, memory usage is not affected by K, thus the formula remains the same as Eq. 12 (note, how-\never, that H and dhead are very different in practice). The compute requirement can be calculated as:\nN SwitchHead\nMAC\n= H\n\u00002Tdheaddmodel + 2TKdhead(dmodel + 1) + 2CT 2dhead + 2CTdheaddmodel\n\u0001\n(13)\nAdditionally, the expert selection logic needs minimal additional resources, which can be ignored.\nNote that the comparison between the MACs of the standard (Eq. 11) and SwitchHead (Eq. 13)\ndepends on the exact values of the hyper-parameters. However, as we\u2019ll see in Sec. 3, in our\ntypical configurations, SwitchHead provides good predictive performance with significantly lower\nH compared to the standard Transformer, resulting in reduced resource usage in the end.\n4\n3\nEXPERIMENTS\nFollowing Csord\u00e1s et al. (2023) we conduct our experiments in a parameter-matched setting which\nbetter reflects the expressivity of language models (than the FLOPS-matched setting often used to\nevaluate MoEs). Without this constraint, with MoEs it is often possible to compensate for a weaker\nmethod by adding more experts. We use and adopt the CUDA kernel of Csord\u00e1s et al. (2023) for our\npurposes. To match the number of parameters of different models, we follow a systematic procedure.\nFirst, we measure the parameter count of the dense Transformer, which serves as our target. Then,\nfor each method, we set the total number of experts (including between heads, HE for SwitchHead)\nto the same as the original number of heads. We increase the head projection size dhead to the\nmaximum that keeps the parameter count below our target. Because our CUDA kernel supports\nonly dhead with multiples of 4, this often remains below the parameter count of the baseline. For\nfurther compensation, we slightly increase dff until we achieve a match that differs from our target\nwith no more than 100k parameters but never exceeds it. We do not claim that this parameter-\nmatching method is optimal, but we aim to have a consistent algorithm that does not require tuning,\nwhich is prohibitively expensive and would have to be done for each model separately. Detailed\nhyperparameters of all our models can be found in Sec. A.2 in the Appendix.\nFor all datasets except the character-level Enwik8 (Hutter, 2006), we use sub-word units (Sennrich\net al., 2016; Schuster & Nakajima, 2012) obtained with a SentencePiece tokenizer (Kudo &\nRichardson, 2018) with a vocabulary size of 8k tokens.\nUnless otherwise noted, all models,\nincluding ours, are Transformer XL (Dai et al., 2019), with the context size being twice the size of\nthe active/current chunk.\nAll models are trained for 100k batches. Some of the datasets we consider (C4 (Raffel et al., 2020),\nand peS2o (Soldaini & Lo, 2023)) are much larger. In this case, we train on the first 105 \u2217 T \u2217 Nbatch\ntokens of the dataset.\n3.1\nWHICH PROJECTIONS REQUIRE AN MOE?\nAs discussed in Sec. 2.2, each linear projection (K, V, Q, O) can potentially be replaced by an MoE.\nHere we first check which projection benefits from such a replacement. As we target the parameter-\nmatched setting, having experts where they are not necessary can have a negative effect. Since they\nuse a significant part of the parameter budget, they can reduce the number of parameters available\nfor the more useful parts of the model. Thus, we did a search over all possible combinations of\nexpert versus fixed projections with two active heads and compared them to the parameter-matched\nbaseline on Wikitext 103. Our models have 47M parameters. We also include a parameter-matched\nbaseline with two heads, which serves as a lower bound for the performance. The results are shown\nin Tab. 1. It can be seen that the output projection is necessary to match the performance of the\nbaseline. Having key and query experts seems to be unnecessary. In fact, without the output and\nvalue experts, they even underperform the dense baseline with H = 2 heads. The best-performing\nmodel is the one with experts for both value and output projections. We use this model variant for\nall the other experiments in this paper.\n3.2\nCOMPARING WITH MOA\nThe method most related to ours is the so-called Mixture of Attention Heads, or MoA (Zhang et al.,\n2022). They use a selection mechanism to choose active attention heads from a set of experts.\nHowever, they have a single set of K and V projections shared between experts; thus, acceleration\nis possible. However, in the original paper, the authors use a high number of selected heads (8-16)\nwhich seems necessary to achieve good performance. Thus, the resource reductions are moderate.\nMoreover, MoA uses three different regularizers, which have to be tuned independently.\nWe compare our method with our reimplementation of MoA with a different number of selected\nheads. Given the complexity of tuning its regularization coefficients, we take them directly from\nZhang et al. (2022). For a fair comparison, we also integrated the non-competitive selection mecha-\nnism of Csord\u00e1s et al. (2023) into MoA. The results are shown in Table 2. Similarly to our method,\nwe found that with non-competitive selection, no regularization is required, and the predictive per-\nformance usually is superior to the original formulation. However, it still underperforms our method\ngiven a similar computation and memory budget.\n5\nTable 1: Performance of SwitchHead with E = 5 experts and H = 2 heads. Different projections\nare either experts or fixed for the given head. Parameter-matched baseline with H = 10 and H = 2\nare shown. Models sorted by perplexity. 47M parameters models on Wikitext 103.\nModel\nnheads\nV expert\nK expert\nQ expert\nO expert\nPerplexity\nSwitchHead\n2\nY\nN\nN\nY\n12.27\nSwitchHead\n2\nN\nN\nN\nY\n12.30\nTransformer XL\n10\n-\n-\n-\n-\n12.31\nSwitchHead\n2\nN\nY\nN\nY\n12.36\nSwitchHead\n2\nY\nY\nN\nY\n12.37\nSwitchHead\n2\nY\nN\nY\nY\n12.42\nSwitchHead\n2\nY\nN\nN\nN\n12.45\nSwitchHead\n2\nN\nN\nY\nY\n12.45\nSwitchHead\n2\nY\nN\nY\nN\n12.51\nSwitchHead\n2\nY\nY\nY\nY\n12.57\nSwitchHead\n2\nN\nY\nY\nY\n12.59\nSwitchHead\n2\nY\nY\nY\nN\n12.61\nSwitchHead\n2\nY\nY\nN\nN\n12.69\nTransformer XL\n2\n-\n-\n-\n-\n12.74\nSwitchHead\n2\nN\nN\nY\nN\n12.75\nSwitchHead\n2\nN\nY\nN\nN\n12.79\nSwitchHead\n2\nN\nY\nY\nN\n12.90\nTable 2: Performance of SwitchHead compared to different MoA variants. \"sel. activation\" denotes\nthe activation function used for selecting the experts. the MoA can outperform the baseline, but only\nat a price of using significantly more computing and memory. Also, SwitchHead outperforms the\nbaseline dense Transformer. Results are on Wikitext 103.\nModel\nsel. activation\nnheads\n#params\nPerplexity\nMACs\nMem (floats)\nMoA\nsigmoid\n8\n47M\n12.13\n390.2M\n2.6M\nMoA\nsigmoid\n6\n47M\n12.16\n306.8M\n1.9M\nSwitchHead\nsigmoid\n2\n47M\n12.27\n170.4M\n0.8M\nTransformer XL\n-\n10\n47M\n12.31\n453.4M\n3.5M\nMoA\nsigmoid\n4\n47M\n12.39\n223.5M\n1.3M\nMoA\nsoftmax\n4\n47M\n12.60\n223.5M\n1.3M\nMoA\nsoftmax\n6\n47M\n12.64\n306.8M\n1.9M\nMoA\nsigmoid\n2\n47M\n12.65\n140.1M\n0.7M\nMoA\nsoftmax\n8\n47M\n12.77\n390.2M\n2.6M\nMoA\nsoftmax\n2\n47M\n12.84\n140.1M\n0.7M\nMoA\nsoftmax\n8\n262M\n9.50\n2.9G\n9.9M\nSwitchHead\nsigmoid\n2\n262M\n9.55\n2.0G\n2.9M\nMoA\nsigmoid\n8\n262M\n9.56\n2.9G\n9.9M\nMoA\nsigmoid\n12\n262M\n9.58\n4.1G\n14.7M\nTransformer XL\n-\n16\n262M\n9.66\n5.4G\n21.0M\nMoA\nsoftmax\n12\n262M\n9.68\n4.1G\n14.7M\nMoA\nsoftmax\n4\n262M\n9.69\n1.7G\n5.1M\nMoA\nsigmoid\n4\n262M\n9.77\n1.7G\n5.1M\nMoA\nsoftmax\n2\n262M\n9.87\n1.1G\n2.7M\nMoA\nsigmoid\n2\n262M\n10.02\n1.1G\n2.7M\n3.3\nPERFORMANCE ON DIFFERENT DATASETS\nWe test our methods on a diverse set of language modeling datasets, including C4 (Raffel et al.,\n2020), Enwik8 (Hutter, 2006), peS2o (Soldaini & Lo, 2023), at two different scales: a 47M and a\n262M parameters. The results are shown in Tab. 3. We compare our models to two baselines: one\nwith the same number of heads as the total number of experts (H \u00b7 E) of the SwitchHead models,\nand the other has the same number of heads as the number of active attention matrices (H) as our\nmodels. Our models always closely match the performance of the full, many-head baseline with the\nfraction of memory and compute requirements. Importantly, our method also achieves a wall-clock\n6\nspeedup, enough to accelerate the entire training pipeline by a factor of around 1.5 (see Appendix\nA.4 for more details). This confirms the competitiveness of our method.\nTable 3: Performance of SwitchHead compared to baselines on different datasets with different\nmodel sizes. It can be seen that the predictive performance of our SwitchHead model is comparable\nto the baselines, and is always better than the baseline with an equal number of heads. Perplexity is\nshown for Wikitext 103, C4 and peS2o datasets, and bits/character (bpc) for Enwik8.\nModel\nDataset\nnheads\n#params\nppl/bpc\nMACs\nMem (floats)\nSwitchHead\nC4\n2\n47M\n22.49\n202.5M\n0.8M\nTransformer XL\nC4\n10\n47M\n22.63\n453.4M\n3.5M\nTransformer XL\nC4\n2\n47M\n23.71\n453.4M\n1.4M\nSwitchHead\nC4\n4\n262M\n16.38\n2.4G\n5.6M\nTransformer XL\nC4\n16\n262M\n16.58\n5.4G\n21.0M\nTransformer XL\nC4\n4\n262M\n17.09\n5.4G\n8.4M\nSwitchHead\nWikitext 103\n2\n47M\n12.31\n170.4M\n0.8M\nTransformer XL\nWikitext 103\n10\n47M\n12.32\n453.4M\n3.5M\nTransformer XL\nWikitext 103\n2\n47M\n12.73\n453.4M\n1.4M\nSwitchHead\nWikitext 103\n2\n262M\n9.77\n2.0G\n2.9M\nTransformer XL\nWikitext 103\n16\n262M\n9.80\n5.4G\n21.0M\nTransformer XL\nWikitext 103\n2\n262M\n10.09\n5.4G\n6.3M\nTransformer XL\npeS2o\n10\n47M\n12.83\n453.4M\n3.5M\nSwitchHead\npeS2o\n2\n47M\n12.84\n202.5M\n0.8M\nTransformer XL\npeS2o\n2\n47M\n13.37\n453.4M\n1.4M\nTransformer XL\npeS2o\n16\n262M\n9.78\n5.4G\n21.0M\nSwitchHead\npeS2o\n4\n262M\n9.86\n2.4G\n5.6M\nTransformer XL\npeS2o\n4\n262M\n10.11\n5.4G\n8.4M\nSwitchHead\nEnwik8\n2\n41M\n1.10\n709.3M\n2.8M\nTransformer XL\nEnwik8\n8\n41M\n1.10\n1.6G\n10.5M\nTransformer XL\nEnwik8\n2\n41M\n1.13\n1.6G\n4.2M\n3.3.1\nSWITCHALL\nThe goal of achieving more resource-efficient Transformers includes reducing the resource require-\nments of both the MLP and the attention layers. Csord\u00e1s et al. (2023) proposed a parameter-efficient\nMoE method to accelerate the MLP layers. However, it remains unclear whether it can be effi-\nciently combined with our SwitchHead, or can have some negative interaction effect if combined in\na \"SwitchAll\", where every layer is MoE-based.\nIn order to verify this, we take the architecture proposed by Csord\u00e1s et al. (2023) without any hy-\nperparameter change and replace the attention layer with SwitchHead. The hyperparameters for the\nattention are directly taken from the experiments shown in Tab. 3. The results are shown in Tab. 4.\nThe combined, fully-MoE model often outperforms the dense baselines for each dataset and model\nsize considered, except in the case of the 259M parameter model on the C4 dataset.\n4\nROPE POSITIONAL ENCODINGS\nAll of our experiments so far have used a Transformer XL model. Thus, it remains unclear whether\nSwitchHead is specific to this model or can be also used with other attention methods. As an al-\nternative, we consider RoPE positional encodings Su et al. (2021) without the XL cache (thus, the\nattention matrices are square). We test these models on Wikitext 103. The results are shown in Tab.\n5. Our method also performs well in this case.\n7\nTable 4: Performance of SwitchAll (SwitchHead + \u03c3-MoE (Csord\u00e1s et al., 2023)) on different\ndatasets and model sizes. Our SwitchAll model is close or better compared to the baselines.\nModel\nDataset\nnheads\n#params\nppl/bpc\nMACs\nMem (floats)\nSwitchAll\nWikitext 103\n2\n47M\n12.17\n170.4M\n0.8M\nTransformer XL\nWikitext 103\n10\n47M\n12.32\n453.4M\n3.5M\nTransformer XL\nWikitext 103\n16\n262M\n9.80\n5.4G\n21.0M\nSwitchAll\nWikitext 103\n4\n259M\n9.81\n2.4G\n5.6M\nSwitchAll\nC4\n2\n47M\n22.09\n202.5M\n0.8M\nTransformer XL\nC4\n10\n47M\n22.63\n453.4M\n3.5M\nSwitchAll\nC4\n4\n259M\n16.45\n2.4G\n5.6M\nTransformer XL\nC4\n16\n262M\n16.58\n5.4G\n21.0M\nSwitchAll\npeS2o\n2\n47M\n12.56\n202.5M\n0.8M\nTransformer XL\npeS2o\n10\n47M\n12.83\n453.4M\n3.5M\nTransformer XL\npeS2o\n16\n262M\n9.78\n5.4G\n21.0M\nSwitchAll\npeS2o\n4\n259M\n9.86\n2.4G\n5.6M\nTable 5: Performance of SwitchHead compared to dense baseline on Wikitext 103, using RoPE\npositional encoding instead of Transformer XL.\nModel\nDataset\nnheads\n#params\nppl/bpc\nMACs\nMem (floats)\nSwitchHead (RoPE)\nWikitext 103\n2\n45M\n12.75\n285.6M\n1.3M\nTransformer (RoPE)\nWikitext 103\n10\n45M\n12.78\n560.9M\n6.1M\nTransformer (RoPE)\nWikitext 103\n2\n45M\n12.96\n560.9M\n1.9M\nSwitchHead (RoPE)\nWikitext 103\n4\n243M\n10.00\n4.2G\n18.4M\nTransformer (RoPE)\nWikitext 103\n16\n244M\n10.17\n6.4G\n37.7M\nTransformer (RoPE)\nWikitext 103\n2\n244M\n10.26\n6.4G\n8.4M\n5\nANALYSIS\nIn order to see how the network uses the attention heads, we trained a small, 6-layer, 8-head Trans-\nformer on ListOps (Nangia & Bowman, 2018; Csord\u00e1s et al., 2022). The reason for this choice is\nthat small, algorithmic tasks tend to be more interpretable compared to language models. We also\ntrain a parameter-matched, 2-head SwitchHead model. Both models achieve around 95% accuracy\non a held-out IID validation set, in contrast to the dense 2-head model, which saturates around 80%.\nNote that ListOps is a classification task and does not use autoregressive masking.\nFollowing Csord\u00e1s et al. (2022), we visualize the maximum of attention heads for each layer, both for\nthe standard Transformer (Fig. 2a) and SwitchHead (Fig. 2b). The attention maps are qualitatively\nsimilar. Note that the initialization and the learning dynamics are different for the two models, thus\nthe overlap would not be perfect even with the same type of model. We show all the attention maps\nfor both models in Fig. 4 and 3 in the Appendix.\nIn addition, we visualize individual attention heads for the SwitchHead model. An example is shown\nin Fig. 2c. In addition to the attention map, we show the weight of the selected experts for both the\nvalue and output projection (denoted by V and O, respectively, on the sides of the attention map).\nOften it is possible to interpret the selection weights: here, the output experts specialize according to\ndifferent operations, while the input ones distinguish numbers and closed parentheses. The attention\nmap itself appears to distribute information about contiguous chunks of numbers. Similar plots for\nall heads are shown in Fig. 5 in the Appendix.\nThe attention maps of the language models are difficult to interpret. However, we visualized the\nattention maps of the 47M parameter Transformer XL and the SwitchHead model from Tab. 3. We\nfound them to be qualitatively similar. We also identified induction heads (Olsson et al., 2022) in\nboth models, some examples shown for SwitchHead in Fig. 6a and for Transformer in Fig. 6b in the\nappendix. Other typical vertical line-lined attention patterns are shown in Fig. 6c and 6d.\n8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(a) Transformer, Layer 3\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(b) SwitchHead Layer 3\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(c) SwitchHead, Layer 3, Head 1\nFigure 2: An attention map of the (a) standard Transformer and (b) SwitchHead. The maximum\nof all heads in the given layer are shown. (c) A head of SwitchHead. On the left side of the\nattention plot, the selection weights of the output projection expert are shown. Similarly, at the\nbottom, the selection weights of the value experts are visible. In the selection maps, dark blue\nalways corresponds to 1, while white is 0. The scale shown on the right is only for the attention.\n6\nRELATED WORK\nThe method most closely related to ours is MoA (Zhang et al., 2022), which introduces a MoE style\nattention. It defines each attention head as an expert but shares the key and value projections between\nthem. Unlike in our case, each of the selected experts requires a separate attention matrix, which\nsignificantly increases its memory usage. Due to the use of a competitive softmax-based activation\nfunction in the selection network, it requires complex regularization to prevent expert collapse. In\nthe original formulation, the number of active heads is high. We also confirmed in our experiments\nthat MoA needs many attention heads to match the performance of the dense baseline (see Sec. 3.2),\nand it is only possible to do so with a significantly higher resource budget than our method.\nNguyen et al. (2022) analyze the attention matrices, and they conclude that they are usually low\nrank. Motivated by this, the authors construct a few (e.g., 2) \"global attention matrices\", and they\ncompute each local matrix for specific heads by a weighted average of those. However, they average\nthe logits, not the final matrix, so each individual head-specific matrix has to be computed. This\nmeans that in the best case, they can only save half of the computation associated with the attention\nmatrix because the readout (Eq. 3) is still needed. For the same reason, memory savings are also\nlow. The authors also use sampling of the attention matrices.\nPeng et al. (2020) proposes to reweight the contribution of each head by a gating function. However,\nthey only reduce the number of total attention heads by one, presumably to compensate for the\nparameters used by the selection logic. Their goal was not to reduce resource usage but to have\nbetter predictive performance, which they achieve. They use a softmax-based competitive selection\nmechanism. To avoid collapse, the gating function is trained only in some steps.\nCsord\u00e1s et al. (2023) introduce the non-competitive \u03c3-MoE method that we also use for our attention\nmechanism. However, the authors focus on accelerating the MLPs and not the attention. More\nbroadly, Shazeer et al. (2017) introduces sparsely-gated mixture of experts in LSTM (Hochreiter &\nSchmidhuber, 1997) networks. Fedus et al. (2021) introduces Mixture of Experts in Transformers.\nLepikhin et al. (2021) trains a MoE-based LLM, and Clark et al. (2022) analyzes the scaling laws of\nMoE models. Lewis et al. (2021) introduces an alternative method for preventing collapse.\nDao et al. (2022) provides a hardware-aware CUDA implementation of the entire attention layer,\nwhich avoids storing the attention matrix. By saving memory bandwidth in this way, they achieve\na significant wall clock time speedup, despite that the attention matrix should be recomputed in the\nbackward pass. This is orthogonal to our method and they can be combined for further acceleration.\n9\n7\nCONCLUSION\nOn a wide range of language modeling datasets with different model sizes, our novel Mixture-of-\nExperts-based attention method called SwitchHead achieves performance on par with parameter-\nmatched dense counterparts, but with only a fraction of the computational cost and memory usage.\nSwitchHead drastically reduces the number of attention matrices that have to be computed, by using\nMoE for the value and output projections. Our method is stable and does not need additional regular-\nization to prevent degenerate solutions (a well-known practical issue in many existing MoE models).\nOur method can also be successfully combined with MoE MLP layers, to obtain \"SwitchAll\" where\nevery layer of the Transformer is MoE-based, achieving a huge reduction in resource requirements.\nACKNOWLEDGEMENTS\nThis research was partially funded by ERC Advanced grant no: 742870, project AlgoRNN, and by\nSwiss National Science Foundation grant no: 200021_192356, project NEUSYM. We are thankful\nfor hardware donations from NVIDIA and IBM. The resources used for this work were partially\nprovided by Swiss National Supercomputing Centre (CSCS) projects d123 and s1205.\nREFERENCES\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Int. Conf. on Learning Representations (ICLR), San Diego, CA,\nUSA, May 2015.\nTom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), Virtual only, December 2020.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco T\u00falio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith GPT-4. Preprint arXiv:2303.12712, 2023.\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,\nPayal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation col-\nlapse of sparse mixture of experts. In Proc. Advances in Neural Information Processing Systems\n(NeurIPS), New Orleans, Louisiana, USA, December 2022.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea\nGane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,\nDavid Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with per-\nformers. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021.\nAidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoff-\nmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den\nDriessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer,\nChris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals,\nJack W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for\nrouted language models. Preprint arXiv:2202.01169, 2022.\nR\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. The neural data router: Adaptive control flow\nin transformers improves systematic generalization. In Int. Conf. on Learning Representations\n(ICLR), Virtual only, April 2022.\nR\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. Approximating two-layer feedforward net-\nworks for efficient transformers. In Findings of the Association for Computational Linguistics:\nEMNLP 2023, November 2023.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proc. Association\nfor Computational Linguistics (ACL), pp. 2978\u20132988, Florence, Italy, 2019.\n10\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Proc. Advances in Neural Information\nProcessing Systems (NeurIPS), New Orleans, Louisiana, USA, December 2022.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Preprint arXiv:2101.03961, 2021.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Journal of Machine Learning Research (JMLR), 23(1):\n5232\u20135270, 2022.\nGeorgi Gerganov. llama.cpp. https://github.com/ggerganov/llama.cpp, 2023.\nSepp Hochreiter and J\u00fcrgen Schmidhuber.\nLong short-term memory.\nNeural computation, pp.\n1735\u20131780, 1997.\nMarcus Hutter. The human knowledge compression prize. http://prize.hutter1.net,\n2006.\nAlekse\u02d8\u0131 Grigorievitch Ivakhnenko and Valentin Grigor\u00e9vich Lapa. Cybernetic Predicting Devices.\nCCM Information Corporation, 1965.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\nof local experts. Neural Compututaion, 3(1):79\u201387, 1991.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are\nRNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on Machine\nLearning (ICML), volume 119, pp. 5156\u20135165, Virtual Only, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio\nand Yann LeCun (eds.), Int. Conf. on Learning Representations (ICLR), San Diego, CA, USA,\nMay 2015.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proc. Conf. on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 66\u201371, Brussels, Belgium, October 2018.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional\ncomputation and automatic sharding. In Int. Conf. on Learning Representations (ICLR), Virtual\nonly, May 2021.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers:\nSimplifying training of large, sparse models. In Marina Meila and Tong Zhang (eds.), Proc. Int.\nConf. on Machine Learning (ICML), volume 139, pp. 6265\u20136274, Virtual only, July 2021.\nNikita Nangia and Samuel R. Bowman.\nListOps: A diagnostic dataset for latent tree learning.\nIn Proc. North American Chapter of the Association for Computational Linguistics on Human\nLanguage Technologies (NAACL-HLT), pp. 92\u201399, New Orleans, USA, June 2018.\nTan Nguyen, Tam Nguyen, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham,\nDuy Khuong Nguyen, Nhat Ho, and Stanley J. Osher. Improving transformer with an admix-\nture of attention heads. In Proc. Advances in Neural Information Processing Systems (NeurIPS),\nNew Orleans, LA, USA, November 2022.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\nBen Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,\nZac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\nLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,\nand Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.\nhttps://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\n11\nOpenAI. GPT-4 technical report. Preprint arXiv:2303.08774, 2023.\nHao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h\nheads. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proc. Associa-\ntion for Computational Linguistics (ACL), pp. 6566\u20136577, Virtual only, July 2020.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\nRandom feature attention. In Int. Conf. on Learning Representations (ICLR), Virtual only, 2021.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research (JMLR), 21:140:1\u2013140:67, 2020.\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight\nprogrammers. In Proc. Int. Conf. on Machine Learning (ICML), volume 139, pp. 9355\u20139366,\nVirtual only, 2021.\nJ\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets.\nTechnical Report FKI-147-91, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen, March\n1991.\nJ\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets.\nNeural Computation, 4(1):131\u2013139, 1992.\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In Proc. IEEE Int. Conf.\non Acoustics, Speech and Signal Processing (ICASSP), pp. 5149\u20135152, Kyoto, Japan, March\n2012.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proc. Association for Computational Linguistics (ACL), pp. 1715\u20131725, Berlin,\nGermany, August 2016.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In\nInt. Conf. on Learning Representations (ICLR), Toulon, France, April 2017.\nLuca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report,\nAllen Institute for AI, 2023. https://github.com/allenai/pes2o.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced transformer\nwith rotary position embedding. Preprint arXiv:2104.09864, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models. Preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural\nInformation Processing Systems (NIPS), pp. 5998\u20136008, Long Beach, CA, USA, December 2017.\nXiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture\nof attention heads: Selecting attention heads per token. In Proc. Conf. on Empirical Methods\nin Natural Language Processing (EMNLP), pp. 4150\u20134162, Abu Dhabi, United Arab Emirates,\nDecember 2022.\n12\nA\nAPPENDIX\nA.1\nRESOURCE REQUIREMENTS OF MOA\nThe resource requirements of MoA (Peng et al., 2020) are very similar to those of Transformer XL\n(see Sec. 2.3 for more details), except that it uses a single shared K and V for each head.\nN MoA\nMAC = (2H + 2)Tdheaddmodel + 2HCT 2dhead + 2CTdheaddmodel\n(14)\nN MoA\nmem = (2H + 2)Tdhead + 2HCT 2 + 2CTdhead\n(15)\nA.2\nHYPERPARAMETERS\nWe train all our models with Adam optimizer (Kingma & Ba, 2015), with a batch size of 64, a\nlearning rate of 0.00025, and gradient clipping with a maximum norm of \u03ba. Large models (> 200K\nparameters) use a learning rate warm-up of 4k steps. All models, except the SwitchAll model,\nuse a dropout on the MLP layers, 0.1 for the small models and 0.2 for the large ones. Detailed\nhyperparameters are shown in the Tab. 6. \u03c3-MoE related hyperparameters for the SwitchAll models\nare identical to those of Csord\u00e1s et al. (2023). For Transformer XL models, we always use a single\nadditional chunk of context, both in training and validation time. dhead and dff are derived in a\nsystematic way, see Sec. 3 for more details.\nA.2.1\nA NOTE ON CHOOSING nHEADS\nOur preliminary experiments showed that a single head is usually not enough to match the perfor-\nmance of the baseline network, but two heads usually work well. Because of this, we always start by\ntraining a model with nheads = 2 and increase it to nheads = 4 if it does not match the performance\nof the baseline. We have not experimented with any other nheads.\nA.3\nA NOTE ON THE PARAMETER COUNT OF THE SWITCHALL\nIt can be seen in Tab. 4 that the parameter count of the SwitchAll models is often less than that of\ntheir dense counterparts. The reason is that we normally compensate for the final difference in the\nnumber of parameters by increasing dff (see Sec. 3 for details of the parameter matching). However,\nthat can only be done in a very coarse-grained way with \u03c3-MoE: the size of all experts must be\nincreased at once, and the CUDA kernel supports only sizes of multiple of 4. Therefore, increasing\nthe size of the experts would add too many parameters and the model would outgrow the baseline.\nFor this reason, we simply keep the hyperparameters for Csord\u00e1s et al. (2023) and combine them\nwith our SwitchHead configuration from Tab. 3.\nA.4\nWALL-CLOCK TIME ESTIMATION\nIn all of our tables, we report the number of multiply-accumulate (MAC) operations following Zhang\net al. (2022). The reason for this is that the actual wall-clock time is highly implementation and\nhardware-dependent. Nevertheless, we measured the runtime and total memory usage of our entire\ntraining pipeline (including the feedforward layer) to demonstrate that our current (suboptimal) im-\nplementation is already capable of providing wall-clock-time acceleration. We show the results in\nTab. 7. The measurements are taken on identical hardware with the same implementation (including\nfor the attention core), the only difference being the MoE-based projections for the attention. It\ncan be seen that for both scales, our method trains around 1.5 times faster, while using 61%-67%\nas much memory as the baseline. Note that these measurements also include the MLP layers, the\noptimizer, and the gradient synchronization in the case of multi-GPU training.\nA.5\nVISALIZING ALL ATTENTION HEADS\nAs discussed in Sec. 5, we analyze the attention maps of SwitchHead and compare them with the\ndense models. We show all the attention maps of the models trained on ListOps in Fig. 3 and Fig.\n3. We show individual heads of SwitchHead, including the expert selection scores in Fig. 5. Some\nselected attention maps of our 47M parameter models on Wikitext 103 are shown in Fig. 6.\n13\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) Layer 1\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(b) Layer 2\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(c) Layer 3\nB\n[MED\n[MED\n[MIN0895]8809]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808]\n[MIN26]5]154]5]]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n(d) Layer 4\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.05\n0.10\n0.15\n0.20\n0.25\n(e) Layer 5\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n(f) Layer 6\nFigure 3: The maximum of all attention maps for a SwitchHead model on ListOps.\n14\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(a) Layer 1\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(b) Layer 2\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(c) Layer 3\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.2\n0.4\n0.6\n0.8\n(d) Layer 4\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(e) Layer 5\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n(f) Layer 6\nFigure 4: The maximum of all attention maps for a standard Transformer model on ListOps.\n15\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(a) Layer 1, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(b) Layer 1, head 2\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(c) Layer 2, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.2\n0.4\n0.6\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(d) Layer 2, head 2\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.2\n0.4\n0.6\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(e) Layer 3, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.0\n0.2\n0.4\n0.6\n0.8\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(f) Layer 3, head 2\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(g) Layer 4, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(h) Layer 4, head 2\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.05\n0.10\n0.15\n0.20\n0.25\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(i) Layer 5, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(j) Layer 5, head 2\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(k) Layer 6, head 1\nB\n[MED\n[MED\n[MIN\n0\n8\n9\n5\n]\n8\n8\n0\n9\n]\n6\n9\n6\n[MAX\n7\n6\n[MAX\n4\n[MAX\n5\n[MAX\n3\n8\n0\n8\n]\n[MIN\n2\n6\n]\n5\n]\n1\n5\n4\n]\n5\n]\n]\nE\ndest\no\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nB\n[MED\n[MED\n[MIN0895 ]8809 ]696\n[MAX76\n[MAX4\n[MAX5\n[MAX3808 ]\n[MIN26 ]5 ]154 ]5 ] ]E\nsrc\nv\n(l) Layer 6, head 2\nFigure 5: Details for individual heads of the SwitchHead model on ListOps. On the left side of\neach attention plot, the selection of the output projection expert is shown. Similarly, at the bottom,\nthe selection of the value projection selection is visible. In the selection maps, dark blue always\ncorresponds to 1, while white is 0. The adaptive scale shown to the right of the attention map is for\nthe map only.\n16\nlob\nster\nred \"on\ncook\ning .M\nating\noccursin\nthe\nsummer ,\nproducing\neggs\nwhich\nare\ncarriedby\nthe\nfemalesforuptoa\nyear\nbeforeh\natch\ning\nintopl\nank\ntoniclarvae .H\nomarusg\nam\nmarusisa\nhighlyeste\nemed\nfood ,\nandis\nwidely\ncaught\nusinglob\nsterpots ,\nmostly\naround\nthe\nBritishIsles .==\nDesc\nription==H\nomarusg\nam\nmarusisa\nlargecr\nust\nacean ,\nwitha\nbody\nlengthupto\n60\ncent\nimetres (\n24in )\nandwe\nigh\ningupto\n5\n6kilog\nrams\nsrc\nster\nred\n\"\non\ncook\ning\n.\nM\nating\noccurs\nin\nthe\nsummer\n,\nproducing\neggs\nwhich\nare\ncarried\nby\nthe\nfemales\nfor\nup\nto\na\nyear\nbefore\nh\natch\ning\ninto\npl\nank\nton\nic\nlar\nv\nae\n.\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nhighly\nest\ne\nemed\nfood\n,\nand\nis\nwidely\ncaught\nusing\nlob\nster\np\nots\n,\nmostly\naround\nthe\nBritish\nIs\nles\n.\n=\n=\nDesc\nription\n=\n=\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nlarge\ncr\nust\nace\nan\n,\nwith\na\nbody\nlength\nup\nto\n6\n0\ncent\nimet\nres\n(\n2\n4\nin\n)\nand\nwe\nigh\ning\nup\nto\n5\n6\nkil\nog\nrams\n(\ndest\n0.0\n0.2\n0.4\n0.6\n0.8\n(a) SwitchHead Layer 12. Induction head.\nlob\nster\nred \"on\ncook\ning .M\nating\noccursin\nthe\nsummer ,\nproducing\neggs\nwhich\nare\ncarriedby\nthe\nfemalesforuptoa\nyear\nbeforeh\natch\ning\nintopl\nank\ntoniclarvae .H\nomarusg\nam\nmarusisa\nhighlyeste\nemed\nfood ,\nandis\nwidely\ncaught\nusinglob\nsterpots ,\nmostly\naround\nthe\nBritishIsles .==\nDesc\nription==H\nomarusg\nam\nmarusisa\nlargecr\nust\nacean ,\nwitha\nbody\nlengthupto\n60\ncent\nimetres (\n24in )\nandwe\nigh\ningupto\n5\n6kilog\nrams\nsrc\nster\nred\n\"\non\ncook\ning\n.\nM\nating\noccurs\nin\nthe\nsummer\n,\nproducing\neggs\nwhich\nare\ncarried\nby\nthe\nfemales\nfor\nup\nto\na\nyear\nbefore\nh\natch\ning\ninto\npl\nank\nton\nic\nlar\nv\nae\n.\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nhighly\nest\ne\nemed\nfood\n,\nand\nis\nwidely\ncaught\nusing\nlob\nster\np\nots\n,\nmostly\naround\nthe\nBritish\nIs\nles\n.\n=\n=\nDesc\nription\n=\n=\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nlarge\ncr\nust\nace\nan\n,\nwith\na\nbody\nlength\nup\nto\n6\n0\ncent\nimet\nres\n(\n2\n4\nin\n)\nand\nwe\nigh\ning\nup\nto\n5\n6\nkil\nog\nrams\n(\ndest\n0.0\n0.2\n0.4\n0.6\n0.8\n(b) Transformer XL Layer 10. Induction head.\nlob\nster\nred \"on\ncook\ning .M\nating\noccursin\nthe\nsummer ,\nproducing\neggs\nwhich\nare\ncarriedby\nthe\nfemalesforuptoa\nyear\nbeforeh\natch\ning\nintopl\nank\ntoniclarvae .H\nomarusg\nam\nmarusisa\nhighlyeste\nemed\nfood ,\nandis\nwidely\ncaught\nusinglob\nsterpots ,\nmostly\naround\nthe\nBritishIsles .==\nDesc\nription==H\nomarusg\nam\nmarusisa\nlargecr\nust\nacean ,\nwitha\nbody\nlengthupto\n60\ncent\nimetres (\n24in )\nandwe\nigh\ningupto\n5\n6kilog\nrams\nsrc\nster\nred\n\"\non\ncook\ning\n.\nM\nating\noccurs\nin\nthe\nsummer\n,\nproducing\neggs\nwhich\nare\ncarried\nby\nthe\nfemales\nfor\nup\nto\na\nyear\nbefore\nh\natch\ning\ninto\npl\nank\nton\nic\nlar\nv\nae\n.\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nhighly\nest\ne\nemed\nfood\n,\nand\nis\nwidely\ncaught\nusing\nlob\nster\np\nots\n,\nmostly\naround\nthe\nBritish\nIs\nles\n.\n=\n=\nDesc\nription\n=\n=\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nlarge\ncr\nust\nace\nan\n,\nwith\na\nbody\nlength\nup\nto\n6\n0\ncent\nimet\nres\n(\n2\n4\nin\n)\nand\nwe\nigh\ning\nup\nto\n5\n6\nkil\nog\nrams\n(\ndest\n0.0\n0.2\n0.4\n0.6\n0.8\n(c) SwitchHead Layer 9. Stripe pattern.\nlob\nster\nred \"on\ncook\ning .M\nating\noccursin\nthe\nsummer ,\nproducing\neggs\nwhich\nare\ncarriedby\nthe\nfemalesforuptoa\nyear\nbeforeh\natch\ning\nintopl\nank\ntoniclarvae .H\nomarusg\nam\nmarusisa\nhighlyeste\nemed\nfood ,\nandis\nwidely\ncaught\nusinglob\nsterpots ,\nmostly\naround\nthe\nBritishIsles .==\nDesc\nription==H\nomarusg\nam\nmarusisa\nlargecr\nust\nacean ,\nwitha\nbody\nlengthupto\n60\ncent\nimetres (\n24in )\nandwe\nigh\ningupto\n5\n6kilog\nrams\nsrc\nster\nred\n\"\non\ncook\ning\n.\nM\nating\noccurs\nin\nthe\nsummer\n,\nproducing\neggs\nwhich\nare\ncarried\nby\nthe\nfemales\nfor\nup\nto\na\nyear\nbefore\nh\natch\ning\ninto\npl\nank\nton\nic\nlar\nv\nae\n.\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nhighly\nest\ne\nemed\nfood\n,\nand\nis\nwidely\ncaught\nusing\nlob\nster\np\nots\n,\nmostly\naround\nthe\nBritish\nIs\nles\n.\n=\n=\nDesc\nription\n=\n=\nH\nom\nar\nus\ng\nam\nmar\nus\nis\na\nlarge\ncr\nust\nace\nan\n,\nwith\na\nbody\nlength\nup\nto\n6\n0\ncent\nimet\nres\n(\n2\n4\nin\n)\nand\nwe\nigh\ning\nup\nto\n5\n6\nkil\nog\nrams\n(\ndest\n0.0\n0.2\n0.4\n0.6\n0.8\n(d) Transformer XL Layer 8. Stripe pattern.\nFigure 6: Induction head copying the rare name \"Homarus\" in (a) SwitchHead and (b) Transformer\nXL baseline. The attention matrix is square because it is the first chunk of the sequence, without any\nextra context. Typical vertical line pattern in (c) SwitchHead and (b) Transformer XL baseline.\n17\nTable 6: Hyperparameters used for our models.\nModel\nDataset\nnheads\n#params\ndhead\ndff\nE\nK\nT\nnlayers\n\u03ba\nSwitchHead\nC4\n2\n47M\n76\n2080\n5\n3\n256\n16\n0.1\nTransformer XL\nC4\n10\n47M\n41\n2053\n-\n-\n256\n16\n0.1\nTransformer XL\nC4\n2\n47M\n205\n2053\n-\n-\n256\n16\n0.1\nSwitchHead\nC4\n4\n262M\n112\n4188\n4\n2\n512\n18\n0.25\nTransformer XL\nC4\n16\n262M\n64\n4110\n-\n-\n512\n18\n0.25\nTransformer XL\nC4\n4\n262M\n256\n4110\n-\n-\n512\n18\n0.25\nSwitchHead\nWikitext 103\n2\n47M\n76\n2080\n5\n2\n256\n16\n0.1\nTransformer XL\nWikitext 103\n10\n47M\n41\n2053\n-\n-\n256\n16\n0.1\nTransformer XL\nWikitext 103\n2\n47M\n205\n2053\n-\n-\n256\n16\n0.1\nSwitchHead\nWikitext 103\n2\n262M\n132\n4147\n8\n4\n512\n18\n0.25\nTransformer XL\nWikitext 103\n16\n262M\n64\n4110\n-\n-\n512\n18\n0.25\nTransformer XL\nWikitext 103\n2\n262M\n512\n4110\n-\n-\n512\n18\n0.25\nSwitchHead\npeS2o\n2\n47M\n76\n2080\n5\n3\n256\n16\n0.1\nTransformer XL\npeS2o\n10\n47M\n41\n2053\n-\n-\n256\n16\n0.1\nTransformer XL\npeS2o\n2\n47M\n205\n2053\n-\n-\n256\n16\n0.1\nSwitchHead\npeS2o\n4\n262M\n112\n4188\n4\n2\n512\n18\n0.25\nTransformer XL\npeS2o\n16\n262M\n64\n4110\n-\n-\n512\n18\n0.25\nTransformer XL\npeS2o\n4\n262M\n256\n4110\n-\n-\n512\n18\n0.25\nSwitchHead\nEnwik8\n2\n41M\n112\n2088\n4\n2\n512\n12\n0.25\nTransformer XL\nEnwik8\n8\n41M\n64\n2053\n-\n-\n512\n12\n0.25\nTransformer XL\nEnwik8\n2\n41M\n256\n2053\n-\n-\n512\n12\n0.25\nSwitchHead (RoPE)\nWikitext 103\n2\n45M\n64\n2092\n5\n3\n512\n16\n0.1\nTransformer (RoPE)\nWikitext 103\n10\n45M\n41\n2053\n-\n-\n512\n16\n0.1\nSwitchHead (RoPE)\nWikitext 103\n4\n243M\n100\n4136\n4\n2\n1024\n18\n0.25\nTransformer (RoPE)\nWikitext 103\n16\n244M\n64\n4110\n-\n-\n1024\n18\n0.25\nSwitchAll\nWikitext 103\n2\n47M\n76\n1648\n5\n2\n256\n16\n0.25\nSwitchAll\nWikitext 103\n4\n259M\n112\n4096\n4\n2\n512\n18\n0.25\nSwitchAll\nC4\n2\n47M\n76\n1648\n5\n3\n256\n16\n0.25\nSwitchAll\nC4\n4\n259M\n112\n4096\n4\n2\n512\n18\n0.25\nSwitchAll\npeS2o\n2\n47M\n76\n1648\n5\n3\n256\n16\n0.25\nSwitchAll\npeS2o\n4\n259M\n112\n4096\n4\n2\n512\n18\n0.25\nTable 7: Real-world resource usage of our method. The numbers shown below are for training time\nfor the whole pipeline, including the feedforward layers. It can be seen that SwitchHead in the\ncurrent implementation reduces both the runtime and the memory usage by a factor of 1.4-1.5.\nModel\nSize\nms/iteration\nRel. iter. time\nRAM/GPU\nRel. Mem.\n#GPUs\nGPU type\nTrafo. XL\n47M\n473ms/iter\n1.0\n20.5G\n1.0\n1\nRTX 3090\nSwitchHead\n342ms/iter\n0.72\n13.5G\n0.65\nTrafo. XL\n262M\n670ms/iter\n1.0\n20.5G\n1.0\n8\nV100\nSwitchHead\n442ms/iter\n0.65\n12.5G\n0.61\n18\n"
  },
  {
    "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet",
    "link": "https://arxiv.org/pdf/2312.08361.pdf",
    "upvote": "23",
    "text": "Distributed Inference and Fine-tuning of\nLarge Language Models Over The Internet\nAlexander Borzunov\u2217\nHSE Univesity, Yandex\nMax Ryabinin\nHSE Univesity, Yandex\nArtem Chumachenko\nNeiro.ai\nDmitry Baranchuk\nYandex\nTim Dettmers\nUniversity of Washington\nYounes Belkada\nHugging Face\nPavel Samygin\nYandex School of Data Analysis\nColin Raffel\nHugging Face\nAbstract\nLarge language models (LLMs) are useful in many NLP tasks and become more\ncapable with size, with the best open-source models having over 50 billion param-\neters. However, using these 50B+ models requires high-end hardware, making\nthem inaccessible to most researchers. In this work, we investigate methods for\ncost-efficient inference and fine-tuning of LLMs, comparing local and distributed\nstrategies. We observe that a large enough model (50B+) can run efficiently even\non geodistributed devices in a consumer-grade network. This could allow running\nLLM efficiently by pooling together idle compute resources of multiple research\ngroups and volunteers. We address two open problems: (1) how to perform infer-\nence and fine-tuning reliably if any device can disconnect abruptly and (2) how\nto partition LLMs between devices with uneven hardware, joining and leaving at\nwill. In order to do that, we develop special fault-tolerant inference algorithms and\nload-balancing protocols that automatically assign devices to maximize the total\nsystem throughput. We showcase these algorithms in PETALS1 \u2014 a decentralized\nsystem that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10\u00d7\nfaster than offloading for interactive generation. We evaluate the performance of\nour system in simulated conditions and a real-world setup spanning two continents.\n1\nIntroduction\nIn recent years, the NLP community has found that pretrained language models greatly accelerated\nprogress on many research problems through either fine-tuning (Radford et al., 2018) or simple\nprompting (Brown et al., 2020). Their quality tends to improve as we increase model scale (Radford\net al., 2019; Kaplan et al., 2020). Following this trend, modern language models often have hundreds\nof billions of parameters (Brown et al., 2020; Rae et al., 2021; Zeng et al., 2021; Kim et al., 2021).\nMost recently, several research groups open-sourced their pretrained LLMs with over 50B parame-\nters (Zhang et al., 2022; BigScience, 2022a; Touvron et al., 2023a,b). However, they are still difficult\nto use due to the sheer size in terms of parameters. For example, OPT-175B and BLOOM-176B need\nover 350 GB accelerator memory for inference and even more for fine-tuning. As a result, even basic\ninference for these LLMs requires multiple high-end GPUs or multi-node clusters. Recent studies\n\u2217Correspondence to: borzunov.alexander@gmail.com\n1PETALS source code and documentation are available at https://petals.dev\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.08361v1  [cs.LG]  13 Dec 2023\nBLOOM layers, part 1/3\nBLOOM layers, part 2/3\nBLOOM layers, part 3/3\n \n\u0421lients\nC++\nFrench\nHindi\nFigure 1: A high-level overview of our system design. Servers store pretrained LLM layers and\ntemporarily hold attention caches for inferencing. Clients hold embedding layers and learned\nprompts/adapters (if used). Arrows denote temporary chains formed for inference.\npropose algorithms for running large models with more affordable hardware (Pudipeddi et al., 2020;\nRen et al., 2021), e.g. by offloading parameters to RAM. However, as we show in Section 3.1, these\ntechniques are inefficient in many use cases, such as LLM-based chatbots and search engines.\nIn this work, we search for a more cost-effective way of running pretrained LLMs in their main\nuse cases: inference, in-context learning, and fine-tuning. We analyze latency and throughput for\nthese use cases and determine which factors become dominant for very large models. Notably, for\nmodels with over 50B parameters, communicating activations over a slow network can be faster than\nswapping layers from local RAM or SSD. Based on these observations, it should be possible to run\nLLMs cost-effectively by pooling together commodity hardware over the Internet.\nHowever, existing LM algorithms are not designed to run inference with unreliable devices or high-\nlatency networks. To bridge this gap, we formulate a novel algorithm for fault-tolerant distributed\nautoregressive inference of very large models. Using dual attention caches, this algorithm can quickly\nrecover from a failed server and reassign the load to one or more replacement servers. Finally, to\nmake sure that there are enough servers for every part of the model, we develop a decentralzied\nload-balancing algorithm that assigns transformer blocks to every server to maximize the total system\nthroughput. The fully decentralized nature of these protocols allows participants to add or remove\ntheir devices at any point, making optimal use of GPU idle time.\nWe summarize the main contributions of this work as such:\n\u2022 We analyze the problem of cost-efficient LLM inference and propose a novel algorithm that can\ninference large (50B+) language models on distributed unreliable devices. To the best of our\nknowledge, this is the first algorithm that can inference LLMs with 50B+ parameters in this setup.\n\u2022 Using this algorithm, we develop PETALS \u2014 a decentralized system for inferencing and fine-tuning\nLLMs over the Internet. The system allows users to run inference and fine-tuning over a swarm of\nunreliable devices with the same correctness guarantees as when running locally. The system runs\npersistently with the help of volunteers.\n\u2022 We benchmark the performance of the proposed algorithms on Llama 2 (70B) (Touvron et al.,\n2023b) and BLOOM (176B) (BigScience, 2022a). We run experiments in controlled conditions,\nwith simulated network latency and server failures, and in the actual geo-distributed system spanning\ntwo continents. With realistic network speeds, our distributed algorithms perform autoregressive\ngeneration \u226510\u00d7 faster than local offloading.\n2\nBackground: efficient training and inference\nThere is a wide variety of methods optimizing training and inference for most deep learning workloads.\nHere, we focus on two areas relevant for our analysis: model parallelism and parameter offloading.\n2.1\nModel parallelism\nModel parallelism is a family of distributed training algorithms that assigns each device to hold a\nsubset of model parameters, run a subset of computations and communicate output activations. Tensor\nparallelism assigns each device to compute a subset of each model layer (e.g., a subset of neurons),\nthen communicate results between each other and proceed to the next layer (Krizhevsky et al., 2012;\n2\nBen-Nun & Hoefler, 2019; Tang et al., 2020). Each device performs a symmetric computation,\napplied to a different slice of model weights, which makes tensor parallelism compatible with MPI-\nbased communication. In turn, the main performance overhead of this strategy comes from all-to-all\ncommunication (and synchronization) after each layer (Krizhevsky, 2014).\nPipeline parallelism reduces the communication overhead by assigning each device with one or\nseveral full layers (Huang et al., 2019; Narayanan et al., 2019; Yang et al., 2019). During the forward\npass, each stage applies its subset of layers to the inputs supplied by the previous stage, then sends\nthe outputs of the last layer to the next stage. For the backward pass, this process is reversed, with\neach pipeline stage passing the gradients to the same device that previously supplied it with input\nactivations. To better utilize the available devices, the pipeline must process multiple microbatches\nper step, allowing each stage to run in parallel on a different batch of inputs. Even with optimal\nexecution, some of the pipeline stages will remain idle some of the time (Huang et al., 2019).\nBoth of these strategies are actively used for training LLMs. Real-world distributed training systems\nusually combine multiple forms of parallelism depending on hardware and network type (Narayanan\net al., 2021; Rajbhandari et al., 2020; Jia et al., 2019). Tensor parallelism is typically used within\na single multi-GPU server or closely interconnected TPU cores (Narayanan et al., 2021; Shazeer\net al., 2018). In turn, pipeline parallelism is used to connect multiple servers (Narayanan et al., 2021).\nRecent works demonstrate that model parallelism can be used for cost-efficient pre-training of LLMs\nby pooling together idle GPU devices (Athlur et al., 2022; Wang et al., 2022; Kuszmaul, 2022; Yuan\net al., 2022; Ryabinin et al., 2023).\n2.2\nOffloading\nParameter offloading relegates model parameters from accelerator memory to a slower but cheaper\nstorage: typically RAM or SSD (Pudipeddi et al., 2020; Ren et al., 2021; Rajbhandari et al., 2021).\nWhen using the model, parameters are loaded to the accelerator just-in-time for computation, one or\nfew layers at a time. In principle, this method allows running large models with a single low-end\naccelerator as long as there is enough RAM (or SSD) to store the model.\nThe main drawback of this strategy is having to load and unload through all model parameters for\neach forward and backward pass, which can be time-consuming. This extra time can be amortized in\nworkloads where model can do a lot of useful computations for each time a parameter is loaded. In\npractice, using offloading to run a single token through the OPT-175B on one GPU in the best-case\nscenario of hardware and bandwidth2 would require 11 seconds per forward pass, or twice that for\ntraining. As we show in Section 4, real-world performance is significantly slower.\nPudipeddi et al. (2020) circumvents this by training with very large batches, and hence, increasing the\ncomputation. In turn, Ren et al. (2021); Rajbhandari et al. (2021) reduce the overhead by overlapping\ncommunication and computation, that is, doing useful computation for the current layer while waiting\nfor the transfer of the next layer to finish. Some of these systems Ren et al. (2021) also partition\noffloaded parameters between devices. However, unlike model-parallel training, distributed offloading\nstill requires each device to compute the full model.\n3\nMethod\nUsing pretrained large language models for NLP tasks consists of two main workloads: inference\nand fine-tuning. The inference workload typically consists of encoding an input text, then generating\ntokens autoregressively. In turn, fine-tuning requires updating either all of the model\u2019s parameters or\n(more commonly for large models) a small set of trainable weights (e.g., adapters or soft prompts) by\nbackpropagation. These two workloads also cover more advanced use cases:\n\u2022 Manually engineering prompts for a given task, then deploying the model with these prompts.\n\u2022 Fine-tuning with adapters (Hu et al., 2021; Houlsby et al., 2019; Liu et al., 2022b) or \u201csoft\u201d\nprompts (Liu et al., 2021b; Lester et al., 2021; Liu et al., 2021a) and inferencing fine-tuned models.\n\u2022 Distillation into a smaller task-specific model for faster inference (Schick & Sch\u00fctze, 2021).\n2Specifically, 16-bit parameters, PCIe gen. 4 at 31.5 GB/s (16 lanes), infinite compute and memory bandwidth.\n3\nCounter-intuitively, we found that inference is more challenging than fine-tuning for cost-efficient\nsetups. To that end, we dedicate most of this section to inference-specific problems. As for fine-tuning,\nwe describe a way to support arbitrary parameter-efficient fine-tuning in Section 3.4.\n3.1\nPerformance bottlenecks of LLM inference\nUnlike training, autoregressive LLM inference cannot be done with a single pass through the model.\nInstead, the model needs to process one token at a time, pass it through the entire model, then generate\nthe next token and repeat the process. In case of model parallelism, training an n-layer3 model on a\nsequence of t tokens needs O(n) communication rounds, while generating the same sequence needs\nO(n \u00b7 t) rounds, making it more susceptible to network latency. Similarly with parameter offloading,\ngenerating a sequence of t tokens needs loading every layer t times, which also takes O(n \u00b7 t) time.\nThe other problem of autoregressive generation is dealing with attention for past tokens (Vaswani\net al., 2017). During an inference step t, each layer needs to attend to t \u2212 1 previous attention\nkeys and values. Existing inference algorithms store past entries in accelerator memory. Caching\nhalf-precision activations of a 2048-token sequence for large models like GPT-3 (Brown et al., 2020)\nor OPT-175B (Zhang et al., 2022) (with 96 layers of 12288 units each) takes up 9.6 GB GPU memory\nfor each sequence. Offloading these cached values faces the same problems as offloading in general.\nAn alternative solution is to recompute all previous tokens on every inference step, storing only one\nset of keys & values at a time. Naturally, this approach needs increasingly more computation with\nsequence length t, for a total of O(t3) time for transformer-based models4.Surprisingly, this approach\nis often more efficient than offloaded caching, especially for shorter sequences due to the overhead\nfrom loading and storing cache from RAM or SSD.\nParameter offloading can still be efficient when generating large amounts of short sequences in bulk.\nEach individual sequence still takes a long time to generate, but the system maintains high throughput\nby running many samples in parallel. Unfortunately, this scenario does not cover many important\nLLM use cases. For instance, it is incompatible with in-context learning or prompt engineering,\nwhere the model needs to process long sequences of training examples (Brown et al., 2020). More\nimportantly, it does not support \u201cinteractive\u201d applications where LLM needs to quickly respond to a\nuser input. This rules out many LLM applications such as conversation systems or input completion\n(e.g. ChatGPT or Smart Compose).\nHence, we explore a new solution based on pipeline-parallelism. A related line of work (Aminabadi\net al., 2022) investigates model parallelism to inference LLMs in GPU clusters. However, their\napproach does not apply to our more affordable setups: cheap \u201cpreemptible\u201d instances or connecting\nexisting resources over the Internet. To operate in these conditions, an inference algorithm needs to\ndeal with node preemption, network errors, and high latency.\n3.2\nDistributed generation with fault tolerance\nIn this section, we formulate an algorithm for inferencing LLMs in a fleet of unreliable geographically\ndistributed devices connected over the Internet. Each device can act as a server, a client, or both.\nA client is a node operated by the user, which runs inference or fine-tuning jobs through the swarm\nof servers. A client only holds input and output embeddings (< 3% of model weights for BLOOM-\n176B) and delegates running transformer blocks (the most expensive computations) to remote servers.\nA server is a GPU-enabled node holding a set of consecutive transformer blocks and processing\nrequests coming from client nodes.\nFor simplicity, we assume that every block is hosted on several servers and examine this assumption\nin the next section. Following this notation, a fault-tolerant algorithm should allow each client to\ncomplete an inference job with reproducible results even if some remote servers fail during inference.\nAs we discuss in Section 3.1, autoregressive generation requires many sequential communication\nrounds, making it sensitive to network latency. However, if every device stores its past attention\n3Here and below, the term model layer (or block) refers to one transformer block that typically combines\nself-attention, a feed-forward network, normalization layers, and a residual connection (Vaswani et al., 2017).\n4All public LLMs with 100B+ parameters use standard attention that scales as O(n2) for sequence length n.\n4\ncache, every round only transfers activations for a single token, i.e. several kilobytes of data5. We use\nthis model to directly minimize the inference time over possible pipeline configurations. As we show\nlater in Section 4.2, this allows efficient inference over a low-bandwidth Internet connection.\nA more challenging problem is how to recover from node and network failures. If a remote server\nshuts down, any cached attention keys stored on that server will be lost with it. There are two na\u00efve\nsolutions to this problem: restarting inference from scratch or recomputing past embeddings on every\nstep. Restarting might be enough at a small scale. However, running 50B+ models may involve\nmany unreliable devices, making it unlikely to generate long sequence without at least one failure. In\nturn recomputing past attention caches requires communicating past tokens on every communication\nround, resulting in O(n \u00b7 t2) total data transferred, where n is the number of pipeline layers and t is\nthe sequence length. In other words, both these solutions struggle to generate long sequences.\nWe address this problem by maintaining two types of cache: server-side cache holds past attention\nkeys and values for their layers, like in existing inference algorithms, while client-side cache holds\npast inputs sent to a given pipeline stage6. If a server disconnects, a client can find another server\nwith that pipeline stage and use client-side cache to restore the server state.\nThe resulting procedure is described in Algorithm 1. For every pipeline stage, the client maintains a\nheap (priority queue) of servers that hold this stage (and may hold additional stages). The servers\nin queue are ordered by the network latency, measured from past communication. These queues\nare maintained through the lifetime of a client. To begin generation, the client runs a beam-search-\nlike procedure to find a sequence of servers that results in the least total inference time under our\nperformance model. When running inference steps, a client keeps track of intermediate activations\nsent between pipeline stages. If a remote server fails or leaves, the client retrieves the next best server\n(or multiple servers) and requests it to restore the attention state from the client\u2019s cached activations.\nWhen servers fail, the algorithm needs to send O(t) data (in one round) for each failed server and\ncompute only the stages held by the failed servers. This can be seen as an interpolation between naive\nand cached inference, depending on the server failure rate. If none of the servers fail, we recover\nO(n \u00b7 t) communication, similarly to Aminabadi et al. (2022). In turn, if all servers fail after one step,\nthe algorithm effectively performs non-caching generation, which is the best option in that scenario.\nIn the basic formulation, all communication between pipeline stages is routed through the client, i.e.\nthe client receives the outputs of every pipeline stage, caches it and sends it to the subsequent stage.\nIn practice, it is more efficient to let pipeline stages communicate directly: once the server obtains\noutput activations, it sends them to both client and the subsequent stage. This reduces the total step\ntime since both messages are a few kilobytes in size an can be sent in parallel. To verify that both\nclient and the next pipeline stage received the same set of activations, they can verify the checksums\n(i.e. hash values) of the received activations asynchronously, without blocking computation.\nAlgorithm 1 can support greedy inference or any sampling variants (including Holtzman et al. (2020)).\nHowever, it requires one more step to support search-based algorithms such as beam search: cache\nreordering. This allows a client to generate multiple continuations of the same input prefix by cloning\nits attention cache and dropping less likely hypotheses. We describe beam search in Appendix C.\nShortest path routing. In the Algorithm 1, the find_best_chain function (line 4) selects a sequence\nof servers that can run the required layers in the least amount of time. To estimate this time we add up\ntwo factors: computation time, determined by server\u2019s compute throughput (\u201cGPU speed\u201d) and the\nnetwork latency between the client and that server. Servers measure their own compute throughput\nand share this information with the clients. In turn, clients measure the network latency between\nthem and a given server by \u201cpinging\u201d the candidate servers during routing. If a server runs multiple\nconsecutive blocks, we multiply the computation time by the number of blocks.\nTo find the best chain of servers, clients find the shortest path between the first and last block, using a\ngraph where edge weights correspond to server inference time, as described in the previous paragraph.\nTo minimize overhead, we do not run pathfinding from scratch on each call to find_best_chain.\nInstead, clients run lifelong pathfinding in the background and reuse it between inference calls. More\nspecifically, we use the D\u2217 Lite (Koenig & Likhachev, 2005) algorithm because it allows clients to\nquickly adjust paths after a server is banned or leaves the network.\n5For GPT-3 and OPT-175B, one 12288-dimensional token embedding in 16-bit precision takes up 24 KiB.\n6Here, a pipeline stage is a set of consecutive model layers hosted on one server (as in pipeline parallelism).\n5\nAlgorithm 1 Generating sequence, client-side code\nInput: prefix_tokens, embeddings, known_servers\n1: generated_sequence = list()\n2: cache = dictionary()\n3: streams = dictionary()\n4: chain = find_best_chain(known_servers)\n5: for server \u2208 chain do\n6:\nstreams[server] = rpc_inference(server)\n7:\ncache[server] = list()\n8: end for\n9:\n10: inputs = embeddings(prefix_tokens)\n11: while should_continue(generated_sequence) do\n12:\ntail_servers = copy(chain)\n13:\nwhile not empty(tail_servers) do\n14:\nserver = tail_servers.pop_left()\n15:\ntry:\n16:\n\u25b7 Attempt normal inference\n17:\noutputs = streams[server].send(inputs)\n18:\ncache[server].append(inputs)\n19:\ninputs = outputs\n20:\ncatch ServerFailed:\n21:\n\u25b7 Replace the failed server\n22:\nstreams.pop(server).close()\n23:\npast_inputs = cache.pop(server)\n24:\nnew_servers = replace_failed_server(\n25:\nserver, past_inputs, cache,\n26:\nstreams, known_servers)\n27:\nchain.replace(server, new_servers)\n28:\ntail_servers.push_left(new_servers)\n29:\nend while\n30:\n31:\nlogits = compute_logits(outputs, embeddings)\n32:\nnext_token = choose_next(logits) {e.g. greedy}\n33:\ngenerated_sequence.append(next_token)\n34:\ninputs = embeddings(next_token)\n35: end while\n36:\n37: for server \u2208 chain do\n38:\nstreams[server].close()\n39: end for\n40: return generated_sequence\nAlgorithm 2 rpc_inference(server)\nInput: local_layers, stream\n1: cache = dictionary()\n2: for layer \u2208 local_layers do\n3:\ncache[layer] = make_empty()\n4: end for\n5: while not stream.closed() do\n6:\ninputs = stream.receive()\n7:\nfor layer \u2208 local_layers do\n8:\npast_kv = cache[layer]\n9:\ninputs, new_kv = forward(\n10:\nlayer, inputs, past_kv)\n11:\ncache[layer].append(new_kv)\n12:\nend for\n13:\nstream.send(inputs)\n14: end while\nAlgorithm 3 replace_failed_server(...)\nInput: server, inputs, cache, streams,\nknown_servers\n1: known_servers.ban(server)\n2: missing_layers = get_layers(server)\n3: chains = select_by_layer(\n4:\nknown_servers, missing_layers)\n5: chain = find_best_chain(chains)\n6: replacements = list()\n7: while not empty(chain) do\n8:\ns = chain.pop_left()\n9:\ntry:\n10:\nstreams[s] = rpc_inference(s)\n11:\noutputs = streams[s].send(inputs)\n12:\nreplacements.append(s)\n13:\ncache[s] = inputs\n14:\nmissing_layers.pop(get_layers(s))\n15:\ninputs = outputs\n16:\ncatch FailedRPC:\n17:\nknown_servers.ban(s)\n18:\nchains = select_by_layer(\n19:\nchains, missing_layers)\n20:\nchain = find_best_chain(chains)\n21: end while\n22: return chain\n3.3\nAutomatic load balancing\nIn order to run inference or fine-tuning, each server needs to be assigned to a pipeline stage, then\nreassigned if other servers join or leave the network. For example, if we deploy an LLM on idle\ncompute resources from several data centers or labs, the number of participants may change over time\nbased on the demand. Moreover, servers may have different compute throughput, network bandwidth,\nand geographical location. To operate in these conditions efficiently, servers should automatically\nchoose which model layers they should serve in a given situation.\nTo that end, servers periodically run a load balancing procedure and switch to new blocks if necessary.\nFormally, servers choose blocks so as to maximize the total system throughput (tokens per second).\nEach server periodically announces its blocks and empirically measured throughput to a distributed\nhash table (Maymounkov & Mazieres, 2002). When a new server joins, it uses this information to\nidentify a contiguous interval7 of blocks that would increase the total system throughput the most.\n7This interval is always contiguous, since splitting it would harm the inference latency.\n6\nSince peers may leave or fail at any time, all nodes periodically check if launching a rebalancing\nprocedure would significantly improve the overall throughput. If it is the case, they switch layers until\nthe throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave\nthe system, this procedure quickly redistributes the remaining resources to close the emerged gaps.\nWe provide a detailed description of the load balancing algorithms in Appendix D and validate their\nproperties in experiments reported in Appendix E.\n3.4\nParameter-efficient fine-tuning\nWhile LLMs achieve high quality on many problems with simple prompt engineering (Brown et al.,\n2020), they often need training to achieve the best results. Traditionally, this is done by fine-tuning\nall model parameters on the downstream task. However, for extremely large models, this strategy\nbecomes impractical due to hardware requirements. For example, fine-tuning BLOOM-176B with\nAdam would require almost 3 TB of GPU memory to store the model, gradients, and optimizer states.\nFortunately, parameter-efficient fine-tuning methods have been developed that keep most of the\npretrained model intact. Some of them choose a subset of existing parameters to update (Sung et al.,\n2021; Guo et al., 2021) while others augment the model with additional trainable weights (Hu et al.,\n2021; Houlsby et al., 2019; Liu et al., 2021b; Lester et al., 2021; Liu et al., 2021a, 2022b). Despite\ntheir lower memory requirements, parameter-efficient approaches are often competitive with full\nmodel fine-tuning (Hu et al., 2021; Liu et al., 2021a; Yong & Nikoulina, 2022) and even outperform\nit in low-data regimes (Liu et al., 2022a). Another appealing property of these approaches for our\nuse-case is that they allow rapidly switching a pretrained LLM between adapters.\nBy focusing on parameter-efficient fine-tuning, we are able to simplify the system design by making\nclients responsible for storing their trainable parameters (see Figure 1). Servers can run backpropa-\ngation through their layers and return gradients with respect to activations, but they do not update\nthe server-side parameters. Even when client communicates learned values (e.g. soft prompts) to a\nserver, the server treats these values same as input activations. Thus, a server can simultaneously run\ndifferent fine-tuning tasks without them interfering with one another. This design choice also allows\nusers to define custom adapters in simple PyTorch without having network engineering expertise.\nUnlike inference, fine-tuning forward and backward passes process the entire batch at one go and do\nnot need to store past attention caches between successive client requests. Thus, in case of a failure,\nwe can discard the incomplete forward/backward pass and just repeat the previous forward/backward\npass request. This algorithm behaves similarly to the cache-less baseline from Section 4.1.\n3.5\nImplementation details\nSince our main intended use-case is running on inexpensive low-end devices, we need to work around\ntheir capabilities. In terms of raw FLOPs, even consumer-grade GPUs like GeForce RTX 3070 could\nrun a complete inference step of BLOOM-176B in less than a second (NVIDIA, 2020). However, the\nGPU memory can only hold a small fraction of model layers: running na\u00efvely would require 44 RTX\n3070 GPUs and 44 communication rounds. To make this more efficient, we use quantization to store\nmore parameters per GPU, reducing the number of consecutive devices and communication rounds.\nOne option for quantization is to use 8-bit mixed matrix decomposition for matrix multiplication to\nquantize the weights to 8-bit precision and reduce the memory footprint compared to 16-bit weights,\nas suggested in Dettmers et al. (2022a). This decomposition separates hidden states and weights into\ntwo portions: about 0.1% of 16-bit outlier and 99.9% of 8-bit regular values, which roughly halves\nthe memory footprint with negligible effect on the model quality (see evaluations in Appendix A).\nAnother option is to use the 4-bit NormalFloat format (Dettmers et al., 2023).\nTo send less data between subsequent pipeline stages, we apply dynamic blockwise quantization\n(Dettmers et al., 2022b) to the hidden states before pipeline-parallel communication, which halves the\nbandwidth requirements without any noticeable effect on generation quality (Ryabinin et al., 2023).\nDuring fine-tuning, we also take advantage of gradient checkpointing (Griewank & Walther, 2000;\nChen et al., 2016) and half precision to reduce VRAM usage \u2014 both are standard practice for large\nlanguage models (Narayanan et al., 2021; Brown et al., 2020; Athlur et al., 2022). In experiments, we\napply the same optimizations to baseline systems for a fair comparison.\n7\nTable 1: Sequential inference speed (steps/second) of BLOOM (7.1B) with varying failure rates. A\nfailure rate p means that sending any set of activations to the next stage of the pipeline fails with\nprobability p. Missing values mean that the algorithm did not finish within 1 hour.\nInference Algorithm\n128 tokens, failure rate:\n1024 tokens, failure rate:\n0\n1e-4\n1e-3\n1e-2\n0\n1e-4\n1e-3\n1e-2\nCaching with restarts\n17.1\n16.7\n12\n0.18\n15.5\n11.8\n0.48\n\u2013\nCache-less inference\n3.44\n3.44\n3.44\n3.44\n0.89\n0.89\n0.89\n0.89\nAlgorithm 1 (ours)\n11.4\n11.4\n10.6\n3.38\n10.7\n10.7\n7.76\n2.17\n4\nExperiments\n4.1\nInference with unreliable servers\nFirst, we conduct small-scale preliminary experiments to test the fault-tolerant generation algorithm\ndescribed in Section 3.2. For these experiments, we use a smaller BLOOM model with 7.1 billion\nparameters (BigScience, 2022b). This model contains 30 transformer blocks with hidden size 4096.\nWe compare our algorithm with baselines when generating a single sequence of length 512. For\nsimplicity, we run all computations and communications in single precision and disregard word\nembeddings and logits for this set of experiments. We measure the time to run a certain number of\ntokens through all blocks and simulate failures by resetting pipeline stages at a certain rate.\nWe compare three inference strategies:\n1. Caching with restarts, which refers to standard inference with servers storing attention caches.\nOn failure, it restarts the entire generation from scratch since the failed server\u2019s caches are lost.\n2. Cache-less inference, which reruns past tokens on every step. On failure, it restarts only the last\ngeneration step.\n3. Algorithm 1, which is specifically designed for fault-tolerant inference.\nAll runs use four pipeline stages with (8, 7, 8, 7) model layers per pipeline stage. Each pipeline\nstage is served by a single GeForce 1080 Ti GPU; the four GPUs are running in a single system\nwith dual Xeon Gold 6148 CPU, 12 DDR4 LRDIMM sticks with 64 GB each. The system has 16\ndedicated PCIe Gen. 3 lanes per GPU in dual root configuration, without using PCIe switches. Each\nstage runs in an isolated Docker containers with virtual network interfaces, but there is no limit to\ncommunication bandwidth for this experiment. We repeat all experiments 50 times and report the\naverage time. The adjusted standard deviation never exceeds 0.2%. We use the pipeline parallelism\nimplementation from Megatron-DeepSpeed (BigScience et al., 2022) for the cache-less baseline.\nWe report performance measurements in Table 1. Unlike baselines, our algorithm provides reasonable\nperformance in all tested conditions, especially for higher failure rates (common for communicating\nover the Internet, using spot/preemptible instances or unreliable hardware). Caching with restarts is\nmost efficient for inference without failures, with our algorithm being somewhat slower due to less\nmature implementation. Finally, the cache-less inference can be competitive for short sequences (128\ntokens), but slows down considerably on 1024 tokens, which agrees with our intuition from 3.1.\nWe provide plots showing additional evaluations for a wider range of failure rates (up to 5%) and\nsequence lengths (up to 2048 tokens) in Appendix F (Figure 3).\n4.2\nExperiments for Llama 2 (70B) and BLOOM (176B)\nIn this section, we evaluate our system on more practical tasks of running Llama 2 (70B) (Touvron\net al., 2023b) and BLOOM (176B) (BigScience, 2022a). First, we consider servers running in\na network with controlled bandwidth and latency8. We measure performance for (a) Llama 2\ndistributed across 3 servers with a T4 GPU each, (b) BLOOM distributed across 3 servers with an\nA100 (80 GB) GPU each, and (c) BLOOM distributed across 10 servers with an RTX 3090 GPU\neach. We use 4-bit NormalFloat quantization (Dettmers et al., 2023) for Llama 2 and 8-bit matrix\ndecomposition (Dettmers et al., 2022a) for BLOOM in all evaluations including the baselines below.\n8We simulate network conditions using tc qdisc.\n8\nTable 2: Performance of Llama 2 (70B) sequential inference steps and parallel forward passes. The\nnetwork parameters refer to bidirectional bandwidth and round-trip latency (RTT).\nGPUs\nClients Bandwidth\nRTT\nSequential inference\nParallel forward\n(steps/s, each client) (tokens/s, each client)\nSequence length\nBatch size\n128\n2048\n1\u00d7128\n64\u00d7128\n3\u00d7 T4 (16 GB)\n1\n1 Gbit/s\n< 5 ms\n2.29\n2.02\n45.4\n155.1\n1\n100 Mbit/s\n< 5 ms\n2.29\n2.01\n37.5\n140.2\n1\n100 Mbit/s 100 ms\n1.57\n1.44\n23.7\n128.7\n3\n1 Gbit/s\n< 5 ms\n2.02\n1.74\n21.2\n124.2\n\u2013\nOffloading\n0.139\n0.139\n18.0\n139.9\nTable 3: Performance of BLOOM (176B) sequential inference steps and parallel forward passes.\nGPUs\nClients Bandwidth\nRTT\nSequential inference\nParallel forward\n(steps/s, each client) (tokens/s, each client)\nSequence length\nBatch size\n128\n2048\n1\u00d7128\n64\u00d7128\n3\u00d7 A100 (80 GB)\n1\n1 Gbit/s\n< 5 ms\n1.71\n1.54\n70.0\n253.6\n1\n100 Mbit/s\n< 5 ms\n1.66\n1.49\n56.4\n182.0\n1\n100 Mbit/s 100 ms\n1.23\n1.11\n19.7\n112.2\n3\n1 Gbit/s\n< 5 ms\n1.65\n1.49\n\u2013\n\u2013\n\u2013\nOffloading\n0.0495\n0.0495\n2.5\n152.4\n\u2013\nLocal PP (NVLink)\n2.46\n2.28\n98.4\n279.5\n1\n1 Gbit/s\n< 5 ms\n1.65\n1.54\n59.1\n230.1\n3\n1 Gbit/s\n< 5 ms\n1.65\n1.54\n54.7\n221.4\n10\u00d7 RTX 3090\n10\n1 Gbit/s\n< 5 ms\n1.17\n1.01\n31.0\n131.0\n(24 GB)\n10\n100 Mbit/s\n< 5 ms\n1.05\n0.99\n20.1\n28.1\n10\n100 Mbit/s 100 ms\n0.34\n0.33\n6.5\n16.8\n\u2013\nOffloading\n0.0427\n0.0427\n2.2\n109.3\n1\n1 Gbit/s\n< 5 ms\n1.24\n1.06\n37.9\n180.0\n12\u00d7 heterogeneous\n1\n100 Mbit/s\n< 5 ms\n1.24\n1.05\n25.6\n66.0\n(virtual servers)\n1\n100 Mbit/s 100 ms\n0.57\n0.53\n5.8\n44.3\n12\n1 Gbit/s\n< 5 ms\n0.90\n0.86\n\u2013\n\u2013\n14\u00d7 heterogeneous\n1\nReal-world setup\n0.83\n0.79\n32.6\n179.4\nTheoretical-best\n\u2013\nOffloading\n0.18\n0.18\n2.7\n170.3\nWe report performance of:\n\u2022 Sequential (autoregressive) inference for batch size 1 (i.e., each step generates 1 token). It is\nmeasured in generation steps per second a client can do and shows the generation latency.\n\u2022 Parallel forward passes for batches of 128-token sequences9. It is measured in tokens per second\na client can process. This shows the system\u2019s throughput during batch processing and fine-tuning.\nSince the backward pass performance depends on a set of trainable weights, batch size, and other\nhyperparameters, we report its performance in different setups separately in Appendix G.\nConcurrent clients.\nWe also investigate the effect of having concurrent clients. We assume that\neach server belongs to a different person, and multiple people (possibly, all of them) are interested in\nrunning inference or fine-tuning at the same time. In order to do that, they run the client interacting\nwith our distributed system. The client runs on the same machine, uses 8 CPU cores and no GPU. We\nreport the speed of sequential inference and parallel forward passes that each client gets on average.\n9Intenally, large batches are split into micro-batches of 1024 tokens each to minimize pipeline bubbles.\n9\nOffloading baseline.\nWe also evaluate parameter offloading, where each user runs independently on\na single GPU, swapping parameters from CPU memory. First, we report the actual throughput of RAM\noffloading in case of DeepSpeed with default recommended parameters and enabled pin_memory\n(gives 1.2\u22122\u00d7 speedup). Next, we report the theoretical-best throughput the offloading baseline\ncan reach for BLOOM. It is calculated as a maximal throughput in the best hardware setup possible\n(CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes), assuming infinite GPU performance. The\ncalculations are detailed in Appendix B.\nLocal pipeline parallelism (NVLink).\nNext, we report performance for BLOOM running on a\nserver with 3\u00d7 A100 (80 GB) GPUs. In this setup, a single server has enough GPU memory to load\nthe entire model, which provides an upper bound for performance reachable with these GPUs. This\nsetup runs pipeline-parallelism from DeepSpeed v0.7.7.\nHeterogeneous servers.\nTo validate that our system works on heterogeneous hardware, we simulate\n12 heterogeneous devices by partitioning each A100 (80 GB) into several virtual servers (3 large and\n1 small). We get 9 servers hosting 7 blocks each, one server with 3 blocks and two more servers with\n2 blocks (70 blocks in total, as required for BLOOM). Additionally, we benchmark the system on\nreal heterogeneous GPUs with diverse compute capabilities in the \"Real-world setup\" below.\nReal-world setup.\nFinally, we benchmark BLOOM in a real-world setup with 14 smaller servers\nholding 2\u00d7RTX 3060, 4\u00d72080Ti, 2\u00d73090, 2\u00d7A4000, and 4\u00d7A5000 GPUs. These are personal\nservers and servers from university labs, spread across Europe and North America and connected to\nthe Internet at speeds of 100\u20131000 Mbit/s. Four of the servers operate from behind firewalls10.\nAnalysis.\nWe report the results for Llama 2 in Table 2 and for BLOOM in Table 3. For inference,\nperformance does not depend much on bandwidth or sequence length but degrades with higher latency.\nIn turn, fine-tuning forward passes for large batches are affected by both bandwidth and latency.\nWe can see that the offloading baseline is about an order of magnitude slower than our system for\ninference, both in practice and in the theoretical-best setup assuming an infinite GPU performance.\nFor parallel forward passes, offloading is competitive if networking is limited to 100 Mbit/s or has\nhigh latency. In other cases, our algorithm offers higher throughput than offloading for training.\nCrucially, our system significantly outperforms offloading even when each GPU node runs its own\nclient doing single-batch inference at the same time. Thus, given the same hardware, a group of\nresearchers will get much better inference speed by collaborating over the Internet using our system\ncompared to each of them running offloading independently.\nFinally, the real-world setup turns out to be slower than the A100 benchmarks due to slower hardware.\nStill, our algorithm outperforms offloading even when communicating between different continents.\nAdditional experiments.\nWe conduct two additional experiments to test individual components of\nour system. We evaluate the load balancing from 3.3 in isolation in Appendix E. We also evaluate the\nperformance of model compression from Section 3.5 in Appendix A. To reiterate, for each model, we\nuse the same compression strategy in our system and all baselines. Finally, we perform a qualitative\nevaluation of fault tolerance by shutting down random servers during inference and fine-tuning to\nverify that the algorithm produces correct outputs and gradients.\n5\nConclusion\nIn this paper, we introduced a novel fault-tolerant algorithm for inferencing large language models.\nOn top of it, we introduced a decentralized system for running LLMs on distributed unreliable devices\nconnected over the Internet, which significantly outperforms other approaches to running inference\non consumer-grade hardware. We demonstrated that the proposed system can scale to the largest\npublicly available language model with hundreds of billions of trainable parameters.\nWhile our work is focused on technical aspects, it is important to consider limitations of our approach,\nsuch as privacy of data processed by outside peers, as well as broader impact of making LLMs more\naccessible. We discuss these issues and outline directions for future work in Appendix H.\n10We use the Circuit Relay protocol from libp2p (libp2p, 2022) to traverse NATs and firewalls.\n10\nReferences\nAI21.\nJurassic-1\nlanguage\nmodels.\n\"https://studio.ai21.com/docs/\njurassic1-language-models\". Accessed: 2022-06-22.\nAminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A., Li, C., Li, D., Zheng, E., Rasley, J.,\nSmith, S., Ruwase, O., et al. Deepspeed inference: Enabling efficient inference of transformer\nmodels at unprecedented scale. arXiv preprint arXiv:2207.00032, 2022.\nAthlur, S., Saran, N., Sivathanu, M., Ramjee, R., and Kwatra, N. Varuna: scalable, low-cost training\nof massive deep learning models. In Proceedings of the Seventeenth European Conference on\nComputer Systems, pp. 472\u2013487, 2022.\nBen-Nun, T. and Hoefler, T. Demystifying parallel and distributed deep learning: An in-depth\nconcurrency analysis. ACM Comput. Surv., 52(4), aug 2019. ISSN 0360-0300. doi: 10.1145/\n3320060. URL https://doi.org/10.1145/3320060.\nBigScience.\nBLOOM: a 176B-parameter open-access multilingual language model.\nArXiv,\nabs/2211.05100, 2022a.\nBigScience.\nA version of BLOOM with 7.1 billion parameters.\nhttps://huggingface.co/\nbigscience/bloom-7b1, 2022b.\nBigScience, Microsoft, and NVIDIA. The fork of Megatron-LM and Megatron-DeepSpeed by\nBigScience. https://github.com/bigscience-workshop/Megatron-DeepSpeed, 2022.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C.,\nMcDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang,\nB., and Weinbach, S. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL\nhttps://arxiv.org/abs/2204.06745.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv\npreprint arXiv:1604.06174, 2016.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for\ntransformers at scale. ArXiv, abs/2208.07339, 2022a.\nDettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit optimizers via block-wise quantization.\nInternational Conference on Learning Representations (ICLR), 2022b.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized\nllms. arXiv preprint arXiv:2305.14314, 2023.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W.,\nFirat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat,\nM., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen,\nZ., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts. CoRR,\nabs/2112.06905, 2021. URL https://arxiv.org/abs/2112.06905.\nEvans, D., Kolesnikov, V., Rosulek, M., et al. A pragmatic introduction to secure multi-party\ncomputation. Foundations and Trends in Privacy and Security, 2(2-3):70\u2013246, 2018.\nFace, H. and contributors. Accelerate: Run your raw pytorch training script on any kind of device.\nGitHub. Note: https://github.com/huggingface/datasets, 1, 2020.\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity, 2021.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell,\nK., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and\nZou, A. A framework for few-shot language model evaluation, September 2021. URL https:\n//doi.org/10.5281/zenodo.5371628.\n11\nGriewank, A. and Walther, A. Algorithm 799: revolve: an implementation of checkpointing for the\nreverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical\nSoftware (TOMS), 26(1):19\u201345, 2000.\nGuo, D., Rush, A. M., and Kim, Y. Parameter-efficient transfer learning with diff pruning. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration.\nIn International Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=rygGQyrFvH.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan,\nM., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on\nMachine Learning, pp. 2790\u20132799. PMLR, 2019.\nHu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, L., and Chen, W. Lora: Low-rank adaptation\nof large language models, 2021.\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y.,\net al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in\nNeural Information Processing Systems, pp. 103\u2013112, 2019.\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model parallelism for deep neural networks.\nIn Talwalkar, A., Smith, V., and Zaharia, M. (eds.), Proceedings of Machine Learning and\nSystems, volume 1, pp. 1\u201313, 2019. URL https://proceedings.mlsys.org/paper/2019/\nfile/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\nWu, J., and Amodei, D. Scaling laws for neural language models, 2020.\nKhrushchev, M., Vasilev, R., Zinov, N., Petrov, A., and Yandex. Yalm 100b, 2022. \"https:\n//huggingface.co/yandex/yalm-100b\".\nKim, B., Kim, H., Lee, S., Lee, G., Kwak, D., Jeon, D. H., Park, S., Kim, S., Kim, S., Seo, D.,\nLee, H., Jeong, M., Lee, S., Kim, M., Ko, S., Kim, S., Park, T., Kim, J., Kang, S., Ryu, N., Yoo,\nK. M., Chang, M., Suh, S., In, S., Park, J., Kim, K., Kim, H., Jeong, J., Yeo, Y. G., Ham, D., Park,\nD., Lee, M. Y., Kang, J., Kang, I., Ha, J., Park, W., and Sung, N. What changes can large-scale\nlanguage models bring? intensive study on hyperclova: Billions-scale korean generative pretrained\ntransformers. CoRR, abs/2109.04650, 2021. URL https://arxiv.org/abs/2109.04650.\nKoenig, S. and Likhachev, M. Fast replanning for navigation in unknown terrain. IEEE Transactions\non Robotics, 21(3):354\u2013363, 2005. doi: 10.1109/TRO.2004.838026.\nKrizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997,\n2014. URL http://arxiv.org/abs/1404.5997.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional\nneural networks. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q. (eds.), Advances\nin Neural Information Processing Systems 25, pp. 1097\u20131105. Curran Associates, Inc., 2012.\nKuszmaul, J. Bamboo trimming revisited: Simple algorithms can do well too. arXiv preprint\narXiv:2201.07350, 2022.\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen,\nZ. Gshard: Scaling giant models with conditional computation and automatic sharding. ArXiv,\nabs/2006.16668, 2020.\nLester, B., Al-Rfou, R., and Constant, N.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pp. 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https:\n//aclanthology.org/2021.emnlp-main.243.\nlibp2p. libp2p circuit relay. https://docs.libp2p.io/concepts/nat/circuit-relay/, 2022.\n12\nLiu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-\nefficient fine-tuning is better and cheaper than in-context learning, 2022a. URL https://arxiv.\norg/abs/2205.05638.\nLiu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-\nefficient fine-tuning is better and cheaper than in-context learning, 2022b. URL https://arxiv.\norg/abs/2205.05638.\nLiu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J. P-tuning v2: Prompt tuning can be comparable\nto fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021a.\nLiu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. Gpt understands, too.\narXiv:2103.10385, 2021b.\nMaymounkov, P. and Mazieres, D. Kademlia: A peer-to-peer information system based on the xor\nmetric. In International Workshop on Peer-to-Peer Systems, pp. 53\u201365. Springer, 2002.\nNarayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons,\nP. B., and Zaharia, M. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings\nof the 27th ACM Symposium on Operating Systems Principles, SOSP \u201919, pp. 1\u201315, New York,\nNY, USA, 2019. Association for Computing Machinery. ISBN 9781450368735. doi: 10.1145/\n3341301.3359646. URL https://doi.org/10.1145/3341301.3359646.\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D.,\nKashinkunti, P., Bernauer, J., Catanzaro, B., et al. Efficient large-scale language model training on\ngpu clusters. arXiv preprint arXiv:2104.04473, 2021.\nNVIDIA.\nNVIDIA\nAmpere\nGA102\nGPU\narchitecture,\n2020.\nURL\nhttps:\n//images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/\nNVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf.\nNVIDIA.\nNvidia confidential computing.\nhttps://www.nvidia.com/en-in/data-center/\nsolutions/confidential-computing/, 2022.\nPudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S. Training large neural networks with\nconstant memory using a new execution algorithm. arXiv preprint arXiv:2002.05645, 2020.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understand-\ning by generative pre-training.\n2018.\nURL https://cdn.openai.com/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are\nunsupervised multitask learners. 2019.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson,\nS., Ring, R., Young, S., and et al. Scaling language models: Methods, analysis & insights from\ntraining gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimization towards training a\ntrillion parameter models. In SC, 2020.\nRajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and He, Y. Zero-infinity: Breaking the gpu\nmemory wall for extreme scale deep learning. arXiv preprint arXiv:2104.07857, 2021.\nRen, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y.\nZero-offload: Democratizing billion-scale model training, 2021.\nRyabinin, M., Dettmers, T., Diskin, M., and Borzunov, A. SWARM Parallelism: Training Large\nModels Can Be Surprisingly Communication-Efficient. In Krause, A., Brunskill, E., Cho, K.,\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 29416\u2013\n29440. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ryabinin23a.\nhtml.\n13\nSchick, T. and Sch\u00fctze, H. Generating datasets with pretrained language models. pp. 6943\u20136951,\nNovember 2021. doi: 10.18653/v1/2021.emnlp-main.555. URL https://aclanthology.org/\n2021.emnlp-main.555.\nShazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H.,\nHong, M., Young, C., Sepassi, R., and Hechtman, B. A. Mesh-tensorflow: Deep learning for\nsupercomputers. CoRR, abs/1811.02084, 2018. URL http://arxiv.org/abs/1811.02084.\nSung, Y.-L., Nair, V., and Raffel, C. Training neural networks with fixed sparse masks. Advances in\nNeural Information Processing Systems, 2021.\nTang, Z., Shi, S., Chu, X., Wang, W., and Li, B. Communication-efficient distributed deep learning:\nA comprehensive survey, 2020.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V.,\nand Stojnic, R. Galactica: A large language model for science. 2022.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal,\nN., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and\nPolosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H.,\nFergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing\nSystems 30, pp. 5998\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/\npaper/7181-attention-is-all-you-need.pdf.\nWang, J., Yuan, B., Rimanic, L., He, Y., Dao, T., Chen, B., Re, C., and Zhang, C. Fine-tuning\nlanguage models over slow networks using activation compression with guarantees, 2022. URL\nhttps://arxiv.org/abs/2206.01299.\nWest, P., Bhagavatula, C., Hessel, J., Hwang, J. D., Jiang, L., Bras, R. L., Lu, X., Welleck, S., and\nChoi, Y. Symbolic knowledge distillation: from general language models to commonsense models.\narXiv preprint arXiv:2110.07178, 2021.\nYang, B., Zhang, J., Li, J., R\u00e9, C., Aberger, C. R., and Sa, C. D. Pipemare: Asynchronous pipeline\nparallel dnn training. ArXiv, abs/1910.05124, 2019.\nYong, Z.-X. and Nikoulina, V. Adapting bigscience multilingual model to unseen languages, 2022.\nURL https://arxiv.org/abs/2204.04873.\nYuan, B., He, Y., Davis, J., Zhang, T., Dao, T., Chen, B., Liang, P. S., Re, C., and Zhang, C.\nDecentralized training of foundation models in heterogeneous environments. Advances in Neural\nInformation Processing Systems, 35:25464\u201325477, 2022.\nZeng, A., Liu, X., Du, Z., Ding, M., Zheng, Q., Lai, H., Wang, Z., Yang, Z., Yu, J., Zhang, X.,\nZheng, W., Xia, X., Xu, Y., Tam, W. L., Dong, Y., Ma, Z., He, J., Sun, Z., Zhai, J., Chen,\nW., Zeng, G., Han, X., Zhao, W., Liu, Z., Xue, Y., Wang, S., Shan, J., Jiang, H., Guo, Z.,\nZhang, P., and Tang, J. GLM-130B: An open bilingual pre-trained model, 2022. URL http:\n//keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/.\nZeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang,\nX., Li, C., Gong, Z., Yao, Y., Huang, X., Wang, J., Yu, J., Guo, Q., Yu, Y., Zhang, Y., Wang,\nJ., Tao, H., Yan, D., Yi, Z., Peng, F., Jiang, F., Zhang, H., Deng, L., Zhang, Y., Lin, Z., Zhang,\nC., Zhang, S., Guo, M., Gu, S., Fan, G., Wang, Y., Jin, X., Liu, Q., and Tian, Y. Pangu-\u03b1:\nLarge-scale autoregressive pretrained chinese language models with auto-parallel computation.\nCoRR, abs/2104.12369, 2021. URL https://arxiv.org/abs/2104.12369.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X.,\nLin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A.,\nWang, T., and Zettlemoyer, L. OPT: open pre-trained transformer language models, 2022. URL\nhttps://arxiv.org/abs/2205.01068.\n14\nAppendix\nA\nQuality and efficiency of BLOOM with 8-bit quantization\nAs shown in Table 4, this method has little effect on LLM quality for major benchmarks. In terms of\ninference time, Table 5 demonstrates that quantization has about 5% of overhead with batch size 1\n(20 tokens), but becomes negligible for larger batches.\nB\nEstimating theoretical best throughput with RAM offloading\nIn this estimate, we use the best possible hardware setup for offloading: CPU RAM offloading\nvia PCIe 4.0 with 16 PCIe lanes per GPU. In 8-bit, the model uses 1 GB of memory per billion\nparameters, and PCIe 4.0 with 16 lanes has a throughput of 256 Gbit/s. We assume an offloading\nlatency of zero in the upper bound estimation. As such, offloading 176B parameters takes at least:\n176 GB \u00b7 8\n256 Gbit/s = 5.5 seconds\nThis gives the upper bound of 1/5.5 \u2248 0.18 tokens/s for the inference speed.\nC\nExtension to beam search algorithms\nThere are several variations of beam-search algorithm used for language model inference, including\nstandard beam search, diverse beam search, constrained beam search, and more. A common thread\nbetween those algorithms is that they maintain a fixed number k of candidate sequences between\nsteps. These sequences are informally referred to as the \u201cbeam\u201d. On every step, these algorithms\ngenerate possible continuations of sequences in the previous beam, then use some fitness criterion to\nselect k of these continuations for the next beam.\nFrom a computational point of view, this procedure is similar to simple \u201cgreedy\u201d inference with a\nbatch of k sequences. However, there is one important difference: unlike batched inference, beam\nsearch algorithms can \u201cshuffle\u201d candidate sequences between steps. In other words, 3rd best sequence\nfrom time step t can produce 1st or 2nd (or any other) sequence on the next step. Furthermore, a\nsingle sequence on time step t can produce multiple sequences selected for step t + 1.\nSince different beam search variantions use different criteria for selecting top sequences, we need\na generic algorithm that can fit any criterion. In our system, we implement this by allowing clients\nto reorder server-side attention cache after each step. Formally, a client can send a list of at most\nk integers in range [1, k], where i-th index specifies which previous attention cache should be used\nwhen generating i-th sequence of the next beam.\nFor instance, when given indices [2, 2, 1, 3, 2], a server will use 2nd best sequence from step t to\nproduce the new 1st, 3rd and 5th best sequences. Previous 1st and 3rd best sequences go to 3rd and\n4th places, respectively. Finally, previous 4th and 5th sequences are discarded. From a technical point\nof view, servers implement this reordering by reordering attention cache with the specified indices\n(torch.gather operation) immediately before performing an inference step.\nTable 4: Zero-shot accuracy for BLOOM-176B and\nOPT-175B with 8-bit and 16-bit weights.\nModel\nBits\nHellaSwag\nLAMBADA\nWinoGrande\nAvg\nBLOOM\n16\n73.0\n67.2\n70.1\n70.1\n8\n72.8\n68.1\n70.1\n70.3\nOPT\n16\n78.5\n74.7\n72.6\n75.3\n8\n78.5\n74.6\n71.7\n74.9\nTable 5: Generation throughput (tokens/s)\nfor BLOOM-176B with 8-bit and 16-bit\nweights on 8\u00d7 A100 GPUs.\nWeights\nBatch size\n1\n8\n32\n16-bit\n4.18\n31.3\n100.6\n8-bit\n3.95\n29.4\n95.8\n15\nD\nDetails of the server load balancing algorithms\nMeasuring throughput.\nBefore joining for the first time, each server measures its Internet connec-\ntion throughput (in tokens/second, using one of public web APIs for doing that) and GPU throughput\n(in tokens/second, using a small benchmark running several forward passes). The minimum of these\nvalues becomes the overall server throughput, which is then cached for future runs.\nInitial block assignment.\nWe assume that each server holds a segment of consecutive transformer\nblocks to minimize inference latency. Clients may request to perform a forward or backward pass for\nthe whole segment of blocks or its subsegment, if necessary. Normally, each server loads as many\nblocks as it can fit in its GPU memory, unless a user limits the number of blocks to utilize the rest of\nmemory for something else.\nBefore starting, each server calculates the values of ti \u2013 the total throughput of servers currently\nholding the i-th block or loading it (to start holding it in a few minutes). Then, to find the best\nsegment of blocks to serve, the server looks for the most narrow bottleneck in the network. Formally,\nif the model has L blocks and the server can hold K of them in its GPU memory, we calculate:\nstart =\nL\u2212K+1\narg min\ni=1\nsorted([ti, ti+1, . . . , ti+K\u22121])\n(1)\nHere, arg min compares the sorted arrays lexicographically and chooses the leftmost start in case of\nmultiple minimums.\nThis way, the next joining server would always cover a block with the smallest ti. If there are multiple\nbottlenecks like this, the server will try to cover as many of them as possible (we choose to cover the\nminimums first because the overall throughput is the minimum of throughputs among model blocks).\nAmong the remaining options, we choose a segment covering as many second minimums as possible,\nand so on.\nQuality of block assignment.\nWhile we are not aware of the exact polynomial-time solution for\nthe problem of assigning the segments optimally, we have conducted computational experiments\nand found out that this greedy algorithm (running in polynomial time) usually finds an assignment\nwith total throughput of 90-100% of the optimal one (found by trying out all possible assignments in\nexponential time), given that the values of throughput are realistic to our setup.\nRebalancing.\nSince servers may leave at any time, each server also periodically checks if the\ncurrent assignment is \"good enough\" compared to the throughput estimated by running the greedy\nsolution for servers currently present in the network.\nFormally, each server periodically looks for a segment of blocks that is more appropriate than the\ncurrently loaded blocks with respect to the arg min rule (1). If it finds one, it simulates how the rest\nof the servers would behave if we replace the current blocks with the new ones (how other servers\nwould change their blocks afterwards). If the eventual throughput is at least p% better, the server\ncommits to the change and announces that it changes the blocks, then other servers do the rest of the\nchanges (eventually increasing the total throughput).\nWe use p = 20% since it gives a reasonable trade-off between the swarm throughput and the frequency\nof block replacements in our experiments (see Appendix E). Specifically, a lower value of p leads to\nblock replacements happening too often, which negatively affects the inference latency since each\nblock replacement resets attention caches for this block.\nStability of the greedy algorithm.\nThe rebalancing algorithm does not cause oscillations since a\nseries of block replacements is executed only if it leads to eventually increasing throughput by at\nleast p%. Once a \"good enough\" throughput is achieved, servers do not change their blocks anymore\n(unless an essential number of servers join or leave). We verified this behavior computationally,\nsimulating a network with thousands of servers with different throughputs.\nTo conclude, this greedy heuristic allows servers to quickly close the gaps if a substantial share (up to\n100%) of servers holding certain blocks leave, but avoids excess block replacements otherwise.\n16\n0\n100\n200\n300\n400\n500\n600\n700\n0\n100\n200\n300\nThroughput (tokens/s)\nPipeline throughput\nNo load balancing\nNew servers only\nFull, p = 20%\nFull, p = 10%\nFull, p = 1%\nUpper bound\n0\n100\n200\n300\n400\n500\n600\n700\nTime (minutes)\n0\n25\n50\n75\n100\nReplacements per minute\nNumber of block replacements per minute (10 min average)\nFigure 2: Behavior of the load balancing algorithms evaluated in Appendix E.\nE\nEvaluation of the server load balancing algorithms\nIn this section, we measure the effectiveness of the load balancing algorithm used in our system.\nWe run all experiments using a fleet of 206 virtual instances that simulate participants. To keep\nexperiment costs manageable, we do not use GPUs for this evaluation, instead simulating uneven\nserver throughput programmatically. For each server, we sample its throughput from the uniform\ndistribution t \u223c U[0, 100] tokens/second, then sample its memory size so it can hold b \u223c U[1, 10]\nblocks (out of 70 blocks in total, as in BLOOM-176B).\nEach server follows a certain availability schedule, i.e. turns on and shuts down at the same predefined\ntime across all experiments. We assign these schedules such that the number of active servers follows\na sine wave, simulating daily activity cycles. The schedule has approximately 100\u2013110 active servers\nduring peak activity and 15\u201325 servers at its lowest points. Note that each peak contains a different\nsubset of 100\u2013110 active servers out of 206 instances in total.\nWe evaluate the following approaches to load balancing:\n1. No load balancing \u2013 a baseline system where servers load a random contiguous interval of model\nblocks.\n2. Balancing new servers only \u2013 a simplified load balancing where servers choose the optimal\nblocks when joining the swarm (using the rule (1) from Appendix D) but never change them.\n3. Full load balancing \u2013 the full algorithm, where every minute each server checks if they need\nto replace their blocks. We use the efficiency threshold p (as described in Appendix D) to avoid\nexcess block replacements.\n4. Upper bound \u2014 the best-case throughput estimate that reassigns contiguous block segments to\nservers optimally every minute.\nWe report their behavior in Figure 2. The full load balancing maintains connectivity throughout the\nexperiment and achieves throughput close to the upper bound (staying within the 10\u201315% range most\nof the time). Higher thresholds p perform slightly worse during peak times but require only relatively\ninfrequent block replacements, unlike the case with p = 1%. Note that using the assignment leading\nto the upper bound is not possible in practice since it requires each server to load a different set of\nlayers every minute, on top of solving the computationally expensive optimization problem.\nCuriously, the baseline running load balancing for new servers only achieves reasonable throughput\nduring periods where servers are actively joining. However, it quickly loses throughput when random\nservers leave, since this creates \u201cbottlenecks\u201d in the pipeline that require rebalancing of existing\npeers. Finally, the naive baseline with random layer assignment has zero throughput most of the time\nbecause it is unable to form a complete pipeline.\n17\nF\nExperiments with a wider range of failure rates\nIn this section, we follow the setup from Section 4.1 and provide additional evaluations for a\nwider range of failure rates (up to 5%) and sequence lengths (up to 2048 tokens). The results are\nshown in Figure 3. Unlike baselines, our algorithm provides reasonable performance in all tested\nconditions, especially for higher failure rates common for communicating over the Internet, using\nspot/preemptible instances or unreliable hardware).\n128256\n512\n1024\n2048\nSequence length\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nAverage steps/sec\nP(failure) = 0.05\nCache-less inference\nCaching with restarts\nOurs\n128256\n512\n1024\n2048\nSequence length\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nP(failure) = 0.02\nCache-less inference\nCaching with restarts\nOurs\n128256\n512\n1024\n2048\nSequence length\n0\n1\n2\n3\n4\n5\nAverage steps/sec\nP(failure) = 0.01\nCache-less inference\nCaching with restarts\nOurs\n128256\n512\n1024\n2048\nSequence length\n0\n2\n4\n6\n8\n10\n12\nP(failure) = 0.001\nCache-less inference\nCaching with restarts\nOurs\n128256\n512\n1024\n2048\nSequence length\n0\n2\n4\n6\n8\n10\n12\n14\n16\nAverage steps/sec\nP(failure) = 0.0001\nCache-less inference\nCaching with restarts\nOurs\n128256\n512\n1024\n2048\nSequence length\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nP(failure) = 0.0\nCache-less inference\nCaching with restarts\nOurs\nFigure 3: Sequential inference speed (steps/s) for BLOOM (7.1B) with varying failure rates. The\nsetup is the same as in Section 4.1. A failure rate p means that sending a set of activations to the next\npipeline stage fails with probability p. Zero speed means that the baseline did not finish within 1 hour.\n18\nTable 6: Throughput (tokens/sec) of forward and backward passes for different tasks, batch sizes,\nprefix lengths.\nMode\nBatch size Prompt length Forward pass Backward pass\nthroughput\nthroughput\nPrompt tuning\n8\n16\n195.6\n57.4\n8\n4\n213.2\n60.8\n32\n16\n272.6\n82.8\n32\n4\n293.1\n84.7\n8\n16\n111.0\n42.0\nPrefix tuning\n8\n4\n178.7\n57.8\n(i.e., \u201cdeep\u201d prompt tuning)\n32\n16\n164.1\n64.4\n32\n4\n255.8\n84.8\nG\nPerformance of training-time forward and backward passes\nIn this section, we evaluate throughput of training-time forward and backward passes and study\nfactors that affect their performance. We will only consider BLOOM-176B and the \u201c3\u00d7 A100,\n1 Gbit/s\u201d setup from Section 4.2 and focus on finetuning-specific hyperparameters, since the influence\nof network bandwidth and latency has already been discussed in the main paper.\nSequence classification.\nFirst, we consider fine-tuning the model on a binary classification task.\nWe take BLOOM-176B, replace the logit layer with a trainable classification head (similar to\ntransformers.BloomForSequenceClassification), and add trainable prompts before the input\nsequence, then train the model on batches of 128-token sequences. We try (a) both prompt tuning\nand prefix tuning (involving \u201cdeep\u201d prompts), (b) two batch sizes (8 and 32), and (c) two prompt\nlengths (16 and 4). The client shares 8 CPU cores with one of the servers and does not use the GPU.\nThe results are provided in Table 6. The prefix tuning turns out to be slower, since it adds several\ntimes more trainable parameters. Increasing prompt length and decreasing batch size also make\ntraining slower. Notably, we observe that moving client-side computations to GPU does not visibly\nimprove performance, since the client does not perform any heavy operations in this setup11.\nLanguage modeling.\nNext, we consider fine-tuning the model on a causal language modeling task.\nWe take BLOOM-176B, keep the logit layer, and add trainable prompts before the input sequence.\nWe explore the same hyperparameters as with sequence classification.\nWe observe that the throughput of the GPU-enabled client is similar (within 10% difference) to the\nthroughput in case of sequence classification, reported in Table 6. Indeed, the client performs only a\nsmall share of GPU computations in the forward and backward passes, and a particular model head\nand a loss function do not have decisive influence on the performance. However, performance of the\nCPU-only client turns out to be 5-10 times worse in this setup, since the client has to multiply the\noutput embedding matrix to the hidden states of all tokens in the batch. This operation is too large to\nbe efficiently computed on CPU12.\nH\nLimitations and broader impact\nPrivacy.\nA key limitation of our approach is that servers hosting the first model blocks may use\ntheir inputs to recover client data. Thus, users working with sensitive data should limit their clients\nto only use trusted servers or, alternatively, set up their own isolated network using our software.\nFor example, if multiple research labs or small companies have access to a specific private dataset\n11In case of sequence classification, the heaviest operation the client does is multiplying 2 \u00d7 h and h \u00d7 b\nmatrices, where h is the hidden dimension (14336 in BLOOM-176B) and b is the batch size.\n12In case of language modeling, the client has to multiply d \u00d7 h and h \u00d7 b matrices, where d is the token\nvocabulary size (250880 in BLOOM-176B). This is \u2248 105 times more FLOPS than used in case of sequence\nclassification.\n19\nand want to process it with a large language model, they may set up an isolated distributed network\nhosting this model to get a better inference speed, compared to running the model independently.\nIn the future, this limitation may be addressed in future work using secure multi-party comput-\ning (Evans et al., 2018) or privacy-preserving hardware (NVIDIA, 2022).\nMotivating contributors.\nSince people using the client are not required to run a server, our system\nmay experience an imbalance between supply (peers who dedicate GPUs to serve model layers) and\ndemand (peers using the servers to perform inference or fine-tuning for their own needs).\nOne way to encourage users to serve model blocks would be to introduce a system of incentives:\npeers running servers would earn reward points, which can be spent on high-priority inference and\nfine-tuning or exchanged for other rewards. To implement this, we can run a few validator peers that\nperiodically traverse all available servers and issue reward points to their owners.\nSecurity.\nWe assume that servers in our system are run by many independent parties. In practice,\nsome of them may turn out to be faulty and return incorrect outputs instead of the actual results of\nforward and backward passes. This may happen due to a malicious intent to influence other people\u2019s\noutputs or, when rewards are introduced (as described above), to earn a reward for serving layers\nwithout actually performing the calculations.\nTo address this issue, we can extend the validator peers, so that they periodically test servers with\nrandom requests of different types and ban them if they respond with incorrect outputs (possibly,\nrevoking their rewards). The validator requests should be difficult to distinguish from requests of\ntypical users, so that malicious servers cannot pretend to be honest to the validators but send wrong\noutputs to other peers. While this approach still leaves a chance of receiving wrong outputs, it allows\nto eventually expose and penalize the faulty servers.\nFinally, clients may reduce the probability of getting faulty outputs by running their data through\nmultiple disjoint chains of servers simultaneously and comparing the outputs against each other.\nBroader impact.\nThis work introduces a general-purpose algorithm for decentralized inference\nand fine-tuning of large models, aiming to simplify access to the latest research in deep learning and\nprovide an alternative way to efficiently run LLMs without high-end hardware. We do not envision\nany direct negative impacts from our research, since models that can be hosted with our system are\nalready widely available and may be used via APIs, offloading, or other means.\n20\n"
  },
  {
    "title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
    "link": "https://arxiv.org/pdf/2312.07661.pdf",
    "upvote": "14",
    "text": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor\nShuyang Sun1,2*\nRunjia Li1*\nPhilip Torr1\nXiuye Gu2\u2020\nSiyang Li2\u2020\n1University of Oxford\n2Google Research\n{kevinsun, runjia, phst}@robots.ox.ac.uk {siyang, xiuyegu}google.com\nhttps://torrvision.com/clip_as_rnn/\nFigure 1. We propose CaR to segment concepts in a vast vocabulary, including fictional characters, landmarks, brands, everyday objects,\nand referring expressions. This figure shows our qualitative results. More visualizations are included in the supplementary material. Best\nviewed in color and with zoom-in.\nAbstract\nExisting open-vocabulary image segmentation methods re-\nquire a fine-tuning step on mask annotations and/or image-\ntext datasets. Mask labels are labor-intensive, which limits\nthe number of categories in segmentation datasets. As a re-\nsult, the open-vocabulary capacity of pre-trained VLMs is\nseverely reduced after fine-tuning. However, without fine-\ntuning, VLMs trained under weak image-text supervision\ntend to make suboptimal mask predictions when there are\ntext queries referring to non-existing concepts in the image.\nTo alleviate these issues, we introduce a novel recurrent\nframework that progressively filters out irrelevant texts and\nenhances mask quality without training efforts. The recur-\nrent unit is a two-stage segmenter built upon a VLM with\nfrozen weights. Thus, our model retains the VLM\u2019s broad\nvocabulary space and strengthens its segmentation capabil-\nity. Experimental results show that our method outperforms\nnot only the training-free counterparts, but also those fine-\ntuned with millions of additional data samples, and sets new\nstate-of-the-art records for both zero-shot semantic and re-\nferring image segmentation tasks. Specifically, we improve\nthe current record by 28.8, 16.0, and 6.9 mIoU on Pascal\n*The first two authors contribute equally to this work. This work is\ndone during Shuyang\u2019s internship at Google Research.\n\u2020Equal advising.\nVOC, COCO Object, and Pascal Context.\n1. Introduction\nNatural language serves a bridge to connect visual elements\nwith human communicable ideas by transforming colors,\nshapes, and objects etc. into descriptive language. On the\nother hand, human can use natural language to easily in-\nstruct computers and robotics to perform their desired tasks.\nBuilt upon the revolutionary vision-language model trained\non Internet-scale image-text pairs, e.g., CLIP [48], a variaty\nof studies [10, 35, 38, 41, 49, 54, 67, 75, 83] have explored\nto use pre-trained VLMs for open-vocabulary image seg-\nmentation \u2014 to segment any concept in the image described\nby arbitrary text queries.\nAmong these advances, several works [35, 39, 75] have\nintegrated pre-trained VLMs with segmenters trained on\nbounding boxes and masks. While these methods exhibit\nsuperior performances on segmentation benchmarks with\ncommon categories, their ability to handle a broader vocab-\nulary is hampered by the small category lists in the segmen-\ntation datasets used for fine-tuning. As depicted in Figure 2,\neven though all three methods incorporate CLIP [48], those\nrelying on fine-tuning with mask annotations [35, 39] fail to\nrecognize the concepts like Pepsi and Coca Cola.\n1\narXiv:2312.07661v2  [cs.CV]  21 Dec 2023\nOVSeg [35]\nGrounded SAM [39]\nCaR (Ours)\nFigure 2. Our method CaR can fully inherit the vast vocabu-\nlary space of CLIP, by directly using features from a pre-trained\nVLM, CLIP, without any fine-tuning. Although the scene in the\nimage is simple, state-of-the-art methods fine-tuned on segmenta-\ntion datasets [35, 39] fail to segment and recognize Pepsi and Coca\nCola correctly.\nSince box and mask annotations are expensive, another\nline of works [10, 38, 41, 49, 50, 67] seek to fine-tune the\nVLM and/or auxiliary segmentation modules with image-\nlevel annotations only, e.g., paired image-text data obtained\nfrom the Internet. This would lead to a complicated fine-\ntuning pipeline. Besides, these segmentation models often\nhave suboptimal mask qualities, as image-level labels can-\nnot directly supervise pixel grouping.\nIn this paper, we eliminate fine-tuning on mask anno-\ntations or additional image-text pairs to fully preserve the\nextensive vocabulary space of the pre-trained VLM. How-\never, the pre-training objectives of VLMs are not specifi-\ncally designed for dense predictions. As a result, existing\napproaches [14, 37, 83] that do not fine-tune the VLMs,\nstruggle to generate accurate visual masks corresponding to\nthe text queries, particularly when some of the text queries\nrefer to non-existing objects in the image. To address this is-\nsue, we repeatedly assess the degree of alignment between\neach mask proposal and text query, and progressively re-\nmove text queries with low confidence. As the text queries\nbecome cleaner, better mask proposals are consequently ob-\ntained. To facilitate this iterative refinement, we propose\na novel recurrent architecture with a two-stage segmenter\nas the recurrent unit, maintaining the same set of weights\nacross all time steps. The two-stage segmenter consists of\na mask proposal generator and a mask classifier to assess\nthe mask proposals. Both are built upon a pre-trained CLIP\nmodel with no modifications. Given an input image and\nmultiple text queries, our model recurrently aligns the vi-\nsual and textual spaces and generates a refined mask as the\nfinal output, continuing until a stable state is achieved. Ow-\ning to its recurrent nature, we name our entire framework as\nCLIP as RNN (CaR).\nExperimental results demonstrate our approach is re-\nmarkably effective. In comparison with methods that do not\nuse additional training data, i.e., zero-shot open-vocabulary\nsemantic segmentation, our approach outperforms the prior\nart by 28.8, 16.0, and 6.9 mIoU on Pascal VOC [19], COCO\nObject [36], and Pascal Context [44], respectively.\nIm-\npressively, even when pitted against models fine-tuned on\nextensive additional data, our strategy surpasses the best\nrecord by 12.6, 4.6, and 0.1 on the three aforementioned\ndatasets, respectively. To assess our model\u2019s capacity to\nhandle more complex text queries, we evaluate on the re-\nferring image segmentation benchmarks, Ref-COCO, Ref-\nCOCO+ and RefCOCOg, and CaR outperforms the zero-\nshot counterparts by a large margin. Moreover, we extend\nour method to the video domain, and establish a zero-shot\nbaseline for the video referring segmentation on Ref-DAVIS\n2017 [29]. As showcased in Figure 1, our proposed ap-\nproach CaR exhibits remarkable success across a broad vo-\ncabulary spectrum, effectively processing diverse queries\nfrom celebrities and landmarks to referring expressions and\ngeneral objects.\nOur contributions can be summarized as follows:\n1. By constructing a recurrent architecture, our method\nCaR performs visual segmentation with arbitrary text\nqueries in a vast vocabulary space.\n2. When compared with previous methods on zero-shot\nopen-vocabulary semantic segmentation and referring\nimage and video segmentation, our method CaR outper-\nforms the prior state of the arts by a large margin.\n2. Related Work\nOpen-vocabulary segmentation with mask annotations.\nThe success of VLMs [25, 34, 48, 58, 72, 77, 78] has mo-\ntivated researchers to push the boundaries of traditional\nimage segmentation tasks, moving them beyond fixed la-\nbel sets and into an open vocabulary by fine-tuning or\ntraining VLMs on segmentation datasets [20, 22, 26, 32,\n35, 39, 43, 69, 75, 79, 80, 84].\nHowever, as collecting\nmask annotations for a vast range of fine-grained labels\nis prohibitively expensive, existing segmentation datasets,\ne.g. [4, 19, 36, 44, 82] have limited vocabularies. Meth-\nods fine-tuned on these mask annotations reduce the open-\nvocabulary capacity inherited from the pre-trained VLMs.\nIn this work, we attempt to preserve the completeness of\nthe vocabulary space in pre-trained VLMs.\nOpen-vocabulary segmentation without mask supervi-\nsion. Several works [6, 10, 11, 23, 41, 45, 49, 50, 54, 67,\n68, 83] avoid the aforementioned vocabulary reduction is-\nsue by not fine-tuning on any mask annotations. Instead,\nresearchers allow semantic grouping to emerge automati-\ncally without any mask supervision. GroupViT [67] learns\nto progressively group semantic regions with weak super-\nvision, using only image-text datasets. Furthermore, it is\npossible to use a pre-trained VLM for open-vocabulary seg-\nmentation without any additional training [27, 54, 83]. For\nexample, MaskCLIP [83] enables CLIP to perform open\nvocabulary segmentation by only modifying its image en-\ncoder. However, these methods often suffer from inferior\nsegmentation performance due to the lack of mask supervi-\n2\nImage \ud835\udc65!\nSegmenter\nMask \ud835\udc66!\n\u210e!\"#\n(a) Compressed \nPipeline\nImage \ud835\udc65#\nSegmenter\nMask \ud835\udc66#\nManchester \nUnited, \nBarcelona, \nArsenal, \nManchester \nCity,\nImage \ud835\udc65$\nSegmenter\nMask \ud835\udc66!\nManchester \nUnited, \nBarcelona, \nArsenal, \nManchester \nCity\n\ud835\udf48\n\ud835\udf48\nManchester \nUnited, \nManchester \nCity\n\u2026\n\u210e! == \u210e!\"#?\n\u210e%\n\u210e#\n\u210e$\nN\nY\nStop & Post-Process\nFinal Mask Prediction\n\u2026\nProposal \nGenerator\n(CLIP+CAM)\n\ud835\udc53(\u22c5.\u22c5)\nManchester United, Barcelona, \nArsenal, Manchester City,\n\u2026\nText query \u210e!\"#\n\u2026\nMask Classifier \n(CLIP) \ud835\udc54(\u22c5,\u22c5)\nManchester \nUnited, \nBarcelona, \nArsenal, \nManchester \nCity\n\ud835\udf48\nMask Proposals \ud835\udc66!\nSegmenter\nImage+Visual Prompts \ud835\udc65!\n&\n\u2026\nFinal Mask Prediction\nText query \u210e!\n(b) Unfolded Pipeline\n(c) Details of each stage\nImage \ud835\udc65!\nFigure 3. The overall framework of our method CaR. (a), (b): given an image, the user provides a set of text queries that they are\ninterested to segment. This initial set, denoted by h0, may refer to non-existing concepts in the image, e.g., Barcelona and Arsenal.\nIn the t-th time step, the frozen segmenter evaluates the degree of alignment between each mask and text query from the previous time step,\nht\u22121, and then low-confidence queries are eliminated by the function \u03c3. (c) depicts the detailed architecture of our two-stage segmenter. It\nconsists a mask proposal generator f(\u00b7, \u00b7), and a mask classifier g(\u00b7, \u00b7) that assesses the alignment of each mask-text pairs.\nsion, and the modification of the pre-trained VLMs. CaR is\nclosely related to these approaches, we are both in a zero-\nshot manner without training. CaR stands out by propos-\ning a recurrent framework on a VLM with fixed weights\nand no alternation on its architecture. Note that our zero-\nshot is different from the zero-shot semantic segmenta-\ntion [2, 3, 17, 24, 33, 65, 83] that mirrors the seen/unseen\nclass separation from zero-shot classification in earlier ages.\nSegmentation with VLM-generated pseudo-labels. As\nan alternative direction, recent works have exploited pre-\ntrained VLMs for generating pseudo-masks in a fixed la-\nbel space, requiring only image-level labels or captions for\ntraining [1, 37, 40, 51, 54, 66, 70, 83]. Once pseudo mask\nlabels are obtained, a segmenter with a fixed vocabulary\n(e.g., DeepLab [12, 13]) can be trained in a fully super-\nvised manner. Among these, CLIP-ES [37] is particularly\nrelevant as it directly uses CLIP for pseudo-mask genera-\ntion given the class names in ground-truth. However, CLIP-\nES [37] requires pseudo-label training while we don\u2019t.\nProgressive refinement for image segmentation.\nPro-\ngressive refinement in image segmentation has seen sig-\nnificant advancements through various approaches.\nRe-\ncent works [8, 15, 16, 59, 61, 74] such as Cascade R-\nCNN [7], DETR [8] and CRF-RNN [81] combine a detector\n(R-CNN [21]), a transformer [60] or a segmenter (dense-\nCRF [31]) repeatedly for iterative refinement. We kindly\nnote that all these works are designed for supervised image\ninstance or semantic segmentation in a closed-set vocabu-\nlary. Our method does not require any training effort, yet\nour way of progressive refinement is fundamentally differ-\nent from these methods.\n3. CLIP as Recurrent Neural Networks\n3.1. A Recap on Recurrent Neural Networks\nWe begin with a concise overview of recurrent neural net-\nworks (RNN). RNNs are specifically designed to process\nsequential data, such as text, speech, and time series. A ba-\nsic RNN, commonly known as a vanilla RNN, uses the same\nset of weights to process data at all time steps. At each time\nstep t, the process can be expressed as follows:\nht = \u03c3(Whhht\u22121 + Wxhxt + bh),\n(1)\nyt = Whyht + by.\n(2)\nxt represents the input, and ht represents the hidden state\nserving as the \u201cmemory\u201d that stores information of previ-\nous inputs. yt denotes the output. Whh, Wxh, and Why\nare weight matrices, bh and by refer to the bias terms, and\n\u03c3 denotes a thresholding function, which introduces non-\nlinearity.\nAn RNN\u2019s core lies in its hidden state, ht, which cap-\ntures information from past time steps.\nThis empowers\nRNNs to exploit temporal dynamics within sequences. In\nour approach CaR, we use a similar process: we iteratively\nalign the textual and visual domains by assessing the accu-\nracy of each text query through a segmenter, using the same\nset of weights as well. The text queries at each step act like\n3\nAlgorithm 1 Pseudo-code of CLIPasRNN in PyTorch style.\n# img: the input image with shape (3, H, W)\n# h_0: a list of the initial N_0 text queries.\n# clip: the CLIP model encoding the image and texts.\n# cam: the gradient-based CAM model for mask proposal\ngeneration.\n# eta: a threshold to binarize the masks for visual\nprompting.\n# theta: a threshold defined in Eq. 6.\nh_{t-1} = h_0\nwhile len(h_{t-1}) > 0:\n# logits: [1, len(h_{t-1})]\nlogits = clip(img, h_{t-1})\nscores = softmax(logits, dim=-1)\n# proposals: [len(h_{t-1}), H, W]\nproposals = cam(clip, img, scores)\n# prompted_img: [len(h_{t-1}), H, W]\nprompted_imgs = apply_visual_prompts(img, proposals\n, eta)\n# mask_logits: [len(h_{t-1}), len(h_{t-1})]\nmask_logits = clip(prompted_imgs, h_{t-1})\nmask_scores = softmax(mask_logits, dim=-1)\n# diag_scores: [len(h_{t-1})]\ndiag_scores = diagonal(mask_scores)\nh_t = []\nfor score, label in zip(diag_scores, h_{t-1}):\nif score > theta:\nh_t.append(label)\nif len(h_t) == len(h_{t-1}):\nbreak\nh_{t-1} = h_t\nfinal_masks = post_process(proposals)\nthe RNN\u2019s hidden state, representing the entities identified\nin the image at each specific time step.\n3.2. Overview\nAs depicted in Figure 3(a) and (b), our training-free\nframework operates in a recurrent manner, with a fixed-\nweight segmenter shared across all time steps. In the t-th\ntime step, the segmenter receives an image xt \u2208 R3\u00d7H\u00d7W\nand a set of text queries ht\u22121 from the preceding step as\nthe input. It then produces two outputs: a set of masks\nyt \u2208 [0, 1]Nt\u22121\u00d7H\u00d7W corresponding to Nt\u22121 input text\nqueries, and the updated text queries ht for the subsequent\nstep. For image segmentation, all different time steps share\nthe same xt.\nTo delve deeper into the design of our framework, we\nformulate its operations through Eq. (3) to Eq. (5).\nyt = f(xt, ht\u22121; Wf).\n(3)\nHere the function f(\u00b7, \u00b7) represents the mask proposal gen-\nerator and Wf denotes its pre-trained weights. The mask\nproposal generator processes the input image xt and the text\nqueries at previous step ht\u22121 to generate candidate mask\nproposals yt. Given the mask proposal generator is not pre-\ntrained for dense prediction, the mask proposals yt from\nf(\u00b7, \u00b7) are inaccurate. To assess these mask proposals, we\ndraw visual prompts e.g., red circles or background blur,\nto the input xt, based on mask proposals to highlight the\nmasked area on the image. The visual prompting function\nRed Circle Red Contour Background\nBlur\nBackground\nGray\nBackground\nMask\nFigure 4. Examples of visual prompts given a mask on the man\nwearing the jersey of Manchester United.\nv(\u00b7, \u00b7) is defined as:\nx\u2032\nt = v(xt, yt).\n(4)\nHere x\u2032\nt represent Nt\u22121 images with the visual prompts.\nThe prompted images x\u2032\nt are then passed to the mask classi-\nfier g(\u00b7, \u00b7) with the pre-trained weights Wg, along with the\ntext queries ht\u22121, to compute a similarity matrix Pt. The\nentire process of the mask classifier can be defined as:\nPt = g(x\u2032\nt, ht\u22121; Wg).\n(5)\nFinally, after going through a thresholding function \u03c3(\u00b7),\ntext queries with similarity scores lower than the threshold\n\u03b8 will be removed so that the text queries ht = \u03c3(Pt) for the\nnext step t are obtained. ht is a potentially reduced set of\nht\u22121. Details of the thresholding function will be given in\nSection 3.3. This recurrent process continues until the text\nqueries remain unchanged between consecutive steps, i.e.,\nht == ht\u22121. We use T to denote this terminal time step.\nFinally, we apply post-processing described in Section 3.4\nto the mask proposals yT generated in the final time step.\nThe pseudo-code in PyTorch-style is given in Algo-\nrithm 1.\nNote that users provide the initial text queries\nh0, which are unrestricted and can include general object\nclasses (\u201ccat\u201d), proper nouns (\u201cSpace Needle\u201d), referring\nphrases (\u201cthe man in red jacket\u201d), etc.\n3.3. The Two-stage Segmenter\nIn this section, we explain the two core components of our\nsegmenter, i.e. a mask proposal generator and a mask classi-\nfier, which serve as the recurrent unit. As illustrated in Fig-\nure 3(c), the mask proposal generator first predicts a mask\nfor each text query and then the mask classifier filters out ir-\nrelevant text queries based on the degree of alignment with\ntheir associated masks. We use the frozen pre-trained CLIP\nmodel weights for both the proposal generator and classi-\nfier, in order to fully preserve the knowledge encapsulated\nin CLIP.\nMask proposal generator. To predict the mask proposal yt,\na gradient-based Class-Activation Map (gradCAM) [37, 53]\nis applied to the pre-trained CLIP. More specifically, the im-\nage xt and text queries ht\u22121 are first fed into CLIP to get\na score between the image and each text. We then back-\npropagate the gradients of the score of each text query (i.e.,\n4\nclass) from the feature maps of the CLIP image encoder to\nobtain a heatmap. Unless otherwise specified, we use the\nstate-of-the-art CLIP-ES [37] as our mask proposal gener-\nator. Apart from the text queries at the current step, we\nexplicitly add a set of background queries describing cate-\ngories that do not exist in the user text queries and calculate\ntheir gradients. This helps to suppress the activation from\nirrelevant texts (e.g., Barcelona and Arsenal in Figure\n3) in the subsequent mask classification process. More de-\ntails of how CLIP works with gradCAM are provided in the\nsupplementary material.\nMask classifier. The masks from the proposal generator\nmay be noisy because the input texts are from an unre-\nstricted vocabulary and may refer to non-existing objects in\nthe input image. To remove this type of proposals, we apply\nanother CLIP model to compute a similarity score between\neach query and its associated mask proposal. A straightfor-\nward approach is blacking out all pixels outside the mask re-\ngion, as shown in the rightmost image in Figure 4, and then\ncomputing the visual embedding for the foreground only.\nHowever, recent works [40, 55] have found several more\neffective visual prompts which can highlight the foreground\nas well as preserve the context in the background. Inspired\nby this, we apply a variety of visual prompts, e.g., red\ncircles, bounding boxes, background blur and gray back-\nground to guide the CLIP model to focus on the foreground\nregion. A threshold \u03b7 is set to first binarize the mask pro-\nposals yt before applying these visual prompts to the im-\nages. Please refer to the supplementary material for more\nimplementation details. After applying visual prompts, we\nobtain Nt\u22121 different prompted images, corresponding to\nNt\u22121 text queries (ht\u22121). We feed these images and text\nqueries into the CLIP classifier g(\u00b7, \u00b7) followed with a soft-\nmax operation along the text query dimension to get the\nsimilarity matrix Pt \u2208 RNt\u22121\u00d7Nt\u22121 given the image and\ntext embeddings. We only keep the diagonal elements of\nPt as the matching score between the i-th mask and the i-th\nquery. If the score is lower than a threshold \u03b8, the query and\nits mask are filtered out. Mathematically, the thresholding\nfunction \u03c3(\u00b7) is defined as follows:\nhi\nt = \u03c3(P ii\nt ) =\n(\nhi\nt\u22121,\nif P ii\nt \u2265 \u03b8\nNULL,\nif P ii\nt < \u03b8\n(6)\nwhere P ii\nt\nis the i-th element of the diagonal of the nor-\nmalized similarity matrix, and \u03b8 is a manually set threshold.\nNULL represents that the i-th text query is filtered out and\nwill not be input to next step.\n3.4. Post-Processing\nOnce the recurrent process stops, we start to post-process\nyT , the masks from the final step T. We employ dense con-\nditional random field (CRF) [31] to refine mask boundaries.\nWhen constructing the CRF, the unary potentials are cal-\nculated based on the mask proposals of the last step. All\nhyper-parameters are set to the defaults in [31]. Finally, an\nargmax operation is applied to the mask output of dense-\nCRF along the dimension of text queries. Thus, for each\nspatial location of the mask we only keep the class (text\nquery) with the highest response.\nAdditionally, we propose to ensemble the CRF-refined\nmasks with SAM [30], as an optional post-processing mod-\nule.\nThis begins with generating a set of mask propos-\nals from SAM using the automask mode, without enter-\ning any prompts into SAM. To match these SAM propos-\nals with the masks processed by denseCRF, we introduce\na novel metric: the Intersection over the Minimum-mask\n(IoM). If the IoM between a mask from SAM and a CRF-\nrefined mask surpasses a threshold \u03d5iom, we consider them\nmatched. Then all SAM proposals matched to the same\nCRF-refined mask are combined into one single mask. Fi-\nnally, we compute the IoU between the combined mask and\nthe original CRF-refined mask. If the IoU is greater than a\nthreshold \u03d5iou, we adopt the combined mask to replace the\noriginal mask, otherwise, we keep using the CRF-refined\nmask. The detailed post-processing steps are explained in\nthe supplementary material.\n4. Experiments\n4.1. Zero-shot Semantic Segmentation\nDatasets.\nSince our method does not require training,\nour discussion will solely focus on the datasets utilized\nfor evaluation purposes. We conduct assessments for se-\nmantic segmentation using the validation (val) splits of\nPascal VOC, Pascal Context, and COCO Object. Specif-\nically, Pascal VOC [18] encompasses 21 categories: 20\nobject classes alongside one background class. For Pascal\nContext [44], our evaluation employs the prevalent version\ncomprising 59 classes including both \u201cthings\u201d and \u201cstuff\u201d\ncategories, and one background (\u201cother\u201d) class for the con-\ncepts not belonging to any of the 59 classes. Following [67],\nwe construct the COCO Object dataset as a derivative of\nCOCO Stuff [5]. We kindly emphasize that the COCO Ob-\nject dataset is not COCO Stuff since it merges all \u201cstuff\u201d\nclasses into one background class thus has 81 classes (80\n\u201cthings\u201d + 1 background) in total.\nWe use the standard\nmean Intersection-over-Union (mIoU) metric to evaluate\nour method\u2019s segmentation performance.\nImplementation details. Our proposed method CaR uti-\nlizes the foundational pre-trained CLIP models as the back-\nbone. More precisely, we harness the CLIP model with ViT-\nB/16 to serve as the underlying framework for the mask pro-\nposal generator f(\u00b7, \u00b7). Concurrently, for the mask classifier\ng(\u00b7, \u00b7), we adopt a larger ViT-L/14 version for higher preci-\nsion based on our ablation study. Unless otherwise spec-\n5\nModels\nIs VLM\npre-trained?\nw/ aux trainable\nmodule?\naux pre-trained\nsegmenter\nAdditional\nTraining Data\n#Images Additional\nSupervision\nPascal\nVOC\nCOCO\nObject\nPascal\nContext\nzero-shot methods fine-tuned with additional data\nViL-Seg [38]\n\u2713\n\u2713\n-\nCC12M\n12M\ntext+self\n34.4\n16.4\n16.3\nGroupViT [67]\n\u00d7\n\u2713\n-\nCC12M+YFCC\n26M\ntext\n52.3\n24.3\n22.4\nGroupViT [67]\n\u00d7\n\u2713\n-\nCC12M+RedCaps\n24M\ntext\n50.8\n27.5\n23.7\nSegCLIP [41]\n\u00d7\n\u2713\n-\nCC3M+COCO\n3.4M\ntext+self\n33.3\n15.2\n19.1\nSegCLIP [41]\n\u2713\n\u2713\n-\nCC3M+COCO\n3.4M\ntext+self\n52.6\n26.5\n24.7\nZeroSeg [11]\n\u2713\n\u2713\n-\nIN-1K\n1.3M\nself\n40.8\n20.2\n20.4\nViewCo [50]\n\u2713\n\u2713\n-\nCC12M+YFCC\n26M\ntext+self\n52.4\n23.5\n23.0\nMixReorg [6]\n\u2713\n\u2713\n-\nCC12M\n12M\ntext\n47.9\n-\n23.9\nCLIPpy [49]\n\u2713\n\u00d7\n-\nHQITP-134M\n134M\ntext+self\n52.2\n32.0\n-\nOVSegmenter [68]\n\u2713\n\u2713\n-\nCC4M\n4M\ntext\n53.8\n25.1\n20.4\nTCL [10]\n\u2713\n\u2713\n-\nCC15M\n15M\ntext+self\n55.0\n31.6\n30.4\nzero-shot methods with SAM\nSAMCLIP [62]\n\u2713\n\u2713\nSAM [30]\nCC15M+YFCC+IN21k\n41M\ntext+self\n60.6\n-\n29.2\nCaR+SAM (Ours)\n\u2713\n-\nHQ-SAM [28]\n-\n-\n-\n70.2\n37.6\n31.1\nzero-shot methods without fine-tuning on CLIP\nReCo\u2020 [54]\n\u2713\n\u00d7\n-\n-\n-\n-\n25.1\n15.7\n19.9\nMaskCLIP\u2020 [83]\n\u2713\n\u00d7\n-\n-\n-\n-\n38.8\n20.6\n23.6\nCaR (Ours)\n\u2713\n\u00d7\n-\n-\n-\n-\n67.6\n36.6\n30.5\n\u2206 w/ the state-of-the-art w/o additional data\n+28.8 +16.0\n+6.9\n\u2206 w/ the state-of-the-art w/ additional data\n+12.6\n+4.6\n+0.1\nTable 1. Comparison to state-of-the-arts zero-shot semantic segmentation approaches. Results annotated with a \u2020 are as reported by\nCha et al. [10]. A \u2713 is placed if either the visual or text encoder of the VLM is pre-trained. The table shows that our method outperforms not\nonly counterparts without fine-tuning by a large margin, but also those that are fine-tuned on millions of data samples. For fair comparison,\nwe compare with methods using CLIP [48] as the backbone.\nified, the reported quantitative results are post-processed\nsolely with a denseCRF, with no SAM masks involved. In\nsetting the threshold hyper-parameters, we assign \u03b7 = 0.4,\n\u03b8 = 0.6, and \u03bb = 0.4 for Pascal VOC, and \u03b7 = 0.5,\n\u03b8 = 0.3, \u03bb = 0.5 for COCO and \u03b7 = 0.6, \u03b8 = 0.2, \u03bb = 0.4\nfor Pascal context. The specific background queries used\nfor the mask generator f(\u00b7, \u00b7) are ablated in Section 4.2 and\ndetailed in the supplementary material. For Pascal Context,\nwe use separate groups of background queries for \u201cthing\u201d\nand \u201cstuff\u201d. For \u201cthing\u201d categories, we will add all \u201cstuff\u201d\ncategories as background queries and vice versa for \u201cstuff\u201d\ncategories.\nAs an optional strategy, we utilize a match-\ning algorithm and perform an ensemble with masks with\nSAM. We set both thresholds, \u03d5iom and \u03d5iou, to 0.7 for all\nthree datasets. We enable half-precision floating point for\nCLIP, and the peak memory cost is about 3.6GB on Pas-\ncal VOC. Since CaR is just a framework designed for in-\nference, all experiments in this paper are conducted on just\none NVIDIA V100 GPU.\nCaR significantly outperforms methods without addi-\ntional training. We also compare CaR with training-free\nmethods like MaskCLIP [83] and ReCo [54]. Across the\nbenchmarks, our model consistently demonstrates an im-\npressive performance uplift. Under a similar setting when\nno additional training data is used, CaR surpasses previous\nDataset\nw/ recurrence?\nCAM\nmIoU\nPascal VOC\nCLIP-ES [37]\n15.2\n\u2713\nCLIP-ES [37]\n67.6\n\u2713\ngradCAM [53]\n41.1\nTable 2. Effect of applying our recurrent architecure and dif-\nferent CAM methods. The recurrence plays a vital role in im-\nproving the performance.\nstate-of-the-art method by 28.8, 16.0 and 6.9 mIoU on Pas-\ncal VOC, COCO Object and Pascal Context, respectively.\nTraining-free CaR even outperforms several methods\nwith additional fine-tuning.\nAs shown in Table 1, we\ncompare our method with previous state-of-the-art meth-\nods including ViL-Seg [38], GroupViT [67], SegCLIP [41],\nZeroSeg [11], ViewCo [50], CLIPpy [49], and TCL [10],\nwhich are augmented with additional data.\nThe prior\nbest results of different datasets are achieved by different\nmethods. Specifically, TCL [10], employing a fully pre-\ntrained CLIP model and fine-tuned on 15M additional data,\nachieves the highest mIoU (55.0 and 30.4) on Pascal VOC\nand Pascal Context. CLIPpy [49] sets the previous high-\nest record on COCO Object but also requires extensive data\nfor fine-tuning. Concretely, it first utilizes a ViT-based im-\nage encoder pre-trained with DINO [9] and a pre-trained\n6\nMask Proposal\nGenerator f(\u00b7, \u00b7)\nMask Classifier\ng(\u00b7, \u00b7)\nPascal\nVOC\nCOCO\nObject\nViT-B/16\nViT-B/16\n54.1\n15.9\nViT-L/14\n67.6\n36.6\nViT-L/14\nViT-B/16\n50.6\n14.1\nViT-L/14\n57.6\n32.5\nTable 3. Effect of CLIP backbones. We compare various CLIP\nbackbones on Pascal VOC and COCO Object. Results show that\nwe can improve the performance by scaling up the mask classifier.\nDataset\nVisual Prompts\nmIoU\ncircle\ncontour blur\ngray\nmask\nPascal\nVOC\n\u2713\n66.9\n\u2713\n66.0\n\u2713\n66.4\n\u2713\n66.1\n\u2713\n61.8\n\u2713\n\u2713\n67.6\n\u2713\n\u2713\n67.1\n\u2713\n\u2713\n66.5\n\u2713\n\u2713\n66.3\n\u2713\n\u2713\n\u2713\n66.8\nTable 4.\nEffect of different visual prompts.\nWhen multiple\nvisual prompts are checked, we will apply all checked visual\nprompts simultaneously on one image. The experiments are con-\nducted on Pascal VOC and results for COCO and Pascal Context\nare shown in supplementary materials.\nT5 text encoder [47], then fine-tunes both encoders with\n134M additional data. Our method, incurring no cost for\nfine-tuning, still outperforms these methods by 12.6, 4.5,\nand 0.1 mIoU on the Pascal VOC, COCO Object, and\nPascal Context datasets, respectively. Since CLIP has en-\ncountered fewer background and \u201cstuff\u201d classes in its pre-\ntraining image-text data, our model exhibits less sensitivity\nto \u201cstuff\u201d classes. Consequently, our gain on Pascal Context\nis relatively small.\nCaR+SAM further boosts the performance. When inte-\ngrated with SAM [28, 30], we compare CaR with a concur-\nrent method SAMCLIP [62] and outperform it by 9.6, 1.9\non Pascal VOC and Pascal Context. Here we use the recent\nvariant HQ-SAM [28] with no prompt given (automask\nmode), then match the generated masks with metrics de-\nsigned in Section 3.4. In other words, SAM is only used\nas a post-processor to refine the boundary of results from\nCaR. By applying SAM into our framework, our results can\nbe further boosted by 2.6, 1.1 and 0.6 mIoU on Pascal VOC,\nCOCO Object and Pascal Context, respectively.\n4.2. Ablation Studies.\nEffect of Recurrence. As illustrated in Table 2, the in-\ncorporation of the recurrent architecture is crucial to our\nmethod. Without recurrence, our method functions simi-\nlarly to CLIP-ES [37] with an additional CLIP classifier,\nand achieves only 15.2% in mIoU. The recurrent frame-\nwork can lead to a 52.4% improvement, reaching an mIoU\nof 67.6%. The significant improvement validates the effec-\ntiveness of the recurrent design of our framework.\nEffect of different CAM methods. Table 2 exhibits that\nour framework is compatible with different CAM methods\nand could be potentially integrated with other CAM-related\ndesigns. When integrated with CLIP-ES [37], our method is\n26.5 mIoU higher than that with gradCAM [53]. We kindly\nnote that we do not carefully search the hyper-parameters on\ngradCAM so the performance could be further improved.\nEffect of different CLIP Backbones. We experiment with\ndifferent settings of CLIP backbones used in the mask pro-\nposal generator f and mask classifier g, on Pascal VOC and\nCOCO Object datasets. Results are displayed in Table 3.\nFor the mask proposal generator, ViT-B/16 outperforms the\nViT-L/14 by over 10 mIoU on both Pascal VOC and COCO\nObject. There is significant mIoU gains when employing\nthe larger ViT-L/14 for the mask classifier over ViT-B/16.\nSimilar observations have been found by Shtedritski et al.\n[55] that a larger backbone can better understand the vi-\nsual prompts, which indicates that the performance of our\nmethod can be potentially improved by using large back-\nbones as the mask classifier.\nEffect of different visual prompts.\nThere are various\nforms of visual prompts, including circle, contour, back-\nground blur (blur), background gray (gray), and back-\nground mask (mask), etc. We study the effects of differ-\nent visual prompts on the Pascal VOC dataset and Table 4\nsummarizes the results when applying one or a combina-\ntion of two of the aforementioned visual prompting meth-\nods. The highest mIoU score is achieved with the combina-\ntion of circle and blur, yielding a mIoU of 67.6. No-\ntably, using mask alone results in the lowest mIoU of 61.8,\nwhich is a conventional common-practice for most previ-\nous open-vocabulary segmentation approaches e.g. [35, 75].\nWe also evaluate the effect of different visual prompts on\nCOCO Object and Pascal Context, and show the results in\nthe supplementary material.\nEffect of hyper-parameters. We perform an ablation study\non the performance impact of various hyper-parameter con-\nfigurations on Pascal VOC, and present the results in Ta-\nble 5.\nHyper-parameters include the mask binarization\nthreshold, \u03b7, defined in Section 3.3, the threshold \u03b8 em-\nployed in the thresholding function defined in Eq. (6), and\nthe parameter \u03bb defined in CLIP-ES [37]. The peak perfor-\nmance is recorded at an mIoU of 67.6 for \u03b7 = 0.4, \u03b8 = 0.6,\nand \u03bb = 0.4 on Pascal VOC and 36.6 for \u03b7 = 0.5, \u03b8 = 0.3,\nand \u03bb = 0.5 on COCO Object. Different parameter combi-\nnations result in mIoU scores that range from 67.0 to 67.6\non Pascal VOC and from 35.4 to 36.6 on COCO Object.\n7\nPascal VOC\nCOCO Object\n\u03b7\n\u03b8\n\u03bb\nmIoU\n\u03b7\n\u03b8\n\u03bb\nmIoU\n0.3 0.6 0.4\n67.0\n0.5\n0.3\n0.6\n35.4\n0.4 0.6 0.4\n67.6\n0.5\n0.3\n0.4\n36.1\n0.5 0.6 0.4\n67.0\n0.4\n0.3\n0.5\n35.8\n0.4 0.5 0.4\n67.4\n0.5\n0.3\n0.5\n36.6\n0.4 0.7 0.4\n67.5\n0.6\n0.3\n0.5\n35.9\n0.4 0.6 0.3\n67.3\n0.5\n0.4\n0.5\n36.3\n0.4 0.6 0.5\n67.0\n0.5\n0.5\n0.5\n36.0\nTable 5. Effect of different hyper-parameters: the threshold to\nbinarize mask proposals (\u03b7), the threshold to remove text queries\n(\u03b8), and parameter of CLIP-ES\u2019s[37] (\u03bb). Experiments are con-\nducted on Pascal VOC and COCO Object.\nDataset\nBackground queries\nmIoU\nTerrestrial\nAquatic\nAtmospheric Man-Made\nPascal\nVOC\n\u00d7\n\u00d7\n\u00d7\n64.3\n\u2713\n\u00d7\n\u00d7\n65.6\n\u00d7\n\u2713\n\u00d7\n64.9\n\u00d7\n\u00d7\n\u2713\n66.4\n\u2713\n\u2713\n\u00d7\n65.8\n\u00d7\n\u2713\n\u2713\n66.4\n\u2713\n\u00d7\n\u2713\n65.8\n\u2713\n\u2713\n\u2713\n67.6\nTable 6.\nEffect of background queries on Pascal VOC. We\ndivide background queries into:\nTerrestrial, Aquatic,\nAtmospheric, and Man-Made. We use \u201cNone\u201d as the back-\nground query for the result in the first row. Specific background\nqueries of each category are shown in the supplementary material.\nEffect of background queries. In Table 6, we explore how\ndifferent background queries (classes not exist in the input\nqueries) can affect CaR\u2019s performance. We find that the\nsegmentation quality improves as we include more diverse\nbackground queries: The combination of all three types of\nbackground queries delivers the highest mIoU of 67.6. For\ndetails of the background queries of each class, please refer\nto the supplementary material.\n4.3. Referring Segmentation\nHere we evaluate CaR on the referring segmentation task for\nboth images and videos. Again, our method is an inference-\nonly pipeline built upon pre-trained CLIP models, and does\nnot need training/fine-tuning on any types of annotations.\nFor referring segmentation we only use denseCRF [31] for\npost-processing, and SAM is not involved for all experi-\nments in this section for fair comparison. Please refer to the\nsupplementary material for the implementation details.\nDatasets.\nFollowing [71, 76], we evaluate on Ref-\nCOCO [73], RefCOCO+ [73], and RefCOCOg [42, 46]\nfor the referring image segmentation task.\nImages used\nModels\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntestA testB\nval\ntestA testB\nval\ntest(U) val(G)\nweakly-supervised\nTSEG [57]\n25.95\n-\n-\n22.62\n-\n-\n23.41\n-\n-\nzero-shot\nGL CLIP [76] 26.20 24.94 26.56 27.80 25.64 27.84 33.52 33.67 33.61\nCaR(Ours)\n33.57 35.36 30.51 34.22 36.03 31.02 36.67 36.57 36.63\nTable 7. Comparison to state-of-the-art methods on referring\nimage segmentation in mIoU. CaR is better than all comparison\nmethods in all splits of the three benchmarks.\nin all three datasets are sourced from the MS COCO [36]\ndataset and the masks are paired with descriptive language\nannotations. In RefCOCO+, the use of location word in de-\nscriptions is prohibited, making the task more challenging.\nThere are two separate splits of the RefCOCOg dataset, one\nby UMD (U) [46] and another by Google (G) [64]. Fol-\nlowing previous work, we use the standard mIoU metric.\nApart from referring image segmentation, we also set up a\nnew baseline for zero-shot referring video segmentation\non Ref-DAVIS 2017 [29]. Following [29], we adopt region\nsimilarity J , contour accuracy F, and the averaged score\nJ &F as the metrics for evaluation.\nJ &F\nJ\nF\n30.34\n28.15 32.53\nTable 8.\nResults on\nRef-DAVIS 2017.\nExperimental results.\nTable 7\ncompares\nthe\nperformance\nof\nCaR with other methods on the\nreferring image segmentation tasks\nacross RefCOCO, RefCOCO+, and\nRefCOCOg. Comparing with other\nzero-shot methods,\nour method\nCaR\noutperforms\nGlobal-Local\nCLIP (GL CLIP) on all splits of these benchmarks. The\nperformance gap is most pronounced on RefCOCO\u2019s\ntestA split, where CaR outperforms 10.42 mIoU, and\nsimilarly on RefCOCO+\u2019s testA split, with a lead of\n10.72 mIoU. We also note that GL CLIP [76] uses a\npre-trained segmenter Free-SOLO [63] for mask extraction,\nwhile CaR is built without any pre-trained segmenter. For\nreferring video segmentation, we demonstrate in Table 8\nthat our method achieves 30.34, 28.15 and 32.53 for J &F,\nJ and F on Ref-DAVIS 2017 [29].\nConsidering our\nmethod CaR requires neither fine-tuning nor annotations\nand operates in a zero-shot manner, this performance\nestablishes a strong baseline.\n5. Conclusion\nWe introduce CLIP as RNN (CaR), which preserves the in-\ntactness of the large vocabulary space of pre-trained VLMs,\nby eliminating the fine-tuning process. By constructing a\nrecurrent pipeline with a shared segmenter in the loop, CaR\ncan perform zero-shot semantic and referring segmentation\nwithout any additional training efforts. Experiments show\n8\nthat our CaR outperforms previous state-of-the-art counter-\nparts by a large margin on Pascal VOC, COCO Object, and\nPascal Context on zero-shot semantic segmentation.\nWe\nalso demonstrate that CaR can handle referring expressions\nand segment fine-grained concepts like anime characters\nand landmarks. We hope our work sheds light on future\nresearch in open vocabulary segmentation aiming to further\nexpand the vocabulary space.\nAcknowledgement.\nThis work is done during Shuyang\u2019s\ninternship at Google Research.\nWe would like to thank\nAnurag Arnab, Xingyi Zhou, Huizhong Chen and Neil\nAlldrin at Google Research for their insightful discussion,\nZhongli Ding for donating demo images.\nShuyang Sun\nand Philip Torr are supported by UKRI grants: Turing\nAI Fellowship EP/W002981/1 and EPSRC/MURI grant:\nEP/N019474/1.\nWe would also like to thank the Royal\nAcademy of Engineering and FiveAI.\nReferences\n[1] Nikita Araslanov and Stefan Roth.\nSingle-stage seman-\ntic segmentation from image labels.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4253\u20134262, 2020. 3\n[2] Donghyeon Baek, Youngmin Oh, and Bumsub Ham. Ex-\nploiting a joint embedding space for generalized zero-shot\nsemantic segmentation. In ICCV, 2021. 3\n[3] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick\nP\u00b4erez. Zero-shot semantic segmentation. Advances in Neural\nInformation Processing Systems, 32, 2019. 3\n[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, 2018. 2\n[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 1209\u20131218, 2018. 5, 15\n[6] Kaixin Cai, Pengzhen Ren, Yi Zhu, Hang Xu, Jianzhuang\nLiu, Changlin Li, Guangrun Wang, and Xiaodan Liang.\nMixreorg: Cross-modal mixed patch reorganization is a good\nmask learner for open-world semantic segmentation.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 1196\u20131205, 2023. 2, 6\n[7] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In CVPR, 2018. 3\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 3\n[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 6\n[10] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learn-\ning to generate text-grounded mask for open-world semantic\nsegmentation from only image-text pairs. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11165\u201311174, 2023. 1, 2, 6, 13\n[11] Jun Chen, Deyao Zhu, Guocheng Qian, Bernard Ghanem,\nZhicheng Yan, Chenchen Zhu, Fanyi Xiao, Mohamed Elho-\nseiny, and Sean Chang Culatana. Exploring open-vocabulary\nsemantic segmentation without human labels. arXiv preprint\narXiv:2306.00450, 2023. 2, 6\n[12] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Semantic image segmen-\ntation with deep convolutional nets and fully connected crfs.\nIn ICLR, 2015. 3\n[13] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 3\n[14] Peijie Chen, Qi Li, Saad Biaz, Trung Bui, and Anh Nguyen.\ngscorecam: What objects is clip looking at? In Proceedings\nof the Asian Conference on Computer Vision, pages 1959\u2013\n1975, 2022. 2\n[15] Bowen Cheng, Alexander G Schwing, and Alexander Kir-\nillov. Per-pixel classification is not all you need for semantic\nsegmentation. In NeurIPS, 2021. 3\n[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. CVPR, 2022.\n3\n[17] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. De-\ncoupling zero-shot semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11583\u201311592, 2022. 3\n[18] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 88:303\u2013338, 2010. 5\n[19] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88:303\u2013338, 2010. 2, 13, 15\n[20] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\ning open-vocabulary image segmentation with image-level\nlabels. In European Conference on Computer Vision, pages\n540\u2013557. Springer, 2022. 2\n[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n580\u2013587, 2014. 3\n[22] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation.\narXiv preprint arXiv:2104.13921,\n2021. 2\n[23] Wenbin He, Suphanut Jamonnak, Liang Gou, and Liu Ren.\nClip-s4: Language-guided self-supervised semantic segmen-\ntation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11207\u201311216,\n2023. 2\n9\n[24] Ping Hu, Stan Sclaroff, and Kate Saenko. Uncertainty-aware\nlearning for zero-shot semantic segmentation. Advances in\nNeural Information Processing Systems, 33:21713\u201321724,\n2020. 3\n[25] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nconference on machine learning, pages 4904\u20134916. PMLR,\n2021. 2\n[26] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1780\u20131790, 2021. 2\n[27] Laurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian\nRupprecht. Diffusion models for zero-shot open-vocabulary\nsegmentation. arXiv preprint arXiv:2306.09316, 2023. 2\n[28] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\nhigh quality. arXiv preprint arXiv:2306.01567, 2023. 6, 7,\n15\n[29] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video\nobject segmentation with language referring expressions. In\nComputer Vision\u2013ACCV 2018: 14th Asian Conference on\nComputer Vision, Perth, Australia, December 2\u20136, 2018, Re-\nvised Selected Papers, Part IV 14, pages 123\u2013141. Springer,\n2019. 2, 8\n[30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 5, 6, 7\n[31] Philipp Kr\u00a8ahenb\u00a8uhl and Vladlen Koltun. Efficient inference\nin fully connected crfs with gaussian edge potentials. Ad-\nvances in neural information processing systems, 24, 2011.\n3, 5, 8\n[32] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Rene Ranftl.\nLanguage-driven semantic seg-\nmentation. In International Conference on Learning Rep-\nresentations, 2022. 2\n[33] Peike Li, Yunchao Wei, and Yi Yang. Consistent structural\nrelation learning for zero-shot segmentation. NeurIPS, 2020.\n3\n[34] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-\nhofer, and Kaiming He. Scaling language-image pre-training\nvia masking. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 23390\u2013\n23400, 2023. 2\n[35] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan\nZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana\nMarculescu. Open-vocabulary semantic segmentation with\nmask-adapted clip. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7061\u20137070, 2023. 1, 2, 7, 13, 15, 18\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2, 8, 13\n[37] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke\nLi, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also\nan efficient segmenter: A text-driven approach for weakly\nsupervised semantic segmentation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15305\u201315314, 2023. 2, 3, 4, 5, 6, 7, 8,\n13, 14\n[38] Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu,\nHang Xu, and Xiaodan Liang. Open-world semantic seg-\nmentation via contrasting and clustering vision-language\nembedding. In European Conference on Computer Vision,\npages 275\u2013292. Springer, 2022. 1, 2, 6\n[39] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 1, 2, 15, 18\n[40] Timo L\u00a8uddecke and Alexander Ecker.\nImage segmenta-\ntion using text and image prompts.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7086\u20137096, 2022. 3, 5\n[41] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,\nand Tianrui Li. Segclip: Patch aggregation with learnable\ncenters for open-vocabulary semantic segmentation. In In-\nternational Conference on Machine Learning, pages 23033\u2013\n23044. PMLR, 2023. 1, 2, 6\n[42] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11\u201320, 2016. 8, 13\n[43] M Minderer, A Gritsenko, A Stone, M Neumann, D Weis-\nsenborn, A Dosovitskiy, A Mahendran, A Arnab, M De-\nhghani, Z Shen, et al. Simple open-vocabulary object de-\ntection with vision transformers. arxiv 2022. arXiv preprint\narXiv:2205.06230, 2022. 2\n[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan Yuille.\nThe role of context for object detection and\nsemantic segmentation in the wild. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2014. 2,\n5, 13, 14\n[45] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang,\nAshish Shah, Philip HS Torr, and Ser-Nam Lim.\nOpen\nvocabulary semantic segmentation with patch aligned con-\ntrastive learning.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n19413\u201319423, 2023. 2, 13\n[46] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Mod-\neling context between objects for referring expression under-\nstanding. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11\u201314,\n2016, Proceedings, Part IV 14, pages 792\u2013807. Springer,\n2016. 8, 13\n10\n[47] Jianmo Ni, Gustavo Hern\u00b4andez \u00b4Abrego, Noah Constant, Ji\nMa, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-\nt5: Scalable sentence encoders from pre-trained text-to-text\nmodels. arXiv preprint arXiv:2108.08877, 2021. 7\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2, 6, 13\n[49] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi,\nYinfei Yang, Alexander Toshev, and Jonathon Shlens. Per-\nceptual grouping in vision-language models. arXiv preprint\narXiv:2210.09996, 2022. 1, 2, 6\n[50] Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guan-\ngrun Wang, Jianzhuang Liu, Xiaojun Chang, and Xiaodan\nLiang. Viewco: Discovering text-supervised segmentation\nmasks via multi-view semantic consistency. arXiv preprint\narXiv:2302.10307, 2023. 2, 6\n[51] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learn-\ning affinity from attention: End-to-end weakly-supervised\nsemantic segmentation with transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16846\u201316855, 2022. 3\n[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Li Fei-Fei. Imagenet large scale visual recognition chal-\nlenge. IJCV, 115:211\u2013252, 2015. 13\n[53] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision, pages 618\u2013626,\n2017. 4, 6, 7, 13\n[54] Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Re-\ntrieve and co-segment for zero-shot transfer. Advances in\nNeural Information Processing Systems, 35:33754\u201333767,\n2022. 1, 2, 3, 6, 13\n[55] Aleksandar Shtedritski,\nChristian Rupprecht,\nand An-\ndrea Vedaldi.\nWhat does clip know about a red cir-\ncle? visual prompt engineering for vlms.\narXiv preprint\narXiv:2304.06712, 2023. 5, 7\n[56] Richard Sinkhorn. A relationship between arbitrary positive\nmatrices and doubly stochastic matrices. The annals of math-\nematical statistics, 35(2):876\u2013879, 1964. 14\n[57] Robin Strudel, Ivan Laptev, and Cordelia Schmid. Weakly-\nsupervised segmentation of referring expressions.\narXiv\npreprint arXiv:2205.04725, 2022. 8\n[58] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 2\n[59] Shuyang Sun, Weijun Wang, Qihang Yu, Andrew Howard,\nPhilip Torr, and Liang-Chieh Chen. Remax: Relaxing for\nbetter training on efficient panoptic segmentation.\narXiv\npreprint arXiv:2306.17319, 2023. 3\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[61] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 3\n[62] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash\nFaghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin\nMehta, Mohammad Rastegari, Oncel Tuzel, and Hadi\nPouransari.\nSam-clip: Merging vision foundation models\ntowards semantic and spatial understanding. arXiv preprint\narXiv:2310.15308, 2023. 6, 7\n[63] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,\nAnima Anandkumar, Chunhua Shen, and Jose M Alvarez.\nFreesolo: Learning to segment objects without annotations.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14176\u201314186, 2022.\n8\n[64] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neu-\nral machine translation system: Bridging the gap between\nhuman and machine translation. arXiv:1609.08144, 2016. 8\n[65] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt\nSchiele, and Zeynep Akata.\nSemantic projection network\nfor zero-and few-label semantic segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8256\u20138265, 2019. 3\n[66] Jinheng Xie, Xianxu Hou, Kai Ye, and Linlin Shen. Clims:\nCross language image matching for weakly supervised se-\nmantic segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n4483\u20134492, 2022. 3\n[67] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,\nThomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:\nSemantic segmentation emerges from text supervision. In\nCVPR, 2022. 1, 2, 5, 6, 13\n[68] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu\nQiao, and Weidi Xie. Learning open-vocabulary semantic\nsegmentation models from natural language supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2935\u20132944, 2023. 2,\n6\n[69] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 2\n[70] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid\nBoussaid, and Dan Xu. Multi-class token transformer for\nweakly supervised semantic segmentation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4310\u20134319, 2022. 3\n[71] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip HS Torr. Lavt: Language-aware\nvision transformer for referring image segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18155\u201318165, 2022. 8\n11\n[72] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. arXiv preprint\narXiv:2205.01917, 2022. 2\n[73] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n8, 13\n[74] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV, 2022. 3\n[75] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Convolutions die hard: Open-vocabulary seg-\nmentation with single frozen convolutional clip.\narXiv\npreprint arXiv:2308.02487, 2023. 1, 2, 7\n[76] Seonghoon Yu, Paul Hongsuck Seo, and Jeany Son. Zero-\nshot referring image segmentation with global-local context\nfeatures. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 19456\u2013\n19465, 2023. 8\n[77] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\nLit: Zero-shot transfer with locked-image text tuning.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18123\u201318133, 2022. 2\n[78] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\narXiv preprint arXiv:2303.15343, 2023. 2\n[79] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-\nNeng Hwang, and Jianfeng Gao. Glipv2: Unifying localiza-\ntion and vision-language understanding. Advances in Neural\nInformation Processing Systems, 35:36067\u201336080, 2022. 2\n[80] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan\nLi, Jianwei Yang, and Lei Zhang. A simple framework for\nopen-vocabulary segmentation and detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 1020\u20131031, 2023. 2\n[81] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-\nParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang\nHuang, and Philip HS Torr. Conditional random fields as\nrecurrent neural networks. In ICCV, 2015. 3\n[82] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 2019. 2\n[83] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free\ndense labels from clip. In European Conference on Com-\nputer Vision, pages 696\u2013712. Springer, 2022. 1, 2, 3, 6, 13\n[84] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision. In European Confer-\nence on Computer Vision, pages 350\u2013368. Springer, 2022.\n2\n12\nAppendix\nA. More Experimental Results\nA.1. Quantitative Analysis on Vocabulary Space.\nWe demonstrate that our method CaR has a larger vo-\ncabulary space compared to the methods fine-tuned with\nmask annotations.\nHere we compare our method with\nOVSeg [35], which is fine-tuned on ImageNet [52] and\nCOCO [36] with a pre-trained CLIP backbone for the task\nof referring image segmentation.\nWe believe that refer-\nring expressions (e.g., \u201cthe person in the red shirt\u201d or \u201cthe\ncat in the mirror\u201d) refers to a specific segment using a\nbroad vocabulary. We conduct a comparative analysis be-\ntween a robust open-vocabulary segmentation benchmark,\nOVSeg [35], and CaR, utilizing standard referring image\nsegmentation benchmarks [42, 46, 73]. We note that Ref-\nCOCO and COCO share the same set of images so OVSeg\nfine-tuning on COCO may not be counted as zero-shot on\nRefCOCO. The results, as detailed in Table I, demonstrate\nthat CaR significantly surpasses OVSeg in performance.\nThis disparity in performance suggests that CaR encom-\npasses a more expansive vocabulary space than OVSeg.\nA.2. Evaluation without Background\nFollowing [37], our methodology benefits from using back-\nground queries in CLIP [48] classification to suppress\nfalse positives (predictions not belonging to the input text\nqueries), enhancing segmentation results. Nevertheless, for\nmore comprehensive comparison, we also assess our ap-\nproach using an alternate evaluation setting, previously es-\ntablished, which omits the background class. Consequently,\nless emphasis is placed on object boundaries in this setting.\nWe test our method on two datasets: Pascal VOC [19] with-\nout background (termed VOC-20) and Pascal Context [44]\nwithout background (termed Context-59). This setting tests\nthe ability of various methods to discriminate between dif-\nferent classes. Our method CaR significantly outperforms\nprevious methods on VOC-20 and Context-59, where all\nmethods use the same setting that ignores the background\nclass. We reached out to the PACL authors to confirm that\nthey did not evaluate background.\nB. Implementation Details of CAM\nIn this paper, we integrate two kinds of gradient-based\nCAM, i.e., Grad-CAM [53] and CLIP-ES [37], respectively.\nIntegration with Grad-CAM.\nWhen integrating Grad-\nCAM [53] into our framework, we first extract the image\nand text feature vectors vx = fI(x), vh = fT (h) from\nthe image and text encoder fI(\u00b7), fT (\u00b7) given an image x\nand text queries h.\nWe compute a similarity score be-\ntween the image and text features using the dot product\nModels\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntestA testB\nval\ntestA testB\nval\ntest(U) val(G)\nOVSeg [35] 22.58 19.38 25.63 19.13 15.74 25.30 27.87 29.09 28.31\nCaR(Ours)\n33.57 35.36 30.51 34.22 36.03 31.02 36.67 36.57 36.63\nTable I. Comparison to open-vocabulary methods on referring\nimage segmentation in mIoU. CaR is better than the comparison\nmethod, OVSeg, in all splits of the three benchmarks.\nModel\nIs VLM\npre-trained?\n#Additional\nImages\nw/o Background\nVOC-20 Context-59\nGroupViT\u2020 [67]\n\u00d7\n26M\n79.7\n23.4\nPACL [45]\n\u2713\n40M\n72.3\n50.1\nTCL [10]\n\u2713\n15M\n77.5\n30.3\nMaskCLIP\u2020 [83]\n\u2713\n-\n74.9\n26.4\nReCo\u2020 [54]\n\u2713\n-\n57.5\n22.3\nCaR (Ours)\n\u2713\n-\n91.4\n39.5\nTable J. Comparison with methods under the setting where\nbackground is ignored. We compare CaR with prior work on\nVOC-20, Context-59 in a setting that considers only the fore-\nground pixels (decided by ground truth). Our method shows com-\nparable performance to prior works despite only relying on pre-\ntrained feature extractors. \u2020: numbers are from [10].\ns = softmax(vx\u00b7v\u22ba\nh), where softmax is applied along the di-\nmension of vh. This score s quantifies the alignment (a.k.a\nsimilarity) between the image x and the text h as perceived\nby the CLIP model. Here h contains multiple queries. To\nintegrate Grad-CAM into our framework, we first compute\nthe gradients of the similarity score with respect to the fea-\nture maps of the image encoder by:\ng = \u2202s\n\u2202Ak ,\nwhere Ak represents the feature maps and g denotes the gra-\ndients. Then we compute the neuron importance weights by\naverage-pooling the gradients:\n\u03b1k = 1\nZ\nX\ni\nX\nj\ngk\nij.\nHere, \u03b1k is the neuron importance weights and Z is the\nnumber of pixels in each feature map. We then calculate a\nweighted combination of the feature maps Ak and the neu-\nron importance \u03b1k:\nL = ReLU\n X\nk\n\u03b1kAk\n!\n,\nan activation function ReLU is applied to filter out all neg-\native activations. Specifically, we use the feature map af-\nter the first normalization layer of the last residual block to\ncompute the gradients for CAM.\n13\nIntegration with CLIP-ES.\nIn summary, the CLIP-\nES [37] we adopted is composed of a Grad-CAM and a\nclass-aware attention-based affinity (CAA) module.\nThe\nCAA module is introduced to enhance the vanilla multi-\nhead self-attention (MHSA) for the Vision Transformer\nin CLIP. Given an image, class-wise CAM maps Mc \u2208\nRh\u00d7w for each target class c and the attention weight\nW attn \u2208 Rhw\u00d7hw are obtained from MHSA. For the at-\ntention weight, which is made asymmetric due to the use of\ndifferent projection layers by the query and key, Sinkhorn\nnormalization [56] is applied (alternately applying row-\nnormalization and column-normalization) to convert it into\na doubly stochastic matrix D, and the symmetric affinity\nmatrix A can be derived as follows:\nA = D + DT\n2\n, where D = Sinkhorn(W attn).\n(7)\nFor the CAM map Mc \u2208 Rh\u00d7w, a mask map for each target\nclass c can be obtained by thresholding the CAM with \u03bb.\nThen a set of bounding boxes can be generated based on\nthe thresholded masks. These boxes are used to mask the\naffinity weight matrix A, and then each pixel can be refined\nbased on the masked affinity weight and its semantically\nsimilar pixels. This refinement process can be formalized\nas follows:\nM aff\nc\n= Bc \u2299 At \u00b7 vec(Mc),\n(8)\nwhere Bc \u2208 R1\u00d7hw represents the box mask obtained from\nthe CAM of class c, \u2299 denotes the Hadamard product, t in-\ndicates the number of refining iterations, and vec(\u00b7) denotes\nthe vectorization of a matrix. It should be noted that the\nattention map and CAM are extracted in the same forward\npass. Therefore, CAA refinement is performed in real time\nand does not need an additional stage. Our implementa-\ntion uses the attention maps from the last 8 layers of Vision\nTransformer for CAA.\nC. Implementation Details of Visual Prompts\nThe Python code of visual prompts is shown in Algo-\nrithm B, which is at the end of the supplementary material.\nD. Breakdown of Background Tokens\nWe break down the background tokens into 3 sub-categories\nfor ablation study (experiment results are shown in the main\nmanuscript in Table 6):\n\u2022 Terrestrial:\n[\u2018ground\u2019,\n\u2018land\u2019,\n\u2018grass\u2019,\n\u2018tree\u2019,\n\u2018mountain\u2019, \u2018rock\u2019, \u2018valley\u2019, \u2018earth\u2019,\u2018terrain\u2019, \u2018forest\u2019,\n\u2018bush\u2019, \u2018hill\u2019, \u2018field\u2019, \u2018pasture\u2019, \u2018meadow\u2019, \u2018plateau\u2019,\n\u2018cliff\u2019, \u2018canyon\u2019, \u2018ridge\u2019, \u2018peak\u2019, \u2018plain\u2019, \u2018prairie\u2019, \u2018tun-\ndra\u2019, \u2018savanna\u2019, \u2018steppe\u2019, \u2018crag\u2019, \u2018knoll\u2019, \u2018dune\u2019, \u2018glen\u2019,\n\u2018dale\u2019, \u2018copse\u2019, \u2018thicket\u2019]\n\u2022 Aquatic-Atmospheric: [\u2018sea\u2019, \u2018ocean\u2019, \u2018lake\u2019, \u2018wa-\nter\u2019, \u2018river\u2019, \u2018sky\u2019, \u2018cloud\u2019, \u2018pond\u2019, \u2018stream\u2019, \u2018lagoon\u2019,\n\u2018bay\u2019, \u2018gulf\u2019, \u2018fjord\u2019, \u2018estuary\u2019, \u2018creek\u2019, \u2018brook\u2019, \u2018reser-\nvoir\u2019, \u2018pool\u2019, \u2018spring\u2019, \u2018marsh\u2019, \u2018swamp\u2019, \u2018wetland\u2019,\n\u2018glacier\u2019, \u2018iceberg\u2019, \u2018atmosphere\u2019, \u2018stratosphere\u2019, \u2018mist\u2019,\n\u2018fog\u2019, \u2018rain\u2019, \u2018drizzle\u2019, \u2018hail\u2019, \u2018sleet\u2019, \u2018snow\u2019, \u2018thunder-\nstorm\u2019, \u2018breeze\u2019, \u2018wind\u2019, \u2018gust\u2019, \u2018hurricane\u2019, \u2018tornado\u2019,\n\u2018monsoon\u2019, \u2018cumulus\u2019, \u2018cirrus\u2019, \u2018stratus\u2019, \u2018nimbus\u2019]\n\u2022 Man-Made:\n[ \u2018building\u2019,\n\u2018house\u2019,\n\u2018wall\u2019,\n\u2018road\u2019,\n\u2018street\u2019, \u2018railway\u2019, \u2018railroad\u2019, \u2018bridge\u2019, \u2018edifice\u2019, \u2018struc-\nture\u2019, \u2018apartment\u2019, \u2018condominium\u2019, \u2018skyscraper\u2019, \u2018high-\nway\u2019, \u2018boulevard\u2019, \u2018lane\u2019, \u2018alley\u2019, \u2018byway\u2019, \u2018avenue\u2019,\n\u2018expressway\u2019, \u2018freeway\u2019, \u2018path\u2019, \u2018overpass\u2019, \u2018underpass\u2019,\n\u2018viaduct\u2019, \u2018tunnel\u2019, \u2018footbridge\u2019, \u2018crosswalk\u2019, \u2018culvert\u2019,\n\u2018dam\u2019, \u2018archway\u2019, \u2018causeway\u2019, \u2018plaza\u2019, \u2018square\u2019, \u2018station\u2019,\n\u2018terminal\u2019 ]\nE. Implementation Details of Mutual Back-\nground for Pascal Context\nOur approach involves creating a list of background queries\nto minimize false positive predictions in mask propos-\nals.\nHowever, in the Pascal Context dataset [44], many\n\u201cstuff\u201d categories (e.g. sky, ground, sea) serve as back-\nground queries for \u201cobject\u201d categories (e.g. bird, car, boat).\nDirectly removing these \u2019stuff\u2019 categories from the back-\nground query list and generating object and stuff masks\nusing CAM leads to noisy results due to the lack of false\npositive background suppression. To address this issue, we\nadopt a mutual background strategy. In this method, ob-\nject and stuff masks are produced separately, using object\ncategories as the background queries for stuff masks and\nvice versa. This technique not only maintains the benefit\nof reducing false positives but also significantly enhances\nperformance in the Pascal Context dataset.\nF. Implementation Details of Referring Image\nSegmentation.\nWe use ViT-B/16 as the backbone of the visual encoder\nfor both the mask proposal generator and mask classifier,\nand use circle and background blur as the visual\nprompts for the inputs of mask classifier.\nThe \u03b7, \u03b8, \u03bb\nwere set to (0.5, 0.3, 0.5), (0.2, 0.1, 0.5), (0.5, 0.1, 0.6)\nfor refCOCO, refCOCO+ and refCOCOg, respectively. All\nsplits of these three datasets share the same set of hyper-\nparameters. We note that we do not apply SAM for refer-\nring image segmentation.\n14\nG. More Visualization Results\nG.1.\nVisualization\nresults\non\ndifferent\npost-\nprocessors\nFigures E and F present a comparative visualization of\nthe post-processing techniques Conditional Random Field\n(CRF) and Segment Anything Model (SAM) [28], applied\nto randomly chosen samples from the VOC [19] and COCO\nObject datasets [5]. Initial observations reveal that the ap-\nplication of CRF in CaR facilitates the generation of high-\nquality masks, albeit with notable limitations in delineating\nboundaries between distinct semantic masks. The integra-\ntion of SAM enhances the precision of these masks, yield-\ning clearer and more well-defined boundaries. Neverthe-\nless, the implementation of SAM is not without drawbacks;\nit occasionally leads to false negative predictions, stemming\nfrom mismatches between CaR raw masks and SAM can-\ndidate masks (the matching algorithm is introduced in the\nmain manuscript), or false positive predictions due to the\noverly coarse nature of SAM masks. Meanwhile, we find\nSAM is not very sensitive to stuff classes, so combining\nSAM on Pascal Context will not lead to much increase in\nmIoU.\nG.2. Visualization comparison for different open-\nvocabulary segmentation methods.\nFigure G presents a qualitative comparison of open-\nvocabulary segmentation results for a variety of non-\nstandard subjects, including unique characters, brands, and\nlandmarks. These subjects are notably distinct from com-\nmon objects.\nThe Grounded SAM [39] method demon-\nstrates proficiency in segmenting prominent objects with\nprecision, yet it often misclassify these segments.\nThe\nOVSeg [35] approach also generates low-quality segmenta-\ntion masks and inaccurate class predictions. In contrast, our\nmethodology CaR excels by creating high-quality masks\nwith accurate semantic class predictions, showcasing its su-\nperior capability in the realm of open-vocabulary segmen-\ntation.\nH. Limitation\nThe primary limitation of our method is that its perfor-\nmance is bounded by the pre-trained VLM. For example,\nsince the CLIP model utilizes horizontal flipping augmen-\ntation during training, it becomes challenging for our model\nto successfully distinguish between the concepts \u201cleft\u201d and\n\u201cright\u201d. However, we believe that this issue can be easily\nresolved through adjustments, such as incorporating better\ndata augmentation techniques during the pre-training phase.\nI. Future Potentials and Broader Impact\nCaR is simple, straightforward yet highly efficient.\nTo\nenhance its performance further, we provide two ways to\nexplore. First, incorporating additional trainable modules\nsuch as Feature-Pyramid Networks can significantly im-\nprove its capability in handling small objects.\nSecond,\nsince our method is fundamentally compatible with various\nVision-Language Models (VLMs), it presents an intrigu-\ning opportunity to investigate integration with other VLMs.\nMoreover, CaR can serve the purpose of generating pseudo-\nlabels for other open-vocabulary segmenters.\n15\nImage\nCaR\nCaR+SAM\nGT\nFigure E. Comparison of different post-processors on randomly selected images from PASCAL VOC.\n16\nImage\nCaR\nCaR+SAM\nGT\nFigure F. Comparison of different post-processors on randomly selected images from COCO Object.\n17\nImage\nOVSeg [35]\nGrounded SAM [39]\nOurs\nFigure G. Visualization comparison of different open-vocabulary segmentation methods.\n18\nAlgorithm B Pseudo-code of CLIP as RNN in PyTorch style.\nimport cv2\nimport numpy as np\nimport torch\nfrom scipy.ndimage import binary_fill_holes\ndef apply_visual_prompts(\nimage_array,\nmask,\nvisual_prompt_type=(\u2019circle\u2019),\nvisualize=False,\ncolor=(255, 0, 0),\nthickness=1,\nblur_strength=(15, 15)):\nprompted_image = image_array\ninv_mask = (1 - mask)[:, :, None]\nif \u2019blur\u2019 in visual_prompt_type:\n# blur the part out side the mask\n# Blur the entire image\nblurred = cv2.GaussianBlur(prompted_image,\nblur_strength, 0)\n# Get the sharp region using the mask\nsharp_region = cv2.bitwise_and(\nprompted_image,\nprompted_image,\nmask=np.clip(mask, 0, 255)))\n# Get the blurred region using the inverted mask\nblurred_region = (blurred * inv_mask)\n# Combine the sharp and blurred regions\nprompted_image = cv2.add(sharp_region,\nblurred_region)\nif \u2019gray\u2019 in visual_prompt_type:\ngray = cv2.cvtColor(prompted_image, cv2.\nCOLOR_BGR2GRAY)\n# make gray part 3 channel\ngray = np.stack([gray, gray, gray], axis=-1)\n# Get the sharp region using the mask\ncolor_region = cv2.bitwise_and(\nprompted_image,\nprompted_image,\nmask=np.clip(mask, 0, 255))\n# Get the blurred region using the inverted mask\ninv_mask = 1 - mask\ngray_region = (gray * inv_mask)\n# Combine the sharp and blurred regions\nprompted_image = cv2.add(color_region,\ngray_region)\nif \u2019black\u2019 in visual_prompt_type:\nprompted_image = cv2.bitwise_and(\nprompted_image,\nprompted_image,\nmask=np.clip(mask, 0, 255))\nif \u2019circle\u2019 in visual_prompt_type:\nmask_center, mask_height, mask_width = mask2chw(\nmask)\ncenter_coordinates = (mask_center[1],\nmask_center[0])\naxes_length = (mask_width // 2, mask_height //\n2)\nprompted_image = cv2.ellipse(prompted_image,\ncenter_coordinates,\naxes_length, 0, 0, 360,\ncolor, thickness)\nif \u2019rectangle\u2019 in visual_prompt_type:\nmask_center, mask_height, mask_width = mask2chw(\nmask)\ncenter_coordinates = (mask_center[1],\nmask_center[0])\nstart_point = (mask_center[1] - mask_width //\n2, mask_center[0] - mask_height // 2)\nend_point = (mask_center[1] + mask_width //\n2, mask_center[0] + mask_height // 2)\nprompted_image = cv2.rectangle(prompted_image,\nstart_point,\nend_point,\ncolor, thickness)\nif \u2019contour\u2019 in visual_prompt_type:\n# Find the contours of the mask\n# fill holes for the mask\nmask = binary_fill_holes(mask)\ncontours, hierarchy = cv2.findContours(mask, cv2\n.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n# Draw the contours on the image\nprompted_image = cv2.drawContours(\nprompted_image, contours, -1, color,\nthickness)\nreturn prompted_image\n19\n"
  },
  {
    "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
    "link": "https://arxiv.org/pdf/2312.07910.pdf",
    "upvote": "14",
    "text": "PromptBench\nPromptBench: A Unified Library for Evaluation of Large\nLanguage Models\nKaijie Zhu1,2\u2217, Qinlin Zhao1,3\u2217, Hao Chen4, Jindong Wang1\u2020, Xing Xie1\n1Microsoft Research Asia\n2Institute of Automation, Chinese Academy of Sciences\n3University of Science and Technology of China\n4Carnegie Mellon University\nEditor: My editor\nAbstract\nThe evaluation of large language models (LLMs) is crucial to assess their performance\nand mitigate potential security risks. In this paper, we introduce PromptBench, a uni-\nfied library to evaluate LLMs.\nIt consists of several key components that can be eas-\nily used and extended by researchers: prompt construction, prompt engineering, dataset\nand model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis\ntools. PromptBench is designed as an open, general, and flexible codebase for research\npurpose. It aims to facilitate original study in creating new benchmarks, deploying down-\nstream applications, and designing new evaluation protocols.\nThe code is available at:\nhttps://github.com/microsoft/promptbench and will be continuously supported.\nKeywords:\nEvaluation, large language models, framework\n1 Introduction\nLarge language models (LLMs) have been revolutionizing various aspects of human life and\nsociety unprecedentedly. Evaluation is of paramount importance to understand the true\ncapabilities of LLMs, mitigate potential risks, and eventually, benefit society further (Eisen-\nstein, 2023; Chang et al., 2023). Recent efforts have evaluated LLMs from diverse aspects\n(Liang et al., 2022; Zheng et al., 2023; Li et al., 2023b; Huang et al., 2023). Among the\nfindings, one of the most important is that current LLMs are sensitive to prompts (Wang\net al., 2023b), vulnerable to adversarial prompt attacks (Zhu et al., 2023b), and exposed to\ndata contamination (Willig et al., 2023; Zhou et al., 2023b; Zhu et al., 2023a), which pose\nsevere security and privacy issues (Wang et al., 2023a; Simmons, 2022). On top of that,\nthere have been various prompt learning algorithms developed based on different evaluation\nmetrics, such as BDPL (Diao et al., 2022), GrIPS (Prasad et al., 2022) and Plum (Pan et al.,\n2023). Given the increasing popularity of LLMs, it is indispensable to develop a unified\ncodebase to enable easy, fast, and flexible evaluation.\nThere are existing libraries such as LlamaIndex (Liu, 2022), semantic kernel (Microsoft,\n2023), and LangChain (Chase, 2022). However, LlamaIndex and LangChain enhance LLM\napplications by incorporating databases and various data sources, enabling context-aware\nfunctionalities. Semantic Kernel aims to merge AI services with programming languages\nfor versatile AI app development. Eval-harness (Gao et al., 2023) offers a comprehensive\n\u2217. The first two authors contributed equally. Work done at MSRA.\n\u2020. Corresponding author: Jindong Wang (jindong.wang@microsoft.com).\n1\narXiv:2312.07910v2  [cs.AI]  5 Jan 2024\nPromptBench: A Unified Library for Evaluation of Large Language Models\nPromptBench\nAdversarial\nOOD\nFoundation\nScenario\nProtocol\nBenchmark\nNatural language\nReasoning\nAgent\nInterdisciplinary\nHallucination\nBias\nOthers\nStandard\nDynamic\nSemantic\nPromptBench\naka.ms/promptbench\nPrincipled\n(a) Components\n(b) Supported evaluation research topics\nProtocols\nStandard \nevaluation\nDynamic \nevaluation\nSemantic \nevaluation\nPrincipled \nguarantee for \nevaluation\n\u2026\u2026\nAttacks\nCharacter-level\nDeepWordBug\nTextBugger\nWord-level\nTextFooler\nBertAttack\nSentence-level\nCheckList\nStressTest\nSemantic-level\nHuman-crafted\n\u2026\u2026\nTasks\nSentiment analysis\nGrammar\ncorrectness\nDuplicate sentence\ndetection\nNatural language\ninference\nMulti-task\nknowledge\nReading\ncomprehension\nTranslation\nMath & reasoning\nAlgorithm\n\u2026\u2026\nModels\nOpen-source \nmodels\n(e.g., Llama2, \nT5, Mistral, Yi, \nVicuna, Phi, \nBaichuan\u2026)\nProprietary \nmodels \n(e.g., GPT, PaLM, \nGemini\u2026)\nDatasets\nGLUE\nMMLU\nSQuAD V2\nUN Multi\nIWSLT 2017\nMathematics\nBIG-Bench \nHard\nGSM8K\n\u2026\u2026\nAnalysis\nBenchmark \nresults\nVisualization \nanalysis\nTransferability \nanalysis\nWord \nfrequency \nanalysis\n\u2026\u2026\nPrompts & \nEngineering\nTask-oriented\nRole-oriented\nZero-shot\nFew-shot\nCoT\nLeast2most\nExpert \nprompting\nEmotionPrompt\nGenerated \nknowledge\n\u2026\u2026\nFigure 1: The components and supported research areas of PromptBench.\nframework for evaluating generative language models, but does not support other evalu-\nations such as adversarial prompt attacks, prompt engineering, and dynamic evaluation.\nZeno (Zeno, 2023) is an AI evaluation platform supporting interaction and visualization,\nbut it is not easy to customize. LiteLLM (BerriAI, 2023) implements a unified API call for\ndifferent LLM service prodiders, but it does not support research topics such as robustness,\nprompt engineering, and new protocols. Detailed comparisons are shown in Appendix A.\nConsequently, there is an urgent need for a unified library dedicated to the comprehensive\nevaluation of diverse aspects, particularly for research purposes.\nThis paper introduces PromptBench, a unified python library for evaluating LLMs\nfrom comprehensive dimensions.1\nIt consists of a wide range of LLMs and evaluation\ndatasets, covering diverse tasks, evaluation protocols, adversarial prompt attacks, and\nprompt engineering techniques.\nAs a holistic library, it also supports several analysis\ntools for interpreting the results.\nOur library is designed in a modular fashion, allow-\ning researchers to easily build evaluation pipelines for their own projects. We open-source\nPromptBench with comprehensive documents and tutorials2 to support easy, flexible, and\ncollaborative evaluation.\nWe believe PromptBench could enhance our understanding of\nLLMs\u2019 capabilities and spur new research within the community.\n2 PromptBench\nPromptBench can be easily installed either via pip install promptbench or git clone.\nIn this section, we briefly introduce the components of PromptBench and how to use it to\nbuild an evaluation pipeline for LLMs. An overview of PromptBench is shown in Figure 1.\n2.1 Components\nModels. PromptBench supports both open-source and proprietary LLMs and it is open\nto add more. Currently, it supports Flan-T5-large (Chung et al., 2022), Dolly (Databricks,\n2023), Vicuna (Chiang et al., 2023), Llama2 series (Touvron et al., 2023b), Cerebras-\n1. The name \u201cPromptBench\u201d is the same as (Zhu et al., 2023b) where we only evaluated the robustness\nagainst adversarial prompt attack. We decided to keep the name and heavily extend that project.\n2. https://promptbench.readthedocs.io/en/latest/\n2\nPromptBench\nGPT (Dey et al., 2023), GPT-NEOX (Black et al., 2022), Flan-UL2 (Brain, 2023), phi-\n1.5 (Li et al., 2023c), PaLM2 (Anil et al., 2023), ChatGPT (OpenAI, 2023a), and GPT-\n4 (OpenAI, 2023b).\nPromptBench provides a unified LLMModel interface to allow easy\nconstruction and inference of a model with specified max generating tokens and generating\ntemperature. More details of the supported models are shown in Appendix B.1.\nDatasets and tasks. Currently, PromptBench consists of 12 diverse tasks with 22 pub-\nlic datasets and naturally supports more. It supports: sentiment analysis (SST-2 (Socher\net al., 2013)), grammar correctness (CoLA (Warstadt et al., 2018)), duplicate sentence de-\ntection (QQP (Wang et al., 2017) and MRPC (Dolan and Brockett, 2005)), natural language\ninference (MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), RTE (Wang et al., 2019),\nand WNLI (Levesque et al., 2012)), multi-task knowledge (MMLU dataset (Hendrycks et al.,\n2021)), reading comprehension (SQuAD V2 dataset (Rajpurkar et al., 2018)), translation\n(UN Multi(Eisele and Chen, 2010), IWSLT 2017 (Cettolo et al., 2017)), math problem-\nsolving (Math (Saxton et al., 2019) and GSM8K(Cobbe et al., 2021)), logical reason-\ning (Boolean Expressions (bench authors, 2023)), commonsense reasoning (Commensense\nQA (Talmor et al., 2019), QASC (Khot et al., 2020), NummerSense (Lin et al., 2020),\nDate (bench authors, 2023) and Object Tracking (bench authors, 2023)), symbolic rea-\nsoning (LastLetterConcat (Wei et al., 2023)), algorithm (Valid Parentheses dataset (bench\nauthors, 2023)) Through the unified DatasetLoader interface, it supports easy and cus-\ntomizable dataset loading and processing.\nPrompts and prompt engineering. PromptBench offers a suite of 4 distinct prompt\ntypes, and additionally, users have the flexibility to craft custom prompts using the Prompt\ninterface. Task-oriented prompts are structured to clearly delineate the specific task ex-\npected of the model, whereas role-oriented prompts position the model in a defined role,\nsuch as an expert, advisor, or translator. These prompt categories are adaptable for both\nzero-shot and few-shot learning contexts, offering diverse application possibilities. Moreover,\nPromptBench currently includes 6 prominent prompt engineering methods: few-shot Chain-\nof-Thought (Wei et al., 2023), zero-shot Chain-of-Thought (Kojima et al., 2022), Emotion-\nPrompt (Li et al., 2023a), Expert Prompting (Xu et al., 2023), Generated Knowledge (Liu\net al., 2022), and Least to Most (Zhou et al., 2023a). Our framework is not only equipped\nfor the easy integration of these existing techniques through the prompt engineering mod-\nule but is also actively evolving to encompass a broader spectrum of prompt engineering\nmethods, enhancing its adaptability in varied evaluation scenarios.\nAdversarial prompt attacks.\nTo facilitate the investigation of LLMs\u2019 robustness\non prompts, PromptBench integrates 7 types of adversarial prompt attacks (Zhu et al.,\n2023b): TextBugger (Li et al., 2019), TextFooler (Jin et al., 2019), BertAttack (Li et al.,\n2020), DeepWordBug (Gao et al., 2018), Checklist (Ribeiro et al., 2020), StressTest (Naik\net al., 2018), and semantics (Zhu et al., 2023b). These attacks can be easily called via\nthe prompt attack interface. It also supports the usage of curated adversarial prompts to\nefficiently evaluate the robustness of LLMs.\nDifferent evaluation protocols. By default, PromptBench supports the standard\nprotocol, i.e., the direct inference.\nPromptBench further supports dynamic (Zhu et al.,\n2023a) and semantic (Liu et al., 2023) evaluation protocols by dynamically generating test-\ning data. It is open to integrate more new protocols to avoid data contamination.\n3\nPromptBench: A Unified Library for Evaluation of Large Language Models\nAnalysis tools.\nFinally, PromptBench offers a series of analysis tools to help re-\nsearchers analyze their results. Particularly, it support sweep running to get the benchmark\nresults. Then, attention visualization analysis can be done through the utils interface.\nPromptBench also supports word frequency analysis to analyze the words used in attacks\nas well as defense analysis by integrating word correction tools.\n2.2 Evaluation pipeline\nPromptBench allows easy construction of an evaluation pipeline via four steps. Firstly,\nspecify task and then load dataset via pb.DatasetLoader. PromptBench offers a stream-\nlined one-line API for loading the desired dataset. Secondly, users can customize LLMs\nusing the pb.LLMModel, which provides integrated inference pipelines compatible with most\nLLMs implemented in Huggingface. Thirdly, the prompt for the specified dataset and task\nis defined via pb.Prompt. Users have the option to input a list of prompts for evaluation\nand performance comparison. In cases where no prompts are supplied, our default prompts\nfor the dataset are utilized. Finally, the pipeline requires the definition of input and out-\nput processing functions via class InputProcess and class OutputProcess defined in\npb.utils.dataprocess, as well as the evaluation function via pb.metrics. The detailed\nintroduction of the components are shown in Appendix B.\n2.3 Supported research topics\nPromptBench is designed mainly for research purpose, thus it is easy to customize for dif-\nferent topics. As shown in Figure 1(b), it supports different evaluation topics from the\nresearch community including benchmarks, scenarios, and protocols. In benchmarks re-\nsearch, it supports standard natural language understanding, natural language generation,\nand reasoning tasks.\nIt can also be extended to support research on AI agent and in-\nterdisciplinary study. In scenario research, it supports adversarial and out-of-distribution\nevaluation, and can also support other topics such as hallucination and bias by changing\nthe metrics and DatasetLoader interface. In protocol, it naturally supports standard and\ndynamic evaluation, and can further be verified by including measurement theory.\nPromptBench offers three leaderboards to allow easy comparison: adversarial prompt\nattack, prompt engineering, and dynamic evaluation, as shown in Appendix C. Researchers\nare welcome to submit new results to our platform. Extensibility is shown in Appendix D\nthat allows convenient extension of the framework.\n3 Conclusion\nWe presented PromptBench, a unified framework for LLMs evaluation.\nThe library is\ndesigned in modular fashion to allow users build evaluation pipeline by composing different\nmodels, tasks, and prompts. It also facilitates several research directions such as prompt\nengineering, adversarial prompt attacks, and dynamic evaluation. We treat PromptBench as\nthe first step towards assessing the true capabilities and exploring the boundaries of current\nLLMs, and believe the benchmark and analysis results from our library could shed lights\non designing more robust and human-aligned models. PromptBench is a long-term project\nand will be actively maintained. We welcome contributions from all potential contributors.\n4\nPromptBench\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\nBIG bench authors. Beyond the imitation game: Quantifying and extrapolating the ca-\npabilities of language models. Transactions on Machine Learning Research, 2023. ISSN\n2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\nBerriAI. https://docs.litellm.ai/, 2023.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai\nPrashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel\nWeinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL\nhttps://arxiv.org/abs/2204.06745.\nGoogle Brain. A new open source flan 20b with ul2, 2023. URL https://www.yitay.net/\nblog/flan-ul2-20b.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00a8uker, Kat-\nsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017\nevaluation campaign.\nIn Proceedings of the 14th International Conference on Spoken\nLanguage Translation, pages 2\u201314, Tokyo, Japan, December 14-15 2017. International\nWorkshop on Spoken Language Translation. URL https://aclanthology.org/2017.\niwslt-1.1.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang,\nXiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al.\nA survey on evaluation of large\nlanguage models. arXiv preprint arXiv:2307.03109, 2023.\nHarrison Chase. Langchain. https://github.com/langchain-ai/langchain, 2022. Date\nreleased: 2022-10-17.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane\nGu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang,\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu,\nSlav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V.\nLe, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https:\n//arxiv.org/abs/2210.11416.\n5\nPromptBench: A Unified Library for Evaluation of Large Language Models\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nContributors.\npromptfoo:\ntest your llm app, 2023a.\nURL https://github.com/\npromptfoo/promptfoo.\nEvals Contributors. Openai evals, 2023b. URL https://github.com/openai/evals.\nOpenCompass Contributors. Opencompass: A universal evaluation platform for foundation\nmodels. https://github.com/open-compass/opencompass, 2023c.\nDatabricks.\nHello\ndolly:\nDemocratizing\nthe\nmagic\nof\nchatgpt\nwith\nopen\nmodels,\n2023.\nURL\nhttps://www.databricks.com/blog/2023/03/24/\nhello-dolly-democratizing-magic-chatgpt-open-models.html.\nNolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu\nPathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language\nmodels trained on the cerebras wafer-scale cluster, 2023.\nShizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, and Tong\nZhang.\nBlack-box prompt learning for pre-trained language models.\narXiv preprint\narXiv:2201.08531, 2022.\nWilliam B. Dolan and Chris Brockett.\nAutomatically constructing a corpus of senten-\ntial paraphrases. In Proceedings of the Third International Workshop on Paraphrasing\n(IWP2005), 2005. URL https://aclanthology.org/I05-5002.\nAndreas Eisele and Yu Chen. MultiUN: A multilingual corpus from united nation docu-\nments. In Proceedings of the Seventh International Conference on Language Resources\nand Evaluation (LREC\u201910), Valletta, Malta, May 2010. European Language Resources\nAssociation (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/\n686_Paper.pdf.\nMichael Eisenstein. A test of artificial intelligence. Nature Outlook: Robotics and artificial\nintelligence, 2023.\nJ. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text se-\nquences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops\n(SPW), pages 50\u201356, May 2018. doi: 10.1109/SPW.2018.00016.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi,\nCharles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle\nMcDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey\nSchoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.\nURL https://zenodo.org/records/10256836.\nGoogle. https://deepmind.google/technologies/gemini/#introduction, 2023.\n6\nPromptBench\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=d7KBjmI3GmQ.\nJordan Hoffmann et al. Training compute-optimal large language models, 2022.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,\nJunteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.\nC-eval:\nA multi-\nlevel multi-discipline chinese evaluation suite for foundation models.\narXiv preprint\narXiv:2305.08322, 2023.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-\ncile Saulnier, L\u00b4elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00b4ee Lacroix, and William El Sayed. Mistral 7b,\n2023.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural\nlanguage attack on text classification and entailment. arXiv preprint arXiv:1907.11932,\n2019.\nTushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc:\nA dataset for question answering via sentence composition, 2020.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Sys-\ntems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf.\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\nIn Thirteenth international conference on the principles of knowledge representation and\nreasoning, 2012.\nCheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo,\nQiang Yang, and Xing Xie. Large language models understand and can be enhanced by\nemotional stimuli, 2023a.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger: Generating adver-\nsarial text against real-world applications. In Proceedings 2019 Network and Distributed\nSystem Security Symposium. Internet Society, 2019. doi: 10.14722/ndss.2019.23138. URL\nhttps://doi.org/10.14722%2Fndss.2019.23138.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK:\nAdversarial attack against BERT using BERT. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 6193\u20136202, On-\nline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.500. URL https://aclanthology.org/2020.emnlp-main.500.\n7\nPromptBench: A Unified Library for Evaluation of Large Language Models\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B Hashimoto.\nAlpacaeval:\nAn automatic evaluator of\ninstruction-following models. Github repository, 2023b.\nYuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and\nYin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint\narXiv:2309.05463, 2023c.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-\nsunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\nevaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! nu-\nmersense: Probing numerical commonsense knowledge of pre-trained language models,\n2020.\nJerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi,\nand Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning,\n2022.\nYachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, and Xing Xie. Meta semantic\ntemplate for evaluation of large language models. arXiv preprint arXiv:2310.01448, 2023.\nMicrosoft. Semantic kernel. https://github.com/microsoft/semantic-kernel, 2023.\nMixtral. Mixtral, 2023. URL https://mistral.ai/news/mixtral-of-experts/.\nAakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neu-\nbig. Stress test evaluation for natural language inference. In ACL, pages 2340\u20132353,\nSanta Fe, New Mexico, USA, August 2018. Association for Computational Linguistics.\nURL https://aclanthology.org/C18-1198.\nOpenAI. https://chat.openai.com.chat, 2023a.\nOpenAI. Gpt-4 technical report, 2023b.\nRui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong\nZhang. Plum: Prompt learning using metaheuristic. arXiv preprint arXiv:2311.08364,\n2023.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.\nGrips:\nGradient-free,\nedit-based instruction search for prompting large language models.\narXiv preprint\narXiv:2203.07281, 2022.\nPranav Rajpurkar, Robin Jia, and Percy Liang.\nKnow what you don\u2019t know: Unan-\nswerable questions for SQuAD.\nIn ACL, pages 784\u2013789, Melbourne, Australia, July\n2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/P18-2124.\nURL\nhttps://aclanthology.org/P18-2124.\n8\nPromptBench\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond ac-\ncuracy: Behavioral testing of NLP models with CheckList. In ACL, pages 4902\u20134912,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.442. URL https://aclanthology.org/2020.acl-main.442.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathemat-\nical reasoning abilities of neural models. In ICLR, 2019. URL https://openreview.\nnet/forum?id=H1gR5iR5FX.\nGabriel Simmons. Moral mimicry: Large language models produce moral rationalizations\ntailored to political identity. arXiv preprint arXiv:2209.12106, 2022.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over\na sentiment treebank. In EMNLP, pages 1631\u20131642, Seattle, Washington, USA, Octo-\nber 2013. Association for Computational Linguistics. URL https://www.aclweb.org/\nanthology/D13-1170.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:\nA question answering challenge targeting commonsense knowledge, 2019.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bow-\nman. GLUE: A multi-task benchmark and analysis platform for natural language under-\nstanding. 2019. In the Proceedings of ICLR.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nBoxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang,\nChejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A compre-\nhensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698,\n2023a.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi\nYang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An\n9\nPromptBench: A Unified Library for Evaluation of Large Language Models\nadversarial and out-of-distribution perspective.\nIn International conference on learn-\ning representations (ICLR) workshop on Trustworthy and Reliable Large-Scale Machine\nLearning Models, 2023b.\nZhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for\nnatural language sentences, 2017.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\njudgments. arXiv preprint arXiv:1805.12471, 2018.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2023.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In NAACL HLT, pages 1112\u20131122. Association\nfor Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\nMoritz Willig, Matej Zecevic, Devendra Singh Dhami, and Kristian Kersting. Causal par-\nrots: Large language models may talk causality but are not causal.\nTransactions on\nmachine learning research (TMLR), 8, 2023.\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and\nZhendong Mao. Expertprompting: Instructing large language models to be distinguished\nexperts, 2023.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,\nDa Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai,\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming\nJi, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao\nMa, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao\nZhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan\nWang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu\nLi, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan\n2: Open large-scale language models, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and\nKarthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language\nmodels. arXiv preprint arXiv:2305.10601, 2023.\nYi. Yi, 2023. URL https://github.com/01-ai/Yi.\nZeno. https://zenoml.com/, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n10\nPromptBench\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompt-\ning enables complex reasoning in large language models, 2023a.\nKun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai\nLin, Ji-Rong Wen, and Jiawei Han.\nDon\u2019t make your llm an evaluation benchmark\ncheater. arXiv preprint arXiv:2311.01964, 2023b.\nKaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie.\nDyval: Graph-informed dynamic evaluation of large language models. arXiv preprint\narXiv:2309.17167, 2023a.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi\nYang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards eval-\nuating the robustness of large language models on adversarial prompts. arXiv preprint\narXiv:2306.04528, 2023b.\n11\nPromptBench: A Unified Library for Evaluation of Large Language Models\nAppendix A. Comparison with Related Code Libraries\nTable 1: Comparison with related code libraries.\nLibrary\nPurpose\nCustomization\nFunctions\nOpenAI Evals\n(Contributors, 2023b)\nA framework for evaluating\nLLMs or systems built using\nLLMs.\nmodel,\ndataset,\nprompt, eval\nEvaluation pipelines\nBenchmarks\nOpenCompass\n(Contributors, 2023c)\nOne-stop platform for large\nmodel evaluation, aiming to\nprovide a transparent bench-\nmark for foundation models.\nmodel, dataset\nEvaluation pipelines\nBenchmarks\nLeaderboards\npromptfoo\n(Contributors, 2023a)\nA framework for evaluating\nprompts and large language\nmodels.\nmodel,\ndataset,\nprompt, eval\nEvaluation pipelines\nLM Evaluation Harness\n(Gao et al., 2023)\nA framework for evaluation of\nautoregressive LMs.\nmodel,\ndataset,\nprompt, eval\nEvaluation pipelines\nBenchmarks\nLeaderboards\nHELM\n(Liang et al., 2022)\nHolistic Evaluation of Lan-\nguage Models.\nmodel\nEvaluation pipelines\nBenchmark\nLeaderboard\nPromptBench (Ours)\nResearch-focused\nevaluation\ntoolkit.\nmodel,\ndataset,\nprompt,\nprompt\nengineering, eval\nEvaluation pipelines\nPrompt Engineering\nPrompt attacks\nDynamic evaluation\nLeaderboard\nAppendix B. Details of PromptBench\nB.1 Models\nIn this section, we list the LLMs implemented in PromptBench.\nOpen-source LLMs:\n\u2022 Flan-T5-large (Chung et al., 2022): Google\u2019s Flan-T5-large, a variation of the Text-\nto-Text Transfer Transformer (T5).\n\u2022 Dolly-6B (Databricks, 2023): The Dolly-6B model, developed by Databricks, is a\n6-billion parameter causal language model. It is an extension of EleutherAI\u2019s GPT-J\n(Wang and Komatsuzaki, 2021), further refined with Stanford\u2019s Alpaca (Taori et al.,\n2023) dataset comprising 52K question/answer pairs.\n\u2022 Vicuna series (Chiang et al., 2023): Developed from the LLaMA-13B base model,\nVicuna-13B integrates over 70K user-shared conversations from ShareGPT.com, leverag-\ning public APIs for data acquisition.\n\u2022 Cerebras series (Dey et al., 2023): Modeled on the GPT-3 architecture, Cerebras-\n13B is part of the Cerebras-GPT series, trained according to Chinchilla scaling laws\n(Hoffmann et al., 2022) to optimize computational efficiency.\n\u2022 Llama2 series (Touvron et al., 2023a): Engineered by Meta AI\u2019s FAIR team, the\nLlama2 model is an autoregressive language model adopting the transformer architecture.\n12\nPromptBench\n\u2022 GPT-NEOX-20B (Black et al., 2022): This variant, part of the extensive GPT\nmodel series, features 20 billion parameters, exemplifying large-scale language model\nimplementation.\n\u2022 Flan-UL2 (Brain, 2023): Flan-UL2, an encoder-decoder model, is grounded in the T5\narchitecture and enhanced with Flan prompt tuning and dataset techniques.\n\u2022 phi-1.5 and phi-2 (Li et al., 2023c): phi-1.5 is an LLM with 1.3 billion parameters,\nbuilds upon the dataset used for phi-1 with the addition of diverse NLP synthetic texts.\n\u2022 Mistral 7B (Jiang et al., 2023): Mistral 7B is trained by Mistral AI team. It excels\nin tasks like reasoning, mathematics, and code generation. It uses grouped-query atten-\ntion for faster inference and sliding window attention for efficient handling of sequences.\nThere\u2019s also an instruction-following version, Mistral 7B-Instruct.\n\u2022 Mixtral8x7B (Mixtral, 2023): Engineering by Mistral AI team, this model is a high-\nquality sparse mixture of experts model (SMoE) with open weights.\nThere\u2019s also an\ninstruction-following version, Mixtral 8x7B Instruct.\n\u2022 Baichuan2 series (Yang et al., 2023): Baichuan2 is developed by Baichuan Intelligent.\nTrained on 2.6 trillion high-quality tokens, it achieves the best results in its size class on\nmultiple authoritative benchmarks in Chinese, English, and multilingual general and\ndomain-specific tasks.\n\u2022 Yi series (Yi, 2023): Developed by 01.AI, the Yi series are next-generation open-source\nlarge language models.\nTrained on a 3T multilingual corpus, they excel in language\nunderstanding, commonsense reasoning, and reading comprehension.\nProprietary LLMs:\n\u2022 ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): OpenAI\u2019s ChatGPT\nand GPT-4 are advanced iterations of the GPT series, with ChatGPT tailored for inter-\nactive tasks and GPT-4 being the most proficient in the series.\n\u2022 PaLM 2 (Anil et al., 2023): PaLM 2 is an advanced language model that excels in\nmultilingual and reasoning capabilities, offering greater computational efficiency than its\npredecessor, PaLM. This Transformer-based model enhances performance across various\nmodel sizes in English, multilingual tasks, and reasoning challenges.\n\u2022 Gemini (Google, 2023): Gemini is Google\u2019s new family of LLMs consisting of Gemini\nNano, Gemini Pro, and Gemini Ultra. Gemini Pro is released via API service.\nB.2 Tasks and Datasets\n\u2022 GLUE (Wang et al., 2019): The GLUE benchmark (General Language Understanding\nEvaluation) offers a suite of tasks to evaluate the capability of NLP models in understand-\ning language. For this research, we employed 8 specific tasks: Sentiment Analysis (SST-2\n(Socher et al., 2013)), Grammar Correctness (CoLA (Warstadt et al., 2018)), Identifying\nDuplicate Sentences (QQP (Wang et al., 2017), MRPC (Dolan and Brockett, 2005)), and\nvarious Natural Language Inference tasks (MNLI (Williams et al., 2018), QNLI (Wang\net al., 2019), RTE (Wang et al., 2019), WNLI (Levesque et al., 2012)).\n13\nPromptBench: A Unified Library for Evaluation of Large Language Models\n\u2022 MMLU (Hendrycks et al., 2021): The MMLU dataset tests the broad knowledge\nand problem-solving skills of large language models through 57 tasks with multiple-choice\nquestions in fields like mathematics, history, and computer science. It is a comprehensive\nmultitask benchmark.\n\u2022 SQuAD V2 (Rajpurkar et al., 2018): The SQuAD v2 dataset is pivotal in training\nand assessing NLP models for reading comprehension. It builds upon the original SQuAD\nby adding unanswerable questions, making it more challenging.\nModels must either\nidentify the correct answer in the text or recognize questions as unanswerable.\n\u2022 UN Multi (Eisele and Chen, 2010): Comprising texts in the six official United\nNations languages, the Multi UN dataset is a vast parallel corpus from UN documents.\nHowever, its focus on formal texts may restrict its use in informal or conversational\nlanguage contexts.\n\u2022 IWSLT 2017 (Cettolo et al., 2017): Designed for spoken language translation system\nevaluation, the IWSLT 2017 dataset includes multilingual, multi-domain text data, pri-\nmarily from the TED Talks Open Translation Project. It encompasses numerous language\npairs, providing a rich resource for translation tasks.\n\u2022 Math (Saxton et al., 2019): The DeepMind Mathematics Dataset assesses AI mod-\nels\u2019 mathematical reasoning by posing a wide array of math problems, from algebra to\ncalculus. It tests the models\u2019 understanding and logical reasoning in mathematics.\n\u2022 BIG-Bench (bench authors, 2023): BIG-bench is a collaborative benchmark designed\nto evaluate the capabilities of large language models and predict their future potential.\nIt consists of over 200 tasks, contributed by 444 authors from 132 institutions, covering\na wide range of topics like linguistics, math, common-sense reasoning, and more. These\ntasks are intended to probe areas believed to be beyond the current capabilities of LMs.\n\u2022 GSM8K (Cobbe et al., 2021): The GSM8K dataset is a collection of 8.5K high-\nquality, linguistically diverse grade school math word problems. It was created by human\nproblem writers and is divided into 7.5K training problems and 1K test problems. These\nproblems, which require 2 to 8 steps to solve, primarily involve basic arithmetic operations\nand are designed to be solvable by a bright middle school student.\n\u2022 CommonsenseQA (Talmor et al., 2019): The CommonsenseQA dataset is a chal-\nlenging commonsense question-answering dataset. It comprises 12,247 questions with 5\nmultiple-choice answers each.\n\u2022 QASC (Khot et al., 2020): QASC (Question Answering via Sentence Composition)\nis a specialized collection designed for question-answering tasks with a focus on sentence\ncomposition. It comprises 9,980 eight-way multiple-choice questions about grade school\nscience, divided into 8,134 for training, 926 for development, and 920 for testing .(In\nour evaluation, we use development part.) The dataset is notable for its emphasis on\nmulti-hop reasoning, requiring the retrieval and composition of facts from a broad corpus\nto answer each question.\n\u2022 NummerSense (Lin et al., 2020): NumerSense is a unique numerical commonsense\nreasoning probing task, featuring a diagnostic dataset with 3,145 masked-word-prediction\nprobes. This dataset has applications in tasks such as knowledge base completion and\nopen-domain question answering.\n14\nPromptBench\nB.3 Evaluation protocols\nDyVal (Zhu et al., 2023a) is an approach for dynamic evaluation of LLMs by creating\ncomplexity-tailored evaluation samples on-the-fly, as opposed to relying on static bench-\nmarks.\nDyVal synthesized seven distinct reasoning tasks, including: (1) Mathematics,\nfocusing on arithmetic calculations and linear equation solving; (2) Logical Reasoning, in-\nvolving boolean, deductive, and abductive logic; and (3) Algorithmic Analysis, covering\nreachability and the maximum sum path problem. MSTemp (Liu et al., 2023) stands for\nthe semantic evalaution protocol which generate out-of-distribution samples by relying on\nevlauator LLMs and word replacement.\nB.4 Prompts\nB.4.1 Prompts\nOur study examines four prompt categories, differentiated by their intended function and\nthe required number of labeled samples.\nTask-oriented prompts are designed to clearly\ndefine the model\u2019s task, prompting it to generate outputs relevant to the task using its\ninherent pre-training knowledge.\nIn contrast, role-oriented prompts position the model\nas a particular entity, like an expert, advisor, or translator, thereby subtly guiding the\nexpected output format and behavior through the assumed role. Both categories can be\nadapted for zero-shot and few-shot learning contexts. We randomly choose three training\nset examples for each task to form the few shot examples. Examples of various prompt\ntypes are illustrated in Table 2.\nTable 2: Examples of 4 types of prompts.\nZero\nshot\nTask\noriented\nDetermine if the given pair of statements can be considered the same by\nresponding with \u2019equivalent\u2019 or \u2019not equivalent\u2019.\nRole\noriented\nAs an instrument for question comparison evaluation, consider the questions\nand determine if their meaning is the same, responding with \u2019equivalent\u2019 for\nsimilar questions or \u2019not equivalent\u2019 for different questions.\nFew\nshot\nTask\noriented\nReview the sentence below and identify whether its grammar is \u2019Acceptable\u2019\nor \u2019Unacceptable\u2019:\nHere are three examples.\nSentence:\nOur friends won\u2019t\nbuy this analysis, let alone the next one we propose.\nAnswer:\nacceptable.\nSentence:\nOne more pseudo generalization and I\u2019m giving up.\nAnswer:\nacceptable.\nSentence:\nThey drank the pub.\nAnswer:\nunacceptable.\nRole\noriented\nFunctioning as a grammar evaluation tool, analyze the given sentence and decide\nif it is grammatically correct, responding with \u2019acceptable\u2019 or \u2019unacceptable\u2019:\nHere are three examples.\nSentence:\nOur friends won\u2019t buy this analysis,\nlet alone the next one we propose.\nAnswer:\nacceptable.\nSentence:\nOne more\npseudo generalization and I\u2019m giving up.\nAnswer:\nacceptable.\nSentence:\nThey\ndrank the pub.\nAnswer:\nunacceptable.\nB.4.2 Prompt Engineering\nPrompt engineering is a process of structuring and optimizing prompts to efficiently use AI\nmodels. Methods in prompt engineering , such as chain-of-thought (Wei et al., 2023), gen-\n15\nPromptBench: A Unified Library for Evaluation of Large Language Models\nerated knowledge prompting (Liu et al., 2022) and so on, help improve reasoning ability and\ntask performance of AI models. We implement 6 prominent prompt engineering methods:\n\u2022 Chain-of-Thought (Wei et al., 2023): This method involves breaking down complex,\nmulti-step problems into smaller, intermediate steps, enabling Models to tackle more intri-\ncate reasoning tasks. Chain-of-Thought differs from standard few-shot prompting by not\njust providing questions and answers but prompting the model to produce intermediate\nreasoning steps before arriving at the final answer.\n\u2022 Zero-Shot Chain-of-Thought (Kojima et al., 2022): Zero-Shot Chain of Thought\nimproves Chain of Thought by simplifying the prompting process. The key innovation in\nZero-Shot Chain-of-Thought is appending the phrase \u201cLet\u2019s think step by step\u201d to\nthe end of a question.\n\u2022 EmotionPrompt (Li et al., 2023a): Drawing inspiration from psychology and social\nscience theories about human emotional intelligence, this method adds emotional stimuli\nto origin prompts. For example: \u201cThis is very important to my career.\u201d\n\u2022 Expert Prompting (Xu et al., 2023): The key idea is to let model be an expert in\nrole playing. To generate the expert identity, we first provide several instruction-expert\npair exemplars, then the model generates an expert identity of this question. Finally, we\nask the model to answer the instruction conditioned on expert identity.\n\u2022 Generated Knowledge (Liu et al., 2022): Generated Knowledge Prompting is a\nmethod where a model first generates knowledge and then uses this generated informa-\ntion as additional input to answer questions. It enhances commonsense reasoning in AI\nwithout requiring task-specific knowledge integration or a structured knowledge base.\n\u2022 Least to Most (Zhou et al., 2023a): Least to Most breaks down a complex problem\ninto a series of simpler subproblems and then solves them in sequence. The key idea is\nto solve each subproblem by using the answers to previously solved subproblems. This\nmethod is particularly useful for tasks that require solving problems harder than the\nexemplars shown in the prompts.\nNote that there are plenty of prompt engineering techniques and we tried our best to\ninclude those general techniques instead of specific prompt engineering techniques such as\nTree of Thoughts (Yao et al., 2023) that requires specific prompt design and decomposition\nof each problem.\nB.4.3 Adversarial Prompt Attacks\nAdversarial prompt attacks, as proposed by Zhu et al. (2023b), aims to simulate potential\ndisturbances that could naturally arise in practical scenarios. The proposed prompt attacks\nare intended to resemble common user errors or expressions, as users often make various\nmistakes when inputting prompts, such as typos, diverse word choices, and different sentence\nconstructions. The prompt attacks encompass four distinct levels:\n\u2022 Character-level:\nTechniques such as TextBugger (Li et al., 2019) and DeepWord-\nBug (Gao et al., 2018) are employed.\nThese methods introduce errors or typos into\nwords by altering characters.\n16\nPromptBench\n\u2022 Word-level: Attacks like BertAttack (Li et al., 2020) and TextFooler (Jin et al., 2019)\nare utilized. They focus on substituting words with their synonyms or contextually similar\nalternatives.\n\u2022 Sentence-level: StressTest (Naik et al., 2018) and CheckList (Ribeiro et al., 2020) are\napplied. These attacks add irrelevant or redundant sentences to prompts.\n\u2022 Semantic-level: To simulate the linguistic styles of different global regions.\nB.5 Pipeline\nThe full pipeline of using PromptBench for evaluation is shown in Figure 2.\nStep 1. Load dataset\nStep 2. Specify model\nStep 3. Define prompts\nStep 4. Define Process function\nprint('All supported models: ')\nprint(pb.SUPPORTED_MODELS)\n# load a model, flan-t5-large, for instance.\nmodel = pb.LLMModel(model='google/flan-t5-large', max_new_tokens=10)\nprint('All supported datasets: ' )\nprint(pb.SUPPORTED_DATASETS)\n# load a dataset, sst2, for instance.\ndataset = pb.DatasetLoader.load_dataset(\"sst2\" )\n# Prompt API supports a list, so you can pass multiple prompts at once.\nprompts = pb.Prompt([\"Classify the sentence as positive or negative: {content} \",\n \n \n \n     \"Determine the emotion of the following sentence as \n \n \n \n       positive or negative: {content} \" ])\n# process input\ninput_text = pb.InputProcess.basic_format()\n# process output\npred = pb.OutputProcess.cls()\nscore = pb.Eval.compute_cls_accuracy(preds, labels)\nFigure 2: A pipeline for evaluation of LLMs.\nAppendix C. Benchmark Results\nC.1 Adversarial prompt robustness\nThe partial results of the robustness of different models on a range of tasks are presented\nin Figure 3. All models exhibit vulnerability to adversarial prompts, with ChatGPT and\nGPT-4 demonstrating the strongest robustness.\nC.2 Prompt engineering\nPrompt engineering results are shown in Figure 4. Most methods are effective for special\nfields, so these methods can not surpass the baseline in every dataset.\n17\nPromptBench: A Unified Library for Evaluation of Large Language Models\nSST-2\nCoLA\nQQP\nMPRC\nMNLI\nQNLI\nRTE\nWNLI\nMMLU\nSQuAD v2\nIWSLT\nUN Multi\nMath\nAvg\nDatasets\n0\n20\n40\n60\n80\nAverage PDR (+5)\nT5-Large\nVicuna-13b-v1.3\nLlama2-13b-chat\nUL2\nChatGPT\nGPT-4\nFigure 3: Adversarial prompt robustness results.\nGSM8K\nCSQA\nObject Tracking\nDate\nDatasets\n0.0\n20.0\n40.0\n60.0\n80.0\nAccuracy\nGPT-3.5-Turbo\nGSM8K\nCSQA\nObject Tracking\nDate\nDatasets\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nAccuracy\nGPT-4\nBaseline\nCoT\nZero-Shot CoT\nEmotionPrompt\nExpert Prompting\nFigure 4: Comparison among different prompt engineering techniques.\nArithmetic\nLinear Equation\nBoolean Logic\nDeductive Logic\nAbductive Logic\nReachability\nMax Sum Path\nDatasets\n0\n20\n40\n60\n80\n100\nAccuracy\nVicuna-13B v1.3\nLLaMA2-13B Chat\nChatGPT\nGPT4\nFigure 5: DyVal results.\n18\nPromptBench\nC.3 Dynamic evaluation\nFigure 5 illustrates the outcomes of dynamic evaluations across various models and tasks.\nGPT-4 outperforms its counterparts significantly, yet there remains potential for enhance-\nment in the performance of linear equation, abductive logic, and max sum path task.\nAppendix D. Extensibility\nEach module in PromptBench can be easily extended. In the following, we provide basic\nguidelines for customizing your own datasets, models, prompt engineering methods, and\nevaluation metrics.\nD.1 Add new datasets\nAdding new datasets involves two steps:\n1. Implementing a New Dataset Class: Datasets are supposed to be implemented in\ndataload/dataset.py and inherit from the Dataset class. For your custom dataset,\nimplement the\ninit\nmethod to load your dataset. We recommend organizing your\ndata samples as dictionaries to facilitate the input process.\n2. Adding an Interface: After customizing the dataset class, register it in the DataLoader\nclass within dataload.py.\nD.2 Add new models\nSimilar to adding new datasets, the addition of new models also consists of two steps.\n1. Implementing a New Model Class: Models should be implemented in models/model.py,\ninheriting from the LLMModel class. In your customized model, you should implement\nself.tokenizer and self.model. You may also customize your own predict func-\ntion for inference. If the predict function is not customized, the default predict\nfunction inherited from LLMModel will be used.\n2. Adding an Interface: After customizing the model class, register it in the create model\nfunction within the class LLMModel and MODEL LIST dictionary in\ninit .py.\nD.3 Add new prompt engineering methods\nAdding new methods in prompt engineering is similar to steps of C.1 and C.2.\n1. Implementing a New Methods Class: Methods should be implemented in\nprompt engineering Module. Firstly, create a new .py file for your methods. Then\nimplement two functions:\ninit\nand query. For unified management, two points\nneed be noticed: 1. all methods should inherits from Base class that has common\ncode for prompt engineering methods. 2. prompts used in methods should be stored\nin prompts/method oriented.py.\n2. Adding an Interface: After implementing a new methods, register it in the METHOD MAP\nthat is used to map method names to their corresponding class.\n19\nPromptBench: A Unified Library for Evaluation of Large Language Models\nD.4 Add new metrics and input/output process functions\nNew evaluation metrics should be implemented as static functions in class Eval within the\nmetrics module. Similarly, new input/output process functions should be implemented as\nstatic functions in class InputProcess and class OutputProcess in the utils module.\n20\n"
  },
  {
    "title": "Foundation Models in Robotics: Applications, Challenges, and the Future",
    "link": "https://arxiv.org/pdf/2312.07843.pdf",
    "upvote": "14",
    "text": "arXiv:2312.07843v1  [cs.RO]  13 Dec 2023\n1\nFoundation Models in Robotics: Applications,\nChallenges, and the Future\nRoya Firoozi1, Johnathan Tucker1, Stephen Tian1, Anirudha Majumdar2,6, Jiankai Sun1,\nWeiyu Liu1, Yuke Zhu3,4, Shuran Song1, Ashish Kapoor5, Karol Hausman1,6,\nBrian Ichter6, Danny Driess6,7, Jiajun Wu1, Cewu Lu8, Mac Schwager1\n1Stanford University, 2Princeton University, 3UT Austin, 4NVIDIA, 5Scaled Foundations,\n6Google DeepMind, 7TU Berlin, 8Shanghai Jiao Tong University\nCONTENTS\nI\nIntroduction\n3\nII\nFoundation Models Background\n4\nII-A\nTerminology and Mathematical Prelim-\ninaries\n. . . . . . . . . . . . . . . . . .\n4\nII-B\nLarge Language Model (LLM) Exam-\nples and Historical Context . . . . . . .\n7\nII-C\nVision Transformers . . . . . . . . . . .\n7\nII-D\nMultimodal Vision-Language Models\n(VLMs)\n. . . . . . . . . . . . . . . . .\n8\nII-E\nEmbodied Multimodal Language Models\n8\nII-F\nVisual Generative Models . . . . . . . .\n8\nIII\nRobotics\n8\nIII-A\nRobot Policy Learning for Decision\nMaking and Control . . . . . . . . . . .\n9\nIII-A1\nLanguage-conditioned\nImi-\ntation Learning for Manipu-\nlation . . . . . . . . . . . . .\n9\nIII-A2\nLanguage-Assisted\nReinforcement Learning . . .\n11\nIII-B\nLanguage-Image\nGoal-Conditioned\nValue Learning . . . . . . . . . . . . . .\n11\nIII-C\nRobot Task Planning using Large Lan-\nguage Models . . . . . . . . . . . . . .\n12\nIII-C1\nLanguage\nInstructions\nfor\nTask Speci\ufb01cation . . . . . .\n12\nIII-C2\nCode Generation using Lan-\nguage Models for Task Plan-\nning\n. . . . . . . . . . . . .\n12\nIII-D\nIn-context Learning (ICL) for Decision-\nMaking . . . . . . . . . . . . . . . . . .\n13\nIII-E\nRobot Transformers . . . . . . . . . . .\n13\nIII-F\nOpen-Vocabulary Robot Navigation and\nManipulation . . . . . . . . . . . . . . .\n15\nIII-F1\nOpen-Vocabulary Navigation\n15\nIII-F2\nOpen-Vocabulary Manipulation 16\nIV\nPerception\n16\nIV-A\nOpen-Vocabulary Object Detection and\n3D Classi\ufb01cation\n. . . . . . . . . . . .\n16\nIV-A1\nObject Detection . . . . . . .\n16\nIV-A2\n3D Classi\ufb01cation\n. . . . . .\n17\nIV-B\nOpen-Vocabulary Semantic Segmentation 18\nIV-C\nOpen-Vocabulary 3D Scene and Object\nRepresentations\n. . . . . . . . . . . . .\n18\nIV-C1\nLanguage Grounding in 3D\nScene . . . . . . . . . . . . .\n18\nIV-C2\nScene Editing\n. . . . . . . .\n19\nIV-C3\nObject Representations\n. . .\n20\nIV-D\nLearned Affordances\n. . . . . . . . . .\n20\nIV-E\nPredictive Models . . . . . . . . . . . .\n21\nV\nEmbodied AI\n21\nV-A\nGeneralist AI\n. . . . . . . . . . . . . .\n22\nV-B\nSimulators . . . . . . . . . . . . . . . .\n23\nVI\nChallenges and Future Directions\n23\nVI-A\nOvercoming Data Scarcity in Training\nFoundation Models for Robotics . . . .\n23\nVI-A1\nScaling Robot Learning Us-\ning Unstructured Play Data\nand Unlabeled Videos of Hu-\nmans . . . . . . . . . . . . .\n23\nVI-A2\nData Augmentation using In-\npainting\n. . . . . . . . . . .\n24\nVI-A3\nOvercoming\n3D\nData\nScarcity\nfor\nTraining\n3D\nFoundation Models\n. . . . .\n24\nVI-A4\nSynthetic Data Generation\nvia High-Fidelity Simulation\n24\nVI-A5\nData\nAugmentation\nusing\nVLMs\n. . . . . . . . . . . .\n24\nVI-A6\nRobot Physical Skills are\nLimited to Distribution of\nSkills . . . . . . . . . . . . .\n24\nVI-B\nReal Time Performance (High Inference\nTime of Foundation Models) . . . . . .\n24\nVI-C\nLimitations in Multimodal Representation 25\nVI-D\nUncertainty Quanti\ufb01cation\n. . . . . . .\n25\nVI-D1\nInstance-Level\nUncertainty\nQuanti\ufb01cation . . . . . . . .\n25\n2\nVI-D2\nDistribution-Level\nUncertainty Quanti\ufb01cation\n.\n25\nVI-D3\nCalibration . . . . . . . . . .\n25\nVI-D4\nDistribution Shift\n. . . . . .\n25\nVI-D5\nCase\nStudy:\nUncertainty\nQuanti\ufb01cation for Language-\nInstructed Robots\n. . . . . .\n26\nVI-E\nSafety Evaluation . . . . . . . . . . . .\n26\nVI-E1\nPre-deployment safety tests .\n26\nVI-E2\nRuntime monitoring and out-\nof-distribution detection . . .\n26\nVI-F\nUsing Existing Foundation Models as\nPlug-and-Play or Building New Foun-\ndation Models for Robotics . . . . . . .\n26\nVI-G\nHigh Variability in Robotic Settings . .\n27\nVI-H\nBenchmarking and Reproducibility in\nRobotics Settings\n. . . . . . . . . . . .\n27\nVII\nConclusion\n27\nReferences\n27\n3\nAbstract\u2014We survey applications of pretrained foundation\nmodels in robotics. Traditional deep learning models in robotics\nare trained on small datasets tailored for speci\ufb01c tasks, which\nlimits their adaptability across diverse applications. In contrast,\nfoundation models pretrained on internet-scale data appear to\nhave superior generalization capabilities, and in some instances\ndisplay an emergent ability to \ufb01nd zero-shot solutions to problems\nthat are not present in the training data. Foundation models\nmay hold the potential to enhance various components of the\nrobot autonomy stack, from perception to decision-making and\ncontrol. For example, large language models can generate code or\nprovide common sense reasoning, while vision-language models\nenable open-vocabulary visual recognition. However, signi\ufb01cant\nopen research challenges remain, particularly around the scarcity\nof robot-relevant training data, safety guarantees and uncertainty\nquanti\ufb01cation, and real-time execution. In this survey, we study\nrecent papers that have used or built foundation models to solve\nrobotics problems. We explore how foundation models contribute\nto improving robot capabilities in the domains of perception,\ndecision-making, and control. We discuss the challenges hin-\ndering the adoption of foundation models in robot autonomy\nand provide opportunities and potential pathways for future\nadvancements. The GitHub project corresponding to this paper1\ncan be found here.\nIndex Terms\u2014Robotics, Large Language Models (LLMs),\nVisual-Language Models (VLM), Large\nPretrained Models,\nFoundation Models\nI. INTRODUCTION\nF\nOUNDATION\nmodels\nare\npretrained\non\nextensive\ninternet-scale data and can be \ufb01ne-tuned for adaptation to\na wide range of downstream tasks. Foundation models have\ndemonstrated signi\ufb01cant breakthroughs in vision and language\nprocessing; examples include BERT [1], GPT-3 [2], GPT-4 [3],\nCLIP [4], DALL-E [5], and PaLM-E [6]. Foundation models\nhave the potential to unlock new possibilities in robotics\ndomains such as autonomous driving, household robotics,\nindustrial robotics, assistive robotics, medical robotics, \ufb01eld\nrobotics, and multi-robot systems. Pretrained Large Language\nModels (LLMs), Large Vision-Language Models (VLMs),\nLarge Audio-Language Models (ALMs), and Large Visual-\nNavigation Models (VNMs) can be utilized to improve various\ntasks in robotics settings. The integration of foundation models\ninto robotics is a rapidly evolving area, and the robotics com-\nmunity has very recently started exploring ways to leverage\nthese large models within the robotics domain for perception,\nprediction, planning, and control.\nPrior to the emergence of foundation models, traditional\ndeep learning models for robotics were typically trained on\nlimited datasets gathered for distinct tasks [7]. Conversely,\nfoundation models are pre-trained on extensive and diverse\ndata, which has been proven in other domains (such as natural\nlanguage processing, computer vision, and healthcare [8]) to\nsigni\ufb01cantly expand adaptability, generalization capability, and\noverall performance. Ultimately, foundation models may hold\nthe potential to yield these same bene\ufb01ts in robotics. Knowl-\nedge transfer from foundation models may reduce training\ntime and computational resources compared to task-speci\ufb01c\n1Preliminary release. We are committed to further enhancing and updating\nthis work to ensure its quality and relevance\nmodels. Particularly relevant to robotics, multimodal founda-\ntion models can fuse and align multimodal heterogeneous data\ngathered from various sensors into compact homogeneous rep-\nresentations needed for robot understanding and reasoning [9].\nThese learned representations hold the potential to be used in\nany part of the autonomy stack including perception, decision-\nmaking, and control. Furthermore, foundation models provide\nzero-shot capabilities, which refer to the ability of an AI\nsystem to perform tasks without prior examples or dedicated\ntraining data for that speci\ufb01c task. The would enable robots to\ngeneralize their learned knowledge to novel cases, enhancing\nadaptability and \ufb02exibility for robots in unstructured settings.\nIntegrating foundation models into robotic systems may\nenable context-aware robotic systems by enhancing the robot\u2019s\nability to perceive and interact with the environment. For\nexample in the perception domain, Large Vision-Language\nModels (VLMs) have been found to provide cross-modal\nunderstanding by learning associations between visual and tex-\ntual data, aiding tasks such as zero-shot image classi\ufb01cation,\nzero-shot object detection [10], and 3D classi\ufb01cation [11].\nAs another example, language grounding in the 3D world\n[12] (aligning contextual understanding of VLMs to the 3-\ndimensional (3D) real world) may enhance a robot\u2019s spatial\nawareness by associating words with speci\ufb01c objects, loca-\ntions, or actions within the 3D environment.\nIn the decision-making or planning domain, LLMs and\nVLMs have been found to assist robots in task speci\ufb01ca-\ntion for high-level planning [13]. Robots can perform more\ncomplex tasks by leveraging linguistic cues in manipulation,\nnavigation, and interaction. For example, for robot policy\nlearning techniques like imitation learning [14] and reinforce-\nment learning [15], foundation models seem to offer the\npossibility to improve data ef\ufb01ciency and enhance contextual\nunderstanding. In particular, language-driven rewards can be\nused to guide RL agents by providing shaped rewards [16].\nAlso, researchers have employed language models to provide\nfeedback for policy learning techniques [17]. Some works have\nshown that a VLM model\u2019s visual question-answering (VQA)\ncapability can be harnessed in robotics use cases. For example,\nresearchers have used VLMs to answer questions related to\nvisual content to aid robots in accomplishing their tasks [18].\nAlso, researchers have stated utilizing VLMs to help with data\nannotation, by generating descriptive labels for visual content\n[19].\nDespite the transformative capabilities of foundation models\nin vision and language processing, the generalization and \ufb01ne-\ntuning of foundation models for real-world robotics tasks re-\nmain challenging. These challenges include: 1) Data Scarcity:\nhow to obtain internet-scale data for robot manipulation,\nlocomotion, navigation, and other robotics tasks, and how\nto perform self-supervised training with this data, 2) High\nVariability: how to deal with the large diversity in physical\nenvironments, physical robot platforms, and potential robot\ntasks while still maintaining the generality required for a foun-\ndation model, 3) Uncertainty Quanti\ufb01cation: how to deal\nwith (i) instance-level uncertainty such as language ambiguity\nor LLM hallucination; (ii) distribution-level uncertainty; and\n(iii) distribution-shift, especially resulting from closed-loop\n4\nrobot deployment, 4) Safety Evaluation: How to rigorously\ntest for the safety of a foundation model-based robotic system\n(i) prior to deployment, (ii) as the model is updated throughout\nits lifecycle, and (iii) as the robot operates in its target\nenvironments. 5) Real-Time Performance: how to deal with\nthe high inference time of some foundation models which\ncould hinder their deployment on robots and how to accelerate\ninference in foundation models to the speed required for online\ndecision-making.\nIn this survey, we study the existing literature on the use of\nfoundation models in robotics. We study current approaches\nand applications, present current challenges, suggest directions\nfor future research to address these challenges, and identify\npotential risks exposed by integrating foundation models into\nrobot autonomy. Another survey on foundation models in\nrobotics appeared simultaneously with ours on arXiv [20]. In\ncomparison with that paper, ours emphasizes future challenges\nand opportunities, including safety and risk, and ours has a\nstronger emphasis on comparisons in applications, algorithms,\nand architectures among the existing papers in this space. In\ncontrast to some existing surveys that focus on a speci\ufb01c in-\ncontext instruction, such as prompts [21], vision transform-\ners [22], or decision-making [13], [23], we provide a broader\nperspective to connect distinct research threads in foundation\nmodels organized around their relevance to and application\nto robotics. Conversely, our scope is much narrower than the\npaper [24], which explores the broad application of foundation\nmodels across many disciplines, of which robotics is one. We\nhope this paper can provide clarity regarding areas of recent\nprogress and existing de\ufb01ciencies in the research, and point\nthe way forward to future opportunities and challenges facing\nthis research area. Ultimately, we aim to give a resource for\nrobotics researchers to learn about this exciting new area.\nWe limit the scope of this survey to papers that fall into one\nof the following categories:\n1) Background Papers: Papers that do not explicitly link\nto robotics, but are nonetheless required for understand-\ning foundation models. These papers are discussed in\nthe background section (section II) of the survey paper.\n2) Robotics Papers: Papers that integrate a foundation\nmodel into a robotic system in a plug-and-play fashion,\npapers that adapt or \ufb01ne-tune foundation models for\nrobotic systems, or papers that build new robotic-speci\ufb01c\nfoundation models.\n3) Robotics-Adjacent Papers: Papers that present meth-\nods or techniques applied to areas adjacent to robotics\n(e.g., computer vision, embodied AI), with a clear path\nto future application in robotics.\nThis survey is organized as follows: In Section II, we\nprovide an introduction to foundation models including LLMs,\nvision transformers, VLMs, embodied multimodal language\nmodels, and visual generative models. In addition, in the last\npart of this section, we discuss different training methods\nused to train foundation models. In Section III, we present a\nreview of how foundation models are integrated into different\ntasks for decision-making in robotics. First, we discuss robot\npolicy learning using language-conditioned imitation learning,\nand language-assisted reinforcement learning. Then, we dis-\ncuss how to use foundation models to design a language-\nconditioned value function that can be used for planning\npurposes. Next, robot task speci\ufb01cation and code generation\nfor task planning using foundation models are presented. In\nSection IV, we study various perception tasks in robotics\nthat have the potential to be enhanced by employing founda-\ntion models. These tasks include semantic segmentation, 3D\nscene representation, zero-shot 3D classi\ufb01cation, affordance\nprediction, and dynamics prediction. In Section V, we present\npapers about Embodied AI agents, generalist AI agents, as\nwell as simulators and benchmarks developed for embodied\nAI research. In Section VI, we conclude the survey by dis-\ncussing different challenges for employing foundation models\nin robotic systems and proposing potential avenues for future\nresearch. Finally, in Section VII we offer the concluding\nremarks.\nII. FOUNDATION MODELS BACKGROUND\nFoundation models have billions of parameters and are\npretrained on massive internet-scale datasets. Training mod-\nels of such scale and complexity involve substantial costs.\nAcquiring, processing, and managing data can be costly. The\ntraining process demands signi\ufb01cant computational resources,\nrequiring specialized hardware such as GPUs or TPUs, as\nwell as software and infrastructure for model training which\nrequires \ufb01nancial resources. Additionally, training a foundation\nmodel is time-intensive, which can translate to even higher\ncosts. Hence these models are often used as plug-and-play\nmodules (which refers to the integration of foundation mod-\nels into various applications without the need for extensive\ncustomization). Table I provides details about commonly used\nfoundation models. In the rest of this section, we introduce\nLLMs, vision transformers, VLMs, embodied multi-modal\nlanguage models, and visual generative models. In the last\npart of this section, we introduce different training methods\nthat are used to train foundation models.\nA. Terminology and Mathematical Preliminaries\nIn this section, we \ufb01rst introduce common terminologies in\nthe context of foundation models and describe basic math-\nematical details and training practices for various types of\nfoundation models.\nTokenization: Given a sequence of characters, tokenization\nis the process of dividing the sequence into smaller units,\ncalled tokens. Depending on the tokenization strategy, tokens\ncan be characters, segments of words, complete words, or\nportions of sentences. Tokens are represented as 1-hot vectors\nof dimension equal to the size of the total vocabulary and are\nmapped to lower-dimensional vectors of real numbers through\na learned embedding matrix. An LLM takes a sequence of\nthese embedding vectors as raw input, producing a sequence\nof embedding vectors as raw output. These output vectors are\nthen mapped back to tokens and hence to text. GPT-3, for\nexample, has a vocabulary of 50,257 different tokens, and an\nembedding dimension of 12,288.\n5\nFoundation Models in Robotics\nRobotics\nRobot Policy Learning\nLanguage-Conditioned\nImitation Learning\ne.g. CLIPort [25], Play-LMP [26], PerAct [27], Multi-Context Imitation [28],\nCACTI [14], Voltron [29]\nLanguage-Assisted\nReinforcement Learning\ne.g. Adaptive Agent (AdA) [30], Palo et al. [15]\nLanguage-Image\nGoal-Conditioned\nValue Learning\ne.g. R3M [31], SayCan [32], Inner Monologue [33], VoxPoser [34], Mahmoudieh et al. [35], VIP [36],\nLIV [37], LOREL [38]\nHigh-Level\nTask Planning\ne.g. NL2TL [39], Chen et al. [40]\nLLM-Based\nCode Generation\ne.g. ProgPrompt [41], Code-as-Policy [42], ChatGPT-Robotics [43]\nRobot Transformers\ne.g. RT-1 [44], RT-2 [45], RT-X [46], PACT [47], Xiao et al. [48], Radosavovic et al. [49], LATTE [50]\nRelevant to Robotics\nPerception\nOpen-Vocabulary\nObject Detection\nand 3D classi\ufb01cation\ne.g. OWL-ViT [51], GLIP [52], Grounding DINO [53],\nPointCLIP [54], PointBERT [55], ULIP [56], [57]\nOpen-Vocabulary\nSemantic Segmentation\ne.g. LSeg [58], Segment Anything [59], FastSAM [60], MobileSAM [61],\nTrack Anything Model (TAM) [62]\nOpen-Vocabulary 3D\nScene Representation\ne.g. CLIP-NERF [63], LERF [64], DFF [65]\nAffordances\ne.g. Affordance Diffusion [66], VRB [67]\nEmbodied AI\ne.g. Huang et al. [68], Statler [69], EmbodiedGPT [70], MineDojo [71], VPT [72], Kwon et al. [16],\nVoyager [73], ELLM [74]\nFig. 1. Overview of Robotics Tasks Leveraging Foundation Models.\nThe token decoding (from low-dimension real-valued em-\nbedding vectors to high-dimension 1-hot vectors) is not deter-\nministic, resulting in a weighting for each possible token in\nthe vocabulary. These weightings are often used by LLMs as\nprobabilities over tokens, to introduce randomness in the text\ngeneration process. For example, the temperature parameter\nin GPT-3 blends between always choosing the top-weighted\ntoken (temperature of 0) and drawing the token based on the\nprobability distribution suggested by the weights (temperature\nof 1). This source of randomness is only in the token decoding\nprocess, not in the LLM itself. To the authors\u2019 knowledge, this\nis, in fact, the only source of randomness in the GPT family\nof models.\nOne of the most common tokenization schemes, which is\nused by the GPT family of models, is called byte-pair encoding\n[75]. Byte-pair encoding starts with a token for each individ-\nual symbol (e.g., letter, punctuation), then recursively builds\ntokens by grouping pairs of symbols that commonly appear\ntogether, building up to assign tokens to larger and larger\ngroups (pairs of pairs, etc) that appear frequently together in a\ntext corpus. The tokenization process can extend beyond text\ndata to diverse contexts, encompassing various data modalities\nlike images, videos, and robot actions. In these scenarios, the\nrespective data modalities can be treated as sequential data and\ntokenized similarly to train generative models. For example,\njust as language constitutes a sequence of words, an image\ncomprises a sequence of image patches, force sensors yield a\nsequence of sensory inputs at each time step, and a series of\nactions represent the sequential nature of tasks for a robot.\nGenerative Models: A generative model is a model that\nlearns to sample from a probability distribution to create\nexamples of data that seem to be from the same distribution\nas the training data. For example, a face generation model can\nproduce images of faces that cannot be distinguished from the\nset of real images used to train the model. These models can\nbe trained to be conditional, meaning they generate samples\nfrom a conditional distribution conditioned on a wide range\nof possible conditioning information. For example, a gender\nconditional face generator can generate images of female or\nmale faces, where the desired gender is given as a conditioning\ninput to the model.\nDiscriminative Models: Discriminative models are used\nfor regression or classi\ufb01cation tasks. In contrast to genera-\ntive models, discriminative models are trained to distinguish\nbetween different classes or categories. Their emphasis lies in\nlearning the boundaries between classes within the input space.\nWhile generative models learn to sample from the distribution\nover the data, discriminative models learn to evaluate the\nprobability distribution of the output labels given the input\nfeatures, or (depending on how the model is trained) learn to\nevaluate some statistic of the probability distribution over the\noutputs, such as the expected output given an input.\nTransformer Architecture: Most foundation models are\nbuilt on the transformer architecture, which has been instru-\n6\nmental in the rise of foundation models and large language\nmodels. The following discussion was synthesized from [76],\nas well as online blogs, unpublished reports, and wikipedia\n[77]\u2013[79]. A transformer acts simultaneously on a collection\nof embedded token vectors (x1, . . . , xN) known as a context\nwindow. The key enabling innovation of the Transformer\narchitecture is the multi-head self-attention mechanism origi-\nnally proposed in the seminal work [76]. In this architecture,\neach attention head computes a vector of importance weights\nthat corresponds to how strongly a token in the context window\nxi correlates with other tokens in the same window xj.\nEach attention head mathematically encodes different notions\nof similarity, through different projection matrices used in\nthe computation of the importance weights. Each head can\nbe trained (backward pass) and evaluated (forward pass) in\nparallel across all tokens and across all heads, leading to faster\ntraining and inference when compared with previous models\nbased on RNNs or LSTMs.\nMathematically, an attention head maps each token xi in the\ncontext window to a \u201cquery\u201d qi = W T\nq xi, and each other token\nin the context head xj to a \u201ckey\u201d kj = W T\nk xj. The similarity\nbetween query and key is then measured through a scaled dot\nproduct, qT\ni kj/\n\u221a\nd, where d is the dimension of the query and\nkey vectors. A softmax is then taken over all j to give weights\n\u03b1ij representing how much xi \u201cattends to\u201d xj. The tokens are\nthen mapped to \u201cvalues\u201d with vj = W T\nv xj, and the output\nof the attention for position i is then given as a sum over\nvalues weighted by attention weights, P\nj \u03b1ijvj. One of the\nkey reasons for the success of the transformer attention model\nis that it can be ef\ufb01ciently computed with GPUs and TPUs by\nparallelizing the preceding steps into matrix computations,\nattn(Q, K, V) = softmax\n\u0012QK\u22a4\n\u221adk\n\u0013\nV,\n(1)\nwhere Q, K, V are matrices with rows qT\ni , kT\ni , and vT\ni ,\nrespectively. Each head in the model produces this compu-\ntation independently, with different Wq, Wk, Wv matrices to\nencode different kinds of attention. The outputs from each\nhead are then concatenated, normalized with a skip connection,\npassed through a fully connected ReLU layer, and normalized\nagain with a skip connection to produce the output of the\nattention layer. Multiple layers are arranged in various ways\nto give \u201cencoders\u201d and \u201cdecoders,\u201d which together make up a\ntransformer.\nThe size of a transformer model is typically quanti\ufb01ed by\n(i) the size of the context window, (ii) the number of attention\nheads per layer, (iii) the size of the attention vectors in each\nhead, and (iii) the number of stacked attention layers. For ex-\nample, GPT-3\u2019s context window is 2048 tokens (corresponding\nto about 1500 words of text), each attention layer has 96 heads,\neach head has attention vectors of 128 dimensions, and there\nare 96 stacked attention layers in the model.\nThe basic multi-head attention mechanism does not im-\npose any inherent sense of sequence or directionality in the\ndata. However, transformers\u2014especially in natural language\napplications\u2014are often used as sequence predictors by impos-\ning a positional encoding on the input token sequence. They\nare then applied to a token sequence autoregressively, meaning\nthey predict the next token in the sequence, add that token to\ntheir context window, and repeat. This concept is elaborated\nbelow.\nAutoregressive Models: The concept of autoregression has\nbeen applied in many \ufb01elds as a representation of random\nprocesses whose outputs depend causally on the previous\noutputs. Autoregressive models use a window of past data to\npredict the next data point in a sequence. The window then\nslides one position forward, recursively ingesting the predicted\ndata point into the window and expelling the oldest data point\nfrom the window. The model again predicts the next data point\nin the sequence, repeating this process inde\ufb01nitely. Classical\nlinear autoregressive models such as Auto-Regressive Moving\nAverage (ARMA) and Auto-Regressive Moving Average with\neXogenous input (ARMAX) models are standard statistical\ntools dating back to at least the 1970s [80]. These modeling\nconcepts were adapted to deep learning models \ufb01rst with\nRNNs, and later LSTMs, which are both types of learn-\nable nonlinear autoregressive models. Transformer models,\nalthough they are not inherently autoregressive, are often\nadapted to an autoregressive framework for text prediction\ntasks.\nFor example, the GPT family [81] builds on the original\ntransformer model by using a modi\ufb01cation introduced in [82]\nthat removes the transformer encoder blocks entirely, retaining\njust the transformer decoder blocks. This has the advantage of\nreducing the number of model parameters by close to half\nwhile reducing redundant information that is learned in both\nthe encoder and decoder. During training, the GPT model\nseeks to produce an output token from the tokenized corpus\nX\n= (x1, ..., xn) to minimize the negative log-likelihood\nwithin the context window of length N,\nLLLM = \u2212\nX\ni\nlog P (xi | xi\u2212N, . . . , xi\u22121) .\n(2)\nThis results in a large pretrained model that autoregressively\npredicts the next likely token given the tokens in the context\nwindow. Although powerful, the unidirectional autoregressive\nnature of the GPT family means that these models may\nlag in performance on bidirectional tasks such as reading\ncomprehension.\nMasked Auto-Encoding: To address the unidirectional lim-\nitation of the GPT family and allow the model to make bidirec-\ntional predictions, works such as BERT [1] use masked auto-\nencoding. This is achieved through an architectural change,\nnamely the addition of a bidirectional encoder, as well as\na novel pre-training objective known as masked language\nmodeling (MLM). The MLM task simply masks a percentage\nof the tokens in the corpus and requires the model to predict\nthese tokens. Through this procedure, the model is encouraged\nto learn the context that surrounds a word rather than just the\nnext likely word in a sequence.\nContrastive Learning: Visual-language foundation models\nsuch as CLIP [4] typically rely on different training methods\nfrom the ones used with large language models which encour-\nage explicitly predictive behavior. Visual-language models use\ncontrastive representation learning, where the goal is to learn a\n7\njoint embedding space between input modalities where similar\nsample pairs are closer than dissimilar ones. The training\nobjective for many VLMs is some variation of the objective\nfunction,\n\u2113(v\u2192u)\ni\n= \u2212 log\nexp (sim (vi, ui)/ \u03c4)\nPN\nk=1 exp (sim (vi, uk)/ \u03c4)\n,\n(3)\n\u2113(u\u2192v)\ni\n= \u2212 log\nexp (sim (ui, vi)/ \u03c4)\nPN\nk=1 exp (sim (ui, vk)/ \u03c4)\n,\n(4)\nL = 1\nN\nN\nX\ni=1\n\u0010\n\u03bb\u2113(v\u2192u)\ni\n+ (1 \u2212 \u03bb)\u2113(u\u2192v)\ni\n\u0011\n.\n(5)\nThis objective function was popularized for multimodal input\nby ConVIRT [83] and \ufb01rst presented in prior works [84]\u2013[87].\nThis objective function trains the image and text encoders to\npreserve mutual information between the true text and image\npairs. In these equations, ui and vi are the ith encoded text\nand image respectively from i \u2208 1, ..., N image and text pairs.\nThe sim operation is the cosine similarity between the text and\nimage embeddings, and \u03c4 is a temperature term. In CLIP [4]\nthe authors use a symmetric cross-entropy loss, meaning the\n\ufb01nal loss is an average of the two loss components where each\nis equally weighted (i.e. \u03bb = 0.5).\nDiffusion Models: Outside of large language models and\nmulti-modal models such as VLMs, diffusion models for\nimage generation (e.g. DALL-E2) [88] are another class of\nfoundation models considered in this survey. Although dif-\nfusion models were established in prior work [89], [90] the\ndiffusion probabilistic model presented in [91] popularized the\nmethod. The diffusion probabilistic model is a deep generative\nmodel that is trained in an iterative forward and reverse\nprocess. The forward process adds Gaussian noise to an input\nx0 in a Markov chain until xT when the result is zero mean\nisotropic noise. This means the forward process produces a\ntrajectory of noise q(x1:T |x0) as,\nq (x1:T | x0) :=\nT\nY\nt=1\nq (xt | xt\u22121) .\n(6)\nAt each time step q(xt|xt\u22121) is described by a normal distri-\nbution with mean \u221a1 \u2212 \u03b2txt\u22121 and covariance \u03b2tI where \u03b2t\nis scheduled or a \ufb01xed hyperparameter.\nThe reverse process requires the model to learn to the\ntransitions that will de-noise the zero mean Gaussian and\nproduce the input image. This process is also de\ufb01ned as\na Markov chain where the transition distribution at time\nt is p\u03b8 (xt\u22121 | xt) := N (xt\u22121; \u00b5\u03b8 (xt, t) , \u03a3\u03b8 (xt, t)). For\ncompleteness, the reverse process Markov chain is given by,\np\u03b8 (x0:T ) := p (xT )\nT\nY\nt=1\np\u03b8 (xt\u22121 | xt) .\n(7)\nDiffusion models are trained using a reduced form of the ev-\nidence lower bound loss function that is typical of variational\ngenerative models like variational autoencoders (VAEs). The\nreduced loss function used for training is\nL = Eq[DKL(q(xT | x0)\u2225p(xT ))\n(8)\n+\nX\nt>1\nDKL(q(xt\u22121 | xt, x0)\u2225p\u03b8(xt\u22121 | xt)\n\u2212 log p\u03b8(x0 | x1)],\nwhere DKL(q||p) denotes Kullback\u2013Leibler divergence, which\nis a measure of how different a distribution q is from a\ndistribution p.\nB. Large Language Model (LLM) Examples and Historical\nContext\nLLMs have billions of parameters and are trained on trillions\nof tokens. This large scale has allowed models such as GPT-2\n[92] and BERT [1] to achieve state-of-the-art performance on\nthe Winograd Schema challenge [93] and the General Lan-\nguage Understanding Evaluation (GLUE) [94] benchmarks,\nrespectively. Their successors include GPT-3 [2], LLaMA [95],\nand PaLM [96] has grown considerably in the number of\nparameters (typically now over 100 billion), the size of the\ncontext window (typically now over 1000 tokens), and the\nsize of the training data set (typically now 10s of terabytes\nof text). GPT-3 is trained on the Common Crawl dataset.\nCommon Crawl contains petabytes of publicly available data\nover 12 years of web crawling and includes raw web page\ndata, metadata, and text extracts. LLMs can also be multi-\nlingual. For example, ChatGLM-6B and GLM-130B [97] is\na bilingual (English and Chinese) pretrained language model\nwith 130 billion parameters. LLMs can also be \ufb01ne-tuned,\na process by which the model parameters are adjusted with\ndomain-speci\ufb01c data to align the performance of the LLM\nto a speci\ufb01c use case. For example, GPT-3 and GPT-4 [3]\nhave been \ufb01ne-tuned using reinforcement learning with human\nfeedback (RLHF).\nC. Vision Transformers\nA Vision Transformer (ViT) [98]\u2013[100] is a transformer\narchitecture for computer vision tasks including image clas-\nsi\ufb01cation segmentation, and object detection. A ViT treats an\nimage as a sequence of image patches referred to as tokens.\nIn the image tokenization process, an image is divided into\npatches of \ufb01xed size. Then the patches are \ufb02attened into a one-\ndimensional vector which is referred to as linear embedding.\nTo capture the spatial relationships between image patches,\npositional information is added to each token. This process is\nreferred to as position embedding. The image tokens incor-\nporated with position encoding are fed into the transformer\nencoder and the self-attention mechanism enables the model\nto capture long-term dependencies and global patterns in the\ninput data. In this paper, we focus only on those ViT models\nwith a large number of parameters. ViT-G [101] scales up the\nViT model and has 2B parameters. Additionally, ViT-e [102]\nhas 4B parameters. ViT-22B [103] is a vision transformer\nmodel at 22 billion parameters, which is used in PaLM-E and\nPaLI-X [104] and helps with robotics tasks.\n8\nDINO [105] is a self-supervised learning method, for train-\ning ViT. DINO is a form of knowledge distillation with no\nlabels. Knowledge distillation is a learning framework where\na smaller model (student network) is trained to mimic the\nbehavior of a larger more complex model (teacher network).\nBoth networks share the same architecture with different sets\nof parameters. Given a \ufb01xed teacher network, the student\nnetwork learns its parameters by minimizing the cross-entropy\nloss w.r.t. the student network parameters. The neural network\narchitecture is composed of ViT or ResNet [106] backbone and\na projection head that includes layers of multi-layer perception\n(MLP). Self-supervised ViT features learned using DINO\ncontain explicit information about the semantic segmentation\nof an image including scene layout and object boundaries with\nsuch clarity that is not achieved using supervised ViTs or\nconvnets.\nDINOv2 [107] provides a variety of pretrained visual mod-\nels that are trained with different vision transformers (ViT) on\nthe LVD-142M dataset introduced in [107]. It is trained using\na discriminative self-supervised method on a compute cluster\nof 20 nodes equipped with 8 V100-32GB GPUs. DINOv2\nprovides various visual features at the image (e.g. detection)\nor pixel level (e.g. segmentation). SAM [59] provides zero-\nshot promptable image segmentation. It is discussed in more\ndetail in Section IV.\nD. Multimodal Vision-Language Models (VLMs)\nMultimodal refers to the ability of a model to accept differ-\nent \u201cmodalities\u201d of inputs, for example, images, texts, or audio\nsignals. Visual-language models (VLM) are a type of multi-\nmodal model that takes in both images and text. A commonly\nused VLM in robotics applications is Contrastive Language-\nImage Pre-training (CLIP) [4]. CLIP offers a method to com-\npare the similarity between textual descriptions and images.\nCLIP uses internet-scale image-text pairs data to capture the\nsemantic information between images and text. CLIP model\narchitecture contains a text encoder [92] and an image encoder\n(a modi\ufb01ed version of vision transformer ViT) that are trained\njointly to maximize the cosine similarity of the image and\ntext embeddings. CLIP uses contrastive learning together with\nlanguage models and visual feature encoders to incorporate\nmodels for zero-shot image classi\ufb01cation.\nBLIP [108] focuses on multimodal learning by jointly opti-\nmizing three objectives during pretraining. These objectives\ninclude Image-Text Contrastive Loss, Image-Text Matching\nLoss, and Language Modeling Loss. The method leverages\nnoisy web data by bootstrapping captions, enhancing the\ntraining process. CLIP2 [109] aims to build well-aligned and\ninstance-based text-image-point proxies. It learns semantic\nand instance-level aligned point cloud representations using\na cross-modal contrastive objective. FILIP [110] focuses on\nachieving \ufb01ner-level alignment in multimodal learning. It\nincorporates a cross-modal late interaction mechanism that\nutilizes token-wise maximum similarity between visual and\ntextual tokens. This mechanism guides the contrastive objec-\ntive and improves the alignment between visual and textual\ninformation. FLIP [111] proposes a simple and more ef\ufb01cient\ntraining method for CLIP. FLIP randomly masks out and\nremoves a signi\ufb01cant portion of image patches during training.\nThis approach aims to improve the training ef\ufb01ciency of CLIP\nwhile maintaining its performance.\nE. Embodied Multimodal Language Models\nAn embodied agent is an AI system that interacts with a\nvirtual or physical world. Examples include virtual assistance\nor robots. Embodied language models are foundation models\nthat incorporate real-world sensor and actuation modalities\ninto pretrained large language models. Typical vision-language\nmodels are trained on general vision-language tasks such\nas image captioning or visual question answering. PaLM-\nE [6] is a multimodal language model that has been trained\non not only internet-scale general vision-language data, but\nalso on embodied, robotics data, simultaneously. In order to\nconnect the model to real world sensor modalities, PaLM-\nE\u2019s architecture injects (continuous) inputs such as images,\nlow-level states, or 3D neural scene representations into the\nlanguage embedding space of a decoder-only language model\nto enable the model to reason about text and other modalities\njointly. The main PaLM-E version is built from the PaLM\nLLM [96] and a ViT [103]. The ViT transforms an image into\na sequence of embedding vectors which are projected into the\nlanguage embedding space via an af\ufb01ne transformation. The\nwhole model is trained end-to-end, starting from a pre-trained\nLLM and ViT model. The authors also explore different\nstrategies such as freezing the LLM and just training the ViT,\nwhich leads to worse performance. Given multimodal inputs,\nthe output of PaLM-E is text decoded auto-regressively. In\norder to connect this output to a robot for control, language\nconditioned short-horizon policies can be used. In this case,\nPaLM-E acts as a high-level control policy. Experiments\nshow that a single PaLM-E, in addition to being a vision-\nlanguage generalist, is able to perform many different robotics\ntasks over multiple robot embodiments. The model exhibits\npositive transfer, i.e. simultaneously training on internet-scale\nlanguage, general vision-language, and embodied domains\nleads to higher performance compared to training the model\non single tasks.\nF. Visual Generative Models\nWeb-scale diffusion models such as OpenAI\u2019s DALL-\nE [112] and DALL-E2 [88] provide zero-shot text-to-image\ngeneration. They are trained on hundreds of millions of image-\ncaption pairs from the internet. These models learn a language-\nconditioned distribution over images from which an image can\nbe generated using a given prompt. The DALL-E2 architecture\nincludes a prior that generates a CLIP image embedding from a\ntext caption, and a decoder that generates an image conditioned\non the image embedding.\nIII. ROBOTICS\nIn this section, we delve into robot decision-making, plan-\nning, and control. Within this realm, Large Language Models\n(LLMs) and Visual Language Models (VLMs) may hold the\n9\nTABLE I\nLARGE PRETRAINED MODELS\nModel\nArchitecture\nSize\nTraining Data\nWhat to Pretrain\nHow to\nPretrain\nHardware\nCLIP [4]\nViT-L/14@336px and a\ntext encoder [92]\n0.307B\n400M image-text pairs\nzero-shot image\nclassi\ufb01cation\ncontrastive\npre-training\n\ufb01ne-tuned\nCLIP model is\ntrained for 12\ndays on 256\nV100 GPUs\nGPT-3 [2]\ntransformer (slight\nmodi\ufb01cation of GPT-2)\n175B\nCommon Crawl (about\na trillion words)\ntext output\nautoregressive\nmodel\nNPA *\nGPT-4 [3]\nNPA\nNPA\nNPA\ntext output\nNPA\nNPA\nPaLI-X [104]\nencoder-decoder\n55B\n10B image-text pairs\nfrom WebLI [102] and\nauxiliary tasks\ntext and image to\ntext output\nautoregressive\nmodel\nruns on\nmulti-TPU\ncloud service\nDALL-E [112]\ndecoder-only\ntransformer\n12B\n250M text-image pairs\nzero-shot\ntext-to-image\ngeneration\nautoregressive\nmodel\nNPA\nDALL-E2 [88]\na prior based on\nCLIP+ a decoder\n3.5B\nCLIP and\nDALL-E [112]\nzero-shot\ntext-to-image\ngeneration\ndiffusion\nNPA\nDINOv2 [107]\nViT-g/14\n1.1B\nLVD-142M [107]\nvisual-features\n(image-level and\npixel-level)\ndiscriminative\n20 nodes\nequipped with\n8 V100-32GB\nGPUs\nSAM [59]\nMAE [113] vision\ntransformer+CLIP\n[114] text encoder\n632M for\nViT-H + 63M\nfor CLIP text\nencoder\nSA-1B dataset [59]\nthat includes 1.1B\nsegmentation masks on\n11M images\nzero-shot\npromptable image\nsegmentation\nsupervised\nlearning\n256 A100\nGPUs for 68\nhours\n* NPA stands for not publicly available.\npotential to serve as valuable tools for enhancing robotic\ncapabilities. For instance, LLMs may facilitate the process\nof task speci\ufb01cation, allowing robots to receive and interpret\nhigh-level instructions from humans. VLMs may also promise\ncontributions to this \ufb01eld. VLMs specialize in the analysis of\nvisual data. This visual understanding is a critical component\nof informed decision-making and complex task execution for\nrobots. Robots can now leverage natural language cues to\nenhance their performance in tasks involving manipulation,\nnavigation, and interaction. Vision-language goal-conditioned\npolicy learning, whether through imitation learning or re-\ninforcement learning, holds promise for improvement using\nfoundation models. Language models also play a role in\noffering feedback for policy learning techniques. This feed-\nback loop fosters continual improvement in robotic decision-\nmaking, as robots can re\ufb01ne their actions based on the\nfeedback received from an LLM. This section underscores\nthe potential contributions of LLMs and VLMs in robot\ndecision-making. Assessing and comparing the contributions\nof papers in this section presents greater challenges compared\nto the other sections like the Perception Section (IV) or the\nEmbodied AI Section (V). This is due to the fact that most\npapers in this section either rely on hardware experiments,\nusing custom elements in the low-level control and planning\nstack that are not easily transferred to other hardware or\nother experimental setups, or they utilize non-physics-based\nsimulators, which allow these low-level parts of the stack to\nbe ignored, but leaving open the issue of non-transferability\nbetween different hardware implementations. In Section VI,\nwe discuss the lack of benchmarking and reproducibility that\nneeds to be addressed in future research.\nA. Robot Policy Learning for Decision Making and Control\nIn this section we discuss robot policy learning including\nlanguage-conditioned imitation learning and language-assisted\nreinforcement learning.\n1) Language-conditioned Imitation Learning for Manipu-\nlation: In language-conditioned imitation learning, a goal-\nconditioned policy \u03c0\u03b8(at|st, l) is learned that outputs actions\nat \u2208 A conditioned on the current state st \u2208 S and language\ninstruction l \u2208 L. The loss function is de\ufb01ned as the maximum\nlikelihood goal conditioned imitation objective:\nLGCIL = E(\u03c4,l)\u223cD\n|\u03c4|\nX\nt=0\nlog\u03c0\u03b8(at|st, l),\n(9)\nwhere D is the language-annotated demonstration dataset\nD = {\u03c4i}N\ni . Demonstrations can be represented as trajec-\ntories, or sequences of images, RGB-D voxel observations,\netc. Language instructions are paired with demonstrations\nto be used as the training dataset. Each language-annotated\ndemonstration \u03c4i consists of \u03c4i = {(s1, l1, a1), (s2, l2, a2), ...}.\nAt test time, the robot is given a series of instructions and the\nlanguage-conditioned visuomotor policy \u03c0\u03b8 provides actions\nat in a closed loop given the instruction at each time step. The\nmain challenges in this domain are: (i) obtaining a suf\ufb01cient\nvolume of demonstrations and conditioning labels to train a\npolicy, (ii) distribution shift under the closed-loop policy\u2014\nthe feedback of the policy can lead the robot into regions of\nthe state space that are not well-covered in the training data,\nnegatively impacting performance. (All the following papers\nin this subsection focus on robot manipulation tasks.)\nSince\ngenerating\nlanguage-annotated\ndata\nby\npairing\ndemonstrations with language instruction is an expensive pro-\n10\ncess, the authors in Play-LMP [26] propose learning from\nteleoperated play data. In this setting, reusable latent plan\nrepresentations are learned from unlabeled play data. Also,\na goal-conditioned policy is learned to decode the inferred\nplan to perform the task speci\ufb01ed by the user. In addition, the\ndistributional shift in imitation learning is analyzed and it is\nshown in this setting that the play data is more robust with\nrespect to perturbation compared to expert positive demon-\nstrations. Note that language goal l in (9) can be substituted\nwith any other type of goal for example goal image, which is\nanother common choice of goal in goal-conditioned imitation\nlearning.\nIn a follow-up work [28], the authors present multi-context\nimitation (MCIL) which uses language-conditioned imitation\nlearning over unstructured data. The multi-Context imitation\nframework is based on relabeled imitation learning and la-\nbeled instruction following. MCIL assumes access to mul-\ntiple contextual imitation datasets, for example, goal image\ndemonstrations, language goal demonstrations, or one-hot task\ndemonstrations. MCIL trains a single latent goal-conditioned\npolicy over all datasets simultaneously by encoding contexts\nin the shared latent space using the associated encoder for each\ncontext. Then a goal-conditioned imitation loss is computed\nby averaging over all datasets. The policy and goal-encoders\nare trained end-to-end. Another approach to tackle the data\nannotation challenge in language-conditioned imitation learn-\ning involves utilizing foundation models to offer feedback\nby labeling demonstrations. In [115], the authors propose to\nuse pretrained foundation models to provide feedback. To\ndeploy a trained policy to a new task or new environment, the\npolicy is played using randomly generated instructions, and\na pretrained foundation model provides feedback by labeling\nthe demonstration. Also, this paired instruction-demonstration\ndata can be used for policy \ufb01ne-tuning. CLIPort [25] also\npresents a language-conditioned imitation learning for vision-\nbased manipulation. A two-stream architecture is presented\nthat combines the semantic understanding of CLIP with the\nspatial precision of Transporter [116]. This end-to-end frame-\nwork solves language-speci\ufb01ed manipulation tasks without\nany explicit representation of the object poses or instance\nsegmentation. CLIPort grounds semantic concepts in precise\nspatial reasoning, but it is limited to 2D observation and\naction spaces. To address this limitation, the authors of PerAct\n(Perceiver-Actor) [27] propose to represent observation and\naction spaces with 3D voxels and employ the 3D structure\nof voxel patches for ef\ufb01cient language-conditioned behavioral\ncloning with transformers to imitate 6-DoF manipulation tasks\nfrom just a few demonstrations. While 2D behavioral cloning\nmethods such as CLIPort are limited to single-view obser-\nvations, 3D approaches such as PerAct allow for multi-view\nobservations as well as 6-DoF action spaces. PerAct uses\nonly CLIP\u2019s language encoder to encode the language goal.\nPerAct takes language goals and RGB-D voxel observations\nas inputs to a Perceiver Transformer and outputs discretized\nactions by detecting the next best voxel action. PerAct is\ntrained through supervised learning with discrete-time input\nactions from the demonstration dataset. The demonstration\ndataset includes voxel observations paired with language goals\nand keyframe action sequences. An action consists of a 6-\nDoF pose, gripper open state, and collision avoidance action.\nDuring training, a tuple is randomly sampled and the agent\npredicts the keyframe action given the observation and goal.\nGrounding semantic representations into a spatial environ-\nment is essential for effective robot interaction. CLIPort and\nPerAct utilize CLIP (which is trained based on contrastive\nlearning) for semantic reasoning and Transporter and Perceiver\nfor spatial reasoning.\nVoltron [29] presents a framework for language-driven\nrepresentation learning in robotics. Voltron captures semantic,\nspatial, and temporal representations that are learned from\nvideos and captions. Contrastive learning captures semantic\nrepresentations but loses spatial relationships, and in contrast,\nmasked autoencoding captures spatial and not semantic rep-\nresentations. Voltron trades off language-conditioned visual\nreconstruction for local spatial representations and visually-\ngrounded language generation to capture semantic represen-\ntations. This framework includes grasp affordance prediction,\nsingle-task visuomotor control, referring expression ground-\ning, language-conditioned imitation, and intent-scoring tasks.\nVoltron models take videos and their associated language\ncaptions as input to a multimodal encoder whose outputs are\nthen decoded to reconstruct one or more frames from a masked\ncontext. Voltron starts with a masked autoencoding backbone\nand adds a dynamic component to the model by conditioning\nthe MAE encoder on a language pre\ufb01x. Temporal information\nis captured by conditioning on multiple frames.\nDeploying robot policy learning techniques that lever-\nage language-conditioned imitation learning with real robots\npresents ongoing challenges. These models rely on end-to-end\nlearning, where the policy maps pixels or voxels to actions. As\nthey are trained through supervised learning on demonstration\ndatasets, they are susceptible to issues related to generalization\nand distribution shifts. To improve robustness and adaptability,\ntechniques such as data augmentation and domain adaptation\ncan make the policies more robust to the distribution shift.\nCACTI [14] is a novel framework designed to enhance\nscalability in robot learning using foundation models such as\nStable Diffusion [117]. CACTI introduces the four stages of\ndata collection, data augmentation, visual representation learn-\ning, and imitation policy training. In the data collection stage,\nlimited in-domain expert demonstration data is collected. In\nthe data augmentation stage, CACTI employs visual generative\nmodels such as Stable Diffusion [117] to boost visual diversity\nby augmenting the data with scene and layout variations.\nIn the visual representation learning stage, CACTI leverages\npretrained zero-shot visual representation models trained on\nout-of-domain data to improve training ef\ufb01ciency. Finally, in\nthe imitation policy training stage, a general multi-task policy\nis learned using imitation learning on the augmented dataset\nwith compressed visual representations as input. CACTI is\ntrained for multi-task and multi-scene manipulation in kitchen\nenvironments, both in simulation and the real world. The\nuse of these techniques enhances the generalization ability of\nthe framework and enables it to learn from a wide range of\nenvironments.\nBeyond language, recent works have investigated other\n11\nforms of task speci\ufb01cation. Notably, MimicPlay [118] presents\na hierarchical imitation learning algorithm that learns high-\nlevel plans in latent spaces from human play data and low-\nlevel motor commands from a small number of teleoperated\ndemonstrations. By harnessing the complementary strengths of\nthese two data sources, this algorithm can signi\ufb01cantly reduce\nthe cost of training visuomotor policies for long-horizon\nmanipulation tasks. Once trained, it is capable of performing\nnew tasks based on one human video demonstration at test\ntime. MUTEX [119] further explores learning a uni\ufb01ed policy\nacross multimodal task speci\ufb01cations in video, image, text,\nand audio, showing improved policy performances over single-\nmodality baselines through cross-modal learning.\n2) Language-Assisted Reinforcement Learning: Reinforce-\nment learning (RL) is a family of methods that enable a robot\nto optimize a policy through interaction with its environment\nby optimizing a reward function. These interactions are usually\nin a simulation environment, sometimes augmented with data\nfrom physical robot hardware for sim-to-real transfer. RL has\nclose ties to optimal control. Unlike imitation learning, RL\ndoes not require human demonstrations, and (in theory) has\nthe potential to attain super-human performance. In the RL\nproblem, the expected return of a policy is maximized using\nthe collected roll-outs from interactions with the environment.\nThe feedback received from the environment in the form\nof a reward signal guides the robot to learn which actions\nlead to favorable results and which do not. In this section,\nwe discuss works that have incorporated foundation models\n(LLM, VLMs, etc.) into RL problems.\nFast and \ufb02exible adaptation is a desired capability of ar-\nti\ufb01cial agents and is essential for progress toward general\nintelligence. In Adaptive Agent (AdA) [30] the authors present\nan RL foundation model that is an agent pretrained on di-\nverse tasks and is designed to quickly adapt to open-ended\nembodied 3D problems by using fast in-context learning from\nfeedback. This work considers navigation, coordination, and\ndivision of labor tasks. Given a few episodes within an unseen\nenvironment at test time, the agent engages in trial-and-error\nexploration to re\ufb01ne its policy toward optimal performance.\nIn AdA a transformer architecture is trained using model-\nbased RL2 [120] to train agents with large-scale attention-\nbased memory, which is required for adaptation. Transformer-\nXL [121] with some modi\ufb01cation is used to enable long\nand variable-length context windows to increase the model\nmemory to capture long-term dependencies. The agent collects\ndiverse data in the XLand environment that includes 1040\npossible tasks [122], in an automated curriculum. In addition,\ndistillation is used to enable scaling to models with more than\n500M parameters.\nPalo et al. [15] propose an approach to enhance reinforce-\nment learning by integrating Large Language Models (LLMs)\nand Visual-Language Models (VLMs) to create a more uni-\n\ufb01ed RL framework. This work considers robot manipulation\ntasks. Their approach addresses core RL challenges related to\nexploration, experience reuse and transfer, skills scheduling,\nand learning from observation. The authors use an LLM to\ndecompose complex tasks into simpler sub-tasks, which are\nthen utilized as inputs for a transformer-based agent to interact\nwith the environment. The agent is trained using a combination\nof supervised and reinforcement learning, enabling it to predict\nthe optimal sub-task to execute based on the current state of\nthe environment.\nB. Language-Image Goal-Conditioned Value Learning\nIn value learning, the aim is to construct a value func-\ntion that aligns goals in different modalities and preserves\ntemporal coherence due to the recursive nature of the value\nfunction. Reusable Representation for Robotic Manipulations\n(R3M) [31] provides pretrained visual representation for robot\nmanipulation using diverse human video datasets such as\nEgo4D and can be used as a frozen perception module for pol-\nicy learning in robot manipulation tasks. R3M\u2019s pretrained vi-\nsual representation is demonstrated on Franka Emika Panda\u2019s\narm and enables different downstream manipulation tasks.\nR3M is trained using time-contrastive learning to capture\ntemporal dependencies, video-language alignment to capture\nsemantic features of the scene (such as objects and their\nrelationships) and L1 penalty to encourage sparse and compact\nrepresentation. For a batch of videos, using time-contrastive\nloss, an encoder is trained to generate a representation wherein\nthe distance between images that are temporally closer is\nminimized compared to images that are farther apart in time\nor from different videos.\nSimilar to R3M, Value-Implicit Pretraining (VIP) [36] em-\nploys time-contrastive learning to capture temporal dependen-\ncies in videos, but it does not require video-language align-\nment. VIP is also focused on robot manipulation tasks. VIP is\na self-supervised approach for learning visual goal-conditioned\nvalue functions and representations from videos. VIP learns vi-\nsual goal-based rewards for downstream tasks and can be used\nfor zero-shot reward speci\ufb01cation. The reward model is derived\nfrom pretrained visual representations. Pretraining involves us-\ning unlabeled human videos. Human videos do not contain any\naction information to be used for robot policy learning, there-\nfore the learned value function does not explicitly depend on\nactions. VIP introduces a novel time contrastive objective that\ngenerates a temporally smooth embedding. The value function\nis implicitly de\ufb01ned via distance embedding. The proposed\nimplicit time contrastive learning attracts the representation of\nthe initial and goal frames in the same trajectory and repels\nthe representation of intermediate frames by recursive one-step\ntemporal difference minimization. This representation captures\nlong-term temporal dependencies across task frames and local\ntemporal smoothness among adjacent frames.\nLanguage-Image Value Learning (LIV) [37] is a control-\ncentric vision-language representation. LIV generalizes the\nprior work VIP by learning multi-modal vision-language value\nfunctions and representations using language-aligned videos.\nFor tasks speci\ufb01ed as language goals or image goals, a multi-\nmodel representation is trained that encodes a universal value\nfunction. LIV is also focused on robot manipulation tasks.\nLIV is a pretrained control-centric vision-language represen-\ntation based on large human video datasets such as EPIC-\nKITCHENS [123]. The representations are kept frozen during\npolicy learning. A simple MLP is used on top of pretrained\n12\nrepresentations for the policy network. Policy learning is de-\ncoupled from language-visual representation pretraining. The\nLIV model is pretrained on arbitrary video activity datasets\nwith text annotation, and the model can be \ufb01ne-tuned on\nsmall datasets of in-domain robot data to ground language\nin a context-speci\ufb01c way. LIV uses a generalization of the\nmutual information-based image-text contrastive representa-\ntion learning objective as used in CLIP, so LIV can be\nconsidered as a combination of CLIP and VIP. Both VIP and\nLIV learn a self-supervised goal-conditioned value-function\nobjective using contrastive learning. The LIV extends the VIP\nframework to multi-modal goal speci\ufb01cations. LOREL [38]\nlearns a language-conditioned reward from of\ufb02ine data and\nuses it during model predictive control to complete language-\nspeci\ufb01ed tasks.\nValue functions can be used to help ground semantic infor-\nmation obtained from an LLM to the physical environment in\nwhich a robot is operating. By leveraging value functions, a\nrobot can associate the information processed by the LLM\nwith speci\ufb01c locations and objects in its surroundings. In\nSayCan [32], researchers investigate the integration of large\nlanguage models with the physical world through learning.\nThey use the language model to provide task-grounding (Say),\nenabling the determination of useful sub-goals based on high-\nlevel instructions, and a learned affordance function to achieve\nworld-grounding (Can), enabling the identi\ufb01cation of feasible\nactions to execute the plan. Inner Monologue [33] studies the\nrole of grounded environment feedback provided to the LLM,\nthus closing the loop with the environment. The feedback\nis used for robot planning with large language models by\nleveraging a collection of perception models (e.g., scene\ndescriptors and success detectors) in tandem with pretrained\nlanguage-conditioned robot skills. Feedback includes task-\nspeci\ufb01c feedback, such as success detection, and scene-speci\ufb01c\nfeedback (either \u201cpassive\u201d or \u201cactive\u201d). In both SayCan and\nInner Monologue robot manipulation and navigation tasks\nare considered using a real-world mobile manipulator robot\nfrom Everyday Robots. Text2Motion [124] is a language-\nbased planning framework for long-horizon robot manipula-\ntion. Similar to SayCan and Inner Monologue, Text2Motion\ncomputes a score (SLMM) associated with each skill at each\ntime step. The task planning problem is to \ufb01nd a sequence of\nskills by maximizing the likelihood of a skill sequence given a\nlanguage instruction and the initial state. In Text2Motion, the\nauthors propose to verify that the generated long-horizon plans\nare symbolically correct and geometrically feasible. Hence, a\ngeometric feasibility score (Sgeo) is de\ufb01ned as the probability\nthat all the skills in the sequence achieve rewards. To compute\nthe overall score, the LLM score is multiplied by the geometric\nfeasibility score (SSkill = SLMM \u00b7 Sgeo).\nVoxPoser [34] builds 3D value maps to ground affordances\nand constraints into the perceptual space. VoxPser considers\nrobot manipulation tasks. Given the RGB-D observation of\nthe environment and language instruction, VoxPoser utilizes\nlarge language models to generate code, which interacts with\nvision-language models to extract a sequence of 3D affordance\nmaps and constraint maps. These maps are composed together\nto create 3D value maps. The value maps are then utilized\nas objective functions to guide motion planners to synthesize\ntrajectories for everyday manipulation tasks without requiring\nany prior training or instruction.\nIn [35], reward shaping using CLIP is presented. This\nwork considers robot manipulation tasks. The proposed model\nutilizes CLIP to ground objects in a scene described by the\ngoal text paired with spatial relationship rules to shape the\nreward by using raw pixels as input. They use developments in\nbuilding large-scale visuo-lingual models like CLIP to devise\na framework that generates the task reward signal from just the\ngoal text description and raw pixel observations. This signal\nis then used to learn the task policy.\nIn [125], Hierarchical Universal Language Conditioned\nPolicies 2.0 (HULC++) is presented. This work considers\nrobot manipulation tasks. A self-supervised visuo-lingual af-\nfordance model is used to learn general-purposed language-\nconditioned robot skills from unstructured of\ufb02ine data in the\nreal world. This method requires annotating as little as 1%\nof the total data with language. The visuo-lingual affordance\nmodel has an encoder-decoder architecture with two decoder\nheads. Both heads share the same encoder and are condi-\ntioned on the input language instruction. One head predicts\na distribution over the image, in which each pixel likelihood\nis an afforded point. The other head predicts a Gaussian\ndistribution from which the corresponding predicted depth is\nsampled. Given visual observations and language instructions\nas input, the affordance model outputs a pixel-wise heat map\nthat represents affordance regions and the corresponding depth\nmap.\nC. Robot Task Planning using Large Language Models\nLLMs can be used to provide high-level task planning for\nperforming complex long-horizon robot tasks.\n1) Language Instructions for Task Speci\ufb01cation: As dis-\ncussed above, SayCan [32] uses an LLM for high-level task\nplanning in language, though with a learned value function to\nground these instructions in the environment.\nTemporal logic is useful for imposing temporal speci\ufb01-\ncations in robotic systems. In [39], translation from natural\nlanguage (NL) to temporal logic (TL) is proposed. A dataset\nwith 28k NL-TL pairs is created and the T5 [126] model\nis \ufb01netuned using the dataset. LLMs are often used to plan\ntask sub-goals. This work considers robot navigation tasks.\nIn [40], instead of direct task planning, a few-shot translation\nfrom a natural language task description to an intermediary\ntask representation is performed. This representation is used\nby a Task and Motion Planning (TAMP) algorithm to jointly\noptimize task and motion plans. Autoregressive re-prompting\nis used to correct synthetic and semantic errors. This work\nalso considers robot navigation tasks.\n2) Code Generation using Language Models for Task\nPlanning: Classical task planning requires extensive domain\nknowledge and the search space is large [127], [128]. LLMs\ncan be used to generate sequences of tasks required to achieve\na high-level task. In ProgPrompt [41], the authors introduce\na prompting method that uses LLMs to generate sequences\nof actions directly with no additional domain knowledge. The\n13\nprompt to the LLM includes speci\ufb01cations of the available\nactions, objects in the environment, and example programs that\ncan be executed. VirtualHome [129] is used as a simulator for\ndemonstration.\nCode-as-Policies [42] explores the use of code-writing\nLLMs to generate robot policy code based on natural lan-\nguage commands. This work considers robot manipulation and\nnavigation tasks using a real-world mobile manipulator robot\nfrom Everyday Robots. The study demonstrates that LLMs can\nbe repurposed to write policy code by expressing functions\nor feedback loops that process perception outputs and invoke\ncontrol primitive APIs. To achieve this, the authors utilize few-\nshot prompting, where example language commands formatted\nas comments are provided along with the corresponding policy\ncode. Without any additional training on this data, they enable\nthe models to autonomously compose API calls and generate\nnew policy code when given new commands. The approach\nleverages classic logic structures and references third-party\nlibraries like NumPy and Shapely to perform arithmetic op-\nerations. By chaining these structures and using contextual\ninformation (behavioral commonsense), the LLMs can gen-\nerate robot policies that exhibit spatial-geometric reasoning,\ngeneralize to new instructions, and provide precise values\n(e.g., velocities) for ambiguous descriptions such as \u201cfaster.\u201d\nThe concept of \u201ccode as policies\u201d formalizes the generation\nof robot policies using language model-generated programs\n(LMPs). These policies can represent reactive policies like\nimpedance controllers, as well as waypoint-based policies such\nas vision-based pick and place or trajectory-based control.\nThe effectiveness of this approach is demonstrated on multiple\nreal robot platforms. A crucial aspect of this approach is the\nhierarchical code generation process, which involves recur-\nsively de\ufb01ning unde\ufb01ned functions. This enables the LLMs to\ngenerate more complex code structures to ful\ufb01ll the desired\npolicy requirements.\nIn [43], the authors provide design principles for using\nChatGPT in robotics and demonstrate how LLMs can help\nrobotic capabilities rapidly generalize to different form factors.\nThis work considers robot manipulation and aerial navigation\ntasks. First, a high-level robot function library that maps to\nmultiple atomic tasks executable by the robot is de\ufb01ned. Then,\na prompt is crafted that includes these functions, and the\nrequired constraints along the task description. ChatGPT then\nprovides executable code speci\ufb01c to the given robot con\ufb01gura-\ntion and task. The generated code can then be evaluated by a\nuser and appropriate feedback with modi\ufb01ed prompts to LLMs\nfurther help re\ufb01ne and generate programs that are safe and\ndeployable on the physical robot. The study demonstrated that\nsuch a methodology can be applied to multiple form factors\nboth in simulation and in the real world.\nD. In-context Learning (ICL) for Decision-Making\nIn-context Learning (ICL) [130] operates without the need\nfor parameter optimization, relying instead on a set of exam-\nples included in the prompt (the concept of prompting). This\nlearning approach is intimately linked with prompt engineering\nand \ufb01nds extensive use in natural language processing. The\nmethod of Chain-of-Thought [131] is a prominent technique\nwithin in-context learning. It involves executing a sequence of\nintermediate steps to arrive at the \ufb01nal solution for complex,\nmulti-step problems. This technique allows models to pro-\nduce step-by-step explanations that parallel human cognitive\nprocesses. However, despite its numerous bene\ufb01ts, ICL also\nfaces certain challenges, including issues related to ambiguity\nand interpretation, domain-speci\ufb01c knowledge, transparency,\nand explainability. In-context learning has had a signi\ufb01cant\nimpact on the \ufb01eld of LLMs in a broad sense, and many\nrobotics works have used it to apply LLMs to speci\ufb01c domains.\nInvestigating this, Mirchandani and colleagues [132] illustrate\nthat Large Language Models (LLMs) possess remarkable pat-\ntern recognition abilities. They reveal that, through in-context\nlearning, LLMs can effectively handle general patterns that ex-\ntend beyond standard language-based prompts. This capability\nallows for the application of LLMs in scenarios such as of\ufb02ine\ntrajectory optimization and online, in-context reinforcement\nlearning. Additionally, Jia and the team in their work on\nChain-of-Thought Predictive Control [133] suggest a method\nto identify speci\ufb01c brief sequences within demonstrations,\ntermed as \u2019chain-of-thought\u2019. They focus on understanding\nand representing the hierarchical structure of these sequences,\nhighlighting the achievement of subgoals within tasks. This\nwork considers robot policy learning from demonstrations for\ncontact-rich object manipulation tasks.\nE. Robot Transformers\nFoundation models can be used for end-to-end control of\nrobots by providing an integrated framework that combines\nperception, decision-making, and action generation.\nXiao et al. [48] demonstrate the effectiveness of self-\nsupervised visual pretraining using real-world images for\nlearning motor control tasks directly from pixel inputs. This\nwork is focused on robot manipulation tasks. They show\nthat without any task-speci\ufb01c \ufb01ne-tuning of the pretrained\nencoder, the visual representations can be utilized for various\nmotor control tasks. This approach highlights the potential\nof leveraging self-supervised learning from real-world images\nto acquire general visual representations that can be applied\nacross different motor control tasks. Similarly, Radosavovic et\nal. [49] investigate the use of self-supervised visual pretraining\non diverse, in-the-wild videos for real-world robotic tasks.\nThis work considers robot manipulation tasks. They \ufb01nd that\nthe pretrained representations obtained from such videos are\neffective in a range of real-world robotic tasks, considering\ndifferent robotic embodiments. This suggests that the learned\nvisual representations generalize well across various tasks\nand robot platforms, demonstrating the broad applicability of\nself-supervised pretraining for real-world robotic applications.\nBoth studies emphasize the advantages of self-supervised\nvisual pretraining, where models are trained on large amounts\nof unlabeled data to learn useful visual representations. By\nleveraging real-world images and videos, these approaches\nenable learning from diverse and unstructured visual data,\nleading to more robust and transferable representations for\nmotor control tasks in robotic systems.\n14\nAnother example of a Transformer-based policy model is\nthe work on Robotics Transformer (RT-1) [44], where the\nauthors demonstrate a model that shows promising scalability\nproperties. To train the model, the authors use a large dataset\nof over 130k real-world robotic experiences, comprising more\nthan 700 tasks, that was collected over 17 months using a\n\ufb02eet of 13 robots. RT-1 receives images and natural language\ninstructions as inputs and outputs discretized base and arm\nactions. It can generalize to new tasks, maintain robustness in\nchanging environments, and execute long-horizon instructions.\nThe authors also demonstrate the model\u2019s capability to effec-\ntively absorb data from diverse domains, including simulations\nand different robots.\nThe follow-up work, called Robotic Transformer 2 (RT-\n2) [45], demonstrates a vision-language-action (VLA) model\nthat takes a step further by learning from both web and robotics\ndata. The model effectively utilizes this data to generate\ngeneralized actions for robotic control. To do so, the authors\nuse pre-existing vision-language models and directly co-\ufb01ne-\ntune them on robot trajectories resulting in a single model that\noperates as a language model, a vision-language model, and a\nrobot policy. To make co-\ufb01ne-tuning possible, the actions are\nrepresented as simple text strings which are then tokenized\nusing an LLM tokenizer into text tokens. The resulting model,\nRT-2, enables vision-language models to output low-level\nclosed-loop control. Similarly to RT-1, actions are produced\nbased on robot instructions paired with camera observations\nand the action space includes 6-DoF positional and rotational\ndisplacement of the robot end-effector, gripper extension, and\nepisode termination command. Via extensive experiments, the\nauthors show that utilizing VLMs aids in the enhancement\nof generalization across visual and semantic concepts and\nenables the robots to respond to the so-called chain of thought\nprompting, where the agent performs more complex, multi-\nstage semantic reasoning. Both RT-1 and RT-2 consider robot\nmanipulation and navigation tasks using a real-world mobile\nmanipulator robot from Everyday Robots. One key limitation\nof RT-2 and other related works in robotics is the fact that the\nrange of physical skills exhibited by the robot is limited to the\ndistribution of skills observed within the robot\u2019s data. While\none way to approach this limitation is to collect more diverse\nand dexterous robotic data, there might be other intriguing\nresearch directions such as using motion data in human videos,\nrobotic simulations, or other robotic embodiments.\nThe next work utilizing the Transformer architecture in-\ndeed focuses on learning from data that combines multiple\nrobotic embodiments. In RT-X [46], the authors provide a\nnumber of datasets in a standardized data format and models\nto make it possible to explore the possibility of training\nlarge cross-embodied robotic models in the context of robotic\nmanipulation. In particular, they assembled a dataset from\n22 different robots collected through a collaboration between\n21 institutions, demonstrating 527 skills (160266 tasks). With\nthis uni\ufb01ed dataset, RT-X demonstrates that RT-1- and RT-2-\nbased models trained on this multi-embodiment, diverse data\nexhibit positive transfer across robotic domains and improve\nthe capabilities of multiple robots by leveraging experience\nfrom other platforms.\nOther works have investigated general pretrained transform-\ners for robot control, trained with self-supervised trajectory\ndata from multiple robots. For example, Perception-Action\nCausal Transformer (PACT) [47] is a generative transformer\narchitecture that builds representations from robot data with\nself-supervision. This work considers robot navigation tasks.\nPACT pretrains a representation useful for multiple tasks on\na given robot. Similar to how large language models learn\nfrom extensive text data, PACT is trained on abundant safe\nstate-action data (trajectories) from a robot, learning to predict\nappropriate safe actions. By predicting states and actions\nover time in an autoregressive manner, the model implicitly\ncaptures dynamics and behaviors speci\ufb01c to a robot. PACT\nwas tested in experiments involving mobile agents: a wheeled\nrobot with a LiDAR sensor (MuSHR) and a simulated agent\nusing \ufb01rst-person RGB images (Habitat). The results show\nthat this robot-speci\ufb01c representation can serve as a starting\npoint for tasks like safe navigation, localization, and mapping.\nAdditionally, the experiments demonstrated that \ufb01ne-tuning\nsmaller task-speci\ufb01c networks on the pre-trained model leads\nto signi\ufb01cantly better performance compared to training a\nsingle model from scratch for all tasks simultaneously, and\ncomparable performance to training a separate large model\nfor each task independently.\nAnother work in this space is Self-supervised Multi-task\npretrAining with contRol Transformer (SMART) [134], which\nintroduces a self-supervised multi-task pertaining to control\ntransformers, providing a pretraining-\ufb01netuning approach tai-\nlored for sequential decision-making tasks. During the pre-\ntraining phase, SMART captures information essential for both\nshort-term and long-term control, facilitating transferability\nacross various tasks. Subsequently, the \ufb01netuning process can\nadapt to a wide variety of tasks spanning diverse domains. Ex-\nperimentation underscores SMART\u2019s ability to enhance learn-\ning ef\ufb01ciency across tasks and domains. This work considers\ncart pole-swing-up, cart pole-balance, hopper-hop, hopper-\nstand, cheetah-run, walker-stand walker-run, and walker-walk\ntasks. The approach demonstrates robustness against distribu-\ntion shifts and proves effective with low-quality pretraining\ndatasets.\nSome works have investigated transformer models in con-\njunction with classical planning and control layers as part of\na modular robot control architecture. For example, in [50], a\nmulti-modal transformer (LATTE) is presented that allows a\nuser to reshape robot trajectories using language instructions.\nThis work considers both robot manipulation and navigation\ntasks. LATTE transformer takes as input geometrical features\nof an initial trajectory guess along with the obstacle map\ncon\ufb01guration, language instructions from a user, and images of\neach object in the environment. The model\u2019s output is modi\ufb01ed\nfor each waypoint in the trajectory so that the \ufb01nal robot\nmotion can adhere to the user\u2019s language instructions. The\ninitial trajectory plan can be generated using any geometric\nplanner such as A\u2217, RRT\u2217, or model predictive control. Sub-\nsequently, this plan is enriched with the semantic objectives\nwithin the model. LATTE leverages pretrained language and\nvisual-language models to harness semantic representations of\nthe world.\n15\nF. Open-Vocabulary Robot Navigation and Manipulation\n1) Open-Vocabulary Navigation: Open-vocabulary naviga-\ntion addresses the challenge of navigating through unseen\nenvironments. The open-vocabulary capability signi\ufb01es that\nthe robot possesses the capacity to comprehend and respond to\nlanguage cues, instructions, or semantic information, without\nbeing restricted to a prede\ufb01ned dataset. In this section, we\nexplore papers that examine the integration of LLMs, VLMs,\nor a combination of both in a plug-and-play manner for\nrobot navigation tasks. Additionally, we discuss papers that\ntake a different approach by constructing foundation models\nexplicitly tailored for robot navigation tasks.\nIn VLN-BERT [135], the authors present a visual-linguistic\ntransformer-based model that leverages multi-modal visual and\nlanguage representations for visual navigation using web data.\nThe model is designed to score the compatibility between an\ninstruction, such as \u201c...stop at the brown sofa,\u201d and a sequence\nof panoramic RGB images captured by the agent.\nSimilarly, LM-Nav [136] considers visual navigation tasks.\nLM-Nav is a system that utilizes pretrained models of images\nand language to provide a textual interface to visual naviga-\ntion. LM-Nav demonstrates visual navigation in a real-world\noutdoor environment from natural language instructions. LM-\nNav utilizes an LLM (GPT-3 [2]), a VLM (CLIP [4]), and a\nVNM (Visual Navigation Model). First, LM-Nav constructs a\ntopological graph of the environment via the VNM estimating\nthe distance between images. The LLM is then used to\ntranslate the natural instructions to sequences of intermediate\nlanguage landmarks. The VLM is used to ground the visual\nobservations in landmark descriptions via a joint probability\ndistribution over landmarks and images. Using the VLM\u2019s\nprobability distribution, the LLM instructions, and the VNM\u2019s\ngraph connectivity, the optimal path is planned using the\nsearch algorithm. Then the plan is executed by the goal-\nconditioned policy of VNM.\nWhile LM-Nav makes use of LLMs and VLMs as plug-and-\nplay for visual navigation tasks, the authors of ViNT [137]\npropose to build a foundation model for visual navigation\ntasks. ViNT is an image goal-conditioned navigation policy\ntrained on diverse training data and can control different\nrobots in zero-shot. It can be \ufb01ne-tuned to be adapted for\ndifferent robotic platforms and various downstream tasks.\nViNT is trained on various navigation datasets from different\nrobotic platforms. It is trained with goal-reaching objectives\nand utilizes a Transformer-based architecture to learn navi-\ngational affordances. ViNT encodes visual observations and\nvisual goals using an Ef\ufb01cientNet CNN and predicts temporal\ndistance and normalized actions in an embodiment-agnostic\nmanner. Additionally, ViNT can be augmented with diffusion-\nbased sub-goal proposals to help explore environments not\nencountered during training. An image-to-image diffusion\ngenerates sub-goal images, which the ViNT then navigates\ntoward while building a topological map in the background.\nAnother work that considers zero-shot navigation tasks is\nAudio Visual Language Maps (AVLMaps) [138]. AVLMaps\npresents a 3D spatial map representation for cross-modal in-\nformation from audio, visual, and language cues. AVLMaps re-\nceives multi-modal prompts and performs zero-shot navigation\ntasks in the real world. The inputs are depth and RGB images,\ncamera pose, and audio. Visual features are encoded using\npretrained foundation models. Visual localization features\n(using NetVLAD [139], SuperPoint [140]), visual-language\nfeatures (using LSeg [58]), and audio-language features (using\nAudioCLIP [141]) are computed and predictions from different\nmodalities are combined into 3D heatmaps. The pixel-wise\njoint probability of the heatmap is computed and used for\nplanning. Additionally, navigation policies are generated as\nexecutable codes with the help of GPT-3. Finally, 3D heatmaps\nare predicted indicating the location of multimodal concepts\nsuch as objects, sounds, and images.\nMany roboticists may wonder about the comparative\nstrengths of classical modular robot navigation systems versus\nend-to-end learned systems. Semantic navigation [142] seeks\nto address this question by presenting an empirical analysis\nof semantic visual navigation methods. The study compares\nrepresentative approaches from classical, modular, and end-\nto-end learning paradigms across six different homes, without\nany prior knowledge, maps, or instrumentation. The \ufb01ndings\nof the study reveal that modular learning methods perform\nwell in real-world scenarios. In contrast, the end-to-end learn-\ning approaches face challenges due to a signi\ufb01cant domain\ngap between simulated and real-world images. This domain\ngap hinders the effectiveness of end-to-end learning methods\nin real-world navigation tasks. For practitioners, the study\nemphasizes that modular learning is a reliable approach to\nobject navigation. The modularity and abstraction in policy\ndesign enable successful transfer from simulation to reality,\nmaking modular learning an effective choice for practical\nimplementations. For researchers, the study also highlights two\ncritical issues that limit the reliability of current simulators as\nevaluation benchmarks. Firstly, there exists a substantial Sim-\nto-Real gap in images, which hampers the transferability of\nlearned policies from simulation to the real world. Secondly,\nthere is a disconnect between simulation and real-world error\nmodes, which further complicates the evaluation process.\nAnother line of work in open-vocabulary navigation is\nobject navigation tasks. In this task, the robot must be able\nto \ufb01nd the object described by humans and navigate towards\nthe object. The navigation task is decomposed into exploration\nwhen the language target is not detected and exploitation when\nthe target is detected and the robot navigates toward the target.\nAs the robot moves in the environment, it creates a top-down\nmap using RGB-D observations and poses estimates. In [143],\nthe authors introduce a zero-shot object navigation setting that\nuses an open-vocabulary classi\ufb01er such as CLIP [4] to compute\nthe cosine similarity between an image and a user-speci\ufb01ed\ndescription.\nCommon datasets and benchmarks for these types of prob-\nlems are Matterport3D [144], [145], Gibson [146] and Habi-\ntat [147]. L3MVN [148] enhances visual target navigation\nby constructing an environment map and selecting long-term\ngoals using the inference capabilities of large language models.\nThe system can determine appropriate long-term goals for\nnavigation by leveraging pretrained language models such\nas RoBERTa-large [149], enabling ef\ufb01cient exploration and\n16\nsearching. Chen et al. [150] presents a training-free and\nmodular system for object goal navigation, which constructs\na structured scene representation through active exploration.\nThe system utilizes semantic information in the scene graphs\nto deduce the location of the target object and integrates\nsemantics with the geometric frontiers to enable the agent to\nnavigate effectively to the most promising areas for object\nsearch while avoiding detours in unfamiliar environments.\nHomeRobot [151] introduces a benchmark for the Open-\nVocabulary Mobile Manipulation (OVMM) task. OVMM task\nis the problem of \ufb01nding an object in any unseen environment,\nnavigating towards the object, picking it up, and navigating\ntowards a goal location to place the object. HomeRobot\nprovides a benchmark in simulation and the real world for\nOVMM tasks.\n2) Open-Vocabulary Manipulation: Open-vocabulary ma-\nnipulation refers to the problem of manipulating any object in\na previously unseen environment. VisuoMotor Attention Agent\n(VIMA) [152] learns robot manipulation from multi-modal\nprompts. VIMA is a transformer-based agent that predicts\nmotor commands conditioned on a task prompt and a history\nof interactions. VIMA It introduces a new form of task speci-\n\ufb01cations that combines textual and visual tokens. Multi-modal\nprompting converts different robot manipulation tasks, such as\nvisual goal-reaching, learning from visual demonstrations, and\nnovel concept grounding into one sequence modeling problem.\nIt offers the training of a uni\ufb01ed policy across diverse tasks,\npotentially allowing for zero-shot generalization to previously\nunseen ones. VIMA-BENCH is introduced as a benchmark\nfor multi-modal robot learning. The VIMA-BENCH simulator\nsupports collections of objects and textures that can be utilized\nin multi-modal prompting. RoboCat [153] is a self-improving\nAI agent. It uses a 1.18B-parameter decoder-only transformer.\nIt learns to operate different robotic arms, solves tasks from as\nfew as 100 demonstrations, and improves from self-generated\ndata. RoboCat is based on Gato [154] architecture and is\ntrained with a self-improvement cycle.\nFor robots to operate effectively in the real world they\nmust be able to manipulate previously unseen objects. Liu\net al. present StructDiffusion [155], which seeks to enable\nrobots to use partial viewpoint clouds and natural language\ninstructions to construct a goal con\ufb01guration for objects that\nwere previously seen or unseen. They accomplish this by\n\ufb01rst using segmentation to break up the scene into objects.\nThen they use a multi-model transformer to combine word\nand point cloud embeddings and output a 6-DoF goal pose\nprediction. The predictions are iteratively re\ufb01ned via diffusion\nand a discriminator that is trained to determine if a sampled\ncon\ufb01guration is feasible. Manipulation of Open-World Objects\n(MOO) [156] leverages a pretrained vision-language model to\nextract object-centric information from the language command\nand the image and conditions the robot policy on the current\nimage, the instructions, and the extracted object information\nin a form of a single-pixel overlaid onto the image. MOO\nuses Owl-ViT for object detection and RT-1 for language-\nconditioned policy learning.\nAnother task in robot manipulation involves autonomous\nscene rearrangement and in-painting. DALL-E-Bot [157] per-\nforms zero-shot autonomous rearrangement in the scene in\na human-like way using pretrained image diffusion model\nDALL-E2 [88]. DALL-E-Bot autonomous object rearrange-\nment does not require any further data collection or train-\ning. First, the initial observation image (of the disorganized\nscene) is converted into a per-object representation including\na segmentation mask using Mask R-CNN [158], an object\ncaption, and a CLIP visual feature vector. Then a text prompt\nis generated by describing the object in the scene and is given\nto DALL-E to create a goal image for the rearrangement\ntask (the objects should be rearranged in a human-like way).\nNext, the objects in the initial and generated images are\nmatched using their CLIP visual features. Poses are estimated\nby aligning their segmentation masks. The robot rearranges\nthe scene based on the estimated poses to create the generated\narrangement.\nIn Table II some robotic-speci\ufb01c foundation models are re-\nported along with information about their size and architecture,\npretrained task, inference time, and hardware setup.\nIV. PERCEPTION\nRobots interacting with their surrounding environments re-\nceive raw sensory information in different modalities such as\nimages, video, audio, and language. This high-dimensional\ndata is crucial for robots to understand, reason, and interact\nin their environments. Foundation models, including those\nthat have been developed in the vision and NLP domains,\nare promising tools for converting these high-dimensional\ninputs into abstract, structured representations that can be\nmore easily interpreted and manipulated. Particularly, multi-\nmodal foundation models enable robots to integrate different\nsensory inputs into a uni\ufb01ed representation encompassing\nsemantic, spatial, temporal, and affordance information. These\nmulti-modal models re\ufb02ect cross-modal interactions, often by\naligning elements across modalities to ensure coherence and\ncorrespondence. For example, text and image data are aligned\nfor image captioning tasks. This section will explore a range\nof tasks related to robot perception that are improved through\naligning modalities using foundation models, with a focus on\nvision and language. There is an extensive body of literature\nstudying multi-modality in the machine learning community,\nand an interested reader is referred to the survey paper [161]\nthat presents a taxonomy of multi-modal learning. We focus\non applications of multi-modal models to robotics.\nA. Open-Vocabulary Object Detection and 3D Classi\ufb01cation\n1) Object Detection: Zero-shot object detection allows\nrobots to identify and locate objects they have never en-\ncountered previously. Grounded Language-Image Pre-training\n(GLIP) [52] integrates object detection and grounding by\nrede\ufb01ning object detection as phrase grounding. This refor-\nmulation enables the learning of a visual representation that is\nboth language-aware and semantically rich at the object level.\nIn this framework, the input to the detection model comprises\nnot only an image but also a text prompt that describes all\nthe potential categories for the detection task. To train GLIP,\na dataset of 27 million grounding instances was compiled,\n17\nTABLE II\nPRETRAINED MODELS FOR ROBOTICS\nPaper\nBackbone\nSize (Pa-\nrameters)\nPretrained Task\nInference\nSpeed\nHardware *\nRoboCat [153]\ndecoder-only transformer\n1.18B\nmanipulation\n10-20Hz\nGato [154]\ndecoder-only transformer\n1.2B\ngeneralist agent\n20Hz\n4 days on 16x16 TPU v3 slice\nPaLM-E-562B [6]\ndecoder-only transformer\n562B\n1Hz for Language\nsubgoals + 5Hz\nlow-level control\npolicies\n5-6Hz\nruns on multi-TPU cloud service\nViNT [137]\nEf\ufb01cientNet+ decoder transformer\n31M\nvisual navigation\n4Hz\nvariety of GPU con\ufb01gurations is\nused including 2\u00d74090, 3\u00d7Titan\nXp, 4\u00d7P100, 8\u00d71080Ti, 8\u00d7V100,\nand 8\u00d7A100\nVPT [72]\na temporal convolution layer, a\nResNet 62 image processing\nstack, and residual unmasked\nattention layers,\n0.5B\nembodied agent in\nMinecraft\n20Hz\n9 days on 720 V100 GPUs\nRT-1 [44]\nConditioned Ef\ufb01cientNet +\nTokenLearner + decoder-only\ntransformer\n35M\nreal-world robotics\ntasks\n3Hz\nRT-2 [45]\nPaLI-X\n55B\nreal-world robotics\ntasks\n1-3Hz\nruns on multi-TPU cloud service\nRT-2-X [46]\nViT and Language model UL2\n[159]\n55B\nreal-world robotics\ntasks\n1-3Hz\nruns on multi-TPU cloud service\nLIV [37]\nCLIP\nreward learning\n15Hz\n8 NVIDIA V100 GPUs\nSMART [134]\ndecoder-only transformer\n11M\nbidirectional\ndynamics prediction\nand masked hindsight\ncontrol\n1 Hz\n8 Nvidia V100 GPUs\nCOMPASS [160]\n3D-Resnet encoder\n20M\nContrastive loss\n30 Hz\n8 Nvidia V100 GPUs\nPACT [47]\ndecoder-only transformer\n12M\nforward dynamics\nand next action\nprediction\n10 Hz\n(edge) / 50\nHz\nNvidia Xavier NX (edge) / 8\nNvidia V100 GPUs\n* Empty \ufb01elds in the table denote no data is reported.\nconsisting of 3 million human-annotated pairs and 24 million\nimage-text pairs obtained by web crawling. The results of\nthe study demonstrate the remarkable zero-shot and few-shot\ntransferability of GLIP to a wide range of object-level recogni-\ntion tasks. Recently, PartSLIP [162] demonstrated that GLIP\ncan be used for low-shot part segmentation on 3D objects.\nPartSLIP renders a 3D point cloud of an object from multiple\nviews and combines 2D bounding boxes in these views to\ndetect object parts. To deal with noisy 2D bounding boxes\nfrom different views, PartSLIP runs a voting and grouping\nmethod on super points from 3D, assigns multi-view 2D labels\nto super points, and \ufb01nally groups super points to obtain\na precise part segmentation. To enable few-shot learning of\n3D part segmentation, prompt tuning, and multi-view feature\naggregation are proposed to improve performance.\nOWL-ViT [51] is an open-vocabulary object detector. OWL-\nViT uses a vision transformer architecture with contrastive\nimage-text pre-training and detection end-to-end \ufb01ne-tuning.\nUnlike GLIP, which frames detection as a phrase grounding\nproblem with a single text query and limits the number of\npossible object categories, OWL-ViT can handle multiple text-\nbased or image-driven queries. OWL-ViT has been applied\nto robot learning for example in VoxPoser [34] as the open-\nvocabulary object detector to \ufb01nd \u201centities of interest\u201d (e.g.,\nvase or drawer handles) and ultimately de\ufb01ne value maps for\noptimizing manipulation trajectories.\nGrounding DINO [53] combines DINO [105] with grounded\npre-training, extending the closed-set DINO model to open-\nset detection by fusing vision and language. Grounding DINO\noutperforms GLIP in open-set object detection. This superior\nperformance is mainly due to the transformer architecture of\nGrounding DINO, which facilitates multi-modal feature fusion\nat multiple stages.\n2) 3D Classi\ufb01cation: Zero-shot 3D classi\ufb01ers can enable\nrobots to classify objects in their environments without explicit\ntraining data. Foundation models are strong candidates for\nperforming 3D classi\ufb01cation. PointCLIP [54] transfers CLIP\u2019s\npre-trained knowledge of 2D images to 3D point cloud un-\nderstanding by aligning point clouds with text. The authors\npropose to project each point onto a series of pre-de\ufb01ned\nimage planes to generate depth maps. Then, the CLIP visual\nencoder is used to encode multi-view features of the point\ncloud and predict labels in natural language for each view. The\n\ufb01nal prediction for the point cloud is computed via weighted\naggregation of the predictions for each view. PointBERT [55]\nuses a transformer-based architecture to extract features from\npoint clouds, generalizing the concept of BERT into 3D point\nclouds.\nUnlike PointCLIP which converts the task of matching point\nclouds and text to image-text alignment, ULIP [56], [57]\nis a Uni\ufb01ed representation of Language, Images, and Point\nclouds for 3D understanding. It achieves this by pre-training\nwith object triplets (image, text, point cloud). The model is\ntrained using a small number of automatically synthesized\ntriplets from ShapeNet55 [163], which is a large-scale 3D\nmodel repository. ULIP uses CLIP as the vision-language\n18\nmodel. During pretraining, the CLIP model is kept frozen\nand a 3D encoder is trained by aligning the 3D features\nof an object with its associated textual and visual features\nfrom CLIP using contrastive learning. The pretraining process\nallows ULIP to learn a joint embedding space where the three\nmodalities are aligned. One of the major advantages of ULIP\nis that it can substantially improve the recognition ability of\n3D backbone models. This is because the pretraining process\nallows ULIP to learn more robust and discriminative features\nfor each modality, which can then be used to improve the\nperformance of 3D models. Another advantage of ULIP is\nthat it is agnostic to the 3D model architecture, and thus can\nbe easily integrated into the pretraining process of existing\n3D pipelines. ULIP adopts masked language modeling from\nBERT to 3D by tokenizing 3D patches randomly masking\nout 3D tokens and predicting them back during pretraining.\nULIP [56], [57] has shown that the performance of recognition\ncapability of models such as PointBERT can be improved by\nusing a uni\ufb01ed multimodal representation of ULIP.\nB. Open-Vocabulary Semantic Segmentation\nSemantic segmentation classi\ufb01es each pixel in an image\ninto semantic classes. This provides \ufb01ne-grained information\nabout object boundaries and locations within an image and\nenables embodied agents to understand and interact with the\nenvironment at a more granular level. Several works explore\nhow foundation models such as CLIP can enhance the gener-\nalizability and \ufb02exibility of semantic segmentation tasks.\nLSeg\nis\na\nlanguage-driven\nsemantic\nsegmentation\nmodel [58] that associates semantically similar labels to\nsimilar regions in an embedding space. LSeg uses a text\nencoder based on the CLIP architecture to compute text\nembeddings and an image encoder with the underlying\narchitecture of Dense Prediction Transformer (DPT) [164].\nSimilar to CLIP, LSeg creates a joint embedding space using\ntext and image embeddings. LSeg freezes the text encoder at\ntraining time and trains the image encoder to maximize the\ncorrelation between the text embedding and the image pixel\nembedding of the ground-truth pixel class. It allows users\nto arbitrarily shrink, expand, or rearrange the label set (with\nunseen categories) for any image at test time.\nSegment Anything Model (SAM) [59] introduces a frame-\nwork for promptable segmentation consisting of the task de\ufb01-\nnition for promptable segmentation, a segmentation foundation\nmodel (the Segment Anything Model, or SAM), and a data\nengine. SAM adapts a pretrained Vision Transformer from\nMasked Auto-Encoder (MAE) [113] as an image encoder\nwhile using a text encoder from CLIP [114] for sparse prompts\n(points, boxes, and text) and a separate dense prompt encoder\nfor masks. In contrast to other foundation models that are\ntrained in an unsupervised manner on web-scale data, SAM\nis trained using supervised learning with data engines that\nhelp scale the number of available annotations. Along with\nthe model, the authors released the Segment Anything 1\nBillion (SA-1B) dataset. It consists of 11M images and 1.1B\nsegmentation masks. In this work, the authors conducted ex-\nperiments on \ufb01ve zero-shot transfer tasks, including point-valid\nmask evaluation, edge detection, object proposal, instance\nsegmentation, and text-to-mask. The system\u2019s composable\ndesign, facilitated by prompt engineering techniques, enables\na broader range of applications compared to systems trained\nspeci\ufb01cally for \ufb01xed task sets. However, one limitation of this\nwork that is particularly relevant to robotic applications is that\nSAM cannot run in real-time.\nFastSAM [60] and MobileSAM [61] achieve compara-\nble performance to SAM at faster inference speeds. The\nTrack Anything Model (TAM) [62] combines SAM and\nXMem [165], an advanced video object segmentation (VOS)\nmodel, to achieve interactive video object tracking and seg-\nmentation. Anything-3D [166] employs a collection of visual-\nlanguage models and SAMs to elevate objects into the realm\nof 3D. It uses BLIP [108] to generate textual descriptions\nwhile using SAM to extract objects of interest from visual\ninput. Then, Anything-3D lifts the extracted objects into a\nNeural Radiance Field (NeRF) [167] representation using a\ntext-to-image diffusion model, enabling their integration into\n3D scenes.\nAmidst these remarkable advancements, achieving \ufb01ne-\ngrained detection with real-time performance still remains\nchallenging. For example, LSeg [58] reports failure cases\nrelated to misclassi\ufb01cation, when the test time input labels\ndo not include the true label for the pixel, and the model thus\nassigns the highest probability to the closest label. Another\nfailure case occurs when multiple labels can be correct for a\nparticular pixel, and the model must classify it as just one of\nthe categories. For example \u201cwindow\u201d and \u201chouse\u201d may both\nbe de\ufb01ned as labels, but during inference, a pixel representing\na \u201cwindow\u201d may be labeled instead as \u201chouse\u201d. SAM also does\nnot provide precise segmentation for \ufb01ne structures and often\nfails to produce crisp boundaries. All models that use SAM\nas a sub-component may encounter similar limitations. In the\nfuture, \ufb01ne-grained semantic segmentation models that can\nassign multiple labels to a pixel when there are multiple correct\ndescriptions should be considered. Additionally, developing\nmodels that can run in real-time will be critical for robotics\napplications.\nC. Open-Vocabulary 3D Scene and Object Representations\nScene representations allow robots to understand their sur-\nroundings, facilitate spatial reasoning, and provide contex-\ntual awareness. Language-driven scene representations align\ntextual descriptions with visual scenes, enabling robots to\nassociate words with objects, locations, and relationships. In\nthis section, we study recent works that use foundation models\nto enhance scene representations.\n1) Language Grounding in 3D Scene: Language grounding\nrefers to combining geometric and semantic representations\nof an environment. One type of representation that can pro-\nvide an agent with a strong geometric prior is an implicit\nrepresentation. One example of an implicit representation is\na Neural Radiance Field (NeRF) [167]\u2013[169]. NeRF creates\nhigh-quality 3D reconstructions of scenes and objects from a\nset of 2D images captured from different viewpoints (without\nthe need for explicit depth information). The NeRF neural\n19\nnetwork takes camera poses as input and predicts the 3D\ngeometry of the scene as well as color and intensity. Most\nNeRF-based models memorize the light \ufb01eld in a single\nenvironment and are not pre-trained on a large data set, hence\nthey are not foundation models. However, foundation models\nsuch as CLIP can be combined with NeRFs to extract semantic\ninformation from an agent\u2019s environment.\nKerr et al. [64] propose language-embedded radiance \ufb01elds\n(LERFs) that ground CLIP embeddings into a dense multi-\nscale 3D \ufb01eld. This results in a 3D representation of the\nenvironment that can be queried to produce semantic relevancy\nmaps. The LERF model takes 3D position (x, y, z), viewing\ndirection (\u03c6, \u03b8), and a scaling factor as input and outputs an\nRGB value, density (\u03c3), as well as DINO [105] and CLIP\nfeatures. The LERF is optimized in two stages: initially, a\nmulti-scale feature pyramid of CLIP embeddings over training\nviews is computed; then, the pyramid is interpolated using the\nimage scale and pixel location to obtain the CLIP embedding;\nand \ufb01nally, the CLIP embeddings are supervised through\ncosine similarity and the RGB and density are supervised using\nthe standard mean squared-error.\nModels such as LERF inherit the shortcomings of CLIP\nand NeRF. For example, CLIP exhibits dif\ufb01culty in capturing\nspatial relationships between objects. In addition, language\nqueries from CLIP can highlight a signi\ufb01cant issue similar to\nthe bag-of-words model, which struggles to distinguish terms\nwith opposite sentiments. Also, NeRF relies on known camera\nposes associated with pre-captured multi-view images.\nIn CLIP-Fields [170], an implicit scene representation\ng(x, y, z) : R3 \u2192 Rd is trained by decoding a d-dimensional\nlatent vector to different modality-speci\ufb01c outputs. The model\ndistills information from pretrained image models by back-\nprojecting the pixel labels to 3D space and training the output\nheads to predict semantic labels from an open-vocab object\ndetector called Detic, the CLIP visual representation, and\none-hot instance labels using a contrastive loss. The scene\nrepresentation can then be used as a spatial database for\nsegmentation, instance identi\ufb01cation, semantic search over\nspace, and 3D view localization from images.\nAnother related work is VLMaps [171], which projects pixel\nembeddings from LSeg to grid cells in a top-down grid map.\nThis method does not require training and instead directly\nbackprojects pixel embeddings to grid cells and averages the\nvalues in overlapping regions. By combining a VLMap with\na code-writing LLM, the authors demonstrate spatial goal\nnavigation using landmarks (e.g., move to the plant) or spatial\nreferences with respect to landmarks (between the keyboard\nand the bowl). Semantic Abstraction (SemAbs) [172] presents\nanother approach for 3D scene understanding by decoupling\nvisual-semantic reasoning and 3D reasoning. In SemAbs,\ngiven an RGB-D image of a scene, a semantic-aware 2D VLM\nextracts 2D relevancy maps for each queried object while\nsemantic-abstracted 3D modules predict the 3D occupancy\nof each object using the relevancy maps. Because the 3D\nmodules are trained irrespective of the speci\ufb01c object labels,\nthe system demonstrates strong generalization capabilities,\nincluding generalization to new object categories and from\nsimulation to the real world.\nCurrent VLMs can reason about 2D images, however, they\nare not grounded in the 3D world. The main challenge for\nbuilding 3D VLM foundation models is the scarcity of 3D\ndata. Particularly, 3D data paired with language description is\nscarce. One strategy to circumvent this issue is to take ad-\nvantage of 2D models trained on large-scale data to supervise\n3D models. For instance, the authors of FeatureNeRF [173]\npropose to learn 3D semantic representations by distilling 2D\nvision foundation models (i.e., DINO or Latent Diffusion)\ninto 3D space via neural rendering. FeatureNeRF predicts a\ncontinuous 3D semantic feature volume from a single or few\nimages which can be used for downstream tasks such as key-\npoint transfer or object part co-segmentation.\nIn 3D-LLM [11], the authors propose to use 2D VLMs\nas backbones to train a 3D-LLM that can take 3D represen-\ntations (i.e., 3D point clouds with their features) as inputs\nand accomplish a series of diverse 3D-related tasks. The\n3D features are extracted from 2D multi-view images and\nmapped to the feature space of 2D pretrained VLMs. To\novercome 3D data scarcity, the authors propose an ef\ufb01cient\nprompting procedure for ChatGPT to generate 3D-language\ndata encompassing a diverse set of tasks. These tasks include\n3D captioning, dense captioning, 3D question answering, 3D\ntask decomposition, 3D grounding, 3D-assisted dialog, and\nnavigation. Also, to capture 3D spatial information, the authors\npropose a 3D localization mechanism by 1) augmenting 3D\nfeatures with position embedding and 2) augmenting LLM\nvocabularies with 3D location tokens. In the \ufb01rst part, the\nposition embeddings of the three dimensions are generated\nand concatenated with 3D features. In the second part, the\ncoordinates of the bounding box representing the grounded\nregion are discretized to voxel integers as location tokens\n< xmin, ymin, zmin, xmax, ymax, zmax >. It is important to\nhighlight that, typically, creating 3D representations necessi-\ntates the use of 2D multi-view images and camera matrices.\nThese resources are not as readily available as the vast amounts\nof internet-scale text and image data that current foundation\nmodels are trained on.\n2) Scene Editing: When an embodied agent relies on an\nimplicit representation of the world, the capability to edit and\nupdate this representation enhances the robot\u2019s adaptability.\nFor instance, consider a scenario where a robot utilizes a\npretrained NeRF model of an environment for navigation and\nmanipulation. If a portion of the environment changes, being\nable to adjust the NeRF without retraining the model from\nscratch saves time and resources.\nIn the case of NeRFs, Wang et al. [63] propose a text and\nimage-driven method for manipulating NeRFs called CLIP-\nNeRF. This approach uses CLIP to disentangle the dependence\nbetween shape and appearance in conditional neural radiance\n\ufb01elds. CLIP-NeRF facilitates the editing of the shape and\nappearance of NeRFs using either image or text prompts. It is\ncomposed of two modules: the disentangled conditional NeRF\nand CLIP-driven manipulation. The former takes the positional\nencoding \u03b3(x, y, z), a shape code zs, viewing direction v(\u03c6, \u03b8),\nand appearance code za as an input and outputs color and\ndensity. The disentanglement is achieved using a deformation\nnetwork that is appended as input to the traditional NeRF MLP\n20\nthat produces density, and by taking the output from this MLP\nand concatenating it with an appearance code to attain the\ncolor value. The CLIP-driven manipulation module takes an\nimage example or text prompt as an input and outputs a shape\ndeformation \u2206zs and an appearance deformation \u2206za from\nshape mapping and appearance mapping MLPs respectively.\nThese deformation values aim to perturb the shape code and\nappearance code in the disentangled conditional NeRF module\nto produce the desired output.\nA key limitation of the CLIP-NeRF approach is that prompt-\ning can impact the entire scene rather than a selected region.\nFor example, prompting to change the color of a \ufb02ower\u2019s petals\nmight also impact the shape and color of its leaves. To address\nthis limitation, Kobayashi et al. propose to train distilled\nfeature \ufb01elds (DFFs) [65] and then manipulate DFFs through\nquery-based scene decomposition and editing. Pre-trained 2D\nVLMs (such as LSeg [58] and DINO [105]) are employed\nas teacher networks and distilled into 3D distilled feature\n\ufb01elds via volume rendering. Editing is achieved by alpha\ncompositing the density and color values of the two NeRF\nscenes. When combined with CLIP-NeRF, this method enables\nCLIP-NeRF to selectively edit speci\ufb01c regions of multi-object\nscenes. A similar approach was explored by Tschernezki et\nal. in [174] where the authors show that enforcing the 3D\nconsistency of features in the NeRF embedding improved\nsegmentation performance compared to using features from\nthe original 2D images.\nAnother approach to more controlled 3D scene editing is to\nuse structured 3D scene representations. Ner\ufb02ets [175] repre-\nsent a 3D scene as a combination of local neural radiance \ufb01elds\nwhere each maintains its own spatial position, orientation, and\ndimension. Instead of employing a single large MLP to predict\ncolors and densities as standard NeRF, individual Ner\ufb02ets are\ncombined to predict these values, modulated by their weights.\nAfter optimizing posed 2D images and segmentations, Ner\ufb02ets\nre\ufb02ect the decomposed scene and support more controlled\nediting.\nOne application of image editing in robotics is for data\naugmentation during policy learning. ROSIE [176] use the\nImagen editor [177] to modify training images to add addi-\ntional distractors and unseen objects and backgrounds to train\nrobust imitation learning policies. GenAug [178] similarly\ngenerates images with in-category and cross-category object\nsubstitutions, visual distractors, and diverse backgrounds. The\nCACTI [14] pipeline includes a step in-painting different plau-\nsible objects via Stable-Diffusion [117] onto training images.\nThese approaches generate photorealistic images for training\nrobust policies; however, generating images with suf\ufb01cient\ndiversity while also maintaining physical realism, e.g. for\nobject contacts, remains a challenge. Existing approaches use\nlearned or provided masks to specify areas of the image to\nkeep, or heuristics based on the particular robotic task.\nAnother direction is to use generative models to de\ufb01ne\ngoal images for planning. DALL-E-Bot [157] uses DALL-E\n2 to de\ufb01ne a goal image of human-like arrangements from\nobservations.\n3) Object Representations: Learning correspondences be-\ntween objects can facilitate manipulation by enabling skill\ntransfer from trained objects to novel object instances in\nknown categories or novel object categories at test time.\nTraditionally, object correspondences have been learned using\nstrong supervision such as keypoints and keyframes. Neural\ndescriptor \ufb01elds (NDFs) [179] remove the need for dense\nannotation by leveraging layer-wise activations from an oc-\ncupancy network; however, this approach still requires many\ntraining shapes for each target object category. Additional\nworks have started to build object representations directly from\nimage features of pretrained vision models.\nFeature Fields for Robotic Manipulation (F3RM) [180]\nbuilds on DFF to develop scene representations that support\n\ufb01nding corresponding object regions. F3RM uses a similar\nfeature representation for 6-DoF poses relative to objects\n(e.g., a grasp on the handle of the mug) to NDF. Besides\nallowing corresponding 6-DoF poses to be found from a few\ndemonstrations, the pose embeddings can also be directly\ncompared to text embeddings from CLIP to leverage language\nguidance (e.g., pick up the bowl). Correspondences between\nobjects have also been directly extracted from DINO features\n[181] without training. This method \ufb01rst extracts dense ViT\nfeature maps of two objects using multiple views. Similar\nregions on the two objects are found by computing the cyclical\ndistance metric [182] on the feature maps. With the 2D patch\ncorrespondences, a 7-D rigid body transform (i.e., a SO(3)\npose, a translation, and a scaling scalar) between the objects\ncan be solved together with RANSAC and Umeyama\u2019s method\n[183].\nD. Learned Affordances\nAffordances refer to the potential of objects, environments,\nor entities to offer speci\ufb01c functions or interactions to an agent.\nThey can include actions such as pushing, pulling, sitting,\nor grasping. Detecting affordances bridges the gap between\nperception and action.\nAffordance Diffusion [66] synthesizes complex interactions\nof e.g. an articulated hand with a given object. Given an\nRGB image, Affordance Diffusion aims to generate images of\nhuman hands for hand-object interaction (HOI). The authors\npropose a two-step generative approach based on large-scale\npretrained diffusion models based on where to interact (layout)\nand how to interact (content). The layout network generates\na 2D spatial arrangement of hand and object. The content\nnetwork then synthesizes images of a hand grasping the object\nconditioned on the given object and the sampled HOI layout.\nAffordance Diffusion outputs both the hand articulation and\napproach orientation.\nVision-Robotic Bridge (VRB) [67] trains a visual affor-\ndance model on internet videos of human behavior. Partic-\nularly, it estimates the likely location and manner in which\na human interacts within a scene. This model captures the\nstructural information of these behavioral affordances. The\nauthors seamlessly integrate the affordance model with four\ndifferent robot learning paradigms. Firstly, they apply of\ufb02ine\nimitation learning, where the robot learns by imitating the\nobserved human interactions from the videos. Secondly, they\nuse exploration techniques to enable the robot to actively\n21\ndiscover and learn new affordances in its environment. Thirdly,\nthe authors incorporate goal-conditioned learning, allowing\nthe robot to learn how to achieve speci\ufb01c objectives by\nleveraging the estimated affordances. Finally, they integrate\naction parameterization for reinforcement learning, enabling\nthe robot to learn complex behaviors by optimizing its actions\nbased on the estimated affordances.\nE. Predictive Models\nPredictive dynamics models, or world models, predict how\nthe state of the world changes given particular agent actions,\nthat is, they attempt to model the state transition function\nof the world [184]. When applied to visual observations,\ndynamics modeling can be formulated as a video prediction\nproblem [185], [186]. While video generation and prediction,\nparticularly over long horizons, is a longstanding challenge\nwith many prior efforts, recent models based on vision trans-\nformers and diffusion models have demonstrated improve-\nments [187], [188]. For instance, the Phenaki model [189]\ngenerates variable length video up to minutes in length con-\nditioned on text prompts.\nSeveral approaches apply these models to robotics in the\nliterature. Note that while learned dynamics or world models\nin robotics have been explored in constrained or smaller-data\nregimes, we focus in this section on works that train on a\ndiversity or volume of data that is characteristic of founda-\ntion models. One strategy is to learn an action-conditioned\nmodel that may be used directly for downstream planning by\noptimizing an action sequence [190], i.e. performing model-\npredictive control, or for policy learning via training on\nsimulated rollouts. One example is the GAIA-1 model which\ngenerates predictions of driving video conditioned on arbitrary\ncombinations of video, action, and text [191]. It was trained\non 4700 hours of proprietary driving data. Another approach\nis to use a video prediction model to generate a plan of future\nstates, and then learn a separate goal-conditioned policy or\ninverse dynamics model to infer control actions based on the\ncurrent and target state. One line of work instantiates this\nby combining text-conditioned video diffusion models with\nimage-goal-conditioned policies to solve manipulation tasks in\nsimulated and real tabletop settings [192]. This approach has\nbeen extended to longer-horizon object manipulation tasks by\nusing the PaLM-E VLM to break down a high-level language\ngoal into smaller substeps, leveraging feedback between the\nVLM and video generation models [193].\nAnother example is COMPASS [160], which \ufb01rst constructs\na comprehensive multimodal graph to capture crucial rela-\ntional information across diverse modalities. The graph is\nthen used to construct a rich spatio-temporal and semantic\nrepresentation. Pretrained on the TartanAir multimodal dataset,\nCOMPASS was demonstrated to address multiple robotic\ntasks including drone navigation, vehicle racing, and visual\nodometry.\nV. EMBODIED AI\nRecently, researchers have shown that the the success of\nLLMs can be extended to embodied AI domains [32], [33],\n[42], [194], where \u201cembodied\u201d typically refers to a virtual\nembodiment in a world simulator, not a physical robot em-\nbodiment. Statler [69] is a framework that endows LLMs\nwith an explicit representation of the world state as a form\nof \u201cmemory\u201d that is maintained over time. Statler uses two\ninstances of general LLMs: a world-model reader and a world-\nmodel writer, that interface with and maintain the world state.\nStatler improves the ability of existing LLMs to reason over\nlonger time horizons without the constraint of context length.\nLarge Scale Language Models (LSLMs) have exhibited\nstrong reasoning ability and the ability to adapt to new tasks\nthrough in-context learning. Dasgupta et al. [195] combine\nthese complementary abilities in a single system consisting\nof three parts: a Planner, an Actor, and a Reporter. The\nPlanner is a pretrained language model that can issue com-\nmands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next\ncommand. Mu et al. [70] build EgoCOT, a dataset consisting\nof carefully selected videos from the Ego4D dataset, along\nwith corresponding high-quality language instructions. Em-\nbodiedGPT [70] utilizes pre\ufb01x adapters to augment the 7B\nlanguage model\u2019s capacity to generate high-quality planning,\ntraining it on the EgoCOT dataset to avoid overly divergent\nlanguage model responses. Comprehensive experiments were\nconducted, demonstrating that the model effectively enhances\nthe performance of embodied tasks such as Embodied Plan-\nning, Embodied Control, Visual Captioning, and Visual Q&A.\nEmbodied agents should autonomously and endlessly explore\nthe environment. They should actively seek new experiences,\nacquire new skills, and improve themselves.\nThe game of Minecraft [196] provides a platform for\ndesigning intelligent agents capable of operating in the open\nworld. MineDojo [71] is a framework for developing generalist\nagents in the game of Minecraft. MineDojo offers thousands\nof open-ended and language-prompted tasks, where the agent\ncan navigate in a progressively generated 3D environment to\nmine, craft tools, and build structures. As part of this work,\nthe authors introduce MiniCLIP, a video-language model that\nlearns to capture the correlations between a video clip and\nits time-aligned text that describes the video. The MineCLIP\nmodel, trained on YouTube videos, can be used as a reward\nfunction to train the agent with reinforcement learning. By\nmaximizing this reward function, it incentivizes the agent\nto make progress toward solving tasks speci\ufb01ed in natural\nlanguage.\nVoyager [73] introduces an LLM-powered embodied life-\nlong learning agent in the realm of Minecraft. Voyager uses\nGPT-4 to continuously explore the environment. It interacts\nwith GPT-4 through in-context prompting and does not re-\nquire model parameter \ufb01ne-tuning. Exploration is maximized\nby querying GPT-4 to provide a stream of new tasks and\nchallenges based on the agent\u2019s history interactions and current\nsituations. Also, the iterative prompting mechanism generates\ncode as the action space to control the Minecraft agent. Iter-\native prompting incorporates environment feedback provided\nby Minecraft, execution errors, and a self-veri\ufb01cation scheme.\nFor self-veri\ufb01cation, GPT-4 acts as a critic by checking task\nsuccess and providing suggestions for task completion in\n22\nthe case of failure. The GPT-4 critic can be replaced by a\nhuman critic to provide on-the-\ufb02y human feedback during task\nexecution. Ghost in the Minecraft (GITM) [197] leverages\nLLM to break down goals into sub-goals and map them\nto structured actions for generating control signals. GITM\nconsists of three components: an LLM Decomposer, an LLM\nPlanner, and an LLM Interface. The LLM Decomposer is\nresponsible for dividing the given Minecraft goal into a sub-\ngoal tree. The LLM Planner then plans an action sequence for\neach sub-goal. Finally, the LLM Interface executes each action\nin the environment using keyboard and mouse operations.\nReinforcement learning in embodied AI virtual environ-\nments has the potential to improve the capabilities of real-\nworld robotics by providing ef\ufb01cient training and optimizing\ncontrol policies in a safe and controlled setting. Reward design\nis a crucial aspect of RL that in\ufb02uences the robot\u2019s learning\nprocess. Rewards should be aligned with the task\u2019s objective\nand guide the robot to achieve the desired task. Foundation\nmodels can be leveraged to design rewards. Kwon et al. [16]\ninvestigate the simpli\ufb01cation of reward design by utilizing\na large language model (LLM), such as GPT-3, as a proxy\nreward function. In this approach, users provide a textual\nprompt that contains a few examples (few-shots) or a descrip-\ntion (zero-shot) of the desired behavior. The proposed method\nincorporates this proxy reward function within a reinforcement\nlearning framework. Users specify a prompt at the start of the\ntraining process. During training, the RL agent\u2019s behavior is\nevaluated by the LLM against the desired behavior outlined\nin the prompt, resulting in a corresponding reward signal\ngenerated by the LLM. Subsequently, the RL agent employs\nthis reward to update its behavior through the learning process.\nIn [74], the authors propose a method called Exploring\nwith LLMs (ELLM) that rewards an agent for achieving\ngoals suggested by a language model. The language model\nis prompted with a description of the agent\u2019s current state.\nTherefore, without having a human in the loop, ELMM guides\nagents toward meaningful behavior.\nZhang et al. [198] explore the potential relationship between\nof\ufb02ine reinforcement learning and language modeling. They\nhypothesize that RL and LM share similarities in predicting\nfuture states based on current and past states, considering both\nlocal and long-range dependencies across states. To validate\nthis assumption, the authors pre-train Transformer models on\ndifferent of\ufb02ine RL tasks and assess their performance on\nvarious language-related tasks. Tarasov et al. [199] present\nan approach to harness pretrained language models in deep\nof\ufb02ine reinforcement learning scenarios that are not inherently\ncompatible with textual representations. The authors suggest a\nmethod that involves transforming the RL states into human-\nreadable text and performing \ufb01ne-tuning of the pretrained lan-\nguage model during training with deep of\ufb02ine RL algorithms.\nAdvances in model architecture (e.g. transformer) for foun-\ndation models allow the model to effectively model and predict\nsequences. To harness the power of these models, some recent\nstudies investigate exploiting these architectures for sequence\nmodeling in RL problems. Reid et al. [200] explore the\npotential of leveraging the sequence modeling formulation\nof reinforcement learning and examine the transferability of\npretrained sequence models across different domains, such as\nvision and language. They speci\ufb01cally focus on the effec-\ntiveness of \ufb01ne-tuning these pretrained models on of\ufb02ine RL\ntasks, including control and games. In addition to investigating\nthe transferability of pretrained sequence models, the authors\npropose techniques to enhance the transfer of knowledge\nbetween these domains. These techniques aim to improve the\nadaptability and performance of the pretrained models when\napplied to new tasks or domains.\nHigh-level task planning using LLMs is demonstrated in\nembodied AI environments. Huang et al. [68] propose em-\nploying pretrained Language Models (LMs) as zero-shot plan-\nners. The approach is evaluated in the VirtualHome [129]\nenvironment. In this work, \ufb01rst, an autoregressive LLM such\nas GPT-3 [2] or Codex [201] is quarried to generate action\nplans for high-level tasks. Some of these action plans might\nnot be executable by the agent due to ambiguity in language\nor referring to objects that are not present or grounded in\nthe environment. So, to select the admissible action plans,\nadmissible environment actions, and generated actions by the\ncausal LLM are embedded using a BERT-style LM. Then for\neach admissible environment action, its semantic distance to\nthe generated action is computed using cousin similarity.\nChain of thought reasoning and action generation are pro-\nposed for embodied agents as well. ReAct [202] combines\nreasoning (e.g. chain of thought) and acting (e.g. sequence of\naction generation) within LLM. Reasoning traces enhance the\nmodel\u2019s ability to deduce, monitor, and revise action plans,\nalong with managing exceptions effectively. Actions facilitate\ninteraction with external resources, like knowledge bases or\nenvironments, enabling it to acquire supplementary informa-\ntion. ReAct showcases its pro\ufb01ciency across a wide array\nof language and decision-making tasks, including question-\nanswering and fact veri\ufb01cation. It enhances interpretability and\ntrust for users by transparently illustrating the process through\nwhich it searches for evidence and formulates conclusions.\nUnlike prior methods that depend on a singular chain-of-\nthought, ReAct engages with a Wikipedia API for pertinent\ninformation retrieval and belief updating. This strategy effec-\ntively mitigates the issues commonly associated with chain-of-\nthought reasoning, such as hallucination and error propagation.\nVPT [72] presents video pretraining in which the agent\nlearns to act by watching unlabeled online videos. It is shown\nthat an inverse dynamic model can be trained with a small\nlabeled dataset and the model can be used to label a huge\nunlabeled data of the internet. Videos of people who have\nplayed Minecraft are used to train an embodied AI agent\nto play Minecraft. The model exhibits zero-shot performance\nand can be \ufb01ne-tuned for more complex skills using imitation\nlearning or reinforcement learning. The VPT model is trained\nwith a standard behavioral cloning loss (9) (negative log-\nlikelihood) while the actions are drawn from the inverse\ndynamic model.\nA. Generalist AI\nA long-standing challenge in robotics research is deploying\nrobots or embodied AI agents in a variety of non-factory real-\nworld applications, performing a range of tasks. To make\n23\ngeneralist robots that can operate in diverse environments\nwith diverse tasks, some researchers have proposed genera-\ntive simulators for robot learning. For example, Generative\nAgents [203] discusses how generative agents can produce\nrealistic imitations of human behavior for interactive appli-\ncations, creating a miniature community of agents similar to\nthose found in games like The Sims. The authors connect\ntheir architecture with the ChatGPT large language model to\ncreate a game environment with 25 agents. The study includes\ntwo evaluations, a controlled evaluation and an end-to-end\nevaluation, which demonstrate the causal effects of the various\ncomponents of their architecture. Xian et al. [204], authors\npropose a fully automated generative pipeline, known as a\ngenerative simulation for robot learning, which utilizes models\nto generate diverse tasks, scenes, and training guidance on a\nlarge scale. This approach can facilitate the scaling up of low-\nlevel skill learning, ultimately leading to a foundational model\nfor robotics that empowers generalist robots.\nAn alternative method for developing generalist AI involves\nusing generalizable multi-modal representations. Gato [154] is\na generalist agent that works as a multi-modal, multi-task,\nmulti-embodiment generalist policy. Using the same neural\nnetwork with the same set of weights, Gato can sense and\nact with different embodiments in various environments across\ndifferent tasks. Gato can play Atari, chat, caption images, stack\nblocks with a real robot arm, navigate in a 3D simulated envi-\nronment, and more. Gato is trained on 604 different tasks with\nvarious modalities, observations, and actions. In this setting,\nlanguage acts as a common grounding across different embod-\niments. Gato has 1.2B parameters and is trained of\ufb02ine in a\nsupervised way. Positioned at the con\ufb02uence of representation\nlearning and reinforcement learning (RL), RRL [205] learns\nbehaviors directly from proprioceptive inputs. By harnessing\npre-trained visual representations, RRL is able to learn from\nvisual inputs, which typically pose challenges in conventional\nRL settings.\nB. Simulators\nHigh-quality simulators or benchmarks are crucial for\nrobotics development. Hence, we put the \u201csimulator\u201d section\nhere to highlight its essential role. To facilitate generalization\nfrom simulation to the real world, Gibson [206] emphasizes\nreal-world perception for embodied agents. To bridge the\ngap between simulation and real-world, iGibson [146] and\nBEHAVIOR-1K [207] further support the simulation of a\nmore diverse set of household tasks and reach high levels\nof simulation realism. As a simulation platform for research\nin Embodied AI, Habitat [208] consists of Habitat-Sim and\nHabitat-API. Habitat-Sim can achieve several thousand frames\nper second (fps) running single-threaded. Rather than mod-\neling into low-level physics, Habitat-Lab [147], is a high-\nlevel library for embodied AI, giving a modular framework\nfor end-to-end development. It facilitates the de\ufb01nition of\nembodied AI tasks, such as navigation, interaction, instruction\nfollowing, and question answering. Additionally, it enables the\ncon\ufb01guration of embodied agents, encompassing their physical\nform, sensors, and capabilities. The library supports various\ntraining methodologies for these agents, including imitation\nlearning, reinforcement learning, and traditional non-learning\napproaches like the SensePlanAct pipelines. Furthermore, it\nprovides standard metrics for evaluating agent performance\nacross these tasks. In line with this, the recent release of\nHabitat 3.0 [209] further expands these capabilities.\nSimilarly, RoboTHOR [210] serves as a platform for the\ndevelopment and evaluation of embodied AI agents, offer-\ning environments in both simulated and physical settings.\nCurrently, RoboTHOR includes a training and validation set\ncomprising 75 simulated scenes. Additionally, there are 14\nscenes each for test-dev and test-standard in the simulation,\nwith corresponding physical counterparts. Key features of\nRoboTHOR include its recon\ufb01gurability and benchmarking\ncapabilities. The physical environments are constructed using\nmodular, movable components, enabling the creation of di-\nverse scene layouts and furniture con\ufb01gurations in a single\nphysical area. Another simulator, VirtualHome [129], models\ncomplex activities that occur in a typical household. It supports\nprogram descriptions for a variety of activities that happen\nin people\u2019s homes. Huang et al. [33] use VirtualHome to\nevaluate the robot planning ability with language models.\nThese simulators have the potential to be applied for evaluating\nLLMs on robotics tasks.\nVI. CHALLENGES AND FUTURE DIRECTIONS\nIn this section, we examine challenges related to integrating\nfoundation models into robotics settings. We also explore\npotential future avenues to address some of these challenges.\nA. Overcoming Data Scarcity in Training Foundation Models\nfor Robotics\nOne main challenge is that compared to the internet-scale\ntext and image data that large models are trained on, robotic-\nspeci\ufb01c data is scarce. We discuss various techniques to over-\ncome data scarcity. For example, to scale up robot learning,\nsome recent works suggest the use of play data instead of\nexpert data for imitation learning. Another technique is data\naugmentation using in-painting techniques.\n1) Scaling Robot Learning Using Unstructured Play Data\nand Unlabeled Videos of Humans:\nLanguage-conditioned\nlearning such as language-conditioned behavioral cloning,\nor language-conditioned affordance learning requires having\naccess to large annotated datasets. To scale up learning,\nin Play-LMP [26], the authors suggest using teleoperated\nhuman-provided play data instead of fully annotated expert\ndemonstrations. Play data is unstructured, unlabeled, cheap to\ncollect, but rich. Collecting play data does not require scene\nstaging, task segmenting, or resetting to an initial state. Also,\nin MimicPlay [118] a goal-conditioned trajectory generation\nmodel is trained based on human-play data. The play data\nincludes unlabeled video sequences of humans interacting\nwith the environment with their hands. Recently works such\nas [125] have shown a very small percentage (as little as 1%)\nof language-annotated data is needed to train a visuo-lingual\naffordance model for robot manipulation tasks.\n24\n2) Data\nAugmentation\nusing\nInpainting:\nCollecting\nrobotics data requires the robot to interact with the real\nphysical world. This data collection process can be associated\nwith signi\ufb01cant costs and potential safety concerns. One\nway to tackle this challenge is to use generative AI such as\ntext-to-image diffusion models for data augmentation. For\nexample, ROSIE (Scaling Robot Learning with Semantically\nImagined Experience) [176] presents a diffusion-based data\naugmentation. Given a robot manipulation dataset, they use\ninpainting to create various unseen objects, backgrounds, and\ndistractors with textual guidance. One important challenge\nfor these methods is developing inpainting strategies that can\ngenerate suf\ufb01cient semantically and visually diverse data,\nwhile at the same time ensuring that this data is physically\nfeasible and accurate. For instance, using inpainting to modify\nan image of an object within a robot\u2019s gripper may result in\nan image with a physically unrealistic grasp, leading to poor\ndownstream training performance. Additional investigation\ninto generative foundation models that are evaluated not only\nfor visual quality but also for physical realism may improve\nthe generality of these methods.\n3) Overcoming 3D Data Scarcity for Training 3D Foun-\ndation Models: Currently, multi-modal Vision-and-Language\nModels (VLMs) can analyze 2D images, but they lack a\nconnection to the 3D world, which encompasses 3D spatial\nrelationships, 3D planning, 3D affordances, and more. The\nprimary obstacle in developing foundational 3D VLM models\nlies in the scarcity of 3D data, especially data that is paired\nwith language descriptions. As discussed, language-driven per-\nception tasks such as language-driven 3D scene representation,\nlanguage-driven 3D scene editing, language-driven 3D scene\nor shape generation, language-driven 3D classi\ufb01cation, and\naffordance prediction require access to 3D data or multi-view\nimages with camera matrices which are not readily available\ndata types. New datasets or data generation methods need to\nbe created in the future to overcome data scarcity in the 3D\ndomain.\n4) Synthetic Data Generation via High-Fidelity Simulation:\nHigh-\ufb01delity simulation via gaming engines can provide an\nef\ufb01cient means to collect data, especially to solve multimodal\nand 3D perception tasks on robots. For example, TartanAir\n[211], a dataset for robot navigation tasks, was collected in\n[212] with the presence of moving objects, changing light, and\nvarious weather conditions. By collecting data in simulations,\nit was possible to obtain multi-modal sensor data and precise\nground truth labels such as the stereo RGB image, depth\nimage, segmentation, optical \ufb02ow, camera poses, and LiDAR\npoint cloud. A large number of environments were set up with\nvarious styles and scenes, covering challenging viewpoints and\ndiverse motion patterns that are dif\ufb01cult to achieve by using\nphysical data collection platforms. An extension TartanAir-\nV2 (https://tartanair.org) furthers the dataset by incorporat-\ning additional environments and modalities, such as \ufb01sheye,\npanoramas, and pinholes, with arbitrary camera intrinsic and\nrotations.\n5) Data Augmentation using VLMs: Data augmentation\ncan be provided using Visual-Language Models (VLMs).\nIn DIAL [213], Data-driven Instruction Augmentation for\nLanguage-conditioned control is introduced. DIAL uses VLM\nto label of\ufb02ine datasets for language-conditioned policy learn-\ning. DIAL performs instruction augmentation using VLMs to\nweakly relabel of\ufb02ine control datasets. DIAL consists of three\nsteps 1) Contrastive \ufb01ne-tuning of a VLM such as CLIP [4] on\na small robot manipulation dataset of trajectories with crowd-\nsourced annotation, 2) producing new instruction labels by\nusing the \ufb01ne-tuned VLM to score relevancy of crowd-sourced\nannotations against a larger dataset of trajectories, 3) training\na language-conditioned policy using behavior cloning on both,\nthe original and re-annotated dataset.\n6) Robot Physical Skills are Limited to Distribution of\nSkills: One key limitation of the existing robot transformers\nand other related works in robotics is that robot physical\nskills are limited to the distribution of skills observed within\nthe robot data. Using these transformers, the robot lacks\nthe capability to generate new movements. To address this\nconstraint, an approach involves using motion data from videos\nthat humans performing various tasks. The inherent motion\ninformation within these videos can then be employed to\nfacilitate the acquisition of physical skills in robotics.\nB. Real Time Performance (High Inference Time of Founda-\ntion Models)\nAnother bottleneck for deploying foundation models on\nrobots is the inference time of these models. In Table II,\nthe inference time for some of these models is reported. As\nseen, the inference time for some of the models still needs to\nbe improved for reliable real-time deployment of the robotic\nsystems. As real-time capability is an essential requirement for\nany robotic system, more research needs to be performed to\nimprove the computational ef\ufb01ciency of foundation models.\nFurthermore, foundation models are most often stored and\nrun in remote data centers, and accessed through APIs that\nrequire network connectivity. Many foundation models (e.g.,\nthe GPT models, the Dall-E models) can only be accessed this\nway, while others are usually accessed this way, but can also\nbe downloaded and run locally with suf\ufb01cient local computing\npower (such as SAM [59], LLaMA [214], and DINOv2 [107]).\nGiven this cloud-service paradigm, the latencies and service\ntimes in response to an API call for a foundation model depend\non the underlying network over which the data is routed and\nthe data center where the computation takes place\u2014factors\nthat are beyond the control of a robot. So network reliability\nshould be taken into account before integrating a foundation\nmodel into a robot\u2019s autonomy stack.\nFor some robotics domains reliance on the network and\n3rd party computing may not be a safe or realistic operating\nparadigm. In autonomous driving, autonomous aircraft, search\nand rescue or emergency response applications, and defense\napplications the robot cannot rely on network connectivity for\ntime-critical perception or control computations. One option is\nto have a safe fall-back mode that relies on classical autonomy\ntools using only local computation, that can take over if\naccess to the cloud is interrupted for some reason. Another\npotential longer-term solution for network-free autonomy is\nthe distillation of large foundation models into smaller-sized\n25\nspecialized models that run on onboard robot hardware. Some\nrecent work has attempted this approach (though without an\nexplicit link to robotics) [215]. Such distilled models would\nlikely give up some aspect of the full model, e.g. restricting\noperation to a certain limited context, in exchange for smaller\nsize and faster compute. This could be an interesting future\ndirection for bringing the power of foundation models to\nsafety-critical robotics systems.\nC. Limitations in Multimodal Representation\nMultimodal interaction implicitly assumes that the modality\nis tokenizable and can be standardized into input sequences\nwithout losing information. The Multimodal models provide\ninformation sharing between multiple modalities and are some\nvariation of multimodal transformers with cross-modal atten-\ntion between every pair of inputs. In multimodal representation\nlearning, it is assumed that cross-modal interactions and the\ndimension of heterogeneity between different modalities can\nall be captured by simple embeddings. In other words, a simple\nembedding is assumed to be suf\ufb01cient to identify the modality\nor for example, how different language is from vision. In\nthe realm of multimodal representation learning, the question\nof whether a single multimodal model can accommodate all\nmodalities remains an open challenge.\nAdditionally, when paired data between a modality and text\nis available one can embed that modality into text directly.\nIn robotics applications there are some modalities for which\nsuf\ufb01cient data is not available and to be able to align them\nwith other modalities, they need to be \ufb01rst converted to other\nmodalities and then be used. For example, 3D point cloud data\nhas various applications in robotics but training a foundation\nmodel using this type of data is challenging since data is scarce\nand is not aligned with text. So, one way to overcome this\nchallenge is \ufb01rst converting this 3D point cloud data to other\nmodalities such as images and subsequently images to text as\nthe secondary step of alignment. Then they can be used in\nfoundation model training. As another example, in Socratic\nmodels [194], each modality, whether visual or auditory, is\ninitially translated into language, after which language models\nattempt to respond to these modalities.\nD. Uncertainty Quanti\ufb01cation\nHow can we provide assurances on the reliability of foun-\ndation models when they are deployed in potentially safety-\ncritical robotics applications [188]? Current foundation models\nsuch as LLMs often hallucinate, i.e., produce outputs that\nare factually incorrect, logically inconsistent, or physically\ninfeasible. While such failures may be acceptable in appli-\ncations where the outputs from the model can be checked by\na human in real-time (e.g., as is often the case for LLM-based\nconversational agents), they are not acceptable when deploying\nautonomous robots that use the outputs of foundation models\nin order to act in human-centered environments. Rigorous\nuncertainty quanti\ufb01cation is a key step toward addressing\nthis challenge and safely integrating foundation models into\nrobotic systems. Below, we highlight challenges and recent\nprogress in uncertainty quanti\ufb01cation for foundation models\nin robotics.\n1) Instance-Level Uncertainty Quanti\ufb01cation: How can we\nquantify the uncertainty in the output of a foundation model\nfor a particular input? As an example, consider the problem\nof image classi\ufb01cation; given a particular image, one may\nquantify uncertainty in the output by producing a set of object\nlabels that the model is uncertain among or a distribution\nover object labels. Instance-level uncertainty quanti\ufb01cation can\ninform the robot\u2019s decisions at runtime. For example, if an\nimage classi\ufb01cation model running on an autonomous vehicle\nproduces a prediction set {Pedestrian, Bicyclist}\nrepresenting that it is uncertain whether a particular agent is\na pedestrian or a bicyclist, the autonomous vehicle can take\nactions that consider both possibilities.\n2) Distribution-Level Uncertainty Quanti\ufb01cation: How can\nwe quantify the uncertainty in the correctness of a foundation\nmodel that will be deployed on a distribution of possible\nfuture inputs? For the problem of image classi\ufb01cation, one\nmay want to compute or bound the probability of errors over\nthe distribution of inputs that a robot may encounter when\ndeployed. Distribution-level uncertainty quanti\ufb01cation allows\nus to decide whether a given model is suf\ufb01ciently reliable to\ndeploy in our target distribution of scenarios. For example, we\nmay want to collect additional data or \ufb01ne-tune the model if\nthe computed probability of error is too high.\n3) Calibration: In order to be useful, estimates of un-\ncertainty (both at the instance-level and distribution level)\nshould be calibrated. If we perform instance-level uncertainty\nquanti\ufb01cation using prediction sets, calibration asks for the\nprediction set to contain the true label with a user-speci\ufb01ed\nprobability (e.g., 95%) over future inputs. If instance-level\nuncertainty is quanti\ufb01ed using a distribution over outputs, it\nshould be the case that outputs that are assigned con\ufb01dence p\nare in fact correct with probability p over future inputs. Simi-\nlarly, distribution-level uncertainty estimates should bound the\ntrue probability of errors when encountering inputs from the\ntarget distribution.\nWe highlight a subtle, but important, point that is often over-\nlooked when performing uncertainty quanti\ufb01cation in robotics:\nit can be crucial to pay attention to the distinction between\nFrequentist and Bayesian interpretations of probabilities. In\nmany robotics contexts \u2014 particularly safety-critical ones \u2014\nthe desired interpretation is often Frequentist in nature. For\nexample, if we produce a bound \u01eb for the probability of\ncollision of an autonomous vehicle, this should bound the\nactual observed rate of collisions when the vehicle is deployed.\nBayesian techniques (e.g., Gaussian processes or Bayesian\nensembles) do not necessarily produce estimates of uncertainty\nthat are calibrated in this Frequentist sense (since the estimates\ndepend on the speci\ufb01c prior that is used to produce the\nestimates). Trusting the resulting uncertainty estimates may\nlead one astray if the goal is to provide statistical guarantees\non the safety or performance of the robotic system when it is\ndeployed.\n4) Distribution Shift: An important challenge in performing\ncalibrated uncertainty quanti\ufb01cation is distribution shift. A\nfoundation model trained on a particular distribution of inputs\nmay not produce calibrated estimates of uncertainty when\ndeployed on a different distribution for a downstream task. A\n26\nmore subtle cause of distribution shift in robotics arises from\nclosed-loop deployment of a model. For example, imagine\nan autonomous vehicle that chooses actions using the output\nof a perception system that relies on a pretrained foundation\nmodel; since the robot\u2019s actions in\ufb02uence future states and\nobservations, the distribution of inputs the perception system\nreceives can be potentially very different from the one it was\ntrained on.\n5) Case Study: Uncertainty Quanti\ufb01cation for Language-\nInstructed Robots: Recently, there has been exciting progress\nin performing rigorous uncertainty quanti\ufb01cation for language-\ninstructed robots [216]. This work proposes an approach called\nKNOWNO for endowing language-instructed robots with the\nability to know when they don\u2019t know and to ask for help\nor clari\ufb01cation from humans in order to resolve uncertainty.\nKNOWNO performs both instance-level and distribution-level\nuncertainty quanti\ufb01cation in a calibrated manner using the\ntheory of conformal prediction. In particular, given a language\ninstruction (and a description of the robot\u2019s environment\ngenerated using its sensors), conformal prediction is used\nto generate a prediction set of candidate actions. If this set\nis a singleton, the robot executes the corresponding action;\notherwise, the robot seeks help from a human by asking\nthem to choose an action from the generated set. Using\nconformal prediction, KNOWNO ensures that asking for help\nin this manner results in a statistically guaranteed level of\ntask success (i.e., distribution-level uncertainty quanti\ufb01cation).\nKNOWNO tackles potential challenges with distribution shift\nby collecting a small amount of calibration data from the target\ndistribution of environments, tasks, and language instructions,\nand using this as part of the conformal prediction calibration\nprocedure. While KNOWNO serves as an example of calibrated\ninstance-level and distribution-level uncertainty quanti\ufb01cation\nfor LLMs, future research should also explore assessing and\nensuring the reliability of various other foundation models,\nsuch as vision-language models, vision-navigation models,\nand vision-language-action models, commonly employed in\nrobotics. In addition, exploring how Bayesian uncertainty\nquanti\ufb01cation techniques (e.g., ensembling [217], [218]) can\nbe combined with approaches such as conformal prediction to\nproduce calibrated estimates of instance-level and distribution-\nlevel uncertainty is a promising direction.\nE. Safety Evaluation\nThe problem of safety evaluation is closely related to\nuncertainty quanti\ufb01cation. How can we rigorously test for the\nsafety of a foundation model-based robotic system (i) before\ndeployment, (ii) as the model is updated during its lifecycle,\nand (iii) as the robot operates in its target environments? We\nhighlight challenges and research opportunities related to these\nproblems below.\n1) Pre-deployment safety tests: Rigorous pre-deployment\ntesting is crucial for ensuring the safety of any robotic system.\nHowever, this can be particularly challenging for robots that\nincorporate foundation models. First, foundation models are\ntrained on vast amounts of data; thus, a rigorous testing\nprocedure should ensure that test scenarios were not seen by\nthe model during training. Second, foundation models often\ncommit errors in ways that are hard to predict a priori; thus,\ntests need to cover a diverse enough range of scenarios to\nuncover \ufb02aws. Third, foundation models such as LLMs are\noften used to produce open-ended outputs (e.g., a plan for\na robot described in natural language). The correctness of\nsuch outputs can be challenging to evaluate in an automated\nmanner if these outputs are evaluated in isolation from the\nentire system.\nThe deployment cycle of current foundation models (in\nnon-robotics applications) involves thorough red-teaming by\nhuman evaluators [3], [219]. Recent work has also considered\npartially automating this process by using foundation models\nthemselves to perform red-teaming [220], [221]. Developing\nways to perform red-teaming (both by humans and in a\npartially automated way) for foundation models in robotics\nis an exciting direction for future research.\nIn addition to evaluating the foundation model in isolation,\nit is also critical to assess the safety of the end-to-end robotic\nsystem. Simulation can play a critical role here, and already\ndoes so for current \ufb01eld-deployed systems such as autonomous\nvehicles [222], [223]. The primary challenges are to ensure\nthat (i) the simulator has high enough \ufb01delity for results to\nmeaningfully transfer to the real world, and (ii) test scenarios\n(manually speci\ufb01ed, replicated from real-world scenarios, or\nautomatically generated via adversarial methods [224]) are\nrepresentative of real-world scenarios and are diverse enough\nto expose \ufb02aws in the underlying foundation models. In\naddition, \ufb01nding ways to augment large-scale simulation-based\ntesting with smaller-scale real-world testing is an important\ndirection for future work. We emphasize the need for perform-\ning such testing throughout the lifecycle of a \ufb01eld-deployed\nrobotic system, especially as updates are made to different\ncomponents (which may interact in unpredictable ways with\nfoundation models).\n2) Runtime monitoring and out-of-distribution detection:\nIn addition to performing rigorous testing of\ufb02ine, robots with\nfoundation model-based components should also perform run-\ntime monitoring. This can take the form of failure prediction\nin a given scenario, which can allow the robot to deploy a\nsafety-preserving fallback policy [225]\u2013[229]. Alternately, the\nrobot can perform out-of-distribution (OOD) detection using\nexperiences collected from a small batch of scenarios in a\nnovel distribution [230]\u2013[233]; this can potentially trigger the\nrobot to cease its operations and collect additional training data\nin the novel distribution in order to re-train its policy. Devel-\noping techniques that perform runtime monitoring and OOD\ndetection with statistical guarantees on false positive/negative\nerror rates in a data-ef\ufb01cient manner remains an important\nresearch direction.\nF. Using Existing Foundation Models as Plug-and-Play or\nBuilding New Foundation Models for Robotics\nTo incorporate foundation models into robotics, either the\nexisting pretrained large models can be employed as plug-and-\nplay or new foundation models can be built using robotics\ndata. Using foundation models as plug-and-play refers to inte-\ngrating foundation models into various applications without\n27\nthe need for extensive customization. A large body of the\nexisting literature on foundation models in robotics is centered\naround the use of foundation models from other domains such\nas language or vision as plug-and-play. The plug-and-play\napproach simpli\ufb01es and facilitates the integration of recent\nAI advances into the robotics domain. While employing these\nmodels as plug-and-play offers a convenient way to harness\nthe power of AI and provide rapid implementation, versatility,\nand scalability, they are not always customized to speci\ufb01c\napplications. When speci\ufb01c domain expertise is needed, it is\nnecessary to build a foundation model from scratch or \ufb01ne-\ntune the existing models. Building a foundation model from\nscratch is resource-intensive and demands signi\ufb01cant compu-\ntational power. However, it provides \ufb01ne-grained control over\nthe architecture, training parameters, and overall behavior.\nG. High Variability in Robotic Settings\nAnother challenge is the high variability in robotic set-\ntings. Robot platforms are inherently diverse with different\nphysical characteristics, con\ufb01gurations, and capabilities. Real-\nworld environments that robots operate in are also diverse\nand uncertain with a wide range of variations. Due to all\nthese variabilities, robotic solutions are usually tailored to\nspeci\ufb01c robot platforms with speci\ufb01c layouts, environments,\nand objects for speci\ufb01c tasks. These solutions are not gener-\nalizable across various embodiments, environments, or tasks.\nHence, to build general-purpose pretrained robotic foundation\nmodels, a key factor is to pre-train large models that are\ntask-agnostic, cross-embodiment, and open-ended and capture\ndiverse robotic data. In ROSIE [176] a diverse dataset is gen-\nerated for robot learning by performing inpainting of various\nunseen objects, backgrounds, and distractors with semantic\ntextual guidance. To overcome variability in robotic settings\nand improve generalization, another solution as ViNT [137]\npresents is to train foundation models on diverse robotic data\nacross various embodiments. RT-X [46] also investigates the\npossibility of training large cross-embodied robotic models in\nthe domain of robotic manipulation. RT-X is trained using a\nmulti-embodiment dataset which is created by collecting data\nfrom different robot platforms collected through a collabora-\ntion between 21 institutions, demonstrating 160266 tasks. RT-\nX demonstrates transfer across embodiment improves robot\ncapabilities by employing experience from diverse robotic\nplatforms.\nH. Benchmarking and Reproducibility in Robotics Settings\nAnother signi\ufb01cant obstacle in incorporating foundation\nmodels into robotics research is the necessary reliance on\nreal-world hardware experiments. This creates challenges for\nreproducibility, as replicating results obtained from hardware\nexperiments may necessitate access to the exact equipment\nemployed. Conversely, many recent works have relied on non-\nphysics-based simulators (e.g., ignoring or greatly simplifying\ncontact physics in gasping) that instead focus on high-level,\nlong-term tasks and visual environment models. Examples of\nthis class of simulators are common and include many of the\nsimulators described above in Sec. V. For example the Gibson\nfamily of simulators [146], [206], the Habitat family [147],\n[208], [209], RobotTHOR [210], and VirtualHome [129] all\nneglect low-level physics in favor of simulating higher level\ntasks with high visual \ufb01delity. This leads to a large sim-to-\nreal gap and introduces variability in real-world performance\nbased on how low-level planning and control modules handle\nthe true physics of the scenario. Even when physics-based\nsimulators are used (e.g., PyBullet or MuJoCo), the absence\nof standardized simulation settings, computing environments,\nand a persistent sim-to-real gap impede efforts to benchmark\nand compare performance across various research endeavors.\nA combination of open hardware, benchmarking in physics-\nbased simulators, and promoting transparency in experimental\nand simulation setups can signi\ufb01cantly alleviate the challenges\nassociated with reproducibility in the integration of foundation\nmodels into robotics research. These practices contribute to\nthe development of a more robust and collaborative research\necosystem within the \ufb01eld.\nVII. CONCLUSION\nThrough examination of the recent literature, we have\nsurveyed the diverse and promising applications of foundation\nmodels in robotics. We have delved into how these models\nhave enhanced the capabilities of robots in areas such as\ndecision-making, planning and control, and perception. We\nalso discussed the literature on embodied AI and generalist\nAI, with an eye toward opportunities for roboticists to extend\nthe concepts in that research \ufb01eld to real-world robotic applica-\ntions. Generalization, zero-shot capabilities, multimodal capa-\nbilities, and scalability of foundation models have the potential\nto transform robotics. However, as we navigate through this\nparadigm shift in incorporating foundation models in robotics\napplications, it is imperative to recognize the challenges and\npotential risks that must be addressed in future research. Data\nscarcity in robotics applications, high variability in robotics\nsettings, uncertainty quanti\ufb01cation, safety evaluation, and real-\ntime performance remain signi\ufb01cant concerns that demand\nfuture research. We have delved into some of these challenges\nand have discussed potential avenues for improvement.\nACKNOWLEDGMENTS\nThe \ufb01rst author was supported on an ASEE e-Fellows\npostdoctoral fellowship. J.T. and S.T. were partially supported\nby NSF Graduate Research Fellowships. This project was\nalso partially supported by DARPA project HR001120C0107\nand by a gift from Meta. We are grateful for this sup-\nport. Anirudha Majumdar was supported by the NSF CA-\nREER Award [#2044149] and the Of\ufb01ce of Naval Research\n[N00014-23-1-2148].\nREFERENCES\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805, 2018.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. Language models are few-shot learners.\nNeurIPS, 33:1877\u20131901, 2020.\n28\n[3] OpenAI. GPT-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023.\n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Ma-\nrina Meila and Tong Zhang, editors, ICML, volume 139 of Proceedings\nof Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul\n2021.\n[5] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-\nto-image generation. In ICML, pages 8821\u20138831. PMLR, 2021.\n[6] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha\nChowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan\nVuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet,\nDaniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Haus-\nman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and\nPete Florence. PaLM-E: An embodied multimodal language model. In\narXiv preprint arXiv:2303.03378, 2023.\n[7] Jiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou, and\nAnimesh Garg. PlaTe: Visually-grounded planning with transformers\nin procedural tasks. IEEE Robotics and Automation Letters, 7(2):4924\u2013\n4930, 2022.\n[8] Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang\nZhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo Xiao, Wu Yuan,\nNingli Wang, Dong Xu, and Benny Lo.\nLarge ai models in health\ninformatics: Applications, challenges, and the future. IEEE Journal of\nBiomedical and Health Informatics, 27(12):6074\u20136087, 2023.\n[9] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang\nChu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe\nGeng, et al. Reasoning with foundation models: Concepts, method-\nologies, and outlook. In Zenodo preprint 10.5281/zenodo.10298866,\n2023.\n[10] Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xi-\naoqing Ye, Zhe Liu, and Xiang Bai. SAM3D: Zero-shot 3D object de-\ntection via segment anything model. arXiv preprint arXiv:2306.02245,\n2023.\n[11] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du,\nZhenfang Chen, and Chuang Gan. 3D-LLM: Injecting the 3D world\ninto large language models. arXiv preprint arXiv:2307.12981, 2023.\n[12] William Chen, Siyi Hu, Rajat Talak, and Luca Carlone. Leveraging\nlarge language models for robot 3D scene understanding. arXiv preprint\narXiv:2209.05629, 2022.\n[13] Sherry Yang, O\ufb01r Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and\nDale Schuurmans. Foundation models for decision making: Problems,\nmethods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.\n[14] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, Shuran Song,\nAravind Rajeswaran, and Vikash Kumar. CACTI: A framework for\nscalable multi-task multi-scene visual imitation learning. arXiv preprint\narXiv:2212.05711, 2022.\n[15] Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus\nWulfmeier, Nicolas Heess, and Martin Riedmiller. Towards a uni\ufb01ed\nagent with foundation models. In Workshop on Reincarnating Rein-\nforcement Learning at ICLR 2023, 2023.\n[16] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh.\nReward design with language models. In ICLR, 2023.\n[17] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue\nYang, Kun Shao, David Mguni, Yali Du, and Jun Wang. ChessGPT:\nBridging policy learning and language modeling.\narXiv preprint\narXiv:2306.09200, 2023.\n[18] Yifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen.\nZero-shot visual question answering with language model feedback.\narXiv preprint arXiv:2305.17006, 2023.\n[19] Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang,\nChen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al.\nAnnoLLM: Making large language models to be better crowdsourced\nannotators. arXiv preprint arXiv:2303.16854, 2023.\n[20] Xuan Xiao, Jiahang Liu, Zhipeng Wang, Yanmin Zhou, Yong Qi, Qian\nCheng, Bin He, and Shuo Jiang. Robot learning in the era of foundation\nmodels: A survey. arXiv preprint arXiv:2311.14379, 2023.\n[21] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi,\nand Graham Neubig.\nPre-train, prompt, and predict: A systematic\nsurvey of prompting methods in natural language processing.\nACM\nComputing Surveys, 55(9):1\u201335, 2023.\n[22] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,\nFahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A\nsurvey. ACM Computing Surveys, 54(10s):1\u201341, 2022.\n[23] Muning Wen, Runji Lin, Hanjing Wang, Yaodong Yang, Ying Wen, Luo\nMai, Jun Wang, Haifeng Zhang, and Weinan Zhang. Large sequence\nmodels for sequential decision-making: a survey. Frontiers of Computer\nScience, 17(6):176349, 2023.\n[24] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\nArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258, 2021.\n[25] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and\nwhere pathways for robotic manipulation. In CoRL, pages 894\u2013906.\nPMLR, 2022.\n[26] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan\nTompson, Sergey Levine, and Pierre Sermanet. Learning latent plans\nfrom play. In CoRL, pages 1113\u20131132. PMLR, 2020.\n[27] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-Actor: A\nmulti-task transformer for robotic manipulation. In CoRL, pages 785\u2013\n799. PMLR, 2023.\n[28] Corey Lynch and Pierre Sermanet.\nLanguage conditioned imitation\nlearning over unstructured data. Robotics: Science and Systems, 2021.\n[29] Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar,\nChelsea Finn, Dorsa Sadigh, and Percy Liang.\nLanguage-driven\nrepresentation learning for robotics. In RSS, 2023.\n[30] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja,\nFeryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg,\nMichael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi,\nLucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem,\nMaria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya\nPathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rockt\u00a8aschel,\nYannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York,\nAlexander Zacherl, and Lei Zhang.\nHuman-timescale adaptation in\nan open-ended task space. In ICML, 2023.\n[31] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and\nAbhinav Gupta.\nR3M: A universal visual representation for robot\nmanipulation. arXiv preprint arXiv:2203.12601, 2022.\n[32] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,\nAlexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang,\nRyan Julian, et al. Do as I can, not as I say: Grounding language in\nrobotic affordances. In CoRL, pages 287\u2013318. PMLR, 2023.\n[33] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete\nFlorence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen\nChebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu,\nSergey Levine, Karol Hausman, and Brian Ichter. Inner monologue:\nEmbodied reasoning through planning with language models. In arXiv\npreprint arXiv:2207.05608, 2022.\n[34] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu,\nand Li Fei-Fei.\nVoxPoser: Composable 3D value maps for robotic\nmanipulation with language models. In CoRL, 2023.\n[35] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell.\nZero-shot\nreward speci\ufb01cation via grounded natural language. In ICML, pages\n14743\u201314752. PMLR, 2022.\n[36] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bas-\ntani, Vikash Kumar, and Amy Zhang. VIP: Towards universal visual\nreward and representation via value-implicit pre-training.\nIn ICLR,\n2023.\n[37] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and\nDinesh Jayaraman. LIV: Language-image representations and rewards\nfor robotic control. In ICML, 2023.\n[38] Suraj Nair, Eric Mitchell, Kevin Chen, brian ichter, Silvio Savarese,\nand Chelsea Finn. Learning language-conditioned robot behavior from\nof\ufb02ine data and crowd-sourced annotation. In CoRL, pages 1303\u20131315.\nPMLR, 08\u201311 Nov 2022.\n[39] Yongchao Chen, Rujul Gandhi, Yang Zhang, and Chuchu Fan. NL2TL:\nTransforming natural languages to temporal logics using large language\nmodels. arXiv preprint arXiv:2305.07766, 2023.\n[40] Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu\nFan. AutoTAMP: Autoregressive task and motion planning with llms\nas translators and checkers. arXiv preprint arXiv:2306.06531, 2023.\n[41] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei\nXu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh\nGarg. ProgPrompt: Generating situated robot task plans using large\nlanguage models. In ICRA, pages 11523\u201311530. IEEE, 2023.\n[42] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian\nIchter, Pete Florence, and Andy Zeng. Code as Policies: Language\nmodel programs for embodied control.\nIn ICRA, pages 9493\u20139500.\nIEEE, 2023.\n29\n[43] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor.\nChatGPT for Robotics: Design principles and model abilities. Technical\nReport MSR-TR-2023-8, Microsoft, 2023.\n[44] Anthony Brohan, Noah Brown, Justice Carbajal, and et al.\nRT-1:\nRobotics transformer for real-world control at scale. In RSS, 2023.\n[45] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei\nXia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan\nVuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh,\nJaspiar Singh, Pierre Sermanet, Pannag R Sanketi, Grecia Salazar,\nMichael S Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor\nMordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee,\nTsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalash-\nnikov, Ryan Julian, Nikhil J Joshi, Alex Irpan, brian ichter, Jasmine\nHsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan,\nChuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey,\nDanny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen,\nYevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan,\nMontserrat Gonzalez Arenas, and Kehang Han. RT-2: Vision-language-\naction models transfer web knowledge to robotic control. In CoRL,\n2023.\n[46] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex\nHerzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh,\nAnthony Brohan, et al. Open X-Embodiment: Robotic learning datasets\nand RT-X models. arXiv preprint arXiv:2310.08864, 2023.\n[47] Rogerio Bonatti, Sai Vemprala, Shuang Ma, Felipe Frujeri, Shuhang\nChen, and Ashish Kapoor. PACT: Perception-action causal transformer\nfor autoregressive robotics pretraining. In IROS. IEEE, 2023.\n[48] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Ma-\nlik.\nMasked visual pre-training for motor control.\narXiv preprint\narXiv:2203.06173, 2022.\n[49] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra\nMalik, and Trevor Darrell.\nReal-world robot learning with masked\nvisual pre-training. In CoRL, pages 416\u2013426. PMLR, 2023.\n[50] Arthur Bucker, Luis Figueredo, Sami Haddadin, Ashish Kapoor,\nShuang Ma, Sai Vemprala, and Rogerio Bonatti. LATTE: LAnguage\nTrajectory TransformEr. In ICRA, pages 7287\u20137294. IEEE, 2023.\n[51] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neu-\nmann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran,\nAnurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-\nvocabulary object detection with vision transformers. In ECCV, pages\n728\u2013755. Springer, 2022.\n[52] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang,\nChunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-\nNeng Hwang, et al. Grounded language-image pre-training. In CVPR,\npages 10965\u201310975, 2022.\n[53] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie\nYang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding\nDINO: Marrying DINO with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023.\n[54] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin\nCui, Yu Qiao, Peng Gao, and Hongsheng Li. PointCLIP: Point cloud\nunderstanding by CLIP. In CVPR, pages 8552\u20138562, 2022.\n[55] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and\nJiwen Lu. Point-BERT: Pre-training 3D point cloud transformers with\nmasked point modeling. In CVPR, pages 19313\u201319322, 2022.\n[56] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Jiajun Wu,\nCaiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese.\nULIP: Learning uni\ufb01ed representation of language, image and point\ncloud for 3D understanding. arXiv preprint arXiv:2212.05171, 2022.\n[57] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nJiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio\nSavarese.\nULIP-2: Towards scalable multimodal pre-training for 3d\nunderstanding. arXiv preprint arXiv:2305.08275, 2023.\n[58] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and\nRene Ranftl. Language-driven semantic segmentation. In ICLR, 2022.\n[59] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe\nRolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.\nBerg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything.\nIn ICCV, pages 4015\u20134026, October 2023.\n[60] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li,\nMing Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint\narXiv:2306.12156, 2023.\n[61] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung Ho\nBae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything:\nTowards lightweight sam for mobile applications.\narXiv preprint\narXiv:2306.14289, 2023.\n[62] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and\nFeng Zheng. Track anything: Segment anything meets videos. arXiv\npreprint arXiv:2304.11968, 2023.\n[63] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing\nLiao.\nCLIP-NeRF: Text-and-image driven manipulation of neural\nradiance \ufb01elds. In CVPR, pages 3835\u20133844, 2022.\n[64] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and\nMatthew Tancik. LERF: Language embedded radiance \ufb01elds. In ICCV,\npages 19729\u201319739, 2023.\n[65] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decom-\nposing NeRF for editing via feature \ufb01eld distillation. In NeurIPS, 2022.\n[66] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birch-\n\ufb01eld, Jiaming Song, Shubham Tulsiani, and Sifei Liu.\nAffordance\ndiffusion: Synthesizing hand-object interactions. In CVPR, 2023.\n[67] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak\nPathak. Affordances from human videos as a versatile representation\nfor robotics. In CVPR, 2023.\n[68] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.\nLanguage models as zero-shot planners: Extracting actionable knowl-\nedge for embodied agents. In ICML, 2022.\n[69] Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong\nJiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and\nMatthew R. Walter.\nStatler: State-maintaining language models for\nembodied reasoning. arXiv preprint arXiv:2306.17840, 2023.\n[70] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,\nJun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. EmbodiedGPT:\nVision-language pre-training via embodied chain of thought.\narXiv\npreprint arXiv:2305.15021, 2023.\n[71] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong\nYang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima\nAnandkumar. MineDojo: Building open-ended embodied agents with\ninternet-scale knowledge. In NeurIPS Datasets and Benchmarks Track,\n2022.\n[72] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang,\nAdrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.\nVideo PreTraining (VPT): Learning to act by watching unlabeled online\nvideos. In NeurIPS, 2022.\n[73] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei\nXiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An\nopen-ended embodied agent with large language models. arXiv preprint\narXiv: Arxiv-2305.16291, 2023.\n[74] Yuqing Du, Olivia Watkins, Zihan Wang, C\u00b4edric Colas, Trevor Darrell,\nPieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretrain-\ning in reinforcement learning with large language models. In ICML,\npages 8657\u20138677. PMLR, 23\u201329 Jul 2023.\n[75] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine\ntranslation of rare words with subword units. In ACL, 2016.\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. NeurIPS, 2017.\n[77] Daniel Dugas. The gpt-3 architecture, on a napkin. [Online; accessed\n28-November-2023].\n[78] Wikipedia. GPT-3. [Online; accessed 28-November-2023].\n[79] John Thickstun.\nThe Transformer Model in Equations.\n[Online;\naccessed 28-November-2023].\n[80] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M\nLjung. Time series analysis: forecasting and control. John Wiley &\nSons, 2015.\n[81] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,\net al. Improving language understanding by generative pre-training.\nhttps://openai.com/research/language-unsupervised, 2018.\n[82] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan\nSepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by\nsummarizing long sequences. In ICLR, 2018.\n[83] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning,\nand Curtis P Langlotz. Contrastive learning of medical visual repre-\nsentations from paired images and text. In MLHC, 2022.\n[84] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey\nHinton. A simple framework for contrastive learning of visual rep-\nresentations. In ICML, 2020.\n[85] Kihyuk Sohn. Improved deep metric learning with multi-class N-pair\nloss objective. In NeurIPS, 2016.\n[86] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsuper-\nvised feature learning via non-parametric instance discrimination. In\nCVPR, 2018.\n30\n[87] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\nRepresen-\ntation learning with contrastive predictive coding.\narXiv preprint\narXiv:1807.03748, 2018.\n[88] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and\nMark Chen. Hierarchical text-conditional image generation with CLIP\nlatents. arXiv preprint arXiv:2204.06125, 2022.\n[89] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\nGanguli. Deep unsupervised learning using nonequilibrium thermody-\nnamics. In ICML, 2015.\n[90] Yang Song and Stefano Ermon. Generative modeling by estimating\ngradients of the data distribution. In NeurIPS, 2019.\n[91] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion\nprobabilistic models. In NeurIPS, 2020.\n[92] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\nIlya Sutskever, et al.\nLanguage models are unsupervised multitask\nlearners. OpenAI Blog, 2019.\n[93] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd\nschema challenge. In KR, 2012.\n[94] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\n[95] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and ef\ufb01cient\nfoundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[96] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du,\nBenton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarc\u00b4\u0131a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou,\nDaphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander\nSpiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omer-\nnick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie\nPellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark D\u00b4\u0131az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\nPaLM: Scaling language modeling with pathways.\narXiv preprint\narXiv:2204.02311, 2022.\n[97] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming\nDing, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam\nTam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: An open\nbilingual pre-trained model. In ICLR, 2023.\n[98] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR, 2021.\n[99] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo,\nZhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A\nsurvey on vision transformer. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022.\n[100] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,\nFahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A\nsurvey. ACM Computing Surveys, 2022.\n[101] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.\nScaling vision transformers. In CVPR, 2022.\n[102] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr\nPadlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil\nMustafa, Lucas Beyer, et al.\nPaLI: A jointly-scaled multilingual\nlanguage-image model. In NeurIPS, 2022.\n[103] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski,\nJonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert\nGeirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to\n22 billion parameters. In ICML, 2023.\n[104] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit\nChangpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman,\nXiao Wang, Yi Tay, et al. PaLI-X: On scaling up a multilingual vision\nand language model. arXiv preprint arXiv:2305.18565, 2023.\n[105] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien\nMairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties\nin self-supervised vision transformers. In ICCV, 2021.\n[106] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep resid-\nual learning for image recognition. arXiv preprint arXiv:1512.03385,\n2015.\n[107] Maxime Oquab, Timoth\u00b4ee Darcet, Theo Moutakanni, Huy V. Vo, Marc\nSzafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco\nMassa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,\nVasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido\nAssran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDINOv2: Learning robust visual features without supervision. arXiv\npreprint arXiv:2304.07193, 2023.\n[108] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP:\nBootstrapping language-image pre-training for uni\ufb01ed vision-language\nunderstanding and generation. In ICML, 2022.\n[109] Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang\nYe, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, and\nHang Xu. CLIP2: Contrastive language-image-point pretraining from\nreal-world point cloud data. In CVPR, 2023.\n[110] Lewei Yao, Runhu Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang\nXu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. FILIP:\nFine-grained interactive language-image pre-training. In ICLR, 2022.\n[111] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and\nKaiming He.\nScaling language-image pre-training via masking.\nIn\nCVPR, 2023.\n[112] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-\nto-image generation. In Marina Meila and Tong Zhang, editors, Pro-\nceedings of the 38th International Conference on Machine Learning,\nvolume 139 of Proceedings of Machine Learning Research, pages\n8821\u20138831. PMLR, 18\u201324 Jul 2021.\n[113] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and\nRoss Girshick. Masked autoencoders are scalable vision learners. In\nCVPR, pages 16000\u201316009, 2022.\n[114] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, pages 8748\u20138763, 2021.\n[115] Ge Yuying, Macaluso Annabella, Erran Li Li, Luo Ping, and Wang\nXiaolong.\nPolicy adaptation from foundation model feedback.\nIn\nCVPR, 2023.\n[116] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan\nChien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong,\nVikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging\nthe visual world for robotic manipulation. In CoRL, 2020.\n[117] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,\nand Bj\u00a8orn Ommer. High-resolution image synthesis with latent diffu-\nsion models. In CVPR, 2022.\n[118] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei\nXu, Yuke Zhu, and Anima Anandkumar.\nMimicPlay: Long-horizon\nimitation learning by watching human play. In CoRL, 2023.\n[119] Rutav Shah, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, and Yuke Zhu. MUTEX: Learning\nuni\ufb01ed policies from multimodal task speci\ufb01cations. In CoRL, 2023.\n[120] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever,\nand Pieter Abbeel.\nRL\u02c62: Fast reinforcement learning via slow\nreinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[121] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet\nLe, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language\nmodels beyond a \ufb01xed-length context. In ACL, 2019.\n[122] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina\nBarros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz,\nMax Jaderberg, Micha\u00a8el Mathieu, Nat McAleese, Nathalie Bradley-\nSchmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph\nHughes-Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-\nended learning leads to generally capable agents.\narXiv preprint\narXiv:2107.12808, 2021.\n[123] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler,\nAntonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan\nMunro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric\nvision: The EPIC-KITCHENS dataset. In ECCV, 2018.\n[124] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and\nJeannette Bohg. Text2Motion: From natural language instructions to\nfeasible plans.\nAutonomous Robots. Special Issue: Large Language\nModels in Robotics, 2023.\n31\n[125] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard.\nGrounding\nlanguage with visual affordances over unstructured data.\nIn ICRA,\npages 11576\u201311582. IEEE, 2023.\n[126] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.\nExploring the limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u2013\n5551, 2020.\n[127] Junning Huang, Sirui Xie, Jiankai Sun, Qiurui Ma, Chunxiao Liu,\nDahua Lin, and Bolei Zhou. Learning a decision module by imitating\ndriver\u2019s control behaviors. In CoRL, pages 1\u201310. PMLR, 2021.\n[128] Jiankai Sun, Hao Sun, Tian Han, and Bolei Zhou.\nNeuro-symbolic\nprogram search for autonomous driving decision module design. In\nCoRL, pages 21\u201330. PMLR, 2021.\n[129] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja\nFidler, and Antonio Torralba.\nVirtualHome: Simulating household\nactivities via programs. In CVPR, pages 8494\u20138502, 2018.\n[130] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao\nChang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context\nlearning. arXiv preprint arXiv:2301.00234, 2022.\n[131] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,\nEd Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting\nelicits reasoning in large language models. NeurIPS, 35:24824\u201324837,\n2022.\n[132] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess,\nMontserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy\nZeng. Large language models as general pattern machines. In arXiv\npreprint arXiv:2307.04721, 2023.\n[133] Zhiwei Jia, Fangchen Liu, Vineet Thumuluri, Linghao Chen, Zhiao\nHuang, and Hao Su. Chain-of-thought predictive control. arXiv preprint\narXiv:2304.00776, 2023.\n[134] Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong\nHuang, and Ashish Kapoor.\nSMART: Self-supervised multi-task\npretraining with control transformers. In ICLR, 2023.\n[135] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi\nParikh, and Dhruv Batra. Improving vision-and-language navigation\nwith image-text pairs from the web. In ECCV, pages 259\u2013274. Springer,\n2020.\n[136] Dhruv Shah, B\u0142a\u02d9zej Osi\u00b4nski, Sergey Levine, et al. LM-Nav: Robotic\nnavigation with large pre-trained models of language, vision, and\naction. In CoRL, pages 492\u2013504. PMLR, 2023.\n[137] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin\nBlack, Noriaki Hirose, and Sergey Levine. ViNT: A Foundation Model\nfor Visual Navigation. In CoRL, 2023.\n[138] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard.\nAudio visual language maps for robot navigation.\narXiv preprint\narXiv:2303.07522, 2023.\n[139] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and\nJosef Sivic. NetVLAD: CNN architecture for weakly supervised place\nrecognition. In CVPR, pages 5297\u20135307, 2016.\n[140] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.\nSu-\nperpoint: Self-supervised interest point detection and description. In\nCVPR Deep Learning for Visual SLAM Workshop, 2018.\n[141] Andrey Guzhov, Federico Raue, J\u00a8orn Hees, and Andreas Dengel.\nAudioCLIP: Extending clip to image, text and audio. In ICASSP, pages\n976\u2013980. IEEE, 2022.\n[142] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and\nDevendra Singh Chaplot. Navigating to objects in the real world. arXiv\npreprint arXiv:2212.00922, 2023.\n[143] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig\nSchmidt, and Shuran Song. CoWs on pasture: Baselines and bench-\nmarks for language-driven zero-shot object navigation. In CVPR, pages\n23171\u201323181, 2023.\n[144] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber,\nMatthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and\nYinda Zhang.\nMatterport3D: Learning from RGB-D data in indoor\nenvironments. In 3DV, pages 667\u2013676, 2017.\n[145] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson,\nNiko S\u00a8underhauf, Ian Reid, Stephen Gould, and Anton Van Den Hen-\ngel.\nVision-and-language navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In CVPR, pages 3674\u2013\n3683, 2018.\n[146] Chengshu Li, Fei Xia, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Michael Lingelbach,\nSanjana Srivastava, Bokui Shen, Kent Elliott Vainio, Cem Gokmen,\nGokul Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon\nGweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese.\niGibson 2.0:\nObject-centric simulation for robot learning of everyday household\ntasks. In CoRL, 2021.\n[147] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili\nZhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh\nChaplot, Oleksandr Maksymets, et al.\nHabitat 2.0: Training home\nassistants to rearrange their habitat. In NeurIPS, 2021.\n[148] Yu Bangguo, Kasaei Hamidreza, and Cao Ming. LL3MVN: Leveraging\nlarge language models for visual target navigation.\narXiv preprint\narXiv:2304.05501, 2023.\n[149] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,\nDanqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\n[150] Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, and\nFisher Yu.\nHow to not train your dragon: Training-free embod-\nied object goal navigation with semantic frontiers.\narXiv preprint\narXiv:2305.16925, 2023.\n[151] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin\nWang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain,\nAlexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, An-\ngel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi,\nYonatan Bisk, and Chris Paxton. Homerobot: Open vocabulary mobile\nmanipulation. arXiv preprint arXiv:2306.11565, 2023.\n[152] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang\nDou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and\nLinxi Fan.\nVIMA: General robot manipulation with multimodal\nprompts. In ICML, 2023.\n[153] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin,\nAlex X Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim\nGupta, Akhil Raju, et al. RoboCat: A self-improving foundation agent\nfor robotic manipulation. arXiv preprint arXiv:2306.11706, 2023.\n[154] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Col-\nmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez,\nYury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist\nagent. arXiv preprint arXiv:2205.06175, 2022.\n[155] Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, and Chris\nPaxton. StructDiffusion: Language-guided creation of physically-valid\nstructures using unseen objects. In RSS, 2023.\n[156] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-\nHuei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna\nZitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world\nobject manipulation using pre-trained vision-language model.\narXiv\npreprint arXiv:2303.00905, 2023.\n[157] Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. DALL-E-Bot:\nIntroducing web-scale diffusion models to robotics.\nIEEE Robotics\nand Automation Letters, 8(7):3956\u20133963, 2023.\n[158] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Girshick. Mask\nR-CNN. In ICCV, pages 2980\u20132988, 2017.\n[159] Tay Yi, Dehghani Mostafa, Tran Vinh Q., Garcia Xavier, Wei Jason,\nWang Xuezhi, Chung Hyung Won, Shakeri Siamak, Bahri Dara,\nSchuster Tal, Steven Zheng Huaixiu, Zhou Denny, Houlsby Neil, and\nDonald Metzler. UL2: Unifying language learning paradigms. arXiv\npreprint arXiv:2205.05131, 2023.\n[160] Shuang Ma, Sai Vemprala, Wenshan Wang, Jayesh K Gupta, Yale\nSong, Daniel McDufft, and Ashish Kapoor. COMPASS:: Contrastive\nmultimodal pretraining for autonomous systems. In IROS, pages 1000\u2013\n1007. IEEE, 2022.\n[161] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency.\nMultimodal machine learning: A survey and taxonomy. IEEE Trans.\nPattern Anal. Mach. Intell., 41(2):423\u2013443, feb 2019.\n[162] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih\nPorikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point\nclouds via pretrained image-language models. In CVPR, pages 21736\u2013\n21746, 2023.\n[163] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan,\nQixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song,\nHao Su, et al. ShapeNet: An information-rich 3D model repository.\narXiv preprint arXiv:1512.03012, 2015.\n[164] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision trans-\nformers for dense prediction. In ICCV, pages 12179\u201312188, 2021.\n[165] Ho Kei Cheng and Alexander G Schwing. XMem: Long-term video\nobject segmentation with an atkinson-shiffrin memory model.\nIn\nECCV, pages 640\u2013658. Springer, 2022.\n[166] Qiuhong Shen, Xingyi Yang, and Xinchao Wang.\nAnything-3D:\nTowards single-view anything reconstruction in the wild. arXiv preprint\narXiv:2304.10261, 2023.\n32\n[167] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T\nBarron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes\nas neural radiance \ufb01elds for view synthesis. Communications of the\nACM, 65(1):99\u2013106, 2021.\n[168] Jiankai Sun, Yan Xu, Mingyu Ding, Hongwei Yi, Chen Wang, Jingdong\nWang, Liangjun Zhang, and Mac Schwager. NeRF-Loc: Transformer-\nbased object localization within neural radiance \ufb01elds. IEEE Robotics\nand Automation Letters, 8(8):5244\u20135250, 2023.\n[169] Jiankai Sun, Jianing Qiu, Chuanyang Zheng, John Tucker, Javier Yu,\nand Mac Schwager. Aria-NeRF: Multimodal egocentric view synthesis.\narXiv preprint arXiv:2311.06455, 2023.\n[170] Nur Muhammad Mahi Sha\ufb01ullah, Chris Paxton, Lerrel Pinto, Soumith\nChintala, and Arthur Szlam. CLIP-Fields: Weakly supervised semantic\n\ufb01elds for robotic memory. In RSS, 2023.\n[171] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard.\nVisual language maps for robot navigation. In 2023 IEEE International\nConference on Robotics and Automation (ICRA), pages 10608\u201310615.\nIEEE, 2023.\n[172] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3D scene\nunderstanding from 2D vision-language models. In CoRL, 2022.\n[173] Jianglong Ye, Naiyan Wang, and Xiaolong Wang.\nFeatureNeRF:\nLearning generalizable nerfs by distilling pre-trained vision foundation\nmodels. arXiv preprint arXiv:2303.12786, 2023.\n[174] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi.\nNeural Feature Fusion Fields: 3D distillation of self-supervised 2D\nimage representations. In 3DV, pages 443\u2013453. IEEE, 2022.\n[175] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser, Leonidas\nGuibas, Hao Su, and Kyle Genova. Ner\ufb02ets: Local radiance \ufb01elds for\nef\ufb01cient structure-aware 3d scene representation from 2d supervision.\nIn CVPR, pages 8274\u20138284, 2023.\n[176] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony\nBrohan, Su Wang, Jaspiar Singh, Clayton Tan, Dee M, Jodilyn Peralta,\nBrian Ichter, Karol Hausman, and Fei Xia. Scaling robot learning with\nsemantically imagined experience. In arXiv preprint arXiv:2302.11550,\n2023.\n[177] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai\nNoy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet,\nRadu Soricut, et al.\nImagen editor and editbench: Advancing and\nevaluating text-guided image inpainting. In CVPR, pages 18359\u201318369,\n2023.\n[178] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. GenAug:\nRetargeting behaviors to unseen situations via generative augmentation.\nIn RSS, 2023.\n[179] Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenen-\nbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann.\nNeural Descriptor Fields: SE(3)-equivariant object representations for\nmanipulation. In ICRA, 2022.\n[180] William Shen, Ge Yang, Alan Yu, Jensen Wong, Leslie Pack Kaelbling,\nand Phillip Isola. Distilled feature \ufb01elds enable few-shot manipulation.\nIn CoRL, 2023.\n[181] Walter Goodwin, Ioannis Havoutis, and Ingmar Posner. You only look\nat one: Category-level object representations for pose estimation from\na single example. In CoRL, 2022.\n[182] Walter Goodwin, Sagar Vaze, Ioannis Havoutis, and Ingmar Posner.\nZero-shot category-level object pose estimation. In ECCV, 2022.\n[183] Shinji Umeyama. Least-squares estimation of transformation parame-\nters between two point patterns. IEEE Transactions on Pattern Analysis\n& Machine Intelligence, 13(04):376\u2013380, 1991.\n[184] Jiankai Sun, Lantao Yu, Pinqian Dong, Bo Lu, and Bolei Zhou.\nAdversarial inverse reinforcement learning with self-attention dynamics\nmodel. IEEE Robotics and Automation Letters, 6(2):1880\u20131886, 2021.\n[185] Jiankai Sun, Shreyas Kousik, David Fridovich-Keil, and Mac Schwa-\nger.\nConnected autonomous vehicle motion planning with video\npredictions from smart, self-supervised infrastructure. arXiv preprint\narXiv:2309.07504, 2023.\n[186] Jiankai Sun, Shreyas Kousik, David Fridovich-Keil, and Mac Schwager.\nSelf-supervised traf\ufb01c advisors: Distributed, multi-view traf\ufb01c predic-\ntion for smart cities. In IEEE ITSC, 2022.\n[187] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine.\nPlanning with diffusion for \ufb02exible behavior synthesis. In ICML, 2022.\n[188] Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Talpur Nobel, Mykel\nKochenderfer,\nand\nMac\nSchwager.\nConformal\nprediction\nfor\nuncertainty-aware\nplanning with diffusion dynamics model.\nIn\nNeurIPS, 2023.\n[189] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Her-\nnan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro,\nJulius Kunze, and D. Erhan. Phenaki: Variable length video generation\nfrom open domain textual description. In ICLR, 2023.\n[190] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette\nBucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and\nChelsea Finn. RoboNet: Large-scale multi-robot learning. In CoRL,\n2019.\n[191] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fe-\ndoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. GAIA-1:\nA generative world model for autonomous driving.\narXiv preprint\narXiv:2309.17080, 2023.\n[192] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, O\ufb01r Nachum, Joshua B\nTenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal\npolicies via text-guided video generation. In NeurIPS, 2023.\n[193] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid,\nBrian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B.\nTenenbaum, Leslie Kaelbling, Andy Zeng, and Jonathan Tompson.\nVideo language planning. arXiv preprint arXiv:2310.10625, 2023.\n[194] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski,\nAdrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,\nand Pete Florence. Socratic Models: Composing zero-shot multimodal\nreasoning with language. arXiv, 2022.\n[195] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja,\nSheila Babayan, Felix Hill, and Rob Fergus.\nCollaborating with\nlanguage models for embodied reasoning.\nIn Second Workshop on\nLanguage and Reinforcement Learning, 2022.\n[196] Herman A. Engelbrecht and Gregor Schiele. Transforming minecraft\ninto a research platform. In IEEE CCNC, 2014.\n[197] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu\nYang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaox-\niang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally capable\nagents for open-world environments via large language models with\ntext-based knowledge and memory. arXiv preprint arXiv:2305.17144,\n2023.\n[198] Ziqi Zhang, Yile Wang, Yue Zhang, and Donglin Wang. Can of\ufb02ine\nreinforcement learning help natural language understanding?\narXiv\npreprint arXiv:2212.03864, 2022.\n[199] Denis Tarasov, Vladislav Kurenkov, and Sergey Kolesnikov. Prompts\nand pre-trained language models for of\ufb02ine reinforcement learning. In\nICLR Workshop on Generalizable Policy Learning in Physical World,\n2022.\n[200] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can Wikipedia\nhelp of\ufb02ine reinforcement learning? arXiv preprint arXiv:2201.12122,\n2022.\n[201] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, et al.\nEvaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\n[202] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik\nNarasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting\nin language models. In ICLR, 2023.\n[203] Joon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel\nMorris, Percy Liang, and Michael S. Bernstein. Generative Agents:\nInteractive simulacra of human behavior. In ACM Symposium on User\nInterface Software and Technology, 2023.\n[204] Zhou Xian, Theophile Gervet, Zhenjia Xu, Yi-Ling Qiao, Tsun-Hsuan\nWang, and Yian Wang.\nTowards generalist robots: A promising\nparadigm via generative simulation. arXiv preprint arXiv:2305.10455,\n2023.\n[205] Rutav Shah and Vikash Kumar.\nRRL: Resnet as representation for\nreinforcement learning. In ICML, 2021.\n[206] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik,\nand Silvio Savarese. Gibson Env: real-world perception for embodied\nagents. In CVPR, 2018.\n[207] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Sri-\nvastava, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Chen Wang, Gabrael Levine, Michael\nLingelbach, Jiankai Sun, Mona Anvari, Minjune Hwang, Manasi\nSharma, Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-Young\nKim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang\nTang, Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon, Karen\nLiu, Jiajun Wu, and Li Fei-Fei.\nBEHAVIOR-1K: A benchmark for\nembodied AI with 1,000 everyday activities and realistic simulation.\nIn CoRL, 2022.\n[208] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili\nZhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen\nKoltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.\nHabitat: A\nPlatform for Embodied AI Research. In ICCV, 2019.\n33\n[209] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote,\nTsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William\nClegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: A co-habitat for\nhumans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023.\n[210] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric\nKolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli Van-\nderBilt, Matthew Wallingford, et al. RoboTHOR: An open simulation-\nto-real embodied AI platform. In CVPR, 2020.\n[211] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng\nQiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer.\nTartanAir: A dataset to push the limits of visual SLAM.\nIn IROS,\n2020.\n[212] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor.\nAirSim: High-\ufb01delity visual and physical simulation for autonomous\nvehicles. In Field and Service Robotics, 2017.\n[213] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Bro-\nhan, Karol Hausman, Sergey Levine, and Jonathan Tompson. Robotic\nskill acquisition via instruction augmentation with vision-language\nmodels. In RSS, 2023.\n[214] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. LLaMA: Open and ef\ufb01cient\nfoundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[215] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,\nand Song Han. AWQ: Activation-aware weight quantization for llm\ncompression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n[216] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh,\nStephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake\nVarley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majum-\ndar. Robots that ask for help: Uncertainty alignment for large language\nmodel planners. In CoRL, 2023.\n[217] Young-Jin Park, Hao Wang, Shervin Ardeshir, and Navid Azizan.\nRepresentation reliability and its impact on downstream tasks. arXiv\npreprint arXiv:2306.00206, 2023.\n[218] Meiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mordatch. Quantifying\nuncertainty in foundation models via ensembles. In NeurIPS Workshop\non Robustness in Sequence Modeling, 2022.\n[219] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao\nBai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer,\nKamal Ndousse, et al.\nRed teaming language models to reduce\nharms: Methods, scaling behaviors, and lessons learned. arXiv preprint\narXiv:2209.07858, 2022.\n[220] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring,\nJohn Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.\nRed teaming language models with language models. In EMNLP, 2022.\n[221] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing\nfailures of multimodal systems with language models. arXiv preprint\narXiv:2306.12105, 2023.\n[222] Kristofer D Kusano, Kurt Beatty, Scott Schnelle, Francesca Favaro,\nCam Crary, and Trent Victor. Collision avoidance testing of the waymo\nautomated driving system. arXiv preprint arXiv:2212.08148, 2022.\n[223] Nick\nWebb,\nDan\nSmith,\nChristopher\nLudwick,\nTrent\nVictor,\nQi Hommes, Francesca Favaro, George Ivanov, and Tom Daniel.\nWaymo\u2019s safety methodologies and safety readiness determinations.\narXiv preprint arXiv:2011.00054, 2020.\n[224] Wenhao Ding, Chejian Xu, Mansur Arief, Haohong Lin, Bo Li, and\nDing Zhao. A survey on safety-critical driving scenario generation\u2014a\nmethodological perspective. IEEE ITSC, 2023.\n[225] Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio\nSavarese, Edward Schmerling, and Marco Pavone.\nSample-ef\ufb01cient\nsafety assurances using conformal prediction. In WAFR, 2022.\n[226] Alec Farid, David Snyder, Allen Z Ren, and Anirudha Majumdar.\nFailure prediction with statistical guarantees for vision-based robot\ncontrol. arXiv preprint arXiv:2202.05894, 2022.\n[227] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco\nPavone.\nTask-relevant failure detection for trajectory predictors in\nautonomous vehicles. In CoRL, 2022.\n[228] Kai-Chieh Hsu, Allen Z Ren, Duy P Nguyen, Anirudha Majumdar,\nand Jaime F Fisac. Sim-to-Lab-to-Real: Safe reinforcement learning\nwith shielding and generalization guarantees.\nArti\ufb01cial Intelligence,\n314:103811, 2023.\n[229] Kai-Chieh Hsu, Haimin Hu, and Jaime Fern\u00b4andez Fisac. The safety\n\ufb01lter: A uni\ufb01ed view of safety-critical control in autonomous systems.\narXiv preprint arXiv:2309.05837, 2023.\n[230] Alec Farid, Sushant Veer, and Anirudha Majumdar. Task-driven out-\nof-distribution detection with statistical guarantees for robot learning.\nIn CoRL, 2021.\n[231] Ido Greenberg and Shie Mannor. Detecting rewards deterioration in\nepisodic reinforcement learning. In ICML, 2021.\n[232] Feiyang Cai and Xenofon Koutsoukos. Real-time out-of-distribution\ndetection in learning-enabled cyber-physical systems. In ICCPS, 2020.\n[233] Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel\nLuo, Spencer M Richards, Yixiao Sun, Edward Schmerling, and Marco\nPavone. A system-level view on out-of-distribution data in robotics.\narXiv preprint arXiv:2212.14020, 2022.\n"
  },
  {
    "title": "Clockwork Diffusion: Efficient Generation With Model-Step Distillation",
    "link": "https://arxiv.org/pdf/2312.08128.pdf",
    "upvote": "10",
    "text": "Clockwork Diffusion: Efficient Generation With Model-Step Distillation\nAmirhossein Habibian*\nAmir Ghodrati*\nNoor Fathima*\nGuillaume Sautiere\nRisheek Garrepalli\nFatih Porikli\nJens Petersen\nQualcomm AI Research\u2020\n{ahabibia, ghodrati, noor, gsautie, rgarrepa, fporikli, jpeterse}@qti.qualcomm.com\nAbstract\nThis work aims to improve the efficiency of text-to-image\ndiffusion models.\nWhile diffusion models use computa-\ntionally expensive UNet-based denoising operations in ev-\nery generation step, we identify that not all operations\nare equally relevant for the final output quality. In par-\nticular, we observe that UNet layers operating on high-\nres feature maps are relatively sensitive to small pertur-\nbations.\nIn contrast, low-res feature maps influence the\nsemantic layout of the final image and can often be per-\nturbed with no noticeable change in the output.\nBased\non this observation, we propose Clockwork Diffusion, a\nmethod that periodically reuses computation from preced-\ning denoising steps to approximate low-res feature maps\nat one or more subsequent steps.\nFor multiple base-\nlines, and for both text-to-image generation and image\nediting, we demonstrate that Clockwork leads to compa-\nrable or improved perceptual scores with drastically re-\nduced computational complexity. As an example, for Sta-\nble Diffusion v1.5 with 8 DPM++ steps we save 32% of\nFLOPs with negligible FID and CLIP change.\nWe re-\nlease code at https://github.com/Qualcomm-\nAI-research/clockwork-diffusion\n1. Introduction\nDiffusion Probabilistic Models (DPM), or Diffusion Mod-\nels for short, have become one of the most popular ap-\nproaches for text-to-image generation [34, 36]. Compared\nto Generative Adversarial Networks (GANs), they allow for\ndiverse synthesized outputs and high perceptual quality [5],\nwhile offering a relatively stable training paradigm [12] and\nhigh controllability.\nOne of the main drawbacks of diffusion models is that they\nare comparatively slow, involving repeated operation of\ncomputationally expensive UNet models [35]. As a result, a\n*Equal contribution\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies,\nInc\n454 ms \n 341 ms\nBaseline\nSD UNet\n330 ms \n 213 ms\nEfficient UNet\n240 ms \n 154 ms\nDistilled Efficient UNet\nWith Clockwork\n24.9%\n35.5%\n35.8%\nFigure 1. Time savings with Clockwork, for different baselines.\nAll pairs have roughly constant FID (computed on MS-COCO\n2017 5K validation set), using 8 sampling steps (DPM++). Clock-\nwork can be applied on top of standard models as well as heavily\noptimized ones. Timings computed on NVIDIA\u00ae RTX\u00ae 3080 at\nbatch size 1 (for distilled model) or 2 (for classifier-free guidance).\nPrompt: \u201cthe bust of a man\u2019s head is next to a vase of flowers\u201d.\nlot of current research focuses on improving their efficiency,\nmainly through two different mechanisms.\nFirst, some\nworks seek to reduce the overall number of sampling steps,\neither by introducing more advanced samplers [26, 27, 43]\nor by performing so-called step distillation [29, 37]. Sec-\nond, some works reduce the required computation per step\ne.g., through classifier-free guidance distillation [13,29], ar-\nchitecture search [21], or with model distillation [17].\nOur work can be viewed as a combination of these two axes.\nWe begin with the observation that lower-resolution repre-\nsentations within diffusion UNets (i.e. those further from\ninput and output) are not only influencing the semantic lay-\nout more than smaller details [4,41,48], they are also more\nresilient to perturbations and thus more amenable to dis-\ntillation into a smaller model. Hence, we propose to per-\nform model distillation on the lower-resolution parts of the\nUNet by reusing their representations from previous sam-\n1\narXiv:2312.08128v2  [cs.CV]  20 Feb 2024\npling steps.\nTo achieve this we make several contribu-\ntions: 1) By approximating internal UNet representations\nwith those from previous sampling steps, we are effectively\nperforming a combination of model- and step distillation,\nwhich we term model-step distillation. 2) We show how to\ndesign a lightweight adaptor architecture to maximize com-\npute savings, and even show performance improvements by\nsimply caching representations in some cases. 3) We show\nthat it is crucial to alternate approximation steps with full\nUNet passes, which is why we call our method Clockwork\nDiffusion. 4) We propose a way to train our approach with-\nout access to an underlying image dataset, and in less than\n24h on a single NVIDIA\u00ae Tesla\u00ae V100 GPU.\nWe apply Clockwork to both text-to-image generation (MS-\nCOCO [22]) and image editing (ImageNet-R-TI2I [48]),\nconsistently demonstrating savings in FLOPs as well as la-\ntency on both GPU and edge device, while maintaining\ncomparable FID and CLIP score. Clockwork is comple-\nmentary to other optimizations like step and guidance dis-\ntillation [29, 37] or efficient samplers: we show savings\neven on an optimized and DPM++ distilled Stable Diffu-\nsion model [27,34], as can be visualized in Fig. 1.\n2. Related work\nFaster solvers.\nDiffusion sampling is equivalent to inte-\ngration of an ODE or SDE [46]. As a result, many works\nattempt to perform integration with as few steps as possi-\nble, often borrowing from existing literature on numerical\nintegration. DDIM [44] introduced deterministic sampling,\ndrastically improving over the original DDPM [12]. Sub-\nsequently, works have experimented with multistep [23],\nhigher-order solvers [7, 15, 16], predictor-corrector meth-\nods [50, 51], or combinations thereof.\nDPM++ [26, 27]\nstands out as one of the fastest solvers, leveraging exponen-\ntial integration, and we conduct most of our experiments\nwith it. However, in our ablation studies in the Appendix-\nTab. 4, we show that the benefit of Clockwork is largely\nindependent of the choice of solver.\nStep Distillation\nstarts with a trained teacher model, and\nthen trains a student to mirror the output of multiple teacher\nmodel steps [28, 37]. It has been extended to guided dif-\nfusion models [21, 29], where Meng et al. [29] first distill\nunconditional and conditional model passes into one and\nthen do step distillation following [37]. Berthelot et al. [1]\nintroduce a multi-phase distillation technique similar to Sal-\nimans and Ho [37], but generalize the concept of distilling\nto a student model with fewer iterations beyond a factor of\ntwo. Other approaches do not distill students to take sev-\neral steps simultaneously, but instead aim to distill straighter\nsampling trajectories, which then admit larger step sizes for\nintegration [24,25,45]. In particular, InstaFlow [25] shows\nimpressive results with single-step generation.\nOur approach incorporates ideas from step distillation\nwherein internal UNet representations from previous steps\nare used to approximate the representations at the same\nlevel for the current step. At the same time, it is largely or-\nthogonal and can be combined with the above. We demon-\nstrate savings on an optimized Stable Diffusion model with\nstep and guidance distillation.\nEfficient Architectures.\nTo reduce the architecture com-\nplexity of UNet, model or knowledge distillation tech-\nniques have been adopted either at output level or feature\nlevel [6, 17, 21]. Model pruning [3, 21] and model quanti-\nzation [8, 30, 39] have also been explored to accelerate in-\nference at lower precision while retaining quality. Another\ndirection has been to optimize kernels for faster on-device\ninference [2], but such solutions are hardware dependent.\nOur work can be considered as model distillation, as we re-\nplace parts of the UNet with more lightweight components.\nBut unlike traditional model distillation, we only replace\nthe full UNet for some steps in the trajectory. Additionally,\nwe provide our lightweight adaptors outputs from previous\nsteps, making it closer to step distillation.\n3. Analysis of perturbation robustness\nOur method design takes root in the observation that lower-\nresolution features in diffusion UNets are robust to pertur-\nbations, as measured by the change in the final output. This\nsection provides a qualitative analysis of this behaviour.\nDuring diffusion sampling, earlier steps contribute more to\nthe semantic layout of the image, while later steps are more\nrelated to high-frequency details [4, 41]. Likewise, lower-\nres UNet representations contribute more to the semantic\nlayout, while higher-res features and skip connections carry\nhigh-frequency content [41, 48]. This can be leveraged to\nperform image editing at a desired level of detail by per-\nforming DDIM inversion [46] and storing feature and atten-\ntion maps to reuse during generation [48]. We extend this by\nfinding that the lower-res representations, which contribute\nmore to the semantic layout, are also more robust to pertur-\nbations. This makes them more amenable to distillation.\nFor our illustrative example, we choose random Gaussian\nnoise to perturb feature maps. In particular, we mix a given\nrepresentation with a random noise sample in a way that\nkeeps activation statistics roughly constant. We assume a\nfeature map to be normal f \u223c N(\u00b5f, \u03c32\nf), and draw a ran-\ndom sample z \u223c N(0, \u03c32\nf). We then update the feature map\nwith:\nf \u2190 \u00b5f + \u221a\u03b1 \u00b7 (f \u2212 \u00b5f) +\n\u221a\n1 \u2212 \u03b1 \u00b7 z\n(1)\nOn average, this will leave the distribution unchanged. We\nset \u03b1 = 0.3 to make the noise the dominant signal.\n2\nLow-res features\nPerturb from step 0\nPerturb from step 1\nPerturb from step 2\nPerturb from step 3\nPerturb from step 4\nPerturb from step 5\nPerturb from step 10\nPerturb from step 15\nMid-res features\nHigh-res features\nFigure 2. Perturbing Stable Diffusion v1.5 UNet representations (outputs of the three upsampling layers), starting from different sampling\nsteps (20 DPM++ steps total, note the reference image as inset in lower-right). Perturbing low-resolution features after only a small number\nof steps has a comparatively small impact on the final output, whereas perturbation of higher-res features results in high-frequency artifacts.\nPrompt: \u201dimage of an astronaut riding a horse on mars.\u201d\nIn Fig. 2 we perform such perturbations on the outputs of\nthe three upsampling layers of the Stable Diffusion v1.5\nUNet [34]. Perturbation starts after a varying number of un-\nperturbed steps and the final output is shown for each case.\nAfter only a small number of steps the lowest-resolution\nfeatures can be perturbed without a noticeable change in\nthe final output, whereas higher-res features are affected for\nlonger along the trajectory. Moreover, early perturbations\nin lower-res layers mostly result in semantic changes, con-\nfirming findings from other works [4, 41]. Implementation\ndetails and additional analyses for other layers are provided\nin Appendix C.\nMotivated by these findings, we propose to approximate\nlower-res UNet representations using more computation-\nally lightweight functions, and in turn reuse information\nfrom previous sampling steps, effectively combining model\nand step distillation. However, we make another crucial\nand non-trivial contribution. Fig. 2 might suggest that one\nshould approximate all representations after a certain sam-\npling step. We instead find that it is beneficial to alternate\napproximation steps and full UNet passes to avoid accumu-\nlating errors. This makes our approach similar to others that\nrun model parts with different temporal granularity [20,40],\nand we consequently name it Clockwork Diffusion.\n4. Clockwork Diffusion\nDiffusion sampling involves iteratively applying a learned\ndenoising function \u03f5\u03b8(\u00b7), or an equivalent reparametriza-\ntion, to denoise a noisy sample xt into a less noisy sam-\nple xt\u22121 at each iteration t, starting from a sample from\nGaussian noise at t = T towards a final generation at\nt = 0 [12,42].\nAs is illustrated in Fig. 3, the noise prediction function \u03f5\n(we omit the parameters \u03b8 for clarity) is most commonly im-\nplemented as a UNet, which can be decomposed into low-\nand high-resolution denoising functions \u03f5L and \u03f5H respec-\ntively. \u03f5H further consists of an input module \u03f5in\nH and an\noutput module \u03f5out\nH , where \u03f5in\nH receives the diffusion latent\nxt and \u03f5out\nH\npredicts the next latent xt\u22121 (usually not di-\nrectly, but by estimating its corresponding noise vector or\ndenoised sample). The low-resolution path \u03f5L receives a\nlower-resolution internal representation rin\nt\nfrom \u03f5in\nH and\npredicts another internal representation rout\nt\nthat is used by\n\u03f5out\nH . We provide a detailed view of the architecture and\nhow to separate it in the Appendix A.\nThe basis of Clockwork Diffusion is the realization that the\noutputs of \u03f5L are relatively robust to perturbations \u2014 as\ndemonstrated in Sec. 3 \u2014 and that it should be possible to\napproximate them with more computationally lightweight\nfunctions if we reuse information from previous sampling\nsteps. The latter part differentiates it from regular model\ndistillation [6, 17]. Overall, there are 4 key contributions\nthat are necessary for optimal performance: a) joint model\nand step distillation, b) efficient adaptor design, c) Clock-\nwork scheduling, and d) training with unrolled sampling\ntrajectories. We describe each below.\n4.1. Model-step distillation\nModel distillation is a well-established concept where a\nsmaller student model is trained to replicate the output of\na larger teacher model, operating on the same input. Step\ndistillation is a common way to speed up sampling for dif-\nfusion models, where a student is trained to replace e.g. two\nteacher model passes. Here the input/output change, but the\nmodel architecture is usually kept the same. We propose to\ncombine the two, replacing part of the diffusion UNet with\na more lightweight adaptor, but in turn giving it access to\noutputs from previous sampling steps (as shown in Fig. 3).\n3\n\ud835\udc65!\"#\n\ud835\udc65!\n\ud835\udc65!$#\nHigh-res in\n\ud835\udf16%\n&'\nHigh-res out\n\ud835\udf16%\n()!\nLow-res\n\ud835\udf16*\n\ud835\udc5f!\"#\n&'\n\ud835\udc5f!\"#\n()!\nHigh-res in\n\ud835\udf16%\n&'\nHigh-res out\n\ud835\udf16%\n()!\nLow-res\n\ud835\udf16*\n\ud835\udc5f!\n&'\n\ud835\udc5f!\n()!\nAdaptor \ud835\udf19\n\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61\n\ud835\udc61\n\ud835\udc65!\n\ud835\udc65\"\n\ud835\udc65#\n\ud835\udc65$\n\ud835\udc65%\n\ud835\udc65&\n\ud835\udc65'\n\ud835\udc65(\n\ud835\udc65)\nforward noise\na)\nregular distillation\nuses images\nb)\ngeneration unroll\ndoes not use images\n\ud835\udc65$\n\ud835\udc65%\n\ud835\udc65&\nConv2d\nConvT2d\n\u2a01\n\u2a01\nResBlock\nResBlock\nLin\n\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61!\"#\n\ud835\udc61!\"#\n//2\nx2\nTRAINING VARIANTS\n\ud835\udf50\nFigure 3. Schematic view of Clockwork. It can be thought of as a combination of model distillation and step distillation. We replace the\nlower-resolution parts of the UNet \u03f5 with a more lightweight adaptor, and at the same time give it access to features from the previous sam-\npling step. Contrary to common step distillation, which constructs latents by forward noising images, we train with sampling trajectories\nunrolled from pure noise. Other modules are conditioned on text and time embeddings (omitted for readability). The gray panel illustrates\nthe difference between regular distillation and our proposed training with unrolled trajectories.\nWe term this procedure model-step distillation.\nIn its simplest form, an adaptor \u03d5\u03b8 is an identity mapping\nthat naively copies a representation rout from step t + 1 to\nt. This works relatively well when the number of sampling\nsteps is high, as for example in our image editing experi-\nments in Sec. 5.3. For a more effective approximation in the\nlow step regime, we rely on a parametric function \u03d5\u03b8 with\nadditional inputs: \u02c6rout\nt\n= \u03d5\u03b8\n\u0000rin\nt , rout\nt+1, temb, textemb\n\u0001\n,\nwhich we describe as follows.\n4.2. Efficient adaptor architecture\nThe design of our adaptor is chosen to minimize heavy com-\npute operations. It uses no attention, and is instead com-\nprised of a strided convolutional layer resulting in two times\nspatial downsampling, followed by addition of a linear pro-\njection of the prompt embedding, two ResNet blocks with\nadditive conditioning on t, and a final transposed convolu-\ntion to go back to the original resolution. We further intro-\nduce a residual connection from input to output. The adap-\ntor architecture is shown in Fig. 3, and we provide more de-\ntails in Appendix A. We ablate several architecture choices\nin Sec. 5.4. The inputs to the adaptor are listed below.\nInput representation rin\nt\nis the representation obtained\nfrom the high-res input module \u03f5in\nH at the current step, as\nshown in Fig. 3. It is concatenated with the next input.\nOutput representation rout\nt+1\nis the equivalent representa-\ntion from the previous sampling step that the adaptor tries\nto approximate for the current step. The high-res output\nmodule predicts the next diffusion latent from it. By condi-\ntioning on rout\nt+1, our approach depends on the sampler and\nstep width (similar to step distillation).\nTime embedding temb\nis an additional input to the adap-\ntor to make it conditional on the diffusion step t, instead\nof training separate adaptor models for each step. For this\npurpose we rely on the standard ResBlocks with time step\nembeddings, as in Rombach et al. [34].\nPrompt embedding textemb\nis an additional input to the\nadaptor to make it conditional on the generation prompt.\nWe rely on the pooled CLIP embedding [32] of the prompt,\nextracted using OpenCLIP\u2019s ViT-g/14 [14], instead of the\nsequence to reduce the complexity.\n4.3. Clockwork scheduling\nInstead of just replacing \u03f5L with an adaptor \u03d5\u03b8 entirely, we\navoid accumulating errors during sampling by alternating\nlightweight adaptor steps with full UNet passes, which is\nthe inspiration for our method\u2019s name, following [20, 40].\nSpecifically, we switch between \u03f5L and \u03d5\u03b8 based on a pre-\ndefined clock schedule C(t) \u2208 {0, 1} as follows:\n\u02c6rout\nt\n=\n(\n\u03f5L\n\u0000rin\nt , temb, textemb\n\u0001\n,\nC(t) = 0\n\u03d5\u03b8\n\u0000rin\nt , rout\nt+1, temb, textemb\n\u0001\n,\nC(t) = 1\nwhere t and c are time step and prompt embeddings, re-\nspectively. C(t) can generally be an arbitrary schedule of\nswitches between \u03f5L and \u03d5\u03b8, but we find that interleaving\nthem at a fixed rate offers a good tradeoff between perfor-\nmance and simplicity. Because we conduct our experiments\nmostly in the low-step regime with \u2264 8 steps, we simply al-\nternate between adaptor and full UNet in consecutive steps\n(i.e. a clock of 2) unless otherwise specified. For sampling\nwith more steps it is possible to use more consecutive adap-\ntor passes, as we show in Appendix D.2 for the text-guided\n4\nimage editing case. For the rest of the paper, we simply use\nthe terminology a clock of N, which means every N steps,\na full UNet pass will be evaluated, all other steps use the\nadaptor.\n4.4. Distillation with unrolled trajectories\nWe seek to train an adaptor that predicts an internal UNet\nrepresentation, based on the same representation from the\nprevious sampling step as well as further inputs. Formally,\nwe minimize the following loss:\nL = E\nt\n\u0002\r\rrout\nt\n\u2212 \u03d5\u03b8\n\u0000rin\nt , rout\nt+1, temb, textemb\n\u0001\r\r\n2\n\u0003\n(2)\nA common choice is to stochastically approximate the ex-\npectation over update steps, i.e. just sample t randomly at\neach training step. Most step distillation approaches [29,37]\nthen construct xt from an image x0 via the diffusion for-\nward process, and perform two UNet passes of a teacher\nmodel to obtain all components required for the loss. In-\nstead of this, we start from a random noise sample and un-\nroll a full sampling trajectory {xT , . . . , x0} with the teacher\nmodel, then use each step as a separate training signal\nfor the adaptor.\nThis is illustrated in Fig. 3.\nWe con-\nstruct a dataset of unrolled sampling trajectories for each\nepoch, which can be efficiently parallelized using larger\nbatch sizes. We compare our unrolled training with the con-\nventional approach in Sec. 5.4.\nOverall training can be done in less than a day on a sin-\ngle NVIDIA\u00ae Tesla\u00ae V100 GPU. As an added benefit,\nthis training scheme does not require access to an image\ndataset and only relies on captions. We provide more details\nin Sec. 5 and include training pseudo-code in Appendix-\nAlgorithm 1.\n5. Experiments\nWe evaluate the effectiveness of Clockwork on two tasks:\ntext-guided image generation in Sec. 5.2 and text-guided\nimage editing in Sec. 5.3. Additionally, we provide several\nablation experiments in Sec. 5.4.\n5.1. Experimental setup\nDatasets and metrics\nWe evaluate our text-guided im-\nage generation experiments by following common prac-\ntices [21, 29, 34] on two public benchmarks: MS-COCO\n2017 (5K captions), and MS-COCO 2014 [22] (30K cap-\ntions) validation sets. We use each caption to generate an\nimage and rely on the CLIP score from a OpenCLIP ViT-\ng/14 model [14] to evaluate the alignment between cap-\ntions and generated images. We also rely on Fr\u00b4echet In-\nception Distance (FID) [11] to estimate perceptual quality.\nFor MS-COCO 2014, the images are resized to 256 \u00d7 256\nbefore computing the FID as in Kim et al. [17].\nWe\nevaluate our text-guided image editing experiments on the\nImageNet-R-TI2I [48] dataset that includes various render-\nings of ImageNet-R [9] object classes. Following [48], we\nuse 3 high-quality images from 10 different classes and 5\nprompt templates to generate 150 image-text pairs for eval-\nuation. In addition to the CLIP score, we measure the DINO\nself-similarity distance as introduced in Splice [47] to mea-\nsure the structural similarity between the source and target\nimages.\nTo measure the computational cost of the different methods,\nwe report the time spent on latent generation, which we call\nlatency for short, as it represents the majority of the total\nprocessing time. This measures the cost spent on UNet for-\nward passes during the generation \u2014 and inversion in case\nof image editing \u2014 but ignores the fixed cost of text en-\ncoding and VAE decoding. Along with latencies we report\nthe number of floating point operations (FLOPs). We mea-\nsure latency using PyTorch\u2019s benchmark utilities on a single\nNVIDIA\u00ae RTX\u00ae 3080 GPU, and use the DeepSpeed [33]\nlibrary to estimate the FLOP count. Finally, to verify the\nefficiency of Clockwork on low-power devices, we mea-\nsure its inference time on a Samsung Galaxy S23 device.\nIt uses a Qualcomm \u201cSnapdragon\u00ae 8 Gen. 2 Mobile Plat-\nform\u201d with a Qualcomm\u00ae HexagonTM processor\nDiffusion models\nWe evaluate the effectiveness of Clock-\nwork on three latent diffusion models with varying compu-\ntational costs: i) SD UNet, the standard UNet from Stable\nDiffusion v1.5 [34]. ii) Efficient UNet, which, inspired by\nLi et al. [21], removes the costly transformer blocks, includ-\ning self-attention and cross-attention operations, from the\nhighest resolution layer of SD UNet. iii) Distilled Efficient\nUNet, which further accelerates Efficient UNet by imple-\nmenting progressive step distillation [37] and classifier-free\nguidance distillation [29]. Since there is no open source\nimplementation [21,29,37] available, we rely on our repli-\ncation as specified in the supplementary materials. In all\nexperiments we use the DPM++ [27] multi-step scheduler\ndue to its superiority in the low number of sampling steps\nregime, which is a key focus of our paper. An exception is\nthe text-guided image editing experiment where we use the\nDDIM scheduler as in Plug-and-Play [48].\nImplementation details\nWe train Clockwork using a\nResNet-based adaptor (as shown in Fig. 3) for a specific\nnumber of generation steps T and with a clock of 2, as\ndescribed in Sec. 4.1, on 50K random captions from the\nLAION-5B dataset [38]. The training involves 120 epochs\nusing the Adam optimizer [19] with a batch size of 16 and\nlearning rate of 0.0001. Thanks to its parameter efficiency\neach training takes less than one day on a single NVIDIA\u00ae\nTesla\u00ae V100 GPU.\n5\n30\n40\nFID [\n]\nSD UNet\nEfficient UNet\nDistilled Efficient UNet\nBaseline\nwith Clockwork\n4\n6\n8\n10\nTFLOPs\n0.26\n0.28\n0.30\nCLIP [\n]\n4\n6\n8\nTFLOPs\n2\n3\n4\nTFLOPs\nFigure 4. Clockwork improves text-to-image generation efficiency\nconsistently over various diffusion models. Models are evaluated\non 512 \u00d7 512 MS-COCO 2017-5K validation set.\n5.2. Text-guided image generation\nWe evaluate the effectiveness of Clockwork in accelerat-\ning text-guided image generation for three different diffu-\nsion models as specified in Sec. 5.1. For each model, we\nmeasure the generation quality and computational cost us-\ning 8, 6 and 4 steps with and without clockwork, as shown\nin Fig. 4. For the baselines (dashed lines) we also include\na point with 3 sampling steps as a reference. Our results\ndemonstrate that applying Clockwork for each model re-\nsults in a high reduction in FLOPs with little changes in\ngeneration qualities (solid lines). For example, at 8 sam-\npling steps, Clockwork reduces the FLOPs of the distilled\nEfficient UNet by 38% from 4.7 TFLOPS to 2.9 TFLOPS\nwith only a minor degradation in CLIP (0.6%) and improve-\nment in FID (5%). Fig. 5 shows generation examples for\nStable Diffusion with and without Clockwork, while Fig. 1\nshows an example for Efficient UNet and its distilled vari-\nant. See Appendix E for more examples.\nOur improvement on the distilled Efficient UNet model\ndemonstrates that Clockwork is complementary to other ac-\nceleration methods and adds savings on top of step distilla-\ntion [37], classifier-free guidance distillation [29], efficient\nbackbones [21] and efficient noise schedulers [27]. More-\nover, Clockwork consistently improves the diffusion effi-\nciency at very low sampling steps, which is the critical op-\nerating point for most time-constrained real-world applica-\ntions, e.g. image generation on phones.\nIn Tab. 1 and Tab. 2 we compare Clockwork to state-of-\nthe-art methods for efficient diffusion on MS-COCO 2017\nand 2014 respectively. The methods include classifier-free\nguidance distillation by Meng et al. [29], SnapFusion [21],\nmodel distillation from BK-SDM [17] and InstaFlow [25].\nFor BK-SDM [17] we use models available in the diffusers\nlibrary [49] for all measurements. For Meng et al. [29],\nSnapFusion [21] and InstaFlow (1 step) [25] we report\nscores from the original papers and implement their archi-\ntecture to measure latency and FLOPS. In terms of quan-\nFigure 5. Text guided generations by SD UNet without (top) and\nwith (bottom) Clockwork at 8 sampling steps (DPM++). Clock-\nwork reduces FLOPs by 32% at a similar generation quality.\nPrompts given in Appendix E.\ntitative performance scores, Clockwork improves FID and\nslightly reduces CLIP on both datasets. Efficient UNet +\nClockwork achieves the best FID out of all methods. In-\nstaFlow has lowest FLOPs and latency as they specifically\noptimize the model for single-step generation, however, in\nterms of FID and CLIP, Clockwork is significantly better.\nCompared to SnapFusion, which is optimized and distilled\nfrom the same Stable Diffusion model, our Distilled Effi-\ncient UNet + Clockwork is significantly more compute effi-\ncient and faster.\n5.3. Text-guided image editing\nWe apply our method to a recent text-guided image-to-\nimage (TI2I) translation method called Plug-and-Play (PnP)\n[48].\nThe method caches convolutional features and at-\ntention maps during source image inversion [46] at certain\nsteps early in the trajectory. These are then injected during\nthe generation using the target prompt at those same steps.\nThis enables semantic meaning of the original image to be\npreserved, while the self-attention keys and queries allow\npreserving the guidance structure.\nPnP, like many image editing works [10, 18, 31], requires\nDDIM inversion [46]. Inversion can quickly become the\ncomplexity bottleneck, as it is often run for many more steps\nthan the generation. For instance, PnP uses 1000 inversion\nsteps and 50 generation steps. We focus on evaluating PnP\n6\nModel\nFID [\u2193]\nCLIP [\u2191]\nTFLOPs\nLatency (GPU)\nLatency (Phone)\nMeng et al. [29]\n26.9\n0.300\n6.4\n320\n-\nSnapFusion [21]\n24.20\n0.300\n4.0\n185\n-\nBK-SDM-Base [17]\n29.26\n0.291\n8.4\n348\n-\nBK-SDM-Small [17]\n29.48\n0.272\n8.2\n336\n-\nBK-SDM-Tiny [17]\n31.48\n0.268\n7.8\n313\n-\nInstaFlow (1 step) [25]\n29.30\n0.283\n0.8\n40\n-\nSD UNet\n24.64\n0.300\n10.8\n454\n3968\n+ Clockwork\n24.11\n0.295\n7.3 (\u221232%)\n341 (\u221225%)\n3176 (\u221220%)\nEfficient UNet\n24.22\n0.302\n9.5\n330\n1960\n+ Clockwork\n23.21\n0.296\n5.9 (\u221238%)\n213 (\u221236%)\n1196 (\u221239%)\nDistilled Efficient UNet\n25.75\n0.297\n4.7\n240\n980\n+ Clockwork\n24.45\n0.295\n2.9 (\u221238%)\n154 (\u221236%)\n598 (\u221239%)\nTable 1. Text guided image generation results on 512 \u00d7 512 MS-\nCOCO 2017-5K validation set. We compare to state-of-the-art\nefficient diffusion models, all at 8 sampling steps (DPM++) except\nwhen specified otherwise. Latency measured in ms.\nand its Clockwork variants on the ImageNet-R-TI2I real\ndataset with SD UNet. Contrary to the rest of the paper,\nwe use the DDIM sampler for these experiments to match\nPnP\u2019s setup. To demonstrate the benefit of Clockwork in\na training-free setting, we use an identity adaptor with a\nclock of 2 both in inversion and generation. We use the offi-\ncial open-source diffusers [49] implementation1 of PnP for\nthese experiments, details in Appendix D.1.\nIn Fig. 6 we show qualitative examples of the same text-\nimage pair with and without Clockwork for different DDIM\ninversion steps and generation fixed to 50 steps. For high\nnumbers of inversion steps, Clockwork leads to little to\nno degradation in quality while consistently reducing la-\ntency by about 25%. At lower numbers of inversions steps,\nwhere less features can be extracted (and hence injected\nat generation), Clockwork outputs start diverging from the\nbaseline\u2019s, yet in semantically meaningful and perceptually\npleasing ways.\nOn the right hand side of Fig. 6, we quantitatively show\nhow, for various number of inversion steps, applying Clock-\nwork enables saving computation cycles while improving\ntext-image similarity and only slightly degrading structural\ndistance. For PnP\u2019s default setting of 1000 inversion steps\nand 50 generation steps (rightmost point on each curve)\nClockwork allows saving 33% of the computational cycles\nwhile significantly improving CLIP score, and only slightly\ndegrading DINO self-similarity.\n5.4. Ablation analysis\nIn this section we inspect different aspects of Clockwork.\nFor all ablations, we follow the same training procedure ex-\nplained in Sec. 5.1 and evaluate on the MS-COCO 2017\ndataset, with a clock of 2 and Efficient Unet as backbone.\nFurther ablations, e.g. results on different solvers, adaptor\ninput variations are shown in Appendix B.\n1https://github.com/MichalGeyer/pnp-diffusers\nModel\nFID [\u2193]\nCLIP [\u2191]\nTFLOPs\nSnapFusion [21]\n14.00\n0.300\n4.0\nBK-SDM-Base [17]\n17.23\n0.287\n8.4\nBK-SDM-Small [17]\n17.72\n0.268\n8.2\nBK-SDM-Tiny [17]\n18.64\n0.265\n7.8\nInstaFlow (1 step) [25]\n20.00\n-\n0.8\nSD UNet\n12.77\n0.296\n10.8\n+ Clockwork\n12.27\n0.291\n7.3 (\u221232%)\nEfficient UNet\n12.33\n0.296\n9.5\n+ Clockwork\n11.14\n0.290\n5.9 (\u221238%)\nDistilled Efficient UNet\n13.92\n0.292\n4.7\n+ Clockwork\n12.37\n0.291\n2.9 (\u221238%)\nTable 2. Text guided image generation results on 256 \u00d7 256 MS-\nCOCO 2014-30K validation set. We compare to state-of-the-art\nefficient diffusion models. Except for InstaFlow [25] all models\nare evaluated at 8 sampling steps using the DPM++ scheduler.\nAdaptor Architecture.\nWe study the effect of different\nparametric functions for the adaptor in terms of perfor-\nmance and complexity. As discussed in Sec. 4.1, \u03d5\u03b8 can be\nas simple as an identity function, where we directly reuse\nlow-res features from the previous time step at the current\nstep. As shown in Tab. 5, Identity function performs reason-\nably well, indicating high correlation in low-level features\nof the UNet across diffusion steps. In addition, we tried\n1) a UNet-like convolutional architecture with two down-\nsampling and upsampling modules, 2) a lighter variant of\nit with 3M parameters and less channels, 3) our proposed\nResNet-like architecture (see Fig. 3). Details for all variants\nare given in Appendix A. From Tab. 5, all adaptors provide\ncomparable performance, however, the ResNet-like adaptor\nobtains better quality-complexity trade-off.\nAdaptor Clock.\nInstead of applying \u03d5\u03b8 in an alternating\nfashion (i.e. a clock of 2), in this ablation we study the ef-\nfect of non-alternating arbitrary clock C(t). For an 8-step\ngeneration, we use 1) C(t) = 1 for t \u2208 {5, 6, 7, 8} and 2)\nC(t) = 1 for t \u2208 {3, 4, 5, 6}, C(t) = 0 otherwise. As shown\nin Tab. 5, both configurations underperform compared to\nthe alternating clock, likely due to error propagation in ap-\nproximation. It is worth noting that approximating earlier\nsteps (config. 2) harms the generation significantly more\nthan later steps (config. 1).\nUNet cut-off.\nWe ablate the splitting point where high-res\nand low-res representations are defined. In particular, we\nset the cut-off at the end of stage 1 or stage 2 of the UNet\n(after first and second downsampling layers, respectively).\nA detailed view of the architecture with splitting points can\nbe found in the supplementary material. The lower the res-\nolution in the UNet we set the cutoff to, the less compute\nwe will save. As shown in Tab. 5, splitting at stage 2 is both\nmore computationally expensive and worse in terms of FID.\nTherefore, we set the cut-off point at stage 1.\n7\n14.4s \n 10.9s\nBaseline\ninv. steps 25\n17.0s \n 12.7s\ninv. steps 50\n22.0s \n 16.2s\ninv. steps 100\n113.4s (PnP's default)\ninv. steps 1000\n0.270\n0.272\n0.274\n0.276\n0.278\n0.280\nCLIP [\n]\nImageNet-R-TI2I real\nWith Clockwork\n-24.3%\n-25.3%\n-26.4%\n\"a toy of a jeep\" (ref)\n200\n400\n600\n800\nTFLOPs\n0.044\n0.046\n0.048\n0.050\nDINO [\n]\nPnP Baseline\nPnP with Clockwork\nFigure 6. Left: text-guided image editing qualitative results comparing the baseline Plug-and-Play to Clockwork with identity adaptor\nwhen using the reference image (bottom right) with the target prompt \u201can embroidery of a minivan\u201d. Across configurations, applying\nClockwork enables matching or outperforming the perceptual quality of the baseline Plug-and-Play while reducing latency by a significant\nmargin. Right: Clockwork improves the efficiency of text-guided image translation on the ImageNet-R-TI2I real dataset. We evaluate both\nthe baseline and its Clockwork variant at different number of DDIM inversion steps: 25, 50, 100, 500 and 1000. The number of DDIM\ngeneration steps is fixed to 50 throughout, except for 25 where we use the same number of generation steps as inversion steps.\nSteps\nFID [\u2193]\nCLIP [\u2191]\nGFLOPs\nEfficient UNet\n8\n24.22\n0.302\n1187\nAdaptor Architecture\nIdentity (0)\n8\n24.36\n0.290\n287\nResNet (14M)\n8\n23.21\n0.296\n301\nUNet (152M)\n8\n23.18\n0.296\n324\nUNet-light (3M)\n8\n23.87\n0.294\n289\nAdaptor Clock\nSteps {2, 4, 6, 8}\n8\n23.21\n0.296\n301\nSteps {5, 6, 7, 8}\n8\n28.07\n0.286\n301\nSteps {3, 4, 5, 6}\n8\n33.10\n0.271\n301\nUNet cut-off\nStage 1 (res 32x32)\n8\n23.21\n0.296\n301\nStage 2 (res 16x16)\n8\n24.49\n0.296\n734\nTable 3. Ablations of Clockwork components. We use 512 \u00d7 512\nMS-COCO 2017-5K, a clock of 2 and Efficient UNet as backbone.\nFLOPs are reported for 1 forward step of UNet with adaptor.\nTraining\nscheme\nand\nrobustness.\nAs\noutlined\nin\nSec. 4.4, the adaptor \u03d5\u03b8 can be trained using 1) the regular\ndistillation setup which employs forward noising of an im-\nage or 2) by unrolling complete sampling trajectories con-\nditioned on a prompt. We compare the two at specific in-\nference steps that use the same clock. Figure 7 shows that\ngeneration unroll performs on par with regular distillation at\nhigher inference steps (6, 8, 16), but performs significantly\nbetter at 4 steps, which is the low compute regime that our\nwork targets.\n4\n6\n8\n10\n12\nTFLOPs\n30\n40\n50\nFID [\n]\nGeneration Unroll\nRegular Distillation\n4\n6\n8\n10\n12\nTFLOPs\n0.225\n0.250\n0.275\n0.300\nCLIP [\n]\nFigure 7. Training scheme ablation. We observe that our training\nwith unrolled trajectories is generally on par with regular distilla-\ntion, but performs significantly better in the low compute regime\n(4 steps). We use 512 \u00d7 512 MS-COCO 2017-5K, a clock of 2\nand Efficient UNet as backbone.\n6. Conclusion\nWe introduce a method for faster sampling with diffusion\nmodels, called Clockwork Diffusion.\nIt combines model\nand step distillation, replacing lower-resolution UNet rep-\nresentations with more lightweight adaptors that reuse in-\nformation from previous sampling steps. In this context, we\nshow how to design an efficient adaptor architecture, and\npresent a sampling scheme that alternates between approxi-\nmated and full UNet passes. We also introduce a new train-\ning scheme that is more robust than regular step distillation\nat very small numbers of steps. It does not require access to\nan image dataset and training can be done in a day on a sin-\ngle GPU. We validate our method on text-to-image gener-\nation and text-conditioned image-to-image translation [48].\nIt can be applied on top of commonly used models like Sta-\nble Diffusion [34], as well as heavily optimized and dis-\n8\ntilled models, and shows consistent savings in FLOPs and\nruntime at comparable FID and CLIP score.\nLimitations.\nLike in step distillation, when learned,\nClockwork is trained for a fixed operating point and does\nnot allow for drastic changes to scheduler or sampling steps\nat a later time. While we find that our unrolled trainings\nworks better than regular distillation at low steps, we have\nnot yet fully understood why that is the case. Finally, we\nhave only demonstrated improvements on UNet-based dif-\nfusion models, and it is unclear how this translates to e.g.\nViT-based implementations.\nReferences\n[1] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap,\nShuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Tal-\nbot, and Eric Gu.\nTract:\nDenoising diffusion models\nwith transitive closure time-distillation.\narXiv preprint\narXiv:2303.04248, 2023. 2\n[2] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang,\nChuo-Ling Chang, Andrei Kulik, and Matthias Grundmann.\nSpeed is all you need: On-device acceleration of large diffu-\nsion models via gpu-aware optimizations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4650\u20134654, 2023. 2\n[3] Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yul-\nhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, and\nHyungjun Kim. Squeezing large-scale diffusion models for\nmobile. arXiv preprint arXiv:2307.01193, 2023. 2\n[4] Kamil Deja, Anna Kuzina, Tomasz Trzci\u00b4nski, and Jakub M.\nTomczak. On analyzing generative and denoising capabili-\nties of diffusion-based deep generative models, 2022. 1, 2,\n3\n[5] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis, 2021. 1\n[6] Tim Dockhorn, Robin Rombach, Andreas Blatmann, and\nYaoliang Yu. Distilling the knowledge in diffusion models.\nIn CVPR Workshop Generative Models for Computer Vision,\n2023. 2, 3\n[7] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. GENIE:\nHigher-Order Denoising Diffusion Solvers. In Advances in\nNeural Information Processing Systems, 2022. 2\n[8] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou,\nand Bohan Zhuang. Ptqd: Accurate post-training quantiza-\ntion for diffusion models. arXiv preprint arXiv:2305.10657,\n2023. 2\n[9] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\nand Justin Gilmer. The many faces of robustness: A criti-\ncal analysis of out-of-distribution generalization. In ICCV,\n2021. 5\n[10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control, 2022. 6\n[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium. In Advances in Neural Information Processing\nSystems, 2017. 5\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models, 2020. 1, 2, 3\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance, 2022. 1\n[14] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. 4, 5\n[15] Alexia Jolicoeur-Martineau, Ke Li, R\u00b4emi Pich\u00b4e-Taillefer,\nTal Kachman, and Ioannis Mitliagkas. Gotta go fast when\ngenerating data with score-based models.\narXiv preprint\narXiv:2105.14080, 2021. 2\n[16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 2\n[17] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and\nShinkook Choi.\nOn architectural compression of text-to-\nimage diffusion models. arXiv preprint arXiv:2305.15798,\n2023. 1, 2, 3, 5, 6, 7\n[18] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, June 2022. 6\n[19] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization, 2017. 5\n[20] Jan Koutn\u00b4\u0131k, Klaus Greff, Faustino Gomez, and J\u00a8urgen\nSchmidhuber. A Clockwork RNN, 2014. 3, 4\n[21] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\n1, 2, 5, 6, 7\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 2, 5\n[23] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao.\nPseudo\nnumerical methods for diffusion models on manifolds. In\nInternational Conference on Learning Representations, Feb\n2022. 2\n[24] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022. 2\n[25] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and\nQiang Liu. Instaflow: One step is enough for high-quality\ndiffusion-based text-to-image generation.\narXiv preprint\narXiv:2309.06380, 2023. 2, 6, 7\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. NeurIPS,\n2022. 1, 2\n9\n[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 1, 2, 5, 6\n[28] Eric Luhman and Troy Luhman. Knowledge distillation in\niterative generative models for improved sampling speed,\n2021. 2\n[29] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, 2023.\n1, 2, 5, 6, 7\n[30] Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, and\nMarkus Nagel. Softmax bias correction for quantized gener-\native models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1453\u20131458, 2023. 2\n[31] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In Special Interest Group on Computer Graphics\nand Interactive Techniques Conference Conference Proceed-\nings, SIGGRAPH \u201923. ACM, July 2023. 6\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 4\n[33] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. Deepspeed: System optimizations enable train-\ning deep learning models with over 100 billion parameters.\nProceedings of the 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining, 2020. 5,\n15\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 2, 3,\n4, 5, 8\n[35] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolu-\ntional networks for biomedical image segmentation. In MIC-\nCAI, 2015. 1\n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding, 2022. 1\n[37] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2021. 1, 2, 5, 6\n[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 5\n[39] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and\nYan Yan. Post-training quantization on diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1972\u20131981, 2023. 2\n[40] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and Trevor\nDarrell. Clockwork convnets for video semantic segmenta-\ntion. In Gang Hua and Herv\u00b4e J\u00b4egou, editors, ECCV Work-\nshops, 2016. 3, 4\n[41] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.\nFreeu:\nFree lunch in diffusion u-net.\narXiv preprint\narXiv:2309.11497, 2023. 1, 2, 3\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In Proceedings of the 32nd\nInternational Conference on Machine Learning, 2015. 3\n[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models, 2020. 1\n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021. 2\n[45] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. 2023. 2\n[46] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, Nov 2020. 2, 6\n[47] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali\nDekel. Splicing vit features for semantic appearance trans-\nfer. In 2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR). IEEE, June 2022. 5\n[48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In CVPR, 2023. 1, 2, 5, 6, 8,\n13, 17, 19, 20\n[49] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 6, 7\n[50] Qinsheng Zhang, Molei Tao, and Yongxin Chen.\ngddim:\nGeneralized denoising diffusion implicit models, 2022. 2\n[51] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu.\nUnipc:\nA unified predictor-corrector frame-\nwork for fast sampling of diffusion models. arXiv preprint\narXiv:2302.04867, 2023. 2\n10\nA. Clockwork details\nUNet Architecture\nIn Fig. 8 we show a detailed\nschematic of the SD UNet architecture. The parts in pink\nare replaced by our lightweight adaptor. We also show pa-\nrameter counts and GMACs per block. In ablations we var-\nied the level at which we introduce the adaptor, as shown\nin Table 3 of the main body. There we compare \u201cStage 1\n(res 32x32)\u201d (our default setup) and \u201cStage 2 (res 16x16)\u201d\n(a variant where DOWN-1 and UP-2 remain in the model),\nfinding better performance for the former. Interestingly, our\nsampling analysis suggested that introducing the adaptor at\nsuch a high resolution, replacing most parts of the UNet,\nshould lead to poor performance. However, this is only true\nif we replace multiple consecutive steps (see adaptor clock\nablations in Table 3 of the main body). By alternating adap-\ntor and full UNet passes we recover much of the perfor-\nmance, and can replace more parts of the UNet than would\notherwise be possible.\nAdaptor Architecture\nIn Fig. 9 we show a schematic of\nour UNet-like adaptor architecture, as discussed in abla-\ntions (Section 5.4 of the main paper). In addition to our\nResNet-like architecture (Fig. 3 of the main paper) We tried\n1) a UNet-like convolutional architecture with 640 chan-\nnels in each block and 4 ResNet blocks in the middle level\n(N = 4), 2) a lighter variant of it with 96 channels and\n2 ResNet blocks in the middle level. While all adaptors\nprovide comparable performance, the ResNet-like adaptor\nobtains better quality-complexity trade-off.\nTraining\nWe provide pseudocode for our unrolled train-\ning in Algorithm 1.\nAlgorithm 1 Adaptor training with unrolled trajectories\nRequire: Teacher model \u03f5\nRequire: Adaptor \u03d5\u03b8\nRequire: Prompt set P\nRequire: Clock schedule C(t)\nfor Ne epochs do\nPD \u2190 RandomSubsetD(P)\n\u25b7 optional\nD \u2190 GenerateTrajectories(PD, \u03f5)\nfor all Trajectory & prompt (T, text) \u2208 D do\nfor all (t, rin\nt , rout\nt\nrout\nt+1) \u2208 T do\nif C(t) = 1 then\n\u02c6rout\nt\n\u2190 \u03d5\u03b8\n\u0000rin\nt , rout\nt+1, temb, textemb\n\u0001\nL \u2190 \u2225rout\nt\n\u2212 \u02c6rout\nt\n\u22252\n\u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207L\nend if\nend for\nend for\nend for\nSteps\nFID\nCLIP\nTFLOPs\nDPM++\n8\n24.22\n0.302\n9.5\n+ Clockwork\n8\n23.21\n0.296\n5.9\nDPM\n8\n24.32\n0.301\n9.5\n+ Clockwork\n8\n23.24\n0.296\n5.9\nPNDM\n8\n35.64\n0.272\n9.5\n+ Clockwork\n8\n33.15\n0.280\n5.9\nDDIM\n8\n34.72\n0.287\n9.5\n+ Clockwork\n8\n38.38\n0.280\n5.9\nTable 4. Clockwork works with different schedulers.\nB. Ablations\nScheduler.\nWe\nevaluate\nClockwork\nacross\nmultiple\nschedulers: DPM++, DPM, PNDM, and DDIM. With the\nexception of DDIM, Clockwork improves FID at negligible\nchange to the CLIP score, while reducing FLOPs by 38%.\nAdaptor inputs.\nWe vary the inputs to the adaptor \u03d5\u03b8. In\nthe simplest version, we only input rin\nt\nand the time em-\nbedding. It leads to a poor FID and CLIP. Using only rout\nt+1\nprovides good performance, indicating the importance of\nusing features from previous steps. Adding rin\nt\nhelps for a\nbetter performance, showcasing the role of the early high-\nres layers of the UNet. Finally, adding the pooled prompt\nembedding textemb doesn\u2019t change FID and CLIP scores.\nModel Distillation\nIn Tab. 5 In previous ablation, we\nused clock of 2. In this ablation, we explore the option to\ndistill the low resolution layers of \u03f5 into the adaptor \u03d5\u03b8, for\nall the steps. Here we train the adaptor in a typical model\ndistillation setting - i.e., the adaptor \u03d5\u03b8 receives as input the\ndownsampled features at current timesteps rin\nt\nalong with\nthe time and text embeddings temb and textemb. It learns to\npredict upsampled features at current timestep rout\nt\n. During\ninference, we use the adaptor during all sampling steps. Re-\nplacing the lower-resolution layers of \u03f5 with a lightweight\nadaptor results in worse performance. It is crucial that the\nadaptor be used with a clock schedule along with input from\na previous upsampled latent.\nTimings for different GPU models\nIn Tab. 6 we report\nlatency of different UNet backbones on different GPU mod-\nels.\nC. Additional perturbation analyses\nIn Section 3 of the main body, we introduced perturbation\nexperiments to demonstrate how lower-resolution features\nin diffusion UNets are more robust to perturbations, and\nthus amenable to distillation with more lightweight compo-\nnents. As a quick reminder, we mix a given representation\nwith a random noise sample by assuming that the feature\n11\nConv2d\nResNet\nBlock\nTransformer\nDown\nSample\nResNet\nBlock\nTransformer\nResNet\nBlock\nResNet\nBlock\nResNet\nBlock\nTransformer\nDown\nSample\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nDown\nSample\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nResNet\nBlock\nResNet\nBlock\nResNet\nBlock\nResNet\nBlock\nUpSample\nResNet\nBlock\nTransformer\nUpSample\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nUpSample\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nConv2d\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nResNet\nBlock\nTransformer\nDOWN-0\n10.52M params (1.22%)\n32.9 GMACs (9.72%)\nDOWN-1\n36.82M params (4.28%)\n31.3 GMACs (9.24%)\nDOWN-2\n140.0M params (16.29%)\n31.5 GMACs (9.29%)\nDOWN-3\n62.28M params (7.25%)\n3.78 GMACs (1.12%)\nUP-0\n162.2M params (18.88%)\n12.9 GMACs (3.81%)\nUP-1\n258.3M params (30.06%)\n75.1 GMACs (22.18%)\nUP-2\n71.41M params (8.31%)\n79.1 GMACs (23.35%)\nUP-3\n18.81M params (2.19%)\n66.0 GMACs (19.48%%)\nMID\n97.04M params (11.29%)\n6.03 GMACs (1.78%)\nTOTAL\n859.52M params\n338.61 GMACs\n320x64x64\n320x32x32\n640x16x16\n1280x8x8\n1280x8x8\n1280x8x8\n1280x16x16\n1280x32x32\n640x64x64\nFigure 8. Detailed view of the SD UNet architecture. We replace the pink/purple parts with a lightweight adaptor, the input to which has 32 \u00d7 32 spatial resolution. For the\nablations in the main body we also tried leaving DOWN-1 and UP-2 in the higher-resolution path, only replacing blocks below.\n12\nFigure 9. Architecture of a variant of the adaptor: UNet and UNet-\nlight. For UNet we set C = 640 and N = 4, while for UNet-light\nwe set C = 96 and N = 2.\nSteps\nFID [\u2193]\nCLIP [\u2191]\nGFLOPs\nDistilled Efficient UNet\n8\n25.75\n0.297\n150\nAdaptor Input\nrIN\nt\n+ temb\n8\n40.73\n0.262\n150\nrOUT\nt+1\n+ temb\n8\n24.76\n0.295\n150\n+ rIN\nt\n8\n24.45\n0.295\n150\n+ textemb\n8\n24.45\n0.295\n150\nModel Distillation\nrIN\nt\n+ temb + textemb\n8\n117.64\n0.06\n150\nTable 5. Ablation of adaptor inputs. We use the MSCOCO-2017\ndataset, Distilled Efficient UNet as backbone and a clock of 2 (ex-\ncept for Model distillation where we use adaptor for all the steps)\n. FLOPs are reported for 1 forward step of UNet with adaptor.\nLatency [ms]\nRTX 3080\nRTX 2080Ti\nV100\nA100\nSD v1.5\n454\n589\n453\n235\n+ Clockwork\n341\n440\n360\n183\n(\u221224.9%)\n(\u221225.3%)\n(\u221220.5%)\n(\u221222.1%)\nEff. UNet\n330\n427\n312\n176\n+ Clockwork\n213\n268\n212\n118\n(\u221235.5%)\n(\u221237.2%)\n(\u221232.1%)\n(\u221233.0%)\nEff. UNet (distilled)\n240\n302\n245\n191\n+ Clockwork\n154\n190\n159\n122\n(\u221235.8%)\n(\u221237.1%)\n(\u221235.1%)\n(\u221236.1%)\nTable 6. Latency improvements [ms] using Clockwork on different\nGPU models. All measurements are averaged over 10 runs, using\nDPM++ with 8 steps and batch size 1 (distilled) or 2 (for classifier-\nfree guidance).\nmap is normal f \u223c N(\u00b5f, \u03c32\nf). We then draw a random\nsample z \u223c N(0, \u03c32\nf) and update the feature map with:\nf \u2190 \u00b5f + \u221a\u03b1 \u00b7 (f \u2212 \u00b5f) +\n\u221a\n1 \u2212 \u03b1 \u00b7 z\n(3)\nFor the example in the main body we set \u03b1 = 0.3, so that\nthe signal is dominated by the noise. Interestingly, we can\nalso fully replace feature maps with noise, i.e. use \u03b1 = 0.0.\nThe result is shown in Fig. 10. Changes are much stronger\nthan before, but lower-resolution perturbations still result\nmostly in semantic changes. However, the output is of lower\nperceptual quality.\nInversion\nSD version\n1.5\nSampler\nDDIM\nInversion prompt\n\u201ca <style> of an <instance>\u201d\nExtract reverse\nFalse\nGeneration\nSD version\n1.5\nSampler\nDDIM\nGuidance scale\n15.0 (for both real and fake images)\nNegative prompt\n\u201cugly, blurry, black, low res, unrealistic\u201d\n\u03c4A\n0.5\n\u03c4f\n0.8\nTable 7. Plug-and-Play hyper-parameters in inversion and gener-\nation. \u03c4A and \u03c4f are expressed as fraction of the sampling trajec-\ntory. For instance, \u03c4f = 0.8 means that for the first 80% steps in\nthe generation, convolutional features will be injected. If one uses\n10 DDIM steps, this means that for the first 8 steps, convolutional\nfeatures will be injected.\nFor the analysis in the main body, as well as Fig. 10, we\nperturb the output of the three upsampling layers in the SD\nUNet. We perform the same analysis for other layers in\nFig. 11. Specifically, there we perturb the output of the bot-\ntleneck layer, the three downsampling layers, and the first\nconvolutional layer of the network (which is also one of\nthe skip connections). Qualitatively, findings remain the\nsame, but perturbation of a given downsampling layer out-\nput leads to more semantic changes (as opposed to artifacts)\ncompared to its upsampling counterpart.\nFinally, we quantify the L2 distance to the unperturbed\noutput as a function of the step where we start perturba-\ntion. Fig. 12 corresponds to the perturbations from the main\nbody, while Fig. 13 shows the same but corresponds to the\ndownsampling perturbations of Fig. 11. Confirming what\nwe saw visually, perturbations to low-resolution layers re-\nsult in smaller changes to the final output than the same\nperturbations to higher-resolution features.\nD. Text-Guided Image Editing\nD.1. Implementation Details\nWe base our implementation of Plug-and-Play (PnP) [48]\noff of the official pnp-diffusers implementation. We sum-\nmarize the different hyper-parameters used to generate the\nresults for both the baseline and Clockworkvariant of PnP in\nTab. 7. Additionally, while conceptually similar we outline\na couple of important differences between what the original\npaper describes and what the code implements. Since we\nuse this code to compute latency and FLOP, we will go over\nthe differences and explain how both are computed. We re-\nfer to Fig. 14 for a visual reference of the implementation\nof the \u201cpnp-diffusers\u201d. For a better understanding, we en-\ncourage the reader to compare it to Fig. 2 from the PnP [48]\npaper.\n13\nLow-res features\nPerturb from step 0\nPerturb from step 1\nPerturb from step 2\nPerturb from step 3\nPerturb from step 4\nPerturb from step 5\nPerturb from step 10\nPerturb from step 15\nMid-res features\nHigh-res features\nFigure 10. Reproduction of Figure 2 from the main body, using \u03b1 = 0.0 (where Figure 2 uses \u03b1 = 0.3). This corresponds to full\nperturbation of the representation, i.e. the representation is completely replaced by noise in each step. Perturbation of low-resolution\nfeatures still mostly results in semantic changes, whereas perturbation of higher-resolution features leads to artifacts.\nBottleneck\nPerturb from step 0\nPerturb from step 1\nPerturb from step 2\nPerturb from step 3\nPerturb from step 4\nPerturb from step 5\nPerturb from step 10\nPerturb from step 15\nDownsample 2\nDownsample 1\nDownsample 0\nIn-convolution\nFigure 11. Reproduction of Figure 2 from the main body, perturbation different layers. Figure 2 perturbs the outputs of the 3 upsampling\nlayers in the SD UNet, here we perturb the outputs of the 3 downsampling layers as well as the bottleneck and the first input convolution.\nQualitative findings remain the same.\n14\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nPerturb from step ...\n0\n25\n50\n75\n100\nL2 Distance to unperturbed output\nLow-res features\nMid-res features\nHigh-res features\nFigure 12. L2 distance to the unperturbed output, when perturbing\nrepresentations with noise (\u03b1 = 0.7), starting after a given number\nof steps. This quantifies what is shown visually in Figure 2 in\nthe main body. Lower-resolution representations are much more\nrobust to perturbations, and converge to the unperturbed output\nfaster.\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nPerturb from step ...\n0\n25\n50\n75\n100\nL2 Distance to unperturbed output\nBottleneck\nDownsample 2\nDownsample 1\nDownsample 0\nIn-convolution\nFigure 13. L2 distance to the unperturbed output, when perturbing\nrepresentations with noise (\u03b1 = 0.7), starting after a given num-\nber of steps. This quantifies what is shown visually in Figure 11.\nLower-resolution representations are much more robust to pertur-\nbations, and converge to the unperturbed output faster.\nWhen are features cached?\nThe paper describes that the\nsource image is first inverted, and only then features are\ncached during DDIM sampling. They are only cached at\nsampling step t falling within the injection schedule, which\nis defined by the two hyper parameters \u03c4f and \u03c4A which\ncorresponds to the sampling steps until which feature and\nself-attention will be injected respectively. The code, in-\nstead of caching features during DDIM generation at time\nsteps corresponding to injection schedule, caches during all\nDDIM inversion steps. This in theory could avoid running\nDDIM sampling using the source or no prompt. However\nas we will see in the next paragraph, since the features are\nnot directly cached but the latents are, we end up spending\nthe compute on DDIM sampling anyway.\nWhat is cached?\nThe paper describes the caching of spa-\ntial features from decoder layers f 4\nt along with their self-\nattention Al\nt, where 4 and l indicate layer indices. The im-\nplementation trades off memory for compute by caching the\nlatent xt instead, and recomputes the activations on the fly\nby stacking the cached latent along the batch axis along with\nan empty prompt. The code does not optimize this opera-\ntion and stacks such latent irrespective of whether it will be\ninjected, which results in a batch size of 3 throughout the\nwhole sampling trajectory: (1) unconditional cached latent\nforward (2) latent conditioned on target prompt and (3) la-\ntent conditioned on negative prompt. This has implications\non the latency and complexity of the solution, and we re-\nflected it on the FLOP count, we show the formula we used\nin Eq. (5).\nOf note, this implementation which caches latents instead\nof features has a specific advantage for Clockwork, as it\nenables mismatching inversion and generation steps and\nclock. During inversion, when the latents are cached, it does\nnot matter whether it is obtained using a full UNet pass or an\nadaptor pass. During generation, when the features should\nbe injected, the cached latent is simply ran through the UNet\nto obtain the features on-the-fly. This is illustrated in Fig. 14\nwhere features are injected at step t + 1 during the gener-\nation although the adaptor was used at the corresponding\nstep during inversion.\nHow do we compute FLOP for PnP?\nTo compute FLOP,\nwe need to understand what data is passed through which\nnetwork during inversion and generation. Summarizing pre-\nvious paragraphs, we know that:\n\u2022 inversion is ran with a batch size of 1 with the source\nprompt only.\n\u2022 generation is ran with a batch size of 3. The first el-\nement in the batch corresponds to the cached latent\nand the empty prompt. The other two corresponds to\nthe typical classifier-free guidance UNet calls using the\ntarget prompt and negative prompt.\n\u2022 both during inversion and generation, if the adaptor is\nused, only the higher-res part of the original UNet will\nbe run, \u03f5H.\nLet us denote N and C the number of steps and the clock,\nthe indices I and G standing for inversion and generation\nrespectively. We first count the number of full UNet pass\nin each, using integer division N full\nI\n= NI div CI (we\nfollow similar logic for N full\nG\n. Additionally, we use FLOP\nestimate for a single forward pass with batch size of 1 in\nUNet, F\u03f5 = 677.8 GFLOPs, and UNet with identity adap-\ntor, F\u03f5H+\u03d5 = F\u03f5H = 228.4 GFLOPs. The estimates are\nobtained using the DeepSpeed library [33]. Finally, we ob-\ntain the FLOP count F as follows:\n15\nGENERATION\nINVERSION\nsource text \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61!\nsource image \ud835\udc3c!\n\u201ca toy of a jeep\u201d\nsource inverted \nlatent \ud835\udc65\"\n!\n\ud835\udf16!\n\ud835\udf19\n\ud835\udc65#\n\ud835\udc65$\n\ud835\udc65#%&\nempty text \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61'('\n\u201c\u201d\ntarget text \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61\"\n\u201ca cartoon of a jeep\u201d\n\ud835\udc65#\n\ud835\udc65#%&\n\ud835\udc65#\n\ud835\udc65#%&\n\ud835\udc65$\nfeature / self-attention \ninjection\ncached latent injection\ntarget image \ud835\udc3c\"\nFigure 14. Overview of the actual diffusers implementation of Plug-and-Play, which contrary to what the paper describes caches latent\nduring inversion, not intermediate features during generation. The features to be injected are re-computed from the cached latents on-the-\nfly during DDIM generation sampling. The red arrows indicate injection, the floppy disk icon indicate that only the latent gets cached /\nsaved to disk. Inversion and generation are ran separately, all operations within each are ran in-memory.\nFI = N full\nI\n\u00b7 F\u03f5 + (NI \u2212 N full\nI\n) \u00b7 F\u03f5H\n(4)\nFG = 3 \u00b7\n\u0010\nN full\nG\n\u00b7 F\u03f5 + (NG \u2212 N full\nG\n) \u00b7 F\u03f5H\n\u0011\n(5)\nF = FI + FG\n(6)\nHow do we compute latency for PnP?\nAs described\nin Section 5, we only compute latency of the inversion\nand generation loops using PyTorch\u2019s benchmark utilities.\nIn particular, we exclude from latency computation any\n\u201cfixed\u201d cost like VAE decoding and text encoding.\nAd-\nditionally, similar to the FLOP computation, we did not\nperform optimization over the official PnP implementation,\nwhich leads to a batch size of 1 in the inversion loop, and a\nbatch size of 3 in the generation loop.\nInterplay between injection and adaptor.\nThe adaptor\nreplaces the lower resolution part of the UNet \u03f5L. Based\non where we split the UNet between low- and high-res, it\nturns out all layers which undergo injection are skipped if\nadaptor \u03d5 is ran instead of \u03f5L. Hence, when adaptor is ran\nduring generation it means no features are being injected.\nAs the number of inversion and generation steps decrease,\nthe effect of skipping injection are more and more visible, in\nparticular structural guidance degrades. One could look into\ncaching and injecting adaptor features to avoid losing struc-\ntural guidance. Note however that this would have no effect\non complexity, and might only affect PnP + Clockworkper-\nformance in terms of CLIP and DINO scores at lower num-\nber of steps. Since optimizing PnP\u2019s performance at very\nlow steps was not a focus of the paper, we did not pursue\nthis thread of work.\nPossible optimizations.\nThe careful reader might under-\nstand that there are low hanging fruits in terms of both la-\ntency and FLOP optimizations for PnP. First, if memory\nwould allow, one could cache the actual activations instead\nof the latent during inversion, which would allow not re-\nrunning the latent through the UNet at generation time. Sec-\n16\nond, it would be simple to modify the generation loop code\nnot to stack the cached latent when t does not fall within the\ninjection schedule. If implemented, a substantial amount\nof FLOP and latency could be saved on the generation, as\nthe default PnP hyper parameters \u03c4f and \u03c4A lead to injec-\ntion in only the first 80% of the sampling trajectory. Note\nhowever that both of these optimizations are orthogonal to\nClockwork, and would benefit both the baseline and Clock-\nworkimplementations of PnP, which is why we did not im-\nplement them.\nD.2. Additional Quantitative Results\nWe provide additional quantitative results for PnP and its\nClockworkvariants.\nIn particular, we provide CLIP and\nDINO scores at different clocks and with a learned ResNet\nadaptor. In addition to the ImageNet-R-TI2I real dataset\nresults, we report scores on ImageNet-R-TI2I fake [48].\nIn the Fig. 15, we can see how larger clock size of 4 enables\nbigger FLOP savings compared to 2, yet degrade perfor-\nmance at very low number of steps, where both CLIP and\nDINO scores underperform at 10 inversion and generation\nsteps. It is interesting to see that the learned ResNet adap-\ntor does not outperform nor match the baseline, which is\nline with our ablation study which shows that Clockwork-\nworks best for all schedulers but DDIM at very low number\nof steps, see Tab. 4.\nWe can see that results transfer well across datasets, where\nabsolute numbers change when going from ImageNet-R-\nTI2I real (top row) to fake (bottom row) but the relative\ndifference between methods stay the same.\nD.3. Additional Qualitative Results\nWe provide additional qualitative examples for PnP for\nImageNet-R-TI2I real in Fig. 16 and Fig. 17. We show ex-\namples at 50 DDIM inversion and generation steps.\nE. Additional examples\nWe provide additional example generations in this section.\nExamples for SD UNet are given in Fig. 18, examples for\nEfficient UNet in Fig. 19, and those for the distilled Effi-\ncient UNet in Fig. 20. In each case the top panel shows the\nreference without Clockwork and the bottom panel shows\ngenerations with Clockwork. Fig. 18 includes the same ex-\namples already shown in the main body so that the layout is\nthe same as for the other models for easier comparison.\nThe prompts that were used for the generations are the fol-\nlowing (left to right, top to bottom), all taken from the MS-\nCOCO 2017 validation set:\n\u2022 \u201ca large white bear standing near a rock.\u201d\n\u2022 \u201ca kitten laying over a keyboard on a laptop.\u201d\n\u2022 \u201cthe vegetables are cooking in the skillet on the stove.\u201d\n\u2022 \u201ca bright kitchen with tulips on the table and plants by\nthe window \u201d\n\u2022 \u201ccars waiting at a red traffic light with a dome shaped\nbuilding in the distance.\u201d\n\u2022 \u201ca big, open room with large windows and wooden\nfloors.\u201d\n\u2022 \u201ca grey cat standing in a window with grey lining.\u201d\n\u2022 \u201cred clouds as sun sets over the ocean\u201d\n\u2022 \u201ca picnic table with pizza on two trays \u201d\n\u2022 \u201ca couple of sandwich slices with lettuce sitting next\nto condiments.\u201d\n\u2022 \u201ca piece of pizza sits next to beer in a bottle and glass.\n\u201d\n\u2022 \u201cthe bust of a man\u2019s head is next to a vase of flowers.\u201d\n\u2022 \u201ca view of a bathroom that needs to be fixed up.\u201d\n\u2022 \u201ca picture of some type of park with benches and no\npeople around.\u201d\n\u2022 \u201ctwo containers containing quiche, a salad, apples and\na banana on the side.\u201d\n17\n0.24\n0.25\n0.26\n0.27\n0.28\nImageNet-R-TI2I real\nText-Image Similarity CLIP [\n]\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\nStructure Distance DINO [\n]\nPnP Baseline\nPnP with Clockwork (identity clock of 2)\nPnP with Clockwork (identity clock of 4)\nPnP with Clockwork (learned clock of 2)\n0\n200\n400\n600\n800\nTFLOPs\n0.24\n0.25\n0.26\n0.27\n0.28\nImageNet-R-TI2I fake\n0\n200\n400\n600\n800\nTFLOPs\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\nFigure 15. Additional quantitative results on ImageNet-R-TI2I real (top) and fake (bottom) for varying number of DDIM inversion steps:\n[10, 20, 25, 50, 100, 200, 500, 1000]. We use 50 generation steps except for inversion steps below 50 where we use the same number for\ninversion and generation.\n18\nReference\na tattoo of a lion\na graffiti of a jeep\nan embroidery of a bear\na sculpture of a castle\nan image of a violin\nBaseline\nWith Clockwork\nReference\na sketch of a pizza\nan embroidery of a bustard\na graphic of a eel\nan origami of a tiger\na tattoo of a penguin\nBaseline\nWith Clockwork\nFigure 16. Examples from ImageNet-R-TI2I real from Plug-and-Play [48] and its Clockworkvariant. We use 50 DDIM inversion and\ngeneration steps, and a clock of 2. Images synthesized with Clockworkare generated 34% faster than the baseline, while being perceptually\nclose if at all distinguishable from baseline.\n19\nReference\na sketch of a jeep\na painting of a tiger\na painting of a penguin\nan art of a castle\nan image of a bobcat\nBaseline\nWith Clockwork\nReference\na photo of a cat\nan embroidery of a hummingbird\na tattoo of a violin\nan embroidery of a goldfish\nan art of a baloon\nBaseline\nWith Clockwork\nFigure 17. Examples from ImageNet-R-TI2I fake from Plug-and-Play [48] and its Clockworkvariant. We use 50 DDIM inversion and\ngeneration steps, and a clock of 2. Images synthesized with Clockworkare generated 34% faster than the baseline, while being perceptually\nclose if at all distinguishable from baseline.\n20\nFigure 18. Additional example generations for SD UNet without (top) and with (bottom) Clockwork. We include the examples shown in\nthe main body so that the layout of this figure matches that of Fig. 19 and Fig. 20.\n21\nFigure 19. Example generations for Efficient UNet without (top) and with (bottom) Clockwork.\n22\nFigure 20. Example generations for Distilled Efficient UNet without (top) and with (bottom) Clockwork.\n23\n"
  },
  {
    "title": "Invariant Graph Transformer",
    "link": "https://arxiv.org/pdf/2312.07859.pdf",
    "upvote": "5",
    "text": "Invariant Graph Transformer\nZhe Xu 1 Menghai Pan 2 Yuzhong Chen 2 Huiyuan Chen 2 Yuchen Yan 1 Mahashweta Das 2 Hanghang Tong 1\nAbstract\nRationale discovery is defined as finding a sub-\nset of the input data that maximally supports the\nprediction of downstream tasks. In graph ma-\nchine learning context, graph rationale is defined\nto locate the critical subgraph in the given graph\ntopology, which fundamentally determines the\nprediction results. In contrast to the rationale\nsubgraph, the remaining subgraph is named the\nenvironment subgraph. Graph rationalization can\nenhance the model performance as the mapping\nbetween the graph rationale and prediction label\nis viewed as invariant, by assumption. To ensure\nthe discriminative power of the extracted ratio-\nnale subgraphs, a key technique named interven-\ntion is applied. The core idea of intervention is\nthat given any changing environment subgraphs,\nthe semantics from the rationale subgraph is in-\nvariant, which guarantees the correct prediction\nresult. However, most, if not all, of the exist-\ning rationalization works on graph data develop\ntheir intervention strategies on the graph level,\nwhich is coarse-grained. In this paper, we pro-\npose well-tailored intervention strategies on graph\ndata. Our idea is driven by the development of\nTransformer models, whose self-attention module\nprovides rich interactions between input nodes.\nBased on the self-attention module, our proposed\ninvariant graph Transformer (IGT) can achieve\nfine-grained, more specifically, node-level and vir-\ntual node-level intervention. Our comprehensive\nexperiments involve 7 real-world datasets, and\nthe proposed IGT shows significant performance\nadvantages compared to 13 baseline methods.\n1University of Illinois Urbana-Champaign, Urbana, IL, USA\n2Visa Research, Palo Alto, CA, USA. Correspondence to: Zhe Xu\n<zhexu3@illinois.edu>.\nPreliminary work. Under review by the International Conference\non Machine Learning (ICML). Do not distribute.\nFigure 1: Illustration of the rationale/environment decom-\nposition and intervention. Round nodes denote graph ra-\ntionales and square nodes (with stripe) denote the environ-\nments. The intervention is the key to ensure the rationale\nfrom graph G truly has the discriminative power for the label\nyG.\n1. Introduction\nRationale refers to a subset of the input features that\nplay a crucial role in model predictions for downstream\ntasks (Chang et al., 2020; Liu et al., 2022; Wu et al., 2022a;\nZhang et al., 2022). In the context of graph machine learn-\ning, graph rationale is defined as a subgraph of the input\ngraph containing the most task-relevant semantics.\nThe application of graph rationale is broad, for example,\nit can greatly enhance model performance for graph-level\ntasks (Wu et al., 2022a) by identifying the key components\nof the input graph. Additionally, the discovery of rationales\ncan improve model explainability (Liu et al., 2022), as it\nhighlights the parts of the input graph that significantly\ncontribute to the final prediction.\nExisting graph rationalization solutions (Liu et al., 2022;\nWu et al., 2022a) employ a trainable augmenter to execute\nthe rationale/environment decomposition. In this process, a\nnode/edge mask is generated by the augmenter to decom-\npose the given graph into a rationale graph and an environ-\nment graph. Inspired by the content-style decomposition\nconcept (Kaddour et al., 2022), the key idea behind graph\nrationalization is to preserve the utility of the graph ratio-\nnale even when faced with changing environment graphs\n(see Figure 1). To achieve this, a technique named interven-\ntion is used, where the environment graph interacts with the\nrationale graph.\nThe intervention mechanism (named intervener) is essential\n1\narXiv:2312.07859v2  [cs.LG]  15 Dec 2023\nInvariant Graph Transformer\nin the graph rationalization process, as it must accurately\nrepresent the interaction between the rationale and the en-\nvironment. Intuitively, the intervener should work in an\nadversarial behavior against the aforementioned augmenter,\na point not emphasized in the existing literature. If the\nintervener is more powerful, it can capture more detailed in-\nteractions between the rationale and environment subgraphs.\nGiven such a powerful intervener, the augmenter is com-\npelled to minimize these interactions between the graph\nrationale and the environment, so as to obtain a \u201cpurer\u201d\ngraph rationale.\nUnfortunately, existing works develop interveners in a\ncoarse and non-parametric manner. After performing ra-\ntionale/environment decomposition on the graph data, they\ncompute graph-level embeddings for the rationale and en-\nvironment subgraphs. The intervention is then designed as\nan interaction between these graph-level embeddings. For\nexample, (Liu et al., 2022) adds the environment embedding\ninto the rationale embedding as the intervened rationale em-\nbedding; (Wu et al., 2022a) define the intervened prediction\nas the Hadamard product between the predictions based on\nthe rationale subgraph and the environment subgraph. We\nargue that such a graph-level non-parametric intervention is\ninsufficient to effectively represent the interaction between\nthe rationale and environment graphs.\nIn response to this limitation, we propose a fine-grained,\nparametric intervention mechanism named Invariant Graph\nTransformer (IGT). Our proposed IGT draws inspiration\nfrom the self-attention module in the Transformer model,\nwhich captures interactions between input tokens. Building\nupon insights from Transformer (Vaswani et al., 2017) and\nits linear variant Linformer (Wang et al., 2020), IGT formu-\nlates the interaction between the rationale and environment\nsubgraphs at the node-level or the virtual node-level. The\ntwo variants are named IGT-N and IGT-VN, respectively.\nAdditionally, to maximize the effectiveness of the interven-\ntion, we formulate a min-max game involving the node\nencoder, augmenter, intervener, and predictor, compelling\nthe rationale subgraph to be as informative as possible.\nTo evaluate the proposed approach, we conduct comprehen-\nsive experiments on 7 graph-level benchmarks and compare\nIGT-N/VN against 13 state-of-the-art baseline methods. The\nresults demonstrate that our proposed IGT and its variants\noutperform the baseline methods, validating their superior\nperformance.\nOur primary contributions in this paper can be summarized\nas follows:\n\u2022 We address the graph rationalization problem by intro-\nducing a fine-grained model IGT, which works at the\nnode/virtual node level.\n\u2022 A min-max objective function is proposed so that the\neffectiveness of the newly-proposed intervener can be\nmaximized.\n\u2022 Extensive experiments covering 13 baseline methods\nand 7 real-world datasets are presented to verify the\nefficacy of the proposed method IGT.\n2. Preliminaries\n2.1. Notations\nWe adopt the following notation conventions: bold upper-\ncase letters for matrices and tensors (e.g., A), bold low-\nercase letters for column vectors (e.g., u), lowercase and\nuppercase letters in regular font for scalars (e.g., d, K),\nand calligraphic letters for sets (e.g., T ). To index vec-\ntors/matrices/tensors, we follow the syntax from NumPy1\n(0-based). Specifically, A[p, :] and A[:, q] represent the p-th\nrow and the q-th column of matrix A respectively; A[p, q]\nrepresents the entry at the p-th row and the q-th column.\nSimilarly, u[p] denotes the p-th entry of vector u. In addi-\ntion, the slicing syntax for vectors/matrices/tensors is also\nused. For example, for a matrix A, A[i : j, :] denotes rows\nfrom the i-th row (included) to the j-th row (excluded) and\nA[:, : k] denotes all the columns before the k-th column.\nThe superscript \u22a4 denotes the transpose of matrices and\nvectors. The symbol \u2299 represents the Hadamard product\nand \u25e6 denotes function composition. We use || to represent\nthe concatenation operation and the specific dimension for\nconcatenation will be clarified based on the context.\nAn attributed graph can be represented as G = (A, X, E),\nwhere A \u2208 Rn\u00d7n is the adjacency matrix, X \u2208 Rn\u00d7dX\nis the node feature matrix, and E \u2208 Rn\u00d7n\u00d7dE is the edge\nfeature tensor. Here, n denotes the number of nodes, and\ndX/dE represents the dimensions of node/edge features,\nrespectively. In this paper, we assume the node and edge\nfeature dimensions are the same (i.e., dX = dE = d) for\nbrevity; if they are different, a simple fully-connected layer\ncan map them into a common feature space. Our main focus\nin this paper is on graph property prediction tasks. The\nground truth of a graph is represented by y.\n2.2. Graph Transformer\nThe core modules of the Transformer architecture (Vaswani\net al., 2017) are the self-attention layer and the feed-forward\nnetwork layer. Given the input as a sequence of symbol\nrepresentations H \u2208 Rn\u00d7dH, it is first transformed into the\nquery, key, and value matrices as\nQ = HWQ, K = HWK, V = HWV ,\n(1)\nwhere WQ \u2208 RdH\u00d7dQ, WK \u2208 RdH\u00d7dK, WV \u2208 RdH\u00d7dV .\nFor the brevity of the presentation, we set dH = dQ =\n1https://numpy.org/doc/stable/index.html\n2\nInvariant Graph Transformer\ndK = dV = d. Then, the self-attention module works as,\nP = Attn(H) = \u03c3(QK\u22a4\n\u221a\nd\n),\n(2a)\nH \u2190 PV + H.\n(2b)\nTypically, the non-linearity \u03c3 is set as softmax.\nThe feed-forward network (FFN) module updates the sym-\nbol representation matrix H as follows:\nH \u2190 FFN(H) + H.\n(3)\nAdditional techniques such as layer/batch normalization (Ba\net al., 2016; Ioffe & Szegedy, 2015), dropout (Srivastava\net al., 2014), and multi-head attention (Vaswani et al., 2017)\ncan be included, but omitted here for brevity.\nWhile the Transformer was originally devised for sequence\nor set data with positional encoding, numerous techniques\nhave since been introduced to adapt Transformers for graph\ndata. Based on the taxonomy outlined by (Min et al., 2022),\nmost graph Transformers are designed from the perspectives\nof (1) incorporating the topology encoding into the node\nfeatures, (2) incorporating the topology encoding into the at-\ntention matrix, and (3) utilizing graph neural networks (Wu\net al., 2021) as auxiliary modules.\nIn fact, it is well-known in both the graph learning (Chen\net al., 2022) and natural language processing communi-\nties (Zaheer et al., 2020; Wang et al., 2020) that, from the\nmessage-passing perspective, the key idea of the Trans-\nformer architecture is to reconstruct a weighted complete\npropagation graph, whose parameterized adjacency matrix\nis given by P = \u03c3\n\u0010\nQK\u22a4\n\u221a\nd\n\u0011\n.\n2.3. Invariant Rationale Discovery on Graphs\nThe graph rationale is a subgraph encoding most down-\nstream task-relevant semantics. A typical example is the\nfunctional groups in polymer graphs (Liu et al., 2022; Wu\net al., 2022a), which fundamentally determines the chem-\nical property of polymers. Mathematically, a given graph\nis decomposed into a rationale graph and an environment\ngraph: G = Gra \u222a Genv. Commonly, the graph embed-\ndings on Gra and Genv are computed as hra and henv.\nTo ensure the rationale graph is invariant w.r.t. the pre-\ndiction results when confronting different environments,\na utility loss is minimized given the rationale embedding\nhra intervened by the environment embedding \u02dchenv, i.e.,\nmin Lutil(hra\nintervene\n\u2190\u2212\u2212\u2212\u2212\u2212 \u02dchenv). Here, \u02dchenv could either\noriginate from the same graph (i.e., \u02dchenv = henv), or could\nbe environment embeddings from other graphs, such as\nthose in the batch. A key difference among existing meth-\nods lies in the intervention operation, of which we mention\ntwo:\n\u2022 GREA (Liu et al., 2022) designs the intervention as\nadding operation, i.e., hra + \u02dchenv;\n\u2022 DIR (Wu et al., 2022a) designs the intervention as an\nelement-wisely weighting of the prediction vector, i.e.,\n\u03b8pred(hra)\u2299Sigmoid(\u03b8pred(\u02dchenv)), where \u2299 is the\nHadamard product and \u03b8pred is a predictor.\nIn Figure 2a an overview of the GREA (Liu et al., 2022) is\npresented.\n3. Invariant Graph Transformer\nIn this section, we introduce our innovative graph rational-\nization method, IGT. At its core, IGT utilizes a module\nbased on the Transformer architecture. Figure 2b provides\nan overview of IGT, highlighting its four main parametric\nmodules: the encoder, augmenter, intervener, and predictor.\nEncoder. The encoder, denoted as \u03b8enc : G \u2192 Rn\u00d7d, ac-\ncepts a graph data as input and produces a node embedding\nmatrix as output. While there are various graph encoders\navailable, such as graph neural networks (GNNs) (Wu et al.,\n2021) and graph Transformers (Min et al., 2022). From\nthe methodology perspective, the encoder module is not the\nmain contribution of this paper, so in this section, we do not\nspecify a specific graph encoder \u03b8enc.\nPredictor.\nThe predictor, denoted as \u03b8pred : Rd \u2192 Rc\ntakes as input a graph embedding and outputs a task-related\nvector/scalar. For graph regression tasks, c = 1, and for\ngraph classification tasks, c is the number of classes. A\ntypical choice of predictor is a multi-layer perceptron (MLP)\nwith appropriate activation functions.\nFurther details of encoder and predictor in our implementa-\ntion are presented in Section 4.\nIn subsequent subsections, we will elaborate on the aug-\nmenter and intervener, two essential modules. Their detailed\ndesigns derive the two variants of the proposed IGT model.\n3.1. Node-Level Variant: IGT-N\n3.1.1. NODE-LEVEL AUGMENTER.\nThe augmenter is a critical module of the proposed IGT.\nFor the node-level variant, termed IGT-N, the augmenter\u2019s\nprimary function is the decomposition of the node set into\ntwo distinct subsets: rationale nodes and environment nodes.\nThis decomposition is operated by parameterizing the node-\nlevel augmenter as a learnable node partitioner, denoted by\n\u03b8aug-N,\nm = sigmoid(MLP(H, \u03b8aug-N)),\n(4)\nwhose input is the node embedding matrix H \u2208 Rn\u00d7d,\nand its output is a partition vector m \u2208 [0, 1]n. MLP is a\nmulti-layer perceptron. Each entry within m, such as m[i],\n3\nInvariant Graph Transformer\n(a) GREA\n(b) IGT (Ours)\nFigure 2: Pipeline comparison between existing work GREA and proposed IGT. \u25e6 denotes function composition. GREA\ndesigns the intervention at the graph level and the proposed IGT designs the intervention at the node/virtual node level. The\naugmented environment \u02dcHenv is from other graphs (through the Encoder and Augmenter) in the batch.\nsignifies the probability of the i-th node being categorized\nas a rationale node.\nFor the node partition vector m, its top-K entries are in-\ndexed as idxra = argtopK(m) which is used to in-\ndex the rationale nodes from the node embedding ma-\ntrix H; naturally, the remaining nodes are categorized\nas the environment nodes whose indices are idxenv =\n{1, . . . , n}\u2212idxra where \u2212 is the set difference operation.\nK is a hyper-parameter whose impact is studied in Section F\n(Appendix).\nUsing the aforementioned indices, rationale and environ-\nment embeddings, denoted as Hra and Henv, respectively,\ncan be extracted from the node embedding matrix H:\nHra = H[idxra, :] \u2208 RK\u00d7d,\n(5a)\nHenv = H[idxenv, :] \u2208 R(n\u2212K)\u00d7d,\n(5b)\n3.1.2. NODE-LEVEL INTERVENER.\nThe design of the fine-grained intervener module draws in-\nspiration from the Transformer architecture (Vaswani et al.,\n2017). Explicitly, the node-level intervener \u03d5 is articulated\nas,\nHinter, P = Transformer(Hra||Henv),\n(6a)\nwhere\nP = Attn(Hra||Henv).\n(6b)\nIn this representation, the operator || concatenates along\nthe first dimension of the matrices Hra and Henv. We\ndub the Eq. (1) to (3) as Transformer and P is the in-\ntermediate attention matrix from the self-attention layer\n(Eq. (6b)). Here, the self-attention module models the inter-\nactions between the rational nodes Hra and the environment\nnodes Henv. \u03d5 includes all the parameters of the Attn\n(Eq. (6b)) and FFN (Eq. (3)) modules. In some context\nwhere the attention matrix P is not explicitly used as an\noutput, input/output of the intervener \u03d5 can be presented as\nHinter = \u03d5(Hra||Henv).\n3.1.3. IGT-N OPTIMIZATION OBJECTIVE.\nThe utility loss is computed as Lutil(Hra||Henv) =\nLtask(\u03b8pred \u25e6Readout\u25e6\u03d5(Hra||Henv), y), where Ltask\nis the task-specific objective. For instance, it could be the\nmean squared error for regression tasks or the cross-entropy\nfor classification tasks. As introduced in Figure 1, the core\nof the invariant rationale discovery is to find the graph ra-\ntionale so that the utility loss attains minimization given\nchanging environments. Thus, the total utility objective is\nLutil = Lutil(Hra||Henv) + \u03b1Lutil(Hra|| \u02dcHenv), (7)\nwhere \u02dcHenv is the node embeddings from the changing\nenvironments. In practical implementations, \u02dcHenv is the\nenvironment node embeddings from other graphs in the\nmini-batch. Additionally, to fully utilize the rich interactions\nfrom the fine-grained intervention module, we apply the\nfollowing partition regularization term,\nLreg(Hra||Henv) = s\u22a4P(1 \u2212 s) + (1 \u2212 s)\u22a4Ps,\n(8)\n4\nInvariant Graph Transformer\nwhere P \u2208 Rn\u00d7n is the self-attention matrix from Eq. (6b),\ns[i] =\n(\n1\nif i < K.\n0\notherwise.\n(9)\n0-based indexing is used so there are in total K non-zero\nentries (i.e., 1) in s. The meaning of the binary s vec-\ntor is to designate whether a particular row of the matrix\nHra||Henv originates from the rationale nodes or the envi-\nronment nodes. The underlying notion of the regularization\nterm Eq. (8) is to impose penalties on interactions between\nthe rationale nodes and the environment nodes. Namely,\nthese two terms s\u22a4P(1 \u2212 s) and (1 \u2212 s)\u22a4Ps denote the to-\ntal weights on the links (i.e., cut) between the rationale and\nenvironment subgraphs. To handle the changing environ-\nments, we introduce an additional regularization term on the\nchanging environments as Lreg(Hra|| \u02dcHenv) where \u02dcHenv\nis the environment node embeddings from another graph\nwithin the same mini-batch. Then, the total regularization\nterm is\nLreg = Lreg(Hra||Henv) + Lreg(Hra|| \u02dcHenv),\n(10)\nand the total objective function is Lutil + \u03b2Lreg.\nTo\nfully harness the capabilities of the fine-grained paramet-\nric intervener, it is crucial to note\u2014as highlighted in the\nintroduction\u2014that the behavior of the intervener \u03d5 oper-\nates in an adversarial fashion to the other modules. As a\nresult, we formulate a min-max game that involves \u03b8 =\n{\u03b8enc, \u03b8aug-N, \u03b8pred} and \u03d5 as,\nmin\n\u03b8\nmax\n\u03d5\nLutil + \u03b2Lreg.\n(11)\nHere, the intervener \u03d5 is trained to decrease the utility of\nthe graph rationale by promoting interactions between the\nrationale nodes and the environment nodes. Conversely, the\nencoder, augmenter, and predictor (i.e., \u03b8) are optimized in\nan opposing manner to the intervener\u2019s objectives.\n3.1.4. COMPLEXITY OF IGT-N\nAs the encoder \u03b8enc and the predictor \u03b8pred are off-the-shelf,\nthe IGT-N introduces two new modules: the node-level aug-\nmenter, \u03b8aug-N, and the Transformer-based intervener, \u03d5. It\nis worth noting that, despite these additions, the increase\nof number of parameters remains modest. The parameters\nfor \u03b8aug-N originate from the MLP defined in Eq. (4). In\na configuration where the MLP has 3 layers with a feature\ndimension of d, the parameter count is O(2d2). The inter-\nvener \u03d5, driven by the Transformer layer in Eq. (6a),\nhas its parameters confined to O(3d2 + 2d2) = O(5d2),\nowing to its query, key, value projection matrices and the\nfeed-forward net (FFN from Eq. (3), typically a 3-layered\nMLP).\nA step-by-step algorithm for IGT-N is provided in Algo-\nrithm 1 (Section A, Appendix). In test phase, the output of\n\u03b8pred \u25e6 Readout \u25e6 \u03d5(Hra||Henv) is evaluated.\n3.2. Virtual Node-Level Variant: IGT-VN\n3.2.1. VIRTUAL NODE-LEVEL AUGMENTER.\nIn the previously introduced IGT-N, its augmenter decom-\nposes the nodes into rationale nodes and environment nodes\nvia a trainable node partitioner \u03b8aug-N. In this section, we\nextend this idea to extract the graph rationale at the vir-\ntual node level. Our idea is partly inspired by the speedup\ntechnique from Linformer (Wang et al., 2020) which refor-\nmulates both the attention matrix and node (token) embed-\nding matrix to dimensions of Rn\u00d7r and Rr\u00d7d, respectively.\nThis reformulation ensures that their multiplication scales\nlinearly with the number of nodes (tokens) n. Within this\nconfiguration, r, a pre-defined rank, is significantly smaller\nthan n, and d represents the feature dimension. Drawing\nfrom the Linformer technique, we propose that the restruc-\ntured token embedding matrix, with dimensions of Rr\u00d7d,\ncan be interpreted as embeddings for r virtual nodes.\nBuilding upon this insight, given node embeddings H origi-\nnating from the encoder, the virtual node embeddings can\nbe presented as:\nHVN = softmax(WN-VN)H.\n(12)\nHere, the row-wise applied softmax function, along with\nsoftmax(WN-VN) \u2208 Rr\u00d7n, yields a trainable matrix as-\nsigning n nodes to r virtual nodes, where r acts as a tunable\nhyper-parameter. In experiments we set r = 8. As all the\nvirtual node embeddings are learned, a subset of the r virtual\nnodes can be designated as rationale virtual nodes, whose\nrationality is data-driven by the intervention procedure dis-\ncussed in subsequent subsections. For brevity, the initial K\nvirtual nodes are deemed as rationale virtual nodes, while\nthe last r \u2212 K nodes is considered the environment virtual\nnode. Similar to the IGT-N, here K is a hyperparameter\nwhose impact is studied in Section F. Thus, rationale and\nenvironment embeddings are presented as:\nHra = HVN[: K, :] \u2208 RK\u00d7d,\n(13a)\nHenv = HVN[K :, :] \u2208 R(r\u2212K)\u00d7d.\n(13b)\nThe parameter of \u03b8aug-VN is WN-VN.\n3.2.2. VIRTUAL NODE-LEVEL INTERVENER.\nThis section discusses the design of a virtual node-level\nintervener, which parallels the framework presented in Sec-\ntion 3.1. The salient difference lies in that the interven-\ntion here functions on the virtual nodes, rather than the\ngiven real nodes. Building upon our previous steps, we ob-\ntain the rationale virtual node embeddings, Hra \u2208 RK\u00d7d,\nand the environment node embedding, Henv \u2208 R(r\u2212K)\u00d7d.\nThanks to the property of the Transformer that it can pro-\ncess sets with variable size, the design of the virtual node-\nlevel intervener \u03d5 is similar to the node-level intervener\n5\nInvariant Graph Transformer\nas Hinter, P = Transformer(Hra||Henv) or short as\nHinter = \u03d5(Hra||Henv) if the attention matrix P is not\nused. Notably, for IGT-VN, P \u2208 Rr\u00d7r describes the inter-\naction among the r virtual nodes.\n3.2.3. IGT-VN OPTIMIZATION OBJECTIVE.\nThe output of \u03b8pred \u25e6 Readout \u25e6 \u03d5(\u00b7) is used for min-\nimizing the utility loss Lutil = Lutil(Hra||Henv) +\n\u03b1Lutil(Hra|| \u02dcHenv),\nwhere\nLutil(Hra||Henv)\n=\nLtask(\u03b8pred\n\u25e6\nReadout\n\u25e6\n\u03d5(Hra||Henv), y)\nand\nLutil(Hra|| \u02dcHenv) is defined similarly.\nFor modeling\nthe changing environment,\n\u02dcHenv is the virtual node\nembeddings from other graphs in the mini-batch.\nAd-\nditionally, the previously proposed regularization term\nEq, (8) can be extended to the virtual node-level variant:\nLreg(Hra||Henv) = s\u22a4P(1 \u2212 s) + (1 \u2212 s)\u22a4Ps. The total\nregularization term, considering the changing environment\n\u02dcHenv, is Lreg = Lreg(Hra||Henv) + Lreg(Hra|| \u02dcHenv).\nAs the P depicts interactions among virtual nodes, we\nconstruct the rationale/environment indicator vector s analo-\ngously to Eq. (9). Put everything together, and the optimiza-\ntion objective of IGT-VN is min\u03b8 max\u03d5 Lutil + \u03b2Lreg,\nwhere \u03b8 = {\u03b8enc, \u03b8aug-VN, \u03b8pred}.\n3.2.4. COMPLEXITY OF IGT-VN\nAs we mentioned the encoder \u03b8enc and the predictor \u03b8pred\nare off-the-shelf. Thus, the extra modules introduced by\nthe IGT-VN are the virtual node-level augmenter \u03b8aug-VN\nand the Transformer-based intervener \u03d5. The parameters\nfor \u03b8aug-VN originate from the matrix WN-VN, as defined\nin Eq. (12). The number of these parameters is of the or-\nder O(nr), where n denotes the number of nodes. For\npractical implementation purposes, n is pre-set; it is set to\n10\u00d7 average size of graphs from the dataset and we trun-\ncate the input graphs if its size is larger than 10\u00d7 average\nsize. For the intervener \u03d5, its parameters originate from\nthe Transformer layer, outlined in Eq. (6a). The num-\nber of parameters here is O(5d2), owing to its query, key,\nvalue projection matrices and the feed-forward net (Eq. (3),\ntypically a 3-layered MLP).\nA step-by-step algorithm for IGT-VN is provided in Algo-\nrithm 2 (Section A, Appendix). In test phase, the output of\n\u03b8pred \u25e6 Readout \u25e6 \u03d5(Hra||Henv) is evaluated.\n4. Experiments\nIn this section, we begin by detailing the dataset and baseline\nmethods. Subsequent subsections evaluate the comparative\neffectiveness of our approach, supplemented by an ablation\nstudy and visualization of the attention matrix. An efficiency\nstudy, a convergence analysis, and a sensitivity study can be\nfound in Appendix, Section D, E, and F, respectively.\n4.1. Datasets\nIn this paper, we select 7 publicly-available real-world\ndatasets: (1) graph classification datasets molhiv (Hu et al.,\n2020), moltox21 (Hu et al., 2020), molbace (Hu et al., 2020),\nmolbbbp (Hu et al., 2020) and (2) graph regression datasets\nZINC (Dwivedi et al., 2023), AQSOL (Dwivedi et al., 2023),\nand mollipo (Hu et al., 2020). We strictly follow the evalu-\nation metrics and dataset split recommended by the given\nbenchmarks. To be concrete, area under the ROC curve\n(ROC-AUC) is the metric for datasets molhiv, moltox21,\nmolbace, molbbbp; root-mean-square deviation (RMSE) is\nthe metric for dataset mollipo; mean absolute error (MAE)\nis the metric of datasets ZINC and AQSOL. The details\nstatistics of the datasets are given in Table 3 (Appendix).\nWe report the average result with standard deviation in 10\nruns.\n4.2. Baseline Methods\nWe selected (1) 4 graph neural network baselines: GIN (Xu\net al., 2019), GAT (Velickovic et al., 2018), GATv2 (Brody\net al., 2022), and GatedGCN (Bresson & Laurent, 2017) (2)\n7 graph Transformers: GT (Dwivedi & Bresson, 2020),\nGraphiT (Mialon et al., 2021), SAN (Kreuzer et al.,\n2021), SAT (Chen et al., 2022), Graphormer (Ying et al.,\n2021), GraphTrans (Wu et al., 2022b), GPS (Ramp\u00b4asek\net al., 2022), and (3) 2 graph rationale discovery methods:\nDIR (Wu et al., 2022a) and GREA (Liu et al., 2022).\n4.3. Effectiveness Study\nThe effectiveness comparison between the proposed IGT-\nN, IGT-VN, and baseline methods are provided in Table 1.\nTo ensure a fair comparison, certain pre-trained models,\nsuch as the pre-trained Graphormer (Ying et al., 2021), are\nnot included. Also, as DIR (Wu et al., 2022a) is designed\nto conduct interventions on the label prediction vectors, it\ncannot be naively applied to graph regression tasks. Notably,\nour proposed IGT-N and IGT-VN consistently outperform,\nor are at least on par with, all the baseline methods on both\nthe graph classification and graph regression datasets.\n4.4. Ablation Study\nWe conducted an ablation study on the proposed models,\nIGT-N and IGT-VN. We selected two ablated variants as\nbaselines: (1) \u03b8enc\u25e6\u03b8pred which is a pure composition of the\nencoder \u03b8enc and the predictor \u03b8pred without any rationale\ndiscovery module; and (2) IGT-N w/o reg and IGT-VN w/o\nreg which remove the regularization term (Eq. (10)) from\nthe objective function. Our results highlight that both the\nTransformer-based intervener and the regularization term\n(Eq. (10)) substantially enhance the performance of IGT-N\nand IGT-VN , as shown in Table 2.\n6\nInvariant Graph Transformer\nTable 1: Effectiveness comparison (mean\u00b1std) with baseline methods. (\u2193) denotes the lower the better and (\u2191) denotes the\nhigher the better. Statistics in grey are reported in the original papers. The best is bold and the second best is underlined.\nN/A means the method cannot work on regression tasks.\nTask\nGraph Regression\nGraph Classification\nDataset\nZINC\nAQSOL\nmollipo\nmolhiv\nmoltox21\nmolbace\nmolbbbp\nMetric\nMAE(\u2193)\nMAE(\u2193)\nRMSE(\u2193)\nROC-AUC(\u2191)\nROC-AUC(\u2191)\nROC-AUC(\u2191)\nROC-AUC(\u2191)\nGIN\n0.350\u00b10.008\n1.237\u00b10.011\n0.783\u00b10.017\n77.1\u00b11.5\n75.6\u00b10.9\n80.7\u00b11.2\n69.5\u00b11.0\nGAT\n0.723\u00b10.010\n1.638\u00b10.048\n0.923\u00b10.011\n75.0\u00b10.5\n72.2\u00b10.6\n75.3\u00b10.8\n67.1\u00b10.6\nGATv2\n0.729\u00b10.015\n1.722\u00b10.022\n0.943\u00b10.021\n72.2\u00b10.5\n73.6\u00b10.2\n76.8\u00b11.6\n65.7\u00b10.7\nGatedGCN\n0.579\u00b10.023\n1.533\u00b10.035\n0.819\u00b10.033\n74.8\u00b11.6\n75.0\u00b10.8\n81.2\u00b11.2\n68.3\u00b10.9\nGT\n0.226\u00b10.014\n1.319\u00b10.026\n0.882\u00b10.020\n73.5\u00b10.4\n75.0\u00b10.6\n77.1\u00b12.3\n65.0\u00b11.1\nGraphiT\n0.202\u00b10.011\n1.162\u00b10.005\n0.846\u00b10.023\n74.6\u00b11.0\n71.8\u00b11.3\n73.4\u00b13.6\n64.6\u00b10.5\nSAN\n0.139\u00b10.006\n1.199\u00b10.218\n0.816\u00b10.112\n77.9\u00b10.2\n71.3\u00b10.8\n79.0\u00b13.1\n63.8\u00b10.9\nSAT\n0.094\u00b10.008\n1.236\u00b10.023\n0.835\u00b10.008\n78.8\u00b10.6\n75.6\u00b10.7\n83.6\u00b12.1\n69.6\u00b11.3\nGraphormer\n0.122\u00b10.006\n1.265\u00b10.025\n0.911\u00b10.015\n79.3\u00b10.4\n77.3\u00b10.8\n79.3\u00b13.0\n67.7\u00b10.9\nGraphTrans\n0.192\u00b10.011\n1.233\u00b10.052\n0.915\u00b10.032\n78.1\u00b10.5\n76.4\u00b10.8\n78.0\u00b11.8\n70.5\u00b10.9\nGPS\n0.070\u00b10.004\n1.032\u00b10.007\n0.780\u00b10.021\n78.8\u00b11.0\n75.7\u00b10.4\n79.6\u00b11.4\n69.6\u00b11.1\nDIR\nN/A\nN/A\nN/A\n77.1\u00b10.6\n73.1\u00b10.2\n74.8\u00b10.3\n70.5\u00b11.4\nGREA\n0.227\u00b10.020\n1.177\u00b10.019\n0.769\u00b10.025\n79.3\u00b10.9\n78.2\u00b10.9\n82.4\u00b12.4\n69.9\u00b11.8\nIGT-N\n0.095\u00b10.008\n0.990\u00b10.012\n0.708\u00b10.013\n80.1\u00b10.7\n78.8\u00b10.5\n85.3\u00b12.0\n73.8\u00b10.7\nIGT-VN\n0.086\u00b10.012\n1.011\u00b10.009\n0.706\u00b10.009\n80.2\u00b11.0\n78.2\u00b10.6\n84.5\u00b11.3\n73.1\u00b10.8\nTable 2: Ablation study (mean\u00b1std) of the proposed model\nIGT. (\u2193) denotes the lower the better and (\u2191) denotes the\nhigher the better.\nDataset\nmollipo\nmolbace\nmolbbbp\nMetric\nRMSE(\u2193)\nROC-AUC(\u2191)\nROC-AUC(\u2191)\n\u03b8enc \u25e6 \u03b8pred\n0.780\u00b10.021\n79.6\u00b11.4\n70.5\u00b10.9\nIGT-N w/o reg\n0.736\u00b10.022\n84.3\u00b10.7\n72.3\u00b11.0\nIGT-VN w/o reg\n0.758\u00b10.018\n83.2\u00b11.5\n71.9\u00b10.7\nIGT-N w/ reg\n0.708\u00b10.013\n85.3\u00b12.0\n73.8\u00b10.7\nIGT-VN w/ reg\n0.706\u00b10.009\n84.5\u00b11.3\n73.1\u00b10.8\n4.5. Attention Visualization\nIn this section, we aim to evaluate the significance of the\nregularization term by visualizing the adjacency matrix P,\neffectively the attention matrix, of the intervener \u03d5 in Fig-\nure 3. For clarity in visualization, we choose IGT-VN. Un-\nlike IGT-N, which works on a variable number of nodes\n(from different graphs), IGT-VN maps nodes to a predeter-\nmined number of virtual nodes, simplifying the presentation.\nSpecifically, we set the number of virtual nodes r to 16 with\nK at 10, designating 10 virtual nodes to rationales and the\nremainder as environments. All the visualization results are\nobtained from the molbace dataset. It is worth noting that\nthe attention matrix P is normalized row-wise by softmax.\nFrom our observations, we highlight two primary insights:\n\u2022 Interestingly, even in the absence of the regulariza-\ntion term, in Figure 3(a), interactions between ratio-\nnales and environments appear significantly weaker\nthan those within the rationales themselves. One po-\ntential explanation is the changing nature of the en-\nvironment. In optimizing the utility loss Lutil =\nLtask(Hra||Henv) + \u03b1Ltask(Hra|| \u02dcHenv), the ever-\nchanging environment ( \u02dcHenv) might lead the model\nto minimize interactions between rationales and en-\nvironments so that the utility of the rationale can be\nensured.\n\u2022 The first observation supports our decision to intro-\nduce the regularization term, which aims to penal-\nize rationale-environment interactions. When the pro-\nposed regularization term (Eq. (8)) is implemented, in\nFigure 3(b), there is a noticeable decline in rationale-\nenvironment interactions (the off-diagonal blocks in\nFigure 3). As demonstrated in our earlier ablation\nstudy, this leads to the enhanced model performance.\n5. Related Work\n5.1. Invariant Learning on Graphs\nInvariant feature learning, which has seen extensive appli-\ncations in many fields such as computer vision (Kaddour\net al., 2022; Arjovsky et al., 2019; Ganin & Lempitsky,\n2015; Chang et al., 2020), is gaining more attention in the\ncommunity of graph machine learning. OOD-GNN (Li\net al., 2023), for instance, applies random Fourier features\n7\nInvariant Graph Transformer\nFigure 3: Heatmap of the adjacency matrix of the intervener\n\u03d5. (a) without the regularization term Eq. (8) and (b) with\nthe regularization term Eq. (8).\nto eliminate the statistical dependence between relevant and\nirrelevant graph representations. (Bevilacqua et al., 2021)\nstudies the graph invariant representation learning with a\nparticular focus on the size discrepancies between the train-\ning/test graphs. DIR (Wu et al., 2022a) decomposes the\ngiven graph into a rationale subgraph and an environment\nsubgraph; after that, it uses a graph classifier to conduct\nprediction on the above two graphs respectively and its inter-\nvention is via the Hadamard product between the prediction\nvectors. Similarly, GREA (Liu et al., 2022) conducts the\nrationale/environment decomposition at the node level and\nits intervention operation is to directly add the environment\ngraph embedding into the rationale graph embedding. In a\nsimilar vein, GIL (Li et al., 2022b) decomposes the given\ngraph into the invariant and variant subgraphs; based on that,\nit infers the environment label, as input of the invariance\nregularization term (Koyama & Yamaguchi, 2020). Further-\nmore, invariant learning can also benefit graph contrastive\nlearning (Li et al., 2022c) to enhance the data augmentation\nprocess. Using invariant learning to address the OOD gen-\neralization challenges on graph (Li et al., 2022a; Gui et al.,\n2022) is promising, but our paper does not concentrate on\nOOD generalization settings, and we leave it as future work.\n5.2. Transformer on Graphs\nThe Transformer architecture (Vaswani et al., 2017) has\nachieved significant success in various domains, including\nnatural language processing (Vaswani et al., 2017), com-\nputer vision (Han et al., 2023), and more (Xu et al., 2022). In\nrecent years, there has been a surge of interest in enhancing\ngraph machine learning methods by leveraging the capabili-\nties of Transformers. For example, GraphTrans (Wu et al.,\n2022b) concatenates the Transformer architecture after var-\nious message-passing neural networks; GPS (Ramp\u00b4asek\net al., 2022) proposes a powerful layer which operates both\na graph convolution layer and a Transformer layer in paral-\nlel; GT (Dwivedi & Bresson, 2020) generalizes the attention\nmodule on graphs via concatenating the spectral vectors\nwith the raw node features and computing the attention\nweights on all the existing edges; Graphormer (Ying et al.,\n2021) encodes both the node centrality and node-pair short-\nest path into the Transformer architecture. A comprehensive\nsurvey about graph Transformer, from the architecture per-\nspective, can be found in (Min et al., 2022).\n6. Conclusion\nIn this paper, we study the invariant rationale discovery\nproblem on graph data. Distinct from existing methods,\nour proposed solutions (IGT-N and IGT-VN) are designed\nat more fine-grained levels, specifically at the node and\nvirtual node levels, so that they can better model the inter-\nactions between the rationale and environment subgraphs.\nMore importantly, we formulate the intervener and the other\nmodel modules in a min-max game, which can significantly\nimprove the quality of the extracted graph rationales. Com-\nprehensive experiments on 7 real-world datasets illustrate\nthe effectiveness of the proposed method compared to 13\nbaseline methods. our evaluation of the efficiency of the\nproposed augmenter and intervener shows that they will not\nconsiderably impact the overall training efficiency.\n7. Limitation and Future Work\nThe main limitation of the proposed IGT is the escalated\nnumber of parameters, which is inevitable, due to the pa-\nrameterizing of the intervener. The computational graph\nexpands since the intervener is appended to the existing\nencoder and predictor modules. For auto-gradient tools,\nsuch as PyTorch, this results in an increase of the training\nduration. Our future work includes employing the proposed\nIGT for OOD generalization and model explainability.\nReferences\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-\nPaz,\nD.\nInvariant risk minimization.\nCoRR,\nabs/1907.02893, 2019. URL http://arxiv.org/\nabs/1907.02893.\nBa, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016. URL http://arxiv.\norg/abs/1607.06450.\nBevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant\ngraph representations for graph classification extrapola-\ntions. In Meila, M. and Zhang, T. (eds.), Proceedings of\nthe 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139\nof Proceedings of Machine Learning Research, pp. 837\u2013\n851. PMLR, 2021.\nURL http://proceedings.\nmlr.press/v139/bevilacqua21a.html.\nBresson, X. and Laurent, T. Residual gated graph convnets.\n8\nInvariant Graph Transformer\nCoRR, abs/1711.07553, 2017. URL http://arxiv.\norg/abs/1711.07553.\nBrody, S., Alon, U., and Yahav, E.\nHow attentive are\ngraph attention networks?\nIn The Tenth Interna-\ntional Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\n2022. URL https://openreview.net/forum?\nid=F72ximsx7C1.\nChang, S., Zhang, Y., Yu, M., and Jaakkola, T. S. Invariant\nrationalization. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pp. 1448\u20131458. PMLR,\n2020. URL http://proceedings.mlr.press/\nv119/chang20c.html.\nChen, D., O\u2019Bray, L., and Borgwardt, K. M. Structure-\naware transformer for graph representation learning. In\nChaudhuri, K., Jegelka, S., Song, L., Szepesv\u00b4ari, C., Niu,\nG., and Sabato, S. (eds.), International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Balti-\nmore, Maryland, USA, volume 162 of Proceedings of\nMachine Learning Research, pp. 3469\u20133489. PMLR,\n2022. URL https://proceedings.mlr.press/\nv162/chen22r.html.\nDwivedi, V. P. and Bresson, X. A generalization of trans-\nformer networks to graphs. CoRR, abs/2012.09699, 2020.\nURL https://arxiv.org/abs/2012.09699.\nDwivedi, V. P., Joshi, C. K., Luu, A. T., Laurent, T., Bengio,\nY., and Bresson, X. Benchmarking graph neural networks.\nJ. Mach. Learn. Res., 24:43:1\u201343:48, 2023. URL http:\n//jmlr.org/papers/v24/22-0567.html.\nGanin, Y. and Lempitsky, V. S. Unsupervised domain adap-\ntation by backpropagation.\nIn Bach, F. R. and Blei,\nD. M. (eds.), Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, volume 37 of JMLR Workshop\nand Conference Proceedings, pp. 1180\u20131189. JMLR.org,\n2015. URL http://proceedings.mlr.press/\nv37/ganin15.html.\nGui,\nS.,\nLi,\nX.,\nWang,\nL.,\nand Ji,\nS.\nGOOD:\nA\ngraph\nout-of-distribution\nbenchmark.\nIn\nNeurIPS, 2022.\nURL http://papers.nips.\ncc/paper_files/paper/2022/hash/\n0dc91de822b71c66a7f54fa121d8cbb9-Abstract-Datasets_\nand_Benchmarks.html.\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z.,\nTang, Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y.,\nand Tao, D. A survey on vision transformer. IEEE Trans.\nPattern Anal. Mach. Intell., 45(1):87\u2013110, 2023. doi:\n10.1109/TPAMI.2022.3152247. URL https://doi.\norg/10.1109/TPAMI.2022.3152247.\nHu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu,\nB., Catasta, M., and Leskovec, J. Open graph bench-\nmark: Datasets for machine learning on graphs. CoRR,\nabs/2005.00687, 2020. URL https://arxiv.org/\nabs/2005.00687.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Bach, F. R. and Blei, D. M. (eds.), Proceedings of\nthe 32nd International Conference on Machine Learning,\nICML 2015, Lille, France, 6-11 July 2015, volume 37 of\nJMLR Workshop and Conference Proceedings, pp. 448\u2013\n456. JMLR.org, 2015. URL http://proceedings.\nmlr.press/v37/ioffe15.html.\nKaddour, J., Lynch, A., Liu, Q., Kusner, M. J., and Silva,\nR. Causal machine learning: A survey and open prob-\nlems. CoRR, abs/2206.15475, 2022. doi: 10.48550/arXiv.\n2206.15475. URL https://doi.org/10.48550/\narXiv.2206.15475.\nKoyama, M. and Yamaguchi, S. Out-of-distribution gen-\neralization with maximal invariant predictor.\nCoRR,\nabs/2008.01883, 2020. URL https://arxiv.org/\nabs/2008.01883.\nKreuzer, D., Beaini, D., Hamilton, W. L., L\u00b4etourneau, V.,\nand Tossou, P. Rethinking graph transformers with spec-\ntral attention. In Ranzato, M., Beygelzimer, A., Dauphin,\nY. N., Liang, P., and Vaughan, J. W. (eds.), Advances in\nNeural Information Processing Systems 34: Annual Con-\nference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pp. 21618\u2013\n21629,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\nb4fd1d2cb085390fbbadae65e07876a7-Abstract.\nhtml.\nLi, H., Wang, X., Zhang, Z., and Zhu, W.\nOut-of-\ndistribution generalization on graphs: A survey. CoRR,\nabs/2202.07987, 2022a. URL https://arxiv.org/\nabs/2202.07987.\nLi, H., Zhang, Z., Wang, X., and Zhu, W. Learning invariant\ngraph representations for out-of-distribution general-\nization. In NeurIPS, 2022b. URL http://papers.\nnips.cc/paper_files/paper/2022/hash/\n4d4e0ab9d8ff180bf5b95c258842d16e-Abstract-Conferen\nhtml.\nLi, H., Wang, X., Zhang, Z., and Zhu, W. OOD-GNN: out-\nof-distribution generalized graph neural network. IEEE\nTrans. Knowl. Data Eng., 35(7):7328\u20137340, 2023. doi:\n10.1109/TKDE.2022.3193725. URL https://doi.\norg/10.1109/TKDE.2022.3193725.\n9\nInvariant Graph Transformer\nLi, S., Wang, X., Zhang, A., Wu, Y., He, X., and Chua,\nT. Let invariant rationale discovery inspire graph con-\ntrastive learning. In Chaudhuri, K., Jegelka, S., Song,\nL., Szepesv\u00b4ari, C., Niu, G., and Sabato, S. (eds.), In-\nternational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA, vol-\nume 162 of Proceedings of Machine Learning Research,\npp. 13052\u201313065. PMLR, 2022c.\nURL https://\nproceedings.mlr.press/v162/li22v.html.\nLiu, G., Zhao, T., Xu, J., Luo, T., and Jiang, M. Graph\nrationalization with environment-based augmentations.\nIn Zhang, A. and Rangwala, H. (eds.), KDD \u201922: The\n28th ACM SIGKDD Conference on Knowledge Discov-\nery and Data Mining, Washington, DC, USA, August\n14 - 18, 2022, pp. 1069\u20131078. ACM, 2022. doi: 10.\n1145/3534678.3539347. URL https://doi.org/\n10.1145/3534678.3539347.\nMialon, G., Chen, D., Selosse, M., and Mairal, J. Graphit:\nEncoding graph structure in transformers.\nCoRR,\nabs/2106.05667, 2021. URL https://arxiv.org/\nabs/2106.05667.\nMin, E., Chen, R., Bian, Y., Xu, T., Zhao, K., Huang,\nW., Zhao, P., Huang, J., Ananiadou, S., and Rong, Y.\nTransformer for graphs: An overview from architec-\nture perspective. CoRR, abs/2202.08455, 2022. URL\nhttps://arxiv.org/abs/2202.08455.\nRamp\u00b4asek,\nL.,\nGalkin,\nM.,\nDwivedi,\nV. P.,\nLuu,\nA. T., Wolf, G., and Beaini, D.\nRecipe for a\ngeneral,\npowerful,\nscalable\ngraph\ntransformer.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.\nnips.cc/paper_files/paper/2022/hash/\n5d4834a159f1547b267a05a4e2b7cf5e-Abstract-Conference.\nhtml.\nSrivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever,\nI., and Salakhutdinov, R. Dropout: a simple way to pre-\nvent neural networks from overfitting. J. Mach. Learn.\nRes., 15(1):1929\u20131958, 2014. doi: 10.5555/2627435.\n2670313.\nURL https://dl.acm.org/doi/10.\n5555/2627435.2670313.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.\nAttention is all you need. In Guyon, I., von Luxburg, U.,\nBengio, S., Wallach, H. M., Fergus, R., Vishwanathan,\nS. V. N., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pp. 5998\u2013\n6008,\n2017.\nURL\nhttps://proceedings.\nneurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.\nhtml.\nVelickovic, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y. Graph attention networks. In 6th\nInternational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings. OpenReview.net,\n2018. URL https://openreview.net/forum?\nid=rJXMpikCZ.\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.\nLinformer: Self-attention with linear complexity. CoRR,\nabs/2006.04768, 2020. URL https://arxiv.org/\nabs/2006.04768.\nWu, Y., Wang, X., Zhang, A., He, X., and Chua, T.\nDiscovering invariant rationales for graph neural net-\nworks. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net, 2022a.\nURL https:\n//openreview.net/forum?id=hGXij5rfiHw.\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S.\nA comprehensive survey on graph neural networks. IEEE\nTrans. Neural Networks Learn. Syst., 32(1):4\u201324, 2021.\ndoi: 10.1109/TNNLS.2020.2978386. URL https://\ndoi.org/10.1109/TNNLS.2020.2978386.\nWu, Z., Jain, P., Wright, M. A., Mirhoseini, A., Gonzalez,\nJ. E., and Stoica, I. Representing long-range context\nfor graph neural networks with global attention. CoRR,\nabs/2201.08821, 2022b. URL https://arxiv.org/\nabs/2201.08821.\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. How pow-\nerful are graph neural networks? In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019. URL https://openreview.net/forum?\nid=ryGs6iA5Km.\nXu, P., Zhu, X., and Clifton, D. A. Multimodal learning\nwith transformers: A survey. CoRR, abs/2206.06488,\n2022. doi: 10.48550/arXiv.2206.06488. URL https:\n//doi.org/10.48550/arXiv.2206.06488.\nYing, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D.,\nShen, Y., and Liu, T. Do transformers really perform\nbadly for graph representation?\nIn Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W. (eds.), Advances in Neural Informa-\ntion Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pp. 28877\u2013\n28888,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\nf1c1592588411002af340cbaedd6fc33-Abstract.\nhtml.\n10\nInvariant Graph Transformer\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,\nAlberti, C., Onta\u02dcn\u00b4on, S., Pham, P., Ravula, A., Wang,\nQ., Yang, L., and Ahmed, A. Big bird: Transformers\nfor longer sequences. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33:\nAn-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual,\n2020.\nURL https://proceedings.\nneurips.cc/paper/2020/hash/\nc8512d142a2d849725f31a9a7a361ab9-Abstract.\nhtml.\nZhang, Z., Du, B., and Tong, H. Suger: A subgraph-based\ngraph convolutional network method for bundle recom-\nmendation. In Hasan, M. A. and Xiong, L. (eds.), Pro-\nceedings of the 31st ACM International Conference on\nInformation & Knowledge Management, Atlanta, GA,\nUSA, October 17-21, 2022, pp. 4712\u20134716. ACM, 2022.\ndoi: 10.1145/3511808.3557707. URL https://doi.\norg/10.1145/3511808.3557707.\n11\nInvariant Graph Transformer\nA. Training Algorithm of IGT\nStep-by-step training algorithms of IGT-N/VN are presented in Algorithm 1 and Algorithm 2.\nAlgorithm 1: IGT-N single training step for every training graph G\nInput\n:a labelled graph (G, y), a sampled graph \u02dcG from the same batch as G, \u03b8 = {\u03b8enc, \u03b8aug-N, \u03b8pred}, \u03d5;\nOutput :updated \u03b8 and \u03d5;\n1 compute the node embedding matrices H = \u03b8enc(G) and \u02dcH = \u03b8enc( \u02dcG);\n2 compute rationale and environment embeddings as (Hra, Henv) = \u03b8aug-N(H), ( \u02dcHra, \u02dcHenv) = \u03b8aug-N( \u02dcH) via\nEqs. (4), (5a), and (5b);\n3 concatenate rationale-environment pairs Hra||Henv and Hra|| \u02dcHenv;\n4 compute Lutil via Eq. (7);\n5 compute Lreg via Eqs. (10) and (9);\n6 compute gradients \u2202(Lutil+\u03b2Lreg)\n\u2202\u03b8\nand \u2202(Lutil+\u03b2Lreg)\n\u2202\u03d5\n;\n7 update \u03b8 via gradient descent with \u2202(Lutil+\u03b2Lreg)\n\u2202\u03b8\n;\n8 update \u03d5 via gradient ascent with \u2202(Lutil+\u03b2Lreg)\n\u2202\u03d5\n;\nAlgorithm 2: IGT-VN single training step for every training graph G\nInput\n:a labelled graph (G, y), a sampled graph \u02dcG from the same batch as G, \u03b8 = {\u03b8enc, \u03b8aug-VN, \u03b8pred}, \u03d5;\nOutput :updated \u03b8 and \u03d5;\n1 compute the node embedding matrices H = \u03b8enc(G) and \u02dcH = \u03b8enc( \u02dcG);\n2 compute rationale and environment embeddings as (Hra, Henv) = \u03b8aug-VN(H), ( \u02dcHra, \u02dcHenv) = \u03b8aug-VN( \u02dcH) via\nEqs. (12), (13a), and (13b);\n3 concatenate rationale-environment pairs Hra||Henv and Hra|| \u02dcHenv;\n4 compute Lutil via Eq. (7);\n5 compute Lreg via Eqs. (10) and (9);\n6 compute gradients \u2202(Lutil+\u03b2Lreg)\n\u2202\u03b8\nand \u2202(Lutil+\u03b2Lreg)\n\u2202\u03d5\n;\n7 update \u03b8 via gradient descent with \u2202(Lutil+\u03b2Lreg)\n\u2202\u03b8\n;\n8 update \u03d5 via gradient ascent with \u2202(Lutil+\u03b2Lreg)\n\u2202\u03d5\n;\nB. Dataset Statistics\nThe statistics of the datasets are presented in Table 3.\nB.1. Hardware and Implementations.\nWe implement IGT-N, IGT-VN, and all the baseline methods in PyTorch2 and PyTorch-geometric3. All the efficiency study\nresults are from one NVIDIA Tesla V100 SXM2-32GB GPU on a server with 96 Intel(R) Xeon(R) Gold 6240R CPU @\n2.40GHz processors and 1.5T RAM. When baseline methods have pre-existing results for specific datasets, we directly\nuse those statistics. In cases where such results are absent, we implement the models based on either the available code or\ndetails described in the associated publications.\nWe first introduce some shared implementations among IGT-N/VN and baseline methods. The batch size is set as 32 and\nthe weight decay is set as 0. The hidden dimension is set as 300. The dropout rate for both the encoder and the intervener\nis searched between {0, 0.5}. The pooling is searched between {mean, sum}. The normalization is searched between\nthe batch normalization (Ioffe & Szegedy, 2015) and layer normalization (Ba et al., 2016). The learning rate is initialized\nas 0.0001 and will decay by a factor 1\n4 if the validation performance is not improved in 10 epochs. Next, we detail the\n2https://pytorch.org/\n3https://pytorch-geometric.readthedocs.io/en/latest/\n12\nInvariant Graph Transformer\nTable 3: Dataset statistics.\nName\n# Graphs\n# Nodes\n# Edges\n# Features\n# Classes\nSplit\nMetric\nZINC\n12000\n23.2\n49.8\n21 (node), 4 (edge)\nN/A\n10000/1000/1000\nMAE\nAQSOL\n9833\n17.6\n35.8\n65 (node), 5 (edge)\nN/A\n7836/998/999\nMAE\nmollipo\n4200\n27.0\n59.0\n9 (node), 3 (edge)\nN/A\n3360/420/420\nRMSE\nmolhiv\n41127\n25.5\n54.9\n9 (node), 3 (edge)\n2\n32901/4113/4113\nROC-AUC\nmoltox21\n7831\n18.6\n38.6\n9 (node), 3 (edge)\n2\n6264/783/784\nROC-AUC\nmolbace\n1513\n34.4\n73.7\n9 (node), 3 (edge)\n2\n1210/151/152\nROC-AUC\nmolbbbp\n2039\n24.1\n51.9\n9 (node), 3 (edge)\n2\n1631/204/204\nROC-AUC\nmolmuv\n93087\n24.2\n52.6\n9 (node), 3 (edge)\n2\n74469/9309/9309\nROC-AUC\nimplementation of IGT-N/VN and baseline methods.\nB.2. Implementation of IGT-N/VN\nThe encoder is set as GPS (Ramp\u00b4asek et al., 2022) on ZINC, AQSOL, mollipo, molhiv, molbace, and set as GraphTrans (Wu\net al., 2022b) on moltox21 and molbbbp. For the predictor module, we follow the typical design as a 3-layered MLP with\nReLU activation in the intermediate layers. In our implementation, we set \u03b2 =\n2\u00d7 \u02c6\u03b2\nn\u00d7(n\u22121) (IGT-N) or \u03b2 =\n2\u00d7 \u02c6\u03b2\nr\u00d7(r\u22121) (IGT-VN).\nThe \u03b1 and \u02c6\u03b2 are searched between [0.2, 2], step size 0.2. In our implementation, the K is set as K = round( \u02c6K \u00d7 n) (for\nIGT-N) or K = round( \u02c6K \u00d7 r) (for IGT-VN). r is searched between {8, 16, 32} for IGT-VN. We have a detailed sensitivity\nstudy to explore the best selection of \u02c6K in Section F, which shows the best \u02dcK is around 0.75.\nB.3. Implementation of Baseline Methods\nWe search the number of layers of GIN (Xu et al., 2019), GAT (Velickovic et al., 2018), GATv2 (Brody et al., 2022),\nGatedGCN (Bresson & Laurent, 2017), DIR (Wu et al., 2022a), and GREA (Liu et al., 2022) between {2, 3, 5, 10} and\nreport the best performance, considering configurations both with and without a virtual node connecting to all the given\nnodes.\nRegarding the Transformer-based baselines (GT (Dwivedi & Bresson, 2020), GraphiT (Mialon et al., 2021), SAN (Kreuzer\net al., 2021), SAT (Chen et al., 2022), Graphormer (Ying et al., 2021), GraphTrans (Wu et al., 2022b), GPS (Ramp\u00b4asek et al.,\n2022)), for the (absolute or relative) positional encoding, we adhere to the suggestions made in their original papers. We\nalso searched the number of layers between {2, 3, 5, 10}.\nOur implementations of GIN, GAT, GATv2, and GatedGCN are from the PyTorch-geometric4 package. Our implementations\nof GT 5, GraphiT 6, SAN 7, SAT 8, Graphormer 9, GraphTrans 10, GPS 11, DIR 12, GREA 13 are adapted from publicly\navailable code.\nC. Number of Parameters\nA detailed number of parameter comparison is presented in Table 4, where we list 4 typical 5-layered encoders (GIN, SAT,\nGraphTrans, and GPS), and our proposed node-/virtual node-level augmenter, intervener modules (i.e., {\u03b8aug-N, \u03d5} and\n{\u03b8aug-VN, \u03d5}). Clearly, our proposed interveners do not significantly increase the size of the whole model.\n4https://pytorch-geometric.readthedocs.io/en/latest/\n5https://github.com/graphdeeplearning/graphtransformer\n6https://github.com/inria-thoth/GraphiT\n7https://github.com/DevinKreuzer/SAN\n8https://github.com/BorgwardtLab/SAT\n9https://github.com/microsoft/Graphormer\n10https://github.com/ucbrise/graphtrans\n11https://github.com/rampasek/GraphGPS\n12https://github.com/Wuyxin/DIR-GNN\n13https://github.com/liugangcode/GREA\n13\nInvariant Graph Transformer\nTable 4: Number of parameter comparison between the proposed augmenter, intervener, and common graph encoders.\nModel\n# Parameters\nGIN\n1, 708, 807\nSAT\n2, 790, 739\nGraphTrans\n2, 793, 307\nGPS\n3, 236, 239\n{\u03b8aug-N, \u03d5}\n453, 001\n{\u03b8aug-VN, \u03d5}\n363, 320\nD. Efficiency Study\nWe compare the model efficiency (training iterations/second) of IGT-N and IGT-VN in Table 5 working with different\nencoders (GIN and GPS). The batch size is set as 32. We note that the inclusion of our proposed augmenter and intervener,\nrepresented as {\u03b8aug-N, \u03d5} or {\u03b8aug-VN, \u03d5}, introduces a slight reduction in training speed. That is because the proposed\nparametric augmenter and intervener increase the steps of the data pipeline, as presented in figure 2b, and enlarge the\ncomputational graph for auto-gradient tools, such as PyTorch. Fortunately, the parameter count of the parametric augmenter\nand intervener is low, ensuring that the overall training speed of the model is not dramatically affected. A detailed comparison\nabout the number of parameters of the newly-introduced node-/virtual node-level augmenter (\u03b8aug-N/\u03b8aug-VN), intervener (\u03d5)\nmodules and other typical encoder modules (\u03b8enc) are presented in Table 4 (Appendix).\nTable 5: Wall-clock time (iterations/second) comparison of different encoder-intervener combinations. The larger the faster.\n(\u2193) denotes the speed degradation compared with the vanilla encoder without intervener.\nEncoder\nIntervener\nmollipo\nmolbace\nmolbbbp\nGIN\nNone\n29.38\n25.62\n27.11\nIGT-N\n23.05(\u21936.33)\n21.23(\u21934.39)\n21.61(\u21935.50)\nIGT-VN\n23.35(\u21936.03)\n21.46(\u21934.16)\n22.32(\u21934.79)\nGPS\nNone\n24.29\n20.51\n22.30\nIGT-N\n19.57(\u21934.72)\n17.83(\u21932.68)\n18.67(\u21933.63)\nIGT-VN\n19.93(\u21934.63)\n18.16(\u21932.35)\n18.88(\u21933.42)\nE. Training Convergence\nA question we concern about is the impact of the min-max objective on the training stability of IGT-N and IGT-VN. We\nmonitor the training losses of both IGT-N and IGT-VN across three datasets (molbace, molbbbp, mollipo) using two encoders\n(GIN and GPS). The results, presented in Figure 4, demonstrate that the training remains stable even when \u03b8 (representing\nthe encoder, augmenter, and predictor) and \u03d5 (representing the intervener) engage in a min-max game.\nF. Sensitivity Study\nIn this section, we carefully study the impact of hyperparameter K (from Eq. (5a), (5b), (13a), and (13b)), which determines\nthe ratio of the rationale and environment subgraphs. In our implementation, we set K = round( \u02c6K \u00d7 n) (for IGT-N) or\nK = round( \u02c6K \u00d7 r) (for IGT-VN). We evaluate the model performance across varying \u02c6K on the molbace and molbbbp\ndatasets in Figure 5. We note that if most nodes/virtual nodes are marked as the environment component, the model\nperformance degrades. Similar performance degradation is observed if too many nodes/virtual nodes are marked as the\nrationale nodes (e.g., \u02c6K = 0.875. That is because for a large \u02c6K (e.g., \u02c6K = 1), the model degenerates to a vanilla graph\nencoder, with less intervention involved. When \u02c6K is set as 0.75 or 0.675, the best performance is observed.\n14\nInvariant Graph Transformer\n(a) molbace, GIN\n(b) molbace, GPS\n(c) molbbbp, GIN\n(d) molbbbp, GPS\n(e) mollipo, GIN\n(f) mollipo, GPS\nFigure 4: Training loss of IGT-N/VN with different datasets and encoders.\n(a) molbace\n(b) molbbbp\nFigure 5: Performance of IGT-N/VN with different \u02c6K.\n15\n"
  },
  {
    "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
    "link": "https://arxiv.org/pdf/2312.08344.pdf",
    "upvote": "5",
    "text": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects\nBowen Wen\nWei Yang\nJan Kautz\nStan Birchfield\nNVIDIA\nAbstract\nWe present FoundationPose, a unified foundation model\nfor 6D object pose estimation and tracking, supporting both\nmodel-based and model-free setups. Our approach can be\ninstantly applied at test-time to a novel object without fine-\ntuning, as long as its CAD model is given, or a small num-\nber of reference images are captured. We bridge the gap be-\ntween these two setups with a neural implicit representation\nthat allows for effective novel view synthesis, keeping the\ndownstream pose estimation modules invariant under the\nsame unified framework. Strong generalizability is achieved\nvia large-scale synthetic training, aided by a large lan-\nguage model (LLM), a novel transformer-based architec-\nture, and contrastive learning formulation. Extensive evalu-\nation on multiple public datasets involving challenging sce-\nnarios and objects indicate our unified approach outper-\nforms existing methods specialized for each task by a large\nmargin. In addition, it even achieves comparable results\nto instance-level methods despite the reduced assumptions.\nProject page: https://nvlabs.github.io/FoundationPose/\n1. Introduction\nComputing the rigid 6D transformation from the object to\nthe camera, also known as object pose estimation, is cru-\ncial for a variety of applications, such as robotic manip-\nulation [28, 60] and mixed reality [39].\nClassic meth-\nods [18, 19, 29, 45] are known as instance-level because\nthey only work on the specific object instance determined at\ntraining time. Such methods usually require a textured CAD\nmodel for generating training data, and they cannot be ap-\nplied to an unseen novel object at test time. While category-\nlevel methods [5, 31, 52, 56, 65] remove these assumptions\n(instance-wise training and CAD models), they are limited\nto objects within the predefined category on which they are\ntrained. Moreover, obtaining category-level training data is\nnotoriously difficult, in part due to additional pose canoni-\ncalization and examination steps [56] that must be applied.\nTo address these limitations, more recent efforts have fo-\ncused on the problem of instant pose estimation of arbitrary\nnovel objects [17, 30, 37, 48, 51]. Two different setups are\nModel-\nModel-free tracking\nModel-based tracking\nFigure 1. Our unified framework enables both 6D pose estimation and\ntracking for novel objects, supporting the model-based and model-free se-\ntups. On each of these four tasks, it outperforms prior work specially de-\nsigned for the task (\u2022 indicates RGB-only, \u00d7 indicates RGBD). The metric\nfor each task is explained in detail in the experimental results.\nconsidered, depending upon what information is available at\ntest time: model-based, where a textured 3D CAD model of\nthe object is provided, and model-free, where a set of refer-\nence images of the object is provided. While much progress\nhas been made on both setups individually, there remains a\nneed for a single method to address both setups in a unified\nway, since different real-world applications provide differ-\nent types of information.\nOrthogonal to single-frame object pose estimation, pose\ntracking methods [8, 27, 33, 36, 49, 55, 59, 62] leverage\ntemporal cues to enable more efficient, smooth and accu-\nrate pose estimation on a video sequence. These methods\nshare the similar aforementioned issues to their counterparts\nin pose estimation, depending on their assumptions on the\nobject knowledge.\nIn this paper we propose a unified framework called\nFoundationPose that performs both pose estimation and\ntracking for novel objects in both the model-based and\nmodel-free setups. As seen in Fig. 1, our method outper-\nforms existing state-of-art methods specialized for each of\nthese four tasks.\nOur strong generalizability is achieved\narXiv:2312.08344v1  [cs.CV]  13 Dec 2023\nvia large-scale synthetic training, aided by a large language\nmodel (LLM), as well as a novel transformer-based archi-\ntecture and contrastive learning. We bridge the gap between\nmodel-based and model-free setups with a neural implicit\nrepresentation that allows for effective novel view synthesis\nwith a small number (\u223c16) of reference images, achieving\nrendering speeds that are significantly faster than previous\nrender-and-compare methods [30, 33, 59]. Our contribu-\ntions can be summarized as follows:\n\u2022 We present a unified framework for both pose estimation\nand tracking for novel objects, supporting both model-\nbased and model-free setups. An object-centric neural\nimplicit representation for effective novel view synthesis\nbridges the gap between the two setups.\n\u2022 We propose a LLM-aided synthetic data generation\npipeline which scales up the variety of 3D training assets\nby diverse texture augmentation.\n\u2022 Our novel design of transformer-based network architec-\ntures and contrastive learning formulation leads to strong\ngeneralization when trained solely on synthetic data.\n\u2022 Our method outperforms existing methods specialized\nfor each task by a large margin across multiple public\ndatasets. It even achieves comparable results to instance-\nlevel methods despite reduced assumptions.\nCode and data developed in this work will be released.\n2. Related Work\nCAD Model-based Object Pose Estimation.\nInstance-\nlevel pose estimation methods [18, 19, 29, 45] assume a\ntextured CAD model is given for the object. Training and\ntesting is performed on the exact same instance. The object\npose is often solved by direct regression [34, 63], or con-\nstructing 2D-3D correspondences followed by PnP [45, 53],\nor 3D-3D correspondences followed by least squares fit-\nting [18, 19]. To relax the assumptions about the object\nknowledge, category-level methods [5, 31, 52, 56, 65, 67]\ncan be applied to novel object instances of the same cate-\ngory, but they cannot generalize to arbitrary novel objects\nbeyond the predefined categories. To address this limita-\ntion, recent efforts [30, 48] aim for instant pose estimation\nof arbitrary novel objects as long as the CAD model is pro-\nvided at test time.\nFew-shot Model-free Object pose estimation.\nModel-\nfree methods remove the requirement of an explicit textured\nmodel. Instead, a number of reference images capturing\nthe target object are provided [17, 20, 46, 51]. RLLG [3]\nand NeRF-Pose [32] propose instance-wise training without\nthe need of an object CAD model. In particular, [32] con-\nstructs a neural radiance field to provide semi-supervision\non the object coordinate map and mask. Differently, we\nintroduce the neural object field built on top of SDF repre-\nsentation for efficient RGB and depth rendering to bridge\nthe gap between the model-based and model-free scenarios.\nIn addition, we focus on generalizable novel object pose es-\ntimation in this work, which is not the case for [3, 32]. To\nhandle novel objects, Gen6D [37] designs a detection, re-\ntrieval and refinement pipeline. However, to avoid difficul-\nties with out-of-distribution test set, it requires fine-tuning.\nOnePose [51] and its extension OnePose++ [17] leverage\nstructure-from-motion (SfM) for object modeling and pre-\ntrain 2D-3D matching networks to solve the pose from cor-\nrespondences. FS6D [20] adopts a similar scheme and fo-\ncuses on RGBD modality. Nevertheless, reliance on cor-\nrespondences becomes fragile when applied to textureless\nobjects or under severe occlusion.\nObject Pose Tracking. 6D object pose tracking aims to\nleverage temporal cues to enable more efficient, smooth and\naccurate pose prediction on video sequence. Through neu-\nral rendering, our method can be trivially extended to the\npose tracking task with high efficiency. Similar to single-\nframe pose estimation, existing tracking methods can be\ncategorized into their counterparts depending on the as-\nsumptions of object knowledge. These include instance-\nlevel methods [8, 33, 59], category-level methods [36, 55],\nmodel-based novel object tracking [27, 49, 62] and model-\nfree novel object tracking [58, 61].\nUnder both model-\nbased and model-free setups, we set a new benchmark\nrecord across public datasets, even outperforming state-of-\nart methods that require instance-level training [8, 33, 59].\n3. Approach\nOur method is described in the following subsections. The\nrelationships between the subsections, and the system as a\nwhole, are illustrated in Fig. 2.\n3.1. Language-aided Data Generation at Scale\nTo achieve strong generalization, a large diversity of ob-\njects and scenes is needed for training. Obtaining such data\nin the real world, and annotating accurate ground-truth 6D\npose, is time- and cost-prohibitive. Synthetic data, on the\nother hand, often lacks the size and diversity in 3D assets.\nWe developed a novel synthetic data generation pipeline\nfor training, powered by the recent emerging resources and\ntechniques: large scale 3D model database [6, 10], large\nlanguage models (LLM), and diffusion models [4, 22, 47].\nThis approach dramatically scales up both the amount and\ndiversity of data compared with prior work [20, 24, 30].\n3D Assets.\nWe obtain training assets from recent large\nscale 3D databases including Objaverse [6] and GSO [10].\nFor Objaverse [6] we chose the objects from the Objaverse-\nLVIS subset that consists of more than 40K objects belong-\ning to 1156 LVIS [12] categories. This list contains the most\nrelevant daily-life objects with reasonable quality, and di-\nversity of shapes and appearances. It also provides a tag\nfor each object describing its category, which benefits au-\nFigure 2. Overview of our framework. To reduce manual efforts for large scale training, we developed a novel synthetic data generation pipeline by\nleveraging recent emerging techniques and resources including 3D model database, large language models and diffusion models (Sec. 3.1). To bridge the\ngap between model-free and model-based setup, we leverage an object-centric neural field (Sec. 3.2) for novel view RGBD rendering for subsequent render-\nand-compare. For pose estimation, we first initialize global poses uniformly around the object, which are then refined by the refinement network (Sec. 3.3).\nFinally, we forward the refined poses to the pose selection module which predicts their scores. The pose with the best score is selected as output (Sec. 3.4).\nA\u00a0traditional\u00a0wooden\u00a0\narmoire\u00a0in\u00a0a\u00a0rich\u00a0mahogany\u00a0\nfinish,\u00a0showcasing\u00a0\nintricate\u00a0carvings\u00a0and\u00a0\nbrass\u00a0hardware\u00a0for\u00a0an\u00a0\nelegant\u00a0look\nA\u00a0vibrant\u00a0red\u00a0bulb\u00a0with\u00a0a\u00a0\ngradient\u00a0of\u00a0orange\u00a0and\u00a0\nyellow,\u00a0emitting\u00a0a\u00a0warm\u00a0glow\nRandom texture blending\nA\u00a0unique\u00a0wineglass\u00a0with\u00a0a\u00a0\nstem\u00a0shaped\u00a0like\u00a0a\u00a0\ncorkscrew,\u00a0showcasing\u00a0a\u00a0\nbowl\u00a0made\u00a0of\u00a0hand\u2010blown\u00a0\nglass\u00a0in\u00a0a\u00a0mix\u00a0of\u00a0swirling\u00a0\nred\u00a0and\u00a0white\n85.00\n87.00\n89.00\n91.00\n93.00\n95.00\n97.00\n99.00\n4\nNum\nADD\u00a0(%)\nAn\u00a0artistic\u00a0wineglass\u00a0\nhand\u2010painted\u00a0with\u00a0vibrant\u00a0\nstrokes\u00a0of\u00a0brown,\u00a0blue,\u00a0\nand\u00a0green,\u00a0creating\u00a0a\u00a0\nstriking\u00a0abstract\u00a0design\nFigure 3. Top: Random texture blending proposed in FS6D [20]. Bot-\ntom: Our LLM-aided texture augmentation yields more realistic appear-\nance. Leftmost is the original 3D assets. Text prompts are automatically\ngenerated by ChatGPT.\ntomatic language prompt generation in the following LLM-\naided texture augmentation step.\nLLM-aided Texture Augmentation.\nWhile most Obja-\nverse objects have high quality shapes, their texture fidelity\nvaries significantly. FS6D [20] proposes to augment object\ntexture by randomly pasting images from ImageNet [7] or\nMS-COCO [35]. However, due to the random UV mapping,\nthis method yields artifacts such as seams on the result-\ning textured mesh (Fig. 3 top); and applying holistic scene\nimages to objects leads to unrealistic results. In contrast,\nwe explore how recent advances in large language mod-\nels and diffusion models can be harnessed for more real-\nistic (and fully automatic) texture augmentation. Specifi-\ncally, we provide a text prompt, an object shape, and a ran-\ndomly initialized noisy texture to TexFusion [4] to produce\nan augmented textured model. Of course, providing such\na prompt manually is not scalable if we want to augment\na large number of objects in diverse styles under different\nprompt guidance. As a result, we introduce a two-level hi-\nerarchical prompt strategy. As illustrated in Fig. 2 top-left,\nwe first prompt ChatGPT, asking it to describe the possi-\nble appearance of an object; this prompt is templated so\nthat each time we only need to replace the tag paired with\nthe object, which is given by the Objaverse-LVIS list. The\nanswer from ChatGPT then becomes the text prompt pro-\nvided to the diffusion model for texture synthesis. Because\nthis approach enables full automation for texture augmenta-\ntion, it facilitates diversified data generation at scale. Fig. 3\npresents more examples including different stylization for\nthe same object.\nData Generation. Our synthetic data generation is imple-\nmented in NVIDIA Isaac Sim, leveraging path tracing for\nhigh-fidelity photo-realistic rendering.1 We perform grav-\nity and physics simulation to produce physically plausible\nscenes. In each scene, we randomly sample objects includ-\ning the original and texture-augmented versions. The object\nsize, material, camera pose, and lighting are also random-\nized; more details can be found in the appendix.\n3.2. Neural Unknown Object Modeling\nIn the absence of CAD models, one key challenge is to rep-\nresent the object to effectively render images with sufficient\nquality for downstream modules. Neural implicit represen-\ntations have not only been shown to be effective for novel\nview synthesis, but they can also be parallelized on a GPU,\nthus providing high computational efficiency when render-\ning multiple pose hypotheses for downstream pose estima-\ntion modules. To this end, we introduce an object-centric\nneural SDF representation for object modeling, inspired by\nprevious work [41, 57, 61, 64].\nField Representation.\nWe represent the object by two\nfunctions [64] as shown in Fig. 2. First, the geometry func-\ntion \u2126 : x 7\u2192 s takes as input a 3D point x \u2208 R3 and\noutputs a signed distance value s \u2208 R. Second, the appear-\nance function \u03a6 : (f\u2126(x), n, d) 7\u2192 c takes the intermedi-\nate feature vector f\u2126(x) from the geometry network, a point\nnormal n \u2208 R3, and a view direction d \u2208 R3, and outputs\nthe color c \u2208 R3\n+. In practice, we apply multi-resolution\nhash encoding [41] to x before forwarding to the network.\nBoth n and d are embedded by a fixed set of second-order\nspherical harmonic coefficients. The implicit object surface\nis obtained by taking the zero level set of the signed distance\nfield: S =\n\b\nx \u2208 R3 | \u2126(x) = 0\n\t\n. Compared to NeRF [40],\nthe SDF representation \u2126 provides higher quality depth ren-\ndering while removing the need to manually select a density\nthreshold.\nField Learning. For texture learning, we follow the volu-\nmetric rendering over truncated near-surface regions [61]:\nc(r) =\nZ z(r)+0.5\u03bb\nz(r)\u2212\u03bb\nw(xi)\u03a6(f\u2126(xi), n(xi), d(xi)) dt,\n(1)\nw(xi) =\n1\n1 + e\u2212\u03b1\u2126(xi)\n1\n1 + e\u03b1\u2126(xi) ,\n(2)\nwhere w(xi) is the bell-shaped probability density function\n[57] that depends on the signed distance \u2126(xi) from the\npoint to the implicit object surface, and \u03b1 adjusts the soft-\nness of the distribution. The probability peaks at the surface\n1https://developer.nvidia.com/isaac-sim\nintersection. In Eq. (1), z(r) is the depth value of the ray\nfrom the depth image, and \u03bb is the truncation distance. We\nignore the contribution from empty space that is more than\n\u03bb away from the surface for more efficient training, and we\nonly integrate up to a 0.5\u03bb penetrating distance to model\nself-occlusion [57]. During training, we compare this quan-\ntity against the reference RGB images for color supervision:\nLc =\n1\n|R|\nX\nr\u2208R\n\u2225c(r) \u2212 \u00afc(r)\u22252 ,\n(3)\nwhere \u00afc(r) denotes the ground-truth color at the pixel where\nthe ray r passes through.\nFor geometry learning, we adopt the hybrid SDF\nmodel [61] by dividing the space into two regions to learn\nthe SDF, leading to the empty space loss and the near-\nsurface loss. We also apply Eikonal regularization [11] to\nthe near-surface SDF:\nLe =\n1\n|Xe|\nX\nx\u2208Xe\n|\u2126(x) \u2212 \u03bb|,\n(4)\nLs =\n1\n|Xs|\nX\nx\u2208Xs\n(\u2126(x) + dx \u2212 dD)2 ,\n(5)\nLeik =\n1\n|Xs|\nX\nx\u2208Xs\n(\u2225\u2207\u2126(x)\u22252 \u2212 1)2,\n(6)\nwhere x denotes a sampled 3D point along the rays in the\ndivided space; dx and dD are the distance from ray origin\nto the sample point and the observed depth point, respec-\ntively. We do not use the uncertain free-space loss [61], as\nthe template images are pre-captured offline in the model-\nfree setup. The total training loss is\nL = wcLc + weLe + wsLs + weikLeik.\n(7)\nThe learning is optimized per object without priors and can\nbe efficiently performed within seconds. When training the\npose refinement (Sec. 3.3) and selection (Sec. 3.4) modules,\nwe first pretrain the neural object field with randomized\nnumber of synthetic reference images capturing the 3D as-\nset. The trained neural object field then provides rendering\nwhich will be mixed with the model-based OpenGL ren-\ndering as input to subsequent networks. This better covers\nthe distribution of both model-based and model-free setups,\nenabling effective generalization as a unified framework.\nRendering. The Neural Field only needs to be trained once\nfor a novel unknown object. Once trained, the field is effi-\nciently rendered at inference, serving as a drop-in replace-\nment for a conventional graphics pipeline. In addition to the\ncolor rendering as in the original NeRF [40], we also need\ndepth rendering for our RGBD based pose estimation and\ntracking. To do so, we perform marching cubes [38] to ex-\ntract a mesh from the zero level set of the SDF. This only\nneeds to be performed once for each object. At inference,\ngiven an object pose, we then render the depth image fol-\nlowing the rasterization process. Alternatively, one could\ndirectly render the depth image using \u2126 online with sphere\ntracing [13]; however, we found this leads to less efficiency,\nespecially when there is a large number of pose hypotheses\nto render in parallel.\n3.3. Pose Hypothesis Generation\nPose Initialization. Given the RGBD image, we first ini-\ntialize the translation using the 3D point located at the me-\ndian depth within the region of interest defined by the 2D\ndetection. To initialize rotations, we uniformly sample Ns\nviewpoints from an icosphere centered on the object with\nthe camera facing the center. These camera poses are fur-\nther augmented with Ni discretized in-plane rotations, re-\nsulting in Ns \u00b7 Ni global pose initializations which are sent\nas input to the pose refiner.\nPose Refinement. Since the coarse pose initializations from\nthe previous step are often quite noisy, a refinement mod-\nule is needed to improve the pose quality. Specifically, we\nbuild a pose refinement network which takes as input the\nrendering of the object conditioned on the coarse pose, and\na crop of the input observation from the camera; the network\noutputs a pose update that improves the pose quality. Un-\nlike MegaPose [30], which renders multiple views around\nthe coarse pose to find the anchor point, we observed ren-\ndering a single view corresponding to the coarse pose suf-\nfices. For the input observation, instead of cropping based\non the 2D detection which is constant, we perform a pose-\nconditioned cropping strategy so as to provide feedback to\nthe translation update. Concretely, we project the object\norigin to the image space to determine the crop center. We\nthen project the slightly enlarged object diameter (the max-\nimum distance between any pair of points on the object sur-\nface) to determine the crop size that encloses the object and\nthe nearby context around the pose hypothesis. This crop\nis thus conditioned on the coarse pose and encourages the\nnetwork to update the translation to make the crop better\naligned with the observation. The refinement process can be\nrepeated multiple times by feeding the latest updated pose\nas input to the next inference, so as to iteratively improve\nthe pose quality.\nThe refinement network architecture is illustrated in\nFig. 2; details are in the appendix. We first extract fea-\nture maps from the two RGBD input branches with a single\nshared CNN encoder. The feature maps are concatenated,\nfed into CNN blocks with residual connection [15], and to-\nkenized by dividing into patches [9] with position embed-\nding. Finally, the network predicts the translation update\n\u2206t \u2208 R3 and rotation update \u2206R \u2208 SO(3), each individu-\nally processed by a transformer encoder [54] and linearly\nprojected to the output dimension. More concretely, \u2206t\nrepresents the object\u2019s translation shift in the camera frame,\n\u2206R represents the object\u2019s orientation update expressed in\nthe camera frame. In practice, the rotations are parameter-\nObservation\nDecreasing\u00a0pose\u00a0rank\nOurs\nW/o\u00a0hierarchical\u00a0comparison\nFigure 4. Pose ranking visualization. Our proposed hierarchical compar-\nison leverages the global context among all pose hypotheses for a better\noverall trend prediction that aligns both shape and texture.\nized with the 6D representation [68]. The input coarse pose\n[R | t] \u2208 SE(3) is then updated by:\nt+ = t + \u2206t\n(8)\nR+ = \u2206R \u2297 R,\n(9)\nwhere \u2297 denotes update on SO(3). Instead of using a single\nhomogeneous pose update, this disentangled representation\nremoves the dependency on the updated orientation when\napplying the translation update. This unifies both the up-\ndates and input observation in the camera coordinate frame\nand thus simplifies the learning process. The network train-\ning is supervised by L2 loss:\nLrefine = w1 \u2225\u2206t \u2212 \u2206\u00aft\u22252 + w2\n\r\r\u2206R \u2212 \u2206 \u00afR\n\r\r\n2 ,\n(10)\nwhere \u00aft and \u00afR are ground truth.\n3.4. Pose Selection\nGiven a list of refined pose hypotheses, we build a pose\nranking network to compute their scores. The pose with the\nhighest score is selected as the final estimate.\nHierarchical Comparison. We propose a two-level com-\nparison strategy. First, for each pose hypothesis, we com-\npare the rendered image against the cropped input obser-\nvation, where the pose-conditioned cropping operation was\nintroduced in Sec. 3.3. This comparison is performed with a\npose ranking encoder, where we utilize the same backbone\narchitecture for feature extraction as in the refinement net-\nwork. The extracted features are concatenated, tokenized\nand forwarded to the multi-head self-attention module so as\nto better leverage the global context for comparison. The\npose ranking encoder performs average pooling to output\na feature embedding F \u2208 R512 describing the alignment\nquality between the rendering and the observation (Fig. 2\nbottom-middle). At this point, we could directly project F\nto a similarity scalar as typically done [2, 30, 42]. How-\never, this would ignore the other pose hypotheses, forcing\nthe network to output an absolute score assignment which\ncan be difficult to learn.\nIntuitively, we would like the network to leverage the\nglobal context of all pose hypotheses in order to make a\nmore informed decision. Therefore, we introduce the sec-\nond level of comparison among all the K pose hypothe-\nses, or more precisely, the concatenated feature embedding\nF = [F0, . . . , FK\u22121]\u22a4 \u2208 RK\u00d7512, which has encoded the\npose alignment information. To adapt to varying K, we\ntreat F as a sequence and perform multi-head self-attention\nwhich naturally generalizes to varying lengths [54]. We also\ndo not apply position encoding to F here, so as to be agnos-\ntic to the permutation. The attended feature is then linearly\nprojected to the scores S \u2208 RK to be assigned to each pose\nhypothesis. The effectivenss of this hierarchical comparison\nstrategy is shown in a typical example in Fig. 4.\nContrast Validation. To train the pose ranking network,\nwe propose a pose-conditioned triplet loss:\nL(i+, i\u2212) = max(S(i\u2212) \u2212 S(i+) + \u03b1, 0),\n(11)\nwhere \u03b1 denotes the contrastive margin; i\u2212 and i+ repre-\nsent the negative and positive pose samples, respectively,\nwhich are determined by computing the ADD metric [63]\nusing ground truth. Note that different from standard triplet\nloss [25], the anchor sample is not shared between the pos-\nitive and negative samples in our case, since the input is\ncropped depending on each pose hypothesis to account for\ntranslations. While we can compute this loss over each pair\nin the list, the comparison becomes ambiguous when both\nposes are far from ground truth. Therefore, we only keep\nthose pose pairs whose positive sample is from a viewpoint\nthat is close enough to the ground truth to make the com-\nparison meaningful:\nV+ = {i : D(Ri, \u00afR) < d}\n(12)\nV\u2212 = {0, 1, 2, . . . , K \u2212 1}\n(13)\nLrank =\nX\ni+,i\u2212\nL(i+, i\u2212)\n(14)\nwhere the summation is over i+ \u2208 V+, i\u2212 \u2208 V\u2212, i+ \u0338= i\u2212;\nRi and \u00afR are the rotation of the hypothesis and ground\ntruth, respectively; D(\u00b7) denotes the geodesic distance be-\ntween rotations; and d is a predefined threshold. We also\nexperimented with the InfoNCE loss [44] as used in [42] but\nobserved worse performance (Sec. 4.5). We attribute this to\nthe perfect translation assumption made in [42] which is not\nthe case in our setup.\n4. Experiments\n4.1. Dataset and Setup\nWe consider 5 datasets:\nLINEMOD [21], Occluded\nLINEMOD [1], YCB-Video [63], T-LESS [23], and YCBI-\nnEOAT [59].\nThese involve various challenging scenar-\nios (dense clutter, multi-instance, static or dynamic scenes,\ntable-top or robotic manipulation), and objects with diverse\nproperties (textureless, shiny, symmetric, varying sizes).\nAs our framework is unified, we consider the combina-\ntions among two setups (model-free and model-based) and\ntwo pose prediction tasks (6D pose estimation and track-\ning), resulting in 4 tasks in total. For the model-free setup,\na number of reference images capturing the novel object\nPREDATOR [26]\nLoFTR [50]\nFS6D-DPM [20]\nOurs\nRef. images\n16\n16\n16\n16\nFinetune-free\n\u2713\n\u2713\n\u2717\n\u2713\nMetrics\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\n002_master_chef_can\n73.0\n17.4\n87.2\n50.6\n92.6\n36.8\n96.9\n91.3\n003_cracker_box\n41.7\n8.3\n71.8\n25.5\n83.9\n24.5\n97.5\n96.2\n004_sugar_box\n53.7\n15.3\n63.9\n13.4\n95.1\n43.9\n97.5\n87.2\n005_tomato_soup_can\n81.2\n44.4\n77.1\n52.9\n93.0\n54.2\n97.6\n93.3\n006_mustard_bottle\n35.5\n5.0\n84.5\n59.0\n97.0\n71.1\n98.4\n97.3\n007_tuna_fish_can\n78.2\n34.2\n72.6\n55.7\n94.5\n53.9\n97.7\n73.7\n008_pudding_box\n73.5\n24.2\n86.5\n68.1\n94.9\n79.6\n98.5\n97.0\n009_gelatin_box\n81.4\n37.5\n71.6\n45.2\n98.3\n32.1\n98.5\n97.3\n010_potted_meat_can\n62.0\n20.9\n67.4\n45.1\n87.6\n54.9\n96.6\n82.3\n011_banana\n57.7\n9.9\n24.2\n1.6\n94.0\n69.1\n98.1\n95.4\n019_pitcher_base\n83.7\n18.1\n58.7\n22.3\n91.1\n40.4\n97.9\n96.6\n021_bleach_cleanser\n88.3\n48.1\n36.9\n16.7\n89.4\n44.1\n97.4\n93.3\n024_bowl\n73.2\n17.4\n32.7\n1.4\n74.7\n0.9\n94.9\n89.7\n025_mug\n84.8\n29.5\n47.3\n23.6\n86.5\n39.2\n96.2\n75.8\n035_power_drill\n60.6\n12.3\n18.8\n1.3\n73.0\n19.8\n98.0\n96.3\n036_wood_block\n70.5\n10.0\n49.9\n1.4\n94.7\n27.9\n97.4\n94.7\n037_scissors\n75.5\n25.0\n32.3\n14.6\n74.2\n27.7\n97.8\n95.5\n040_large_marker\n81.8\n38.9\n20.7\n8.4\n97.4\n74.2\n98.6\n96.5\n051_large_clamp\n83.0\n34.4\n24.1\n11.2\n82.7\n34.7\n96.9\n92.7\n052_extra_large_clamp\n72.9\n24.1\n15.0\n1.8\n65.7\n10.1\n97.6\n94.1\n061_foam_brick\n79.2\n35.5\n59.4\n31.4\n95.7\n45.8\n98.1\n93.4\nMEAN\n71.0\n24.3\n52.5\n26.2\n88.4\n42.1\n97.4\n91.5\nTable 1. Model-free pose estimation results measured by AUC of ADD\nand ADD-S on YCB-Video dataset. \u201cFinetuned\u201d means the method was\nfine-tuned with group split of object instances on the testing dataset, as\nintroduced by [20].\nare selected from the training split of the datasets, equipped\nwith the ground-truth annotation of the object pose, follow-\ning [20]. For the model-based setup, a CAD model is pro-\nvided for the novel object. In all evaluation except for ab-\nlation, our method always uses the same trained model and\nconfigurations for inference without any fine-tuning.\n4.2. Metric\nTo closely follow the baseline protocols on each setup, we\nconsider the following metrics:\n\u2022 Area under the curve (AUC) of ADD and ADD-S [63].\n\u2022 Recall of ADD that is less than 0.1 of the object diameter\n(ADD-0.1d), as used in [17, 20].\n\u2022 Average recall (AR) of VSD, MSSD and MSPD metrics\nintroduced in the BOP challenge [24].\n4.3. Pose Estimation Comparison\nModel-free. Table 1 presents the comparison results against\nthe state-of-art RGBD methods [20, 26, 50] on YCB-Video\ndataset. The baselines results are adopted from [20]. Fol-\nlowing [20], all methods are given the perturbed ground-\ntruth bounding box as 2D detection for fair comparison. Ta-\nble 2 presents the comparison results on LINEMOD dataset.\nThe baseline results are adopted from [17, 20]. RGB-based\nmethods [17, 37, 51] are given the privilege of much larger\nnumber of reference images to compensate for the lack of\ndepth. Among RGBD methods, FS6D [20] requires fine-\ntuning on the target dataset. Our method significantly out-\nperforms the existing methods on both datasets without\nfine-tuning on the target dataset or ICP refinement.\nFig. 5 visualizes the qualitative comparison. We do not\nhave access to the pose predictions of FS6D [20] for qual-\nitative results, since its code is not publicly released. The\nMethod\nModality\nFinetune-\nRef.\nObjects\nAvg.\nfree\nimages\nape\nbenchwise\ncam\ncan\ncat\ndriller\nduck\neggbox\nglue\nholepuncher\niron\nlamp\nphone\nGen6D [37]\nRGB\n\u2717\n200\n-\n77\n66.1\n-\n60.7\n67.4\n40.5\n95.7\n87.2\n-\n-\n-\n-\n-\nGen6D* [37]\nRGB\n\u2713\n200\n-\n62.1\n45.6\n-\n40.9\n48.8\n16.2\n-\n-\n-\n-\n-\n-\n-\nOnePose [51]\nRGB\n\u2713\n200\n11.8\n92.6\n88.1\n77.2\n47.9\n74.5\n34.2\n71.3\n37.5\n54.9\n89.2\n87.6\n60.6\n63.6\nOnePose++ [17]\nRGB\n\u2713\n200\n31.2\n97.3\n88.0\n89.8\n70.4\n92.5\n42.3\n99.7\n48.0\n69.7\n97.4\n97.8\n76.0\n76.9\nLatentFusion [46]\nRGBD\n\u2713\n16\n88.0\n92.4\n74.4\n88.8\n94.5\n91.7\n68.1\n96.3\n94.9\n82.1\n74.6\n94.7\n91.5\n87.1\nFS6D [20]\nRGBD\n\u2717\n16\n74.0\n86.0\n88.5\n86.0\n98.5\n81.0\n68.5\n100.0\n99.5\n97.0\n92.5\n85.0\n99.0\n88.9\nFS6D [20] + ICP\nRGBD\n\u2717\n16\n78.0\n88.5\n91.0\n89.5\n97.5\n92.0\n75.5\n99.5\n99.5\n96.0\n87.5\n97.0\n97.5\n91.5\nOurs\nRGBD\n\u2713\n16\n99.0\n100.0\n100.0\n100.0\n100.0\n100.0\n99.4\n100.0\n100.0\n99.9\n100.0\n100.0\n100.0\n99.9\nTable 2. Model-free pose estimation results measured by ADD-0.1d on LINEMOD dataset. Gen6D* [37] represents the variation without fine-tuning.\nInput\nOnePose++\nLatentFusion\nOurs\nFigure 5. Qualitative comparison of pose estimation on LINEMOD dataset\nunder the model-free setup. Images are cropped and zoomed-in for better\nvisualization.\nsevere self-occlusion and lack of texture on the glue largely\nchallenge OnePose++ [17] and LatentFusion [46], while\nour method successfully estimates the pose.\nMethod\nUnseen\nDataset\nMean\nobjects\nLM-O\nT-LESS\nYCB-V\nSurfEmb [14] + ICP\n\u2717\n75.8\n82.8\n80.6\n79.7\nOSOP [48] + ICP\n\u2713\n48.2\n-\n57.2\n-\n(PPF, Sift) + Zephyr [43]\n\u2713\n59.8\n-\n51.6\n-\nMegaPose-RGBD [30]\n\u2713\n58.3\n54.3\n63.3\n58.6\nOVE6D [2]\n\u2713\n49.6\n52.3\n-\n-\nGCPose [66]\n\u2713\n65.2\n67.9\n-\n-\nOurs\n\u2713\n78.8\n83.0\n88.0\n83.3\nTable 3. Model-based pose estimation results measured by AR score on\nrepresentative BOP datasets. All methods use the RGBD modality.\nModel-based.\nTable 3 presents the comparison re-\nsults among RGBD methods on 3 core datasets from\nBOP: Occluded-LINEMOD [1], YCB-Video [63] and T-\nLESS [23]. All methods use Mask R-CNN [16] for 2D de-\ntection. Our method outperforms the existing model-based\nmethods that deal with novel objects by a large margin, in-\ncluding the instance-level method [14].\n4.4. Pose Tracking Comparison\nse(3)-\nRGF\nBundle-\nBundle-\nW\u00fcthrich\nOurs\nOurs\u2020\nTrackNet [59]\n[27]\nTrack [58]\nSDF [61]\n[62]\nProperties\nNovel object\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nInitial pose\nGT\nGT\nGT\nGT\nGT\nGT\nEst.\ncracker_box\nADD-S\n94.06\n55.44\n89.41\n90.63\n88.13\n95.10\n94.92\nADD\n90.76\n34.78\n85.07\n85.37\n79.00\n91.32\n91.54\nbleach_cleanser\nADD-S\n94.44\n45.03\n94.72\n94.28\n68.96\n95.96\n96.36\nADD\n89.58\n29.40\n89.34\n87.46\n61.47\n91.45\n92.63\nsugar_box\nADD-S\n94.80\n16.87\n90.22\n93.81\n92.75\n96.67\n96.61\nADD\n92.43\n15.82\n85.56\n88.62\n86.78\n94.14\n93.96\ntomato_soup_can\nADD-S\n96.95\n26.44\n95.13\n95.24\n93.17\n96.58\n96.54\nADD\n93.40\n15.13\n86.00\n83.10\n63.71\n91.71\n91.85\nmustard_bottle\nADD-S\n97.92\n60.17\n95.35\n95.75\n95.31\n97.89\n97.77\nADD\n97.00\n56.49\n92.26\n89.87\n91.31\n96.34\n95.95\nAll\nADD-S\n95.53\n39.90\n92.53\n93.77\n89.18\n96.42\n96.40\nADD\n92.66\n29.98\n87.34\n86.95\n78.28\n93.09\n93.22\nTable 4. Pose tracking results measured by AUC of ADD and ADD-S on\nYCBInEOAT dataset. Ours\u2020 represents our unified pipeline that uses the\npose estimation module for pose initialization.\nUnless otherwise specified, no re-initialization is applied\nto the evaluated methods in the case of tracking lost, in order\nto evaluate long-term tracking robustness. We defer to our\nsupplemental materials for qualitative results.\nTable 5 presents the comparison results of pose track-\ning on YCB-Video [63] dataset.\nAmong the baselines,\nDeepIM [33], se(3)-TrackNet [59] and PoseRBPF [8] need\ntraining on the same object instances, while W\u00fcthrich et\nal. [62], RGF [27], ICG [49] and our method can be in-\nstantly applied to novel objects when provided with a CAD\nmodel.\nSolely evaluating on table-top static scenes does not ex-\npose challenges of abrupt out-of-plane rotations, dynamic\nexternal occlusions and disentangled camera motions [59].\nThus, for more comprehensive comparison, we also evalu-\nate pose tracking methods on the YCBInEOAT [59] dataset\nwhich includes videos of dynamic robotic manipulation.\nResults under the model-based setup are presented in Ta-\nble 4.\nOur method achieves the best performance and\neven outperforms the instance-wise training method [59]\nwith ground-truth pose initialization. Moreover, our uni-\nfied framework also allows for end-to-end pose estimation\nand tracking without external pose initialization, which is\nthe only method with such capability, noted as Ours\u2020 in the\ntable.\n4.5. Analysis\nAblation Study. Table 6 presents the ablation study of crit-\nical design choices. The results are evaluated by AUC of\nADD and ADD-S metrics on the YCB-Video dataset. Ours\n(proposed) is the default version under the model-free (16\nreference images) setup. W/o LLM texture augmentation\nremoves the LLM-aided texture augmentation for synthetic\ntraining. In W/o transformer, we replace the transformer-\nbased architecture by convolutional and linear layers while\nkeeping the similar number of parameters. W/o hierarchical\ncomparison only compares the rendering and the cropped\ninput trained by pose-conditioned triplet loss (Eq. 11) with-\nout two-level hierarchical comparison. At test time, it com-\npares each pose hypothesis with the input observation inde-\npendently and outputs the pose with the highest score. Ex-\nample qualitative result is shown in Fig. 4. Ours-InfoNCE\nreplaces contrast validated pair-wise loss (Eq. 14) by the In-\nfoNCE loss as used in [42].\nEffects of number of reference images. We study how\nthe number of reference images affects the results measured\nApproach\nDeeplM [33]\nse(3)-TrackNet\nPoseRBPF [8]\nW\u00fcthrich [62]\nRGF [27]\nICG [49]\nOurs\nOurs\u2020\n[59]\n+ SDF\nInitial pose\nGT\nGT\nPoseCNN\nGT\nGT\nGT\nGT\nGT\nRe-initialization\nYes (290)\nNo\nYes (2)\nNo\nNo\nNo\nNo\nNo\nNovel object\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nObject setup\nModel-based\nModel-based\nModel-based\nModel-based\nModel-based\nModel-based\nModel-based\nModel-free\nMetric\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\nADD\nADD-S\n002_master_chef_can\n89.0\n93.8\n93.9\n96.3\n89.3\n96.7\n55.6\n90.7\n46.2\n90.2\n66.4\n89.7\n93.6\n97.0\n91.2\n96.9\n003_cracker_box\n88.5\n93.0\n96.5\n97.2\n96.0\n97.1\n96.4\n97.2\n57.0\n72.3\n82.4\n92.1\n96.9\n97.8\n96.2\n97.5\n004_sugar_box\n94.3\n96.3\n97.6\n98.1\n94.0\n96.4\n97.1\n97.9\n50.4\n72.7\n96.1\n98.4\n96.9\n98.2\n94.5\n97.4\n005_tomato_soup_can\n89.1\n93.2\n95.0\n97.2\n87.2\n95.2\n64.7\n89.5\n72.4\n91.6\n73.2\n97.3\n96.3\n98.1\n94.3\n97.9\n006_mustard_bottle\n92.0\n95.1\n95.8\n97.4\n98.3\n98.5\n97.1\n98.0\n87.7\n98.2\n96.2\n98.4\n97.3\n98.4\n97.3\n98.5\n007_tuna_fish_can\n92.0\n96.4\n86.5\n91.1\n86.8\n93.6\n69.1\n93.3\n28.7\n52.9\n73.2\n95.8\n96.9\n98.5\n84.0\n97.8\n008_pudding_box\n80.1\n88.3\n97.9\n98.4\n60.9\n87.1\n96.8\n97.9\n12.7\n18.0\n73.8\n88.9\n97.8\n98.5\n96.9\n98.5\n009_gelatin_box\n92.0\n94.4\n97.8\n98.4\n98.2\n98.6\n97.5\n98.4\n49.1\n70.7\n97.2\n98.8\n97.7\n98.5\n97.6\n98.5\n010_potted_meat_can\n78.0\n88.9\n77.8\n84.2\n76.4\n83.5\n83.7\n86.7\n44.1\n45.6\n93.3\n97.3\n95.1\n97.7\n94.8\n97.5\n011_banana\n81.0\n90.5\n94.9\n97.2\n92.8\n97.7\n86.3\n96.1\n93.3\n97.7\n95.6\n98.4\n96.4\n98.4\n95.6\n98.1\n019_pitcher_base\n90.4\n94.7\n96.8\n97.5\n97.7\n98.1\n97.3\n97.7\n97.9\n98.2\n97.0\n98.8\n96.7\n98.0\n96.8\n98.0\n021_bleach_cleanser\n81.7\n90.5\n95.9\n97.2\n95.9\n97.0\n95.2\n97.2\n95.9\n97.3\n92.6\n97.5\n95.5\n97.8\n94.7\n97.5\n024_bowl\n38.8\n90.6\n80.9\n94.5\n34.0\n93.0\n30.4\n97.2\n24.2\n82.4\n74.4\n98.4\n95.2\n97.6\n90.5\n95.3\n025_mug\n83.2\n92.0\n91.5\n96.9\n86.9\n96.7\n83.2\n93.3\n60.0\n71.2\n95.6\n98.5\n95.6\n97.9\n91.5\n96.1\n035_power_drill\n85.4\n92.3\n96.4\n97.4\n97.8\n98.2\n97.1\n97.8\n97.9\n98.3\n96.7\n98.5\n96.9\n98.2\n96.3\n97.9\n036_wood_block\n44.3\n75.4\n95.2\n96.7\n37.8\n93.6\n95.5\n96.9\n45.7\n62.5\n93.5\n97.2\n93.2\n97.0\n92.9\n97.0\n037_scissors\n70.3\n84.5\n95.7\n97s\n72.7\n85.5\n4.2\n16.2\n20.9\n38.6\n93.5\n97.3\n94.8\n97.5\n95.5\n97.8\n040_large_marker\n80.4\n91.2\n92.2\n96.0\n89.2\n97.3\n35.6\n53.0\n12.2\n18.9\n88.5\n97.8\n96.9\n98.6\n96.6\n98.6\n051_large_clamp\n73.9\n84.1\n94.7\n96.9\n90.1\n95.5\n61.2\n72.3\n62.8\n80.1\n91.8\n96.9\n93.6\n97.3\n92.5\n96.7\n052_extra_large_clamp\n49.3\n90.3\n91.7\n95.8\n84.4\n94.1\n93.7\n96.6\n67.5\n69.7\n85.9\n94.3\n94.4\n97.5\n93.4\n97.3\n061_foam_brick\n91.6\n95.5\n93.7\n96.7\n96.1\n98.3\n96.8\n98.1\n70.0\n86.5\n96.2\n98.5\n97.9\n98.6\n96.8\n98.3\nAll Frames\n82.3\n91.9\n93.0\n95.7\n87.5\n95.2\n78.0\n90.2\n59.2\n74.3\n86.4\n96.5\n96.0\n97.9\n93.7\n97.5\nTable 5. Pose tracking results measured by AUC of ADD and ADD-S on YCB-Video dataset. Ours\u2020 represents our method under the model-free setup with\nreference images.\nADD\nADD-S\nOurs (proposed)\n91.52\n97.40\nW/o LLM texture augmentation\n90.83\n97.38\nW/o transformer\n90.77\n97.33\nW/o hierarchical comparison\n89.05\n96.67\nOurs-InfoNCE\n89.39\n97.29\nTable 6. Ablation study of critical design choices.\nby AUC of ADD and ADD-S on YCB-Video dataset, as\nshown in Fig. 6. Overall, our method is robust to the num-\nber of reference images especially on the ADD-S metric,\nand saturates at 12 images for both metrics. Notably, even\nwhen only 4 reference images are provided, our method still\nyields stronger performance than FS6D [20] equipped with\n16 reference images (Table 1).\nTraining data scaling law. Theoretically, an unbounded\namount of synthetic data can be produced for training.\nFig. 7 presents how the amount of training data affects the\nresults measured by AUC of ADD and ADD-S metrics on\nYCB-Video dataset. The gain saturates around 1M.\nRunning time. We measure the running time on the hard-\n60.00\n65.00\n70.00\n75.00\n80.00\n85.00\n90.00\n95.00\n100.00\n3\n5\n7\n9\n11\n13\n15\n17\nADD\nADD-S\nNum\u00a0reference\u00a0images\nAUC (%)\n93.00\n95.00\n97.00\n99.00\nC\u00a0(%)\nFigure 6. Effects of number of reference images.\nA\u00a0traditional\u00a0wooden\u00a0\narmoire\u00a0in\u00a0a\u00a0rich\u00a0mahogany\u00a0\nfinish,\u00a0showcasing\u00a0\nintricate\u00a0carvings\u00a0and\u00a0\nbrass\u00a0hardware\u00a0for\u00a0an\u00a0\nelegant\u00a0look\nA\u00a0vibrant\u00a0red\u00a0bulb\u00a0with\u00a0a\u00a0\ngradient\u00a0of\u00a0orange\u00a0and\u00a0\nyellow,\u00a0emitting\u00a0a\u00a0warm\u00a0glow\nRandom texture blending\nA\u00a0unique\u00a0wineglass\u00a0with\u00a0a\u00a0\nstem\u00a0shaped\u00a0like\u00a0a\u00a0\ncorkscrew,\u00a0showcasing\u00a0a\u00a0\nbowl\u00a0made\u00a0of\u00a0hand\u2010blown\u00a0\nglass\u00a0in\u00a0a\u00a0mix\u00a0of\u00a0swirling\u00a0\nred\u00a0and\u00a0white\n60.00\n65.00\n70.00\n75.00\n80.00\n85.00\n90.00\n95.00\n100.00\n3\n5\n7\n9\n11\n13\n15\n17\nADD\nADD-S\nNum\u00a0reference\u00a0images\nAUC (%)\nAn\u00a0artistic\u00a0wineglass\u00a0\nhand\u2010painted\u00a0with\u00a0vibrant\u00a0\nstrokes\u00a0of\u00a0brown,\u00a0blue,\u00a0\nand\u00a0green,\u00a0creating\u00a0a\u00a0\nstriking\u00a0abstract\u00a0design\n85.00\n87.00\n89.00\n91.00\n93.00\n95.00\n97.00\n99.00\n3.5\n4\n4.5\n5\n5.5\n6\nADD\nADD-S\nTrain\u00a0data\u00a0size\u00a0(log10)\nAUC\u00a0(%)\nFigure 7. Effects of training data size.\nware of Intel i9-10980XE CPU and NVIDIA RTX 3090\nGPU. The pose estimation takes about 1.3 s for one ob-\nject, where pose initialization takes 4 ms, refinement takes\n0.88 s, pose selection takes 0.42 s. Tracking runs much\nfaster at \u223c32 Hz, since only pose refinement is needed and\nthere are not multiple pose hypotheses.\nIn practice, we\ncan run pose estimation once for initialization and switch\nto tracking mode for real-time performance.\n5. Conclusion\nWe present a unified foundation model for 6D pose estima-\ntion and tracking of novel objects, supporting both model-\nbased and model-free setups.\nExtensive experiments on\nthe combinations of 4 different tasks indicate it is not only\nversatile but also outperforms existing state-of-art methods\nspecially designed for each task by a considerable margin.\nIt even achieves comparable results to those methods requir-\ning instance-level training. In future work, exploring state\nestimation beyond single rigid object will be of interest.\nReferences\n[1] Eric Brachmann, Alexander Krull, Frank Michel, Stefan\nGumhold, Jamie Shotton, and Carsten Rother. Learning 6D\nobject pose estimation using 3d object coordinates. In 13th\nEuropean Conference on Computer Vision (ECCV), pages\n536\u2013551, 2014. 6, 7\n[2] Dingding Cai, Janne Heikkil\u00e4, and Esa Rahtu. OVE6D: Ob-\nject viewpoint encoding for depth-based 6D object pose es-\ntimation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n6803\u20136813, 2022. 5, 7\n[3] Ming Cai and Ian Reid. Reconstruct locally, localize glob-\nally: A model free method for object pose estimation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 3153\u20133163, 2020. 2\n[4] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,\nand Kangxue Yin.\nTexFusion: Synthesizing 3D textures\nwith text-guided image diffusion models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 4169\u20134181, 2023. 2, 3\n[5] Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu. Learn-\ning canonical shape space for category-level 6D object pose\nand size estimation.\nIn Proceedings of the IEEE Inter-\nnational Conference on Computer Vision (CVPR), pages\n11973\u201311982, 2020. 1, 2\n[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3D objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 13142\u201313153, 2023. 2\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical im-\nage database. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n248\u2013255, 2009. 3\n[8] Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timo-\nthy Bretl, and Dieter Fox. PoseRBPF: A Rao-Blackwellized\nparticle filter for 6D object pose tracking. In Robotics: Sci-\nence and Systems (RSS), 2019. 1, 2, 7, 8\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International Con-\nference on Learning Representations (ICLR), 2021. 5\n[10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3D scanned household items.\nIn Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560, 2022. 2\n[11] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. In International Conference on Machine Learning\n(ICML), pages 3789\u20133799, 2020. 4\n[12] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLVIS: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5356\u20135364, 2019. 2\n[13] John C Hart. Sphere tracing: A geometric method for the\nantialiased ray tracing of implicit surfaces. The Visual Com-\nputer, 12(10):527\u2013545, 1996. 5\n[14] Rasmus Laurvig Haugaard and Anders Glent Buch.\nSur-\nfemb: Dense and continuous correspondence distributions\nfor object pose estimation with learnt surface embeddings.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 6749\u20136758,\n2022. 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 770\u2013778, 2016. 5\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir-\nshick.\nMask R-CNN.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 2961\u20132969, 2017. 7\n[17] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun\nBao, and Xiaowei Zhou. OnePose++: Keypoint-free one-\nshot object pose estimation without CAD models. Advances\nin Neural Information Processing Systems (NeurIPS), 35:\n35103\u201335115, 2022. 1, 2, 6, 7\n[18] Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang\nFan, and Jian Sun. PVN3D: A deep point-wise 3D keypoints\nvoting network for 6DoF pose estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11632\u201311641, 2020. 1, 2\n[19] Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and\nJian Sun. FFB6D: A full flow bidirectional fusion network\nfor 6D pose estimation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 3003\u20133013, 2021. 1, 2\n[20] Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, and Qifeng\nChen. FS6D: Few-shot 6D pose estimation of novel objects.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 6814\u20136824,\n2022. 2, 3, 6, 7, 8\n[21] Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobo-\ndan Ilic, Kurt Konolige, Nassir Navab, and Vincent Lepetit.\nMultimodal templates for real-time detection of texture-less\nobjects in heavily cluttered scenes. In International Confer-\nence on Computer Vision (ICCV), pages 858\u2013865, 2011. 6\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems (NeurIPS), 33:6840\u20136851, 2020. 2\n[23] Tom\u00e1\u0161 Hodan, Pavel Haluza, \u0160tep\u00e1n Obdr\u017e\u00e1lek, Jiri Matas,\nManolis Lourakis, and Xenophon Zabulis.\nT-LESS: An\nRGB-D dataset for 6D pose estimation of texture-less ob-\njects. In IEEE Winter Conference on Applications of Com-\nputer Vision (WACV), pages 880\u2013888, 2017. 6, 7\n[24] Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl,\nAnders GlentBuch, Dirk Kraft, Bertram Drost, Joel Vidal,\nStephan Ihrke, Xenophon Zabulis, et al. BOP: Benchmark\nfor 6D object pose estimation. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pages 19\u201334,\n2018. 2, 6\n[25] Elad Hoffer and Nir Ailon. Deep metric learning using triplet\nnetwork.\nIn Third International Workshop on Similarity-\nBased Pattern Recognition (SIMBAD), pages 84\u201392, 2015.\n6\n[26] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas\nWieser, and Konrad Schindler.\nPREDATOR: Registration\nof 3D point clouds with low overlap.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4267\u20134276, 2021. 6\n[27] Jan Issac, Manuel W\u00fcthrich, Cristina Garcia Cifuentes, Jean-\nnette Bohg, Sebastian Trimpe, and Stefan Schaal. Depth-\nbased object tracking using a robust gaussian filter.\nIn\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 608\u2013615, 2016. 1, 2, 7, 8\n[28] Daniel Kappler, Franziska Meier, Jan Issac, Jim Main-\nprice, Cristina Garcia Cifuentes, Manuel W\u00fcthrich, Vin-\ncent Berenz, Stefan Schaal, Nathan Ratliff, and Jeannette\nBohg. Real-time perception meets reactive motion gener-\nation. IEEE Robotics and Automation Letters, 3(3):1864\u2013\n1871, 2018. 1\n[29] Yann Labb\u00e9, Justin Carpentier, Mathieu Aubry, and Josef\nSivic.\nCosyPose: Consistent multi-view multi-object 6D\npose estimation. In European Conference on Computer Vi-\nsion (ECCV), pages 574\u2013591, 2020. 1, 2\n[30] Yann Labb\u00e9, Lucas Manuelli, Arsalan Mousavian, Stephen\nTyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier,\nMathieu Aubry, Dieter Fox, and Josef Sivic. MegaPose: 6D\npose estimation of novel objects via render & compare. In\n6th Annual Conference on Robot Learning (CoRL), 2022. 1,\n2, 5, 7\n[31] Taeyeop Lee, Jonathan Tremblay, Valts Blukis, Bowen Wen,\nByeong-Uk Lee, Inkyu Shin, Stan Birchfield, In So Kweon,\nand Kuk-Jin Yoon.\nTTA-COPE: Test-time adaptation for\ncategory-level object pose estimation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 21285\u201321295, 2023. 1, 2\n[32] Fu Li, Shishir Reddy Vutukur, Hao Yu, Ivan Shugurov, Ben-\njamin Busam, Shaowu Yang, and Slobodan Ilic.\nNeRF-\nPose: A first-reconstruct-then-regress approach for weakly-\nsupervised 6D object pose estimation.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 2123\u20132133, 2023. 2\n[33] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.\nDeepIM: Deep iterative matching for 6D pose estimation. In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 683\u2013698, 2018. 1, 2, 7, 8\n[34] Zhigang Li,\nGu Wang,\nand Xiangyang Ji.\nCDPN:\nCoordinates-based disentangled pose network for real-time\nRGB-based 6-DoF object pose estimation. In CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 7677\u2013\n7686, 2019. 2\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick.\nMicrosoft COCO: Common objects in context.\nIn 13th European Conference on Computer Vision (ECCV),\npages 740\u2013755, 2014. 3\n[36] Yunzhi Lin, Jonathan Tremblay, Stephen Tyree, Patricio A\nVela, and Stan Birchfield. Keypoint-based category-level ob-\nject pose tracking from an RGB sequence with uncertainty\nestimation. In International Conference on Robotics and Au-\ntomation (ICRA), 2022. 1, 2\n[37] Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long,\nTaku Komura, and Wenping Wang. Gen6D: Generalizable\nmodel-free 6-DoF object pose estimation from RGB images.\nECCV, 2022. 1, 2, 6, 7\n[38] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 4\n[39] Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.\nPose estimation for augmented reality: A hands-on survey.\nIEEE Transactions on Visualization and Computer Graphics\n(TVCG), 22(12):2633\u20132651, 2015. 1\n[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n4\n[41] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 4\n[42] Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salz-\nmann, and Vincent Lepetit. Templates for 3D object pose es-\ntimation revisited: Generalization to new objects and robust-\nness to occlusions. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 6771\u20136780, 2022. 5, 6, 7\n[43] Brian Okorn, Qiao Gu, Martial Hebert, and David Held.\nZephyr: Zero-shot pose hypothesis rating. In IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 14141\u201314148, 2021. 7\n[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 6\n[45] Kiru Park, Timothy Patten, and Markus Vincze. Pix2Pose:\nPixel-wise coordinate regression of objects for 6D pose es-\ntimation.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 7668\u20137677,\n2019. 1, 2\n[46] Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter\nFox. LatentFusion: End-to-end differentiable reconstruction\nand rendering for unseen object pose estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10710\u201310719, 2020. 2,\n7\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 2\n[48] Ivan Shugurov, Fu Li, Benjamin Busam, and Slobodan Ilic.\nOSOP: A multi-stage one shot object pose estimation frame-\nwork. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 6835\u2013\n6844, 2022. 1, 2, 7\n[49] Manuel Stoiber, Martin Sundermeyer, and Rudolph Triebel.\nIterative corresponding geometry: Fusing region and depth\nfor highly efficient 3D tracking of textureless objects. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 6855\u20136865, 2022.\n1, 2, 7, 8\n[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-free local feature matching\nwith transformers. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 8922\u20138931,\n2021. 6\n[51] Jiaming Sun,\nZihao Wang,\nSiyu Zhang,\nXingyi He,\nHongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.\nOnePose: One-shot object pose estimation without CAD\nmodels.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n6825\u20136834, 2022. 1, 2, 6, 7\n[52] Meng Tian, Marcelo H Ang, and Gim Hee Lee. Shape prior\ndeformation for categorical 6D object pose and size estima-\ntion. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 530\u2013546, 2020. 1, 2\n[53] Jonathan Tremblay, Thang To, Balakumar Sundaralingam,\nYu Xiang, Dieter Fox, and Stan Birchfield. Deep object pose\nestimation for semantic robotic grasping of household ob-\njects. In Conference on Robot Learning (CoRL), pages 306\u2013\n316, 2018. 2\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems (NeurIPS), 30, 2017. 5, 6\n[55] Chen Wang, Roberto Mart\u00edn-Mart\u00edn, Danfei Xu, Jun Lv,\nCewu Lu, Li Fei-Fei, Silvio Savarese, and Yuke Zhu.\n6-\nPACK: Category-level 6D pose tracker with anchor-based\nkeypoints. In IEEE International Conference on Robotics\nand Automation (ICRA), pages 10059\u201310066, 2020. 1, 2\n[56] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,\nShuran Song, and Leonidas J Guibas.\nNormalized object\ncoordinate space for category-level 6D object pose and size\nestimation. In Proceedings of the IEEE International Confer-\nence on Computer Vision (CVPR), pages 2642\u20132651, 2019.\n1, 2\n[57] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction.\nIn Advances in Neural Information Processing\nSystems (NeurIPS), 2021. 4\n[58] Bowen Wen and Kostas Bekris. BundleTrack: 6D pose track-\ning for novel objects without instance or category-level 3D\nmodels. In IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), pages 8067\u20138074, 2021. 2,\n7\n[59] Bowen Wen, Chaitanya Mitash, Baozhang Ren, and Kostas E\nBekris.\nse(3)-TrackNet:\nData-driven 6D pose tracking\nby calibrating image residuals in synthetic domains.\nIn\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 10367\u201310373, 2020. 1, 2, 6, 7, 8\n[60] Bowen Wen, Wenzhao Lian, Kostas Bekris, and Stefan\nSchaal.\nCatGrasp: Learning category-level task-relevant\ngrasping in clutter from simulation. In International Confer-\nence on Robotics and Automation (ICRA), pages 6401\u20136408,\n2022. 1\n[61] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen\nTyree, Thomas M\u00fcller, Alex Evans, Dieter Fox, Jan Kautz,\nand Stan Birchfield. BundleSDF: Neural 6-DoF tracking and\n3D reconstruction of unknown objects. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 606\u2013617, 2023. 2, 4, 7\n[62] Manuel W\u00fcthrich, Peter Pastor, Mrinal Kalakrishnan, Jean-\nnette Bohg, and Stefan Schaal. Probabilistic object tracking\nusing a range camera. In IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 3195\u20133202,\n2013. 1, 2, 7, 8\n[63] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and\nDieter Fox. PoseCNN: A convolutional neural network for\n6D object pose estimation in cluttered scenes. In Robotics:\nScience and Systems (RSS), 2018. 2, 6, 7\n[64] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance. Advances in Neural Information Processing Sys-\ntems (NeurIPS), 33:2492\u20132502, 2020. 4\n[65] Ruida Zhang, Yan Di, Fabian Manhardt, Federico Tombari,\nand Xiangyang Ji. SSP-Pose: Symmetry-aware shape prior\ndeformation for direct category-level object pose estimation.\nIn IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 7452\u20137459, 2022. 1, 2\n[66] Heng Zhao, Shenxing Wei, Dahu Shi, Wenming Tan,\nZheyang Li, Ye Ren, Xing Wei, Yi Yang, and Shiliang Pu.\nLearning symmetry-aware geometry correspondences for 6D\nobject pose estimation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n14045\u201314054, 2023. 7\n[67] Linfang Zheng, Chen Wang, Yinghan Sun, Esha Dasgupta,\nHua Chen, Ale\u0161 Leonardis, Wei Zhang, and Hyung Jin\nChang.\nHS-Pose:\nHybrid scope feature extraction for\ncategory-level object pose estimation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 17163\u201317173, 2023. 2\n[68] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and\nHao Li. On the continuity of rotation representations in neu-\nral networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5745\u20135753, 2019. 5\n"
  },
  {
    "title": "ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2312.08136.pdf",
    "upvote": "2",
    "text": "ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained\nImplicit Neural Radiance Fields\nJuan Luis Gonzalez Bello*, Minh-Quan Viet Bui*, Munchurl Kim\nKorea Advanced Institute of Science and Technology (KAIST), South Korea\nAbstract\nRecent advances in neural rendering have shown that, albeit\nslow, implicit compact models can learn a scene\u2019s geome-\ntries and view-dependent appearances from multiple views.\nTo maintain such a small memory footprint but achieve faster\ninference times, recent works have adopted \u2018sampler\u2019 net-\nworks that adaptively sample a small subset of points along\neach ray in the implicit neural radiance fields. Although these\nmethods achieve up to a 10\u00d7 reduction in rendering time,\nthey still suffer from considerable quality degradation com-\npared to the vanilla NeRF. In contrast, we propose ProNeRF,\nwhich provides an optimal trade-off between memory foot-\nprint (similar to NeRF), speed (faster than HyperReel), and\nquality (better than K-Planes). ProNeRF is equipped with\na novel projection-aware sampling (PAS) network together\nwith a new training strategy for ray exploration and exploita-\ntion, allowing for efficient fine-grained particle sampling. Our\nProNeRF yields state-of-the-art metrics, being 15-23\u00d7 faster\nwith 0.65dB higher PSNR than NeRF and yielding 0.95dB\nhigher PSNR than the best published sampler-based method,\nHyperReel. Our exploration and exploitation training strat-\negy allows ProNeRF to learn the full scenes\u2019 color and den-\nsity distributions while also learning efficient ray sampling\nfocused on the highest-density regions. We provide exten-\nsive experimental results that support the effectiveness of\nour method on the widely adopted forward-facing and 360\ndatasets, LLFF and Blender, respectively.\n1\nIntroduction\nNeural radiance fields (NeRFs) (Mildenhall et al. 2020) have\ngained significant attention in the computer vision commu-\nnity due to their greater ability to compactly represent com-\nplex scenes\u2019 3D geometries and view-dependent specular-\nity, in comparison with other implicit representations (Flynn\net al. 2019; Sitzmann et al. 2020). The efficacy of NeRFs\ncan be attributed to several key features such as: (i) the vol-\numetric rendering technique (Drebin, Carpenter, and Han-\nrahan 1988), which aggregates estimated RGB-density val-\nues along rendering rays, (ii) their implicit representation by\na multi-layer perception (MLP) network that incorporates\npositional encoding (Mildenhall et al. 2020), and (iii) their\ncoarse-to-fine rendering strategy that enables dense fine-\ngrained ray sampling for high-quality rendering.\n*These authors contributed equally.\nFigure 1: Performance trade-off of neural rendering (mem-\nory, speed, quality) on the LLFF dataset.\nAlthough NeRFs offer a compact representation of 3D\ngeometry and view-dependent effects, there is still signifi-\ncant room for improvement in rendering quality and infer-\nence times. To speed up the rendering times, recent trends\nhave explored caching diffuse color estimation into an ex-\nplicit voxel-based structure (Yu et al. 2021a; Hedman et al.\n2021; Garbin et al. 2021; Hu et al. 2022) or leveraging tex-\nture features stored in an explicit representation such as hash\ngirds (M\u00a8uller et al. 2022), meshes (Chen et al. 2023), or 3D\nGaussians (Kerbl et al. 2023). While these methods achieve\nSOTA results on object-centric 360 datasets, they underper-\nform for the forward-facing scene cases and require consid-\nerably larger memory footprints than NeRF.\nIn a different line of work, the prior literature of (Neff\net al. 2021; Piala and Clark 2021; Lin et al. 2022; Kurz et al.\n2022; Attal et al. 2023) has proposed training single-pass\nlightweight \u201csampler\u201d networks, aimed to reduce the num-\nber of ray samples required for volumetric rendering. Al-\nthough fast and memory compact, previous sampler-based\nmethods often fall short in rendering quality compared to\nthe computationally expensive vanilla NeRF.\nIn contrast, our proposed method with a Projection-Aware\nSampling (PAS) network and an exploration-exploitation\ntraining strategy, denoted as \u201cProNeRF,\u201d greatly reduces the\ninference times while simultaneously achieving superior im-\nage quality and more details than the current high-quality\nmethods (Chen et al. 2022; Sara Fridovich-Keil and Gi-\narXiv:2312.08136v1  [cs.CV]  13 Dec 2023\nacomo Meanti et al. 2023). In conjunction with its small\nmemory footprint (as small as NeRF), our ProNeRF yields\nthe best performance profiling (memory, speed, quality)\ntrade-off. Our main contributions are as follows1:\n\u2022 Faster rendering times. Our ProNeRF leverages multi-\nview color-to-ray projections to yield a few precise 3D\nquery points, allowing up to 23\u00d7 faster inference times\nthan vanilla NeRF under a similar memory footprint.\n\u2022 Higher rendering quality. Our proposed PAS and\nexploration-exploitation training strategy allow for sparse\nfine-grained ray sampling in an end-to-end manner, yield-\ning rendered images with improved quality metrics com-\npared to the implicit baseline NeRF.\n\u2022 Comprehensive experimental validation. The robust-\nness of ProNeRF is extensively evaluated on forward-\nfacing and 360 object-centric multi-view datasets. Specif-\nically, in the context of forward-facing scenes, ProNeRF\nestablishes SOTA renders, outperforming implicit and ex-\nplicit radiance fields, including NeRF, TensoRF, and K-\nPlanes with a considerably more optimal performance\nprofile in terms of memory, speed, and quality.\n2\nRelated Work\nThe most relevant works concerning our proposed method\nfocus on maintaining the compactness of implicit NeRFs\nwhile reducing the rendering times by learning sampling\nnetworks for efficient ray querying.\nNevertheless, other works leverage data structures for\nbaking radiance fields, that is, caching diffuse color and\nlatent view-dependent features from a pre-trained NeRF\nto accelerate the rendering pipelines (as in SNeRG (Hed-\nman et al. 2021)). Similarly, Yu et al. (2021a) proposed\nPlenoctrees to store spatial densities and spherical harmon-\nics (SH) coefficients for fast rendering. Subsequently, to re-\nduce the redundant computation in empty space, Plenoxels\n(Fridovich-Keil et al. 2022) learns a sparse voxel grid of SH\ncoefficients. On the other hand, Efficient-NeRF (Hu et al.\n2022) presents an innovative caching representation referred\nto as \u201cNeRF-tree,\u201d enhancing caching efficiency and render-\ning performance. However, these approaches require a pre-\ntrained NeRF and a considerably larger memory footprint to\nstore their corresponding scene representations.\nExplicit data structures have also been used for storing la-\ntent textures in explicit texture radiance fields to speed up\nthe training and inference times. Particularly, INGP (M\u00a8uller\net al. 2022) proposes quickly estimating the radiance val-\nues by interpolating latent features stored in multi-scaled\nhash grids. Drawing inspiration from tensorial decomposi-\ntion, in TensoRF, Chen et al. (2022) factorize the scene\u2019s\nradiance field into multiple low-rank latent tensor compo-\nnents. Following a similar decomposition principle, Sara\nFridovich-Keil and Giacomo Meanti et al. (2023) introduced\nK-Planes for multi-plane decomposition of 3D scenes. Re-\ncently, MobileNeRF (Chen et al. 2023) and 3DGS (Kerbl\net al. 2023) concurrently propose merging the rasterization\nprocess with explicit meshes or 3D Gaussians for real-time\n1Visit our project website at https://kaist-viclab.github.io/\npronerf-site/\nrendering. Similar to the baked radiance fields, MobileN-\neRF and 3DGS demonstrate the capability to achieve incred-\nibly rapid rendering, up to several hundred frames per sec-\nond. However, they demand a considerably elevated mem-\nory footprint, which might be inappropriate in resource-\nconstrained scenarios where real-time swapping of neural\nradiance fields is required, such as streaming, as discussed\nby Kurz et al. (2022).\nInspired by the concept proposed in (Levoy and Hanra-\nhan 1996), recent studies have also explored the learning\nof neural light fields which only require a single network\nevaluation for each casted ray. Light field networks such as\nLFNR (Suhail et al. 2022b) and GPNR (Suhail et al. 2022a)\npresently exhibit optimal rendering performance across di-\nverse novel view synthesis datasets. Nevertheless, they adopt\nexpensive computational attention operations for aggregat-\ning multi-view projected features. Additionally, it\u2019s worth\nnoting that similar to generalizable radiance fields (e.g.,\nIBRNet (Wang et al. 2021), or NeuRay (Liu et al. 2022)),\nLFNR and GPNR necessitate the storage of all training input\nimages for epipolar feature projection, leading to increased\nmemory requirements. Conversely, our method, ProNeRF,\nleverages color-to-ray projections while guaranteeing con-\nsistent memory footprints by robustly managing a small and\nfixed subset of reference views for rendering any novel view\nin the target scene. This eliminates the necessity for nearest-\nneighbor projection among all available training views in\neach novel scene. To balance computational cost and render-\ning quality for neural light fields, RSEN (Attal et al. 2022)\nintroduces a novel ray parameterization and space subdi-\nvision structure of the 3D scenes. On the other hand, R2L\n(Wang et al. 2022) distills a compact neural light field with\na pre-trained NeRF. Although R2L achieves better inference\ntime and quality than RSEN, it necessitates the generation\nof numerous pseudo-images from a pre-trained NeRF to per-\nform exhaustive training on dense pseudo-data. This process\ncan extend over days of optimization.\nIn addition to IBRNet and NeuRay, other generalizable\nradiance fields have also been explored in (Yu et al. 2021b;\nLi et al. 2021), but are less relevant to our work.\nLearning sampling networks. In AutoInt, Lindell, Mar-\ntel, and Wetzstein (2021) propose to train anti-derivative net-\nworks that describe the piece-wise color and density inte-\ngrals of discrete ray segments whose distances are individ-\nually estimated by a sampler network. In DONeRF (Neff\net al. 2021) and TermiNeRF (Piala and Clark 2021), the\ncoarse NeRF in vanilla NeRF is replaced with a sampling\nnetwork that learns to predict the depth of objects\u2019 surfaces\nusing either depth ground truth (GT) or dense depths from a\npre-trained NeRF. The requirement of hard-to-obtain dense\ndepths severely limits DONeRF and TermiNeRF for broader\napplications. ENeRF (Lin et al. 2022) learns to estimate the\ndepth distribution from multi-view images in an end-to-end\nmanner. In particular, ENeRF adopts cost-volume aggrega-\ntion and 3D CNNs to enhance geometry prediction.\nInstead of predicting a continuous depth distribution,\nAdaNeRF (Kurz et al. 2022) proposes a sampler network\nthat maps rays to fixed and discretized distance probabilities.\nDuring test, only the samples with the highest probabilities\nare fed into the shader (NeRF) network for volumetric ren-\ndering. AdaNeRF is trained in a dense-to-sparse multi-stage\nmanner without needing a pre-trained NeRF. The shader is\nfirst trained with computationally expensive dense sampling\npoints, where sparsification is later introduced to prune in-\nsignificant samples, and then followed by simultaneous sam-\npling and shading network fine-tuning. In MipNeRF360,\nBarron et al. (2022) introduce online distillation to train\nthe sampling network. Nevertheless, the sampler utilized in\nMipNeRF360 remains structured as a radiance field, neces-\nsitating a per-point forward pass. Consequently, incorporat-\ning this sampler does not yield substantial improvements in\nrendering latency. On the other hand, in the recent work of\nHyperReel, Attal et al. (2023) proposed a sampling network\nfor learning the geometry primitives in grid-based render-\ning models such as TensoRF. HyperReel inherits the fast-\ntraining properties of TensoRF but also yields limited ren-\ndering quality with a considerably increased memory foot-\nprint compared to the vanilla NeRF.\nContrary to the existing literature, we present a sampler-\nbased method, ProNeRF, that allows for fast neural ren-\ndering while substantially outperforming the implicit and\nexplicit NeRFs quantitatively and qualitatively in recon-\nstructing forward-facing captured scenes. The main com-\nponents of ProNeRF are a novel PAS network and a new\nlearning strategy that borrows from the reinforcement learn-\ning concepts of exploration and exploitation. Moreover,\nall the previous sampler-based methods require either pre-\ntrained NeRFs (TermiNeRF), depth GTs (DoNeRF), com-\nplex dense-ray sampling and multi-stage training strategies\n(AdaNeRF), or large memory footprint (HyperReel). In con-\ntrast, our proposed method can more effectively learn the\nneural rendering in an end-to-end manner from sparse rays,\neven with shorter training cycles than NeRF.\n3\nProposed Method\nFig. 2 depicts a high-level overview of our ProNeRF, which\nis equipped with a projection-aware sampling (PAS) net-\nwork and a shader network (a.k.a NeRF) for few-point vol-\numetric rendering. ProNeRF performs PAS in a coarse-to-\nfine manner. First, for a given target ray, ProNeRF maps the\nray direction and origin into coarse sampling points with the\nhelp of an MLP head (F\u03b8c). By tracing lines from these sam-\npling points into the camera centers of the reference views\nin the training set, ProNeRF performs a color-to-ray projec-\ntion which is aggregated to the coarse sampling points and\nis processed in a second MLP head (F\u03b8f ). F\u03b8f then outputs\nthe refined 3D points that are fed into the shading network\n(F\u03b8s) for the further volumetric rendering of the ray color \u02c6c.\nSee Section 3.2 for more details.\nTraining a ProNeRF as depicted in Fig. 2 is not a trivial\ntask, as the implicit shader needs to learn the full color and\ndensity distributions in the scenes while the PAS network\ntries to predict ray points that focus on specific regions with\nthe highest densities. Previous works, such as DoNERF, Ter-\nmiNeRF, and AdaNeRF go around this problem at the ex-\npense of requiring depth GTs, pre-trained NeRF models, or\nexpensive dense sampling. To overcome this issue, we pro-\npose an alternating learning strategy that borrows from rein-\nforcement learning which (i) allows the shading network to\nexplore the scene\u2019s rays and learn the full scene distributions\nand (ii) leads the PAS network to exploit the ray samples\nwith the highest densities. See Section 3.3 for more details.\n3.1\nPAS-Guided Volumetric Rendering\nVolumetric rendering synthesizes images by traversing the\nrays that originate in the target view camera center into a\n3D volume of color and densities. As noted by Mildenhall\net al. (2020), the continuous volumetric rendering equation\n(VRE) of a ray color c(r) can be efficiently approximated\nby alpha compositing, which is expressed as:\n\u02c6c(r) = PN\ni=1\n\u0000Qi\u22121\nj=11 \u2212 \u03b1j\n\u0001\n\u03b1ici,\n(1)\nwhere N is the total number of sampling points and \u03b1i de-\nnotes the opacity at the ith sample in ray r as given by\n\u03b1i = 1 \u2212 e\u2212\u03c3i(ti+1\u2212ti).\n(2)\nHere, \u03c3i and ci respectively indicate the density and colors\nat the 3D location given by r(ti) for the ith sampling point\non r. A point on r in distance t is r(t) = ro + rdt where ro\nand rd are the ray origin and direction, respectively.\nIn NeRF (Mildenhall et al. 2020), a large number of N\nsamples along the ray is considered to precisely approxi-\nmate the original integral version of the VRE. In contrast,\nour objective is to perform high-quality volumetric render-\ning with a smaller number of samples Ns << N. Rendering\na ray with a few samples in our ProNeRF can be possible by\naccurately sampling the 3D particles with the highest den-\nsities along the ray. Thanks to the PAS, our ProNeRF can\nyield a sparse set of accurate sampling distances, denoted as\nT = {t1, t2, ..., tNs}, by which the shading network F\u03b8s is\nqueried for each point corresponding to the ray distances in\nT (along with rd) to obtain ci and \u03c3i as\n[ci, \u03c3i] = F\u03b8s(r(ti), rd).\n(3)\nFurthermore, similar to AdaNeRF, our ProNeRF adjusts the\nfinal sample opacities \u03b1i, which allows for fewer-sample\nrendering and back-propagation during training. However,\nunlike the AdaNeRF that re-scales the sample densities, we\nshift and scale the \u03b1 values in our ProNeRF, yielding \u02c6\u03b1:\n\u02c6\u03b1i = ai(1 \u2212 e\u2212(\u03c3i+bi)(ti+1\u2212ti)),\n(4)\nwhere ai and bi are estimated by the PAS network as At =\n{a1, a2, ..., aNs} and Bt = {b1, b2, ..., bNs}. We then render\nthe final ray color in our PAS-guided VRE according to\n\u02c6c(r) = PNs\ni=1\n\u0000Qi\u22121\nj=11 \u2212 \u02c6\u03b1j\n\u0001\n\u02c6\u03b1ici.\n(5)\n3.2\nPAS: Projection-Aware Sampling\nSimilar to previous sampler-based methods, our PAS net-\nwork in the ProNeRF runs only once per ray, which is a\nvery efficient operation during both training and testing. As\ndepicted in Fig. 2, our ProNeRF employs two MLP heads\nthat map rays into the optimal ray distances T and the cor-\nresponding shift and scale in density values At and Bt re-\nquired in the PAS-guided VRE.\nFigure 2: A conceptual illustration of our fast and high-quality projection-aware sampling of neural radiance fields (ProNeRF).\nThe reference views are available during training and testing. The target view is drawn for illustrative purposes only.\nThe first step in the PAS of our ProNeRF is to map the\nray\u2019s origin and direction (ro and rd) into a representation\nthat facilitates the mapping of training rays and interpolation\nof unseen rays. Feeding the raw ro and rd into F\u03b8c can mis-\nlead to overfitting, as there are a few ray origins in a given\nscene (as many as reference views). To tackle this problem,\nprevious works have proposed to encode rays as 3D points\n(TermiNeRF) or as a Pl\u00a8ucker coordinate which is the cross-\nproduct ro \u00d7 rd (LightFields and HyperReel). Motivated by\nthese works, we combine the Pl\u00a8ucker and ray-point embed-\nding into a \u2018Pl\u00a8ucker ray-point representation\u2019. Including the\nspecific points in the ray aids in making the input representa-\ntion more discriminative, as it incorporates not only the ray\norigin but also the range of the ray, while the vanilla Pl\u00a8ucker\nray can only represent an infinitely long ray. The embedded\nray rpr is then given by\nrpr = [rd, ro + rd \u2299 tnf, (ro + rd \u2299 tnf) \u00d7 rd]\n(6)\nwhere tnf is a vector whose Npr elements are evenly spaced\nbetween the scene\u2019s near and far bounds (tn and tf), \u2299 is\nthe Hadamard product, and [\u00b7, \u00b7] is the concatenation oper-\nation. The ProNeRF processes the encoded ray rpr via F\u03b8c\nin the first stage of PAS to yield the coarse sampling dis-\ntances T \u2032 = {t\u2032\n1, t\u2032\n2, ..., t\u2032\nNs} along r. F\u03b8c also predicts the\nshifts and scales in opacity values At and Bt. Furthermore,\ninspired by light-fields, F\u03b8c yields a light-field color output\n\u02c6cc which is supervised to approximate the GT color c(r) to\nfurther regularize F\u03b8c and improve the overall learning. The\nmultiple outputs of F\u03b8c are then given by\n[T \u2032, At, Bt, \u02c6cc] = F\u03b8c(rpr).\n(7)\nWhile the previous sampler-based methods attempt to\nsample radiance fields with a single network such as F\u03b8c, we\npropose a coarse-to-fine PAS in ProNeRF. In our ProNeRF,\nthe second MLP head F\u03b8f is fed with the coarse sampling\npoints r(t\u2032\ni) and color-to-ray projections which are obtained\nby tracing lines between the estimated coarse 3D ray points\nand the camera centers of Nn neighboring views from a pool\nof Nt available images, as shown in Fig. 2. The pool of Nt\nimages in the training phase consists of all training images.\nHowever, it is worth noticing that only a significantly small\nnumber of Nt views is needed for inference. The color-to-\nray projections make ProNeRF projection-aware and enable\nF\u03b8f to better understand the detailed geometry in the scenes\nas they contain not only image gradient information but also\ngeometric information that can be implicitly learned for each\npoint in space. That is, high-density points tend to contain\nsimilarly-valued multi-view color-to-ray projections.\nAlthough previous image-based rendering methods have\nproposed to directly exploit projected reference-view-\nfeatures onto the shading network, such as the works of T\net al. (2023) and Suhail et al. (2022b), these approaches ne-\ncessitate computationally expensive attention mechanisms\nand all training views storage for inference, hence increas-\ning the inference latency and memory footprint. On the other\nhand, we propose to incorporate color-to-ray projections not\nfor directly rendering the novel views but for fine-grained\nray sampling of radiance fields. As we learn to sample im-\nplicit NeRFs sparsely, our framework provides a superior\ntrade-off between memory, speed, and quality.\nThe color-to-ray projections are concatenated with the\nPl\u00a8ucker-ray-point-encoded r\u2032\npr of coarse ray distances T \u2032,\nwhich is then fed into F\u03b8f , as shown in Fig. 2. In turn, F\u03b8f\nimproves T \u2032 by yielding a set of inter-sampling refinement\nweights, denoted as 0 \u2264 \u2206T \u2264 1. The refined ray distances\nT are obtained by the linear interpolation between consec-\nutive elements of the expanded set of coarse ray distances\n\u02d9T = {tn, t\u2032\n1, t\u2032\n2, ..., t\u2032\nNs, tf} from T \u2032, as given by\nT =\nn\n1\n2\n\u0010\n( \u02d9Ti + \u02d9Ti+1) + \u2206Ti( \u02d9Ti+2 \u2212 \u02d9Ti)\n\u0011oNs\ni=1 .\n(8)\nOur inter-sampling residual refinement aids in training sta-\nbility by reusing and maintaining the order of the coarse\nsamples T \u2032. \u2206T is predicted by F\u03b8f as given by\n[\u2206T , W, M] = F\u03b8f ([r\u2032\npr, fp1, fp2, ..., fpNs ]),\n(9)\nwhere fpi = [c1\npi, c2\npi, ..., cNn\npi ] and ck\npi is the kth color-to-\nray projection from the Nn views at 3D point pi = r(t\u2032\ni).\nNote that W and M in Eq. (9) are the auxiliary outputs\nof softmax and sigmoid for network regularization, respec-\ntively. In contrast with F\u03b8c, F\u03b8f is projection-aware, thus\n\u02c6cf is obtained by exploiting the color-to-ray projections in\nan approximated version of volumetric rendering (AVR). In\nAVR, ck\npi and W \u2208 RNs are employed to approximate the\nVRE (Eq. 1). The terms\n\u0000Qi\u22121\nj=11 \u2212 \u03b1j\n\u0001\n\u03b1i in VRE are ap-\nproximated by W while ci is approximated by projected\ncolor ck\npi for the kth view in Nn neighbors. AVR then yields\nck\navr = PNs\ni=1Wick\npi,\n(10)\nresulting in Nn sub-light-field views. The final light-field\noutput \u02c6cf is aggregated by M \u2208 RNn with ck\navr as\n\u02c6cf = PNn\nk=1Mkck\navr\n(11)\nAlgorithm 1: Exploration and exploitation end2end training\n1: procedure PRONERF TRAINING\n2:\nInit Data, PAS, F\u03b8s, Opts, Optcfs\n3:\nfor it = 0 to 7 \u00d7 105 do\n4:\nSample random ray r\n5:\nAt, Bt, T, \u02c6cc, \u02c6cf \u2190 PAS(r)\n6:\nif 2|it and it < 4\u00d7105 then\n\u25b7 Exploration pass\n7:\nN +\ns \u2190 RandInt(Ns, N)\n8:\nT + \u2190 Sample(T, N +\ns )\n9:\nT + \u2190 T + + noise\n10:\n{ci, \u03c3i}\nN+\ns\ni=1 \u2190 F\u03b8s(ro + rd \u2299 T +)\n11:\n\u02c6c(r) \u2190 V RE({ci, \u03c3i}\nN+\ns\ni=1, T +) (Eq. 1)\n12:\nloss \u2190 |\u02c6c(r) \u2212 c(r)|2\n13:\nBack-propagate and update by Opts\n14:\nelse\n\u25b7 Exploitation pass\n15:\n{ci, \u03c3i}Ns\ni=1 \u2190 F\u03b8s(ro + rd \u2299 T)\n16:\n\u02c6c(r) \u2190 V RE({ci, \u03c3i}Ns\ni=1, At, Bt, T) (Eq. 5)\n17:\nloss \u2190 |\u02c6c(r) \u2212 c(r)|2\n18:\nif it < 4\u00d7105 then\n19:\nloss \u2190 loss + |\u02c6cc \u2212 c(r)|2 + |\u02c6cf \u2212 c(r)|2\n20:\nBack-propagate and update by Optcfs\n3.3\nNovel Exploration-Exploitation Training\nOur training strategy alternates between ray sampling explo-\nration and exploitation as shown in Algorithm 1. As noted\nin line(L)-2, we first initialize the dataset (composed of cal-\nibrated multi-views) by extracting the target rays and col-\nors, followed by ProNeRF\u2019s networks\u2019 initialization. We im-\nplement two optimizers, one for exploration (Opts) and the\nother for exploitation (Optcfs). Opts updates the weights in\nF\u03b8s, while Optcfs updates all weights in F\u03b8c, F\u03b8f , F\u03b8s. The\nfirst step in a training cycle is to obtain the PAS outputs (At,\nBt, T, \u02c6cc, \u02c6cf), as denoted in line 5 of Algorithm 1.\nIn the exploration pass (Algorithm 1 L-7 to 13), F\u03b8s learns\nthe scene\u2019s full color and density distributions by randomly\ninterpolating Ns estimated T distances into N +\ns piece-wise\nevenly-spaced exploration sample distances T +. For exam-\nple, if the number of estimated ray distances is Ns = 8 and\nthe exploration samples are randomly set to N +\ns = 32, the\ndistance between each sample in T will be evenly divided\ninto four bins such that the sample count is 32. Moreover,\nwe add Gaussian noise to T + as shown in of Algorithm 1 L-\n9, further allowing the F\u03b8s to explore the scene\u2019s full color\nand density distributions. We then query F\u03b8s for the N +\ns ex-\nploration points to obtain ci and \u03c3i in the original VRE (Eq.\n1). Finally, F\u03b8s is updated in the exploration pass.\nIn the exploitation pass, described in Algorithm 1 L-15 to\n20, we let the PAS and F\u03b8s be greedy by only querying the\nsamples corresponding to T and using the PAS-guided VRE\n(Eq. 5). Additionally, we provide GT color supervision to the\nauxiliary PAS network light-field outputs \u02c6cc and \u02c6cf for the\nfirst 60% of the training iterations. For the remaining 40%,\nProNeRF focuses on the exploitation and disables the auxil-\niary loss as described by Algorithm 1 L-18 and 19. Note that\nfor rendering a ray color with a few points during exploita-\ntion and testing, adjusting \u03b1i in Eq. 4 is needed to compen-\nsate for the subsampled accumulated transmittance which is\nlearned for the full ray distribution in the exploration pass.\nIn summary, during exploration, we approximate the VRE\nwith Monte Carlo sampling, where a random number of\nsamples, ranging from Ns to N, are drawn around the es-\ntimated T. When training under exploitation, we sparsely\nsample the target ray r given by T. Furthermore, we only\nupdate F\u03b8s during the exploration pass while using the orig-\ninal VRE (Eq. 1). However, in our exploitation pass, we up-\ndate all MLP heads while using the PAS-guided VRE (Eq.\n5). See Section 4 for more implementation details.\n3.4\nObjective functions\nSimilar to previous works, we guide ProNeRF to generate\nGT colors from the queried ray points with an l2 penalty as\nl =\n1\nNr\nP\nNr||\u02c6c(r) \u2212 c(r)||2,\n(12)\nwhich is averaged over the Nr rays in a batch. In con-\ntrast with the previous sampler-based networks (TermiN-\neRF, AdaNeRF, DoNeRF, HyperReel), our ProNeRF pre-\ndicts additional light-field outputs, which further regularize\nlearning, and is trained with an auxiliary loss la, as given by\nla =\n1\nNr\nP\nNr||\u02c6cc(r) \u2212 c(r)||2 + ||\u02c6cf(r) \u2212 c(r)||2. (13)\nOur total objective loss is lT = l+\u03bbla, where \u03bb is 1 for 60%\nof the training and then set to 0 afterward.\n4\nExperiments and Results\nWe provide extensive experimental results on the LLFF\n(Mildenhall et al. 2019) and Blender (Mildenhall et al. 2020)\ndatasets to show the effectiveness of our method in compari-\nson with recent SOTA methods. Also, we present a compre-\nhensive ablation study that supports our design choices and\nmain contributions. More results are shown in Supplemental.\nWe evaluate the rendering quality of our method by three\nwidely used metrics: Peak Signal-to-Noise Ratio (PSNR),\nStructural Similarity (SSIM) (Wang et al. 2004) and Learned\nPerceptual Image Patch Similarity (LPIPS) (Zhang et al.\n2018). When it comes to SSIM, there are two common im-\nplementations available, one from Tensorflow (Abadi et al.\n2015) (used in the reported metrics from NeRF, MobileN-\neRF, and IBRnet), and another from sci-kit image (van der\nWalt et al. 2014) (employed in ENeRF, RSeN, NLF). We\ndenoted the metrics from Tensorflow and scikit-image as\nSSIMt and SSIMs, respectively. Similarly, for LPIPS, we\ncan choose between two backbone options, namely AlexNet\n(Krizhevsky, Sutskever, and Hinton 2012) and VGG (Si-\nmonyan and Zisserman 2014). We present our SSIM and\nLPIPS results across all available choices to ensure a fair and\ncomprehensive evaluation of our method\u2019s performance.\n4.1\nImplementation Details\nWe train our ProNeRF with PyTorch on an NVIDIA A100\nGPU using the Adam optimizer with a batch of Nr = 4, 096\nrandomly sampled rays. The initial learning rate is set to\n5 \u00d7 10\u22124 and is exponentially decayed for 700K iterations.\nWe used TensoRT on a single RTX 3090 GPU with model\nweights quantized to half-precision FP16 for testing. We set\nthe point number in the Pl\u00a8ucker ray-point encoding for our\nPAS network to 48. We set the maximum number of explo-\nration samples to N = 64. F\u03b8c and F\u03b8f consist of 6 fully-\nconnected layers with 256 neurons followed by ELU non-\nlinearities. Finally, we adopt the shading network introduced\nin DONeRF, which has 8 layers with 256 neurons.\n4.2\nResults\nForward-Facing (LLFF). This dataset comprises 8 chal-\nlenging real scenes with 20 to 64 front-facing handheld cap-\ntured views. We conduct experiments on 756 \u00d7 1008 im-\nages to compare with previous methods, holding out every\n8th image for evaluation. We also provide the quantitative\nresults on 378 \u00d7 504 images for a fair comparison to the\nmethods evaluated on the lower resolution.\nOur quantitative and qualitative results, respectively\nshown in Table 1 and Fig. 3, demonstrate the superiority of\nour ProNeRF over the implicit NeRF and the previous ex-\nplicit methods, e.g, TensoRF and K-Planes. Our model with\n8 samples, ProNeRF-8, is the first sampler-based method\nthat outperforms the vanilla NeRF by 0.28dB PSNR while\nbeing more than 20\u00d7 faster. Furthermore, our ProNeRF-\n12 yields rendered images with 0.65dB higher PSNR while\nbeing about 15\u00d7 faster than vanilla NeRF. Our improve-\nments are reflected in the superior visual quality of the ren-\ndered images, as shown in Fig. 3. On the lower resolution,\nProNeRF-8 outperforms the second-best R2L by 0.28dB and\nthe latest sampler-based HypeRreel by 0.58dB with faster\nrendering. In Table 1, compared to the explicit grid-based\nmethods of INGP, Plenoxels and MobileNeRF, our ProNeRF\nshows a good trade-off between memory, speed, and quality.\nWe also present the quantitative results of the auxiliary\nPAS light field outputs in Table 1, denoted as PAS-8 cf for\nboth the regression (Reg) and AVR cases. We observed no\ndifference in the final color output when Reg or AVR were\nused in ProNeRF-8. However, PAS-8 cf (AVR) yields con-\nsiderably better metrics than its Reg counterpart.\nInspired by the higher FPS from PAS-8 cf (AVR), we also\nexplored pruning ProNeRF by running the F\u03b8s only for the\n\u201ccomplex rays\u201d. We achieve ProNeRF-8 prune by training\na complementary MLP head F\u03b8m which has the same com-\nplexity as F\u03b8c and predicts the error between \u02c6cf and \u02c6c out-\nputs. When the error is low, we render the ray by PAS-8 cf\n(AVR); otherwise, we subsequently run the shader network\nF\u03b8s. While pruning requires an additional 3.3 MB in mem-\nory, the pruned ProNeRF-8 is 23% faster than ProNeRF-8\nwith a small PSNR drop and negligible SSIM and LPIPS\ndegradations, as shown in Table 1. Note that other previous\nsampler-based methods cannot be pruned similarly, as they\ndo not incorporate the auxiliary light-filed output. Training\npruning is fast (5min). See more details in Supplemental.\nRes. Methods\nPSNR\nSSIMt/s\nLPIPSvgg/alex FPS Mem(MB)\nNeRF (ECCV20)\n26.50\n0.811 / -\n0.250 / -\n0.3\n3.8\nINGP (SIGGRAPH22)\n25.60\n0.758 / -\n0.267 / -\n7.3\n64.0\n756\nPlenoxels (CVPR22)\n26.30\n0.839 / -\n0.210 / -\n9.1\n3629.8\n\u00d7\nMipNeRF360 (CVPR22)\n26.86\n0.858 / -\n- / 0.128\n0.1\n8.2\n1008\nTensoRF (ECCV22)\n26.73\n0.839 / -\n0.204 / 0.124\n1.1\n179.7\nK-Planes (CVPR23)\n26.92\n0.847 / -\n0.182 / -\n0.7\n214\nSNeRG (ICCV21)\n25.63\n0.818 / -\n0.183 / -\n50.7\n337.3\nENeRF (SIGGRAPHA22) 24.89\n- / 0.865\n0.159 / -\n8.9\n10.7\nAdaNeRF (ECCV22)\n25.70\n- / -\n- / -\n7.7\n4.1\nHyperreel (CVPR23)\n26.20\n- / -\n- / -\n4.0\n58.8\nMobileNeRF (CVPR23)\n25.91\n0.825 / -\n0.183 / -\n348\n201.5\nPAS-8 cf (Reg) (Ours)\n24.86 0.787 / 0.855\n0.236 / 0.150\n29.4\n2.7\nPAS-8 cf (AVR) (Ours)\n25.15 0.793 / 0.860\n0.234 / 0.146\n25.6\n5.0\nProNeRF-8 Prune (Ours)\n26.54 0.825 / 0.883\n0.219 / 0.120\n8.5\n6.8\nProNeRF-8 (Ours)\n26.78 0.825 / 0.884\n0.228 / 0.119\n6.9\n3.5\nProNeRF-12 (Ours)\n27.15 0.838 / 0.894\n0.217 / 0.109\n4.4\n3.5\nFastNeRF (ICCV21)\n26.04\n- / 0.856\n- / 0.085\n700\n4100\n378\nEfficientNeRF (CVPR22) 27.39\n- / 0.912\n- / 0.082\n219\n2800\n\u00d7\nRSEN (CVPR22)\n27.45\n- / 0.905\n- / 0.060\n0.34\n5.4\n504\nR2L (ECCV22)\n27.79\n- / -\n- / 0.097\n5.6\n22.6\nHyperreel (CVPR23)\n27.50\n- / -\n- / -\n4.0\n58.8\nProNeRF-8 (Ours)\n28.08 0.879 / 0.916\n0.129 / 0.060\n6.9\n3.5\nProNeRF-12 (Ours)\n28.33 0.885 / 0.920\n0.129 / 0.058\n4.4\n3.5\nTable 1: Results on LLFF. Metrics are the lower the better\nand the higher the better . (-) metrics are not provided in the\noriginal literature.\nMethods\nPSNR SSIM LPIPS\nNo exploration pass\n24.00 0.754 0.299\nNo exploitation pass 24.31 0.779 0.278\nNo \u03c3 shift (no Bt)\n24.2 0.773 0.264\nNo aux. loss (no la) 24.26 0.766 0.296\nNo \u02c6\u03b1 (no At, Bt)\n24.69 0.785 0.260\nNo Pl\u00a8ucker ray-point 24.72 0.782 0.257\nNo color-to-ray proj 24.83 0.789 0.245\nProNeRF-12 Nn=4\n25.17 0.809 0.244\nAvg Nt PSNR SSIM LPIPS Mem(MB)\n4.00\n27.15 0.838 0.217\n3.5\n8.00\n27.16 0.838 0.216\n4.2\n12.00\n27.15 0.837 0.217\n4.9\n32.75\n27.15 0.838 0.216\n8.4\nTable 2: ProNeRF ablations on LLFF. (Left) Network de-\nsigns on Fern. (Right) Ablation of # of available ref. views.\n360 Blender. This is an object-centric 360-captured syn-\nthetic dataset for which our ProNeRF-32 achieves a reason-\nably good performance of 31.92 dB PSNR, 3.2 FPS (after\npruning) and 6.3 MB Mem. It should be also noted that\nthe ProNeRF-32 outperforms NeRF, SNeRG, Plenoctree,\nand Plenoxels while still displaying a favorable performance\nprofiling. See Supplemental for detailed results.\n4.3\nAblation Studies\nWe ablate our ProNeRF on the LLFF\u2019s Fern scene in Table 2\n(left). We first show that infusing exploration and exploita-\ntion into our training strategy is critical for high-quality\nneural rendering. As shown in the top section of Table 2\n(left), exploration- or exploitation-only leads to sub-optimal\nresults as neither the shading network is allowed to learn the\nfull scene distributions nor the PAS network is made to focus\non the regions with the highest densities.\nNext, we explore our network design by ablating each\nGround Truth\nProNeRF\nTensoRF (ECCV 2022)\nNeRF (ECCV 2020)\nFigure 3: Qualitative comparisons for the LLFF (Mildenhall et al. 2019) dataset. Zoom in for better visualization.\nFigure 4: Cameras distribution on the LLFF\u2019s Fortress scene.\nGreen cameras denote available training views. Red cameras\ndenote selected and fixed subset of Nt frames for projection.\ndesign choice. As noted in Table 2 (left), removing \u03b1 scales\n(At) and shifts (Bt) severely impact the rendering quality.\nWe also observed that the auxiliary loss (la) is critical to\nproperly train our sampler since its removal causes almost\n1dB drop in PSNR. The importance of our Pl\u00a8ucker ray-point\nencoding is shown in Table 2 (left), having an impact of al-\nmost 0.5dB PSNR drop when disabled. Finally, we show that\nthe color-to-ray projection in the PAS of our ProNeRF is the\nkey feature for high-quality rendering.\nMemory footprint consistency. This experiment proves\nProNeRF yields a consistent usage of memory footprint. As\nmentioned in Section 2, light-fields and image-based ren-\ndering methods, which rely on multi-view color projections,\ntypically require large storage for all available training views\nfor rendering a novel view. This is because they utilize the\nnearest reference views to the target pose from the entire\npool of available images. In contrast, our ProNeRF takes a\ndistinct approach by consistently selecting a fixed subset of\nNt reference views when rendering any novel viewpoint in\nthe inference stage. This is possible because (i) we randomly\nselect any Nn neighboring views (from the entire training\npool) during training; and (ii) our final rendered color is ob-\ntained by sparsely querying a radiance field, not by directly\nprocessing projected features/colors. As a result, our frame-\nwork yields a consistent memory footprint for storing ref-\nerence views, which is advantageous for efficient hardware\ndesign. To select the Nt views, we leverage the sparse point\ncloud reconstructed from COLMAP and a greedy algorithm\nto identify the optimal combination of potential frames. As\nshown in Fig. 4, the Nt views become a subset across all\navailable training images that comprehensively cover the tar-\nget scene (see details in Supplemental). As shown in Table\n2 (right), we set the number of neighbors in PAS to Nn = 4\nand adjust Nt to 4, 8, 12, and all training views (32.75).\nPlease note our ProNeRF\u2019s rendering quality remains stable\nwhile modulating Nt, attesting to the stability and robust-\nness of our approach across varying configurations.\n4.4\nLimitations\nWhile not technically constrained to forward-facing scenes\n(such as NeX) and yielding better metrics than vanilla NeRF\nand several other works, our method is behind grid-based\nexplicit models such as INGP for the Blender dataset. The\nmethods like INGP contain data structures that better ac-\ncommodate these kinds of scenes. Our method requires more\nsamples for this data type, evidencing that our method is\nmore efficient and shines on forward-facing datasets.\n5\nConclusions\nOur ProNeRF, a sampler-based neural rendering method,\nsignificantly outperforms the vanilla NeRF quantitatively\nand qualitatively for the first time. It also outperforms the\nexisting explicit voxel/grid-based methods by large mar-\ngins while preserving a small memory footprint and fast\ninference. Furthermore, we showed that our exploration\nand exploitation training is crucial for learning high-quality\nrendering. Future research might extend our ProNeRF for\ndynamic-scenes and cross-scene generalization.\nAcknowledgements\nThis work was supported by IITP grant funded by the Korea\ngovernment (MSIT) (No. RS2022-00144444, Deep Learn-\ning Based Visual Representational Learning and Rendering\nof Static and Dynamic Scenes).\nReferences\nAbadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.;\nCitro, C.; Corrado, G. S.; Davis, A.; Dean, J.; Devin, M.;\nGhemawat, S.; Goodfellow, I.; Harp, A.; Irving, G.; Isard,\nM.; Jia, Y.; Jozefowicz, R.; Kaiser, L.; Kudlur, M.; Leven-\nberg, J.; Man\u00b4e, D.; Monga, R.; Moore, S.; Murray, D.; Olah,\nC.; Schuster, M.; Shlens, J.; Steiner, B.; Sutskever, I.; Tal-\nwar, K.; Tucker, P.; Vanhoucke, V.; Vasudevan, V.; Vi\u00b4egas,\nF.; Vinyals, O.; Warden, P.; Wattenberg, M.; Wicke, M.; Yu,\nY.; and Zheng, X. 2015. TensorFlow: Large-Scale Machine\nLearning on Heterogeneous Systems.\nSoftware available\nfrom tensorflow.org.\nAttal, B.; Huang, J.; Richardt, C.; Zollh\u00a8ofer, M.; Kopf, J.;\nO\u2019Toole, M.; and Kim, C. 2023. HyperReel: High-Fidelity\n6-DoF Video with Ray-Conditioned Sampling.\nCoRR,\nabs/2301.02238.\nAttal, B.; Huang, J.-B.; Zollh\u00a8ofer, M.; Kopf, J.; and Kim,\nC. 2022. Learning Neural Light Fields with Ray-Space Em-\nbedding Networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR).\nBarron, J. T.; Mildenhall, B.; Verbin, D.; Srinivasan, P. P.;\nand Hedman, P. 2022. Mip-NeRF 360: Unbounded Anti-\nAliased Neural Radiance Fields. CVPR.\nChen, A.; Xu, Z.; Geiger, A.; Yu, J.; and Su, H. 2022. Ten-\nsoRF: Tensorial Radiance Fields. In Avidan, S.; Brostow,\nG. J.; Ciss\u00b4e, M.; Farinella, G. M.; and Hassner, T., eds.,\nComputer Vision - ECCV 2022 - 17th European Confer-\nence, Tel Aviv, Israel, October 23-27, 2022, Proceedings,\nPart XXXII, volume 13692 of Lecture Notes in Computer\nScience, 333\u2013350. Springer.\nChen, Z.; Funkhouser, T.; Hedman, P.; and Tagliasacchi, A.\n2023. MobileNeRF: Exploiting the Polygon Rasterization\nPipeline for Efficient Neural Field Rendering on Mobile Ar-\nchitectures. In The Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nDrebin, R. A.; Carpenter, L.; and Hanrahan, P. 1988. Volume\nrendering. ACM Siggraph Computer Graphics, 22(4): 65\u2013\n74.\nFlynn, J.; Broxton, M.; Debevec, P.; DuVall, M.; Fyffe, G.;\nOverbeck, R.; Snavely, N.; and Tucker, R. 2019. Deepview:\nView synthesis with learned gradient descent. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2367\u20132376.\nFridovich-Keil, S.; Yu, A.; Tancik, M.; Chen, Q.; Recht, B.;\nand Kanazawa, A. 2022. Plenoxels: Radiance Fields without\nNeural Networks. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, 5491\u20135500. IEEE.\nGarbin, S. J.; Kowalski, M.; Johnson, M.; Shotton, J.; and\nValentin, J. P. C. 2021.\nFastNeRF: High-Fidelity Neural\nRendering at 200FPS.\nIn 2021 IEEE/CVF International\nConference on Computer Vision, ICCV 2021, Montreal, QC,\nCanada, October 10-17, 2021, 14326\u201314335. IEEE.\nHedman, P.; Srinivasan, P. P.; Mildenhall, B.; Barron, J. T.;\nand Debevec, P. E. 2021. Baking Neural Radiance Fields\nfor Real-Time View Synthesis. In 2021 IEEE/CVF Inter-\nnational Conference on Computer Vision, ICCV 2021, Mon-\ntreal, QC, Canada, October 10-17, 2021, 5855\u20135864. IEEE.\nHu, T.; Liu, S.; Chen, Y.; Shen, T.; and Jia, J. 2022. Effi-\ncientNeRF - Efficient Neural Radiance Fields. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR 2022, New Orleans, LA, USA, June 18-24, 2022,\n12892\u201312901. IEEE.\nKerbl, B.; Kopanas, G.; Leimk\u00a8uhler, T.; and Drettakis, G.\n2023. 3D Gaussian Splatting for Real-Time Radiance Field\nRendering. ACM Transactions on Graphics, 42(4).\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25.\nKurz, A.; Neff, T.; Lv, Z.; Zollh\u00a8ofer, M.; and Steinberger, M.\n2022. AdaNeRF: Adaptive Sampling for Real-Time Render-\ning of Neural Radiance Fields. In Avidan, S.; Brostow, G. J.;\nCiss\u00b4e, M.; Farinella, G. M.; and Hassner, T., eds., Computer\nVision - ECCV 2022 - 17th European Conference, Tel Aviv,\nIsrael, October 23-27, 2022, Proceedings, Part XVII, vol-\nume 13677 of Lecture Notes in Computer Science, 254\u2013270.\nSpringer.\nLevoy, M.; and Hanrahan, P. 1996.\nLight Field Render-\ning. In Fujii, J., ed., Proceedings of the 23rd Annual Con-\nference on Computer Graphics and Interactive Techniques,\nSIGGRAPH 1996, New Orleans, LA, USA, August 4-9, 1996,\n31\u201342. ACM.\nLi, J.; Feng, Z.; She, Q.; Ding, H.; Wang, C.; and Lee, G. H.\n2021. MINE: Towards Continuous Depth MPI with NeRF\nfor Novel View Synthesis. In 2021 IEEE/CVF International\nConference on Computer Vision, ICCV 2021, Montreal, QC,\nCanada, October 10-17, 2021, 12558\u201312568. IEEE.\nLin, H.; Peng, S.; Xu, Z.; Yan, Y.; Shuai, Q.; Bao, H.; and\nZhou, X. 2022. Efficient Neural Radiance Fields for Interac-\ntive Free-viewpoint Video. In SIGGRAPH Asia Conference\nProceedings.\nLindell, D. B.; Martel, J. N. P.; and Wetzstein, G. 2021. Au-\ntoInt: Automatic Integration for Fast Neural Volume Ren-\ndering. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021, 14556\u2013\n14565. Computer Vision Foundation / IEEE.\nLiu, Y.; Peng, S.; Liu, L.; Wang, Q.; Wang, P.; Theobalt, C.;\nZhou, X.; and Wang, W. 2022. Neural Rays for Occlusion-\naware Image-based Rendering. In CVPR.\nMildenhall, B.; Srinivasan, P. P.; Cayon, R. O.; Kalantari,\nN. K.; Ramamoorthi, R.; Ng, R.; and Kar, A. 2019.\nLo-\ncal light field fusion: practical view synthesis with prescrip-\ntive sampling guidelines. ACM Trans. Graph., 38(4): 29:1\u2013\n29:14.\nMildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.;\nRamamoorthi, R.; and Ng, R. 2020. NeRF: Representing\nScenes as Neural Radiance Fields for View Synthesis. In\nVedaldi, A.; Bischof, H.; Brox, T.; and Frahm, J., eds., Com-\nputer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part I, vol-\nume 12346 of Lecture Notes in Computer Science, 405\u2013421.\nSpringer.\nM\u00a8uller, T.; Evans, A.; Schied, C.; and Keller, A. 2022. In-\nstant neural graphics primitives with a multiresolution hash\nencoding. ACM Trans. Graph., 41(4): 102:1\u2013102:15.\nNeff, T.; Stadlbauer, P.; Parger, M.; Kurz, A.; Mueller, J. H.;\nChaitanya, C. R. A.; Kaplanyan, A.; and Steinberger, M.\n2021. DONeRF: Towards Real-Time Rendering of Com-\npact Neural Radiance Fields using Depth Oracle Networks.\nComput. Graph. Forum, 40(4): 45\u201359.\nPiala, M.; and Clark, R. 2021. TermiNeRF: Ray Termination\nPrediction for Efficient Neural Rendering. In International\nConference on 3D Vision, 3DV 2021, London, United King-\ndom, December 1-3, 2021, 1106\u20131114. IEEE.\nSara Fridovich-Keil and Giacomo Meanti; Warburg, F. R.;\nRecht, B.; and Kanazawa, A. 2023. K-Planes: Explicit Ra-\ndiance Fields in Space, Time, and Appearance. In CVPR.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSitzmann, V.; Martel, J.; Bergman, A.; Lindell, D.; and Wet-\nzstein, G. 2020. Implicit neural representations with peri-\nodic activation functions. Advances in Neural Information\nProcessing Systems, 33: 7462\u20137473.\nSuhail, M.; Esteves, C.; Sigal, L.; and Makadia, A. 2022a.\nGeneralizable Patch-Based Neural Rendering. In European\nConference on Computer Vision. Springer.\nSuhail, M.; Esteves, C.; Sigal, L.; and Makadia, A. 2022b.\nLight Field Neural Rendering. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, 8259\u20138269. IEEE.\nT, M. V.; Wang, P.; Chen, X.; Chen, T.; Venugopalan, S.;\nand Wang, Z. 2023. Is Attention All That NeRF Needs?\nIn The Eleventh International Conference on Learning Rep-\nresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net.\nvan der Walt, S.; Sch\u00a8onberger, J. L.; Nunez-Iglesias, J.;\nBoulogne, F.; Warner, J. D.; Yager, N.; Gouillart, E.; Yu, T.;\nand the scikit-image contributors. 2014. scikit-image: image\nprocessing in Python. PeerJ, 2: e453.\nWang, H.; Ren, J.; Huang, Z.; Olszewski, K.; Chai, M.; Fu,\nY.; and Tulyakov, S. 2022. R2L: Distilling Neural Radiance\nField to Neural Light Field for Efficient Novel View Synthe-\nsis. In ECCV.\nWang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou,\nH.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and\nFunkhouser, T. A. 2021.\nIBRNet: Learning Multi-View\nImage-Based Rendering.\nIn IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2021, virtual,\nJune 19-25, 2021, 4690\u20134699. Computer Vision Foundation\n/ IEEE.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004.\nImage quality assessment: from error visibility to\nstructural similarity.\nIEEE Trans. Image Process., 13(4):\n600\u2013612.\nYu, A.; Li, R.; Tancik, M.; Li, H.; Ng, R.; and Kanazawa, A.\n2021a. PlenOctrees for Real-time Rendering of Neural Ra-\ndiance Fields. In 2021 IEEE/CVF International Conference\non Computer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021, 5732\u20135741. IEEE.\nYu, A.; Ye, V.; Tancik, M.; and Kanazawa, A. 2021b. pix-\nelNeRF: Neural Radiance Fields From One or Few Images.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2021, virtual, June 19-25, 2021, 4578\u20134587.\nComputer Vision Foundation / IEEE.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The Unreasonable Effectiveness of Deep Features\nas a Perceptual Metric. In 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018, Salt Lake\nCity, UT, USA, June 18-22, 2018, 586\u2013595. Computer Vi-\nsion Foundation / IEEE Computer Society.\n"
  }
]