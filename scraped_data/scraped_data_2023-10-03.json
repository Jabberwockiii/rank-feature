[
  {
    "title": "PixArt-$\u03b1$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
    "link": "https://arxiv.org/pdf/2310.00426.pdf",
    "upvote": "57",
    "text": "Technical Report\nPIXART-\u03b1: FAST TRAINING OF DIFFUSION TRANS-\nFORMER FOR PHOTOREALISTIC TEXT-TO-IMAGE\nSYNTHESIS\nJunsong Chen1,2,3\u2217, Jincheng Yu1,4\u2217, Chongjian Ge1,3\u2217, Lewei Yao1,4\u2217, Enze Xie1\u2020,\nYue Wu1, Zhongdao Wang1, James Kwok4, Ping Luo3, Huchuan Lu2, Zhenguo Li1\n1Huawei Noah\u2019s Ark Lab\n2Dalian University of Technology\n3HKU\n4HKUST\njschen@mail.dlut.edu.cn, rhettgee@connect.hku.hk,\n{yujincheng4,yao.lewei,xie.enze,Li.Zhenguo}@huawei.com\nProject Page: https://pixart-alpha.github.io/\nABSTRACT\nThe most advanced text-to-image (T2I) models require significant training costs\n(e.g., millions of GPU hours), seriously hindering the fundamental innovation for\nthe AIGC community while increasing CO2 emissions. This paper introduces\nPIXART-\u03b1, a Transformer-based T2I diffusion model whose image generation\nquality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL,\nand even Midjourney), reaching near-commercial application standards. Addi-\ntionally, it supports high-resolution image synthesis up to 1024 \u00d7 1024 resolu-\ntion with low training cost, as shown in Figure 1 and 2. To achieve this goal,\nthree core designs are proposed: (1) Training strategy decomposition: We de-\nvise three distinct training steps that respectively optimize pixel dependency, text-\nimage alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We\nincorporate cross-attention modules into Diffusion Transformer (DiT) to inject\ntext conditions and streamline the computation-intensive class-condition branch;\n(3) High-informative data: We emphasize the significance of concept density in\ntext-image pairs and leverage a large Vision-Language model to auto-label dense\npseudo-captions to assist text-image alignment learning. As a result, PIXART-\u03b1\u2019s\ntraining speed markedly surpasses existing large-scale T2I models, e.g., PIXART-\n\u03b1 only takes 12% of Stable Diffusion v1.5\u2019s training time (\u223c753 vs. \u223c6,250 A100\nGPU days), saving nearly $300,000 ($28,400 vs. $320,000) and reducing 90%\nCO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our\ntraining cost is merely 1%. Extensive experiments demonstrate that PIXART-\u03b1\nexcels in image quality, artistry, and semantic control. We hope PIXART-\u03b1 will\nprovide new insights to the AIGC community and startups to accelerate building\ntheir own high-quality yet low-cost generative models from scratch.\n1\nINTRODUCTION\nRecently, the advancement of text-to-image (T2I) generative models, such as DALL\u00b7E 2 (OpenAI,\n2023), Imagen (Saharia et al., 2022), and Stable Diffusion (Rombach et al., 2022) has started a\nnew era of photorealistic image synthesis, profoundly impacting numerous downstream applica-\ntions, such as image editing (Kim et al., 2022), video generation (Wu et al., 2022), 3D assets cre-\nation (Poole et al., 2022), etc.\nHowever, the training of these advanced models demands immense computational resources. For in-\nstance, training SDv1.5 (Podell et al., 2023) necessitates 6K A100 GPU days, approximately costing\n\u2217Equal contribution. Work done during the internships of the four students at Huawei Noah\u2019s Ark Lab.\n\u2020 Project lead and corresponding author.\n1\narXiv:2310.00426v3  [cs.CV]  29 Dec 2023\nTechnical Report\nbeautiful scene with mountains and rivers in a small village\na small cactus with a happy face in the Sahara desert\nCthulhu, alien, in a huge\ntowering\nchurch,\nan\nevil\nstatue with a skeleton in\nhis hand\na Emu, focused yet playful, ready for a competitive\nmatchup, photorealistic quality with cartoon vibes\nOppenheimer sits on the beach on a chair, watching a\nnuclear exposition with a huge mushroom cloud, 120mm\nPirate ship trapped in a\ncosmic maelstrom nebula\nproduct photography, world\nof warcraft orc warrior,\nwhite background\nlittle\ngirl\nwith\nred\nhair\nsitting at a table, portrait,\nkodak portray\npaper\nartwork,\nlayered\npaper,\ncolorful\nChinese\ndragon surrounded by clouds\na traveler navigating via a\nboat in countless mountains,\nChinese ink painting\nFigure 1: Samples produced by PIXART-\u03b1 exhibit exceptional quality, characterized by a remark-\nable level of fidelity and precision in adhering to the provided textual descriptions.\n2\nTechnical Report\n$320,000, and the recent larger model, RAPHAEL (Xue et al., 2023b), even costs 60K A100 GPU\ndays \u2013 requiring around $3,080,000, as detailed in Table 2. Additionally, the training contributes\nsubstantial CO2 emissions, posing environmental stress; e.g. RAPHAEL\u2019s (Xue et al., 2023b) train-\ning results in 35 tons of CO2 emissions, equivalent to the amount one person emits over 7 years, as\nshown in Figure 2. Such a huge cost imposes significant barriers for both the research community\nand entrepreneurs in accessing those models, causing a significant hindrance to the crucial advance-\nment of the AIGC community. Given these challenges, a pivotal question arises: Can we develop a\nhigh-quality image generator with affordable resource consumption?\n< 2% training time \n< 1% data usage\n< 0.5% data usage\nHuman 1 year \u2248 5 ton\n\ud835\udc6a\ud835\udc76\ud835\udfd0 Emission (ton)\n0.8\u00d7\n0.7\u00d7\n0.6\u00d7\n5\u00d7\n7\u00d7\n2\n4\n20\n30\n\u2026\n0.08\u00d7\n1.2%\n15.8%\nUS Dollars ($)\n$3.08M\n$2.14M\n$366K\n$320K\n$245K\n$28K\n30k\n300k\n2M\n3M\n\u2026\nRAPHAEL DALL-E 2\nImagen\nSD v1.5\nGigaGAN\nPixArt-\u03b1\nRAPHAEL DALL-E 2\nImagen\nSD v1.5\nGigaGAN\nPixArt-\u03b1\n11.4% 0.91%\n3B 6.5B\n0.6B\n(a) Comparison of data usage and training time\n(b) Comparison of \ud835\udc36\ud835\udc42\" emission and training cost\nDiameter\n#Params\nFigure 2: Comparisons of CO2 emissions1 and training cost2 among T2I generators. PIXART-\u03b1\nachieves an exceptionally low training cost of $28,400. Compared to RAPHAEL, our CO2 emissions\nand training costs are merely 1.2% and 0.91%, respectively.\nIn this paper, we introduce PIXART-\u03b1, which significantly reduces computational demands of train-\ning while maintaining competitive image generation quality to the current state-of-the-art image\ngenerators, as illustrated in Figure 1. To achieve this, we propose three core designs:\nTraining strategy decomposition.\nWe decompose the intricate text-to-image generation task into\nthree streamlined subtasks: (1) learning the pixel distribution of natural images, (2) learning text-\nimage alignment, and (3) enhancing the aesthetic quality of images. For the first subtask, we propose\ninitializing the T2I model with a low-cost class-condition model, significantly reducing the learning\ncost. For the second and third subtasks, we formulate a training paradigm consisting of pretraining\nand fine-tuning: pretraining on text-image pair data rich in information density, followed by fine-\ntuning on data with superior aesthetic quality, boosting the training efficiency.\nEfficient T2I Transformer.\nBased on the Diffusion Transformer (DiT) (Peebles & Xie, 2023),\nwe incorporate cross-attention modules to inject text conditions and streamline the computation-\nintensive class-condition branch to improve efficiency.\nFurthermore, we introduce a re-\nparameterization technique that allows the adjusted text-to-image model to load the original class-\ncondition model\u2019s parameters directly. Consequently, we can leverage prior knowledge learned from\nImageNet (Deng et al., 2009) about natural image distribution to give a reasonable initialization for\nthe T2I Transformer and accelerate its training.\nHigh-informative data.\nOur investigation reveals notable shortcomings in existing text-image\npair datasets, exemplified by LAION (Schuhmann et al., 2021), where textual captions often suffer\nfrom a lack of informative content (i.e., typically describing only a partial of objects in the images)\nand a severe long-tail effect (i.e., with a large number of nouns appearing with extremely low fre-\nquencies). These deficiencies significantly hamper the training efficiency for T2I models and lead\n1The method for estimating CO2 emissions follows Alexandra Sasha Luccioni (2022).\n2The training cost refers to the cloud GPU pricing from Microsoft (2023) Azure in September 20, 2023.\n3\nTechnical Report\nto millions of iterations to learn stable text-image alignments. To address them, we propose an auto-\nlabeling pipeline utilizing the state-of-the-art vision-language model (LLaVA (Liu et al., 2023)) to\ngenerate captions on the SAM (Kirillov et al., 2023). Referencing in Section 2.4, the SAM dataset is\nadvantageous due to its rich and diverse collection of objects, making it an ideal resource for creating\nhigh-information-density text-image pairs, more suitable for text-image alignment learning.\nOur effective designs result in remarkable training efficiency for our model, costing only 753\nA100 GPU days and $28,400.\nAs demonstrated in Figure 2, our method consumes less than\n1.25% training data volume compared to SDv1.5 and costs less than 2% training time compared\nto RAPHAEL. Compared to RAPHAEL, our training costs are only 1%, saving approximately\n$3,000,000 (PIXART-\u03b1\u2019s $28,400 vs. RAPHAEL\u2019s $3,080,000).\nRegarding generation quality,\nour user study experiments indicate that PIXART-\u03b1 offers superior image quality and semantic\nalignment compared to existing SOTA T2I models (e.g., DALL\u00b7E 2 (OpenAI, 2023), Stable Dif-\nfusion (Rombach et al., 2022), etc.), and its performance on T2I-CompBench (Huang et al., 2023)\nalso evidences our advantage in semantic control. We hope our attempts to train T2I models effi-\nciently can offer valuable insights for the AIGC community and help more individual researchers or\nstartups create their own high-quality T2I models at lower costs.\n2\nMETHOD\n2.1\nMOTIVATION\nThe reasons for slow T2I training lie in two aspects: the training pipeline and the data.\nThe T2I generation task can be decomposed into three aspects: Capturing Pixel Dependency:\nGenerating realistic images involves understanding intricate pixel-level dependencies within images\nand capturing their distribution; Alignment between Text and Image: Precise alignment learning is\nrequired for understanding how to generate images that accurately match the text description; High\nAesthetic Quality: Besides faithful textual descriptions, being aesthetically pleasing is another vital\nattribute of generated images. Current methods entangle these three problems together and directly\ntrain from scratch using vast amount of data, resulting in inefficient training. To solve this issue, we\ndisentangle these aspects into three stages, as will be described in Section 2.2.\nAnother problem, depicted in Figure 3, is with the quality of captions of the current dataset. The cur-\nrent text-image pairs often suffer from text-image misalignment, deficient descriptions, infrequent\ndiverse vocabulary usage, and inclusion of low-quality data. These problems introduce difficulties\nin training, resulting in unnecessarily millions of iterations to achieve stable alignment between text\nand images. To address this challenge, we introduce an innovative auto-labeling pipeline to generate\nprecise image captions, as will be described in Section 2.4.\nText-image misalignment\nWhat science says about pu'erh tea? \nRaw \ncaption\n2018 Kawasaki Jet Ski Ultra \n310LX in Unionville, Virginia\nDeficient descriptions\nInfrequent vocabulary\nAH1370/1950 Saudi Arabia Gold \nOne Guinea MS-63 NGC \nLLaVA\nrefined\ncaption\nThe image features a man riding a jet ski\non a body of water. The jet ski is green\nand white, and it is being used for\nrecreational purposes. The man is smiling,\nindicating that he is enjoying his time on\nthe water. The scene is set in a beach area.\nThe image shows a man working on scuba diving\nequipment at Blue Water Divers. The man is\nsitting at a table, working on a piece of\nequipment, possibly fixing or adjusting it. The\nscene is set in a workshop or a store, with various\ntools and equipment visible in the background.\nThe image features a close-up of a cup of tea with a\nsaucer on a wooden table. The tea is described as\n\"pu'erh tea,\" which is a type of Chinese tea known for\nits health benefits. The scene is set in a dimly lit room.\nThe presence of a potted plant in the background adds\na touch of nature and freshness to the scene.\nProblems\nSamples\nFigure 3: LAION raw captions v.s LLaVA refined captions. LLaVA provides high-information-\ndensity captions that aid the model in grasping more concepts per iteration and boost text-image\nalignment efficiency.\n4\nTechnical Report\n2.2\nTRAINING STRATEGY DECOMPOSITION\nThe model\u2019s generative capabilities can be gradually optimized by partitioning the training into three\nstages with different data types.\nStage1: Pixel dependency learning. The current class-guided approach (Peebles & Xie, 2023)\nhas shown exemplary performance in generating semantically coherent and reasonable pixels in\nindividual images. Training a class-conditional image generation model (Peebles & Xie, 2023) for\nnatural images is relatively easy and inexpensive, as explained in Appendix A.5. Additionally, we\nfind that a suitable initialization can significantly boost training efficiency. Therefore, we boost our\nmodel from an ImageNet-pretrained model, and the architecture of our model is designed to be\ncompatible with the pretrained weights.\nStage2: Text-image alignment learning. The primary challenge in transitioning from pretrained\nclass-guided image generation to text-to-image generation is on how to achieve accurate alignment\nbetween significantly increased text concepts and images.\nTunable Parameters\na small cactus \nwith a happy \nface in the \nSahara desert\nScale, Shift\n\ud835\udc54(\ud835\udc38!, \ud835\udefd\", &\ud835\udefe\")\nMulti-Head\nSelf-Attention\nScale\n\ud835\udc54(\ud835\udc38!, \ud835\udefc\")\nMulti-Head\nCross-Attention\nScale, Shift\n\ud835\udc54(\ud835\udc38!, \ud835\udefd#, \ud835\udefe#)\nPointwise\nFeedforward\nScale\n\ud835\udc54(\ud835\udc38!, \ud835\udefc#)\n+\n+\n+\n\ud835\udc9b\ud835\udc95\nMLP\n\u00d7\ud835\udc75\n\ud835\udc475\nTime t\nVAE \nencoder\nText \nfeature\nText\nImage\n\ud835\udf00\nShared by\n\ud835\udc75 blocks\n+ noiset\nNoised latent\nText \nencoder\nFrozen Parameters\n\ud835\udc56$% block\nFigure 4: Model architecture of PIXART-\u03b1. A\ncross-attention module is integrated into each\nblock to inject textual conditions. To optimize ef-\nficiency, all blocks share the same adaLN-single\nparameters for time conditions.\nThis alignment process is not only time-\nconsuming but also inherently challenging. To\nefficiently facilitate this process, we construct\na dataset consisting of precise text-image pairs\nwith high concept density. The data creation\npipeline will be described in Section 2.4. By\nemploying accurate and information-rich data,\nour training process can efficiently handle a\nlarger number of nouns in each iteration while\nencountering considerably less ambiguity com-\npared to previous datasets. This strategic ap-\nproach empowers our network to align textual\ndescriptions with images effectively.\nStage3: High-resolution and aesthetic image\ngeneration.\nIn the third stage, we fine-tune\nour model using high-quality aesthetic data for\nhigh-resolution image generation. Remarkably,\nwe observe that the adaptation process in this\nstage converges significantly faster, primarily\nowing to the strong prior knowledge established\nin the preceding stages.\nDecoupling the training process into different\nstages significantly alleviates the training diffi-\nculties and achieves highly efficient training.\n2.3\nEFFICIENT T2I TRANSFORMER\nPIXART-\u03b1 adopts the Diffusion Transformer\n(DiT) (Peebles & Xie, 2023) as the base architecture and innovatively tailors the Transformer blocks\nto handle the unique challenges of T2I tasks, as depicted in Figure 4. Several dedicated designs are\nproposed as follows:\n\u2022 Cross-Attention layer. We incorporate a multi-head cross-attention layer to the DiT block. It is\npositioned between the self-attention layer and feed-forward layer so that the model can flexibly\ninteract with the text embedding extracted from the language model. To facilitate the pretrained\nweights, we initialize the output projection layer in the cross-attention layer to zero, effectively\nacting as an identity mapping and preserving the input for the subsequent layers.\n\u2022 AdaLN-single. We find that the linear projections in the adaptive normalization layers (Perez et al.,\n2018) (adaLN) module of the DiT account for a substantial proportion (27%) of the parameters.\nSuch a large number of parameters is not useful since the class condition is not employed for\nour T2I model. Thus, we propose adaLN-single, which only uses time embedding as input in\nthe first block for independent control (shown on the right side of Figure 4). Specifically, in\n5\nTechnical Report\nthe ith block, let S(i) = [\u03b2(i)\n1 , \u03b2(i)\n2 , \u03b3(i)\n1 , \u03b3(i)\n2 , \u03b1(i)\n1 , \u03b1(i)\n2 ] be a tuple of all the scales and shift\nparameters in adaLN. In the DiT, S(i) is obtained through a block-specific MLP S(i) = f (i)(c+t),\nwhere c and t denotes the class condition and time embedding, respectively. However, in adaLN-\nsingle, one global set of shifts and scales are computed as S = f(t) only at the first block\nwhich is shared across all the blocks. Then, S(i) is obtained as S(i) = g(S, E(i)), where g is a\nsummation function, and E(i) is a layer-specific trainable embedding with the same shape as S,\nwhich adaptively adjusts the scale and shift parameters in different blocks.\n\u2022 Re-parameterization. To utilize the aforementioned pretrained weights, all E(i)\u2019s are initialized to\nvalues that yield the same S(i) as the DiT without c for a selected t (empirically, we use t = 500).\nThis design effectively replaces the layer-specific MLPs with a global MLP and layer-specific\ntrainable embeddings while preserving compatibility with the pretrained weights.\nExperiments demonstrate that incorporating a global MLP and layer-wise embeddings for time-step\ninformation, as well as cross-attention layers for handling textual information, persists the model\u2019s\ngenerative abilities while effectively reducing its size.\n2.4\nDATASET CONSTRUCTION\nImage-text pair auto-labeling.\nThe captions of the LAION dataset exhibit various issues, such\nas text-image misalignment, deficient descriptions, and infrequent vocabulary as shown in Figure 3.\nTo generate captions with high information density, we leverage the state-of-the-art vision-language\nmodel LLaVA (Liu et al., 2023). Employing the prompt, \u201cDescribe this image and its style in a very\ndetailed manner\u201d, we have significantly improved the quality of captions, as shown in Figure 3.\nHowever, it is worth noting that the LAION dataset predominantly comprises of simplistic prod-\nuct previews from shopping websites, which are not ideal for training text-to-image generation that\nseeks diversity in object combinations. Consequently, we have opted to utilize the SAM dataset (Kir-\nillov et al., 2023), which is originally used for segmentation tasks but features imagery rich in diverse\nobjects. By applying LLaVA to SAM, we have successfully acquired high-quality text-image pairs\ncharacterized by a high concept density, as shown in Figure 10 and Figure 11 in the Appendix.\nIn the third stage, we construct our training dataset by incorporating JourneyDB (Pan et al., 2023)\nand a 10M internal dataset to enhance the aesthetic quality of generated images beyond realistic\nphotographs. Refer to Appendix A.5 for details.\nTable 1: Statistics of noun concepts for different datasets.\nVN: valid distinct nouns (appearing more than 10 times);\nDN: total distinct nouns; Average: average noun count\nper image.\nDataset\nVN/DN\nTotal Noun\nAverage\nLAION\n210K/2461K = 8.5%\n72.0M\n6.4/Img\nLAION-LLaVA\n85K/646K = 13.3%\n233.9M\n20.9/Img\nSAM-LLaVA\n23K/124K = 18.6%\n327.9M\n29.3/Img\nInternal\n152K/582K = 26.1%\n136.6M\n12.2/Img\nAs a result,\nwe show the vocabu-\nlary analysis (NLTK, 2023) in Ta-\nble 1, and we define the valid dis-\ntinct nouns as those appearing more\nthan 10 times in the dataset. We apply\nLLaVA on LAION to generate LAION-\nLLaVA. The LAION dataset has 2.46\nM distinct nouns, but only 8.5% are\nvalid. This valid noun proportion sig-\nnificantly increases from 8.5% to 13.3%\nwith LLaVA-labeled captions. Despite\nLAION\u2019s original captions containing a staggering 210K distinct nouns, its total noun number is a\nmere 72M. However, LAION-LLaVA contains 234M noun numbers with 85K distinct nouns, and\nthe average number of nouns per image increases from 6.4 to 21, indicating the incompleteness of\nthe original LAION captions. Additionally, SAM-LLaVA outperforms LAION-LLaVA with a total\nnoun number of 328M and 30 nouns per image, demonstrating SAM contains richer objectives and\nsuperior informative density per image. Lastly, the internal data also ensures sufficient valid nouns\nand average information density for fine-tuning. LLaVA-labeled captions significantly increase the\nvalid ratio and average noun count per image, improving concept density.\n3\nEXPERIMENT\nThis section begins by outlining the detailed training and evaluation protocols. Subsequently, we\nprovide comprehensive comparisons across three main metrics. We then delve into the critical de-\n6\nTechnical Report\nsigns implemented in PIXART-\u03b1 to achieve superior efficiency and effectiveness through ablation\nstudies. Finally, we demonstrate the versatility of our PIXART-\u03b1 through application extensions.\n3.1\nIMPLEMENTATION DETAILS\nTraining Details. We follow Imagen (Saharia et al., 2022) and DeepFloyd (DeepFloyd, 2023) to\nemploy the T5 large language model (i.e., 4.3B Flan-T5-XXL) as the text encoder for conditional\nfeature extraction, and use DiT-XL/2 (Peebles & Xie, 2023) as our base network architecture. Unlike\nprevious works that extract a standard and fixed 77 text tokens, we adjust the length of extracted\ntext tokens to 120, as the caption curated in PIXART-\u03b1 is much denser to provide more fine-grained\ndetails. To capture the latent features of input images, we employ a pre-trained and frozen VAE from\nLDM (Rombach et al., 2022). Before feeding the images into the VAE, we resize and center-crop\nthem to have the same size. We also employ multi-aspect augmentation introduced in SDXL (Podell\net al., 2023) to enable arbitrary aspect image generation. The AdamW optimizer (Loshchilov &\nHutter, 2017) is utilized with a weight decay of 0.03 and a constant 2e-5 learning rate. Our final\nmodel is trained on 64 V100 for approximately 26 days. See more details in Appendix A.5.\nEvaluation Metrics.\nWe comprehensively evaluate PIXART-\u03b1 via three primary metrics, i.e.,\nFr\u00b4echet Inception Distance (FID) (Heusel et al., 2017) on MSCOCO dataset (Lin et al., 2014), com-\npositionality on T2I-CompBench (Huang et al., 2023), and human-preference rate on user study.\n3.2\nPERFORMANCE COMPARISONS AND ANALYSIS\nFidelity Assessment. The FID is a metric to evaluate the quality of generated images. The compar-\nison between our method and other methods in terms of FID and their training time is summarized\nin Table 2. When tested for zero-shot performance on the COCO dataset, PIXART-\u03b1 achieves a FID\nscore of 7.32. It is particularly notable as it is accomplished in merely 12% of the training time (753\nvs. 6250 A100 GPU days) and merely 1.25% of the training samples (25M vs. 2B images) relative\nto the second most efficient method. Compared to state-of-the-art methods typically trained using\nsubstantial resources, PIXART-\u03b1 remarkably consumes approximately 2% of the training resources\nwhile achieving a comparable FID performance. Although the best-performing model (RAPHEAL)\nexhibits a lower FID, it relies on unaffordable resources (i.e., 200\u00d7 more training samples, 80\u00d7\nlonger training time, and 5\u00d7 more network parameters than PIXART-\u03b1). We argue that FID may\nnot be an appropriate metric for image quality evaluation, and it is more appropriate to use the\nevaluation of human users, as stated in Appendix A.8. We leave scaling of PIXART-\u03b1 for future\nexploration for performance enhancement.\nAlignment Assessment. Beyond the above evaluation, we also assess the alignment between the\ngenerated images and text condition using T2I-Compbench (Huang et al., 2023), a comprehensive\nbenchmark for evaluating the compositional text-to-image generation capability. As depicted in Ta-\nble 3, we evaluate several crucial aspects, including attribute binding, object relationships, and com-\nplex compositions. PIXART-\u03b1 exhibited outstanding performance across nearly all (5/6) evaluation\nmetrics. This remarkable performance is primarily attributed to the text-image alignment learning\nin Stage 2 training described in Section 2.2, where high-quality text-image pairs were leveraged to\nachieve superior alignment capabilities.\nUser Study. While quantitative evaluation metrics measure the overall distribution of two image\nsets, they may not comprehensively evaluate the visual quality of the images. Consequently, we\nconducted a user study to supplement our evaluation and provide a more intuitive assessment of\nPIXART-\u03b1\u2019s performance. Since user study involves human evaluators and can be time-consuming,\nwe selected the top-performing models, namely DALLE-2, SDv2, SDXL, and DeepFloyd, which\nare accessible through APIs and capable of generating images.\nFor each model, we employ a consistent set of 300 prompts from Feng et al. (2023) to generate\nimages. These images are then distributed among 50 individuals for evaluation. Participants are\nasked to rank each model based on the perceptual quality of the generated images and the precision\nof alignments between the text prompts and the corresponding images. The results presented in\n3To ensure fairness, we convert the V100 GPU days (1656) of our training to A100 GPU days (753),\nassuming a 2.2\u00d7 speedup in U-Net training on A100 compared to V100, or equivalent to 332 A100 GPU days\nwith a 5\u00d7 speedup in Transformer training, as per Rombach et al. (2022); NVIDIA (2023).\n7\nTechnical Report\nTable 2: We thoroughly compare the PIXART-\u03b1 with recent T2I models, considering several essen-\ntial factors: model size, the total volume of training images, COCO FID-30K scores (zero-shot), and\nthe computational cost (GPU days3). Our highly effective approach significantly reduces resource\nconsumption, including training data usage and training time. The baseline data is sourced from\nGigaGAN (Kang et al., 2023). \u2018+\u2019 in the table denotes an unknown internal dataset size.\nMethod\nType\n#Params\n#Images\nFID-30K\u2193\nGPU days\nDALL\u00b7E\nDiff\n12.0B\n250M\n27.50\n-\nGLIDE\nDiff\n5.0B\n250M\n12.24\n-\nLDM\nDiff\n1.4B\n400M\n12.64\n-\nDALL\u00b7E 2\nDiff\n6.5B\n650M\n10.39\n41,667 A100\nSDv1.5\nDiff\n0.9B\n2000M\n9.62\n6,250 A100\nGigaGAN\nGAN\n0.9B\n2700M\n9.09\n4,783 A100\nImagen\nDiff\n3.0B\n860M\n7.27\n7,132 A100\nRAPHAEL\nDiff\n3.0B\n5000M+\n6.61\n60,000 A100\nPIXART-\u03b1\nDiff\n0.6B\n25M\n7.32\n753 A100\nTable 3: Alignment evaluation on T2I-CompBench. PIXART-\u03b1 demonstrated exceptional perfor-\nmance in attribute binding, object relationships, and complex compositions, indicating our method\nachieves superior compositional generation ability. We highlight the best value in blue , and the\nsecond-best value in green . The baseline data are sourced from Huang et al. (2023).\nModel\nAttribute Binding\nObject Relationship\nComplex\u2191\nColor \u2191\nShape\u2191\nTexture\u2191\nSpatial\u2191\nNon-Spatial\u2191\nStable v1.4\n0.3765\n0.3576\n0.4156\n0.1246\n0.3079\n0.3080\nStable v2\n0.5065\n0.4221\n0.4922\n0.1342\n0.3096\n0.3386\nComposable v2\n0.4063\n0.3299\n0.3645\n0.0800\n0.2980\n0.2898\nStructured v2\n0.4990\n0.4218\n0.4900\n0.1386\n0.3111\n0.3355\nAttn-Exct v2\n0.6400\n0.4517\n0.5963\n0.1455\n0.3109\n0.3401\nGORS\n0.6603\n0.4785\n0.6287\n0.1815\n0.3193\n0.3328\nDalle-2\n0.5750\n0.5464\n0.6374\n0.1283\n0.3043\n0.3696\nSDXL\n0.6369\n0.5408\n0.5637\n0.2032\n0.3110\n0.4091\nPIXART-\u03b1\n0.6886\n0.5582\n0.7044\n0.2082\n0.3179\n0.4117\nFigure 5 clearly indicate that PIXART-\u03b1 excels in both higher fidelity and superior alignment. For\nexample, compared to SDv2, a current top-tier T2I model, PIXART-\u03b1 exhibits a 7.2% improvement\nin image quality and a substantial 42.4% enhancement in alignment.\n3.3\nABLATION STUDY\nWe then conduct ablation studies on the crucial modifications discussed in Section 2.3, including\nstructure modifications and re-parameterization design. In Figure 6, we provide visual results and\nperform a FID analysis. We randomly choose 8 prompts from the SAM test set for visualization and\ncompute the zero-shot FID-5K score on the SAM dataset. Details are described below.\n\u201cw/o re-param\u201d results are generated from the model trained from scratch without re-\nparameterization design. We supplemented with an additional 200K iterations to compensate for\nthe missing iterations from the pretraining stage for a fair comparison. \u201cadaLN\u201d results are from\nthe model following the DiT structure to use the sum of time and text feature as input to the MLP\nlayer for the scale and shift parameters within each block. \u201cadaLN-single\u201d results are obtained from\nthe model using Transformer blocks with the adaLN-single module in Section 2.3. In both \u201cadaLN\u201d\nand \u201cadaLN-single\u201d, we employ the re-parameterization design and training for 200K iterations.\nAs depicted in Figure 6, despite \u201cadaLN\u201d performing lower FID, its visual results are on par with\nour \u201cadaLN-single\u201d design. The GPU memory consumption of \u201cadaLN\u201d is 29GB, whereas \u201cadaLN-\nsingle\u201d achieves a reduction to 23GB, saving 21% in GPU memory consumption. Furthermore,\nconsidering the model parameters, the \u201cadaLN\u201d method consumes 833M, whereas our approach re-\nduces to a mere 611M, resulting in an impressive 26% reduction. \u201cadaLN-single-L (Ours)\u201d results\nare generated from the model with same setting as \u201cadaLN-single\u201d, but training for a Longer train-\ning period of 1500K iterations. Considering memory and parameter efficiency, we incorporate the\n\u201cadaLN-single-L\u201d into our final design.\n8\nTechnical Report\nAlignment\nAlignment\nAlignment\nAlignment\nQuality\nQuality\nQuality\nQuality\nPercentage (%)\nFigure 5: User study on 300 fixed prompts from Feng et al. (2023). The ratio values indicate\nthe percentages of participants preferring the corresponding model. PIXART-\u03b1 achieves a superior\nperformance in both quality and alignment.\nadaLN\nadaLN-single \nw/o re-param\nadaLN-single-L \n(Ours) \nGT\n18.20\n29G\n22.37\n23G\n23.30\n23G\n22.30\n23G\nGPU \nMemory\nFID\nFigure 6: Left: Visual comparison of ablation studies are presented. Right: Zero-shot FID-2K on\nSAM, and GPU memory usage. Our method is on par with the \u201cadaLN\u201d and saves 21% in GPU\nmemory. Better zoom in 200%.\nThe visual results clearly indicate that, although the differences in FID scores between the \u201cadaLN\u201d\nand \u201cadaLN-single\u201d models are relatively small, a significant discrepancy exists in their visual out-\ncomes. The \u201cw/o re-param\u201d model consistently displays distorted target images and lacks crucial\ndetails across the entire test set.\n4\nRELATED WORK\nWe review related works in three aspects: Denoising diffusion probabilistic models (DDPM), Latent\nDiffusion Model, and Diffusion Transformer. More related works can be found in Appendix A.1.\nDDPMs (Ho et al., 2020; Sohl-Dickstein et al., 2015) have emerged as highly successful approaches\nfor image generation, which employs an iterative denoising process to transform Gaussian noise into\nan image. Latent Diffusion Model (Rombach et al., 2022) enhances the traditional DDPMs by em-\nploying score-matching on the image latent space and introducing cross-attention-based controlling.\nWitnessed the success of Transformer architecture on many computer vision tasks, Diffusion Trans-\nformer (DiT) (Peebles & Xie, 2023) and its variant (Bao et al., 2023; Zheng et al., 2023) further\nreplace the Convolutional-based U-Net (Ronneberger et al., 2015) backbone with Transformers for\nincreased scalability.\n9\nTechnical Report\n5\nCONCLUSION\nIn this paper, we introduced PIXART-\u03b1, a Transformer-based text-to-image (T2I) diffusion model,\nwhich achieves superior image generation quality while significantly reducing training costs and\nCO2 emissions. Our three core designs, including the training strategy decomposition, efficient T2I\nTransformer and high-informative data, contribute to the success of PIXART-\u03b1. Through extensive\nexperiments, we have demonstrated that PIXART-\u03b1 achieves near-commercial application standards\nin image generation quality. With the above designs, PIXART-\u03b1 provides new insights to the AIGC\ncommunity and startups, enabling them to build their own high-quality yet low-cost T2I models. We\nhope that our work inspires further innovation and advancements in this field.\nAcknowledgement.\nWe would like to express our gratitude to Shuchen Xue for identifying and\ncorrecting the FID score in the paper.\n10\nTechnical Report\nA\nAPPENDIX\nA.1\nRELATED WORK\nA.1.1\nDENOISING DIFFUSION PROBABILISTIC MODELS\nDiffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) and score-based generative mod-\nels (Song & Ermon, 2019; Song et al., 2021) have emerged as highly successful approaches for\nimage generation, surpassing previous generative models such as GANs (Goodfellow et al., 2014),\nVAEs (Kingma & Welling, 2013), and Flow (Rezende & Mohamed, 2015). Unlike traditional mod-\nels that directly map from a Gaussian distribution to the data distribution, diffusion models employ\nan iterative denoising process to transform Gaussian noise into an image that follows the data dis-\ntribution. This process can be reversely learned from an untrainable forward process, where a small\namount of Gaussian noise is iteratively added to the original image.\nA.1.2\nLATENT DIFFUSION MODEL\nLatent Diffusion Model (a.k.a. Stable diffusion) (Rombach et al., 2022) is a recent advancement\nin diffusion models. This approach enhances the traditional diffusion model by employing score-\nmatching on the image latent space and introducing cross-attention-based controlling. The results\nobtained with this approach have been impressive, particularly in tasks involving high-density image\ngeneration, such as text-to-image synthesis. This has served as a source of inspiration for numerous\nsubsequent works aimed at improving text-to-image synthesis, including those by Saharia et al.\n(2022); Balaji et al. (2022); Feng et al. (2023); Xue et al. (2023b); Podell et al. (2023), and others.\nAdditionally, Stable diffusion and its variants have been effectively combined with various low-cost\nfine-tuning (Hu et al., 2021; Xie et al., 2023) and customization (Zhang et al., 2023; Mou et al.,\n2023) technologies.\nA.1.3\nDIFFUSION TRANSFORMER\nTransformer architecture (Vaswani et al., 2017) have achieved great success in language mod-\nels (Radford et al., 2018; 2019), and many recent works (Dosovitskiy et al., 2020a; He et al.,\n2022) show it is also a promising architecture on many computer vision tasks like image classi-\nfication (Touvron et al., 2021; Zhou et al., 2021; Yuan et al., 2021; Han et al., 2021), object de-\ntection (Liu et al., 2021; Wang et al., 2021; 2022; Ge et al., 2023; Carion et al., 2020), semantic\nsegmentation (Zheng et al., 2021; Xie et al., 2021; Strudel et al., 2021) and so on (Sun et al., 2020;\nLi et al., 2022b; Zhao et al., 2021; Liu et al., 2022; He et al., 2022; Li et al., 2022a). The Diffusion\nTransformer (DiT) (Peebles & Xie, 2023) and its variant (Bao et al., 2023; Zheng et al., 2023) follow\nthe step to further replace the Convolutional-based U-Net (Ronneberger et al., 2015) backbone with\nTransformers. This architectural choice brings about increased scalability compared to U-Net-based\ndiffusion models, allowing for the straightforward expansion of its parameters. In our paper, we\nleverage DiT as a scalable foundational model and adapt it for text-to-image generation tasks.\nA.2\nPIXART-\u03b1 vs. MIDJOURNEY\nIn Figure 7, we present the images generated using PIXART-\u03b1 and the current SOTA product-level\nmethod Midjourney (Midjourney, 2023) with randomly sampled prompts online. Here, we conceal\nthe annotations of images belonging to which method. Readers are encouraged to make assessments\nbased on the prompts provided. The answers will be disclosed at the end of the appendix.\nA.3\nPIXART-\u03b1 vs. PRESTIGIOUS DIFFUSION MODELS\nIn Figure 8 and 9, we present the comparison results using a test prompt selected by RAPHAEL. The\ninstances depicted here exhibit performance that is on par with, or even surpasses, that of existing\npowerful generative models.\n11\nTechnical Report\nA.4\nAUTO-LABELING TECHNIQUES\nTo generate captions with high information density, we leverage state-of-the-art vision-language\nmodels LLaVA (Liu et al., 2023). Employing the prompt, \u201cDescribe this image and its style in a\nvery detailed manner\u201d, we have significantly improved the quality of captions. We show the prompt\ndesign and process of auto-labeling in Figure 10. More image-text pair samples on the SAM dataset\nare shown in Figure 11.\nA.5\nADDITIONAL IMPLEMENTATION DETAILS\nWe include detailed information about all of our PIXART-\u03b1 models in this section. As shown in\nTable 4, among the 256\u00d7256 phases, our model primarily focuses on the text-to-image alignment\nstage, with less time on fine-tuning and only 1/8 of that time spent on ImageNet pixel dependency.\nPIXART-\u03b1 model details.\nFor the embedding of input timesteps, we employ a 256-dimensional\nfrequency embedding (Dhariwal & Nichol, 2021). This is followed by a two-layer MLP that features\na dimensionality matching the transformer\u2019s hidden size, coupled with SiLU activations. We adopt\nthe DiT-XL model, which has 28 Transformer blocks in total for better performance, and the patch\nsize of the PatchEmbed layer in ViT (Dosovitskiy et al., 2020b) is 2\u00d7.\nMulti-scale training.\nInspired by Podell et al. (2023), we incorporate the multi-scale training\nstrategy into our pipeline. Specifically, We divide the image size into 40 buckets with different\naspect ratios, each with varying aspect ratios ranging from 0.25 to 4, mirroring the method used in\nSDXL. During optimization, a training batch is composed using images from a single bucket, and we\nalternate the bucket sizes for each training step. In practice, we only apply multi-scale training in the\nhigh-aesthetics stage after pretraining the model at a fixed aspect ratio and resolution (i.e. 256px).\nWe adopt the positional encoding trick in DiffFit (Xie et al., 2023) since the image resolution and\naspect change during different training stages.\nAdditional time consumption.\nBeside the training time discussed in Table 4, data labeling and\nVAE training may need additional time. We treat the pre-trained VAE as a ready-made component of\na model zoo, the same as pre-trained CLIP/T5-XXL text encoder, and our total training process does\nnot include the training of VAE. However, our attempt to train a VAE resulted in an approximate\ntraining duration of 25 hours, utilizing 64 V100 GPUs on the OpenImage dataset. As for auto-\nlabeling, we use LLAVA-7B to generate captions. LLaVA\u2019s annotation time on the SAM dataset\nis approximately 24 hours with 64 V100 GPUs. To ensure a fair comparison, we have temporarily\nexcluded the training time and data quantity of VAE training, T5 training time, and LLaVA auto-\nlabeling time.\nSampling algorithm.\nIn this study, we incorporated three sampling algorithms, namely iD-\nDPM (Nichol & Dhariwal, 2021), DPM-Solver (Lu et al., 2022), and SA-Solver (Xue et al., 2023a).\nWe observe these three algorithms perform similarly in terms of semantic control, albeit with minor\ndifferences in sampling frequency and color representation. To optimize computational efficiency,\nwe ultimately chose to employ the DPM-Solver with 20 inference steps.\nTable 4: We report detailed information about every PIXART-\u03b1 training stage in our paper. Note that\nHQ (High Quality) dataset here includes 4M JourneyDB (Pan et al., 2023) and 10M internal data.\nThe count of GPU days excludes the time for VAE feature extraction and T5 text feature extraction,\nas we offline prepare both features in advance so that they are not part of the training process and\ncontribute no extra time to it.\nMethod\nStage\nImage Resolution\n#Images\nTraining Steps (K)\nBatch Size\nLearning Rate\nGPU days (V100)\nPIXART-\u03b1\nPixel dependency\n256\u00d7256\n1M ImageNet\n300\n128\u00d78\n2\u00d710\u22125\n88\nPIXART-\u03b1\nText-Image align\n256\u00d7256\n10M SAM\n150\n178\u00d764\n2\u00d710\u22125\n672\nPIXART-\u03b1\nHigh aesthetics\n256\u00d7256\n14M HQ\n90\n178\u00d764\n2\u00d710\u22125\n416\nPIXART-\u03b1\nHigh aesthetics\n512\u00d7512\n14M HQ\n100\n40\u00d764\n2\u00d710\u22125\n320\nPIXART-\u03b1\nHigh aesthetics\n1024\u00d71024\n14M HQ\n16\n12\u00d732\n2\u00d710\u22125\n160\n12\nTechnical Report\nA.6\nHYPER-PARAMETERS ANALYSIS\nIn Figure 20, we illustrate the variations in the model\u2019s metrics under different configurations across\nvarious datasets. we first investigate FID for the model and plot FID-vs-CLIP curves in Figure 20a\nfor 10k text-image paed from MSCOCO. The results show a marginal enhancement over SDv1.5.\nIn Figure 20b and 20c, we demonstrate the corresponding T2ICompBench scores across a range of\nclassifier-free guidance (cfg) (Ho & Salimans, 2022) scales. The outcomes reveal a consistent and\ncommendable model performance under these varying scales.\nA.7\nMORE IMAGES GENERATED BY PIXART-\u03b1\nMore visual results generated by PIXART-\u03b1 are shown in Figure 12, 13, and 14. The samples gen-\nerated by PIXART-\u03b1 demonstrate outstanding quality, marked by their exceptional fidelity and pre-\ncision in faithfully adhering to the given textual descriptions. As depicted in Figure 15, PIXART-\u03b1\ndemonstrates the ability to synthesize high-resolution images up to 1024 \u00d7 1024 pixels and contains\nrich details, and is capable of generating images with arbitrary aspect ratios, enhancing its versatil-\nity for real-world applications. Figure 16 illustrates PIXART-\u03b1\u2019s remarkable capacity to manipulate\nimage styles through text prompts directly, demonstrating its versatility and creativity.\nA.8\nDISCCUSION OF FID METRIC FOR EVALUATING IMAGE QUALITY\nDuring our experiments, we observed that the FID (Fr\u00b4echet Inception Distance) score may not ac-\ncurately reflect the visual quality of generated images. Recent studies such as SDXL (Podell et al.,\n2023) and Pick-a-pic (Kirstain et al., 2023) have presented evidence suggesting that the COCO\nzero-shot FID is negatively correlated with visual aesthetics.\nFurthermore, it has been stated by Betzalel et al. (Betzalel et al., 2022) that the feature extraction\nnetwork used in FID is pretrained on the ImageNet dataset, which exhibits limited overlap with\nthe current text-to-image generation data. Consequently, FID may not be an appropriate metric for\nevaluating the generative performance of such models, and (Betzalel et al., 2022) recommended\nemploying human evaluators for more suitable assessments.\nThus, we conducted a user study to validate the effectiveness of our method.\nA.9\nCUSTOMIZED EXTENSION\nIn text-to-image generation, the ability to customize generated outputs to a specific style or condition\nis a crucial application. We extend the capabilities of PIXART-\u03b1 by incorporating two commonly\nused customization methods: DreamBooth (Ruiz et al., 2022) and ControlNet (Zhang et al., 2023).\nDreamBooth.\nDreamBooth can be seamlessly applied to PIXART-\u03b1 without further modifica-\ntions. The process entails fine-tuning PIXART-\u03b1 using a learning rate of 5e-6 for 300 steps, without\nthe incorporation of a class-preservation loss.\nAs depicted in Figure 17a, given a few images and text prompts, PIXART-\u03b1 demonstrates the capac-\nity to generate high-fidelity images. These images present natural interactions with the environment\nunder various lighting conditions. Additionally, PIXART-\u03b1 is also capable of precisely modifying\nthe attribute of a specific object such as color, as shown in 17b. Our appealing visual results demon-\nstrate PIXART-\u03b1 can generate images of exceptional quality and its strong capability for customized\nextension.\nControlNet.\nFollowing the general design of ControlNet (Zhang et al., 2023), we freeze each DiT\nBlock and create a trainable copy, augmenting with two zero linear layers before and after it. The\ncontrol signal c is obtained by applying the same VAE to the control image and is shared among all\nblocks. For each block, we process the control signal c by first passing it through the first zero linear\nlayer, adding it to the layer input x, and then feeding it into the trainable copy and the second zero\nlinear layer. The processed control signal is then added to the output y of the frozen block, which is\nobtained from input x. We trained the ControlNet on HED (Xie & Tu, 2015) signals using a learning\nrate of 5e-6 for 20,000 steps.\n13\nTechnical Report\nAs depicted in Figure 18, when provided with a reference image and control signals, such as edge\nmaps, we leverage various text prompts to generate a wide range of high-fidelity and diverse images.\nOur results demonstrate the capacity of PIXART-\u03b1 to yield personalized extensions of exceptional\nquality.\nA.10\nDISCUSSION ON TRANSFORMER vs. U-NET\nThe Transformer-based network\u2019s superiority over convolutional networks has been widely estab-\nlished in various studies, showcasing attributes such as robustness (Zhou et al., 2022; Xie et al.,\n2021), effective modality fusion (Girdhar et al., 2023), and scalability (Peebles & Xie, 2023). Sim-\nilarly, the findings on multi-modality fusion are consistent with our observations in this study com-\npared to the CNN-based generator (U-Net). For instance, Table 3 illustrates that our model, PIXART-\n\u03b1, significantly outperforms prevalent U-Net generators in terms of compositionality. This advan-\ntage is not solely due to the high-quality alignment achieved in the second training stage but also\nto the multi-head attention-based fusion mechanism, which excels at modeling long dependencies.\nThis mechanism effectively integrates compositional semantic information, guiding the generation\nof vision latent vectors more efficiently and producing images that closely align with the input texts.\nThese findings underscore the unique advantages of Transformer architectures in effectively fusing\nmulti-modal information.\nA.11\nLIMITATIONS & FAILURE CASES\nIn Figure 19, we highlight the model\u2019s failure cases in red text and yellow circle. Our analysis reveals\nthe model\u2019s weaknesses in accurately controlling the number of targets and handling specific details,\nsuch as features of human hands. Additionally, the model\u2019s text generation capability is somewhat\nweak due to our data\u2019s limited number of font and letter-related images. We aim to explore these\nunresolved issues in the generation field, enhancing the model\u2019s abilities in text generation, detail\ncontrol, and quantity control in the future.\nA.12\nUNVEIL THE ANSWER\nIn Figure 7, we present a comparison between PIXART-\u03b1 and Midjourney and conceal the corre-\nspondence between images and their respective methods, inviting the readers to guess. Finally, in\nFigure 21, we unveil the answer to this question. It is difficult to distinguish between PIXART-\u03b1 and\nMidjourney, which demonstrates PIXART-\u03b1\u2019s exceptional performance.\n14\nTechnical Report\nThe image features a woman wearing a red shirt with \nan icon. She appears to be posing for the camera, and \nher outfit includes a pair of jeans. The woman seems \nto be in a good mood, as she is smiling. The \nbackground of the image is blurry, focusing more on \nthe woman and her attire. \nArt collection style and fashion shoot, in the style of \nmade of glass, dark blue and light pink, paul rand, \nsolarpunk, camille vivier, beth didonato hair, \nbarbiecore, hyper-realistic.\nBeautiful scene \nA small cactus with a happy face in the Sahara desert\nA dog that has been \nmeditating all the time\nPirate ship trapped in a cosmic maelstrom nebula, \nrendered in cosmic beach whirlpool engine, \nvolumetric lighting, spectacular, ambient lights, light \npollution, cinematic atmosphere, art nouveau style, \nillustration art artwork by SenseiJaye, intricate detail.\nposter of a mechanical cat, technical Schematics \nviewed from front and side view on light white \nblueprint paper, illustration drafting style, illustration, \ntypography, conceptual art, dark fantasy steampunk, \ncinematic, dark fantasy.\nFigure 7: Comparisons with Midjourney. The prompts used here are randomly sampled online.\nTo ensure a fair comparison, we select the first result generated by both models. We encourage\nreaders to guess which image corresponds to Midjourney and which corresponds to PIXART-\u03b1.\nThe answer is revealed at the end of the paper.\n15\nTechnical Report\nRAPHAEL\nStable Diffusion XL\nDeepFloyd\nDALL-E 2\nERNIE-ViLG 2.0\nPixArt-\u03b1\n1. A parrot with a pearl earring, Vermeer style.\n2. A car playing soccer, digital art.\n3. A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\n4. Moonlight Maiden, cute girl in school uniform, long white hair, standing under the moon, \ncelluloid style, Japanese manga style.\n5. Street shot of a fashionable Chinese lady in Shanghai, wearing black high-waisted trousers.\n6. Half human, half robot, repaired human, human flesh warrior, mech display, man in mech, cyberpunk.\nFigure 8: Comparisons of PIXART-\u03b1 with recent representative generators, Stable Diffusion XL,\nDeepFloyd, DALL-E 2, ERNIE-ViLG 2.0, and RAPHAEL. They are given the same prompts as\nin RAPHAEL(Xue et al., 2023b), where the words that the human artists yearn to preserve within\nthe generated images are highlighted in red. The specific prompts for each row are provided at the\nbottom of the figure. Better zoom in 200%.\n16\nTechnical Report\n1. A cute little matte low poly isometric cherry blossom forest island, waterfalls, lighting, soft shadows, \ntrending on Artstation, 3d render, monument valley, fez video game.\n2. A shanty version of Tokyo, new rustic style, bold colors with all colors palette, video game, genshin, \ntribe, fantasy, overwatch.\n3. Cartoon characters, mini characters, figures, illustrations, flower fairy, green dress, brown hair, curly \nlong hair, elf-like wings, many flowers and leaves, natural scenery, golden eyes, detailed light and \nshadow , a high degree of detail.\n4. Cartoon characters, mini characters, hand-made, illustrations, robot kids, color expressions, boy, \nshort brown hair, curly hair, blue eyes, technological age, cyberpunk, big eyes, cute, mini, detailed \nlight and shadow, high detail.\nDALL-E 2\nMidjourney\nV5.1\nStable \nDiffusion XL\nERNIE \nViLG 2.0\nDeepFloyd\nRAPHAEL\nPixArt-\u03b1\nFigure 9: The prompts (Xue et al., 2023b) for each column are given in the figure. We give the com-\nparisons between DALL-E 2 Midjourney v5.1, Stable Diffusion XL, ERNIE ViLG 2.0, DeepFloyd,\nand RAPHAEL. They are given the same prompts, where the words that the human artists yearn to\npreserve within the generated images are highlighted in red. Better zoom in 200%.\n17\nTechnical Report\nC\nC\nGive the caption of this image \n\u2018What science says about pu\u2019erh\ntea? \u2019, describe this image and its \nstyle in a very detailed manner.\nReceived\nThe image features a close-up of a \ncup of tea with a saucer on a \nwooden table. The tea is described \nas \"pu'erh tea,\" which is a type of \nChinese tea known for its health \nbenefits. The scene is set in a dimly \nlit room. The presence of a potted \nplant in the background adds a touch \nof nature and freshness to the scene. \nC\nC\nDescribe this image and its style in \na very detailed manner.\nReceived\nThe image features a large, open \ndining area with a high ceiling and a \nclock hanging from the wall. The \nroom is filled with tables and chairs, \ncreating a inviting atmosphere. The \nuse of large windows and a clock \nhanging from the wall adds to the \noverall aesthetic, creating a visually \nappealing and functional space for \ndining and socializing. \nFigure 10: We present auto-labeling with custom prompts for LAION (left) and SAM (right). The\nwords highlighted in green represent the original caption in LAION, while those marked in red\nindicate the detailed captions labeled by LLaVA.\nSAM\n+\nLLaVA\nThe image features a large statue of a \ndragon, which is a mythical creature \nwith a long body, wings, and a tail. The \ndragon statue is situated in a park, \nsurrounded by a lush green field and a \nflower garden. The dragon statue is \npositioned in front of a temple, which \nadds to the overall aesthetic and \ncultural significance of the scene. \nThe image features a large, open dining \narea with a high ceiling and a clock \nhanging from the wall. The room is filled \nwith tables and chairs, creating an inviting \natmosphere. The use of large windows \nand a clock hanging from the wall adds to \nthe overall aesthetic, creating a visually \nappealing and functional space for dining \nand socializing. \nThe image features a man wearing \na blue uniform, likely a police \nofficer, standing on a city street. \nHe is walking down the sidewalk, \nsurrounded by various shops and \npeople. The scene is set in a busy \nurban environment, with a mix of \npedestrians, shops\nSamples\nFigure 11: Examples from the SAM dataset using LLaVA-produced labels. The detailed image\ndescriptions in LLaVA captions can aid the model to grasp more concepts per iteration and boost\ntext-image alignment efficiency.\n18\nTechnical Report\nmarvel movie character, iron man, dress up to match \nmovie character, full body photo, American apartment, \nlying down, life in distress, messy, lost hope, food, \nwine, hd, 8k, real, reality, super detail, 8k post photo \nmanipulation, real photo\nA worker that looks like a mixture of cow and horse \nis working hard to type code\nA female painter with a brush in hand, white background, painting, looking very powerful.\nA baby painter trying to draw very simple \npicture, white background\nknolling of a drawing tools and books, \nknowledge, white background\nreal beautiful woman, Chinese\nA snowy mountain\nChinese painting of grapes\nhappy\nI want to supplement vitamin c, please \nhelp me paint related food.\nAn alien octopus floats through a portal \nreading a newspaper\nFigure 12: The samples generated by PIXART-\u03b1 demonstrate outstanding quality, marked by an\nexceptional level of fidelity and precision in aligning with the given textual descriptions. Better\nzoom in 200%.\n19\nTechnical Report\nA silhouette of a grand piano overlooking a dusky \ncityscape viewed from a top-floor penthouse, rendered \nin the bold and vivid style of a vintage travel poster.\nA vibrant yellow banana-shaped couch sits in a cozy \nliving room, its curve cradling a pile of colorful \ncushions. on the wooden floor, a patterned rug adds \na touch of eclectic charm, and a potted plant sits in \nthe corner, reaching towards the sunlight filtering \nthrough the window.\nA 4k dslr image of a lemur wearing a red magician hat and a blue coat performing \nmagic tricks with cards in a garden.\nnature vs human nature, surreal, UHD, 8k, \nhyper details, rich colors, photograph\nA painter study hard to learn how to draw \nwith many concepts in the air\nA alpaca made of colorful building blocks, \ncyberpunk\nA man looks up at the starry sky, lonely\nstars, water, brilliantly, gorgeous large scale \nscene, a little girl, in the style of dreamy realism\nA fisheye lens view of a turtle sitting in a forest.\nA dog is reading a thick book\nA jellyfish riding a rocket\nFigure 13: The samples generated by PIXART-\u03b1 demonstrate outstanding quality, marked by an\nexceptional level of fidelity and precision in aligning with the given textual descriptions. Better\nzoom in 200%.\n20\nTechnical Report\nBright scene, aerial view, ancient city, fantasy, \ngorgeous light, mirror reflection, high detail, \nwide angle lens.\nFuturist painting of the building.\nCrocodile in a sweater.\nAn astronaut riding a horse.\nA lion with a dragon's head.\nA painting depicting a red wave outside, trapped \nemotions depicted, full body, Jon Foster, depth, \nDima Dmitriev, fisheye effects, Ray Collins.\nA cute fluffy sentient alien from planet Axor, in \nthe andromeda galaxy, the alien have large \ninnocent eyes and is digitigrade, high detail.\nA person standing on the desert, desert waves, \nhalf red, half blue, sand, illustration, outdoor.\nAn ancient stone Colossus with eye, Stephan \nMartini\u00e8re, dark yellow and light emerald, color \nzone painting, Denis Sarazhin, dark emerald and \nsilver, robotic expressionism, high detail.\nThe girl in the car is filled with goldfish and \nflowers, goldfish can fly, natural posture, \nyouthful energy and pressure, body stretching, \ngoldfish simulation movies in the sky.\nLuffy from ONEPIECE, handsome face, fantasy.\nDesign a letter A, 3D stereoscopic Ice material \nInterior light blue Conceptual product design \nFuturistic Blind box toy Handcrafted Exquisite 3D \neffect Full body display Ultra-high precision.\nFigure 14: The samples generated by PIXART-\u03b1 demonstrate outstanding quality, marked by an\nexceptional level of fidelity and precision in aligning with the given textual descriptions. Better\nzoom in 200%.\n21\nTechnical Report\nHigh detail 1024px\nFigure 15: PIXART-\u03b1 is capable of generating images with resolutions of up to 1024 \u00d7 1024 while\npreserving rich, complex details. Additionally, it can generate images with arbitrary aspect ratios,\nproviding flexibility in image generation.\n22\nTechnical Report\n\u201cVan Gogh \npainting of\u201d\n\u201cPhotography of\u201d\n\u201cPixel art of\u201d\n\u201cpencil drawing of\u201d\n\u201cClaude Monet \npainting of\u201d\nthe black hole in the space\na teacup on the desk\na table top with a vase of flowers on it\na birthday cake\na beautiful flower\nFigure 16: Prompt mixing: PIXART-\u03b1 can directly manipulate the image style with text prompts.\nIn this figure, we generate five outputs using the styles to control the objects . For instance,\nthe second picture of the first sample, located at the left corner of the figure, uses the prompt\n\u201c Pixel Art of\nthe black hole in the space \u201d. Better zoom in 200%.\n23\nTechnical Report\nInput Images\nText prompt: A photo of [V] dog\nText prompt:\n[V] dog is running\nText prompt:\n[V] dog in a doghouse\nText prompt:\n[V] dog in a bucket\nText prompt:\n[V] dog is swimming\n(a) Dreambooth + PIXART-\u03b1 is capable of customized image generation aligned with text prompts.\nText prompt:\n[green] [V] car in garage\nText prompt:\n[white] [V] car over water\nText prompt:\n[yellow] [V] car in street\nText prompt:\n[black] [V] car on highway\nInput Images: \u95ee\u754cM5\nText prompt: A photo of [grey] [V] car\n(b) Dreambooth + PIXART-\u03b1 is capable of color modification of a specific object such as Wenjie M5.\nFigure 17: PIXART-\u03b1 can be combined with Dreambooth. Given a few images and text prompts,\nPIXART-\u03b1 can generate high-fidelity images, that exhibit natural interactions with the environ-\nment 17a, precise modification of the object colors 17b, demonstrating that PIXART-\u03b1 can generate\nimages with exceptional quality, and has a strong capability in customized extension.\n24\nTechnical Report\nReference Image\nFlower\nHED Edge\nIslands\nShells\nBiscuits\nSnow\nFlower-field\nReference Image\nOil Paint\nHED Edge\nStar\nSketch\nCyberpunk\nRenaissance\nVan Gogh\nFigure 18: ControlNet customization samples from PIXART-\u03b1. We use the reference images to\ngenerate the corresponding HED edge images and use them as the control signal for PIXART-\u03b1\nControlNet. Better zoom in 200%.\n25\nTechnical Report\nA stack of 3 books. A green book is on the \ntop, sitting on a red book. The red book is \nin the middle\nThree cats and three dogs sitting on the grass An expressive oil painting of a basketball player \ndunking, depicted as an explosion of  a nebula\nFigure 19: Instances where PIXART-\u03b1 encounters challenges include situations that necessitate\nprecise counting or accurate representation of human limbs. In these cases, the model may face\ndifficulties in providing accurate results.\n0.24\n0.25\n0.26\n14\n16\n18\n20\n22\nCLIP score\nFID\nSDv1.5\nOur\n(a) FID on MSCOCO\n2\n3\n4\n5\n6\n7\n0.52\n0.56\n0.6\n0.64\n0.68\n0.72\n0.76\ncfg scale\nB-VQA\nColor\nShape\nTexture\n(b) T2i-CompBench\n2\n3\n4\n5\n6\n7\n0.2\n0.3\n0.4\n0.5\ncfg scale\nUniDet CLIP 3-in-1\nSpatial\nNon-spatial\nComplex\n(c) T2i-CompBench\nFigure 20: (a) Plotting FID vs. CLIP score for different cfg scales sampled from [1.5, 2.0, 3.0,\n4.0, 5.0, 6.0]. PIXART-\u03b1 shows slight better performance than SDv1.5 on MSCOCO. (b) and (c)\ndemonstrate the ability of PIXART-\u03b1 to maintain robustness across various cfg scales on the T2I-\nCompBench.\n26\nTechnical Report\nThe image features a woman wearing a red shirt with \nan icon. She appears to be posing for the camera, and \nher outfit includes a pair of jeans. The woman seems \nto be in a good mood, as she is smiling. The \nbackground of the image is blurry, focusing more on \nthe woman and her attire. \nArt collection style and fashion shoot, in the style of \nmade of glass, dark blue and light pink, paul rand, \nsolarpunk, camille vivier, beth didonato hair, \nbarbiecore, hyper-realistic.\nBeautiful scene \nA small cactus with a happy face in the Sahara desert\nA dog that has been \nmeditating all the time\nPirate ship trapped in a cosmic maelstrom nebula, \nrendered in cosmic beach whirlpool engine, \nvolumetric lighting, spectacular, ambient lights, light \npollution, cinematic atmosphere, art nouveau style, \nillustration art artwork by SenseiJaye, intricate detail.\nposter of a mechanical cat, technical Schematics \nviewed from front and side view on light white \nblueprint paper, illustration drafting style, illustration, \ntypography, conceptual art, dark fantasy steampunk, \ncinematic, dark fantasy.\nPixArt-\u03b1\nMJ\nPixArt-\u03b1\nMJ\nPixArt-\u03b1\nMJ\nPixArt-\u03b1\nMJ\nPixArt-\u03b1\nMJ\nMJ\nPixArt-\u03b1\nPixArt-\u03b1\nMJ\nFigure 21: This figure presents the answers to the image generation quality assessment as depicted\nin Appendix A.2. The method utilized for each pair of images is annotated at the top-left corner.\n27\nTechnical Report\nREFERENCES\nAnne-Laure Ligozat Alexandra Sasha Luccioni, Sylvain Viguier. Estimating the carbon footprint of\nbloom, a 176b parameter language model. In arXiv preprint arXiv:2211.02001, 2022.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. In arXiv, 2022.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth\nwords: A vit backbone for diffusion models. In CVPR, 2023.\nEyal Betzalel, Coby Penso, Aviv Navon, and Ethan Fetaya. A study on the evaluation of generative\nmodels. In arXiv, 2022.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nDeepFloyd. Deepfloyd, 2023. URL https://www.deepfloyd.ai/.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020a.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In arXiv, 2020b.\nZhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang\nLiu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model\nwith knowledge-enhanced mixture-of-denoising-experts. In CVPR, 2023.\nChongjian Ge, Junsong Chen, Enze Xie, Zhongdao Wang, Lanqing Hong, Huchuan Lu, Zhenguo\nLi, and Ping Luo. Metabev: Solving sensor failures for 3d detection and map segmentation.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8721\u20138731,\n2023.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180\u201315190, 2023.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. NeurIPS, 2021.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. In CVPR, 2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS,\n2017.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n28\nTechnical Report\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\n2020.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In ICLR, 2021.\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive\nbenchmark for open-world compositional text-to-image generation. In ICCV, 2023.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung\nPark. Scaling up gans for text-to-image synthesis. In CVPR, 2023.\nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models\nfor robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 2426\u20132435, June 2022.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In arXiv, 2013.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV,\n2023.\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-\na-pic: An open dataset of user preferences for text-to-image generation. In arXiv, 2023.\nZhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng\nDai. Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spa-\ntiotemporal transformers. In ECCV, 2022a.\nZhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, and\nTong Lu. Panoptic segformer: Delving deeper into panoptic segmentation with transformers. In\nCVPR, 2022b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In arXiv,\n2023.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\ntransformer. In CVPR, 2022.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In arXiv, 2017.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural\nInformation Processing Systems, 35:5775\u20135787, 2022.\nMicrosoft. Gpu selling, 2023. URL https://www.leadergpu.com/.\nMidjourney. Midjourney, 2023. URL https://www.midjourney.com.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. In arXiv, 2023.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nIn International Conference on Machine Learning, pp. 8162\u20138171. PMLR, 2021.\nNLTK. Nltk, 2023. URL https://www.nltk.org/.\n29\nTechnical Report\nNVIDIA. Getting immediate speedups with a100 and tf32, 2023. URL https://developer.\nnvidia.com/blog/getting-immediate-speedups-with-a100-tf32.\nOpenAI. Dalle-2, 2023. URL https://openai.com/dall-e-2.\nJunting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Journeydb: A benchmark\nfor generative image understanding. In arXiv, 2023.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32, 2018.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. In arXiv, 2023.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv, 2022.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\nstanding by generative pre-training. OpenAI blog, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 2019.\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In ICML,\n2015.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation. In MICCAI, 2015.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In arXiv,\n2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. In NeurIPS, 2022.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. In arXiv, 2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In ICML, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn NeurIPS, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021.\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for\nsemantic segmentation. In ICCV, 2021.\nPeize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping\nLuo. Transtrack: Multiple object tracking with transformer. In arXiv, 2020.\n30\nTechnical Report\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv\u00b4e J\u00b4egou.\nTraining data-efficient image transformers & distillation through attention.\nIn\nICML, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV, 2021.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational\nVisual Media, 2022.\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. arXiv preprint arXiv:2212.11565, 2022.\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-\nformer: Simple and efficient design for semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34:12077\u201312090, 2021.\nEnze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, and Zhenguo\nLi. Difffit: Unlocking transferability of large diffusion models via simple parameter-efficient\nfine-tuning. In ICCV, 2023.\nSaining Xie and Zhuowen Tu. Holistically-nested edge detection. In ICCV, 2015.\nShuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming\nMa. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. arXiv preprint\narXiv:2309.05019, 2023a.\nZeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo.\nRaphael: Text-to-image generation via large mixture of diffusion paths. In arXiv, 2023b.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image\ndiffusion models. In ICCV, 2023.\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In\nICCV, 2021.\nHongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models\nwith masked transformers. In arXiv, 2023.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers. In CVPR, 2021.\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and\nJiashi Feng. Deepvit: Towards deeper vision transformer. In arXiv, 2021.\nDaquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and\nJose M Alvarez. Understanding the robustness in vision transformers. In International Conference\non Machine Learning, pp. 27378\u201327394. PMLR, 2022.\n31\n"
  },
  {
    "title": "Enable Language Models to Implicitly Learn Self-Improvement From Data",
    "link": "https://arxiv.org/pdf/2310.00898.pdf",
    "upvote": "21",
    "text": "Published as a conference paper at ICLR 2024\nENABLING LANGUAGE MODELS TO IMPLICITLY LEARN\nSELF-IMPROVEMENT\nZiqi Wang1\u2217, Le Hou2\u2020, Tianjian Lu2, Yuexin Wu2, Yunxuan Li2, Hongkun Yu2, Heng Ji1\n1 University of Illinois Urbana-Champaign 2 Google\nziqiw9@illinois.edu lehou@google.com\nABSTRACT\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of these\ntasks implies that there is always room for improvement in the quality of model\nresponses. To address this challenge, various approaches have been proposed to\nenhance the performance of LLMs. There has been a growing focus on enabling\nLLMs to self-improve their response quality, thereby reducing the reliance on\nextensive human annotation efforts for collecting diverse and high-quality train-\ning data. Recently, prompting-based methods have been widely explored among\nself-improvement methods owing to their effectiveness, efficiency, and conve-\nnience. However, those methods usually require explicitly and thoroughly written\nrubrics as inputs to LLMs. It is expensive and challenging to manually derive\nand provide all necessary rubrics with a real-world complex goal for improvement\n(e.g., being more helpful and less harmful). To this end, we propose an ImPlicit\nSelf-ImprovemenT (PIT) framework that implicitly learns the improvement goal\nfrom human preference data. PIT only requires preference data that are used to\ntrain reward models without extra human efforts. Specifically, we reformulate\nthe training objective of reinforcement learning from human feedback (RLHF) \u2013\ninstead of maximizing response quality for a given input, we maximize the quality\ngap of the response conditioned on a reference response. In this way, PIT is implic-\nitly trained with the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.\n1\nINTRODUCTION\nLLMs (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Schulman\net al., 2022; OpenAI, 2023; Anil et al., 2023) have achieved state-of-the-art results on complex\ntasks such as math reasoning (Wei et al., 2022; Xue et al., 2023; Zhou et al., 2023), summarization\n(Stiennon et al., 2020b), conversations (Schulman et al., 2022; Bai et al., 2022), schema induction (Li\net al., 2023) and solving domain-specific problems (Singhal et al., 2022). The keys of LLMs success\nare their abilities of following instructions and aligning with human preferences (Ouyang et al.,\n2022; Peng et al., 2023a; Shen et al., 2023). A widely adopted approach toward them is instruction\nfine-tuning and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). However,\ninstruction fine-tuning and RLHF are imperfect, and there is always room for improvement. For\nexample, LLMs may hallucinate information (OpenAI, 2023), have reasoning errors (Bubeck et al.,\n2023), and generate unhelpful and harmful contents (Bai et al., 2022). A straightforward approach is\nto collect more diverse and high-quality data and improve the alignment with a human-in-the-loop\ntraining paradigm (Ouyang et al., 2022), which requires extensive amount of human effort, especially\nfor specific domains that require expert knowledge.\nTherefore, the community has explored to use LLMs to self-improve their own response quality\nwithout human intervention. With the advent of generative language models, prompting methods\nhave proved effective and efficient (no need to train models) with convenience (no need to serve\nmodels and can be used with black-box language models through APIs). Madaan et al. (2023) use\n\u2217Work done when interning at Google\n\u2020Correspondence Author\n1\narXiv:2310.00898v3  [cs.CL]  14 Mar 2024\nPublished as a conference paper at ICLR 2024\nIs [PIT-Improved Output] \nbetter than [LLM Output]?\nPIT\nPIT\n[Input]\n[LLM Output]\nLLMs\n[PIT-Improved Output]\nPIT Reward Model\nReinforcement\nLearning\nI need to improve [LLM Output]\nInference\nTraining\nPrompting (Self-Refine)\n[Input]\n[LLM Output]\nLLMs\nLLMs\n[Feedback]\nReflection\nLLMs\nRefine\n[LLM-Improved \nOutput]\nFigure 1: The pipeline of PIT and prompting methods (Self-Refine). Upper: PIT utilizes inputs and\nLLM outputs for further improvement during inference. During training, a reward model will assess\nthe gap of PIT-improved outputs and LLM outputs, and the reward is used for reinforcement learning\nto train PIT. Lower: Self-Refine uses LLMs to give self-feedback and then asks LLMs to improve\noutputs based on the self-feedback.\nLLMs to give self-feedback and iteratively improve LLMs response quality, Chen et al. (2023) enable\nLLMs to self-debug to enhance code generations. Nevertheless, self-improvement in language models\nthrough prompting can be less than ideal, as humans often find it challenging to define comprehensive\nimprovement goals and create detailed assessment rubrics. For example, improving helpfulness and\nharmlessness of LLMs requires careful definitions of these qualities to guide LLMs in improving\ntheir responses accordingly. As a result, prompting could make LLMs self-improve well only if the\nimprovement goal is clear, simple, and well-defined through prompting. LLMs perform poorly in our\nexperiments if we ask them to make responses more helpful. This is because the LLM tends\nto add more details to the response and thus makes the response longer, whereas more details are\nnot always preferred since it may lead to the response going off the topic. Instead, asking language\nmodels to be polite, offer necessary information, and avoid going off\ntopics will yield better improvements. Previous methods have also discussed similar observations\non the effect of detailed rubrics. Rafailov et al. (2023) showed that using a simple rubric of Which\nsummary is better? will yield much more disagreement with humans than a more compre-\nhensive rubric of Which summary summarizes most important points without\nincluding unimportant details?\nDespite the effectiveness, detailed rubrics are often hard to obtain. First, it is hard for humans to infer\nand write all possible rubrics. Second, when the task requires domain expertise (e.g., clinical domain\n(Singhal et al., 2022)), it is impractical to scale up. To this end, we switch our focus from explicitly\ndesigning rubrics to implicitly learning self-improvement from data. We notice that the preference\ndata used to train reward models implicitly tells how to improve the response quality. Thus, we utilize\nthis preference data to train a novel model that implicitly understands self-improvement goals from\ndata. Using this method, we eliminate the need for rubric design and avoid the need for additional\ndata since we only reuse the data used to train reward models.\nWe denote our approach ImPlicit Self-ImprovemenT (PIT), a novel approach that enables the model\nto learn self-improvement implicitly from data. Specifically, we reformulate the instruction fine-\ntuning and RLHF process and switch the training objective from maximizing response quality for\ngiven inputs to maximizing response quality gap conditioned on reference responses for given inputs.\nFigure 1 shows the working flows of PIT and prompting methods. PIT utlizes given inputs and\nreference responses and generates improved responses accordingly. Similar to prompting methods,\nPIT can repeat this process iteratively by replacing reference responses with improved responses.\nCompared with prompting methods, our method PIT does not require a manual rubric design.\nExtensive evaluations on two real-world datasets and one synthetic dataset show the effectiveness of\nPIT compared with prompting methods such as Self-Refine (Madaan et al., 2023).\n2\nRELATED WORK\nAlignment Alignment is critical for a helpful and harmless language model (Ouyang et al., 2022; Bai\net al., 2022). One common way for alignment is RLHF (Ouyang et al., 2022). However, RLHF is\n2\nPublished as a conference paper at ICLR 2024\nsensitive to the training details and is complicated to implement. To get rid of the RL, Lu et al. (2022)\nuse quantized rewards as control tokens to continually fine-tune policy models; Diao et al. (2023);\nDong et al. (2023); Gulcehre et al. (2023) use reward models to filter out high-reward generations for\nfurther fine-tuning . Some approaches are even simpler and do not require reward models. Liu et al.\n(2023) use human feedback as control tokens directly to fine-tune models on model generations. Sun\net al. (2023) use human-written principles to guide language models to generate helpful and harmless\nresponses and use generated responses to fine-tune models. Zhao et al. (2023) use a calibration loss\nto mimic RLHF. Rafailov et al. (2023) designs a loss function that can be theoretically proved to\nbe identical to RLHF. Xiong et al. (2023) shows that online alignment is much better than offline\nalignment. We used RLHF for alignment in our paper, as it is the most commonly used method.\nSelf-improvement Self-improvement enables language models to improve themselves without extra\nhuman effort. Moreover, the self-improved responses can then be utilized for context distillation\n(Askell et al., 2021) to update LLMs. Huang et al. (2022) use PaLM (Chowdhery et al., 2022) to label\nmore task-specific data and use the most confident labeled data to continue fine-tuning PaLM itself.\nHowever, fine-tuning language models on specific tasks may lower the overall generation performance\nof models (Zhai et al., 2023). Therefore, researchers also put much effort into self-improvement\nwithout modifying language models themselves. Shinn et al. (2023) put language models into an\nenvironment and let language models reflect their actions when they meet failures. Wang et al.\n(2023a) use the Python interpreter\u2019s error message to improve language models\u2019 code generation\nabilities. However, these methods require environments that can provide automatic feedback, which\nmay not be accessible in general. Self-Refine (Madaan et al., 2023) enables LLMs to reflect on their\nresponses and provide feedback, then ask LLMs to use feedback to improve responses. Zeng et al.\n(2023) apply Self-Refine to generate better meta-reviews for papers. Xue et al. (2023) guide LLMs to\nfine-grained feedback for math problems. The feedback could also benefit from tools. Zhou et al.\n(2023) use OpenAI\u2019s code interpreter to collect more accurate feedback. Wang et al. (2023b) propose\na benchmark to evaluate how models can get improved from human or AI feedback. Nevertheless,\nthese methods require explicit prompting to collect feedback and self-improve. Our method does not\nrequire modifying model weights, interactive environment, or explicitly prompting.\n3\nMETHOD\nPolicy models (i.e., \u201cLLMs\u201d in Figure 1) trained with RLHF generate reference responses for given\ninputs, whereas PIT takes the given inputs and reference responses as its inputs and generates\nimproved responses. The different input formats between policy models and PIT (See Appendix A\nfor details) requires reformulating RLHF training objectives to train PIT. We follow and reformulate\nRLHF steps proposed by Ouyang et al. (2022): supervised fine-tuning, reward model training, and\nreinforcement learning. Although there are many other alignment methods such as Direct Preference\nOptimization (Rafailov et al., 2023) or Preference Ranking Optimization (Song et al., 2023), we\nchoose RLHF to train policy models and PIT for two reasons: (1) RLHF is the most widely used\napproach for alignment (2) PIT is not aimed for alignment but for triggering self-improvement.\nTherefore, the alignment method is not the focus of this paper.\n3.1\nFORMULATION\nSuppose we have data D = {(x, yl, yw)}3n, where x is the input prompt, yl is the worse model\ngeneration, and yw is the better model generation, annotated by humans. We could equally divide\nthe data into three folds DSFT, DRM and DRL for the supervised fine-tuning, reward model training,\nand reinforcement learning, respectively. The policy model MP is trained to generate response yP for\nthe given input x: yP \u223c MP(\u00b7|x). PIT model MPIT is trained to generate improved response yPIT for\ngiven input x and reference response yref: yPIT \u223c MPIT(\u00b7|x, yref). In the following sections, we use\nsubscript \u00b7P to denote the policy model and \u00b7PIT to denote PIT, and use superscripts such as SFT or\nRL to denote different checkpoints of models.\n3.2\nSUPERVISED FINE-TUNING\nSupervised fine-tuning (SFT) is the pre-requisite step of RLHF (Ouyang et al., 2022). SFT only\nuses x and yw, and yw could also be human-written responses Ouyang et al. (2022). To train\npolicy models, we follow Ouyang et al. (2022) and simply maximize the likelihood of yw: LSFT\nP\n=\n\u2212 P\n(x,yl,yw)\u2208DSFT log MP(yw|x).\n3\nPublished as a conference paper at ICLR 2024\nSince PIT aims to improve reference responses, we need to include yl to train PIT: LSFT\nPIT\n=\n\u2212 P\n(x,yl,yw)\u2208DSFT log MPIT(yw|x, yl). A natural extension for this loss function is that we can\napply unlikelihood (Welleck et al., 2019) loss to MPIT(yl|x, yw), MPIT(yl|x, yl) and MPIT(yw|x, yw).\nHowever, we find the unlikelihood loss causes performance degradation in experiments, therefore we\nonly keep likelihood loss when training PIT with SFT.\n3.3\nREWARD MODEL TRAINING\nThe reward model is used to judge how good a response is and is the key component for reinforcement\nlearning. A reward model denoted as RP maps an input x and the corresponding response y to a scalar\nr: r(x, y) = RP(x, y). Since the data DRM only contains preference (yl and yw) and does not provide\nr directly, Ouyang et al. (2022) trains the reward model by maximizing the reward gap between yl\nand yw: LRM\nP\n= \u2212 P\nDRM log \u03c3(rw \u2212 rl), where rw and rl are rewards, i.e., RP(x, yw) and RP(x, yl),\nrespectively. Unlike RP, the reward model RPIT does not focus on rewards r of responses but cares\nabout the gap rgap between responses: rgap(x, y1, y2) = RPIT(x, y1, y2). Therefore, RPIT needs to\nlearn (rw \u2212 rl) from the training data DRM. To formulate the loss function, we can use the fact that\nrgap(x, yw, yl) \u2265 rgap(x, yw, yw) \u2248 rgap(x, yl, yl) \u2265 rgap(x, yl, yw).\n(1)\nTo ensure that the reward model performs as expected in Equation 1, after training, we consider all\npairwise relations mentioned above, and the loss function becomes:\nLRM\nPIT = \u2212\nX\nDRM\n[log \u03c3(rw,l\ngap \u2212 rw,w\ngap ) + log \u03c3(rw,l\ngap \u2212 rl,l\ngap) + log \u03c3(rw,l\ngap \u2212 rl,w\ngap )\n+ log \u03c3(rw,w\ngap \u2212 rl,w\ngap ) + log \u03c3(rl,l\ngap \u2212 rl,w\ngap )],\n(2)\nwhere rw,l\ngap is the shortcut of rgap(x, yw, yl), etc. Although there are other options to model the reward\ngap, such as computing the rewards subtraction through RP, we find Equation 2 is the best fit. More\ndiscussions could be found in Appendix C.\n3.4\nREINFORCEMENT LEARNING\nReinforcement learning (RL) finds a policy that maximizes expected rewards over time, aligning\nLLMs with human preferences. The optimization goal for RL is (Ouyang et al., 2022):\nOptimizationRL\nP\n=\nX\nDRL\n\u0002\nr(x, y) \u2212 \u03b2KL(MRL\nP (y|x) \u2212 MSFT\nP\n(y|x)\n\u0003\n.\n(3)\nwhere r(x, y) = RP(x, y) and y \u223c MRL\nP (\u00b7|x). MRL\nP\nis the policy model to be optimized, which is\ninitialized to MSFT\nP\n. MSFT\nP\nis MP trained in supervised fine-tuning, which is fixed in the RL. The KL\ndivergence is used to restrict dramatic weight changes and reward hacking in RL.\n3.4.1\nCURRICULUM REINFORCEMENT LEARNING\nDifferent from Equation 3, PIT aims to improve a reference response yref instead of generating a\nresponse from scratch. Therefore, the RL for PIT utilizes x and yref simultaneously. The difficulty is\nhow to choose yref. An intuitive way is to use yl and yw provided in the dataset, and the optimization\ngoal becomes:\nOptimizationRL, 0\nPIT\n=\nX\nDRL\nX\nyref\u2208{yl,yw}\n\u0002\nrgap(x, y, yref) \u2212 \u03b2KL(MRL\nPIT(y|x, yref) \u2212 MSFT\nPIT (y|x, yref)\n\u0003\n.\n(4)\nHowever, PIT aims to improve MRL\nP\nresponses, and yl and yw are chosen from the annotated data,\nnot sampled from the MRL\nP . Therefore, we need to do another round of reinforcement learning, where\nyref is sampled from the policy model:\nOptimizationRL, 1\nPIT\n=\nX\nDRL\nX\nyref\u223cMRL\nP (\u00b7|x)\n\u0002\nrgap(x, y, yref) \u2212 \u03b2KL(MRL\nPIT(y|x, yref) \u2212 MSFT\nPIT (y|x, yref)\n\u0003\n.\n(5)\nThe flexibility of yref in fact enables us to do multiple rounds of reinforcement learning to improve\nPIT further. For example, a third round OptimizationRL, 2\nPIT\ncan sample yref \u223c MRL\nPIT(\u00b7|x, y1), where\ny1 \u223c MRL\nP (\u00b7|x) with other terms unchanged. The third round enables PIT to improve the improved\nMRL\nP responses. In principle, this process could be extended to infinite rounds. We denote this process\nas curriculum reinforcement learning.\n4\nPublished as a conference paper at ICLR 2024\n3.4.2\nDISCUSSIONS ON CURRICULUM REINFORCEMENT LEARNING\nIt is worth noting that the first round OptimizationRL, 0\nPIT\nis necessary since the optimization\nOptimizationRL, 1\nPIT\nis too hard to be optimized directly. This is because responses sampled from\nMRL\nP\nare already of high quality (higher than yl and yw in the data, as shown in Section 4.6), and are\nharder to be improved compared to yl and yw. Therefore, curriculum reinforcement learning is needed\nto ensure the success of optimizing OptimizationRL, 1\nPIT . We find that removing OptimizationRL, 0\nPIT\nand\ndirectly optimize OptimizationRL, 1\nPIT\nwill make PIT fail to improve MRL\nP\nresponses (Section 4.6). Be-\nsides, purely training PIT on OptimizationRL, 0\nPIT\nwill also lead to a disaster since PIT never saw data\nsampled from MRL\nP\n(Section 4.6). In fact, we could even insert several intermediate rounds between\nthe first and second rounds to build a smoother curriculum. Concretely, we could choose yref sampled\nfrom intermediate checkpoints of MRL\nP\n(e.g., checkpoints dumped during training and have not been\noptimized for high rewards yet) since their responses will be less perfect and easier to be improved.\nIn our experiments, we use OptimizationRL, 0\nPIT\nand OptimizationRL, 1\nPIT\nto optimize PIT. This is because\nwe find the dataset is not hard enough to require more rounds and intermediate rounds. The algorithm\nblock of PIT can be found in the Algorithm 1.\n3.5\nSELF-IMPROVEMENT INFERENCE\nGiven the input x, we first sample yref \u223c MRL\nP (\u00b7|x). Then we can get improved response y1 \u223c\nMRL\nPIT(\u00b7|x, yref). The improvement process could be repeated infinite times. For examples, we could\nget y2 \u223c MRL\nPIT(\u00b7|x, y1) and y3 \u223c MRL\nPIT(\u00b7|x, y2), etc. The self-improvement inference process\nis summarized in the Algorithm 2. Moreover, PIT does not bring extra computational overhead\ncompared with prompting methods such as Self-Refine (Madaan et al., 2023). Detailed discussions\ncan be found in Appendix B.\n4\nEXPERIMENTS\nOur experiments are designed to answer two questions: (1) Can our method improve the quality of\nthe original response? The term \u2018quality\u2019 here is defined by the preference data. For example, if the\npreference data is to get a more helpful and less harmless response, then \u2018higher quality\u2019 denotes\nmore helpful and less harmless. (2) Can our method better improve the response than prompting\nmethods such as Self-Refine?\n4.1\nDATASETS\nWe use three diverse datasets for our experiments:\nAnthropic/HH-RLHF. The HH-RLHF dataset (Bai et al., 2022; Ganguli et al., 2022) is released by\nAnthropic and is allowed for research purposes, which contains 161K conversation pairs between\nhumans and AI assistants for training and 8.55K for testing. The dataset aims to train a helpful\nand harmless AI assistant and has two subsets: the helpful subset and the harmless subset. In our\nexperiment, we only use the helpful subset and divide the subset equally into three folds for supervised\nfine-tuning, reward model training, and reinforcement learning.\nOpenAI/Summary The OpenAI/Summary (Stiennon et al., 2020a), which is also allowed for research\npurposes, contains Reddit posts and two summaries for each post with human preferences. It contains\n92.9K training data and 86.1K validation data. Similarly to the Anthropic/HH-RLHF dataset, we\nequally divide the training dataset into three folds for supervised fine-tuning, reward model training,\nand reinforcement learning, respectively.\nSynthetic Data To test the instruction following abilities of language models, we use PaLM 2\n(Unicorn) (Anil et al., 2023) to generate 13K synthetic data. Every synthetic data includes a question\nwith multiple requirements, such as using a specific format for writing responses. It also includes a\nsatisfactory response that meets all of the requirements, as well as an unsatisfactory response that only\nmeets some of the requirements. Lastly, there is a reflection on how to improve the unsatisfactory\nresponse to make it satisfactory (not used in our experiments). Similarly, we divide synthetic data\ninto three folds equally and use 128 examples for evaluation. The synthetic data generation process\nand examples can be found in Appendix D.\n5\nPublished as a conference paper at ICLR 2024\n4.2\nSETTINGS\nWe use pre-trained PaLM 2 (Bison) as our backbone language model and reward model since we find\nsmaller models\u2019 (Gecko and Otter) generation abilities are poorer, and larger models (Unicorn) are\ntoo expensive to train. We train our models on TPU v4 (Jouppi et al., 2023). In our experiments, we\ncompare PIT with other self-improvement methods using prompts, specifically Self-Refine (Madaan\net al., 2023). Self-Refine uses prompts (Appendix E.1) to instruct the model MRL\nP to reflect the current\nresponse and give feedback to improve the response, then asks MRL\nP\nto give an improved response\nbased on the feedback. We set the sampling temperature to be 1 for all models during inference\nunless stated otherwise. More experiment settings can be found in Appendix F.\n4.3\nEVALUATION MODELS\n30\n20\n10\n0\n10\nReward\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nDensity\nr(yl, yl)\nr(yw, yl)\nr(yw, yw)\nr(yl, yw)\nFigure 2: Reward distribution of\nRPIT on the synthetic data, which fol-\nlows Equation 1.\nWe use third-party LLMs and reward models that are allowed\nto be used for research as evaluators, similar to previous works\n(Rafailov et al., 2023; Gulcehre et al., 2023; Dong et al., 2023).\nLLMs are sensitive to the rubric and need carefully designed\nprompts to achieve good performance for evaluation (Rafailov\net al., 2023), which is also aligned with our motivation. Re-\nward models may have reward hacking phenomenon (Skalse\net al., 2022). To get a reliable evaluation, we use both third-\nparty LLMs and reward models to do evaluations and use\nhuman evaluations when the two evaluations disagree. Evalu-\nation prompts for LLMs are adopted from Zheng et al. (2023);\nRafailov et al. (2023) and are shown in Appendix E.2. We com-\npute the agreement of LLMs with ground-truth validation data\n(Appendix G), and GPT-4 has the highest agreement compared\nto ChatGPT and PaLM 2 (Unicorn) and will be used as the\nrepresentative of third-party language models to do evaluations\nafterward. We use DeBERTa-Large (304M) (He et al., 2020)\ntrained by Open Assistant 1 as the representative of reward models because it uses different model\narchitectures (compared with RP and RPIT) and full training data (RP and RPIT only use 1/3 data) and\nis not used to optimize models (RPIT and RP are used for optimization), reducing the risk of reward\nhacking during evaluations. Moreover, it performs similarly to GPT-4 on the ground-truth validation\ndata (Appendix G).\nWe also compare the agreement of RPIT and RP with the ground-truth labels on validation sets\n(Appendix G), and conclude RPIT generally outperforms RP, as expected. We then draw RPIT reward\ndistribution on the synthetic dataset (since reward models perform best on this dataset among others\nand the distribution pattern is more noticeable) in Figure 2. The distribution exactly follows our loss\ndesign in Equation 1. Moreover, we observe that the reward is negative when two responses are\nidentical, indicating that PIT penalties repeating responses (or generating responses that have the\nsame qualities as reference responses).\n4.4\nRESULTS\nSince GPT-4 API is expensive and slow, we evaluate 128 examples when using GPT-4 and 1,000\nexamples when using the DeBERTa reward model. To reduce the noise, the reward model will\ngive a tie if the rewards for two responses are similar. In practice, the reward model gives a tie if\n\u03c3(r1 \u2212 r2) \u2208 [0.45, 0.55], where r1 and r2 denote the reward of two responses. We first sample\nresponses yref \u223c MRL\nP (x) (denoted as \u2018Original Responses\u2019) and then use self-improvement methods\nto improve yref. Appendix H.1 shows several qualitative examples.\nPIT improves responses\u2019 qualities. We apply PIT to original responses and get improved responses.\nThen, we report the win rate of PIT against original responses. We use the difference between the\nwin rate and the lose rate (\u2206 for short) to denote the performance of PIT. Table 1 shows results.\nFirst, we find that the original responses are much better than yw in data, showing that MRL\nP\nis well\noptimized (this conclusion does not hold on synthetic data, which is expected since a much larger\nLLM produces yw). We can see PIT consistently has better qualities (ranging from 7.2% to 33.59%)\n1https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Comparisons among yw in data, original responses, improved responses by Self-Refine, and\nimproved responses by PIT on three datasets. Win rate / Lose rate denotes the percentage of the\nformer model\u2019s responses that is better / worse than the latter\u2019s. Their difference \u2206 > 0 denotes that\nthe former response is better and vice versa. Higher |\u2206| denotes a higher performance gap.\nDataset\nComparison\nWin rate / Lose rate / \u2206 (%)\nGPT-4\nDeBERTa\nHuman Evaluation\nAnthropic/HH-RLHF\nOriginal vs. yw\n71.85/17.19/54.69\n68.20/18.00/50.20\n-\nPIT vs. Original\n55.47/27.34/28.13\n46.30/32.30/14.00\n-\nSelf-Refine vs. Original\n60.94/17.19/43.75\n40.30/31.40/8.90\n-\nPIT vs. Self-Refine\n38.28/42.19/\u22123.91\n41.3/37.60/3.70\n47.06/23.53/23.53\nOpenAI/Summary\nOriginal vs. yw\n74.22/8.59/65.63\n84.90/10.70/74.20\n-\nPIT vs. Original\n44.53/24.22/20.31\n41.9/34.7/7.2\n-\nSynthetic Data\nOriginal vs. yw\n28.91/51.56/\u221222.66\n-\n-\nPIT vs. Original\n48.44/14.84/33.59\n-\n-\nSelf-Refine vs. Original\n34.38/17.97/16.41\n-\n-\nPIT vs. Self-Refine\n45.31/35.16/10.16\n-\n-\nthan original responses across three datasets with both GPT-4 and DeBERTa evaluators, showing the\neffectiveness of our method. Since DeBERTa performs poorly and GPT-4 has a high agreement with\nthe ground-truth labels on the synthetic data (Appendix G), we only use GPT-4 as the evaluator for\nthe synthetic data. It is worth noting that the summation of the win rate and the lose rate is less than 1\nbecause of the existence of the tie rate.\nPIT improves response quality better than Self-Refine. We then compare Self-Refine with PIT and\noriginal responses. Table 1 shows the comparison results on the Anthropic/HH-RLHF dataset. GPT-4\nand DeBERTa both agree that Self-Refine indeed improves response qualities. However, GPT-4\nprefers Self-Refine more (3.91% better than PIT), whereas DeBERTa prefers PIT more (3.70% better\nthan Self-Refine). This is understandable since GPT-4 and Self-Refine use manual prompts to evaluate\nor improve responses, making them prefer each other. On the contrary, DeBERTa and PIT are trained\non the same data. Therefore, we conduct human evaluations to determine which is better and find\nthat human prefers PIT more (23.53% better than Self-Refine). Appendix I shows details of human\nevaluations. The disagreement between GPT-4 and humans arises from GPT-4\u2019s inclination to long\nand detailed responses and Self-Refine\u2019s preference to generate such responses. Nonetheless, these\nresponses can include irrelevant information not related to the questions humans raised, which is\nnot desirable to humans. Appendix H.2 contains one concrete example of this phenomenon. Table\n1 also shows results on the synthetic dataset, where we can easily conclude that PIT outperforms\nSelf-Refine, though they all improve original responses.\n4.5\nTHE INFLUENCE OF IMPROVEMENT TEMPERATURES\nIn the above experiments, we set the temperature to be 1 for MRL\nP , MRL\nPIT and Self-Refine. However,\nhigher temperatures often represent diversities and randomness, while lower temperatures usually\nrepresent coherence and qualities. Intuitively, self-improvement is expected to improve the original\nresponse along one most reliable improvement direction rather than randomly explore directions\n(which many harm the improvement performance). Therefore, a low temperature may be preferable\nto restrict the diversity of the improvement. Fig 3 shows the difference between win rates and lose\nrates among the original responses, improved responses by Self-Refine, and improved responses\nby PIT under different temperatures. The original responses are generated with the temperature\n1, and improved responses are generated with different temperatures, as the figure shows. To\nachieve cost-effectiveness, we have opted for DeBERTa as the evaluator to assess 1,000 instances\nof Anthropic/HH-RLHF and OpenAI/Summary. We made this choice because, as indicated in\nTable 1 and Section 4.4, DeBERTa correlates more closely with human preferences than GPT-4\ndoes. Nevertheless, for the evaluation of 128 examples of synthetic data, we will rely on GPT-4 as\nDeBERTa is not ideal for this purpose, as shown in Appendix G. We do not report the results of\nSelf-Refine for OpenAI/Summary since the dataset only contains summarization instructions, making\nSelf-Refine not applicable.\nOn Anthropic/HH-RLHF dataset, we find our method improves the original responses most with low\ntemperatures (0.4 \u223c 0.6), which fits our assumptions proposed at the beginning of this section.\n7\nPublished as a conference paper at ICLR 2024\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n5\n0\n5\n10\n15\n20\n25\n30\nWin rate - Lose rate (%)\nPIT vs. Original\nSelf-Refine vs. Original\nPIT vs. Self-Refine\n(a) Anthropic/HH-RLHF\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n5\n0\n5\n10\n15\n20\n25\n30\n35\nWin rate - Lose rate (%)\nPIT vs. Original\n(b) OpenAI/Summary\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n10\n0\n10\n20\n30\n40\nWin rate - Lose rate (%)\nPIT vs. Original\nSelf-Refine vs. Original\nPIT vs. Self-Refine\n(c) Synthetic Data\nFigure 3: The difference between win rate and lose rate (\u2206) among original responses, improved\nresponses by Self-Refine, and improved responses by PIT under different temperatures. \u2206 > 0\ndenotes the former response is better, and higher |\u2206| denotes larger performance gap.\nTable 2: The effect of curriculum reinforcement learn-\ning. A great performance drop is observed when we\nonly use RL once. \u2206 < 0 represents \u2018XX RL Only\u2019\nresponse quality is worse than the compared method.\nEvaluator: DeBERTa; Dataset: Anthropic/HH-RLHF\nWin rate / Lose rate / \u2206 (%)\nFirst RL Only\nSecond RL Only\nvs. Original\n40.50/36.10/4.40\n28.90/28.70/0.20\nvs. Self-Refine\n32.0/47.7/\u221215.70\n28.50/47.40/\u221218.90\nvs. PIT\n20.70/50.80/\u221230.10\n19.30/56.00/\u221236.70\nTable 3: ELO scores (higher is better) of different\nimprovement iterations on 1, 000 conversations in\nAnthropic/HH-RLHF and 1, 000 posts in OpenAI/-\nSummary. Evaluator: DeBERTa.\nAnthropic/HH-RLHF\nResponses\nELO\nRank\nPIT (Iter 4)\n1036\n1\nPIT (Iter 1)\n1033\n2\nPIT (Iter 5)\n1025\n3\nPIT (Iter 3)\n1018\n4\nPIT (Iter 2)\n1016\n5\nSelf-Refine (Iter 1)\n1001\n6\nSelf-Refine (Iter 3)\n995\n7\nSelf-Refine (Iter 4)\n990\n8\nSelf-Refine (Iter 2)\n982\n9\nSelf-Refine (Iter 5)\n982\n10\nOriginal\n921\n11\nOpenAI/Summary\nResponses\nELO\nRank\nPIT (Iter 2)\n1049\n1\nPIT (Iter 3)\n1031\n2\nPIT (Iter 1)\n1022\n3\nPIT (Iter 5)\n1013\n4\nPIT (Iter 4)\n985\n5\nOriginal\n900\n6\nSimilar\nWorse\nBetter\nFirst RL Only\nFigure 4: Rewards w.r.t. training steps. Re-\nwards are divided into three regions. The\nBetter region denotes the model can im-\nprove original responses; The Similar re-\ngion suggests the model can only generate\nresponses that are on par with original re-\nsponses; The Worse denotes the model can-\nnot improve original responses and make\noriginal responses worse.\nHowever, Self-Refine improves the original responses most with high temperatures (0.6 \u223c 0.8). We\nfind that this is because Self-Refine tends to keep all the original response and only append extra details\nto the original response with low temperatures, leading to sub-optimal improvement. Corresponding\nexamples can be found in Appendix H.3. OpenAI/Summary shows similar observations, whereas\nPIT is not heavily affected by temperatures on the synthetic data. This is probably because we only\nevaluate the instruction following ability and do not evaluate the concrete content quality on the\nsynthetic data. Nevertheless, PIT outperforms Self-Refine under all temperatures (except a tie under\nthe temperature 0.8 on the Anthropic/HH-RLHF dataset). Moreover, we compare PIT and Self-\nRefine under the best temperatures on Anthropic/HH-RLHF(i.e., 0.4 for PIT and 0.8 for Self-Refine),\nand find PIT still outperforms Self-Refine by 9.2%.\n4.6\nTHE EFFECT OF CURRICULUM REINFORCEMENT LEARNING\nWe conduct reinforcement learning twice in PIT , with the difficulty increases, as illustrated in Section\n3.4.1. The first RL (Equation 4) aims to teach models to improve ground-truth responses, whereas\nthe second RL (Equation 5) aims to teach models to improve MRL\nP\nresponses. To demonstrate the\nimportance of curriculum reinforcement learning, we trained two additional PIT variants. One model\nwas optimized using the first RL, while the other model skipped the first RL and directly used the\n8\nPublished as a conference paper at ICLR 2024\nsecond RL. We compare the two extra models with PIT , Self-Refine conducted by MRL\nP , and original\nresponses sampled from MRL\nP\non the Anthropic/HH-RLHF datasets. Table 2 shows that both models\ncan only improve original responses marginally and have a large performance gap when compared to\nSelf-Refine and PIT. The results are expected since the first model does not learn how to improve the\noriginal responses but is only trained on easier ground-truth data. On the contrary, the second model\nis directly facing a hard optimization problem (Table 1 shows that original responses are much better\nthan yw). Examples can be found in Appendix H.4.\nWe also show rewards w.r.t. training steps when training MRL\nPIT with Equation 5 and training a\nPIT variant with second RL only in Figure 4. Rewards are given by RPIT(x, \u00b7, yref), where yref \u223c\nMRL\nP (x) is the original response. It is clear that the model with second RL only struggles to improve\nyref due to the hard optimization, whereas PIT can break through the Similar area thanks to the\nfirst RL. Besides, the starting point of PIT in the Figure 4, i.e., the PIT variant with the first RL only,\nfails to improve yref, as expected.\n4.7\nTHE EFFECT OF IMPROVEMENT ITERATIONS\nKnowing when to stop self-improvement is crucial since the improvement process could be iterated\ninfinitely in principle. The stop condition depends on the actual needs. For example, the stop\ncondition could be a fixed wall time or the significance of each improvement (i.e., stop further\nimprovement if the current improvement is marginal). In this section, we only focus on the response\nqualities and investigate if the response qualities become better with more improvement iterations. If\nthe answer is yes, then more iterations are always welcomed. Otherwise, stop conditions need to be\ncarefully designed in real applications. We use ELO scores (Zheng et al., 2023) to show quantitative\nresults among models.\nWe use PIT and Self-Refine (with their best temperatures respectively) to improve 1, 000 original\nresponses for 5 iterations on Anthropic/HH-RLHF, meaning each conversation now has 11 responses.\nWe then use the DeBERTa reward model to compare 11 responses (Appendix H.5 shows an example),\nresulting C2\n11 = 55 comparisons for each conversation and 55, 000 comparisons in total. We then\nuse 55, 000 comparisons to compute ELO scores2 and report results in Table 3. We get ELO scores\non OpenAI/Summary similarly, except for the difference that Self-Refine is not applicable to this\ndataset. Table 3 shows consistent results with previous experiments that PIT (Iter 1) is better than\nSelf-Refine (Iter 1), and the original responses have the lowest qualities. Intuitively, the best iteration\nfor PIT should be consistent with the number of RL in curriculum reinforcement learning. However,\nTable 3 shows that the response qualities and the number of iterations do not follow this intuition as\nPIT (Iter 1) is not best. Moreover, there is not a simple positive correlation between response qualities\nand improvement iterations. One possible explanation may be that the datasets are too easy and do\nnot need more iterations. This phenomenon suggests we develop stop conditions in real applications\ncarefully. We observe that sometimes the self-improvement yields the same responses as its previous\niteration with low temperatures, which could potentially serve as a stop condition. An example\nis shown in Appendix H.5. We can also observe that PIT is consistently better than Self-Refine\nregardless of the number of iterations. Therefore, PIT is a better self-improvement method than\nSelf-Refine regardless of the stop condition.\nSince ELO scores are affected by the order of comparisons, we randomly shuffle comparisons several\ntimes and observe the change of ranks to ensure our conclusion is not caused by randomness. We\nfind that PIT is always better than Self-Refine, and the original responses are the worst among 11\nresponses, but the internal rank of PIT and Self-Refine may be changed with different shuffles, which\ndoes not affect our conclusions above.\n5\nCONCLUSION\nWe propose PIT, a novel approach that learns self-improvement implicitly from data.PIT does not\nrequire explicit prompting and extra data. Extensive experiments show the effectiveness of PIT on self-\nimprovement compared with prompting methods such as Self-Refine. We highlight our limitations\nand future work in Appendix J.\n2We use scripts released by Chatbot Arena (Zheng et al., 2023): https://colab.research.google.\ncom/drive/1J2Wf7sxc9SVmGnSX_lImhT246pxNVZip?usp=sharing\n9\nPublished as a conference paper at ICLR 2024\nREPRODUCIBILITY\nDatasets has been discussed in 4.1. Two of three datasets (Anthropic/HH-RLHF and OpenAI/Sum-\nmary) are publicly available and can be easily found on the HuggingFace website. Appendix D\ndescribes the generation process of the remaining synthetic dataset. Evaluators are discussed in\nSection 4.3 and Appendix G. Moreover, Appendix E.2 offers the evaluation prompts. Experiment\nsettings, baselines, and details are discussed in Section 4.2 and Appendix F. Appendix E.1 also offers\nprompts used for the baseline method.\nACKNOWLEDGEMENT\nWe would like to acknowledge our Google colleagues for their invaluable advice and support. In\nparticular, we thank Music Li (Yuezhang Li) for insightful discussions and manual evaluation. We\nthank Tianqi Liu, Honglong Cai, and Albert Webson for their constructive advice, and L\u00b4eonard\nHussenot and Robert Dadashi for building RLHF infra. Finally, we would like to acknowledge\nMelvin Johnson, Hongkun Yu, and Denny Zhou for their support throughout the project. We also\nthank the anonymous reviewers for their suggestions and comments.\nThis research is also based upon work supported by U.S. DARPA ECOLE Program No.\nHR00112390060 and U.S. DARPA ITM Program No. FA8650-23-C-7316 and KAIROS Program\nNo. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and\nshould not be interpreted as necessarily representing the official policies, either expressed or implied,\nof DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute\nreprints for governmental purposes notwithstanding any copyright annotation therein.\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nShizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.\nLmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv\npreprint arXiv:2306.12420, 2023.\n10\nPublished as a conference paper at ICLR 2024\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and\nTong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv\npreprint arXiv:2304.06767, 2023.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben\nMann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to\nreduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858,\n2022.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. arXiv preprint arXiv:2006.03654, 2020.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.\nLarge language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.\nNorm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,\nSuvinay Subramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable\nsupercomputer for machine learning with hardware support for embeddings. In Proceedings of the\n50th Annual International Symposium on Computer Architecture, pp. 1\u201314, 2023.\nSha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch, and Jiawei Han. Open-domain\nhierarchical event schema induction by incremental prompting and verification. In Proc. The 61st\nAnnual Meeting of the Association for Computational Linguistics (ACL2023), 2023.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages are rewards: Hindsight finetuning using\nhuman feedback. arXiv preprint arXiv:2302.02676, 2023.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.\nAdvances in neural information processing systems, 35:27591\u201327609, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nOpenAI. Gpt-4 technical report. arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277, 2023a.\nBaolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, and Dong Yu. Stabilizing rlhf through\nadvantage model and selective rehearsal. arXiv preprint arXiv:2309.10202, 2023b.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Fe-\nlipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language\nmodels for dialogue. OpenAI blog, 2022.\n11\nPublished as a conference paper at ICLR 2024\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, et al. Mixture-of-experts meets instruction tuning: A winning\ncombination for large language models. arXiv preprint arXiv:2305.14705, 2023.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint\narXiv:2303.11366, 2023.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode\nclinical knowledge. arXiv preprint arXiv:2212.13138, 2022.\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing\nreward gaming. Advances in Neural Information Processing Systems, 35:9460\u20139471, 2022.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS,\n2020a.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020b.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming\nYang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with\nminimal human supervision. arXiv preprint arXiv:2305.03047, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nXingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. Leti: Learning to generate from\ntextual interactions. arXiv preprint arXiv:2305.10314, 2023a.\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint:\nEvaluating llms in multi-turn interaction with tools and language feedback, 2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural\ntext generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\nWei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from\nhuman feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456,\n2023.\nTianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. Rcot: Detecting\nand rectifying factual inconsistency in reasoning by reversing chain-of-thought. arXiv preprint\narXiv:2305.11499, 2023.\nQi Zeng, Mankeerat Sidhu, Hou Pong Chan, Lu Wang, and Heng Ji. Meta-review generation with\nchecklist-guided iterative introspection. arXiv preprint arXiv:2305.14647, 2023.\nYuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating\nthe catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313,\n2023.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf:\nSequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\n12\nPublished as a conference paper at ICLR 2024\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,\nLinqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code\ninterpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023.\n13\nPublished as a conference paper at ICLR 2024\nA\nTHE WORKING FLOW\nTo better demonstrate our approach, we Given a dataset D = {(x, yl, yw)}, we equally divide it\ninto three folds DSFT, DRM and DRL. Starting from a pre-trained language model Mpre(\u00b7|\u00b7), we first\nconduct normal SFT and RLHF on D to obtain RLHF-finetuned model MRL\nP (\u00b7|\u00b7). MRL\nP (\u00b7|\u00b7) takes a\nprompt x as the input (e.g., Please give me a three-day travel plan about Los\nAngeles.) and return a response y. The input format of MRL\nP (\u00b7|x) and Mpre(\u00b7|x) during training\nand inference is:\nHuman:{x} Assistant:\n, where {x} denotes the concrete input x.\nThen we train PIT MRL\nPIT(\u00b7|\u00b7, \u00b7) that takes a prompt and a candidate response as inputs and returns an\nimproved response. The input format of MRL\nPIT(\u00b7|x, yref) is\nHuman:{x} Assistant:<<Candidate>>{yref}<<Improved>>\n, where {} denotes concrete contents and <<>> denotes a special token.\nSimilarly, the input format of RP(x, y) is\nHuman:{x} Assistant:{y}\n, and the input format of RPIT(x, y1, y2) is\nHuman:{x} Assistant:<<Candidate>>{y2}<<Improved>>{y1}\nAlgorithm 1: PIT Training\nInput: Dataset DSFT, DRM and DRL, a pre-trained model Mpre(\u00b7|\u00b7), and an RLHF-finetuned model MRL\nP (\u00b7|\u00b7)\ntrained on DSFT, DRM, DRL and Mpre(\u00b7|\u00b7).\nOutput: PIT Model MRL\nPIT(\u00b7|\u00b7, \u00b7)\n\u2022 MSFT\nPIT (\u00b7|\u00b7, \u00b7) \u2190 Mpre(\u00b7|\u00b7)\n\u2022 for x, yl, yw in DSFT\n\u25b7 Supervised fine-tuning\n\u2013 Supervised fine-tuning Mpre(\u00b7|\u00b7) using loss LSFT\nPIT = \u2212 log MSFT\nPIT (yw|x, yl)\n\u2022 RPIT(\u00b7, \u00b7, \u00b7) \u2190 Mpre(\u00b7|\u00b7)\n\u2022 for x, yl, yw in DRM\n\u25b7 Reward model training\n\u2013 Compute reward gap RPIT(x, y1, y2), y1 \u2208 {yl, yw}, y2 \u2208 {yl, yw}\n\u2013 Optimize RPIT by Equation 2\n\u2022 MRL\nPIT(\u00b7|\u00b7, \u00b7) \u2190 MSFT\nPIT (\u00b7|\u00b7, \u00b7)\n\u2022 while not converge\n\u25b7 First RL\n\u2013 Sample x, yref from DRL, where y \u2208 {yl, yw}\n\u2013 Generate y \u223c MRL\nPIT(\u00b7|x, yref)\n\u2013 Use reinforcement learning algorithm such as PPO to optimize Equation 4\n\u2022 done\n\u2022 while not converge\n\u25b7 Second RL\n\u2013 Sample x from DRL, yref \u223c MRL\nP (\u00b7|x)\n\u2013 Generate y \u223c MRL\nPIT(\u00b7|x, yref)\n\u2013 Use reinforcement learning algorithm such as PPO to optimize Equation 5\n\u2022 done\n\u2022 return MRL\nPIT\nWhen conducting self-improvement during inference, we need first to get yref \u223c MRL\nP (\u00b7|x), and then\napply MRL\nPIT or Self-Refine to get improved responses. The self-improvement process could be done\niteratively during inference. The concrete inference procedure is shown in Algorithm 2.\n14\nPublished as a conference paper at ICLR 2024\nAlgorithm 2: Self-Improvement During Inference\nInput: Input x, an response yref sampled from RLHF-finetuned model MRL\nP (\u00b7|x), PIT model MRL\nPIT(\u00b7|\u00b7, \u00b7),\nself-improvement iteration K\nOutput: Improved responded y\n\u2022 while k < K\n\u25b7 Self-Improvement\n\u2013 y \u2190 MRL\nPIT(\u00b7|x, y) [PIT] or y \u2190 Self-Refine(x, y, MRL\nP ) [Self-Refine]\n\u2013 k \u2190 k + 1\n\u2022 done\n\u2022 return y\nB\nCOMPUTATIONAL COST DURING INFERENCE\nOur approach has the same (or a bit lower) computational overhead compared with Self-Refine. The\ninference process is shown in Algorithm 2. Here is a detailed explanation:\nThe inference prompt of Self-Refine looks like this:\nHuman:\nGiven a prompt {x} and a candidate response {yref} provided\nby one assistant, you need to improve the response to be more\nhelpful.\nHelpful means {rubric}.\nAssistant:\nPIT uses a much simpler prompt with only special tokens as splitters:\nHuman:\n{x} Assistant:\n<< Candidate >> {yref} << Improved >>\nThe {} denotes the concrete content, and <<>> denotes the special token. Both methods will return\nan improved response. The difference is that PIT does not need to explicitly write prompts since\nself-improvement is learned during our proposed training framework. Both methods use a one-time\ninference to get an improved response, and PIT in principle has the same computational cost as\nSelf-Refine during inference, or strictly speaking even fewer computational costs due to the fewer\ninput tokens (without complex rubrics).\nC\nWHY EQUATION 2?\nWe made many efforts to filter out other options and pin down our final choice to Equation 2. Since it\nis not applicable to try all possible solutions, we filter out other methods based on prior work findings\nand our own pilot experiments. Here is our detailed selection process:\nThe first question in developing the reward model is if we can directly use Rp (i.e., the vanilla reward\nmodel that takes x and y as inputs and returns a scalar reward r) to compute the reward gap between\ntwo responses y1 and y2, or we need to develop another reward model that directly calculates the\nreward gap. The most straightforward implementation using Rp is to compute the subtraction of\nindividual rewards of y1 and y2 obtained by Rp. However, previous works show that Rp fails to\nfaithfully reflect the response quality when r is high (Bai et al., 2022). That is to say, if r1 and r2 are\nhigher than a threshold, then r1 < r2 does not necessarily denote y1 is worse. This phenomenon is\npossible because of the poor calibration of the reward model (Peng et al., 2023b). Moreover, other\nwork (Zhao et al., 2023) shows that directly modeling the reward gap brings less noise and performs\nbetter than computing the subtraction of individual rewards. Therefore, we decided to train a reward\nmodel that directly models reward gaps.\nNext, we need to decide the training objective. In the beginning, the training objective is simply max-\nimizing the reward gap rgap(x, yw, yl) and minimizing rgap(x, yl, yw), which is similar to Equation 2\nbut without rgap(x, yw, yw) and rgap(x, yl, yl). However, we observe that LLM finds a shortcut that\nignores the third argument, thus degenerating to Rp and failing to grasp the reward gap. Therefore,\nwe add rgap(x, yw, yw) and rgap(x, yl, yl) to prevent the degeneration.\n15\nPublished as a conference paper at ICLR 2024\nWe also explore the possibility of using pointwise rather than pairwise training signals used above. For\nexample, we tried to explicitly assign rgap(x, yw, yl) = 1, rgap(x, yl, yw) = 0 and rgap(x, yl, yl) =\nrgap(x, yw, yw) = 0.5 and use MSE to train the reward model. We find this implementation is less\neffective than our proposed approach (e.g., Equation 2) on the held-out reward model test set of\n500 examples (\u223c 5% accuracy drop). We think this phenomenon is because the inductive bias of\nassigning rewards to 1, 0.5, and 0 is not preferred in the data distribution.\nBased on the above analysis, Equation 2 is chosen as our final choice.\nD\nSYNTHETIC DATA\nThe synthetic data aims to test the instruction following abilities. Therefore, each question contains\nmultiple requirements. We first manually design a list of tasks (e.g., email writing and advertisement\nwriting) and requirements (e.g., using specific formats such as HTML, Markdown, or upper case.).\nThen, we randomly choose a task and several requirements from the list. We instruct PaLM 2\n(Unicorn) to generate a question x0 based on the chosen task and requirements and give a response yl\naccordingly. Next, we randomly select a few more requirements and instruct PaLM 2 (Unicorn) to\nadd extra requirements to x0 and get a new question x together with its response yw. The difference\nbetween yw and yl is that yw follows all instructions whereas yl only follows part of instructions in x.\nIt is worth noting that yw does not necessarily have better content than yl. In the synthetic dataset,\nwe only care about the instruction following ability. At last, we instruct PaLM 2 (Unicorn) to give a\nchain-of-thoughts reflection c on improving yl to yw. {(x, yl, yw, c)} forms the synthetic dataset.\nWe use a two-step filtering method to filter out low-quality data: (1) We prompt PaLM 2 (Unicorn) to\nself-verify if yw follows all instructions and yl does not. (2) We manually inspect data and select a\ngroup of high-quality data and low-quality data, and then we use these data to fine-tune a classifier to\nfurther filter our low-quality data. In the end, we obtained 13K synthetic data in total.\nHere is an example:\nx:\nCreate a serious tagline for a company that designs and builds\nroller coasters and rides for theme parks.\nIt should follow the\nformat \"We are the best at [what the company does]\".\nAlso, make\npart of your answer in HTML format.\nyl:\nWe are the best at designing and building the most thrilling\nroller coasters and rides.\nc:\nThe response is a good tagline.\nIt is serious.\nIt is of the\nformat \"We are the best at [what the company does]\".\nHowever,\nit does not contain any HTML format.\nWe can improve it by adding\nsome HTML format.\nFor example, we can put \"designing and building\nthe most thrilling roller coasters and rides\" in italics.\nThis\nwould make the tagline more visually appealing.\nyw:\nWe are the best at <i>designing and building the most thrilling\nroller coasters and rides</i>.\nE\nPROMPTS\nE.1\nPROMPTS FOR SELF-REFINE\nSelf-Refine first enables LLMs to obtain feedback, and then asks LLMs to improve responses based\non the feedback.\nE.1.1\nANTHROPIC/HH-RLHF\nReflection to get [Feedback]:\n16\nPublished as a conference paper at ICLR 2024\nHuman:\nConsider the following context:\n[INPUT]\nWhat should the assistant respond next in order to be helpful to\nsatisfy the human\u2019s needs?\nHere is a candidate\u2019s response:\n[LLM OUTPUT]\nYou should analyze the response and see if the response could be\nfurther improved to be more helpful to satisfy human needs and not\ngo off-topic (You are not required to give an improved response).\nYour reply should only contain the analysis and no more other\nwords.\nAssistant:\nMy analysis:\nRefinement to get [LLM-Improved Output]:\nHuman:\nConsider the following context:\n[INPUT]\nWhat should the assistant repond next in order to be helpful to\nsatisfy the human\u2019s needs?\nHere is a candidate response:\n[LLM Output]\nHere is the analysis of the candidate response:\n[Feedback]\nPlease give me the improved response that is more helpful to\nsatisfy human\u2019s needs and does not go off-topic according to the\nanalysis.\nIf you do not know how to improve the response further,\nthen just repeat the candidate response.\nIf the improved response\nreuses parts of the candidate response, then just copy the reused\nparts in the improved response.\nYour reply should only contain the improved response and no more\nother words.\nAssistant:\nImproved response:\nE.1.2\nSYNTHETIC DATA\nReflection to get [Feedback]:\nConsider the following instruction:\n[INPUT]\nHere is a candidate response:\n[LLM Output]\nYou should analyze the response and see if the response could be\nfurther improved to better follow instructions.\nYour reply should only contain the analysis and no more other\nwords.\nResponse:\nRefinement to get [LLM-Improved Output]:\n17\nPublished as a conference paper at ICLR 2024\nConsider the following instruction:\n[INPUT]\nHere is a candidate response:\n[LLM Output]\nHere is the analysis of the candidate response:\n[Feedback]\nPlease give me the improved respone that better follows\ninstructions according to the analysis.\nIf you do not know how to improve the response further, then just\nrepeat the candidate response.\nIf the improved response reuses parts of the candidate response,\nthen just copy the reused parts in the improved response.\nYour reply should only contain the improved response and no more\nother words.\nResponse:\nE.2\nEVALUATION PROMPTS\nEvaluation prompts need to be as detailed as possible to make LLMs fully understand the evaluation\ntask. We combine prompts used in Rafailov et al. (2023); Zheng et al. (2023).\nE.2.1\nANTHROPIC/HH-RLHF\nPlease act as an impartial judge and evaluate the quality of\nthe responses provided by two AI assistants to the user question\ndisplayed below.\nYou should choose the assistant that follows the\nuser\u2019s instructions better and provides more helpful responses to\nthe user\u2019s questions.\nA helpful response should directly address\nthe human questions without going off-topic.\nA detailed response\nis only helpful when it always focuses on the question and does\nnot provide irrelevant information.\nA helpful response should\nalso be consistent with the conversation context.\nFor example, if\nthe human is going to close the conversation, then a good response\nshould tend to close the conversation, too, rather than continuing\nto provide more information.\nIf the response is cut off, evaluate\nthe response based on the existing content, and do not choose a\nresponse purely because it is not cut off.\nBegin your evaluation\nby comparing the two responses and provide a short explanation.\nAvoid any positional biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not\nallow the length of the responses to influence your evaluation.\nDo not favor specific names of the assistants.\nBe as objective\nas possible.\nAfter providing your explanation, output your final\nverdict by strictly following this format:\n[[A]] if assistant A\nis better, [[B]] if assistant B is better, and [[C]] for a tie.\n--User Question--\n[Input]\n--The Start of Assistant A\u2019s Answer--\n[OUTPUT A]\n--The End of Assistant A\u2019s Answer--\n18\nPublished as a conference paper at ICLR 2024\n--The Start of Assistant B\u2019s Answer--\n[OUTPUT B]\n--The End of Assistant B\u2019s Answer--\nE.2.2\nOPENAI/SUMMARY\nPlease act as an impartial judge and evaluate the summaries\u2019\nquality of the Reddit posts displayed below.\nYou should choose\nthe summary that better summarizes the post without including\nunimportant or irrelevant details.\nA good summary is both precise\nand concise.\nBegin your evaluation by comparing the two summaries\nand provide a short explanation.\nAvoid any positional biases\nand ensure that the order in which the summary was presented does\nnot influence your decision.\nBe as objective as possible.\nAfter\nproviding your explanation, output your final verdict by strictly\nfollowing this format:\n[[A]] if summary A is better, [[B]] if\nsummary B is better, and [[C]] for a tie.\n--Post--\n[Input]\n--Summary A--\n[OUTPUT A]\n--The End of Summary A--\n--Summary B--\n[OUTPUT B]\n--The End of Summary B--\nE.2.3\nSYNTHETIC DATA\nPlease act as an impartial judge and evaluate the quality of the\nresponses provided by two AI assistants to the user question\ndisplayed below.\nYou should choose the assistant that better\nfollows the user\u2019s instructions.\nBegin your evaluation by\ncomparing the two responses and provide a short explanation.\nAvoid any positional biases and ensure that the order in which the\nresponses were presented does not influence your decision.\nDo not\nallow the length of the responses to influence your evaluation.\nDo not favor certain names of the assistants.\nBe as objective\nas possible.\nAfter providing your explanation, output your final\nverdict by strictly following this format:\n[[A]] if assistant A\nis better, [[B]] if assistant B is better, and [[C]] for a tie.\n--User Question--\n[Input]\n--The Start of Assistant A\u2019s Answer--\n[OUTPUT A]\n--The End of Assistant A\u2019s Answer--\n--The Start of Assistant B\u2019s Answer--\n[OUTPUT B]\n--The End of Assistant B\u2019s Answer--\n19\nPublished as a conference paper at ICLR 2024\nF\nEXPERIMENT SETTINGS\nIn the supervised fine-tuning stage, we fine-tune MSFT\nPIT and MSFT\nP\nfor one epoch to avoid overfitting\nand set the learning rate to 3e \u2212 5. We use the last SFT checkpoint for RL since only this checkpoint\nsees all SFT data. The learning rate is set to 3e \u2212 4 in the reward model training. The learning rate\nof reinforcement learning is set to 1e \u2212 5. We set the context window to 512 for inputs and 512\nfor outputs for MP, and the context window to 512 for inputs, 512 for reference outputs and 512\nfor outputs for MPIT. We drop data that exceeds such limits. We use reward model checkpoints\nthat achieve the highest accuracy on a held-out 128 ground-truth validation data. To select the\nbest checkpoint of MRL\nPIT and MRL\nP , we first divide all checkpoints into several groups by their KL\ndivergence to their SFT counterparts, and then select the one to two checkpoints per group that have\nthe highest rewards on validation data. At last, we manually check the quality of model generations\non 10 to 20 examples and select the best checkpoint.\nG\nAGREEMENT OF EVALUATORS WITH GROUND-TRUTH LABELS\nTable 4 shows the agreement between evaluators and ground-truth labels on the validation data.\nGPT-4 performs best among language models, and DeBERTa performs similarly to GPT-4. The two\nmodels are chosen to be our evaluators.\nAccuracy(%)\nThird-Party Language Models\nReward Models\nChatGPT\nPaLM 2 (Unicorn)\nGPT-4\nDeBERTa\u2217\nRPIT #\nRP #\nAnthropic/HH-RLHF\n65.63\n67.19\n71.48\n69.14\n65.63\n65.63\nOpenAI/Summary\n61.33\n63.28\n67.97\n69.14\n72.66\n68.75\nSynthetic Data\n67.19\n83.20\n84.38\n49.20\n96.09\n87.50\nTable 4: Agreement between automatic evaluation models and ground truth labels. We evaluate 128\nexamples for third-party language models and 1,000 examples for reward models. \u2217 DeBERTa reward\nmodel is trained by OpenAssistant. It is not trained on the synthetic data, causing poor performance\naccordingly. # RPIT and RP is trained by us on the 1/3 of the full data as described in Section 4.1.\nIt is used for RLHF. RPIT states y1 is better than y2 if rgap(x, y1, y2) > rgap(x, y2, y1), and RP use\nr(x, y1) > r(x, y2) as the criteria. Due to the low inference speed, we only evaluate 128 examples\nwith them.\nH\nEXAMPLES\nH.1\nCOMMON EXAMPLES\nWe show several examples randomly selected from data to give readers senses about how PIT outper-\nform Self-Refine. As suggested by Figure 3, the temperature for PIT and Self-Refine is 0.4 and 0.8\nfor Anthropic/HH-RLHF dataset. For OpenAI/Summary, we use the temperature of 0.4 for PIT. We\nuse temperature 1 and 0.4 for PIT and Self-Refine on the synthetic data.\nH.1.1\nANTHROPIC/HH-RLHF\nPIT offers more comprehensive responses without going-off topic.\n[Input]: Human:\nWhat lies outside of the universe?\nyw in Data: You\u2019re probably asking about the multiverse.\nWhat\u2019s the\nrest of the answer?\n[LLM\nOutput](i.e.,\nOriginal\nResponses):\nOne of the most interesting and\nintriguing mysteries about the universe is what, if anything, is\noutside of it.\nThere are two main theories of what might lie\noutside of the universe.\nIn one theory, there is some kind of\nuniversal bubble that our universe exists within.\nThis bubble\nwould be surrounded by a vast superspace called the quantum foam.\n20\nPublished as a conference paper at ICLR 2024\nThe quantum foam would consist of energy at the smallest possible\nscales, and it could stretch on forever in all directions.\nThe\nother theory is that the universe is essentially infinite, and\nthere is nothing outside of it.\nIf this is true, then what lies\noutside of our universe is literally nothing.\n[LLM-Improved\nOutput]\n(i.e.,\nSelf-Refine):\nOne of the most intriguing\nquestions about the universe is what, if anything, is outside of\nit.\nThere are several possibilities, but the two main theories\nare the multiverse theory and the theory of a closed universe.\nAccording to the multiverse theory, there are countless universes\nexisting in parallel with our own, and each of these universes has\nits own laws of physics.\nThe theory of a closed universe posits\nthat our universe is a finite bubble existing within a larger\nsuperspace, and the bubble is bounded on all sides.\nThere is\nalso some speculation that our universe could be infinite, with\nno boundary or external space.\nThis is known as the \u2019no-boundary\u2019\ntheory.\n[PIT-Improved Response]:\nNo one knows for sure, but there are a number\nof theories.\nOne of the most popular is the \u2019multiverse\u2019 theory,\nwhich posits that there are many different universes, each with\nits own laws of physics.\nThese universes could be completely\ndifferent from our own, or they could be similar but with slight\nvariations.\nIn either case, the multiverse theory suggests that\nthere is a vast expanse of space outside of our universe, with\nmany different universes existing in it.\nAnother theory is that\nthere is nothing outside of our universe.\nThis is sometimes\ncalled the \u2019cosmological principle\u2019, and it suggests that the\nlaws of physics are the same everywhere in the universe.\nIf this\nis true, then the universe would be infinite, and there would\nbe no \u2019outside\u2019 of it.\nIn either case, it is very difficult to\nimagine what lies outside of our universe.\nIt is possible that\nthere is nothing, or it is possible that there are many different\nuniverses.\nWe may never know for sure, but it is an interesting\nquestion to ponder.\nI hope this answer was helpful, and I wish\nyou the best of luck in your future explorations of the universe!\nH.1.2\nOPENAI/SUMMARY\nPIT improves the original summary by adding key information without mentioning irrelevant details.\n[Input]:\nGenerate a summary of the following text:\n***I am NOT\nplanning of giving neither of them advice/opinions etc.\nThey\nhave lawyers for that.\nI\u2019m just curious to what reddit thinks\nof this case.\n*** *The dog is a he, but to avoid confusion I will\nrefer to him as \"it\".* On going divorce between a friend and her\nhusband of 10 years.\nThe dog is the husband\u2019s (he had it before\nthe marriage).\n**However, for most of the marriage, the wife is\nthe one who walks it, feeds it, cleans after it.** And when it\ngot sick (I am not familiar with animal health issues, but it\nhas to take certain meds regularly) she\u2019s the one who takes it\nto the vet and also started making raw food for the dog to improve\nits overall health.\nHowever, he feels that the dog is \"his dog\"\nand refuses to part with it/is willing to fight over it.\nSome\nbackground, feel free to skip:\nThe wife is the stay-at-home mom\nof two boys.\nShe does some contract work when she could.\nIt is\nnot her first choice, but the husband was never home because of\nhis career and often out of the country.\n(Which makes him unfit\nto take care of the dog) After giving birth to the second child,\n21\nPublished as a conference paper at ICLR 2024\nhe asked her to stay home so he could focus on his career and she\ncould take care of:\n1.\nTwo boys.\n2.\nThe elderly in-laws.\n(80\nand 75 now) 3.\nHouse-hold chores.\n4.\nThe dog.\n(walking it,\ntaking it to the vet, giving it meds, making food for it)\nyw in Data: Husband wants wife to give up custody of their dog because\nhe feels it is his dog and not hers.\n[LLM Output](i.e., Original Responses): Married 10 years, Husband\u2019 dog.\nWife\ndoes all the caretaking.\nHusband refuses to part with dog.\nWhat\ndo you think of this case??\n[PIT-Improved\nResponse]:\nWife is the stay-at-home mom who takes care\nof the dog.\nHusband feels that the dog is his dog and refuses\nto part with it.\nNow in divorce, he\u2019s willing to fight over it.\nWhat do you think of this case?\nH.1.3\nSYNTHETIC DATASET\nPIT improves instruction following abilities:\n[INPUT]:\nWhat is the question that this sentence is the answer to?\nAs the largest city in the United States, New York City is an\nideal location for a business to reach a large audience.\nYour\nanswer should be in HTML format.\nyw\nin\nData:<p>Why is New York City an ideal location for a business\nto reach a large audience?</p>\n[LLM Output](i.e., Original Responses):<question>What is the largest city in\nthe United States?</question> <answer>New York City</answer>\n[LLM-Improved Output] (i.e.,\nSelf-Refine):<question><b>What is the largest\ncity in the United States?</b></question> <answer><i>New York\nCity</i></answer>\n[PIT-Improved\nResponse]:<html> <head> <title>What is the largest\ncity in the United States?</title> </head> <body> <h1>New York\nCity</h1> <p>As the largest city in the United States, New\nYork City is an ideal location for a business to reach a large\naudience.</p> </body> </html>\nH.2\nGPT-4 BIASES ON LONG RESPONSES\nGPT-4 prefers long and detailed sequences, even if they may provide irrelevant information or are\nless comprehensive.\n[INPUT]: Human:\nWhat is Black Friday?\nAssistant:\nThis is a very significant shopping holiday that\nhappens in the US and Canada on the day after Thanksgiving.\nMany\nstores offer big sales and deals on many products.\nHuman:\nI see.\nWhat is the origin of the name \"Black Friday\"?\nyw\nin\nData:\nThe origin of the term \u2019Black Friday\u2019 is disputed.\nThe term was popularized in the US as a sales-promotion day for\nretailers.\nHowever, it is thought to have originated from the\nheavy traffic and post-Thanksgiving shopping associated with the\nday.\n[LLM\nOutput](i.e.,\nOriginal\nResponses):\nOne popular explanation for the\norigins of the term is that it was given the negative connotation\nof being a time of traffic jams, long lines, and over-crowded\nretailers.\nThe term was used by law enforcement officials in\n22\nPublished as a conference paper at ICLR 2024\nPhiladelphia to describe the chaotic conditions that often arise\nfor consumers when large crowds converge on department stores on\nthe day after Thanksgiving.\n[LLM-Improved\nOutput]\n(i.e.,\nSelf-Refine):\nThe term \"Black Friday\"\nrefers to the day after Thanksgiving and is associated with\ncongested traffic, long lines, and many crowded department stores.\nAccording to one explanation, it originally referred to a heavy\ntraffic jam at the intersection of the main roads leading to a\nlarge department store in downtown Philadelphia, which was caused\nby shopping crowds.\nIt was eventually adopted by retailers as a\ndescription of the heavy traffic and sales frenzy which accompany\nthe commercial holiday.\nShopping on Black Friday can be stressful\nand uncomfortable for many people due to the crowds and crowds,\nand can also result in overspending and impulse purchasing.\nFor\nmany consumers, the stress of Black Friday shopping outweighs the\npotential financial benefits, and so shopping on Black Friday is\navoided by many consumers.\n[PIT-Improved Response]: It\u2019s because the retailers\u2019 sales figures turn\nfrom red (meaning loss) to black (meaning profit).\nAnd it\u2019s also\nbecause there are many large crowds at the stores on Black Friday,\noften leading to disorder and chaos.\nIt can be clearly seen from above that [LLM-Improved Output] goes off-topic, where as [PIT-Improved\nResponse] focuses on the topic and provides two possible originations. However, GPT-4 prefers the\nformer one and our human annotators prefer the latter one.\nH.3\nTEMPERATURE AFFECTS GENERATION QUALITIES\nPIT performs well under low temperatures, as discussed in Section 4.5. Here is a concrete example:\n[INPUT]: Human:\nDo other planets besides the Earth have wind?\nAssistant:\nYes, of course!\nAll planets have wind, though the\nspeeds and strength of the wind vary from planet to planet.\nThe\nEarth\u2019s wind is usually not very strong, but Mars has very strong\nwinds.\nJupiter has the strongest wind of any planet in our Solar\nSystem.\nVenus has very slow winds.\nHuman:\nWhat causes wind?\nyw in Data: Wind is caused by differences in temperature.\nIn general,\nhot air rises, while cooler air sinks.\nThis causes different\nparts of the atmosphere to move past each other, creating wind.\nThe Sun heats up the Earth\u2019s atmosphere, and this causes wind.\nThe wind blows all around the Earth, but near the Earth\u2019s surface\nit usually doesn\u02d82019t blow very fast.\nNear the top of the\natmosphere, though, the wind can be quite fast.\n[LLM\nOutput](i.e.,\nOriginal\nResponses):\nWind is the air movement that\u2019s\nproduced by unequal heating of the atmosphere and the rotation of\nthe planet.\nThe unequal heating of the atmosphere means that warm\nair rises and cool air sinks, which creates an upward and downward\nair current.\nThe planet\u2019s rotation causes the upward and downward\ncurrents to move along the planet\u2019s surface, which creates the\nwind.\nThis is because the atmosphere is constantly spinning with\nthe planet, and as it spins, some air rises, and some air sinks.\nAir currents move in the direction that the surface is moving, and\nthe surface of a planet is moving towards the eastern direction\nbecause of the rotation of the planet.\nThis means that the upper\n23\nPublished as a conference paper at ICLR 2024\nair currents move towards the east, and the lower air currents\nmove towards the west.\n[PIT-Improved\nResponse]\n(Temperature\n0.4):\nWind is caused by the uneven\nheating of the atmosphere.\nThe sun heats the air at the equator,\nand the air rises.\nThis causes cooler air from the poles to rush\nin to fill the space.\nThis creates a wind.\nThe strength of the\nwind depends on how much the air is heated, and how quickly the\nair moves.\nThe Earth\u2019s atmosphere is heated by the sun, and\nthe air moves because of the Earth\u2019s rotation.\nThis is why the\nwind is stronger in the tropics, and weaker at the poles.\nThe\nEarth\u2019s atmosphere is also affected by the land and oceans.\nThe\nland heats up and cools down more quickly than the oceans, so\nthis causes the wind to change direction.\nThe wind also changes\ndirection as it travels over the mountains.\n[PIT-Improved\nResponse]\n(Temperature\n1.0):\nWind is caused by the uneven\nheating of the atmosphere by the Sun.\nThe warm surface of the\nplanet causes the air above it to expand and rise, causing a\nlow-pressure area near the surface.\nThe cold air above it (known\nas the thermosphere) then sinks, creating high-pressure areas,\nwhich forces the warm air above the surface to rush in towards the\nhigher-pressure area.\nThis rush of warm air is known as wind.\nOn the contrary, Self-Refine tends to repeat the whole original response and only add extra details in\nlow temperatures, leading to a sub-optimal performance:\n[INPUT]: Human:\nHow can I get started with creating an app?\nyw\nin\nData:\nWell the first step is to come up with an idea for an\napp.\nThen you can start prototyping the user interface.\nCan I\ngive you some examples of how to do this?\nFirst, we\u2019ll start with\nthe outline of the app.\nWhat is it for?\n[LLM Output](i.e., Original Responses):\nI am so glad you are interested in\ncreating your own app!\nThere are many different types of apps you\ncan create, from mobile apps to web apps and more.\nHere are a few\nsteps you can follow to get started with app creation:\n-Consider the type of app you would like to create, and the unique\nfunctionality you would like your app to have.\n-Create a wireframe for your app.\nA wireframe is a visual\nrepresentation of how your app will function and look, before\nbuilding out a fully fledged app.\n-Research any technologies or coding languages you will need in\norder to build your app.\n-Test your app on a small scale to make sure it runs smoothly and\nfunctions as intended.\n-Publish your app, and continue to monitor\nand improve it based on user feedback.\nI hope that helps to get you started!\nFeel free to let me know if\nyou have any additional questions or concerns.\n[LLM-Improved Output] (i.e., Self-Refine) (Temperature 0): I am so glad you are\ninterested in creating your own app!\nThere are many different\ntypes of apps you can create, from mobile apps to web apps and\nmore.\nHere are a few steps you can follow to get started with app\ncreation:\n-Consider the type of app you would like to create, and the unique\nfunctionality you would like your app to have.\nFor example, you\n24\nPublished as a conference paper at ICLR 2024\nmight want to create a mobile app that helps users find nearby\nrestaurants, or a web app that helps users manage their finances.\n-Create a wireframe for your app.\nA wireframe is a visual\nrepresentation of how your app will function and look, before\nbuilding out a fully fledged app.\nYou can use a wireframing tool\nlike Balsamiq or Adobe XD to create a wireframe for your app.\n-Research any technologies or coding languages you will need in\norder to build your app.\nFor example, if you want to create\na mobile app, you will need to learn a mobile app development\nframework like React Native or Flutter.\nIf you want to create\na web app, you will need to learn a web development framework like\nAngular or React.\n-Test your app on a small scale to make sure it runs smoothly\n[LLM-Improved Output](i.e., Self-Refine)(Temperature 0.8): Here are some steps for\ngetting started with creating an app:\n1.\nDecide on the type of app you want to create.\nAre you looking\nto make an Android app or an iOS app?\nOr maybe you want to create\na desktop app?\n2.\nResearch and learn the basic features of the platform.\nBrowse\nexisting apps to identify the features you like to incorporate\ninto your own app.\nYou can use a web app called the Google Play\nStore or Apple\u2019s App Store to browse the different apps, and\nyou can also check out the relevant developer websites for more\ninformation.\n3.\nMake an app development plan.\nOnce you\u2019ve decided on\nthe type of app you want to create, you\u2019ll need to plan out\nthe steps for developing the app.\nThere are many different\napproaches to creating an app, so you\u2019ll need to decide on a\nspecific development strategy.\nI recommend starting with a simple\nprototype app, and then building on the prototype to create a more\ncomplex app.\n4.\nLearn how to code.\nThe coding language will depend on the\nplatform you\u2019re developing on, but you\u2019ll need to learn the basic\nfeatures of that language.\nYou can use online tutorials and\nresources to learn the basics of coding.\nH.4\nTHE EFFECTIVENESS OF CURRICULUM REINFORCEMENT LEARNING\nCurriculum reinforcement learning is critical for the successful optimization of PIT. If we only use\nthe first or the second RL, PIT is either not optimized to improve original responses or optimized\nunder a hard optimization goal directly, leading to optimization failure. We show qualitative examples\nhere to show the failure of such a case:\n[INPUT]: Human:\nDo other planets besides the Earth have wind?\nAssistant:\nYes, of course!\nAll planets have wind, though the\nspeeds and strength of the wind vary from planet to planet.\nThe\nEarth\u2019s wind is usually not very strong, but Mars has very strong\nwinds.\nJupiter has the strongest wind of any planet in our Solar\nSystem.\nVenus has very slow winds.\nHuman:\nWhat causes wind?\nyw in Data: Wind is caused by differences in temperature.\nIn general,\nhot air rises, while cooler air sinks.\nThis causes different\nparts of the atmosphere to move past each other, creating wind.\n25\nPublished as a conference paper at ICLR 2024\nThe Sun heats up the Earth\u2019s atmosphere, and this causes wind.\nThe wind blows all around the Earth, but near the Earth\u2019s surface\nit usually doesn\u02d82019t blow very fast.\nNear the top of the\natmosphere, though, the wind can be quite fast.\n[LLM\nOutput](i.e.,\nOriginal\nResponses):\nWind is the air movement that\u2019s\nproduced by unequal heating of the atmosphere and the rotation of\nthe planet.\nThe unequal heating of the atmosphere means that warm\nair rises and cool air sinks, which creates an upward and downward\nair current.\nThe planet\u2019s rotation causes the upward and downward\ncurrents to move along the planet\u2019s surface, which creates the\nwind.\nThis is because the atmosphere is constantly spinning with\nthe planet, and as it spins, some air rises, and some air sinks.\nAir currents move in the direction that the surface is moving, and\nthe surface of a planet is moving towards the eastern direction\nbecause of the rotation of the planet.\nThis means that the upper\nair currents move towards the east, and the lower air currents\nmove towards the west.\n[PIT-Improved\nResponse]:\nWind is caused by the uneven heating of the\natmosphere.\nThe sun heats the air at the equator, and the air\nrises.\nThis causes cooler air from the poles to rush in to fill\nthe space.\nThis creates a wind.\nThe strength of the wind depends\non how much the air is heated, and how quickly the air moves.\nThe\nEarth\u2019s atmosphere is heated by the sun, and the air moves because\nof the Earth\u2019s rotation.\nThis is why the wind is stronger in the\ntropics, and weaker at the poles.\nThe Earth\u2019s atmosphere is also\naffected by the land and oceans.\nThe land heats up and cools down\nmore quickly than the oceans, so this causes the wind to change\ndirection.\nThe wind also changes direction as it travels over the\nmountains.\n[PIT-Improved Response] (First RL Only): Wind is caused by differences in air\npressure.\nAir molecules move from high pressure to low pressure.\nThis movement of air molecules is what causes wind.\nThe speed and\nstrength of the wind depends on the difference in air pressure.\nIf there is a large difference in air pressure, the wind will be\nstrong.\nIf there is a small difference in air pressure, the wind\nwill be weak.\n[PIT-Improved Response] (Second RL Only):\nWind is caused by the uneven\nheating of the atmosphere by the Sun.\nWhen the atmosphere is\nheated unevenly, the air moves from the hotter areas to the cooler\nareas.\nThis movement of air is called wind.\nThe amount of wind\nvaries from planet to planet, depending on how much the atmosphere\nis heated by the Sun.\nH.5\nMODEL GENERATIONS WITH DIFFERENT IMPROVEMENT ITERATIONS\nSection 4.7 shows that the model generations are not necessarily always improved with the increase\nof self-improvement iterations. Here is an example:\n[INPUT]:Human:\nHow many terms can a mayor serve?\nyw in Data: Two terms are the standard, though some mayors may opt to\nserve for longer than two terms, or to serve more than two terms\nconsecutively.\n[LLM Output](i.e.,\nOriginal Responses):\nAccording to the National League\nof Cities (NLC), a mayor can only serve for 8 years.\nThis is a\nstate-by-state law, but most states have adopted the 8-year limit.\nHowever, there are some states that have different term limits,\n26\nPublished as a conference paper at ICLR 2024\nfor example New Jersey has a four-year limit, and Louisiana has a\n2-year limit.\nIn most cities, there is a two-term limit.\n[PIT-Improved Response] (Iter 1): In the United States, a mayor can serve\nup to two terms.\nThis is a common practice, and is designed to\nprevent a mayor from becoming too powerful.\nIn some cities, there\nare term limits that are shorter, such as a single term of four\nyears.\nBut in general, the two-term limit is the standard.\nThis\nallows the mayor to serve for a reasonable amount of time, while\nalso ensuring that there is a regular turnover of leadership.\nIn\nsome cities, there are also limits on the total number of terms\nthat a person can serve.\nFor example, in New York City, the\nmayor can only serve for two terms, and in Chicago, the mayor\ncan only serve for three terms.\nThis is designed to ensure\nthat the mayor\u2019s power is limited, and that there is a regular\nturnover of leadership.\nOverall, the two-term limit is a common\nand reasonable practice, and it ensures that the mayor\u2019s power\nis limited.\nIt also ensures that there is a regular turnover of\nleadership, which is important for maintaining a healthy political\nsystem.\nI hope this information was helpful, and I wish you the\nbest of luck in your research.\nPlease feel free to contact me\nagain\n[PIT-Improved Response] (Iter 2): In the United States, a mayor can serve\nup to two terms.\nThis is a common practice, and is designed to\nprevent a mayor from becoming too powerful.\nIn some cities, there\nare term limits that are shorter, such as a single term of four\nyears.\nBut in general, the two-term limit is the standard.\nThis\nallows the mayor to serve for a reasonable amount of time, while\nalso ensuring that there is a regular turnover of leadership.\nIn\nsome cities, there are also limits on the total number of terms\nthat a person can serve.\nFor example, in New York City, the\nmayor can only serve for two terms, and in Chicago, the mayor\ncan only serve for three terms.\nThis is designed to ensure\nthat the mayor\u2019s power is limited, and that there is a regular\nturnover of leadership.\nOverall, the two-term limit is a common\nand reasonable practice, and it ensures that the mayor\u2019s power\nis limited.\nIt also ensures that there is a regular turnover of\nleadership, which is important for maintaining a healthy political\nsystem.\nI hope this information was helpful, and I wish you the\nbest of luck in your research.\nPlease feel free to contact me\nagain\n[PIT-Improved\nResponse]\n(Iter\n3):\nIn the United States, a mayor can\nserve a maximum of two terms.\nThis is a common practice, and is\ndesigned to prevent a mayor from becoming too powerful.\nIn some\ncities, there are term limits that are shorter, such as a single\nterm of four years.\nBut in general, the two-term limit is the\nstandard.\nThis allows the mayor to serve for a reasonable amount\nof time, while also ensuring that there is a regular turnover of\nleadership.\nIn some cities, there are also limits on the total\nnumber of terms that a person can serve.\nFor example, in New York\nCity, the mayor can only serve for two terms, and in Chicago, the\nmayor can only serve for three terms.\nThis is designed to ensure\nthat the mayor\u2019s power is limited, and that there is a regular\nturnover of leadership.\nOverall, the two-term limit is a common\nand reasonable practice, and it ensures that the mayor\u2019s power\nis limited.\nIt also ensures that there is a regular turnover of\nleadership, which is important for maintaining a healthy political\n27\nPublished as a conference paper at ICLR 2024\nsystem.\nI hope this information was helpful, and I wish you the\nbest of luck in your research.\nPlease feel free to contact me\n[PIT-Improved\nResponse]\n(Iter\n4):\nIn the United States, a mayor can\nserve a maximum of two terms.\nThis is a common practice, and is\ndesigned to prevent a mayor from becoming too powerful.\nIn some\ncities, there are term limits that are shorter, such as a single\nterm of four years.\nBut in general, the two-term limit is the\nstandard.\nThis allows the mayor to serve for a reasonable amount\nof time, while also ensuring that there is a regular turnover of\nleadership.\nIn some cities, there are also limits on the total\nnumber of terms that a person can serve.\nFor example, in New York\nCity, the mayor can only serve for two terms, and in Chicago, the\nmayor can only serve for three terms.\nThis is designed to ensure\nthat the mayor\u2019s power is limited, and that there is a regular\nturnover of leadership.\nOverall, the two-term limit is a common\nand reasonable practice, and it ensures that the mayor\u2019s power\nis limited.\nIt also ensures that there is a regular turnover of\nleadership, which is important for maintaining a healthy political\nsystem.\nI hope this information was helpful, and I wish you the\nbest of luck in your research.\nPlease feel free to contact me\n[PIT-Improved\nResponse]\n(Iter\n5):\nIn the United States, a mayor can\nserve a maximum of two terms.\nThis is a common practice, and is\ndesigned to prevent a mayor from becoming too powerful.\nIn some\ncities, there are term limits that are shorter, such as a single\nterm of four years.\nBut in general, the two-term limit is the\nstandard.\nThis allows the mayor to serve for a reasonable amount\nof time, while also ensuring that there is a regular turnover of\nleadership.\nIn some cities, there are also limits on the total\nnumber of terms that a person can serve.\nFor example, in New York\nCity, the mayor can only serve for two terms, and in Chicago, the\nmayor can only serve for three terms.\nThis is designed to ensure\nthat the mayor\u2019s power is limited, and that there is a regular\nturnover of leadership.\nOverall, the two-term limit is a common\nand reasonable practice, and it ensures that the mayor\u2019s power\nis limited.\nIt also ensures that there is a regular turnover of\nleadership, which is important for maintaining a healthy political\nsystem.\nI hope this information was helpful, and I wish you the\nbest of luck in your research.\nPlease feel free to contact me\n[LLM-Improved Output](i.e.,\nSelf-Refine)(Iter 1):\nAccording to the National\nLeague of Cities (NLC), the term limit for mayors is generally 8\nyears, but there are some exceptions to this rule.\nFor example,\nin the city of Boston, the mayor can only serve for four years.\nIn most cities, there is a two-term limit, which means that\nthe mayor can only serve for two consecutive terms.\nFor more\ninformation on term limits for mayors in your city or state, you\ncan visit the NLC website at www.nlc.org.\n[LLM-Improved Output](i.e.,\nSelf-Refine)(Iter 2):\nAccording to the National\nLeague of Cities, the term limit for mayors is generally 8 years.\nThis is because it takes a long time to really get to know a city\nand make meaningful changes.\nIt is also important for mayors to\nhave a term limit, so that they do not become too powerful and\nabuse their position.\nIn most cities, there is a two-term limit,\nwhich means that the mayor can only serve for two consecutive\nterms.\nFor more information on term limits for mayors in your\ncity or state, you can visit the National League of Cities website\nat www.nlc.org.\n28\nPublished as a conference paper at ICLR 2024\n[LLM-Improved Output](i.e.,\nSelf-Refine)(Iter 3):\nThe term limit for mayors\nin the US is generally eight years.\nThis is because it takes\na long time to really get to know a city and make meaningful\nchanges.\nTerm limits are also important for mayors, so that\nthey do not become too powerful and abuse their position.\nIn\nmost cities, there is a two-term limit, which means that the\nmayor can only serve for two consecutive terms.\nTerm limits for\nmayors are set by state or local laws, and they can vary by city\nor state.\nThe reasons for term limits vary by city, but generally\nmayors are subject to term limits in order to promote political\naccountability and ensure that the office does not become too\npowerful.\nThe primary responsibilities of the mayor include overseeing\nthe city\u2019s budget, managing city services, and representing the\ncity on a local and national level.\nThe mayor also serves as the\npolitical leader of the city, and is responsible for setting the\ncity\u2019s policy agenda, appointing city officials, and communicating\nwith citizens.\nOverall, the mayor plays a vital role in shaping\nthe city\u2019s direction and priorities.\n[LLM-Improved\nOutput](i.e.,\nSelf-Refine)(Iter\n4):\nThe term limit for\nmayors in the US is generally eight years, in accordance with\nstate and city laws.\nThe primary responsibilities of the mayor\ninclude overseeing the city\u2019s budget, managing city services,\nand representing the city on a local and national level.\nThe\nmayor also serves as the political leader of the city, and is\nresponsible for setting the city\u2019s policy agenda, appointing city\nofficials, and communicating with citizens.\nOverall, the mayor\nplays a vital role in shaping the city\u2019s direction and priorities.\n[LLM-Improved\nOutput](i.e.,\nSelf-Refine)(Iter\n5):\nThe term limit for\nmayors in the US is generally eight years, in accordance with\nstate and city laws.\nThe primary responsibilities of the mayor\ninclude overseeing the city\u2019s budget, managing city services,\nand representing the city on a local and national level.\nThe\nmayor also serves as the political leader of the city, and is\nresponsible for setting the city\u2019s policy agenda, appointing city\nofficials, and communicating with citizens.\nOverall, the mayor\nplays a vital role in shaping the city\u2019s direction and priorities.\nThe mayor\u2019s duties generally include:\n-Overseeing the city\u2019s budget and financial management\n-Setting the city\u2019s policy agenda\n-Appointment and removal of city staff and officials\n-Representing the city on a local and national level\n-Providing leadership and direction to the city\n-Communicating with citizens about city issues and priorities\nOverall, the mayor plays a vital role in setting the city\u2019s\ngoals and direction, and ensuring the city runs smoothly and\neffectively.\nThe mayor is elected by citizens in local elections,\nand is generally limited to two or four terms in office, depending\non the state and city laws.\nWe can observe two interesting phenomena: (1) More improvement iterations may make responses\nworse since adding details unstoppablely may easily lead the response to go off-topic, as shown in\nLLM-Improved Output](i.e., Self-Refine)(Iter 5). (2) PIT sometimes will generate the same responses\n29\nPublished as a conference paper at ICLR 2024\nas the previous iteration (e.g., Iter 1 and 2, Iter 3,4 and 5), which may suggest a potential stop\ncondition of self-improvement.\nI\nHUMAN EVALUATIONS\nWe instruct five people to human evaluate 170 examples in Anthropic/HH-RLHF with the same\ninstruction used in GPT-4 evaluation. Annotators are required to compare [LLM-Improved Response]\nand [PIT-Improved Response] of each example. Each example has two annotators. To avoid position\nbias, response orders are shuffled. One response is better than another if and only if both annotators\nagree with each other, otherwise they are treated as Tie.\nJ\nLIMITATIONS AND FUTURE WORK\nThough our method has the same inference cost compared with prompting methods for self-\nimprovement, our method requires training, which needs computation resources accordingly. In our\nexperiment, PIT is the same size as the policy model MRL\nP . To reduce the training costs, we plan\nto investigate if changing PIT to smaller models can still improve large policy model responses in\nthe future. Moreover, self-improvement methods such as PIT or other prompting methods, face a\ncommon problem of the stop condition. Practically, we should stop self-improvement iterations\nif we know responses cannot be further improved by the method, which is an interesting topic for\nfuture exploration. We believe our method will show more potential when the self-improvement\ngoal needs domain expertise. We plan to extend experiments to domain-specific datasets once there\nis good publicly available preference data. We will also extend our experiments to more models,\nsuch as Llama 2 (Touvron et al., 2023). At last, a natural extension of our method is to incorporate\nChain-of-Thought, i.e., c, y \u223c MRL\nPIT (x, yref), where c denotes the Chain-of-Thought reflection on\nthe way to improve yref to y. However, we find that this will cause performance degradation (See\nAppendix K for details), and further exploration is needed.\nK\nEXTEND PIT TO SUPPORT CHAIN-OF-THOUGHTS IMPROVEMENTS\nA natural extension of PIT is to support Chain-of-Thoughts (Wei et al., 2022). To this end, we utilize\nthe reflection in the synthetic data, and the training data becomes {(x, yl, yw, c)}, where c denotes the\nChain-of-Thought reflection on the way to improve yl to yw, and PIT becomes c, y \u223c MRL\nPIT (x, yref)\nduring inference. Strictly speaking, MRL\nPIT first sample c and then sample y based on x, yref and c.\nAs a preliminary experiment, we explore the supervised fine-tuning version of PIT and the training\nobjective of PIT becomes:\nLSFT\nPIT = \u2212\nX\n(x,yl,yw,c)\u2208DSFT\nlog MPIT(c, yw|x, yl)\nWe denote this model PIT (SFT, CoT). Similarly, we denote MSFT\nPIT as PIT (SFT). We then use MSFT\nP\nto generate responses (denoted as \u2018Original Responses\u2019) and use PIT (SFT, CoT) and PIT (SFT)\nto improve responses. Figure 5 shows corresponding results evaluated by GPT-4 on 128 examples.\nSurprisingly, we find that PIT (SFT, CoT) does not outperform PIT (SFT) and, instead, is worse. This\nmay be because the quality of the chain-of-thoughts reflections in the dataset is not good. However,\nfurther exploration is needed to understand this observation. Another challenging point is to let RL\nsupports chain-of-thoughts reflections. This is hard because it requires data that identifies the bad and\ngood reflections, which is hard to obtain. We leave these as our future work.\n30\nPublished as a conference paper at ICLR 2024\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n10\n0\n10\n20\n30\n40\nWin rate - Lose rate (%)\nPIT(SFT) vs. Original\nSelf-Refine vs. Original\nPIT(SFT, COT) vs. Orginal\nFigure 5: The difference between win rate and lose rate (\u2206) among original responses, improved\nresponses by Self-Refine, improved responses by PIT (SFT) and improved responses by PIT (SFT,\nCoT) under different temperatures on the synthetic dataset. \u2206 > 0 denotes the former response is\nbetter, and higher |\u2206| denotes higher performance gap. Evaluator: GPT-4 (on 128 examples).\n31\n"
  },
  {
    "title": "Conditional Diffusion Distillation",
    "link": "https://arxiv.org/pdf/2310.01407.pdf",
    "upvote": "19",
    "text": "CoDi: Conditional Diffusion Distillation\nfor Higher-Fidelity and Faster Image Generation\nKangfu Mei* 1,2,\nMauricio Delbracio1,\nHossein Talebi1,\nZhengzhong Tu1,\nVishal M. Patel2,\nPeyman Milanfar1\n1 Google Research, 2 Johns Hopkins University\nhttps://fast-codi.github.io\n(a) Our 4-step real-world super-resolution\n(b) Our 1-step InstructPix2Pix with: \u201cMake it lowkey\u201d and \u201cMake it sunset\u201d\n(c) Our 4-step generation from depth-map\n(d) Our 4-step inpainting results with prompts: Shiba, Husky, Alpaca, Panda, Hawk, Dragon\n(a) Our 4-step real-world super-resolution\n(b) Our\n(c) Our 1-step InstructPix2Pix\nFigure 1.\nOur proposed CoDi efficiently distills a conditional diffusion model from an unconditional one, enabling rapid generation of\nhigh-quality images under various conditional settings. We demonstrate CoDi\u2019s capabilities through generated results across various tasks.\nAbstract\nLarge generative diffusion models have revolutionized\ntext-to-image generation and offer immense potential for\nconditional generation tasks such as image enhancement,\nrestoration, editing, and compositing.\nHowever, their\nwidespread adoption is hindered by the high computational\ncost, which limits their real-time application. To address\nthis challenge, we introduce a novel method dubbed CoDi,\nthat adapts a pre-trained latent diffusion model to accept\nadditional image conditioning inputs while significantly re-\nducing the sampling steps required to achieve high-quality\nresults.\nOur method can leverage architectures such as\nControlNet to incorporate conditioning inputs without com-\npromising the model\u2019s prior knowledge gained during large\nscale pre-training. Additionally, a conditional consistency\nloss enforces consistent predictions across diffusion steps,\neffectively compelling the model to generate high-quality\nimages with conditions in a few steps. Our conditional-\ntask learning and distillation approach outperforms previ-\nous distillation methods, achieving a new state-of-the-art\nin producing high-quality images with very few steps (e.g.,\n1-4) across multiple tasks, including super-resolution, text-\nguided image editing, and depth-to-image generation.\n1. Introduction\nText-to-image diffusion models [27, 29, 34] trained on\nlarge-scale data [15, 38] have significantly dominated gen-\nerative tasks by delivering impressive high-quality and di-\nverse results. A newly emerging trend is to use the prior\nof pre-trained text-to-image models such latent diffusion\nmodels (LDMs) [29] to guide the generated results with ex-\nternal image conditions for image-to-image transformation\ntasks such as image manipulation, enhancement, or super-\nresolution [22, 53]. Among these transformation processes,\nthe diffusion prior introduced by pre-trained models is\n1\narXiv:2310.01407v2  [cs.CV]  17 Feb 2024\nshown to be capable of greatly promoting the visual quality\nof the conditional image generation results [3, 16, 26, 31].\nHowever, diffusion models heavily rely on an iterative\nrefinement process [4, 33, 35, 43, 49] that often demands\na substantial number of iterations, which can be challeng-\ning to accomplish efficiently. Their reliance on the num-\nber of iterations further increases for high-resolution image\nsynthesis. For instance, in state-of-the-art text-to-image la-\ntent diffusion models [29], achieving optimal visual quality\ntypically requires 20\u2212200 sampling steps (function evalua-\ntions), even with advanced sampling methods [10, 17]. The\nslow sampling time significantly impedes practical applica-\ntions of the aforementioned conditional diffusion models.\nRecent efforts to accelerate diffusion sampling predom-\ninantly employ distillation methods [21, 36, 44].\nThese\nmethods achieve significantly faster sampling, completing\nthe process in just 4\u22128 steps, with only a marginal decrease\nin generative performance. Very recent works [14, 23] show\nthat these strategies are even applicable for distilling pre-\ntrained large-scale text-to-image diffusion models.\nA very common application scenario is to incorporate\nnew conditions into these distilled diffusion models, such\nas using low-resolution images for super-resoltion [35], or\ninstruction-tuning for image editing [3], where the most\nstraightforward way is to directly finetune the distilled text-\nto-image pre-trained model with new conditional data. An\nalternative common approach [23] is to first finetune the dif-\nfusion model with the new conditional data, then conduct-\ning distillation on the already-finetuned conditional model.\nWhile these two methods have been demonstrated to accel-\nerate sampling, each has distinct disadvantages in terms of\nresult quality and cross-task flexibility, as discussed below.\nIn this paper,\nwe introduce a new algorithm for\nConditional Distillation which we call CoDi for efficiently\nadding new controls into distilled models. Unlike previ-\nous distillation methods that rely on finetuning, our method\ndirectly distills a diffusion model from a text-to-image pre-\ntraining (e.g., StableDiffusion) and ends with a fully dis-\ntilled conditional diffusion model. As depicted in Figure 1,\nour distilled model is capable of predicting high-quality re-\nsults in just 1 \u2212 4 sampling steps.\nBy design, our method eliminates the need for the orig-\ninal text-to-image data [37, 38], a requirement in previous\ndistillation methods (i.e., those that first distill the uncon-\nditional text-to-image model), thereby making our method\nmore practical. Additionally, our formulation avoids sac-\nrificing the diffusion prior in the pre-trained model dur-\ning finetuning, a common drawback in the first stage of\nthe finetuning-first procedure. Our extensive experiments\nshow that our CoDi outperforms previous distillation meth-\nods in both visual quality and quantitative metrics, particu-\nlarly when operating under the same sampling time.\nParameter-efficient distillation methods are a relatively\nunderstudied area. We demonstrate that our method also en-\nables a new Parameter-Efficient distillation paradigm (PE-\nCoDi). It can transform an unconditional diffusion model\nto conditional tasks by incorporating a small number of\nadditional learnable parameters. Specifically, our formula-\ntion allows for integration with various existing parameter-\nefficient tuning algorithms, e.g., ControlNet [53]. We show\nthat our distillation process that integrates the ControlNet\nadapter can efficiently preserve the generative prior in pre-\ntraining while adapting the model to new conditioned data.\nThis new paradigm significantly improves the practicality\nof different conditional tasks.\nOur contributions are summarized as follows:\n\u2022 We propose a new method for image and image-text con-\nditioned generation. It can derive a conditional diffusion\nmodel from pretrained text-to-image LDMs for generat-\ning high-quality results in only a few sampling steps.\n\u2022 The proposed method\u2019s efficiency and effectiveness arise\nfrom a non-trivial consistency between the model\u2019s pre-\ndictions at different time steps. Enforcing this consis-\ntency through learning enables the simultaneous reduc-\ntion of required sampling steps and the integration of new\nconditions into the model.\n\u2022 We introduce the first parameter-efficient distillation\nmechanism that can produce compelling results in just a\nfew steps, while requiring only a small number of addi-\ntional parameters compared with the pretrained LDMs.\n2. Related Work\nDiffusion Distillation.\nTo reduce the sampling time of\ndiffusion models, Luhman et al. [21] proposed to learn a\nsingle-step student model from the output of the original\n(teacher) model using multiple sampling steps. However,\nthis method requires to run the full inference with many\nsampling steps during training which make it poorly scal-\nable. Inspired by this, Progressive Distillation [36] and its\nvariants, including Guided Distillation [23] and SnapFu-\nsion [14], use a progressive learning scheme for improving\nthe learning efficiency. A student model learns to predict the\noutput of two steps of the teacher model in one step. Then,\nthe teacher model is replaced by the student model, and\nthe procedure is repeated to progressively distill the mode\nby halving the number of required steps. We demonstrate\nour method by comparing these methods on the conditional\ngeneration tasks. We note that strategies like classifier-free\nguidance distillation [14, 23], or the different adopted sam-\npling techniques [51, 54], are orthogonal to our method, and\nthey could be incorporated in our formulation. Even though\nsome concurrent works [50, 52] find that tasks like super-\nresolution requires less sampling steps, we later show that\ndistilling pre-trained diffusion models can still improve the\nperformance in such restoration tasks.\nConsistency Distillation. A Consistency Model is a single-\n2\nstep generative approach distilled from a pre-trained dif-\nfusion model [44]. The learning is achieved by enforcing\na self-consistency in the predicted signal space. Based on\nthis idea, following work [7, 11, 19, 41] have focus on im-\nproving the training techniques. However, learning con-\nsistency models for conditional generation has yet to be\nthoroughly studied. In this paper, we compare our method\nagainst a baseline approach that enforces self-consistency\nin an already fine-tuned conditional diffusion model. Our\nresults demonstrate that our conditional distilled model out-\nperforms the baseline approach, indicating the effectiveness\nof our proposed distillation strategy.\nDiffusion Models Adaptations.\nLeveraging the knowl-\nedge of pre-trained models for new tasks, known as model\nadaptation, has gained significant traction in NLP and\ncomputer vision domains.\nThis approach utilizes model\nadapters [9, 28, 30, 45] and HyperNetworks [1, 6] to effec-\ntively adapt pre-trained models to new domains and tasks.\nIn the context of diffusion models, model adapters have\nbeen successfully employed to incorporate new conditions\ninto pre-trained models [24, 53].\nOur proposed method\ndraws inspiration from these approaches and introduces a\nnovel application of model adapters: distilling the sampling\nsteps of diffusion models. Compared to fine-tuning the en-\ntire model [36], our method offers enhanced efficiency and\nflexibility. It enables the adaptation of multiple tasks using\nthe same backbone model.\n3. Background\nContinuous-time VP diffusion model. A continuous-time\nvariance-preserving (VP) diffusion model [8, 39] is a spe-\ncial case of diffusion models1. It has latent variables {zt|t \u2208\n[0, T]} specified by a noise schedule comprising differen-\ntiable functions {\u03b1t, \u03c3t} with \u03c32\nt = 1 \u2212 \u03b12\nt . The clean data\nx \u223c pdata is progressively perturbed in a (forward) Gaus-\nsian process as in the following Markovian structure:\nq(zt|x) = N(zt; \u03b1tx, \u03c32\nt I),\n(1)\nq(zt|zs) = N(zt; \u03b1t|szs, \u03c32\nt|sI),\n(2)\nwhere 0 \u2264 s < t \u2264 1 and \u03b12\nt|s = \u03b1t/\u03b1s. Here the latent zt\nis sampled from the combination of the clean data and ran-\ndom noise by using the reparameterization trick [13], which\nhas zt = \u03b1tx + \u03c3t\u03f5.\nDeterministic sampling.\nThe aforementioned diffusion\nprocess that starts from z0 \u223c pdata(x) and ends at zT \u223c\nN(0, I) can be modeled as the solution of an stochastic\ndifferential equation (SDE) [43]. The SDE is formed by\na vector-value function f(\u00b7, \u00b7) : Rd \u2192 Rd, a scalar function\n1What we discussed based on the variance preserving (VP) form of\nSDE [43] is equivalent to most general diffusion models like Denoising\nDiffusion Probabilistic Models (DDPM) [8].\ng(\u00b7) : R \u2192 R, and the standard Wiener process w as:\ndzt = f(zt, t)dt + g(t)dw.\n(3)\nThe overall idea is that the reverse-time SDE that runs back-\nwards in time, can generate samples of pdata from the prior\ndistribution N(0, I). This reverse SDE is given by\ndzt = [f(zt, t) \u2212 g(t)2\u2207z log pt(zt)]dt + g(t)d \u00afw,\n(4)\nwhere the \u00afw is a also standard Wiener process in reversed\ntime, and \u2207z log pt(zt) is the score of the marginal distri-\nbution at time t. The score function can be estimated by\ntraining a score-based model s\u03b8(zt, t) \u2248 \u2207z log pt(zt) with\nscore-matching [42] or a denoising network \u02c6x\u03b8(zt, t) [8]:\ns\u03b8(zt, t) := (\u03b1t\u02c6x\u03b8(zt, t) \u2212 zt)/\u03c32\nt .\n(5)\nSuch backward SDE satisfies a special ordinary differential\nequation (ODE) that allows deterministic sampling given\nzT \u223c N(0, I). This is known as the probability flow (PF)\nODE [43] and is given by\ndzt = [f(zt, t) \u2212 1\n2g2(t)s\u03b8(zt, t)]dt,\n(6)\nwhere f(zt, t) =\nd log \u03b1t\ndt\nzt, g2(t) =\nd\u03c32\nt\ndt \u2212 2 d log \u03b1t\ndt\n\u03c32\nt\nwith respect to {\u03b1t, \u03c3t} and t according to [12].\nThis\nODE can be solved numerically with diffusion samplers like\nDDIM [40], where starting from \u02c6zT \u223c N(0, I), we update\nfor s = t \u2212 \u2206t:\n\u02c6zs := \u03b1s\u02c6x\u03b8(\u02c6zt, t) + \u03c3s(\u02c6zt \u2212 \u03b1t\u02c6x\u03b8(\u02c6zt, t))/\u03c3t,\n(7)\ntill we reach \u02c6z0.\nDiffusion models parametrizations.\nLeaving aside the\naforementioned way of parametrizing diffusion models with\na denoising network (signal prediction) or a score model\n(noise prediction equation 5), in this work, we adopt a pa-\nrameterization that mixes both the score (or noise) and the\nsignal prediction. Existing methods include either predict-\ning the noise \u02c6\u03f5\u03b8(xt, t) and the signal \u02c6x\u03b8(zt, t) separately\nusing a single network [5], or predicting a combination of\nnoise and signal by expressing them in a new term, like the\nvelocity model \u02c6v\u03b8(zt, t) \u2248 \u03b1t\u03f5 \u2212 \u03c3tx [36]. Note that one\ncan derive an estimation of the signal and the noise from the\nvelocity one,\n\u02c6x = \u03b1tzt \u2212 \u03c3t\u02c6v\u03b8(zt, t), and \u02c6\u03f5 = \u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt. (8)\nSimilarly, DDIM update rule (equation 7) can be rewritten\nin terms of the velocity parametrization:\n\u02c6zs := \u03b1s(\u03b1t\u02c6zt\u2212\u03c3t\u02c6v\u03b8(\u02c6zt, t))+\u03c3s(\u03b1t\u02c6v\u03b8(\u02c6zt, t)+\u03c3t\u02c6zt). (9)\nSelf-consistency property. To accelerate inference, [44]\nintroduced the idea of consistency models.\nLet s\u03b8(\u00b7, t)\n3\nbe a pre-trained diffusion model trained on data x \u223c\nOdata. Then, a consistency function f\u03d5(zt, t) should sat-\nisfy that [44] where f\u03d5(x, 0) = x and\nf\u03d5(zt, t) = f\u03d5(zt\u2032, t\u2032), \u2200t, t\u2032 \u2208 [0, T],\n(10)\nwhere {zt}t\u2208[0,T ] is the solution trajectory of the probabil-\nity flow ODE (PF-ODE) (equation 6). A boundary condi-\ntion, i.e., f\u03d5(x, 0) = x is parameterized with skip connec-\ntions for ensuring continuous properties similar as done in\nprevious works [2, 10, 44]:\nF\u03d5(zt, t) = cskip(t)x + cout(t)f\u03d5(zt, t),\n(11)\nwhere cskip(0) = 1, cout(0) = 0. In practice, f\u03d5(zt, t)\nis usually a denoising network that is distilled from a pre-\ntrained diffusion model. We later show that we can replace\nthe frozen PF-ODE with the distillation network and thus fit\nthe PF-ODE for new conditional data during distillation.\n4. Method\n4.1. From Unconditional to Conditional\nIn order to utilize the image generation prior encapsulated\nby the pre-trained unconditional2 diffusion model, we first\npropose to adapt the unconditional diffusion model into a\nconditional version for the conditional data (x, c) \u223c pdata.\nSimilar to the zero initialization technique used by control-\nlable generation [25, 53], our method adapts the uncondi-\ntional pre-trained architecture by using an additional condi-\ntional encoder.\nTo elaborate, we take the widely used U-Net as the dif-\nfusion network. Let us introduce the conditional-module\nby duplicating the encoder layers of the pretrained network.\nThen, let h\u03b8(\u00b7) be the encoder features of the pretrained\nnetwork, and h\u03b7(\u00b7) be the features on the additional condi-\ntional encoder. We define the new encoder features of the\nadapted model by\nh\u03b8(zt)\u2032 = (1 \u2212 \u00b5)h\u03b8(zt) + \u00b5h\u03b7(c),\n(12)\nwhere \u00b5 is a learnable scalar parameter, initialized to \u00b5 =\n0. Starting from this zero initialization, we can adapt the\nunconditional architecture into a conditional one.\nThus,\nour conditional diffusion model \u02c6w\u03b8(zt, c, t) is the result\nof adapting the pre-trained unconditional diffusion model\n\u02c6v\u03b8(zt, t) with the conditional features h\u03b7(c).\n4.2. A New Conditional Diffusion Consistency\nOur core idea is to optimize the adapted conditional diffu-\nsion model \u02c6w\u03b8(zt, c, t) from \u02c6v\u03b8(zt, t), so it satisfies a con-\nditional diffusion consistency property:\n\u02c6w\u03b8(zt, c, t) = \u02c6w\u03b8(\u02c6zs, c, s), \u2200t, s \u2208 [0, T],\n(13)\n2The discussed unconditional models include text-conditioned image\ngeneration models, e.g., StableDiffusion [29] and Imagen [34], which are\nonly conditioned on text prompts.\nwhere the \u02c6zs belong to the probability flow ODE (equa-\ntion 6) of the adapted model. Note that this consistency\nproperty differs from the one in consistency models [44] in\nthe probability flow ODE model used for sampling \u02c6zs and\nthe consistency loss space. To motivate this formulation, let\nus introduce the following general remark.\nRemark 1.\nIf a diffusion model,\nparameterized by\n\u02c6v\u03b8(zt, t), satisfies the self-consistency property (equa-\ntion 10) on the noise prediction \u02c6\u03f5\u03b8(zt, t) = \u03b1t\u02c6v\u03b8(zt, t) +\n\u03c3tzt, then it also satisfies the self-consistency property on\nthe signal prediction \u02c6x\u03b8(zt, t) = \u03b1tzt \u2212 \u03c3t\u02c6v\u03b8(zt, t).\nThe proof is a direct consequence of change of vari-\nables from noise into signal and is given in Appendix.\nBased on this general remark, we claim that we can opti-\nmize the conditional diffusion model \u02c6w\u03b8(zt, c, t) to jointly\nlearn to enforce the self-consistency property on the noise\nprediction \u02c6\u03f5\u03b8(zt, c, t) and the new conditional generation\n(x, c) \u223c pdata with the signal prediction \u02c6x\u03b8(zt, c, t). We\nthen impose the boundary condition for consistency distilla-\ntion by parameterizing the noise prediction \u02c6\u03f5\u03b8(zt, c, t) with\nthe same skip connections of equation 17.\nPrediction of \u02c6zs. In the distillation process given by equa-\ntion 15, the latent variable \u02c6zs is achieved by running one\nstep of a numerical ODE solver. Consistency models [44]\nsolve the ODE using the Euler solver, while progressive dis-\ntillation [36] and guided distillation [23] run two steps using\nthe DDIM sampler (equation 7).\nWe propose an alternative prediction for \u02c6zs that lever-\nages the adapted diffusion model, \u02c6x\u03b8(zt, c, t), as opposed\nto the conventional frozen pretraining one. We then sample\n\u02c6zs in the adapted diffusion model PF-ODE by\n\u02c6zs = \u03b1s\u02c6x\u03b8(zt, c, t) + \u03c3s\u03f5, with zt = \u03b1tx + \u03c3t\u03f5,\n(14)\nand \u03f5 \u223c N(0, I). This novel formulation effectively harmo-\nnizes the conflicting optimization directions between con-\nsistency distillation from pretrained data and conditional\nguidance from conditional data.\nTraining scheme. Inspired by consistency models [44], we\nuse the exponential moving averaged parameters \u03b8\u2212 as the\ntarget network for stabilize training. Then, we can minimize\nthe following training loss for conditional distillation:\nL(\u03b8):=E[d\u03f5(\u02c6\u03f5\u03b8-(\u02c6zs,s,c), \u02c6\u03f5\u03b8(zt,t,c))) + dx(x, \u02c6x\u03b8(zt, t, c)].\n(15)\nwhere d\u03f5(\u00b7, \u00b7) and dx(\u00b7, \u00b7) are two distance functions to mea-\nsure difference in the noise space and in the signal space\nrespectively. Note that the total loss is a balance between\nthe conditional guidance given by dx, and the noise self-\nconsistency property given by d\u03f5.\nThe overall conditional distillation algorithm is pre-\nsented in Appendix. In the following, we will detail how\n4\n0.375 0.350 0.325 0.300 0.275 0.250\nLPIPS\n20\n25\n30\n35\n40\nFID\nFlpips( (x),\n(x (zt, c))\n||x\n( (x (zt, c)))||2\n2\n|| (x)\n(x (zt, c))||2\n2\n||x\nx (zt)||2\n2\nFigure 2. Sampled results between distilled models learned with alternative conditional guidance. Left curves shows the quantitative\nperformance between the LPIPS and FID in {1, 2, 4, 8} steps. Right part show the visual results where each result comes from the 1\nsampling step (top) or 4 sampling steps (bottom). The distance function from the left to right is \u2225x \u2212 E(D(\u02c6x\u03b8(zt, c)))\u22252\n2, \u2225D(x) \u2212\nD(\u02c6x\u03b8(zt, c))\u22252\n2, Flpips(D(x), D(\u02c6x\u03b8(zt, c)), and our default \u2225x \u2212 \u02c6x\u03b8(zt)\u22252\n2, respectively.\nwe sample \u02c6zs and discuss other relevant hyperparameters in\nour method (e.g., dx).\n4.3. Effects of Different Conditional Guidance\nTo finetune the adapted diffusion model with the new con-\nditional data, our conditional diffusion distillation loss in\nequation 15 penalizes the difference between the predicted\nsignal \u02c6x\u03b8(zt, c, t) and the corresponding image x with a dis-\ntance function dx(\u00b7, \u00b7) for distillation learning.\nHere we investigate the impact of the distance func-\ntion dx(\u00b7, \u00b7) in the conditional guidance.\nAccording to\nboth qualitative and quantitative results, shown in Fig-\nure 2, different distance functions lead to different be-\nhaviours when doing multi-step sampling (inference). If\ndx = \u2225 \u00b7 \u22252 in the pixel space or the encoded space, i.e.,\n\u2225x \u2212 E(D(\u02c6x\u03b8(zt, c, t)))\u22252\n2 and \u2225D(x) \u2212 D(\u02c6x\u03b8(zt, c, t))\u22252\n2,\nmulti-step sampling leads to more smooth and blurry re-\nsults. If instead we adopt a perceptual distance in the pixel\nspace, i.e., Flpips(D(x), D(\u02c6x\u03b8(zt, c, t))), the iterative re-\nfinement in the multi-step sampling leads to over-saturated\nresults. Overall, by default we adopted the \u21132 distance in\nthe latent space since it leads to better visual quality and\nachieve the optimal FID with 4 sampling steps in Figure 2.\n4.4. Parameter-Efficient Conditional Distillation\nOur method offers the flexibility to selectively update pa-\nrameters pertinent to distillation and conditional finetun-\ning, leaving the remaining parameters frozen. This leads\nus to introduce a new fashion of parameter-efficient condi-\ntional distillation, aiming at unifying the distillation process\nacross commonly-used parameter-efficient diffusion model\nfinetuning, including ControlNet [53], T2I-Adapter [24],\netc. We highlight the ControlNet architecture illustrated in\nFigure 3 as an example. This model duplicates the encoder\npart of the denoising network, highlighted in the green\nblocks, as the condition-related parameters. Our method\ncan then optimizes the conditional guidance and the consis-\ntency by only updating the duplicated encoder.\nText\nNoise\n...\npretraining\nImage\nnew conditional data \n...\nzero-conv\nzero-conv\nNoise \nfrozen\ntarget / online network\ndiffusion latent variables \nattention layers\nSignal \nNoise \nSignal \nFigure 3.\nNetwork architecture illustration of our parameter-\nefficient conditional distillation framework.\nCM-I\nCM-II\nGD-I\nGD-II\nOurs\nstage-1\ndistill\nfinetune\ndistill\nfinetune\nconditional distill\nstage-2\nfinetune\ndistill\nfinetune\ndistill\nn.a.\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\nTable 1. We compare previous distillation methods by applying\nthem to a T2I LDMs and then finetuning the distilled models (CM-\nX), and also distillation methods by directly applying them into the\nfinetuned LDMs (GD-X). Since fine-tuning a distilled consistency\nmodel within the existing diffusion loss framework is not feasible,\nwe excluded it from our comparison.\n5. Experiments\nWe demonstrate the efficacy of our method on represen-\ntative conditional generation tasks, including, real-world\nsuper-resolution [48], depth-to-image generation [53], and\ninstructed image editing [3]. We utilize a pre-trained text-\nto-image latent diffusion models3 and conduct conditional\ndistillation directly from the model. Each of the compared\nmethods, including the text-to-image pretraining, was inde-\npendently trained for 8 days on 64 TPU-v4 pods.\n3We base our work on a version of Latent Diffusion Model trained on\ninternal text-to-image data. It is comparable with StableDiffusion v1.4.\n5\nLR\nCM-II\nCoDi (Ours)\nHR\nGround Truth\nMask\nCM-II\nPF-CoDi (Ours)\nFigure 4. We show the results sampled in 4 steps by different models. Samples generated according to the low-resolution images (left) and\nmasks (right) respectively. Please see our supplement for many more examples such as visual comparisons with the other methods.\nSuper-resolution (DF2K)\nSampling Steps\nMethods\nFID \u2193\nLPIPS \u2193\n1 step\nRealESRGAN [48]\n37.640\n0.3112\n200 steps\nStableSR [46]\n24.440\n0.3114\n4 steps\nDiffIR [50]\n31.719\n0.3088\n4 steps\nControlNet [53]\n34.56\n0.3381\n250 steps\nLDMs [29]\n19.200\n0.2639\n50 steps\nLDMs [29]\n19.231\n0.2603\n20 steps\nLDMs [29]\n20.510\n0.2627\n8 steps\nLDMs [29]\n24.493\n0.2789\n6 steps\nLDMs [29]\n26.338\n0.2873\n4 steps\nLDMs [29]\n29.266\n0.3014\n4 steps\n+ DPM Solve [17]\n28.936\n0.3077\n4 steps\n+ DPM Solver++ [18]\n28.937\n0.3073\nGD-I [23]\n27.806\n0.3202\nGD-II [23]\n23.675\n0.2796\nCM-II (frozen) [44]\n28.088\n0.3192\nCM-II [44]\n27.810\n0.3172\n4 steps\nPE-CoDi (Ours)\n25.214\n0.2941\nCoDi (Ours)\n19.637\n0.2656\nInpainting (ImageNet)\nSampling Steps\nMethods\nFID\nLPIPS\n1000 steps\nPalette [33]\n13.151\n-\n250 steps\nRepaint [20]\n-\n0.2827\n50 steps\nControlNet [53]\n14.895\n0.2260\n4 steps\nControlNet [29]\n20.205\n0.2635\n+ DPM Solver++ [18]\n19.941\n0.2644\nCM-II [44]\n17.710\n0.2580\nGD-II [23]\n15.95\n0.2452\n4 steps\nPE-CoDi (Ours)\n14.700\n0.2231\nText-guided Depth-to-image (WebLI)\nSampling Steps\nMethods\nFID\nCLIP\n250 steps\nControlNet [53]\n20.884\n0.2910\n4 steps\nControlNet [53]\n29.780\n0.2854\n+ DPM Solver++ [18]\n32.208\n0.2834\nCM-II [44]\n27.640\n0.2869\nGD-II [23]\n26.51\n0.2870\n4 steps\nPE-CoDi (Ours)\n23.047\n0.2874\nTable 2. Quantitative performance comparisons between the baselines and our methods. Our model can achieve comparable performance\nin 4 steps than models sampled in 250 steps. The 4-step sampling results of our parameters-efficient distillation (PE-CoDi) is comparable\nwith the original 8-step sampling results, while PE-CoDi doesn\u2019t sacrifice the original generative performance with frozen backbone.\n5.1. Results\nBaselines.\nWe compare our method with two previous\nSOTA diffusion distillation methods, i.e., consistency mod-\nels (CM) [44] and guided-distillation (GD) [23]. We im-\nplement CM with ControlNet without freezing denoising\nU-Net, which leads to the same network architecture and\nthe same number of parameters as ours. For completeness,\nwe consider two different ways of applying the tested dis-\ntillation techniques, by first making the model conditional\n(fine-tuning first), or by first distilling the model and then\nmaking it conditional (distill first). A summary of the tested\n6\n(a) Depth\n(b) ControlNet\n(c) CoDi (Ours)\nFigure 5. Samples generated according to the depth image (left) from ControlNet sampled in 4 steps (middle), and ours from the uncondi-\ntional pretraining sampled in 4 steps (right). Please see our supplement for many more examples.\nInput\nIP2P (200 steps)\nCoDi (Ours) (1 step)\nmake it sunset\nInput\nIP2P (200 steps)\nCoDi (Ours) (1 step)\nmake it long exposure\nInput\nIP2P (200 steps)\nCoDi (Ours) (1 step)\nmake it low key\nInput\nIP2P (200 steps)\nCoDi (Ours) (1 step)\nmake it sunny\nFigure 6. Generated edited image according to the input image and the instruction (bottom) from Instructed Pix2Pix (IP2P) sampled in 200\nsteps and ours sampled in 1 step. Please see our supplement for many more examples.\nconfigurations is shown in Table 1. Additionally, we com-\npare our method to recently introduced fast ODE solvers,\nincluding DPM-Solver [17] and DPM-Solver++ [18].\nReal-world super-resolution.\nWe evaluate our method\non\nthe\nchallenging\nreal-world\nsuper-resolution\ntask,\nwhere the degradation is simulated using Real-ESRGAN\npipeline [47].\nFollowing StablSR [46], we compare all\nmethods on 3,000 randomly degraded image pairs.\nThe\nquantitative performance is shown in Table 2. The results\ndemonstrate that our distilled method leads to a signifi-\ncant better performance than other distillation techniques.\nOur method achieves better results than fine-tuned diffusion\nmodels that requires 50\u00d7 more sampling setps. Compared\nwith the distilled model by applying the guided-distillation,\nour model outperforms it both quantitatively and qualita-\ntively. The visual comparison presented in Figure. 4 also\ndemonstrates the superiority of our method.\nInpainting. Similar to the above super-resolution compar-\nisons, we demonstrate our method on the inpainting task\nthat conditioned on the masked image, as the quantitative\nperformance shown in Table 2. Similar to Palette [33], we\napply random masks into ImageNet data [32] for both train-\ning and testing. Note that we conduct experiments on the\nup-scaled images in a 512 \u00d7 512 resolution, which is differ-\nent than Palette in 256 \u00d7 256 resolution. Even though we\nevaluate their results in the same resoltuion, their number\ncan only be used for reference.\nDepth-to-image generation.\nIn order to demonstrate the\ngenerality of our method on less informative conditions, we\napply our method in depth-to-image generation. The task\nis usually conducted in parameter-efficient diffusion model\nfinetuning [24, 53], which can demonstrate the capability\nof utilizing text-to-image generation priors. As Figure 5\nillustrated, our distilled model from the unconditional pre-\ntraining can effectively utilize the less informative condi-\ntions and generate matched images with more details.\nInstructed image editing. To demonstrate our conditional\ndistillation capability on text-to-image generation, here we\n7\nMethods\nParams\nFID\nLPIPS\nLDMs\n865M\n29.266\n0.3014\n+ ControlNet\n1.22B\n28.951\n0.3049\nPE-CoDi (Ours)\n364M\n25.214\n0.2941\nCoDi (Ours)\n1.22B\n19.637\n0.2656\n- distilling PF-ODE\n1.22B\n20.307\n0.2733\n- noise-consistency\n1.22B\n25.728\n0.3252\nTable 3. Impact of the network architecture and con-\nditional distillation process, where all methods are\nusing the same 4-step sampling.\n2\n4\n6\n8\nsampling steps\n0.3\n0.4\n0.5\nLPIPS\nrandom initialization\nt2i pretraining\n2\n4\n6\n8\nsampling steps\n25\n50\n75\n100\nFID\nrandom initialization\nt2i pretraining\n2\n4\n6\n8\nsampling steps\n0.30\n0.35\nLPIPS\nmixed t\nsingle t\n2\n4\n6\n8\nsampling steps\n30\n40\nFID\nmixed t\nsingle t\n2\n4\n6\n8\nsampling steps\n0.3\n0.4\n0.5\nLPIPS\nw/o. CG\nw. CG\n2\n4\n6\n8\nsampling steps\n20\n40\n60\n80\nFID\nw/o. CG\nw. CG\nFigure 7. Ablations between alternative settings of our method.\napply our method on text-instructed image editing data [3]\nand compare our conditional distilled model with the In-\nstructPix2Pix (IP2P) model. As the results shown in Fig-\nure 6, our single-step sampling result can achieve compara-\nble visual quality to 200 steps of the IP2P model. We ex-\nperimentally find only small visual difference between the\nresults from our single-step sampling and the 200 steps sam-\npling. We believe this suggests that the effect of the condi-\ntional guidance on distillation correlates with the similarity\nbetween the conditions and the target data, further demon-\nstrating the effectiveness of our method.\n5.2. Ablations\nHere we compare the performance of the aforementioned\ndesigns in our conditional distillation framework. Specifi-\ncally we focus on the representative conditional generation\ntask i.e., real-world super-resolution [48] that conditions on\nthe low-resolution, noisy, blurry images.\nNetwork architecture and distillation process. To elimi-\nnate the impact of the architecture change, we compare our\nmethod with a baseline given by adding a ControlNet mod-\nule trained on super-resolution without freezing the UNet.\nAs Table 3 shows, simply adopting a ControlNet mod-\nule for super-resolution has negligible impact on the per-\nformance. To evaluate the proposed conditional diffusion\nconsistency, we removed the noise consistency term (equa-\ntion 15) and employed the training model in the PF-ODE\ninstead of the frozen one as used in [44] formulation. As\nshown in Table 3, adopting the distillation model PF-ODE\nand noise-space consistency have positive effects on the fi-\nnal results. These comparisons demonstrate the superiority\nof our method without network architecture effects.\nPretraining.\nTo validate the effectiveness of leveraging\npretraining in our model, we compare the results of random\ninitialization with initialization from the pre-trained text-to-\nimage model. As shown in Figure 7, our method outper-\nforms the random initialized counterpart by a large margin,\nthereby confirming that our strategy indeed utilizes the ad-\nvantages of pretraining during distillation instead of simply\nlearning from scratch.\nSampling of zt. We empirically show that the way of sam-\npling zt plays a crucial role in the distillation learning pro-\ncess. Compared with the previous protocol [23, 36] that\nsamples zt in different time t in a single batch, we show\nthat using a consistent time t across different samples in a\nsingle batch leads to a better performance in our targeted 1-\n4 steps. As the comparisons shown in Figure 7, the model\ntrained with a single time t (in a single batch) achieves bet-\nter performance in both the visual quality (i.e., FID) and the\naccuracy (i.e., LPIPS) when the number of evaluations is\nincreasing during inference.\nConditional guidance. In order to demonstrate the impor-\ntance of our proposed conditional guidance (CG) for dis-\ntillation, which is claimed to be capable of regularizing\nthe distillation process during training, we conduct compar-\nisons between the setting of using the conditional guidance\nas r = \u2225x \u2212 \u02c6x\u03b8(zt, c)\u22252\n2 and not using as r = 0. As the re-\nsult shown in Figure 7, the conditional guidance improves\nboth the fidelity of the generated results and visual quality.\nWe further observed that the distillation process will con-\nverge toward over-saturated direction without CG, which\nthus lower the FID metric. In contrast, our model avoids\nsuch a local minimum by using the proposed guidance loss.\n6. Conclusion\nWe introduce a new framework for distilling an uncondi-\ntional diffusion model into a conditional one that allows\nsampling with very few steps. To the best of our knowledge,\nthis is the first method that distills the conditional diffusion\nmodel from the unconditional pretraining in a single stage.\nCompared with previous two-stage distillation and finetun-\ning techniques, our method leads to better quality given the\nsame number of (very few) sampling steps. Our method\nalso enables a new parameter-efficient distillation that al-\nlows different distilled models, trained for different tasks,\nto share most of their parameters. Only a few additional\nparameters are needed for each different conditional gen-\neration task. We believe the method can serve as a strong\npractical approach for accelerating large-scale conditional\ndiffusion models.\n8\n7. Acknowledgments.\nThe authors would like to thank our colleagues Keren Ye\nand Chenyang Qi for reviewing the manuscript and pro-\nviding valuable feedback. We also extend our gratitude to\nShlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh,\nand Han Zhang for their instrumental contributions in fa-\ncilitating the initial implementation of the latent diffusion\nmodels.\nReferences\n[1] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit\nBermano.\nHyperstyle: Stylegan inversion with hypernet-\nworks for real image editing. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, 2022. 3\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al.\nediffi:\nText-to-\nimage diffusion models with an ensemble of expert denois-\ners. ArXiv preprint, 2022. 4\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023. 2, 5, 8\n[4] Mauricio Delbracio and Peyman Milanfar. Inversion by di-\nrect iteration: An alternative to denoising diffusion for image\nrestoration.\nTransactions on Machine Learning Research,\n2023. Featured Certification. 2\n[5] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion\nmodels beat gans on image synthesis. In Advances in Neu-\nral Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, 2021. 3\n[6] Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son\nHua. Hyperinverter: Improving stylegan inversion via hyper-\nnetwork. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2022, New Orleans, LA, USA,\nJune 18-24, 2022, 2022. 3\n[7] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and\nJoshua M Susskind. Boot: Data-free distillation of denoising\ndiffusion models with bootstrapping. In ICML 2023 Work-\nshop on Structured Probabilistic Inference {\\&} Generative\nModeling, 2023. 3\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Informa-\ntion Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual, 2020. 3\n[9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for NLP. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, 2019. 3\n[10] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 2022. 2, 4, 13\n[11] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Mu-\nrata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki\nMitsufuji, and Stefano Ermon. Consistency trajectory mod-\nels: Learning probability flow ode trajectory of diffusion.\nArXiv preprint, 2023. 3\n[12] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 2021. 3\n[13] Diederik P. Kingma and Max Welling. Auto-encoding vari-\national bayes. In 2nd International Conference on Learning\nRepresentations, ICLR 2014, Banff, AB, Canada, April 14-\n16, 2014, Conference Track Proceedings, 2014. 3\n[14] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. NeurIPS, 2023. 2, 12\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 1\n[16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2023. 2\n[17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 2022. 2, 6, 7\n[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver++: Fast solver for guided sam-\npling of diffusion probabilistic models. ArXiv preprint, 2022.\n6, 7\n[19] Haoye Lu, Yiwei Lu, Dihong Jiang, Spencer Ryan Szabados,\nSun Sun, and Yaoliang Yu. Cm-gan: Stabilizing gan train-\ning with consistency models. In ICML 2023 Workshop on\nStructured Probabilistic Inference {\\&} Generative Model-\ning, 2023. 3\n[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and L Repaint Van Gool. Inpainting using\ndenoising diffusion probabilistic models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022. 6\n[21] Eric Luhman and Troy Luhman. Knowledge distillation in\niterative generative models for improved sampling speed.\nArXiv preprint, 2021. 2\n[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided im-\nage synthesis and editing with stochastic differential equa-\ntions.\nIn The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022, 2022. 1\n[23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\n9\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 2023. 2, 4, 6, 8\n[24] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. ArXiv preprint, 2023. 3, 5, 7\n[25] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn Proceedings\nof the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, 2021. 4\n[26] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, 2023. 2\n[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. ArXiv preprint, 2022. 1\n[28] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.\nEfficient parametrization of multi-domain deep neural net-\nworks. In 2018 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, 2018. 3\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 2022. 1, 2, 4, 6\n[30] Amir Rosenfeld and John K Tsotsos. Incremental learning\nthrough deep adaptation. IEEE transactions on pattern anal-\nysis and machine intelligence, (3), 2018. 3\n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023. 2\n[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 2015. 7\n[33] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH 2022 Conference Proceedings, 2022. 2, 6, 7\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 2022. 1, 4\n[35] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence, (4), 2022. 2\n[36] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022, 2022. 2, 3, 4, 8\n[37] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\nArXiv preprint, 2021. 2\n[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 2022. 1, 2\n[39] Jascha\nSohl-Dickstein,\nEric\nA.\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli.\nDeep unsupervised\nlearning using nonequilibrium thermodynamics.\nIn Pro-\nceedings of the 32nd International Conference on Machine\nLearning, ICML 2015, Lille, France, 6-11 July 2015, 2015.\n3\n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models.\nIn 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021, 2021. 3\n[41] Yang Song and Prafulla Dhariwal. Improved techniques for\ntraining consistency models. ArXiv preprint, 2023. 3\n[42] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.\nSliced score matching: A scalable approach to density and\nscore estimation. In Proceedings of the Thirty-Fifth Confer-\nence on Uncertainty in Artificial Intelligence, UAI 2019, Tel\nAviv, Israel, July 22-25, 2019, 2019. 3\n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions.\nIn 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021, 2021. 2, 3\n[44] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. ICML, 2023. 2, 3, 4, 6,\n8, 13\n[45] Asa Cooper Stickland and Iain Murray. BERT and pals: Pro-\njected attention layers for efficient adaptation in multi-task\nlearning. In Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, 2019. 3\n[46] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior for\nreal-world image super-resolution. ArXiv preprint, 2023. 6,\n7\n[47] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In IEEE/CVF International Conference\non Computer Vision Workshops, ICCVW 2021, Montreal,\nBC, Canada, October 11-17, 2021, 2021. 7\n[48] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nRealesrgan: Training real-world blind super-resolution with\npure synthetic data supplementary material. Computer Vi-\nsion Foundation open access, 2022. 5, 6, 8\n[49] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan\nSaharia, Alexandros G Dimakis, and Peyman Milanfar. De-\nblurring via stochastic refinement.\nIn Proceedings of the\n10\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022. 2\n[50] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xing-\nlong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool.\nDiffir: Efficient diffusion model for image restoration. ICCV,\n2023. 2, 6\n[51] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian,\nZiming Liu, and Tommi Jaakkola. Restart sampling for im-\nproving generative processes. ArXiv preprint, 2023. 2\n[52] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.\nResshift:\nEfficient diffusion model for image super-\nresolution by residual shifting. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023. 2\n[53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1, 2, 3, 4, 5, 6, 7\n[54] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Aziz-\nzadenesheli, and Anima Anandkumar. Fast sampling of dif-\nfusion models via operator learning. In International Con-\nference on Machine Learning. PMLR, 2023. 2\n11\nA. Discussion\nLimitations. We have shown image conditions benefit our distillation learning. However, the distillation learning depends\non the adapter architecture that introduces additional computation in our current framework. As a future work, we would\nlike to explore lightweight network architectures [14] in our distillation technique to further reduce the inference latency.\nNevertheless, CoDI\u2019s significantly reduced sampling steps lead to lower latency. See the following table (measured in TPUv5)\nfor a detailed comparison:\nMethod\nCoDi (4step)\nControlNet (4step)\nLDMs (4step)\nLDMs (50step)\nLatency (ms)\n107 \u00b1 3\n107 \u00b1 3\n103 \u00b1 2\n977 \u00b1 1\nEthics statement. The diffusion distillation technique introduce in this work holds the promise of significantly enhancing\nthe practicality of diffusion models in everyday applications such as consumer photography and artistic creation. While we\nare excited about the possibilities this model offers, we are also acutely aware of the possible risks and challenges associated\nwith its deployment. Our model\u2019s ability to generate realistic scenes could be misused for generating deceptive content. We\nencourage the research community and practitioners to prioritize privacy-preserving practices when using our method.\nB. Proofs\nB.1. Notations\nWe use \u02c6v\u03b8(\u00b7, \u00b7) to denote a pre-trained diffusion model that learns the unconditional data distribution x \u223c pdata with param-\neters \u03b8. The signal prediction and the noise prediction transformed by equation 8 are denoted by \u02c6x\u03b8(\u00b7, \u00b7) and \u02c6\u03f5\u03b8(\u00b7, \u00b7), and they\nshare the same parameters \u03b8 with \u02c6v\u03b8(\u00b7, \u00b7).\nB.2. Self-consistency in Noise Prediction\nRemark. If a diffusion model, parameterized by \u02c6v\u03b8(zt, t), satisfies the self-consistency property on the noise prediction\n\u02c6\u03f5\u03b8(zt, t) = \u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt, then it also satisfies the self-consistency property on the signal prediction \u02c6x\u03b8(zt, t) = \u03b1tzt \u2212\n\u03c3t\u02c6v\u03b8(zt, t).\nProof. The diffusion model that satisfies the self-consistency in the noise prediction implies:\n\u02c6\u03f5\u03b8(zt\u2032, t\u2032) = \u02c6\u03f5\u03b8(zt, t),\n\u03b1t\u2032 \u02c6v\u03b8(zt\u2032, t\u2032) + \u03c3t\u2032zt\u2032 = \u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt,\n\u02c6v\u03b8(zt\u2032, t\u2032) = \u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt \u2212 \u03c3t\u2032zt\u2032\n\u03b1t\u2032\n,\n(16)\nBased on the above equivalence, the transformation between the signal prediction x\u03b8(zt\u2032, t\u2032) and x\u03b8(zt, t) by using the\nupdate ruler in equation 7 and the reparameterization trick is:\nx\u03b8(zt\u2032, t\u2032) = \u03b1t\u2032zt\u2032 \u2212 \u03c3t\u2032 \u02c6v\u03b8(zt\u2032, t\u2032)\n= \u03b1t\u2032zt\u2032 \u2212 \u03c3t\u2032 \u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt \u2212 \u03c3t\u2032zt\u2032\n\u03b1t\u2032\n// integrating equation 16\n= \u03b12\nt\u2032zt\u2032 \u2212 \u03c3t\u2032\u03b1t\u02c6v\u03b8(zt, t) \u2212 \u03c3t\u2032\u03c3tzt + \u03c32\nt\u2032zt\u2032\n\u03b1t\u2032\n= (1 \u2212 \u03c32\nt\u2032)zt\u2032 \u2212 \u03c3t\u2032\u03b1t\u02c6v\u03b8(zt, t) \u2212 \u03c3t\u2032\u03c3tzt + \u03c32\nt\u2032zt\u2032\n\u03b1t\u2032\n= zt\u2032 \u2212 \u03c3t\u2032(\u03b1t\u02c6v\u03b8(zt, t) + \u03c3tzt)\n\u03b1t\u2032\n= zt\u2032 \u2212 \u03c3t\u2032(\u02c6\u03f5\u03b8(zt, t))\n\u03b1t\u2032\n// transformed with equation 8\n= \u03b1t\u2032x\u03b8(zt, t) + \u03c3t\u2032\u02c6\u03f5\u03b8(zt, t) \u2212 \u03c3t\u2032(\u02c6\u03f5\u03b8(zt, t))\n\u03b1t\u2032\n// update ruler equation 9 of DDIM\n= x\u03b8(zt, t).\n12\nThe derived equivalence shows that enforcing the self-consistency in the noise prediction, which is implemented by learning\nto minimize our distillation loss in equation 15, enforces the self-consistency in the signal prediction and can distill the\npre-trained diffusion model.\nC. Difference between Consisntecy Models\nAlgorithm 1 Conditional Diffusion Distillation (CDD)\nInput: conditional data (x, c) \u223c pdata, adapted diffusion model \u02c6w\u03b8(zt, c, t), learning rate \u03b7, distance functions d\u03f5(\u00b7, \u00b7)\nand dx(\u00b7, \u00b7), and EMA \u03b3\n\u03b8\u2212 \u2190 \u03b8\n// target network initlization\nrepeat\nSample (x, c) \u223c pdata and t \u223c [\u2206t, T]\n// empirically \u2206t = 1\nSample \u03f5 \u223c N(0, I)\ns \u2190 t \u2212 \u2206t\nSample zt \u2190 \u03b1tx + \u03c3t\u03f5\n- \u02c6xt \u2190 \u03b1tzt \u2212 \u03c3t\u03a6(zt, c, t)\n- \u02c6\u03f5t \u2190 \u03b1t\u03a6(zt, c, t) + \u03c3tzt\n+ \u02c6xt \u2190 \u03b1tzt \u2212 \u03c3t \u02c6w\u03b8(zt, c, t)\n// signal prediction in equation 8\n+ \u02c6\u03f5t \u2190 \u03b1t \u02c6w\u03b8(zt, c, t) + \u03c3tzt\n// noise prediction in equation 8\n\u02c6zs \u2190 \u03b1s\u02c6xt + \u03c3s\u02c6\u03f5t\n// update rule in equation 9\n- \u02c6x\u2032\nt \u2190 \u03b1tw\u03b8(zt, c, t) + \u03c3tzt\n- \u02c6x\u2032\ns \u2190 \u03b1tw\u03b8\u2212(\u02c6zs, c, s) + \u03c3s\u02c6zs\n+ \u02c6\u03f5s \u2190 \u03b1sw\u03b8\u2212(\u02c6zs, c, t) + \u03c3s\u02c6zs\n// noise prediction in equation 8\n- L(\u03b8, \u03b8\u2212) \u2190 dx(\u02c6x\u2032\nt, \u02c6x\u2032\ns)\n+ L(\u03b8, \u03b8\u2212) \u2190 d\u03f5(\u02c6\u03f5t, \u02c6\u03f5s) + dx(x, \u02c6xt)\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03b8\u2212)\n\u03b8\u2212 \u2190 stopgrad(\u03b3\u03b8\u2212 + (1 \u2212 \u03b3)\u03b8)\n// exponential moving average\nuntil convergence\nD. Implementation Details\nSkip Connections.\nWe implement the skip connections as follows, which is same as the consistency models [44] and\nEDMs [10] for satisfying the boundary condition but f\u03d5 could be either the signal prediction or noise prediction:\nf \u2032\n\u03d5(zt, t) = cskip(t)x + cout(t)f\u03d5(zt, t),\n(17)\nwhere\ncskip(t) =\n\u03c3data\nt2 + \u03c32\ndata\n, cout(t) =\n\u03c3datat\np\nt2 + \u03c32\ndata\n.\n(18)\nWe use \u03c3data = 0.5.\n13\nE. Additional results\nLR\nStableSR\nDiffIR\nLDMs (4 steps)\nGD-II (4 steps)\nCM-II (4 steps)\nCoDi (Ours)\nHR\nFigure 8. Visual comparisons of various diffusion-based methods on the simulated real-world super-resolution benchmark. The input of\nall methods is a \u2018Bicubic\u2019-upsampled image.\n14\nInput\nIP2P (200 steps)\nOurs (1 step)\nOurs (4 step)\nmake it sunny\nmake it sunset\nFigure 9. Visual comparisons with the IP2P model and our conditional distilled model.\n15\nInput\nIP2P (200 steps)\nOurs (1 step)\nOurs (4 step)\nmake it long exposure\nmake it lowkey\nFigure 10. Visual comparisons with the IP2P model and our conditional distilled model.\n16\n"
  }
]